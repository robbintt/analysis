---
ver: rpa2
title: 'MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning'
arxiv_id: '2501.14105'
source_url: https://arxiv.org/abs/2501.14105
tags:
- section
- clinical
- notes
- error
- history
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study fine-tuned small open-source large language models (LLMs)
  to extract key sections from clinical notes, focusing on History of Present Illness,
  Interval History, and Assessment and Plan. A dataset of 487 annotated notes was
  used to train three models (Llama 3.2 1B, 3B, and 3.1 8B), which were compared against
  proprietary models (GPT-4o, GPT-4o mini) and rule-based baselines.
---

# MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning

## Quick Facts
- arXiv ID: 2501.14105
- Source URL: https://arxiv.org/abs/2501.14105
- Reference count: 40
- Fine-tuned Llama 3.1 8B achieves F1=0.92 for RCH and 0.94 for A&P, outperforming GPT-4o (F1=0.79 for RCH)

## Executive Summary
This study demonstrates that fine-tuned small open-source large language models can outperform proprietary models for clinical note sectioning. The MedSlice approach uses Llama 3.2 and 3.1 models fine-tuned with rsLoRA on 487 annotated clinical notes to extract History of Present Illness, Interval History, and Assessment and Plan sections. The fine-tuned Llama 3.1 8B achieved F1 scores of 0.92 for Recent Clinical History and 0.94 for Assessment & Plan, surpassing GPT-4o's performance while offering cost-effective, privacy-conscious alternatives for clinical documentation analysis.

## Method Summary
The method involves supervised fine-tuning of Llama instruct models using parameter-efficient rsLoRA on annotated clinical notes, where sections are defined by first and last 5-word boundaries. The models are trained to output JSON containing these 5-word markers, which are then matched to source text using fuzzy Levenshtein distance (>80% similarity). Evaluation uses bootstrap sampling (n=1000, 3 runs) to compute token-level F1 scores, with statistical significance assessed via Friedman test and Wilcoxon post-hoc analysis. The approach combines HPI and Interval History into a single Recent Clinical History label to reduce annotation ambiguity.

## Key Results
- Fine-tuned Llama 3.1 8B achieved F1=0.92 for RCH and 0.94 for A&P
- GPT-4o achieved F1=0.79 for RCH, significantly lower than fine-tuned model
- External validity on UCSF notes showed F1=0.85 for RCH and 0.89 for A&P

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning with parameter-efficient methods enables small models (<8B parameters) to outperform proprietary models on specialized clinical tasks.
- Mechanism: Rank-stabilized LoRA (rsLoRA) adapts model weights for the specific task of identifying section boundaries, learning domain-specific patterns in clinical documentation formatting without full parameter updates.
- Core assumption: The target task is sufficiently narrow that a small fine-tuned model can capture its distribution more precisely than a larger general-purpose model.
- Evidence anchors:
  - [abstract] "Fine-tuned Llama 3.1 8B achieved an F1 score of 0.92, outperforming GPT-4o (F1 = 0.79)"
  - [section] Table D shows Llama 3.1 8B base model achieved only F1=0.53 for RCH and 0.51 for A&P; after SFT, F1 improved to 0.89 and 0.94 respectively
  - [corpus] Weak direct corpus support for rsLoRA specifically; related papers confirm fine-tuning improves clinical LLM performance but don't isolate parameter-efficient methods
- Break condition: If clinical note formats diverge significantly from training distribution (e.g., community hospital notes vs. academic medical centers), fine-tuned advantages may degrade.

### Mechanism 2
- Claim: Fuzzy 5-gram boundary matching enables robust span extraction despite LLM generation variability.
- Mechanism: Rather than requiring exact text generation, the model outputs first and last 5 words of each section; Levenshtein distance-based fuzzy matching (>80% similarity) locates these boundaries in source text, accommodating minor LLM output variations.
- Core assumption: Generated 5-grams will maintain sufficient similarity to source text even with tokenization or transcription variations.
- Evidence anchors:
  - [section] "Due to the generative nature of LLMs, achieving an exact 5-gram match was uncommon... fuzzy matching was employed... Matches with a similarity score exceeding 80% were considered valid"
  - [section] Appendix E documents "Hallucination of index" errors where LLM produces non-existent end phrases, indicating fuzzy matching handles some but not all generation failures
  - [corpus] No direct corpus comparison of fuzzy vs. exact matching for clinical span extraction
- Break condition: If LLM hallucinates boundaries that don't exist in source text (documented in error analysis), fuzzy matching fails to recover.

### Mechanism 3
- Claim: Combined label formulation (Recent Clinical History) reduces annotation ambiguity and improves model learning.
- Mechanism: History of Present Illness and Interval History were merged into a single "recent clinical history" (RCH) label, reducing boundary confusion and providing more training signal per category.
- Core assumption: HPI and Interval History share sufficient semantic similarity that combining them doesn't lose task-relevant distinctions.
- Evidence anchors:
  - [section] "Due to variability in documentation, the history of present illness and interval history were combined into a single label, recent clinical history (RCH)"
  - [section] Codebook (Appendix A) provides detailed inclusion/exclusion criteria for RCH, suggesting careful boundary definition
  - [corpus] Weak corpus support; related papers don't evaluate label consolidation strategies
- Break condition: If downstream applications require distinguishing HPI from Interval History, this consolidation becomes inappropriate.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper uses rsLoRA for efficient fine-tuning; understanding how LoRA decomposes weight updates into low-rank matrices explains why training completes in 20-60 minutes on a single A100.
  - Quick check question: Can you explain why LoRA reduces memory requirements compared to full fine-tuning?

- Concept: **Token-level vs. Span-level Extraction**
  - Why needed here: MedSlice extracts text spans (continuous sections) rather than classifying individual tokens; this affects both the prompting strategy and evaluation methodology (F1 based on span overlap).
  - Quick check question: How would evaluation differ if this were a token classification task instead of span extraction?

- Concept: **Internal vs. External Validity in Clinical NLP**
  - Why needed here: The paper evaluates on same-institution different-population (internal) and different-institution (external) test sets; understanding this distinction is critical for assessing generalization claims.
  - Quick check question: Why might a model show strong internal validity but degraded external validity?

## Architecture Onboarding

- Component map:
  - Data Pipeline: Clinical notes → Annotation (Jaccard threshold ≥80% or adjudication) → RCH + A&P labels
  - Fine-tuning: Llama instruct models + Unsloth library + rsLoRA (rank=16, alpha=16, 5 epochs, LR=2e-4)
  - Inference: vLLM serving → JSON output (5-gram boundaries) → Fuzzy matching (Levenshtein >80%) → Span extraction
  - Evaluation: Bootstrap (n=1000, 3 runs) → Precision/Recall/F1 → Friedman test + Wilcoxon post-hoc

- Critical path:
  1. Annotation quality (Jaccard threshold determines label reliability)
  2. Fine-tuning convergence (monitored via loss on held-out notes)
  3. Fuzzy matching threshold (80% similarity balances recall vs. precision)

- Design tradeoffs:
  - Smaller models (1B/3B) trade ~3-6 F1 points for faster training/inference
  - Combined RCH label simplifies task but loses HPI/Interval History distinction
  - 5-gram boundaries reduce output tokens but require post-processing pipeline

- Failure signatures:
  - **Hallucinated boundaries**: LLM generates 5-grams not found in source text (see Appendix E, "Hallucination of index")
  - **Overprediction**: Model extends beyond section boundaries, especially preceding context
  - **Failed generation**: LLM identifies section but produces no output (7 instances in Neurological notes)

- First 3 experiments:
  1. Reproduce baseline comparison: Run SecTag and MedSpaCy on 20 sample notes to establish expected F1 ranges (target: ~0.19-0.30).
  2. Ablate fuzzy matching threshold: Test 70%, 80%, 90% similarity thresholds on validation set to quantify precision-recall tradeoff.
  3. Cross-institution validation: Train on breast cancer notes, test on GI notes to verify internal validity claims (target F1 drop <0.10 from training performance).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance generalize to clinical notes authored by non-physician/non-NP/non-PA staff (e.g., physical therapists, occupational therapists, nutritionists)?
- Basis in paper: [explicit] Limitations section states the study "focused exclusively on notes authored by physicians, nurse practitioners, and physician assistants, without evaluating notes written by other clinical staff."
- Why unresolved: Different clinical roles may use distinct documentation styles, terminology, and section structures not represented in the training data.
- What evidence would resolve it: Evaluation of the fine-tuned model on a held-out test set of notes from PTs, OTs, and nutritionists, with F1 score comparisons to the current benchmark.

### Open Question 2
- Question: Does model performance transfer to community hospital settings, or is it specific to academic medical centers?
- Basis in paper: [explicit] Limitations section notes "all analyzed notes originated from academic medical centers, limiting the assessment of variability in note styles across different types of hospital systems, such as community hospitals."
- Why unresolved: Community hospitals may have different EHR templates, documentation practices, and note formatting conventions.
- What evidence would resolve it: External validation study on notes from 2-3 community hospitals, comparing F1 scores to the academic center baseline (0.85 external validity F1).

### Open Question 3
- Question: Would incorporating additional section labels (e.g., separating HPI from Interval History, adding ROS, Physical Exam) reduce over/underprediction errors?
- Basis in paper: [explicit] Error analysis section states errors "suggest areas for improvement, such as incorporating additional section labels to enhance discriminatory power."
- Why unresolved: The current binary labeling (RCH, A&P) may lack granularity to distinguish adjacent sections, leading to boundary errors.
- What evidence would resolve it: Ablation study training models with finer-grained labels and measuring change in over/underprediction rates and overall F1.

## Limitations

- Evaluation relies on single annotation standard from one breast cancer center, limiting generalizability to different clinical settings
- Combined RCH label may not be appropriate for downstream tasks requiring distinction between HPI and Interval History
- Use of 80% fuzzy matching threshold introduces evaluation uncertainty without systematic sensitivity analysis

## Confidence

**High Confidence**: Fine-tuned Llama 3.1 8B achieves F1=0.92 for RCH and 0.94 for A&P, significantly outperforming GPT-4o (F1=0.79 for RCH); internal and external validity scores (F1=0.88, 0.85) are statistically significant.

**Medium Confidence**: Small open-source models (1B/3B) achieve competitive F1=0.82-0.85 for RCH; rsLoRA enables efficient training while maintaining performance; combined RCH label reduces annotation ambiguity.

**Low Confidence**: Claims of surpassing proprietary models are based on single task and dataset; generalizability to community hospitals and non-oncology domains is limited; effectiveness of 80% fuzzy matching threshold lacks systematic validation.

## Next Checks

1. **Cross-specialty validation**: Evaluate MedSlice on clinical notes from primary care, emergency department, and outpatient specialty clinics to assess generalizability beyond oncology progress notes. Compare performance degradation patterns across different note types.

2. **Ablation study on fuzzy matching**: Systematically vary the Levenshtein similarity threshold (70%, 80%, 90%, 95%) and document precision-recall tradeoffs. Compare against exact matching and alternative span alignment methods to validate the 80% threshold choice.

3. **Open vs. closed source comparison on diverse datasets**: Test fine-tuned Llama 3.1 8B against GPT-4o on notes from at least three different institutions and clinical specialties, including community hospitals. Measure performance differences in terms of F1 score, hallucination rate, and computational cost to validate cost-effectiveness claims.