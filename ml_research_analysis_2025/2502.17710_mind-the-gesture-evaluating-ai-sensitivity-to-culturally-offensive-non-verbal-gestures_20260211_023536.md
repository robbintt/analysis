---
ver: rpa2
title: 'Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal
  Gestures'
arxiv_id: '2502.17710'
source_url: https://arxiv.org/abs/2502.17710
tags:
- gestures
- offensive
- gesture
- country
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MC-SIGNS, a novel dataset of 288 gesture-country
  pairs spanning 25 gestures and 85 countries, to evaluate AI systems' cultural awareness
  of offensive non-verbal gestures. The dataset is annotated for offensiveness, cultural
  significance, and contextual factors.
---

# Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures

## Quick Facts
- arXiv ID: 2502.17710
- Source URL: https://arxiv.org/abs/2502.17710
- Reference count: 40
- Primary result: Current AI systems exhibit significant US-centric biases in cultural gesture interpretation, over-flagging non-US offensive gestures while performing better on US contexts.

## Executive Summary
This paper introduces MC-SIGNS, a novel dataset of 288 gesture-country pairs spanning 25 gestures and 85 countries, to evaluate AI systems' cultural awareness of offensive non-verbal gestures. The dataset is annotated for offensiveness, cultural significance, and contextual factors. Systematic evaluation reveals that current AI systems exhibit significant limitations: text-to-image models struggle to reject offensive gestures, large language models over-flag gestures as offensive, and vision-language models frequently default to US-centric interpretations. All model types show higher accuracy in identifying US-offensive gestures compared to non-US ones, demonstrating clear US-centric biases. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment.

## Method Summary
The study evaluates three AI model types (text-to-image, large language models, and vision-language models) using MC-SIGNS, a dataset of 288 gesture-country pairs with 1,408 annotations from 268 annotators across 18 UN geoscheme regions. Models are tested using three prompt paradigms: explicit country mentions, country+scene descriptions, and implicit US-meaning descriptions. Performance is measured using recall and specificity for explicit mentions and error rates for implicit mentions. The dataset uses a threshold-based aggregation approach (θGen. Off ≥ 3) rather than majority voting to account for the subjective nature of cultural offensiveness.

## Key Results
- Text-to-image models struggle to reject offensive gestures and perform better in US contexts than non-US ones
- Large language models tend to over-flag gestures as offensive, resulting in high recall (63–99%) but poor specificity (1–61%)
- All model types show higher accuracy in identifying US-offensive gestures compared to non-US ones, demonstrating clear US-centric biases
- Adding scene context reduces T2I safety filter effectiveness, weakening rejection rates from 10.7% to 4.5%

## Why This Works (Mechanism)

### Mechanism 1: US-Centric Training Data Skew
- Training data imbalance leads to stronger neural associations for US cultural patterns, resulting in better calibrated offensiveness detection for US contexts and reliance on US defaults for non-US cultures.

### Mechanism 2: Context Dilution in Safety Filters
- Adding scene descriptions reduces T2I safety filter effectiveness because safety classifiers distribute attention across more features, reducing the weight given to cultural offensiveness signals.

### Mechanism 3: Over-Flagging from Conservative Safety Alignment
- Safety training penalizes missed offensive content more heavily than false positives, leading models to adopt conservative classification thresholds with high recall but poor specificity on culturally-variable content.

## Foundational Learning

- **Cultural context as a conditioning variable**: Essential for understanding that the same gesture can have radically different meanings depending on country/cultural context—the core test of cultural competence. Quick check: Can you explain why "crossed fingers" is positive in the US but potentially offensive in Vietnam?

- **Threshold-based annotation aggregation**: Used instead of majority voting because offensiveness is inherently subjective. Quick check: Why might a threshold-based approach be preferable to majority voting for subjective cultural judgments?

- **Explicit vs. implicit mention evaluation paradigms**: Tests both direct gesture references and US-based meaning descriptions to separate cultural knowledge retrieval from default behavior. Quick check: What does it reveal when a model fails implicit mention tests but passes explicit ones?

## Architecture Onboarding

- **Component map**: MC-SIGNS dataset → Annotation framework (5-level offensiveness scale + confidence + cultural meaning + scenario) → Threshold classifier (θGen. Off ≥ 3) → Three evaluation protocols (Explicit Country, Explicit Country+Scene, Implicit US-meaning) → Model classes tested (T2I, LLMs, VLMs)

- **Critical path**: Gesture-country pair selection → Cultural in-group annotation (5 annotators per pair) → Threshold-based offensiveness labeling → Prompt construction → Metric computation

- **Design tradeoffs**: Regional granularity (UN geoscheme subregions vs. country-level), annotation threshold (lower thresholds capture more harm but increase noise), scene context inclusion (more realistic but degrades performance), GPT-4o as judge (scalable but introduces potential bias)

- **Failure signatures**: High recall + very low specificity (<10%) indicates over-flagging; near-chance performance (~50%) suggests lack of cultural gesture knowledge; large US vs. non-US accuracy gap (>15%) indicates strong US-centric bias; scene context degradation shows safety filter attention dilution

- **First 3 experiments**: 1) Baseline diagnostic: Run all 25 gestures through your model with Country prompts only and compare to reported baselines. 2) US-centric bias probe: Measure accuracy difference between US context and matched non-US contexts for gestures offensive in US but not elsewhere. 3) Implicit mention stress test: For gestures with benign US meanings, prompt with their US-meaning descriptions and target offensive countries to measure default behavior.

## Open Questions the Paper Calls Out

- Would finetuning AI systems on culturally-specific gesture datasets effectively reduce US-centric biases in gesture interpretation and generation?

- Would collecting annotations in annotators' native languages capture cultural nuances in gesture interpretation that are missed by English-only annotations?

- How can AI systems maintain up-to-date awareness of gesture meanings as cultural interpretations evolve over time?

- Would involving local cultural experts in AI model development reduce the observed over-flagging tendency and US-centric bias?

## Limitations

- The dataset uses UN geoscheme subregions rather than country-level granularity, potentially obscuring significant intra-country cultural variations
- All annotations were collected in English, potentially missing cultural nuances that rely on idiomatic or symbolic expressions
- GPT-4o-as-a-judge introduces potential circularity since it may share the US-centric biases being measured
- The study focuses on English-language prompts, limiting generalizability to multilingual contexts

## Confidence

**High Confidence** (based on direct experimental evidence):
- Current AI systems exhibit US-centric bias in gesture interpretation
- LLMs show high recall but very low specificity for offensive gesture detection
- T2I safety filters perform better with explicit gesture references than implicit cultural context
- All model types show higher accuracy for US-offensive gestures compared to non-US ones

**Medium Confidence** (mechanisms inferred from patterns but not directly tested):
- US-centric training data skew explains performance gaps
- Safety filter attention dilution causes context degradation
- Conservative safety alignment drives over-flagging behavior

**Low Confidence** (speculative mechanisms without direct evidence):
- Specific attention distribution patterns in safety filters
- Exact training objective formulations causing over-flagging
- Cultural knowledge representation in model latent spaces

## Next Checks

1. **Cross-Validation with Alternative Judge Models**: Re-run the implicit mention evaluation using multiple judge models (including non-US-based LLMs and human annotators) to verify that GPT-4o-as-a-judge findings are not artifacts of judge model bias.

2. **Temporal Stability Assessment**: Re-annotate a subset of gesture-country pairs after 6-12 months to measure how gesture interpretations change over time, validating the dataset's temporal stability and identifying rapidly evolving cultural meanings.

3. **Multilingual Prompt Evaluation**: Translate the implicit mention prompts into 3-5 major world languages and evaluate the same models to determine whether observed biases persist across linguistic contexts or are primarily English-language artifacts.