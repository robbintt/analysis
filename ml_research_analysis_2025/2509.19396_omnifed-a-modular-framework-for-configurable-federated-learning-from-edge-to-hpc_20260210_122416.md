---
ver: rpa2
title: 'OmniFed: A Modular Framework for Configurable Federated Learning from Edge
  to HPC'
arxiv_id: '2509.19396'
source_url: https://arxiv.org/abs/2509.19396
tags:
- learning
- data
- federated
- training
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniFed, a modular federated learning framework
  that enables flexible, privacy-preserving training across heterogeneous environments
  from edge devices to HPC systems. The framework is designed around separation of
  concerns, allowing users to configure topologies, choose communication protocols,
  and plug in algorithms, privacy mechanisms, and compression strategies via a configuration-driven,
  override-what-you-need approach.
---

# OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC

## Quick Facts
- arXiv ID: 2509.19396
- Source URL: https://arxiv.org/abs/2509.19396
- Reference count: 40
- Primary result: A modular FL framework supporting 10+ algorithms, privacy mechanisms, and compression strategies across edge to HPC environments

## Executive Summary
OmniFed introduces a modular federated learning framework that enables flexible, privacy-preserving training across heterogeneous environments from edge devices to HPC systems. The framework is designed around separation of concerns, allowing users to configure topologies, choose communication protocols, and plug in algorithms, privacy mechanisms, and compression strategies via a configuration-driven, override-what-you-need approach. Key features include support for multiple topologies (centralized, decentralized, hierarchical), mixed communication protocols (MPI, gRPC, MQTT), and built-in privacy tools like differential privacy, homomorphic encryption, and secure aggregation.

## Method Summary
OmniFed implements a modular architecture where training logic is decoupled from system orchestration through a separation of concerns design. The framework uses Hydra for hierarchical YAML configuration, Ray for distributed orchestration, and supports PyTorch as the ML backend. Experiments evaluate 10+ FL algorithms (FedAvg, FedProx, Scaffold, etc.) across four model-dataset combinations (ResNet18/CIFAR10, VGG11/CIFAR100, AlexNet/CalTech101, MobileNetV3/CalTech256) with various compression techniques and privacy mechanisms. The evaluation is conducted on an NVIDIA DGX system with 8x H100 GPUs and 16 clients, measuring test accuracy, epoch completion time, compression overhead, and privacy compute cost.

## Key Results
- Competitive accuracy across 10+ FL algorithms with flexible configuration
- Demonstrated performance trade-offs between compression techniques and model accuracy
- Validated framework's ability to handle diverse FL scenarios from edge to HPC
- Showed significant computational overhead for homomorphic encryption and secure aggregation compared to differential privacy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** OmniFed reduces the friction of prototyping Federated Learning (FL) algorithms by decoupling training logic from system orchestration.
- **Mechanism:** The framework implements a "separation of concerns" architecture where the `Algorithm` module handles learning strategy (e.g., FedAvg, FedProx) via lifecycle hooks, while the `Engine` and `Node` modules manage resource allocation and distributed execution. This allows users to swap algorithms by changing a single line in a configuration file rather than rewriting orchestration code.
- **Core assumption:** The bottleneck for FL researchers is the complexity of distributed systems scaffolding, not the implementation of the mathematical algorithms themselves.
- **Evidence anchors:** Abstract states "...modular framework designed around decoupling and clear separation of concerns..." and section 3.2 notes "With distinct and minimal interfaces... users can add or remove different privacy features... with minimal code change."

### Mechanism 2
- **Claim:** OmniFed enables efficient cross-facility training by abstracting communication protocols to match network topologies.
- **Mechanism:** The `Communicator` module provides a unified API that masks underlying protocols. This allows a single deployment to use high-bandwidth MPI for intra-cluster communication (inner comm) and fault-tolerant gRPC for inter-cluster communication (outer comm) over high-latency networks.
- **Core assumption:** Significant performance gains can be achieved by using specific protocols for specific network links (e.g., ring-allreduce within a cluster, gRPC across facilities) within the same training job.
- **Evidence anchors:** Abstract mentions "...support different topologies, mixed communication protocols within a single deployment..." and section 3.4.5 explains "Aggregation within a site can thus leverage bandwidth-optimal MPI collectives... cross-site communication may use gRPC."

### Mechanism 3
- **Claim:** OmniFed allows for experimental validation of privacy-utility trade-offs by integrating privacy mechanisms as pluggable, configurable modules.
- **Mechanism:** Privacy techniques like Differential Privacy (DP) and Homomorphic Encryption (HE) are implemented as distinct modules invoked during the training/aggregation lifecycle. Users can toggle these features and adjust parameters (e.g., DP epsilon) via YAML configuration, allowing direct comparison of accuracy vs. privacy overhead.
- **Core assumption:** Users accept significant computational overhead (especially for HE and Secure Aggregation) in exchange for theoretical privacy guarantees.
- **Evidence anchors:** Abstract lists "...optional privacy mechanisms including Differential Privacy (DP), Homomorphic Encryption (HE)..." and section 3.4.4 shows Table 3b demonstrating HE and SA carry "significant computational overhead compared to DP."

## Foundational Learning

- **Concept:** **Federated Averaging (FedAvg) and FL Algorithms**
  - **Why needed here:** The paper uses FedAvg as a baseline and implements 10+ variants (FedProx, Scaffold, etc.). Understanding that FL algorithms differ in how they aggregate updates and handle data heterogeneity is essential for using the `Algorithm` module.
  - **Quick check question:** Can you explain how FedProx differs from FedAvg in handling heterogeneous (non-IID) data distributions?

- **Concept:** **Hydra Configuration Framework**
  - **Why needed here:** The entire OmniFed architecture relies on Hydra for hierarchical YAML configuration. Users must understand how to override defaults and structure configs to control experiments without changing code.
  - **Quick check question:** How would you override the default learning rate and batch size in a Hydra config file from the command line?

- **Concept:** **RPC vs. Message Passing (gRPC vs. MPI)**
  - **Why needed here:** OmniFed explicitly supports mixed-protocol communication. Understanding the trade-offs (MPI for speed/RDMA vs. gRPC for internet traversal) is critical for configuring the `Topology` and `Communicator`.
  - **Quick check question:** Why is MPI generally preferred for communication within a single HPC cluster, while gRPC is preferred for communication between different organizations?

## Architecture Onboarding

- **Component map:** Engine -> Topology -> Node -> Communicator -> Algorithm
- **Critical path:**
  1. Define the experiment in a YAML file (selecting model, dataset, algorithm, topology).
  2. Initialize the `Engine`, which parses the config.
  3. The `Engine` spawns `Node` actors based on the `Topology`.
  4. `Nodes` execute the `Algorithm` (local training).
  5. `Nodes` use the `Communicator` to exchange model updates.
  6. Aggregation occurs (either centrally or decentralized), and the cycle repeats.

- **Design tradeoffs:**
  - **Flexibility vs. Simplicity:** The "override-what-you-need" design offers high flexibility but requires learning a complex configuration schema.
  - **Privacy vs. Speed:** Strong privacy (HE/SA) introduces massive latency (Table 3b shows 68s vs 1.45s for ResNet18), making them unsuitable for rapid prototyping on limited hardware.
  - **Compression vs. Accuracy:** Aggressive compression (Topk-1000x) saves bandwidth but drops accuracy significantly (Table 2 shows VGG11 drops from ~86% to ~78%).

- **Failure signatures:**
  - **Silent Convergence Failure:** Algorithm hyperparameters (e.g., proximal term in FedProx) are set incorrectly in the config, leading to flat loss curves.
  - **Communication Deadlock:** Incorrect `Topology` configuration where nodes wait for messages from neighbors that are not connected.
  - **Memory Overflow:** Enabling Homomorphic Encryption on large models without sufficient RAM, as HE expands tensor sizes significantly.

- **First 3 experiments:**
  1. **Baseline Run:** Run the config in Figure 2 (ResNet18, FedAvg, gRPC) to verify the environment and establish a baseline accuracy and epoch time.
  2. **Topology Stress Test:** Switch from `CentralizedTopology` to `HierarchicalTopology` (if implemented/available) or a custom graph to observe the impact on aggregation time and network load.
  3. **Efficiency Analysis:** Enable `TopK` compression (Figure 4) on the `Communicator` and plot the trade-off between communication time reduction and final test accuracy compared to the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OmniFed perform in large-scale, geo-distributed deployments compared to the current single-facility benchmarks?
- Basis in paper: [explicit] The authors state that the "current evaluation setup is limited to controlled single-facility settings" and explicitly list "large-scale FL deployment via Docker and Slurm integration" as future work.
- Why unresolved: The current results are derived from simulations on a single NVIDIA DGX server with 16 clients, which does not validate the framework's ability to handle the network latency, fault tolerance, and orchestration challenges of truly distributed environments.
- What evidence would resolve it: Benchmarks demonstrating convergence time and system throughput across multiple physical sites or cloud regions using the proposed Docker/Slurm orchestration.

### Open Question 2
- Question: Can the computational overhead of Homomorphic Encryption (HE) and Secure Aggregation (SA) be reduced to viable levels for complex models within the framework?
- Basis in paper: [inferred] Table 3b shows that HE and SA introduce massive compute penalties (e.g., VGG11 SA takes 2300s vs 14.4s for DP), leading the authors to acknowledge the "need for further optimizations to accelerate HE and SA."
- Why unresolved: While the framework supports these privacy methods, the current implementation appears too computationally expensive for practical use on deep neural networks, limiting the "configurability" to DP or no privacy for large models.
- What evidence would resolve it: Performance metrics showing optimized implementations of HE/SA (e.g., using hardware acceleration or efficient libraries) achieving competitive training times relative to standard federated averaging.

### Open Question 3
- Question: How will the integration of optimized ML backends like DeepSpeed and ExecuTorch impact the modularity and configuration abstraction of the current framework?
- Basis in paper: [explicit] The authors list plans to "integrate with optimized ML backends like DeepSpeed... and TorchTitan for large models, and ExecuTorch for edge devices."
- Why unresolved: These backends often require specific compilation steps, memory management strategies, or communication hooks that may conflict with OmniFed's current Hydra-based configuration and Ray-based orchestration logic.
- What evidence would resolve it: A demonstration of training a large language model (via DeepSpeed) or an edge model (via ExecuTorch) using OmniFed without requiring users to write custom boilerplate code that breaks the framework's unified API.

## Limitations
- Lack of specification for client data partitioning strategy (Non-IID distribution parameters) critical for exact reproducibility
- Current evaluation limited to single-facility settings without validation of large-scale geo-distributed performance
- Significant computational overhead for homomorphic encryption and secure aggregation makes them impractical for complex models

## Confidence
- **High Confidence:** The modular architecture design and separation of concerns (Mechanism 1) is well-supported by the codebase structure and configuration examples
- **Medium Confidence:** Performance claims for mixed-protocol communication (Mechanism 2) are theoretically sound but lack comprehensive benchmarking across different network conditions
- **Medium Confidence:** Privacy-utility trade-off experiments (Mechanism 3) demonstrate the framework's capability but don't provide thorough cost-benefit analysis across different privacy mechanisms

## Next Checks
1. Implement and test a Dirichlet-based Non-IID data split to verify convergence behavior matches the reported accuracy curves
2. Conduct scaling experiments with >32 clients to identify bottlenecks in the Ray-based orchestration layer
3. Measure the actual serialization overhead of the Communicator abstraction layer by comparing raw protocol performance vs. the unified API implementation