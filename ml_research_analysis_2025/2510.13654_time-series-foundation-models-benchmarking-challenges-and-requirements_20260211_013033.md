---
ver: rpa2
title: 'Time Series Foundation Models: Benchmarking Challenges and Requirements'
arxiv_id: '2510.13654'
source_url: https://arxiv.org/abs/2510.13654
tags:
- archive
- ismail
- fawaz
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies critical evaluation challenges for Time Series
  Foundation Models (TSFMs), including test set contamination and global pattern memorization,
  which arise from multi-purpose dataset use and external shocks like pandemics. It
  analyzes 15 prominent TSFMs and finds widespread confusion over data partitions,
  leading to inflated performance estimates.
---

# Time Series Foundation Models: Benchmarking Challenges and Requirements

## Quick Facts
- **arXiv ID**: 2510.13654
- **Source URL**: https://arxiv.org/abs/2510.13654
- **Reference count**: 27
- **Key outcome**: Analysis of evaluation challenges for TSFMs, including test set contamination and global pattern memorization from external shocks, with quantitative evidence of 7.6-59 percentage points MAPE inflation due to leakage.

## Executive Summary
This paper identifies critical evaluation challenges for Time Series Foundation Models (TSFMs), including test set contamination and global pattern memorization, which arise from multi-purpose dataset use and external shocks like pandemics. The study analyzes 15 prominent TSFMs and finds widespread confusion over data partitions, leading to inflated performance estimates. The authors argue for new, principled benchmarking approaches, such as truly out-of-sample future data evaluation, to safeguard TSFM integrity and ensure fair comparisons.

## Method Summary
The study performs a systematic analysis of 15 prominent TSFMs by examining their training and evaluation dataset usage across major collections (Monash, Informer, GluonTS, LOTSA). The methodology involves mapping dataset lineages, identifying overlaps between pre-training and test sets, and quantifying contamination effects through experiments like the Moirai leakage test. The authors also analyze the impact of global external shocks (e.g., COVID-19) on model performance and propose prospective evaluation frameworks using future data.

## Key Results
- Test set contamination through multi-purpose dataset use inflates performance metrics by 7.6-59 percentage points MAPE
- External shocks like pandemics create shared temporal patterns across datasets, enabling information leakage even with disjoint data
- Dataset remixing and renaming practices obscure data provenance, making it impossible to track which data points have been used for training vs. testing
- Current benchmarks fail to properly distinguish zero-shot forecasting from fine-tuned models

## Why This Works (Mechanism)

### Mechanism 1: Test Set Contamination Through Multi-Purpose Dataset Use
When the same datasets are used across different purposes (pre-training, train/test, zero-shot) for different TSFMs, information leakage occurs. TSFMs can memorize specific time series patterns during pre-training that later appear in test sets, making models appear superior not due to better architecture but because they've already seen the patterns. This occurs because TSFMs have sufficient capacity to memorize specific time series patterns from pre-training data.

### Mechanism 2: Global Pattern Memorization from External Shocks
External shocks like pandemics create shared temporal patterns across diverse datasets, allowing TSFMs to exploit information leakage even when datasets are formally disjoint. When a TSFM is trained on data containing COVID-19 impacts (2020-2021) and evaluated on different datasets from the same period, it memorizes the shock pattern and applies it to test series, gaining an unfair advantage.

### Mechanism 3: Dataset Remixing and Renaming Obscures Lineage
The practice of remixing, renaming, and normalizing datasets across collections makes it virtually impossible to track which data points have been used for training versus testing. Normalization and scaling preserve recognizable patterns that TSFMs can memorize across differently-named datasets, breaking provenance chains and causing models to inadvertently train on test data.

## Foundational Learning

### Concept 1: Time Series Foundation Models vs. Traditional Forecasting Models
**Why needed**: Understanding the fundamental shift from training individual models on single time series to pre-training on massive datasets is essential for grasping why contamination issues arise.
**Quick check**: Can you explain why a traditional ARIMA model trained on electricity demand wouldn't suffer from test set contamination, while a TSFM might?

### Concept 2: Zero-Shot Forecasting in Time Series Context
**Why needed**: Zero-shot in TSFMs means forecasting on unseen time series (different domains/frequencies), not different tasks as in LLMs. This distinction is critical for understanding the evaluation challenges.
**Quick check**: What are three dimensions along which a TSFM must generalize in zero-shot forecasting?

### Concept 3: Time-Based vs. Random Splits in Time Series
**Why needed**: Unlike standard ML, time series requires temporal ordering. A global temporal split point across all series in a collection prevents both direct contamination and global pattern leakage.
**Quick check**: Why can't you randomly sample data points from a time series for your test set?

## Architecture Onboarding

### Component Map
Data Lineage Tracker -> Temporal Split Validator -> Contamination Detector -> Evaluation Framework -> Future Data Pipeline

### Critical Path
1. Start with prospective evaluation framework (future data only)
2. Implement dataset fingerprinting to detect renamed/transformed series
3. Build temporal overlap detection for global pattern identification
4. Create standardized benchmark datasets with clear provenance
5. Deploy live evaluation platform with rolling test windows

### Design Tradeoffs
- **Synthetic vs. Real Data**: Synthetic data guarantees no contamination but may lack real-world relevance
- **Retrospective vs. Prospective**: Retrospective evaluation enables comparison with existing models; prospective ensures no leakage
- **Resource vs. Rigor**: Retraining all TSFMs on standardized data is rigorous but computationally prohibitive
- **Open vs. Closed Models**: Closed-source models cannot be retrained, limiting benchmarking options

### Failure Signatures
1. **Sudden Performance Spike**: Model performs 50%+ better on specific datasets (indicates prior exposure)
2. **Temporal Inconsistency**: Model performs well on 2020 data but poorly on 2019 data (global shock memorization)
3. **Cross-Domain Impossible Accuracy**: Model predicts unrelated domains with suspicious accuracy (dataset overlap)
4. **Normalization Reversal**: Scaled/transformed datasets show identical error patterns to originals

### First 3 Experiments
1. **Lineage Audit**: Trace the training data lineage of 3 TSFMs (Chronos, Moirai, TimesFM) and identify all datasets used for both training and evaluation
2. **Leakage Quantification**: Replicate the Moirai leakage experiment by training a simple TSFM on contaminated vs. clean data and measure MAPE difference
3. **Global Pattern Test**: Evaluate a COVID-aware TSFM on energy datasets from 2020 vs. 2019 to quantify global shock memorization effect

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: What is the specific quantitative impact of global pattern memorization on TSFM performance, and how does this magnitude vary across different domains and types of external shocks?
**Basis**: Section 6 states the magnitude is "largely unknown" and varies by domain and event type.
**Why unresolved**: Training and test sets often span the same time periods (e.g., COVID-19), creating indirect dependencies that inflate performance metrics without direct data overlap.
**What evidence would resolve it**: A controlled study using temporal splits that strictly separate global events in training and test sets, comparing performance against models trained on data stripped of such global shocks.

### Open Question 2
**Question**: How does the effect size of test set contamination scale with model capacity (parameters) and training data volume in TSFMs?
**Basis**: Section 6 notes that negative impact has "only been proven to be significant in isolated cases" like the Moirai experiment.
**Why unresolved**: Lack of systematic cross-model analysis; current evidence is anecdotal or limited to single architectures.
**What evidence would resolve it**: A comprehensive benchmark comparing "clean" vs. "contaminated" performance across a standardized set of TSFMs with varying parameter sizes.

### Open Question 3
**Question**: How can a "future data" evaluation framework be operationalized to effectively distinguish between zero-shot capabilities and fine-tuning while preventing information leakage?
**Basis**: Section 5 proposes a framework based on "continuously advancing global temporal splits" but admits distinguishing zero-shot from fine-tuned forecasts could be difficult.
**Why unresolved**: Logistical and methodological challenges of separating pre-trained (zero-shot) inference from adapted (fine-tuned) inference in a live, evolving benchmark haven't been solved.
**What evidence would resolve it**: The design and validation of a protocol or platform that registers model capabilities (zero-shot vs. fine-tuned) before the future evaluation data exists.

## Limitations

- Limited direct empirical evidence for global pattern memorization effects across multiple external shocks
- Incomplete dataset lineage reconstruction for closed-source models due to lack of pre-training data manifests
- Current benchmarks fail to properly isolate and quantify the magnitude of information leakage effects

## Confidence

- **High confidence**: Test set contamination through multi-purpose dataset use (supported by Moirai leakage experiment showing 7.6-59 percentage points MAPE inflation)
- **Medium confidence**: Global pattern memorization from external shocks (theoretical framework is sound but empirical evidence is limited)
- **Low confidence**: Completeness of dataset lineage reconstruction for closed-source models (may go undetected without access to pre-training data manifests)

## Next Checks

1. **Temporal Split Validation**: Audit all 15 TSFMs to verify they use a global temporal split point rather than random or per-series splits, measuring the extent of global pattern leakage
2. **Prospective Evaluation Deployment**: Implement a live evaluation platform that tests TSFMs on truly future data (e.g., 2024-2025 time series) to establish contamination-free performance baselines
3. **Synthetic Contamination Test**: Create controlled synthetic datasets with known global patterns (e.g., simulated pandemic shocks) and measure TSFM memorization effects across domains