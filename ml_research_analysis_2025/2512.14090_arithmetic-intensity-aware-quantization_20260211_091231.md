---
ver: rpa2
title: Arithmetic-Intensity-Aware Quantization
arxiv_id: '2512.14090'
source_url: https://arxiv.org/abs/2512.14090
tags:
- quantization
- accuracy
- layers
- arxiv
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Arithmetic-Intensity-Aware Quantization (AIQ) is a mixed-precision
  quantization framework that optimizes per-layer bit-widths to maximize arithmetic
  intensity (AI) while minimizing accuracy loss. It treats quantization as a resource-allocation
  problem in the Roofline model space, using search algorithms to find configurations
  that improve AI and throughput on memory-bound models.
---

# Arithmetic-Intensity-Aware Quantization

## Quick Facts
- arXiv ID: 2512.14090
- Source URL: https://arxiv.org/abs/2512.14090
- Reference count: 18
- Primary result: AIQ optimizes per-layer bit-widths to maximize arithmetic intensity while minimizing accuracy loss, achieving ~50% AI gain on ResNet-20 and 1.66× throughput on memory-bound MobileNetV2.

## Executive Summary
Arithmetic-Intensity-Aware Quantization (AIQ) is a mixed-precision quantization framework that treats quantization as a resource-allocation problem in the Roofline model space. It optimizes per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss, particularly for memory-bound workloads. The method naturally quantizes wider and later layers more aggressively, as they contribute more to memory traffic, and uses search algorithms to find configurations that improve AI and throughput.

## Method Summary
AIQ operates on pretrained FP32 models using post-training quantization. It implements greedy and coordinate descent search algorithms over Q={FP32, INT8, INT4} per layer to optimize the objective L(q;λ) = -λ·AI(q) + (1-λ)·AccLoss(q). AI is computed analytically from per-layer FLOPs and memory traffic, while accuracy is measured on small validation subsets. The method assigns bit-widths based on each layer's contribution to total memory traffic and sensitivity to quantization, with wider/later layers typically quantized more aggressively.

## Key Results
- On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over FP32 while keeping accuracy within 1 percentage point
- For memory-bound MobileNetV2 on CPU, AIQ achieves 1.66× higher throughput than FP32 with minimal accuracy loss
- AIQ naturally quantizes wider and later layers more aggressively, as they contribute more to memory traffic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing arithmetic intensity (AI) improves throughput specifically for memory-bound workloads.
- **Mechanism:** Quantization reduces memory traffic while preserving FLOPs, increasing the FLOPs/byte ratio. For kernels below the ridge point in the Roofline model, throughput scales linearly with AI until hitting the compute ceiling.
- **Core assumption:** The target deployment is genuinely memory-bound.
- **Evidence anchors:** Abstract states networks are increasingly memory-bound; Section I.A explains quantization reduces memory traffic; related work confirms LLM token generation is typically memory-bound.
- **Break condition:** Deployment on high-bandwidth hardware where layers are already compute-bound.

### Mechanism 2
- **Claim:** Per-layer bit-width allocation outperforms uniform quantization by exploiting heterogeneous layer sensitivity and memory contribution.
- **Mechanism:** Different layers contribute unequally to total memory traffic and have unequal accuracy sensitivity. Mixed-precision can aggressively quantize high-traffic, low-sensitivity layers while preserving precision elsewhere.
- **Core assumption:** Accuracy sensitivity and memory contribution are not perfectly correlated.
- **Evidence anchors:** Abstract notes AIQ naturally quantizes larger layers more aggressively; Section III.A shows layer-quantization impacts AI differently than accuracy; Section III.E identifies channel count as dominant driver.
- **Break condition:** Architectures where all layers have similar width and sensitivity.

### Mechanism 3
- **Claim:** Joint optimization of AI and accuracy via search algorithms finds near-Pareto-optimal configurations without requiring gradient-based training.
- **Mechanism:** The AIQ loss L(q;λ) = -λAI(q) + (1-λ)AccLoss(q) trades off AI and accuracy. Since AI(q) is analytically computable and AccLoss(q) is measured on small validation sets, greedy or coordinate descent search can efficiently explore the space.
- **Core assumption:** Small validation sets provide reliable accuracy loss estimates.
- **Evidence anchors:** Section I.B notes both algorithms are cheap due to analytical AI computation; Section III.B shows AI-Aware lies on the Pareto frontier; Section III.C demonstrates coordinate descent produces high AI with low accuracy loss.
- **Break condition:** Models with highly non-monotonic accuracy response to quantization.

## Foundational Learning

- **Concept: Roofline Model and Arithmetic Intensity**
  - **Why needed here:** AIQ's core objective is grounded in Roofline theory; understanding memory-bound vs. compute-bound regimes is prerequisite to interpreting results.
  - **Quick check question:** Given a kernel with 100 FLOPs and 50 bytes transferred, what is its AI? If peak bandwidth is 100 GB/s and peak compute is 500 GFLOP/s, is it memory-bound?

- **Concept: Post-Training Quantization (PTQ)**
  - **Why needed here:** AIQ operates on pretrained FP32 models without retraining; PTQ basics clarify what precision reduction does to weights and activations.
  - **Quick check question:** How does uniform INT8 PTQ differ from quantization-aware training (QAT)? What accuracy recovery mechanisms does PTQ typically require?

- **Concept: Mixed-Precision Quantization Search Space**
  - **Why needed here:** AIQ's search operates combinatorially over per-layer bit-widths; understanding search complexity motivates greedy/coordinate descent choices.
  - **Quick check question:** For a 20-layer network with 3 bit-width options (FP32, INT8, INT4), how many configurations exist? Why is exhaustive search impractical?

## Architecture Onboarding

- **Component map:** Profiling module -> Validation evaluator -> Search engine -> Loss aggregator -> Deployment wrapper
- **Critical path:** Profile model layers → Initialize to FP32 → Iterate search (evaluate AI analytically, accuracy empirically) → Select configuration minimizing L(q;λ) → Deploy quantized model
- **Design tradeoffs:**
  - λ selection: Higher λ prioritizes AI gains; lower λ protects accuracy. Paper uses λ=0.9, favoring efficiency.
  - Search algorithm: Greedy is faster per iteration; coordinate descent may find better optima but requires more passes.
  - Bit-width set: Restricting to {INT8, INT4} vs. including FP16 affects granularity vs. search cost.
  - Validation subset size: Smaller sets accelerate search but increase variance in accuracy estimates.
- **Failure signatures:**
  - Accuracy collapse (>5% drop): λ too high; validation set unrepresentative; sensitive layer over-quantized.
  - No throughput gain: Model not memory-bound on target hardware; bandwidth ceiling not reached.
  - Search does not converge: Loss surface highly non-smooth; consider expanding validation set or switching search algorithm.
- **First 3 experiments:**
  1. Single-layer sensitivity sweep: Quantize each layer individually to INT4; plot AI gain vs. accuracy drop per layer to validate heterogeneity assumption.
  2. Greedy vs. coordinate descent comparison: Run both algorithms on ResNet-20/CIFAR-10 with λ=0.9; compare final AI, accuracy, and wall-clock search time.
  3. Memory-bound vs. compute-bound deployment: Deploy AIQ-quantized MobileNetV2 on both bandwidth-limited CPU and high-bandwidth GPU; verify throughput gains only materialize on memory-bound configuration.

## Open Questions the Paper Calls Out
- Extending AIQ to larger architectures including LLMs and larger vision architectures
- Integrating AIQ into quantization-aware training to allow for better accuracy-efficiency tradeoffs
- Studying per-channel effects of quantization

## Limitations
- Method assumes memory-bound deployment; on compute-bound hardware, AI gains will not translate to throughput improvements
- Accuracy search relies on small validation subsets which may poorly represent true accuracy sensitivity
- Paper focuses on CNNs and evaluates on CIFAR-10, leaving open questions about performance on larger-scale vision tasks or NLP models

## Confidence
- **High confidence:** Roofline-based mechanism linking AI gains to throughput on memory-bound kernels is theoretically sound and empirically validated
- **Medium confidence:** Per-layer heterogeneity in quantization sensitivity and memory contribution is demonstrated but requires broader validation
- **Low confidence:** Search efficiency claims lack direct corpus validation; small validation subset assumption for accuracy estimation remains unverified

## Next Checks
1. Deploy AIQ-quantized MobileNetV2 on both memory-bound (2-vCPU Xeon) and compute-bound (GPU) configurations; verify throughput gains only materialize on bandwidth-limited hardware.
2. Implement per-layer sensitivity analysis on ResNet-20 (quantize each layer individually to INT4); confirm wider/later layers are consistently more aggressively quantized.
3. Compare greedy vs. coordinate descent search outcomes on ResNet-20; measure both final AI/accuracy and wall-clock search time to validate algorithm efficiency claims.