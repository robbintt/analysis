---
ver: rpa2
title: 'Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue
  Generation, and Agentic Planning'
arxiv_id: '2511.14445'
source_url: https://arxiv.org/abs/2511.14445
tags:
- mental
- well-being
- health
- dialogue
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tell Me is a mental well-being assistant integrating retrieval-augmented
  generation, synthetic dialogue generation, and agentic planning to offer context-aware,
  reflective support. It addresses data scarcity and the static nature of current
  well-being tools by using open datasets for grounding, generating customizable synthetic
  therapist-client dialogues, and dynamically adapting self-care routines via CrewAI.
---

# Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning

## Quick Facts
- **arXiv ID**: 2511.14445
- **Source URL**: https://arxiv.org/abs/2511.14445
- **Reference count**: 11
- **Primary result**: RAG assistant outperformed non-RAG in clarity and helpfulness (mean ratings 3.8 vs. 3.6) in evaluation with 10 human participants

## Executive Summary
Tell Me is a mental well-being assistant that integrates retrieval-augmented generation (RAG), synthetic dialogue generation, and agentic planning to provide context-aware, reflective support. The system addresses limitations in current well-being tools by using open datasets for grounding, generating customizable synthetic therapist-client dialogues, and dynamically adapting self-care routines through CrewAI. Evaluation using both LLM-as-a-judge and human participants (n=10) found the RAG assistant delivered better empathy and conversational depth, though response times were slightly slower. The system serves as a testbed for safe, interdisciplinary innovation in AI-supported mental well-being.

## Method Summary
Tell Me combines multiple AI techniques to create an interactive mental well-being assistant. The system uses RAG to retrieve relevant information from open datasets for grounding conversations, synthetic dialogue generation to create customizable therapist-client interactions, and agentic planning via CrewAI to dynamically adapt self-care routines. The evaluation involved comparing RAG and non-RAG versions using both LLM-as-a-judge methodology and human participants, with models like Claude and Gemma-3 showing the highest performance for empathy and safety. The system is available as both a live demo and open-source repository.

## Key Results
- RAG assistant outperformed non-RAG in clarity and helpfulness (mean ratings 3.8 vs. 3.6)
- Models such as Claude and Gemma-3 ranked highest for empathy and safety
- Human participants noted better empathy and conversational depth with RAG, despite slightly slower response times
- Retrieval grounding improved reflective dialogue quality, though clinical validation remains needed

## Why This Works (Mechanism)
The system leverages RAG to provide context-aware responses grounded in evidence-based mental health resources, synthetic dialogue generation to create realistic therapeutic interactions, and agentic planning to personalize self-care routines dynamically. This multi-modal approach addresses data scarcity and static limitations of traditional well-being tools by combining real-time information retrieval with adaptive conversation flows and personalized care planning.

## Foundational Learning
- **Retrieval-Augmented Generation (RAG)**: Why needed - to ground responses in evidence-based mental health resources; Quick check - verify retrieval relevance and response accuracy
- **Synthetic Dialogue Generation**: Why needed - to address data scarcity and create customizable therapeutic interactions; Quick check - validate dialogue quality and safety
- **Agentic Planning**: Why needed - to dynamically adapt self-care routines based on user context; Quick check - assess personalization effectiveness and safety
- **LLM-as-a-judge Evaluation**: Why needed - to scale evaluation when human testing is limited; Quick check - validate judge consistency and bias mitigation
- **CrewAI Integration**: Why needed - to coordinate multiple AI agents for complex task management; Quick check - verify task coordination and workflow efficiency
- **Open Dataset Grounding**: Why needed - to provide evidence-based content for responses; Quick check - ensure dataset coverage and accuracy

## Architecture Onboarding

Component Map: User Input -> RAG Engine -> Synthetic Dialogue Generator -> Agentic Planner (CrewAI) -> Response Generator -> User Output

Critical Path: User query → RAG retrieval → Dialogue synthesis → Care plan adaptation → Response delivery

Design Tradeoffs: RAG improves accuracy but adds latency; synthetic dialogues enhance customization but require safety validation; agentic planning increases personalization but adds complexity

Failure Signatures: RAG failures cause irrelevant responses; dialogue generation issues produce unnatural interactions; agentic planning errors result in inappropriate care recommendations

Three First Experiments:
1. Compare response quality and latency between RAG and non-RAG versions with controlled queries
2. Test synthetic dialogue generation across different mental health scenarios for safety and coherence
3. Evaluate agentic planner's ability to adapt self-care routines based on simulated user progress

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Small human participant sample (n=10) limits generalizability of results
- No clinical validation conducted, leaving safety and therapeutic efficacy unproven
- Reliance on LLM-as-a-judge methodology introduces potential bias in evaluation

## Confidence

High confidence: Technical implementation of RAG, synthetic dialogue generation, and agentic planning components

Medium confidence: Comparative performance metrics between RAG and non-RAG versions based on current evaluation

Low confidence: Clinical safety, therapeutic efficacy, and real-world deployment readiness

## Next Checks
1. Conduct a randomized controlled trial with a larger, diverse participant pool (n≥100) including clinical outcomes and standardized well-being measures
2. Perform adversarial testing with mental health professionals to identify potential safety failures, harmful advice patterns, or cultural blindspots
3. Implement and evaluate a longitudinal deployment study tracking user engagement, reported outcomes, and adverse events over 3-6 months of actual use