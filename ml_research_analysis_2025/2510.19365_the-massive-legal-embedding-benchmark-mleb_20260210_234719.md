---
ver: rpa2
title: The Massive Legal Embedding Benchmark (MLEB)
arxiv_id: '2510.19365'
source_url: https://arxiv.org/abs/2510.19365
tags:
- legal
- contractual
- clause
- retrieval
- provision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Massive Legal Embedding Benchmark (MLEB),
  the largest and most diverse open-source benchmark for legal information retrieval.
  MLEB addresses the limitations of existing legal benchmarks, which are often narrow
  in scope, jurisdiction, and document types.
---

# The Massive Legal Embedding Benchmark (MLEB)

## Quick Facts
- arXiv ID: 2510.19365
- Source URL: https://arxiv.org/abs/2510.19365
- Reference count: 25
- Largest and most diverse open-source benchmark for legal information retrieval

## Executive Summary
This paper introduces the Massive Legal Embedding Benchmark (MLEB), the largest and most diverse open-source benchmark for legal information retrieval. MLEB addresses the limitations of existing legal benchmarks, which are often narrow in scope, jurisdiction, and document types. It consists of ten expert-annotated datasets spanning six jurisdictions (US, UK, EU, Australia, Ireland, Singapore) and five document types (cases, legislation, regulatory guidance, contracts, literature), covering three task types (search, zero-shot classification, question answering). Seven of the datasets were newly constructed to fill domain and jurisdictional gaps. The benchmark was evaluated using 21 embedding models, with Isaacus' Kanon 2 Embedder achieving the highest NDCG@10 score of 86.03, followed by Voyage 3 Large (85.71) and Voyage 3.5 (84.07). The results demonstrate that legal domain adaptation significantly improves performance, with models optimized for legal information retrieval outperforming general-purpose models. MLEB, its datasets, and evaluation code are publicly released under open-source licenses.

## Method Summary
MLEB evaluates embedding models for legal information retrieval using 10 expert-annotated datasets spanning six jurisdictions and five document types. The benchmark covers three task types: search/retrieval, zero-shot classification, and question answering. Models are assessed using NDCG@10 as the primary metric, with scores aggregated across task and domain averages. The evaluation includes 21 embedding models ranging from commercial APIs to open-source options. Seven datasets were newly constructed to address gaps in domain and jurisdictional coverage. Evaluation uses batch size of 16 for documents and 1 for queries, with special handling for long sequences on Voyage AI models.

## Key Results
- Isaacus' Kanon 2 Embedder achieved the highest NDCG@10 score of 86.03 across all models
- Legal domain-adapted models consistently outperformed general-purpose models
- Models optimized for legal IR showed superior performance despite being older or smaller than competitors
- Judicial tasks generally scored lower than regulatory tasks across all models

## Why This Works (Mechanism)

### Mechanism 1: Legal Domain Adaptation Transfer
- Claim: Models pretrained and/or finetuned on legal documents achieve higher performance on legal IR tasks than general-purpose models.
- Mechanism: Domain-adapted models develop representations that better capture legal semantics, terminology, and reasoning patterns specific to legal text structures.
- Core assumption: Legal text contains domain-specific semantic and syntactic patterns that differ meaningfully from general text corpora.
- Evidence anchors: [abstract] "The results demonstrate that legal domain adaptation significantly improves performance, with models optimized for legal information retrieval outperforming general-purpose models."

### Mechanism 2: Benchmark Diversity-Generalization Link
- Claim: Multi-jurisdiction, multi-document-type benchmarks provide more predictive signals for real-world legal IR performance than narrow benchmarks.
- Mechanism: Combining diverse evaluation tasks (search, classification, QA) and document types (cases, legislation, contracts, guidance) tests embedding robustness across retrieval scenarios practitioners actually encounter.
- Core assumption: Real-world legal information needs span multiple document types and reasoning modes beyond contracts.
- Evidence anchors: [abstract] "MLEB addresses the limitations of existing legal benchmarks, which are often narrow in scope, jurisdiction, and document types."

### Mechanism 3: Expert Annotation Quality Effect
- Claim: Expert-annotated query-passage pairs provide more reliable evaluation signals than automated construction methods.
- Mechanism: Legal subject matter experts can identify semantically relevant relationships that automated citation-based methods miss or mislabel.
- Core assumption: Accurately assessing legal relevance requires domain expertise that automated methods cannot replicate.
- Evidence anchors: [abstract] "MLEB consists of ten expert-annotated datasets."

## Foundational Learning

- Concept: **Embedding Models and Semantic Similarity**
  - Why needed here: MLEB evaluates embedding models that convert documents and queries into numerical vectors; understanding how cosine similarity enables semantic matching beyond keywords is essential.
  - Quick check question: Can you explain why two documents with no shared keywords might still have high embedding similarity?

- Concept: **NDCG@10 (Normalized Discounted Cumulative Gain)**
  - Why needed here: All MLEB results are reported as NDCG@10 scores; interpreting these requires understanding position-weighted relevance.
  - Quick check question: Why does NDCG penalize a relevant result at position 10 more than the same result at position 1?

- Concept: **RAG Pipeline and Retrieval Quality**
  - Why needed here: The paper explicitly links embedding quality to RAG response quality and hallucination rates.
  - Quick check question: In a RAG system, how does retrieval failure at the embedding stage propagate to generation errors?

## Architecture Onboarding

- Component map: 10 datasets across 3 domains: Judicial (4), Contractual (3), Regulatory (3) -> 6 jurisdictions: US, UK, EU, Australia, Ireland, Singapore -> 3 task types: retrieval/search, zero-shot classification, question answering -> Primary metric: NDCG@10, with domain-averaged and task-averaged breakdowns -> 21 evaluated models spanning commercial APIs and open-source options

- Critical path:
  1. Identify your target jurisdiction(s) and document type(s) from Table 1
  2. Review domain-specific performance (Table 2) - note judicial tasks generally score lower than regulatory across all models
  3. Check inference time vs. accuracy tradeoffs (Figure 2) against your latency budget
  4. Validate on your specific documents before production deployment

- Design tradeoffs:
  - **Legal-adapted vs. general models**: Voyage Law 2 outperforms Text Embedding 3 Large despite being older/smaller, but may lack multilingual capabilities
  - **Model scale vs. speed**: Qwen3 8B scores 5.8 NDCG points higher than Qwen3 0.6B; quantify acceptable latency before selecting
  - **API vs. self-hosted**: Commercial APIs (Voyage, Jina, Google) may have terms requiring data sharing for training, potentially contaminating benchmark results
  - **Jurisdiction alignment**: Models may perform differently across legal systems; Table 2 aggregates domains but not individual jurisdictions

- Failure signatures:
  - **MTEB-MLEB inversion**: Gemini Embedding ranks 1st on MTEB but 7th on MLEB; relying solely on general benchmarks for legal applications will overestimate performance
  - **Domain-specific collapse**: EmbeddingGemma scores 66.85 on judicial vs. 85.32 on regulatory (18.5 point gap) - single aggregate scores hide critical weaknesses
  - **Contract overfitting**: Models optimized only for contract tasks may fail on case law retrieval

- First 3 experiments:
  1. Run the official MLEB evaluation code (github.com/isaacus-dev/mleb) on your current embedding model using the dataset(s) closest to your target domain
  2. A/B test a legal-adapted model (Voyage Law 2 or Kanon 2 if accessible) against your baseline on a held-out sample of your own legal documents, measuring precision@5 and recall@10
  3. Evaluate cross-jurisdiction robustness by testing your selected model on at least one MLEB dataset outside your primary jurisdiction to quantify generalization gaps

## Open Questions the Paper Calls Out

- To what degree does potential data leakage affect the reliability of the benchmark rankings for commercial models?
  - Basis: Section 4.3 notes that Voyage AI, Jina, and Google terms of service may allow user data to be used for training, creating a "potential for data leakage."
  - Why unresolved: The authors cannot audit the internal training data of these proprietary models to verify if MLEB data was seen during training.
  - What evidence would resolve it: Re-evaluating the specific model versions using a hold-out test set of newly constructed legal queries released after the model's training cutoff.

- How effectively do embedding models optimized for common law jurisdictions in MLEB generalize to civil law systems?
  - Basis: Section 2.2 argues that legal systems often differ fundamentally, yet MLEB focuses largely on Anglosphere and EU jurisdictions.
  - Why unresolved: While the benchmark includes diverse jurisdictions, it excludes many major civil law systems, leaving their specific retrieval performance characteristics untested.
  - What evidence would resolve it: Extending the benchmark to include expert-annotated datasets from civil law jurisdictions (e.g., France, Germany, Japan) and comparing model performance drops.

## Limitations

- The benchmark lacks inter-annotator agreement metrics for expert-annotated datasets, making annotation quality assessment impossible
- Aggregated scoring masks jurisdiction-specific performance gaps that only emerge in per-jurisdiction analysis
- The study doesn't investigate model robustness to adversarial or malformed legal queries critical for real-world applications

## Confidence

**High Confidence**: The claim that legal domain-adapted models outperform general-purpose models is strongly supported by consistent performance patterns across all 21 evaluated models, with Kanon 2 Embedder and Voyage Law 2 demonstrating clear advantages on legal-specific tasks.

**Medium Confidence**: The assertion that expert-annotation provides more reliable evaluation signals than automated methods is plausible based on qualitative observations but lacks quantitative comparison or agreement statistics.

**Medium Confidence**: The conclusion that multi-jurisdiction, multi-document-type benchmarks better predict real-world performance is reasonable given the diversity of legal information needs, but the aggregated scoring approach may obscure jurisdiction-specific weaknesses.

## Next Checks

1. **Inter-annotator Reliability Assessment**: Compute and report Fleiss' kappa or similar agreement metrics across all expert-annotated datasets to quantify annotation quality and identify potential bias sources.

2. **Per-Jurisdiction Performance Analysis**: Recompute and report model rankings separately for each jurisdiction (US, UK, EU, Australia, Ireland, Singapore) to identify cross-jurisdictional generalization gaps and inform deployment decisions.

3. **Adversarial Query Robustness Testing**: Design and evaluate a test suite of malformed, ambiguous, or adversarial legal queries to assess model robustness beyond the standard benchmark, measuring precision@5 degradation under stress conditions.