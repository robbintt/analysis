---
ver: rpa2
title: 'RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive
  Benchmark'
arxiv_id: '2509.24897'
source_url: https://arxiv.org/abs/2509.24897
tags:
- generation
- understanding
- unified
- tasks
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RealUnify, the first benchmark designed
  to evaluate bidirectional capability synergy between visual understanding and generation
  in unified multimodal models. Unlike prior work that assesses understanding and
  generation in isolation or only in one direction, RealUnify includes 1,000 human-annotated
  instances across 10 categories and 32 subtasks, structured around two axes: Understanding
  Enhances Generation (UEG) and Generation Enhances Understanding (GEU).'
---

# RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark

## Quick Facts
- arXiv ID: 2509.24897
- Source URL: https://arxiv.org/abs/2509.24897
- Authors: Yang Shi, Yuhao Dong, Yue Ding, Yuran Wang, Xuanyu Zhu, Sheng Zhou, Wenting Liu, Haochen Tian, Rundong Wang, Huanqian Wang, Zuyan Liu, Bohan Zeng, Ruizhe Chen, Qixun Wang, Zhuoran Zhang, Xinlong Chen, Chengzhuo Tong, Bozhou Li, Chaoyou Fu, Qiang Liu, Haotian Wang, Wenjing Yang, Yuanxing Zhang, Pengfei Wan, Yi-Fan Zhang, Ziwei Liu
- Reference count: 18
- Unified models score only 37.5% on UEG tasks and struggle with bidirectional capability synergy

## Executive Summary
This paper introduces RealUnify, the first benchmark designed to evaluate bidirectional capability synergy between visual understanding and generation in unified multimodal models. Unlike prior work that assesses understanding and generation in isolation or only in one direction, RealUnify includes 1,000 human-annotated instances across 10 categories and 32 subtasks, structured around two axes: Understanding Enhances Generation (UEG) and Generation Enhances Understanding (GEU). A dual-evaluation protocol combines direct end-to-end assessment with stepwise decomposition to isolate performance bottlenecks. Experiments with 12 unified models and 6 specialized baselines show that unified models perform poorly on both UEG (average 37.5%) and GEU tasks, indicating that architectural unification alone does not enable effective capability synergy. Stepwise evaluation reveals that while models possess individual understanding and generation skills, they struggle to integrate them seamlessly in complex tasks, suggesting the need for improved training strategies and inductive biases.

## Method Summary
RealUnify employs a dual-evaluation protocol to assess bidirectional capability synergy in unified multimodal models. The benchmark includes 1,000 human-annotated instances across 10 categories and 32 subtasks, split between Understanding Enhances Generation (UEG) and Generation Enhances Understanding (GEU) tracks. Direct evaluation tests end-to-end integration capability, while stepwise decomposition explicitly separates understanding and generation phases to identify bottlenecks. UEG tasks use polling questions to verify generated images, while GEU tasks use multiple-choice questions. Performance is measured using Gemini 2.5 Pro as an external judge, with human evaluation confirming alignment.

## Key Results
- Unified models achieve only 37.5% average accuracy on UEG tasks, significantly below the 72.7% oracle upper bound
- Stepwise evaluation reveals models possess individual understanding and generation capabilities but fail to integrate them seamlessly
- UEG performance improves from 32.7% to 47.7% when tasks are decomposed, confirming knowledge exists but isn't accessed end-to-end
- GEU tasks degrade when decomposed, suggesting models default to understanding shortcuts rather than leveraging generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified models possess latent understanding and generation capabilities but fail to spontaneously integrate them in end-to-end tasks.
- Mechanism: Stepwise decomposition forces explicit knowledge articulation before generation, bypassing integration bottlenecks. Models perform significantly better when capabilities are externally scaffolded rather than internally unified.
- Core assumption: Knowledge required for complex tasks exists within model weights but is inaccessible during end-to-end inference due to weak cross-capability pathways.
- Evidence anchors: Abstract states stepwise evaluation reveals models "struggle to integrate them seamlessly in complex tasks"; Section 4.1 shows BAGEL improves from 32.7% to 47.7% when decomposed; corpus evidence from "Turning Internal Gap into Self-Improvement" confirms widespread internal gaps.

### Mechanism 2
- Claim: GEU tasks expose a "generation for reasoning" deficit where models default to understanding shortcuts rather than leveraging generative simulation.
- Mechanism: When forced to generate intermediate visual states, models produce outputs that degrade rather than enhance downstream reasoning. The generative capability is insufficiently precise for analytical purposes.
- Core assumption: Effective generative simulation requires fidelity levels beyond current unified models can produce.
- Evidence anchors: Section 4.1 shows decomposing GEU tasks causes performance to degrade; Section 5 states adapting generative capabilities for practical problem-solving remains challenging; limited direct corpus evidence available.

### Mechanism 3
- Claim: Architectural unification alone is insufficient without training strategies that explicitly reinforce cross-capability transfer.
- Mechanism: Current unified architectures co-locate understanding and generation modules without inducing functional synergy. The inductive biases necessary for capabilities to mutually enhance each other are missing from standard training objectives.
- Core assumption: Synergy requires explicit training signals that reward successful capability integration, not merely exposure to both task types.
- Evidence anchors: Abstract states "architectural unification alone is insufficient"; Section 4.2 shows oracle model combining specialist models achieves 72.7% vs. unified model at 37.5%; GapEval similarly investigates whether capabilities are genuinely integrated.

## Foundational Learning

- Concept: **Dual-task decomposition analysis**
  - Why needed here: RealUnify's diagnostic value comes from comparing direct vs. stepwise performance. Understanding this pattern reveals whether failure stems from weak individual capabilities or integration deficits.
  - Quick check question: If a model scores 30% on direct evaluation but 45% on stepwise evaluation, what does this indicate about its capabilities?

- Concept: **Synergy vs. coexistence in multimodal models**
  - Why needed here: The paper distinguishes between models that merely perform both tasks (coexistence) versus models where capabilities genuinely enhance each other (synergy). This distinction is central to the benchmark's design.
  - Quick check question: How would you design an experiment to distinguish between functional coexistence and genuine capability synergy?

- Concept: **Oracle-based upper bound estimation**
  - Why needed here: The paper uses specialist model combinations to estimate potential performance ceilings. This technique quantifies the gap between current unified models and theoretically achievable synergy.
  - Quick check question: Why is combining the best understanding model with the best generation model a useful baseline, even if it's not a practical deployment approach?

## Architecture Onboarding

- Component map:
  - **UEG pipeline**: Text prompt → Understanding/reasoning module → Generation module → Image output → Polling evaluation (Gemini-2.5-Pro judge)
  - **GEU pipeline**: Disordered/complex image → Generation/reconstruction module → Enhanced image → Understanding module → Answer selection
  - **Stepwise protocol**: Explicit decomposition with intermediate text (UEG) or image (GEU) artifacts that can be manually inspected

- Critical path:
  1. Task categorization into UEG vs. GEU determines evaluation direction
  2. Direct evaluation tests end-to-end integration capability
  3. Stepwise evaluation isolates bottleneck: if performance improves, integration is the problem; if it degrades, individual capabilities are insufficient for subtask decomposition

- Design tradeoffs:
  - Direct evaluation reflects realistic deployment but cannot diagnose failure sources
  - Stepwise evaluation is diagnostic but introduces distribution shift (models not trained for explicit decomposition)
  - Polling evaluation with external judge (Gemini-2.5-Pro) provides reliable verification but adds dependency; human evaluation confirms alignment (Table 4)

- Failure signatures:
  - **UEG failures**: Models generate plausible but incorrect images (e.g., "lightsaber" → "blaster rifle"); stepwise helps when knowledge exists but isn't accessed
  - **GEU failures**: Generated intermediate images introduce artifacts that mislead understanding; models default to ignoring generated content
  - **Common patterns**: Attribute entanglement, quantity errors, spatial relationship confusion (Figures 11-12)

- First 3 experiments:
  1. **Baseline establishment**: Run your unified model on RealUnify using direct evaluation only; compare against reported benchmarks (BAGEL: 35.3% total, Nano Banana: 50.5%) to position your model
  2. **Bottleneck diagnosis**: Implement stepwise evaluation on a 100-sample subset; compute gap between direct and stepwise performance to identify whether integration or individual capability is the primary limitation
  3. **Failure mode analysis**: Manually inspect 20 failed UEG cases where stepwise evaluation succeeded; categorize whether failures stem from reasoning errors (understanding phase) or generation fidelity issues (generation phase) to guide targeted improvements

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific training paradigms or objective functions can enable unified models to spontaneously integrate understanding and generation end-to-end, eliminating the performance gap between direct and stepwise evaluation?
- **Basis in paper:** The authors find that models like BAGEL significantly improve on Understanding Enhances Generation (UEG) tasks when evaluated stepwise (32.7% to 47.7%), proving they "possess the required knowledge but cannot seamlessly integrate it" (p. 3, 7).
- **Why unresolved:** Current architectures appear to treat understanding and generation as separate, co-located functions rather than integrated processes, preventing the model from leveraging its own reasoning to guide generation without external decomposition.
- **What evidence would resolve it:** A training methodology that allows a unified model to match or exceed the performance of its own stepwise decomposition baseline on the RealUnify UEG benchmark.

### Open Question 2
- **Question:** Why does explicit decomposition degrade performance in Generation Enhances Understanding (GEU) tasks, and how can models be trained to leverage generation for reasoning rather than defaulting to understanding shortcuts?
- **Basis in paper:** The paper notes a "revealing dissociation" where decomposing GEU tasks causes performance to degrade, "suggesting that models default to relying on understanding shortcuts rather than effectively leveraging generation" (p. 3, 8).
- **Why unresolved:** The mechanism for "mental simulation" or "thinking with images" is missing; models currently treat generation as a task rather than a tool to aid reasoning, leading them to ignore helpful visual reconstruction when forced to generate.
- **What evidence would resolve it:** Identifying a training signal or architecture that results in higher accuracy for stepwise "generation-then-understanding" protocols compared to direct inference on GEU tasks.

### Open Question 3
- **Question:** What inductive biases are required to bridge the performance gap between unified models and the "oracle" upper bound (specialist pipelines)?
- **Basis in paper:** The authors show that an oracle model combining Gemini-2.5-Pro and GPT-Image-1 achieves 72.7% on UEG tasks, establishing a high upper bound that proves "architectural unification alone is insufficient" (p. 3, 9).
- **Why unresolved:** It is unclear if the failure to reach the oracle bound is due to parameter interference in unified weights, lack of specialized reasoning pathways, or simply insufficient training data density for synergetic tasks.
- **What evidence would resolve it:** The proposal of specific architectural inductive biases (e.g., specialized attention mechanisms for cross-modal reasoning) that allow a single unified model to approach the 72.7% benchmark.

## Limitations

- The evaluation framework depends heavily on external judge models (Gemini 2.5 Pro), creating a potential single point of failure despite human evaluation confirmation
- Stepwise evaluation protocol introduces distribution shift since models are not trained to produce explicit intermediate artifacts
- The corpus evidence supporting GEU mechanism explanations is notably weaker than for other mechanisms

## Confidence

**High Confidence (8-10/10)**: The core finding that unified models underperform specialized baselines on both UEG (37.5% vs. 72.7% oracle) and GEU tasks is well-supported by direct experimental evidence. The stepwise decomposition revealing integration bottlenecks is methodologically sound and produces consistent results across multiple model families.

**Medium Confidence (5-7/10)**: The interpretation that architectural unification alone is insufficient without explicit training for cross-capability transfer is reasonable but requires additional validation. The Oracle model comparison demonstrates large performance gaps, but alternative explanations cannot be fully ruled out.

**Low Confidence (2-4/10)**: The specific mechanisms explaining GEU failures (generation introducing noise that degrades reasoning) have limited direct evidence. The claim that models "default to understanding shortcuts" rather than leveraging generated content requires more systematic investigation.

## Next Checks

1. **Judge Model Ablation**: Evaluate a subset of RealUnify instances using multiple judge models (e.g., Claude-3.5-Sonnet, GPT-4o) to quantify inter-judge agreement and identify potential systematic biases in the current Gemini 2.5 Pro evaluation.

2. **Stepwise Protocol Refinement**: Develop a fine-tuning procedure that explicitly trains models to produce stepwise intermediate artifacts, then re-evaluate performance to distinguish between architectural limitations and lack of exposure to decomposition tasks.

3. **Generation Fidelity Analysis**: Conduct controlled experiments varying generation quality parameters (temperature, guidance scale, decoding strategy) on GEU tasks to quantify the relationship between generative fidelity and downstream understanding performance, isolating whether quality degradation or fundamental architectural constraints drive GEU failures.