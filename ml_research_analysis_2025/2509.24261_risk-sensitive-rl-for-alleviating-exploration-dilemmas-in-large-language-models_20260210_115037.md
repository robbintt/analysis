---
ver: rpa2
title: Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models
arxiv_id: '2509.24261'
source_url: https://arxiv.org/abs/2509.24261
tags:
- pass
- arxiv
- policy
- risk-sensitive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the exploration dilemma in Reinforcement Learning
  with Verifiable Rewards (RLVR) for Large Language Models (LLMs). Existing methods
  suffer from sharp initial policy distributions that lead to convergence around limited
  solutions, improving single-solution accuracy (pass@1) but reducing solution diversity
  and multi-solution performance (pass@k).
---

# Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models

## Quick Facts
- arXiv ID: 2509.24261
- Source URL: https://arxiv.org/abs/2509.24261
- Reference count: 40
- One-line primary result: Risk-Sensitive GRPO improves pass@k while maintaining pass@1 on mathematical reasoning benchmarks

## Executive Summary
This paper addresses the exploration dilemma in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models (LLMs), where existing methods converge around limited solutions and reduce solution diversity. The authors propose a Risk-Sensitive Reinforcement Learning framework that employs a risk-seeking objective interpolating between mean and maximum rewards, leading to a novel algorithm called Risk-Sensitive GRPO (RS-GRPO). This method drives deeper exploration by amplifying learning from challenging prompts through exponential advantage re-weighting, requiring only minor code modifications as a drop-in replacement for standard RL pipelines.

## Method Summary
The paper introduces RS-GRPO, which modifies the standard advantage calculation in GRPO by replacing the linear advantage function with an exponential risk-seeking formulation. The method computes a risk-sensitive advantage using the formula Â_β(yi) = (1/β)(e^{βr(yi)} / mean(e^{βr(yj)}) − 1), where β controls the risk-seeking behavior. This approach re-weights the optimization focus toward challenging prompts where the model is currently failing, rather than exploiting inputs where it already succeeds. The algorithm uses binary rewards from Math_Verify, dynamic sampling with DAPO, and maintains a non-zero gradient signal even for high-accuracy prompts to preserve Pass@1 performance while improving diversity.

## Key Results
- RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy across six mathematical reasoning benchmarks
- For several models, standard GRPO underperforms the base model at high k values, while RS-GRPO surpasses it, demonstrating expanded exploratory boundaries
- RS-GRPO finds more unique solutions and successfully transitions accuracy on previously unsolved prompts
- The method requires only minor code modifications as a drop-in replacement for standard RL pipelines

## Why This Works (Mechanism)

### Mechanism 1: Exponential Advantage Re-weighting
Standard RL uses a risk-neutral objective (optimizing mean reward). RS-GRPO employs an exponential utility function J_RS ≈ (1/β)log E[e^{βr}]. As β increases, the objective transforms from optimizing mean reward to approximating maximum reward, altering the advantage calculation to heavily amplify gradient signals for high-reward outliers (rare correct solutions) while suppressing penalties for low-reward samples on difficult prompts. This works because the pre-trained model's policy distribution supports the correct solution, even if initially suppressed.

### Mechanism 2: Dynamic Difficulty-Dependent Weighting
The risk-sensitive objective implicitly prioritizes learning from "hard" prompts (low initial accuracy) over "easy" prompts, preventing premature convergence. As β increases, the cumulative advantage magnitude shifts toward prompts with lower accuracy, re-weighting optimization focus toward challenging inputs where the model is currently failing rather than exploiting inputs where it already succeeds.

### Mechanism 3: Dense Gradient for Pass@1/Pass@k Trade-off
Unlike prior Pass@k optimization methods that zero out gradients once a threshold is met, RS-GRPO maintains non-zero gradient signals, preserving Pass@1 performance while improving diversity. The exponential utility formulation E[e^{βr}] ensures the denominator never collapses to zero magnitude for high-accuracy prompts, allowing the model to continue refining its peak while the risk-seeking tail explores.

## Foundational Learning

### Concept: Risk-Sensitive vs. Risk-Neutral RL
- **Why needed here:** This paper reframes LLM fine-tuning as a risk-management problem where we seek "risk-seeking" behaviors (high variance, high potential reward) to force exploration
- **Quick check question:** If you set the risk parameter β → 0, does RS-GRPO behave like standard GRPO or PPO? (Answer: It recovers the standard risk-neutral objective)

### Concept: Sharply Peaked Initial Policies (The Exploration Dilemma)
- **Why needed here:** The theoretical motivation relies on the fact that pre-trained LLMs are not random agents; they have strong priors (biases). Standard RL exploits these biases (sharpening the peak), whereas this method tries to broaden them
- **Quick check question:** Why does standard RL improve Pass@1 but often hurt Pass@k? (Answer: It sharpens the probability mass on the most likely solution, reducing diversity)

### Concept: The Advantage Function in Policy Gradients
- **Why needed here:** RS-GRPO is implemented entirely as a "drop-in replacement" for the Advantage calculation
- **Quick check question:** In RS-GRPO, what term replaces the standard baseline (mean reward) in the advantage calculation? (Answer: The log-expectation of the exponential reward utility)

## Architecture Onboarding

### Component map:
Policy (LLM) -> Reward Function (Math_Verify) -> RS-Advantage Module -> Optimizer (AdamW)

### Critical path:
1. Sample N responses per prompt from π_θ
2. Compute scalar rewards r (0 or 1)
3. Compute exponential scaling e^{βr} for all samples
4. Normalize by group mean of exponential rewards
5. Calculate Risk-Sensitive Advantage Â^β
6. Update policy via gradient

### Design tradeoffs:
- **β (Risk-Sensitivity):** High β prioritizes finding maximum reward (better Pass@k/exploration) but may slow convergence. Low β prioritizes mean reward (faster Pass@1/exploitation)
- **Batch Size (N):** Accurate estimation of exponential expectation E[e^{βr}] requires sufficient samples per prompt (paper uses 16)

### Failure signatures:
- **Entropy Collapse:** If β is too low or standard GRPO dynamics dominate, the model solves only easy problems (Pass@k drops)
- **Gradient Instability:** If β is very high and rewards are large, e^{βr} may cause numerical overflow
- **Base Model Underperformance:** On some models, RS-GRPO failed to beat the base model at high k, suggesting initial policy was too far from global optimum

### First 3 experiments:
1. **Bandit Sanity Check:** Replicate the 100-armed bandit experiment to verify RS-PG update rule successfully escapes local optimum trap
2. **β Sweep:** Train on math12k with β ∈ {0, 2, 4, 8}. Plot Pass@1 vs. Pass@32 to find the "knee" in the trade-off curve
3. **Advantage Distribution Visualization:** Log histograms of standard advantage vs. RS-advantage values during training to confirm RS-advantage suppresses gradients for "easy" prompts and boosts them for "hard" prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an optimal dynamic β adjustment strategy outperform fixed β=2 training, and if so, what scheduling approach (decay, adaptive difficulty-based, or phased switching) best balances exploration and exploitation?
- Basis in paper: The authors state: "Devising an optimal dynamic strategy to balance exploration and exploitation remains a challenging open problem" after reporting that heuristics including decay schedules and adaptive β based on prompt difficulty failed to improve upon fixed β=2
- Why unresolved: Multiple heuristic approaches were attempted but none succeeded, suggesting the problem requires deeper theoretical understanding or novel algorithmic approaches
- What evidence would resolve it: A dynamic β strategy achieving statistically significant improvements over fixed β=2 on pass@1 while maintaining pass@k gains across multiple benchmarks and model architectures

### Open Question 2
- Question: Under what conditions does the distance between initial policy distribution and optimal policy cause RS-GRPO to fail at improving pass@k, and can curriculum learning or initialization strategies mitigate this?
- Basis in paper: The authors speculate that for some models, "the optimal policy is prohibitively distant from the initial distribution, causing RS-GRPO to converge to a local optimum"
- Why unresolved: The speculation is not empirically validated; no analysis or intervention was conducted to confirm or address this failure mode
- What evidence would resolve it: Experiments measuring distributional distance metrics (e.g., KL divergence) between initial and optimal policies, combined with interventions that successfully restore RS-GRPO effectiveness

### Open Question 3
- Question: How does RS-GRPO interact with complementary exploration techniques such as intrinsic motivation, entropy regularization, or self-correction mechanisms?
- Basis in paper: The conclusion states: "Future work could... investigate their interplay with other exploration techniques"
- Why unresolved: The paper evaluates RS-GRPO in isolation; no combination experiments with orthogonal exploration methods were conducted
- What evidence would resolve it: Ablation studies combining RS-GRPO with methods like intrinsic motivation or self-reflection, measuring whether improvements are additive, synergistic, or interfering

## Limitations
- Lack of ablation studies on the DAPO dynamic sampling and clip-higher mechanisms, which are critical to RS-GRPO's performance
- Exponential utility formulation may cause numerical instability for large β values when rewards are not strictly binary
- Some models (e.g., Llama3) showed base model underperformance at high k values, indicating initial policy distance issues

## Confidence
- **High confidence**: The core claim that RS-GRPO improves pass@k while maintaining pass@1 is well-supported by experimental results across six benchmarks and five models
- **Medium confidence**: The claim that RS-GRPO specifically addresses exploration dilemmas caused by sharply peaked pre-trained distributions is supported by evidence but could benefit from more direct analysis of policy entropy changes
- **Medium confidence**: The assertion that RS-GRPO finds more unique solutions and transitions accuracy on previously unsolved prompts is demonstrated through aggregate metrics but lacks per-prompt case studies

## Next Checks
1. **Component Ablation**: Implement a version of RS-GRPO without DAPO sampling and clip-higher to isolate the contribution of the risk-sensitive advantage function versus the sampling strategy
2. **Numerical Stability Test**: Systematically evaluate RS-GRPO with non-binary rewards and varying reward magnitudes to assess the stability of the exponential utility formulation
3. **Case Study Analysis**: Select 5-10 prompts where RS-GRPO successfully solved problems that standard GRPO failed, and perform detailed qualitative analysis of the reasoning paths to identify newly discovered strategies