---
ver: rpa2
title: 'Evaluate Bias without Manual Test Sets: A Concept Representation Perspective
  for LLMs'
arxiv_id: '2505.15524'
source_url: https://arxiv.org/abs/2505.15524
tags:
- bias
- concept
- lens
- concepts
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiasLens, a test-set-free framework for detecting
  bias in large language models. It uses Concept Activation Vectors (CAVs) and Sparse
  Autoencoders (SAEs) to extract interpretable concept representations, then measures
  representational similarity differences to quantify bias.
---

# Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs

## Quick Facts
- arXiv ID: 2505.15524
- Source URL: https://arxiv.org/abs/2505.15524
- Reference count: 40
- Primary result: Introduces BiasLens, a test-set-free framework for detecting bias in LLMs using CAVs and SAEs, achieving Spearman correlations > 0.85 with established bias metrics.

## Executive Summary
This paper introduces BiasLens, a fully automated framework for detecting bias in large language models without requiring manual test sets. The method extracts interpretable concept representations using Concept Activation Vectors (CAVs) and Sparse Autoencoders (SAEs), then measures representational similarity differences to quantify bias. Evaluated on three LLMs (Gemma 2 2B/9B, Llama 3.1 8B), BiasLens achieves high correlations with established bias metrics across six sentiment concepts and gender bias in 40 occupations. The framework is approximately 50x faster than traditional approaches and can uncover novel bias patterns in medical and educational domains.

## Method Summary
BiasLens measures representational bias by computing the asymmetry in cosine similarity between a target concept vector and two reference concept vectors. The method involves three main steps: (1) training CAVs to isolate concept directions in activation space using logistic regression classifiers, (2) extracting concept representations by steering model activations toward the target concept and encoding the difference with SAEs, and (3) calculating bias scores as the absolute difference in cosine similarity between the target and reference concepts. The approach requires no manual test data, instead using synthetic probe sentences generated by GPT-4o and pre-trained SAEs from SAELens.

## Key Results
- Achieved Spearman correlations > 0.85 with established bias metrics (|F1-Diff|, EOD, SEAT, Perplexity test) across six sentiment concepts and 40 occupations
- Discovered novel bias patterns in medical domain (illness vs. insurance status) and educational domain (math ability vs. income)
- Demonstrated approximately 50x speedup compared to manual test-set-based approaches
- Validated on three LLM architectures: Gemma 2 2B/9B and Llama 3.1 8B

## Why This Works (Mechanism)

### Mechanism 1
Linear directions in activation space (CAVs) effectively isolate high-level concepts, allowing the model to be "steered" toward specific ideas. A logistic regression classifier is trained to distinguish activations from concept-relevant sentences versus random sentences. The resulting normal vector defines the Concept Activation Vector (CAV). Core assumption: concepts are encoded linearly within the activation space of the LLM layers. Evidence: mentions combining CAVs with SAEs to extract representations, describes training logistic regression on positive vs. negative activations, and cites fundamental limits of perfect concept erasure. Break condition: if the target concept is non-linearly represented or highly distributed, the linear CAV may fail to capture it.

### Mechanism 2
Projecting the activation shift caused by steering into a sparse space amplifies the semantic signal of the target concept. The model is iteratively steered (activations modified by CAV) until prediction confidence is high. The difference between the SAE-encoded "steered" state and the "original" state creates a contrastive vector that highlights concept-specific features. Core assumption: the SAE features are monosemantic and the steering effect is localized to relevant dimensions. Evidence: describes CAV-based steering and SAE-based extraction via normalized difference, shows AUC increasing as extraction moves from raw to normalized differences, but corpus evidence is weak for this specific CAV+SAE combination. Break condition: if the SAE is not well-trained or the steering perturbs unrelated features significantly, the resulting vector will be noisy.

### Mechanism 3
The asymmetry in cosine similarity between a target concept and reference concepts serves as a reliable proxy for behavioral bias. Bias is defined as |cos(C_target, C_ref1) - cos(C_target, C_ref2)|. A large difference implies the target is asymmetrically associated with one reference group over another in the model's geometry. Core assumption: internal representational alignment correlates with external behavioral bias. Evidence: defines conceptual correlation bias formally, shows Spearman correlations > 0.85 with extrinsic metrics like |F1-Diff| on Gemma models, and cites BiasMap which similarly uses attention-based geometry to proxy bias. Break condition: if the model's internal geometry is disconnected from its output behavior, the metric becomes a false positive.

## Foundational Learning

**Concept: Concept Activation Vectors (CAVs)**
Why needed: This is the fundamental probe used to isolate concepts. Without understanding CAVs, the "steering" step appears magical.
Quick check question: Can you explain how a linear classifier trained on activations defines a "direction" in the model's hidden state?

**Concept: Sparse Autoencoders (SAEs)**
Why needed: The paper relies on SAEs to translate dense, uninterpretable activations into sparse, readable features.
Quick check question: Why is sparsity necessary for interpreting the "steered" difference vector?

**Concept: Word Embedding Association Test (WEAT)**
Why needed: The paper explicitly positions itself as an evolution of static word embedding bias tests (like WEAT) applied to dynamic LLM concepts.
Quick check question: How does measuring cosine similarity between vectors measure "bias" or "stereotype"?

## Architecture Onboarding

**Component map:** Probe Generator -> CAV Trainer -> Steering Loop -> SAE Encoder -> Bias Calculator

**Critical path:** The Steering Loop -> SAE Encoder interface is critical. If the steering step does not shift the SAE latent space significantly, the extracted vector is invalid.

**Design tradeoffs:** Last-layer focus trades off lower-level syntactic bias detection for high-level semantic bias. Synthetic Probes rely on GPT-4o for probe data, which is faster than manual annotation but risks model-specific biases contaminating the probe.

**Failure signatures:** Flat Steering (activations do not change after steering loop), Random Alignment (bias scores are uniform across all concepts), Low Correlation (Spearman correlation with established metrics drops).

**First 3 experiments:**
1. Validation: Replicate Figure 3b. Check if Norm(z_steer) - Norm(z_ori) actually ranks food-related features higher than raw z_steer.
2. CAV Sanity Check: Verify Table 6 claims. Ensure your CAV classifiers achieve >90% accuracy on the probe dataset splits.
3. Occupation Bias: Run the "doctor" vs "male/female" test on Gemma 2 2B and compare the score against the SEAT baseline provided in Table 11.

## Open Questions the Paper Calls Out

**Open Question 1:** Does averaging bias scores over multiple diverse steering prompts reduce variability in BiasLens results compared to using a single prompt? The authors use only one prompt per experiment but acknowledge this as a limitation without empirical validation of multi-prompt averaging.

**Open Question 2:** Would more expressive similarity metrics (geometric distances, kernel-based measures) better capture complex representational relationships than cosine similarity? The paper only validates cosine similarity; no alternative metrics are tested or compared.

**Open Question 3:** Why does BiasLens show negative correlations with some baseline metrics on Llama 3.1 8B but positive correlations on Gemma models? The paper does not explain this discrepancy in performance across different model architectures.

## Limitations
- Method shows high correlations on Gemma models but lower performance on Llama 3.1 8B, suggesting architecture dependence
- Relies entirely on GPT-4o-generated probes, introducing potential circularity in bias detection
- Assumes SAE features are monosemantic and interpretable without direct validation of feature semantics

## Confidence
- **High Confidence:** CAV+SAE mechanism for extracting concept representations is technically sound and the steering algorithm is well-defined.
- **Medium Confidence:** 50x speed improvement claim based on comparison to manual approaches but lacks detailed timing methodology; generalizability across architectures uncertain.
- **Low Confidence:** Discovery of new bias patterns presented without behavioral validation; internal correlations may not translate to actual model behavior.

## Next Checks
1. Apply BiasLens to at least three additional model families (e.g., Mistral, Phi-3, Claude) to determine if effectiveness is architecture-dependent or universal.
2. Generate probe sets using different methods (random human annotation, smaller models, rule-based generation) and measure how detection accuracy varies with probe quality.
3. For newly discovered bias patterns (medical, educational domains), design behavioral experiments to test whether internal representational bias actually manifests in model outputs or decisions.