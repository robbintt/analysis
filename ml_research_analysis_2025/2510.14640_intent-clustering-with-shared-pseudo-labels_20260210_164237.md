---
ver: rpa2
title: Intent Clustering with Shared Pseudo-Labels
arxiv_id: '2510.14640'
source_url: https://arxiv.org/abs/2510.14640
tags:
- text
- clustering
- intent
- pseudo-labels
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free and label-free method for
  intent clustering that uses lightweight, open-source LLMs to generate pseudo-labels
  for each text, followed by multi-label classification. The method assumes that texts
  in the same cluster share more labels and thus have closer embeddings.
---

# Intent Clustering with Shared Pseudo-Labels

## Quick Facts
- **arXiv ID**: 2510.14640
- **Source URL**: https://arxiv.org/abs/2510.14640
- **Reference count**: 32
- **Primary result**: A training-free, label-free intent clustering method that uses lightweight LLMs to generate pseudo-labels, outperforming or matching baselines on four benchmark datasets.

## Executive Summary
This paper presents a novel approach to intent clustering that generates pseudo-labels using open-source LLMs and leverages shared labels for clustering. The method iteratively refines pseudo-labels and uses them to create more semantically coherent embeddings, achieving state-of-the-art results on four benchmark datasets without requiring prior knowledge of cluster counts. The approach is computationally efficient and stable across different LLM models, with the added benefit of producing human-readable pseudo-labels for interpretability.

## Method Summary
The method works by first using a pre-trained embedder to compute initial embeddings for all texts. For each text, it retrieves the top 10 nearest neighbors and prompts an open-source LLM to generate an initial pseudo-label that represents the text and its neighbors. It then iteratively refines these labels: each text is re-encoded with its current pseudo-labels concatenated, new neighbors are retrieved, and the LLM selects additional labels from the neighbor set. This process continues until label sets stabilize. Finally, K-means clustering (with K set to ground truth only for evaluation) produces the final cluster assignments.

## Key Results
- Outperforms or matches recent baselines on Bank77, CLINC150, MTOP, and Massive datasets
- Stable performance across different LLM models (Gemma, Llama, Qwen)
- Superior to methods using proprietary LLMs while being computationally efficient
- Generated pseudo-labels are human-readable and aid interpretability

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Label Semantic Anchoring
Concatenating pseudo-labels to text before embedding draws semantically similar texts closer in embedding space. For texts in the same cluster, adding a shared pseudo-label that captures partial semantic meaning reduces the distance between their embeddings compared to the original texts alone. This works because the embedder encodes shared lexical/semantic content in a way that amplifies cluster-level similarity signals.

### Mechanism 2: Iterative Multi-Label Convergence via Neighbor Propagation
The iterative process assigns multiple pseudo-labels from nearest neighbors, increasing label overlap within true clusters and sharpening embedding clusters over iterations. At each iteration, texts retrieve neighbors using label-augmented embeddings, then the LLM selects matching labels from those neighbors. Labels accumulate with nested structure, ensuring shared labels propagate across cluster members. This creates a positive feedback loop where neighbors increasingly belong to the same true cluster.

### Mechanism 3: Neighbor Context for Cluster-Aware Label Generation
Providing the LLM with the closest texts during initial label generation produces pseudo-labels reflecting cluster-level semantics rather than instance-specific quirks. The LLM is prompted to create labels that generalize across the target utterance and matching neighbors, explicitly filtering poorly-matched neighbors in the prompt. This ensures initial pseudo-labels capture broader cluster themes even before refinement.

## Foundational Learning

- **Embedding spaces and distance metrics (cosine similarity)**: The method relies on encoding texts and label-augmented texts into vectors, then using cosine similarity to retrieve neighbors for iterative refinement. Quick check: Given vectors [1, 0, 1] and [1, 1, 0], their cosine similarity is 0.5.

- **Multi-label classification vs. single-label**: The iterative stage selects multiple pseudo-labels per text; understanding how labels aggregate is critical for debugging convergence. Quick check: If a text has labels {A, B} and can select from {B, C, D}, the maximum label set under nested structure is {A, B, C, D}.

- **K-means clustering assumptions and limitations**: Final clustering uses K-means with ground-truth K for evaluation; understanding K-means sensitivity helps interpret results. Quick check: K-means does not guarantee the same clusters across runs with different random seeds.

## Architecture Onboarding

- **Component map**: Pre-trained Embedder (Instructor-large) -> Open-source LLM (Gemma/Llama/Qwen) -> Neighbor Retrieval Module -> Iterative Label Aggregator -> K-means Clusterer

- **Critical path**: 1) Encode all texts → retrieve 10 neighbors per text. 2) Prompt LLM for initial pseudo-label. 3) Encode label⊕text → re-retrieve neighbors. 4) Prompt LLM for multi-label selection. 5) Repeat until labels stabilize. 6) Run K-means on final embeddings.

- **Design tradeoffs**: m=10 neighbors balances context richness vs. LLM prompt length; fewer neighbors risks sparse context, more risks noise. Lightweight LLMs reduce cost but may generate less consistent labels. Nested label accumulation preserves history but increases embedding complexity.

- **Failure signatures**: Pseudo-labels appearing across multiple distinct clusters indicate over-generalization. No convergence suggests neighbor retrieval isn't improving. Performance dropping below baseline suggests label noise is overwhelming signal.

- **First 3 experiments**: 1) Reproduce baseline on Bank77 to validate pipeline (expect Acc ~65.5%). 2) Ablation on iteration count to measure Acc gap (~2-4% drop). 3) LLM swap test between Gemma and Llama to assess model sensitivity.

## Open Questions the Paper Calls Out

1. **Long text adaptation**: How to handle longer texts without exceeding token limits or incurring prohibitive inference costs. The current method focuses on short text clustering, and the authors identify this as a future direction without proposing specific mechanisms.

2. **Error propagation robustness**: How robust the iterative process is against error propagation if the LLM generates noisy or ambiguous pseudo-labels early on. While iterations generally help, the paper doesn't analyze if bad labels lock texts into incorrect neighborhoods.

3. **Computational scaling**: How computational cost scales with dataset size compared to training-free baselines. The method is labeled efficient for small benchmarks but requires iterative LLM inference for every data point, with performance on industrial-scale datasets unverified.

## Limitations
- Performance depends critically on the quality of the pre-trained embedder and LLM, with no exploration of robustness to embedder failure or out-of-domain data
- Convergence criteria for iterative label refinement are underspecified, allowing variation in results
- The assumption that shared pseudo-labels reliably indicate cluster membership is not directly validated with qualitative label distribution analysis
- Results are benchmarked only on short, single-domain text datasets; generalization to longer or multi-domain data is untested

## Confidence
- **High Confidence**: Claims about computational efficiency and training-free operation are well-supported by experimental design and baselines
- **Medium Confidence**: Claims about state-of-the-art performance and stability across different LLMs are supported by results but depend on underlying model quality
- **Low Confidence**: Claims about interpretability of pseudo-labels and their utility for downstream analysis are not empirically validated beyond method description

## Next Checks
1. **Embedder Sensitivity Test**: Swap Instructor-large for a weaker embedder (e.g., sentence-transformers) and measure clustering performance drop to check if neighbor quality degrades enough to break iterative refinement.

2. **Convergence Robustness**: Run iterative label refinement with a strict max-iteration cap (e.g., 10) and monitor label-set stability to quantify performance loss if oscillations persist.

3. **Label Coherence Audit**: Manually inspect 20 pseudo-labels from a converged run, mapping each to its ground-truth cluster to compute the fraction of over-generalized labels versus cluster-specific ones.