---
ver: rpa2
title: PPO in the Fisher-Rao geometry
arxiv_id: '2506.03757'
source_url: https://arxiv.org/abs/2506.03757
tags:
- policy
- lemma
- theorem
- convergence
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fisher-Rao Proximal Policy Optimization (FR-PPO),
  a theoretically grounded variant of PPO that leverages the Fisher-Rao geometry instead
  of the clipped surrogate objective. The authors derive a tighter lower bound on
  the difference of value functions using the Fisher-Rao metric, which leads to a
  smooth surrogate objective amenable to analysis.
---

# PPO in the Fisher-Rao geometry

## Quick Facts
- arXiv ID: 2506.03757
- Source URL: https://arxiv.org/abs/2506.03757
- Reference count: 40
- Primary result: Theoretically grounded PPO variant using Fisher-Rao geometry with provable convergence guarantees

## Executive Summary
This paper introduces Fisher-Rao Proximal Policy Optimization (FR-PPO), a theoretically grounded variant of PPO that leverages the Fisher-Rao geometry instead of the clipped surrogate objective. The authors derive a tighter lower bound on the difference of value functions using the Fisher-Rao metric, which leads to a smooth surrogate objective amenable to analysis. The key contributions include proving that the value function is smooth relative to the Fisher-Rao distance, establishing sub-linear convergence of FR-PPO with no dependence on state or action space dimensions in the direct parametrization setting, and demonstrating improved convergence rates under compatible function approximation. Empirical results show that FR-PPO performs comparably to PPO across standard RL tasks including Atari and Mujoco environments.

## Method Summary
FR-PPO replaces PPO's clipped surrogate objective with a Fisher-Rao geometry-based penalty. The policy update maximizes the standard advantage-weighted probability ratio while adding a quadratic penalty term weighted by the old policy density. This creates a smooth objective that enables convergence analysis via mirror descent methods. The algorithm uses GAE for advantage estimation and is implemented as a modification to standard PPO codebases, replacing the clipping mechanism with a carefully tuned penalty parameter.

## Key Results
- Proves value function smoothness relative to Fisher-Rao distance
- Establishes sub-linear convergence with no dependence on state/action space dimensions
- Achieves O(1/n) convergence rate under compatible function approximation (vs O(1/√n) for standard PPO)
- Shows comparable performance to PPO across 11 Atari and Mujoco environments

## Why This Works (Mechanism)

### Mechanism 1: Bregman Divergence Induced Smoothness
Replacing Total Variation distance with Fisher-Rao metric enables Mirror Descent analysis by inducing Bregman divergence structure. The TV² term in the performance lower bound can be upper-bounded by squared Fisher-Rao distance (FR²), which generates a χ²-divergence. This transformation implies the value function is "smooth relative to the FR geometry," satisfying mirror descent framework conditions.

### Mechanism 2: Three-Point Lemma for Variational Updates
Convergence proof relies on variational representation of policy updates rather than parameter gradient analysis. The Three-Point Lemma (Bregman proximal inequality) is applied to the FR-PPO update scheme, avoiding smoothness assumptions on the policy with respect to its parameters. This enables establishing sub-linear O(1/n) convergence directly in policy space.

### Mechanism 3: Isometry via Compatible Function Approximation
For linear parametrization, the Projected Natural Policy Gradient update is mathematically equivalent to the FR-PPO update. An isometry exists between parameter space (Fisher metric) and policy space (L₂ metric on squared densities). Under compatible function approximation, natural gradient direction aligns exactly with FR-geometry update, improving convergence rates from O(1/√n) to O(1/n).

## Foundational Learning

- **Bregman Divergence & Mirror Descent**: Why needed - Paper frames FR-PPO as mirror descent algorithm. Understanding that Bregman divergences generalize squared Euclidean distance to non-linear spaces is essential to grasp why FR² is preferred over TV. Quick check - Why does Total Variation (TV) distance fail to qualify as a Bregman divergence, and how does this prevent standard convergence analysis?

- **Fisher-Rao Geometry (Information Geometry)**: Why needed - Core innovation shifts from "flat" Euclidean geometry to spherical geometry of Fisher-Rao metric. Quick check - How does the Fisher-Rao metric define distance between two probability distributions, and why is it related to the χ²-divergence in this context?

- **Compatible Function Approximation**: Why needed - To extend theoretical guarantees from tabular to function approximation settings. Quick check - What condition must the feature basis φ satisfy for projection onto parameter space to match update in policy space?

## Architecture Onboarding

- **Component map**: Calculate probability ratio r(θ) → Calculate Advantage Â → Compute FR-PPO Loss (Eq 3.10) → Update via gradient ascent
- **Critical path**: 1) Calculate standard probability ratio r(θ) 2) Calculate Advantage Â (via GAE) 3) Compute FR-PPO Loss: Maximize r·Â while penalizing (r-1)² scaled by 1/τ 4) Update via gradient ascent
- **Design tradeoffs**: Sensitivity - FR-PPO more sensitive to penalty parameter τ than PPO is to clip range ε. Implementation - Requires access to policy density π_old inside penalty term, using quadratic penalty vs clipping mask.
- **Failure signatures**: Divergence - If τ too large, policy ratio may explode. Stagnation - If τ too small, policy updates too slowly. Approximation Error - Low network capacity may dominate convergence benefits.
- **First 3 experiments**: 1) Penalty Tuning Sweep - Grid search τ on Mujoco environment comparing against PPO's ε sensitivity. 2) Ablation on Penalty Term - Compare FR-PPO (Quadratic) vs PPO (Clip) vs PPO-Penalty (KL). 3) Convergence Rate Validation - Train on tabular environment to verify O(1/n) convergence.

## Open Questions the Paper Calls Out
- **Adaptive penalty schemes**: Can adaptive schemes or meta-learning be developed to reduce high sensitivity of FR-PPO to penalty parameter τ, matching robustness of PPO's clipping parameter ε?
- **General function approximation**: Can convergence guarantees extend to general non-linear function approximation (deep neural networks) without relying on restrictive compatible function approximation assumption?
- **Broader environment validation**: Does FR-PPO maintain theoretical stability and comparable performance when scaled to broader set of high-dimensional environments beyond limited benchmarks tested?

## Limitations
- High sensitivity to penalty parameter τ requiring careful tuning
- Theoretical framework assumes access to policy densities that may not translate directly to deep neural networks
- Gap between theoretical convergence guarantees and practical performance gains

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical convergence in direct parametrization | High |
| O(1/n) convergence under compatible approximation | Medium |
| Practical performance comparable to PPO | Medium |

## Next Checks
1. **Penalty Parameter Sensitivity Analysis**: Systematically sweep τ across multiple orders of magnitude on representative Mujoco task to characterize stability/performance tradeoffs.
2. **Theoretical Rate Verification**: Implement FR-PPO on tabular environment and plot sub-optimality gap vs update steps to empirically verify claimed O(1/n) convergence rate.
3. **Approximation Error Impact**: Train FR-PPO with progressively weaker function approximation to quantify how accumulated approximation error affects both theoretical rates and practical performance.