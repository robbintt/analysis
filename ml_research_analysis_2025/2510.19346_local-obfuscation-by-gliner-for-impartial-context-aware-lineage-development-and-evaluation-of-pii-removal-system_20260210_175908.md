---
ver: rpa2
title: 'Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development
  and evaluation of PII Removal system'
arxiv_id: '2510.19346'
source_url: https://arxiv.org/abs/2510.19346
tags:
- data
- address
- entity
- number
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LOGICAL is a PII removal system built on a fine-tuned GLiNER model.
  It achieved an overall micro-average F1-score of 0.980, outperforming LLM-based
  solutions like Gemini-Pro-2.5 (F1-score: 0.845) and Llama-3.3-70B-Instruct (F1-score:
  0.778).'
---

# Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system

## Quick Facts
- arXiv ID: 2510.19346
- Source URL: https://arxiv.org/abs/2510.19346
- Reference count: 40
- Achieved micro-average F1-score of 0.980 on PII detection task

## Executive Summary
LOGICAL is a PII removal system built on a fine-tuned GLiNER model for clinical text de-identification. The system achieves high accuracy (F1-score 0.980) while preserving contextual lineage through consistent placeholder replacement. Unlike LLM-based solutions, LOGICAL operates efficiently on standard hardware without GPU requirements and completely sanitizes 95% of documents. The model's dual-encoding architecture enables flexible entity detection across nine PII categories while maintaining clinical context for downstream tasks.

## Method Summary
The method fine-tunes the `modern-gliner-bi-large-v1.0` model on 2,849 instances of annotated psychiatric EHR data (1,515 documents), using focal loss and differentiated learning rates to handle class imbalance. Documents are chunked (≤4,000 words with 25-word overlap) and processed through GLiNER's dual-encoding architecture for entity detection. A corpus-level replacement dictionary maintains lineage preservation by consistently replacing unique entities with the same placeholders across documents. The system achieves complete sanitization of 95% of documents with efficient inference on standard hardware.

## Key Results
- Achieved overall micro-average F1-score of 0.980 on PII detection task
- Completely sanitized 95% of documents compared to 64% for next-best LLM solution
- Outperformed LLM-based solutions: Gemini-Pro-2.5 (F1: 0.845) and Llama-3.3-70B-Instruct (F1: 0.778)

## Why This Works (Mechanism)

### Mechanism 1: Dual-encoding architecture for flexible PII detection
GLiNER's dual-encoding architecture enables efficient, flexible PII detection by matching semantic embeddings of text spans to entity label embeddings. The model uses Modern-BERT to encode clinical text and a separate encoder for entity labels (e.g., "person," "address"). Entity extraction occurs via semantic similarity matching between text span embeddings and label embeddings, allowing zero-shot adaptation to arbitrary entity types without restructured training data. Core assumption: Semantic embeddings of entity labels meaningfully align with embeddings of actual PII instances in clinical text.

### Mechanism 2: Focal loss and differentiated learning rates for small dataset optimization
Fine-tuning with focal loss (alpha=0.75, gamma=2) and differentiated learning rates achieves high accuracy on small clinical datasets while preserving pre-trained knowledge. Focal loss upweights rare entities (e.g., identification numbers), while differential learning rates (3e-6 for BERT layers, 3e-5 for classifier layers) prevent catastrophic forgetting of pre-trained linguistic knowledge during task adaptation. Core assumption: Pre-trained Modern-BERT knowledge transfers effectively to clinical text, and explicit loss re-weighting addresses class imbalance.

### Mechanism 3: Lineage-preserving placeholder replacement for context maintenance
Lineage-preserving placeholder replacement maintains clinical context for downstream tasks while removing identifiers. The pipeline maintains corpus-level replacement dictionaries mapped by entity type. Fuzzy matching ensures consistent replacements across documents, preserving temporal and relational patterns needed for clinical reasoning. Core assumption: Placeholder consistency is sufficient for downstream ML tasks while preventing re-identification.

## Foundational Learning

- **Named Entity Recognition (NER) as token classification**: PII detection is framed as classifying text spans into entity categories, not text generation—critical for understanding GLiNER's design. Quick check: Why is token-level classification more suitable than sequence-to-sequence generation for PII detection?

- **Semantic embeddings and similarity**: GLiNER relies on embeddings where "person" label vectors align with actual name vectors in text. Quick check: If you encode "Dr. Smith" and the label "person" into vectors, what should their cosine similarity indicate?

- **Precision vs. recall trade-offs in privacy tasks**: PII detection historically optimizes for recall (don't miss PII), but the paper notes this can compromise downstream data utility. Quick check: For de-identification, which is worse—flagging non-PII as PII (false positive) or missing actual PII (false negative)?

## Architecture Onboarding

- **Component map**: Input preprocessing -> GLiNER model (Modern-BERT + token classifier) -> Entity extraction (character-level spans) -> Replacement engine (fuzzy matching + dictionary) -> Human-in-the-loop interface
- **Critical path**: Raw clinical text → chunking with overlap → GLiNER inference (entity spans + types) → human review interface → confirmed entities → placeholder replacement → sanitized chunks → document reassembly
- **Design tradeoffs**: Model size (0.53B) vs. deployment flexibility (laptop inference), chunking overlap (25 words) vs. inference overhead, aggressive replacement vs. clinical timeline preservation, annotation investment (~2800 instances) vs. target accuracy
- **Failure signatures**: Entity boundary errors (partial matches), rare entity misses (identification numbers), false positives (medical terms flagged as names), context loss (over-aggressive date removal)
- **First 3 experiments**: 1) Baseline validation: Run LOGICAL on 10-20 documents with known PII locations; measure character-level precision/recall vs. Microsoft Presidio. 2) Boundary case testing: Construct ambiguous test cases (e.g., "Bangalore" as city vs. medical term) to map decision boundaries. 3) Latency profiling: Measure inference time at 100, 500, 1000, 2000 words on target hardware to validate "standard laptop without GPU" claims.

## Open Questions the Paper Calls Out

- **Generalization to non-psychiatric domains**: How well does the fine-tuned LOGICAL model generalize to non-psychiatric clinical domains and different cultural or geographical contexts without specific retraining? The model was trained exclusively on 1,515 clinical documents from a single psychiatric hospital in India, potentially limiting its efficacy on general medical notes or text from regions with different naming conventions and jargon.

- **Downstream bias and utility preservation**: Does the "lineage-preserving" obfuscation method effectively mitigate model bias and preserve clinical utility in downstream tasks better than standard redaction? The study only evaluates the accuracy of PII detection, not the quality or impartiality of models trained on the sanitized data.

- **Rare entity detection improvement**: Can the detection of rare, high-entropy entities like identification numbers be improved in GLiNER architectures to match or exceed the recall of zero-shot LLMs? While GLiNER excels at semantic entities, its recall for "Identification number" (0.828) was lower than Gemini-Pro-2.5 (1.000).

## Limitations

- **Data availability constraints**: Core training dataset unavailable due to patient privacy, limiting reproducibility and cross-domain validation
- **Benchmarking scope**: Limited comparative analysis with established clinical de-identification baselines like Microsoft Presidio
- **Error analysis granularity**: Lacks detailed breakdown of failure modes by entity type, document complexity, or temporal patterns

## Confidence

- **High confidence**: Character-level micro-averaged F1-score of 0.980 on test set (376 instances), complete sanitization of 95% of documents, efficient inference on standard hardware without GPU
- **Medium confidence**: Superiority over LLM-based solutions given limited comparative benchmarks, generalizability of results to clinical domains beyond psychiatric EHR
- **Low confidence**: Long-term reliability of lineage preservation for complex clinical reasoning tasks and robustness to diverse PII formats not represented in training data

## Next Checks

1. **Cross-domain generalization**: Test LOGICAL on clinical documents from different specialties (e.g., oncology, cardiology) to assess performance stability when encountering unfamiliar entity patterns and medical terminology.

2. **Longitudinal pattern analysis**: Evaluate whether placeholder replacement preserves temporal relationships in multi-document patient histories, measuring impact on downstream clinical reasoning tasks that depend on timeline coherence.

3. **Human-in-the-loop efficiency**: Measure time and accuracy of human reviewers using the web interface across different document complexities, quantifying the trade-off between automation and manual verification effort.