---
ver: rpa2
title: 'PrimeX: A Dataset of Worldview, Opinion, and Explanation'
arxiv_id: '2510.00174'
source_url: https://arxiv.org/abs/2510.00174
tags:
- user
- explanations
- world
- opinion
- e-01
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRIMEX introduces a novel dataset combining public opinion surveys,
  free-form explanations, and Primal World Beliefs from 858 US respondents. It enables
  analysis of how worldview and explanations influence language model personalization.
---

# PrimeX: A Dataset of Worldview, Opinion, and Explanation

## Quick Facts
- arXiv ID: 2510.00174
- Source URL: https://arxiv.org/abs/2510.00174
- Reference count: 40
- Primary result: Dataset of 858 US respondents with opinion surveys, explanations, and Primal World Beliefs that improves opinion prediction accuracy

## Executive Summary
PRIMEX introduces a novel dataset combining public opinion surveys, free-form explanations, and Primal World Beliefs from 858 US respondents. The dataset enables analysis of how worldview and explanations influence language model personalization. When incorporated into user representations, PRIMEX data improves opinion prediction accuracy, with notable gains in cross-topic generalization. The dataset also enables predicting worldview scores from opinions and explanations, opening new avenues for user representation in NLP systems.

## Method Summary
The PRIMEX dataset combines three data sources: American Trends Panel survey responses (30 opinions across three waves), free-text explanations for selected opinions (3 per survey), and PI-18 Primal World Belief scores (18 items measuring Good, Safe, Alive, and Enticing dimensions). The dataset includes demographic information and is split into 430 training and 428 test users. Opinion prediction uses zero-shot prompting with GPT-4o and Mistral 7B, while Primal prediction uses finetuned models. User representations range from basic demographics to PRIMEXPERSONA combining all available data.

## Key Results
- Opinion prediction accuracy improves from ~30% (DEMOGRAPHICS baseline) to ~38% (PRIMEXPERSONA) in All Topics setting
- Cross Topic evaluation shows ~40% accuracy with PRIMEXPERSONA versus ~25% baseline
- Primal prediction achieves MSE scores of 0.128 (Good), 0.071 (Safe), 0.051 (Alive), 0.033 (Enticing) using finetuned models

## Why This Works (Mechanism)
The dataset captures the relationship between worldview beliefs and opinion formation through explanations. When users provide explanations for their opinions, they often reveal underlying Primal World Beliefs that shape their perspectives. By incorporating these explanations and Primal scores into user representations, models can better predict how users will respond to new opinion questions, particularly those outside their initial training topics.

## Foundational Learning
- **Primal World Beliefs**: Fundamental assumptions about the world's nature (Good, Safe, Alive, Enticing). Why needed: These beliefs shape how people interpret experiences and form opinions. Quick check: Verify PI-18 scoring follows established psychological validation procedures.
- **Explanation utility scoring**: Measures how much explanations improve prediction accuracy. Why needed: Identifies which explanations contain worldview-relevant information. Quick check: Confirm utility score calculation uses expected log-likelihood difference correctly.
- **Cross-topic generalization**: Testing model performance on opinions from surveys not used in training. Why needed: Evaluates whether worldview representations transfer beyond specific topics. Quick check: Verify test questions come exclusively from different survey waves than training data.

## Architecture Onboarding
**Component map**: User Data -> User Representation -> Opinion Prediction Model -> Accuracy Score

**Critical path**: Load PRIMEX data → Build user representations → Prompt model with test questions → Compare predictions to ground truth

**Design tradeoffs**: Single-token matching simplifies evaluation but loses nuance; Cross Topic setting tests generalization but has sparse explanation coverage

**Failure signatures**: 
- Low accuracy despite explanations suggests poor explanation selection or representation
- High baseline accuracy may indicate test questions are too similar to training data
- Inconsistent Primal predictions suggest incorrect reverse-scoring of PI-18 items

**First experiments**:
1. Test opinion prediction with DEMOGRAPHICS representation only
2. Add Primals to user representation and measure accuracy improvement
3. Add explanations to user representation and measure change in utility scores

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset limited to 858 US respondents with specific demographic characteristics (85% White, median income $80k-100k)
- Single-token matching metric oversimplifies opinion response evaluation
- Cross Topic setting has sparse explanation coverage (only 3 of 10 seed opinions have explanations)

## Confidence
- High confidence: Dataset construction methodology and basic statistics are well-documented
- Medium confidence: Improvements from Primals and explanations are consistently demonstrated
- Low confidence: Claims about new avenues for user representation are speculative

## Next Checks
1. Apply opinion prediction pipeline to demographic subsets to assess generalization
2. Re-administer opinion questions and PI-18 items after 3-6 months to measure stability
3. Implement opinion prediction using alternative worldview frameworks for comparison