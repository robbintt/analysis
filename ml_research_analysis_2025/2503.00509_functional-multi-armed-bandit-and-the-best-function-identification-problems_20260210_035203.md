---
ver: rpa2
title: Functional multi-armed bandit and the best function identification problems
arxiv_id: '2503.00509'
source_url: https://arxiv.org/abs/2503.00509
tags:
- problem
- algorithm
- function
- optimization
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two new problem classes: the functional
  multi-armed bandit (FMAB) and the best function identification (BFI) problems. These
  problems generalize the classic multi-armed bandit framework by treating each arm
  as an unknown black-box function rather than a random variable with a fixed reward
  distribution.'
---

# Functional multi-armed bandit and the best function identification problems

## Quick Facts
- **arXiv ID:** 2503.00509
- **Source URL:** https://arxiv.org/abs/2503.00509
- **Reference count:** 40
- **Primary result:** Introduces FMAB and BFI problems with F-LCB algorithm using optimization convergence rates as confidence bounds

## Executive Summary
This paper introduces two new problem classes that generalize the classic multi-armed bandit framework by treating each arm as an unknown black-box function rather than a random variable with fixed reward distribution. The authors propose a novel reduction scheme called F-LCB that constructs UCB-type algorithms for these problems by leveraging convergence rates of base optimization algorithms. The framework provides theoretical regret bounds for both deterministic and stochastic settings while demonstrating practical effectiveness on synthetic and real-world optimization tasks.

## Method Summary
The F-LCB algorithm reduces functional optimization to a bandit problem by treating the convergence rate of a base optimizer as a confidence interval. For each function (arm), it maintains a lower confidence bound estimate using the current optimization progress minus the theoretical suboptimality bound. The algorithm selects the arm with the minimum lower confidence bound for optimization, dynamically allocating computational resources to the most promising functions. This approach generalizes standard multi-armed bandit techniques to functional optimization while providing theoretical guarantees based on the convergence properties of the base optimization methods.

## Key Results
- Establishes regret upper bounds for deterministic and stochastic FMAB settings based on base optimizer convergence rates
- Proves a lower bound on the number of iterations required to achieve a given regret threshold in the deterministic BFI problem
- Demonstrates effective identification of optimal functions on synthetic smooth and non-smooth convex functions
- Successfully identifies the best neural network architecture among candidates in a real-world CIFAR10 image classification task

## Why This Works (Mechanism)

### Mechanism 1: Convergence-Rate-as-Confidence-Interval
The F-LCB algorithm treats the known convergence rate of a base optimizer as a confidence bound, allowing it to approximate the potential (lower bound) of an unknown function's minimum. The algorithm maintains an index $LCB_i(k, \delta) = f_i(x_k) - g(k, \delta)$ for each function (arm), where $g(k, \delta)$ is the theoretical suboptimality bound of the base optimizer. By selecting the arm with the minimum LCB, the system prioritizes the function that plausibly offers the best global minimum based on current optimization progress. This mechanism assumes the function belongs to a class where the deterministic or high-probability convergence rate holds strictly.

### Mechanism 2: Optimism-Based Resource Allocation
The algorithm minimizes cumulative regret by dynamically shifting computational budget to the "best" function identified so far, effectively stopping optimization on inferior functions early. This is an "optimism in the face of uncertainty" approach where the algorithm ceases to pull arms whose lower bound is already worse than the optimistic estimate of the leader. This mechanism relies on the existence of a gap between optimal values of the best function and suboptimal ones, allowing for eventual separation.

### Mechanism 3: Stochastic Robustness via Clean Events
The framework extends to stochastic settings by defining a "clean event" where optimization error is bounded with high probability, limiting the penalty of noise. In stochastic FMAB, the oracle returns noisy gradients/values, and the algorithm assumes a "clean event" $E_{clean}$ where the convergence bound holds. The total expected regret is bounded by the sum of the regret inside the clean event plus the probability of the "bad event" times the maximum possible regret. This mechanism requires noise to satisfy specific tail bounds and function values to be bounded by a constant.

## Foundational Learning

- **Concept: Multi-Armed Bandits (MAB) & Regret**
  - **Why needed here:** The paper frames optimization as a bandit problem. Understanding "regret" (the loss incurred by not choosing the optimal path every time) and the "exploration-exploitation" trade-off is essential to understand the F-LCB selection rule.
  - **Quick check question:** Can you explain why minimizing regret is different from simply minimizing the final loss in a standard optimization problem?

- **Concept: Convex Optimization Convergence Rates**
  - **Why needed here:** The core reduction scheme depends on knowing $g(k)$ (e.g., $O(1/\sqrt{k})$ for Subgradient Descent). You must know which optimizer provides which rate for specific function classes (smooth vs. non-smooth) to plug into the F-LCB equation.
  - **Quick check question:** If you switch the base optimizer from Gradient Descent to Accelerated Gradient Descent on a smooth convex function, how does the regret bound of F-LCB change?

- **Concept: Black-Box Optimization Oracles**
  - **Why needed here:** The paper treats functions as "arms" accessible only via an oracle $O(x)$. Understanding the difference between zero-order (function value only) and first-order (gradient) oracles is critical for implementing the base optimizers.
  - **Quick check question:** If you only have a zero-order oracle (no gradients), which base optimizer from the paper's experiments would you be forced to use (or modify)?

## Architecture Onboarding

- **Component map:** F-LCB Core -> Base Optimizer Wrapper -> LCB Evaluator -> Oracle

- **Critical path:**
  1. **Initialization:** Run 1 step of the base optimizer for all K functions to initialize LCB
  2. **Selection:** Identify $i_t = \text{argmin}_i LCB_i$ (optimistic selection)
  3. **Execution:** Call `Optimizer.Step(function_i, x_i)`
  4. **Update:** Update $x_i$, increment $k_i$, and re-calculate $LCB_i$
  5. **Repeat:** Loop for total budget T

- **Design tradeoffs:**
  - **Heuristic vs. Rigorous LCB:** Using theoretical bounds guarantees regret but restricts you to convex problems; heuristics allow application to Deep Learning but lose theoretical safety
  - **Base Optimizer Choice:** AGD yields $O(1/t^2)$ rates (better for smooth functions), while Subgradient methods yield $O(1/\sqrt{t})$. AGD is less robust to noise

- **Failure signatures:**
  - **Premature Convergence:** Poor initialization or high noise can cause a suboptimal arm's LCB to stay artificially low
  - **Non-convex Oscillation:** In non-convex settings, learning rate spikes can cause the LCB bound to break or fluctuate wildly

- **First 3 experiments:**
  1. **Sanity Check (Synthetic Smooth):** Implement F-LCB with AGD on 3 quadratic functions with distinct minima (replicate Figure 1)
  2. **Stress Test (Stochastic):** Add Gaussian noise to gradients (replicate Figure 3) and tune confidence parameter δ
  3. **Real-world Application (CIFAR):** Apply heuristic F-LCB to select between ResNet and MLP (Section 4.4)

## Open Questions the Paper Calls Out
None

## Limitations
- Strong structural assumptions about function classes and oracle access constrain applicability
- Theoretical guarantees critically depend on convergence rates that may not hold for non-convex, non-smooth, or non-Lipschitz functions
- Heuristic approach for deep learning applications lacks theoretical grounding, making performance predictions unreliable for general non-convex problems

## Confidence
- **High Confidence:** Deterministic FMAB regret bounds (Theorems 1 and 2) and BFI lower bound (Theorem 4) are well-supported by mathematical proofs
- **Medium Confidence:** Stochastic FMAB analysis (Theorem 3) and experimental results are reasonable but rely on strong assumptions about noise structure
- **Low Confidence:** Claim that framework "generalizes" classic MAB is somewhat overstated—requires significantly more structure than standard bandits

## Next Checks
1. Test F-LCB on functions with heavy-tailed or unbounded noise to quantify how violations of noise assumptions affect regret bounds
2. Systematically evaluate the heuristic F-LCB on a benchmark suite of non-convex optimization problems to characterize when the approach breaks down
3. Compare total oracle calls between F-LCB and standard optimization baselines across varying numbers of candidate functions and budget constraints