---
ver: rpa2
title: Scale-Wise VAR is Secretly Discrete Diffusion
arxiv_id: '2509.22636'
source_url: https://arxiv.org/abs/2509.22636
tags:
- diffusion
- discrete
- srdd
- generation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical connection between autoregressive
  (AR) visual generation and discrete diffusion models, showing that a Markovian variant
  of the VAR model is mathematically equivalent to a structured discrete diffusion
  process. By reinterpreting VAR through this lens, the authors introduce SRDD (Scalable
  Visual Refinement with Discrete Diffusion), which enables principled integration
  of diffusion-style techniques such as classifier-free guidance, token resampling,
  and distillation into AR generation.
---

# Scale-Wise VAR is Secretly Discrete Diffusion
## Quick Facts
- arXiv ID: 2509.22636
- Source URL: https://arxiv.org/abs/2509.22636
- Authors: Amandeep Kumar; Nithin Gopalakrishnan Nair; Vishal M. Patel
- Reference count: 18
- Primary result: Markovian VAR is mathematically equivalent to discrete diffusion, enabling state-of-the-art visual generation performance.

## Executive Summary
This paper establishes a theoretical connection between autoregressive (AR) visual generation and discrete diffusion models, showing that a Markovian variant of the VAR model is mathematically equivalent to a structured discrete diffusion process. By reinterpreting VAR through this lens, the authors introduce SRDD (Scalable Visual Refinement with Discrete Diffusion), which enables principled integration of diffusion-style techniques such as classifier-free guidance, token resampling, and distillation into AR generation. Empirically, SRDD achieves state-of-the-art performance on multiple datasets (MiniImageNet, SUN397, FFHQ, AFHQ), with FID scores improving over VAR by up to 20.2% and IS scores increasing by up to 31.1%. The method also demonstrates superior zero-shot performance on inpainting, outpainting, and super-resolution tasks.

## Method Summary
The method replaces VAR's block-wise causal attention mask with a Markovian mask that restricts each scale to attend only to the immediate previous scale, creating a Markov chain over scales equivalent to discrete diffusion. This reformulation enables principled integration of classifier-free guidance, masked resampling (MR) for token refinement, and scale distillation. The model uses a frozen VQ-VAE codebook shared across scales and trains with cross-entropy loss using AdamW optimizer. At inference, MR resamples low-confidence tokens (probability <0.01) for 5-25 iterations per scale, while CFG (optimal at 5.0) provides stable conditioning.

## Key Results
- SRDD achieves up to 20.2% FID improvement over VAR on MiniImageNet and SUN397 datasets
- IS scores increase by up to 31.1% compared to VAR baseline
- Zero-shot inpainting, outpainting, and super-resolution tasks show significant performance gains
- Inference speed improves 1.75× with 3× memory reduction compared to original VAR

## Why This Works (Mechanism)
### Mechanism 1
A Markovian variant of VAR (conditioning only on the immediate previous scale) is mathematically equivalent to a structured discrete diffusion process. Replacing VAR's block-wise causal attention mask with a Markovian mask restricts each scale to attend only to scale $s-1$, yielding a Markov chain satisfying the same transition structure as discrete diffusion with a deterministic degradation matrix $M(n)$ for downsampling.

### Mechanism 2
Classifier-free guidance (CFG) interacts predictably with the Markovian formulation, enabling stronger conditioning without collapse. In the Markovian variant, iterative resampling re-injects entropy, stabilizing guidance. The diffusion interpretation provides a principled cfg schedule similar to continuous diffusion's timestep conditioning.

### Mechanism 3
Masked Resampling (MR) refines low-confidence tokens per scale, improving fidelity without full regeneration. At each scale, tokens with prediction probability below threshold (empirically $\approx 0.01$) are resampled multiple times (5-25 iterations), leveraging remaining high-confidence context as conditioning.

## Foundational Learning
- **Discrete Diffusion (D3PMs)**: SRDD reinterprets VAR as discrete diffusion with deterministic transition matrix. Understanding forward/reverse processes, ELBO, and categorical transitions is essential.
  - Quick check: Given a transition matrix $Q_t$ over discrete vocabulary, write forward process $q(x_t|x_0)$ and explain how reverse $p_\theta(x_{t-1}|x_t)$ is trained.
- **Multi-Scale Tokenization (VQ-VAE)**: VAR and SRDD operate on multi-scale discrete token grids from shared VQ-VAE codebook. Understanding residual accumulation across scales is critical.
  - Quick check: If $I_N$ is highest-resolution token grid, how is $I_n$ derived via $M(n)$, and what does residual $I_n - (I_{n-1})^{\uparrow(n)}$ represent?
- **Autoregressive vs Markov Factorization**: Key architectural change replaces full autoregressive conditioning with Markov conditioning. Understanding ELBO implications clarifies why Markovian assumption aligns with diffusion.
  - Quick check: Compare ELBO for autoregressive model and Markov chain. Why does Markovian assumption yield tighter bound under certain conditions?

## Architecture Onboarding
- **Component map**: VQ-VAE tokenizer -> Transformer backbone (Markovian attention) -> Scale embeddings -> CFG conditioning -> MR refinement
- **Critical path**: 1) Freeze VQ-VAE codebook, 2) Initialize transformer with VAR weights, 3) Replace attention mask with Markovian mask, 4) Train with cross-entropy loss (AdamW, β1=0.95, β2=0.05, lr=1e-4), 5) Apply CFG (cfg=5.0) and MR (threshold 0.01, 5 iterations) at inference, 6) Optionally distill by pruning early scales
- **Design tradeoffs**: Markovian vs full autoregressive reduces KV cache memory (~3×) and compute (~1.75× speedup) but may lose long-range dependencies; MR threshold and iterations balance quality vs speed; scale pruning yields speed gains with minimal FID degradation
- **Failure signatures**: CFG > 10 in original VAR causes FID collapse; MR threshold < 0.001 insufficient refinement; MR threshold > 0.1 excessive token replacement; random sparse scale schedules harm FID
- **First 3 experiments**: 1) Ablate attention mask: Compare VAR vs SDD on MiniImageNet (expect ~14-20% FID reduction), 2) Sweep MR threshold and iterations to find compute-optimal frontier, 3) Zero-shot inpainting/outpainting on AFHQ to quantify gains

## Open Questions the Paper Calls Out
- Can a learned policy network replace the fixed probability threshold in Masked Resampling to optimize token selection dynamically? The authors suggest this might yield further gains.
- Does a hybrid pipeline combining SRDD for coarse scales with continuous decoder for high-frequency details outperform purely discrete models? The authors propose exploring continuous-discrete hybrid diffusion.
- Do the performance gains of Markovian attention mask hold for complex, text-conditional generation tasks? Current evaluations are restricted to class-conditional datasets.

## Limitations
- The theoretical equivalence relies on structural similarity rather than exhaustive verification of the full generative process
- The claim about CFG "predictability" lacks formal analysis of why guidance scale maps cleanly between continuous and discrete domains
- Empirical success may be due to improved optimization dynamics rather than fundamental alignment with diffusion principles

## Confidence
- **High confidence**: Empirical improvements from SRDD (FID/IS gains, zero-shot task performance) are well-documented across multiple datasets with clear ablation studies
- **Medium confidence**: Markovian reformulation as discrete diffusion is mathematically sound but proof relies on structural similarity
- **Low confidence**: Claim about CFG "predictability" lacks theoretical grounding

## Next Checks
1. **ELBO verification**: Derive and compare evidence lower bounds for full autoregressive VAR, Markovian VAR, and discrete diffusion to quantify information-theoretic cost of Markovian assumption
2. **Long-range dependency test**: Design synthetic datasets with controlled long-range spatial correlations to measure whether Markovian conditioning introduces measurable fidelity loss in structured images
3. **CFG schedule ablation**: Systematically vary CFG scale parameter across discrete diffusion timeline to identify whether optimal guidance schedule follows continuous diffusion principles or represents new regime specific to discrete token modeling