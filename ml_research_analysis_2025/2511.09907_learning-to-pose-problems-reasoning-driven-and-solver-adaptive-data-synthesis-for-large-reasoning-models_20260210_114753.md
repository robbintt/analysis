---
ver: rpa2
title: 'Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis
  for Large Reasoning Models'
arxiv_id: '2511.09907'
source_url: https://arxiv.org/abs/2511.09907
tags:
- problem
- reasoning
- solver
- problems
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reasoning-driven problem generator that
  explicitly plans problem directions before synthesis and adapts difficulty to the
  solver's ability. The method constructs related problem pairs, augments them with
  intermediate problem-design chain-of-thought from a reasoning model, and uses solver
  feedback as a verifiable reward signal to calibrate difficulty.
---

# Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models

## Quick Facts
- arXiv ID: 2511.09907
- Source URL: https://arxiv.org/abs/2511.09907
- Reference count: 28
- Primary result: Introduces a reasoning-driven, solver-adaptive problem generator achieving 3.4% average improvement across 10 benchmarks, with further 0.7% gain via co-evolution

## Executive Summary
This paper presents a novel framework for synthesizing high-quality reasoning problems tailored to the strengths and weaknesses of large reasoning models. The approach uses a reasoning model to explicitly plan problem directions and generate intermediate reasoning traces, then adapts problem difficulty based on solver feedback. The system constructs related problem pairs and iteratively refines both generator and solver, enabling co-evolution for improved performance. Experiments demonstrate consistent gains over baseline synthesis methods, with robust generalization to both language and vision-language models.

## Method Summary
The method introduces a reasoning-driven problem generator that plans problem directions before synthesis and adapts difficulty to the solver's ability. It constructs related problem pairs, augments them with intermediate problem-design chain-of-thought from a reasoning model, and uses solver feedback as a verifiable reward signal to calibrate difficulty. The framework employs an iterative co-evolution process where a solver trained on synthesized data provides improved rewards for continued generator training, yielding further performance gains.

## Key Results
- Achieves an average improvement of 3.4% across 10 mathematical and general reasoning benchmarks
- Outperforms existing data synthesis methods by 2.17% on mathematical reasoning tasks and 2.98% on general-domain reasoning
- Co-evolution mechanism yields a further 0.7% performance gain by iteratively refining both generator and solver

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to explicitly plan problem directions using a reasoning model, generate intermediate reasoning traces for guidance, and adaptively calibrate problem difficulty based on solver feedback. This reasoning-driven, solver-adaptive approach ensures that generated problems are both relevant and appropriately challenging, maximizing learning efficiency for the target model.

## Foundational Learning
- **Problem direction planning**: Why needed - ensures generated problems target relevant reasoning skills; Quick check - verify planner produces coherent problem sequences
- **Solver-adaptive difficulty calibration**: Why needed - prevents over- or under-challenging the model; Quick check - confirm difficulty adjustment matches solver performance trends
- **Intermediate CoT generation**: Why needed - provides traceable reasoning for both problem design and solver training; Quick check - validate CoT traces are logically consistent
- **Verifiable reward signals**: Why needed - enables objective evaluation of problem difficulty; Quick check - test reward signal correlates with actual solver success rates
- **Iterative co-evolution**: Why needed - continuously improves both generator and solver quality; Quick check - monitor diversity and difficulty of generated problems over iterations
- **Related problem pair construction**: Why needed - creates structured learning progression; Quick check - ensure pairs exhibit meaningful difficulty gradients

## Architecture Onboarding
**Component Map**: Problem Planner -> Difficulty Calibrator -> CoT Generator -> Problem Synthesizer -> Solver Evaluator -> Reward Aggregator -> Generator Updater

**Critical Path**: Problem Planner → CoT Generator → Problem Synthesizer → Solver Evaluator → Reward Aggregator → Generator Updater (iterative loop)

**Design Tradeoffs**: Balances explicit reasoning planning against computational overhead; trades off diversity for targeted difficulty adaptation; iterative co-evolution risks overfitting but enables mutual improvement

**Failure Signatures**: 
- Poor planner quality leads to irrelevant or incoherent problems
- Inaccurate difficulty calibration causes solver disengagement or frustration
- CoT generation failures result in untraceable reasoning paths
- Reward signal misalignment produces suboptimal problem difficulty
- Co-evolution divergence leads to mode collapse or overfitting

**3 First Experiments**:
1. Test problem planner coherence by evaluating generated problem sequences for logical consistency
2. Validate difficulty calibration by comparing solver success rates before and after adjustment
3. Assess CoT generation quality by measuring trace consistency and relevance to problem solutions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to 10 benchmarks, raising questions about broader generalization
- Potential bottleneck and propagation of weaknesses from underlying reasoning model
- Computational overhead of adaptive synthesis loop not quantified
- Co-evolution risks overfitting to solver idiosyncrasies and may reduce problem diversity over time

## Confidence
- **High Confidence**: Core methodology (reasoning-driven planning, solver-adaptive calibration, verifiable rewards) is technically sound with consistent performance improvements
- **Medium Confidence**: Generalization to vision-language models and effectiveness of co-evolution mechanism, though evaluation scope is limited
- **Low Confidence**: Long-term stability and diversity of co-evolution process, and absence of computational cost analysis

## Next Checks
1. Evaluate framework on broader, more diverse reasoning benchmarks (e.g., code, scientific reasoning, multi-step logic) to assess generalization
2. Run multiple co-evolution iterations and analyze problem diversity and difficulty distribution to detect overfitting or mode collapse
3. Quantify computational cost (GPU hours, inference steps) of adaptive synthesis loop and benchmark against simpler baselines for practical viability