---
ver: rpa2
title: 'Explainable AI for Smart Greenhouse Control: Interpretability of Temporal
  Fusion Transformer in the Internet of Robotic Things'
arxiv_id: '2512.11852'
source_url: https://arxiv.org/abs/2512.11852
tags:
- control
- greenhouse
- actuator
- lime
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for transparency in autonomous greenhouse
  control systems by proposing an interpretable deep learning framework that integrates
  a Temporal Fusion Transformer (TFT) with explainability methods. The model predicts
  actuator settings for environmental control using multivariate sensor data and incorporates
  Variable Selection Network (VSN), SHAP, and LIME to provide both global and local
  explanations.
---

# Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things

## Quick Facts
- arXiv ID: 2512.11852
- Source URL: https://arxiv.org/abs/2512.11852
- Authors: Muhammad Jawad Bashir; Shagufta Henna; Eoghan Furey
- Reference count: 27
- Primary result: 95% classification accuracy on class-imbalanced dataset with interpretable TFT model for greenhouse actuator control

## Executive Summary
This paper addresses the critical need for transparency in autonomous greenhouse control systems by integrating a Temporal Fusion Transformer (TFT) with explainability methods. The proposed framework predicts actuator settings for environmental control using multivariate sensor data while providing interpretable insights into the model's decision-making process. By combining Variable Selection Network, SHAP, and LIME techniques, the system offers both global and local explanations of how environmental factors influence control decisions. The approach enhances trust, supports model debugging, and enables domain experts to fine-tune control strategies in precision agriculture applications.

## Method Summary
The proposed framework integrates a Temporal Fusion Transformer with explainability methods to create an interpretable autonomous greenhouse control system. The TFT model processes multivariate sensor data including temperature, humidity, CO2 levels, light, and time-lagged values to predict optimal actuator settings. Variable Selection Network identifies relevant input features, while SHAP and LIME provide both global and local interpretability. The model was trained and evaluated on a class-imbalanced dataset, achieving 95% classification accuracy. The explainability components work together to reveal how specific environmental conditions influence control decisions, making the black-box model transparent to domain experts.

## Key Results
- Achieved 95% classification accuracy on a class-imbalanced dataset
- Successfully integrated TFT with SHAP and LIME for interpretable greenhouse control
- Demonstrated feature importance insights showing how temperature, humidity, CO2, light, and time-lagged values influence actuator decisions
- Validated the approach's potential for enhancing trust and enabling expert fine-tuning in precision agriculture

## Why This Works (Mechanism)
The approach works by leveraging TFT's ability to handle multivariate time series data while incorporating attention mechanisms that capture temporal dependencies. The Variable Selection Network filters relevant features before they enter the TFT, reducing noise and improving prediction accuracy. SHAP and LIME then decompose the model's complex decision-making into interpretable components by analyzing feature contributions at both global (dataset-wide) and local (individual prediction) levels. This combination allows the system to maintain high predictive performance while providing actionable insights into the reasoning behind each control decision.

## Foundational Learning

**Temporal Fusion Transformer (TFT)**
- *Why needed*: Handles multivariate time series data with varying temporal dependencies and missing values
- *Quick check*: Verify the model can process sequences of varying lengths and handle missing sensor readings

**SHAP (SHapley Additive exPlanations)**
- *Why needed*: Provides game-theoretic attribution of feature importance with solid theoretical foundations
- *Quick check*: Confirm SHAP values sum to the difference between model prediction and baseline

**LIME (Local Interpretable Model-agnostic Explanations)**
- *Why needed*: Offers local interpretability by approximating complex models with simpler, interpretable ones
- *Quick check*: Test that the local linear approximation accurately represents the model's behavior in the neighborhood of specific predictions

## Architecture Onboarding

**Component Map**
TFT -> Variable Selection Network -> SHAP/LIME -> Interpretability Layer -> Actuator Control Output

**Critical Path**
Sensor data → Variable Selection Network → TFT processing → SHAP/LIME analysis → Actuator control decision with explanation

**Design Tradeoffs**
The system balances predictive accuracy (achieved through complex TFT architecture) with interpretability (through SHAP/LIME). The Variable Selection Network adds computational overhead but improves both accuracy and interpretability by focusing on relevant features. The choice of classification over regression for actuator control simplifies interpretation but may reduce precision in continuous control scenarios.

**Failure Signatures**
- Poor Variable Selection Network performance leading to noisy feature inputs
- SHAP/LIME explanations that contradict domain knowledge, indicating potential model misalignment
- Degradation in accuracy when handling previously unseen environmental conditions
- Temporal dependencies not captured properly, resulting in delayed or inappropriate control responses

**3 First Experiments**
1. Ablation study removing Variable Selection Network to quantify its impact on accuracy and interpretability
2. Cross-validation on different greenhouse datasets to test generalizability
3. Comparison of SHAP vs LIME explanation consistency and alignment with expert knowledge

## Open Questions the Paper Calls Out
None

## Limitations
- Limited information about dataset size, distribution, and external validation
- Explainability methods may not fully capture complex temporal dynamics in black-box models
- No experimental validation of edge deployment or active learning integration
- Focus on actuator control prediction without addressing safety or robustness in real-world deployment

## Confidence

**High Confidence**
- TFT model's 95% accuracy is well-supported with appropriate methodology
- Use of established XAI techniques (SHAP, LIME) with solid theoretical foundations

**Medium Confidence**
- Interpretability insights provided by XAI methods are plausible but require expert validation
- Integration approach appears sound but needs real-world testing

**Low Confidence**
- Future work on edge deployment and active learning lacks experimental evidence
- No safety or robustness validation for real-world deployment scenarios

## Next Checks
1. Conduct external validation on a separate, independently collected greenhouse dataset to assess generalizability
2. Perform user study with domain experts to evaluate practical utility and interpretability of explanations
3. Prototype edge deployment and test performance under resource-constrained conditions to validate feasibility