---
ver: rpa2
title: 'CAMEO: Collection of Multilingual Emotional Speech Corpora'
arxiv_id: '2505.11051'
source_url: https://arxiv.org/abs/2505.11051
tags:
- speech
- emotional
- datasets
- dataset
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CAMEO, a publicly available collection of
  13 multilingual emotional speech datasets spanning 8 languages and 17 emotions.
  The collection provides standardized metadata, audio normalization, and open evaluation
  tools to facilitate reproducible benchmarking in speech emotion recognition (SER).
---

# CAMEO: Collection of Multilingual Emotional Speech Corpora

## Quick Facts
- arXiv ID: 2505.11051
- Source URL: https://arxiv.org/abs/2505.11051
- Authors: Iwona Christop; Maciej Czajka
- Reference count: 0
- Primary result: Collection of 13 multilingual emotional speech datasets spanning 8 languages and 17 emotions with standardized evaluation protocol

## Executive Summary
This work introduces CAMEO, a publicly available collection of 13 multilingual emotional speech datasets spanning 8 languages and 17 emotions. The collection provides standardized metadata, audio normalization, and open evaluation tools to facilitate reproducible benchmarking in speech emotion recognition (SER). Four models were evaluated across different temperature settings, with the best model (Qwen2-Audio) achieving a macro F1 score of 0.20, indicating the inherent difficulty of multilingual zero-shot SER. Sadness was most reliably recognized, while neutral speech posed the greatest challenge. Performance disparities across languages and datasets suggest possible training data overlap, highlighting the need for careful dataset curation and evaluation protocols. CAMEO is released with a public leaderboard on Hugging Face to support ongoing model comparison and future expansion.

## Method Summary
CAMEO curates 13 emotional speech datasets totaling 41,265 samples across 8 languages and 17 emotions. The authors unified metadata and transcription formats, converted all audio to FLAC (16-bit, 16 kHz), and serialized metadata into JSON Lines format. The collection provides open evaluation tools including a Hugging Face leaderboard where models are evaluated zero-shot using text instructions and audio inputs. A Levenshtein-based post-processing approach normalizes model responses to valid emotion labels. The evaluation uses macro-averaged F1 score as the primary metric to account for imbalanced emotion distributions across datasets.

## Key Results
- Qwen2-Audio achieved the highest macro F1 score of 0.20 across all evaluated models
- Sadness was most reliably recognized due to distinctive acoustic cues like reduced pitch variation and slower tempo
- Neutral speech was the most challenging category with F1 scores of only 0.03-0.06
- Performance disparities across languages and datasets suggest potential training data contamination

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standardized audio formatting and unified metadata enable cross-dataset benchmarking that would otherwise be impossible due to heterogeneity.
- **Mechanism:** Converting all audio to FLAC (16-bit, 16 kHz) and serializing metadata into JSON Lines format creates a consistent interface. This eliminates per-dataset preprocessing pipelines and allows models to be evaluated uniformly across 13 corpora.
- **Core assumption:** Emotion labels are comparable across datasets despite different annotation protocols and cultural contexts.
- **Evidence anchors:**
  - [abstract] "The authors curated 13 datasets totaling 41,265 samples, unified metadata and transcription formats"
  - [section 3.3] "Audio samples were converted to FLAC (16-bit, 16 kHz). Metadata and transcriptions were unified across datasets."
  - [corpus] Neighbor corpus EM2LDL addresses similar heterogeneity through label distribution learning, suggesting standardization remains an open challenge.
- **Break condition:** If emotion taxonomies are fundamentally incompatible across languages/cultures, unified benchmarking yields misleading comparisons.

### Mechanism 2
- **Claim:** Levenshtein-based post-processing recovers valid emotion labels from generative model outputs without penalizing lexical variation.
- **Mechanism:** When models generate descriptive responses ("The answer is sadness") instead of single-word labels, the system splits responses into words, computes Levenshtein ratios against target labels, filters by threshold (0.57), and aggregates scores. This bridges the gap between generative behavior and classification requirements.
- **Core assumption:** Noun-adjective pairs (e.g., "sadness"/"sad") represent the primary lexical variation; other variations are noise.
- **Evidence anchors:**
  - [section 4] "If a generated response is not an exact match for any of the labels, it is normalized and split into words. Then, the Levenshtein ratio between each target label and each word in the generated response is calculated."
  - [Table 3] Documents Levenshtein ratios between noun and adjective forms (range: 0.57–0.94), justifying the 0.57 threshold.
  - [corpus] No direct corpus comparison; mechanism is paper-specific.
- **Break condition:** If models generate emotionally relevant but lexically distant synonyms (e.g., "grief" for "sadness"), the threshold will filter them out incorrectly.

### Mechanism 3
- **Claim:** Low zero-shot SER performance (macro F1 ≤ 0.20) reflects a fundamental gap between acoustic pretraining objectives and emotion label alignment across languages.
- **Mechanism:** AudioLLMs are trained primarily on speech recognition or general audio tasks, not emotion classification. Without task-specific fine-tuning, models cannot reliably map acoustic emotion cues (pitch variation, tempo) to discrete labels—especially across languages with different prosodic patterns.
- **Core assumption:** The evaluated models had limited or no exposure to emotion-annotated speech during pretraining.
- **Evidence anchors:**
  - [abstract] "achieving low performance across all models (macro F1 up to 0.20), highlighting the challenge of multilingual SER in zero-shot settings"
  - [section 5] "Sadness was the most reliably recognized emotion... due to the fact that sadness often exhibits more stable and distinctive acoustic cues, such as reduced pitch variation and slower tempo. In contrast, neutral speech is inherently heterogeneous."
  - [corpus] Neighbor paper "Large Language Models Meet Contrastive Learning" explicitly addresses zero-shot SER limitations using contrastive learning, corroborating the difficulty claim.
- **Break condition:** If observed low performance stems primarily from data contamination (models overfitting to seen datasets) rather than fundamental capability gaps, the zero-shot difficulty claim weakens. The paper notes potential contamination for CREMA-D and RAVDESS.

## Foundational Learning

- **Concept: Macro-averaged F1 score**
  - **Why needed here:** The primary evaluation metric; equally weights all emotion classes regardless of frequency. Critical because emotion distributions are imbalanced (neutral/anger more common than disgust/surprise).
  - **Quick check question:** If a model achieves 90% accuracy but only predicts "neutral" for all samples, what would its macro F1 score be? (Answer: Low, because per-class F1 for non-neutral emotions would be 0.)

- **Concept: Zero-shot evaluation**
  - **Why needed here:** The benchmark evaluates models without task-specific fine-tuning. Understanding this explains why performance is low and what the results actually measure (generalization, not task mastery).
  - **Quick check question:** What is the difference between zero-shot evaluation and few-shot prompting? Which does CAMEO use? (Answer: Zero-shot = no examples provided; few-shot = examples given. CAMEO uses zero-shot with single audio input + instruction.)

- **Concept: Data contamination in benchmarks**
  - **Why needed here:** The paper notes that elevated performance on CREMA-D/RAVDESS may indicate training exposure, not true generalization. This affects how you interpret leaderboard results.
  - **Quick check question:** If a model scores 0.80 on CREMA-D but 0.12 on SUBESCO (Bengali), what are two possible explanations? (Answer: 1) CREMA-D was in training data (contamination); 2) Model generalizes better to English-accented emotional speech than Bengali.)

## Architecture Onboarding

- **Component map:** 13 datasets → unified FLAC audio + JSON Lines metadata (file_id, emotion, transcription, speaker_id, gender, age, dataset, language, license) → Hugging Face dataset loader → model inference (text instruction + audio) → post-processing (Levenshtein matching) → metrics (macro F1, weighted F1, accuracy) → Public leaderboard on Hugging Face Spaces

- **Critical path:**
  1. Load sample (audio + metadata) from Hugging Face
  2. Construct prompt: instruction + audio input (no speaker/language metadata)
  3. Generate model response
  4. Apply post-processing: exact match → if fail, Levenshtein filtering (threshold 0.57) → aggregated score → best match
  5. Compute per-emotion and aggregate metrics

- **Design tradeoffs:**
  - **No train/test splits:** Maximizes data for benchmarking but prevents contamination-controlled evaluation. Assumption: users will manage their own splits if training.
  - **Post-processing threshold (0.57):** Chosen based on noun-adjective pairs; may miss valid synonyms. Lower threshold = more false positives; higher = more false negatives.
  - **Single-word output instruction:** Models often ignore this; post-processing compensates but adds complexity.

- **Failure signatures:**
  - **Models generate adjectives instead of nouns:** "happy" vs. "happiness" → caught by post-processing
  - **Models generate sentences:** "The emotion is sadness" → caught by post-processing
  - **Models generate invalid emotions:** "excited" (not in label set) → filtered out, likely misclassified
  - **Neutral speech misclassification:** Most common failure (F1 = 0.03–0.06); confused with low-arousal emotions

- **First 3 experiments:**
  1. **Baseline reproduction:** Load CAMEO from Hugging Face, run Qwen2-Audio with temperature=0.0, verify macro F1 ≈ 0.20. This confirms your pipeline matches the paper's.
  2. **Ablate post-processing:** Bypass Levenshtein matching and require exact label matches. Expect macro F1 to drop significantly (models generate varied responses).
  3. **Language-specific analysis:** Filter to high-resource (English, French, German) vs. low-resource (Bengali, Polish) languages. Compare per-language F1 to quantify the performance gap and assess whether multilingual training is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the expansion of CAMEO to include low-resource languages impact the cross-lingual generalization of current AudioLLMs?
- Basis in paper: [explicit] The conclusion states future work will expand the collection to include "datasets representing low-resource languages and underrepresented emotional states."
- Why unresolved: The current collection is dominated by English (~1/3 of samples), which may mask the specific difficulties models face with languages lacking substantial training data.
- Evidence: Comparative benchmark results (Macro F1) on new, explicitly low-resource languages versus the current high-resource baseline.

### Open Question 2
- Question: To what extent do high performance scores on specific datasets (e.g., CREMA-D) indicate data contamination rather than genuine emotional understanding?
- Basis in paper: [inferred] The authors note that elevated performance on widely used benchmarks suggests these datasets "were likely encountered during training."
- Why unresolved: The paper evaluates zero-shot capabilities but cannot definitively separate memorization from generalization without auditing the models' pre-training data.
- Evidence: A comparative analysis of model performance on held-out, novel datasets versus the "contaminated" widely-used datasets within the same languages.

### Open Question 3
- Question: What specific modeling improvements are required to distinguish "neutral" speech from low-arousal states like sadness?
- Basis in paper: [inferred] The analysis highlights that "neutral speech was the most challenging" category, frequently misclassified due to acoustic overlaps with low-arousal emotions.
- Why unresolved: Current AudioLLMs appear to rely on salient prosodic markers absent in neutral speech, leading to a systematic failure mode.
- Evidence: A targeted ablation study or fine-tuning experiment focusing on the decision boundary between neutral and sad emotion classes.

## Limitations
- Potential data contamination in widely-used datasets (CREMA-D, RAVDESS) may inflate performance scores
- Assumption that emotion labels are directly comparable across different cultural contexts may be invalid
- Arbitrary Levenshtein threshold may not generalize to all lexical variations across languages

## Confidence
- **High confidence:** Dataset curation and unification methodology is reproducible and well-documented
- **Medium confidence:** Evaluation methodology is internally consistent but contamination concerns limit broader claims
- **Low confidence:** Claims about fundamental difficulty of multilingual SER should be viewed cautiously due to potential contamination

## Next Checks
1. Analyze the training data of evaluated models to confirm or deny overlap with CREMA-D and RAVDESS
2. Conduct a pilot study with native speakers to verify whether emotion labels are semantically equivalent across cultures
3. Implement a few-shot evaluation protocol using CAMEO's train/test splits and compare results to zero-shot scores