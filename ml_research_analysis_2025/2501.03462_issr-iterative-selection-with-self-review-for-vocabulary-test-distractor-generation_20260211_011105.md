---
ver: rpa2
title: 'ISSR: Iterative Selection with Self-Review for Vocabulary Test Distractor
  Generation'
arxiv_id: '2501.03462'
source_url: https://arxiv.org/abs/2501.03462
tags:
- distractors
- distractor
- word
- vocabulary
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using large language models (LLMs) to automatically
  generate distractors for English vocabulary test questions. Previous methods rely
  on lexical databases and often produce distractors that can invalidate the question
  by introducing multiple correct answers.
---

# ISSR: Iterative Selection with Self-Review for Vocabulary Test Distractor Generation

## Quick Facts
- arXiv ID: 2501.03462
- Source URL: https://arxiv.org/abs/2501.03462
- Reference count: 38
- Primary result: ISSR achieves F1@3 of 1.55% compared to 0.35-0.51% for other methods

## Executive Summary
This paper addresses the challenge of automatically generating high-quality distractors for English vocabulary test questions using large language models (LLMs). Previous methods relying on lexical databases often produce distractors that introduce multiple correct answers, invalidating questions. The authors propose the Iterative Selection with Self-Review (ISSR) framework, which uses a two-stage approach: generating a candidate pool via masked language models, then selecting and validating distractors through LLMs with a binary-choice self-review mechanism. ISSR outperforms direct LLM generation and other baselines on GSAT vocabulary questions, achieving significantly higher F1@3 scores while effectively filtering invalid distractors.

## Method Summary
ISSR uses a three-module pipeline: (1) Candidate Generator using CDGP-CSG (BERT-base fine-tuned on CLOTH) to predict masked words and filter candidates by part-of-speech, difficulty level (≤1 level difference), and length (≤2 character difference), producing up to 50 candidates; (2) Distractor Selector using GPT-3.5 with zero-shot prompting to select top-k distractors from the candidate pool in batches of 3; (3) Distractor Validator using binary-choice self-review where each distractor is paired with the target word and rejected if the LLM selects the distractor as correct. The process iterates until 3 valid distractors are obtained. Temperature is set to 0.7 for distractor selection.

## Key Results
- ISSR achieves F1@3 of 1.55% compared to 0.35-0.51% for other methods
- Self-review mechanism filters invalid distractors with 97.2% accuracy on binary choice validation
- Selecting distractors from candidate pool is more effective than direct LLM generation
- Requesting fewer distractors per round (size 3) improves selection quality versus larger batches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting distractors from a curated candidate pool outperforms direct LLM generation.
- Mechanism: PLMs generate contextually plausible candidates via masked word prediction; LLMs then rank/select rather than generate from scratch. This bypasses LLM instability and repetition when generating large quantities.
- Core assumption: LLMs have stronger discriminative judgment than generative consistency for this task.
- Evidence anchors: Abstract states "Results show that selecting distractors from a candidate pool is more effective than direct generation"; Table 2 shows ISSR F1@3=1.55% vs GPT-3.5 few-shot=0.35%; related work (Difficulty-Controllable Cloze Question Distractor Generation) similarly separates candidate generation from ranking.
- Break condition: If candidate pool lacks any plausible distractors, selection cannot recover quality.

### Mechanism 2
- Claim: Binary choice self-review filters distractors that would create multiple correct answers.
- Mechanism: Each distractor is paired with the correct answer in a binary-choice prompt; if the LLM selects the distractor as correct, it is rejected. This leverages LLMs' strength at solving vocabulary questions (95-100% accuracy per Appendix E).
- Core assumption: LLMs can reliably distinguish correct from incorrect options when explicitly forced to choose.
- Evidence anchors: Abstract states "the self-review mechanism effectively filters out distractors that could invalidate the question"; Table 5 shows binary choice validation: 563/579 correct selections (97.2%); outperforms independent suitability judgment (4/579) and semantic consistency check (397/579).
- Break condition: If LLM accuracy on vocabulary questions drops significantly, false positives/negatives in filtering increase.

### Mechanism 3
- Claim: Requesting fewer distractors per round improves selection quality.
- Mechanism: Smaller batch sizes (3 vs 10 vs 30) reduce LLM confusion and output drift; iterative rounds maintain selection consistency.
- Core assumption: LLM attention degrades with larger candidate set sizes and more simultaneous selections.
- Evidence anchors: Table 7 shows selection size 3 yields F1@3=1.55%, NDCG@3=3.57%; size 30 yields F1@3=0.52%, NDCG@3=1.30%; Table 5 shows selection rate drops from 98.79% (size 50) to 90.67% (size 300).
- Break condition: If latency constraints prevent multiple iterative rounds, larger batches may be necessary with quality tradeoffs.

## Foundational Learning

- Concept: **Cloze-style vocabulary questions**
  - Why needed here: ISSR targets fill-in-the-blank format (GSAT style) where context constrains valid answers; different from synonym-selection formats (TOEFL iBT).
  - Quick check question: Given "The dancer moved with remarkable ____ across the stage" (target: grace), would "speed" be a valid distractor? Why or why not?

- Concept: **Distractor validity criteria (Heaton 1988)**
  - Why needed here: Filtering rules enforce same part-of-speech, similar difficulty level (≤1 level difference), similar length (≤2 characters), no synonym pairs.
  - Quick check question: If target word is "abandon" (verb, level 4), which is invalid: "desert" (verb, level 3) or "abandoned" (adjective, level 5)?

- Concept: **Masked Language Model prediction**
  - Why needed here: Candidate generator uses BERT-style PLM to predict plausible fillers for [MASK] token; understanding prediction distribution informs candidate pool quality.
  - Quick check question: Why might a standard BERT model generate suboptimal distractors compared to CDGP-CSG fine-tuned on CLOTH?

## Architecture Onboarding

- Component map: Candidate Generator (CDGP-CSG/BERT-base) -> Distractor Selector (GPT-3.5/Llama) -> Distractor Validator (Self-review) -> Iteration Controller
- Critical path: Stem + target word -> mask + predict -> filter candidates (PoS, difficulty, length) -> LLM selects 3 -> validate each via binary choice -> return valid set or iterate
- Design tradeoffs: Candidate pool size: 50 balances selection variety vs LLM attention stability; Selection batch size: 3 per round optimizes quality but increases API calls; Validator prompt: Binary choice outperforms open-ended suitability judgment (97% vs 0.7% accuracy)
- Failure signatures: High repetition in direct generation indicates need for selection-based approach; LLM outputs words not in candidate set means candidate pool too large; Multiple "valid" answers detected means self-review mechanism triggered
- First 3 experiments: 1) Validate candidate generator quality: Generate 50 candidates for 20 GSAT questions; compute recall of actual teacher-designed distractors within candidate pool; 2) Ablate self-review: Run ISSR with/without binary choice validation; measure invalid distractor rate via human annotation on 30 questions; 3) Stress-test batch size: Compare F1@3 for selection sizes {3, 10, 30} on held-out questions; verify Table 7 reproduction

## Open Questions the Paper Calls Out

- **Open Question 1**: How can distractor generation methods account for polysemous words tested in less common senses, which students find more challenging? Basis: Authors acknowledge not developing specific methods for polysemous words despite finding questions testing uncommon meanings have 4.10% lower pass rates. Unresolved because ISSR framework does not incorporate polysemy-aware distractor generation.
- **Open Question 2**: Can ISSR's computational efficiency be improved while maintaining distractor quality, particularly for the self-review mechanism? Basis: Authors acknowledge ISSR requires significantly more computing resources than similar work and generation is slow. Unresolved because self-review mechanism validates each distractor individually via binary choice queries, creating a bottleneck.
- **Open Question 3**: How well does ISSR generalize to vocabulary tests from different educational contexts (e.g., EIKEN, TEPS, TOEFL iBT) beyond Taiwan's GSAT? Basis: Authors note criteria and findings derived from GSAT dataset are tailored to its specific characteristics, limiting generalizability. Unresolved because ISSR's filtering rules are specific to Taiwanese high school exams.
- **Open Question 4**: Why does Llama3-8B perform comparably to Llama3-70B on distractor selection, and does this pattern hold across different candidate pool compositions? Basis: Table 4 shows Llama3-8B achieves similar performance to Llama3-70B, and authors note "model size does not necessarily correlate with improved performance" without further investigation.

## Limitations
- ISSR's performance depends heavily on LLM vocabulary proficiency, which may degrade with different model variants or domains
- Candidate pool quality directly constrains ISSR's upper performance bound - if CDGP-CSG fails to generate plausible distractors, selection cannot recover quality
- Study focuses exclusively on GSAT fill-in-the-blank vocabulary questions with single correct answers, limiting generalization to other test formats or subject domains
- Prompt engineering details are incomplete in the appendix, particularly formatting instructions for distractor selector and self-review validator

## Confidence
- **High**: ISSR outperforms direct LLM generation and baseline methods (F1@3=1.55% vs 0.35-0.51%). The two-stage selection approach with curated candidates is superior to generation-only methods.
- **Medium**: The self-review mechanism effectively filters invalid distractors (97.2% accuracy on binary choice validation). This depends on consistent LLM performance on vocabulary questions.
- **Medium**: Requesting fewer distractors per round improves quality (F1@3=1.55% for size 3 vs 0.52% for size 30). Batch-size effects are demonstrated but require more extensive validation.

## Next Checks
1. **Generalization Test**: Apply ISSR to a different vocabulary test format (e.g., TOEFL synonym selection) and measure performance drop. Compare candidate pool recall rates between GSAT and new dataset.
2. **Ablation Study**: Remove the self-review mechanism and measure invalid distractor rate on 50 questions via human annotation. Quantify the contribution of binary-choice validation to overall quality.
3. **Batch-Size Stress Test**: Systematically vary selection batch sizes (1, 3, 5, 10, 20) on 30 held-out questions. Plot F1@3 and NDCG@3 curves to identify optimal batch size for different question complexities.