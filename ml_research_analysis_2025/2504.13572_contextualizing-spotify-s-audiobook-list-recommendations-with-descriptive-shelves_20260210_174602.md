---
ver: rpa2
title: Contextualizing Spotify's Audiobook List Recommendations with Descriptive Shelves
arxiv_id: '2504.13572'
source_url: https://arxiv.org/abs/2504.13572
tags:
- shelves
- recommendations
- shelf
- user
- descriptors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating contextualized list
  recommendations for audiobooks on Spotify, where rich metadata is often limited.
  To overcome this, the authors use Large Language Models (LLMs) to enrich item metadata
  with descriptors based on a taxonomy (e.g., genres, themes, moods, characters) extracted
  from available information like title, author, and description.
---

# Contextualizing Spotify's Audiobook List Recommendations with Descriptive Shelves

## Quick Facts
- arXiv ID: 2504.13572
- Source URL: https://arxiv.org/abs/2504.13572
- Reference count: 27
- Primary result: Descriptive shelves on a dedicated audiobook subfeed significantly outperformed editorial shelves in engagement (+35.25% i2c, +86.96% i2s) and discovery (+627.27% # impressed, +804.56% # interacted)

## Executive Summary
This paper addresses the challenge of generating contextualized list recommendations for audiobooks on Spotify, where rich metadata is often limited. The authors use Large Language Models (LLMs) to enrich item metadata with descriptors based on a taxonomy (e.g., genres, themes, moods, characters) extracted from available information like title, author, and description. These enriched descriptors are then used to create personalized, descriptive shelves that group recommendations into thematic lists for each user. Two A/B tests were conducted: the first on Spotify's main home surface showed improved discovery but declined engagement, while the second on a dedicated audiobook subfeed with multiple shelves and clearer personalization indicators significantly outperformed editorially curated shelves in both engagement and discovery metrics.

## Method Summary
The method involves using LLMs to extract structured descriptors from sparse audiobook metadata using a 10-category taxonomy. These descriptors are then used to create personalized shelves through a pipeline of descriptor ranking, greedy diversification using content embeddings, and item filtering. The system was tested through two A/B experiments, with the second test demonstrating significant improvements in engagement and discovery when implemented on a dedicated audiobook subfeed with multiple descriptive shelves and personalization cues.

## Key Results
- First A/B test (main home surface): Discovery improved but engagement declined, likely due to limited shelf display and unclear personalization cues for cold-start users
- Second A/B test (dedicated audiobook subfeed): Descriptive shelves significantly outperformed editorial shelves with i2c +35.25%, i2s +86.96%, # impressed +627.27%, # interacted +804.56%
- Descriptive shelves enabled better discovery and engagement for users with explicit audiobook intent when implemented with proper surface context and personalization indicators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can reliably extract structured descriptors from sparse metadata when grounded in a predefined taxonomy
- **Mechanism:** The system uses a 10-category taxonomy (genres, themes, moods, characters, settings, personal situations, story tropes, target audiences, objective-based, named entities) derived from search queries and forum requests. The LLM receives item metadata (title, author, description, BISAC genres) and extracts descriptors for each category, returning empty lists when unavailable
- **Core assumption:** The taxonomy reflects how users naturally conceptualize and search for audiobooks
- **Evidence anchors:** [abstract]: "use Large Language Models (LLMs) to enrich each item's metadata based on a taxonomy created for this domain"; [section 3.1]: "Manual and automatic evaluations showed high accuracy in the task—by grounding the process on all the available metadata and instructing the model with the taxonomy, we minimize the risks of false positives"
- **Break condition:** LLM hallucinations produce descriptors not grounded in item metadata; taxonomy categories fail to match user mental models

### Mechanism 2
- **Claim:** Ranking descriptors by user affinity and diversifying by embedding similarity produces shelves that balance personalization with catalog exploration
- **Mechanism:** From a candidate recommendation list, the system extracts all distinct descriptors, ranks them by predicted relevance to the user, then applies greedy diversification using content embeddings to filter similar descriptors
- **Core assumption:** Users benefit from diverse topical entry points rather than multiple variations of the same theme
- **Evidence anchors:** [abstract]: "create several shelves for topics the user has an affinity to... recommend a diverse set of items"; [section 3.2]: "apply a greedy diversification approach that will filter out descriptors based on the similarity of content embeddings"
- **Break condition:** Over-diversification removes relevant descriptors; under-diversification creates redundant shelves that confuse users

### Mechanism 3
- **Claim:** Descriptive shelves with explicit personalization cues and multiple shelf displays significantly improve engagement for users with clear intent, but may harm engagement for cold-start users in low-context surfaces
- **Mechanism:** The first A/B test replaced a single generic shelf on the main home surface with one descriptive shelf. Discovery improved but engagement declined—likely because cold-start users lacked context and the single-shelf display limited topic diversity. The second test added "Audiobooks for you" labels and displayed multiple shelves in a dedicated subfeed where users had signaled audiobook intent
- **Core assumption:** Users with explicit intent benefit from granular thematic organization; cold-start users need broader, less specific recommendations
- **Evidence anchors:** [section 4]: "the interface did not indicate that the descriptive shelves are personalized... the majority of users in the test were cold-start users"; [section 4, Table 1]: Second test results—i2c +35.25%, i2s +86.96%, # impressed +627.27%, # interacted +804.56%
- **Break condition:** Applying descriptive shelves to cold-start populations on general surfaces without personalization cues; displaying single shelves when users expect variety

## Foundational Learning

- **Concept:** Cold-start problem in recommendations
  - **Why needed here:** Audiobooks are new to Spotify; users lack interaction history, and items lack rich metadata like reviews or tags
  - **Quick check question:** Can you explain why content-based approaches (using item metadata) are preferred over collaborative filtering when user-item interaction data is sparse?

- **Concept:** Greedy diversification in ranking
  - **Why needed here:** The shelf generation pipeline must avoid presenting semantically similar descriptors (e.g., "Romance" and "Romantic Comedy") as separate shelves
  - **Quick check question:** Given a list of descriptors ranked by relevance, how would you implement greedy diversification using embedding similarity?

- **Concept:** Trade-off between specificity and coverage in taxonomy design
  - **Why needed here:** Fine-grained descriptors (e.g., "Enemies to Lovers") may be too narrow for cold-start users; coarse descriptors (e.g., "Fiction") may not provide useful context
  - **Quick check question:** If 80% of your audiobooks share a broad descriptor like "Fiction," would this be a good shelf title? Why or why not?

## Architecture Onboarding

- **Component map:** LLM descriptor extraction -> Two-tower recommendation model -> Descriptor ranking -> Greedy diversification -> Item ranking/filtering -> Shelf decoration
- **Critical path:** LLM descriptor extraction accuracy -> descriptor-user affinity scoring -> diversity filtering threshold -> surface placement decision
- **Design tradeoffs:**
  - Single shelf (main home) vs. multiple shelves (subfeed): Single shelf limits diversity but fits space constraints; multiple shelves improve discovery but require user intent signal
  - Coarse vs. fine-grained descriptors: Coarse descriptors cover more items but provide less context; fine-grained descriptors are more informative but fragment the catalog
  - Generic "for you" labeling vs. pure descriptive titles: Personalization cues improve engagement but add character count
- **Failure signatures:**
  - High shelf impression count but low click-through: Likely descriptor-user affinity mismatch or poor personalization cues
  - Low # impressed (distinct audiobooks shown): Over-filtering during item ranking or overly narrow descriptors
  - Engagement drop for cold-start users: Single narrow shelf on general surface without "for you" indicator
- **First 3 experiments:**
  1. **Taxonomy validation:** Before LLM extraction, manually label a sample of audiobooks with the 10 descriptor types. Measure inter-annotator agreement and coverage (percentage of items with at least one non-empty descriptor)
  2. **Descriptor diversification threshold sweep:** A/B test different embedding similarity thresholds for the greedy diversification step. Measure shelf diversity (distinct descriptors per user) and engagement metrics
  3. **Surface placement experiment:** Run a factorial A/B test crossing (single shelf vs. multiple shelves) × (with "for you" label vs. without) × (cold-start vs. warm-start users). This isolates which factors drive the engagement gains observed in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can recommender system explanations be further personalized to change dynamically for each individual user?
- **Basis in paper:** [explicit] The conclusion states, "In future work, we plan to explore further different types of recommender system explanations that could enhance user experience, including personalized explanations that change for each user."
- **Why unresolved:** The current implementation uses user affinities to select shelves, but the explanations (shelf titles and descriptors) are derived from a static taxonomy applied to item metadata rather than generated uniquely per user
- **What evidence would resolve it:** A comparison of the current shelf-level explanations against a model that generates unique natural language justifications tailored to specific user profiles

### Open Question 2
- **Question:** What is the optimal balance between fine-grained and coarse-grained descriptors to maximize relevance without sacrificing shelf diversity?
- **Basis in paper:** [inferred] Section 3.2 discusses the trade-off where fine-grained descriptors (e.g., specific themes) may be too narrow, while coarse-grained descriptors (e.g., genres) might be too broad, but does not offer a solution for automating this balance
- **Why unresolved:** The paper employs a diversification step but relies on handcrafted templates and heuristics, leaving the dynamic optimization of descriptor specificity an open challenge
- **What evidence would resolve it:** An analysis of engagement metrics mapped against the specificity level of the generated descriptors to identify a "sweet spot" for different user segments

### Open Question 3
- **Question:** Can descriptive shelves be effectively designed for cold-start users on general home surfaces who lack clear intent?
- **Basis in paper:** [inferred] The authors note that descriptive shelves underperformed on the main home surface, hypothesizing that cold-start users prefer general "Audiobooks for you" lists over topic-specific shelves
- **Why unresolved:** The second A/B test succeeded by moving to a dedicated subfeed with high-intent users, effectively bypassing the difficulty of engaging cold-start users on the main feed
- **What evidence would resolve it:** A specific A/B test on the home surface utilizing hybrid strategies (e.g., blending popular items into descriptive shelves) targeted at users with no listening history

## Limitations

- The LLM descriptor extraction quality depends heavily on prompt engineering and specific model details that are not disclosed in the paper
- The first A/B test's negative engagement result for cold-start users suggests potential risks when applying descriptive shelves to users without sufficient context
- The taxonomy design and descriptor granularity balance remains largely heuristic without systematic validation

## Confidence

- **High confidence:** The overall mechanism of using LLM-enriched descriptors for contextual shelf generation and the second A/B test results showing engagement gains
- **Medium confidence:** The generalizability of results to other recommendation domains and the robustness of the greedy diversification approach
- **Low confidence:** The specific LLM implementation details and the scalability of the descriptor extraction pipeline to larger catalogs

## Next Checks

1. Conduct inter-annotator reliability testing on the 10-category taxonomy to validate descriptor extraction accuracy and coverage across the audiobook catalog
2. Perform ablation studies on the diversification threshold and embedding model choice to quantify their impact on shelf diversity and user engagement
3. Run controlled experiments testing the cold-start user scenario with different surface placements and personalization cue combinations to replicate and understand the first A/B test's engagement decline