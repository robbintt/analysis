---
ver: rpa2
title: "Discovering Scaling Exponents with Physics-Informed M\xFCntz-Sz\xE1sz Networks"
arxiv_id: '2601.22751'
source_url: https://arxiv.org/abs/2601.22751
tags:
- exponents
- exponent
- msn-pinn
- error
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MSN-PINN combines power-law neural networks with physics-informed
  training to discover scaling exponents from PDE constraints without requiring labeled
  solution data. Unlike standard networks that hide exponents in millions of weights,
  MSN outputs exponents as explicit learnable parameters.
---

# Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks

## Quick Facts
- arXiv ID: 2601.22751
- Source URL: https://arxiv.org/abs/2601.22751
- Reference count: 40
- Primary result: Recovers scaling exponents from PDE constraints with 0.009% error on corner singularities without labeled data

## Executive Summary
MSN-PINN introduces a neural network architecture that treats scaling exponents as explicit trainable parameters rather than hidden in millions of weights. By representing solutions as sums of power-law terms, the method directly outputs both the solution and its scaling structure. The approach combines this explicit parameterization with physics-informed training and constraint-aware optimization to discover exponents from PDE constraints alone, achieving high accuracy on singular problems like corner singularities where traditional PINNs struggle.

## Method Summary
MSN-PINN represents solutions as u(x) = Σ cₖx^μₖ where both coefficients {cₖ} and exponents {μₖ} are learnable parameters. The method uses physics-informed training with PDE residuals and boundary conditions, enhanced by constraint-aware training that encodes physical constraints as loss terms. A two-timescale optimization scheme (η_μ/η_c ≈ 0.5) stabilizes training by letting coefficients adapt faster than exponents. The approach requires no labeled solution data, making it suitable for problems where exact solutions are unknown but physical constraints are well-defined.

## Key Results
- Recovers Kondrat'ev corner singularity exponent (2/3) with 0.009% relative error
- Solves singular Poisson problems with 0.03-0.05% error on smooth solutions
- Outperforms naive PINN training by three orders of magnitude on wedge problems
- Theoretical error bounds scale as O(σ/c_min Δ²√N) for noise σ, coefficient magnitude c_min, and exponent separation Δ

## Why This Works (Mechanism)

### Mechanism 1: Explicit Exponent Parameterization
Representing solutions as power-law sums with learnable exponents makes scaling structure directly accessible rather than buried in weights. The MSN ansatz u(x) = Σ cₖx^μₖ parameterizes both coefficients {cₖ} and exponents {μₖ}. For power-law targets, exact representation requires only one term (Proposition 3.2), versus O(ε⁻²) ReLU units for standard networks.

### Mechanism 2: Constraint-Aware Training for Exponent Identifiability
Physical constraints provide the gradient signal needed for exponent selection when the PDE residual alone is degenerate. For the Laplacian on a wedge, all functions r^μ sin(μθ) are harmonic—the residual loss L_res is flat in μ. Adding L_constraint = Σ|cₖ|sin²(μₖω) creates gradients ∂L/∂μₖ ∝ sin(2μₖω) that push exponents toward physically valid discrete values.

### Mechanism 3: Two-Timescale Optimization
Optimizing coefficients faster than exponents stabilizes training by keeping the linear subproblem well-solved at each step. Coefficients appear linearly (unique solution for fixed μ); exponents appear nonlinearly (complex landscape). Using η_μ < η_c (typically 0.1-0.5 ratio) lets coefficients track current exponents while exponents explore slowly.

## Foundational Learning

- **Physics-Informed Neural Networks (PINNs)**: Encode PDEs as loss terms rather than requiring labeled data. Why needed: MSN-PINN builds on PINN methodology for solving PDEs without solutions. Quick check: Can you explain how a PDE residual loss differs from supervised data fitting?

- **Singularity theory and power-law scaling**: Understanding how solutions exhibit singular behavior with characteristic exponents. Why needed: The method targets problems with corner singularities and crack tips. Quick check: Why does a re-entrant corner (270° wedge) produce a different exponent than a 90° corner?

- **Identifiability and the separation condition**: Recovery error scales as O(σ/(c_min Δ²√N)) where Δ is exponent separation. Why needed: Determines when close exponents can be reliably distinguished. Quick check: If two true exponents differ by 0.05, what does the theory predict about recovery difficulty?

## Architecture Onboarding

- **Component map**: Input x → MSN Layer: Σ cₖ|x|^μₖ → Analytical derivatives → Loss = w_r·L_res + w_b·L_BC + w_s·L_sparse + w_con·L_constraint → Two-timescale Adam: [μ slow, c fast]

- **Critical path**: 1) Identify power-law structure of your problem 2) Derive constraint form from boundary conditions 3) Initialize μ uniformly in [μ_min, μ_max] and c near zero 4) Run warmup phase with BC-only loss, then ramp constraint weight

- **Design tradeoffs**: More terms K: Better for multi-exponent problems but harder identifiability. Stronger constraint weight w_con: Faster convergence but may overshoot if constraint form is approximate. Collocation concentration: More points near singularities improves accuracy.

- **Failure signatures**: Exponents clustering together → separation Δ too small. Exponents drifting to boundary values → initialization range poorly matched. L_res→0 but L_constraint remains high → constraint form may be wrong. Oscillating training curves → learning rate ratio too large.

- **First 3 experiments**: 1) Single exponent recovery: Fit f(x) = x^0.5 with K=2 terms, N=100 points; expect ~1-2% error 2) Singular ODE: Solve xu'' + 0.5u' = 0, u(1)=1; recover μ≈0.5 from physics alone 3) Corner singularity: Laplace on 270° wedge with DD boundaries; include constraint term or expect 14% drift error

## Open Questions the Paper Calls Out

- **Can the MSN basis be effectively extended to represent logarithmic corrections (x^α log x) typical of degenerate corners and critical phenomena?** The current architecture lacks log-modified basis functions, limiting applicability to singularities where logarithmic behavior is physically mandated. Resolution requires successful recovery of exponents in synthetic tests involving solutions of the form x^α log x.

- **Do the theoretical stability bounds and constraint-aware training protocols generalize to three-dimensional domains containing singular edges?** The current experiments and identifiability proofs are restricted to 2D geometries; 3D edge singularities involve more complex coupling. Resolution requires application to 3D polyhedral domains demonstrating convergence to known edge singularity exponents.

- **How can the necessary physical constraints (e.g., boundary condition compatibility) be learned automatically rather than manually encoded?** Constraint-aware training currently requires analytical insight to define the quantization condition (e.g., sin(μω)=0) that breaks PDE gradient degeneracy. Resolution requires a modified algorithm that infers the correct quantization constraints solely from boundary collocation data.

## Limitations
- Cannot represent logarithmic corrections (x^α log x) that appear in many physical problems
- Requires manual derivation of constraint forms from boundary conditions
- Performance degrades when true exponents are very close together (Δ < 0.1)

## Confidence

**High Confidence:** Explicit exponent parameterization mechanism is well-supported with theoretical justification and clear empirical evidence.

**Medium Confidence:** Constraint-aware training shows dramatic improvements but relies on problem-specific constraint forms that may not generalize.

**Medium Confidence:** Two-timescale optimization is supported by empirical experiments but specific learning rate ratios appear tuned rather than theoretically derived.

## Next Checks

1. **Generalization Test:** Apply MSN-PINN to problems with known logarithmic corrections (e.g., x^α log x) to quantify approximation error and determine the method's limits for non-power-law scaling.

2. **Constraint Form Verification:** Systematically vary the constraint loss formulation for different boundary condition types to establish robustness when the constraint form is only approximately known.

3. **Multi-Scale Problem:** Test MSN-PINN on problems exhibiting multiple distinct scaling regimes (e.g., composite materials with different singularities) to evaluate performance when the true solution cannot be expressed as a single power-law sum.