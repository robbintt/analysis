---
ver: rpa2
title: 'Deferring Concept Bottleneck Models: Learning to Defer Interventions to Inaccurate
  Experts'
arxiv_id: '2503.16199'
source_url: https://arxiv.org/abs/2503.16199
tags:
- concept
- deferring
- defer
- task
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deferring Concept Bottleneck Models (DCBMs),
  a novel framework that extends Concept Bottleneck Models (CBMs) with the ability
  to learn when to defer concept or task predictions to human experts. The key idea
  is to model each concept and task prediction as a deferring system, where the model
  decides between making a prediction or deferring to a human expert.
---

# Deferring Concept Bottleneck Models: Learning to Defer Interventions to Inaccurate Experts

## Quick Facts
- **arXiv ID**: 2503.16199
- **Source URL**: https://arxiv.org/abs/2503.16199
- **Reference count**: 40
- **Primary result**: DCBMs improve task accuracy by intelligently deferring predictions to human experts when beneficial, especially in concept-incomplete scenarios.

## Executive Summary
This paper introduces Deferring Concept Bottleneck Models (DCBMs), a novel framework that extends Concept Bottleneck Models (CBMs) with the ability to learn when to defer concept or task predictions to human experts. The key idea is to model each concept and task prediction as a deferring system, where the model decides between making a prediction or deferring to a human expert. DCBMs are trained using a consistent surrogate loss derived from the negative log-likelihood of the model's probabilistic formulation. The experiments show that DCBMs can significantly improve task accuracy, especially in concept-incomplete scenarios, by intelligently deferring to human experts when beneficial. Additionally, DCBMs provide interpretable explanations for their deferral decisions by highlighting which concepts require human intervention.

## Method Summary
DCBMs extend standard CBMs by adding a deferral mechanism to each concept and task prediction. Each concept predictor is an MLP that outputs probabilities over K+1 classes (K concepts plus a defer token). The model decides whether to make a prediction or defer to a human expert based on expected loss. Training uses a consistent surrogate loss that combines the negative log-likelihood of the model's probabilistic formulation with deferral costs. The framework supports both independent training (concepts first, then task) and joint training, though independent training is required for theoretical consistency. The surrogate loss is parameterized using Asymmetric Softmax (ASM) to handle the deferral decision.

## Key Results
- DCBMs achieve higher task accuracy than standard CBMs, especially in concept-incomplete scenarios where some ground-truth concepts are missing during training
- The model provides interpretable explanations by highlighting which concepts require human intervention through its deferral decisions
- DCBMs demonstrate improved coverage-accuracy trade-offs compared to baseline methods across multiple datasets (CUB, CIFAR10-H, Completeness)

## Why This Works (Mechanism)
DCBMs work by treating the deferral decision as an integral part of the prediction process rather than a post-hoc operation. By incorporating deferral into the loss function through the surrogate loss, the model learns to make deferral decisions that optimize overall expected accuracy. The consistent surrogate loss ensures that training the model to minimize this loss corresponds to minimizing the expected zero-one loss on the original task. The framework's probabilistic formulation allows for principled handling of uncertainty in both predictions and deferral decisions.

## Foundational Learning
**Concept Bottleneck Models (CBMs)**: Why needed - CBMs provide interpretable intermediate representations between raw inputs and task predictions. Quick check - Verify the model architecture separates concept prediction from task prediction modules.

**Deferral Mechanisms**: Why needed - Allows models to acknowledge uncertainty and leverage human expertise. Quick check - Confirm the surrogate loss properly incorporates deferral costs.

**Consistent Surrogate Losses**: Why needed - Ensures theoretical guarantees about optimization objectives matching true performance metrics. Quick check - Verify the surrogate loss derivation from the probabilistic formulation.

**Independent vs Joint Training**: Why needed - Different training strategies have different theoretical properties. Quick check - Confirm concept and task predictors are trained separately to maintain consistency guarantees.

## Architecture Onboarding

**Component Map**: Input -> Frozen Encoder -> Concept Predictors (MLPs) -> Task Predictor (MLP) -> Output with Deferral

**Critical Path**: Raw input → Frozen feature extractor → Concept predictors → Task predictor → Final output with deferral decision

**Design Tradeoffs**: 
- Independent training ensures theoretical consistency but may be suboptimal compared to joint training
- Single deferral cost λ across all concepts and tasks simplifies optimization but may not reflect real-world cost differences
- Frozen encoders reduce computational cost but limit end-to-end optimization

**Failure Signatures**:
- Model always defers (zero coverage) - typically indicates λ is too low relative to human accuracy
- Poor task accuracy despite high concept accuracy - suggests task predictor isn't effectively using deferred concepts
- Inconsistent training results - likely due to improper implementation of independent training requirement

**First Experiments**:
1. **Verify MLP architecture**: Test different hidden layer sizes [128, 64, 32] vs [256, 128, 64] to match reported performance
2. **Validate human simulation**: Compare random label flipping vs. class-dependent error rates for simulating human experts
3. **Test concept mappings**: Implement multiple plausible CIFAR10-H concept mappings to verify which produces consistent results

## Open Questions the Paper Calls Out
**Open Question 1**: How does performance change when concept-level deferral costs differ from task-level deferral costs? The current framework uses a single λ parameter, but real-world costs may vary significantly between providing concept labels versus final task labels.

**Open Question 2**: Can a consistent surrogate loss be derived for joint training of concept and task predictors? Current theoretical guarantees require independent training, but joint training often performs better in practice.

**Open Question 3**: How can the framework be extended to model inter-concept relationships or hierarchical structures? The current model treats concepts as conditionally independent, ignoring semantic relationships that exist in many real-world tasks.

## Limitations
- Underspecified MLP architecture parameters (hidden layer dimensions, Leaky-ReLU slope) make exact reproduction difficult
- Human expert simulation methodology is not clearly described, particularly the distinction between training and evaluation
- CIFAR10-H concept definitions require external reference (Oikarinen et al., 2023) that's not fully specified in the paper

## Confidence
- **High confidence**: Theoretical framework and surrogate loss formulation are clearly specified and mathematically sound
- **Medium confidence**: Overall experimental methodology and evaluation metrics are well-defined
- **Low confidence**: Exact reproduction of results is challenging due to underspecified architectural parameters and dataset preprocessing details

## Next Checks
1. **Validate MLP architecture**: Implement multiple MLP configurations with varying hidden layer sizes and Leaky-ReLU slopes to determine which configuration best matches reported results.

2. **Test human expert simulation**: Implement and compare different corruption strategies for simulating human experts - random label flipping vs. class-dependent error rates - to verify which approach produces the reported deferral behavior.

3. **Verify concept definitions**: Contact authors for explicit CIFAR10-H concept mappings or implement multiple plausible mappings based on the 16 "superclass" concepts to determine which produces results consistent with the paper.