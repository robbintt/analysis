---
ver: rpa2
title: Monocular pose estimation of articulated open surgery tools -- in the wild
arxiv_id: '2407.12138'
source_url: https://arxiv.org/abs/2407.12138
tags:
- pose
- estimation
- object
- data
- surgical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for monocular 6D pose estimation
  of articulated surgical instruments in open surgery. The method addresses the scarcity
  of annotated real-world surgical data by generating synthetic datasets with physically-based
  rendering, 3D scanning, and articulation modeling.
---

# Monocular pose estimation of articulated open surgery tools -- in the wild

## Quick Facts
- arXiv ID: 2407.12138
- Source URL: https://arxiv.org/abs/2407.12138
- Reference count: 40
- Key outcome: A novel framework combining synthetic data generation with domain adaptation achieves 0.79 AP for 6D pose estimation of articulated surgical tools without manual annotation.

## Executive Summary
This paper introduces a monocular 6D pose estimation framework for articulated surgical instruments in open surgery, addressing the critical challenge of scarce annotated surgical data. The method generates high-fidelity synthetic datasets using photogrammetry-scanned tools, articulation modeling, and physically-based rendering. A tailored pose estimation network based on GDR-Net is combined with YOLOv8 for object detection, enhanced for articulated tools and domain adaptation. The training strategy leverages both synthetic data and real unannotated videos through pseudo-labeling, achieving significant improvements in both detection (AP from 0.51 to 0.74) and pose estimation (AP from 0.67 to 0.79) on real surgical videos.

## Method Summary
The framework generates synthetic training data by scanning surgical tools via photogrammetry, rigging them for articulation (15 needle-holder poses, 10 tweezers poses), and rendering with BlenderProc using physically-based materials and MANO hand models for occlusion. The pose estimation pipeline combines YOLOv8 for object detection with a GDRNPP-based network for pose estimation, using a ViT backbone and UPerNet architecture. During training, a differentiable Patch-PnP module converts dense 2D-3D correspondences into pose, while PnP-RANSAC is used at inference. Domain adaptation is achieved through iterative pseudo-labeling: high-confidence detections in real videos are reprojected to generate refined bounding boxes, which retrain the detector in a feedback loop without manual annotation.

## Key Results
- Pose estimation AP improves from 0.67 (synthetic-only) to 0.79 with domain adaptation on real surgical videos
- Object detection AP increases from 0.51 to 0.74 through the pseudo-labeling loop
- Ablation shows 14% AP drop without hand modeling, validating the importance of occlusion handling
- The method eliminates manual annotation requirements while maintaining strong real-world performance

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data Generation with Articulation Modeling
The framework mitigates annotated data scarcity by generating 80,000 synthetic images with perfect ground-truth labels through photogrammetry-scanned, rigged tools. By modeling articulation (bending for tweezers, separation for needle holders) and rendering with physically-based materials, the system learns dense 2D-3D correspondences without manual annotation. This synthetic prior effectively transfers to real instruments despite differences in specularity and lighting.

### Mechanism 2: Dense Correspondence-Based Pose Estimation
Instead of direct pose regression, the network predicts dense 2D-3D correspondence maps (NOCS) and masks, which a differentiable Patch-PnP module converts to 6D pose during training. This intermediate representation stabilizes optimization compared to directly regressing raw rotation matrices or quaternions. PnP-RANSAC provides robust inference on noisy real data.

### Mechanism 3: Iterative Pseudo-Label Domain Adaptation
Synthetic-to-real adaptation occurs through iterative pseudo-labeling: high-confidence detections in real videos (conf>0.999, 3+ consecutive frames) are reprojected to generate refined bounding boxes, retraining the detector. This feedback loop improves both detection and pose estimation without manual intervention, leveraging the assumption that high-confidence poses correspond to visually accurate reprojections.

## Foundational Learning

- **6D Rotation Representation (Continuous 6D)**: Used to avoid discontinuities and ambiguities in standard representations like quaternions or Euler angles during gradient descent. Quick check: Why use a 6D vector to represent a 3x3 rotation matrix, and what mathematical property (orthogonality) must be enforced afterward?

- **Perspective-n-Point (PnP)**: Bridges 2D image features and 3D pose by solving for camera/object pose given 3D points and their 2D projections. Quick check: What's the difference between differentiable Patch-PnP (training) and PnP-RANSAC (inference)?

- **Allocentric vs. Egocentric Rotation**: The paper predicts rotation relative to the object's center (allocentric) rather than camera center (egocentric) to decouple translation from rotation, simplifying the learning objective. Quick check: How does allocentric rotation simplify the problem when the object moves across the camera field of view?

## Architecture Onboarding

- **Component map**: RGB Image -> YOLOv8 (Detection) -> Crop ROI -> ViT Backbone + UPerNet (Features) -> Three Heads (Category, Articulation, Dense Correspondence) -> Patch-PnP/PnP-RANSAC -> 6D Pose -> Domain Adaptation Loop

- **Critical path**: The Dense Correspondence Map (M_2D-3D) generation is the bottleneck. Inaccurate correspondence maps cause PnP solver failure and propagate errors through pseudo-labeling.

- **Design tradeoffs**:
  - Hand Modeling: Full synthetic hands provide context but are complex; masking is simpler but loses grasping context
  - PnP Strategy: Patch-PnP enables end-to-end training but may be less robust than PnP-RANSAC on noisy real data
  - Assumption: Temporal stability (3 consecutive frames) assumes smooth tool movement

- **Failure signatures**:
  - Drift in Articulation: Wrong opening angle causes floating or intersecting reprojections
  - False Positive Detection: High confidence on specular reflections (glare) mimicking metal tools
  - Domain Collapse: Low pseudo-label thresholds cause overfitting to model errors

- **First 3 experiments**:
  1. Synthetic Overfit Test: Train pose network only on synthetic data to verify Patch-PnP and loss functions work
  2. Occlusion Ablation: Replicate "Masked Tools" vs. "Synthetic Hands" experiment to assess rigging cost
  3. Threshold Sensitivity: Vary class confidence (0.999) and outlier count (<30) thresholds to observe AP changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative accuracy of 3D depth estimation compared to 2D reprojection metrics?
- Basis: Section 6 notes current evaluation "does not fully capture depth information" and suggests incorporating depth evaluation
- Why unresolved: Ground-truth 6D pose annotations in real surgeries are impractical to obtain
- Evidence needed: Phantom data with motion capture or RGB-D sensors to quantify absolute translation errors

### Open Question 2
- Question: How well does the framework generalize to wider variety of surgical instruments and anatomical sites?
- Basis: Section 6 acknowledges limitation to two tools (needle-holder, tweezers) and one site (CABG)
- Why unresolved: Experiments only validate on specific surgical tools within CABG procedures
- Evidence needed: Performance benchmarks on different tools (scalpel, forceps) and surgery types without re-engineering

### Open Question 3
- Question: How sensitive is domain adaptation performance to specific filtering thresholds for pseudo-label generation?
- Basis: Section 6 notes lack of comprehensive hyperparameter search for pseudo-label quality criteria
- Why unresolved: Method relies on specific thresholds (e.g., confidence > 0.999) without stability analysis
- Evidence needed: Ablation studies showing AP variance as confidence, outlier count, and reprojection error thresholds are systematically adjusted

## Limitations
- Specific hyperparameters for loss weighting and domain adaptation thresholds are not disclosed, hindering exact reproduction
- High sensitivity to occlusion modeling: 14% AP drop without hand modeling indicates critical dependency
- Limited generalizability: Only validated on two specific surgical tools (needle-holder, tweezers)

## Confidence
- **High Confidence**: Synthetic data generation pipeline (photogrammetry + BlenderProc) is well-described with explicit validation of 80,000 training images
- **Medium Confidence**: Domain adaptation strategy is conceptually clear but lacks specific quantitative thresholds and sensitivity analysis
- **Medium Confidence**: Pose estimation architecture is detailed but exact DINOv2 checkpoint and articulation head modifications are unspecified

## Next Checks
1. Replicate synthetic data generation with alternative CAD models to verify 14% AP drop without hand modeling
2. Perform ablation on pseudo-label confidence threshold (0.999) by varying in 0.01 increments to assess stability
3. Test pose network on held-out synthetic data with randomized articulation angles to verify allocentric rotation consistently outperforms egocentric alternatives