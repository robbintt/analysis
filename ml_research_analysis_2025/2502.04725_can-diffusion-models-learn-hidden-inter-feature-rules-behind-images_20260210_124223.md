---
ver: rpa2
title: Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?
arxiv_id: '2502.04725'
source_url: https://arxiv.org/abs/2502.04725
tags:
- rules
- diffusion
- inter-feature
- rule
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether diffusion models can learn hidden
  inter-feature rules from images, a capability not previously studied systematically.
  The authors first identify real-world failure cases where diffusion models fail
  to capture relationships between image features, such as inconsistent lighting-shadow
  relationships and mismatched object-mirror reflections.
---

# Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?

## Quick Facts
- **arXiv ID:** 2502.04725
- **Source URL:** https://arxiv.org/abs/2502.04725
- **Reference count:** 40
- **Primary result:** Diffusion models learn coarse-grained inter-feature rules but struggle with fine-grained rules requiring precise feature relationships, due to fundamental incompatibility between denoising score matching and rule conformity.

## Executive Summary
This paper systematically investigates whether diffusion models can learn hidden inter-feature rules from images - a capability previously studied only anecdotally. Through four synthetic tasks with well-defined rules at different granularity levels, the authors demonstrate that while diffusion models can avoid coarse-grained violations by staying within training distribution support, they fail to capture precise fine-grained relationships due to constant errors in denoising score matching training. The theoretical analysis proves that the DSM objective is fundamentally incompatible with rule conformity, as the score function's structure prevents learning the exact inter-feature constraints. Classifier guidance during sampling provides limited improvement, revealing that the subtle signals of fine-grained rules make accurate classification particularly challenging.

## Method Summary
The authors design four synthetic tasks (A-D) with different inter-feature rules at coarse and fine-grained levels. They train unconditional DDPM models with U-Net and DiT backbones using denoising score matching with T=1000 timesteps and linear noise schedules. Models are trained for varying epochs (400-1600) depending on task complexity. Evaluation uses color-based masking and contour detection to extract features, then computes R² metrics between estimated and ground-truth rule relationships. To mitigate fine-grained learning failures, they construct contrastive training data and train classifiers for guidance during sampling, testing both constant and piecewise gradient weighting strategies.

## Key Results
- Diffusion models achieve near-zero coarse-grained violations across all synthetic tasks, staying within training distribution support
- Fine-grained rule learning fails with R² values consistently below 0.9, regardless of model capacity or training data scale
- Theoretical analysis proves constant bias and variance errors in score estimation prevent rule conformity learning
- Classifier guidance provides limited improvement (R² gains of 0.05-0.10) due to subtle signal separability challenges

## Why This Works (Mechanism)

### Mechanism 1: Coarse-Grained Rule Learning via Distribution Support Avoidance
Diffusion models learn coarse-grained inter-feature rules because violating them requires generating out-of-distribution samples, which the denoising process inherently avoids. The DDPM training objective learns to estimate the score function ∇log p(xt) across the diffusion path. Coarse violations (e.g., sun and shadow on same side of pole) represent never-observed modes in training data. The model's denoising trajectory remains within the learned support, naturally avoiding these OOD regions without explicit rule encoding.

### Mechanism 2: Fine-Grained Rule Failure via Score Function Estimation Error
The denoising score matching (DSM) objective exhibits constant non-vanishing errors in learning precise inter-feature dependencies due to fundamental incompatibility between joint distribution estimation and conditional rule conformity. For data with patches x(1)=ζu, x(2)=(1-ζ)v satisfying ||x(1)||+||x(2)||=1, the ground-truth score function has a structural constraint that two-layer networks trained via DSM cannot satisfy. This results in bias error C0 from the difference between E[ζ] and the score coefficient, and variance error C1 from noise decomposition independent of feature directions.

### Mechanism 3: Limited Mitigation via Classifier Guidance with Subtle Signal Challenge
Classifier guidance provides limited improvement for fine-grained rules because contrastive training data exhibits weak discriminative signals, making accurate rule classification fundamentally difficult. Constructed contrastive pairs differ only in fine-grained rule compliance while appearing nearly identical visually. The classifier trained with cross-entropy and NT-Xent loss achieves only 60-80% accuracy because the feature representations are nearly inseparable. During guided sampling, gradients from this uncertain classifier provide weak directional signals.

## Foundational Learning

- **Concept: Denoising Score Matching (DSM)**
  - Why needed here: The paper's theoretical analysis proves DSM has constant errors for rule learning; understanding L = E[||s_w(x_t) - ϵ_t||²] is essential to grasp why the objective is incompatible with rule conformity.
  - Quick check question: Why does minimizing prediction error of Gaussian noise fail to enforce inter-feature constraints?

- **Concept: Score Function Decomposition**
  - Why needed here: Theorem 4.2 shows the ground-truth score has structure ∇log p_t(x_t) = [-β_t^{-2}x_t + α_t β_t^{-2}E[π_t·ζ]u, ...]; understanding this decomposition reveals where the constraint (*) should appear.
  - Quick check question: What does the identity E[π_t·ζ] + E[π_t·(1-ζ)] = 1 represent in terms of the inter-feature rule?

- **Concept: Rule-Conforming Error Decomposition**
  - Why needed here: The paper decomposes error into bias (E[ψ_t] - α_t/β_t²)² and variance Var(ψ_t); this explains why both systematic deviation and sampling variability prevent rule learning.
  - Quick check question: Why do both bias and variance errors have constant lower bounds regardless of network width or training time?

## Architecture Onboarding

- **Component map:**
  Training Pipeline: Synthetic Data Generator (Tasks A-D) → DDPM with U-Net/DiT backbone → DSM Loss Training
  Evaluation Pipeline: Generated Samples → Color-based Masking → Contour Detection → Feature Extraction (l₁, l₂, h₁, h₂) → Rule Verification (coarse/fine)
  Mitigation Pipeline: Contrastive Data Construction → Classifier Training (U-Net) → Guided Sampling / Filtering

- **Critical path:**
  1. Start with synthetic task generation (Task A is simplest: sun-pole-shadow with rule l₁h₂ = l₁h₁)
  2. Train unconditional DDPM for 400-1600 epochs with T=1000 timesteps
  3. Generate 2000+ samples and apply 3-step evaluation pipeline
  4. Compute R² between estimated and ground-truth feature relationships
  5. If R² < 0.9, fine-grained rule learning has failed (expected per Theorem 4.5)

- **Design tradeoffs:**
  - **Pixel-space vs. Latent-space DDPM:** Paper uses pixel-space for cleaner analysis; latent-space (SD-3.5, Flux.1) introduces additional compression artifacts that may worsen rule violations
  - **Model capacity:** Increasing parameters from 14M (U-Net) to 130M (DiT-B/2) does not improve fine-grained rule learning (Appendix D.3, Figure 13)
  - **Training data scale:** Scaling from 4K to 40K samples shows no R² improvement (Appendix D.3, Figure 14)
  - **Guidance timing:** Paper tests both constant and piecewise guidance weighting; piecewise (last 20 steps) works better for Task C

- **Failure signatures:**
  - Coarse violations: Sun and shadow on same side (Task A, Figure 9) - indicates severe distribution shift
  - Fine violations: Ratio l₂h₁/l₁h₂ ≠ 1.0 despite valid spatial layout (Figure 10) - expected per theory
  - Memorization vs. generation: Check nearest-neighbor distances in 4D (structure) vs. 13D (structure+color) space; distances >0.3 indicate true generation
  - Classifier failure: Test accuracy stuck at 60-80% on contrastive data indicates subtle signal problem

- **First 3 experiments:**
  1. **Reproduce coarse/fine rule learning gap:** Train DDPM on Task A (sun-shadow) with 4K samples, 400 epochs. Verify Table 1 results (coarse violations ≈0) and Figure 4 results (R² ≈0.85 < 1.0 for fine rule).
  2. **Validate theoretical error decomposition:** Implement two-layer score network on multi-patch data (Definition 4.1) with d=100, m=20 neurons. Measure ψ_t(x_t) distribution across timesteps and verify non-zero bias/variance (Figure 6).
  3. **Test classifier guidance limits:** Construct contrastive pairs for Task A with class 1 (fine-grained compliant) vs. classes 0/2 (coarse-only). Train U-Net classifier and measure test accuracy. Apply guided sampling with gradient scale 7 for last 20 steps. Verify limited R² improvement (0.85→0.90).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative training objectives beyond denoising score matching (DSM) be designed to better capture fine-grained inter-feature rules?
- Basis in paper: Section 4 proves that "DMs trained via denoising score matching (DSM) exhibit constant errors in learning hidden rules, as the DSM objective is not compatible with rule conformity."
- Why unresolved: The theoretical analysis identifies a fundamental incompatibility between the DSM objective and rule conformity, but no alternative objectives are explored.
- What evidence would resolve it: Developing and testing new training objectives that directly incorporate rule-conformity constraints, demonstrating improved fine-grained rule adherence on synthetic benchmarks.

### Open Question 2
- Question: How can guidance mechanisms be improved to better detect subtle fine-grained rule violations that current classifiers struggle to distinguish?
- Basis in paper: Section 5.2 states "the subtle signals of fine-grained rules make accurate classifier training particularly challenging" with classification accuracy only 60-80% even on simple synthetic tasks.
- Why unresolved: The contrastive data is nearly inseparable in CLIP representation space, making conventional classifier training difficult.
- What evidence would resolve it: Novel architectures or training methods achieving >95% accuracy on fine-grained rule classification, leading to substantially improved guidance effectiveness.

### Open Question 3
- Question: How can DMs learn fine-grained rules in real-world scenarios where rules are implicit and contrastive data cannot be manually constructed?
- Basis in paper: Section 5.2 states "In real-world scenarios, fine-grained rules are often difficult to accurately define and detect, making the construction of contrastive data impossible. We leave the solution to DMs' inability to learn fine-grained rules in real-world scenarios for future work."
- Why unresolved: The proposed mitigation strategy requires explicit rule definitions and contrastive pairs, which are unavailable for real-world applications.
- What evidence would resolve it: Methods that automatically discover and enforce inter-feature relationships without requiring manually-specified rules or contrastive data.

### Open Question 4
- Question: Do latent-space diffusion models suffer from additional information loss in learning inter-feature rules compared to pixel-space models?
- Basis in paper: The paper notes "pixel-space DDPM makes the conformity of inter-feature relationships potentially simpler, as no additional compression-induced information loss occurs" (Section 3.2), but directly compares mainstream latent-space models (SD-3.5, Flux) only qualitatively.
- Why unresolved: No controlled comparison between pixel-space and latent-space models on identical tasks was conducted.
- What evidence would resolve it: Systematic comparison of pixel-space vs. latent-space diffusion models on synthetic tasks, quantifying compression-induced rule-learning degradation.

## Limitations
- Theoretical analysis based on simplified multi-patch model assumptions that may not fully capture real-world inter-feature relationships
- Classifier guidance approach requires manually constructed contrastive data, making it inapplicable to real-world scenarios where fine-grained rules are implicit
- Limited empirical validation of theoretical error bounds across different network architectures and real image datasets

## Confidence
- **High Confidence:** Coarse-grained rule learning via distribution avoidance - extensively validated through synthetic experiments with near-zero violations across all tasks
- **Medium Confidence:** Fine-grained rule learning failure via DSM incompatibility - theoretically proven but based on simplified model assumptions; empirical verification limited to specific network architectures
- **Low Confidence:** Classifier guidance effectiveness - limited improvement observed (R² gains of 0.05-0.10), with classifier accuracy stuck at 60-80% indicating fundamental difficulty in capturing subtle signals

## Next Checks
1. **Empirical Error Bound Verification:** Implement the two-layer network analysis (Theorem 4.5) on real diffusion model checkpoints across different architectures (U-Net, DiT) to measure actual bias/variance decomposition and compare against theoretical constants C0 and C1
2. **Cross-Domain Generalization Test:** Apply the evaluation framework to real image datasets (e.g., CLEVR for compositional rules, natural scenes for lighting consistency) to assess whether theoretical limitations hold beyond synthetic data
3. **Alternative Training Objective Comparison:** Train diffusion models using conditional score matching or physics-informed losses on Task A and measure fine-grained rule learning performance against standard DSM baseline to test the hypothesis about objective incompatibility