---
ver: rpa2
title: 'FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual
  Machine Translation'
arxiv_id: '2505.14256'
source_url: https://arxiv.org/abs/2505.14256
tags:
- language
- lang
- translation
- fuximt
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FuxiMT addresses the need for high-quality Chinese-centric multilingual
  machine translation by leveraging a sparsified large language model. It employs
  a two-stage training strategy: pre-training on a massive Chinese corpus followed
  by multilingual fine-tuning on a parallel dataset of 65 languages, incorporating
  Mixture-of-Experts (MoEs) and curriculum learning.'
---

# FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation

## Quick Facts
- arXiv ID: 2505.14256
- Source URL: https://arxiv.org/abs/2505.14256
- Reference count: 29
- Primary result: Achieves BLEU scores up to 53.12 for high-resource languages and 29.92 under low-resource conditions with strong zero-shot capabilities

## Executive Summary
FuxiMT introduces a novel approach to Chinese-centric multilingual machine translation by leveraging a sparsified large language model. The system employs a two-stage training strategy that first pre-trains on a massive Chinese corpus to establish deep linguistic understanding, then fine-tunes on multilingual parallel data across 65 languages. By incorporating Mixture-of-Experts modules with a frozen BLOOMz backbone and curriculum learning, FuxiMT achieves significant performance improvements while maintaining efficiency through sparse routing.

## Method Summary
FuxiMT uses a two-stage training approach with a frozen BLOOMz-7B backbone and sparse Mixture-of-Experts layers. Stage 1 involves pre-training on 5B Chinese sentences using causal language modeling. Stage 2 fine-tunes on 65 languages with >50% Chinese-centric parallel data, inserting MoE layers every 8 decoder layers with 8 experts each. Curriculum learning gradually introduces low-resource languages, while back-translation augmentation enhances data quality. The model uses top-1 routing with load-balancing auxiliary loss and trains with AdamW optimizer on 8×A100-80G GPUs.

## Key Results
- Achieves BLEU scores of 53.12 for high-resource languages, outperforming strong baselines
- Maintains strong performance under low-resource conditions with 29.92 BLEU
- Demonstrates notable zero-shot translation capabilities for unseen language pairs (Tigrinyan, Tibetan, Turkmen, Pijin)
- Shows superior performance across multiple resource levels: High (37.03), Medium (25.27), Low (20.64), Very Low (20.66) BLEU

## Why This Works (Mechanism)

### Mechanism 1: Frozen Backbone with Sparse MoE
- **Claim:** Adding sparse Mixture-of-Experts modules to a frozen pre-trained backbone enables efficient multilingual specialization while preserving foundational linguistic knowledge.
- **Mechanism:** The frozen BLOOMz components retain general language understanding while MoE layers provide specialized routing where a learned gating network selects among 8 expert FFNs per token.
- **Core assumption:** Expert specialization emerges from gradient updates to MoE parameters only, while frozen backbone provides stable cross-lingual representations.
- **Evidence anchors:** Abstract mentions "incorporates Mixture-of-Experts (MoEs)"; section 3.1 describes frozen components and router functionality.
- **Break condition:** Routing collapse or insufficient backbone representation for low-resource languages.

### Mechanism 2: Curriculum Learning for Balanced Multilingual Performance
- **Claim:** Curriculum learning with weighted loss enables balanced multilingual performance by gradually incorporating low-resource languages.
- **Mechanism:** Training starts with high-resource pairs to establish stable patterns, then progressively increases low-resource language contributions via weight schedule.
- **Core assumption:** Translation competence transfers from high-resource to low-resource languages through shared representations.
- **Evidence anchors:** Abstract mentions "curriculum learning strategy"; section 3.2 describes the gradual incorporation approach.
- **Break condition:** Language interference outweighing transfer benefits or low-resource languages requiring fundamentally different representations.

### Mechanism 3: Chinese-Centric Pre-Training for Cross-Lingual Transfer
- **Claim:** Chinese-centric pre-training creates language-specific representations that serve as an effective pivot for multilingual translation, including zero-shot pairs.
- **Mechanism:** Stage 1 trains on 5B Chinese sentences, creating strong Chinese representations that bridge to other languages via shared MoE experts in Stage 2.
- **Core assumption:** Chinese representations learned monolingually transfer to translation contexts, enabling zero-shot translation through Chinese as intermediate representation space.
- **Evidence anchors:** Abstract mentions "pre-training on a massive Chinese corpus"; appendix A.6 shows zero-shot results on four languages.
- **Break condition:** Pivot-based transfer degrading for languages distant from Chinese or monolingual pre-training conflicting with translation objectives.

## Foundational Learning

- **Mixture-of-Experts Routing:**
  - **Why needed here:** Understanding how tokens are routed to specialized experts explains model capacity and efficiency
  - **Quick check question:** Can you explain why sparse routing (activating subset of experts per token) is more efficient than dense routing while maintaining expressiveness?

- **Curriculum Learning:**
  - **Why needed here:** Essential for understanding the training dynamics that enable low-resource performance
  - **Quick check question:** How does gradually increasing low-resource language weight in the loss function prevent both overfitting and underfitting?

- **Transfer Learning with Frozen Backbones:**
  - **Why needed here:** Critical for understanding the architectural choice to freeze BLOOMz and train only MoE layers
  - **Quick check question:** What are the trade-offs between freezing vs. fine-tuning the entire model in terms of catastrophic forgetting and adaptation capacity?

## Architecture Onboarding

- **Component map:** Input → Tokenizer (SentencePiece) → Frozen BLOOMz-7B backbone (30 layers, 4096 hidden, 32 heads) → Sparse MoE layers (every 8th layer, 8 experts each) → Router (gating network) → Selected Expert FFNs → Output

- **Critical path:**
  1. Data preprocessing: 6-stage pipeline for parallel corpora (length filtering, character ratios, deduplication, normalization)
  2. Chinese pre-training: 5B sentences, causal LM, cross-entropy loss
  3. MoE insertion: Initialize experts (mixed random + BLOOMz weights), freeze backbone
  4. Multilingual fine-tuning: Curriculum schedule, 40 instruction templates, back-translation augmentation

- **Design tradeoffs:**
  - Sparsity level (sparse_step=8): Higher sparsity = more efficiency but potentially less capacity per layer
  - Expert count (8 per layer): More experts = finer specialization but routing complexity
  - Frozen vs. trainable backbone: Stability vs. adaptability
  - Chinese data ratio (>50%): Strong Chinese performance vs. potential imbalance for non-Chinese pairs

- **Failure signatures:**
  - Routing collapse: All tokens routed to single expert (check expert utilization logs)
  - Catastrophic forgetting: Performance drop on high-resource languages after low-resource introduction (monitor per-language BLEU throughout training)
  - Expert underutilization: Some experts never activated (check load balancing)
  - Zero-shot failure: Unseen language pairs performing near random (check cross-lingual representation quality)

- **First 3 experiments:**
  1. **Baseline comparison:** Evaluate FuxiMT against BLOOMz-7B and NLLB on standardized test sets across all 64 language pairs, reporting BLEU and chrF stratified by resource level
  2. **Ablation study:** Train variants (random-init experts, no curriculum, fully trainable backbone) to isolate contribution of each component
  3. **Zero-shot evaluation:** Test on held-out language pairs (Tigrinyan, Tibetan, Turkmen, Pijin) to validate cross-lingual transfer without direct training data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the strong Chinese-centric bias impact translation quality for non-Chinese-centric language pairs (e.g., translating between two low-resource non-Chinese languages)?
- **Basis in paper:** [inferred] Experimental results focus almost exclusively on xx→zh pairs, leaving zh→xx or xx→yy directions unstated
- **Why unresolved:** Model specialization to Chinese may degrade performance on other language pairs compared to balanced multilingual models
- **What evidence would resolve it:** BLEU/CHRF scores on benchmark set of non-Chinese-centric translation pairs (e.g., Swahili to Arabic)

### Open Question 2
- **Question:** Does freezing the BLOOMz backbone limit the model's ability to adapt its fundamental linguistic representations for extremely low-resource languages?
- **Basis in paper:** [explicit] Section 3.1 states the base model is "Frozen" to prevent catastrophic forgetting
- **Why unresolved:** While freezing preserves general knowledge, it may restrict formation of unique representations required by low-resource languages that differ significantly from Chinese or English
- **What evidence would resolve it:** Analysis of internal representations of low-resource tokens or comparison against partially unfrozen backbone model

### Open Question 3
- **Question:** What is the optimal ratio of "reused" versus "randomly initialized" experts in the MoE layers for balancing knowledge transfer and new capacity?
- **Basis in paper:** [inferred] Section 3.1 mentions "mixed strategy" for initialization, and section 4.3 shows Reuse-Init outperforms Random-Init, but specific mixture ratio is not explored
- **Why unresolved:** Unknown if 50/50 split is optimal, or if leaning more heavily on reused weights (for stability) or random weights (for capacity) would yield better low-resource performance
- **What evidence would resolve it:** Parameter sweep ablation study varying ratio of initialized vs. random experts

## Limitations

- The specific curriculum learning weighting schedule for low-resource languages is not detailed, making it difficult to verify balanced multilingual performance
- MoE initialization strategy lacks precise specification of weight mixture ratio between BLOOM FFN weights and random initialization
- Training duration (steps/epochs) for both stages is not provided, limiting reproducibility and performance attribution
- Limited evaluation data for claimed zero-shot capabilities on languages like Tigrinyan, Tibetan, Turkmen, and Pijin

## Confidence

- **High Confidence:** BLEU score improvements over baselines (53.12 vs 52.11 for high-resource languages) are well-supported by experimental results
- **Medium Confidence:** Two-stage training strategy and MoE architecture are clearly described, but specific implementation details needed for exact reproduction are missing
- **Low Confidence:** Claimed superiority under low-resource conditions (29.92 BLEU) is based on single experiment without systematic ablation studies across all low-resource languages

## Next Checks

1. **Routing Stability Analysis:** Monitor expert utilization patterns during training to verify that routing collapse is avoided and that sparse efficiency maintains performance

2. **Curriculum Learning Effectiveness:** Conduct ablation experiments comparing FuxiMT with and without curriculum learning across different resource levels to isolate its contribution to robust performance

3. **Zero-Shot Transfer Verification:** Evaluate FuxiMT on held-out language pairs not seen during training (Tigrinyan, Tibetan, Turkmen, Pijin) using standardized test sets to confirm zero-shot translation capabilities