---
ver: rpa2
title: A Comparative Benchmark of Large Language Models for Labelling Wind Turbine
  Maintenance Logs
arxiv_id: '2509.06813'
source_url: https://arxiv.org/abs/2509.06813
tags:
- data
- maintenance
- llms
- reliability
- logs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks LLMs for classifying unstructured wind turbine
  maintenance logs, a key barrier to automated O&M analysis. We developed an open-source
  framework to evaluate 12 state-of-the-art models, focusing on accuracy, efficiency,
  and calibration.
---

# A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs

## Quick Facts
- arXiv ID: 2509.06813
- Source URL: https://arxiv.org/abs/2509.06813
- Reference count: 10
- Proprietary models (GPT-5, Gemini-2.5) achieved highest alignment (F1-scores up to 0.83), while open-source models lagged (F1-scores ~0.62)

## Executive Summary
This study benchmarks LLMs for classifying unstructured wind turbine maintenance logs, addressing a key barrier to automated O&M analysis. We developed an open-source framework to evaluate 12 state-of-the-art models, focusing on accuracy, efficiency, and calibration. Proprietary models achieved the highest alignment, while open-source models showed significant performance gaps. The study reveals that classification performance is higher for objective component identification than for interpretive maintenance actions, and that calibration varies significantly across models. The framework and data schema are publicly available for further research and deployment.

## Method Summary
The study evaluated 12 LLMs (both proprietary and open-source) for zero-shot classification of wind turbine maintenance logs into two hierarchical taxonomies: Maintenance Type (16 labels) and Issue Category (26 labels). A dynamic prompt engineering strategy filtered labels based on component codes to reduce contextual noise. Models were tested via API or locally via Ollama, with evaluation metrics including F1-score, precision, recall, calibration curves, and token usage. The benchmark used 400+ curated maintenance logs, with proprietary models showing superior performance and calibration compared to open-source alternatives.

## Key Results
- Proprietary models (GPT-5, Gemini-2.5) achieved F1-scores up to 0.83, while open-source models scored ~0.62
- GPT-o3 showed reliable confidence scoring, while Phi-4:14b was overconfident
- Classification performance was higher for objective component identification than interpretive maintenance actions
- No model achieved perfect accuracy; a Human-in-the-Loop system is recommended

## Why This Works (Mechanism)

### Mechanism 1
Dynamic label filtering improves classification accuracy and reduces token consumption compared to static full-schema prompting. The system identifies the component code in a log entry and injects only the relevant labels into the prompt, reducing contextual noise and preventing performance degradation from large label spaces. Break condition: if pre-processing fails to extract the correct component code, the LLM receives an incomplete label list, causing classification errors.

### Mechanism 2
Model calibration (confidence-accuracy alignment) determines HITL workflow viability more than raw accuracy alone. A well-calibrated model allows humans to sort predictions by confidence, auto-accepting high-confidence labels while manually reviewing low-confidence cases. Break condition: if a model is overconfident (like Phi-4:14b), humans cannot trust confidence scores to filter workload, negating efficiency gains.

### Mechanism 3
Semantic ambiguity in source text dictates the accuracy ceiling for both LLMs and human experts. Maintenance logs lack clear boundaries between actions ("repair" vs. "inspection"), with models aligning better on objective nouns than interpretive verbs. Break condition: if label definitions are refined to be mutually exclusive and exhaustive, this ambiguity ceiling might rise, but it currently remains the primary bottleneck.

## Foundational Learning

- **Model Calibration**: Why needed - Raw F1-scores are insufficient for industrial deployment; you must understand if the model "knows when it doesn't know" to design safe review workflows. Quick check: Does a "90% confidence" prediction correspond to a 90% empirical accuracy rate?
- **Zero-shot vs. Fine-tuning**: Why needed - This study relies on zero-shot capabilities; understanding this baseline is required before deciding if costly domain-specific fine-tuning is necessary. Quick check: Can the model classify a new hydraulic failure without having seen specific examples in the prompt?
- **Inter-annotator Agreement (Cohen's Kappa)**: Why needed - Since no model is perfect, agreement scores measure how consistently models cluster around specific answers, serving as reliability proxy. Quick check: Is the model agreeing with the benchmark because it's correct, or because both share similar bias?

## Architecture Onboarding

- **Component map**: Raw Logs -> Cleaning -> Semantic Deduplication -> Balanced Dataset -> Dynamic Prompt Constructor -> LLM Execution Engine -> Validation Layer -> Evaluation Interface
- **Critical path**: The Prompt Constructor. If dynamic mapping logic fails to filter label list correctly, the LLM suffers from high cardinality noise, hallucinations increase, and token costs spike.
- **Design tradeoffs**:
  - Proprietary vs. Local: Proprietary models offer high alignment (F1 0.83) and multilingual support but incur data privacy risks; local models offer privacy but suffer from hallucinations and lower accuracy (F1 ~0.62)
  - Cost vs. Reliability: Gemini-2.5-Flash is cheapest/fastest but has 2% error rate; GPT-5 is expensive but 0% error rate
- **Failure signatures**:
  - Hallucination: Model invents label not present in schema (common in Mistral:7b)
  - Overconfidence: Model assigns "High" confidence to incorrect labels (Phi-4:14b)
  - API Drift: Changes in token limits or pricing structures breaking cost/throughput assumptions
- **First 3 experiments**:
  1. Calibration Audit: Run 100 logs through GPT-o3 and Phi-4, plot Confidence vs. Accuracy to verify overconfidence finding
  2. Static vs. Dynamic Prompting A/B Test: Measure F1-score difference when disabling component-based label filtering
  3. HITL Simulation: Flag logs with Confidence < 0.8 for human review, calculate time saved vs. reviewing 100% of logs

## Open Questions the Paper Calls Out

### Open Question 1
Can domain-specific fine-tuning of open-source models bridge the performance and calibration gap with proprietary models? The authors state a "crucial next step" is fine-tuning high-performing open-source models on expert-verified datasets to create specialized, cost-effective tools. This study only evaluated models in zero-shot or few-shot settings; fine-tuning experiments were outside scope. Evidence needed: benchmark comparison showing fine-tuned open-source models achieving F1-scores and calibration reliability statistically similar to proprietary leaders like GPT-5.

### Open Question 2
How does integrating LLM-structured maintenance logs with synchronous SCADA data improve root cause analysis accuracy? The paper suggests future work should integrate structured output with data fusion methodologies to create an "Enriched Health History." This study focused solely on structuring unstructured text without merging with quantitative sensor data. Evidence needed: study demonstrating combined data model identifies fault precursors or root causes with higher precision than SCADA or logs alone.

### Open Question 3
What is the tangible business impact of LLM-assisted HITL workflow on maintenance costs and Levelised Cost of Energy (LCOE)? The conclusion calls for longitudinal study to quantify impact on key reliability metrics and overall LCOE. This research was technical benchmark without measuring long-term economic outcomes or operational efficiency gains in live deployment. Evidence needed: operational data showing reduction in downtime or O&M expenditures following LLM-assisted labelling system deployment.

## Limitations
- Proprietary models' superiority is limited by auditability - lack of published model weights or detailed inference parameters
- Open-source models' poor performance could stem from prompt engineering limitations rather than inherent model capabilities
- Benchmark's ground truth relies on GPT-5 outputs, creating circular validation when GPT-5 is also top performer

## Confidence

- **High Confidence**: Dynamic label filtering mechanism (proven by token reduction and accuracy gains), model calibration importance for HITL workflows (supported by empirical calibration curves), semantic ambiguity limiting accuracy ceiling (validated by inter-expert disagreement literature)
- **Medium Confidence**: Proprietary model superiority (limited by auditability), open-source models' performance (potentially improvable with better prompting), specific F1-score thresholds (dependent on benchmark quality)
- **Low Confidence**: Cost-effectiveness comparisons (based on current API pricing that may change), generalization to other maintenance domains (only tested on wind turbines), claim that no model is "perfect" (lacks formal impossibility proof)

## Next Checks

1. **Calibration Replication**: Run 200 logs through Phi-4:14b and GPT-o3, plotting confidence vs. accuracy to verify overconfidence/underconfidence patterns hold in your specific dataset before deploying HITL workflows
2. **Static Prompting Control**: Disable component-based label filtering and re-run 100 logs to measure F1-score degradation, confirming dynamic prompt mechanism's value claim
3. **Domain Transfer Test**: Apply same benchmark framework to different maintenance domain (e.g., solar panel logs or manufacturing equipment) to assess whether semantic ambiguity and model performance patterns replicate