---
ver: rpa2
title: A Set of Quebec-French Corpus of Regional Expressions and Terms
arxiv_id: '2510.05026'
source_url: https://arxiv.org/abs/2510.05026
tags:
- language
- dialect
- understanding
- idiom
- idioms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two new benchmark datasets for evaluating
  dialect understanding in the Quebecois dialect of French: QFrCoRE (4,633 idiomatic
  phrases) and QFrCoRT (171 idiomatic words). The authors propose that understanding
  regional idioms is a strong test of dialect comprehension, since such idioms are
  deeply rooted in local culture and history, unlike general language rules that can
  be inferred from the prestige dialect.'
---

# A Set of Quebec-French Corpus of Regional Expressions and Terms

## Quick Facts
- arXiv ID: 2510.05026
- Source URL: https://arxiv.org/abs/2510.05026
- Authors: David Beauchemin; Yan Tremblay; Mohamed Amine Youssef; Richard Khoury
- Reference count: 20
- Key outcome: Over 40% of models performed worse than random guessing on Quebecois idiom benchmarks, confirming dialect-specific knowledge is critical for understanding regional expressions.

## Executive Summary
This paper introduces two new benchmark datasets for evaluating dialect understanding in the Quebecois dialect of French: QFrCoRE (4,633 idiomatic phrases) and QFrCoRT (171 idiomatic words). The authors propose that understanding regional idioms is a strong test of dialect comprehension, since such idioms are deeply rooted in local culture and history, unlike general language rules that can be inferred from the prestige dialect. Both datasets were manually curated from dictionaries and online sources, with distractor definitions generated by a state-of-the-art LLM and filtered for semantic plausibility. Evaluation across 94 large language models (including both open-source and proprietary models) using a zero-shot setup showed that over 40% of models performed worse than random guessing, especially fine-tuned French models, highlighting the challenge of understanding regional dialects.

## Method Summary
The benchmark construction pipeline extracts idiomatic expressions and terms from Quebec-specific dictionaries and online sources, manually filtering for quality and relevance. GPT-4o-mini generates distractor definitions which are validated against the correct definition using a weighted semantic similarity score (BLEU, ROUGE, BERTScore). Definitions with similarity scores above 0.45 are rejected and regenerated. The zero-shot evaluation uses a standardized prompt format with explicit index-based answer requirements. The benchmark tests model ability to identify the correct definition among 10 options (1 correct + 9 distractors) for each Quebecois idiom.

## Key Results
- Over 40% of models performed worse than random guessing (10%) on Quebecois idiom comprehension
- French fine-tuned models (Chocolatine, French-Alpaca, Lucie) showed below-random performance due to negative transfer from prestige French
- Proprietary models outperformed open-source models, with all high-performing models being proprietary and all low/intermediate performers being open-source

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regional idioms function as a rigorous probe for dialect comprehension because they cannot be inferred from prestige-dialect training.
- Mechanism: Idioms derive meaning from local culture, history, and folklore rather than compositional semantics. Since prestige dialects lack these cultural anchors, models trained predominantly on them lack the prerequisite knowledge, making idiom understanding a binary signal of dialect exposure.
- Core assumption: Dialect-specific lexical knowledge is not systematically recoverable via reasoning or transfer from related dialects.
- Evidence anchors:
  - [abstract]: "regional idioms are a reliable tool for measuring a model's proficiency in a specific dialect."
  - [Section 3]: "the dialect's linguistic rules, syntax and grammar can be approximated or inferred from the prestige language... However, the dialect's idioms are unique to it."
  - [corpus]: Related work (Kantharuban et al., 2023) confirms dialect gaps across 12 languages, but corpus evidence specifically linking idioms to dialect proficiency is limited to this paper's hypothesis.

### Mechanism 2
- Claim: Models trained on prestige dialects exhibit negative transfer when evaluated on regional idioms.
- Mechanism: Prestige-dialect knowledge activates semantically plausible but incorrect associations. Models select distractors that would be valid in the prestige dialect but are wrong regionally, causing below-random performance.
- Core assumption: Distractors generated by LLMs (GPT-4o-mini) produce prestige-dialect-plausible definitions that actively mislead models lacking regional knowledge.
- Evidence anchors:
  - [Section 5.1]: "over 40% of models performed worse than random guessing... actively being misled by the distractors... negative transfer."
  - [Section 5.3]: French fine-tuned models (Chocolatine, French-Alpaca, Lucie) performed poorly because "none of these models were exposed to data written in the Quebecois dialect."

### Mechanism 3
- Claim: Proprietary models outperform open-source models due to training data diversity that includes regional content, not architecture or reasoning capabilities.
- Mechanism: Large-scale web crawls used by proprietary models incidentally capture Quebec-specific websites, Wikipedia pages, and regional content. This incidental exposure provides the lexical knowledge required, independent of model size or specialized training.
- Core assumption: Quebec content exists in sufficient quantity within proprietary training corpora to confer measurable advantages.
- Evidence anchors:
  - [Section 5.4]: "all low- and intermediate-performance models are open-source, while all high-performance models are proprietary... The real difference stems from training data."

## Foundational Learning

- Concept: **Idiom Non-Compositionality**
  - Why needed here: Idioms cannot be understood by analyzing constituent words (e.g., "hoist with his own petard"). This distinguishes idiom benchmarks from general language understanding tasks.
  - Quick check question: Can you explain why "bury the hatchet" requires cultural/historical knowledge beyond word-level semantics?

- Concept: **Prestige Dialect vs. Regional Dialect**
  - Why needed here: The paper's core thesis relies on understanding that prestige dialects (Standard French) dominate training data, creating systematic blind spots for regional variants (Quebecois).
  - Quick check question: How would a model trained exclusively on Parisian French likely interpret the Quebec term "Tiguidou!" without regional exposure?

- Concept: **Zero-Shot Evaluation**
  - Why needed here: The benchmark uses zero-shot setup to measure out-of-the-box capabilities without task-specific fine-tuning, isolating pretrained knowledge from adaptation effects.
  - Quick check question: Why does zero-shot evaluation better reflect real-world dialect handling than few-shot or fine-tuned approaches?

## Architecture Onboarding

- Component map:
  - Dictionary/online source extraction -> Manual filtering (remove anglicisms, duplicates) -> GPT-4o-mini distractor generation -> Semantic similarity filtering (BLEU/ROUGE/BERTScore threshold <0.45) -> Random answer positioning

- Critical path:
  1. Idiom-definition pairs extracted from Quebec-specific sources (Dictionnaire des expressions québécoises, McGill glossaries, Canada-Media)
  2. Distractors generated and filtered for semantic plausibility without overlapping correct definitions
  3. Zero-shot prompts formatted with explicit index-based answer requirements
  4. Accuracy compared against random baseline (10%) and stratified by model characteristics

- Design tradeoffs:
  - **AI-generated distractors**: Scalable but risk model-family artifacts inflating scores for related architectures (acknowledged in Limitations)
  - **Multiple-choice format**: Objective evaluation but doesn't test pragmatic appropriateness or social register understanding
  - **Zero-shot only**: Clear baseline but omits adaptation strategies that could improve performance

- Failure signatures:
  - Models scoring <10% (random baseline) indicate active negative transfer from prestige-dialect training
  - French fine-tuned models underperforming multilingual baselines signal training data provenance issues (translated or non-regional sources)
  - High variance within model families (e.g., DeepSeek: 6.4%–92.4%) suggests dialect knowledge is data-dependent, not architecture-driven

- First 3 experiments:
  1. **Baseline Establishment**: Run random selection baseline (seed=42) on both QFrCoRE and QFrCoRT to confirm 10% floor; verify your evaluation harness produces identical scores for deterministic models across runs.
  2. **Negative Transfer Validation**: Compare French fine-tuned models (e.g., Chocolatine, Lucie) against their base multilingual counterparts. Expect fine-tuned versions to underperform if training data lacks Quebec content.
  3. **Data Composition Analysis**: If possible, inspect training data provenance for top-performing open-source models (e.g., Qwen3-14B at 50.88%) vs. low performers to identify whether Quebec-specific sources correlate with higher scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the significant performance gap between proprietary and open-source models persist when evaluating larger open-source models (70B+ parameters) that were excluded due to hardware constraints?
- Basis in paper: [inferred] The authors note a hardware limitation restricted open-source testing to models ≤32B parameters, leaving the performance of the most capable open-source models (e.g., 70B+) unknown.
- Why unresolved: The study could not determine if the "access paradigm" gap is due to training data quality or simply the raw scale difference between the tested models.
- What evidence would resolve it: Benchmarking leading large open-source models (e.g., Llama-3.1-70B) against the proprietary leaders on QFrCoRE and QFrCoRT.

### Open Question 2
- Question: To what extent do in-context learning (few-shot) or fine-tuning strategies improve model performance compared to the zero-shot setup used in this study?
- Basis in paper: [inferred] The authors acknowledge their evaluation scope is limited to zero-shot settings and omits adaptation strategies like few-shot examples or fine-tuning.
- Why unresolved: It is unclear if the poor performance by French fine-tuned models is an inherent data issue or if specific dialectal few-shot prompting could bridge the performance gap.
- What evidence would resolve it: Running the same benchmarks using few-shot prompting with Quebecois examples and measuring the performance delta.

### Open Question 3
- Question: What is the human performance baseline on these benchmarks for both Quebec French speakers and speakers of the prestige France French dialect?
- Basis in paper: [explicit] The authors state: "We also aim to obtain human evaluations... from speakers of each dialect and from speakers of other French dialects, in order to establish human performance baselines."
- Why unresolved: Without human baselines, the "dialect gap" is currently defined only by the poor performance of models relative to random guessing, rather than relative to actual human comprehension.
- What evidence would resolve it: Conducting user studies with diverse Francophone populations to establish accuracy rates on the QFrCoRE and QFrCoRT tasks.

## Limitations
- Benchmark relies on AI-generated distractors which may introduce artifacts that artificially inflate scores for certain model families
- Evaluation only tests recognition of idiomatic meanings through multiple-choice selection, not pragmatic appropriateness or contextual usage
- Negative transfer phenomenon in French fine-tuned models could reflect dataset contamination or translation artifacts rather than genuine dialect confusion

## Confidence

- **High Confidence**: The fundamental claim that regional idioms serve as valid probes for dialect comprehension (supported by strong theoretical reasoning and empirical performance gaps). The observation that model size and reasoning capabilities weakly correlate with idiom understanding (supported by consistent performance stratification across model tiers).
- **Medium Confidence**: The negative transfer mechanism in French fine-tuned models (empirical but lacks direct corpus evidence for the underlying mechanism). The proprietary vs. open-source performance gap being primarily data-driven (supported by performance patterns but not verified through training data inspection).
- **Low Confidence**: The assertion that Quebecois idioms cannot be inferred from prestige-dialect training through reasoning or transfer (requires stronger empirical validation that models cannot bootstrap idiom knowledge from context).

## Next Checks

1. **Distractor Artifact Validation**: Retest a subset of models using manually curated distractors rather than AI-generated ones to determine if performance drops correlate with distractor generation method rather than genuine dialect knowledge gaps.

2. **Contextual Usage Evaluation**: Extend the benchmark to include cloze tests or free-response prompts where models must generate or appropriately use Quebecois idioms in sentences, testing pragmatic competence beyond recognition.

3. **Training Data Attribution Analysis**: For top-performing open-source models, attempt to identify whether Quebec-specific content appears in their training corpora through keyword analysis or domain attribution, directly testing the data-driven advantage hypothesis.