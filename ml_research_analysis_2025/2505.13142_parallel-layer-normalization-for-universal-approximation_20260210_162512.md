---
ver: rpa2
title: Parallel Layer Normalization for Universal Approximation
arxiv_id: '2505.13142'
source_url: https://arxiv.org/abs/2505.13142
tags:
- approximation
- have
- normalization
- relu
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves that a neural network composed solely of parallel\
  \ layer normalization (PLN) and linear layers has universal approximation capacity,\
  \ a novel theoretical result for networks incorporating normalization layers. The\
  \ authors derive that such a PLN-Net with width d(\u230AL/2\u03B5\u230B+1) can approximate\
  \ any L-Lipchitz continuous function on [0,1] within L\u221E error \u03B5, where\
  \ d is the norm size."
---

# Parallel Layer Normalization for Universal Approximation

## Quick Facts
- **arXiv ID:** 2505.13142
- **Source URL:** https://arxiv.org/abs/2505.13142
- **Reference count:** 40
- **Primary result:** Proves PLN-Net (parallel layer normalization + linear layers) is a universal approximator, outperforming traditional activations on CIFAR-10 and Transformers.

## Executive Summary
This paper introduces Parallel Layer Normalization (PLN) and proves it can replace traditional activation functions while maintaining universal approximation capacity. The authors demonstrate that a network composed solely of PLN and linear layers can approximate any Lipschitz continuous function, with theoretical bounds on required width. Experiments show PLN excels at high-dimensional function approximation and improves performance when substituting BatchNorm or LayerNorm in standard architectures, achieving 89.45% CIFAR-10 accuracy and improving BLEU scores from 30.82 to 42.91 in Transformers.

## Method Summary
PLN divides neurons into groups of size d≥2, applying Layer Normalization independently to each group before concatenation. For CNNs, Channel-PLN normalizes across channel dimensions separately per spatial position. The method replaces standard activations (ReLU, sigmoid, tanh) and normalization layers (BatchNorm, LayerNorm) in various architectures. Training uses standard optimizers (Adam, SGD) with typical learning rates and batch sizes, with specific configurations detailed for each experiment.

## Key Results
- PLN-Net with width d(⌊L/2ε⌋+1) can approximate any L-Lipschitz function on [0,1] within L∞ error ε
- On CIFAR-10, PLN-8 in VGG-16 and ResNet-20 achieves 89.45% and 92.89% accuracy respectively
- Replacing LayerNorm with PLN-8 in Transformers improves Multi30K BLEU scores from 30.82 to 42.91
- PLN acts as both activation and normalization, stabilizing gradients in deep networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PLN-Net is a universal approximator for continuous functions.
- **Mechanism:** Layer Normalization on R² with linear transformations can construct step functions by limiting the LN denominator to approach zero. Since linear combinations of step functions approximate any L-Lipschitz continuous function, PLN-Net can approximate any target function.
- **Core assumption:** The small constant δ in LN can be treated as a limiting parameter (δ→0) during theoretical construction.
- **Evidence anchors:** Universal approximation proof in Appendix A.1, Lemma 1 showing LN→sign function as δ→0.
- **Break condition:** If d=1 or width < d(⌊L/2ε⌋+1).

### Mechanism 2
- **Claim:** PLN functions as both activation and normalization layer.
- **Mechanism:** PLN provides necessary nonlinearity while maintaining gradient flow through scale invariance. Unlike ReLU, which suffers gradient explosion in deep ResNets without BN, PLN keeps gradients bounded.
- **Core assumption:** Normalization nonlinearity is sufficient for task complexity with smooth optimization landscape.
- **Evidence anchors:** Section 4.2 Table 2 showing ResNet-110 gradient norm 1.2×10¹³ for ReLU vs 41.01 for PLN-8.
- **Break condition:** If d is too small (e.g., d=2), outputs become ±1 blocking gradient backpropagation.

### Mechanism 3
- **Claim:** PLN improves Transformer performance by amplifying nonlinearity.
- **Mechanism:** Standard LayerNorm provides limited nonlinearity. PLN-8 introduces additional nonlinear transformations within normalization itself, creating "dual activation" with ReLU that increases representation capacity.
- **Core assumption:** Transformer architecture under-utilizes potential nonlinearity at normalization stage.
- **Evidence anchors:** Section 4.3.2 showing BLEU improvement from 30.82 (Identity+LN) to 42.91 (ReLU+PLN-8).
- **Break condition:** If task relies on specific scaling properties of standard LayerNorm that PLN grouping disrupts.

## Foundational Learning

- **Concept: Universal Approximation Theorem (UAT)**
  - **Why needed here:** The paper re-proves UAT for normalization-based networks, contrasting with traditional sigmoid/ReLU proofs.
  - **Quick check question:** Can a network with only linear layers approximate f(x)=x²? (Answer: No, requires nonlinear activation like PLN).

- **Concept: Lipschitz Continuity**
  - **Why needed here:** Theoretical bounds for PLN-Net's approximation error are defined for L-Lipschitz continuous functions.
  - **Quick check question:** Does f(x)=|x| satisfy the condition for approximation bounds? (Answer: Yes, it is 1-Lipschitz).

- **Concept: Norm Size (Group Size)**
  - **Why needed here:** Critical hyperparameter in PLN (d) determines if it acts like step function (d=2) or smoother operator (d=8).
  - **Quick check question:** If PLN norm size is d=2, why might optimization fail despite good theoretical capacity? (Answer: Outputs restricted to ±1, blocking gradients).

## Architecture Onboarding

- **Component map:** Input → Linear Layer → PLN Layer → Output
- **PLN Layer:** Splits input vector of width W into N groups of size d (W=N×d). Applies Layer Normalization independently to each group, then concatenates normalized groups.
- **Critical path:** Defining Norm Size (d)
  - d=2: Max theoretical nonlinearity (step function), poor optimization (gradient blocking)
  - d≥4: Balanced optimization and capacity
  - **Default Recommendation:** d=8 (based on CIFAR/Transformer experiments)

- **Design tradeoffs:**
  - **Width vs. Norm Size:** For fixed width W, increasing d decreases number of parallel groups N. Since approximation error depends on N, arbitrarily large d might hurt representation if width isn't increased.
  - **PLN vs. Standard LN:** PLN adds nonlinearity but is structurally more complex than standard LN.

- **Failure signatures:**
  - **Gradient Vanishing in Shallow Nets:** Occurs if d is too small (e.g., PLN-2) in shallow networks
  - **Gradient Explosion in Deep Nets:** Occurs if using ReLU without normalization; PLN specifically fixes this
  - **High Error in Approximation:** Occurs if network width < d(⌊L/2ε⌋+1)

- **First 3 experiments:**
  1. **Ablation on Norm Size:** Train shallow MLP (width 128) on 1D sin(2x) function. Compare d∈{2,4,8,16} to verify gradient blocking of d=2 and stability of d=8.
  2. **Deep Net Sanity Check:** Train VGG-16 on CIFAR-10 without BatchNorm. Compare ReLU vs. PLN-8. Expect ReLU to fail/stagnate, PLN-8 to achieve >85% accuracy.
  3. **Transformer Integration:** Fine-tune small Transformer on Multi30K replacing LayerNorm with PLN-8. Keep standard ReLU activations. Measure if "dual nonlinearity" boosts BLEU score over baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the precise minimum approximation width for PLN-Net with high-dimensional inputs (n>1)?
  - **Basis:** Section 3.1 states this case will be studied in future work, as Theorem 1 only solves 1D case while Theorem 2 proves density for C([0,1]ⁿ) without quantitative bounds.
  - **Why unresolved:** Theorem 2 proves existence but not quantitative width bounds established for 1D case.
  - **What evidence would resolve it:** Mathematical proof extending width bounds W(LN-d) to n-dimensional input spaces, quantifying relationship between input dimension n, error ε, and Lipschitz constant L.

- **Open Question 2:** Can PLN maintain dual role effectiveness when scaled to large-scale datasets and architectures?
  - **Basis:** "Limitation and Future Work" section explicitly notes effectiveness is only verified on small-scale networks and datasets.
  - **Why unresolved:** Success on CIFAR-10 and Multi30K is unverified for massive models like LLMs or high-resolution vision tasks.
  - **What evidence would resolve it:** Benchmark results on large-scale tasks (ImageNet or WMT) showing PLN can replace standard layers without auxiliary techniques.

- **Open Question 3:** What are theoretical and empirical guidelines for selecting norm size d and other hyperparameters?
  - **Basis:** Conclusion states "many hyperparameters we have not studied and no training trick of PLN is concluded."
  - **Why unresolved:** Paper empirically selects d=8 but admits potential capabilities are not fully exploited, leaving interaction between norm size, network width, and approximation error loosely defined.
  - **What evidence would resolve it:** Systematic ablation study or theoretical derivation establishing optimal norm sizes relative to network width and target function complexity.

## Limitations

- Theoretical bounds depend on unachievable δ→0 assumption in finite-precision implementations
- Norm size hyperparameter sensitivity lacks full characterization across parameter space
- Architectural generality unproven beyond image classification and translation tasks

## Confidence

**High Confidence:**
- Universal approximation proof methodology is sound and follows established frameworks
- CIFAR-10 results showing PLN-8 outperforming ReLU and BatchNorm are well-documented
- Gradient stability advantage over ReLU in deep networks is clearly demonstrated

**Medium Confidence:**
- Mechanism explaining PLN's dual role is theoretically justified but lacks separate component validation
- Transformer improvements may be partially attributable to increased model capacity

**Low Confidence:**
- Claims about high-dimensional function approximation are based on synthetic data without comparison to specialized approximators
- Relationship between norm size d and approximation error ε is theoretically derived but not empirically validated across full parameter space

## Next Checks

1. **Norm Size Sensitivity Analysis:** Systematically vary d∈{2,4,8,16} on CIFAR-10 training with ResNet-20, measuring final accuracy and gradient norms throughout training to map transition from unstable to stable optimization.

2. **Ablation on Normalization vs. Nonlinearity:** Create PLN-Net variants separating normalization and nonlinear components. Train with (a) standard LN+ReLU, (b) PLN without normalization, and (c) full PLN-8. Compare approximation capacity on synthetic 1D functions to isolate improvement sources.

3. **Cross-Domain Generalization:** Test PLN-8 on object detection (COCO) and semantic segmentation (Cityscapes) using standard architectures (Faster R-CNN, U-Net), comparing against BatchNorm baselines to verify translation task improvements generalize to other vision domains.