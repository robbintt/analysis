---
ver: rpa2
title: Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech
  Recognition
arxiv_id: '2601.19451'
source_url: https://arxiv.org/abs/2601.19451
tags:
- expert
- projector
- multilingual
- speech
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multilingual ASR in LLM-based
  systems, where a single projector struggles to capture diverse acoustic-to-semantic
  mappings across typologically distinct languages. To overcome this, the authors
  propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense
  gradient flow to all experts, preventing expert collapse and enabling cross-lingual
  sharing.
---

# Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition

## Quick Facts
- arXiv ID: 2601.19451
- Source URL: https://arxiv.org/abs/2601.19451
- Authors: Isha Pandey; Ashish Mittal; Vartul Bahuguna; Ganesh Ramakrishnan
- Reference count: 0
- Primary result: Up to 7.6% relative WER reduction over single-projector baseline on four mid-resource Indic languages

## Executive Summary
This paper addresses the challenge of multilingual ASR in LLM-based systems, where a single projector struggles to capture diverse acoustic-to-semantic mappings across typologically distinct languages. To overcome this, the authors propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse and enabling cross-lingual sharing. Evaluated on four mid-resource Indic languages (Hindi, Marathi, Tamil, Telugu), SMEAR-MoE achieves up to a 7.6% relative WER reduction over the single-projector baseline while maintaining comparable runtime efficiency. Routing analysis further reveals linguistically meaningful expert specialization, with related languages sharing experts, making it a promising approach for scalable, robust multilingual ASR.

## Method Summary
SMEAR-MoE introduces a stabilized Mixture-of-Experts projector for multilingual ASR that connects frozen speech encoder (Whisper large-v3) to frozen LLM (Gemma-2-9B). The method uses a shared Conv1D downsampler (~13M params) to learn common acoustic representations across all languages, followed by a gating network that produces token-level probabilities for routing to M=4 lightweight expert MLPs (~9M params each). Instead of hard top-k routing, SMEAR constructs a virtual expert by linearly combining all expert parameters according to gating weights (W̄ = ΣḡₘWₘ), ensuring dense gradient flow to prevent expert collapse. The model is trained with utterance-level averaged gates and load-balancing loss, achieving better performance than single-projector and dense ensemble baselines while maintaining runtime efficiency.

## Key Results
- 7.6% relative WER reduction over single-projector baseline on VISTAR benchmark
- RTF comparable to single-projector baseline despite dynamic routing
- Routing analysis shows linguistically meaningful specialization: Hindi/Marathi share experts (Indo-Aryan), Tamil has specialized expert (Dravidian)
- SMEAR-MoE outperforms dense ensemble (72M params) while using fewer parameters (52M)

## Why This Works (Mechanism)

### Mechanism 1: Soft parameter merging prevents expert collapse
Soft merging of expert parameters prevents expert collapse by ensuring dense gradient flow to all experts during training. Instead of hard top-k routing where only selected experts receive gradient updates, SMEAR constructs a differentiable "virtual expert" by linearly combining all expert parameters according to gating weights. Every expert receives gradient signal proportional to its gate weight, eliminating the sparse gradient problem that causes under-utilization in conventional MoEs.

### Mechanism 2: Shared acoustic downsampling with language-specialized MLPs
Decoupling shared acoustic downsampling from language-specialized MLPs enables cross-lingual transfer while maintaining capacity for language-specific mappings. A shared Conv1D downsampler reduces sequence length and learns common acoustic representations across all languages. Lightweight per-expert MLPs then specialize to particular acoustic-to-semantic mappings. The shared component enables positive transfer, while experts capture typological diversity (Indo-Aryan vs. Dravidian patterns).

### Mechanism 3: Utterance-level averaged gates enable unsupervised linguistic specialization
Utterance-level averaged gates produce linguistically meaningful routing without explicit language supervision. Token-level gate probabilities are averaged across the utterance, producing stable routing decisions per sample. The network learns to associate acoustic patterns with expert weights, emerging as language-family clustering (Hindi-Marathi → Expert 4; Tamil → Expert 2).

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**
  - Why needed here: SMEAR-MoE modifies standard MoE routing; understanding top-k vs. soft routing is prerequisite to grasping the innovation.
  - Quick check question: In standard top-2 MoE, what fraction of experts receive non-zero gradients for a given token?

- **Gradient Flow in Sparse vs. Dense Architectures**
  - Why needed here: The paper's core claim is that sparse routing causes expert collapse via poor gradient flow; dense merging solves this.
  - Quick check question: If only 2 of 8 experts are active per batch, how does this affect parameter updates across training?

- **Speech Encoder → LLM Projector Paradigm**
  - Why needed here: The method operates specifically on the projector layer connecting frozen Whisper encoder to frozen Gemma LLM.
  - Quick check question: What representations must the projector transform, and why is this harder for multilingual than monolingual settings?

## Architecture Onboarding

- **Component map:**
  Frozen Whisper large-v3 encoder → produces HS → Shared Conv1D downsampler D(·) → produces ZS → Gating network → produces G ∈ R^(T×M) → M=4 expert MLPs {Pm} → Virtual expert construction → W̄ = ΣḡₘWm, b̄ = Σḡₘbm → Frozen Gemma-2-9B LLM

- **Critical path:**
  1. Verify encoder outputs are frozen (no gradient)
  2. Confirm shared downsampler trains across all languages
  3. Check gate averaging produces stable ḡ per utterance
  4. Validate virtual expert construction applies merged weights to ZS
  5. Ensure LLM receives projected embeddings in expected format

- **Design tradeoffs:**
  - Monolithic (18M): Simple, fast, but representational bottleneck across languages
  - Dense Ensemble (72M): Best sharing, but 4× compute, static weights
  - Hard MoE (52M): Dynamic but suffers expert collapse
  - SMEAR-MoE (52M): Dense gradients + dynamic routing, RTF ≈ baseline

- **Failure signatures:**
  - Expert collapse: Gate entropy drops; 1-2 experts dominate (>80% routing)
  - No specialization: Uniform gate distribution across all inputs regardless of language
  - Training instability: WER fluctuates without convergence after warmup
  - Cross-lingual interference: WER increases on low-resource language when adding high-resource data

- **First 3 experiments:**
  1. **Baseline sanity check**: Reproduce monolithic projector WER on single language (expect ~19% WER Hindi per Table 1). If significantly different, check data preprocessing or LLM prompt format.
  2. **Expert utilization audit**: Log gate distributions per language during training. If entropy collapses within first 10k steps with standard MoE, confirms the gradient flow problem SMEAR addresses.
  3. **Ablation on expert count**: Train SMEAR-MoE with M=2, 4, 8 experts on held-out validation set. If M=2 matches M=8, either data insufficient for specialization or shared downsampler already captures needed mappings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does SMEAR-MoE's routing specialization and performance gains scale effectively to a much larger number of languages (e.g., 50-100+), or does the soft-merging strategy become a bottleneck?
- Basis in paper: The paper evaluates only four Indic languages and states that static multi-projector approaches "scale poorly as the number of languages grows," but does not test SMEAR-MoE beyond four languages.
- Why unresolved: The virtual expert computation (merging all expert parameters) has complexity proportional to the number of experts, and it is unclear whether dense gradient flow remains efficient or effective with significantly more languages.
- What evidence would resolve it: Experiments scaling to 10, 25, 50+ languages with analysis of WER trends, expert utilization patterns, and computational overhead relative to baselines.

### Open Question 2
- Question: How does SMEAR-MoE perform on truly low-resource languages (e.g., <10 hours of training data), where expert specialization may conflict with data scarcity?
- Basis in paper: The paper trains on ~250 hours per language (mid-resource) and claims data-constrained stability, but explicitly notes that language-specific projectors "perform poorly under limited data."
- Why unresolved: With minimal data, the router may not learn meaningful specialization, and the soft-merging might dilute already sparse linguistic signal rather than help.
- What evidence would resolve it: Controlled experiments varying training hours (5h, 10h, 50h, 100h) across languages, comparing SMEAR-MoE against single-projector and dense ensemble baselines.

### Open Question 3
- Question: Would incorporating explicit linguistic or script information into the gating network improve routing and cross-lingual transfer?
- Basis in paper: The routing analysis reveals "linguistically meaningful expert sharing" emerges implicitly, but Telugu shows "more distributed probabilities" despite being Dravidian, suggesting suboptimal routing without explicit linguistic guidance.
- Why unresolved: It is unclear whether implicit routing discovery is sufficient or whether providing family/script metadata could further enhance specialization and sharing.
- What evidence would resolve it: Ablation studies comparing implicit routing against routing conditioned on language family or script identifiers, with analysis of expert assignment patterns and WER.

## Limitations
- Evaluation limited to four mid-resource Indic languages; generalization to diverse language families unclear
- No analysis of robustness to noisy inputs, out-of-domain speech, or code-switching scenarios
- Runtime efficiency claims based on 4 experts; scaling to more experts may impact real-time performance

## Confidence
**High Confidence**: Claims about SMEAR-MoE outperforming single-projector baselines on the tested Indic languages (7.6% relative WER reduction). The ablation results and routing analysis are internally consistent and supported by the reported data.

**Medium Confidence**: Generalization of the soft merging mechanism to other language families or larger expert counts. While the principle is sound, empirical validation on typologically diverse languages is needed.

**Low Confidence**: Claims about SMEAR-MoE's efficiency relative to dense ensembles in large-scale deployments. The 52M parameter count is favorable vs. 72M dense, but no comparison of training/inference throughput or memory footprint is provided.

## Next Checks
1. **Cross-linguistic generalization test**: Evaluate SMEAR-MoE on a dataset with typologically diverse languages (e.g., Romance, Slavic, and East Asian languages) to assess whether the soft merging mechanism prevents collapse across broader linguistic distances.

2. **Stress test for expert collapse**: Intentionally train a standard top-k MoE projector on the same data and monitor gate entropy over time. Compare the rate and severity of collapse against SMEAR-MoE to quantify the practical impact of dense gradient flow.

3. **Runtime and scalability audit**: Measure RTF and memory usage for SMEAR-MoE with M=4, 8, and 16 experts on the same hardware. Determine whether the claimed efficiency holds as expert count increases, and identify the practical upper bound for real-time applications.