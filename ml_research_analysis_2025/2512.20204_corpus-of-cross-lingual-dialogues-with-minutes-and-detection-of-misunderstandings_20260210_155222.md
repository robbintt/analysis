---
ver: rpa2
title: Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings
arxiv_id: '2512.20204'
source_url: https://arxiv.org/abs/2512.20204
tags:
- language
- misunderstandings
- meetings
- corpus
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents InCroMin, a new corpus of cross-lingual dialogues
  between individuals without a common language, facilitated by automatic simultaneous
  speech translation. The corpus contains 5 hours of speech recordings with ASR transcripts
  in 12 original languages and automatic and corrected translations into English.
---

# Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings

## Quick Facts
- arXiv ID: 2512.20204
- Source URL: https://arxiv.org/abs/2512.20204
- Reference count: 8
- Primary result: LLM detects misunderstandings in cross-lingual dialogues with 77% recall but only 47% precision

## Executive Summary
This paper introduces InCroMin, a novel corpus of cross-lingual dialogues featuring automatic speech translation and human-annotated misunderstandings. The corpus includes 5 hours of audio in 12 source languages, with ASR transcripts, translations, and human-generated meeting minutes. The authors evaluate Gemini 1.5 Pro's ability to automatically detect misunderstandings in translated dialogue transcripts, finding promising recall but limited precision. The work addresses the critical need for realistic evaluation data in cross-lingual speech processing while initiating research into automatic misunderstanding detection.

## Method Summary
The InCroMin corpus was created using a cascaded speech translation system (Whisper for ASR → NLLB for translation) during real cross-lingual meetings. Fourteen two-participant meetings were manually annotated for misunderstandings, with annotators identifying spans, classifying awareness status, and attributing causes (translation error, delay, technical problem, or genuine misunderstanding). Gemini 1.5 Pro was then evaluated on its ability to detect these misunderstandings from timestamped English translation transcripts using a carefully designed system prompt. The corpus also includes minutes for cross-lingual summarization research.

## Key Results
- Gemini model detects misunderstandings with 77% recall and 47% precision
- Only 14 meetings annotated, with 222 total misunderstandings identified
- 36.5% of misunderstandings attributed to translation errors, 14.4% to delay, 19.4% to technical problems, and 29.7% to genuine misunderstandings
- Inter-annotator agreement is moderate for misunderstanding counts but weak for fine-grained span boundaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can detect cross-lingual misunderstandings from translated dialogue transcripts with usable recall but low precision.
- Mechanism: The Gemini 1.5 Pro model processes timestamped conversation transcripts and identifies semantic misalignments—where speakers' topics diverge, clarification is requested, or responses don't match preceding utterances.
- Core assumption: Surface-level textual cues in English translation reliably signal underlying communicative breakdowns.
- Evidence anchors: Abstract reports 77% recall and 47% precision; confusion matrix shows 122 true positives, 137 false positives, 36 false negatives; corpus limited to 14 meetings with explicit warning against generalization.
- Break condition: If misunderstandings primarily manifest through prosody, silence, or non-verbal cues absent from transcripts, detection will degrade significantly.

### Mechanism 2
- Claim: Simultaneous speech translation pipelines introduce systematic error sources traceable to specific components.
- Mechanism: The cascaded architecture (ASR → translation → display) creates distinct failure modes: Whisper translation errors, NLLB target-language errors, and system latency causing turn-taking confusion.
- Core assumption: Error types are partially separable by analyzing user reactions and transcript patterns.
- Evidence anchors: 36.5% translation errors, 14.4% delay-related, 19.4% technical problems, 29.7% genuine misunderstandings; user reports of "wrong target language," "audio segmentation errors," unclear turn-taking signals; Czech meetings showed 14% more technical errors but fewer translation errors—possibly due to shared background knowledge between professor-student pairs.
- Break condition: If ASR and MT errors compound non-linearly, root cause attribution becomes unreliable.

### Mechanism 3
- Claim: Human annotators can achieve moderate agreement on misunderstanding counts but disagree on boundary-level classifications.
- Mechanism: Annotators identify "markables" (misunderstanding spans), classify awareness (acknowledged/unrecognized), and attribute causes—but granularity of span delimiting varies.
- Core assumption: Annotators working from English translations can validly assess multilingual dialogue breakdowns.
- Evidence anchors: Pearson correlations: 0.94 for translation errors, 0.72 for technical problems, but only 0.55 for genuine misunderstandings; -0.13 correlation on Gemini false positives—"one annotator disapproved 4 misunderstandings suggested by Gemini but the other annotator did not reject any"; 11 meetings double-annotated; authors report agreement at count-level only, not item-level.
- Break condition: If task requires precise span boundaries or fine-grained cause attribution, inter-annotator reliability may be insufficient for training.

## Foundational Learning

- **Concept: Cascaded Speech Translation (ASR → MT)**
  - Why needed here: InCroMin uses Whisper for speech-to-English translation, then NLLB for English-to-target translation; errors compound across stages.
  - Quick check question: Can you trace a user complaint about "wrong translation" to ASR misrecognition vs. MT semantic drift?

- **Concept: Simultaneous vs. Batch Translation**
  - Why needed here: Real-time incremental output creates unique failure modes (delays, incomplete segments) absent in offline translation.
  - Quick check question: What user experience problems arise when translation lags 3-5 seconds behind speech?

- **Concept: Long-Context LLM Prompting for Dialogue**
  - Why needed here: Gemini 1.5 Pro was chosen for its long context window to process full meeting transcripts; prompt engineering affects detection quality.
  - Quick check question: How would you structure a prompt to distinguish translation errors from genuine semantic misunderstandings?

## Architecture Onboarding

- Component map:
Fairmeeting (video) → Minuteman → Whisper Large-v3 (speech → English translation) → NLLB-200-distilled-600M (English → target languages) → Display layer → Data collection

- Critical path:
1. Audio capture per-participant (no diarization needed—separate tracks)
2. Real-time Whisper translation to English (bottleneck for latency)
3. NLLB translation to each participant's selected language
4. Log all intermediate outputs for corpus creation

- Design tradeoffs:
- Whisper Large-v3 chosen for quality; latency impacts real-time UX
- NLLB-200-distilled-600M (not full model) for speed; may sacrifice translation quality
- English as pivot language; errors in first translation propagate to all targets
- Text-only output (no audio playback of translations); reduces cognitive load but loses prosody

- Failure signatures:
- "Meta-language" errors: Dialogue management phrases ("who speaks first?") mistranslated, derailing conversation structure
- False affirmation: Short affirmative responses generated from noise/silence
- Turn-taking confusion: Users unsure if processing complete; speak over incomplete translations
- Target language switching: NLLB occasionally outputs wrong language despite correct settings

- First 3 experiments:
1. Baseline misunderstanding detection: Replicate Gemini prompt on held-out meetings; measure precision/recall against human annotations.
2. Error source ablation: For meetings with corrected transcripts, compare misunderstanding rates using raw ASR vs. gold transcripts to isolate ASR contribution.
3. Prompt iteration: Test alternative prompts that explicitly request classification by cause (translation error vs. delay vs. genuine); measure whether precision improves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an automatic system successfully propose mitigation actions, such as clarification questions, when a risk of misunderstanding is detected?
- Basis in paper: The introduction states that an automatic system "could alert when there is a risk of misunderstanding and propose mitigation actions, such as clarification questions."
- Why unresolved: The paper focuses solely on the detection of misunderstandings; it does not implement or evaluate any automated mitigation or clarification strategies.
- What evidence would resolve it: A study measuring the success rate of automatically generated clarification questions in resolving detected misunderstandings during live cross-lingual dialogues.

### Open Question 2
- Question: Does delaying the interlocutor's audio to synchronize with the appearing translation text improve turn-taking clarity and reduce user confusion?
- Basis in paper: Section 4 reports user feedback suggesting "improving synchronization by delaying the other party’s audio to make it match the translation as it is appearing."
- Why unresolved: The current Minuteman system displays translation incrementally, which decouples the text from the audio stream, leading to uncertainty about turn completion.
- What evidence would resolve it: A comparative user interface study evaluating turn-taking efficiency and user confusion levels in systems with and without audio-delay synchronization.

### Open Question 3
- Question: Can the precision of Large Language Models in detecting misunderstandings be improved to reduce the high rate of false positives?
- Basis in paper: Section 3.4 reports that the Gemini model achieved only 47% precision, and Section 5 identifies the need for "higher quality speech processing" in future work.
- Why unresolved: The current LLM approach identifies many potential misunderstandings (high recall) but incorrectly flags valid exchanges as errors (low precision), making the alert system potentially distracting.
- What evidence would resolve it: Experimentation with fine-tuned models or refined prompts that achieve significantly higher precision (>80%) on the InCroMin corpus without degrading recall.

## Limitations

- The corpus is extremely small (only 14 meetings with 222 misunderstandings), limiting generalizability and statistical power.
- Detection performance shows high recall (77%) but problematic precision (47%), with false positive identification particularly unreliable between annotators.
- The English pivot translation approach may introduce systematic biases that don't reflect direct speech-to-speech translation scenarios.

## Confidence

- **Medium confidence** in the corpus utility claim: The InCroMin corpus represents a novel resource, but with only 14 meetings and no established baselines for cross-lingual misunderstanding detection, its research value remains to be demonstrated through community adoption.
- **Low confidence** in the LLM misunderstanding detection capability: With precision of 47% and particularly unreliable false positive identification (inter-annotator correlation of -0.13), the current approach shows promise for recall but lacks practical deployment readiness.
- **Medium confidence** in the error analysis framework: The decomposition of misunderstandings into translation errors, delays, technical problems, and genuine misunderstandings provides a useful taxonomy, though the separability of these categories remains partially validated.

## Next Checks

1. **Cross-lingual evaluation**: Test the Gemini misunderstanding detection model on translated versions of the same dialogues (non-English transcripts) to assess whether detection performance degrades when not operating on English pivot translations.

2. **Prompt engineering ablation**: Systematically vary the LLM prompt structure—testing explicit cause classification requests, different context window sizes, and alternative formulations—to determine whether precision improvements are achievable through better prompting rather than model selection.

3. **Domain transfer validation**: Apply the misunderstanding detection approach to a different domain (e.g., customer service dialogues, casual conversation) to assess whether the current methodology generalizes beyond technical/scientific discussions.