---
ver: rpa2
title: 'Automatic Speech Recognition Biases in Newcastle English: an Error Analysis'
arxiv_id: '2506.16558'
source_url: https://arxiv.org/abs/2506.16558
tags:
- errors
- english
- speech
- were
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates regional dialect bias in ASR systems by
  analyzing performance on Newcastle English, a dialect known to challenge existing
  ASR models. Through manual error analysis and automated case studies, the research
  identifies phonological, lexical, and morphosyntactic features that cause ASR recognition
  errors, finding that dialect-specific linguistic patterns drive errors more than
  social factors like age or gender.
---

# Automatic Speech Recognition Biases in Newcastle English: an Error Analysis

## Quick Facts
- arXiv ID: 2506.16558
- Source URL: https://arxiv.org/abs/2506.16558
- Authors: Dana Serditova; Kevin Tang; Jochen Steffens
- Reference count: 0
- One-line primary result: Dialect-specific linguistic patterns drive ASR errors more than social factors in Newcastle English.

## Executive Summary
This study investigates regional dialect bias in automatic speech recognition (ASR) systems by analyzing performance on Newcastle English, a dialect known to challenge existing ASR models. Through manual error analysis and automated case studies, the research identifies phonological, lexical, and morphosyntactic features that cause ASR recognition errors. The study finds that dialect-specific linguistic patterns drive errors more than social factors like age or gender, with glottalisation, monophthongisation, and local pronoun usage presenting significant challenges for ASR systems.

## Method Summary
The study uses the Diachronic Electronic Corpus of Tyneside English (DECTE) containing 72 hours of speech from 160 speakers. Audio files were processed through Rev AI ASR using the "English UK" setting, with outputs aligned to ground truth transcripts. Manual error classification was performed on 1,076 "meaningful" errors after filtering out noise-related issues. Statistical analysis employed mixed-effects logistic regression to examine error correlations with demographic variables. The research combined manual sociolinguistic coding with automated scripts for pronoun recognition analysis.

## Key Results
- Word Error Rate (WER) for Newcastle English was 31.95%, significantly higher than standard English benchmarks.
- Glottalisation of /t/ was the most challenging phonological feature, causing nearly 23% of phonological errors.
- Local pronoun usage (e.g., "yous," "wor") accounted for a third of all morphosyntactic errors.
- Demographic factors like age and gender showed error rate variation that correlated with feature usage rather than acoustic differences.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR errors in regional dialects are primarily driven by acoustic mismatches in phonological realization rather than social demographics.
- Mechanism: The acoustic model expects standard spectral patterns (e.g., clear plosives), but receives dialect-specific variants (e.g., glottalisation or monophthongisation). This lowers the confidence score for the correct phoneme sequence, forcing the decoder to rely on the language model or hallucinate a similar-sounding standard word.
- Core assumption: The ASR architecture uses an acoustic model trained predominantly on Standard Southern British English (SSBE) or General American, lacking weights for Newcastle-specific phonetic variants.
- Evidence anchors:
  - [abstract]: "identifies phonological... features that cause ASR recognition errors, finding that dialect-specific linguistic patterns drive errors more than social factors."
  - [section 3.2.2]: "Near glottalisation of /t/ was the most challenging feature for ASR, causing nearly 23% of phonological errors."
  - [corpus]: Weak direct support; neighbor papers discuss accent robustness generally but do not validate specific Newcastle phoneme error rates.
- Break condition: If the acoustic model is fine-tuned or adapted to recognize glottal stops as valid allophones of /t/.

### Mechanism 2
- Claim: Language Model (LM) bias causes "standardisation" errors where valid dialectal grammar is overwritten by statistically dominant standard forms.
- Mechanism: The decoder searches for the most probable word sequence. Even if the acoustic model detects "yous" or "wor" (dialect pronouns), the LM assigns them a lower probability than "you" or "our". Consequently, the system overrides the acoustic evidence to satisfy the standard grammar prior.
- Core assumption: The ASR decoder relies on a prior probability distribution that penalizes non-standard morphosyntax, and the vocabulary lacks high-weight entries for regional lexical items.
- Evidence anchors:
  - [section 3.1]: "Standardisation errors involved replacing dialectal features with Standard Southern British English forms (e.g., 'me life' -> 'my life')."
  - [section 3.2.3]: "Local pronoun usage... caused a third of all morphosyntactic errors."
  - [corpus]: Neighbor paper "Automatic Speech Recognition of African American English" confirms morphosyntactic feature mismatch (CCR/ING-reduction) as a systematic error source in other dialects.
- Break condition: If the n-gram or neural LM is re-weighted or trained on text corpora containing Geordie syntax.

### Mechanism 3
- Claim: Sociolinguistic "covert prestige" and age-grading indirectly increase error rates by modulating the intensity of non-standard feature usage.
- Mechanism: Social factors (gender, age) do not cause errors directly. Instead, they correlate with higher usage rates of difficult features (e.g., men use more local lexicon; young/old use more phonological variants). The ASR fails because the *feature density* is higher in these demographics.
- Core assumption: WER disparities across demographics are epiphenomena of the underlying linguistic variation, not inherent acoustic differences based on gender or age physiology.
- Evidence anchors:
  - [section 4]: "The higher proportion of lexical errors for men likely reflects their greater use of non-standard forms... The variation in error proportions... can be explained by age grading."
  - [corpus]: N/A (No specific sociolinguistic prestige mechanisms found in corpus neighbors).
- Break condition: If a speaker from a high-error demographic (e.g., older male) adopts a standard "clinical" speaking style.

## Foundational Learning

- Concept: **Word Error Rate (WER) Decomposition**
  - Why needed here: The paper argues that high WER is not enough; you must decompose errors into Substitution, Deletion, and Insertion to distinguish between "hearing the wrong word" (acoustic failure) and "correcting the grammar" (LM bias).
  - Quick check question: If an ASR outputs "you" when the speaker says "yous", is this a failure of the acoustic model or the language model? (Answer: Likely LM bias/Standardization, as phonetics are distinct).

- Concept: **Allophonic Variation**
  - Why needed here: Understanding that a phoneme (e.g., /t/) has different physical realizations (allophones) in different contexts (e.g., [t] vs. glottal stop [Ê”]). The system must map both to the same abstract symbol or text output.
  - Quick check question: Why would replacing /t/ with a glottal stop cause an ASR to output "skiing" instead of "skipping"? (Answer: The acoustic signature of the stop gap is missing or changed, breaking the phoneme alignment).

- Concept: **Age Grading vs. Change in Progress**
  - Why needed here: To interpret why certain age groups spike in error rates. It helps distinguish between a speaker permanently changing their dialect (Change) vs. using more vernacular at specific life stages (Age Grading), which affects how we target training data.
  - Quick check question: If WER drops for 40-year-olds but rises for 60-year-olds, is the model failing more on older voices, or are 60-year-olds just using more complex dialect features?

## Architecture Onboarding

- Component map: Audio Input (DECTE Corpus) -> Feature Extraction (Spectrogram) -> Acoustic Model (Rev AI/CrisperWhisper) -> Pronunciation Dictionary (Implicit/Neural) -> Language Model -> Decoder -> Text Output.
- Critical path: The **Acoustic Model -> Decoder** interface is the primary friction point. The paper identifies that features like "FOOT/STRUT split absence" or "monophthongisation" create acoustic embeddings that the decoder cannot match to the expected phoneme sequences for the target words.
- Design tradeoffs:
  - **Dialect-specific vs. General Models:** Fine-tuning on Newcastle English (DECTE) reduces WER for locals but may degrade performance on Standard English (catastrophic forgetting).
  - **Manual vs. Automated Error Analysis:** The paper used manual sociolinguistic coding (high validity, low scale) vs. automated script for pronouns (low validity, high scale).
- Failure signatures:
  - **Standardization:** Outputting "our" when audio is clearly "wor" (LM override).
  - **Homophone Confusion:** "Whale" -> "Wheel" caused by monophthongisation collapsing distinct vowel spaces into a shared acoustic region.
  - **Lexical Gaps:** Proper nouns like "Geordie" or "Sunderland" frequently misrecognized (OOV or low-frequency in training).
- First 3 experiments:
  1. **Pronoun Substitution Test:** Synthesize or extract clips of "yous" and "wor" and run them through the system with the Language Model disabled (or generic) to isolate if the error is acoustic or probabilistic.
  2. **Dialectal Vowel Confusion Matrix:** Map the frequency of specific vowel substitutions (e.g., FACE /e:/ being heard as FLEECE /i:/) to quantify the "acoustic distance" the model must bridge.
  3. **Age-Stratified Noise Injection:** Add varying levels of noise (SNR) to different age groups to verify if the "age bias" persists once recording quality is normalized, confirming the paper's claim that errors are linguistic, not just noisy data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific phonological features (e.g., glottalisation, monophthongisation) systematically impact ASR error rates when isolated from other linguistic variables?
- Basis in paper: [explicit] The Discussion states that "These errors [phonological] should be studied systematically in future research."
- Why unresolved: The current study provides a proportional overview of errors but does not isolate the quantitative contribution of individual features to the Word Error Rate.
- What evidence would resolve it: Controlled experiments manipulating specific phonological variables (e.g., FACE monophthongisation) to measure their independent effect on recognition accuracy.

### Open Question 2
- Question: Can automated annotation tools effectively replace manual sociolinguistic analysis for tagging dialectal features in ASR evaluation?
- Basis in paper: [explicit] The authors state "Methodologically, automating the annotation of dialectal features is a crucial step in examining these errors at scale."
- Why unresolved: Manual analysis is resource-intensive; the paper notes that without automated tagging, valid dialect patterns are often misclassified as errors during evaluation.
- What evidence would resolve it: Development of a computational pipeline that tags Newcastle English features with accuracy comparable to trained sociophoneticians.

### Open Question 3
- Question: Does fine-tuning ASR models on dialect-specific data effectively mitigate the misrecognition of local morphosyntactic features like "yous" and "wor"?
- Basis in paper: [inferred] The Conclusion advocates for "dialectal diversity in ASR training data," but the study only evaluates an existing commercial system (Rev AI) without testing this intervention.
- Why unresolved: It remains unclear if simply adding diverse data is sufficient or if architectural changes are needed to handle non-standard morphosyntax.
- What evidence would resolve it: A pre-post comparison of ASR performance on the DECTE corpus after fine-tuning the model on Newcastle English speech samples.

## Limitations
- The study relies on a single commercial ASR system (Rev AI) without comparison to open-source alternatives like Whisper.
- Manual error annotation lacks detailed inter-rater reliability metrics, introducing potential subjectivity in error classification.
- The analysis focuses on Newcastle English, limiting generalizability to other regional dialects.

## Confidence
- **High Confidence:** The identification of specific phonological features (glottalisation, monophthongisation) as error sources is well-supported by error data and linguistic theory. The claim that demographic factors correlate with feature usage rather than causing errors directly is also strongly supported.
- **Medium Confidence:** The assertion that language model bias causes standardization errors is supported by error patterns but could be more definitively proven through ablation studies with disabled LMs. The age-grading interpretation of demographic error patterns is plausible but could have alternative explanations.
- **Low Confidence:** The specific WER percentages (31.95%) cannot be reproduced without access to the exact Rev AI model version used, making absolute performance claims time-sensitive.

## Next Checks
1. **Cross-System Validation:** Run the same DECTE corpus through multiple ASR systems (Rev AI, Whisper, Google Speech-to-Text) to verify if error patterns are consistent across architectures or system-specific.
2. **LM Ablation Test:** Process the same audio with the language model disabled or using a dialect-specific LM to isolate whether standardization errors persist when the prior probability is removed.
3. **Speaker Adaptation Experiment:** Select high-error speakers and fine-tune a model on their individual speech patterns to determine if errors are primarily due to acoustic mismatch or systematic feature underrepresentation in training data.