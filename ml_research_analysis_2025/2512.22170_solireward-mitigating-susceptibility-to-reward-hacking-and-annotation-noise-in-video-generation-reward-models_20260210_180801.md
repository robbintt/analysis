---
ver: rpa2
title: 'SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise
  in Video Generation Reward Models'
arxiv_id: '2512.22170'
source_url: https://arxiv.org/abs/2512.22170
tags:
- reward
- video
- samples
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SoliReward introduces a systematic framework for training video
  reward models to address annotation noise and reward hacking. It uses single-item
  binary annotations with a cross-prompt pairing strategy to reduce labeling ambiguity,
  and proposes a BT-WT loss to regularize positive sample distributions and mitigate
  reward hacking.
---

# SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models

## Quick Facts
- arXiv ID: 2512.22170
- Source URL: https://arxiv.org/abs/2512.22170
- Reference count: 40
- Primary result: Improves reward model accuracy (78.48→80.08 ID→OOD) and post-training video generation quality (VBench2 Fidelity 0.8999)

## Executive Summary
SoliReward addresses two key challenges in video reward model (RM) training: annotation noise and reward hacking. It introduces single-item binary annotations with cross-prompt pairing to reduce labeling ambiguity, and proposes a BT-WT loss to regularize positive sample distributions and mitigate reward hacking. The architecture employs a Hierarchical Progressive Query Attention adapter to fuse multi-level features from the VLM backbone. Evaluated on physical plausibility, subject deformity, and semantic alignment tasks, SoliReward improves both reward model accuracy and post-training video generation quality.

## Method Summary
SoliReward trains video reward models using 250k in-house videos with single-item binary annotations (Pass/Fail) across three dimensions. Preference pairs are constructed via cross-prompt pairing (350k win-lose + 150k win-tie). The InternVL3 backbone with HPQA adapter progressively refines learnable queries across multiple LM layers, combining with final-layer residual information. Training uses BT-WT loss with µ=1 for wins and 0.5 for ties. Post-training fine-tunes HunyuanVideo 14B with DanceGRPO using the trained RM for reward guidance.

## Key Results
- Improves reward model accuracy from 78.48→80.08 (ID→OOD)
- Achieves VBench2 Human Fidelity of 0.8999 compared to 0.8693 baseline
- Demonstrates superior generalization and robustness against reward hacking

## Why This Works (Mechanism)

### Mechanism 1: Single-item binary annotation with cross-prompt pairing
- Reduces labeling noise while maintaining preference signal quality
- Uses independent Pass/Fail judgments per video, then constructs preference pairs across different prompts
- Assumes binary judgments have higher inter-annotator consistency than pairwise comparisons
- Evidence: Single-item achieves α=0.4939, κ=0.4925, 77.33% agreement vs pairwise α=0.3516, κ=0.3494, 54.67%

### Mechanism 2: BT-WT loss for reward hacking mitigation
- Regularizes positive sample score distributions to prevent reward spikes on shortcut features
- Adds win-tie pairs (Pass-Pass) with µ=0.5 penalty to explicitly constrain intra-positive variance
- Assumes "Pass" samples can be treated as equivalent for tie purposes
- Evidence: BT-WT achieves VBench2 0.8999 vs BT 0.8693; smaller intra-group advantage variance

### Mechanism 3: Hierarchical Progressive Query Attention
- Captures multi-level semantic information more effectively than single-token extraction
- Progressively refines learnable query across multiple LM layers via MHA, preserving final-layer information
- Assumes transformer layers exhibit functional specialization with distributed reward-relevant information
- Evidence: HPQA achieves 78.48/80.08 (ID/OOD) vs Linear Head 74.69/78.66; avoids score clustering

## Foundational Learning

- **Bradley-Terry Model**
  - Why needed here: Foundation for preference-based RM training; understanding cross-prompt pairing requires grasping BT's relative utility framework
  - Quick check question: Given pairs (A≻B) and (C≻D) from different prompts, can BT learn a unified ranking? Why or why not?

- **Reward Hacking / Reward Gaming**
  - Why needed here: Central problem this paper addresses; understanding score distribution regularization requires recognizing policy networks exploit proxy reward imperfections
  - Quick check question: If an RM assigns higher scores to videos with saturated colors regardless of semantic correctness, what happens during RL fine-tuning?

- **Transformer Layer Specialization**
  - Why needed here: Justifies HPQA architecture; papers like Vig & Belinkov (2019) show mid-layers capture syntax, deep layers capture long-range dependencies
  - Quick check question: Why might using only the final token embedding discard useful information for reward prediction?

## Architecture Onboarding

- **Component map**: Video + prompt → InternVL3 → HPQA adapter → RewardHead → scalar r
- **Critical path**: 1) VLM processes video+prompt → hidden states at specified layers; 2) Learnable query q^(0) attends to H_{l₁} → q^(1); 3) Progressive refinement: q^(i-1) attends to H_{l_i} → q^(i) for i=2..N; 4) Separate q_res attends to H_L → o_res; 5) Combine: q_prog + o_res → RewardHead → scalar r; 6) Loss computation with BT-WT over paired samples
- **Design tradeoffs**: Layer selection affects computational cost vs. information capture; win-tie ratio balances regularization vs. discrimination; model scaling shows diminishing returns beyond 8B
- **Failure signatures**: Score clustering (discrete values instead of continuous separation); collapsed margin (<3.0 despite similar accuracy); OOD degradation (large ID-OOD accuracy gaps)
- **First 3 experiments**: 1) Ablate layer selection with single-layer extraction vs. full progressive aggregation; 2) Win-tie ratio sensitivity (0%→50%) measuring accuracy and VBench scores; 3) Architecture comparison on identical BT-WT data to isolate architectural contribution

## Open Questions the Paper Calls Out

- Can SoliReward framework be extended to conditional video generation tasks beyond T2V, such as I2V? (Conclusion states "Future work can be extended to other conditional generation tasks, such as image-to-video (I2V)")

- Does increasing training data volume alleviate diminishing returns observed when scaling reward model backbone from 8B to 14B parameters? (Section 7.2 observes performance saturation at 14B, hypothesizes data limitation)

- Can multiple evaluation dimensions be fused into a single unified reward model using multiple learnable queries? (Conclusion suggests "multiple dimensions can be fused into a single RM by designing several learnable queries")

## Limitations

- Data provenance uncertainty: No details about annotator expertise, annotation interface, or inter-annotator reliability beyond reported scores
- Architectural specification gaps: HPQA layer indices and RewardHead MLP details are unspecified, making exact reproduction impossible
- Post-training generalization claims: Improvements demonstrated on HunyuanVideo 14B specifically; may not transfer to other models or scales

## Confidence

- **High confidence**: BT-WT loss formulation and theoretical motivation are sound and well-supported
- **Medium confidence**: Single-item binary annotation improves efficiency and agreement, but impact on final quality depends on pairing strategy
- **Medium confidence**: HPQA architecture shows measurable improvements, but layer selection criteria are unspecified and claims rely on indirect evidence

## Next Checks

1. **Layer selection ablation**: Train HPQA with progressive aggregation across different layer index sets to empirically determine which layers contribute most to reward prediction

2. **Win-tie ratio sensitivity**: Systematically vary win-tie pair ratio (0%→50%) while holding all else constant to find optimal regularization-utility tradeoff

3. **Architecture-confounded evaluation**: Train Linear Head, Special Token Head, and HPQA on identical BT-WT data to isolate architectural contribution from data quality confounds