---
ver: rpa2
title: Unsupervised Feature Transformation via In-context Generation, Generator-critic
  LLM Agents, and Duet-play Teaming
arxiv_id: '2504.21304'
source_url: https://arxiv.org/abs/2504.21304
tags:
- feature
- agent
- features
- critic
- transformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unsupervised feature transformation,
  particularly in domains like material performance screening where dimensionality
  is high and obtaining labeled data is expensive. The authors propose a duet-play
  generator-critic LLM agents framework that leverages in-context learning to derive
  pseudo-supervision from unsupervised data.
---

# Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming

## Quick Facts
- arXiv ID: 2504.21304
- Source URL: https://arxiv.org/abs/2504.21304
- Reference count: 9
- Primary result: LPFG framework achieves 0.906 accuracy on balance, 0.561 on cmc, and 0.792 on credit-g datasets, outperforming supervised baselines in unsupervised feature transformation

## Executive Summary
This paper addresses unsupervised feature transformation (UFT) in high-dimensional domains like material performance screening where labeled data is scarce and expensive. The authors propose a duet-play generator-critic LLM agents framework that leverages in-context learning to derive pseudo-supervision from unsupervised data. The framework consists of three interconnected steps: a critic agent diagnoses data to generate actionable advice, a generator agent produces tokenized feature transformations guided by the critic's advice, and iterative refinement ensures continuous improvement through feedback between agents. Extensive experiments on 12 public datasets demonstrate that the proposed framework outperforms even supervised baselines in feature transformation efficiency, robustness, and practical applicability.

## Method Summary
The proposed framework operates through a three-step iterative process where a critic LLM analyzes dataset metadata (feature names, statistics, task description) to generate semantic and distributional diagnosis advice, which serves as textual gradients or pseudo-supervision. A generator LLM then produces tokenized feature transformation sequences (e.g., "(f1/f2), log(f3)") conditioned on this advice and a predefined operator set. The parser converts these sequences into executable transformations, and the iteration controller manages the feedback loop between agents. The framework can be generalized to human-agent collaboration by replacing the critic agent with human experts, and experiments demonstrate superior performance on 12 datasets compared to baseline methods like TTG, AutoFeat, and GRFG.

## Key Results
- LPFG achieves 0.906 accuracy on balance dataset, 0.561 on cmc, and 0.792 on credit-g dataset
- Outperforms supervised baselines on 12 public datasets including balance, cmc, credit-g, diabetes, tic-tac-toe, pc1, airlines, jungle, health, pharyngitis, spaceship, and playground
- Ablation studies show textual gradients from critic guidance outperform accuracy-guided and importance-guided alternatives on 4/4 tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critic agent diagnoses generate "textual gradients" that substitute for supervised feedback in unsupervised feature transformation
- Mechanism: The critic LLM performs semantic analysis (feature-task relationships) and distributional analysis (data patterns, skewness) to produce actionable advice serving as pseudo-supervision
- Core assumption: LLMs encode sufficient general knowledge about feature engineering principles to diagnose data quality issues and suggest meaningful transformations without labeled examples
- Evidence anchors: Abstract mentions critic generates "actionable advice... to derive pseudo-supervision from unsupervised data"; section 3.2 describes generating "issue diagnosis and improvement advice of feature space as textual gradients"; corpus lacks direct evidence on textual gradients as supervision substitutes
- Break condition: Critic advice degrades to generic or irrelevant suggestions when datasets contain highly specialized domain features with unclear semantic relationships

### Mechanism 2
- Claim: Tokenizing feature transformations as sequences enables LLMs to generate features directly without searching exponential combination spaces
- Mechanism: Features, operators, and transformations are represented as token sequences, making feature transformation a next-token generation task that bypasses discrete/continuous search methods
- Core assumption: The space of useful feature transformations can be captured by LLM token generation conditioned on dataset context and critic advice, rather than requiring explicit exploration
- Evidence anchors: Abstract mentions "generator agent produces tokenized feature transformations guided by the critic's advice"; section 3.3 proposes representing "mechanism-unknown feature space knowledge into symbolic sequential tokens"; corpus lacks evidence on tokenization efficacy for feature generation
- Break condition: Generated token sequences produce syntactically invalid or semantically meaningless transformations when operator sets or feature naming conventions are ambiguous

### Mechanism 3
- Claim: Iterative generator-critic duet-play creates a pseudo-optimization loop that converges on improved feature spaces without labeled supervision
- Mechanism: Critic advice augments the generator's in-context prompt in each iteration, with the generator producing refined features that are re-diagnosed by the critic, emulating gradient descent where textual gradients steer generation toward better representations
- Core assumption: Iterative textual feedback provides sufficient optimization signal for convergence, and LLM agents can maintain coherent multi-turn reasoning across iterations
- Evidence anchors: Abstract mentions "iterative refinement ensures continuous improvement through feedback between agents"; section 3.4 describes dynamic adaptation "by integrating task-specific diagnosis and advices"; Figure 4b shows LPFG outperforms alternatives on 4/4 datasets; corpus lacks verification of convergence properties
- Break condition: Iterations produce oscillating or divergent feature sets when critic advice contradicts prior suggestions or when generator fails to incorporate feedback consistently

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: The framework relies on LLMs learning from prompts containing dataset descriptions, statistics, and critic advice without weight updates
  - Quick check question: Can you explain why ICL enables rapid adaptation to new datasets but may fail on out-of-distribution feature types?

- Concept: Generator-Critic Architectures
  - Why needed here: LPFG separates reasoning (critic) from generation (generator) to avoid coupling uncertainty
  - Quick check question: What are the trade-offs of splitting reasoning and generation across two LLM calls versus a single unified agent?

- Concept: Pseudo-Labeling and Self-Supervision
  - Why needed here: "Pseudo-supervision" derives from unsupervised data via critic diagnosis
  - Quick check question: Under what conditions might pseudo-supervision from LLM diagnosis introduce systematic bias into downstream models?

## Architecture Onboarding

- Component map:
  - Critic Agent: LLM receives dataset metadata → outputs semantic + distributional advice as text
  - Generator Agent: LLM receives critic advice + operator set + dataset metadata → outputs tokenized feature sequences
  - Parser: Converts token sequences into executable transformation code
  - Iteration Controller: Manages feedback loop, aggregates critic advice into generator prompts across iterations
  - Human Extension: Critic agent replaceable by human expert for conversational mode

- Critical path:
  1. Dataset metadata extraction (feature names, types, distribution statistics)
  2. Critic prompt construction → semantic + distributional diagnosis
  3. Generator prompt construction (critic advice + operator set + examples)
  4. Parse generated sequences → apply transformations
  5. (Optional) Iterate: feed transformed features back to critic

- Design tradeoffs:
  - GPT-3.5-turbo vs. GPT-4o shows marginal performance difference; cheaper models may suffice for critic tasks but increase risk of low-quality advice
  - Iteration count unspecified; more iterations increase cost and risk of overfitting to critic's biases
  - Larger operator sets increase transformation diversity but may generate unstable or redundant features
  - Human experts improve customization but introduce latency and require domain expertise

- Failure signatures:
  - Generic advice: Critic outputs vague suggestions without actionable specificity → generator produces irrelevant features
  - Parsing failures: Generator outputs malformed sequences → transformation pipeline crashes
  - Performance plateau: Iterations yield no improvement after 2-3 rounds → critic advice exhausted or circular
  - Domain mismatch: Feature names are opaque codes → semantic diagnosis fails; requires enriched metadata

- First 3 experiments:
  1. Reproduce Table 1 on 3 datasets (balance, cmc, credit-g) with GPT-3.5-turbo; log per-iteration accuracy and token costs to validate reported gains
  2. Ablate critic guidance: Run LPFG-o (no critic), LPFG-a (accuracy-guided), LPFG-i (importance-guided) on diabetes dataset; confirm Figure 4b pattern
  3. Stress-test parsing: Feed generator outputs with intentionally ambiguous feature names to identify when semantic diagnosis degrades

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the tokenization and in-context prompting mechanism scale to extremely high-dimensional datasets (e.g., "millions of candidate features") given the fixed context window limitations of current LLMs?
- Basis in paper: Introduction identifies "millions of candidate material ingredients" as key motivation, but experimental validation is restricted to 12 standard public datasets with low dimensionality
- Why unresolved: LLM context windows cannot handle "millions" of tokens, and the paper does not propose a chunking, sampling, or hierarchical strategy to bridge this gap
- What evidence would resolve it: Empirical results on datasets with >10,000 features or theoretical analysis showing how the critic agent selects relevant subsets

### Open Question 2
- Question: How robust is the framework when the critic agent provides semantically plausible but statistically incorrect advice (hallucinations), and does the iterative loop correct this?
- Basis in paper: Method relies on critic using "general knowledge" to provide "textual gradients" but assumes LLM's semantic diagnosis is beneficial without mechanism to verify statistical validity
- Why unresolved: LLMs are known to hallucinate; false semantic premises could degrade feature space, but paper does not analyze failure cases induced by bad advice
- What evidence would resolve it: Ablation study injecting noise or incorrect advice into the critic loop, or error analysis on datasets with obscure feature names

### Open Question 3
- Question: Does the "pseudo optimization" process through iterative duet-play guarantee convergence, or does it risk oscillating between sub-optimal feature states?
- Basis in paper: Authors draw strong analogy between critic's advice and "gradient descent," terming it "pseudo optimization" but provide no theoretical guarantee or empirical bound on iterations required
- Why unresolved: Unlike mathematical gradient descent, textual feedback is discrete and non-differentiable; without defined stopping criterion or convergence metric, agents could theoretically loop indefinitely
- What evidence would resolve it: Convergence plot showing downstream model performance vs. number of duet-play iterations across datasets, specifically looking for plateaus or performance drops

## Limitations
- Reliance on LLM-based pseudo-supervision introduces uncertainty about generalization to highly specialized domains with ambiguous feature semantics
- Absence of ablation studies for iteration count, operator set size, and transformation depth limits understanding of optimal configuration
- Parsing mechanism for converting tokenized sequences into executable transformations is not detailed, creating potential reproducibility barriers

## Confidence
- **High confidence**: The core architecture (generator-critic duet-play) and its separation of reasoning from generation is well-defined and theoretically sound
- **Medium confidence**: The in-context learning approach for feature transformation, given strong empirical results but limited theoretical grounding for convergence
- **Low confidence**: The scalability and robustness claims, particularly regarding iteration depth, operator set generalization, and performance on highly specialized or high-dimensional domains

## Next Checks
1. **Iteration Convergence Study**: Run LPFG on diabetes dataset with 1-10 iterations, plotting accuracy and feature diversity per iteration to identify optimal stopping point and detect potential overfitting or cycling behavior

2. **Operator Set Ablation**: Test LPFG with restricted operator sets (arithmetic-only, non-linear-only, and full set) on credit-g dataset to quantify contribution of each transformation category and assess sensitivity to operator selection

3. **Domain Transfer Stress Test**: Apply LPFG to a domain with opaque feature naming (e.g., genomics or abstract material science descriptors) and measure degradation in critic advice quality and downstream accuracy compared to natural language feature names