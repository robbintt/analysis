---
ver: rpa2
title: Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers
arxiv_id: '2510.07761'
source_url: https://arxiv.org/abs/2510.07761
tags:
- reasoning
- choices-only
- answer
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how test-time reasoning (TTR) affects large
  language models (LLMs) on multiple-choice question answering (MCQA), comparing full
  input (question + choices) and choices-only settings. The authors run 12 reasoning-capable
  LLMs on three benchmarks (ARC, MMLU, Super GPQA) with and without TTR, using prompts
  that elicit step-by-step reasoning.
---

# Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers

## Quick Facts
- arXiv ID: 2510.07761
- Source URL: https://arxiv.org/abs/2510.07761
- Reference count: 40
- Primary result: Test-time reasoning improves MCQA accuracy in 27/36 full-input cases and 15/36 choices-only cases

## Executive Summary
This paper investigates how test-time reasoning (TTR) affects large language models (LLMs) on multiple-choice question answering (MCQA), comparing performance when given full inputs (question + choices) versus choices-only. The authors run 12 reasoning-capable LLMs on three benchmarks (ARC, MMLU, Super GPQA) with and without TTR, using prompts that elicit step-by-step reasoning. Results show TTR improves accuracy more consistently in full-input settings, while choices-only accuracy rises only slightly with longer reasoning traces. Qualitative analysis of 180 traces reveals LLMs use both shallow heuristics and legitimate test-taking strategies, including inferring missing questions—suggesting that partial-input success isn't necessarily a flaw but may reflect test-taking skill.

## Method Summary
The study uses zero-shot prompting via LiteLLM to run 12 reasoning-capable LLMs on 1000 MCQs each from ARC, MMLU, and Super GPQA test sets. Models are tested in four conditions: Full Base (no reasoning), Full Reason (with TTR), Choices-Only Base, and Choices-Only Reason. Base prompts select answers directly with reasoning effort "none," while Reason prompts require step-by-step reasoning with effort "medium" or explicit CoT between `<reasoning>` tags. Output is parsed for `<answer letter>` tags. For qualitative analysis, 180 traces are sampled and coded into six strategy categories, with logistic regression predicting success based on strategy usage.

## Key Results
- TTR improves accuracy in 27/36 full-input cases versus 15/36 choices-only cases
- Choices-only accuracy increases only slightly with longer reasoning traces, suggesting similar strategies across effort levels
- In choices-only success, models infer the original question 83% of times in ARC (77% in MMLU) but only 9% for failures
- Logistic regression shows SHALLOW and INCONS strategies predict failure at α = 0.05

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve above-random MCQA accuracy without questions by inferring plausible underlying questions from choice structure
- Mechanism: When presented with choices-only, reasoning models abductively generate candidate questions that would make choices coherent, then apply domain knowledge to select the most defensible answer
- Core assumption: Reasoning traces are "soft evidence"—informative but not proven faithful representations of actual model computation
- Evidence anchors:
  - [abstract]: "they more often employ strategies like inferring missing questions or identifying patterns in choices—skills the MCQ was meant to test"
  - [Section 3.4]: "In choices-only success, LLMs guess the original one 83% of times in ARC (77%, MMLU) but only 9% for failure"
  - [corpus]: Related work on cognitive foundations (arXiv:2511.16660) suggests LLM reasoning may differ fundamentally from human cognition—this mechanism may not reflect genuine understanding
- Break condition: If reasoning traces are post-hoc rationalizations (unfaithful), the inferred-question strategy may describe what the model claims to do rather than what it actually does

### Mechanism 2
- Claim: Reasoning traces reveal a mixture of shallow shortcuts and legitimate test-taking strategies, with success correlating negatively with shallow-cue usage
- Mechanism: Models deploy five strategies—FACT (recalling facts), ELIM (eliminating wrong options), PATTERNS (identifying choice properties), INFER Q (question inference), and SHALLOW (superficial heuristics). Logistic regression shows only SHALLOW and INCONS (inconsistent traces) predict failure at α = 0.05
- Core assumption: The qualitative coding of 180 traces generalizes to broader model behavior
- Evidence anchors:
  - [abstract]: "they sometimes use shallow shortcuts (e.g., picking the 'messiest' number), they more often employ strategies like inferring missing questions"
  - [Section 3.4, Table 1]: Full strategy definitions with examples
  - [corpus]: No direct corpus corroboration for this specific strategy taxonomy; related work on faithfulness (arXiv:2402.04614) warns that traces may be plausible but unfaithful
- Break condition: If prompt engineering or dataset artifacts bias which strategies appear, the taxonomy may not transfer to other MCQA settings

### Mechanism 3
- Claim: Scaling test-time reasoning (longer traces) yields diminishing returns for choices-only accuracy, suggesting models converge on similar strategies regardless of compute
- Mechanism: Increasing reasoning effort from low→medium→high increases token count substantially but improves choices-only accuracy only slightly (Figure 2, replicated across GPT-5 Mini, Gemini-2.5 Flash, Claude-4 Sonnet)
- Core assumption: API-level "reasoning effort" settings translate monotonically to actual reasoning depth
- Evidence anchors:
  - [Section 3.2]: "choices-only success is barely affected by the length of reasoning traces"
  - [Figure 2 and Appendix A.5]: Accuracy plateaus despite 2-10x increases in reasoning tokens
  - [corpus]: Related work (arXiv:2505.17813) finds shorter chains can be more effective, suggesting overthinking may not help
- Break condition: If models are not actually using additional compute for deeper reasoning (e.g., padding tokens), scaling behavior would reflect implementation artifacts rather than strategic convergence

## Foundational Learning

- Concept: **Partial-input baselines**
  - Why needed here: The core experiment compares full-input vs. choices-only accuracy; understanding this paradigm is essential to interpret why removing the question doesn't collapse performance
  - Quick check question: If a model scores 55% on choices-only MCQA where random is 25%, what does this suggest about its reliance on the question text?

- Concept: **Reasoning trace faithfulness**
  - Why needed here: The paper treats traces as "soft evidence" after passing three faithfulness checks; you must understand why faithfulness is unprovable and what checks are used
  - Quick check question: A model maintains its answer after adding reasoning and GPT-5 can predict its choice from the trace. Does this prove the trace caused the answer?

- Concept: **Multiple-choice item-writing guidelines**
  - Why needed here: The paper proposes using trace analysis to identify MCQ design flaws (e.g., non-homogeneous choices enabling "outlier" shortcuts)
  - Quick check question: If all distractors share a property the correct answer lacks, what shortcut does this enable?

## Architecture Onboarding

- Component map: Input layer (full vs. choices-only prompts) -> Reasoning layer (models with TTR enabled) -> Output layer (structured answer parsing) -> Analysis layer (qualitative coding and regression)

- Critical path:
  1. Run each model on 1000 MCQs per benchmark (ARC, MMLU, Super GPQA) in both input settings
  2. Apply Base (no reasoning) and Reason (with TTR) prompts
  3. Extract accuracy deltas between settings
  4. For trace analysis: sample traces, code strategies, fit logistic regression

- Design tradeoffs:
  - **API reasoning effort vs. CoT prompting**: API settings may not fully disable reasoning (see DeepSeek-R1/Qwen-3 issues in Appendix A.3)
  - **Zero-shot vs. few-shot**: Zero-shot chosen to avoid contamination; may underestimate model capability
  - **Qualitative coding scale**: 180 traces provide signal but limited statistical power for fine-grained analysis

- Failure signatures:
  - **Small accuracy gaps between full and choices-only**: May indicate full accuracy relies on choice artifacts rather than question comprehension
  - **SHALLOW strategy dominance**: Predicts failure in regression; high frequency signals problematic benchmark design
  - **INCONS (inconsistent) traces**: Trace contradicts answer; rare but indicates unfaithful generation

- First 3 experiments:
  1. **Baseline replication**: Run a reasoning LLM on ARC with both full and choices-only inputs, comparing Base vs. Reason prompts to verify the 15/36 improvement pattern
  2. **Faithfulness sanity check**: Duplicate the correct answer in choices (Appendix A.4, Trace A.1) and verify the model either changes its selection or surfaces the perturbation in its trace
  3. **Strategy coding pilot**: Sample 20 choices-only traces from a correct-run and 20 from an incorrect-run; manually code strategies to test whether SHALLOW correlates with failure before running full regression

## Open Questions the Paper Calls Out
None

## Limitations
- Reasoning traces are treated as "soft evidence" rather than proven faithful representations of model computation
- The 180-trace sample provides suggestive patterns but limited statistical power for fine-grained analysis
- API-level reasoning effort settings may not translate to actual reasoning depth—models could pad tokens without deeper computation

## Confidence

**High confidence**: The quantitative finding that TTR improves accuracy in 27/36 full-input cases and 15/36 choices-only cases is directly observable from the reported accuracy tables. The diminishing returns of longer reasoning traces on choices-only accuracy is also empirically robust across multiple models and benchmarks.

**Medium confidence**: The mechanism that LLMs "infer missing questions" to achieve choices-only success is supported by trace examples but remains inferential—the traces describe what models claim to do, not necessarily what they actually compute. The strategy taxonomy (FACT, ELIM, PATTERNS, INFER Q, SHALLOW, INCONS) is conceptually coherent and shows statistical relationships with success, but the 180-trace sample may not capture full behavioral diversity.

**Low confidence**: The proposal that trace analysis can systematically improve MCQ item quality assumes that current benchmarks reliably capture the strategies they claim to test. This requires assuming that reasoning traces faithfully represent model strategies, which the paper itself cannot prove.

## Next Checks

1. **Faithfulness stress test**: Systematically vary answer-choice ordering and duplicate correct answers across MCQs, then verify that model traces either change answers accordingly or explicitly acknowledge the perturbation. This would provide stronger evidence that traces causally influence rather than merely rationalize decisions.

2. **Cross-dataset strategy validation**: Apply the same 180-trace qualitative coding protocol to a non-MCQA reasoning task (e.g., grade-school math word problems) to test whether the five-strategy taxonomy generalizes or is specific to MCQA structure.

3. **Direct strategy intervention**: Design prompts that explicitly forbid each of the five strategies (e.g., "do not use shallow heuristics, do not eliminate choices, do not infer the question") and measure whether this degrades choices-only accuracy differently than full-input accuracy, testing whether the strategies are genuinely compensating for missing information.