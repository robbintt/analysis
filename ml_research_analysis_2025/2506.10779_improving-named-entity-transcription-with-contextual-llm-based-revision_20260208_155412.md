---
ver: rpa2
title: Improving Named Entity Transcription with Contextual LLM-based Revision
arxiv_id: '2506.10779'
source_url: https://arxiv.org/abs/2506.10779
tags:
- named
- entities
- context
- entity
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LLM-based method to improve ASR transcription
  of named entities by leveraging local context documents (e.g., lecture slides).
  The approach extracts named entities from both ASR predictions and context, uses
  phonetic and semantic matching to identify similar-sounding context entities, and
  employs an LLM to revise ASR predictions accordingly.
---

# Improving Named Entity Transcription with Contextual LLM-based Revision

## Quick Facts
- arXiv ID: 2506.10779
- Source URL: https://arxiv.org/abs/2506.10779
- Reference count: 0
- Up to 30% relative WER reduction for named entities on MIT OpenCourseWare dataset

## Executive Summary
This paper introduces an LLM-based method to improve ASR transcription of named entities by leveraging local context documents (e.g., lecture slides). The approach extracts named entities from both ASR predictions and context, uses phonetic and semantic matching to identify similar-sounding context entities, and employs an LLM to revise ASR predictions accordingly. The method is evaluated on a new 45-hour MIT OpenCourseWare dataset and achieves up to 30% relative WER reduction for named entities compared to baseline Whisper performance, while maintaining low non-entity WER. The approach works effectively across different LLM models (GPT-4o, Llama-3.1-70B) and ASR systems (Whisper small/medium/large, Canary-1B), demonstrating robust generalization.

## Method Summary
The method follows a pipeline architecture: first, ASR transcribes audio to text, then named entities are extracted from both ASR output and context documents using Flair NER. Phonetic matching via Double Metaphone algorithm identifies similar-sounding context entities to filter candidates. The LLM receives a structured prompt containing context entities, their sentences, ASR predictions, and entity probabilities, then revises only the named entities while preserving surrounding text. The system is evaluated on MIT OpenCourseWare data with lecture slides as context, targeting 8 entity types (PERSON, ORG, GPE, LOC, PRODUCT, EVENT, NORP, FAC).

## Key Results
- 30% relative WER reduction for named entities compared to Whisper baseline
- Works across ASR systems: Whisper small/medium/large and Canary-1B (7-10% absolute WER reduction)
- Effective with different LLMs: GPT-4o (22.7% WER) and Llama-3.1-70B (22.8% WER)
- Maintains low non-entity WER (7.0-8.2%) while improving named entity accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Phonetic pre-filtering of context entities prevents LLM hallucination and improves revision accuracy.
- **Mechanism:** Double Metaphone algorithm narrows candidate entities to those phonetically similar to ASR predictions. This constrained candidate set (~5-10 entities rather than full 46K-word context) keeps the LLM focused on plausible corrections rather than irrelevant substitutions.
- **Core assumption:** Similar-sounding ASR errors map to correct entities in the context documents.
- **Evidence anchors:**
  - [abstract] "uses phonetic and semantic matching to identify similar-sounding context entities"
  - [section 5.2] Full context with GPT-4o-mini increases WER from 32.3% to 43.4%; proposed method reduces to 22.7%
  - [corpus] PARCO paper similarly uses phoneme-level contrastive disambiguation for contextual ASR, supporting phonetic pre-filtering effectiveness
- **Break condition:** If ASR errors are phonetically distant from ground truth (e.g., completely different syllable structure), filtering will exclude correct entities.

### Mechanism 2
- **Claim:** LLM semantic reasoning resolves phonetic ambiguity that pure string matching cannot.
- **Mechanism:** The LLM receives context sentences containing candidate entities, not just entity lists. This enables contextual disambiguation—"Margaret Mead" vs "Margaret MIT" requires understanding that the sentence discusses anthropology students.
- **Core assumption:** LLMs can perform reliable semantic similarity judgments between ASR output and context sentences.
- **Evidence anchors:**
  - [section 5.3] Random replacement achieves 25.3% WER vs 22.7% with LLM; fails on "margaret mead" → "margaret mit" error
  - [section 3] "LLM analyzes the ASR predictions... as well as the named entities in the context and the sentences containing them"
  - [corpus] Neighbor papers on LLM-ASR integration (Customizing Speech Recognition with LLM Feedback) similarly leverage LLM semantic knowledge
- **Break condition:** If context sentences are noisy (OCR errors in slides) or lack semantic coherence with speech content.

### Mechanism 3
- **Claim:** Pipeline architecture with component decoupling enables cross-model generalization.
- **Mechanism:** ASR, NER, phonetic matching, and LLM revision operate as sequential independent modules. This allows swapping Whisper → Canary or GPT-4o → Llama without retraining.
- **Core assumption:** Named entity errors are consistent enough across ASR systems that the same revision logic applies.
- **Evidence anchors:**
  - [section 5.2/Table 2] Works across Whisper small/medium/large and Canary-1B; WER reductions of 7-10% absolute across all
  - [abstract] "demonstrating robust generalization" across LLM and ASR combinations
  - [corpus] Limited direct corpus validation—neighbor papers focus on integrated neural approaches rather than modular pipelines
- **Break condition:** If different ASR systems produce fundamentally different error distributions requiring model-specific revision strategies.

## Foundational Learning

- **Concept:** Named Entity Recognition (NER) with type constraints
  - **Why needed here:** Extracting entities from both ASR output and context documents; filtering by entity type (PERSON, ORG, etc.) reduces false matches
  - **Quick check question:** Can you explain why the paper uses Flair NER with OntoNotes types rather than a generic noun-phrase extractor?

- **Concept:** Phonetic algorithms (Double Metaphone)
  - **Why needed here:** Detecting similar-sounding words across ASR errors and context entities; robust to spelling variations
  - **Quick check question:** Why would edit distance be insufficient compared to phonetic encoding for this task?

- **Concept:** Prompt engineering for constrained output
  - **Why needed here:** Ensuring LLM only revises named entities without altering surrounding text; using delimiters (<< @ @ >>) for parsing
  - **Quick check question:** What happens if the prompt doesn't explicitly forbid modifying non-entity words?

## Architecture Onboarding

- **Component map:**
  Audio → ASR (Whisper/Canary) → Text → NER extraction → ASR entities
  Context docs → PDF parsing → Text → NER extraction → Context entities
  Phonetic matching (Double Metaphone) → Filtered candidates + context sentences → LLM (GPT-4o/Llama) → Revised transcript

- **Critical path:** Phonetic filtering is the bottleneck. The paper shows that without it (full context), smaller LLMs degrade performance. The matching quality directly limits LLM effectiveness.

- **Design tradeoffs:**
  - Latency: 1.2s per utterance with entities (GPT-4o-mini API) vs local Llama-3.1-70B requiring A100 GPU
  - NER recall vs precision: Flair recall (86%) means some entities are never considered for correction
  - Context quality: Slides with OCR errors reduce matching accuracy; 60-66% of ground-truth entities appear in slides

- **Failure signatures:**
  - WER increase with full context → LLM hallucinating irrelevant corrections (Table 1: 32.3% → 43.4%)
  - Wrong entity substitution despite phonetic match → Random replacement baseline (25.3% vs 22.7%)
  - Open-source LLM degradation with short prompt → Llama-3.1-70B needs detailed instructions (Table 3: 24.8% → 32.3%)

- **First 3 experiments:**
  1. Replicate the full-context vs filtered-context comparison on a sample of 100 utterances to confirm phonetic filtering is the key mechanism
  2. Ablate the context sentence provision—test whether LLM receives only entity lists vs entity + sentences to validate semantic reasoning contribution
  3. Test error analysis: manually inspect 50 revised utterances to characterize failure modes (phonetic mismatch, NER misses, LLM reasoning errors)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the LLM-based revision framework be extended to improve transcription of rare words beyond named entities, and what detection mechanisms would be required?
  - **Basis in paper:** [explicit] "the method could be extended to revise rare words with the development of rare word detection models."
  - **Why unresolved:** Current implementation relies on standard NER tools (Flair) that detect only named entities; rare words lack standardized definitions and detection tools.
  - **What evidence would resolve it:** Experiments applying the phonetic-semantic matching pipeline to a rare word lexicon, comparing WER reductions for rare words versus named entities on the same dataset.

- **Open Question 2:** Does integrating visual information from context documents (via OCR or image captioning) further improve named entity revision accuracy?
  - **Basis in paper:** [explicit] "the method could be expanded to integrate visual information, such as image captioning or Optical Character Recognition (OCR), so as to better harness the available context documents."
  - **Why unresolved:** Current approach only extracts text from PDFs; lecture slides contain images with entity-relevant visual content (e.g., diagrams, photos with captions) that are ignored.
  - **What evidence would resolve it:** A comparison study where OCR/image captioning extracts additional entities from slide images, measuring incremental WER reduction over text-only context.

- **Open Question 3:** How does the method perform when named entities in speech are absent from the provided context documents?
  - **Basis in paper:** [inferred] "about 66% of these words appear in the slides"—meaning 34% of ground-truth entities lack context coverage, yet the paper does not analyze performance specifically on this subset.
  - **Why unresolved:** The phonetic-semantic matching step requires candidate entities from context; entities absent from context cannot be retrieved or revised.
  - **What evidence would resolve it:** A stratified analysis reporting WER separately for context-present vs. context-absent entities, potentially with ablations testing fallback strategies (e.g., external knowledge bases).

- **Open Question 4:** Does the approach generalize effectively to domains beyond educational lectures (e.g., medical, legal, technical)?
  - **Basis in paper:** [inferred] The evaluation uses only MIT OpenCourseWare data; the introduction notes that existing contextual ASR datasets "are often tailored to industry settings or specific needs, such as earnings calls."
  - **Why unresolved:** Different domains have distinct entity types, jargon density, and context formats; lecture slides may not represent the challenges of medical notes or legal briefs.
  - **What evidence would resolve it:** Cross-domain experiments applying the method unchanged to existing contextual ASR benchmarks (e.g., ConEC for earnings calls) or new domain-specific datasets, reporting WER and error analysis.

## Limitations
- Dataset accessibility and reproducibility - The paper introduces a new 45-hour NER-MIT-OpenCourseWare dataset but provides no access mechanism, blocking direct reproduction of claimed results.
- ASR confidence computation ambiguity - The method references "entity whose probability is below asr confidence threshold" but Whisper does not natively output per-entity confidence scores.
- Context quality dependency - The approach relies on lecture slides containing the correct named entities (60-66% coverage reported), with performance characterization of noisy context or missing entities not provided.

## Confidence
- **High confidence** - The core mechanism (phonetic filtering + LLM revision) is well-specified and produces consistent WER improvements across multiple ASR systems (Whisper variants, Canary) and LLM models (GPT-4o, Llama-3.1-70B).
- **Medium confidence** - The 30% relative NE-WER reduction is based on proprietary dataset and specific prompt engineering details that aren't fully disclosed.
- **Low confidence** - Generalization to domains outside educational content (where lecture slides provide context) remains unproven.

## Next Checks
1. **Phonetic filtering ablation** - Test the system with and without Double Metaphone filtering on a sample of 100 utterances. Confirm that full context causes WER degradation (as Table 1 shows: 32.3% → 43.4%) while filtered context maintains or improves performance.

2. **Semantic reasoning isolation** - Compare LLM revision performance when providing only entity lists versus entity lists with context sentences. This validates whether semantic reasoning (not just candidate restriction) drives the improvement over the random replacement baseline (25.3% vs 22.7% WER).

3. **Context coverage sensitivity** - Systematically vary the percentage of ground-truth entities present in context documents (0%, 30%, 60%, 90%) to characterize the method's robustness to incomplete context and identify the minimum coverage threshold for effective revision.