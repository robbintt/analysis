---
ver: rpa2
title: An Empirical Study on Chinese Character Decomposition in Multiword Expression-Aware
  Neural Machine Translation
arxiv_id: '2512.15556'
source_url: https://arxiv.org/abs/2512.15556
tags:
- character
- chinese
- translation
- word
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Chinese character decomposition as a means
  to improve multiword expression (MWE) translation in neural machine translation
  (NMT). The authors propose decomposing Chinese characters into radicals and strokes
  to enhance word meaning representation and integrate automatically extracted bilingual
  MWE pairs into training.
---

# An Empirical Study on Chinese Character Decomposition in Multiword Expression-Aware Neural Machine Translation

## Quick Facts
- arXiv ID: 2512.15556
- Source URL: https://arxiv.org/abs/2512.15556
- Reference count: 0
- Primary result: Character decomposition reduces parameters by 10% and improves MWE translation accuracy in Chinese-English NMT

## Executive Summary
This paper investigates Chinese character decomposition as a means to improve multiword expression (MWE) translation in neural machine translation (NMT). The authors propose decomposing Chinese characters into radicals and strokes to enhance word meaning representation and integrate automatically extracted bilingual MWE pairs into training. Experiments with RNN and Transformer-based NMT models on Chinese-to-English translation show that decomposition models (especially RXD3+BiMWE) outperform word-based baselines on MWE translation accuracy, despite similar BLEU scores. Character decomposition reduces model parameters by over 10% and better handles low-frequency words and MWEs. Expert validation confirms superior translation quality over automated metrics, highlighting the value of deeper character-level analysis for ideographic languages.

## Method Summary
The authors use CHISE IDS dictionary to recursively decompose Chinese characters into radicals and strokes at three levels (RXD1→RXD3). They train Transformer-based NMT models with decomposed character sequences as input, comparing against word-level baselines. Bilingual MWE pairs are automatically extracted using POS patterns and alignment scores (≥0.85) then added to training data. Models are evaluated on WMT-2017 Chinese-to-English test set using BLEU, hLEPOR, CharacTER, BEER, crowd-sourced Direct Assessment, and expert human evaluation on MWE accuracy.

## Key Results
- Character decomposition reduces model parameters by over 10% while improving MWE translation accuracy
- RXD3 (deepest decomposition level) outperforms RXD1 and RXD2, which produce semantic drift
- Expert validation shows RXD3+BiMWE correctly translates "他杀" (homicide) while baseline produces "found dead"
- BLEU scores show minimal differences between models, but MWE-specific evaluation reveals clear quality improvements

## Why This Works (Mechanism)

### Mechanism 1
Chinese character decomposition preserves semantic information while reducing vocabulary and handling rare words better than word-level representations. Characters are decomposed into radicals (semantic part) and strokes in hierarchical levels (RXD1 → RXD3). The radical carries compositional meaning (e.g., "木" indicates wood/material), allowing the model to infer meanings of unseen character combinations through shared sub-character components.

### Mechanism 2
Multi-level embeddings (word + character + radical) outperform single-granularity representations by capturing complementary information. The model concatenates embeddings from multiple granularities before attention, allowing the decoder to attend to both fine-grained semantic features (radicals) and coarse-grained context (words/characters).

### Mechanism 3
BiMWE augmentation with decomposition improves MWE translation accuracy even when BLEU scores appear similar. Automatically extracted bilingual MWE pairs (via POS patterns + alignment scores ≥0.85) are added to training data. When Chinese MWEs are decomposed, the model learns mappings between decomposed representations and target MWE translations.

## Foundational Learning

- **Chinese Character Structure (Radicals/Strokes)**: Why needed here: The decomposition approach relies on understanding that characters have semantic (radical) and phonetic parts. Without this, you cannot interpret why RXD1 separates specific components. Quick check: Given character "橋" (bridge), what does radical "木" indicate about its historical meaning?

- **Multiword Expression Non-compositionality**: Why needed here: MWEs like "kick the bucket" cannot be translated word-by-word. The paper targets idiomatic MWEs specifically. Quick check: Why would "by and large" (prep+conj+adj) fail under standard compositional translation?

- **Sub-word Modeling vs. Ideographic Decomposition**: Why needed here: BPE works for alphabetic languages via morpheme/stem extraction but doesn't transfer to Chinese. Decomposition is the analog. Quick check: What is the linguistic mapping proposed between English stems and Chinese radicals?

## Architecture Onboarding

- **Component map**: Chinese text → segment words → decompose characters to target level → separate embeddings per granularity → BiRNN/Transformer encoder → attention mechanism → decoder → output translation

- **Critical path**: 
  1. Preprocess Chinese text → segment words → decompose characters to target level
  2. Extract BiMWE pairs from parallel corpus → decompose Chinese MWEs → augment training
  3. Train Transformer (7+7 layers, batch 6250, 32k BPE on English side)
  4. Evaluate via BLEU + crowd-sourced DA + expert MWE validation

- **Design tradeoffs**:
  - RXD1 (radical+phonetic split) vs. RXD3 (near-stroke level): RXD3 gives better semantic preservation but longer sequences
  - Word boundaries preserved vs. removed: Removing boundaries (c+r) drops performance—boundaries are necessary
  - BiMWE augmentation: Helps low-resource/low-performance settings; diminishing returns in high-resource scenarios

- **Failure signatures**:
  - RXD2 degradation: Produces independent small characters with unrelated meanings (e.g., "吞"=swallow from "橋"=bridge), causing semantic drift
  - Baseline MWE errors: Translates "他杀" as "found dead" (missing homicide meaning) or "发言人" as gendered "spokeswoman"
  - BLEU/DA mismatch: BLEU shows BiMWE improvement; crowd-sourced DA shows no significant difference—requires expert validation

- **First 3 experiments**:
  1. Train word-level Transformer on 5M zh→en WMT data; measure BLEU on WMT17 test set
  2. Replace Chinese input with RXD1/RXD2/RXD3 decomposed sequences; compare BLEU, parameter count, and vocabulary size
  3. Extract BiMWE pairs with 0.85 threshold → augment training → compare RXD3 vs. RXD3+BiMWE on expert-annotated MWE subset (30 sentences with MWE error tagging)

## Open Questions the Paper Calls Out

### Open Question 1
Does character decomposition improve terminology translation accuracy in specialized domains like medical or legal texts? The authors state, "In the future, we intend to carry out separate experiments on how decomposition models can improve term translation with quantitative and qualitative analysis in selected domains, e.g. using medical or legal data." The current experiments utilized general news domain data (NIST, WMT), leaving the efficacy of decomposition for highly technical domain-specific terminology unverified.

### Open Question 2
How can the "semantic noise" introduced at intermediate decomposition levels (specifically Level 2) be mitigated? The authors note that Level 2 (RXD2) performed poorly because it generates "smaller-sized characters... with new meanings that are not directly related to the root character" (e.g., generating "swallow" from "bridge"), creating semantic interference. The paper applies a fixed-depth decomposition strategy without exploring methods to dynamically filter or mask unrelated sub-character components.

### Open Question 3
Is the proposed decomposition method effective in low-resource language scenarios? The authors observe that while BiMWE helps, the gains in their "high-resource" (5M sentences) setting were nuanced. They contrast this with previous work suggesting such methods are useful for "less-resourced language pairs," implying the need to verify performance in data-scarce environments. The study relied on large datasets (1.25M and 5M sentences) to achieve strong baselines; it did not simulate low-resource conditions.

## Limitations
- The CHISE IDS dictionary's decomposition quality and coverage limits semantic preservation accuracy, particularly for rare characters or domain-specific terminology
- BiMWE extraction quality depends heavily on POS pattern templates and alignment thresholds, which are not fully specified
- Expert validation covers only 30 sentences with MWE error tagging, which may not represent the full distribution of MWE types or difficulty levels

## Confidence
- **High**: Character decomposition reduces model parameters by over 10% and improves handling of low-frequency words. This is directly supported by experimental results across multiple metrics.
- **Medium**: Decomposition models outperform word-based baselines on MWE translation accuracy. While supported by expert validation, the BLEU/DA mismatch suggests automated metrics may not fully capture improvements.
- **Low**: BiMWE augmentation shows consistent benefits across all resource settings. The paper notes diminishing returns in high-resource scenarios but does not provide detailed analysis of when BiMWE integration becomes counterproductive.

## Next Checks
1. Run CHISE decomposition on a held-out set of 100 diverse Chinese characters and manually verify whether semantic meaning is preserved across RXD1→RXD3 levels, focusing on rare characters and domain-specific terms
2. Train RXD3+BiMWE models with BiMWE extraction thresholds ranging from 0.75 to 0.95 and measure impact on MWE translation accuracy and overall BLEU scores to identify optimal threshold ranges
3. Categorize MWE errors in baseline vs. RXD3+BiMWE outputs (idiomatic vs. compositional, verbal vs. nominal) to determine which MWE types benefit most from decomposition and BiMWE augmentation