---
ver: rpa2
title: Evaluating Hierarchical Clinical Document Classification Using Reasoning-Based
  LLMs
arxiv_id: '2507.03001'
source_url: https://arxiv.org/abs/2507.03001
tags:
- level
- gemini
- icd-10
- primary
- levels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated how well large language models (LLMs) can
  classify ICD-10 diagnostic codes from hospital discharge summaries, a task that
  is essential but error-prone in healthcare. Using 1,500 discharge summaries from
  the MIMIC-IV dataset and focusing on the 10 most frequent ICD-10 codes, the study
  tested 11 LLMs, including both reasoning and non-reasoning models.
---

# Evaluating Hierarchical Clinical Document Classification Using Reasoning-Based LLMs

## Quick Facts
- arXiv ID: 2507.03001
- Source URL: https://arxiv.org/abs/2507.03001
- Authors: Akram Mustafa; Usman Naseem; Mostafa Rahimi Azghadi
- Reference count: 40
- Primary result: Reasoning-based LLMs outperform non-reasoning models in ICD-10 classification, with Gemini 2.5 Pro achieving the highest F1 score (~57%), but no model reaches reliable automation levels.

## Executive Summary
This study evaluates the capability of large language models (LLMs) to classify ICD-10 diagnostic codes from hospital discharge summaries, a critical task in healthcare revenue cycle management. Using 1,500 discharge summaries from the MIMIC-IV dataset and focusing on the 10 most frequent ICD-10 codes, the study tested 11 LLMs, including both reasoning and non-reasoning models. Medical terms were extracted using cTAKES, a clinical NLP tool, and models were prompted in a structured, coder-like format. The results show that reasoning-based models generally outperform non-reasoning ones, with Gemini 2.5 Pro performing best overall. However, no model achieved an F1 score above 57%, with performance declining as code specificity increased. The findings suggest that while LLMs can assist human coders, they are not yet reliable enough for full automation.

## Method Summary
The study used MIMIC-IV discharge summaries (1,500 total, 150 per code) for 10 top ICD-10 codes. Medical terms were extracted using cTAKES and formatted with frequency and polarity. Eleven LLMs (reasoning and non-reasoning) were prompted via API with a structured "Clinical Coder" template to output primary and secondary diagnoses. Responses were parsed for ICD codes in ###.### format and evaluated using F1 score at three hierarchical levels (3-5) for both primary diagnosis and all diagnoses.

## Key Results
- Reasoning models outperformed non-reasoning models, with Gemini 2.5 Pro achieving the highest overall F1 score (~57%).
- Performance declined as code specificity increased, from Level 3 to Level 5.
- Certain codes, such as I25 (chronic ischemic heart disease), were classified more accurately than others, which often had near-zero F1 scores.
- Administrative codes like Y92, Z51, and Z87 consistently yielded near-zero F1 scores across all models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning-capable LLMs outperform standard models in hierarchical ICD-10 classification tasks, particularly at broader classification levels.
- Mechanism: Reasoning models employ structured, multi-step reasoning processes (chain-of-thought) that mimic clinical cognitive processes, enabling better semantic interpretation of complex clinical narratives and implicit associations among symptoms, test results, and procedures.
- Core assumption: Structured reasoning prompts help models decompose classification decisions in ways analogous to human coder workflows.
- Evidence anchors:
  - [abstract] "Reasoning-based models generally outperformed non-reasoning ones, with Gemini 2.5 Pro performing best overall."
  - [section] Figure 3 shows reasoning models achieved 26% F1 vs 22% for non-reasoning at Level 3 (all diagnoses).
  - [corpus] Neighbor paper "Toward Reliable Clinical Coding with Language Models" confirms off-the-shelf LLMs struggle with hierarchical code accuracy, suggesting verification mechanisms are needed beyond base model capability.
- Break condition: The advantage diminishes at Level 5 (most granular), where performance converges—reasoning alone cannot overcome semantic complexity of fine-grained codes.

### Mechanism 2
- Claim: Preprocessing discharge summaries through clinical NLP (cTAKES) before LLM input improves classification by focusing model attention on clinically relevant tokens.
- Mechanism: cTAKES extracts medically relevant tokens (medications, symptoms, diseases, lab results, procedures) with polarity marking (affirmed/negated), reducing noise and prompt length while preserving diagnostic signal. This aligns LLM attention with clinically meaningful content rather than narrative filler.
- Core assumption: Removing non-essential words and preserving term frequency/polarity does not discard information critical to diagnosis coding.
- Evidence anchors:
  - [section] "cTAKES applies NLP and machine learning techniques to extract medically relevant tokens... This approach offers multiple benefits... removes non-essential words, allowing LLMs to focus exclusively on medically relevant information."
  - [abstract] "Medical terms were extracted using cTAKES, a clinical NLP tool."
  - [corpus] Corpus evidence is limited—no direct neighbor comparison of cTAKES vs. raw text input exists; effectiveness is inferred from study design.
- Break condition: If critical context resides in clinical discourse markers (temporal relations, causality) rather than extracted entities, token-based representation may underperform full-text approaches.

### Mechanism 3
- Claim: Classification accuracy varies systematically across ICD-10 categories, with structural/semantic properties of certain codes making them inherently more tractable for LLMs.
- Mechanism: Codes like I25 (chronic ischemic heart disease) have well-defined clinical vocabularies and strong symptom-diagnosis mappings, while Y92 (place of occurrence), Z51 (medical care), and Z87 (personal history) are context-dependent administrative codes with ambiguous linguistic markers in clinical text.
- Core assumption: The linguistic regularity and clinical specificity of diagnosis descriptions correlate with LLM classification performance.
- Evidence anchors:
  - [section] "Certain codes, such as I25 (chronic ischemic heart disease), were classified more accurately than others, like Y92 or Z51, which consistently had near-zero F1 scores."
  - [section] Table 9 shows I25 achieved up to 56.5% F1 (GPT o3 Mini, Level 3 primary), while Tables 10-11 show Y92 and Z51 at 0% across most models.
  - [corpus] Neighbor paper "Practical Design and Benchmarking of Generative AI Applications for Surgical Billing and Coding" confirms foundational LLMs perform poorly on accurate ICD code generation, reinforcing task difficulty varies by code type.
- Break condition: Performance may improve for difficult codes with domain-specific fine-tuning or structured data integration (labs, vitals)—the limitation may be training data, not code semantics.

## Foundational Learning

- **ICD-10 Hierarchical Structure (5 levels)**
  - Why needed here: Classification performance is evaluated at Levels 3-5; understanding granularity is essential for interpreting F1 degradation patterns.
  - Quick check question: Given code I25.42, identify the Level 3, Level 4, and Level 5 components.

- **F1 Score for Multi-Label Classification**
  - Why needed here: The study uses F1 as the primary metric across imbalanced ICD-10 code distributions; understanding precision-recall tradeoffs explains why "all diagnoses" outperforms "primary diagnosis."
  - Quick check question: If a model predicts 10 codes for a summary with 3 true codes, and 2 predictions are correct, what are precision, recall, and F1?

- **Reasoning vs. Non-Reasoning LLM Architectures**
  - Why needed here: The core comparison distinguishes models with explicit chain-of-thought capabilities (DeepSeek Reasoner, Gemini 2.5 Pro) from direct-response models.
  - Quick check question: What inference pattern differentiates a "reasoning" LLM from a standard autoregressive model at generation time?

## Architecture Onboarding

- **Component map:**
  - MIMIC-IV discharge summaries → cTAKES preprocessing → tokenized medical concepts with frequency/polarity → 11 LLMs via API → structured "Clinical Coder" prompt → response parsing → ICD-10 code extraction → F1 calculation at Levels 3, 4, 5

- **Critical path:**
  1. cTAKES tokenization quality determines signal-to-noise ratio in prompts
  2. Prompt structure enforces output format consistency for automated parsing
  3. Hierarchical F1 evaluation requires correct code normalization (###.### format)

- **Design tradeoffs:**
  - Sample size (1,500) limited by API cost—statistical power vs. budget
  - Token-based input reduces prompt length but may lose discourse context
  - Single-prompt design vs. multi-turn reasoning: study uses single structured prompt, not iterative refinement

- **Failure signatures:**
  - Near-zero F1 for Y92, Z51, I13 in primary diagnosis task across all models
  - Sharp performance drop from Level 3 to Level 5 (e.g., Gemini 2.5 Pro: 39% → 24% F1, all diagnoses)
  - Inconsistent model rankings across codes (DeepSeek Chat leads on I21 Level 3 primary at 41%, but fails at Levels 4-5)

- **First 3 experiments:**
  1. **Ablate preprocessing:** Run identical prompts on raw discharge summaries vs. cTAKES-tokenized input; measure F1 delta to isolate preprocessing contribution.
  2. **Code-type stratification:** Group ICD codes by semantic category (disease vs. administrative vs. history); analyze whether reasoning advantage holds uniformly or is category-specific.
  3. **Hierarchical consistency check:** Measure whether Level 5 predictions are hierarchical descendants of Level 3 predictions; quantify "close but wrong" errors that exact-match F1 penalizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of structured clinical data (e.g., laboratory results, vital signs) with unstructured text improve LLM classification accuracy at high-specificity ICD-10 levels (Levels 4 and 5)?
- Basis in paper: [explicit] The Discussion notes that current models lack access to structured data which is "critical for accurate coding decisions," and explicitly recommends future research "explore multimodal models that integrate structured and unstructured data."
- Why unresolved: This study isolated unstructured text (processed via cTAKES) as the sole input, intentionally excluding the structured EHR data that human coders typically reference.
- What evidence would resolve it: A comparative benchmark evaluating the same LLMs on the MIMIC-IV dataset using both text and structured inputs versus text alone.

### Open Question 2
- Question: Can ensemble strategies combining the outputs of reasoning and non-reasoning models outperform the current best single models (e.g., Gemini 2.5 Pro) in hierarchical ICD-10 classification?
- Basis in paper: [explicit] The Conclusion suggests that to overcome current limitations, future work should "assess ensemble strategies that combine the strengths of multiple LLMs."
- Why unresolved: The paper evaluated 11 models independently; it did not test whether aggregating predictions (e.g., voting or weighting) could mitigate the specific weaknesses of individual models.
- What evidence would resolve it: Experiments combining predictions from top-performing reasoning models (DeepSeek Reasoner) and non-reasoning models to see if the aggregate F1 score exceeds the ~57% ceiling.

### Open Question 3
- Question: Does domain-specific fine-tuning on diverse clinical conditions resolve the near-zero F1 scores observed for administrative and context-specific codes (e.g., Z51, Y92)?
- Basis in paper: [inferred] The Results section highlights that codes like Z51 and Y92 consistently yield near-zero F1 scores. The Conclusion argues that performance inconsistency underscores the need for "domain-specific fine-tuning" and "expanding datasets with diverse clinical conditions."
- Why unresolved: The study utilized general-purpose or reasoning-enhanced models without specific training on the linguistic nuances of administrative coding guidelines, which differ from clinical pathology descriptions.
- What evidence would resolve it: Evaluation of a model fine-tuned specifically on the "Z" and "Y" code chapters of ICD-10 to see if it lifts performance above the zero-baseline.

### Open Question 4
- Question: What is the quantitative impact of semi-automated LLM assistance on the speed and accuracy of human clinical coders compared to manual coding?
- Basis in paper: [explicit] The Introduction asks if LLMs can "serve effectively as intelligent assistants," and the Conclusion states they are "not yet reliable enough for full automation" but are ready for "semi-automated clinical coding."
- Why unresolved: The methodology evaluated the LLMs in isolation against a gold standard, rather than measuring the synergistic performance of a human working with LLM suggestions.
- What evidence would resolve it: A user study measuring time-to-completion and error rates for human coders working with LLM suggestions versus coders working without AI assistance.

## Limitations

- Sample size (1,500 summaries) and focus on only 10 ICD-10 codes limit generalizability to the full code set.
- The benefit of cTAKES preprocessing is assumed but not validated through ablation studies against raw text input.
- Near-zero F1 scores for administrative codes suggest architectural limitations, but the study does not explore whether structured clinical data (labs, vitals) could improve performance.

## Confidence

- **High Confidence**: Reasoning models outperform non-reasoning models at broader classification levels (Level 3). This is directly supported by Figure 3 showing consistent F1 advantages (26% vs 22%) across models.
- **Medium Confidence**: cTAKES preprocessing improves classification by focusing attention on clinically relevant tokens. While the mechanism is well-explained, no comparative analysis against raw text input exists to confirm the benefit.
- **Medium Confidence**: Classification accuracy varies systematically by ICD-10 category due to structural/semantic properties. This is supported by clear performance differences (I25 at 56.5% F1 vs Y92/Z51 at 0%), but the underlying cause (code semantics vs. training data bias) remains unproven.

## Next Checks

1. **Preprocessing Ablation Study**: Run identical classification tasks on raw discharge summaries versus cTAKES-tokenized input across all 11 LLMs. Measure F1 differences to quantify preprocessing contribution and test whether clinical NLP adds value beyond model capability.

2. **Structured Data Integration**: Augment discharge summaries with structured clinical data (lab results, vital signs, medication dosages) for the lowest-performing codes (Y92, Z51, Z87). Evaluate whether this improves F1 scores, distinguishing between text-based limitations and data availability issues.

3. **Hierarchical Error Analysis**: For each model, calculate the proportion of Level 5 predictions that are direct descendants of Level 3 predictions versus unrelated codes. This will reveal whether models learn hierarchical relationships or make random fine-grained predictions, informing whether exact-match F1 is appropriate.