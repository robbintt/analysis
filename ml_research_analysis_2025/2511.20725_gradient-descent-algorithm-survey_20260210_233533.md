---
ver: rpa2
title: Gradient Descent Algorithm Survey
arxiv_id: '2511.20725'
source_url: https://arxiv.org/abs/2511.20725
tags:
- momentum
- learning
- gradient
- training
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey and experimental comparison
  of five gradient descent optimization algorithms: SGD, Mini-batch SGD, Momentum,
  Adam, and Lion. It analyzes their theoretical foundations, core principles, advantages,
  and limitations.'
---

# Gradient Descent Algorithm Survey

## Quick Facts
- arXiv ID: 2511.20725
- Source URL: https://arxiv.org/abs/2511.20725
- Reference count: 17
- Primary result: Comprehensive survey and experimental comparison of five gradient descent optimization algorithms (SGD, Mini-batch SGD, Momentum, Adam, Lion) across MNIST and California Housing datasets, with Lion and Momentum performing best overall.

## Executive Summary
This paper provides a comprehensive survey and experimental comparison of five gradient descent optimization algorithms. The study analyzes theoretical foundations, core principles, advantages, and limitations of SGD, Mini-batch SGD, Momentum, Adam, and Lion. Through extensive experiments on MNIST and California Housing datasets, the paper evaluates convergence speed, stability, and generalization performance. Results show Lion and Momentum as top performers, with Lion excelling on noisy data and Momentum on structured data. The paper concludes with practical recommendations for optimizer selection based on task characteristics, emphasizing the importance of hyperparameter tuning and dataset-specific optimization strategies.

## Method Summary
The paper conducts a systematic survey of five gradient descent optimization algorithms through theoretical analysis and empirical evaluation. The methodology involves implementing each optimizer with standardized hyperparameter configurations, testing on two benchmark datasets (MNIST classification and California Housing regression), and measuring convergence speed, stability, and generalization performance. Experiments compare learning curves, convergence rates, and final accuracy/RMSE metrics. The study also includes hyperparameter sensitivity analysis and provides practical guidance for optimizer selection based on dataset characteristics and computational constraints.

## Key Results
- Lion and Momentum optimizers achieve the best overall performance across datasets
- Lion excels on noisy data while Momentum performs better on structured data
- SGD provides high stability but slower convergence compared to adaptive methods
- Mini-batch SGD and Adam show sensitivity to hyperparameter tuning, particularly learning rate
- Hyperparameter tuning significantly impacts optimizer performance and generalization

## Why This Works (Mechanism)

### Mechanism 1: Momentum Accumulation and Oscillation Damping
If gradients consistently point in the same direction, velocity accumulation accelerates convergence; if gradients oscillate (noise/high curvature), velocity accumulation dampens updates, stabilizing the path. The algorithm maintains a velocity vector $v_t$ which is an exponential moving average of past gradients. In consistent directions, $v_t$ grows (acceleration). In noisy directions, positive and negative gradients cancel out (damping), smoothing the trajectory.

Core assumption: The "inertia" from historical gradients correctly distinguishes between signal (consistent descent direction) and noise (random fluctuations).

Evidence anchors:
- [abstract] "Momentum... accelerates convergence and damps oscillations by accumulating a velocity vector."
- [section II.C.3] "Momentum accumulates updates along consistent directions... while offsetting partial noise when the gradient direction changes rapidly."
- [corpus] "First and Second Order Approximations..." discusses momentum dynamics producing better results in specific cases.

Break condition: If the momentum coefficient $\beta$ is set too high relative to the learning rate, the system may overshoot narrow valleys or diverge due to excessive inertia.

### Mechanism 2: Adaptive Learning Rate Scaling (Adam)
Scaling the update step by the inverse root of the second moment (uncentered variance) allows rapid progress on sparse or inconsistent features while preventing divergence on frequently updated features. Adam maintains $m_t$ (mean) and $v_t$ (uncentered variance). The update $\eta \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)$ effectively normalizes the step size per parameter. Parameters with large past gradients get smaller effective steps; those with small/rare gradients get larger steps.

Core assumption: The historical magnitude of gradients is a reliable proxy for the optimal step size, and the loss landscape benefits from invariant step sizes across parameter dimensions.

Evidence anchors:
- [section II.D.2] "Coordinates with larger gradient magnitudes will have smaller step sizes... giving the algorithm a certain degree of invariance."
- [section IV] "Adam... performs poorly in this experiment, further indicating that its default hyperparameters are highly sensitive to different tasks."
- [corpus] "Frankenstein Optimizer" notes adaptive algorithms "struggle to find 'flat minima' reliably."

Break condition: If the assumption of stationarity in gradient variance is violated (e.g., non-stationary objectives or complex noise distributions), the adaptive scaling may generalize poorly compared to non-adaptive methods.

### Mechanism 3: Sign-based Gradient Filtering (Lion)
If gradient updates are determined solely by the sign of the momentum-weighted gradient rather than its magnitude, the optimizer effectively filters noise and maintains uniform update magnitudes, improving stability on noisy data. Lion computes $u_t = \beta_2 m_{t-1} + (1-\beta_2)g_t$ and applies $\text{sign}(u_t)$. This discards magnitude information, acting as a filter against "short-term noise" while preserving the long-term direction.

Core assumption: The direction of the gradient is more critical than the magnitude for the specific task, and a uniform step size across parameters is sufficient for convergence.

Evidence anchors:
- [abstract] "Lion excelling on noisy data."
- [section II.E.3] "The sign operation reduces sensitivity to gradient magnitude, which can act as a filtering effect under high-noise mini-batch gradients."
- [corpus] Weak/missing direct corpus evidence for Lion's specific sign mechanism in the provided neighbors; evidence relies primarily on the paper text.

Break condition: If precise gradient magnitude information is required to navigate complex curvature (e.g., ravines with varying steepness), the sign-only update may result in suboptimal convergence speeds or instability.

## Foundational Learning

Concept: **Stochastic vs. Batch Gradient Estimation**
Why needed here: To understand the trade-off between computational cost, gradient variance (noise), and convergence stability inherent in SGD vs. Mini-batch SGD.
Quick check question: Does increasing the batch size in SGD typically increase or decrease the variance of the gradient estimate?

Concept: **Bias-Variance Trade-off in Optimization**
Why needed here: To grasp why Adam (low variance via adaptation) might generalize worse than SGD (high variance/noise as regularization) or Lion (noise filtering).
Quick check question: Why might high gradient variance (noise) actually help a model generalize better, according to the paper?

Concept: **Hyperparameter Decay Schedules**
Why needed here: To recognize that convergence proofs often rely on specific learning rate decay conditions ($\sum \eta_t^2 < \infty$) and that practical optimizers require tuning strategies (warmup, cosine decay).
Quick check question: What are the standard Robbins-Monro conditions required for SGD convergence?

## Architecture Onboarding

Component map: Data Loader -> Model -> Optimizer State -> Update Rule -> Weights

Critical path:
1. Define model architecture
2. Select optimizer based on data noise profile (Lion for noisy/complex, Momentum for structured) and memory constraints
3. Configure hyperparameters ($\eta$, $\beta$, batch size)
4. Implement training loop with validation monitoring for generalization gap

Design tradeoffs:
- **Lion vs. Adam:** Lion is more memory efficient (one state variable vs. two) but requires smaller learning rates and careful warmup; Adam is robust to default settings but potentially lower generalization
- **Speed vs. Stability:** Momentum/Lion offer speed on specific data types; SGD offers high stability/simplicity but slower convergence

Failure signatures:
- **Exploding Loss/Loss Spikes:** Learning rate too high (common in Adam/Lion without warmup)
- **Slow Convergence/Stalling:** Learning rate too low or momentum too low
- **Generalization Gap (Train $\ll$ Val):** Overfitting, potentially due to low batch noise or Adam's adaptation (consider switching to SGD/Momentum)

First 3 experiments:
1. **Baseline Stability Test (MNIST):** Train a simple MLP using SGD and Momentum to verify implementation correctness and reproduce the paper's finding that Momentum accelerates convergence on structured data
2. **Noise Robustness Test (California Housing):** Compare Lion vs. Adam on the regression task. Verify that Lion's RMSE curve is smoother and converges faster/better than Adam, validating the "sign update" mechanism on noisy inputs
3. **Hyperparameter Sensitivity Analysis:** Run a grid search on Adam's learning rate vs. Lion's learning rate on a subset of data. Confirm the paper's guidance that Lion requires a smaller learning rate than Adam

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical scope testing only two datasets (MNIST and California Housing) restricts generalizability to more complex tasks
- Lion optimizer evaluation relies heavily on paper's own theoretical claims with weak independent validation
- Paper does not thoroughly explore interaction between batch size and optimizer performance

## Confidence
- **High Confidence:** SGD and Momentum performance characteristics, basic theoretical foundations
- **Medium Confidence:** Adam hyperparameter sensitivity results, Mini-batch SGD convergence analysis
- **Low Confidence:** Lion optimizer superiority claims, generalization gap interpretation, noise filtering mechanism validation

## Next Checks
1. Replicate the Lion vs. Adam comparison on CIFAR-10/CIFAR-100 to verify noise robustness claims on image classification tasks
2. Conduct systematic ablation studies varying batch sizes (8, 32, 128) for each optimizer to quantify the batch size interaction effect
3. Implement and compare additional optimizers (RMSprop, Adagrad) to contextualize the relative performance of the five algorithms studied