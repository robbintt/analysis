---
ver: rpa2
title: Which Features are Best for Successor Features?
arxiv_id: '2502.10790'
source_url: https://arxiv.org/abs/2502.10790
tags:
- features
- reward
- policy
- random
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the problem of identifying the optimal
  base features for successor features (SFs) in reinforcement learning. The key question
  is: which features should be learned during training to maximize performance on
  downstream tasks at test time?'
---

# Which Features are Best for Successor Features?

## Quick Facts
- arXiv ID: 2502.10790
- Source URL: https://arxiv.org/abs/2502.10790
- Reference count: 7
- This paper investigates optimal base features for successor features in reinforcement learning

## Executive Summary
This paper addresses the fundamental question of which features should be learned during training to maximize performance on downstream tasks at test time. The author analyzes three generic models of downstream tasks (random Gaussian rewards, goal-reaching, and scattered sparse rewards) under large behavior cloning regularization. The key theoretical result shows that optimal features are given by the eigenfunctions of specific operators derived from the Laplacian of the reference policy, rather than Laplacian eigenfunctions themselves. This work provides a principled theoretical foundation for feature selection in successor features.

## Method Summary
The paper approaches the feature selection problem by maximizing expected regularized return under three task families. For each family, it derives an objective criterion based on maximizing the trace of the feature matrix multiplied by an advantage kernel operator. The analysis shows that under large KL-regularization with respect to a reference policy, the optimal features emerge from spectral properties of the MDP's Laplacian operator. The paper proves that the same optimal features work across all three task families, providing a unified framework for feature selection.

## Key Results
- Optimal features are the same for all three task families (Gaussian rewards, goal-reaching, scattered sparse rewards)
- The optimal features are eigenfunctions of Δ⁻¹ + (Δ⁻¹)* rather than Laplacian eigenfunctions
- The norm of Bellman gaps is not informative for selecting features, contrary to common practice
- Results hold under large behavior cloning regularization with respect to a reference policy

## Why This Works (Mechanism)

### Mechanism 1: Advantage Kernel Optimization
- Claim: Features maximizing expected downstream performance are those that maximize the advantage kernel trace Tr(φᵀA^π₀φ)
- Mechanism: The advantage kernel A^π₀ captures the norm of the advantage function for each reward. Under KL-regularized policy improvement (Theorem 2), the optimality gap equals the advantage function error. Therefore, features preserving high advantage-norm rewards retain the most actionable information.
- Core assumption: Large behavior cloning regularization temperature T (policies stay close to reference policy π₀), with optimality claims holding up to O(1/T²) error
- Evidence anchors:
  - [abstract] "we identify the optimal base features based on an objective criterion of downstream performance... without assuming the downstream tasks are linear in the features"
  - [Theorem 8 & Corollary 9, Section 5] Derivation of Tr(φᵀA^π₀φ) as the quantity to maximize for all three reward models
  - [corpus] Weak direct evidence—no corpus papers explicitly address advantage-kernel feature selection; nearest neighbor (Quasimetric Representations) uses contrastive successor features but not this criterion
- Break condition: If T is small (low regularization), the O(1/T²) approximation breaks down and the analysis may not hold

### Mechanism 2: Spectral Structure via Inverse Laplacian Symmetrization
- Claim: Optimal features are the largest eigenfunctions of Δ⁻¹ + (Δ⁻¹)* (for γ→1, deterministic environments), NOT the eigenfunctions of Δ + Δ*
- Mechanism: The advantage kernel quadratic form rᵀA^π₀r equals ⟨r, (Δ⁻¹ + (Δ⁻¹)* - Id)r⟩ for γ=1 (Theorem 10). Symmetrizing the inverse Laplacian captures long-range (low-frequency) reward variations. Critically, (Δ⁻¹ + (Δ⁻¹)*) ≠ (Δ + Δ*)⁻¹ except in reversible environments—a condition that fails in kinematic settings where velocity is part of state.
- Core assumption: Deterministic environment and discount factor γ close to 1; finite state-action spaces (continuous extension noted but not proven)
- Evidence anchors:
  - [Theorem 10, Section 5] "the optimal features are the largest eigenfunctions of Δ⁻¹ + (Δ⁻¹)*"
  - [Discussion after Theorem 10] Explicit comparison with Laplacian eigenfunctions and explanation of non-equivalence
  - [corpus] No corpus papers examine this specific operator; Laplacian eigenfunctions are used in related work (WTN18 cited) but not the inverse-symmetrized form
- Break condition: For γ→0, optimal features switch to smallest eigenfunctions of P*π₀Pπ₀ (high-frequency), per Proposition 11

### Mechanism 3: Task-Distribution Invariance
- Claim: The same optimal features emerge across three structurally different downstream task families: Gaussian rewards, goal-reaching (Dirac), and scattered sparse rewards
- Mechanism: All three models yield E[rrᵀ] = ℓ_ρ⁻¹ (up to constants), so the expected advantage kernel contribution reduces to the same trace maximization problem. This surprising robustness suggests the result captures fundamental MDP geometry rather than task-specific structure.
- Core assumption: Reward models are defined relative to stationary distribution ρ (not uniform over states), making results data-dependent
- Evidence anchors:
  - [abstract] "The features yielding optimal expected downstream performance turn out to be the same for these three task families"
  - [Proposition 14, Section 6] Derivation of E[rrᵀ] for each model
  - [corpus] No corpus evidence on task-distribution invariance for feature selection
- Break condition: If downstream tasks have reward structure strongly violating these models (e.g., adversarial or highly structured rewards), optimality is not guaranteed

## Foundational Learning

- Concept: **Successor Features (SFs)**
  - Why needed here: The entire paper addresses which base features φ maximize downstream SF performance. SFs compute ψ(s,a) = E[∑γᵗφ(st,at) | s,a,π₀], enabling zero-shot task adaptation via linear projection.
  - Quick check question: Given a new reward r, how does SF compute the task embedding z and estimated Q-function?

- Concept: **KL-Regularized Policy Optimization**
  - Why needed here: All results derive under large KL-regularization where policies remain near reference π₀. Theorem 2 links regularization directly to advantage-function error.
  - Quick check question: How does the regularized return G^π_r differ from standard return, and what does T→∞ imply?

- Concept: **Laplacian Operator in MDPs**
  - Why needed here: Δ = Id - γPπ₀ is the central operator; the paper analyzes its inverse and adjoint properties to derive optimal features.
  - Quick check question: Why is Δ non-invertible at γ=1, and how does the paper handle this (L²₀(ρ))?

## Architecture Onboarding

- Component map:
  1. Reference policy π₀ — exploration policy providing training distribution ρ; assumed ergodic
  2. Feature map φ: S×A → ℝᵈ — base features to be optimized (train-time)
  3. Successor feature map ψ — solves ψ = φ + γPπ₀ψ for each feature dimension
  4. Task encoder — computes z = C⁻¹E_ρ[r·φ] at test time
  5. Boltzmann policy head — outputs π = Bolt_π₀(ẑᵀψ) with temperature T

- Critical path:
  - Feature selection → ψ computation (offline) → task embedding z → policy deployment
  - The key innovation: replace Laplacian eigenfunctions with Δ⁻¹ + (Δ⁻¹)* eigenfunctions

- Design tradeoffs:
  - **Feature dimensionality d**: Higher d captures more eigendirections but increases sample complexity for z estimation
  - **Temperature T**: Larger T (stronger regularization) makes theoretical guarantees tighter but may limit expressiveness
  - **γ selection**: Near 1 optimizes for long-horizon; near 0 optimizes for bandit-like tasks

- Failure signatures:
  - Using Laplacian eigenfunctions (Δ + Δ*) instead of inverse-symmetrized form: suboptimal but may still work empirically
  - Small T violating O(1/T²) approximation: theoretical optimality no longer guaranteed
  - Stochastic environments: require the more general matrix form from Theorem 8, not the simplified spectral expression

- First 3 experiments:
  1. **Sanity check**: Implement feature extraction via top-d eigenvectors of Δ⁻¹ + (Δ⁻¹)* on a small deterministic gridworld; compare downstream performance vs. Laplacian eigenfunctions on held-out goal-reaching tasks
  2. **Ablation on γ**: Test feature quality as γ varies from 0.1 to 0.99; verify transition from P*π₀Pπ₀ eigenfunctions to Δ⁻¹ + (Δ⁻¹)*
  3. **Robustness to stochasticity**: Compare deterministic-environment features (Theorem 10) vs. general matrix features (Theorem 8) on a stochastic transition MDP; measure performance gap

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting that continuous-space extensions are possible but not proven.

## Limitations
- Results rely heavily on the assumption of large KL-regularization, which may not hold in practical settings
- Analysis is primarily developed for deterministic environments with γ→1
- The paper lacks empirical validation of the theoretical predictions
- The O(1/T²) approximation error means results may not hold for practical values of regularization temperature

## Confidence

- **High Confidence**: The derivation of advantage kernel maximization as the core criterion (Mechanism 1) and the connection between regularization and advantage function error are mathematically rigorous and well-supported.
- **Medium Confidence**: The spectral characterization of optimal features via Δ⁻¹ + (Δ⁻¹)* eigenfunctions (Mechanism 2) is theoretically sound but lacks empirical verification and assumes deterministic environments.
- **Medium Confidence**: The task-distribution invariance across three reward models (Mechanism 3) is a surprising theoretical result but hasn't been tested against alternative task families or adversarial reward structures.

## Next Checks
1. **Empirical verification**: Implement the proposed features on a benchmark RL environment (e.g., gridworld or MuJoCo) and compare downstream performance against Laplacian eigenfunctions across multiple downstream tasks.
2. **Temperature sensitivity analysis**: Systematically vary the KL-regularization temperature T and measure how the approximation error O(1/T²) manifests in actual performance degradation.
3. **Stochastic environment testing**: Validate whether the general matrix formulation from Theorem 8 provides measurable improvements over the simplified spectral form in environments with stochastic transitions.