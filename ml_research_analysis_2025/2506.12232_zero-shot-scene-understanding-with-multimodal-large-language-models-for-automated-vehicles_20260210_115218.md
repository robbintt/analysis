---
ver: rpa2
title: Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated
  Vehicles
arxiv_id: '2506.12232'
source_url: https://arxiv.org/abs/2506.12232
tags:
- scene
- performance
- attributes
- traffic
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multimodal large language models (MLLMs) for
  zero-shot traffic scene understanding from an ego vehicle's perspective using the
  Honda Scenes Dataset. Four MLLMs (GPT-4o, Pixtral 12B, Gemini 1.5 Flash-8B, and
  Llama-3.2-11B-Vision) are assessed individually and in ensemble configurations using
  a structured JSON-formatted prompt framework covering 21 traffic scene attributes.
---

# Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated Vehicles

## Quick Facts
- arXiv ID: 2506.12232
- Source URL: https://arxiv.org/abs/2506.12232
- Reference count: 40
- Primary result: GPT-4o demonstrates the strongest performance for zero-shot traffic scene understanding, while ensemble approaches show mixed results across 21 traffic attributes.

## Executive Summary
This study evaluates multimodal large language models (MLLMs) for zero-shot traffic scene understanding from an ego vehicle's perspective using the Honda Scenes Dataset. Four MLLMs (GPT-4o, Pixtral 12B, Gemini 1.5 Flash-8B, and Llama-3.2-11B-Vision) are assessed individually and in ensemble configurations using a structured JSON-formatted prompt framework covering 21 traffic scene attributes. GPT-4o demonstrates the strongest overall performance with highest F1-score, recall, and precision across attributes, while the performance gap between GPT-4o and smaller models remains modest. Ensemble approaches using majority voting yield mixed results—improving performance on some attributes like "Intersection (3-way)" and "Weather" while declining on others such as "Zebra Crossing."

## Method Summary
The study uses the Honda Scenes Dataset, extracting frames at 1 fps and filtering to 5,006 annotated frames. Four MLLMs are evaluated using a structured JSON-formatted prompt framework with 21 traffic scene attributes. Models are assessed individually and in ensemble configurations using majority voting. Temporal "Stage" outputs (Approaching/Entering/Passing) are converted to binary values for frame-based evaluation. Performance is measured using weighted average F1-score, precision, and recall.

## Key Results
- GPT-4o achieves the highest F1-score, recall, and precision across all traffic attributes
- Performance gap between GPT-4o and smaller models remains modest, suggesting potential for optimization
- Ensemble approaches improve some attributes (Intersection, Weather) but decline on others (Zebra Crossing, Merge Gore)
- Structured JSON prompting effectively standardizes model outputs for evaluation

## Why This Works (Mechanism)

### Mechanism 1: Structured Schema Enforcement via JSON Prompting
Converting unstructured visual driving data into a fixed JSON schema with 21 attributes constrains model hallucinations and standardizes outputs. The prompt forces MLLMs to map complex visual features to discrete numerical values, reducing the generation search space and aligning outputs with machine-readable downstream tasks.

### Mechanism 2: Zero-Shot Generalization from Pre-Training
Large-scale pre-training on general image-text pairs enables MLLMs to recognize domain-specific traffic concepts without exposure to the target dataset. The study exploits this by treating scene understanding as a Visual Question Answering task, assuming traffic concepts are generic enough to be zero-shot transferable.

### Mechanism 3: Majority Voting Ensemble (Conditional Utility)
Aggregating outputs from diverse architectures can smooth out random errors when model mistakes are uncorrelated. For attributes with strong visual features (Weather), consensus is high and performance improves. For subtle features (Zebra Crossing), disagreement is high and majority voting can filter out true positives.

## Foundational Learning

- **Zero-Shot In-Context Learning (ICL):** The methodology relies on models understanding tasks solely from prompts without weight updates. Without ICL understanding, one might incorrectly assume models were trained on Honda Scenes.
  - *Quick check:* If the model fails to detect "Fog," does it need more training data or a better prompt description?

- **Weighted F1-Score vs. Accuracy:** F1-score is emphasized because traffic attributes are imbalanced (e.g., "Tunnel" rarely appears). Accuracy would be misleading when some classes dominate.
  - *Quick check:* Why would a model that always predicts "Sunny" have high accuracy but low F1-score for "Rain"?

- **Ego-Vehicle Perspective:** Attributes (Approaching/Entering/Passing) are relative to the car's position. Models must understand spatial relations from first-person view, not bird's-eye view.
  - *Quick check:* How does "Approaching" definition change between dashcam versus surveillance camera perspectives?

## Architecture Onboarding

- **Component map:** Honda Scenes Dataset (Video → 1 FPS Frames) → MLLMs (GPT-4o, Gemini, Pixtral, Llama) → Structured JSON Prompt → Majority Voting Logic → Precision/Recall/F1 calculation
- **Critical path:** Prompt engineering (Appendix A) is most critical. If prompt definitions don't align with model's semantic representation, the zero-shot pipeline fails immediately.
- **Design tradeoffs:** Single model (GPT-4o) vs. ensemble—GPT-4o offers best performance, ensembling adds cost with mixed gains. Binary vs. staged output—simplifies evaluation but trades temporal nuance for stability.
- **Failure signatures:** Schema drift (text instead of JSON), ensemble regression (accuracy drops on rare classes), confusion (high performance on "Day" but low on "Dawn/Dusk").
- **First 3 experiments:**
  1. Baseline Validation: Run Appendix A prompt on 50 frames using GPT-4o to reproduce performance claims.
  2. Ablation on Schema: Remove explicit definitions from prompt to test reliance on provided definitions vs. model priors.
  3. Confidence-Weighted Ensemble: Replace majority voting with weighted votes based on model log-probability or historical F1-score to improve "Zebra Crossing" detection.

## Open Questions the Paper Calls Out

- Can confidence-based aggregation or weighted voting strategies outperform simple majority voting in ensemble MLLM configurations for traffic scene understanding? The mixed results from majority voting highlight the need for more advanced ensemble techniques.
- To what extent can retrieval-augmented generation (RAG) or fine-tuning close the performance gap between smaller models (e.g., Llama-3.2-11B-Vision) and state-of-the-art models like GPT-4o? The modest performance gap suggests advanced techniques could further optimize smaller models.
- How does MLLM accuracy in detecting dynamic traffic events change when processing continuous video streams compared to the static frame-based approach? The methodology acknowledges temporal stages were simplified to binary outcomes, discarding sequential context.

## Limitations
- Reliance on a single dataset (Honda Scenes) with potentially limited driving scenario diversity
- Ensemble approach shows inconsistent improvements, particularly struggling with nuanced attributes
- Conversion of temporal "Stage" attributes to binary values may oversimplify dynamic scene progression
- Modest performance gap between GPT-4o and smaller models suggests zero-shot approach may not fully exploit architectural differences

## Confidence
- **High Confidence:** GPT-4o's superior performance as best individual model; structured JSON prompting effectiveness; methodology's ability to produce comparable metrics
- **Medium Confidence:** Zero-shot generalization effectiveness; modest performance gap claims; mixed results of ensemble approaches
- **Low Confidence:** Specific impact of prompt engineering; generalizability to other datasets; optimal ensemble strategy for consistent gains

## Next Checks
1. Evaluate the same models on multiple autonomous driving datasets (nuScenes, Waymo Open) to assess generalizability beyond Honda Scenes
2. Systematically remove or modify prompt components to isolate which elements most significantly impact performance, particularly for challenging attributes like "Zebra Crossing"
3. Implement weighted voting based on individual model performance metrics or confidence scores to improve ensemble reliability, especially for attributes where simple majority voting underperforms