---
ver: rpa2
title: Do You Know About My Nation? Investigating Multilingual Language Models' Cultural
  Literacy Through Factual Knowledge
arxiv_id: '2511.00657'
source_url: https://arxiv.org/abs/2511.00657
tags:
- languages
- across
- language
- china
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XNationQA addresses the gap in multilingual benchmarks that are
  Western-centric and fail to evaluate cultural literacy across diverse regions. The
  authors created a large-scale multilingual QA dataset covering nine countries and
  seven languages, with 49,280 factual questions on wars, leaders, monuments, and
  national parks.
---

# Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge

## Quick Facts
- arXiv ID: 2511.00657
- Source URL: https://arxiv.org/abs/2511.00657
- Reference count: 40
- Models exhibit significant Western language bias in cultural knowledge recall

## Executive Summary
This paper introduces XNationQA, a large-scale multilingual benchmark designed to evaluate the cultural literacy of language models across diverse global regions. The dataset covers nine countries and seven languages with 49,280 factual questions about wars, leaders, monuments, and national parks. Through systematic evaluation of eight multilingual models, the authors demonstrate that open-source models show particularly poor knowledge transfer across languages, with performance gaps between Western and non-Western languages that do not correlate with knowledge of Western countries. The findings reveal that cultural knowledge and linguistic competence are decoupled in current multilingual models, highlighting the need for more culturally inclusive training and evaluation approaches.

## Method Summary
The authors constructed XNationQA by extracting 1,760 entities from English Wikipedia and Wikidata across four domains (wars, leaders, monuments, national parks) covering nine countries. They generated multiple-choice questions using four template variants per domain, with distractors carefully selected for each question type. The dataset was translated into Hindi, Spanish, Chinese, Japanese, Russian, and German using Google Translate and GPT-4, followed by validation through back-translation, semantic similarity checks, and human evaluation. Eight multilingual models (7B-14B scale plus GPT-4) were evaluated using accuracy metrics plus two novel transference metrics: Total Coverage (TC) measuring cross-lingual knowledge consistency and Smooth Coverage (SC) capturing partial knowledge patterns.

## Key Results
- Open-source models demonstrate particularly poor knowledge transfer across languages
- Models show greater knowledge of cultural information in English than in the dominant language of the respective culture
- Large gap exists between Western (EN, DE, ES, RU) and non-Western (HI, JA, ZH) language performance
- Models are generally more proficient at recalling locations for monuments and national parks than specific years for wars or leaders
- GPT-4 shows the strongest cultural literacy overall among evaluated models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training language distribution creates systematic Western language bias in cultural knowledge access
- Mechanism: Models project different languages into a common representation space, but the quality of this alignment directly depends on training corpus composition. When Western languages dominate pre-training data, factual knowledge becomes more reliably accessible via Western language prompts, even for non-Western cultural content.
- Core assumption: The representation space quality varies predictably with training data language proportions.
- Evidence anchors:
  - [abstract]: "open-source models demonstrate particularly poor knowledge transfer across languages"
  - [section 5.2]: "Mixtral, despite being primarily trained on English, German, and Spanish, performs substantially better in Russian than in other non-primary languages like Hindi, Japanese or Chinese"
  - [corpus]: "Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon" confirms asymmetric transfer patterns
- Break condition: Models trained on truly balanced multilingual corpora (like Aya with 101 languages) should show reduced Western language bias—but Aya underperforms, suggesting data volume alone doesn't solve the problem.

### Mechanism 2
- Claim: Cultural knowledge and linguistic competence are decoupled in multilingual LLMs
- Mechanism: Factual knowledge about a nation can be encoded primarily in high-resource language representations (especially English), creating a situation where querying in the nation's native language fails to reliably activate that knowledge.
- Core assumption: Knowledge storage is language-agnostic at the conceptual level but language-gated at the retrieval level.
- Evidence anchors:
  - [abstract]: "a model demonstrates greater knowledge of cultural information in English than in the dominant language of the respective culture"
  - [section 5.5]: "models often demonstrate deeper cultural knowledge of countries like China, India, and Japan than of Western nations like Germany or Spain... even while the models perform poorly in the native languages of those non-Western nations"
  - [corpus]: Weak corpus evidence for mechanism explanation; corpus focuses on bias identification rather than storage mechanisms
- Break condition: If knowledge-language decoupling is complete, translation-based retrieval should work; partial decoupling suggests gradient degradation.

### Mechanism 3
- Claim: Domain-specific factual knowledge transfers across languages at different rates
- Mechanism: Spatial/geographic facts (monument locations, national parks) achieve higher cross-lingual consistency than temporal facts (dates of wars, leader birth years), potentially because spatial entities have more consistent multilingual representations in training data.
- Core assumption: Different fact types have different cross-lingual representation stability.
- Evidence anchors:
  - [section 5.1]: "Models are generally more proficient at recalling locations for monuments and national parks than they are at recalling specific years for wars or leaders"
  - [section A.7]: "for war and leader domain questions, [Bloomz] shows modest to poor performance... worse than random in some instances"
  - [corpus]: No direct corpus evidence for domain-specific transfer rates
- Break condition: Temporal facts that are more culturally prominent (e.g., widely-documented wars) should transfer better than obscure ones.

## Foundational Learning

- Concept: **Cultural Literacy (Hirsch, 1983)**
  - Why needed here: This is the theoretical framework the paper uses to evaluate LLMs—understanding it clarifies why factual recall serves as a proxy for cultural understanding
  - Quick check question: Can you explain why the paper treats factual knowledge (dates, locations) as a valid proxy for "cultural literacy" rather than subjective cultural knowledge?

- Concept: **Cross-Lingual Representation Alignment**
  - Why needed here: The paper's findings about poor knowledge transfer rely on the assumption that multilingual models create shared representation spaces; understanding this clarifies why transfer fails
  - Quick check question: Why would a model that "knows" a fact in English fail to access it when queried in Hindi?

- Concept: **Coverage Metrics (TC and SC)**
  - Why needed here: The paper introduces two novel evaluation metrics; understanding their design is essential for interpreting results and applying them to other contexts
  - Quick check question: What is the key difference between Total Coverage (TC) and Smooth Coverage (SC), and why might SC be more informative for models with partial knowledge?

## Architecture Onboarding

- Component map:
  Wikipedia entity extraction -> Wikidata enrichment -> Template generation -> Translation (Google Translate + GPT-4) -> Validation (back-translation, semantic similarity, human eval) -> Model evaluation -> TC/SC computation

- Critical path:
  1. Select countries and domains for culturally-grounded factual queries
  2. Extract entities and ground-truth answers from Wikipedia/Wikidata
  3. Generate multiple-choice questions with controlled distractor sampling
  4. Translate to create parallel multilingual corpus
  5. Evaluate models and compute coverage metrics

- Design tradeoffs:
  - Multiple-choice format enables scalable evaluation but may not capture generative cultural understanding
  - Translation-based parallel corpus ensures comparability but may introduce translation artifacts (mitigated via human eval, BLEU > 50)
  - Factual focus provides objective ground truth but excludes nuanced cultural knowledge
  - Country-level granularity misses sub-national cultural variation (acknowledged in limitations)

- Failure signatures:
  - High accuracy in one language with near-zero Total Coverage indicates knowledge isolation
  - Better performance in English than native language for country-specific facts indicates Anglo-centric knowledge encoding
  - Near-random performance on temporal facts (wars, leaders) suggests date encodings are not robustly multilingual
  - Large gap between TC(All) and TC(Pre-Train) confirms training data language coverage as primary driver

- First 3 experiments:
  1. **Baseline diagnostic**: Evaluate your model on XNationQA, compute accuracy by language and TC scores across all four scenarios to identify which language pairs have the worst transfer
  2. **Language-specific probing**: For facts where your model succeeds in English but fails in the target language, test whether chain-of-thought in the target language improves retrieval (diagnoses retrieval vs. encoding failure)
  3. **Translation augmentation**: Fine-tune on a subset of XNationQA entity-answer pairs translated into underperforming languages, then measure TC improvement to test whether minimal targeted data can close transfer gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cultural literacy of colossal-scale multilingual LLMs compare to the 7-13B parameter models evaluated in this study?
- Basis in paper: [explicit] The authors state in the Limitations section that "the cultural literacy of colossal models remains an important direction for future investigation" because they could not experiment with larger LLMs due to computational constraints.
- Why unresolved: The study was restricted to specific model sizes (7B, 13B, etc.) and GPT-4 due to budget and hardware limitations.
- What evidence would resolve it: Benchmarking XNationQA on open-source models with significantly larger parameter counts (e.g., 70B+).

### Open Question 2
- Question: How do topic distribution and biases within training corpora contribute to the lower factual recall capabilities of models like Bloomz and Aya compared to less "multilingual-aligned" models?
- Basis in paper: [explicit] The authors note that analyzing the "topic distribution and biases in their publicly available training corpora" is a valuable avenue to explain why models specifically trained for multilingual alignment underperformed.
- Why unresolved: The study focused on performance benchmarking rather than a deep dive into the specific content composition of the models' pre-training data.
- What evidence would resolve it: A comparative analysis of the pre-training datasets (e.g., factual density, source diversity) aligned with XNationQA accuracy scores.

### Open Question 3
- Question: Does the observed performance gap between Western and non-Western languages persist when evaluating cultural literacy for nations in Africa and South America?
- Basis in paper: [inferred] The authors explicitly list the lack of coverage for countries from Africa and South America as a limitation of their nation selection methodology.
- Why unresolved: The selection criteria prioritized nations based on widely spoken languages, inadvertently excluding these major geographical regions.
- What evidence would resolve it: Expanding XNationQA to include African and South American nations and evaluating the same models on these new entities.

## Limitations
- Factual knowledge focus may not capture full complexity of cultural understanding
- Translation pipeline introduces potential artifacts despite careful validation
- Corpus analysis provides limited insight into underlying knowledge transfer mechanisms
- Country-level granularity misses sub-national cultural variation

## Confidence
- **High confidence**: Systematic Western language bias in open-source models' factual recall is well-demonstrated through multiple evaluation scenarios (TC, SC, accuracy by language/country)
- **Medium confidence**: The decoupling hypothesis between cultural knowledge and linguistic competence is supported but not definitively proven
- **Medium confidence**: Domain-specific transfer differences (spatial vs. temporal facts) are observed but mechanism explanation remains speculative

## Next Checks
1. **Prompt Sensitivity Analysis**: Replicate the evaluation using different prompting strategies (chain-of-thought, different template phrasings) for the worst-performing language pairs to determine whether retrieval failures are prompt-dependent or reflect genuine knowledge gaps

2. **Fine-tuning Transfer Experiment**: Select 100-200 entity-answer pairs from underrepresented cultures and fine-tune a model (e.g., Mixtral) on these pairs translated into the target language, then measure TC improvement to test whether minimal targeted data can close transfer gaps

3. **Cross-Domain Knowledge Transfer**: For entities that models "know" in one domain (e.g., monument location), test whether this knowledge transfers to related domains (e.g., historical context about the monument's construction) to assess whether factual knowledge is modular or integrated across cultural dimensions