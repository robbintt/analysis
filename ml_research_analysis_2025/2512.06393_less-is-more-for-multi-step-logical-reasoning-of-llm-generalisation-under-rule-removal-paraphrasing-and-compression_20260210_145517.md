---
ver: rpa2
title: Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule
  Removal, Paraphrasing, and Compression
arxiv_id: '2512.06393'
source_url: https://arxiv.org/abs/2512.06393
tags:
- rule
- reasoning
- anne
- cold
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models achieve high accuracy on multi-step logical
  reasoning when trained on complete rule chains, but performance drops sharply when
  essential rules are removed or explicit contradictions are introduced. Single logical
  equivalence transformations preserve accuracy, while composed multi-law rewrites
  expose model-dependent sensitivity.
---

# Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression

## Quick Facts
- arXiv ID: 2512.06393
- Source URL: https://arxiv.org/abs/2512.06393
- Authors: Qiming Bao; Xiaoxuan Fu
- Reference count: 7
- Key outcome: Large language models achieve high accuracy on multi-step logical reasoning when trained on complete rule chains, but performance drops sharply when essential rules are removed or explicit contradictions are introduced.

## Executive Summary
This study investigates how large language models generalize multi-step logical reasoning under controlled perturbations including rule removal, contradiction injection, and logic-preserving transformations. Using a synthetic dataset with 5-step inference chains, the authors evaluate BERT, Qwen2, and TinyLlama models on 11 variants of each reasoning task. All models achieve perfect accuracy on the base condition, but show dramatic performance degradation when essential rules are removed (near-chance accuracy), contradictions are introduced (zero accuracy), or multiple logical transformations are composed (architecture-dependent sensitivity).

The findings reveal that LLMs learn to associate question types with answer patterns from complete rule chains rather than performing rigorous premise verification. While single logical equivalence transformations preserve accuracy, composed multi-law rewrites expose model-dependent sensitivity, with BERT showing robustness and Qwen2 exhibiting substantial degradation. The study highlights persistent deficiencies in compositional generalization and the need for targeted interventions to improve logical reasoning capabilities.

## Method Summary
The study uses a synthetic dataset generator to create 100 base groups of multi-step logical reasoning problems, each containing one fact, a 5-6 step rule chain, and 4 binary questions. Eleven variants are generated from each base example through rule deletion (redundant vs. essential), contradiction injection, single logical equivalence transformations (contraposition, De Morgan, implication-to-disjunction, double negation, identity, commutativity), and multi-law stacking (2-5 transformations). Models (BERT-base, Qwen2-1.5B, TinyLlama-1.1B) are fine-tuned on 80 base examples only using LoRA (rank=8) with batch size 4, learning rate 2e-5, and 10 epochs. Evaluation measures accuracy and deviation from base across all variants.

## Key Results
- Models reach perfect accuracy on base reasoning chains but drop to near-chance performance (~0.25-0.30) when essential rules are removed
- Explicit contradiction injection reduces accuracy to 0.0000 across all models
- Multi-law stacking exposes architecture-dependent sensitivity: BERT maintains base accuracy, TinyLlama shows marginal degradation, Qwen2 drops substantially
- Redundant rule deletion preserves perfect accuracy, confirming models can ignore irrelevant premises

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models trained on complete rule chains exhibit template-matching behavior rather than rigorous premise verification during inference.
- **Mechanism:** During fine-tuning on base examples, models learn to associate question types with answer patterns derived from complete rule chains. When essential rules are removed at test time, models continue predicting along learned trajectories, producing near-chance accuracy (~0.25-0.30) rather than recognizing that the inference cannot be completed.
- **Core assumption:** The near-chance performance on essential rule deletion reflects inference pattern completion rather than random guessing.
- **Evidence anchors:**
  - [abstract] "essential rule deletion yields a pronounced decrease to near-chance performance"
  - [page 4] "models often behave as if the missing link were still present and tend to 'complete' the original inference pattern even when intermediate support is removed"
  - [corpus] Related work (Bao et al. 2022) shows models "fail to extrapolate to longer or structurally shifted chains, highlighting persistent deficiencies in compositional generalisation"
- **Break condition:** If models were performing true logical inference with premise tracking, essential rule deletion should produce systematic "cannot determine" responses or consistent failure patterns, not near-random accuracy.

### Mechanism 2
- **Claim:** Composed logical equivalence transformations create cumulative distribution shifts that exceed single-transformation robustness, with degradation severity varying by architecture family.
- **Mechanism:** Single-law transformations (contraposition, De Morgan, etc.) alter surface patterns within trained distribution neighborhoods. Multi-law stacking (2-5 transformations) compounds surface-form deviations while preserving semantics, pushing inputs further from training distribution despite logical equivalence. BERT's bidirectional attention may provide more robust cross-passage integration compared to decoder-only causal attention patterns.
- **Core assumption:** The model-dependent sensitivity reflects architectural differences in how attention mechanisms handle structurally complex but semantically equivalent inputs.
- **Evidence anchors:**
  - [abstract] "multi-law stacking exposes model-dependent sensitivity: BERT matches the base condition, TinyLlama shows only marginal degradation, and Qwen2 exhibits a substantial drop"
  - [page 4] "composing multiple equivalence transformations introduces an effective distribution shift beyond isolated rewrites, likely through increased operator density and structural complexity"
  - [corpus] "Order Doesn't Matter, But Reasoning Does" paper finds "LLMs struggle with reasoning order variations and fail to generalize across logically equivalent transformations"
- **Break condition:** If degradation were purely semantic, all models would fail similarly; if purely surface-form, no model would succeed on single-law transformations.

### Mechanism 3
- **Claim:** Models lack mechanisms for conservative reasoning under inconsistency, defaulting to derived conclusions even when explicit contradictions invalidate the inference chain.
- **Mechanism:** Training on consistent rule chains establishes strong association between rule-application patterns and conclusions. When contradictory facts are injected, models lack training signal for withholding judgment or engaging in paraconsistent reasoning, resulting in 0.0 accuracy under the benchmark's scoring semantics.
- **Core assumption:** The complete failure on contradiction injection reflects absence of inconsistency-detection mechanisms rather than alternative reasoning strategies.
- **Evidence anchors:**
  - [abstract] "injecting explicit contradictions reduces accuracy to 0.0000"
  - [page 4] "models do not reliably adopt contradiction-aware conservative reasoning under the benchmark's scoring semantics"
  - [corpus] "Standard Neural Computation Alone Is Insufficient for Logical Intelligence" argues that neural architectures "lack the structural properties required for rigorous deductive reasoning"
- **Break condition:** Models with explicit inconsistency-handling training or architectures would show non-zero accuracy by recognizing conflicts and withholding conclusions.

## Foundational Learning

- **Concept: First-order logic rule chains and deductive closure**
  - Why needed here: The benchmark constructs multi-step inference chains (e.g., green→cold→rough→young→nice) where each step's output becomes the next step's input; understanding derivability is essential to interpret why redundant vs. essential rule deletion produce different outcomes.
  - Quick check question: Given rules A→B, B→C, C→D and fact A, which rules can be removed while preserving the ability to derive D?

- **Concept: Logical equivalence laws (contraposition, De Morgan, implication-to-disjunction, double negation)**
  - Why needed here: The study applies these transformations to create semantically equivalent but surface-different variants; recognizing that ¬A∨B ≡ A→B is necessary to understand why the benchmark treats these as "logic-preserving."
  - Quick check question: Rewrite "If someone is green then they are cold" using (a) contraposition and (b) implication-to-disjunction.

- **Concept: Distribution shift vs. semantic shift in evaluation**
  - Why needed here: The paper's central distinction is that models fail under distribution shift (multi-law stacking, missing rules, contradictions) even when semantics are preserved or explicitly violated; distinguishing these failure modes requires understanding what the model was trained on versus what it's tested on.
  - Quick check question: If a model trained on "A→B" statements is tested on "¬B→¬A" statements, is this a semantic shift, distribution shift, or both?

## Architecture Onboarding

- **Component map:** Synthetic dataset generator → produces base facts + 5-step rule chains + 4 questions per instance → Variant transformer → applies 11 perturbations → Training pipeline → LoRA fine-tuning on 80 base examples → Evaluation suite → tests on all variants → Model zoo → BERT-base, Qwen2-1.5B, TinyLlama-1.1B

- **Critical path:**
  1. Dataset construction: Generate 100 base groups → split 80/20 train/test → apply all variants to all groups
  2. Fine-tuning: Train ONLY on base examples; variants must remain unseen
  3. Evaluation: Run each model on all 11 test splits → compute accuracy and Δ for each variant
  4. Analysis: Compare model families on variant-level degradation patterns

- **Design tradeoffs:**
  - Synthetic vs. natural language: Synthetic ensures controlled perturbations but may limit real-world applicability
  - Binary classification vs. generation: Binary outputs enable clean accuracy metrics but prevent analysis of model uncertainty or explanation
  - LoRA vs. full fine-tuning: Parameter efficiency enables fair comparison across architectures but may underrepresent model capacity
  - Single-hop vs. multi-hop chain length: 5-step chains provide multi-step stress testing but may not reflect typical reasoning depths

- **Failure signatures:**
  - Near-chance accuracy on essential rule deletion → indicates template completion without premise verification
  - 0.0 accuracy on contradiction injection → indicates no inconsistency detection
  - Model-specific degradation on multi-law stacking → indicates architectural sensitivity to composed transformations (Qwen2 most brittle)
  - No degradation on redundant rule deletion → confirms models can ignore irrelevant premises

- **First 3 experiments:**
  1. Replicate the base evaluation on a single model (e.g., TinyLlama) across all 11 variants to validate the pipeline and establish baseline Δ values.
  2. Extend multi-law stacking to include 6-10 transformations to test whether degradation continues linearly or saturates; compare against BERT to probe architectural robustness limits.
  3. Add a "soft contradiction" variant where facts conflict with non-essential intermediate conclusions (not final answers) to distinguish global inconsistency detection from local conflict resolution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can targeted interventions, such as deletion-aware training or explicit proof verification, successfully mitigate the tendency of LLMs to hallucinate missing inferential links when essential rules are removed?
- **Basis in paper:** [explicit] The Discussion section explicitly motivates "targeted interventions such as deletion-aware training, explicit proof/rationale verification" to address the observed failure mode where models rely on "learned chain templates" rather than verifying premises.
- **Why unresolved:** The paper identifies the failure mode (models reasoning as if missing links still exist) but does not implement or test the proposed remediation strategies to see if they improve sensitivity to essential rule deletion.
- **What evidence would resolve it:** An experiment where models are fine-tuned on data containing unanswerable queries due to missing links, demonstrating improved accuracy (e.g., predicting "False" or "Unknown") on the essential deletion variant.

### Open Question 2
- **Question:** What specific architectural or pre-training characteristics cause modern decoder-only models (like Qwen2) to degrade significantly under multi-law equivalence stacking while encoder-only models (like BERT) remain robust?
- **Basis in paper:** [explicit] The Discussion notes that multi-law stacking exposes "model-dependent sensitivity," specifically citing Qwen2's "pronounced decrease" ($\Delta = -0.3550$) versus BERT's perfect stability.
- **Why unresolved:** The study observes the performance gap but only speculates that the degradation is due to "increased operator density and structural complexity," leaving the precise internal failure mechanism unexplored.
- **What evidence would resolve it:** A mechanistic interpretability analysis comparing attention head behavior in BERT versus Qwen2 when processing composed logical transformations to identify where the semantic representation collapses.

### Open Question 3
- **Question:** How can LLMs be trained to adopt contradiction-aware, conservative reasoning strategies (withholding conclusions) rather than defaulting to complete failure when explicit contradictions are injected?
- **Basis in paper:** [explicit] The Results section highlights that injecting explicit contradictions reduces accuracy to 0.0000, indicating that models "do not reliably revise conclusions in the presence of inconsistency."
- **Why unresolved:** The paper establishes the brittleness (uniform breakdown) but does not propose or test methods for implementing paraconsistent logic or uncertainty calibration to handle conflicting evidence.
- **What evidence would resolve it:** A training objective that penalizes confident predictions on contradictory premises, resulting in models that output "Invalid" or "Contradiction" instead of a binary guess.

## Limitations

- Synthetic dataset with 100 examples may not capture full distribution of logical reasoning scenarios
- Binary classification prevents analysis of model uncertainty or explanation quality
- LoRA fine-tuning may underrepresent model capacity and architectural capabilities
- Limited to 5-step inference chains, which may not reflect typical reasoning depths

## Confidence

- **High Confidence:** Claims about near-chance performance on essential rule deletion and zero accuracy on contradiction injection are well-supported by the experimental design and results.
- **Medium Confidence:** The architectural sensitivity findings (BERT robustness vs. Qwen2 brittleness on multi-law stacking) are supported but could reflect training data differences or fine-tuning nuances beyond architectural properties.
- **Low Confidence:** The mechanism explanations for why near-chance performance occurs (template completion vs. other failure modes) remain speculative without additional probing experiments.

## Next Checks

1. **Dataset Size Scaling:** Replicate the essential rule deletion experiment with datasets of 500, 1000, and 5000 examples to determine whether the near-chance performance pattern holds with increased training diversity.

2. **Uncertainty-Aware Evaluation:** Replace binary classification with confidence scoring to measure whether models show higher uncertainty on essential rule deletion variants compared to base cases.

3. **Architectural Ablation:** Compare BERT-base with a BERT variant using causal attention while keeping all other factors constant to isolate whether the bidirectional attention mechanism specifically contributes to robustness against multi-law stacking.