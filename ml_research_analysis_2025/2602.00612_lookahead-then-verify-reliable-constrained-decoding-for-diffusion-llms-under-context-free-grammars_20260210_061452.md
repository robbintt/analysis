---
ver: rpa2
title: 'Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under
  Context-Free Grammars'
arxiv_id: '2602.00612'
source_url: https://arxiv.org/abs/2602.00612
tags:
- arxiv
- decoding
- expression
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating syntactically
  valid formal language outputs (e.g., code, chemical expressions) using Diffusion
  Large Language Models (dLLMs). The key problem is that dLLMs generate tokens non-autoregressively,
  making it difficult to apply traditional constrained decoding techniques designed
  for autoregressive models.
---

# Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars

## Quick Facts
- **arXiv ID:** 2602.00612
- **Source URL:** https://arxiv.org/abs/2602.00612
- **Reference count:** 40
- **Primary result:** Achieves near-100% syntactic correctness on C++, JSON, and SMILES generation using diffusion LLMs with negligible runtime overhead

## Executive Summary
This paper addresses the challenge of generating syntactically valid formal language outputs (e.g., code, chemical expressions) using Diffusion Large Language Models (dLLMs). The key problem is that dLLMs generate tokens non-autoregressively, making it difficult to apply traditional constrained decoding techniques designed for autoregressive models. The authors propose "lookahead-then-verify," where when a dLLM proposes a token for a masked position, the approach samples multiple potential completions of the incomplete prefix based on the model's predicted token distributions, then uses a grammar parser to verify if any of these completions can be extended into valid sentences. If so, the token is accepted; otherwise, it is rejected. This significantly improves syntactic correctness across multiple benchmarks and models, achieving near-100% syntactic correctness rates with minimal runtime overhead.

## Method Summary
The core innovation is a lookahead-based verification mechanism that samples multiple potential completions of incomplete prefixes when a token is proposed during dLLM decoding. For each proposed token, the method samples N=10 potential completions for remaining masked positions using the model's parallel predictions, then uses an Earley parser to check if any completion is extendable into valid sentences. If valid extensions exist, the token is accepted; otherwise, it's rejected and the model resamples. The approach includes a cache-enhanced recovery mechanism that triggers when consecutive failures exceed a threshold (τ=5), replacing the current prefix with a previously verified state and forcing the next token from the valid set. This enables constrained decoding under context-free grammars for non-autoregressive models, addressing the fundamental mismatch between dLLMs' parallel token generation and the sequential nature of grammar validation.

## Key Results
- Achieves near-100% syntactic correctness rates across C++, JSON, and SMILES generation tasks
- Improves functional correctness while maintaining minimal computational overhead (3.5% runtime increase)
- Demonstrates effectiveness across multiple diffusion models (LLaDA-8B, LLaDA-1.5, Dream-7B, DiffuCoder-7B)

## Why This Works (Mechanism)
The method works by transforming the non-autoregressive token selection problem into a verification problem that can be solved with existing parsing algorithms. Traditional autoregressive models can validate each token sequentially against the grammar, but dLLMs generate tokens in parallel, making this impossible. Lookahead-then-verify solves this by using the model's predicted distributions to sample potential future states, then applying a deterministic parser to check if those futures lead to valid completions. This creates a bridge between the probabilistic, parallel nature of dLLMs and the deterministic, sequential requirements of grammar validation. The lookahead sampling effectively provides a "proof of existence" that the current token choice can lead to valid output, allowing the non-autoregressive model to make globally coherent decisions while respecting local grammar constraints.

## Foundational Learning

**Context-Free Grammars (CFGs):** Formal systems defining syntax rules for programming languages and structured data. *Why needed:* The target domain requires generation that adheres to specific syntactic structures. *Quick check:* Can define a simple grammar for arithmetic expressions and validate strings against it.

**Earley Parser:** An efficient parsing algorithm that can handle any context-free grammar, including ambiguous ones. *Why needed:* Required to efficiently verify if partial strings can be extended to valid sentences. *Quick check:* Can implement a basic Earley parser for a simple grammar and test extendability of prefixes.

**Non-autoregressive Decoding:** Generation where multiple tokens are predicted in parallel rather than sequentially. *Why needed:* dLLMs operate this way, creating the core challenge the paper addresses. *Quick check:* Can contrast parallel vs sequential token generation and explain the implications for grammar validation.

**Subword Tokenization:** Breaking text into smaller units (e.g., BPE, WordPiece) for model input. *Why needed:* Models use subwords, but grammars use full terminals, creating alignment challenges. *Quick check:* Can explain how "return" might be tokenized as "re" + "turn" and why this matters for grammar parsing.

## Architecture Onboarding

**Component map:** Model predictions → Lookahead sampling → Earley parser → Token acceptance/rejection → Cache recovery → Final output

**Critical path:** Token proposal → Sampling N completions → Parser verification → Acceptance decision → Output generation

**Design tradeoffs:** The lookahead size N=10 balances verification accuracy against computational cost. Larger N improves verification reliability but increases overhead. The failure threshold τ=5 determines when to trigger recovery - too low wastes valid tokens, too high allows unproductive loops.

**Failure signatures:** 
- Infinite rejection loops indicate the model is in a syntactically impossible state
- High rejection rates suggest lookahead sampling is insufficient for the grammar complexity
- False rejections occur when valid tokens are rejected due to unlucky lookahead samples

**First experiments:**
1. Implement the Earley parser with IsExtendable functionality and test on simple grammars
2. Create a basic lookahead sampling mechanism that samples N completions from marginal distributions
3. Build the token acceptance/rejection loop and measure rejection rates on simple generation tasks

## Open Questions the Paper Calls Out

**Open Question 1:** How can the Lookahead-then-Verify framework be effectively combined with repetition penalty mechanisms to prevent the model from stalling in repetitive patterns without violating grammar constraints? The paper identifies that remaining syntax errors are caused by repetitive pattern generation (e.g., whitespace) until max length is reached, but combining repetition avoidance with grammar constraints remains unresolved.

**Open Question 2:** Does the computational efficiency of the lookahead sampling degrade as required generation length increases significantly beyond 256 tokens? The fixed lookahead size (N=10) may become insufficient for longer sequences where more masked positions need to be sampled, potentially increasing verification time non-linearly.

**Open Question 3:** Can the verification step be extended to enforce semantic constraints (e.g., type correctness in code) rather than just syntactic extendability? While CFG satisfaction is necessary, functional correctness requires semantic validation that goes beyond the current grammar-based approach.

## Limitations

- Lookahead sampling strategy details are underspecified, particularly the temperature and sampling parameters for verification vs main generation
- Potential subword-token-to-grammar-terminal alignment issues are acknowledged but not fully resolved in implementation
- Recovery mechanism may introduce biases by forcing predictions when stuck, potentially limiting exploration of valid but unconventional solutions

## Confidence

**Core contribution:** High confidence - the lookahead-then-verify algorithm addresses a real problem with clear improvements in syntactic correctness
**Scalability claims:** Medium confidence - 3.5% overhead measured on specific hardware may vary with implementation details
**Generalizability:** Medium confidence - experiments on three grammars show promise but larger benchmark suites would strengthen claims

## Next Checks

1. **Sample strategy ablation study:** Systematically evaluate how different lookahead sampling strategies (varying temperature, top-k, top-p parameters) affect both syntactic correctness rates and computational overhead to identify optimal configurations.

2. **Tokenization alignment implementation:** Develop and test explicit mechanisms for handling subword-token-to-grammar-terminal alignment during the incomplete prefix stage, measuring impact on rejection rates and final output quality.

3. **Cross-grammar generalization test:** Apply the method to additional grammars (e.g., SQL, HTML, mathematical expressions) with larger benchmark suites to validate robustness across diverse formal languages and assess whether near-100% syntactic correctness rates generalize.