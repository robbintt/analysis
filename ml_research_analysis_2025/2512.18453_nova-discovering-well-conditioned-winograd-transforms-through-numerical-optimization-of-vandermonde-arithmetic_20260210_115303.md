---
ver: rpa2
title: 'NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization
  of Vandermonde Arithmetic'
arxiv_id: '2512.18453'
source_url: https://arxiv.org/abs/2512.18453
tags:
- points
- error
- winograd
- standard
- discovered
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses numerical instability in Winograd convolution
  for low-precision inference. Standard integer interpolation points cause exponentially
  growing condition numbers in Vandermonde matrices, making large tiles unusable in
  FP16/INT8.
---

# NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic

## Quick Facts
- arXiv ID: 2512.18453
- Source URL: https://arxiv.org/abs/2512.18453
- Reference count: 40
- Primary result: Fractional interpolation points discovered via Evolution Strategy improve Winograd transform conditioning by 415× for F(8,3), enabling stable FP16/INT8 inference with 70+ percentage points accuracy recovery

## Executive Summary
This paper addresses numerical instability in Winograd convolution for low-precision inference. Standard integer interpolation points cause exponentially growing condition numbers in Vandermonde matrices, making large tiles unusable in FP16/INT8. The paper reframes point selection as a continuous optimization problem, using Evolution Strategy to search the full space Rn-1, then snapping to simple rationals and verifying symbolically. This open discovery approach finds fractional points like {±5/6,±7/6} that dramatically improve conditioning by 415× for F(8,3), which squares to 172,484× improvement in 2D. In ImageNet FP16 inference, standard transforms collapse to random chance (4.7% accuracy), while discovered points restore full accuracy (75-78%), recovering 70+ percentage points without retraining. These drop-in replacements enable stable FP16/INT8 deployment of large Winograd tiles.

## Method Summary
NOVA treats Winograd point selection as a continuous optimization problem over R^(n-1), using Evolution Strategy to search for configurations that minimize Vandermonde condition number while maintaining exact polynomial interpolation. The algorithm evolves a population of candidate point sets, evaluating fitness based on reconstruction error and condition number. Continuous solutions are snapped to simple rational fractions (denominator ≤ 10), then verified symbolically using exact rational arithmetic to ensure decomposition accuracy. The method discovers fractional points like {±5/6,±7/6} that cluster more tightly than integers, dramatically reducing Vandermonde condition numbers and enabling stable low-precision inference. The approach is validated through controlled experiments on ImageNet using ResNet-18 and VGG-16 architectures.

## Key Results
- Discovered fractional points {±5/6,±7/6} improve Vandermonde condition number by 415× for F(8,3)
- 2D convolution improvements square to 172,484× due to Kronecker product structure
- FP16 inference accuracy recovers from 4.7% (random chance) to 75-78% with discovered points
- 70+ percentage points accuracy recovery without retraining or architectural changes
- Enables stable FP16/INT8 deployment of large Winograd tiles previously unusable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering interpolation points closer together reduces Vandermonde condition number, which propagates to transform matrix conditioning.
- Mechanism: Vandermonde entries V_ij = α_i^j grow exponentially when points are spread far apart (e.g., integers {0,±1,±2,±3}). Fractional points like {±5/6,±7/6} cluster within [−1.2,1.2], keeping entries small → lower κ(V) → lower κ(A), κ(B), κ(G) → reduced error amplification.
- Core assumption: Condition number bounds forward error amplification in Winograd transforms; empirical CNN validation confirms this propagation.
- Evidence anchors:
  - [abstract] "Standard integer interpolation points cause exponentially growing condition numbers in Vandermonde matrices"
  - [Section 2.2] "κ(V) serves as a proxy for overall numerical stability: points that minimize κ(V) consistently yield lower κ(A), κ(B), κ(G)"
  - [Table 3] Shows κ(V) improvements (2.9–415×) propagate to all transform matrices (1.8–270×)
  - [corpus] Weak direct evidence; neighbor papers focus on general convolution benchmarking, not Vandermonde conditioning specifically.
- Break condition: If transform matrices are computed/stored in higher precision than inference arithmetic, conditioning effects may be masked (see Table 31 showing FP32 compute with INT8 weights shows no difference).

### Mechanism 2
- Claim: 1D conditioning improvements square in 2D via Kronecker product structure.
- Mechanism: For 2D convolution, G²ᴰ = G⊗G. Singular values multiply: σ(A⊗A) = σ(A)² → κ(A⊗A) = κ(A)². Thus 27× 1D improvement → 733× 2D improvement.
- Core assumption: Standard Kronecker construction is used; direct 2D point selection might find different optima.
- Evidence anchors:
  - [abstract] "which squares to a 172,484× improvement for 2D convolution"
  - [Section 3.4] "We confirmed this property by directly computing κ₂(A⊗A) for F(4,3) and F(6,3). All ratios κ₂(A⊗A)/κ₂(A)² = 1.0000 exactly"
  - [Table 17] 2D κ values: F(6×6,3×3) standard 4.3×10⁶, discovered 5,873 (733× improvement)
  - [corpus] No direct corpus evidence; this is a mathematical property from Higham 2002 cited in paper.
- Break condition: Direct 2D optimization (not via Kronecker products) could potentially find different configurations.

### Mechanism 3
- Claim: Reduced conditioning enables FP16/INT8 deployment of larger Winograd tiles that would otherwise suffer catastrophic accuracy collapse.
- Mechanism: κ(V) reduction → smaller norm products ∥A∥∥B∥∥G∥ → lower quantization error amplification → network outputs remain meaningful. Standard F(6,3) at FP16: κ=2,075 causes 10.8% accuracy; discovered κ=77 recovers to 77.8%.
- Core assumption: Error compounds across layers multiplicatively; no systematic error cancellation.
- Evidence anchors:
  - [abstract] "In ImageNet FP16 inference, standard transforms collapse to random chance (4.7% accuracy), while discovered points restore full accuracy (75-78%)"
  - [Table 8] ResNet-18 F(6,3): Standard 10.8% → Discovered 77.8% (+67pp)
  - [Table 28] INT8 CNN-level: Standard error 6.89 (689%), Discovered 0.072 (7.2%) → 96× reduction
  - [corpus] Neighbor "Range-Arithmetic" paper addresses verifiable inference on untrusted parties but not Winograd specifically.
- Break condition: Per-channel quantization (production INT8) partially masks conditioning issues via per-channel scales; F(8,3) remains challenging even with discovered points (50% accuracy partial recovery).

## Foundational Learning

- Concept: **Vandermonde Matrices and Condition Numbers**
  - Why needed here: The entire problem stems from ill-conditioned Vandermonde matrices from integer points. Understanding κ(V) = σ_max/σ_min bounds error amplification.
  - Quick check question: Given points {0,±1,±2}, can you sketch why κ(V) grows with tile size? (Hint: entries grow as α_i^j in columns)

- Concept: **Cook-Toom/Winograd Convolution**
  - Why needed here: NOVA modifies interpolation points in the standard Winograd pipeline. Need to understand: input transform B^T, kernel transform G, element-wise multiply, output transform A^T.
  - Quick check question: Why does the infinity point enable minimal multiplications? (Hint: extracts leading coefficient without explicit evaluation)

- Concept: **Evolution Strategy (ES) Basics**
  - Why needed here: NOVA uses ES (not gradient descent) to search R^(n-1). Population-based, adaptive mutation, no gradients required.
  - Quick check question: Why might ES be preferred over gradient-based methods for this search? (Hint: fitness involves discrete snap-to-rational, κ computation)

## Architecture Onboarding

- Component map: Search Space: R^(n-1) → Evolution Strategy → Population sampling, fitness evaluation, elite selection, μ update → Snap-to-Rational → d_max=10, find nearest a/b with b≤10 → Symbolic Verification → SymPy exact rational arithmetic, check ||T_conv - decomposition|| < 10^-10 → Valid Configuration → {0, ±5/6, ±7/6, ...} with κ(V) reported

- Critical path: **Snap-to-rational + verification**. ES finds continuous solutions; snapping must preserve enough structure for symbolic verification. Invalid snaps → neighborhood search (±1.2 distance). Verification uses exact rational arithmetic (Python Fraction objects), not floating-point.

- Design tradeoffs:
  - **Unconstrained vs. dtype-aware**: {±5/6,±7/6} achieves κ=14.5 but 5/6 has FP16 representation error; {±3/4,±5/4} achieves κ=15.2 with exact FP16 representation.
  - **d_max selection**: d_max=6 finds same F(4,3) points; d_max>10 shows no κ improvement but complicates implementation.
  - **Symmetric vs. asymmetric**: Symmetric search is faster but may miss optimal asymmetric configurations.

- Failure signatures:
  - κ(V) > 10^5: FP16 accuracy collapses to random (e.g., F(8,3) standard: 4.7%)
  - κ(V) > 2000: INT8 per-tensor fails catastrophically (F(6,3) standard: 3990% error)
  - Snap-to-rational invalid: Tolerance exceeded → increase d_max or neighborhood search radius
  - F(8,3) at INT8: Even discovered (59% layer error) remains challenging; combine with scaling methods

- First 3 experiments:
  1. **Reproduce F(4,3) discovery**: Run ES on R^5 (5 finite points), verify {0, ±5/6, ±7/6, ∞} yields κ=14.5. Validate with SymPy verification. Runtime: <1 second.
  2. **INT8 layer error measurement**: Implement 2D Winograd with standard vs. discovered F(4,3) points. Use symmetric per-tensor quantization, Gaussian inputs. Expected: 7.1% → 2.1% error (3.4× improvement).
  3. **FP16 ResNet-18 inference**: Replace F(6,3) layers with discovered points {0, ±3/5, ±1, ±7/6, ∞}. Run on ImageNetV2 subset. Expected: 10.8% → 77.8% accuracy recovery. Verify no latency penalty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can combining discovered fractional points with learned diagonal scaling enable stable F(8,3) inference at INT8/FP16 precision?
- Basis in paper: [explicit] The paper states F(8,3) achieves only 50% accuracy with discovered points alone and "layer-level error remains 59.2% even with discovered points, indicating F(8,3) INT8 deployment requires additional stabilization beyond point selection alone."
- Why unresolved: The paper demonstrates orthogonality between point selection and tap-wise scaling (+0.4% marginal gain on F(6,3)) but does not evaluate the combination at F(8,3) where conditioning alone is insufficient.
- What evidence would resolve it: Evaluation of F(8,3) INT8/FP16 accuracy when discovered points are combined with learned per-channel scaling methods (PAW, LoWino, tap-wise).

### Open Question 2
- Question: What is the formal backward error analysis for the complete Winograd pipeline relating κ(V) to end-to-end inference error?
- Basis in paper: [explicit] "We do not provide a formal backward error analysis for the complete Winograd pipeline. The condition number κ bounds worst-case error amplification but does not account for error cancellation, input statistics, or accumulation patterns."
- Why unresolved: The paper empirically validates that κ improvements translate to accuracy gains but leaves the theoretical relationship—including potential error cancellation—unanalyzed.
- What evidence would resolve it: A formal analysis deriving error bounds that account for correlated transforms (A, B, G from same points), input distributions, and multi-layer error propagation.

### Open Question 3
- Question: Does direct 2D interpolation point selection discover configurations with better conditioning than Kronecker products of 1D solutions?
- Basis in paper: [explicit] "Direct 2D point selection could potentially find better configurations that do not factor as products of 1D transforms. However, the search space for direct 2D optimization is substantially larger."
- Why unresolved: The paper uses 1D optimization followed by Kronecker extension, which may not reach the global optimum for 2D convolution.
- What evidence would resolve it: Running ES in the 2D search space (R^(n²-1)) for small tiles and comparing κ(A⊗A) against κ from direct 2D optimization.

### Open Question 4
- Question: How do discovered fractional points interact with production cuDNN/TensorRT Winograd implementations at scale?
- Basis in paper: [explicit] "Key extensions include... integration with cuDNN/TensorRT for deployment" and "We do not provide end-to-end network latency (Winograd vs. direct conv) as this requires cuDNN-level integration."
- Why unresolved: Validation used a PyTorch research implementation; production integration with hardware-optimized libraries remains untested.
- What evidence would resolve it: Benchmarks showing inference latency and accuracy on ImageNet with discovered points integrated into cuDNN or TensorRT Winograd kernels.

## Limitations

- **Neighborhood Search Uncertainty**: The procedure for handling invalid snap-to-rational results lacks detailed algorithmic specifications, potentially affecting reproducibility.
- **Fundamental F(8,3) Limits**: Even with discovered points, F(8,3) shows only partial accuracy recovery (50%), suggesting inherent limits not fully explained by conditioning alone.
- **Production Integration Gap**: The paper validates using PyTorch research implementation but doesn't address integration with production cuDNN/TensorRT implementations.

## Confidence

- **High Confidence**: The mathematical framework connecting Vandermonde condition numbers to transform matrix stability is well-established. Empirical evidence for F(4,3) and F(6,3) improvements is robust, with clear accuracy recovery in controlled experiments.
- **Medium Confidence**: The 2D conditioning improvements follow from established Kronecker product properties, but direct 2D optimization remains unexplored. The 70+ percentage point accuracy recovery claims are dramatic and rely on the assumption that error compounds multiplicatively without cancellation.
- **Low Confidence**: F(8,3) results show only partial recovery (50% accuracy), suggesting fundamental limits not fully explained. The paper doesn't address whether discovered points are optimal or merely locally optimal within the search constraints.

## Next Checks

1. **Algorithmic Completeness**: Request detailed pseudocode for the neighborhood search procedure when snap-to-rational fails validation, including exact bounds and enumeration logic used in practice.

2. **Cross-Platform Verification**: Implement the complete pipeline (ES discovery + symbolic verification + CNN inference) on a different platform (e.g., PyTorch/C++ vs. Python) to verify numerical consistency and rule out platform-specific artifacts.

3. **Error Propagation Analysis**: Conduct ablation studies measuring per-layer quantization errors and their accumulation patterns in ResNet-18, comparing standard vs. discovered points to validate the multiplicative error assumption and identify potential cancellation effects.