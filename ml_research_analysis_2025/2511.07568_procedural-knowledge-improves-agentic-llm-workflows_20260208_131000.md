---
ver: rpa2
title: Procedural Knowledge Improves Agentic LLM Workflows
arxiv_id: '2511.07568'
source_url: https://arxiv.org/abs/2511.07568
tags:
- files
- task
- notes
- problem
- request
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether embedding procedural knowledge
  into LLM workflows improves performance on agentic tasks. The authors formalize
  complex tasks as an MDP and integrate Hierarchical Task Networks (HTNs) to decompose
  tasks into manageable subtasks.
---

# Procedural Knowledge Improves Agentic LLM Workflows

## Quick Facts
- **arXiv ID**: 2511.07568
- **Source URL**: https://arxiv.org/abs/2511.07568
- **Reference count**: 40
- **Primary result**: Embedding hierarchical task networks (HTNs) into LLM workflows significantly improves performance on complex, multi-step agentic tasks, with smaller models sometimes outperforming larger ones when augmented with procedural knowledge.

## Executive Summary
This paper investigates whether embedding procedural knowledge into LLM workflows improves performance on agentic tasks. The authors formalize complex tasks as an MDP and integrate Hierarchical Task Networks (HTNs) to decompose tasks into manageable subtasks. They implement and evaluate ProcLLM, a hybrid system combining HTNs with an agentic LLM, on four benchmarks including travel planning and block world. Results show that hand-coded HTNs significantly improve LLM performance across all domains, with smaller models benefiting more and sometimes outperforming larger models without HTNs. While LLM-generated HTNs also improve performance, their gains are less consistent than human-designed ones. Overall, the work demonstrates that leveraging procedural knowledge from humans or documents is an effective way to enhance LLM workflows on complex, multi-step tasks.

## Method Summary
The ProcLLM system formalizes agentic tasks as MDPs and integrates hierarchical task networks (HTNs) to decompose complex tasks into manageable subtasks. The architecture consists of a task stack, an agent-LLM for action generation, an environment (file system), and a verify-LLM for checking subtask completion. The system iteratively decomposes tasks using HTN methods, focuses the LLM on the current subtask context, executes actions in the environment, and verifies completion before proceeding. The authors evaluate this approach on four benchmarks: Travel Planning (tool use), Recipe Generator (algorithmic tool use), Blocks World (planning), and Unit Movement (game). They compare performance with and without HTNs, testing both hand-coded and LLM-generated task networks.

## Key Results
- Hand-coded HTNs dramatically improve LLM performance on agentic tasks across all four benchmark domains.
- Smaller models (20b parameters) augmented with HTNs can outperform much larger models (120b parameters) without HTNs.
- LLM-generated HTNs also improve performance but yield more mixed and inconsistent results compared to human-designed networks.
- The verification step, while adding overhead, helps prevent cascading errors by ensuring subtask completion before proceeding.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition via HTNs
- **Claim**: Explicit procedural knowledge in the form of Hierarchical Task Networks (HTNs) improves LLM performance on multi-step agentic tasks by reducing the complexity of the action-selection problem at each step.
- **Mechanism**: Complex tasks are decomposed into a sequence of manageable subtasks using a hand-coded or LLM-generated HTN. At each iteration, the LLM is presented with a focused context related only to the current primitive task, rather than the entire problem history. This decomposition constrains the problem space, thereby increasing the probability of selecting a correct action.
- **Core assumption**: LLM performance degrades with task complexity, and breaking a task into smaller, independent subtasks increases the likelihood of successful completion for each subtask.
- **Evidence anchors**: The abstract states that hand-coded HTNs can dramatically improve LLM performance on agentic tasks. Section 3 provides the mathematical formalization showing that decomposition improves task success if the probability of completing the full task is less than the product of probabilities for each subtask.
- **Break condition**: The benefit diminishes if subtasks are poorly defined, have high interdependencies not managed by the HTN, or if the overhead of managing the decomposition exceeds the LLM's context capacity.

### Mechanism 2: Contextual Scaffolding and Verification
- **Claim**: The ProcLLM architecture improves reliability by iteratively scaffolding the LLM's context and explicitly verifying subtask completion before proceeding.
- **Mechanism**: The system maintains a task stack and uses an UpdateTask function to decompose the current task. The LLM is prompted with a context assembled from the environment state, execution trace, and the current subtask description with its intended effect. After the LLM takes an action, a verify action can be triggered, which uses a separate verify-LLM to check if the subtask's effect has been met against specific files.
- **Core assumption**: LLMs are prone to losing context or making premature jumps. Explicit, step-wise verification against a defined state prevents cascading errors and keeps the agent focused.
- **Evidence anchors**: Section 4, Algorithm 1 details the verification loop where the system checks if a subtask's effect has been achieved and only advances upon successful verification.
- **Break condition**: The verification process can fail if the verify-LLM is itself unreliable, if the "effect" description is ambiguous, or if the verification files do not capture the true state of completion.

### Mechanism 3: Cross-Model Amplification
- **Claim**: Embedding procedural knowledge via HTNs disproportionately benefits smaller models, allowing them to achieve performance comparable to or exceeding larger, non-augmented models.
- **Mechanism**: The procedural knowledge encoded in the HTN compensates for the weaker planning and reasoning capabilities of smaller LLMs. By externalizing the task structure and decision flow, the LLM is only required to perform simpler action-selection within a constrained scope.
- **Core assumption**: A significant portion of the performance gap between small and large models on agentic tasks is attributable to the large model's superior ability to plan and maintain complex context, not just its ability to select individual actions.
- **Evidence anchors**: The abstract states that using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Section 5 shows that the smaller GPT-oss-20b Human-TN significantly outperforms GPT-oss-120b [No-TN].
- **Break condition**: This finding is based on specific benchmarks and model families tested, and may not generalize to all task types or model architectures.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - Why needed here: The paper formalizes the agentic task as an MDP (M = (S, A, T, r, H)). Understanding states, actions, transitions, and the horizon is essential to grasp how the system operates and why sequential decision-making is difficult for an LLM.
  - Quick check question: In the ProcLLM framework, what component corresponds to the MDP's *policy* (Ï€)?

- **Concept: Hierarchical Task Network (HTN)**
  - Why needed here: This is the core contribution. An HTN is a structure for decomposing high-level abstract tasks into a totally ordered sequence of primitive, executable tasks via "methods." Understanding this decomposition is key to the paper's mechanism.
  - Quick check question: In an HTN, what is the relationship between a "method" and a "task"?

- **Concept: Context Window**
  - Why needed here: A central problem identified is that LLMs struggle to manage long horizons and dependencies. The paper's solution explicitly reduces the "context complexity" presented to the LLM at each step.
  - Quick check question: How does ProcLLM's use of subtasks aim to mitigate the limitations of an LLM's context window?

## Architecture Onboarding

- **Component map**: Task Stack (K) -> Agent-LLM -> Environment (file system) -> Verify-LLM -> UpdateTask -> Task Stack (K)

- **Critical path**:
  1. Initialization: A top-level task is placed on the stack.
  2. Decomposition: UpdateTask recursively decomposes the task using relevant methods from M, pushing primitive subtasks onto the stack.
  3. Action Generation: The Agent-LLM is called with a context focused on the primitive task at the top of the stack. It returns a JSON action.
  4. Execution & Verification: If the action is external, it's executed in the environment. If the action is verify, the Verify-LLM checks the effect. Upon successful verification, the task is popped from the stack.
  5. Loop: The process repeats from step 2 until the stack is empty or the horizon is reached.

- **Design tradeoffs**:
  - Hand-coded vs. LLM-generated HTNs: Human-designed HTNs provide consistent, high performance but require domain expertise. LLM-generated HTNs are more flexible but yield mixed, often inferior, results.
  - Performance vs. Iterations: Using an HTN typically increases the time per iteration (due to more context assembly and verification steps) but decreases the total number of iterations required to solve a task.
  - Flexibility vs. Structure: The system restricts the LLM to a predefined workflow. This prevents failure modes like action looping but may be brittle if the predefined procedure cannot handle novel problem instances.

- **Failure signatures**:
  - Infinite Looping (No-TN): The Agent-LLM repeats the same action sequence without making progress, leading to a timeout.
  - Verification Stuck: The Verify-LLM repeatedly fails to confirm a subtask's effect, preventing progress.
  - Decomposition Failure: The UpdateTask function fails to find a relevant method for a task, halting progress.

- **First 3 experiments**:
  1. Replicate with a New Domain: Implement ProcLLM for a novel, multi-step domain (e.g., a simplified software bug-fixing task). Write the domain specification, a few problem instances, and a hand-coded HTN. Compare the success rate and total cycles of an LLM with and without the HTN.
  2. Ablation of Verification: Run the system on an existing benchmark (like Blocks World) but disable the verify step in the workflow. Measure the impact on success rate to understand the contribution of explicit verification versus the task decomposition alone.
  3. Test the Cross-Model Claim: Select a small (e.g., 7B-20B) and a large (e.g., 70B-120B) open-source model. Run both on a ProcLLM benchmark (like Unit Movement) with and without the Human-TN. Plot success rates against problem complexity to see if the smaller model with HTN can match or exceed the larger model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can LLMs learn to construct procedural task networks effectively using a curriculum learning approach?
- **Basis in paper**: The authors state that future work could explore if "LLMs can build task networks from the ground up in a curriculum learning fashion, starting with simple tasks that gradually become more complex."
- **Why unresolved**: The current study only evaluates LLMs generating networks for single, static tasks rather than progressively learning network construction over increasing complexity.
- **What evidence would resolve it**: Empirical results showing an LLM improving its ability to generate valid HTNs when trained or prompted on a curriculum of tasks ranging from simple to difficult.

### Open Question 2
- **Question**: Can RAG or fine-tuning be utilized to automate the distillation of task networks directly from unstructured domain documents?
- **Basis in paper**: The conclusion suggests exploring "methods for using RAG (e.g., (22)) or fine-tuning (e.g., (30)) to improve the distillation of task networks from domain documents."
- **Why unresolved**: The current implementation relies on hand-coded networks or networks generated from a prompt, rather than extracting procedures from sources like manuals or standard operating procedures (SOPs).
- **What evidence would resolve it**: A system demonstration where an LLM ingests technical documentation and produces an HTN that achieves performance parity with human-designed networks.

### Open Question 3
- **Question**: What mechanisms are required to bridge the performance gap between LLM-generated and human-authored Hierarchical Task Networks (HTNs)?
- **Basis in paper**: The authors note that while LLM-created HTNs improve performance, the "results are mixed compared to hand-coded HTNs" because LLMs "generally lack the specificity of human generated networks."
- **Why unresolved**: It is unclear if the limitation is due to prompt design, model reasoning capabilities, or the fundamental difficulty of structuring abstract procedural knowledge without human oversight.
- **What evidence would resolve it**: Ablation studies identifying specific missing elements in LLM-generated HTNs (e.g., depth of decomposition or precondition verification) that, when corrected, match human-HTN performance.

## Limitations
- The performance gains from HTNs are heavily dependent on the availability and quality of well-designed procedural knowledge.
- The verification step introduces a new failure mode where the system can get stuck if the verify-LLM fails to confirm a subtask's effect.
- The reported performance improvements for smaller models may be specific to the model families and hardware configurations tested.

## Confidence
- **High Confidence**: The core mechanism of hierarchical task decomposition via HTNs improving LLM performance is well-supported with clear algorithmic definitions and consistent quantitative improvements.
- **Medium Confidence**: The claim that smaller models can outperform larger ones with HTNs is based on the data but requires broader validation across different model architectures and computational environments.
- **Low Confidence**: The contribution of the verification step to overall performance is not rigorously isolated and lacks ablation studies to quantify its specific impact.

## Next Checks
1. Implement a version of ProcLLM that removes the verify action and the Verify-LLM component, then run it on one or two existing benchmarks to isolate the contribution of explicit verification.
2. Select a different, smaller model (e.g., 7B or 13B) and test it on a ProcLLM benchmark with and without a hand-coded HTN to validate the cross-model amplification claim.
3. Adapt ProcLLM to a novel, practical multi-step task outside the tested domains to evaluate real-world applicability and performance improvement.