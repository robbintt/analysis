---
ver: rpa2
title: Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation
arxiv_id: '2601.22228'
source_url: https://arxiv.org/abs/2601.22228
tags:
- camera
- vlms
- reasoning
- spatial
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of Vision-Language Models (VLMs)
  to perform relative camera pose estimation (RCPE), a fundamental task in computer
  vision requiring the inference of relative camera translation and rotation from
  image pairs. To evaluate this, the authors introduce VRRPI-Bench, a benchmark derived
  from unlabeled egocentric videos with verbalized annotations, and VRRPI-Diag, a
  diagnostic benchmark isolating individual motion degrees of freedom.
---

# Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation

## Quick Facts
- arXiv ID: 2601.22228
- Source URL: https://arxiv.org/abs/2601.22228
- Reference count: 40
- VLMs struggle with relative camera pose estimation, falling short of both classical geometric baselines and human performance

## Executive Summary
This paper investigates whether Vision-Language Models (VLMs) can perform relative camera pose estimation (RCPE), a fundamental task in computer vision that requires inferring the relative camera translation and rotation between two images. The authors introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations, and VRRPI-Diag, a diagnostic benchmark isolating individual motion degrees of freedom. Despite the task's simplicity, VLMs struggle to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations. Even state-of-the-art models like GPT-5 (0.64 F1-score) fall short of classical geometric baselines (0.97) and human performance (0.92). VLMs also exhibit inconsistent multi-image reasoning, with a best consistency of 59.7%.

## Method Summary
The authors evaluate VLMs on relative camera pose estimation through two benchmarks. VRRPI-Bench uses egocentric video data with verbalized annotations to create a real-world testing environment. VRRPI-Diag isolates individual motion degrees of freedom to understand specific weaknesses. VLMs are prompted to classify motion types (e.g., "leftward" or "downward") from image pairs. Performance is measured against classical geometric baselines and human annotators, with consistency metrics evaluating multi-image reasoning capabilities.

## Key Results
- VLMs achieve 0.64 F1-score (GPT-5) on RCPE, far below classical baselines (0.97) and humans (0.92)
- Models struggle specifically with depth changes and roll transformations, relying on 2D heuristics
- Multi-image reasoning consistency peaks at only 59.7%, revealing fundamental limitations in spatial understanding

## Why This Works (Mechanism)
The results demonstrate that VLMs lack robust 3D spatial grounding, instead relying on superficial 2D visual patterns. The diagnostic benchmark reveals that while models can handle simple translational motions, they fail to reason about depth and orientation changes that require understanding of camera geometry. This suggests VLMs have not learned to integrate multiple views into a coherent 3D representation.

## Foundational Learning
- Relative Camera Pose Estimation: The task of determining how one camera view relates to another in 3D space. Needed to evaluate spatial reasoning capabilities; quick check: can the model distinguish between lateral and depth movements?
- Egocentric Video Processing: Using first-person video sequences as data source. Needed for natural motion patterns; quick check: does the model understand the continuous nature of camera movement?
- 3D Spatial Grounding: The ability to connect 2D image observations to 3D spatial relationships. Needed for robust pose estimation; quick check: can the model handle novel camera orientations?

## Architecture Onboarding
Component map: Image encoder -> VLM backbone -> Motion classification head -> Consistency checker
Critical path: Input images → VLM processing → Spatial reasoning → Output classification
Design tradeoffs: VLMs prioritize general language understanding over specialized geometric reasoning, limiting their effectiveness for spatial tasks
Failure signatures: Over-reliance on 2D visual patterns, inability to handle depth changes, inconsistent multi-view reasoning
First experiments: 1) Test on synthetic controlled camera motions, 2) Evaluate geometric fine-tuning impact, 3) Combine with explicit 3D reasoning modules

## Open Questions the Paper Calls Out
None

## Limitations
- VLMs fail to generalize beyond 2D visual heuristics for spatial reasoning
- Significant performance gaps exist for depth changes and roll transformations
- Multi-image reasoning consistency remains below 60%, indicating fundamental limitations

## Confidence
- VLM performance limitations: High confidence (systematic benchmarking, clear baselines)
- Specific weaknesses in depth/roll estimation: High confidence (diagnostic benchmark results)
- Multi-image reasoning inconsistency: High confidence (consistency metrics provided)

## Next Checks
1. Test VLMs on synthetic datasets with controlled camera motions to isolate whether performance issues stem from data quality or fundamental reasoning limitations
2. Evaluate whether additional geometric pre-training or fine-tuning on pose estimation tasks can close the performance gap with classical methods
3. Investigate whether combining VLMs with explicit 3D reasoning modules (as suggested by related work on SpaceMind) can improve performance on these tasks