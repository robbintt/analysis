---
ver: rpa2
title: Calibration without Ground Truth
arxiv_id: '2601.19862'
source_url: https://arxiv.org/abs/2601.19862
tags:
- calibration
- calibrated
- reference
- loss
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving model calibration
  when ground truth labels are unavailable, a scenario becoming increasingly relevant
  as publicly available training data becomes exhausted. The authors propose a label-free
  post-processing framework that improves a miscalibrated primary model using a weaker
  yet better-calibrated reference model.
---

# Calibration without Ground Truth

## Quick Facts
- arXiv ID: 2601.19862
- Source URL: https://arxiv.org/abs/2601.19862
- Reference count: 40
- One-line primary result: Label-free post-processing framework that improves miscalibrated LLM calibration using a better-calibrated reference model, achieving 8%+ Brier Score and 30%+ ECE improvements without ground truth labels

## Executive Summary
This paper addresses the challenge of improving model calibration when ground truth labels are unavailable, a scenario becoming increasingly relevant as publicly available training data becomes exhausted. The authors propose a label-free post-processing framework that improves a miscalibrated primary model using a weaker yet better-calibrated reference model. The core idea is to exploit the contradictions between the two models' predictions through mutual calibration analysis. When models are not mutually calibrated, this inconsistency can be leveraged to improve calibration without requiring ground truth labels.

The method works by projecting the primary model's outputs onto a reference-compatible set of predictors that are mutually calibrated with the reference model, using Bregman divergence as the distance metric. This approach guarantees a strict performance improvement under any proper loss function. Experiments on three different LLM families (QWen3, LLaMA-3.1, and Ministral-3) across various model sizes show significant reductions in both Brier Score and Expected Calibration Error, with improvements exceeding 8% in Brier Score and 30% in ECE compared to the primary models. Notably, despite being label-free, the method achieves calibration performance competitive with supervised baselines while preserving or even slightly improving prediction accuracy in some cases.

## Method Summary
The paper proposes a label-free post-processing framework that improves a miscalibrated primary model using a better-calibrated reference model without requiring ground truth labels. The method estimates the joint distribution of outputs from both models on an unlabeled dataset, then finds the minimal transformation of the primary model that makes it mutually calibrated with the reference model. This transformation is computed by solving a convex optimization problem that minimizes Bregman divergence to a reference-compatible set of predictors. The approach guarantees strict performance improvement under proper loss functions and can be applied as a lightweight post-processing step that preserves the primary model's architecture.

## Key Results
- Achieved 8%+ reduction in Brier Score and 30%+ reduction in Expected Calibration Error compared to primary models
- Demonstrated consistent improvements across three LLM families (QWen3, LLaMA-3.1, Ministral-3) and various model sizes
- Calibration performance competitive with supervised baselines despite being label-free
- Preserved or slightly improved prediction accuracy while significantly improving calibration

## Why This Works (Mechanism)
The method exploits the inconsistency between a miscalibrated primary model and a better-calibrated reference model. When models are not mutually calibrated, this contradiction can be leveraged to improve calibration. The algorithm projects the primary model's outputs onto a reference-compatible set using Bregman divergence, which guarantees a strict performance improvement under proper loss functions. This projection finds the minimal transformation that makes the primary model mutually calibrated with the reference model, effectively transferring the reference model's calibration properties to the primary model without requiring any ground truth labels.

## Foundational Learning

- **Concept: Calibration**
  - **Why needed here:** The entire premise rests on one model being more "calibrated" than another. You must understand that a model is calibrated if, among all predictions made with 70% confidence, the accuracy is actually 70%. Overconfident models are poorly calibrated.
  - **Quick check question:** A model predicts 100 samples will be correct with 90% confidence, and 90 of them are correct. Is it calibrated on these predictions? (Answer: Yes).

- **Concept: Bregman Divergence**
  - **Why needed here:** This is the mathematical "distance" used to measure how much the algorithm changes the model. It connects directly to the "proper loss" function the user wants to optimize (e.g., Squared Error for Brier Score).
  - **Quick check question:** The algorithm projects the primary model's predictions to be "closer" to the reference-compatible set according to which measure? (Answer: Bregman Divergence).

- **Concept: Proper Loss**
  - **Why needed here:** The paper's guarantees only apply to proper loss functions (like Brier Score, Log Loss). These are loss functions where the expected loss is minimized by predicting the true probability. The method's Bregman projection is generated by this loss.
  - **Quick check question:** Why does the method's guarantee of loss reduction depend on using a proper loss function? (Answer: The Bregman divergence used in the algorithm is derived from the convex generator of a proper loss. The Pythagorean theorem for Bregman divergence provides the theoretical guarantee for improvement).

## Architecture Onboarding

- **Component Map:**
  1. Unlabeled dataset D_{Q0,Q1} containing outputs from primary model (Q1) and reference model (Q0)
  2. Joint Distribution Estimator estimates empirical joint distribution over model outputs
  3. Constraint Generator defines convex "reference-compatible set" H_comp based on mutual calibration constraints
  4. Projection Optimizer solves argmin_{hâˆˆH_comp} E[D_l(h(Q1), Q1)] using efficient algorithm
  5. Transformation Function applies learned map h* to new primary model outputs

- **Critical Path:**
  1. Obtain outputs from both primary and reference models on unlabeled dataset
  2. Construct joint distribution of these outputs
  3. Solve convex optimization problem to find Bregman projection onto reference-compatible set
  4. Apply resulting transformation to primary model's outputs during inference

- **Design Tradeoffs:**
  - Label-Free vs. Supervised: Does not need ground-truth labels, a major advantage in data-scarce regimes, but performance is upper-bounded by quality of reference model's calibration
  - Post-processing vs. Retraining: Lightweight post-processing step that preserves primary model's architecture and weights, easy to deploy but does not improve underlying feature extraction
  - Full Distribution Re-estimation vs. Confidence Scaling: Re-estimating full distribution allows for potential accuracy gains but is more complex than simply scaling confidence scores

- **Failure Signatures:**
  - No Improvement: Occurs if primary and reference models are already mutually calibrated, or if reference model is not meaningfully better calibrated
  - Performance Degradation: Theoretical bounds suggest degradation is avoided in worst case under primary assumptions, but poorly calibrated reference could introduce error if its calibration error term dominates
  - Sparse Data: Estimation of joint distribution can be poor if unlabeled dataset is too small or if predictions have low diversity

- **First 3 Experiments:**
  1. **Sanity Check on Synthetics:** Create synthetic dataset with known ground truth distribution and two models where one is deliberately miscalibrated. Run algorithm and verify projected model has lower Brier Score and Log Loss and is closer to ground truth distribution.
  2. **Benchmark Comparison:** Evaluate on standard LLM benchmark (e.g., MMLU-Redux) using strong Instruct model (Q1) and Base counterpart (Q0). Compare Brier Score, ECE, and Accuracy of Q1, Q0, Q1 with Temperature Scaling, and Q1 with label-free method.
  3. **Ablation on Reference Quality:** Systematically degrade calibration of reference model (e.g., by applying adversarial noise function to its probabilities) and observe how improvement guarantees and final performance of transformed primary model degrade, testing sensitivity to Assumption 2.1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Bregman projection framework be extended to an online setting where predictions arrive sequentially, allowing for dynamic updates without re-estimating the full joint distribution?
- Basis in paper: [explicit] Section 6 states, "A promising direction is extending this to online settings, allowing for dynamic updates as predictions arrive sequentially."
- Why unresolved: Current methodology requires batch of unlabeled data to estimate joint distribution, creating latency and preventing real-time adaptation to new data streams
- What evidence would resolve it: Algorithm that incrementally updates transformation map h as new (Q0, Q1) pairs arrive while maintaining worst-case loss reduction guarantee

### Open Question 2
- Question: How can the mutual calibration framework be adapted for token-level text generation, where output space is significantly larger and less structured than multiple-choice settings tested?
- Basis in paper: [explicit] Section 6 notes, "future work could adapt this framework to token-level text generation."
- Why unresolved: Current implementation relies on finite, discrete report space (answers with confidence) suitable for multiple-choice, but text generation involves open vocabularies and semantic equivalence which complicate definition of "mutual calibration"
- What evidence would resolve it: Modification of elicitable property definitions and joint distribution estimation that functions effectively on open-vocabulary text, specifically demonstrating calibration improvements on generative tasks

### Open Question 3
- Question: What is the quantitative "tipping point" regarding reference model's miscalibration at which algorithm ceases to provide strict improvement over primary model?
- Basis in paper: [inferred] Section 6 mentions that "non-negligible miscalibration could weaken our theoretical guarantees," and Theorem B.1 introduces a "Miscalibration Risk" term
- Why unresolved: While Theorem B.1 provides upper bound on risk, precise empirical threshold where miscalibrated reference hurts primary model more than it helps (where risk outweighs "Geometric Gain") is not mapped
- What evidence would resolve it: Experiments systematically varying calibration error of reference model to identify specific error rate where expected loss reduction becomes negative

### Open Question 4
- Question: Does a multi-objective optimization approach exist that effectively balances calibration improvements across distinct proper losses, given current limitation that no single transformation is optimal for all?
- Basis in paper: [explicit] Section 6 states, "there is no universal transformation optimal for all proper losses simultaneously."
- Why unresolved: Current method optimizes for specific proper loss (e.g., Confidence Loss), potentially leaving performance gains on table for other metrics like Brier Score or Log Loss in way not currently characterized
- What evidence would resolve it: Identification of transformation function that lies on Pareto frontier for multiple proper losses, or algorithm that smoothly interpolates between loss-specific projections

## Limitations

- Theoretical guarantees heavily rely on Assumption 2.1 regarding reference model's calibration quality; if reference model is poorly calibrated, method's performance guarantees may not hold
- Optimization procedure for finding Bregman projection onto reference-compatible set is not fully specified, which could impact reproducibility and scalability
- Paper doesn't address computational complexity in detail, and joint distribution estimation step could become bottleneck for very large language models or datasets

## Confidence

**High Confidence:** Core theoretical framework connecting mutual calibration analysis to performance improvement under proper losses. Empirical results showing consistent improvements in Brier Score and ECE across three different LLM families are compelling and well-documented.

**Medium Confidence:** Claim that method achieves calibration performance "competitive with supervised baselines" needs careful interpretation. While true in terms of calibration metrics, comparison assumes access to appropriate reference model, which itself represents form of supervision that may not always be available.

**Low Confidence:** Practical scalability of method for very large language models or extremely large unlabeled datasets. Paper doesn't address computational complexity in detail, and joint distribution estimation step could become bottleneck.

## Next Checks

1. **Reference Model Sensitivity Analysis:** Systematically vary calibration quality of reference model through controlled perturbations and measure impact on primary model's calibration improvement. This would empirically test robustness of theoretical guarantees.

2. **Cross-Domain Generalization:** Evaluate method when unlabeled dataset used for joint distribution estimation comes from different domain than test set. This would reveal whether mutual calibration constraints learned on one distribution transfer effectively.

3. **Computational Efficiency Benchmarking:** Measure wall-clock time and memory usage for joint distribution estimation and Bregman projection steps across different model sizes (e.g., comparing 8B vs 70B parameter models) to assess practical scalability limitations.