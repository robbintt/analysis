---
ver: rpa2
title: 'DiRe: Diversity-promoting Regularization for Dataset Condensation'
arxiv_id: '2512.13083'
source_url: https://arxiv.org/abs/2512.13083
tags:
- dataset
- dire
- diversity
- sre2l
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a diversity-promoting regularizer (DiRe) for
  dataset condensation, addressing the lack of diversity in synthetic datasets generated
  by existing methods. DiRe combines three components: Cosine Diversity loss (CD)
  to spread synthetic embeddings apart, Cosine Distribution Matching loss (CDM) to
  align synthetic directions with real data, and Euclidean Distribution Matching loss
  (EDM) to bring synthetic embeddings close to real ones.'
---

# DiRe: Diversity-promoting Regularization for Dataset Condensation

## Quick Facts
- **arXiv ID:** 2512.13083
- **Source URL:** https://arxiv.org/abs/2512.13083
- **Reference count:** 40
- **Key outcome:** DiRe significantly improves both generalization accuracy and diversity metrics across multiple dataset condensation methods and benchmarks.

## Executive Summary
This paper addresses the critical issue of diversity in synthetic datasets generated by dataset condensation methods. The authors propose DiRe, a diversity-promoting regularizer that can be seamlessly integrated into existing optimization-based dataset condensation approaches. DiRe combines three complementary components—Cosine Diversity loss, Cosine Distribution Matching loss, and Euclidean Distribution Matching loss—to produce synthetic datasets that are both diverse and representative of the original data distribution. Extensive experiments show that DiRe consistently improves performance across multiple state-of-the-art methods (SRe2L, DWA, CDA, UFC, G-VBSM, DELT, MTT, DM) on benchmark datasets including CIFAR-10/100, Tiny ImageNet, and ImageNet-1K.

## Method Summary
DiRe is a plug-in regularizer for optimization-based dataset condensation that operates on penultimate layer embeddings. It consists of three components: (1) Cosine Diversity loss minimizes pairwise cosine similarity among synthetic embeddings to spread them apart in feature space; (2) Cosine Distribution Matching loss aligns synthetic embeddings with real data directions while maintaining diversity; and (3) Euclidean Distribution Matching loss anchors synthetic samples near real data points to prevent drift. The total loss is L_total = L_ce + L_bn + r_c·(l_cos_div + l_cos_dm) + r_e·l_euc_dm. DiRe is computationally efficient using torchmetrics for pairwise operations and adds approximately 17% overhead to synthesis time.

## Key Results
- On CIFAR-100 with IPC=50, SRe2L accuracy improves from 52.2% to 63.4% with DiRe
- Coverage increases from 14.87% to 23.12% on CIFAR-100 with IPC=50
- DiRe consistently improves diversity metrics (Vendi Score, Coverage, intra-class cosine similarity) across all tested methods and datasets
- Cross-architecture experiments show improved generalization across various CNN and transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing pairwise cosine similarity among synthetic embeddings increases their spread in feature space, improving representation of the original data manifold.
- **Mechanism:** The Cosine Diversity (CD) loss minimizes S_cos(E_syn, E_syn), pushing synthetic embeddings toward orthogonality, which relates to maximizing volume spanned by vectors in feature space.
- **Core assumption:** The penultimate layer embeddings capture semantically meaningful structure; orthogonality in this space correlates with semantic diversity.
- **Evidence anchors:** [abstract] "Cosine Diversity loss (CD) to spread synthetic embeddings apart"; [Section 4.8, Table 14] CD alone improves Vendi Score (0.50 normalized) and Coverage (0.41 normalized)
- **Break condition:** If embeddings are not semantically aligned, pushing them apart may produce diverse but meaningless samples.

### Mechanism 2
- **Claim:** Aligning synthetic embeddings with real data directions via cosine similarity preserves semantic relevance while maintaining diversity.
- **Mechanism:** The Cosine Distribution Matching (CDM) loss minimizes 1 - S_cos(E_syn, E_real), encouraging synthetic samples to share directional alignment with real samples while CD pushes them apart from each other.
- **Core assumption:** Direction in embedding space correlates with semantic content; magnitude can vary independently.
- **Evidence anchors:** [abstract] "Cosine Distribution Matching loss (CDM) to align synthetic directions with real data"; [Appendix B.3] "Minimizing both losses jointly aligns synthetic data with real data in: Direction (via cosine similarity)"
- **Break condition:** If real data has highly clustered embeddings, directional alignment may reintroduce redundancy.

### Mechanism 3
- **Claim:** Euclidean distance matching anchors synthetic samples near real data points, preventing drift into unrepresentative regions of feature space.
- **Mechanism:** The Euclidean Distribution Matching (EDM) loss minimizes D_euc(E_syn, E_real), pulling synthetic samples toward real data in magnitude, triangulating with CDM (direction) and CD (spread).
- **Core assumption:** Euclidean proximity in embedding space correlates with functional similarity for training.
- **Evidence anchors:** [abstract] "Euclidean Distribution Matching loss (EDM) to bring synthetic embeddings close to real ones"; [Section 4.8, Table 14] EDM has strongest impact on Coverage (1.00 normalized)
- **Break condition:** If embedding space is anisotropic, Euclidean distance may be dominated by high-variance dimensions, misguiding synthesis.

## Foundational Learning

- **Concept: Dataset Condensation (DC)**
  - **Why needed here:** DiRe is a plug-in regularizer for DC methods; understanding the bi-level optimization clarifies where DiRe fits.
  - **Quick check question:** Can you explain why DC differs from coreset selection (subset vs. synthetic data)?

- **Concept: Embedding Space Geometry**
  - **Why needed here:** All three DiRe components operate on penultimate layer embeddings; intuition for cosine similarity vs. Euclidean distance is essential.
  - **Quick check question:** Given two vectors with cosine similarity 0.9, what does low Euclidean distance imply vs. high Euclidean distance?

- **Concept: Diversity Metrics (Coverage, Vendi Score)**
  - **Why needed here:** The paper evaluates diversity quantitatively; understanding these metrics ensures proper benchmarking.
  - **Quick check question:** Why might intra-class cosine similarity alone be insufficient to measure diversity?

## Architecture Onboarding

- **Component map:** Real dataset V -> Pretrained teacher f_θ -> Penultimate layer h_θ (512-dim avgpool) -> DiRe losses (CD, CDM, EDM) -> Synthetic dataset S_T
- **Critical path:**
  1. Precompute real embeddings E_real_c for all classes (one-time forward pass)
  2. Per iteration: compute E_syn_c, then CD, CDM, EDM losses
  3. Use torchmetrics for O(K²) pairwise operations (~30× speedup)
  4. Tune weights r_c (cosine) and r_e (Euclidean) via validation
- **Design tradeoffs:**
  - **Time vs. diversity:** Section 4.9 reports ~17% overhead over SRe2L for CIFAR-100 synthesis
  - **Coverage vs. similarity:** Table 14 shows EDM maximizes Coverage but may increase intra-class similarity; CD reduces similarity but has weaker Coverage
  - **Memory:** Table 19 shows 13GB vs. 10.6GB for ImageNet-1K (IPC=10)
- **Failure signatures:**
  - Synthetic images appear visually similar despite DiRe → check r_c weight, may be too low
  - Accuracy drops → CDM/EDM may be too weak, allowing drift from real data manifold
  - OOM on large-scale datasets → batch real embeddings per class; use precomputed E_real
- **First 3 experiments:**
  1. Reproduce CIFAR-10/100 results with SRe2L + DiRe (Tables 2, 4); verify ~10-15% accuracy gain
  2. Ablation study (Table 14 style): isolate CD, CDM, EDM on Tiny ImageNet IPC=10; confirm EDM drives Coverage, CD reduces similarity
  3. Cross-architecture test: synthesize with ResNet-18 teacher, evaluate on ViT (Table 10); target ~10-20% gain over vanilla SRe2L

## Open Questions the Paper Calls Out

- **Can Determinantal Point Processes (DPP) or submodular functions effectively replace pairwise cosine similarity in the DiRe regularizer?**
  - Basis in paper: [explicit] The conclusion states future work includes "using functions such as the Determinantal Point Process objective or other submodular functions instead of cosine similarity for devising new diversity regularizers."
  - Why unresolved: The current implementation relies on cosine similarity and Euclidean distance; the authors have not yet explored these alternative diversity metrics.
  - What evidence would resolve it: Comparative experiments showing that a DPP-based regularizer yields higher Vendi Scores or generalization accuracy than the current formulation.

- **Can coresets be utilized to reduce the computational overhead of Euclidean distance calculations within DiRe?**
  - Basis in paper: [explicit] The conclusion suggests "making the Euclidean distance computation in DiRe more efficient using coresets."
  - Why unresolved: The current pairwise Euclidean distance calculation contributes to the increased synthesis time (approx. 17% overhead over SRe2L) noted in the timing analysis.
  - What evidence would resolve it: An implementation using coresets that significantly lowers synthesis time per image (IPC) without degrading the Coverage or accuracy metrics.

- **Can DiRe be effectively adapted to improve diversity in generative modeling tasks?**
  - Basis in paper: [explicit] The conclusion lists "applying DiRe in the field of generative modeling" as an avenue for future work.
  - Why unresolved: The paper currently validates DiRe only on dataset condensation for discriminative tasks (image classification).
  - What evidence would resolve it: Experiments applying DiRe to GAN or Diffusion Model training, demonstrating improved mode coverage or reduced sample redundancy.

## Limitations
- The computational overhead (17% on CIFAR-100, 23% on ImageNet) may become prohibitive for larger-scale applications
- The optimal weighting between diversity and distribution matching losses (r_c, r_e) appears to require per-method tuning
- The choice of penultimate layer embeddings as the diversity metric space may not be optimal for all architectures or tasks

## Confidence
- **High:** The effectiveness of DiRe across multiple baseline methods (SRe2L, DWA, CDA, etc.)
- **Medium:** The mechanism claims about how diversity and distribution matching interact
- **Medium:** The generalizability across different architectures and dataset scales

## Next Checks
1. Conduct ablation studies isolating each DiRe component (CD, CDM, EDM) across multiple baselines to verify claimed mechanisms
2. Test DiRe with non-CNN architectures (Vision Transformers, MLP-Mixers) as teachers to validate cross-architecture claims
3. Measure the impact of DiRe on downstream tasks beyond classification (object detection, segmentation) to assess broader applicability