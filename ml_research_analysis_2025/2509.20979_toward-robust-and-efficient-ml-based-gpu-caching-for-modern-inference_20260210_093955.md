---
ver: rpa2
title: Toward Robust and Efficient ML-Based GPU Caching for Modern Inference
arxiv_id: '2509.20979'
source_url: https://arxiv.org/abs/2509.20979
tags:
- cache
- laru
- predictions
- caching
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Toward Robust and Efficient ML-Based GPU Caching for Modern Inference

## Quick Facts
- arXiv ID: 2509.20979
- Source URL: https://arxiv.org/abs/2509.20979
- Reference count: 40
- Primary result: LARU algorithm achieves up to 24.2% throughput gain over LRU while maintaining robustness to prediction errors.

## Executive Summary
This paper introduces LCR (Learning-based Cache Replacement), a framework for GPU caching in modern inference workloads. The key innovation is LARU (Learning-Aware Robustified Update), an algorithm that uses ML predictions while guaranteeing robustness against prediction errors. Unlike previous approaches that fail catastrophically when predictions are wrong, LARU adapts in real-time to prediction accuracy through online error estimation, maintaining near-optimal performance when predictions are accurate and degrading gracefully to near-LRU performance when they're not.

## Method Summary
The paper proposes LARU, a learning-aware cache replacement algorithm that combines ML predictions with classical LRU behavior. The method uses LightGBM as the predictor, trained online with features like request intervals and decayed counters. LARU operates in phases and detects prediction-induced misses by checking if evicted items are needed before older cached items. When errors are detected, it dynamically reduces the influence of predictions through an adaptive confidence parameter λ. The framework supports both synchronous (LLM) and asynchronous (DLRM) prediction modes to minimize overhead. Cache indices are stored in GPU HBM for DLRMs (SlabHash) and DRAM for LLMs (RadixTree).

## Key Results
- LARU achieves up to 24.2% throughput improvement over LRU in DLRM workloads
- Maintains competitive hit rates while providing robustness guarantees
- Gracefully degrades to near-LRU performance when predictions are inaccurate
- Supports both synchronous and asynchronous prediction modes for different workload characteristics

## Why This Works (Mechanism)

### Mechanism 1: Phase-Based Error Detection
LARU detects prediction errors in real-time by identifying "prediction-induced misses," triggering a fallback to LRU behavior. The algorithm operates in phases, checking if a requested item x was evicted earlier in the current phase while older items remained unrequested. If true, this indicates the prediction erroneously evicted x instead of a less useful item. This prevents continuous cache misses from a single misprediction.

### Mechanism 2: Adaptive Confidence Parameter
LARU dynamically throttles the influence of the predictor using an adaptive confidence parameter λ. Upon detecting a prediction-induced miss, LARU reduces λ, shrinking the eviction candidate set size. As errors accumulate, λ approaches 1, forcing LRU behavior. When predictions are accurate, λ remains high, allowing the predictor to select from a larger candidate set.

### Mechanism 3: Workload-Aware Prediction Modes
The framework isolates prediction overhead using synchronous vs. asynchronous modes tailored to workload concurrency. For high-concurrency DLRMs, LCR uses asynchronous predictions where indices reside in HBM. For low-concurrency LLMs, it uses synchronous predictions where indices reside in DRAM and prediction latency is negligible relative to TTFT.

## Foundational Learning

**Competitive Ratio (Consistency vs. Robustness)**
- Why needed: This is the theoretical foundation for LARU, explaining why the paper optimizes for worst-case bounds rather than just average accuracy.
- Quick check: Why is a "robust" algorithm with a bounded competitive ratio preferred over a purely accurate ML model in a production serving system?

**Belady's Optimal Eviction Policy**
- Why needed: The ML predictor attempts to approximate Belady's rule (evict the item reused furthest in the future).
- Quick check: If you know the future, which item do you evict?

**Gradient Boosted Decision Trees (LightGBM)**
- Why needed: The paper selects LightGBM specifically for its low latency and ability to handle online training.
- Quick check: Why is inference latency critical for the predictor in a GPU caching loop?

## Architecture Onboarding

**Component map**: Request -> Cache Miss -> Check Phase/History -> Update λ (if error) -> Invoke Predictor (if safe) -> Select Eviction Candidate -> Fetch Data

**Critical path**: Request flows through cache miss detection, phase checking, potential λ update, predictor invocation, eviction selection, and data fetching.

**Design tradeoffs**: 
- Index Location: HBM storage is faster but consumes valuable memory; DRAM storage saves HBM but adds latency.
- Candidate Set Size: Larger l allows better optimization but increases risk of evicting useful items if predictor is noisy.

**Failure signatures**:
- Oscillation: λ fluctuates rapidly, indicating a noisy predictor or unstable workload pattern.
- Throughput Collapse: System performs worse than LRU, likely due to blocking predictor calls in critical path.

**First 3 experiments**:
1. Robustness Verification: Inject controlled noise into the system and plot Hit Rate vs. Noise Probability.
2. End-to-End Latency: Measure P99 TTFT (LLM) and SLS Throughput (DLRM) against LRU and FPB baselines.
3. Overhead Analysis: Profile LightGBM predictor and LARU logic latency to ensure it's <2ms (DLRM) or negligible compared to TTFT (LLM).

## Open Questions the Paper Calls Out

**Open Question 1**: Can the LCR framework be effectively adapted for GNN training and approximate nearest-neighbor search?
- Basis: Section 7 states the design principles "extend naturally to broader scenarios."
- Why unresolved: Evaluation is restricted to DLRM and LLM inference scenarios.

**Open Question 2**: Does LARU maintain its consistency and robustness guarantees when applied to variable-size cache items?
- Basis: Section 1 explicitly scopes the problem to "fixed-size cache items."
- Why unresolved: Theoretical analysis relies on uniform item size, and bounds may change with variable sizes.

**Open Question 3**: What is the optimal trade-off between predictor inference latency and accuracy for LARU in high-concurrency DLRM scenarios?
- Basis: Section 3.2 notes deep models are rejected due to latency but doesn't quantify the threshold.
- Why unresolved: Unclear if more accurate (slower) predictors could yield better net throughput.

## Limitations
- The framework is currently limited to fixed-size cache items, limiting applicability to traditional paging systems.
- Reliance on proprietary datasets (AD-CTR-User and Online-QA) for key performance claims introduces uncertainty about generalizability.
- LightGBM hyperparameters are unspecified, potentially affecting reproducibility of results.

## Confidence

**High Confidence**: Core mechanism of LARU (phase-based error detection, adaptive λ decay) is clearly described and theoretically sound.

**Medium Confidence**: Experimental results showing LARU's superiority are convincing but limited by lack of hyperparameter details and proprietary datasets.

**Low Confidence**: Claims regarding LightGBM predictor overhead and SlabHash/RadixTree effectiveness are not fully validated; specific implementation details are not provided.

## Next Checks

1. **Noise Injection Robustness Test**: Implement controlled experiment where predictor's accuracy is systematically degraded by flipping eviction predictions. Measure LARU's hit rate and latency as noise increases.

2. **Hyperparameter Sensitivity Analysis**: Conduct grid search over LightGBM's key hyperparameters (number of trees, tree depth, learning rate) to determine their impact on LARU's performance.

3. **Open-Source Dataset Validation**: Replicate experiments using only publicly available datasets (QB-video and Aibrix-Synthetic) to assess whether reported performance gains are achievable without proprietary datasets.