---
ver: rpa2
title: 'SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions'
arxiv_id: '2506.23046'
source_url: https://arxiv.org/abs/2506.23046
tags:
- crafting
- table
- planks
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SoMi-ToM, a novel benchmark for evaluating
  Theory of Mind (ToM) in embodied multi-agent interactions within Minecraft. Unlike
  existing text-based benchmarks, SoMi-ToM leverages rich multimodal data (visual,
  dialogue, actions) from complex social scenarios involving collaboration and obstruction.
---

# SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions

## Quick Facts
- arXiv ID: 2506.23046
- Source URL: https://arxiv.org/abs/2506.23046
- Reference count: 40
- State-of-the-art LVLMs achieve 40-59% lower accuracy than humans on Theory of Mind tasks in embodied multi-agent interactions

## Executive Summary
This paper introduces SoMi-ToM, a novel benchmark for evaluating Theory of Mind (ToM) capabilities in embodied multi-agent interactions within Minecraft. Unlike existing text-based benchmarks, SoMi-ToM leverages rich multimodal data from complex social scenarios involving collaboration and obstruction. The benchmark supports both first-person real-time state inference and third-person goal/behavior inference after task completion. Human evaluation shows near-perfect performance (90-93% accuracy), while state-of-the-art LVLMs perform significantly worse, highlighting the need for improved ToM capabilities in embodied AI systems.

## Method Summary
The benchmark evaluates LVLMs on 1225 multiple-choice questions derived from 35 third-person videos and 363 first-person screenshots, spanning two evaluation modes: first-person (real-time state inference) and third-person (post-task goal/behavior inference). Models receive multimodal inputs including visual data, dialogue history, and system feedback. Evaluation uses majority voting over 3 random option orderings with 5 repetitions each, testing both vanilla prompting and Chain-of-Thought approaches across seven different LVLMs.

## Key Results
- LVLMs show significant performance gaps compared to humans (40-59% accuracy deficit)
- First-person evaluation reveals larger accuracy gaps (40.1%) than third-person (26.4%)
- Models struggle with resource tracking, distinguishing plans from actual behavior, and understanding hierarchical task structures
- Chain-of-Thought prompting aids quantitative state calculation but degrades qualitative holistic inference for most models

## Why This Works (Mechanism)

### Mechanism 1: Perspective-Dependent State Inference
First-person evaluation forces real-time state updates based on transient system feedback and dialogue history, while third-person evaluation requires integrating longer temporal sequences to infer completed goals. The performance gap (40.1% vs 26.4%) suggests current LVLMs lack the architecture to switch between these modes effectively.

### Mechanism 2: Resource Tracking Failure via Intention-Behavior Decoupling
LVLMs fail state inference because they overweight verbal intentions over actual system feedback, creating semantic representations of plans but failing to decrement state variables when action execution deviates from the plan. This indicates a lack of simulation or game engine module to verify actions.

### Mechanism 3: Negative Transfer of CoT on Abstract Inference
While Chain-of-Thought aids quantitative state calculation (1st person), it degrades qualitative holistic inference (3rd person) for most models by encouraging over-reasoning and forcing attention to irrelevant details or hallucinated logical connections.

## Foundational Learning

- **Concept: Theory of Mind (ToM) in AI**
  - Why needed here: The entire benchmark tests if AI can infer "beliefs, goals, behaviors, attitudes" of other agents, requiring distinction between knowledge and belief.
  - Quick check question: If Agent A collects wood but the system bugs and doesn't add it to their inventory, what is Agent A's belief vs the ground truth?

- **Concept: Embodied Multi-Agent Interaction**
  - Why needed here: The environment involves physics, partial observability, and asynchronous communication where state changes based on actions, not just dialogue.
  - Quick check question: Why does the paper require a "three-tiered asynchronous communication architecture" instead of a standard turn-based chat loop?

- **Concept: Hierarchical Task Planning**
  - Why needed here: LVLMs fail to identify hierarchical goal structures (e.g., "collecting logs" is a sub-goal of "craft a boat"), requiring understanding of task decomposition.
  - Quick check question: In the crafting tree, why is failing to track the "Crafting Table" dependency critical for the "Stone Pickaxe" goal?

## Architecture Onboarding

- **Component map:** SoMi Environment -> AAct Module -> Multi-agents Module -> SoMi-Mindcraft
- **Critical path:** Data Generation (Run SoMi Environment -> Agents interact -> Capture logs/video) -> Annotation (Experts annotate QA pairs -> Validate with humans) -> Evaluation (Feed multimodal inputs to Target LVLM -> Score accuracy)
- **Design tradeoffs:** Simplified crafting goals to avoid excessive noise; first-person uses sequential screenshots for easier state alignment; third-person uses video requiring longer context; token limits constrain video frames per model
- **Failure signatures:** "Inventory Hallucination" (reports inventory based on last command not system feedback), "Spatial Confusion" (conflates agent identities), "Plan-Action Drift" (scores success based on verbal plan rather than actual outcome)
- **First 3 experiments:** 1) Run evaluation script on GPT-4o to verify 35-40% accuracy range; 2) Mask "system feedback" lines to measure reliance on execution logs; 3) Evaluate on incomplete goal tasks to see if model hallucinates or recognizes incomplete state

## Open Questions the Paper Calls Out
- How does introducing detailed social background information for agents affect LVLM ToM reasoning in embodied settings?
- Can LVLMs maintain ToM capabilities when generalized to complex, long-horizon tasks that exceed current simplified crafting goals?
- Does the performance gap between humans and LVLMs widen or narrow when expanding evaluation to include ToM concepts beyond state, goal, and behavior inference?

## Limitations
- CoT prompting variability not fully specified, making degradation causes unclear
- Third-person video evaluation constrained by token limits with different models receiving different numbers of frames
- Error analysis relies on manual inspection of 100 samples per task, potentially missing systematic biases

## Confidence

- **High Confidence:** Fundamental finding that LVLMs struggle with resource tracking and plan-behavior decoupling (consistently observed across models)
- **Medium Confidence:** Perspective-dependent performance gap and its interpretation (numerical gap clear but causal mechanism requires validation)
- **Low Confidence:** CoT negative transfer explanation (degradation noted but insufficient evidence about why specific models fail)

## Next Checks

1. **Control for Token Constraints:** Re-run third-person evaluation with uniform frame sampling across all models to isolate whether performance differences stem from input quality or model capability.

2. **Probe the Simulation Gap:** Design a minimal benchmark where models must track inventory through a single crafting operation, then systematically remove dialogue context to measure reliance on verbal plans vs. system feedback.

3. **Test CoT Template Sensitivity:** Create multiple CoT variants (minimal vs. detailed reasoning) and evaluate on state-tracking vs. holistic inference tasks to determine if negative transfer is template-dependent or inherent to video understanding.