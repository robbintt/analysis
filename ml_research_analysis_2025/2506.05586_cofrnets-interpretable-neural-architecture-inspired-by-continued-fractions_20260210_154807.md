---
ver: rpa2
title: 'CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions'
arxiv_id: '2506.05586'
source_url: https://arxiv.org/abs/2506.05586
tags:
- functions
- neural
- function
- such
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoFrNets, a novel neural architecture inspired
  by continued fractions, which are known for their fast convergence properties in
  number theory. The architecture represents each layer as a linear function of the
  input with a reciprocal activation, creating a ladder-like structure.
---

# CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions

## Quick Facts
- arXiv ID: 2506.05586
- Source URL: https://arxiv.org/abs/2506.05586
- Reference count: 40
- Key outcome: CoFrNets achieve competitive performance on real datasets while maintaining interpretability through continued fraction structure

## Executive Summary
CoFrNets introduce a novel neural architecture inspired by continued fractions, leveraging their fast convergence properties to create interpretable models. Each layer applies a linear function to the input followed by a reciprocal activation, forming a ladder-like structure that can be interpreted through continued fraction theory. The architecture proves universal approximation capability using a distinct proof strategy from standard neural networks, and demonstrates competitive performance on diverse datasets while enabling feature attribution analysis and recovery of functional forms.

## Method Summary
The architecture represents each layer as a linear function of the input with a reciprocal activation, creating a ladder-like structure where each rung computes f(x) = a₀ + 1/(a₁ + 1/(a₂ + ...)) with aₖ = wₖᵀx. To prevent division by zero, the activation is modified to sgn(z)/max(|z|, ε). Three variants are introduced: CoFrNet-F (full input to all ladders), CoFrNet-D (diagonal - single feature per ladder), and CoFrNet-DL (hybrid). Universal approximation is proven using Stone-Weierstrass theorem applied to polynomials of linear functions, and two interpretation strategies leverage continued fraction theory using continuants and power series expansions.

## Key Results
- On seven real datasets spanning tabular, text, and image modalities, CoFrNets achieve competitive or superior performance compared to interpretable models and MLPs
- Single-ladder CoFrNet-F with depth 6 recovers functional forms from synthetic data with MAPE <15%
- CoFrNet-D matches GAM/NAM interpretability while CoFrNet-DL approaches MLP accuracy on tabular data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued fraction ladder structures can approximate complex functions with compact parameterization.
- Mechanism: Each ladder computes f(x) = a₀ + 1/(a₁ + 1/(a₂ + ...)) where each aₖ = wₖᵀx is a linear function of input. This recursive reciprocal structure creates rational functions capable of representing power series expansions, with a one-to-one correspondence between CF coefficients and polynomial terms.
- Core assumption: The input domain is bounded and continuous; the function being approximated is smooth enough for polynomial approximation.
- Evidence anchors: [abstract] states efficient training and interpretation; [section 3, equation 1] shows equivalence between CF form and power series; [corpus] has weak external validation.

### Mechanism 2
- Claim: Reciprocal activation (1/z) with pole mitigation enables stable gradient-based training.
- Mechanism: The nonlinear activation z → 1/z is applied at each ladder rung. To prevent division by zero (poles), the activation is modified to sgn(z)/max(|z|, ε) where ε ≈ 0.1 in experiments. This preserves the rational function structure while ensuring numerical stability.
- Core assumption: ε is set large enough to avoid singularities but small enough to preserve approximation power.
- Evidence anchors: [section 4, "Handling Poles"] describes pole mitigation; [section 6.2] uses ε=0.1; [corpus] lacks validation in neighboring literature.

### Mechanism 3
- Claim: Linear combinations of finite-depth continued fractions are universal approximators.
- Mechanism: The proof strategy proceeds in three steps: (1) show polynomials of linear functions form a unital subalgebra that separates points on [0,1]^p, (2) apply Stone-Weierstrass to prove PL is dense in C(χ, ℝ), (3) show PL ⊂ linear combinations of CFs (each monomial can be represented as difference of two CFs via Euler's formula).
- Core assumption: The domain is compact; the target function is continuous and bounded.
- Evidence anchors: [section 5, Theorem 2] proves density; [section 5, equation 9-10] shows Euler's formula; [corpus] lacks external validation.

## Foundational Learning

- **Concept: Continued Fractions in Number Theory**
  - Why needed here: The architecture is directly derived from canonical CF form; understanding convergence properties (best rational approximation) motivates the design.
  - Quick check question: Given a CF a₀ + 1/(a₁ + 1/a₂), compute its second convergent (truncation after a₁). Answer: (a₀a₁ + 1)/a₁

- **Concept: Stone-Weierstrass Theorem**
  - Why needed here: The universal approximation proof relies on this theorem rather than traditional Cybenko-style arguments.
  - Quick check question: What three properties must a function class P satisfy for Stone-Weierstrass to guarantee density in C(χ, ℝ)? Answer: (1) unital subalgebra, (2) separates points, (3) domain is compact

- **Concept: Continuants and Recursive Polynomial Computation**
  - Why needed here: Interpretation strategy IC uses continuants K_k to compute feature attributions in O(dp) time.
  - Quick check question: Given K₀=1, K₁(a_d)=a_d, and K_k = a_{d-k+1}·K_{k-1} + K_{k-2}, compute K₂(a₁, a₂). Answer: a₁a₂ + 1

## Architecture Onboarding

- **Component map:**
  - Input layer: x ∈ ℝ^p passed to ALL layers (unlike MLP where input goes only to first layer)
  - Ladder (depth d): Sequence of linear transforms wₖᵀx followed by reciprocal activation, composed recursively
  - Multi-ladder output: Linear combination α·[f₁(x; w^(1)), ..., f_L(x; w^(L))] where each f is a ladder
  - Three variants: CoFrNet-F (full input to all ladders), CoFrNet-D (diagonal—single feature per ladder, additive model), CoFrNet-DL (hybrid with diagonal + increasing-depth full ladders)

- **Critical path:**
  1. Initialize weights wₖ for each layer (standard neural net initialization)
  2. Forward pass: compute aₖ = wₖᵀx at each layer, then compose with reciprocal activations bottom-up
  3. Handle poles: apply sgn(z)/max(|z|, ε) at each reciprocal operation
  4. Aggregate ladders: linear combination with learnable coefficients
  5. Interpretation (optional): compute continuants for feature attribution or power series for global interpretation

- **Design tradeoffs:**
  - CoFrNet-F: Most expressive, hardest to interpret (requires IC or IPS methods)
  - CoFrNet-D: Interpretable as additive model (univariate functions), but cannot capture interactions
  - CoFrNet-DL: Balance—diagonal ladders for main effects + shallow full ladders for low-order interactions
  - Depth vs. ladders: More depth → higher-degree polynomials; more ladders → more terms in linear combination
  - ε selection: Larger ε → more stable training, less expressive; paper uses ε=0.1

- **Failure signatures:**
  - Exploding gradients near poles (ε too small or denominator collapses)
  - Underfitting on highly non-rational functions (e.g., functions with essential singularities)
  - CoFrNet-D failing on pure interaction tasks (e.g., x₁·x₂ with no main effects)
  - Training divergence if learning rate too high relative to ε

- **First 3 experiments:**
  1. **Synthetic function recovery**: Train single-ladder CoFrNet-F (depth=6) on Rosenbrock or Matyas function with 300 samples; verify that IPS recovers coefficients matching the true function (target: MAPE <15%, coefficient recovery within 0.1 for dominant terms).
  2. **Ablation across variants**: Compare CoFrNet-F, CoFrNet-D, and CoFrNet-DL on tabular dataset (e.g., Credit Card); measure accuracy vs. interpretability (CoFrNet-D should match GAM/NAM; CoFrNet-DL should approach MLP accuracy while remaining interpretable).
  3. **Pole sensitivity**: Sweep ε ∈ {0.01, 0.05, 0.1, 0.5} on synthetic rational function f(x) = 1/(x + 0.01); monitor training stability and final MAPE to identify stability-accuracy tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do CoFrNets exhibit superior adversarial robustness compared to standard neural networks?
- Basis in paper: [explicit] The authors state, "We hypothesize that CoFrNets have favorable adversarial robustness properties... and we intend to test this in future work."
- Why unresolved: The current study focuses on interpretability and approximation accuracy but does not evaluate the architecture's resilience to adversarial attacks.
- What evidence would resolve it: Analytical computation of robustness metrics (e.g., CLEVER score) or empirical evaluation against standard adversarial attacks compared to baseline models.

### Open Question 2
- Question: Can alternative training strategies, such as incremental layer fitting or boosting ladders, outperform standard joint backpropagation?
- Basis in paper: [explicit] The paper notes that strategies where "each ladder could be built rung by rung" or "linear combination of ladders could be built incrementally" could be advantageous.
- Why unresolved: All reported experimental results rely on standard joint training using Adam optimizer and backpropagation.
- What evidence would resolve it: Comparative experiments measuring convergence speed and final test accuracy between joint training and the proposed incremental or boosting strategies.

### Open Question 3
- Question: How can architectural tactics like convolutional blocks be integrated with CoFrNets, particularly leveraging continued fraction filter design?
- Basis in paper: [explicit] The authors suggest, "It is possible that convolutional blocks may have a natural implementation based on older filter design literature in signal processing that builds upon CFs."
- Why unresolved: The paper evaluates the base architecture variants (CoFrNet-F, -D, -DL) without incorporating convolutional or pooling layers.
- What evidence would resolve it: Performance evaluation of hybrid CoFrNet-CNN architectures on image datasets compared to standard CNNs.

## Limitations

- The universal approximation proof does not bound the number of ladders required for practical approximation accuracy
- Pole handling through ε=0.1 is empirically justified but lacks theoretical grounding for optimal selection
- Interpretation methods assume the model is well-trained and may degrade with poor convergence or high-depth architectures

## Confidence

- **High confidence**: Universal approximation theorem (mathematical proof provided), competitive performance on real datasets (empirical validation), stable training with pole mitigation (experimental evidence)
- **Medium confidence**: Interpretation quality and feature attribution accuracy (only demonstrated on synthetic data), optimal depth/ε selection (heuristic choices with limited ablation), scalability to high-dimensional data (experiments limited to p≤128)

## Next Checks

1. **Bound the approximation rate**: Determine the minimum number of ladders L needed to achieve ε-accuracy for benchmark functions (e.g., sin, exp, step functions), comparing CoFrNets to MLP depth requirements
2. **Robustness to pole sensitivity**: Systematically sweep ε across orders of magnitude on synthetic rational functions; measure training stability and final approximation error to identify the stability-accuracy tradeoff curve
3. **Interpretability validation**: On high-dimensional real datasets, apply both interpretation methods (IC and IPS) and compare feature attributions to ground-truth feature importance from domain knowledge or ablation studies