---
ver: rpa2
title: 'MicroNAS: An Automated Framework for Developing a Fall Detection System'
arxiv_id: '2504.07397'
source_url: https://arxiv.org/abs/2504.07397
tags:
- data
- fall
- https
- memory
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MicroNAS is an automated neural architecture search tool designed
  to create models optimized for microcontrollers with small memory resources, specifically
  the ESP32 with 320 KB of memory. The key innovation is a novel method for optimizing
  convolutional neural network and gated recurrent unit architectures by considering
  the memory size of the target microcontroller as a guide, eliminating the need for
  separate pruning stages.
---

# MicroNAS: An Automated Framework for Developing a Fall Detection System

## Quick Facts
- arXiv ID: 2504.07397
- Source URL: https://arxiv.org/abs/2504.07397
- Reference count: 40
- Primary result: MicroNAS generates memory-constrained models optimized for microcontrollers, achieving 0.66 ± 0.18 F1-score on fall detection for lower-limb amputees.

## Executive Summary
MicroNAS is an automated neural architecture search tool designed to create models optimized for microcontrollers with small memory resources, specifically the ESP32 with 320 KB of memory. The key innovation is a novel method for optimizing convolutional neural network and gated recurrent unit architectures by considering the memory size of the target microcontroller as a guide, eliminating the need for separate pruning stages. A comparison with traditional two-stage pruning methods showed MicroNAS achieves comparable F1-scores while using less memory. When applied to a fall detection system for lower-limb amputees, MicroNAS models achieved higher F1-scores (0.66 ± 0.18) than ensemble methods and H2O AutoML (0.22 ± 0.07). The 1D CNN and GRU models generated by MicroNAS outperformed other approaches in handling class imbalance in the dataset, demonstrating the effectiveness of the framework for real-time fall detection system development on resource-constrained devices.

## Method Summary
MicroNAS automates model design by searching over depth, width, and temporal resolution under memory constraints rather than manual tuning. IMU data (3-axis accelerometer, 3-axis gyroscope at 100 Hz) is segmented into 1.2-second windows (120 samples) with 90% overlap. The framework uses random search to explore layer sequences and hyperparameters (filter counts, kernel sizes, strides, pooling configurations) but only evaluates architectures that satisfy the 320 KB memory constraint for ESP32-S2. Weighted binary cross-entropy loss is applied to handle the severe class imbalance (~58:1 ADL to Fall ratio). The best-performing model under constraint is selected based on F1-score for the minority fall class.

## Key Results
- MicroNAS models achieved F1-score of 0.66 ± 0.18 on fall detection for lower-limb amputees, outperforming ensemble methods (0.44) and H2O AutoML (0.22)
- Generated 1D CNN models used 198.22 ± 89 KB memory compared to pruned models at 261.60 ± 56 KB (p < 0.01)
- Weighted loss handling of class imbalance proved more effective than undersampling (RUSBoost, EasyEnsemble) or oversampling (H2O AutoML) approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained search space with memory-aware architecture selection produces more efficient models than unconstrained search followed by post-hoc pruning.
- Mechanism: MicroNAS uses the target microcontroller's memory budget (320 KB for ESP32-S2) as a hard constraint during the neural architecture search. Random search explores layer sequences and hyperparameters (filter counts, kernel sizes, strides, pooling configurations) but only evaluates architectures that satisfy the memory constraint. This avoids the "search-then-compress" pipeline where a potentially optimal architecture is discovered but then degraded through aggressive pruning.
- Core assumption: The optimal architecture under memory constraint exists within the search space defined, and direct search under constraint preserves performance better than finding a high-performing model then reducing it.
- Evidence anchors:
  - [section 2.2.4.1]: "MicroNAS utilizes the run time memory size and the search space as inputs, creating models that can be deployed on microcontrollers"
  - [section 4]: "MicroNAS creates smaller 1D CNN models (198.22 ± 89 KB) compared to the pruned 1D CNN created by NAS (261.60 ± 56 KB) (p < 0.01)"
  - [corpus]: Weak direct evidence—related papers on TinyML and MCU deployment (e.g., "Gait Recognition Based on Tiny ML and IMU Sensors," "Implementing Keyword Spotting on the MCUX947") discuss edge deployment but do not specifically validate memory-constrained NAS versus pruning strategies.
- Break condition: If the search space definition excludes architectures that would be optimal under constraint, or if the memory estimation during search does not accurately reflect runtime memory (e.g., ignoring intermediate activation buffers), the mechanism may yield suboptimal or non-deployable models.

### Mechanism 2
- Claim: Weighted binary cross-entropy loss improves minority class (fall) detection under severe class imbalance compared to undersampling or oversampling approaches.
- Mechanism: Given an imbalance ratio of ~58:1 (98.3% ADL, 1.7% Fall), MicroNAS applies class weights inversely proportional to class frequency in the cross-entropy loss: wi = n / (k × ni). This causes gradient updates to penalize fall misclassifications more heavily, shifting decision boundaries toward better recall for falls without discarding ADL samples (as undersampling does) or synthesizing potentially unrealistic fall samples (as oversampling might).
- Core assumption: The weighting formula appropriately balances precision and recall for the use case; the fall class distribution in training is representative of deployment despite its sparsity.
- Evidence anchors:
  - [section 2.2.2]: "To handle class imbalance in neural networks, we employed a weighting method that assigns higher weights to the fall class"
  - [section 4]: MicroNAS models (F1 = 0.64–0.66) outperformed RUSBoost and EasyEnsemble (F1 = 0.44), which use undersampling, and H2O AutoML (F1 = 0.22), which applies oversampling
  - [corpus]: Limited validation—no corpus papers directly compare weighted loss against sampling-based methods for fall detection on IMU time-series.
- Break condition: If the cost of false positives (unnecessary alerts) becomes unacceptable relative to false negatives (missed falls), or if the training class distribution diverges significantly from real-world fall frequency, the weighting scheme may require recalibration.

### Mechanism 3
- Claim: Sliding window segmentation with high overlap captures temporal dynamics of falls and improves detection over point-wise classification.
- Mechanism: Raw IMU data (3-axis accelerometer, 3-axis gyroscope at 100 Hz) is segmented into 1.2-second windows (120 samples) with 90% overlap. This preserves motion context before and during the fall event, enabling 1D CNN filters to extract temporal hierarchies and GRU gates to capture sequential dependencies. Ensemble baselines operate sample-by-sample with post-hoc majority voting, which may miss transient fall signatures.
- Core assumption: Falls manifest within ~1 second with identifiable pre-fall motion cues, and 90% overlap provides sufficient temporal resolution without excessive redundancy.
- Evidence anchors:
  - [section 2.2.3]: "Fall duration was approximately 1 second across all participants and pre-fall motions can often indicate an incoming fall, so we considered a window size of 1.2 seconds"
  - [section 4]: "MicroNAS receives a window of 120 samples of IMU data... It then extracts time-domain features from each window and transforms the representation"
  - [corpus]: Weak—corpus papers on fall detection (e.g., "Human Fall Detection using Transfer Learning-based 3D CNN") use video-based approaches rather than IMU windowing strategies.
- Break condition: If fall dynamics in real-world deployment differ from lab-simulated falls (e.g., slower falls, near-falls without ground contact, unexpected perturbations), the fixed window size and overlap may not capture the relevant temporal patterns.

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: MicroNAS automates model design by searching over depth, width, and temporal resolution under memory constraints rather than manual tuning.
  - Quick check question: Can you explain why a constrained NAS approach might yield different architectures than training a large model and compressing it afterward?

- Concept: Class Imbalance in Classification
  - Why needed here: Falls are rare events; without explicit handling, models default to predicting the majority class (ADL), achieving high accuracy but missing falls entirely.
  - Quick check question: Why might weighted loss be preferable to undersampling the majority class when the minority class is extremely sparse?

- Concept: Memory Budgeting for Edge Deployment
  - Why needed here: Microcontrollers like ESP32 have fixed SRAM (320 KB); models must fit within this budget including weights, activations, and runtime overhead.
  - Quick check question: What components of a neural network consume memory at inference time, and which are typically dominant for 1D CNNs versus GRUs?

## Architecture Onboarding

- Component map:
  Data Ingestion -> Window Segmentation (1.2s, 90% overlap) -> Class Weight Calculation -> NAS Search Space Definition (layer sequences, hyperparameters) -> Memory-Constrained Random Search (Keras Tuner) -> Model Training (Adam, weighted cross-entropy, early stopping) -> Model Export for ESP32

- Critical path:
  1. Prepare IMU time-series data with sliding windows aligned to your sensor rate.
  2. Compute class weights from training set imbalance ratio.
  3. Define memory budget equal to target microcontroller SRAM minus runtime overhead.
  4. Run MicroNAS with fixed budget (e.g., 20 architectures) and select highest F1 model under constraint.

- Design tradeoffs:
  - 1D CNN vs. GRU: CNNs enable parallel processing (faster inference on dual-core ESP32) but may need more parameters for long dependencies; GRUs capture sequential patterns more naturally but process sequentially (higher latency).
  - Window size vs. latency: Larger windows capture more context but delay detection.
  - Overlap vs. compute: Higher overlap increases training data volume and inference frequency but raises computational load.

- Failure signatures:
  - Models that train successfully but fail memory validation at export (memory estimator mismatch).
  - High recall but extremely low precision (too many false alarms—class weight too aggressive or threshold poorly calibrated).
  - F1 near zero on held-out participant despite high training F1 (overfitting to training subjects; leave-one-participant-out validation not respected).

- First 3 experiments:
  1. Replicate the paper's 1D CNN search on the provided dataset with ESP32 memory constraint; verify F1 and memory match reported ranges (F1 ≈ 0.64 ± 0.17, memory ≈ 198 KB).
  2. Ablate class weighting: train identical architecture with unweighted loss; compare F1 to quantify the contribution of imbalance handling.
  3. Test generalization: train on control participants only, evaluate on amputee participants; assess whether prosthetic gait patterns degrade performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MicroNAS models trained on laboratory-based simulated falls maintain statistical performance parity when deployed in real-world scenarios for lower-limb amputees?
- Basis in paper: [explicit] The authors state, "In future research, we plan to assess the performance of our FDS... against data collected from lower-limb amputees in real-world scenarios."
- Why unresolved: Current results are derived from controlled laboratory simulations which lack the "suddenness," environmental variability (e.g., slippery surfaces), and complex biomechanics of actual falls.
- What evidence would resolve it: A statistical comparison of F1-scores derived from laboratory test data versus F1-scores from field data collected in unconstrained real-world environments.

### Open Question 2
- Question: Does an ML model developed with a majority of control participants achieve similar F1-scores for lower-limb amputees compared to control participants?
- Basis in paper: [explicit] The authors note, "In the future, we will investigate whether an ML model developed with a majority of control participants will achieve similar performance... for participants with lower limb amputation."
- Why unresolved: The current training dataset is imbalanced (30 control vs. 5 amputee participants), potentially biasing the model toward the biomechanics of non-amputees.
- What evidence would resolve it: A comparative analysis of F1-scores on a held-out amputee test set using models trained on control-majority data versus amputee-majority data.

### Open Question 3
- Question: Can synthetic data generation techniques that preserve temporal dependencies improve class imbalance handling compared to the current weighting method?
- Basis in paper: [explicit] The authors state, "Investigating synthetic data generation techniques that consider temporal dependencies in IMU data represents a promising direction for future research."
- Why unresolved: The current method uses a weighting formula in the loss function, but synthetic data (like advanced SMOTE variants) might provide more robust training examples for the minority "Fall" class.
- What evidence would resolve it: Benchmarking F1-scores of models trained using temporal-aware synthetic oversampling against models using the current weighted cross-entropy loss.

## Limitations

- The memory estimation method during NAS search is not mathematically defined, raising concerns about whether the 320 KB constraint accurately reflects runtime SRAM usage including activations.
- The paper reports strong results on a specific dataset of 35 participants (5 amputees) without publicly available data or code, limiting independent validation.
- The comparison to traditional pruning methods assumes pruning degrades performance, but ablation of this assumption is not provided.

## Confidence

- **High Confidence:** The mechanism of weighted cross-entropy improving minority class detection under severe imbalance is well-supported by results (F1: 0.66 vs 0.22 for H2O AutoML).
- **Medium Confidence:** The claim that constrained NAS produces more efficient models than post-hoc pruning is supported by memory comparisons (198 KB vs 262 KB) but lacks direct ablation of the pruning step.
- **Low Confidence:** The memory estimation and deployment feasibility on ESP32 are assumed from the constraint logic but not independently verified in the paper.

## Next Checks

1. Replicate the 1D CNN search on the described dataset with ESP32 memory constraint; verify F1 and memory match reported ranges (F1 ≈ 0.64 ± 0.17, memory ≈ 198 KB).
2. Ablate class weighting: train identical architecture with unweighted loss; compare F1 to quantify the contribution of imbalance handling.
3. Test generalization: train on control participants only, evaluate on amputee participants; assess whether prosthetic gait patterns degrade performance.