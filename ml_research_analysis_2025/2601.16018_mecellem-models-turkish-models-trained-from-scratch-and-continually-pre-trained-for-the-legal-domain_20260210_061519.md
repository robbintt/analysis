---
ver: rpa2
title: 'Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained
  for the Legal Domain'
arxiv_id: '2601.16018'
source_url: https://arxiv.org/abs/2601.16018
tags:
- legal
- training
- turkish
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mecellem, a framework for developing specialized
  Turkish legal language models through domain adaptation. The authors address the
  challenge of adapting large language models to the Turkish legal domain, which requires
  handling complex morphological structures and domain-specific terminology.
---

# Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain

## Quick Facts
- **arXiv ID:** 2601.16018
- **Source URL:** https://arxiv.org/abs/2601.16018
- **Reference count:** 40
- **Primary result:** Mecellem encoder models rank among top Turkish legal retrieval models using fewer parameters than competitors

## Executive Summary
This paper introduces Mecellem, a framework for developing specialized Turkish legal language models through domain adaptation. The authors address the challenge of adapting large language models to the Turkish legal domain, which requires handling complex morphological structures and domain-specific terminology. They employ two complementary approaches: (1) pre-training ModernBERT-based encoder models from scratch on a Turkish-dominant corpus of 112.7 billion tokens, and (2) continual pre-training Qwen3 decoder models with curriculum learning. The encoder models achieve competitive performance on Turkish legal retrieval benchmarks, ranking among the top models despite using fewer parameters. The decoder models show 36.2% perplexity reduction on Turkish legal text.

## Method Summary
The Mecellem framework employs two parallel approaches for Turkish legal language model development. First, ModernBERT encoder models (155M and 403M parameters) are pre-trained from scratch on 112.7 billion tokens of Turkish legal and web data using MLM objectives. Critically, checkpoints are selected based on downstream retrieval performance rather than MLM loss minimization, with optimal checkpoints occurring before pre-training loss reaches minimum. Second, Qwen3 decoder models (1.7B and 4B parameters) undergo continual pre-training with curriculum learning across four phases: general texts, legal content, long normative texts, and domain refinement. Post-training converts both encoder and decoder outputs into embedding models using GISTEmbed with InfoNCE loss. Custom Turkish morphology filters and morphologically-aware tokenization are employed throughout.

## Key Results
- Mecellem encoder models rank among top Turkish legal retrieval models despite using fewer parameters (155M vs 1B+ competitors)
- Four-phase curriculum learning achieves 36.2% perplexity reduction on Turkish legal text for Qwen3 models
- Checkpoint selection based on downstream retrieval performance outperforms MLM loss minimization for morphologically rich languages
- Morphologically-aware tokenization with 14K+ Turkish tokens and 6K+ pure tokens correlates with 40-58% performance on legal tasks

## Why This Works (Mechanism)

### Mechanism 1: Downstream-Aware Checkpoint Selection
Selecting pre-training checkpoints based on downstream retrieval performance rather than MLM loss minimization yields better embedding models for morphologically rich languages. MLM loss and embedding quality exhibit non-linear relationships in Turkish; intermediate checkpoints often outperform those with minimal pre-training loss because morphological complexity creates distinct optimization landscapes where continued training beyond convergence can degrade generalization.

### Mechanism 2: Four-Phase Curriculum Learning for CPT
Progressive curriculum learning with replay buffers enables domain adaptation while mitigating catastrophic forgetting in smaller models. Four phases (general texts → legal content → long normative texts → domain refinement) provide gradual distribution shifts; replay buffer from Phase 1 reinforces foundational terminology in later phases, preventing abrupt knowledge overwriting.

### Mechanism 3: Morphologically-Aware Tokenization
Tokenizers with high Turkish vocabulary coverage and morphological integrity (pure tokens representing complete morphological units) correlate with stronger downstream performance on legal retrieval tasks. Turkish is agglutinative—tokenizers that preserve morphological boundaries enable the model to learn suffix patterns and case constructions, which are critical for legal documents with complex nominal constructions.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM) vs. Downstream Task Alignment**
  - Why needed here: Understanding why MLM loss doesn't guarantee embedding quality is essential for checkpoint selection.
  - Quick check question: Can you explain why minimizing pre-training loss might degrade downstream retrieval performance?

- **Concept: Catastrophic Forgetting in Continual Learning**
  - Why needed here: CPT requires balancing new domain knowledge with preservation of general capabilities.
  - Quick check question: What are two strategies used in this paper to mitigate catastrophic forgetting?

- **Concept: Contrastive Learning with False-Negative Filtering**
  - Why needed here: GISTEmbed with guide models outperforms standard InfoNCE by preventing harmful gradient signals.
  - Quick check question: Why would in-batch negatives sometimes be false negatives in legal document retrieval?

## Architecture Onboarding

- **Component map:** Custom tokenizer → ModernBERT encoder pre-training → Checkpoint evaluation on retrieval → GISTEmbed post-training → Embedding model
- **Critical path:** 1) Custom tokenizer training on Turkish legal + web corpus (BPE with Llama pre-tokenization pattern) → 2) Pre-training with checkpoint evaluation on downstream retrieval tasks (not just MLM loss) → 3) Post-training with GISTEmbed using BGE-M3 or EmbeddingGemma-300M as guide model
- **Design tradeoffs:** Sequence length 256 tokens matches training data distribution but causes 8.5% legal domain degradation vs. 2048 tokens; legal documents require long-context understanding
- **Failure signatures:** Checkpoints with minimal MLM loss but degraded retrieval performance (v6 pattern); Decoder-to-encoder conversion underperforms without multi-stage training infrastructure (4B converted model ranks 9th vs. 155M encoder at rank 2)
- **First 3 experiments:** 1) Tokenizer quality benchmark: Evaluate your tokenizer's Turkish Token Count and Pure Token Count against methodology in Section 3.3.3; target >14,000 Turkish tokens and >6,000 pure tokens → 2) Checkpoint sweep with downstream evaluation: During pre-training, save checkpoints every 10% of training; evaluate each on held-out retrieval benchmark before selecting final checkpoint → 3) Sequence length ablation: Train embedding models at seq_len=256, 1024, and 2048; compare performance on long-document legal retrieval tasks to determine optimal context window

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does implementing the full multi-stage training pipeline (including large-scale synthetic data generation and model merging) on Mecellem decoder models significantly close the performance gap with state-of-the-art encoder models?
- Basis in paper: [explicit] The authors state, "Future work will explore advanced decoder-to-encoder conversion techniques," acknowledging that their simplified, single-stage implementation contributes to the performance gap observed against models like BGE-M3.
- Why unresolved: The current study utilized a resource-constrained, single-stage supervised approach (MS MARCO-TR) rather than the complex pipelines used by top reference models.
- What evidence would resolve it: Retraining the Qwen3 CPT models using the official Qwen3 Embedding methodology (e.g., 150M synthetic samples) and comparing the resulting benchmark scores.

### Open Question 2
- Question: Can the divergence between Masked Language Modeling (MLM) loss minimization and downstream retrieval quality be predicted or mitigated in morphologically rich languages without exhaustive checkpoint evaluation?
- Basis in paper: [inferred] The authors observe "non-linear relationships" where optimal checkpoints (v5 for base, v2 for large) occur before minimal loss, but they do not establish a predictive heuristic for this phenomenon.
- Why unresolved: The study identifies the issue but relies on manual evaluation across checkpoints rather than proposing a generalizable method to predict the optimal stopping point.
- What evidence would resolve it: Identifying a training metric or linguistic proxy that correlates more strongly with downstream embedding quality than MLM loss in agglutinative languages.

### Open Question 3
- Question: Does curriculum learning provide distinct benefits for catastrophic forgetting prevention in larger models (e.g., 4B+ parameters), or does high model capacity render phased training redundant?
- Basis in paper: [inferred] The authors note that the 4B model achieved competitive results with single-phase training, whereas the 1.7B model required a four-phase curriculum, leaving the scalability of the curriculum strategy ambiguous.
- Why unresolved: The experimental design varied the training strategy by model size (curriculum for 1.7B, single-phase for 4B), making it difficult to isolate the effect of scale versus strategy.
- What evidence would resolve it: An ablation study applying the four-phase curriculum strategy to the Qwen3-4B model and comparing it against the single-phase baseline.

## Limitations

- Data provenance and filtering validation: The paper claims rigorous quality filtering but provides limited quantitative validation of these filtering steps, and overlap between web data and legal sources is not addressed.
- Generalizability beyond Turkish legal domain: The methodology's effectiveness for other morphologically rich languages or legal systems remains untested.
- Ablation of key design choices: Several critical design decisions lack systematic ablation studies, with conclusions about curriculum learning confounded by scale differences.

## Confidence

**High Confidence:** The observation that checkpoint selection based on downstream retrieval performance outperforms pure MLM loss minimization is strongly supported by empirical evidence showing regression at v6 despite lower loss values.

**Medium Confidence:** The claim that morphologically-aware tokenization directly causes improved embedding quality rather than merely correlating with it, though strong correlations are demonstrated.

**Low Confidence:** The assertion that four-phase curriculum learning is essential for smaller models (1.7B) while larger models (4B) can succeed with single-phase training, based on comparing different model scales rather than systematic ablation.

## Next Checks

**Validation Check 1:** Implement systematic ablation of curriculum phases within the same model scale (e.g., Qwen3-1.7B) by testing: (a) single-phase training, (b) two-phase training (general→legal), and (c) full four-phase training. Measure not only final performance but also training stability and catastrophic forgetting indicators throughout training.

**Validation Check 2:** Evaluate the morphologically-aware tokenizer design methodology on another agglutinative language with legal corpora (e.g., Finnish, Hungarian, or Korean). Apply identical corpus filtering and tokenizer training procedures, then assess whether the same performance patterns emerge.

**Validation Check 3:** Conduct systematic testing of checkpoint selection strategies across multiple downstream tasks beyond legal retrieval (e.g., legal question answering, contract analysis). Implement automated checkpoint evaluation pipelines that score intermediate checkpoints on diverse legal NLP tasks, then analyze whether the "stop before minimal loss" pattern holds across task types.