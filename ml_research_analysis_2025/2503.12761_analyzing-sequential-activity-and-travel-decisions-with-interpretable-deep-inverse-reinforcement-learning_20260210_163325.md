---
ver: rpa2
title: Analyzing sequential activity and travel decisions with interpretable deep
  inverse reinforcement learning
arxiv_id: '2503.12761'
source_url: https://arxiv.org/abs/2503.12761
tags:
- reward
- activity
- travel
- function
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an interpretable deep inverse reinforcement
  learning (DIRL) framework for analyzing sequential activity-travel decisions. The
  framework models activity-travel decisions as a Markov Decision Process and uses
  an adversarial IRL approach to infer reward and policy functions.
---

# Analyzing sequential activity and travel decisions with interpretable deep inverse reinforcement learning

## Quick Facts
- arXiv ID: 2503.12761
- Source URL: https://arxiv.org/abs/2503.12761
- Reference count: 8
- This study introduces an interpretable deep inverse reinforcement learning (DIRL) framework for analyzing sequential activity-travel decisions.

## Executive Summary
This study presents a novel interpretable deep inverse reinforcement learning (DIRL) framework for analyzing sequential activity-travel decisions. The framework models activity-travel decisions as a Markov Decision Process and uses an adversarial IRL approach to infer reward and policy functions. The policy function is interpreted using a surrogate interpretable model, while the reward function is analyzed by mapping behavioral sequences to reward values. Using Singapore travel survey data, the model demonstrates strong prediction performance (accuracy of 0.804, BLEU score of 0.876) and provides interpretable insights into behavioral patterns and preferences.

## Method Summary
The framework adapts adversarial inverse reinforcement learning (AIRL) to infer reward and policy functions from sequential activity-travel data. The state space includes current time, activity type, stay duration, activity count, and duration since leaving home. The model uses a policy network (generator) with embeddings and 3-layer feedforward networks to predict action probabilities, and a reward network (discriminator) to estimate immediate rewards and shaping terms. Proximal Policy Optimization (PPO) trains the policy to maximize rewards, while the discriminator distinguishes expert from generated trajectories. A Multinomial Logit (MNL) model serves as a surrogate interpretable model, trained on the policy network's soft labels to extract behavioral utilities. Long-term returns are calculated by aggregating rewards, enabling clustering of users by preference profiles.

## Key Results
- Strong prediction performance with accuracy of 0.804 and BLEU score of 0.876
- Distinct activity-travel patterns identified across employment types and socio-demographic groups
- Model reveals interpretable insights into behavioral patterns and preferences through the surrogate MNL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training stabilizes the recovery of a reward function that explains human preferences without requiring explicit manual feature engineering.
- Mechanism: The system frames Inverse Reinforcement Learning (IRL) as a Generative Adversarial Network (GAN). The discriminator estimates a reward function $D$ to distinguish expert demonstrations from generated trajectories, while the generator (policy $\pi$) learns to produce trajectories that maximize this reward. The reward function is shaped to be invariant to dynamics, disentangling the agent's goals from environmental constraints.
- Core assumption: The observed behavior is near-optimal with respect to an unknown reward function, and this reward function is transferable across different states.
- Evidence anchors:
  - [abstract]: "Our proposed framework adapts an adversarial IRL approach to infer the reward and policy functions..."
  - [section 3.1.2]: "The discriminator distinguishes between generated and observed behavior by inferring a reward function... The policy function aims to identify an optimal policy that maximizes the expected reward..."
  - [corpus]: Related work "Statistical analysis of Inverse Entropy-regularized Reinforcement Learning" supports the theoretical basis for disentangling reward structures, though it notes the inherent non-uniqueness of recovered rewards.
- Break condition: The mechanism may fail if the discriminator overpowers the generator (mode collapse), resulting in a reward function that is indistinguishable from noise or fails to generalize to unseen trajectories.

### Mechanism 2
- Claim: A deep policy network's decision logic can be approximated and explained by a simpler, interpretable surrogate model via knowledge distillation.
- Mechanism: The complex, black-box policy network $\pi_{IRL}$ produces "soft labels" (probability distributions over actions) that capture subtle behavioral nuances. A Multinomial Logit (MNL) model is trained on these soft labels rather than hard ground-truth labels. This forces the MNL to mimic the internal decision boundaries of the deep network, allowing coefficients to be interpreted as behavioral utilities.
- Core assumption: The knowledge learned by the deep neural network is linearly separable or approximable by a logistic regression structure.
- Evidence anchors:
  - [abstract]: "The policy function is interpreted through a surrogate interpretable model based on choice probabilities..."
  - [section 3.2.1]: "We adapt a knowledge distillation method... training a surrogate, interpretable model based on the soft labels predicted by a policy network."
  - [corpus]: Weak direct evidence in corpus; "Deceptive Sequential Decision-Making" deals with policy concealment rather than interpretation, highlighting the novelty of this distillation approach.
- Break condition: If the policy network learns highly non-linear, hierarchical decision rules (e.g., "if time > 8am AND income > $50k AND raining"), the linear MNL surrogate will have high distillation loss, rendering the interpretable coefficients misleading.

### Mechanism 3
- Claim: Mapping discrete activity sequences to continuous reward values enables the identification of latent user types and the ranking of behavioral utility.
- Mechanism: The learned reward network assigns an immediate scalar reward to every state-action pair. By aggregating these rewards into a long-term return $U$, the model compresses a complex daily trajectory into a single utility value. Clustering these reward sequences groups users by preference profiles (e.g., "routine" vs. "explorer") rather than just observed actions.
- Core assumption: The long-term return calculated by the model correlates with the human-perceived utility or satisfaction of the activity sequence.
- Evidence anchors:
  - [abstract]: "...reward function is interpreted by deriving both short-term rewards and long-term returns for various activity-travel patterns."
  - [section 4.4.2]: "By sorting the activity sequence based on their corresponding return value, we can then have a sense of what types of activity planning are more optimal..."
  - [corpus]: No direct corpus validation for this specific mapping utility.
- Break condition: If the reward network fails to converge or learns a trivial solution (e.g., reward proportional only to "staying home"), the clustering will identify only frequency differences rather than preference differences.

## Foundational Learning

- Concept: **Markov Decision Process (MDP)**
  - Why needed here: This is the mathematical scaffolding for the entire framework. You cannot define the State, Action, or Reward without understanding that the paper models sequential decisions as $M = \{S, A, T, R, \gamma\}$.
  - Quick check question: Can you define the "State" tuple used in this paper (hint: it has 5 components) and explain why "Current time" is included?

- Concept: **Knowledge Distillation (Teacher-Student)**
  - Why needed here: This is the bridge between "Black-box AI" and "Interpretable Economics." The student (MNL) learns from the teacher (DNN) to make the weights interpretable.
  - Quick check question: Why would training the MNL on "soft labels" (probabilities) from the DNN be better than training on the hard ground-truth labels directly?

- Concept: **Adversarial Learning (GANs)**
  - Why needed here: The AIRL algorithm relies on a discriminator and generator. Understanding this dynamic is crucial to debugging why the model might fail to learn valid rewards.
  - Quick check question: In this context, does the Discriminator classify "Real vs Fake" images, or does it classify "Expert vs Generated" trajectories?

## Architecture Onboarding

- Component map:
  - Travel Survey Data (Activity sequences + Socio-demographics) -> AIRL Model (Policy Net + Reward Net) -> Prediction Metrics (Accuracy, BLEU) -> Knowledge Distillation (Policy Net -> MNL) -> Interpretable Coefficients -> Reward Aggregation -> Clustering by Preference Profiles

- Critical path:
  1. **Data Prep**: Discretize time into 96 intervals (15-min slots). Encode categorical states (Activity, Employment) into embeddings.
  2. **Pre-training (Implicit)**: Initialize networks.
  3. **AIRL Loop**: Generator samples trajectory -> Discriminator estimates reward -> Generator updates via PPO to maximize reward -> Discriminator updates to distinguish expert from generator.
  4. **Distillation**: Train MNL on the *frozen* policy network's outputs ($\alpha=1$ setting found best in paper).
  5. **Analysis**: Calculate returns $U = \sum \gamma r_t$ and cluster agents.

- Design tradeoffs:
  - **Soft vs. Hard Labels**: The paper uses a trade-off weight $\alpha$. Setting $\alpha=1$ (pure distillation) yielded the best balance of performance and interpretability in their experiments, effectively trusting the DNN's "reasoning" over the raw data noise.
  - **Interpretability vs. Accuracy**: The MNL surrogate is less accurate than the DNN but provides readable coefficients. The DNN is the "ground truth" of behavior, but the MNL is the "explanation."
  - **Reward Shaping**: The paper uses $h_\phi(s)$ to make rewards robust to dynamics. Removing this might make the reward specific to the training environment (Singapore data) and less generalizable.

- Failure signatures:
  - **Low BLEU/High Edit Distance**: The policy network is not capturing sequential dependencies; check if PPO learning rate is too high or state features are insufficient.
  - **High Distillation Loss**: The MNL cannot replicate the DNN. The behavior is likely too complex for a linear model; consider interaction terms in the MNL or accepting lower fidelity.
  - **Incoherent Rewards**: If "Work" has a lower reward than "Travel" for employees, the discriminator may have failed to converge or the reward shaping term is dominating the immediate reward.

- First 3 experiments:
  1. **Baseline Reproduction**: Run the AIRL model on the Singapore data subset. Verify that the prediction accuracy approaches ~0.80 as stated in Table 1.
  2. **Ablation on Alpha ($\alpha$)**: Retrain the surrogate MNL with $\alpha=0$ (ground truth only) vs $\alpha=1$ (soft labels only). Compare the coefficients to see what "nuance" the DNN added that was missing in the raw data.
  3. **Reward Validity Check**: Take two distinct clusters (e.g., "Full-time Students" vs. "Homemakers"). Plot the distribution of their learned long-term returns to verify that the model assigns higher utility to "regular" patterns as claimed in Section 4.4.2.

## Open Questions the Paper Calls Out
None

## Limitations
- The model uses only one city's travel survey data, raising concerns about generalizability to different urban contexts and cultural settings.
- The interpretation of reward values as "behavioral utility" lacks behavioral economics validation - the paper does not confirm whether higher rewards actually correlate with higher user satisfaction or real-world utility.
- The distillation approach assumes the DNN's soft labels are superior to ground truth, but this may not hold if the DNN overfits to training patterns or fails to capture novel behavioral strategies.

## Confidence

- **High Confidence**: The AIRL architecture implementation and training procedure are well-defined. The prediction performance metrics (accuracy 0.804, BLEU 0.876) are specific and verifiable.
- **Medium Confidence**: The interpretation of policy coefficients through the MNL surrogate is methodologically sound, but the practical utility of these interpretations depends on how well the linear approximation captures complex decision boundaries.
- **Low Confidence**: The mapping of reward values to "behavioral utility" and the clustering of agents by preference profiles lacks behavioral validation. The claim that the model identifies "more optimal" activity patterns is subjective without user satisfaction data.

## Next Checks

1. **Generalizability Test**: Apply the trained model to travel data from a different city or country. Compare prediction accuracy and BLEU scores to verify the model captures universal decision patterns versus Singapore-specific behaviors.

2. **Behavioral Validation Study**: Conduct a survey with users from different clusters (identified by the model) to measure their actual satisfaction with their daily activity patterns. Correlate these satisfaction scores with the model's assigned reward values to validate the utility mapping.

3. **Ablation on Reward Shaping**: Remove the shaping term $h_\phi(s)$ from the reward network and retrain. Compare the resulting policy and reward interpretations to assess whether the shaping term adds meaningful behavioral insights or merely serves as a technical stabilizer.