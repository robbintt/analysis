---
ver: rpa2
title: 'R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling'
arxiv_id: '2508.15204'
source_url: https://arxiv.org/abs/2508.15204
tags:
- task
- cannot
- time
- simultaneously
- start
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces R-ConstraintBench, a benchmarking framework\
  \ to evaluate large language models on Resource-Constrained Project Scheduling Problems\
  \ (RCPSP), an NP-complete class of scheduling problems. The framework systematically\
  \ increases constraint complexity\u2014from pure precedence to multi-axis constraints\
  \ like downtime, temporal windows, and disjunctive exclusions\u2014and provides\
  \ automated feasibility verification with detailed error classification."
---

# R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling

## Quick Facts
- arXiv ID: 2508.15204
- Source URL: https://arxiv.org/abs/2508.15204
- Reference count: 0
- Key outcome: Constraint interaction, not graph depth, is the critical bottleneck for LLM scheduling; feasibility collapses when heterogeneous constraints couple across tasks.

## Executive Summary
This paper introduces R-ConstraintBench, a benchmarking framework to evaluate large language models on Resource-Constrained Project Scheduling Problems (RCPSP), an NP-complete class of scheduling problems. The framework systematically increases constraint complexity—from pure precedence to multi-axis constraints like downtime, temporal windows, and disjunctive exclusions—and provides automated feasibility verification with detailed error classification. In evaluations across nine leading models, strong performance on precedence-only DAGs did not translate to success under full operational constraints; feasibility collapsed when constraint interactions increased, with the strongest models degrading near the highest difficulty levels. Domain-grounded scenarios (data center migration) revealed further gaps, indicating that synthetic performance alone does not guarantee practical reliability. The study concludes that constraint interaction, not graph depth, is the critical bottleneck for LLM scheduling, highlighting the need for improved constraint-aware reasoning.

## Method Summary
The framework generates layered DAGs (2–5 layers) with exactly k non-redundant edges at level k, then injects multi-axis constraints (downtime, temporal windows, disjunctive pairs) at ~75% probability. All instances are validated for solvability via CP-SAT before evaluation. Models receive single-shot prompts and generate JSON schedules, which are automatically verified by a CP-SAT solver that checks feasibility and classifies errors by type (precedence, resource/downtime, temporal, disjunctive). Performance is measured by feasibility rate, weighted AUC (later levels weighted more), breakpoint level (first k where feasibility < 70%), and detailed error taxonomy.

## Key Results
- Constraint interaction, not graph depth, is the primary driver of infeasibility in LLM scheduling.
- Automated verification with detailed error attribution isolates specific reasoning failures (e.g., precedence vs. resource conflicts).
- Domain grounding alters the failure surface compared to clean synthetic data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraint interaction, rather than graph depth, is the primary driver of infeasibility in LLM scheduling.
- Mechanism: Models process precedence-only DAGs (linear time complexity) near-ceiling but fail when heterogeneous constraints (downtime, temporal windows, disjunctive exclusions) couple across tasks. The cognitive load stems from maintaining global consistency across interacting rule types, not just traversing graph edges.
- Core assumption: The observed performance collapse is intrinsic to the model's reasoning architecture and not solely an artifact of context window compression or tokenization of constraint notation.
- Evidence anchors:
  - [abstract] "feasibility collapsed when constraint interactions increased... implicating constraint interaction, not graph depth, as the principal bottleneck."
  - [Section 8.1] "constraint interaction... rapidly erodes feasibility... globally consistent reasoning across heterogeneous rules."
  - [corpus] Related work on constrained reconfiguration and motion planning confirms that multi-agent or multi-rule coordination in constrained environments creates fundamental complexity challenges.
- Break condition: If models were allowed tool-use (e.g., external solvers) rather than single-shot reasoning, this bottleneck would likely shift from reasoning to tool invocation.

### Mechanism 2
- Claim: Automated verification with detailed error attribution isolates specific reasoning failures (e.g., precedence vs. resource conflicts).
- Mechanism: A CP-SAT verifier parses the LLM's proposed schedule into structured time intervals and checks them against hard constraints. Violations are categorized (precedence, resource/downtime, temporal, disjunctive), revealing if a model fails to understand ordering, capacity, or mutual exclusion.
- Core assumption: The translation of natural language schedules into verifier inputs preserves the semantic intent of the model's proposed times without introducing parsing artifacts.
- Evidence anchors:
  - [Section 4.3] "Violations are automatically classified by type... enabling a fine-grained infeasibility analysis."
  - [Section 9] Shows distinct failure profiles: o3 fails on precedence (93.7%), while Grok 4 fails on disjunctive constraints (48.4%).
- Break condition: If the constraint specification in the prompt is ambiguous, verification may flag "model errors" that are actually "spec interpretation" errors.

### Mechanism 3
- Claim: Domain grounding alters the failure surface compared to clean synthetic data.
- Mechanism: Mapping abstract RCPSP structures to realistic scenarios (e.g., data center migration) introduces semantic coupling and implicit common-sense constraints (e.g., "Forklift" implies capacity 1). Models may rely on statistical priors from training data which help or hinder based on how well the synthetic instance matches these priors.
- Core assumption: The "real-world" gap observed is due to the semantic density and operational coupling of the scenario, not merely the increased token count of the description.
- Evidence anchors:
  - [Section 7.3] "domain-specific coupling... alters the failure surface in ways not fully captured by synthetic MCI instances."
  - [Section 8.2] "Grok 4 improves when constraints are embedded in realistic couplings... while o3 exhibits the largest negative gap."
- Break condition: If a model has been heavily fine-tuned on similar domain data, the gap may invert, showing artificial competence on domain instances without improved reasoning.

## Foundational Learning

- Concept: **NP-Completeness & Feasibility vs. Optimization**
  - Why needed here: RCPSP is NP-complete, meaning finding a *valid* schedule (feasibility) is computationally hard, distinct from finding the *shortest* one (optimization). This paper isolates feasibility to test pure reasoning.
  - Quick check question: Can you explain why checking if a schedule is valid is faster than generating one from scratch for NP-complete problems?

- Concept: **Directed Acyclic Graphs (DAGs) & Topological Sorting**
  - Why needed here: Phase I of the benchmark uses pure-precedence DAGs. Understanding that a valid schedule is essentially a topological sort helps interpret why models perform well here (linear time solution exists) versus Phase II.
  - Quick check question: If a cycle existed in the precedence graph, why would the schedule be impossible?

- Concept: **Disjunctive & Temporal Constraints**
  - Why needed here: These are the specific "axes" added in Phase II. Disjunctive constraints (mutual exclusion) and temporal windows (deadlines/release times) force the model to reason about resource states and absolute time, not just relative order.
  - Quick check question: How does a disjunctive constraint ("Task A and B cannot overlap") differ fundamentally from a precedence constraint ("Task A must finish before B starts")?

## Architecture Onboarding

- Component map: Instance Generator -> Prompt Interface -> LLM Scheduler -> Verifier
- Critical path: Ensuring the Instance Generator produces *solvable* problems is critical. If the generator creates impossible instances, model failures become uninterpretable. The verification step validates both the generator (pre-evaluation) and the model (post-evaluation).
- Design tradeoffs:
  - **Synthetic vs. Domain**: Synthetic ramps allow controlled variable isolation (varying $k$ edges), while domain scenarios test transferability but introduce confounding factors.
  - **Feasibility vs. Makespan**: The paper evaluates binary feasibility. Ignoring optimization (minimizing makespan) simplifies verification but may hide "trivial" valid schedules that are operationally useless.
- Failure signatures:
  - **High Precedence Errors (e.g., o3)**: Indicates failure to track the dependency chain or "lost" context of early tasks.
  - **High Disjunctive Errors (e.g., Grok 4)**: Indicates failure to respect mutual exclusion/capacity limits, often treating resources as infinite.
  - **Negative Real-World Gap**: Indicates over-reliance on clean synthetic patterns or failure to map semantic terms (e.g., "Forklift") to hard constraints.
- First 3 experiments:
  1. **Baseline Phase I**: Run the model on 2-layer and 5-layer pure-precedence DAGs to establish a graph-reasoning baseline.
  2. **Constraint Ablation**: Run Phase II instances with only one added constraint type (e.g., only temporal windows) to identify which specific axis causes degradation.
  3. **Domain Transfer Test**: Compare performance on a synthetic instance vs. its semantically mapped "data center" equivalent to measure the grounding gap.

## Open Questions the Paper Calls Out

- **Question:** Does finetuning LLMs on RCPSP-style problems improve constraint satisfaction rates under high-density conditions?
  - Basis: [explicit] "Explore whether finetuning models on RCPSP-style problems... improves constraint satisfaction rates and robustness under high-density conditions."
  - Why unresolved: This study evaluates only base model capabilities via single-shot prompting; no finetuning experiments were conducted.
  - What evidence would resolve it: Compare pre- and post-finetuning W AUC scores and breakpoint levels on Phase II benchmarks.

- **Question:** Can LLMs that produce feasible schedules also satisfy optimization objectives (makespan, cost) without solver assistance?
  - Basis: [explicit] "leaves open the question of whether feasible outputs can also meet makespan or cost objectives without solver assistance."
  - Why unresolved: The benchmark evaluates binary feasibility only, intentionally isolating reasoning from optimization.
  - What evidence would resolve it: Extend the verifier to score schedules on makespan distance from optimal CP-SAT solutions.

- **Question:** How do chain-of-thought prompting, verification prompts, and retrieval-augmented generation affect scheduling feasibility?
  - Basis: [explicit] "Systematically study the impact of chain-of-thought prompting, step-by-step reasoning scaffolds, verification prompts, and retrieval-augmented generation on schedule feasibility."
  - Why unresolved: Single-shot prompting was used throughout; multi-step strategies were intentionally excluded to isolate core reasoning.
  - What evidence would resolve it: Run ablations across prompting strategies on Phase IIa/IIb and compare feasibility curves and error taxonomies.

- **Question:** Does synthetic RCPSP performance predict reliability across other operational domains (e.g., construction, logistics, satellite tasking)?
  - Basis: [explicit] "Introduce new operational domains beyond data center migration... to evaluate transfer learning and domain adaptation."
  - Why unresolved: Only one domain instantiation (data center migration) was tested; transfer patterns differed sharply across models.
  - What evidence would resolve it: Benchmark the same models on multi-domain RCPSP instantiations and compute synthetic-to-domain correlation coefficients.

## Limitations

- **Task and Duration Parameters:** Exact task count per instance and D_max (maximum task duration) distribution are unspecified, which could affect model complexity and performance.
- **Constraint Sampling Strategy:** The precise method for selecting which tasks receive downtime, temporal windows, and disjunctive constraints is not detailed, potentially impacting the difficulty and nature of generated instances.
- **Single-Shot Prompting:** The study exclusively uses single-shot prompting, limiting insights into how multi-step reasoning strategies might improve performance.

## Confidence

- **High Confidence:** Constraint interaction, not graph depth, is the primary bottleneck for LLM scheduling feasibility.
- **Medium Confidence:** Domain grounding significantly alters the failure surface compared to synthetic data.
- **Medium Confidence:** Automated verification with detailed error attribution accurately isolates specific reasoning failures.

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary task count and D_max to determine their impact on model performance and identify if a specific parameter range is critical for the observed bottleneck.
2. **Constraint Ablation Study:** Generate and evaluate instances with only one constraint type at a time (e.g., only temporal windows, only disjunctive constraints) to isolate the individual contribution of each constraint axis to the overall difficulty.
3. **Semantic Density Experiment:** Compare model performance on semantically dense domain scenarios (e.g., data center migration) with equivalent token-count descriptions of abstract RCPSP instances to determine if the "real-world gap" is due to semantic coupling or simply increased description length.