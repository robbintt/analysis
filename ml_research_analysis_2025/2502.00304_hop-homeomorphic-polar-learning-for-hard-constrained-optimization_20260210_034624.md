---
ver: rpa2
title: 'HoP: Homeomorphic Polar Learning for Hard Constrained Optimization'
arxiv_id: '2502.00304'
source_url: https://arxiv.org/abs/2502.00304
tags:
- optimization
- learning
- problem
- polar
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach, HoP, for solving hard-constrained
  optimization problems using homeomorphic polar learning. The method leverages homeomorphic
  mapping to transform neural network outputs from polar coordinates to Cartesian
  coordinates, ensuring feasibility without requiring penalty functions or post-corrections.
---

# HoP: Homeomorphic Polar Learning for Hard Constrained Optimization

## Quick Facts
- **arXiv ID:** 2502.00304
- **Source URL:** https://arxiv.org/abs/2502.00304
- **Reference count:** 40
- **Key outcome:** Proposes HoP, a method for hard-constrained optimization using homeomorphic polar learning that achieves near-optimal solutions with zero constraint violations while being significantly faster than traditional optimizers.

## Executive Summary
This paper introduces HoP (Homeomorphic Polar Learning), a novel approach for solving hard-constrained optimization problems. The method leverages homeomorphic mapping to transform neural network outputs from polar coordinates to Cartesian coordinates, ensuring feasibility without requiring penalty functions or post-corrections. By using polar sphere vectors mapped through spherical homeomorphic transformations, HoP guarantees that all generated solutions satisfy star-convex constraints. The framework addresses stagnation issues through reconnection strategies and dynamic learning rate adjustments. Extensive experiments demonstrate that HoP achieves near-optimal solutions with zero constraint violations while being significantly faster than traditional optimizers and competitive with learning-based methods.

## Method Summary
HoP solves hard-constrained optimization problems by training a neural network to output polar parameters that are mapped through a homeomorphic transformation to feasible Cartesian coordinates. The method requires identifying a star-convex constraint set with respect to a center point y₀, then computing boundary distances or angles for each direction. The neural network outputs polar parameters [z_θ, z_r] which are transformed through bounded activations and reconnection strategies before being mapped via the homeomorphic function H to produce feasible solutions. The framework uses self-supervised learning with direct objective loss, avoiding penalty terms entirely.

## Key Results
- Achieves 0% constraint violation across all benchmark types while traditional optimizers and learning-based methods show violations
- Demonstrates 10-100× speedup compared to traditional optimizers (SLSQP, IPOPT) on synthetic and real-world problems
- Outperforms learning-based methods (DC3, FSNet) in both objective value and feasibility on star-convex constraints
- Maintains near-optimal performance on high-dimensional semi-unbounded problems where traditional methods struggle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Homeomorphic mapping guarantees all neural network outputs satisfy star-convex constraints without penalty terms or post-correction.
- Mechanism: A bijection H maps from a bounded polar parameter space (unit hypersphere) to the feasible region Y_x. Since H is continuous with continuous inverse, every point in the bounded parameter space maps to exactly one feasible point, and vice versa. The NN outputs polar sphere vectors [v_θ, z̄_r] which are inherently bounded, then H transforms them to Cartesian coordinates within constraints.
- Core assumption: The constraint set Y_x is star-convex with respect to a known interior point y_0 (Definition 3.1).
- Evidence anchors:
  - [abstract] "The bijective structure enables end-to-end training without extra penalty or correction."
  - [section 3.2] Eq. (4): ĥ = y_0 + rv_θR(v_θ, Y_x) where R finds boundary distance; Proposition 3.3 guarantees boundary points lie on actual constraint boundary even with redundant constraints.
  - [corpus] FSNet (arxiv:2506.00362) similarly uses feasible-by-design layers but via orthogonal projection rather than homeomorphism.
- Break condition: If Y_x is not star-convex (e.g., non-convex disconnected regions), the polar ray intersection property fails and feasibility guarantees break.

### Mechanism 2
- Claim: Reconnection strategies prevent optimizer stagnation caused by polar coordinate non-negativity constraints.
- Mechanism: In polar coordinates, r ≥ 0 causes "radial stagnation" when r→0 and "angular freezing" when gradients vanish at r=0 (Remark C.1). The reconnection strategy (Eq. 10): when r < 0, map r → |r| and θ → θ + π, effectively reconnecting the polar space across the discontinuity. This allows gradient flow through what would otherwise be a boundary.
- Core assumption: The optimization landscape in polar coordinates has meaningful structure on both sides of r=0 that can be exploited by reconnecting.
- Evidence anchors:
  - [section 3.3] "When r < 0, we expand the angular and radial domains with reconnecting the polar space by r → |r|, θ → θ + π."
  - [section 3.3] Figure 4 shows Cartesian vs. polar paths with disconnection vs. reconnection—right column shows smooth trajectories after reconnection.
  - [corpus] Weak/no direct corpus evidence for polar reconnection specifically in L2O.
- Break condition: If the true optimum requires precisely r=0 and the reconnection creates artificial gradients pushing away from zero, convergence may be delayed.

### Mechanism 3
- Claim: Spherical coordinate transformation extends feasibility guarantees to semi-unbounded constraint regions.
- Mechanism: For unbounded directions where R(v_θ, Y_x) → ∞, direct polar scaling fails. The spherical mapping (Eq. 8-9) transforms the problem: boundary distance R maps to angle φ = tan⁻¹(R), then ψ = z̄_r·φ with ψ ∈ [0, π/2 - ε). Points at infinity correspond to ψ → π/2, keeping all outputs bounded while covering arbitrarily large feasible regions.
- Core assumption: Semi-unbounded regions can be characterized by finite φ values for all directions (the supremum of φ corresponds to asymptotic directions).
- Evidence anchors:
  - [section 3.2] Eq. (8): ĥ = y_0 + v_θ tan(ψ); "points at infinity in the green space correspond to the angle on equator where ψ = π/2."
  - [appendix B.2] Theorem B.3: Jacobian determinant diverges as ψ → π/2, requiring regularization parameter ε > 0.
  - [corpus] No direct corpus comparison for semi-unbounded handling in L2O.
- Break condition: If ε is set too small, numerical instability from Jacobian divergence (det ∼ 1/ε^{d+1}) causes gradient explosion.

## Foundational Learning

- Concept: **Star-convexity vs. convexity**
  - Why needed here: HoP's guarantees depend critically on star-convexity (weaker than convexity)—a set Y is star-convex if ∃y_0 such that all line segments from y_0 to any y ∈ Y lie in Y. This is the minimal structure enabling polar ray intersection.
  - Quick check question: Given a constraint set, can you identify if it's star-convex and find a valid center point y_0?

- Concept: **Homeomorphism and topological equivalence**
  - Why needed here: The core claim rests on H being a homeomorphism (bijective, continuous, with continuous inverse). This ensures the NN's bounded output space is topologically equivalent to the feasible region—no "holes" or "tears" in the mapping.
  - Quick check question: For a mapping from (0,1)² to a polygon, what properties must hold for it to be a homeomorphism?

- Concept: **Gradient flow through polar coordinates**
  - Why needed here: Backpropagation through H requires Jacobians. The spherical mapping's Jacobian (det = tan(ψ)^{d-1} sec²(ψ)) diverges near ψ=π/2, creating numerical challenges the ε-regularization addresses.
  - Quick check question: What happens to gradient magnitudes when ψ approaches π/2 without regularization?

## Architecture Onboarding

- Component map:
  - **Input layer**: Problem parameters x (e.g., constraint RHS b, objective coefficients)
  - **NN backbone**: 3-layer MLP with ReLU, outputs raw [z_θ, z_r]
  - **Bounded activation**: B(·) (e.g., Sigmoid) maps z_r → (0,1)
  - **Direction normalization**: Eq. (6) produces unit vector v_θ with reconnection sign handling
  - **Homeomorphic mapping H**: Eq. (8) for semi-unbounded, Eq. (4) for bounded; requires precomputed y_0 and R(v_θ, Y_x) or φ
  - **Loss**: Direct objective f_x(ĥ) with no penalty terms

- Critical path:
  1. Precompute y_0 (Chebyshev center or convex optimization solution) for each problem instance
  2. For each direction v_θ, compute boundary distance R(v_θ, Y_x) or angle φ = tan⁻¹(R)
  3. Forward pass: x → NN → [z_θ, z_r] → [v_θ, z̄_r] → H → ĥ
  4. Loss = f_x(ĥ); backprop through H (requires Jacobian of tan(ψ) and normalization)

- Design tradeoffs:
  - **ε selection**: Smaller ε → better coverage of large feasible regions, but worse numerical stability; paper uses implicit ε from finite precision
  - **y_0 computation**: Chebyshev center is robust but requires solving a convex subproblem per instance; poor y_0 placement reduces effective coverage
  - **Reconnection vs. dynamic LR**: Reconnection (Eq. 6, 10) is more robust than scaled learning rates (Eq. 25) but changes the optimization landscape

- Failure signatures:
  - **Constraint violation > 0%**: Indicates y_0 not in interior, or R(v_θ, Y_x) computed incorrectly for non-star-convex regions
  - **Gradient explosion near boundary**: ε too small, Jacobian divergence dominates
  - **Angular freezing**: Reconnection not applied correctly; check sign(z_r) handling in Eq. (6)
  - **Objective much worse than optimizer**: May indicate H mapping is bijective but gradients through tan(ψ) are poorly conditioned

- First 3 experiments:
  1. **2-D polygon validation**: Implement HoP on 8-sided polygon with sinusoidal QP objective. Verify 0% violation and compare objective to SLSQP. Debug: if violation > 0, check y_0 interior condition.
  2. **ℓ_p-norm constraint test**: Test with p=0.5 (non-convex star-convex). Compare against DC3 baseline. Expected: HoP achieves near-optimal objective with 0% violation; DC3 may have 0% violation but worse objective due to correction bias.
  3. **Ablation on reconnection**: Train with and without Eq. (6) reconnection on semi-unbounded problem. Measure convergence speed and final objective. Expected: without reconnection, stagnation near r=0 causes slower convergence and worse solutions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can HoP be extended to handle non-star-convex constraint sets, such as general non-convex feasible regions with disconnected components?
- Basis in paper: [explicit] The authors state: "HoP to solve the star-convex hard-constrained optimization" and "star-convexity is a weaker condition than convexity" — explicitly limiting the scope to star-convex domains.
- Why unresolved: The homeomorphic mapping relies on the existence of a center point y₀ such that all rays from y₀ intersect the boundary exactly once. Non-star-convex sets violate this geometric property, making the current polar formulation inapplicable.
- What evidence would resolve it: A theoretical extension or modified mapping that handles multi-modal or disconnected feasible regions, validated on benchmarks with non-star-convex constraints.

### Open Question 2
- Question: How does the optimality gap scale with problem dimension and constraint complexity, and can this gap be provably bounded?
- Basis in paper: [inferred] Table 3 shows HoP achieving -4.7683 versus the optimizer's -7.6901 (a 38% gap) on 20-dimensional semi-unbounded problems, while maintaining zero violations.
- Why unresolved: The paper demonstrates empirical near-optimality but provides no theoretical bounds on the suboptimality introduced by restricting solutions to the polar representation and the neural network's approximation capacity.
- What evidence would resolve it: Theoretical analysis establishing optimality gap bounds under specific conditions, or improved experimental results showing convergence to near-optimal solutions across diverse high-dimensional problems.

### Open Question 3
- Question: What is the computational overhead of computing R(v_θ, Y_x) for complex nonlinear constraints, and does this limit scalability?
- Basis in paper: [inferred] The method requires finding "the distance from y₀ to the boundary of Y_x in the direction specified by θ" via Proposition 3.3, but no analysis of computational cost for complex constraint sets is provided.
- Why unresolved: For simple linear or ℓ_p-norm constraints, R computation is tractable, but for complex nonlinear constraints (e.g., the QoS constraints in Eq. 14), this boundary-finding subproblem may itself be computationally expensive.
- What evidence would resolve it: Complexity analysis of the R computation for various constraint types, and empirical timing breakdowns isolating this cost from NN inference time.

## Limitations
- Theoretical guarantees depend critically on star-convexity assumptions that may not hold in general non-convex constraint sets
- No theoretical analysis of convergence rates and bounds for non-convex star-convex problems
- Numerical stability near ψ=π/2 remains a concern despite regularization, particularly for high-dimensional semi-unbounded problems

## Confidence
- **High confidence**: Feasibility guarantees for star-convex constraints (empirical validation across multiple benchmark types), superior speed compared to traditional optimizers, effectiveness of reconnection strategies in preventing stagnation
- **Medium confidence**: Performance claims relative to learning-based methods (DC3, FSNet) - while experiments show improvement, the comparison methodology and hyperparameter tuning across methods could be more detailed
- **Low confidence**: Theoretical analysis of convergence rates and bounds for non-convex star-convex problems, scalability to very high dimensions (>100), robustness to poor y₀ placement in complex constraint geometries

## Next Checks
1. **Robustness to y₀ placement**: Systematically vary y₀ from Chebyshev center to boundary-proximal points and measure constraint violation rates and objective degradation across all benchmark types.
2. **Non-star-convex generalization**: Test HoP on non-star-convex constraint sets (e.g., disjoint regions, non-convex polygons with reflex angles) and document failure modes and constraint violation rates.
3. **Scaling analysis**: Evaluate HoP on high-dimensional problems (d>50) with semi-unbounded constraints, measuring training stability, numerical Jacobian conditioning, and runtime scaling compared to both traditional and L2O methods.