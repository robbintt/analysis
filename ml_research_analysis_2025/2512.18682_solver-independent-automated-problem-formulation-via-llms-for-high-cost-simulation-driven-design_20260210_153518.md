---
ver: rpa2
title: Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven
  Design
arxiv_id: '2512.18682'
source_url: https://arxiv.org/abs/2512.18682
tags:
- data
- design
- power
- requirements
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating natural language
  design requirements into precise mathematical optimization models in high-cost simulation-driven
  design. The authors propose APF, a solver-independent framework that automatically
  generates executable optimization formulations by leveraging large language models
  (LLMs).
---

# Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design

## Quick Facts
- arXiv ID: 2512.18682
- Source URL: https://arxiv.org/abs/2512.18682
- Reference count: 40
- Primary result: APF achieves alignment score of 0.7976 on antenna design optimization, outperforming GPT-4o (0.6651) and Chain-of-Experts (0.7252) without expensive solver feedback.

## Executive Summary
This paper introduces APF, a solver-independent framework that automatically translates natural language design requirements into executable mathematical optimization models for high-cost simulation-driven design. The key innovation is using LLM-based reference rankings to evaluate data quality without requiring expensive solver feedback, enabling effective fine-tuning on synthetic datasets derived from historical simulation records. The method demonstrates superior performance in antenna design optimization, achieving high alignment scores and producing designs that meet all specified performance requirements.

## Method Summary
APF operates through a four-stage pipeline: (1) Data Generation - LLM generates Python optimization code from natural language requirements; (2) Test Instance Annotation - LLM produces reference rankings for test performance curves; (3) Data Evaluation/Selection - Keeps samples where Spearman correlation between predicted and reference rankings exceeds 0.7; (4) SFT Training - Fine-tunes backbone LLM with LoRA on filtered dataset. The framework leverages domain-grounded synthetic data from simulation records and applies semantic paraphrasing plus order permutation for robust generalization.

## Key Results
- Alignment score of 0.7976 achieved on antenna design optimization task
- Outperforms GPT-4o (0.6651) and Chain-of-Experts (0.7252) baselines
- Optimized antenna designs meet all specified performance requirements in passband and stopband
- Robustness validated through ablation showing degradation without augmentation (-0.0423 alignment drop)

## Why This Works (Mechanism)

### Mechanism 1: Solver-Independent Quality Estimation via LLM Ranking
Replaces expensive solver feedback with LLM-based reference rankings for data quality assessment. An LLM produces listwise rankings treating it as a domain expert, then generated equations are executed to produce predicted rankings via hierarchical sorting. Alignment is measured as Spearman correlation between predicted and expert rankings.

### Mechanism 2: Domain-Grounded Synthetic Data from Simulation Records
Derives training samples from historical simulation data to ensure physical feasibility. Extracts structured tuples (Z, M, C) from simulation outcomes where operating conditions define subregions Z and performance metrics populate constraints C, grounding each sample in physically realizable designs.

### Mechanism 3: Robustness via Semantic and Structural Augmentation
Combines semantic paraphrasing (LLM rewrites requirements while preserving numerical values) and order permutation to eliminate positional bias. This exposes models to linguistic variations and structural diversity during training, reducing overfitting to specific phrasings.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) with LoRA**: Fine-tunes 7-8B parameter open-source LLMs on synthetic domain-specific data to specialize in requirement-to-equation translation. Quick check: Can you explain why LoRA is preferred over full fine-tuning for 7B models on domain tasks?

- **Ranking Correlation Metrics (Spearman)**: Core evaluation relies on Spearman rank correlation between predicted and reference rankings. Quick check: Given rankings [1,2,3,4,5] and [1,3,2,4,5], can you compute the Spearman correlation?

- **Constrained Optimization Formulation**: Output equations must distinguish objectives (minimization targets) from constraints (satisfaction conditions). Quick check: How would you formalize "radiated power must exceed -4.49 dB in the passband" as a constraint function?

## Architecture Onboarding

- **Component map**: Historical Simulation Data → Data Generation → Test Instance Annotation → Data Evaluation → SFT Training → Inference
- **Critical path**: Simulation data quality → LLM ranking reliability → Correlation-based filtering → SFT data quality. Errors propagate forward; weak rankings yield noisy selection, degrading final model.
- **Design tradeoffs**: 
  - Threshold selection (0.7 chosen): Higher thresholds improve precision but reduce training set size
  - LLM-as-judge vs. human annotation: Listwise ranking with LLM is scalable but inherits LLM biases
  - Backbone choice: LLaMA3.1-8B performed best (0.7976), but all three backbones improved substantially
- **Failure signatures**: 
  - Low correlation scores (< 0.5): Indicates LLM failing to map requirements to correct mathematical operations
  - Passband violations: Suggests objective/constraint distinction failed
  - High variance in alignment scores: May indicate overfitting to augmented patterns
- **First 3 experiments**:
  1. Implement data generation pipeline on provided simulation records; verify correlation score distribution matches Figure 4
  2. Train models with thresholds [0.5, 0.6, 0.7, 0.8, 0.9] to characterize precision-coverage tradeoff
  3. Apply pipeline to different simulation-driven domain (e.g., aerodynamics) using same architecture

## Open Questions the Paper Calls Out

- **Cross-Domain Generalization**: Can APF generalize to other high-cost simulation-driven design domains beyond antenna design, such as aerodynamics and structural optimization? The paper plans to validate in broader physics-based engineering fields.

- **LLM-as-Judge Reliability**: How does the reliability of LLM-as-a-judge rankings compare to human expert rankings when test instances involve subtle trade-offs? The framework uses LLM rankings without validating alignment with human experts beyond a single one-shot example.

- **Quality Score Threshold Sensitivity**: How sensitive is APF performance to the quality score threshold used for data selection? The paper applies a 0.7 threshold but doesn't ablate or justify this value systematically.

## Limitations
- Data Dependency: Success hinges on quality and representativeness of historical simulation records; biased or incomplete records may propagate limitations
- LLM Reliability: Reliance on LLMs for ranking introduces potential biases that may affect correlation-based filtering
- Domain Generalization: Applicability to other simulation-driven domains remains untested; may require domain-specific prompt engineering

## Confidence

- **High Confidence**: Solver-independent quality estimation mechanism (Mechanism 1) is well-supported by experimental results and ablation studies
- **Medium Confidence**: Domain-grounded synthetic data generation (Mechanism 2) is logically sound but actual dataset quality is not fully disclosed
- **Low Confidence**: Robustness claims via semantic and structural augmentation (Mechanism 3) are supported by ablation results, but potential for semantic drift remains a concern

## Next Checks

1. **Cross-Domain Transfer Test**: Apply pipeline to a different simulation-driven domain (e.g., aerodynamics) to assess whether grounding strategy transfers effectively or requires domain-specific adjustments.

2. **Ablation on Filtering Threshold**: Train models with varying quality score thresholds (0.5 to 0.9) to characterize precision-coverage tradeoff and identify optimal settings.

3. **Human Annotation Validation**: Conduct small-scale human annotation study to validate LLM-generated rankings used for data selection, assessing reliability of solver-independent evaluation mechanism.