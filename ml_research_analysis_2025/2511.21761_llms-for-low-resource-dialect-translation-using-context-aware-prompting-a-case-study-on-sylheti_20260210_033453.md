---
ver: rpa2
title: 'LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A
  Case Study on Sylheti'
arxiv_id: '2511.21761'
source_url: https://arxiv.org/abs/2511.21761
tags:
- sylheti
- bangla
- translation
- sylheti-cap
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates Large Language Models (LLMs)
  for translating between Bangla and Sylheti, a low-resource dialect of Bangla. Five
  advanced LLMs (GPT-4.1, LLaMA 4, Grok 3, DeepSeek V3.2, and Gemini 2.5 Flash) were
  tested in both translation directions, revealing that LLMs struggle with dialect-specific
  vocabulary and morphology.
---

# LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A Case Study on Sylheti

## Quick Facts
- **arXiv ID**: 2511.21761
- **Source URL**: https://arxiv.org/abs/2511.21761
- **Reference count**: 25
- **Key outcome**: Sylheti-CAP, a context-aware prompting framework, consistently improves Bangla-Sylheti dialect translation across 5 LLMs and metrics, reducing hallucinations and mistranslations while outperforming zero-shot, few-shot, and chain-of-thought strategies.

## Executive Summary
This study addresses the challenge of translating between Bangla and Sylheti, a low-resource dialect of Bangla, using Large Language Models (LLMs). The authors propose Sylheti-CAP, a context-aware prompting framework that integrates linguistic rules, a bilingual dictionary of 2,260 core vocabulary items and idioms, and authenticity checks into prompts. Extensive experiments demonstrate that Sylheti-CAP consistently improves translation quality across five advanced LLMs (GPT-4.1, LLaMA 4, Grok 3, DeepSeek V3.2, and Gemini 2.5 Flash) and evaluation metrics (BLEU, METEOR, ChrF). Human evaluations and MQM analysis confirm that Sylheti-CAP reduces hallucinations, mistranslations, and awkward phrasing, establishing it as a scalable solution for dialectal and low-resource machine translation.

## Method Summary
The authors systematically evaluate five advanced LLMs for translating between Bangla and Sylheti, a low-resource dialect of Bangla. They propose Sylheti-CAP, a context-aware prompting framework that integrates linguistic rules, a bilingual dictionary of 2,260 core vocabulary items and idioms, and an authenticity check into prompts. The framework is compared against zero-shot, few-shot, and chain-of-thought strategies across multiple models and metrics. Extensive experiments demonstrate that Sylheti-CAP consistently improves translation quality, reducing hallucinations and mistranslations while outperforming baseline approaches.

## Key Results
- Sylheti-CAP improves BLEU scores by 0.1-0.2 points and reduces hallucinations to 12.6-13.8% versus 15-17% for baselines
- Human evaluations show Sylheti-CAP achieves 46-56% "Good" ratings vs. lower for baselines
- MQM scores drop from 2.54→1.62 (Ben→Syl) indicating fewer errors with Sylheti-CAP
- Persistent directional asymmetry: Sylheti→Bangla BLEU scores ~1.3-1.4× higher than Bangla→Sylheti

## Why This Works (Mechanism)

### Mechanism 1: Structured Linguistic Rule Injection
Embedding explicit grammatical and phonological rules into prompts enables LLMs to apply dialect-specific transformations more reliably than relying on parametric knowledge alone. Sylheti-CAP provides 12 structured rules covering pronoun substitution, copula transformations, verb deaspiration, negation patterns, and SOV word order enforcement. These rules are presented as explicit condition-action pairs that the model can reference during generation. Evidence shows this reduces normalization errors (e.g., "েময়াডা" → "ফুিড়টা").

### Mechanism 2: Dictionary-Based Lexical Grounding
Providing explicit word-level mappings reduces hallucinations and improves lexical accuracy by constraining generation to attested dialectal vocabulary. A bilingual dictionary of 2,260 Bangla⇔Sylheti word pairs derived from parallel corpora is embedded in the prompt. This provides concrete retrieval targets for dialect-specific terms, reducing reliance on parametric knowledge which may be sparse or incorrect for low-resource dialects. Evidence shows hallucination rates drop to 12.6-13.8% with Sylheti-CAP versus 15-17% for baselines.

### Mechanism 3: Authenticity Constraints Reducing Standard-Language Normalization
Explicit instructions prioritizing "natural spoken Sylheti" over "literal translation" help counteract LLMs' bias toward standard language forms acquired during pre-training. Step 3 of Sylheti-CAP includes meta-instructions: "Ensure the translation sounds like natural Sylheti speech, not written Bangla." This creates a soft constraint encouraging dialectal authenticity and discouraging normalization to standard Bangla. Evidence shows this improves human preference ratings and reduces MQM error scores.

## Foundational Learning

**In-Context Learning (ICL) for Low-Resource Languages**
- Why needed here: Sylheti-CAP operates entirely through prompting without fine-tuning. Understanding ICL capacity and limits is essential for predicting when this approach will succeed versus requiring parameter updates.
- Quick check question: Why might few-shot examples perform worse than structured rules for dialect translation? (Answer: Few-shot provides implicit patterns that may not generalize to unseen vocabulary; explicit rules provide compositional guidance.)

**Dialect-Level vs. Language-Level Translation**
- Why needed here: Sylheti shares vocabulary and structure with Bangla but has systematic differences. Setting appropriate expectations requires understanding this intermediate difficulty level—harder than standard language pairs but easier than unrelated low-resource languages.
- Quick check question: What percentage of words in Table 8 are identical between Bangla and Sylheti, and how does this affect translation difficulty compared to translating between unrelated languages?

**Hallucination in Low-Resource MT**
- Why needed here: The paper specifically measures hallucination reduction as a key outcome. Understanding what constitutes hallucination (vs. mistranslation vs. omission) is necessary for interpreting results and debugging failures.
- Quick check question: If a model translates "আমার বাফর" (my father) as "আমার আব্বার" (my father's), is this a hallucination or a mistranslation? What about if it produces "আমার বাফর খুব ভালো" when the source says nothing about "very good"?

## Architecture Onboarding

**Component map:**
Input Sentence (Bangla) -> Prompt Constructor (Linguistic Rulebook + Bilingual Dictionary + Input Sentence + Authenticity Instructions) -> Target LLM -> Generated Translation (Sylheti) -> Evaluation Pipeline (Automatic + Human + MQM + LLM-as-Judge)

**Critical path:**
1. Dictionary construction: Merge parallel corpora (Vashantor + ONUBAD + Sylheti Dataset = 8,107 sentences), extract unique word pairs
2. Rulebook formulation: Requires linguistic expertise to identify and formalize dialect-specific transformations
3. Prompt template design: Balance comprehensiveness vs. context window (~1,500-2,000 tokens)
4. Translation generation: Temperature=1 across all experiments
5. Multi-metric evaluation: Run all four evaluation methods

**Design tradeoffs:**
- Prompt length vs. dictionary coverage: Full 2,260-entry dictionary likely exceeds practical context
- Prompting-only vs. fine-tuning: Paper explicitly notes limitation (Section 7)
- Generalizability vs. specificity: Rules are Sylheti-specific; applying to other dialects requires rebuilding
- Single-model vs. ensemble: Paper evaluates 5 models independently

**Failure signatures:**
- High BLEU, low human preference: Model produces fluent output but wrong dialect
- Persistent directional asymmetry: If Bangla→Sylheti BLEU remains >25% lower than Sylheti→Bangla
- MQM error clustering: If "mistranslation" penalties dominate, focus on dictionary expansion
- Hallucination rate >15%: Indicates dictionary coverage insufficient

**First 3 experiments:**
1. Baseline replication: Implement all four prompting strategies on 375-sentence Vashantor test set with at least two models
2. Ablation study: Run Sylheti-CAP variants removing one component at a time (rules-only, dictionary-only, no authenticity)
3. Dictionary coverage analysis: Extract all unique tokens from test set; compute percentage covered by 2,260-entry dictionary

## Open Questions the Paper Calls Out
- Can Sylheti-CAP generalize effectively to other low-resource Bangla dialects with similar rulebook and dictionary injection?
- Would combining Sylheti-CAP with fine-tuning or adapter-based methods yield additive improvements over prompting alone?
- What is the minimum dictionary coverage required for Sylheti-CAP to achieve meaningful translation quality gains?
- How does prompt length and complexity in Sylheti-CAP affect inference cost and translation latency compared to simpler baselines?

## Limitations
- The framework relies solely on prompting without model fine-tuning, limiting potential improvements
- Dictionary coverage may be incomplete due to context window constraints, affecting rare vocabulary
- Exact prompt template details (rule ordering, exemplar selection) are not fully specified, creating replication uncertainty
- Human evaluation sample size (200 samples with 3 native speakers) and annotator details are limited

## Confidence

**High Confidence (95%):** LLMs struggle with low-resource dialect translation due to parametric knowledge gaps. Evidence: BLEU gaps of 31-38% favoring Sylheti→Bangla direction, systematic normalization errors across all baselines.

**Medium Confidence (75%):** Sylheti-CAP consistently improves translation quality across models and metrics. Evidence: Statistically significant improvements across multiple evaluation methods, though exact magnitude depends on prompt details.

**Low Confidence (60%):** Sylheti-CAP reduces hallucinations to 12.6-13.8% versus 15-17% for baselines. Evidence: Observed reduction, but measurement methodology and dictionary coverage limitations create uncertainty.

## Next Checks
1. **Ablation Study on Prompt Components:** Systematically remove each Sylheti-CAP component (rules, dictionary, authenticity instructions) individually and measure BLEU degradation to identify which mechanism contributes most to improvements.

2. **Coverage Analysis of Dictionary Effectiveness:** Extract all unique tokens from the test set and compute coverage percentage by the 2,260-word dictionary. For uncovered tokens, analyze whether the model produces correct Sylheti forms or errors to establish the upper bound of prompt-only approaches.

3. **Template Robustness Testing:** Vary prompt template parameters systematically (rule ordering, dictionary entry quantity, exemplar selection method) across multiple model families to test whether performance improvements are robust to template variations or specific to exact implementation details.