---
ver: rpa2
title: 'Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for
  Persistent Conversational Context'
arxiv_id: '2508.12630'
source_url: https://arxiv.org/abs/2508.12630
tags:
- discourse
- memory
- semantic
- coreference
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Semantic Anchoring, a hybrid agentic memory
  architecture that improves long-term conversational recall by integrating explicit
  linguistic structures into retrieval. The method combines dense vector embeddings
  with symbolic cues from dependency parsing, coreference resolution, and discourse
  tagging, stored in a hybrid index.
---

# Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context

## Quick Facts
- arXiv ID: 2508.12630
- Source URL: https://arxiv.org/abs/2508.12630
- Authors: Maitreyi Chatterjee; Devansh Agarwal
- Reference count: 38
- Primary result: Hybrid agentic memory architecture combining dense embeddings with linguistic cues achieves 83.5% factual recall and 80.8% discourse coherence on long conversational datasets

## Executive Summary
This paper introduces Semantic Anchoring, a hybrid memory architecture that significantly improves long-term conversational recall by integrating explicit linguistic structures into retrieval. The system combines dense vector embeddings with symbolic cues from dependency parsing, coreference resolution, and discourse tagging, stored in a hybrid index. Retrieval uses a weighted fusion of semantic similarity, entity matching, and discourse alignment. Evaluated on MultiWOZ-Long and DialogRE-L, Semantic Anchoring achieves 83.5% factual recall and 80.8% discourse coherence, outperforming strong RAG baselines by up to 18% on these metrics.

## Method Summary
The method processes each utterance through syntactic, coreference, and discourse parsers to extract structured memory entries. These entries are stored in a dual index: FAISS for dense vectors and Whoosh for symbolic features (dependency triples, coreference IDs, discourse tags). During retrieval, the system computes a weighted fusion score combining cosine similarity, entity matching, and discourse alignment. The λ weights are tuned via grid search on validation data. The approach addresses the limitation of dense-only retrieval in maintaining entity and discourse continuity across multi-session dialogues.

## Key Results
- Achieves 83.5% factual recall and 80.8% discourse coherence on long conversational datasets
- Outperforms strong RAG baselines by up to 18% on factual recall and discourse coherence metrics
- Ablation studies show discourse and coreference features are especially critical for performance
- Human evaluations rate continuity satisfaction at 4.3/5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating coreference resolution into memory indexing improves factual recall by unifying entity mentions across sessions, overcoming a key limitation of dense-only retrieval.
- Mechanism: The system uses an end-to-end coreference resolver to assign persistent IDs (CorefIDs) to all entity mentions (pronouns, names). This allows a query containing "he" or "it" to retrieve prior utterances containing the antecedent entity, even without semantic similarity.
- Core assumption: The underlying coreference resolution model is sufficiently accurate for the target dialogue domain; its errors are not catastrophic.
- Evidence anchors:
  - [abstract] "Our approach combines... coreference resolution to create structured memory entries."
  - [section 5.6] "Coreference mistakes (27%) and parsing errors (19%) are the most common failure modes."
- Break condition: Performance degrades significantly in domains with high coreference ambiguity (e.g., many participants of the same gender) or when dialogue is heavily disjointed.

### Mechanism 2
- Claim: Adding discourse relation tags to the retrieval score function improves the coherence of multi-turn interactions.
- Mechanism: A PDTB-style discourse parser labels utterances with relational roles (e.g., Elaboration, Contrast, Cause). During retrieval, a weighted fusion score (λc · discourse match) prioritizes memories that provide contextually relevant follow-up, not just semantic matches.
- Core assumption: A static, pre-defined set of discourse relations captures the key logical structures needed for retrieval.
- Evidence anchors:
  - [abstract] "Retrieval uses a weighted fusion of semantic similarity, entity matching, and discourse alignment."
  - [section 3.1] Step 3 details "Tags discourse relations to encode conversational flow."
- Break condition: Fails when discourse relations are implicit, sarcastic, or non-standard (e.g., "Great, another early flight" interpreted as a request instead of a complaint).

### Mechanism 3
- Claim: A hybrid index enables interpretable retrieval by providing explicit, symbolic anchors for recall decisions.
- Mechanism: The architecture maintains both a dense vector index (FAISS) and a symbolic inverted index (Whoosh). A final fusion step combines scores from both. This allows developers to inspect why a memory was retrieved (e.g., it matched on a specific dependency triple), aiding debugging.
- Core assumption: The computational overhead of running and maintaining two separate indexes is acceptable for the target application's latency budget.
- Evidence anchors:
  - [section 3.3] "The hybrid memory store comprises two components: 1. Dense Index... 2. Symbolic Index..."
  - [abstract] "...hybrid agentic memory architecture that enriches vector-based storage with explicit linguistic cues..."
- Break condition: Becomes unwieldy if the symbolic index grows too large or sparse, leading to retrieval latency issues or poor fusion score calibration.

## Foundational Learning

- **Coreference Resolution**
  - Why needed here: This is the foundational NLP task that enables the system to connect a pronoun in a later session (e.g., "book it again") to a specific, named entity in a prior session, which is critical for cross-session memory.
  - Quick check question: Given a dialogue where "Project A" is introduced, how would the system process the later query "How is it going?"

- **Dependency Parsing**
  - Why needed here: Used to extract structured triplets (head lemma, dep label, child lemma) for the symbolic index, allowing for matching based on grammatical structure (e.g., identifying a 'booker' of a 'hotel').
  - Quick check question: What specific information is stored in the symbolic index derived from the dependency parse of the sentence "John booked the hotel"?

- **Rank Fusion**
  - Why needed here: This is the core retrieval logic, combining similarity scores from heterogeneous sources (dense vectors, symbolic matches) into a single ranking.
  - Quick check question: What are the three components of the final retrieval score, and how are their relative weights (λs, λe, λc) determined?

## Architecture Onboarding

- **Component map:** Syntactic/Coref/Discourse Parsers → Memory Entry Creator → Dual Indexer (FAISS + Whoosh) → Weighted Fusion Retriever → LLM Context
- **Critical path:** The most sensitive component is the Weighted Fusion Retriever. A bug or poor tuning of the λ weights (λs, λe, λc) can negate the benefits of the entire symbolic indexing pipeline by, for example, over-weighting sparse symbolic matches.
- **Design tradeoffs:** The system trades off retrieval latency (~175ms total) and pipeline complexity (maintaining multiple NLP models and indexes) for improved factual recall and interpretability over standard RAG.
- **Failure signatures:** Look for retrieval sets that are completely disjoint from the query's entities (coref failure), or that miss obvious logical follow-ups (discourse failure). Check for error logs related to coref cluster merging or parsing timeouts.
- **First 3 experiments:**
  1. **Ablation on Index Types:** Run the full pipeline against a Vector-RAG baseline on a held-out set of long dialogues. Then, systematically disable the symbolic index to quantify the contribution of the linguistic features.
  2. **Hyperparameter Grid Search:** Using a validation set, run a grid search over the λ weights (λs, λe, λc) to find the optimal fusion balance for the target domain, as the paper suggests these require tuning.
  3. **Error Audit on Coref:** Manually evaluate 50-100 retrieval failures and classify them as coref, parsing, or discourse errors to establish a baseline for model improvement efforts.

## Open Questions the Paper Calls Out

- **Can Semantic Anchoring be adapted for incremental, real-time parsing in live conversation?**
  - Basis in paper: [explicit] The conclusion explicitly calls out "integrating incremental parsing for real-time adaptability" as a future direction.
  - Why unresolved: Current pipeline processes utterances through multiple sequential NLP components (dependency parsing, coreference, discourse tagging), which adds latency (~175ms total) that may not scale for truly real-time interaction.
  - What evidence would resolve it: Implementation of streaming/incremental versions of each component with latency <50ms per turn, evaluated on live human-AI conversations with human-perceived responsiveness ratings.

- **How robust is semantic anchoring when applied to morphologically rich or structurally divergent languages?**
  - Basis in paper: [explicit] Conclusion identifies "scaling to multilingual contexts" as a future direction; current evaluation uses only English datasets.
  - Why unresolved: Dependency parsing conventions, coreference phenomena, and discourse markers differ substantially across languages; entity tracking via coreference chains may not work identically in pro-drop languages.
  - What evidence would resolve it: Cross-lingual evaluation on non-English multi-session dialogue corpora (e.g., Chinese, Arabic, Finnish), measuring whether FR and DC gains persist under different linguistic structures.

- **Can user-editable memory mechanisms be integrated without disrupting linguistically-anchored retrieval?**
  - Basis in paper: [explicit] Conclusion lists "enabling user-editable memories for greater transparency" as a future direction.
  - Why unresolved: Edits to memory entries (e.g., correcting facts, removing sensitive data) may invalidate coreference chains or discourse relations; it is unclear how to maintain index consistency after user modifications.
  - What evidence would resolve it: Prototype with edit functionality, measuring retrieval accuracy before and after user edits, plus user satisfaction with correction transparency.

## Limitations
- Performance claims hinge on proprietary datasets (MultiWOZ-Long, DialogRE-L) that are not publicly available, preventing independent validation
- Coreference and discourse parsing components are treated as black boxes with undisclosed accuracy and training data
- Weighted fusion parameters (λs, λe, λc) were tuned on validation splits, but sensitivity to domain shifts is unexplored
- 175ms latency claim assumes static indexes; dynamic memory updates or real-time indexing are not addressed

## Confidence
- **High Confidence:** The hybrid retrieval architecture (combining dense and symbolic indices) is technically sound and well-explained. The ablation study demonstrating the importance of linguistic features is robust and reproducible.
- **Medium Confidence:** The reported performance improvements (83.5% recall, 80.8% coherence) are plausible given the architectural advantages, but are difficult to verify without access to the evaluation datasets and exact implementation details.
- **Low Confidence:** The generalizability of the coreference and discourse parsing modules across domains with different entity types, discourse styles, or ambiguity profiles is uncertain.

## Next Checks
1. **Dataset Release and Replication:** Request or reconstruct the MultiWOZ-Long and DialogRE-L datasets to enable independent replication of the reported performance metrics.
2. **Component Robustness Testing:** Evaluate the coreference and discourse parsing modules on out-of-domain dialogues (e.g., technical support, casual conversation) to measure degradation and identify failure patterns.
3. **Weight Sensitivity Analysis:** Conduct a formal sensitivity analysis on the λ fusion weights across different dataset sizes and domains to determine stability and optimal tuning strategies.