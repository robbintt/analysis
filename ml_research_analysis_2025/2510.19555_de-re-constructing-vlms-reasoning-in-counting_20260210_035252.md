---
ver: rpa2
title: '[De|Re]constructing VLMs'' Reasoning in Counting'
arxiv_id: '2510.19555'
source_url: https://arxiv.org/abs/2510.19555
tags:
- objects
- counting
- vlms
- accuracy
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why Vision-Language Models (VLMs) struggle
  with visual counting tasks. The authors evaluate seven state-of-the-art VLMs on
  controlled synthetic datasets and find that counting accuracy is highly sensitive
  to object class, attributes, position, and distractors.
---

# [De|Re]constructing VLMs' Reasoning in Counting

## Quick Facts
- **arXiv ID**: 2510.19555
- **Source URL**: https://arxiv.org/abs/2510.19555
- **Reference count**: 40
- **Primary result**: VLMs struggle with counting due to representation-to-output mapping issues, which can be improved by fine-tuning only the output layer

## Executive Summary
This paper investigates the fundamental limitations of Vision-Language Models (VLMs) in performing visual counting tasks. Through systematic evaluation on controlled synthetic datasets, the authors demonstrate that counting accuracy is highly sensitive to object characteristics and scene complexity. Layer-wise analysis reveals that the primary source of counting errors stems from incorrect mapping between the final layer representations and the output space, rather than from earlier stages of processing. The proposed solution of fine-tuning only the output layer achieves significant accuracy improvements of up to 21% on synthetic data while maintaining consistent gains on real-world datasets.

## Method Summary
The authors evaluated seven state-of-the-art VLMs on controlled synthetic datasets designed to isolate different aspects of counting difficulty. They conducted a comprehensive layer-wise analysis to identify where errors originate in the model architecture. The proposed solution involves fine-tuning only the output layer of the models, avoiding the computational expense and potential degradation of full fine-tuning. The evaluation considered various factors including object class, attributes, position, and distractors to understand their impact on counting performance.

## Key Results
- Counting accuracy varies significantly with object class, attributes, position, and distractors
- Layer-wise analysis identifies incorrect mapping of last-layer representations to output space as the main source of errors
- Fine-tuning only the output layer improves accuracy by up to 21% on synthetic data
- The approach achieves consistent gains on real-world datasets while avoiding full fine-tuning

## Why This Works (Mechanism)
The effectiveness of the proposed solution stems from the discovery that VLMs' counting failures are not due to fundamental limitations in visual perception or language understanding, but rather from a specific architectural bottleneck at the final mapping stage. The models successfully process visual information and understand the counting task conceptually, but struggle to translate their internal representations into accurate numerical outputs. By focusing fine-tuning efforts on the output layer, the approach directly addresses this bottleneck without disrupting the well-learned lower-level features and reasoning capabilities.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Neural architectures that process both visual and textual inputs simultaneously, needed to understand how multimodal reasoning works in modern AI systems
- **Layer-wise analysis**: Technique for examining model behavior at different depth levels, required to identify where errors originate in complex architectures
- **Output space mapping**: The process of converting internal representations to final predictions, crucial for understanding the bottleneck in counting tasks
- **Fine-tuning**: Adapting pre-trained models to specific tasks, important for efficient model adaptation without full retraining
- **Synthetic dataset design**: Creating controlled test environments, essential for isolating specific factors that affect model performance
- **Representation-to-output mapping**: The critical transformation between learned features and task-specific predictions, fundamental to understanding model limitations

## Architecture Onboarding
- **Component map**: Vision encoder -> Transformer layers -> Output layer -> Numerical prediction
- **Critical path**: Visual features → Multi-modal fusion → Final representation → Output mapping → Count prediction
- **Design tradeoffs**: Full fine-tuning (better accuracy, higher cost) vs. Output layer fine-tuning (good accuracy, lower cost)
- **Failure signatures**: Correct visual processing but incorrect numerical output, sensitivity to object attributes and scene complexity
- **First experiment**: Test counting accuracy on simple synthetic datasets with varying object classes
- **Second experiment**: Perform layer-wise ablation to identify error sources
- **Third experiment**: Apply output layer fine-tuning and measure accuracy improvements

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond its primary findings and proposed solution.

## Limitations
- Evaluation is confined to synthetic datasets, which may not fully capture real-world complexity
- The fine-tuning approach targets only the output layer, leaving questions about scalability to more complex reasoning tasks
- The analysis does not explore alternative architectural modifications or their impact on other reasoning capabilities
- The sample size and diversity of tested models are relatively limited

## Confidence
- **High confidence**: Counting errors stem from representation-to-output mapping issues, supported by layer-wise analysis
- **Medium confidence**: Fine-tuning the output layer is an effective solution, validated on controlled datasets
- **Low confidence**: Targeted architectural modifications alone are sufficient for improving reasoning in VLMs, given the narrow study scope

## Next Checks
1. Test the fine-tuning approach on real-world datasets with diverse and complex visual contexts to assess its robustness
2. Evaluate the impact of the proposed solution on other reasoning tasks (e.g., comparison, spatial reasoning) to ensure no degradation in performance
3. Explore alternative architectural modifications (e.g., attention mechanisms, multi-step reasoning) to compare their effectiveness against the output-layer fine-tuning approach