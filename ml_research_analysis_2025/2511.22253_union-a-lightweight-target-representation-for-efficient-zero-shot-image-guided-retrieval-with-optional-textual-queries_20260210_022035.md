---
ver: rpa2
title: 'UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided
  Retrieval with Optional Textual Queries'
arxiv_id: '2511.22253'
source_url: https://arxiv.org/abs/2511.22253
tags:
- image
- retrieval
- union
- target
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UNION, a lightweight target representation
  for efficient zero-shot image-guided retrieval with optional textual queries (IGROT),
  which unifies Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR).
  The core idea is to enhance the target image embedding by fusing it with a null-text
  prompt through a lightweight Transformer-MLP stack, aligning target features with
  multimodal queries.
---

# UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries

## Quick Facts
- arXiv ID: 2511.22253
- Source URL: https://arxiv.org/abs/2511.22253
- Reference count: 39
- Key result: 38.5 mAP@50 on CIRCO, 82.7 mAP@200 on Sketchy with only 5k samples

## Executive Summary
This paper introduces UNION, a lightweight target representation for zero-shot image-guided retrieval with optional textual queries (IGROT). The core innovation is a Transformer-MLP module that adaptively fuses target image embeddings with null-text prompts to create semantically aligned representations. This approach unifies Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR) under minimal supervision, achieving competitive results without modifying pretrained vision-language models.

## Method Summary
UNION enhances target image embeddings by fusing them with null-text prompt embeddings through a lightweight Transformer-MLP stack. The method operates on frozen VLM encoders (CLIP-B/L/14, BLIP ViT-B), concatenating target image embeddings with null-text embeddings and passing them through a 2-layer Transformer followed by pooling and MLP weighting. The final representation is an adaptive interpolation between visual and null-text features. Training uses BBC loss with 5k samples from LlavaSCo (for CIR) and Training-Sketchy (for SBIR), requiring only 2 epochs. SBIR uses fixed prompt "a real image of sketch" during training but null text at inference.

## Key Results
- Achieves 38.5 mAP@50 on CIRCO validation set
- Achieves 82.7 mAP@200 on Sketchy benchmark
- Outperforms many heavily supervised baselines with only 5k training samples
- Shows consistent improvements across multiple backbone architectures (CLIP-B, CLIP-L, BLIP)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Null-text conditioning bridges the modality gap between unimodal target embeddings and multimodal query representations.
- Mechanism: The null-text embedding (empty string through VLM encoder) introduces a neutral linguistic prior that implicitly projects target features into the same multimodal embedding space as composed queries, reducing asymmetry in similarity computation.
- Core assumption: The VLM's null-text embedding captures a meaningful latent linguistic context that can semantically align with fused image-text queries.
- Evidence anchors: [abstract] "fuses the image embedding with a null-text prompt... enhancing semantic alignment with multimodal queries"; [Section III-B] "The null-text embedding introduces a neutral linguistic prior that implicitly brings the target closer to the multimodal space used for queries"

### Mechanism 2
- Claim: Learned adaptive weighting outperforms naive feature summation for target representation.
- Mechanism: The Transformer-MLP stack produces instance-specific weights (w_t, w_η) that interpolate between visual and null-text features based on the concatenated representation, allowing the model to dynamically adjust semantic emphasis per target.
- Core assumption: The relationship between image content and optimal null-text contribution is learnable from limited supervision (5k samples).
- Evidence anchors: [Section III-B] "The final UNION feature is a learned interpolation between the image and null-text features: U = w_t · e_t + w_η · e_η"; [Figure 7] Sum-based features consistently underperform (e.g., QuickDraw 20.2 vs. UNION 33.4 mAP)

### Mechanism 3
- Claim: Caption refinement amplifies UNION's effectiveness by strengthening text-image grounding during training.
- Mechanism: LLaVA-generated detailed captions provide richer semantic supervision, enabling the UNION module to learn better alignments between compositional queries and enriched target representations.
- Core assumption: Caption quality, not quantity, determines generalization in low-data regimes.
- Evidence anchors: [Section IV-E] "BLIP's average score increases from 35.4 to 48.6 (original), and from 32.1 to 49.8 (UNION)" with LlavaSCo captions

## Foundational Learning

- Concept: Contrastive learning with Batch-Based Classification (BBC) loss
  - Why needed here: UNION is trained via BBC loss, which pushes each query toward its correct target while repelling others in the batch. Understanding negative sampling dynamics is essential for debugging retrieval failures.
  - Quick check question: Given a batch of 32 query-target pairs, can you compute the BBC loss gradient for a single positive pair?

- Concept: Vision-language embedding spaces (CLIP/BLIP)
  - Why needed here: UNION operates on pretrained VLM embeddings and must preserve their semantic structure. Misalignment between backbone choice and UNION initialization can destabilize training.
  - Quick check question: Why might CLIP-L benefit more from UNION than BLIP, as observed in ablations?

- Concept: Transformer pooling strategies (first-token vs. average)
  - Why needed here: The UNION Transformer outputs R^(B×2×D), which must be pooled to R^(B×D) before MLP weighting. Pooling choice affects which cross-modal interactions are preserved.
  - Quick check question: If pooling discards position information, how might this affect the model's ability to distinguish image vs. null-text contributions?

## Architecture Onboarding

- Component map: VLM Encoder (frozen) -> UNION Transformer -> Pool -> MLP -> Adaptive weights -> UNION feature -> BBC loss
- Critical path: 1) Extract e_t (target image) and e_η (null-text) from frozen VLM encoder; 2) Concatenate → e_tη = [e_t; e_η]; 3) Pass through UNION Transformer → f_tη; 4) Pool → single vector per instance; 5) MLP → weights w_t, w_η; 6) Compute U = w_t · e_t + w_η · e_η; 7) Compute similarity with fused query, apply BBC loss
- Design tradeoffs: Inference latency adds 271-656 seconds for embedding large image pools; CLIP-L shows strongest UNION gains but BLIP converges faster with LlavaSCo captions; 5k samples sufficient but caption quality is critical
- Failure signatures: UNION underperforms baseline without LlavaSCo captions; SBIR training/inference mismatch; weight collapse (w_t→0 or w_t→1)
- First 3 experiments: 1) Reproduce Table I entry for TransAgg + UNION (BLIP-B) on CIRCO validation; target mAP@50 ≈ 34-38.5; 2) Compare original vs. sum vs. UNION features on CLIP-B with/without LlavaSCo captions; 3) Train on LlavaSCo only, evaluate on Sketchy zero-shot

## Open Questions the Paper Calls Out
- How can the inference latency introduced by the UNION feature extraction be optimized to support real-time retrieval applications?
- Can efficient captioning strategies replace LLaVA-1.6 Mistral to reduce the dataset construction costs while maintaining the semantic richness of LlavaSCo?
- Does the UNION representation continue to yield significant improvements when trained on datasets containing millions of triplets, or is it primarily effective in the low-data regime?

## Limitations
- Inference latency of 271-656 seconds for embedding large image pools creates trade-off between representation quality and computational efficiency
- Dataset construction requires running LLaVA which is computationally expensive and time-consuming at scale
- The approach relies on high-quality captions for effective training, limiting applicability in low-resource captioning scenarios

## Confidence
- **High confidence:** The core architectural contribution (UNION module with learned adaptive weighting) is well-specified and experimentally validated across multiple benchmarks
- **Medium confidence:** Claims about caption refinement's amplifying effect and null-text alignment benefits are supported by internal ablations but lack external validation
- **Low confidence:** The assertion that UNION "unifies" CIR and SBIR under minimal supervision is overstated—the approach requires task-specific prompt engineering

## Next Checks
1. Test null-text embedding stability across three different VLMs (e.g., CLIP, BLIP, OpenCLIP) to verify the alignment mechanism isn't architecture-specific
2. Conduct a controlled ablation comparing LLaVA-generated captions vs. human-written captions vs. no captions on the same 5k-sample training set
3. Evaluate UNION on a third retrieval task (e.g., fine-grained visual categorization with optional attributes) to test true zero-shot generalization beyond CIR/SBIR