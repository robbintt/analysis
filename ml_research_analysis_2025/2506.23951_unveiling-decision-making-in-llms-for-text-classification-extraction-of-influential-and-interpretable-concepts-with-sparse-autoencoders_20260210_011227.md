---
ver: rpa2
title: 'Unveiling Decision-Making in LLMs for Text Classification : Extraction of
  influential and interpretable concepts with Sparse Autoencoders'
arxiv_id: '2506.23951'
source_url: https://arxiv.org/abs/2506.23951
tags:
- concepts
- features
- concept
- classification
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ClassifSAE, a supervised sparse autoencoder
  (SAE) method tailored for extracting interpretable concepts from large language
  models (LLMs) fine-tuned for sentence classification. The approach incorporates
  a joint classifier and an activation rate sparsity loss to encourage task-relevant
  and diverse concept features.
---

# Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders

## Quick Facts
- arXiv ID: 2506.23951
- Source URL: https://arxiv.org/abs/2506.23951
- Reference count: 40
- Key outcome: ClassifSAE achieves up to 33.23% conditional treatment effect and ConceptSim scores up to 0.1377, outperforming baselines in extracting interpretable concepts from fine-tuned LLMs for text classification.

## Executive Summary
This paper introduces ClassifSAE, a supervised sparse autoencoder method that extracts interpretable, causal concepts from LLMs fine-tuned for sentence classification. Unlike pre-trained SAE approaches, ClassifSAE trains directly on the classification dataset to uncover task-relevant features. The method incorporates a joint classifier and activation rate sparsity loss to encourage feature disentanglement and diversity. Evaluated on AG News and IMDB datasets using four Pythia LLMs, ClassifSAE demonstrates superior performance in both causality (up to 33.23% conditional treatment effect) and interpretability (ConceptSim scores up to 0.1377) compared to baselines like ICA, ConceptSHAP, and standard TopK SAE.

## Method Summary
ClassifSAE trains a sparse autoencoder directly on the classification dataset using the hidden states from a fine-tuned LLM's penultimate layer. The approach uses TopK activation (K=10) with a joint linear classifier on a subset of features (z_class) and an activation rate sparsity loss to promote task-relevant, diverse concepts. The SAE is trained to reconstruct the hidden states while the classifier uses only z_class features to reproduce the LLM's predictions. Training involves three loss components: reconstruction loss, classification loss, and activation rate sparsity loss with linearly decaying weights. The method differs from pre-trained SAE approaches by training in-domain and focusing on task-relevant features.

## Key Results
- ClassifSAE achieves up to 33.23% conditional treatment effect on the treated (TVD_cond), significantly higher than baseline methods
- ConceptSim interpretability scores reach up to 0.1377, demonstrating more precise and fine-grained concepts
- Maximum feature activation rate drops from 54.08% (standard SAE) to 16.30% (ClassifSAE), indicating better feature disentanglement
- Reconstruction accuracy remains high (>97%) while improving feature interpretability

## Why This Works (Mechanism)

### Mechanism 1: Joint Classifier for Task-Relevant Feature Clustering
Training a classifier jointly with the SAE on a restricted latent subset (z_class) concentrates task-relevant features into a small, interpretable set. The classifier receives only z_class ⊂ z as input and must reproduce the LLM's predictions, creating pressure for the encoder to map classification-relevant information into z_class while reconstruction handles remaining dimensions. This creates explicit gradient signals that reward feature discriminativeness.

### Mechanism 2: Activation Rate Sparsity Loss
Penalizing features that activate beyond a target rate (γ) prevents collapse into highly-correlated, over-active features and promotes diverse, discriminative concepts. For each feature j, the loss identifies the top ⌊γB⌋ activations in a batch and penalizes all activations outside this set, forcing the model to distribute information more evenly across features rather than relying on a few "catch-all" features.

### Mechanism 3: Task-Specific SAE Training
Training the SAE directly on the classification dataset extracts only task-relevant concepts, avoiding the need to filter thousands of generic features from pre-trained SAEs. By training on ~100K task-specific examples, the SAE learns a vocabulary of features directly shaped by the classification distribution, making the decoder columns directions that reconstruct task-relevant variations in hidden states.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs) and the Superposition Hypothesis**
  - Why needed here: ClassifSAE builds on the assumption that LLM hidden states superpose many features in a shared space; SAEs decompose these into interpretable directions.
  - Quick check question: Can you explain why a single neuron in an LLM might respond to multiple unrelated concepts, and how an SAE addresses this?

- **Concept: Causality via Ablation**
  - Why needed here: The paper evaluates extracted concepts by ablating (zeroing) feature activations and measuring prediction changes. Understanding conditional vs. global importance is critical.
  - Quick check question: What is the difference between ATE (Average Treatment Effect) and ATT (Average Treatment Effect on the Treated) in the context of feature ablation?

- **Concept: TopK Activation Function**
  - Why needed here: ClassifSAE uses TopK to enforce exact sparsity (k active features per input). This differs from L1 regularization and directly controls reconstruction capacity.
  - Quick check question: How does TopK differ from L1 sparsity in terms of feature selection behavior and dead feature risk?

## Architecture Onboarding

- **Component map:**
LLM (frozen, fine-tuned classifier) → h^ℓ (penultimate layer residual stream)
↓
Encoder: z = σ(W_enc·h + b_enc) [TopK, k=10]
↓
┌──────────────────┴──────────────────┐
↓                                      ↓
z_class (subset, e.g., 20 dims)         z_residual (remaining dims)
↓                                      ↓
Classifier g_θ (linear)                Decoder: ĥ = W_dec·z + b_dec
↓                                      ↓
Cross-Entropy Loss                   Reconstruction Loss (MSE)
↓                                      ↓
└──────────────────┬──────────────────┘
↓
L_sparse_feature (activation rate penalty)

- **Critical path:**
1. Extract hidden states h^ℓ from fine-tuned LLM at penultimate transformer block for all training sentences
2. Train SAE with TopK activation (k=10) jointly with linear classifier on z_class
3. Apply activation rate sparsity loss with γ=0.1 to prevent feature collapse
4. Evaluate on test set: compute RAcc (completeness), TVD_cond (causality), ConceptSim (interpretability)

- **Design tradeoffs:**
- z_class size: Smaller = more focused concepts but may miss nuance; larger = better coverage but dilutes interpretability. Paper uses 20.
- Expansion factor (d_sae / d_model): Larger hidden layer improves causality (TVD_cond) but increases compute. Paper tests 512–4096 for d_model=2048.
- γ (activation rate threshold): Lower = sparser features, better ConceptSim; too low hurts reconstruction. Paper uses 0.1.
- Loss weighting (λ₁, λ₂, λ₃): Imbalance causes classifier or reconstruction to dominate. Paper uses decaying λ₁, λ₃ over training.

- **Failure signatures:**
- Low RAcc (<90%): SAE reconstruction is poor; features don't capture hidden state structure. Check expansion factor or TopK value.
- High max activation rate (>50%): Feature collapse; activation rate sparsity not working. Check γ and λ₃.
- Low TVD_cond (~0): Features are not causal; classifier may be ignoring z_class. Verify classifier receives correct input slice.
- Dead features: Many features never activate. Increase auxiliary loss (ghost grads) or reduce sparsity pressure.

- **First 3 experiments:**
1. Baseline sanity check: Train vanilla TopK SAE (no classifier, no activation rate loss) on AG News hidden states. Measure RAcc and dead feature rate. Target: RAcc >95%.
2. Ablation of components: Train ClassifSAE with (a) only joint classifier, (b) only activation rate loss, (c) both. Compare TVD_cond and ConceptSim to isolate contribution of each component.
3. Cross-dataset transfer: Train ClassifSAE on AG News, evaluate extracted features on IMDB (same model). Measure ConceptSim drop to assess task-specificity of learned concepts.

## Open Questions the Paper Calls Out

- **Can ClassifSAE effectively extract interpretable concepts from model architectures other than the autoregressive Pythia family, such as encoder-only models (e.g., BERT) or larger commercial-scale LLMs?**
  - The authors state in the Limitations section: "Evaluating ClassifSAE on models with different architectures is therefore a promising direction for future work," and suggest testing on "larger models."

- **Does the supervised SAE approach transfer effectively to multimodal tasks where concept representations may span different data types (e.g., vision and language)?**
  - The authors explicitly list this as a direction for future work: "...assess the effectiveness of our method in settings beyond text classification, such as multimodal tasks, where concept extraction may operate over more complex representations."

- **How does ClassifSAE perform in low-data regimes where the classification dataset is significantly smaller than the ~100,000 examples used in the current study?**
  - The paper states the method requires a "moderate size" dataset (approx. 100k) and differs from methods that pre-train on large datasets, but does not test the lower bound of data requirements.

## Limitations

- The dimensionality of z_class (number of task-relevant features) is fixed at 20 across all experiments without justification or sensitivity analysis
- The exact architecture of the classifier g_θ (input/output dimensions) is underspecified, making exact replication difficult
- The activation rate sparsity mechanism, while showing improved ConceptSim scores, lacks direct theoretical grounding in SAE literature

## Confidence

- **High**: RAcc and TVD_cond results showing ClassifSAE outperforms baselines
- **Medium**: ConceptSim interpretability improvements (dependent on embedding quality and clustering stability)
- **Medium**: Claims about mechanism 1 (joint classifier for feature clustering)
- **Low**: Claims about mechanism 2 (activation rate sparsity preventing feature collapse)

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary z_class size (10, 20, 50) and γ (0.05, 0.1, 0.2) to map the performance landscape and confirm reported values are optimal rather than arbitrary.

2. **Ablation on feature selection**: Train ClassifSAE without z_class subsetting (use all features for classification) to quantify the specific contribution of task-relevant feature clustering to TVD_cond improvements.

3. **Cross-dataset concept stability**: Train on AG News, then evaluate extracted concepts on a held-out dataset (e.g., Yelp reviews) to measure ConceptSim degradation and confirm task-specificity vs. general linguistic concepts.