---
ver: rpa2
title: Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware
  Cost Function
arxiv_id: '2509.09197'
source_url: https://arxiv.org/abs/2509.09197
tags:
- biasing
- words
- tcpgen
- contextual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of improving rare word recognition
  in automatic speech recognition (ASR) by adapting contextual biasing models to synthetic
  data while mitigating overfitting caused by artifacts in synthetic audio. The core
  method introduces a keyword-aware loss function for the TCPGen contextual biasing
  approach, replacing the standard ASR loss with two complementary terms: a masked
  cross-entropy loss for predicting biased word probabilities and a binary classification
  loss for detecting biasing word positions.'
---

# Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function

## Quick Facts
- arXiv ID: 2509.09197
- Source URL: https://arxiv.org/abs/2509.09197
- Authors: Chin Yuen Kwok; Jia Qi Yip; Eng Siong Chng
- Reference count: 0
- Primary result: WER reduced from 29.71% to 11.81% on NSC Part 2 test set using keyword-aware loss function for TCPGen contextual biasing

## Executive Summary
This paper addresses the challenge of improving rare word recognition in automatic speech recognition (ASR) by adapting contextual biasing models to synthetic data while mitigating overfitting caused by artifacts in synthetic audio. The authors propose a novel keyword-aware loss function for the TCPGen contextual biasing approach that replaces the standard ASR loss with two complementary terms: a masked cross-entropy loss for predicting biased word probabilities and a binary classification loss for detecting biasing word positions. When applied to Whisper adapted to 10 hours of synthetic data, the method achieved a significant reduction in word error rate on the NSC Part 2 test set from 29.71% to 11.81%, representing up to 16.6% relative WER improvement.

## Method Summary
The method introduces a keyword-aware loss function that replaces the standard ASR loss with two complementary terms: a masked cross-entropy loss for predicting biased word probabilities and a binary classification loss for detecting biasing word positions. The TCPGen module outputs both P_ptr (which token to bias) and P_gen (interpolation weight), trained with separate losses rather than the ASR loss. The approach uses NSC-Part-2 (20h real train, 10h synthetic VITS-SPKSET1, 2h test) and DSTC2 (10h) datasets, with rare words defined as those not in a 10K common word list. Training employs SpeechBrain toolkit with whisper-small/Qwen-Audio, 2 epochs, batch=6, AGEM regularization, and learning rates of 0.005/0.0001 with AdamW optimizer.

## Key Results
- WER reduced from 29.71% to 11.81% on NSC Part 2 test set when using keyword-aware loss function
- Significant improvement in biasing accuracy with True Acceptance Rate of 67.9% and False Acceptance Rate of 9.59% at optimal α=0.7
- The approach achieves up to 16.6% relative WER improvement compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Replacing ASR loss with masked cross-entropy for biased word prediction reduces overfitting to synthetic audio artifacts by restricting optimization only to positions i∈K (tokens requiring biasing), eliminating redundant gradient signals that cause the model to memorize synthetic audio patterns.

### Mechanism 2
Binary classification loss for P_gen enables effective "when to bias" decisions and provides interpretable thresholding by explicitly training P_gen_i → 1 when position i needs biasing and → 0 otherwise, creating a 0.5 decision boundary enabling TAR/FAR analysis.

### Mechanism 3
Removing the (1-P_gen_i) scaling term for non-biased tokens prevents overbiasing suppression by preserving P_mdl probabilities for non-biased tokens when ℓ_gen pushes P_gen_i → 1 for biasing positions, avoiding complete suppression of all non-biased token probabilities.

## Foundational Learning

- **Contextual Biasing in ASR**: Why needed here - the entire method assumes understanding that contextual biasing adds a module to prioritize user-provided rare words (contact names, domain terms) via interpolation with base model outputs. Quick check: Given P_mdl(y_i) = [0.7, 0.2, 0.1] for tokens [A, B, C] and bias list containing B, what should the biasing module try to do?

- **Pointer Networks / TCPGen Architecture**: Why needed here - understanding that TCPGen outputs both P_ptr (which token to bias) and P_gen (interpolation weight) is essential to grasp why separate loss terms are needed. Quick check: Why can't a single loss term jointly optimize both "what to bias" and "when to bias"?

- **Synthetic Data Overfitting**: Why needed here - the paper's core problem is that TTS-generated audio contains artifacts (unnatural prosody, acoustic distortions) that biasing modules memorize instead of learning genuine pronunciation patterns. Quick check: Why would a model achieve near-zero training loss but poor test performance on synthetic data?

## Architecture Onboarding

- **Component map**: Audio -> Frozen Whisper Encoder -> h_enc -> [Whisper Decoder] -> h_dec_i-1, P_mdl(y_i) -> [TCPGen] -> P_ptr(y_i), P_gen_i -> [Interpolation] -> P(y_i)

- **Critical path**: The TCPGen module takes the decoder's last hidden state and must learn to (1) attend to correct bias list entries via P_ptr, and (2) gate biasing appropriately via P_gen. Both outputs are trained with separate losses, NOT the ASR loss.

- **Design tradeoffs**: α parameter (Eq. 3): Higher α → higher TAR (more true biasing) but higher FAR (more false biasing). Table 3 shows α=0.5 achieves TAR=61.6%/FAR=4.28% while α=0.7 achieves TAR=67.9%/FAR=9.59%.

- **Failure signatures**: P_ptr(y_i) always close to 0: Indicates biasing module failed to learn; typically occurs when fine-tuned base model already achieves near-zero ASR loss on training data. High FAR with low TAR: α set too low; biasing decisions are too conservative. WER increases with N (distractors): Model cannot distinguish true bias words from distractors.

- **First 3 experiments**: 
  1. Apply vanilla TCPGen (with ASR loss) to AGEM-adapted Whisper on synthetic data. Expect: No improvement, P_ptr near 0. This confirms the overfitting problem.
  2. Train TCPGen-2L with α ∈ {0.1, 0.3, 0.5, 0.7} on dev set. Plot TAR/FAR/B-WER tradeoffs to select operating point.
  3. Train on synthetic NSC-Part-2, evaluate on real NSC-Part-2 test set with N ∈ {10, 50, 100} distractors. Compare: (a) FT only, (b) AGEM, (c) AGEM + TCPGen (vanilla), (d) AGEM + TCPGen-2L. Expect: (d) achieves best B-WER with stable U-WER.

## Open Questions the Paper Calls Out

### Open Question 1
Can the keyword-aware loss function effectively mitigate overfitting in encoder-based architectures like RNN-Transducers, or is its utility limited to decoder-based models like Whisper? The authors note in the Introduction that prior biasing methods assume frame-level predictions or separate encoders, which do not apply to Whisper. The method is demonstrated only on decoder-based Whisper and Qwen-Audio (Section 3.2).

### Open Question 2
Does the proposed training objective remain robust when using synthetic data generated by lower-fidelity or diverse TTS systems with different artifact profiles? Section 3.1 specifies the use of a single TTS system (VITS) trained on the VCTK corpus. The authors attribute overfitting to "artifacts in the synthetic audio," but the solution is validated on only one type of synthetic artifact distribution.

### Open Question 3
Can the weighting factor $\alpha$ for the binary classification loss be dynamically optimized to better balance the trade-off between True Acceptance Rate (TAR) and False Acceptance Rate (FAR)? Section 2.2.1 introduces a static weight $\alpha$ to handle class imbalance, and Table 3 demonstrates that varying $\alpha$ creates a trade-off between WER and FAR that requires manual sweeping.

## Limitations

- The TCPGen module architecture details are only referenced to prior work without full implementation specifications, creating barriers to exact reproduction
- The method's effectiveness is demonstrated exclusively on NSC-Part-2 synthetic data generated with VITS-SPKSET1, with unknown performance on other synthetic datasets
- The study does not explore whether distractor quality (semantically similar vs. random words) affects performance differently from just quantity

## Confidence

**High Confidence Claims**:
- The keyword-aware loss function (ℓ_gen + ℓ_ptr) successfully reduces overfitting when training TCPGen on synthetic data
- The method achieves significant WER improvement (29.71% → 11.81%) on the NSC-Part-2 test set
- Binary classification framing enables explicit FAR/TAR analysis for biasing decisions

**Medium Confidence Claims**:
- The overfitting mechanism specifically stems from P_ptr learning spurious correlations at non-rare-word positions
- Removing (1-P_gen_i) scaling prevents overbiasing suppression
- The α=0.7 operating point provides optimal tradeoff for the NSC dataset

**Low Confidence Claims**:
- Generalizability to other synthetic datasets beyond NSC-Part-2
- Performance on real audio without synthetic pre-training
- Effectiveness when biasing lists contain many false positives or homographs

## Next Checks

**Check 1: Cross-Dataset Generalization** - Train the complete system on a different synthetic dataset (e.g., Librispeech TTS or custom VITS generation) and evaluate on NSC-Part-2 test set. Compare performance against training only on NSC-Part-2 synthetic data.

**Check 2: Ablation on Distractor Quality** - Create biasing lists with different distractor types: (a) random words, (b) semantically similar words to bias terms, (c) homophones. Train and evaluate with N=50 distractors to determine if the model's robustness depends on distractor characteristics rather than just quantity.

**Check 3: Real Data Validation** - Apply the fine-tuned model (trained on synthetic data with proposed loss) to real audio from NSC-Part-2 or other conversational datasets. Measure degradation compared to synthetic test performance to quantify the synthetic-to-real gap.