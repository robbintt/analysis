---
ver: rpa2
title: Differentially Private Wasserstein Barycenters
arxiv_id: '2510.03021'
source_url: https://arxiv.org/abs/2510.03021
tags:
- private
- wasserstein
- barycenter
- algorithm
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first algorithms for computing Wasserstein
  barycenters under differential privacy (DP). The authors propose two main approaches:
  a private coreset method using DP Wasserstein distance coresets, and an output perturbation
  method that adds Gaussian noise to barycenter outputs.'
---

# Differentially Private Wasserstein Barycenters

## Quick Facts
- arXiv ID: 2510.03021
- Source URL: https://arxiv.org/abs/2510.03021
- Reference count: 40
- Primary result: Introduces first DP algorithms for Wasserstein barycenters with coreset and output perturbation methods

## Executive Summary
This paper introduces the first algorithms for computing Wasserstein barycenters under differential privacy (DP). The authors propose two main approaches: a private coreset method using DP Wasserstein distance coresets, and an output perturbation method that adds Gaussian noise to barycenter outputs. Their coreset approach leverages dimensionality reduction via the Johnson-Lindenstrauss transform to handle high-dimensional data, while the output perturbation method exploits clustered structure in real-world datasets to achieve better utility. The theoretical results provide privacy-utility tradeoffs for both methods, with the output perturbation approach showing advantages when data is clustered. Experiments on synthetic data, MNIST, and large-scale U.S. population datasets demonstrate that both methods produce high-quality private barycenters, with the output perturbation method performing particularly well on clustered data.

## Method Summary
The paper presents two DP algorithms for Wasserstein barycenters: (1) A coreset-based method that constructs private coresets using hierarchical binary partitioning with Laplace noise, projects data to low dimensions via Johnson-Lindenstrauss transform, computes the barycenter in the projected space, and lifts the solution back to original dimensions. (2) An output perturbation method that computes the barycenter normally and adds Gaussian noise calibrated to the ℓ₂-sensitivity of the output. The coreset method provides pure ε-DP while the output perturbation method provides (ε,δ)-DP. For clustered data, the output perturbation can be enhanced by subsampling distributions and averaging multiple independent barycenter computations to reduce noise variance.

## Key Results
- Introduces first DP algorithms for Wasserstein barycenters with provable privacy guarantees
- Coreset method achieves ε-DP with error scaling as n^{-1/d} where d is dimension
- Output perturbation method achieves (ε,δ)-DP with error scaling as (md/ε²k²)^{p/2} for clustered data
- Empirical evaluation shows both methods produce high-quality private barycenters on synthetic, MNIST, and U.S. population datasets

## Why This Works (Mechanism)

### Mechanism 1: Private Coreset with JL Dimensionality Reduction
Reducing dimension via Johnson-Lindenstrauss (JL) transform preserves barycenter cost while enabling polynomial-time private computation in high dimensions. The JL transform projects data from d dimensions to d' = O(log n) dimensions while approximately preserving all pairwise distances. After projection, the barycenter problem becomes tractable even with exponential dependence on dimension. Private coresets are constructed for each distribution in the projected space, then the barycenter is computed and lifted back to the original space using the preserved transport weights. The solution structure (transport weights) is preserved under JL projection—specifically, cost(S) ≈_{1+γ} cost(Π♯S) for all solutions.

### Mechanism 2: Output Perturbation with Gaussian Mechanism
Adding Gaussian noise calibrated to barycenter sensitivity provides (ε,δ)-DP with utility that scales as (md/ε²k²)^{p/2} rather than suffering curse of dimensionality. The barycenter output is treated as a vector in R^{md} (m support points × d dimensions). The ℓ₂-sensitivity is bounded by √m/k (changing one datapoint affects at most 1/k of the mass in each support point). Gaussian noise N(0, σ²I) with σ² ∝ m·log(1/δ)/(εk)² is added.

### Mechanism 3: Clustered Structure Enables Subsampling Benefits
When input distributions are approximately clusterable, splitting each distribution into k' sub-distributions and averaging over kk' independent barycenter computations reduces effective noise while bounding approximation error. Under (m,Δ,c)-approximate clusterability, empirical distributions converge faster to their population counterparts. Splitting creates kk' "pseudo-distributions" that each receive independent Gaussian noise, but the noise variance scales with (εkk')² instead of (εk)². The bias from subsampling is bounded by the clusterability parameter.

## Foundational Learning

- **Wasserstein Distance and Barycenters**: The entire paper operates in Wasserstein space; understanding that the barycenter minimizes average Wasserstein distance (not Euclidean distance) is foundational. Quick check: Given two point clouds, can you explain why their Wasserstein barycenter is not simply the pointwise average?

- **Differential Privacy (ε-DP and (ε,δ)-DP)**: The coreset method provides pure ε-DP while output perturbation provides approximate (ε,δ)-DP; the distinction matters for composition and utility. Quick check: If you run two ε-DP algorithms on disjoint subsets of data, what is the total privacy guarantee? (Answer: Still ε-DP by parallel composition—Lemma C.1.)

- **Sensitivity and the Gaussian Mechanism**: Output perturbation requires computing ℓ₂-sensitivity of the barycenter function to calibrate noise. Quick check: Why is the sensitivity √m/k rather than m/k or 1/k?

## Architecture Onboarding

- **Component map**: Data → JL Projection → Private Coreset Construction → Barycenter Computation → Support Reconstruction
- **Critical path**: 1) Data validation: Ensure all points lie in B₀(1/2)—rescale if needed. 2) Choose method: Coreset if dimension is primary concern; Output perturbation if data is clustered. 3) Parameter selection: For output perturbation, set k' using optimal formula from Theorem 5.4. 4) Noise calibration: Use σ² = 2m·log(1.25/δ)/(εkk')² for subsampled output perturbation.
- **Design tradeoffs**: Coreset vs. Output Perturbation: Coreset gives pure ε-DP but suffers curse of dimensionality; output perturbation gives (ε,δ)-DP with better scaling for clustered data but requires clusterability assumptions. Subsampling k': Larger k' reduces noise but increases bias; optimal k' balances these. Support size m: Larger m captures more structure but increases both computational cost and noise.
- **Failure signatures**: Coreset method fails: Error plateaus or increases with n—likely d' too small or coreset quality degraded. Output perturbation fails: Cost significantly exceeds non-private baseline—check if data is actually clustered; k' may be too large. JL projection fails: Solutions in low dimension don't lift to good high-dim solutions—increase d' or check Theorem 4.4 assumptions.
- **First 3 experiments**: 1) Synthetic Gaussians (Figure 2): Replicate with 4 Gaussians in R^10, varying n, ε, d'. Verify error decreases as n^{-1/d} for coreset method. 2) Ablation on k': Fix clustered data, vary k' from 1 to n/m. Plot cost vs. k' to validate optimal k'* formula. 3) Clusterability stress test: Generate data with varying c (unclustered fraction). Identify the threshold where output perturbation degrades below coreset method.

## Open Questions the Paper Calls Out

- Can private Wasserstein barycenter algorithms be developed under the local or shuffle models of differential privacy? (Basis: explicit statement in conclusion)

- Can privatized versions of entropic OT or Gaussian-smoothed OT alleviate the curse of dimensionality for private Wasserstein barycenters when data lies near a low-dimensional subspace? (Basis: explicit identification as future work)

- How can private Wasserstein barycenter algorithms handle the setting where one individual contributes an entire probability measure rather than a single datapoint? (Basis: explicit note in conclusion)

- Can the uniform weight assumption be relaxed while maintaining efficient computation and provable privacy guarantees? (Basis: inferred from Section 3 discussion)

## Limitations

- The coreset method requires d' = O(log n) dimensions to be tractable, but this may still be too large for practical computation when n is large
- The output perturbation method's utility degrades exponentially with dimension when data lacks clusterable structure
- The clusterability assumption for the subsampled output perturbation method is unverifiable from empirical data without ground-truth clustering information

## Confidence

- **High Confidence**: Privacy guarantees (ε-DP for coreset, (ε,δ)-DP for output perturbation) follow standard mechanisms and composition theorems
- **Medium Confidence**: JL projection preserves barycenter structure based on known distance preservation properties, but specific application to DP barycenters hasn't been validated empirically
- **Low Confidence**: Clusterability assumptions and their impact on utility rely on idealized conditions that may not hold in real-world data

## Next Checks

1. **Dimensionality Sensitivity Analysis**: Systematically vary d from 10 to 100 in synthetic Gaussian experiments to identify the exact threshold where coreset error plateaus, validating the theoretical O(n^{-1/d}) scaling

2. **Clusterability Verification**: Implement the (m,Δ,c)-approximate clusterability test from the paper and apply it to MNIST digits and US census data to quantify how close these datasets come to satisfying theoretical assumptions

3. **Subsampling Optimal k' Validation**: Reproduce Figure 2 with varying k' values for clustered synthetic data, plotting cost vs. k' to empirically verify the optimal k'* formula from Theorem 5.4 against theoretical prediction