---
ver: rpa2
title: Combining LLMs with Logic-Based Framework to Explain MCTS
arxiv_id: '2505.00610'
source_url: https://arxiv.org/abs/2505.00610
tags:
- framework
- mcts
- queries
- logic
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LogiExExplainer, a novel framework that combines
  large language models (LLMs) with logic-based reasoning to explain Monte Carlo Tree
  Search (MCTS) decisions in sequential planning. The framework transforms natural
  language queries into logic expressions, evaluates search trees based on these expressions,
  and generates factually consistent explanations.
---

# Combining LLMs with Logic-Based Framework to Explain MCTS
## Quick Facts
- **arXiv ID:** 2505.00610
- **Source URL:** https://arxiv.org/abs/2505.00610
- **Reference count:** 23
- **Primary result:** LogiExExplainer achieves 2.40× better factual consistency and 7.92× better relevance than baseline LLMs for explaining MCTS decisions

## Executive Summary
LogiExExplainer is a novel framework that enhances the explainability of Monte Carlo Tree Search (MCTS) decisions by combining large language models (LLMs) with logic-based reasoning. The system transforms natural language queries into formal logic expressions, evaluates these against MCTS search trees, and generates explanations that are both factually consistent and relevant to user inquiries. Tested in a paratransit planning scenario, LogiExExplainer demonstrates significant improvements over baseline LLMs in explanation quality, with particular success in maintaining consistency with environmental dynamics and constraints.

## Method Summary
The LogiExExplainer framework operates through a multi-stage pipeline that bridges natural language understanding with formal logic reasoning. It first processes user queries using an LLM to generate intermediate logic expressions, then evaluates these expressions against MCTS search tree data to verify factual consistency. The framework incorporates a knowledge graph of the planning environment and uses rule-based and model-based reasoning to ensure explanations align with system dynamics. Finally, it generates natural language explanations that maintain both logical correctness and relevance to the original query, addressing the common issue of hallucinations in LLM-generated explanations.

## Key Results
- 2.40× improvement in factual consistency (FactCC) compared to baseline LLMs
- 7.92× improvement in relevance (BERTScore) when using Llama3.1 as backbone
- Successfully handles both post-hoc explanations and knowledge-based inquiries in paratransit planning domain

## Why This Works (Mechanism)
The framework's effectiveness stems from its integration of formal logic with LLM capabilities, creating a hybrid system that leverages the strengths of both approaches. By converting natural language queries into logic expressions, the system establishes a rigorous foundation for evaluating the factual correctness of potential explanations. This logical framework acts as a constraint satisfaction mechanism that filters out inconsistent or hallucinated explanations before they reach the user, while still allowing the LLM to generate natural, readable explanations that maintain the benefits of language model fluency and contextual understanding.

## Foundational Learning
**Logic Expression Generation** - Converting natural language queries into formal logic is essential for establishing a rigorous evaluation framework. Quick check: Verify that the generated logic expressions accurately capture the intent and constraints expressed in natural language queries across diverse examples.

**MCTS Search Tree Analysis** - Understanding the structure and statistics of MCTS search trees is crucial for extracting meaningful decision-making patterns. Quick check: Confirm that the logic-based evaluation correctly identifies relevant nodes and paths that correspond to user query intent.

**Factual Consistency Verification** - The ability to verify that explanations align with both the search tree data and environmental constraints prevents hallucination. Quick check: Test that explanations fail validation when presented with logically inconsistent or counterfactual scenarios.

**Knowledge Graph Integration** - Encoding environmental dynamics and constraints in a structured knowledge base enables accurate reasoning about system behavior. Quick check: Validate that the knowledge graph correctly captures all relevant constraints and relationships in the planning domain.

## Architecture Onboarding
**Component Map:** User Query -> LLM Logic Generator -> Logic Expression Evaluator -> MCTS Tree Analyzer -> Fact Checker -> Natural Language Generator -> Explanation Output

**Critical Path:** The logic expression evaluation stage is the critical path, as it determines whether generated explanations are factually consistent with the search tree data and environmental constraints before they are presented to the user.

**Design Tradeoffs:** The framework trades computational complexity for accuracy by performing rigorous logic-based validation, which may increase latency but ensures higher quality explanations. The choice of LLM backbone significantly impacts performance, creating a tradeoff between using more powerful models versus computational efficiency.

**Failure Signatures:** The system may fail when natural language queries contain ambiguities that cannot be resolved into precise logic expressions, or when the underlying LLM generates incorrect logic expressions despite having access to accurate environmental descriptions.

**First Experiments:** 1) Test the framework with simple, unambiguous queries to establish baseline performance, 2) Introduce increasingly complex queries with temporal or causal reasoning to identify limitations, 3) Compare performance across different LLM backbones to quantify the impact of model choice.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions for future research.

## Limitations
- Performance highly dependent on the underlying LLM backbone with significant variations between models
- Evaluation limited to a single paratransit planning scenario, potentially limiting generalizability
- Complexity of transforming natural language into logic expressions may become prohibitive for queries involving temporal or causal reasoning

## Confidence
- High confidence in factual consistency improvements (2.40× with FactCC) as this is a direct metric comparison
- Medium confidence in relevance improvements (7.92× with BERTScore) due to subjective nature of relevance metrics
- Low confidence in generalizability beyond paratransit domain without testing across diverse applications

## Next Checks
1. Test LogiExExplainer across multiple domains (game playing, robotics path planning, resource allocation) to assess generalizability and identify domain-specific limitations.

2. Conduct ablation studies to quantify individual contributions of logic-based reasoning versus LLM backbone components.

3. Implement user studies with domain experts to evaluate practical utility and interpretability of generated explanations.