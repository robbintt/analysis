---
ver: rpa2
title: Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses
arxiv_id: '2602.01285'
source_url: https://arxiv.org/abs/2602.01285
tags:
- coverage
- maci
- retention
- conformal
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MACI addresses the challenge of ensuring factuality in LLM-generated
  responses for high-stakes domains by reformulating conformal inference as a multiplicative
  filtering process. It models factuality as the product of claim-level scores and
  uses an ensemble of multiple LLMs to produce more accurate factuality scores.
---

# Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses

## Quick Facts
- **arXiv ID:** 2602.01285
- **Source URL:** https://arxiv.org/abs/2602.01285
- **Reference count:** 40
- **Primary result:** MACI achieves user-specified coverage (80%-95%) with higher retention of true claims than baselines across medical, biography, and expert QA datasets.

## Executive Summary
MACI addresses the challenge of ensuring factuality in LLM-generated responses for high-stakes domains by reformulating conformal inference as a multiplicative filtering process. It models factuality as the product of claim-level scores and uses an ensemble of multiple LLMs to produce more accurate factuality scores. By applying group-conditional calibration, MACI achieves user-specified coverage while significantly improving retention of true claims compared to baselines. Experiments show MACI consistently meets coverage targets with higher retention ratios and lower computational costs across datasets like MedLFQA, WikiBio, and ExpertQA.

## Method Summary
MACI implements a conformal inference framework where LLM responses are decomposed into atomic claims, each assigned a factuality score by multiple verifier LLMs. An ensemble method optimizes weights to minimize false positive rates while maintaining true positive rates. Group-conditional calibration computes separate thresholds for different semantic groups to ensure coverage guarantees across subpopulations. The final filtering applies these calibrated thresholds to retain claims while guaranteeing the specified coverage level.

## Key Results
- MACI achieves target coverage (80%-95%) across 9 groups with higher retention ratios than baseline methods
- The ensemble approach reduces mean squared error in factuality estimation, improving retention efficiency
- Group-conditional calibration ensures coverage guarantees for all subpopulations without sacrificing overall performance
- MACI demonstrates lower computational costs than existing methods while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1: Multiplicative Filtering Framework
- **Claim:** Modeling factuality as a cumulative product of claim-level scores yields higher retention while preserving coverage guarantees.
- **Mechanism:** The oracle filtering rule shows that optimal filter retains claims in descending order of factuality scores until cumulative product falls below threshold τ. This aggregates uncertainty across retained claims, making conformity score less sensitive to individual estimation errors.
- **Core assumption:** Claims are conditionally independent given the prompt (used for theoretical analysis but relaxed in practice).
- **Evidence anchors:** [abstract] "modeling factuality as a product of claim-level scores...led to higher retention"; [Section 4.1] Definition 1 formalizes oracle filtering rule with cumulative products.

### Mechanism 2: Multi-LLM Ensemble for Improved Factuality Estimation
- **Claim:** Aggregating factuality scores from multiple LLMs via FPR-minimizing optimization reduces estimation error and improves retention ratio.
- **Mechanism:** Theorem 3 bounds retention gap by polynomial function of MSE: ∆ ≤ C'·E[(p̂ - p*)²]^(β/(β+2)). Since ensemble reduces variance in bias-variance tradeoff, it lowers MSE.
- **Core assumption:** The margin condition (|p*(P,c) - τ| ≤ ε bounded by Cε^β) characterizes score separation near thresholds.
- **Evidence anchors:** [abstract] "leverages ensembles to produce more accurate factuality-scores"; [Section 4.3] Theorem 3 formally links MSE to retention.

### Mechanism 3: Group-Conditional Calibration
- **Claim:** Calibrating thresholds separately per group ensures coverage guarantees across semantically distinct subpopulations without sacrificing retention.
- **Mechanism:** Theorem 2 extends Theorem 1 using Mondrian conformal inference: for each group k, compute (1-α) quantile from calibration samples in that group. This guarantees P(F ⊆ A | group=k) ≥ 1-α for all k.
- **Core assumption:** Exchangeability holds within each group; group definitions are known and finite (K groups).
- **Evidence anchors:** [abstract] "validity is preserved through group-conditional calibration"; [Section 4.2] Theorem 2 provides finite-sample guarantee.

## Foundational Learning

- **Concept: Conformal Prediction Fundamentals**
  - **Why needed here:** MACI builds on conformal inference's core result: exchangeable data + quantile calibration → distribution-free coverage guarantees.
  - **Quick check question:** Given calibration scores [0.1, 0.3, 0.5, 0.7, 0.9], what threshold achieves 80% coverage for new test point? (Answer: 0.7, the 4th/5th quantile)

- **Concept: Claim Decomposition and Factuality Scoring**
  - **Why needed here:** MACI operates on atomic claims with estimated factuality scores p̂ ∈ [0,1]. Understanding how LLM responses decompose and how verifiers assign scores is critical for implementation.
  - **Quick check question:** Why might "GPT-4 generates bio" response decompose differently than medical Q&A? (Answer: biographical claims are entity-attribute pairs; medical claims involve diagnostic/treatment assertions)

- **Concept: Marginal vs. Conditional Coverage**
  - **Why needed here:** BCI provides only marginal coverage (guarantee averaged over all groups), which can hide subgroup failures. MACI's group-conditional guarantee ensures each subpopulation meets target.
  - **Quick check question:** If filter achieves 90% marginal coverage but only 70% for high-risk medical queries, is it suitable for deployment? (Answer: No; high-risk subgroups need explicit guarantees)

## Architecture Onboarding

- **Component map:** Input prompts and responses → Claim decomposition → Multi-LLM factuality scoring → Ensemble weight optimization → Group-conditional calibration → Conformity score calculation → Filtering module → Output filtered claims
- **Critical path:**
  1. Implement claim decomposition (atomic claim extraction from responses)
  2. Build factuality scoring pipeline (multi-LLM queries with fact-checking prompts)
  3. Implement ensemble weight optimization (FPR minimization with TPR constraint)
  4. Compute conformity scores (cumulative product in log-space for numerical stability)
  5. Calibrate group-specific thresholds via quantiles
  6. Deploy filter with randomization at boundary claims
- **Design tradeoffs:**
  - **Retention vs. Coverage:** Higher coverage targets reduce retention; MACI's efficiency gains come from better factuality estimation
  - **Group Granularity vs. Sample Size:** More groups → finer fairness but fewer calibration samples per group → higher variance
  - **Ensemble Size vs. Latency:** M=3 models used; retention improves with k=1→2→3, but inference cost scales linearly
  - **Log-Space vs. Direct Product:** log-space avoids underflow; mathematically equivalent via monotonic transformation
- **Failure signatures:**
  - **Undercoverage in specific groups:** Check if calibration set has sufficient samples per group (≥50 for stable quantiles)
  - **Low retention despite high coverage:** Verify ensemble weights are optimizing FPR; ensure factuality scores are calibrated
  - **Covariate shift:** If test queries differ from calibration, coverage may drop; implement MACI-DRE with density-ratio estimation
  - **Numerical instability:** Claims with p̂ near 1.0 can cause underflow; verify log-space implementation
- **First 3 experiments:**
  1. **Baseline reproduction:** Implement BCI and CCI on MedLFQA with single LLM; measure coverage and retention across groups
  2. **Ablation on ensemble size:** Run MACI with M=1, 2, 3 LLMs; plot retention ratio vs. number of models on WikiBio
  3. **Group sensitivity test:** Define 2 groups vs. 6 groups vs. 24 groups on ExpertQA; measure coverage variance across groups

## Open Questions the Paper Calls Out

- **Open Question 1:** Can explicit modeling of dependencies between atomic claims improve retention ratio without violating coverage guarantees? (Basis: Appendix E discusses independence assumption as "convenient theoretical simplification" with similar performance from joint modeling)
- **Open Question 2:** How can strict group-conditional coverage be maintained when number of subgroups is large relative to calibration sample size? (Basis: Appendix E states clustering "trades strict finite-sample group-conditional coverage for lower variance")
- **Open Question 3:** To what extent does correlation of systematic errors among ensemble LLM verifiers degrade retention efficiency? (Basis: Section 4.3 links retention gaps to MSE, but doesn't analyze correlated hallucination patterns)

## Limitations

- Multiplicative filtering framework relies on conditional independence of claims, which may not hold in practice with strongly correlated assertions
- Ensemble approach depends on base LLM diversity - if all models share systematic biases, theoretical MSE reduction may not materialize
- Group-conditional calibration faces sample size constraints: with K=24 groups, quantile estimation becomes unstable unless calibration samples are pooled

## Confidence

- **High Confidence:** Conformal inference foundation, baseline implementations, and group-conditional coverage results. Core mechanism connecting MSE reduction to retention gains is well-supported.
- **Medium Confidence:** Multiplicative aggregation model's practical performance under dependent claims, and ensemble weight optimization's robustness to different LLM combinations.
- **Low Confidence:** Exact impact of claim decomposition quality on downstream performance, and generalizability beyond the three datasets used.

## Next Checks

1. **Dependence Sensitivity Test:** Design synthetic experiments where claims are artificially correlated and measure MACI's retention/coverage degradation compared to oracle bounds.

2. **Ensemble Robustness Evaluation:** Replace one of the three base LLMs with a model known to have systematic bias in a specific domain and measure whether ensemble still achieves theoretical MSE reduction.

3. **Sample Size Threshold Analysis:** Systematically vary the number of calibration samples per group (n_k = 10, 30, 100) and measure coverage variance and retention stability, particularly for K=24 grouping configuration.