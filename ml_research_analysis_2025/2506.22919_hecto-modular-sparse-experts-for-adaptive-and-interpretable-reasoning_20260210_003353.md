---
ver: rpa2
title: 'Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning'
arxiv_id: '2506.22919'
source_url: https://arxiv.org/abs/2506.22919
tags:
- expert
- routing
- ffnn
- hecto
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hecto introduces a novel MoE architecture that combines a GRU expert
  for temporal reasoning and an FFNN expert for static abstraction under a sparse
  Top-1 gating mechanism. The model is evaluated on three reasoning benchmarks (AG
  News, SST-2, HotpotQA) and a regression task (STS-B), achieving competitive performance
  while demonstrating clear expert specialization.
---

# Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning

## Quick Facts
- arXiv ID: 2506.22919
- Source URL: https://arxiv.org/abs/2506.22919
- Reference count: 40
- Sparse MoE with heterogeneous experts achieves competitive accuracy while demonstrating interpretable expert specialization

## Executive Summary
Hecto introduces a novel MoE architecture that combines a GRU expert for temporal reasoning and an FFNN expert for static abstraction under a sparse Top-1 gating mechanism. The model is evaluated on three reasoning benchmarks (AG News, SST-2, HotpotQA) and a regression task (STS-B), achieving competitive performance while demonstrating clear expert specialization. Across tasks, the GRU expert is predominantly selected, indicating effective alignment with sequential reasoning demands. When trained with larger batch sizes, Hecto outperforms homogeneous baselines, showcasing its scalability. The model maintains strong performance even with a frozen encoder, achieving 87.76% accuracy on AG News. Hecto establishes a new benchmark for conditional computation, offering a lightweight, interpretable framework for specialized reasoning in low-resource regimes.

## Method Summary
Hecto employs a two-expert MoE architecture where a DistilBERT encoder feeds into a gating network that selects between a GRU expert (for temporal reasoning) and an FFNN expert (for static abstraction). The gating network operates over dual projections of the encoder output - the [CLS] token for the FFNN and the full sequence for the GRU. A Top-1 sparse routing mechanism with straight-through estimation enables efficient inference while maintaining differentiability during training. Entropy and diversity regularization prevent routing collapse and encourage balanced expert usage. The architecture is trained with AdamW (lr=2e-5, batch=16) for 5 epochs, with performance evaluated across classification and regression tasks.

## Key Results
- Hecto achieves 90.82% accuracy on AG News at batch size 64, surpassing all baselines trained at batch size 16
- The GRU expert is predominantly selected (80-92% usage) across tasks, with the FFNN specializing in local pattern recognition
- When using a frozen encoder, Hecto maintains 87.76% accuracy on AG News with 92.5% GRU usage
- Larger batch sizes (64 vs 16) improve Hecto's performance by ~0.8 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Architectural heterogeneity between experts (GRU for temporal reasoning, FFNN for static abstraction) produces more interpretable specialization than homogeneous expert pools.
- Mechanism: Distinct inductive biases force the gating network to route based on reasoning demands—sequential inputs favor the GRU, static pattern-matching inputs favor the FFNN. Isolated input representations (full sequence for GRU, [CLS] token for FFNN) reinforce this separation.
- Core assumption: Temporal vs. static reasoning cleanly maps to sequential vs. fixed representations in the evaluated tasks.
- Evidence anchors:
  - [abstract]: "Ablation results isolate architectural diversity as the source of Hecto's stability and interpretability across diverse reasoning tasks."
  - [Section 3]: "The gating network operates over isolated features from the shared encoder and must choose between functionally incompatible experts, thereby enforcing a form of implicit reasoning-based specialization."
  - [corpus]: Related work (arXiv:2512.19765, arXiv:2510.01185) explores specialization via hyperparameter tuning and Dirichlet priors, but doesn't test architectural heterogeneity directly—mechanism remains specific to Hecto's design.
- Break condition: If both experts receive identical input representations (e.g., both get [CLS] only), architectural diversity advantage likely diminishes.

### Mechanism 2
- Claim: Entropy and diversity regularization prevent routing collapse while maintaining confident, interpretable expert selection.
- Mechanism: Entropy penalty (λ_ent = 0.05) pushes gate probabilities toward 0 or 1 for confident selection. Diversity penalty (λ_div = 0.08) prevents one expert from dominating all inputs. Combined, they encourage stable, semantically aligned routing.
- Core assumption: Balanced expert usage correlates with meaningful specialization—may not hold if one reasoning type genuinely dominates a task.
- Evidence anchors:
  - [Section 3.2]: "Together, these regularizers guide the gating mechanism to be both confident per input and fair across experts."
  - [Appendix B.2]: Without regularization, "entropy collapses from 0.40 bits to 0.02 bits" and "the FFNN path receives 99.7% of inputs; the GRU is effectively bypassed."
  - [corpus]: Weak—corpus papers don't specifically analyze this dual-regularization approach for interpretable routing.
- Break condition: If regularization weights are too aggressive, they force artificial balance that contradicts true reasoning demands.

### Mechanism 3
- Claim: Heterogeneous MoE architectures benefit more from larger batch sizes than homogeneous baselines due to improved gradient flow to sparse experts.
- Mechanism: Larger batches increase per-expert sample counts per update, reducing gradient variance and allowing structurally diverse experts to specialize more effectively.
- Core assumption: Assumption—the batch-size benefit is specific to heterogeneous architectures, not a general MoE optimization effect (not directly tested against homogeneous baselines at batch=64).
- Evidence anchors:
  - [abstract]: "At larger batch sizes, Hecto exhibits improved performance, benefiting from relaxed computational constraints that allow its heterogeneous architecture to optimize more effectively."
  - [Appendix C]: At batch=64, Hecto achieves 90.82% accuracy vs. 90.02% at batch=16, "surpassing all baseline models trained with batch size 16."
  - [corpus]: Weak—corpus papers don't address batch-size effects on heterogeneous MoE specialization.
- Break condition: If homogeneous baselines were also tested at batch=64 and showed similar gains, the mechanism would generalize to all sparse MoEs, not just heterogeneous designs.

## Foundational Learning

- Concept: **Mixture-of-Experts with Sparse Routing**
  - Why needed here: Hecto's core contribution is conditional computation via Top-1 routing—understanding how inputs are dispatched to subsets of experts is foundational.
  - Quick check question: Can you explain why sparse (Top-1) routing improves interpretability compared to dense (all-expert) routing?

- Concept: **Inductive Biases in Neural Architectures**
  - Why needed here: The paper's core hypothesis is that GRUs naturally suit temporal reasoning and FFNNs suit static abstraction—understanding why these architectures have different strengths is essential.
  - Quick check question: Why would a GRU process sequential token information differently than an FFNN operating on a [CLS] embedding?

- Concept: **Straight-Through Estimation for Discrete Decisions**
  - Why needed here: Hecto uses straight-through sampling to train Top-1 discrete routing while maintaining differentiability.
  - Quick check question: How does straight-through estimation allow gradients to flow through a discrete (non-differentiable) routing decision?

## Architecture Onboarding

- Component map:
  - DistilBERT encoder -> Dual projection layer -> Gating network -> Top-1 selection -> Selected expert -> Output logits

- Critical path: Input → DistilBERT → Dual Projection → Gating Network computes probabilities → Top-1 selects expert → Only selected expert executes → Output logits

- Design tradeoffs:
  - **Top-1 vs. Top-2 routing**: Top-1 preserves single-expert interpretability; Top-2 adds +0.58pp accuracy but "degenerates into always run the GRU" and loses clear attribution (Appendix B.1)
  - **Frozen vs. fine-tuned encoder**: Frozen reduces compute, maintains 87.76% accuracy with sharper GRU bias (92.5% usage)—acceptable for low-resource deployment (Appendix A)
  - **Expert count**: 4-expert variant collapses to single-expert behavior with lower accuracy (87.53%)—2-expert design is optimal (Appendix B.3)

- Failure signatures:
  - **Routing collapse**: One expert receives >99% of inputs (check gate entropy <0.05)
  - **Forced balance without semantics**: High diversity but low task-aligned specialization
  - **Accuracy drop with expert scaling**: Adding experts without diversity signals wastes capacity (Appendix B.3)

- First 3 experiments:
  1. Replicate baseline comparison (FFNN+FFNN, GRU+GRU, FFNN+GRU) on AG News with batch=16, measure accuracy and expert usage distribution.
  2. Ablate regularization: train with λ_ent=0 and λ_div=0, observe routing collapse (expect entropy drop to ~0.02 and single-expert dominance per Appendix B.2).
  3. Test batch-size sensitivity: compare batch=16 vs. batch=64 on AG News for FFNN+GRU, verify if heterogeneous architecture benefits more than homogeneous baselines (requires re-running all variants at batch=64).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Hecto framework effectively generalize to generative tasks (such as text generation or decoding) and multilingual settings?
- Basis in paper: [explicit] The authors state in the Limitations section that the "current scope remains limited to supervised, English-language datasets" and explicitly call for "future work" to expand toward "generative tasks, multilingual settings, and domain-shifted scenarios."
- Why unresolved: The current evaluation is restricted to classification (AG News, SST-2), QA (HotpotQA), and regression (STS-B), all in English. It is unclear if the GRU/FFNN heterogeneity benefits or hinders sequence generation or cross-lingual transfer.
- What evidence would resolve it: Evaluation of a modified Hecto architecture on generative benchmarks (e.g., translation, summarization) and multilingual datasets to observe if expert specialization patterns hold across languages and generation tasks.

### Open Question 2
- Question: How can the model move beyond aggregate usage statistics to provide granular, per-input explanations for specific routing decisions?
- Basis in paper: [explicit] The Limitations section notes that "quantifying why a specific expert was chosen for a given input remains challenging" and suggests that "finer-grained interpretability tools (e.g., token saliency, gate introspection)" are needed to ground routing decisions in measurable evidence.
- Why unresolved: While the paper shows that the GRU dominates compositional tasks, it currently relies on high-level usage distributions rather than explaining the specific input features that trigger a gate for a single sample.
- What evidence would resolve it: Development and application of interpretability methods (e.g., saliency maps on the gating network) that correlate specific input tokens or structural features with the probability of selecting the FFNN vs. GRU expert for individual examples.

### Open Question 3
- Question: Does architectural heterogeneity provide similar efficiency and specialization benefits when deployed on edge hardware (CPUs, mobile) as it does on cloud GPUs?
- Basis in paper: [explicit] The authors identify "Incomplete Edge Profiling" as a limitation, noting that "Latency results are reported only on a single T4 GPU" and stating that "profiling on CPUs and mobile hardware is necessary to assess real-world deployability."
- Why unresolved: Hecto is designed for efficiency, but the inference time benefits of sparse, heterogeneous execution have only been validated on a standard server GPU, not on the low-power hardware targeted by the "lightweight" design.
- What evidence would resolve it: Benchmarks of inference latency and energy consumption running Hecto on consumer-grade CPUs or mobile system-on-chips compared to homogeneous baselines.

### Open Question 4
- Question: Does the performance and stability of Hecto improve consistently when scaling to larger backbone encoders or fully unconstrained computational regimes?
- Basis in paper: [explicit] The Limitations section highlights that all experiments were conducted under "tight resource budgets" and suggests that "future work should re-evaluate its performance under less constrained settings." Additionally, Appendix C shows batch size 64 improves performance, implying the model is currently under-explored.
- Why unresolved: It is uncertain if the performance gains observed when increasing batch size from 16 to 64 are the ceiling, or if the architecture would benefit further from larger encoders (e.g., BERT-Large) or larger batch sizes without losing interpretability.
- What evidence would resolve it: Experiments training Hecto with larger transformer backbones and higher batch sizes to determine if the heterogeneous expert advantage scales linearly or saturates compared to homogeneous baselines.

## Limitations

- Limited architectural exploration: Only tests 2-expert design with GRU+FFNN; 4-expert variants collapse to single-expert behavior
- Training efficiency trade-offs: Modest latency advantage (8.0 ms vs. 8.8 ms) without addressing computational overhead of multiple expert pathways during training
- Task diversity constraints: All evaluated tasks are text-based classification/QA, limiting generalizability claims to "diverse reasoning tasks"

## Confidence

**High confidence**: The GRU expert's dominance across tasks (80-92% usage) is well-supported by empirical results and consistent with the sequential nature of the evaluated tasks. The claim that heterogeneous experts enable interpretable routing is substantiated by clear expert specialization patterns and ablation studies.

**Medium confidence**: The assertion that larger batch sizes specifically benefit heterogeneous architectures more than homogeneous ones. While batch=64 shows improvement, the comparison lacks homogeneous baselines trained at the same batch size, making it difficult to isolate the architectural effect from general optimization benefits.

**Low confidence**: The generalizability claim that Hecto's interpretability benefits extend to "diverse reasoning tasks." The evaluation is limited to text-based classification and QA, which may not adequately represent the full spectrum of reasoning types where architectural heterogeneity could provide advantages.

## Next Checks

1. **Batch-size ablation with homogeneous baselines**: Re-run all baseline models (FFNN+FFNN, GRU+GRU) at batch size 64 and compare performance gains to the heterogeneous Hecto model. This will determine whether the batch-size benefit is specific to architectural heterogeneity or a general MoE optimization effect.

2. **Expert architecture sensitivity analysis**: Replace the GRU expert with alternative sequence models (LSTM, Transformer) and the FFNN with different static architectures (MLP, linear projection) to test whether the interpretability benefits depend on the specific GRU+FFNN pairing or generalize to any heterogeneous combination.

3. **Cross-domain generalization test**: Evaluate Hecto on a non-textual reasoning task (e.g., mathematical problem solving or visual reasoning) to test whether the claimed interpretability benefits hold when sequential vs. static reasoning mappings don't naturally align with the architecture choices.