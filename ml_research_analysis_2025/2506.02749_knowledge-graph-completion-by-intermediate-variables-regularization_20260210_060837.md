---
ver: rpa2
title: Knowledge Graph Completion by Intermediate Variables Regularization
arxiv_id: '2506.02749'
source_url: https://arxiv.org/abs/2506.02749
tags:
- regularization
- tensor
- rank
- have
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses knowledge graph completion using tensor decomposition-based
  models, which suffer from overfitting despite their expressiveness. The authors
  propose Intermediate Variables Regularization (IVR), a novel regularization technique
  that minimizes the norms of intermediate variables involved in computing the predicted
  tensor, rather than just embedding norms.
---

# Knowledge Graph Completion by Intermediate Variables Regularization

## Quick Facts
- arXiv ID: 2506.02749
- Source URL: https://arxiv.org/abs/2506.02749
- Authors: Changyi Xiao; Yixin Cao
- Reference count: 40
- Primary result: IVR regularization improves TuckER MRR from 0.446 to 0.501 on WN18RR

## Executive Summary
This paper addresses knowledge graph completion using tensor decomposition-based models, which suffer from overfitting despite their expressiveness. The authors propose Intermediate Variables Regularization (IVR), a novel regularization technique that minimizes the norms of intermediate variables involved in computing the predicted tensor, rather than just embedding norms. IVR is theoretically grounded, with proofs showing it serves as an upper bound for the overlapped trace norm, promoting low-rank structure and reducing overfitting. Experiments on WN18RR, FB15k-237, and YAGO3-10 datasets demonstrate IVR's effectiveness across multiple models (CP, ComplEx, SimplE, ANALOGY, QuatE, TuckER), achieving state-of-the-art results. For example, TuckER with IVR improves MRR from 0.446 to 0.501 on WN18RR. Ablation studies confirm the upper bound's reliability, and theoretical analysis supports its efficacy in regularizing embeddings. The method is computationally tractable and broadly applicable to most tensor decomposition-based models.

## Method Summary
IVR introduces a regularization term that minimizes the norms of intermediate variables in tensor decomposition models. The method expresses various TDB models (CP, TuckER, etc.) through a general form as linear combinations of dot products, then regularizes the intermediate terms that combine embeddings with the core tensor. This upper bounds the overlapped trace norm of the predicted tensor, encouraging low-rank structure. The approach is applied to six TDB models across three standard KG datasets using multiclass log-loss and Adagrad optimization.

## Key Results
- TuckER with IVR achieves MRR of 0.501 on WN18RR, improving from 0.446 baseline
- IVR improves all tested models (CP, ComplEx, SimplE, ANALOGY, QuatE, TuckER) across WN18RR, FB15k-237, and YAGO3-10 datasets
- Ablation studies confirm effectiveness of the upper bound formulation and show optimal performance at α=3.0 for WN18RR
- The method is computationally tractable and introduces minimal overhead

## Why This Works (Mechanism)

### Mechanism 1: Upper Bound Optimization of Trace Norm
- **Claim:** Minimizing the norms of intermediate variables effectively minimizes the overlapped trace norm of the predicted tensor, thereby reducing overfitting by encouraging low-rank structures.
- **Mechanism:** The authors prove that their Intermediate Variables Regularization (IVR) term serves as a tight upper bound for the overlapped trace norm (sum of trace norms of unfolding matrices). By minimizing this upper bound, the model is constrained to learn predicted tensors with lower trace norms, which acts as a convex surrogate for low rank.
- **Core assumption:** The overlapped trace norm is a valid proxy for the underlying structure of knowledge graphs, specifically capturing correlations among entities and relations.
- **Evidence anchors:**
  - [abstract] "...proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting."
  - [section 3.3] "Proposition 1 establishes that the r.h.s. of Eq.(5) provides a tight upper bound for $2\sqrt{\lambda_1\lambda_4}L(X; \alpha)$..."
  - [corpus] Evidence is weak; neighboring papers focus on LLM-empowerment or context filtering rather than tensor norm bounds.
- **Break condition:** If the knowledge graph does not exhibit low-rank structure or if the unfolded matrices do not capture relevant correlation patterns, minimizing this norm may over-constrain the model.

### Mechanism 2: Regularizing Interactions over Embeddings
- **Claim:** Penalizing the intermediate variables (combinations of embeddings) is more effective than penalizing the raw embedding norms alone.
- **Mechanism:** Standard regularization (e.g., N3, Frobenius) minimizes $\|H\|, \|R\|, \|T\|$. IVR minimizes terms like $\|W \times_1 H_{:d:} \times_2 R_{:d:}\|$. This specifically targets the "interaction space" where the score is computed, preventing the model from relying on large embedding values that cancel out or overfit specific triplet interactions without reducing the overall tensor complexity.
- **Core assumption:** Overfitting in TDB models is driven primarily by uncontrolled complexity in the interaction layers (tensor products) rather than just the magnitude of individual entity/relation vectors.
- **Evidence anchors:**
  - [abstract] "Existing regularization methods merely minimize the norms of embeddings... leading to suboptimal performance."
  - [section 3.2] "Therefore, we should minimize the (power of Frobenius) norms... and $\|W \times_2 R_{:d:} \times_3 T_{:d:}\|^\alpha_F$..."
  - [corpus] "Rethinking Regularization Methods..." suggests a broader need to revisit standard regularization, but does not explicitly validate intermediate variables.
- **Break condition:** If the computational cost of calculating intermediate norms outweighs the generalization benefit (though authors claim tractability), or if the specific TDB model structure makes these intermediates constant.

### Mechanism 3: General Form Unification
- **Claim:** A unified regularization framework is possible because diverse TDB models share a general block-term decomposition structure.
- **Mechanism:** By expressing CP, ComplEx, TuckER, etc., via a general form (Eq. 2: $X = \sum W \times H \times R \times T$), IVR can be applied uniformly. The regularization scales with the number of parts $P$ and the core tensor $W$, allowing it to adapt from simple CP ($P=1$) to complex TuckER ($P=D$).
- **Core assumption:** The general form accurately captures the computational graph of the target TDB models, and the intermediate variables derived from this form are the correct points of intervention.
- **Evidence anchors:**
  - [section 3.1] "...we observe that all TDB models can be expressed as a linear combination of several dot product... we can derive a general form..."
  - [section 4.2] "For SimplE, ANALOGY, QuatE, and TuckER... IVR significantly boosts model performance."
  - [corpus] Neighbors like "KG-TRICK" focus on unifying textual/relational info, not tensor decomposition forms; no direct external validation of this specific general form found in corpus.
- **Break condition:** When applying to a TDB model that fundamentally cannot be expressed as a sum of products with a shared core tensor $W$ (though the paper claims broad coverage).

## Foundational Learning

- **Concept: Tensor Unfolding (Matricization)**
  - **Why needed here:** The theoretical justification for IVR relies on the "overlapped trace norm," which is the sum of trace norms of the tensor's mode-$n$ unfoldings. You must understand how a 3D tensor flattens into matrices to grasp Proposition 1.
  - **Quick check question:** If tensor $X$ represents (Head, Relation, Tail), what does a row in the mode-1 unfolding $X_{(1)}$ represent?

- **Concept: Tucker Decomposition vs. CP Decomposition**
  - **Why needed here:** The paper unifies these under a "General Form." Understanding that CP is a special case of Tucker (where the core tensor is super-diagonal) is necessary to see why IVR works on both.
  - **Quick check question:** How does the core tensor $W$ differ between the CP model and the TuckER model in the general form?

- **Concept: Trace Norm as a Rank Surrogate**
  - **Why needed here:** The paper argues IVR works because it minimizes the trace norm, which encourages "low rank." You need to know that minimizing the trace norm (sum of singular values) is a standard convex relaxation for minimizing matrix rank.
  - **Quick check question:** Why is the trace norm used instead of directly counting non-zero singular values (rank) in optimization?

## Architecture Onboarding

- **Component map:**
  - Embeddings $H, R, T$ partitioned into $P$ parts
  - Core tensor $W$ (constant or learnable)
  - Intermediate terms: $Z_1 = W \times H$, $Z_2 = W \times R$, etc.
  - Score computation through outer products/interactions
  - IVR regularizer module that computes norms of intermediates
  - Multiclass log-loss objective

- **Critical path:**
  1. Identify model type (CP, TuckER, etc.) to set $P$ and $W$
  2. Implement the General Form Eq. (2) for the forward pass
  3. Implement IVR Eq. (4) by hooking into the intermediate tensors generated in step 2
  4. Tune hyper-parameters $\alpha$ (power) and $\lambda_i$ (coeffs)

- **Design tradeoffs:**
  - **Complexity vs. Generality:** IVR adds computational overhead ($O(DP^2)$), but allows a single regularizer implementation for many models
  - **Hyper-parameters:** IVR introduces up to 4 coefficients ($\lambda_{1..4}$) and power $\alpha$. The paper suggests setting $\lambda_1=\lambda_3, \lambda_2=\lambda_4$ to reduce search space (Table 6)
  - **Tightness:** Proposition 2 shows the bound is not always tight for all models (e.g., not tight for CP), suggesting potential inefficiency in regularization signal for specific architectures

- **Failure signatures:**
  - **High $\lambda$:** Aggressive regularization drops MRR significantly (underfitting), visible in the "descending after peak" pattern in ablation studies
  - **Wrong $P$:** If the partitioning $P$ does not match the model architecture (e.g., using $P=1$ for QuatE which needs $P=4$), the math for intermediate variables breaks
  - **Instability:** If initialization is poor and norms explode, the IVR term (with high $\alpha$) might cause gradient instability

- **First 3 experiments:**
  1. **Sanity Check (CP/DistMult):** Apply IVR to a simple CP model on WN18RR. Verify that it reproduces the "N3-like" or better performance gains to ensure the general form is implemented correctly for $P=1$
  2. **Ablation on $\alpha$:** Run ComplEx on WN18RR varying $\alpha \in \{2.0, 3.0, 3.5\}$ (as per Table 7) to confirm the sensitivity of the regularization power before tuning $\lambda$
  3. **Trace Norm Verification:** Train TuckER with and without IVR on a small dataset (e.g., Kinship, as used in Table 3). Compute the actual overlapped trace norm $L(X)$ of the resulting tensor to confirm IVR effectively reduces it compared to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Intermediate Variables Regularization (IVR) be effectively adapted for translation-based or neural network-based knowledge graph completion models?
- **Basis in paper:** [explicit] The Conclusion states, "Our regularization is limited to TDB models, hoping that more regularization will be proposed in other types of models, such as translation-based models and neural networks models."
- **Why unresolved:** The theoretical foundation of IVR relies on the specific block-term decomposition general form (Eq. 2) and the trace norm of unfolding matrices, operations that do not have direct, standard equivalents in translation-based (e.g., TransE) or deep neural architectures.
- **What evidence would resolve it:** A derivation of an IVR-compatible regularizer for non-TDB models that improves MRR and Hits@N on standard benchmarks without increasing training instability.

### Open Question 2
- **Question:** Can the IVR framework be generalized to improve performance in broader tensor completion tasks outside of knowledge graphs?
- **Basis in paper:** [explicit] The Conclusion notes, "We also intend to explore how to apply our regularization to other fields, such as tensor completion."
- **Why unresolved:** The current method is validated specifically on 3-order binary tensors representing KGs; it is unclear if the overlapped trace norm upper bound remains a useful inductive bias for continuous-valued tensors or tensors of significantly higher order.
- **What evidence would resolve it:** Application of IVR to general tensor completion benchmarks (e.g., visual data or collaborative filtering) showing superior reconstruction accuracy compared to standard nuclear norm regularization.

### Open Question 3
- **Question:** Does a tighter upper bound for the overlapped trace norm exist for specific TDB models with low parts count ($P$), such as CP or ComplEx?
- **Basis in paper:** [inferred] Section 3.3 discusses that the bound in Proposition 1 is tight only for TuckER ($P=D$), while Proposition 2 is not always tight, implying the current general IVR formulation might be mathematically "loose" for simpler models.
- **Why unresolved:** The authors combine two bounds to create a general regularizer, but they acknowledge that for specific models like CP, the nuclear 2-norm is a tighter bound than the overlapped trace norm; finding a similar tight bound for ComplEx/SimplE remains unexplored.
- **What evidence would resolve it:** A theoretical derivation of a specific IVR variant for ComplEx that proves to be a tighter bound than the general formulation, resulting in empirically lower trace norms and higher MRR.

## Limitations
- The method is limited to tensor decomposition-based models and cannot be directly applied to translation-based or neural network architectures
- The theoretical upper bound may not be tight for all TDB models, particularly those with low parts count like CP and ComplEx
- Optimal hyperparameters are dataset-specific and require grid search, suggesting limited generalization across different knowledge graphs

## Confidence
- **Trace norm efficacy:** Medium - While Proposition 1 provides theoretical grounding, the empirical correlation between trace norm minimization and KG completion performance is not directly validated
- **General form completeness:** Medium - The unified representation claims broad coverage, but complex models like QuatE may have architectural nuances not fully captured
- **Hyperparameter sensitivity:** Medium - The ablation studies show IVR works across λ and α ranges, but the optimal settings appear dataset-specific

## Next Checks
1. **Trace norm correlation study:** Compute the actual overlapped trace norm L(X) of learned tensors with and without IVR across multiple epochs and datasets. Plot L(X) against validation MRR to empirically validate the regularization's effect on tensor complexity.
2. **Model expressiveness test:** Apply IVR to a TDB model with known low-rank structure (e.g., Kinship) and verify whether IVR's trace norm minimization aligns with preserving meaningful latent relationships versus merely reducing numerical rank.
3. **Hyperparameter generalization:** Take optimal IVR hyperparameters from WN18RR and apply them unchanged to FB15k-237 and YAGO3-10. Measure performance degradation to quantify the method's sensitivity to dataset-specific tuning.