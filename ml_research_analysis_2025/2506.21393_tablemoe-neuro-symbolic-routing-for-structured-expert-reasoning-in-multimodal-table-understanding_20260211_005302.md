---
ver: rpa2
title: 'TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal
  Table Understanding'
arxiv_id: '2506.21393'
source_url: https://arxiv.org/abs/2506.21393
tags:
- table
- reasoning
- image
- tablemoe
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TableMoE introduces a neuro-symbolic Mixture-of-Experts (MoE) framework
  with token-role-aware routing for robust multimodal table understanding under WildStruct
  conditions. It integrates symbolic semantic role prediction (header, data, formula,
  etc.) with confidence-aware expert fusion across three modality-specific experts
  (HTML, JSON, Code).
---

# TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding

## Quick Facts
- **arXiv ID**: 2506.21393
- **Source URL**: https://arxiv.org/abs/2506.21393
- **Authors**: Junwen Zhang; Pu Chen; Yin Zhang
- **Reference count**: 40
- **Key outcome**: TableMoE introduces a neuro-symbolic Mixture-of-Experts (MoE) framework with token-role-aware routing for robust multimodal table understanding under WildStruct conditions. It integrates symbolic semantic role prediction (header, data, formula, etc.) with confidence-aware expert fusion across three modality-specific experts (HTML, JSON, Code). Trained on a 1.2M-scale TableMoE-Align corpus and fine-tuned via curriculum-guided neuro-symbolic annealing, TableMoE achieves up to +9.2% accuracy gains on FinanceMath, MMMU-Tables, and WildStruct benchmarks, demonstrating superior robustness to visual degradation and structural complexity compared to state-of-the-art MLLMs.

## Executive Summary
TableMoE addresses the challenge of multimodal table understanding by combining symbolic semantic role prediction with neuro-symbolic expert routing. The framework leverages structured representations (HTML, JSON, Code) alongside visual inputs to enhance reasoning under structurally complex and visually degraded conditions. Through curriculum-guided training and token-role-aware routing, TableMoE achieves significant performance gains over existing models on multiple benchmarks.

## Method Summary
TableMoE is a neuro-symbolic Mixture-of-Experts (MoE) framework designed for multimodal table understanding. It employs three modality-specific experts (HTML, JSON, Code) and a token-role-aware routing mechanism that leverages symbolic semantic role prediction (header, data, formula, etc.). The system is trained on a large-scale TableMoE-Align corpus and fine-tuned using curriculum-guided neuro-symbolic annealing to enhance robustness under WildStruct conditions.

## Key Results
- Achieves up to +9.2% accuracy gains on FinanceMath, MMMU-Tables, and WildStruct benchmarks
- Demonstrates superior robustness to visual degradation and structural complexity
- Outperforms state-of-the-art MLLMs in multimodal table understanding tasks

## Why This Works (Mechanism)
TableMoE integrates symbolic semantic role prediction with neuro-symbolic expert routing to enhance table understanding. By leveraging structured representations (HTML, JSON, Code) alongside visual inputs, the model can better reason about table structure and content. The token-role-aware routing mechanism ensures that different table elements (headers, data cells, formulas) are processed by the most appropriate expert, improving overall accuracy and robustness.

## Foundational Learning
- **Symbolic Semantic Role Prediction**: Identifies roles of table elements (header, data, formula) to guide expert routing. *Why needed*: Enables context-aware processing of different table components. *Quick check*: Verify role prediction accuracy on diverse table structures.
- **Mixture-of-Experts (MoE)**: Uses multiple specialized experts for different modalities. *Why needed*: Allows task-specific processing and improves generalization. *Quick check*: Evaluate expert specialization and contribution to overall performance.
- **Curriculum-Guided Neuro-Symbolic Annealing**: Gradually introduces symbolic reasoning during training. *Why needed*: Balances symbolic and neural learning for robust performance. *Quick check*: Monitor training stability and convergence across curriculum stages.

## Architecture Onboarding
- **Component Map**: Input (Image/HTML/JSON/Code) -> Symbolic Role Predictor -> Token-Role-Aware Router -> Modality-Specific Experts (HTML/JSON/Code) -> Confidence-Aware Fusion -> Output
- **Critical Path**: Symbolic role prediction guides expert routing, which determines how each table element is processed by the most suitable expert.
- **Design Tradeoffs**: Uses structured representations for enhanced reasoning but may struggle with purely unstructured tables lacking HTML/JSON/Code metadata.
- **Failure Signatures**: Performance may degrade when symbolic role prediction is inaccurate or when structured metadata is unavailable or noisy.
- **First Experiments**:
  1. Evaluate symbolic role predictor accuracy on diverse table structures
  2. Test expert specialization by ablating individual modality experts
  3. Assess routing quality by comparing token-role-aware routing versus random routing

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on curated HTML/JSON/Code representations may limit real-world generalization when such structured metadata is unavailable or noisy
- Effectiveness of token-role-aware routing depends heavily on the quality of the symbolic role predictor, which is not independently evaluated for robustness under WildStruct conditions
- Curriculum-guided annealing strategy's impact is not ablated, making it unclear how much of the performance gain stems from neuro-symbolic integration versus training schedule design

## Confidence
- **High confidence**: Relative improvements over baseline models on curated benchmarks (FinanceMath, MMMU-Tables, WildStruct)
- **Medium confidence**: Robustness claims under visual degradation, as this depends on simulation quality and may not reflect all real-world degradation modes
- **Low confidence**: Generalization to fully unstructured tables without HTML/JSON/Code representations, and the independent contribution of the symbolic role predictor to routing quality

## Next Checks
1. Evaluate TableMoE on benchmarks containing purely unstructured table images without underlying code representations to assess real-world applicability
2. Perform ablation studies isolating the contribution of the symbolic semantic role predictor versus the expert routing mechanism
3. Test TableMoE's robustness across a wider range of visual corruption types and severity levels not covered in the WildStruct benchmark