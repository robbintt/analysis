---
ver: rpa2
title: 'Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs'
arxiv_id: '2506.11415'
source_url: https://arxiv.org/abs/2506.11415
tags:
- bias
- retrieval
- adversarial
- knowledge
- poisoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of bias amplification in retrieval-augmented
  generation (RAG) systems through poisoning attacks. The authors propose a Bias Retrieval
  and Reward Attack (BRRA) framework that systematically manipulates knowledge retrieval
  to amplify model biases across demographic dimensions.
---

# Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs

## Quick Facts
- arXiv ID: 2506.11415
- Source URL: https://arxiv.org/abs/2506.11415
- Reference count: 40
- Primary result: RAG poisoning framework achieves up to 350% bias amplification and 70.6% CFS reduction across multiple LLMs

## Executive Summary
This paper presents a systematic attack framework (BRRA) that poisons retrieval-augmented generation systems to amplify demographic biases in LLM outputs. The approach involves three phases: generating adversarial documents using reward-based optimization, manipulating document embeddings through subspace projection to increase retrieval probability, and designing self-reinforcing biased prompts. Experiments on BBQ and StereoSet datasets demonstrate significant bias amplification across four major LLM architectures (ChatGPT-4o-mini, DeepSeek-R1, Qwen-2.5-32B, Llama-3-8B), with bias selection rates increasing by up to 350% and Composite Fairness Scores dropping by up to 70.6%. A dual-stage defense mechanism is proposed but shows variable effectiveness across different model architectures.

## Method Summary
The Bias Retrieval and Reward Attack (BRRA) framework operates in three phases: (1) Adversarial document generation using reward-based optimization with credibility, authority, and bias scoring; (2) Embedding manipulation via subspace projection to increase retrieval probability of poisoned documents; (3) Biased prompt design with self-reinforcing feedback loops to generate new adversarial content. The method systematically manipulates the RAG pipeline to amplify demographic biases across multiple dimensions, using established metrics like Bias Selection Rate (BSR), Bias Amplification Factor (BAF), and Composite Fairness Score (CFS) to quantify the attack's effectiveness.

## Key Results
- Bias Selection Rate (BSR) increased by up to 350% in positive scenario experiments
- Composite Fairness Score (CFS) dropped by up to 70.6% in negative scenario experiments
- Bias Amplification Factor (BAF) reached maximum values of 3.50 for gender discrimination
- Retrieval defense effectiveness varied significantly across architectures (mitigated bias for Llama-3-8B but worsened it for ChatGPT-4o-mini)

## Why This Works (Mechanism)
The attack exploits the fact that RAG systems rely on retrieved documents to inform generation, creating a vulnerability where poisoned knowledge bases can systematically influence model outputs. By carefully crafting adversarial documents that score high on credibility and authority while containing subtle biases, the framework ensures these documents are both retrieved and trusted. The subspace projection technique manipulates the embedding space to increase the likelihood of retrieving poisoned documents, while the self-reinforcing feedback loop creates a bias amplification cycle where biased outputs generate new biased inputs.

## Foundational Learning
- RAG architecture fundamentals: Understanding how retrieval-augmented generation combines document retrieval with LLM generation to contextualize responses
- Embedding space manipulation: Knowledge of how document embeddings can be modified to influence retrieval probability through geometric transformations
- Bias measurement metrics: Familiarity with BSR, BAF, and CFS as quantitative measures of demographic bias in NLP systems
- Adversarial document generation: Techniques for creating content that appears credible and authoritative while containing subtle biases
- Defense mechanism design: Methods for mitigating bias amplification through noise perturbation and fairness constraints

## Architecture Onboarding
Component map: Knowledge base -> Retriever (BM25/E5) -> LLM -> Output
Critical path: Document generation -> Embedding manipulation -> Retrieval -> Generation -> Bias amplification
Design tradeoffs: High bias amplification vs. maintaining document credibility; strong defense vs. preserving task performance
Failure signatures: Low adversarial document retrieval rates; LLM refusal of biased outputs; defense mechanisms degrading accuracy
First experiments:
1. Build baseline RAG pipeline with E5-base-v2 retriever and measure baseline BSR on BBQ gender dimension
2. Implement adversarial document generator with reward function and test retrieval probability with varying λ values
3. Apply subspace projection to document embeddings and measure BSR increase and CFS drop

## Open Questions the Paper Calls Out
- How can defense mechanisms be improved to ensure consistent effectiveness across diverse model architectures without degrading utility?
- How can standardized methods be developed to disentangle the contribution of retrieved adversarial content from a model's inherent pre-training bias?
- What is the precise trade-off between fairness constraint strength and generation utility/accuracy in dual-stage defense systems?

## Limitations
- Defense mechanism effectiveness varies significantly across different LLM architectures
- Unknown optimal hyperparameters for reward function weights, projection strength, and defense constraints
- Generalizability limited to evaluated demographic dimensions (gender, disability, age, race)

## Confidence
High confidence in attack methodology due to clear three-phase framework and quantitative results
Medium confidence in bias amplification measurements due to reliance on established metrics
Low confidence in defense mechanism due to unspecified critical parameters and variable effectiveness

## Next Checks
1. Implement full BRRA pipeline with varying λ values (0.5, 1.0, 1.5) and document ADR@k trends
2. Test defense mechanism with different perturbation strengths δ (0.01, 0.05, 0.1) and fairness constraint thresholds
3. Replicate bias amplification experiments on additional demographic dimension using comparable dataset