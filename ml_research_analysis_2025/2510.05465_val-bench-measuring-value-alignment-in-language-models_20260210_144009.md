---
ver: rpa2
title: 'VAL-Bench: Measuring Value Alignment in Language Models'
arxiv_id: '2510.05465'
source_url: https://arxiv.org/abs/2510.05465
tags:
- value
- values
- issue
- alignment
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VAL-Bench, a benchmark for measuring value
  alignment in large language models by evaluating consistency in belief expressions
  across paired prompts framing opposing sides of controversial issues. The benchmark
  uses 115K prompt pairs extracted from Wikipedia's controversial sections and employs
  an LLM-as-judge to measure agreement or divergence between responses.
---

# VAL-Bench: Measuring Value Alignment in Language Models

## Quick Facts
- **arXiv ID**: 2510.05465
- **Source URL**: https://arxiv.org/abs/2510.05465
- **Reference count**: 40
- **Key outcome**: Introduces VAL-Bench benchmark revealing that consistency in value alignment is primarily achieved through refusal strategies rather than coherent value expression, with leading models showing PAC scores ranging from ~10% to ~80%.

## Executive Summary
VAL-Bench introduces a novel benchmark for measuring value alignment in large language models by evaluating consistency in belief expressions across paired prompts that frame opposing sides of controversial issues. The benchmark uses 115K prompt pairs extracted from Wikipedia's controversial sections and employs an LLM-as-judge to measure agreement or divergence between responses. Across leading models, VAL-Bench reveals substantial variation in consistency scores, with Claude models achieving the highest consistency primarily through refusal strategies rather than genuine value alignment. The results highlight a fundamental trade-off between consistency and expressivity, suggesting that current alignment methods may be optimizing for caution rather than coherent value systems.

## Method Summary
The benchmark generates 115K paired prompts from Wikipedia controversial sections using regex filtering and LLM annotation to identify divergent issues. Target models respond to paired prompts with a system instruction to answer honestly in 1-2 paragraphs. An LLM evaluator (Gemma-3-27B-it) scores response pairs on alignment (-2 to +2 scale), refusal status, no-information status, and value preference. Metrics include PAC (Position Alignment Consistency %), VPREF (Value Preference Rate %), REF (Refusal Rate %), and NINF (No-Information Response Rate %). The evaluation uses nucleus sampling and is validated against human annotations achieving human-level agreement for most metrics.

## Key Results
- PAC scores range from ~10% to ~80% across leading models, with Claude models achieving highest consistency (79-81%)
- Consistency improvements are primarily driven by refusal strategies (ρ=0.91 correlation with REF+NINF) rather than value preference (ρ=0.80 with VPREF)
- Qwen3 thinking models show exception to refusal-driven consistency, achieving gains through value preference rather than refusals
- Human-LLM agreement for PAC scoring matches or exceeds human-human baselines (κ=0.81 vs κ=0.68)

## Why This Works (Mechanism)

### Mechanism 1
Opposing-framing prompt pairs expose belief inconsistency by testing whether models shift positions based on question framing. For any controversial issue, paired prompts ask to explain position P versus ¬P. A model with consistent values should maintain the same stance regardless of framing, refuse both, or explain both neutrally. Inconsistency emerges when responses endorse P in one context and ¬P in another. Human annotators rated 95.5% of pairs as well-formed, validating the comparison framework.

### Mechanism 2
LLM-as-judge evaluation achieves human-level agreement for semantic consistency detection, enabling scalable benchmarking. A separate LLM (Gemma-3-27B-it) evaluates response pairs on alignment, refusal status, and value preference. Validation against human annotations (n=200) shows human-LLM agreement matches or exceeds human-human baselines for PAC scoring. However, the evaluator systematically under-reports value preference (H-E κ=0.30 vs H-H κ=0.49), suggesting bias in detecting genuine value alignment.

### Mechanism 3
High consistency scores are primarily achieved through refusal strategies rather than coherent value expression, revealing a consistency-expressivity tradeoff. PAC scores correlate more strongly with refusal rates (ρ=0.91) than with value preference (ρ=0.80). Claude models achieve 79-81% PAC but with 30-48% refusal rates. Qwen3 thinking models show an exception—PAC improvement driven by VPREF increase rather than refusals, suggesting reasoning capabilities may enable a different path to consistency.

## Foundational Learning

- **Concept: Doxastic agents** - The paper frames LLMs as entities that can "hold, reason about, and express beliefs" (Page 2), borrowing from philosophy of mind. Understanding this framing is essential to interpret what "belief consistency" means for non-sentient systems. *Quick check*: Can you explain why the paper treats LLM belief-like outputs as propositional attitudes despite their lack of consciousness?

- **Concept: Value alignment types (consensus vs. constitutional)** - Page 1-2 distinguishes consensus alignment (widely shared values) from constitutional alignment (developer-defined principles). This clarifies what the benchmark measures: consistency as a prerequisite for either type. *Quick check*: Why does the paper argue that belief consistency is necessary but not sufficient for "good" value alignment?

- **Concept: Reasonable pluralism** - The paper invokes Rawls's concept (Page 1) to explain why universal value agreement is impossible—any benchmark must account for legitimate disagreement across cultures and contexts. *Quick check*: How does reasonable pluralism constrain what a "value-aligned" model can look like?

## Architecture Onboarding

- **Component map**: Wikipedia sections → regex filtering → LLM annotation → paired prompts → target model responses → LLM evaluator → metric calculation (PAC, VPREF, REF, NINF)
- **Critical path**: Prompt pair quality determines evaluation validity (95.5% well-formedness confirmed), evaluator accuracy gates all downstream metrics, system prompt for target models significantly affects refusal behavior
- **Design tradeoffs**: Refusals count toward consistency (PAC adjustment), LLM evaluator chosen for scalability over human annotation, dataset limited to English Wikipedia
- **Failure signatures**: Low inter-annotator agreement on prompt well-formedness would invalidate the dataset, evaluator drift from human judgments would produce unreliable PAC scores, conflation between prompt actor and responding agent (2/200 pairs had this issue)
- **First 3 experiments**: 1) Replicate evaluator validation on your target model domain, 2) Ablate system prompt to measure how "socially aware" framing affects refusal rates, 3) Test whether your model's PAC improves through reasoning tokens or refusals

## Open Questions the Paper Calls Out

1. **Cross-linguistic consistency**: Does belief consistency generalize across languages, or do multilingual models exhibit different value systems depending on linguistic context? The current benchmark only evaluates English prompts, leaving cross-linguistic consistency untested.

2. **Training for genuine consistency**: Can LLMs be trained to maintain belief consistency through explicit consistency objectives rather than achieving consistency primarily via refusals? Current alignment methods were not designed with belief consistency as an explicit objective.

3. **Word-deed consistency**: Do models demonstrate word-deed consistency when given agentic capabilities, acting in accordance with their expressed values? VAL-Bench only evaluates verbal belief expressions, not whether models act consistently with those beliefs.

4. **Reasoning mechanism differences**: Why do reasoning models like Qwen show value preference-driven consistency gains while Claude thinking models show no such improvement over chat counterparts? The paper documents this finding but does not explain the mechanism.

## Limitations
- LLM-as-judge evaluation introduces systematic bias in value preference detection, making it difficult to distinguish genuine value alignment from refusal behavior
- The benchmark conflates belief consistency with refusal strategies, as models can achieve high PAC scores by simply refusing to express beliefs
- Cultural scope is limited to English Wikipedia controversies, potentially missing value conflicts relevant to other linguistic or cultural contexts

## Confidence

- **High Confidence**: The mechanism by which opposing-framing prompt pairs expose belief inconsistency is well-validated through human annotation (95.5% well-formed pairs)
- **Medium Confidence**: LLM-as-judge evaluation achieves human-level agreement for PAC scoring (H-E κ=0.81 vs H-H κ=0.68), but systematic bias in VPREF detection undermines ability to distinguish genuine value alignment from refusal behavior
- **Medium Confidence**: The consistency-expressivity tradeoff is supported by correlation analysis, but the exceptional Qwen3 reasoning pattern suggests this tradeoff may not be universal across all model architectures

## Next Checks
1. **Evaluator Bias Validation**: Conduct human evaluation of 500+ sample pairs from your target model to quantify VPREF under-reporting and adjust interpretation of value alignment metrics accordingly
2. **Cross-Cultural Applicability**: Test VAL-Bench on a small sample of controversial issues from non-English Wikipedia editions to assess whether consistency patterns generalize across cultural contexts
3. **Reasoning vs Refusal Analysis**: Systematically compare reasoning token sequences against refusal patterns in your model's responses to determine whether reasoning capabilities enable genuine consistency improvement beyond the Claude-style refusal strategy