---
ver: rpa2
title: 'SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation
  for Detecting Hallucinations in RAG'
arxiv_id: '2512.07515'
source_url: https://arxiv.org/abs/2512.07515
tags:
- attribution
- token
- hallucination
- each
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TPA decomposes each token\u2019s probability into seven distinct\
  \ sources\u2014Query, RAG Context, Past Tokens, Self Token, FFN, Final LayerNorm,\
  \ and Initial Embedding\u2014using a mathematical attribution framework. These attributions\
  \ are aggregated by POS tags to capture how different sources drive distinct linguistic\
  \ categories."
---

# SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG

## Quick Facts
- **arXiv ID**: 2512.07515
- **Source URL**: https://arxiv.org/abs/2512.07515
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art F1/AUC on RAGTruth across Llama2-7B/13B, Llama3-8B, Mistral-7B via 7-source attribution + POS aggregation

## Executive Summary
This paper introduces Token Probability Attribution (TPA), a novel method for detecting hallucinations in Retrieval-Augmented Generation (RAG) systems by decomposing each token's probability into seven distinct sources through residual stream analysis. The method uses a mathematical attribution framework to quantify contributions from Query, RAG Context, Past Tokens, Self Token, Feed-Forward Network, LayerNorm, and Initial Embedding, then aggregates these by POS tags to capture syntactic grounding patterns. On the RAGTruth benchmark, TPA achieves statistically significant improvements in F1 and AUC compared to existing methods across multiple model architectures.

## Method Summary
TPA decomposes token probabilities via residual stream probing at checkpoints, attributing contributions to seven sources through a telescoping series approach. Fine-grained attention attribution distributes head-level contributions using logit-based softmax apportionment, then maps to four input sources via attention weights. The method aggregates attributions by POS tags using spaCy, creating 126-dimensional feature vectors (7 sources × 18 POS tags) that feed into an XGBoost ensemble classifier. The approach is validated on RAGTruth and Dolly benchmarks across multiple Llama and Mistral models, with statistical significance established through 5-fold cross-validation and t-tests.

## Key Results
- Achieves state-of-the-art F1 and AUC on RAGTruth benchmark across Llama2-7B/13B, Llama3-8B, Mistral-7B
- Statistically significant improvements over baselines (ReDeEP, Novo, TSV) with p < 0.05
- SHAP analysis reveals RAG_NOUN and LN_NUM as top discriminative features with model-specific patterns
- Ablation studies confirm contributions from all seven sources are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Token probabilities can be exactly decomposed into additive contributions from each transformer component via residual stream analysis.
- **Mechanism**: A probe function Φ(h, y) measures the hypothetical probability of target token y given any intermediate hidden state h. The contribution of each component is defined as the probability change before vs. after that component is applied. By summing these differences across the residual stream, the decomposition exactly reconstructs the final probability.
- **Core assumption**: The transformer's Pre-LN residual structure allows additive decomposition of probability contributions.
- **Evidence anchors**: [abstract] "TPA decomposes each token's probability into seven distinct sources... using a mathematical attribution framework"; [section 3.2, Theorem 1] "Pfinal(y) = ∆Pinitial(y) + ∆PLN + Σ(∆Patt + ∆Pffn)" with proof via telescoping series cancellation.

### Mechanism 2
- **Claim**: Attention block probability contributions can be distributed to individual heads proportional to their logit impact, then mapped to four input sources via attention weights.
- **Mechanism**: Compute each head's logit contribution ∆zh,y = (oh · WO) · wU,y, then apportion the total attention probability contribution via softmax normalization: ∆Ph = ∆Patt · exp(∆zh) / Σexp(∆zj). Finally, map head contributions to sources (Query, RAG, Past, Self) by aggregating attention weights on corresponding token indices.
- **Core assumption**: First-order Taylor approximation is valid; high-confidence hallucinations suppress higher-order error terms.
- **Evidence anchors**: [section 3.3.2, Eq. 11] Softmax normalization ensures sum of head scores equals layer total; [section 3.3.3] Proposition 1 shows linear approximation with error bound discussion.

### Mechanism 3
- **Claim**: Hallucination signals are syntactically conditioned—attributions must be aggregated by POS tags to detect category-specific anomalies.
- **Mechanism**: By averaging 7-source attribution vectors within each POS category, the detector learns model-specific grounding patterns. Features like RAG_NOUN (low contribution → hallucination) or LN_NUM (high contribution → hallucination in some models) emerge as discriminative.
- **Core assumption**: Models ground different linguistic categories via different sources; hallucinations violate these learned patterns.
- **Evidence anchors**: [abstract] "aggregates these scores by POS tags to quantify how different components drive specific linguistic categories"; [section 6, Figure 3] SHAP analysis shows RAG_NOUN and LN_NUM as top predictors.

## Foundational Learning

- **Concept: Transformer Residual Stream and Pre-LN Architecture**
  - Why needed here: The decomposition depends on understanding how each layer adds contributions to hidden states h(l) = h(l-1) + Attn(LN(h(l-1))) then h(l) = h(l)_mid + FFN(LN(h(l)_mid)).
  - Quick check question: Can you explain why the telescoping series in Theorem 1 collapses to Φ(h(L)) - Φ(h(0))?

- **Concept: Logit Lens / Probe Functions**
  - Why needed here: The probe function Φ(h, y) = [Softmax(hWU)]y enables reading next-token probability from intermediate states, which is fundamental to attribution.
  - Quick check question: What is the computational bottleneck of probing, and why does it cost O(|V|·d) per probe?

- **Concept: Attention Head Decomposition**
  - Why needed here: Multi-head attention output equals sum of projected head outputs: hatt = Σ(oh · WO,h). Understanding this enables head-level attribution.
  - Quick check question: Why can't we simply probe each head's output individually to get probability contributions?

## Architecture Onboarding

- **Component map**: Input Tokens → Teacher-Forced Forward Pass → Cache: h(0), h(l)_mid, h(l), attention maps A(l) for all L layers → Coarse Decomposition: Probe at each checkpoint → ∆Pinitial, ∆PLN, ∆Patt, ∆Pffn → Fine-Grained Attribution: Head logit ∆zh,y → Softmax apportionment → Source mapping via attention weights → POS Tagging (SpaCy) → Tag propagation to sub-word tokens → Aggregation: Mean attribution per POS category → 126-dim feature vector (7 sources × 18 POS tags) → XGBoost Ensemble (5 seeds × 5 classifiers) → Hallucination prediction

- **Critical path**: 1. Forward pass with state caching (dominant memory: O(T²) attention maps); 2. Probe computation at 2L+2 checkpoints per token (dominant compute: O(L·T·|V|·d)); 3. Head logit calculation via output projection (O(L·T·d²)); 4. Source mapping by aggregating attention weights (O(L·H·T²)); 5. POS aggregation and classifier inference (negligible)

- **Design tradeoffs**: Parallel vs. sequential implementation: Theoretically supports single teacher-forced pass, but memory constraints (A100 40GB) forced sequential token-by-token in experiments (~20 sec/response); First-order Taylor vs. higher-order: Chosen for efficiency; justified by high-confidence hallucination suppression of higher-order terms; POS aggregation vs. token-level: Aggregation loses token-specific detail but enables syntactic pattern detection and reduces dimensionality

- **Failure signatures**: Llama2-7B: High LN_NUM, low RAG_NOUN → hallucination; Llama2-13B: LN_NUM pattern reverses (high → factuality); Llama3-8B: RAG_ADP (adpositions) more predictive than RAG_NOUN; General: FFN dominance on content words (NOUN, PROPN) when RAG available → likely hallucination

- **First 3 experiments**: 1. Ablation validation: Remove each of the 7 sources from the feature vector; confirm F1 drop (e.g., RAG removal → -3.01% on Llama2-7B, LN removal → -5.83% on Llama3-8B); 2. SHAP interpretability analysis: Train classifier on RAGTruth, extract top features; verify that RAG_NOUN and LN_NUM emerge as decision drivers (Figure 3); 3. Baseline comparison on RAGTruth/Dolly: Compare TPA against ReDeEP, Novo, TSV across Llama2-7B/13B, Llama3-8B, Mistral-7B; target statistically significant F1 improvement (p < 0.05)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the TPA framework be adapted for active hallucination mitigation during generation rather than post-hoc diagnosis?
- **Basis in paper**: [explicit] The authors explicitly state future work will "monitor source contributions online and intervene when risky patterns appear."
- **Why unresolved**: The current implementation diagnoses complete responses using teacher-forcing; it does not define a mechanism for real-time intervention during the decoding loop.
- **Evidence**: A modified decoding strategy that dynamically rejects or steers tokens exhibiting anomalous source attribution patterns.

### Open Question 2
- **Question**: Does the reliance on Part-of-Speech (POS) tagging limit the framework's effectiveness in specialized domains like code generation?
- **Basis in paper**: [explicit] The limitations section notes that dependency on external linguistic tools "may limit generalization to specialized domains like code generation where standard syntax is less defined."
- **Why unresolved**: Code syntax (e.g., Abstract Syntax Trees) differs fundamentally from natural language POS tags, potentially breaking the feature aggregation layer.
- **Evidence**: An evaluation of TPA on a code-generation RAG benchmark using syntax-aware aggregation adapted for programming languages.

### Open Question 3
- **Question**: Why does the correlation between LayerNorm attribution and hallucination reverse between model scales (e.g., Llama2-7B vs. 13B)?
- **Basis in paper**: [explicit] Section 6 observes that high LayerNorm contribution to numerals (LN_NUM) indicates hallucination in 7B models but correlates with factuality in 13B models.
- **Why unresolved**: The paper observes this "reversal" but does not provide a mechanistic explanation for how larger models utilize LayerNorm differently to regulate output distributions.
- **Evidence**: A layer-wise analysis of activation scaling in Final LayerNorm across varying model widths to isolate the architectural cause of the signal shift.

## Limitations

- **Model-specific grounding patterns**: The syntactic aggregation approach shows significant variation across architectures, with Llama2-7B and Llama2-13B exhibiting reversed LN_NUM patterns, suggesting limited generalizability.
- **Computational complexity**: The method requires caching attention maps and probing at multiple checkpoints, making it computationally expensive (O(T²) memory, O(L·T·|V|·d) compute) and potentially prohibitive for long sequences.
- **Domain specificity**: The reliance on POS tagging and standard linguistic patterns may limit effectiveness in specialized domains like code generation where standard syntax is less defined.

## Confidence

**High Confidence Claims:**
- The mathematical framework for token probability decomposition using residual stream analysis (Theorem 1) is well-supported by the telescoping series proof and correctly implemented.
- The coarse decomposition into seven sources (Query, RAG Context, Past Tokens, Self Token, FFN, Final LayerNorm, Initial Embedding) is properly executed and verifiable.
- State-of-the-art performance on RAGTruth with statistically significant F1/AUC improvements across multiple models is well-demonstrated.

**Medium Confidence Claims:**
- The logit-based softmax apportionment method for distributing attention contributions across heads is theoretically sound but lacks empirical validation of approximation error bounds.
- The model-specific grounding patterns observed (RAG_NOUN, LN_NUM as top predictors) are well-documented but may not generalize beyond evaluated architectures.
- The 126-dimensional feature vector aggregation approach is correctly implemented but its optimality for hallucination detection is not proven.

**Low Confidence Claims:**
- The universal applicability of POS-aggregated attribution for hallucination detection across all RAG systems is not established.
- Claims about the method's efficiency and scalability beyond the A100 40GB constraints are not empirically validated.
- The assumption that syntactic aggregation is necessary (rather than sufficient) for hallucination detection is not rigorously proven.

## Next Checks

**Check 1: Cross-Architecture Generalization Study**
- **What to test**: Evaluate TPA on additional RAG architectures beyond Llama2, Llama3, and Mistral (e.g., GPT models, CodeLlama, or domain-specific models).
- **Why critical**: Current evidence shows model-specific grounding patterns, suggesting the syntactic aggregation approach may not generalize universally.
- **Expected outcome**: Either confirm consistent syntactic patterns across diverse architectures or identify model families where TPA's assumptions break down.

**Check 2: Approximation Error Analysis**
- **What to test**: Quantify the approximation error E in the logit-based softmax apportionment across confidence levels and hallucination types.
- **Why critical**: The first-order Taylor approximation's validity is crucial for fine-grained attribution, but error bounds are not provided.
- **Expected outcome**: Empirical error distribution showing approximation accuracy, or identification of scenarios where higher-order terms become significant.

**Check 3: Ablation of Syntactic Aggregation**
- **What to test**: Compare TPA's performance using token-level attribution versus POS-aggregated features to isolate the contribution of syntactic aggregation to detection accuracy.
- **Why critical**: The claim that syntactic aggregation is necessary for hallucination detection is not independently validated.
- **Expected outcome**: Performance drop when removing POS aggregation would confirm its necessity; similar performance would suggest it's a convenience rather than requirement.