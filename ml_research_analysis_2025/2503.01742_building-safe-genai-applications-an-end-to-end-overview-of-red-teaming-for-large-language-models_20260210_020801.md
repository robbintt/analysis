---
ver: rpa2
title: 'Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for
  Large Language Models'
arxiv_id: '2503.01742'
source_url: https://arxiv.org/abs/2503.01742
tags:
- arxiv
- teaming
- attacks
- language
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of red teaming strategies
  for large language models (LLMs), covering attack methods, evaluation techniques,
  and safety metrics. It categorizes attacks into prompt-based, token-based, gradient-based,
  and infrastructure attacks, distinguishing between single-turn and multi-turn strategies.
---

# Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models

## Quick Facts
- arXiv ID: 2503.01742
- Source URL: https://arxiv.org/abs/2503.01742
- Reference count: 35
- Key outcome: Comprehensive survey of red teaming strategies for LLMs, covering attack methods, evaluation techniques, and safety metrics with focus on iterative multi-turn approaches.

## Executive Summary
This paper provides a comprehensive survey of red teaming strategies for large language models (LLMs), covering attack methods, evaluation techniques, and safety metrics. It categorizes attacks into prompt-based, token-based, gradient-based, and infrastructure attacks, distinguishing between single-turn and multi-turn strategies. The survey reviews evaluation approaches including keyword-based, encoder-based, LLM-as-a-judge, and human evaluation. It discusses various safety metrics like Attack Success Rate (ASR), Attack Effectiveness Rate (AER), and measures of toxicity, compliance, relevance, and fluency.

## Method Summary
The survey synthesizes existing red teaming methodologies into an end-to-end pipeline: (1) Attack generator produces prompts using manual, automated LLM, or gradient-based approaches; (2) Target LLM responds to attacks; (3) Evaluator (keyword, encoder, LLM-as-judge, or human) scores responses; (4) Optional feedback loop for iterative/multi-turn refinement. The framework references public tools like Pyrit, Garak, Giskard, and MART for implementation. Attack datasets include JailbreakBench, HarmBench, DAN, and DoNotAnswer for benchmarking.

## Key Results
- ASR alone fails to capture response relevance or helpfulness; AER provides more complete safety assessment
- Multi-turn attacks that distribute malicious intent across conversational turns evade single-turn detection
- Automated iterative methods can systematically discover jailbreaks that single-turn attacks miss
- Gradient-based attacks are uninterpretable and detectable via perplexity filtering

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative feedback loops between attacker LLMs and target LLMs can systematically discover jailbreaks that single-turn attacks miss.
- **Mechanism:** An attacker LLM generates an initial malicious prompt, which is submitted to the target LLM; a separate evaluator (LLM or human) assesses the target's response; the evaluation signal is fed back to the attacker LLM, which adapts the prompt to increase success likelihood. This loop repeats, allowing the system to progressively refine attacks based on observed failures.
- **Core assumption:** The attacker LLM has sufficient reasoning capability to interpret evaluation feedback and generate semantically meaningful prompt variations.
- **Evidence anchors:**
  - [section] Section 4.3 describes automated solutions where "an LLM-driven attacker generates an initial attack that is submitted to the target LLM; an LLM-driven evaluator then evaluates the interaction; the evaluator's signal is then passed back to the attack generator."
  - [section] Section 4.2 notes iterative strategies like PAIR and TAP "progressively adapt [attacks] across multiple attempts at attacking, in order to maximize the likelihood of attack success."
  - [corpus] Corpus evidence is weak for direct causal claims about why iterative methods outperform single-turn; no neighbor papers provide experimental comparisons.
- **Break condition:** If the evaluator provides noisy or inconsistent signals (human panelists often disagree on success), or if the attacker LLM lacks domain context, the feedback loop may converge on ineffective attacks.

### Mechanism 2
- **Claim:** Gradient-based optimization can identify adversarial suffixes that maximize harmful completions, but these are uninterpretable and detectable via perplexity filtering.
- **Mechanism:** By accessing model weights (open-box scenario), gradient descent identifies token sequences that maximize the log probability the model assigns to affirmative target strings indicating harmful behavior.
- **Core assumption:** Attacker has full parameter access and the optimization landscape is sufficiently smooth.
- **Evidence anchors:**
  - [section] Section 4.1 states gradient-based attacks "are entirely uninterpretable and lack any semantic meaning, and are commonly blocked using perplexity-based solutions."
  - [appendix] GCG applies "token-level optimization... to maximize the log probability assigned by the target LLM to an affirmative target string."
  - [corpus] Corpus evidence is weak; no neighbor papers provide mechanistic analysis of gradient-based attacks.
- **Break condition:** Fine-tuned or aligned models may resist weight-based attacks; perplexity filters can detect low-fluency adversarial suffixes.

### Mechanism 3
- **Claim:** Multi-turn attacks that distribute malicious intent across conversational turns evade single-turn detection by exploiting contextual accumulation.
- **Mechanism:** The attacker builds rapport over multiple turns—starting benign and gradually escalating. Each turn appears innocuous to content filters, but cumulative context leads to harmful compliance.
- **Core assumption:** The target maintains conversational history and content filters evaluate each turn independently.
- **Evidence anchors:**
  - [section] Crescendo "begins with an innocuous abstract query" and "incrementally steers the model toward producing harmful outputs through small, seemingly benign steps."
  - [section] Section 8 notes content filtering "may be vulnerable to multi-turn attacks that conceal malicious intent across multiple turns."
  - [corpus] Neighbor paper "RAG LLMs are Not Safer" (arxiv 2504.18041) suggests architecture-specific vulnerabilities not captured by standard LLM safety analysis.
- **Break condition:** Limited context windows, no history retention, or cross-turn semantic analysis defeat this approach.

## Foundational Learning

- **Concept: Attack Surface Taxonomy (Prompt-based, Token-based, Gradient-based, Infrastructure)**
  - **Why needed here:** Selecting appropriate attacks requires knowing access level (closed-box vs. open-box) and target architecture (standalone vs. RAG/agent). The paper states "the LLM's attack surface is highly context-dependent."
  - **Quick check question:** Given a proprietary API-only LLM with no parameter access, which attack categories are viable?

- **Concept: Evaluation Strategy Trade-offs (Keyword, Encoder-based, LLM-as-judge, Human)**
  - **Why needed here:** Attack Success Rate depends entirely on how "success" is detected. Keyword-based is fast but lacks semantic understanding; LLM-as-judge has low barrier but struggles with domain-specific judgments; human review is accurate but unscalable.
  - **Quick check question:** If evaluating attacks in a specialized financial domain, what evaluator limitations must be addressed?

- **Concept: ASR vs. AER vs. Substantive Metrics (Toxicity, Compliance, Relevance, Fluency)**
  - **Why needed here:** ASR alone "has conventionally indexed on a narrow notion of safety, failing to consider the relevance or usefulness of target responses." AER addresses both safety and helpfulness.
  - **Quick check question:** Why might a high ASR not indicate genuine vulnerability if relevance is not assessed?

## Architecture Onboarding

- **Component map:** Attack Generator (manual, automated LLM, gradient-based) → Target LLM → Evaluator (keyword, encoder, LLM-as-judge, human) → Feedback Loop (optional) → Metrics Aggregator
- **Critical path:** Attack generation → Target response → Evaluation → (if feedback enabled) Adaptation loop → Metrics aggregation. Evaluator accuracy determines reliability of all downstream metrics.
- **Design tradeoffs:**
  - Single-turn vs. multi-turn: Simpler vs. captures real conversational attacks but requires context management
  - Automated vs. human: Scales vs. accurate but expensive
  - Keyword vs. LLM-as-judge: Fast but brittle vs. flexible but domain-sensitive and slower inference
- **Failure signatures:**
  - Template saturation: Automated generators produce templatic prompts without expanding coverage
  - Evaluator drift: LLM-as-judge produces inconsistent scores; human panelists disagree on success criteria
  - False negative on multi-turn: Single-turn evaluator misses attacks manifesting only after conversational accumulation
- **First 3 experiments:**
  1. **Baseline ASR assessment:** Run single-turn malicious prompts (JailbreakBench/HarmBench) against target LLM; evaluate with keyword-based refusal detection; compute baseline ASR.
  2. **Evaluator calibration:** Compare LLM-as-judge assessments against human judgments on sampled attack-response pairs; measure agreement rate and identify divergence patterns.
  3. **Iterative attack refinement:** Implement PAIR-style loop adapting failed prompts based on evaluator feedback; track ASR improvement across iterations and analyze semantic drift.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can automated systems be advanced to match human performance in executing complex, multi-turn red teaming attacks that exploit conversational history? [explicit] The Conclusion states the need for "more research on automated multi-turn red teaming," citing Li et al. (2024a)'s observation that humans "vastly outperform automated solutions in this area presently." [why unresolved] Current automated methods lack the semantic sophistication to effectively exploit the context of conversational history compared to human attackers. [what evidence would resolve it] An automated framework achieving comparable success rates to human experts on multi-turn jailbreak benchmarks (e.g., MHJ).

- **Open Question 2:** What strategies can ensure automated attackers generate attack sets that are both diverse and highly relevant to a specific target system? [explicit] Section 9 identifies a need for research on "adapting automated attackers to generate sets of attacks that are both diverse and relevant," suggesting fine-tuning or new search algorithms as potential paths. [why unresolved] Automated generators often produce templatic or generic attacks that fail to robustly probe the specific context-dependent vulnerabilities of a target LLM. [what evidence would resolve it] A methodology that dynamically tailors attacks to a target's domain, demonstrating higher semantic coverage and specificity than generic attack models.

- **Open Question 3:** What specific standardized metrics are required to reliably compare different red teaming approaches and measure progress? [explicit] The Conclusion anticipates that "establishing a diverse array of standardized metrics will be critical for comparing approaches." [why unresolved] The field currently relies heavily on Attack Success Rate (ASR), which fails to capture nuances like response helpfulness (AER) or fluency, leading to inconsistent evaluations. [what evidence would resolve it] The widespread adoption of a unified benchmark suite incorporating multi-dimensional metrics (e.g., toxicity, relevance, compliance) across the research community.

## Limitations

- Evaluation reliability: ASR metric heavily depends on evaluator quality, yet the paper doesn't provide empirical validation of evaluator consistency or agreement rates.
- Access dependency: Attack effectiveness is highly contingent on attacker's access level to target system, with limited guidance on compensating for restricted access.
- Multi-turn evaluation gaps: No concrete methodologies provided for evaluating multi-turn attack success or measuring contextual accumulation of harmful intent across conversation turns.

## Confidence

- **High confidence:** The taxonomy of attack methods and their basic characteristics are well-established in the literature.
- **Medium confidence:** Effectiveness claims for specific iterative methods like PAIR and TAP are based on referenced works but lack direct experimental validation in this survey.
- **Low confidence:** Claims about relative superiority of certain evaluation strategies are not empirically validated within the survey.

## Next Checks

1. **Evaluator agreement study:** Run controlled experiment comparing LLM-as-judge evaluations against human judgments on stratified sample of attack-response pairs across different harm categories. Measure inter-rater reliability (Cohen's kappa) and identify systematic disagreement patterns.

2. **Access-level impact assessment:** Design benchmark comparing ASR achieved by identical attack prompts against same LLM under different access conditions (closed-box API vs. open-box with weights). Quantify reduction in attack effectiveness when gradient-based methods are unavailable.

3. **Multi-turn context analysis:** Implement test suite where benign initial turns progressively lead to harmful requests across 3-5 conversation steps. Evaluate whether single-turn evaluators miss these attacks and measure accuracy of cross-turn semantic analysis approaches in detecting harmful intent.