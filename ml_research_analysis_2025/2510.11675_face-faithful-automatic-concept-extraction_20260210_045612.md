---
ver: rpa2
title: 'FACE: Faithful Automatic Concept Extraction'
arxiv_id: '2510.11675'
source_url: https://arxiv.org/abs/2510.11675
tags:
- concept
- face
- c-ins
- c-del
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of existing concept-based explanation
  methods that fail to align extracted concepts with the model's true decision-making
  process, compromising explanation faithfulness. The authors propose FACE (Faithful
  Automatic Concept Extraction), which augments Non-negative Matrix Factorization
  (NMF) with a Kullback-Leibler (KL) divergence regularization term to enforce predictive
  consistency between original and concept-based model predictions.
---

# FACE: Faithful Automatic Concept Extraction

## Quick Facts
- arXiv ID: 2510.11675
- Source URL: https://arxiv.org/abs/2510.11675
- Reference count: 40
- Primary result: FACE achieves C-Ins scores of 0.969-0.974 and C-Gini scores of 0.895-0.949, outperforming existing concept-based explanation methods in faithfulness and sparsity.

## Executive Summary
FACE (Faithful Automatic Concept Extraction) addresses a critical limitation in concept-based model explanations by ensuring that extracted concepts truly align with a model's decision-making process. Unlike prior methods that rely solely on encoder activations, FACE incorporates classifier supervision through KL divergence regularization, enforcing predictive consistency between original and concept-based predictions. This approach guarantees that discovered concepts remain faithful to the model's actual reasoning rather than being artifacts of the encoding process.

The method augments Non-negative Matrix Factorization (NMF) with a KL divergence term that bounds the deviation in predictive distributions, promoting faithful local linearity in the learned concept space. Systematic evaluations across ImageNet, COCO, and CelebA datasets demonstrate that FACE significantly outperforms existing methods like TCAV, ACE, and C-TME in both faithfulness (C-Ins scores reaching 0.969-0.974) and sparsity metrics (C-Gini scores of 0.895-0.949). The approach requires only a single forward pass for concept extraction, offering computational advantages over baselines that need multiple passes.

## Method Summary
FACE combines NMF with KL divergence regularization to enforce predictive consistency between original model predictions and concept-based representations. The method learns concepts by optimizing an objective that includes both reconstruction fidelity and a KL divergence term measuring the difference between predictive distributions from the original model and the concept-based approximation. This dual objective ensures that extracted concepts not only reconstruct input data well but also preserve the model's predictive behavior. The approach operates by first extracting concept activations from encoder layers, then applying the regularized NMF to discover a concept basis that maintains faithfulness to the classifier's decision boundaries.

## Key Results
- FACE achieves C-Ins faithfulness scores of 0.969-0.974 compared to 0.908-0.932 for baseline methods
- C-Gini sparsity scores reach 0.895-0.949, demonstrating effective concept representation
- Single forward pass requirement provides computational efficiency over multi-pass baseline approaches
- Systematic improvements across ImageNet, COCO, and CelebA datasets validate robustness

## Why This Works (Mechanism)
FACE works by addressing the fundamental disconnect between concept extraction and model decision-making in existing methods. Traditional approaches extract concepts solely from encoder activations without considering how these concepts relate to the classifier's actual reasoning process. By incorporating KL divergence regularization, FACE ensures that the learned concept space preserves the predictive distribution of the original model. This creates a direct link between concept extraction and model decision boundaries, preventing the discovery of concepts that may be statistically present in the data but irrelevant to the model's predictions.

## Foundational Learning
- **Non-negative Matrix Factorization (NMF)**: Factorizes data matrices into non-negative components, useful for discovering interpretable parts-based representations in data. Needed because it provides a principled way to extract concept bases from high-dimensional activations.
- **KL Divergence**: Measures the difference between two probability distributions, used here to enforce predictive consistency. Needed because it provides a differentiable objective that bounds predictive distribution deviation.
- **Concept-based Explanations**: Interpretability methods that explain model decisions through human-understandable concepts rather than individual features. Needed because they provide more intuitive explanations for end users.
- **Encoder-Classifier Separation**: The architectural division between feature extraction and decision-making in neural networks. Needed because FACE leverages both components to ensure faithful concept extraction.
- **Predictive Consistency**: The requirement that concept-based models preserve the original model's predictive behavior. Needed because it ensures faithfulness to the actual decision-making process.
- **Local Linearity**: The property that model behavior can be approximated linearly in local regions of the input space. Needed because it enables concept-based explanations to remain faithful across input variations.

## Architecture Onboarding

**Component Map**: Input Data -> Encoder -> Concept Extractor -> KL Divergence Regularization -> Classifier Supervision -> Faithful Concepts

**Critical Path**: The critical computational path involves a single forward pass through the encoder to obtain activations, followed by the NMF-based concept extraction with KL divergence regularization. This contrasts with baseline methods that require multiple passes for different concept evaluations.

**Design Tradeoffs**: FACE trades increased model complexity (additional regularization term) for improved faithfulness and interpretability. The method sacrifices some reconstruction accuracy to ensure that concepts remain aligned with model predictions, prioritizing faithfulness over pure data reconstruction.

**Failure Signatures**: The method may fail when concepts extracted are too abstract or when the KL divergence regularization becomes too restrictive, potentially limiting the diversity of discovered concepts. Additionally, performance may degrade on tasks where the model's decision boundaries are highly non-linear or discontinuous.

**First Experiments**:
1. **Ablation Study**: Remove the KL divergence term to demonstrate its necessity for faithfulness, showing performance degradation without regularization.
2. **Concept Quality Analysis**: Visualize extracted concepts on sample images to verify that they correspond to meaningful semantic patterns rather than arbitrary features.
3. **Predictive Distribution Comparison**: Compare the predictive distributions of the original model and the concept-based approximation to quantify the effectiveness of KL divergence regularization.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis shows medium confidence with limited formal proofs beyond bounding predictive distribution deviation
- Evaluation primarily focused on image classification tasks, limiting generalizability to other data modalities
- Computational efficiency claims lack detailed runtime and memory usage comparisons across different model scales

## Confidence
- Theoretical grounding: Medium - Limited formal proofs for KL divergence regularization effectiveness
- Evaluation methodology: Medium - Comprehensive but primarily image-focused datasets
- Computational efficiency: Medium - Single forward pass advantage not fully quantified across configurations

## Next Checks
1. **Cross-modal validation**: Test FACE's performance on non-image datasets (text, tabular, or multimodal data) to assess generalizability beyond the computer vision domain where current results are established.

2. **Scalability analysis**: Conduct systematic evaluation of computational efficiency across different model architectures (varying number of layers, parameter counts) and hardware configurations to verify claimed efficiency advantages.

3. **Longitudinal faithfulness testing**: Perform extended validation where concepts are extracted and evaluated across multiple training epochs to assess whether the faithfulness properties remain stable as model weights evolve during training.