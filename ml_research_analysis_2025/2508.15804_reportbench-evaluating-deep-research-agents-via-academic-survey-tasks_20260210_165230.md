---
ver: rpa2
title: 'ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks'
arxiv_id: '2508.15804'
source_url: https://arxiv.org/abs/2508.15804
tags:
- research
- deep
- arxiv
- survey
- cited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ReportBench, a benchmark designed to evaluate
  the content quality of research reports generated by large language models (LLMs).
  The benchmark focuses on two critical dimensions: the quality and relevance of cited
  literature, and the faithfulness and veracity of the statements within the generated
  reports.'
---

# ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks

## Quick Facts
- **arXiv ID**: 2508.15804
- **Source URL**: https://arxiv.org/abs/2508.15804
- **Reference count**: 33
- **Key outcome**: Commercial Deep Research agents generate more comprehensive and reliable reports than base LLMs with search tools, but substantial room for improvement remains in research coverage and factual consistency

## Executive Summary
ReportBench is a benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). The benchmark focuses on two critical dimensions: the quality and relevance of cited literature, and the faithfulness and veracity of statements within generated reports. By leveraging high-quality published survey papers on arXiv as gold-standard references, ReportBench provides a systematic method for evaluating AI-generated research reports through automated analysis of citation quality and statement factuality. Empirical evaluations demonstrate that commercial Deep Research agents like OpenAI and Google consistently outperform standalone LLMs augmented with search tools, though both face challenges with citation hallucination and low recall of ground-truth references.

## Method Summary
ReportBench constructs evaluation tasks by reverse-engineering prompts from arXiv survey papers, using temporal constraints to prevent knowledge leakage. The benchmark extracts bibliographic references from peer-reviewed surveys as gold-standard reference sets, then evaluates generated reports by computing precision and recall of their cited references. An automated agent-based framework analyzes reports by extracting citations and statements, checking faithfulness of cited content against original sources through semantic matching, and validating non-cited claims using multi-model web-based voting mechanisms. The evaluation pipeline processes reference extraction, statement classification, and parallel verification tracks for cited (semantic consistency) and non-cited (web-voting) statements.

## Key Results
- Commercial Deep Research agents produce 88.2 cited statements on average compared to 16.16 for base LLMs with search tools
- OpenAI Deep Research achieves 0.385 precision and 0.033 recall versus Gemini's 0.145 precision and 0.036 recall on reference quality
- Cited statement match rates reach 83.7% for OpenAI versus 78.1% for Gemini, while non-cited statement factual accuracy is 81.1% versus 82.4%

## Why This Works (Mechanism)

### Mechanism 1
- Published survey papers serve as high-quality ground truth for evaluating research report quality through reference overlap comparison
- The benchmark extracts bibliographic references from peer-reviewed arXiv survey papers as gold-standard reference sets, evaluating generated reports by computing precision and recall of cited references
- Core assumption: Expert-authored, peer-reviewed surveys contain comprehensive and relevant reference lists representing ideal retrieval outcomes
- Evidence anchors: "ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references" [abstract]; "The resulting dataset constitutes a gold-standard benchmark for evaluating retrieval precision" [section 2.1.1]
- Break condition: If survey papers have incomplete or biased reference lists, ground-truth validity degrades

### Mechanism 2
- Decomposing statement verification into cited vs. non-cited tracks improves factual accuracy assessment
- Cited statements undergo three-stage validation: extraction, source retrieval via web scraping, and LLM-based semantic consistency verification; non-cited statements use multi-model voting where multiple web-connected LLMs independently verify claims
- Core assumption: Cited statements should be verifiable against explicit sources; non-cited statements require external corroboration
- Evidence anchors: "checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources" [abstract]; "For cited statements, we verify alignment with source documents via semantic matching" [section 2.2]
- Break condition: Multi-model voting fails when web-connected models share systematic biases

### Mechanism 3
- Reverse prompt engineering from survey papers creates domain-specific test prompts with temporal constraints preventing knowledge leakage
- Given a survey paper's publication date and full text, an LLM generates a query whose ideal answer is that paper; temporal cut-off dates ensure LLMs' retrieval windows match the survey's citation horizon
- Core assumption: Generated prompts accurately capture the scope, methodology, and temporal boundaries of the original survey
- Evidence anchors: "from which we apply reverse prompt engineering to derive domain-specific prompts" [abstract]; "we require each generated prompt to include a cut-off date corresponding to the most recent update of the paper" [section 2.1.2]
- Break condition: Models may ignore temporal constraints or generate prompts that don't fully capture survey scope

## Foundational Learning

- **Deep Research Agents vs. Base LLMs with Search Tools**: The benchmark explicitly compares commercial DRAs against standalone LLMs augmented with search tools, showing significant performance gaps in statement quality and citation alignment. Quick check: Can you explain why OpenAI Deep Research produces 88.2 cited statements on average while o3 produces only 16.16?

- **Citation Semantic Consistency**: The benchmark's core metric for cited statements measures whether statements are semantically supported by their cited sources, distinguishing between proper citation and "citation hallucination." Quick check: How would you distinguish between a statement that hallucinates content vs. one that hallucinates the citation itself?

- **Multi-Model Voting for Fact Verification**: Non-cited statements cannot be verified against explicit sources, so the benchmark uses multiple web-connected models to vote on factual correctness, reducing single-model bias. Quick check: What are the failure modes if all voting models share the same training data biases?

## Architecture Onboarding

- **Component map**: Dataset Construction Pipeline (Phase I: survey identification + reference extraction from LaTeX → Phase II: reverse prompt generation with temporal constraints → Phase III: domain classification + down-sampling to 100 tasks) → Evaluation Pipeline (Reference extraction from generated reports → Cited statement extraction → Non-cited statement extraction → Parallel verification tracks) → Supporting Infrastructure (gpt-4o for extraction/verification, gemini-2.5-pro + gemini-2.5-flash for web-based fact-checking)

- **Critical path**: 1) Prompt generation with correct temporal cut-off dates 2) Reference title extraction from generated reports (URL-based citation parsing) 3) Cited statement → source document → semantic consistency verification chain 4) Non-cited statement → multi-model web voting → aggregation

- **Design tradeoffs**: Precision vs. Recall (Gemini achieves higher recall but much lower precision than OpenAI); Volume vs. Accuracy (Gemini generates 3x more cited statements but with lower match rates); URL-based citations chosen for consistent chunked evaluation but increases text length

- **Failure signatures**: Statement hallucination (content deviates from cited source); Citation hallucination (fabricated URLs that resolve to nothing); Temporal constraint violation (models ignoring cut-off dates)

- **First 3 experiments**: 1) Run a single prompt through both OpenAI Deep Research and base o3 with search tools; compare cited statement counts and match rates 2) Validate multi-model voting robustness by checking agreement rates between gemini-2.5-pro and gemini-2.5-flash 3) Test temporal constraint adherence by manually auditing whether generated reports cite papers published after the specified cut-off date

## Open Questions the Paper Calls Out

- **How can the subjective dimension of "writing quality" be objectively integrated into automated DRA evaluation frameworks?** The authors explicitly state in the Introduction that they focus on content quality, "leaving the assessment of writing quality to future work." Writing style evaluation lacks consensus criteria and is difficult to automate with the same rigor as factual verification. A validated rubric or metric that correlates strongly with human expert rankings of report coherence and style would resolve this.

- **Does the performance of agents on ReportBench generalize to non-STEM domains where arXiv-style surveys are unavailable?** The Limitations section notes the dataset relies on arXiv, creating a "domain skew" that "may limit applicability to other research areas." The current benchmark is biased toward the specific structure and citation density of academic STEM literature. Comparative evaluation of agent performance on legal, financial, or humanities reports using domain-specific gold standards would resolve this.

- **Can Deep Research agents significantly improve recall (currently <4%) of ground-truth references without increasing hallucination rates?** The results show very low recall (0.033–0.036) despite high statement accuracy, and the Analysis section highlights "citation hallucination" as a persistent failure mode. It is unclear if current retrieval-augmented architectures can scale coverage to match human-level literature reviews without fabricating sources. Demonstration of an agent architecture achieving >20% recall while maintaining near-zero citation fabrication rate would resolve this.

## Limitations

- The benchmark relies on expert-authored survey papers that may contain incomplete reference lists or author bias, potentially limiting precision-recall metric validity
- The temporal cut-off mechanism is vulnerable to prompt hacking where models retrieve post-publication sources despite explicit constraints
- The benchmark's focus on STEM fields restricts generalizability to other academic domains where arXiv-style surveys are unavailable

## Confidence

- **High confidence**: The benchmark's two-track evaluation methodology (cited vs. non-cited statements) is methodologically sound and the comparative advantage of commercial Deep Research agents over base LLMs is well-supported by empirical data
- **Medium confidence**: The reverse prompt engineering approach produces valid test prompts, though the exact prompt formulation is unspecified and may affect temporal constraint adherence
- **Medium confidence**: The multi-model voting mechanism for non-cited statement verification reduces single-model bias, but all voting models share similar training data and web access patterns, limiting independence

## Next Checks

1. **Temporal Constraint Validation**: Audit a sample of generated reports to verify whether cited papers are published after the specified cut-off dates, quantifying the rate of temporal leakage

2. **Multi-Model Voting Independence**: Measure agreement rates between gemini-2.5-pro and gemini-2.5-flash on non-cited statements to assess whether shared biases affect factual verification accuracy

3. **Ground Truth Completeness**: Analyze a subset of survey papers to determine whether their reference lists are comprehensive and representative of the field, or whether they exhibit author bias toward self-citation