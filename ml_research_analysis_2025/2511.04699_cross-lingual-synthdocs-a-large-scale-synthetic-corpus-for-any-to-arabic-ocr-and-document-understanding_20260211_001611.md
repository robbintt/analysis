---
ver: rpa2
title: 'Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic
  OCR and Document Understanding'
arxiv_id: '2511.04699'
source_url: https://arxiv.org/abs/2511.04699
tags:
- arabic
- tables
- text
- synthetic
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-Lingual SynthDocs addresses the lack of large-scale Arabic
  resources for OCR and document understanding by creating a synthetic corpus of over
  2.5 million samples, including text, tables, and charts. The dataset leverages authentic
  backgrounds, bilingual layouts, and diacritic-aware fonts to simulate real Arabic
  document complexity.
---

# Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding

## Quick Facts
- arXiv ID: 2511.04699
- Source URL: https://arxiv.org/abs/2511.04699
- Authors: Haneen Al-Homoud; Asma Ibrahim; Murtadha Al-Jubran; Fahad Al-Otaibi; Yazeed Al-Harbi; Daulet Toibazar; Kesen Wang; Pedro J. Moreno
- Reference count: 27
- Primary result: Synthetic cross-lingual data improves Arabic OCR WER/CER and document parsing on multiple benchmarks.

## Executive Summary
Cross-Lingual SynthDocs addresses the scarcity of large-scale Arabic resources for OCR and document understanding by generating a synthetic corpus of over 2.5 million samples, including text, tables, and charts. The dataset leverages authentic backgrounds, bilingual layouts, and diacritic-aware fonts to simulate real Arabic document complexity. Generated data was used to fine-tune Qwen-2.5-VL models, achieving consistent improvements in OCR metrics like WER and CER across public Arabic benchmarks. Parsing performance also improved in TEDS for tables and CharTeX for charts. The results show that synthetic, visually realistic data effectively closes the resource gap and advances Arabic document analysis capabilities.

## Method Summary
The method involves translating English documents into Arabic while preserving layout and structure, then rendering with RTL-aware fonts and controlled variation in diacritics, numerals, and fonts. Text snippets are cropped and rendered with selective diacritization and diverse Arabic fonts. Full-page documents are generated by clustering English lines into semantic paragraphs via adjacency graphs, translating via LLM (Gemini 2.0 Flash) with symbol/abbreviation constraints, and re-rendering with RTL direction over original backgrounds. Tables and charts are synthesized with consistent and random styles, and combined into a large-scale corpus for fine-tuning Qwen-2.5-VL models.

## Key Results
- Fine-tuning Qwen-2.5-VL on SynthDocs yields consistent improvements in WER and CER across multiple public Arabic benchmarks.
- Table parsing performance (TEDS) improved from 81.65 to 83.68 on Qwen-2.5-VL-32B, with gains in multi-row headers and merged cells.
- Chart parsing (CharTeX) improved with randomized themes and backgrounds, demonstrating robustness to out-of-distribution conditions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic cross-lingual data generation can substitute for scarce human-annotated Arabic corpora when structural metadata is preserved.
- Mechanism: The pipeline takes existing English documents with OCR annotations, groups lines into semantic paragraphs via adjacency graphs, translates via LLM (Gemini 2.0 Flash) while constraining abbreviation/symbol preservation, then re-renders with RTL-aware Arabic fonts over original backgrounds—retaining layout while swapping language.
- Core assumption: Document structure and visual realism transfer across languages; translation quality is sufficient for OCR training signals.
- Evidence anchors:
  - [abstract] "Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in WER and CER... across multiple public Arabic benchmarks."
  - [section III-B] Paragraph grouping via adjacency graph uses baseline spacing and horizontal overlap thresholds; translation API constrained to preserve symbols and abbreviations.
  - [corpus] Neighboring work (QARI-OCR, Baseer) shows synthetic data and VLM fine-tuning improve Arabic OCR, supporting plausibility but not proving this specific pipeline.
- Break condition: If translation introduces semantic drift or if Arabic-specific ligature/diacritic patterns are under-represented, synthetic→real transfer degrades.

### Mechanism 2
- Claim: Diacritic-aware and font-diverse rendering improves robustness to typographic variation in Arabic OCR.
- Mechanism: Text snippets are rendered with selective diacritization (light/medium/heavy removal + random insertion), Eastern/Western numerals, and 15+ Arabic font families spanning calligraphic to monospaced styles—forcing the model to learn shape invariance.
- Core assumption: Explicit variation during training generalizes to unseen real-world fonts and diacritic patterns.
- Evidence anchors:
  - [section III-A] "selective removal of diacritics at light, medium, or heavy levels; random insertion of diacritics to increase variability."
  - [section IV-A1] Lists 15+ fonts including Amiri, Kacst variants, Cairo; 9 text colors randomized.
  - [corpus] KITAB-Bench paper (neighbor) identifies font/diacritic diversity as key Arabic OCR challenges, aligning with this design.
- Break condition: If synthetic font/numeral distributions diverge significantly from deployment documents, model may overfit to synthetic artifacts.

### Mechanism 3
- Claim: Augmenting with random-style tables and charts improves structural parsing under noise and OOD conditions.
- Mechanism: Tables include "consistent style" (professional, uniform) and "random style" (per-cell font/color variation, mixed languages, irregular alignment); charts randomize themes, backgrounds, text rotation—creating controlled OOD stress.
- Core assumption: Models trained on both clean and noisy structural examples generalize better to real-world heterogeneity.
- Evidence anchors:
  - [section III-C] Random style tables have "font family, size, and color differ from one cell to another... unpredictable elements such as headers, footers, captions, and merged cells."
  - [section IV-B3] Qwen-2.5-VL-32B TEDS improved from 81.65→83.68; "significant in the context of structural understanding" with multi-row headers and merged cells.
  - [corpus] Weak direct corpus validation for random-style augmentation specifically; neighboring papers don't isolate this mechanism.
- Break condition: If random style introduces unrealistic patterns absent in real documents, model may waste capacity or learn spurious correlations.

## Foundational Learning

- Concept: Right-to-Left (RTL) text rendering with bidirectional Unicode markers
  - Why needed here: Arabic script requires RTL rendering; improper BiDi handling causes glyph reordering and illegible output.
  - Quick check question: Given an Arabic string with embedded English numerals, can you predict the visual order without rendering?

- Concept: Tree-Edit Distance Similarity (TEDS) for table structure evaluation
  - Why needed here: Tables are parsed into HTML; TEDS measures structural + content similarity via edit distance on tree representations.
  - Quick check question: If a model merges two correct cells but preserves all text content, would TEDS penalize it?

- Concept: Diacritization levels in Arabic (tashkeel)
  - Why needed here: Arabic text may include optional diacritics (harakat); OCR must handle both fully vocalized and unvocalized inputs.
  - Quick check question: Does removing diacritics change the word's core spelling, or only its phonetic specificity?

## Architecture Onboarding

- Component map: Text Snippet Generator -> Document Synthesizer -> Table Generator -> Chart Generator -> Training Pipeline
- Critical path: IDL document -> adjacency graph -> translation -> RTL rendering -> VLM fine-tuning -> benchmark evaluation (WER/CER/TEDS/CharTeX)
- Design tradeoffs:
  - Synthetic realism vs. scale: Authentic backgrounds and fonts improve realism but increase generation cost vs. purely synthetic layouts.
  - Consistent vs. random style tables: Consistent aids clean parsing; random improves OOD robustness but may introduce noise.
  - LLM translation vs. human annotation: Faster, cheaper, but risks semantic drift; symbol/abbreviation constraints mitigate but don't eliminate.
- Failure signatures:
  - High CER on diacritic-heavy real documents -> synthetic diacritic coverage insufficient
  - TEDS degradation on merged-cell tables -> random-style tables under-represent complex hierarchies
  - English WER increases post-fine-tuning -> catastrophic forgetting (paper claims parity, but monitor closely)
- First 3 experiments:
  1. Ablate diacritic variation levels (none vs. light/medium/heavy) on SynthesizedAr benchmark; expect CER sensitivity.
  2. Train with consistent-style tables only vs. mixed styles; measure TEDS gap on KITAB-Bench tables with merged cells.
  3. Compare Gemini translation vs. alternative (e.g., NLLB) on paragraph grouping fidelity; spot-check semantic preservation in technical documents.

## Open Questions the Paper Calls Out

- Extending to forms and infographics to evaluate whether the synthesis pipeline generalizes to other document structures.
- Integrating real-world distortions to improve robustness on noisy benchmarks beyond current authentic background use.
- Investigating whether smaller open-source translation models can produce comparable synthetic data quality to Gemini 2.0 Flash.

## Limitations

- Translation quality and semantic drift risks from using LLM (Gemini 2.0 Flash) without human validation or quantitative fidelity checks.
- Fine-tuning hyperparameters are unspecified (learning rate, batch size, epochs), limiting reproducibility and optimal performance claims.
- Random-style table augmentation lacks direct corpus-level validation; effectiveness on complex merged-cell hierarchies untested.

## Confidence

- High confidence: WER/CER improvements on public Arabic OCR benchmarks (KITAB-Bench) post-fine-tuning, supported by clear before/after numbers and neighbor work on synthetic data benefits.
- Medium confidence: Structural parsing gains (TEDS/CharTeX), given reasonable methodology but weaker direct validation of random-style augmentation effectiveness.
- Low confidence: Translation pipeline fidelity and semantic preservation—critical for cross-lingual transfer but unvalidated beyond qualitative constraints.

## Next Checks

1. Ablate diacritization levels (none vs. light/medium/heavy) on SynthesizedAr benchmark; measure CER sensitivity to identify optimal synthetic variation.
2. Compare Gemini translation vs. alternative (e.g., NLLB) on paragraph grouping fidelity; spot-check semantic preservation in technical documents.
3. Train with consistent-style tables only vs. mixed styles; measure TEDS gap on KITAB-Bench tables with merged cells to isolate random-style benefit.