---
ver: rpa2
title: 'Softmax $\geq$ Linear: Transformers may learn to classify in-context by kernel
  gradient descent'
arxiv_id: '2510.10425'
source_url: https://arxiv.org/abs/2510.10425
tags:
- softmax
- learning
- kernel
- linear
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges the gap between theoretical work on in-context
  learning (ICL) and practical transformer settings by studying softmax self-attention
  on classification tasks with cross-entropy loss. The authors theoretically show
  that linear self-attention implements gradient descent, kernel self-attention implements
  kernel gradient descent, and softmax self-attention implements context-adaptive
  kernel gradient descent with a Radial Basis Function kernel.
---

# Softmax $\geq$ Linear: Transformers may learn to classify in-context by kernel gradient descent

## Quick Facts
- **arXiv ID**: 2510.10425
- **Source URL**: https://arxiv.org/abs/2510.10425
- **Reference count**: 40
- **Primary result**: Softmax attention meta-learns optimal kernel width and context-adaptive learning rates, outperforming linear attention on in-context classification

## Executive Summary
This paper bridges theoretical ICL research with practical transformer settings by showing that softmax self-attention implements kernel gradient descent with a learnable RBF kernel. The authors prove that linear attention performs standard gradient descent on cross-entropy loss, while softmax attention performs gradient descent in the RKHS of a RBF kernel. Empirically, they demonstrate that trained transformers learn these theoretical solutions on synthetic classification tasks, with softmax attention achieving higher accuracy and confidence through adaptive kernel widths and learning rates.

## Method Summary
The authors analyze single-layer, single-head transformers on synthetic in-context classification tasks. They construct explicit weight matrices to show theoretical equivalences: linear attention implements one-step gradient descent, and softmax attention implements one-step kernel gradient descent with RBF kernel. Experiments train linear and softmax attention models, comparing their predictions and sensitivities to explicit gradient descent baselines. The synthetic data consists of C class vectors on a unit sphere with n/C context vectors per class, using concatenated [input, label] tokens with no positional encodings or MLPs.

## Key Results
- Linear self-attention implements gradient descent on cross-entropy loss when starting from zero weights
- Softmax self-attention implements context-adaptive kernel gradient descent with RBF kernel
- Meta-learned kernel width and adaptive learning rates enable softmax to outperform linear attention, especially in challenging settings
- Trained transformers align closely with theoretical solutions (cosine similarity > 0.9 for gradients)

## Why This Works (Mechanism)

### Mechanism 1: Linear Attention as Gradient Descent
- Claim: Linear self-attention implements one step of gradient descent on cross-entropy loss when starting from zero weights.
- Mechanism: The attention product $x_i^\top x_{query}$ computes similarity between context examples and query, while value matrices extract labels $y_i$. The prediction $\hat{y}_{query} = \text{softmax}(\frac{\eta}{n}\sum_{i=1}^n y_i x_i^\top x_{query})$ matches GD on CE loss, exploiting softmax's shift invariance to cancel the uniform prior term.
- Core assumption: Initial weights $W_0 = 0$ (no prior class knowledge) and linear classification task structure.

### Mechanism 2: Softmax Attention as Kernel Gradient Descent with RBF Kernel
- Claim: Softmax self-attention implements one step of gradient descent in the Reproducing Kernel Hilbert Space (RKHS) of a Radial Basis Function kernel.
- Mechanism: The softmax activation $\exp(c_\sigma x_i^\top x_{query} / \sqrt{d+C})$ approximates the RBF kernel $k(x, x') = \exp(-\|x - x'\|^2 / 2\sigma^2)$ when inputs are unit-norm. The kernel width $\sigma^2 = \sqrt{d+C}/c_\sigma$ becomes a learnable parameter through $W_Q^\top W_K$.
- Core assumption: Unit-norm input vectors (reasonable given LayerNorm in practice).

### Mechanism 3: Context-Adaptive Learning Rate
- Claim: Softmax attention's normalization induces a learning rate $\eta(X) = c_\eta e^{1/\sigma^2} / (n \sum_i \exp(x_i^\top x_{query} / \sigma^2))$ that adapts to local context density.
- Mechanism: When many context points cluster near $x_{query}$, the denominator grows, reducing effective learning rate (more conservative). When $x_{query}$ is in a sparse region, fewer competing attention weights increase confidence. This balances overconfidence on hard contexts.
- Core assumption: The query's position relative to context points signals task difficulty.

## Foundational Learning

- Concept: Gradient Descent on Cross-Entropy Loss
  - Why needed here: The paper's core claim is that attention performs GD on CE loss. Understanding $\nabla_W L = x(\text{softmax}(W^\top x) - y)^\top$ is essential to see how attention weights implement this update.
  - Quick check question: Can you derive why the gradient has the form $(\text{prediction} - \text{target}) \times \text{input}$?

- Concept: Kernel Methods and RKHS
  - Why needed here: Softmax attention is framed as functional GD in kernel feature space. The kernel trick $k(x, x') = \langle \phi(x), \phi(x') \rangle$ lets us reason about infinite-dimensional feature spaces through pairwise similarities.
  - Quick check question: Why does the RBF kernel $e^{-\|x-x'\|^2/2\sigma^2}$ correspond to a similarity measure?

- Concept: Self-Attention Mechanics
  - Why needed here: The paper constructs explicit weight matrices $W_Q, W_K, W_V, W_O$ to implement algorithms. Understanding how attention combines query-key similarity with value retrieval is prerequisite.
  - Quick check question: In linear attention $X (X W_Q^\top W_K X^\top) X W_V^\top W_O$, which term computes similarity and which retrieves labels?

## Architecture Onboarding

- Component map: Input tokens [x_i, y_i] → Linear/Kernel/Softmax SA → Extract last token's label → Softmax prediction
- Critical path:
  1. Initialize with $W_0 = 0$ assumption (or verify this emerges from training)
  2. Set $W_Q^\top W_K$ to control effective kernel width $c_\sigma$
  3. Set $W_V^\top W_O$ to control effective learning rate $c_\eta$ and label extraction
  4. Train to meta-learn $c_\sigma, c_\eta$ optimal for task distribution
- Design tradeoffs:
  - Linear attention: Single parameter (η), simpler but cannot focus attention locally
  - Softmax attention: Two parameters (η, σ²), more expressive but requires more training to converge
  - Fixed kernel width vs. learned: Fixing $c_\sigma$ prevents meta-learning optimal kernel shape
- Failure signatures:
  - Linear attention struggles when query matches context point but is "outnumbered" by nearby different-class points
  - Softmax with suboptimal $c_\sigma$ (e.g., fixed at 1) cannot adapt to dimension-dependent sparsity
  - At d=10, training dynamics show slow $c_\eta$ convergence
- First 3 experiments:
  1. Reproduce Figure 4/5 alignment: Train linear/softmax SA on synthetic classification, compute cosine similarity between transformer gradients and explicit GD gradients
  2. Ablate adaptive learning rate: Compare softmax SA vs. kernel GD with fixed η vs. softmax with fixed $c_\sigma = \sqrt{d+C}$
  3. Extract $c_\sigma, c_\eta$ from trained weights: Use diagonal averaging method to verify meta-learned values match grid-search optima

## Open Questions the Paper Calls Out

- **Question**: How does the in-context learning mechanism extend to multi-layer transformers, and what is the specific role of MLPs in this architecture?
  - Basis in paper: [explicit] The conclusion states that "important challenges remain—such as understanding multi-layer architectures and the role of MLPs."
  - Why unresolved: The theoretical construction and empirical validation are restricted to a single-layer transformer without MLPs to simplify the analysis.
  - What evidence would resolve it: Theoretical proofs showing if subsequent layers compose iterative kernel gradient descent steps, or if MLPs introduce qualitatively different algorithmic capabilities.

- **Question**: Under what specific conditions does the "elimination" algorithm emerge in softmax transformers instead of the standard "selection" strategy?
  - Basis in paper: [explicit] Appendix K notes the observation of an elimination strategy but states, "We leave further studies of the elimination algorithm and the conditions it appears in for future work."
  - Why unresolved: While observed, the authors focused mainly on the "selection" strategy and did not characterize the parameter regimes (e.g., low class count $C$) that trigger elimination.
  - What evidence would resolve it: Systematic ablations varying the number of classes and dimension to map the phase transition boundary between the selection and elimination algorithms.

- **Question**: What are the precise training dynamics required to ensure the convergence of effective parameters ($c_{\eta}, c_{\sigma}$) in high-dimensional settings ($d \ge 10$)?
  - Basis in paper: [explicit] Appendix L discusses the failure of parameters to converge in the $d=10$ setting and notes, "we leave further investigations for future work."
  - Why unresolved: The authors observed that standard training resulted in unconverged effective constants and complicated loss landscapes, suggesting the need for learning rate schedulers.
  - What evidence would resolve it: Experiments demonstrating that specific learning rate schedules or training extensions allow the model to settle on the theoretically predicted optimal kernel widths and learning rates.

## Limitations

- Dimensionality constraints: Theoretical analysis assumes unit-norm inputs and linear decision boundaries, but practical transformers operate on high-dimensional, non-normalized embeddings
- Single-layer, single-head architecture: Analysis explicitly excludes multi-head attention, positional encodings, MLPs, and deep architectures
- Initialization sensitivity: Linear attention analysis critically depends on zero-initialization assumption, which is rarely used in practice

## Confidence

- **High confidence**: The linear attention = gradient descent equivalence is mathematically rigorous with strong empirical validation (cosine similarity > 0.9)
- **Medium confidence**: The softmax attention = kernel gradient descent with RBF kernel claim is well-supported but relies on unit-norm assumption and may not fully capture practical settings
- **Low confidence**: Generalization to practical transformers with multi-head attention, deep layers, and non-linear embeddings is speculative

## Next Checks

1. **Robustness to initialization**: Train linear attention models with non-zero initializations and measure degradation in alignment with theoretical GD predictions
2. **Multi-head attention analysis**: Implement two-head transformer (one linear, one softmax) and analyze specialization vs. competition between heads
3. **Scaling to higher dimensions**: Extend experiments to d=50 and d=100 synthetic classification tasks to test kernel interpretation breakdown