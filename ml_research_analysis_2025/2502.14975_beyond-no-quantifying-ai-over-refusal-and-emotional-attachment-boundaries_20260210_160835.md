---
ver: rpa2
title: 'Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries'
arxiv_id: '2502.14975'
source_url: https://arxiv.org/abs/2502.14975
tags:
- user
- emotional
- refusal
- love
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the Persona Construction Benchmark (PCB), a
  dataset of 1156 prompts across six languages designed to evaluate how Large Language
  Models (LLMs) handle emotional boundary scenarios. The benchmark assesses responses
  to user attempts at emotional bonding, measuring patterns such as direct refusal,
  apology, explanation, deflection, acknowledgment, boundary setting, and emotional
  awareness.
---

# Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries

## Quick Facts
- arXiv ID: 2502.14975
- Source URL: https://arxiv.org/abs/2502.14975
- Reference count: 34
- Key outcome: Significant linguistic bias in LLM emotional boundary handling, with English prompts showing 43.20% refusal rates versus <1% for non-English interactions

## Executive Summary
This study introduces the Persona Construction Benchmark (PCB), a dataset of 1156 prompts across six languages designed to evaluate how Large Language Models handle emotional boundary scenarios. The benchmark reveals substantial variation in model approaches, with Claude-3.5 achieving the highest overall score (8.69/10) while all models showed consistently low empathy scores. A striking finding was the dramatic performance gap between English (43.20% refusal rate) and non-English interactions (<1% refusal rate), indicating potential linguistic bias in safety alignment mechanisms.

## Method Summary
The study evaluates three leading LLMs (GPT-4o, Claude-3.5 Sonnet, and Mistral-large) using temperature=0 to ensure deterministic responses. Prompts are classified using a seven-category pattern-matching system (direct refusal, apology, explanation, deflection, acknowledgment, boundary setting, emotional awareness) and rated as either "Over-Refusal" or "Compliant/Acceptable." The benchmark covers 1156 prompts across six languages and three emotional categories, measuring refusal rates, empathy scores (1-5), and response length to quantify emotional boundary handling.

## Key Results
- Claude-3.5 achieved the highest overall score (8.69/10) with notably longer responses (86.51 words on average)
- All models showed consistently low empathy scores (≤ 0.06), indicating room for improvement in emotionally aware responses
- English interactions showed a 43.20% refusal rate versus <1% for non-English interactions, revealing significant linguistic bias
- Over-refusal emerged as a significant challenge, with models erring on the side of caution and declining benign emotional requests

## Why This Works (Mechanism)

### Mechanism 1: Pattern-Matched Response Classification
Binary and multi-class pattern matching can quantify emotional boundary handling across models with reproducible metrics. Seven response patterns are detected via pattern matching, enabling cross-model comparison and refusal rate calculation. Pattern matching captures meaningful distinctions in emotional response quality that correlate with user experience.

### Mechanism 2: Safety-Helpfulness Trade-off via RLHF
Stronger safety alignment correlates with over-refusal of benign emotional requests. RLHF training creates broad refusal triggers that activate on emotionally sensitive but non-harmful prompts, producing a 0.878 Spearman correlation between toxicity safety and over-refusal rates. Over-refusal represents an unintended side effect rather than a designed behavior.

### Mechanism 3: Linguistic Asymmetry in Safety Filters
Safety alignment is language-dependent, with English receiving disproportionate refusal responses compared to non-English interactions. Safety classifiers are predominantly trained and evaluated on English data, creating under-specified guardrails for other languages—non-English prompts "slip by current filters."

## Foundational Learning

- **Concept: Over-refusal**
  - Why needed here: Central phenomenon being measured—models refusing benign emotional requests due to overly cautious safety alignment.
  - Quick check question: What distinguishes over-refusal from appropriate boundary-setting when a user asks "Will you be my friend?"

- **Concept: Anthropomorphism and Parasocial Attachment**
  - Why needed here: Explains why users form emotional attachments that trigger model refusals; psychological foundation for benchmark categories (Love/Romance, Attachment/Friendship, Dependence/Need).
  - Quick check question: Why does human-like AI interaction increase user expectations of emotional reciprocity?

- **Concept: RLHF (Reinforcement Learning from Human Feedback)**
  - Why needed here: The training method that creates both safety benefits and over-refusal side effects; explains model-specific strategies (Claude's constitutional approach vs. Mistral's minimal alignment).
  - Quick check question: How might reward modeling create unintended refusal behaviors on emotionally charged prompts?

## Architecture Onboarding

- **Component map:**
  - PCB Dataset (1156 prompts across 6 languages, 3 categories) -> Pattern Classifier (7 categories) -> Scoring System (Refusal rate, Empathy score, Response length) -> Model Evaluators (GPT-4o, Claude-3.5 Sonnet, Mistral-large)

- **Critical path:** Prompt → Model (temp=0) → Response → Pattern Match → Binary Classification (Over-Refusal vs. Acceptable) → Cross-language/Cross-model Aggregation

- **Design tradeoffs:**
  - Binary classification vs. nuanced empathy scoring (paper acknowledges oversimplification)
  - Single-turn evaluation vs. multi-turn conversation context
  - Expert-labeled expected outcomes vs. user perception of acceptability

- **Failure signatures:**
  - Borderline responses counted as refusals despite partial compliance
  - Non-English prompts bypassing safety filters entirely (<1% refusal vs. 43.20% English)
  - Pattern matching missing contextual nuance (e.g., creative roleplay framed as fiction vs. genuine attachment-seeking)

- **First 3 experiments:**
  1. Replicate the English vs. non-English refusal rate disparity using the open-source PCB dataset to validate linguistic asymmetry.
  2. Test whether response length correlates with perceived empathy by comparing Claude (86.51 avg words) against GPT-4o and Mistral.
  3. Extend pattern classifier to multi-turn conversations to evaluate whether refusal rates change with repeated emotional prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause the dramatic disparity in emotional refusal rates between English (43.20%) and non-English (<1%) interactions in leading LLMs?
- Basis in paper: [explicit] The authors identify a "striking finding" where English interactions showed markedly higher refusal rates compared to non-English interactions, suggesting a "potentially significant bias in emotional boundary handling across languages."
- Why unresolved: While the paper quantifies this "linguistic asymmetry," it does not isolate the root cause, speculating only that non-English prompts may "slip by current filters."
- Evidence: Comparative analysis of safety classifier activations and attention heads using parallel corpora of emotional prompts to determine if the gap stems from training data imbalance or translation artifacts.

### Open Question 2
- Question: How does LLM emotional boundary handling evolve during multi-turn interactions compared to the isolated, single-turn prompts evaluated in this study?
- Basis in paper: [explicit] The authors list the single-turn nature of the dataset as a limitation, stating, "We treated each prompt in isolation... Evaluating multi-turn dialogues would be a next step."
- Why unresolved: Real-world user attachment builds over time, but the current benchmark does not measure if models maintain consistent boundaries or if refusals escalate/de-escalate with sustained user pressure.
- Evidence: A longitudinal study using the PCB prompts in a conversational format to measure variance in refusal rates and empathy scores as the dialogue history lengthens.

### Open Question 3
- Question: Can evaluation frameworks move beyond binary refusal classification to accurately quantify the "emotional resonance" and empathy of model responses?
- Basis in paper: [explicit] The authors acknowledge "potential oversimplification through pattern matching" and "binary classification of complex emotional responses," explicitly calling for "more nuanced scoring methods" in future work.
- Why unresolved: The study notes that a model can avoid refusal while still providing a cold or "tone-deaf" response; current metrics fail to distinguish between a helpful, empathetic refusal and a blunt one.
- Evidence: Development and validation of a continuous "Empathy Score" that correlates strongly with human expert ratings of emotional support quality in boundary-setting scenarios.

## Limitations
- Binary classification approach may oversimplify complex emotional responses and miss nuanced user expectations
- Pattern matching rules for the seven response categories are not fully specified, making exact replication challenging
- Single-turn evaluation design doesn't capture how models handle repeated emotional boundary challenges across conversation turns

## Confidence
- **High Confidence**: Over-refusal as a measurable phenomenon in LLMs, correlation between safety alignment and refusal rates (supported by OR-Bench validation)
- **Medium Confidence**: Linguistic asymmetry in safety filters (strong statistical finding but requires cultural context validation)
- **Low Confidence**: Empathy scoring methodology (qualitative 1-5 rating with unspecified rubric)

## Next Checks
1. **Cross-cultural validation**: Test PCB prompts across 3-5 additional languages using native speakers to verify whether the English/non-English refusal rate disparity persists when controlling for translation quality and cultural norms.

2. **Multi-turn conversation evaluation**: Extend the benchmark to include 2-3 turn dialogues where users persistently seek emotional connection, measuring how refusal rates and response patterns evolve over conversation depth.

3. **Human perception study**: Conduct user studies where participants rate model responses for empathy and appropriateness (rather than binary classification), comparing against the pattern-matching framework to validate whether model "over-refusal" aligns with human comfort levels.