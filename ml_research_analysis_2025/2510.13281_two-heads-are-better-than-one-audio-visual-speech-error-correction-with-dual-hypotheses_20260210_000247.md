---
ver: rpa2
title: 'Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual
  Hypotheses'
arxiv_id: '2510.13281'
source_url: https://arxiv.org/abs/2510.13281
tags:
- dualhyp
- hypotheses
- speech
- audio
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DualHyp, a generative error correction (GER)
  framework for audio-visual speech recognition (AVSR) that leverages modality-specific
  hypotheses from separate ASR and VSR models. Instead of fusing audio-visual features
  early, DualHyp uses an LLM to compose text-only hypotheses from independent ASR
  and VSR streams, avoiding cross-modal interference.
---

# Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses

## Quick Facts
- arXiv ID: 2510.13281
- Source URL: https://arxiv.org/abs/2510.13281
- Reference count: 33
- Primary result: DualHyp + RelPrompt achieves up to 57.7% relative WER reduction over standard ASR baselines across various noise conditions

## Executive Summary
This paper introduces DualHyp, a generative error correction framework for audio-visual speech recognition that processes dual hypotheses from separate ASR and VSR models using an LLM. Rather than early fusion of audio-visual features, DualHyp composes text-only hypotheses from independent streams, avoiding cross-modal interference. The framework incorporates RelPrompt, a noise-aware guidance mechanism that provides reliability tokens indicating temporal signal quality for each modality. Experiments on LRS2 and LRS3 benchmarks demonstrate significant WER improvements over single-stream approaches, with the method scaling to larger LLMs and showing multilingual capability.

## Method Summary
DualHyp processes dual hypotheses from separate ASR and VSR models, using an LLM to compose text-only hypotheses from independent streams. The framework leverages RelPrompt, which provides noise-aware reliability tokens (Clean, Noisy, Mixed) for 0.4-second segments based on temporal signal quality. These tokens guide the LLM in weighting trust between modalities during composition. The approach uses LoRA fine-tuning (rank=16) with 5 epochs, batch size 32, and learning rates of 1e-4 for the LLM and 2e-4 for predictors. Reliability predictors use 1D-CNNs on encoder features to output segment-level masks, while the LLM processes formatted strings containing N-best lists and reliability masks.

## Key Results
- Achieves up to 57.7% relative WER reduction over standard ASR baselines on LRS2 under audio-visual corruption
- Outperforms single-stream GER approaches by maintaining high WERR even at negative SNR levels where audio-only methods fail
- Demonstrates scalability with LLM size, showing monotonic improvement from TinyLlama to Llama-3.2-3B
- Shows multilingual capability, though performance degrades when VSR head quality is significantly lower than ASR

## Why This Works (Mechanism)

### Mechanism 1: Language-Space Compositional Reasoning
Delaying modality fusion until the language generation stage prevents cross-modal contamination and allows the LLM to reconstruct accurate transcriptions from complementary fragments. Unlike early-fusion methods that combine audio-video features in embedding space, DualHyp processes ASR and VSR streams independently. The LLM receives distinct textual hypotheses and uses its context window to either select the dominant stream or weave correct fragments from both into a single output.

### Mechanism 2: Reliability-Guided Arbitration (RelPrompt)
Providing explicit temporal reliability tokens enables the LLM to dynamically weight its trust between modalities based on estimated signal quality. Lightweight 1D-CNN predictors analyze encoder features to output discrete tokens (Clean, Noisy, Mixed) for 0.4-second segments. These tokens are appended to the prompt, allowing the LLM to learn the correlation between a mask label and hypothesis accuracy.

### Mechanism 3: Asynchronous Robustness
Visual signals provide a fallback invariant to acoustic noise, breaking the performance ceiling of audio-only GER. Since visual corruption is often uncorrelated with audio corruption, the system maintains a high probability of retaining the correct words in at least one of the two N-best lists.

## Foundational Learning

- **N-best Hypotheses Generation**: DualHyp requires a ranked list of candidates from upstream ASR and VSR models. How does increasing N-best list size affect the trade-off between recall and LLM processing latency?

- **Cross-Modal Contamination**: The paper frames its contribution against early-fusion methods where noisy audio features degrade the video representation. Why would adding noisy audio embeddings to a clean visual representation result in worse performance than using the visual representation alone?

- **In-Context Learning (ICL)**: The LLM is not trained to transcribe speech directly from audio/video features but to "read" the provided text hypotheses and masks to make a decision. Does the model require gradient updates to understand the reliability tokens, or can it interpret them zero-shot?

## Architecture Onboarding

- **Component map**: Upstream Heads (Whisper-large-v3 ASR + BRAVEn-large VSR) -> Reliability Predictors (1D-CNNs on encoder features) -> Fusion Module (LLM processing formatted string)
- **Critical path**: The construction of the prompt. You must align the temporal reliability masks with the text hypotheses. Misalignment here will confuse the LLM.
- **Design tradeoffs**: Modularity vs. Latency (using separate models allows easy swapping but introduces sequential latency unsuitable for real-time streaming); VSR Quality (system works because VSR errors are different from ASR errors, not fewer).
- **Failure signatures**: Semantic Hallucination (LLM generates plausible but incorrect text based on internal priors); Over-correction (LLM ignores correct hypothesis in favor of fluent but incorrect one).
- **First 3 experiments**: 1) Oracle Upper Bound: Combine best fragments from ASR+VSR hypotheses to verify compositional oracle WER matches claim (~4.5%). 2) RelPrompt Ablation: Compare DualHyp with/without reliability mask tokens on "Clean Audio / Noisy Video" split. 3) Corruption Generalization: Test on corruption type not seen during training to ensure reliability predictors generalize.

## Open Questions the Paper Calls Out

### Open Question 1
How can the computational latency introduced by the sequential LLM correction step be minimized to enable real-time deployment on resource-constrained edge devices? The Limitations section states that multiple modules introduce computational latency and that the sequential LLM correction step creates an unavoidable bottleneck, making deployment on edge devices a significant hurdle.

### Open Question 2
How can the framework be modified to control LLM hallucination and semantic association errors when input hypotheses from both modalities are highly ambiguous or erroneous? Appendix C.2 identifies "Hallucination and Semantic Association Errors" as a failure mode where the LLM generates plausible but incorrect terms based on internal knowledge rather than evidence.

### Open Question 3
Can the DualHyp framework maintain robustness when the performance disparity between the ASR and VSR upstream models is extremely large (e.g., >50% WER gap)? Appendix D.3 shows that for languages where the VSR head is significantly worse than the ASR head, the framework struggles to improve performance or even degrades.

### Open Question 4
How can the framework be adapted for low-resource languages where high-quality pre-trained VSR models are unavailable or severely underperform? The Limitations section states that the framework's dependency on upstream SR quality "currently restricts the framework's applicability beyond English" due to the lack of high-quality multilingual VSR models.

## Limitations
- Computational latency introduced by sequential LLM correction step makes real-time deployment challenging
- Dependency on upstream SR quality restricts applicability beyond English to languages with high-quality VSR models
- Potential for LLM hallucination when both input hypotheses are highly ambiguous or erroneous

## Confidence

**High Confidence**:
- DualHyp architecture reduces WER compared to single-stream GER baselines
- RelPrompt improves performance over DualHyp without reliability guidance
- The framework scales with LLM size

**Medium Confidence**:
- Cross-modal contamination is effectively avoided through language-space fusion
- Visual modality serves as reliable fallback in severe audio corruption
- Reliability predictors maintain >90% precision across corruption types

**Low Confidence**:
- Real-time applicability (computational latency not measured)
- Multilingual generalization beyond English LRS datasets
- Performance on languages with different phonetic-visual correlation structures

## Next Checks

1. **Predictor Robustness Test**: Evaluate RelPrompt predictors on a held-out corruption type not seen during training to measure precision/recall degradation and quantify generalization limits.

2. **Streaming Feasibility Analysis**: Profile end-to-end latency of DualHyp pipeline (ASR + VSR + LLM processing) on a single sample and compare against real-time constraints to assess practical deployment barriers.

3. **Multimodal Failure Mode Characterization**: Create test cases where both ASR and VSR produce plausible but contradictory hypotheses to analyze if the LLM can correctly arbitrate or defaults to one modality's preference.