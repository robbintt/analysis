---
ver: rpa2
title: 'Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic
  Planning'
arxiv_id: '2509.13351'
source_url: https://arxiv.org/abs/2509.13351
tags:
- clear
- planning
- reasoning
- plan
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PDDL-INSTRUCT, a novel instruction tuning
  framework that enhances Large Language Models' symbolic planning capabilities through
  logical chain-of-thought reasoning. The approach teaches models to rigorously reason
  about action applicability, state transitions, and plan validity using explicit
  logical inference steps and external verification feedback.
---

# Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning

## Quick Facts
- arXiv ID: 2509.13351
- Source URL: https://arxiv.org/abs/2509.13351
- Reference count: 40
- Primary result: PDDL-INSTRUCT achieves up to 94% planning accuracy, representing a 66% absolute improvement over baselines

## Executive Summary
This paper introduces PDDL-INSTRUCT, a novel instruction tuning framework that enhances Large Language Models' symbolic planning capabilities through logical chain-of-thought reasoning. The approach teaches models to rigorously reason about action applicability, state transitions, and plan validity using explicit logical inference steps and external verification feedback. By decomposing the planning process into verifiable reasoning chains and incorporating detailed feedback from a plan validator, the framework enables LLMs to self-correct their planning processes. Experimental results show that PDDL-INSTRUCT achieves planning accuracy of up to 94% on standard benchmarks, representing a 66% absolute improvement over baseline models. The approach generalizes across domains and demonstrates that explicit logical reasoning, combined with external validation, significantly improves LLM planning capabilities compared to traditional instruction tuning or chain-of-thought methods alone.

## Method Summary
PDDL-INSTRUCT is a two-phase instruction tuning framework for symbolic planning. Phase 1 performs basic instruction tuning on domain/problem/plan pairs. Phase 2 implements a chain-of-thought tuning loop where the model generates reasoning chains about action validity, validates them with VAL, and receives detailed feedback about precondition violations, effect application errors, and goal achievement failures. The framework uses a two-stage optimization process: Stage 1 optimizes reasoning chain generation, while Stage 2 optimizes final plan generation, with separate learning rates to preserve reasoning capabilities.

## Key Results
- Achieves 94% planning accuracy on Blocksworld, 89% on Mystery Blocksworld, and 57% on Logistics benchmarks
- Represents a 66% absolute improvement over baseline instruction-tuned models
- Detailed feedback outperforms binary feedback by 5-15 percentage points across domains
- Two-stage optimization shows complementary strengths (Phase 1: 78%/32%/23% vs Phase 2 alone: 72%/17%/45%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing planning verification into explicit atomic reasoning steps enables LLMs to systematically track state transitions.
- Mechanism: The framework teaches models to generate structured chains of ⟨state_i-1, action_i, state_i⟩ triplets, forcing explicit precondition checking, effect application, and state tracking at each step rather than jumping directly to action sequences.
- Core assumption: LLMs can learn to faithfully execute step-by-step logical inference when trained on structured reasoning examples with ground-truth state labels.
- Evidence anchors:
  - [abstract] "decomposing the planning process into verifiable reasoning chains about precondition satisfaction, effect application, and invariant preservation"
  - [Section 5.1] "explicitly instruct it to analyze why each action in a plan is valid by explaining precondition satisfaction and effect application"
  - [corpus] VeriCoT paper validates that neuro-symbolic consistency checks improve reasoning faithfulness
- Break condition: If the generated reasoning chains become unfaithful (plausible-sounding but logically inconsistent), the mechanism fails. The paper acknowledges this risk but mitigates via external validation.

### Mechanism 2
- Claim: External validator feedback provides grounded supervision that enables iterative self-correction.
- Mechanism: VAL validator produces deterministic, formally correct feedback about which preconditions failed or effects were misapplied. This feedback is incorporated into training via the feedback loss L_feedback, penalizing specific error types (α_precond=1.0, α_effect=1.0, α_goal=1.5).
- Core assumption: Detailed feedback about *why* actions fail provides stronger learning signal than binary valid/invalid labels.
- Evidence anchors:
  - [Section 5.1] "detailed feedback... provides specific reasoning about each action generated by VAL in terms of which preconditions failed or which effects were incorrectly applied"
  - [Table 1] Detailed feedback with η=15 achieves 5-15 percentage point improvements over binary feedback across domains
  - [corpus] Weak corpus evidence—neighbor papers on feedback-driven reasoning exist but don't directly validate this specific feedback mechanism
- Break condition: If VAL's feedback doesn't correlate with actual reasoning errors the model can learn from, or if the model overfits to specific feedback patterns, performance plateaus. The paper limits iterations (η) to prevent this.

### Mechanism 3
- Claim: Two-stage optimization separates reasoning skill acquisition from end-task performance.
- Mechanism: Stage 1 optimizes L_reasoning on individual state-action-state triplets with higher learning rate (δ_1=1e-5). Stage 2 then optimizes L_final on complete plans with lower learning rate (δ_2=5e-6), preserving reasoning capabilities while improving plan generation.
- Core assumption: Reasoning and planning are partially separable skills that benefit from ordered training.
- Evidence anchors:
  - [Section 5.2] "Stage 1 develops the logical foundation needed for planning, while Stage 2 ensures these capabilities are properly applied"
  - [Table 4] Phase 1 alone achieves 78%/32%/23% vs Phase 2 alone at 72%/17%/45%, suggesting complementary strengths
  - [corpus] No direct corpus validation of two-stage optimization for planning
- Break condition: If Stage 2 optimization disrupts Stage 1 reasoning capabilities (catastrophic forgetting), the separation provides no benefit.

## Foundational Learning

- Concept: **STRIPS Planning Representation**
  - Why needed here: The entire framework operates on PDDL domains with precondition/add/delete action semantics. Without understanding that states are sets of fluents and actions transform states via set operations, the reasoning chains are meaningless.
  - Quick check question: Given action (pick-up ?x) with preconditions (clear ?x) ∧ (ontable ?x) ∧ (handempty) and effects (holding ?x) plus deletions, what state results from applying it to {(ontable a), (clear a), (handempty)}?

- Concept: **Chain-of-Thought Reasoning**
  - Why needed here: Phase 2 training requires generating intermediate reasoning steps that are externally verifiable. Understanding that CoT produces explicit reasoning chains z_1, z_2, ..., z_K where each step transforms the previous state is essential.
  - Quick check question: What distinguishes "logical coherence" from "progressive refinement" in CoT, and which does PDDL-INSTRUCT guarantee?

- Concept: **Instruction Tuning Objective**
  - Why needed here: The framework minimizes expected loss between model predictions and target responses across instruction datasets. Understanding the basic fine-tuning paradigm (Equation 1) is prerequisite to understanding the two-stage optimization.
  - Quick check question: How does the instruction tuning objective in Equation 1 differ from the reasoning chain loss L_reasoning in Equation 4?

## Architecture Onboarding

- Component map:
  [PDDL Domain/Problem] → [Phase 1 Tuning: Basic Instruction] 
                              ↓
                         [Phase 2 CoT Generator] → [VAL Validator] 
                              ↑                           ↓
                         [Model Update] ← [Detailed Feedback]
                              ↓
                         [Final Plan Output]

- Critical path:
  1. Start with pre-trained LLM (Llama-3-8B or GPT-4)
  2. Phase 1: Train on D_1 (domain, problem, plan + explanation pairs) for 5 epochs with lr=2e-5
  3. Phase 2: For η iterations, generate CoT plans, validate with VAL, construct D_reasoning and D_final datasets, run two-stage optimization
  4. Evaluate on unseen D_test problems

- Design tradeoffs:
  - **Binary vs. detailed feedback**: Detailed feedback (+5-15% accuracy) requires more compute for VAL integration and longer training sequences
  - **Iteration limit η**: Higher η (15 vs. 10) improves accuracy (~3-4%) but increases training time linearly; paper notes diminishing returns expected
  - **Learning rate ordering**: Stage 1 uses higher lr (1e-5) than Stage 2 (5e-6) to prioritize reasoning foundation; reversing may cause instability
  - **Satisficing vs. optimal**: Framework generates valid (not optimal) plans, trading completeness for tractability

- Failure signatures:
  - **Mystery Blocksworld underperformance** (64% vs. 94%): Semantic obfuscation degrades pattern matching—model may be memorizing domain-specific vocabulary
  - **Precondition violations persisting**: Detailed feedback loop not converging; check if λ_feedback (0.1) is too low relative to state distance loss
  - **Effect application errors in Logistics**: Complex state interactions with multiple objects exceed training distribution
  - **Phase 2-only underperformance** (17% vs. 78% in Mystery BW): Missing foundational planning knowledge from Phase 1

- First 3 experiments:
  1. **Ablate feedback type**: Run Phase 2 with binary vs. detailed feedback on single domain (Blocksworld), measuring both final accuracy and convergence rate across η ∈ {5, 10, 15}. Expect detailed feedback to show steeper early improvement.
  2. **Single-stage vs. two-stage optimization**: Compare full PDDL-INSTRUCT against merging Stage 1 and Stage 2 into single optimization with combined loss. Monitor whether reasoning quality degrades in merged approach.
  3. **Cross-domain transfer**: Train on Blocksworld only, evaluate zero-shot on Logistics. This tests whether logical reasoning skills transfer or if domain-specific patterns dominate learning.

## Open Questions the Paper Calls Out

- **Question**: Can the framework be extended to generate optimal plans rather than just satisficing ones?
  - Basis in paper: [explicit] The authors state "Advancing to Optimal Planning" is a key future direction to guide models toward minimal resource usage.
  - Why unresolved: The current two-stage optimization process and feedback mechanisms target logical validity (satisficing) rather than minimizing plan length or cost.
  - What evidence would resolve it: Experiments comparing the model's average plan length against ground-truth optimal solutions generated by classical planners.

- **Question**: Does the logical reasoning chain maintain performance when applied to advanced PDDL features like conditional effects or temporal constraints?
  - Basis in paper: [explicit] The authors note a limitation that they currently "limit to use only a subset of PDDL features" to simplify the reasoning effort.
  - Why unresolved: The reasoning decomposition assumes simple STRIPS-like structures, which may fail to capture the complex dependencies introduced by conditional or durative actions.
  - What evidence would resolve it: Evaluation results on planning domains extended to include derived predicates and temporal constraints.

- **Question**: Can the model internalize the external validator's logic to perform accurate self-correction without ground-truth feedback?
  - Basis in paper: [explicit] The authors identify "Self-Verification Capabilities" as a future direction to reduce dependence on external verifiers like VAL.
  - Why unresolved: The method relies entirely on external signals for convergence; current LLMs generally lack the ability to reliably self-critique reasoning errors without this grounding.
  - What evidence would resolve it: Ablation studies measuring the performance drop when the external verification feedback loop is removed during training or inference.

## Limitations

- The framework's performance varies significantly across domains (94% vs. 57%), suggesting domain-specific limitations that aren't fully characterized
- The approach relies heavily on VAL validator for feedback, creating a critical dependency without extensive validation of VAL's own accuracy
- Mystery Blocksworld (semantic obfuscation) significantly underperforms regular Blocksworld (64% vs. 94%), raising questions about generalizability to real-world planning tasks

## Confidence

- **High Confidence**: The two-stage optimization approach and general framework design are well-specified and technically sound. The 66% absolute improvement over baselines is clearly demonstrated through controlled experiments.
- **Medium Confidence**: The specific mechanism claims about why detailed feedback and logical reasoning chains improve performance have theoretical justification but limited direct empirical validation. The ablation studies show correlation but don't definitively prove causation for individual mechanisms.
- **Low Confidence**: Claims about generalizability to real-world planning domains and the robustness of the approach to semantic obfuscation are not empirically validated beyond the benchmark domains tested.

## Next Checks

1. **Validator Independence Test**: Run the framework with a different validator (e.g., Fast Downward) to assess whether VAL-specific feedback patterns are driving performance, or if the general feedback mechanism is what matters.

2. **Zero-Shot Domain Transfer**: Evaluate the model trained on Blocksworld/Livestock on completely novel planning domains (e.g., Gripper, Transport) to test whether logical reasoning skills transfer or if the model is overfitting to domain-specific patterns.

3. **Error Type Analysis**: Systematically categorize and analyze the 6-43% of failed plans to determine if specific error types (precondition violations vs. effect application vs. goal achievement) are more prevalent in certain domains, which would inform targeted improvements.