---
ver: rpa2
title: 'YOLOA: Real-Time Affordance Detection via LLM Adapter'
arxiv_id: '2512.03418'
source_url: https://arxiv.org/abs/2512.03418
tags:
- affordance
- object
- detection
- adapter
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: YOLOA addresses the challenge of integrating object detection and
  affordance learning for embodied AI systems by jointly estimating "what", "where",
  and "how" of objects. The proposed approach employs a dual-branch architecture with
  a large language model (LLM) adapter that enables bidirectional interaction between
  object detection and affordance learning branches.
---

# YOLOA: Real-Time Affordance Detection via LLM Adapter

## Quick Facts
- arXiv ID: 2512.03418
- Source URL: https://arxiv.org/abs/2512.03418
- Reference count: 40
- Primary result: YOLOA achieves 52.8/73.1 mAP on relabeled ADG-Det/IIT-Heat benchmarks while maintaining real-time efficiency (up to 89.77 FPS, and 846.24 FPS for lightweight variant).

## Executive Summary
YOLOA introduces a novel dual-branch architecture that jointly estimates object categories, bounding boxes, and affordance heatmaps in real-time for embodied AI systems. The key innovation is an LLM adapter that enables bidirectional interaction between object detection and affordance learning branches through semantic refinement. This approach outperforms existing methods on relabeled ADG-Det and IIT-Heat benchmarks while maintaining computational efficiency, with a lightweight variant that removes the LLM at inference while retaining accuracy.

## Method Summary
YOLOA builds on YOLOv11 architecture with a dual-branch design: one branch for object detection and another for affordance heatmap prediction. The LLM adapter serves as a bridge between these branches, generating semantic refinements (class priors, box offsets, and affordance gates) that improve both detection and affordance estimation. During training, the adapter learns to align visual features with language understanding through LoRA fine-tuning, while at inference, a lightweight variant operates without the LLM by leveraging the learned visual-linguistic alignment.

## Key Results
- State-of-the-art performance: 52.8 mAP on ADG-Det and 73.1 mAP on IIT-Heat
- Real-time efficiency: Up to 89.77 FPS for full model, 846.24 FPS for YOLOA-light variant
- Bidirectional refinement: LLM adapter improves both object detection (by 3.2 mAP) and affordance learning (by reducing KLD by 0.264)
- Spatially-constrained affordance: Affordance gates confine heatmaps to object interiors, preventing cross-object contamination

## Why This Works (Mechanism)

### Mechanism 1: LLM-Mediated Bidirectional Cross-Branch Refinement
The LLM adapter creates a closed feedback loop where object detection and affordance learning mutually enhance each other through language-grounded semantic reasoning. Preliminary predictions are encoded as visual-textual embeddings and processed by a frozen LLM with LoRA fine-tuning, generating refinements that modulate both branches via residual updates.

### Mechanism 2: Spatially-Gated Affordance Confinement
Affordance gates derived from detected bounding boxes constrain affordance heatmaps to object interiors, suppressing spurious activations outside functional regions. This prevents cross-object contamination and ensures precise localization of functional areas.

### Mechanism 3: Train-Time Semantic Distillation for Inference-Time Efficiency
The LLM adapter transfers semantic reasoning capability into the dual-branch backbone during training, enabling its removal at inference without substantial accuracy loss. This allows YOLOA-light to maintain semantic consistency while achieving extreme speed.

## Foundational Learning

- **One-Stage Object Detection (YOLO paradigm):** Understanding single-shot detection, anchor-free heads, and multi-scale feature pyramids is prerequisite to comprehending the dual-branch extension. *Quick check:* Can you explain how YOLO predicts bounding boxes and class probabilities simultaneously from grid cells?

- **Affordance Representation (Heatmap vs. Segmentation):** The paper reformulates affordance as heatmaps rather than dense segmentation masks. *Quick check:* What is the difference between predicting an affordance segmentation mask and an affordance heatmap, and why might heatmaps be preferable for real-time performance?

- **Vision-Language Alignment via Frozen LLM + Adapter:** The LLM adapter uses a frozen backbone with LoRA fine-tuning to bridge visual and textual modalities. *Quick check:* Why freeze the LLM backbone and only train adapter layers, rather than fine-tuning the entire model?

## Architecture Onboarding

- **Component map:**
  Input Image → Darknet Backbone → Multi-scale Features → Dual Branches (Object Detection + Affordance Learning) → Top-k Selection → LLM Adapter → Residual Refinement → Final Predictions

- **Critical path:**
  1. Dual-branch preliminary prediction—ensure F_det and F_aff produce stable initial outputs
  2. Top-k selection—k=5 for ADG-Det, k=6 for IIT-Heat; excessive k introduces noise
  3. LLM Adapter training—start with ground-truth labels during warm-up, then switch to predictions
  4. Refinement weight tuning—critical hyperparameters: α=β=0.01, γ=0.001

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Full YOLOA (with LLM at inference) | Maximum accuracy (52.8/73.1 mAP) | Slower (73.76/89.77 FPS) |
  | YOLOA-light (LLM removed) | Extreme speed (470–846 FPS) | ~1 mAP accuracy drop |
  | Larger LLM (LLaMA-3 8B vs. Phi-4-Mini 3.8B) | Better semantic refinement | More GPU memory during training |

- **Failure signatures:**
  - Missing detections → Malformed affordance masks: When F_det fails, affordance gates lack spatial constraints
  - Noisy top-k predictions → Degraded adapter reasoning: Low-confidence proposals introduce semantic noise
  - High KLD with good mAP: Object detection succeeds but affordance localization is imprecise

- **First 3 experiments:**
  1. Baseline sanity check: Train YOLOA without LLM Adapter on ADG-Det to establish dual-branch baseline (~49.6 mAP)
  2. Adapter component ablation: Enable one refinement at a time to verify independent contributions
  3. Top-k sensitivity analysis: Sweep k from 1 to 10 on validation set to identify optimal value

## Open Questions the Paper Calls Out

### Open Question 1
How can the affordance learning branch maintain robustness in cluttered scenes where the object detection branch fails to localize target instances? The current architecture relies on a closed-loop interaction where affordance gates depend on detected bounding boxes; if the detector fails, the refinement loop breaks.

### Open Question 2
Does the LLM Adapter enable zero-shot generalization to novel object categories or affordances not present in the training data? The method leverages a frozen LLM known for common-sense understanding, yet experiments are strictly confined to the 50 classes of ADG-Det and 10 classes of IIT-Heat.

### Open Question 3
Does the conversion of segmentation masks to heatmaps (IIT-Heat) result in a loss of boundary precision required for fine-grained robotic manipulation? While the paper shows superior heatmap metrics, it does not quantify if this provides sufficient geometric fidelity for actual physical tasks.

## Limitations

- **Underspecified implementation details:** Exact LoRA configuration, visual projection layer dimensions, and adapter head architectures are not fully disclosed.
- **Relabeling protocol ambiguity:** The ADG-Det relabeling protocol for 2,000 images is not described, raising questions about annotation consistency.
- **Warm-up strategy gaps:** The ground-truth warm-up strategy lacks detailed scheduling information for when predictions replace ground truth.

## Confidence

- **High Confidence:** Dual-branch architecture, LLM adapter for bidirectional refinement, and FPS measurements are well-supported.
- **Medium Confidence:** Training procedure details (learning rates, warm-up, loss weights) appear complete but may require tuning.
- **Low Confidence:** Exact LLM adapter implementation details and ADG-Det relabeling methodology are insufficiently specified.

## Next Checks

1. **Component ablation verification:** Train YOLOA without LLM adapter on ADG-Det to establish baseline performance and confirm reported improvements.
2. **Top-k sensitivity validation:** Systematically sweep the top-k parameter from 1 to 10 on validation data to identify optimal value.
3. **Affordance mask containment test:** Verify that affordance heatmaps remain spatially confined within object bounding boxes by measuring mask-object IoU across validation set.