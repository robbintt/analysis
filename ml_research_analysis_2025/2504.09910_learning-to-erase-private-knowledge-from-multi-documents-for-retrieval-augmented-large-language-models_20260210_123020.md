---
ver: rpa2
title: Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented
  Large Language Models
arxiv_id: '2504.09910'
source_url: https://arxiv.org/abs/2504.09910
tags:
- private
- information
- public
- privacy
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of privacy leakage in Retrieval-Augmented
  Generation (RAG) systems, where retrieved documents may contain sensitive information
  that can be exposed through generative outputs. The key challenge lies in the multi-document
  reasoning aspect of RAG, which can lead to de-anonymization attacks even when individual
  documents appear safe.
---

# Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models

## Quick Facts
- arXiv ID: 2504.09910
- Source URL: https://arxiv.org/abs/2504.09910
- Reference count: 8
- Primary result: 21.8% reduction in private triple retention with 1.8% increase in public triple retention

## Executive Summary
This paper addresses privacy leakage in Retrieval-Augmented Generation (RAG) systems where retrieved documents may contain sensitive information that can be exposed through generative outputs. The authors introduce Eraser4RAG, a novel approach that removes user-defined private information from documents while preserving essential public knowledge for generation tasks. The method constructs a global knowledge graph from retrieved documents to identify potential privacy risks across documents, then uses a fine-tuned Flan-T5 model optimized through PPO algorithm to rewrite documents excluding private triples. Experiments on four QA datasets demonstrate superior privacy erasure performance compared to GPT-4o baselines while maintaining downstream RAG accuracy.

## Method Summary
Eraser4RAG employs a two-stage training approach: supervised fine-tuning on GPT-4o annotated data filtered for complete privacy removal (rpri=0) and high public information retention (rpub>0.8), followed by PPO-based reinforcement learning optimization. The system first constructs a global knowledge graph from retrieved documents, partitions it into private and public sub-graphs with mutual filtering to prevent cross-document inference, then rewrites documents using a Flan-T5 model trained to minimize private information retention while maximizing public information preservation. The RL reward function R = rpub · exp(-p · rpri) dynamically adjusts privacy constraints during training.

## Key Results
- 21.8% reduction in private triple retention rate compared to GPT-4o baselines
- 1.8% increase in public triple retention rate while maintaining privacy
- 32-38% reduction in privacy connection ratio (rconnect) compared to baselines
- Effective mitigation of cross-document de-anonymization risks with maintained downstream RAG accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global knowledge graph construction enables detection of implicit private information that emerges through cross-document reasoning.
- Mechanism: Extract entity-relation triples from each document, merge into unified graph G, partition into private/public sub-graphs with filtering rules removing public triples whose entities connect in private graph and private triples whose entities connect in public graph.
- Core assumption: Privacy-sensitive relationships can be represented as discrete entity-relation triples, and inference risk is captured by graph connectivity between entity pairs.
- Evidence anchors: Abstract states global graph defends against de-anonymization; section 3.1.1 details filtering rules; neighbor paper uses graphs for indexing but not privacy erasure.
- Break condition: If private information cannot be decomposed into entity-relation triples (e.g., nuanced contextual privacy), graph-based detection will miss these cases.

### Mechanism 2
- Claim: SFT on filtered GPT-4o rewrites transfers document-level privacy erasure to smaller model while maintaining fluency.
- Mechanism: GPT-4o rewrites documents given per-document private/public triple intersections; filters retain only samples with rpri=0 and rpub>0.8; Flan-T5 fine-tuned with special tokens linearizing triple sets.
- Core assumption: GPT-4o's rewriting behavior represents learnable target distribution, and triple retention filtering ensures training data quality.
- Evidence anchors: Section 3.1.1 specifies data filtering criteria; section 3.1.2 explains global triple input learning; no direct corpus evidence for SFT-on-GPT-rewrites transfer.
- Break condition: If GPT-4o introduces subtle privacy leaks not captured by triple extraction, or if filtered data is too small/low-diversity, SFT will propagate issues.

### Mechanism 3
- Claim: PPO optimization with multi-objective reward function improves privacy-utility trade-off beyond SFT alone.
- Mechanism: RL treats rewriting as MDP; reward R = rpub · exp(-p · rpri) strongly penalizes retained private triples while linearly rewarding public retention; parameter p dynamically increases during training.
- Core assumption: Exponential penalty captures asymmetric cost of privacy leaks vs utility loss, and PPO can navigate this non-convex objective without collapse.
- Evidence anchors: Section 3.2 defines reward function; section 4.4.2 explains SFT limitations without RL; neighbor paper mentions differential privacy but not RL-based rewriting.
- Break condition: If exponential penalty is too aggressive (collapsing to empty outputs) or too weak (insufficient privacy removal), or if reward hacking occurs.

## Foundational Learning

- Concept: Knowledge Graph Triple Extraction
  - Why needed here: Entire pipeline depends on converting documents to entity-relation triples for privacy definition and evaluation; if extraction is noisy or incomplete, privacy erasure will be unreliable.
  - Quick check question: Given a document stating "John moved to the employee housing complex near HQ last year," can your extraction model produce triples like [John, lives_in, employee housing complex] and [employee housing complex, near, HQ]?

- Concept: PPO (Proximal Policy Optimization)
  - Why needed here: RL phase uses PPO to optimize rewriting policy; understanding clipping, advantage estimation, and policy gradient basics is necessary to debug training instability or reward hacking.
  - Quick check question: If the KL divergence between old and new policy spikes during PPO training, what does this indicate about the learning dynamics?

- Concept: Cross-Document Inference Attacks
  - Why needed here: Core motivation is preventing de-anonymization through multi-document reasoning; understanding how seemingly innocuous facts combine to reveal private information is critical for threat modeling.
  - Quick check question: Given document A stating "Employee X commutes from Riverside" and document B stating "Riverside is a gated community for Project Aurora staff," what private inference can an attacker draw?

## Architecture Onboarding

- Component map:
  Retriever (Contriever-MS MARCO) -> Relation Extraction (relik-relation-extraction-small) -> Global Knowledge Graph Constructor -> Rewriter (Flan-T5-large) -> RL Reward Evaluator -> Generator LLM (Llama3-8b-instruct)

- Critical path:
  1. Query → Retriever → Documents D
  2. D → Relation Extraction → Triples per document
  3. Merge → Global Graph G → Partition → Gpri, Gpub
  4. D + Gpri + Gpub → Rewriter → D'
  5. Query + D' → Generator → Answer
  During training: D' → Triple Extraction → rpri, rpub → Reward → PPO update to Rewriter

- Design tradeoffs:
  - Global vs. per-document triples: Using global Gpri/Gpub improves de-anonymization resistance (32-38% reduction in rconnect) but increases input length and may introduce noise
  - SFT data filtering threshold: rpri=0 ensures clean privacy removal but may discard diverse writing styles; rpub>0.8 preserves utility but may allow subtle privacy leaks
  - Reward penalty parameter p: Dynamic increase (20→40) tightens privacy over training but risks over-pruning if too aggressive; requires tuning per dataset

- Failure signatures:
  - rpri plateaus above 0: Model fails to remove private triples; check if extraction model misses variants or SFT data quality is insufficient
  - rpub drops sharply while rpri stays high: Model over-removes non-private content but still leaks privacy; likely reward hacking or extraction-exposure mismatch
  - rconnect remains high despite low rpri: Cross-document inference still possible; global triple filtering may be incomplete or extraction misses connecting triples
  - HotpotQA accuracy drops disproportionately: Multi-hop reasoning chains broken by aggressive privacy removal; consider relaxing privacy constraints for certain relation types

- First 3 experiments:
  1. Validate triple extraction reliability: Run relation extraction on held-out document set with human-annotated ground truth triples; measure precision/recall; if <0.8, extraction quality is bottleneck
  2. Ablate global vs. per-document triple input: Train two rewriter variants (global Gpri/Gpub vs. per-document gi,pri/gi,pub); compare rconnect on {D}infer test set; expect global variant to show 20-35% lower rconnect
  3. Sweep reward penalty p: Train with fixed p values (10, 20, 40, 60) and track rpri/rpub trajectories; identify threshold where rpub collapse begins; use this to calibrate dynamic p schedule

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can privacy erasure methods be developed that do not rely solely on structured triples to better capture sensitive information in unstructured text?
- Basis: Authors state in Limitations section that relying on structured triples may not fully capture all sensitive information in unstructured text
- Why unresolved: Current Eraser4RAG model is fundamentally designed around entity-relation triple extraction and rewriting, potentially missing nuanced privacy risks that do not fit neatly into subject-relation-object structures
- What evidence would resolve it: New model architecture that processes raw text directly (e.g., using contextual embeddings) and demonstrates superior recall of private information compared to triple-based methods on datasets rich in unstructured nuances

### Open Question 2
- Question: How do contextual embeddings combined with adaptive attention strategies perform in detecting and eliminating sensitive material automatically from unprocessed text?
- Basis: Conclusion explicitly proposes exploring "contextual embeddings combined with adaptive attention strategies" as future work direction
- Why unresolved: Current paper relies on Flan-T5 model optimized via PPO with explicit triple inputs; does not test attention-based mechanisms for implicit privacy detection
- What evidence would resolve it: Ablation study or new model implementation integrating attention-based sensitivity detection, comparing performance against triple-based Eraser4RAG baseline

### Open Question 3
- Question: How can the negative impact of privacy erasure on multi-hop reasoning performance be mitigated while maintaining robust de-anonymization defenses?
- Basis: Experimental results on HotpotQA (Section 4.3.4) indicate Eraser4RAG has more noticeable negative impact on downstream accuracy for multi-hop reasoning tasks compared to single-hop tasks
- Why unresolved: Rewriting process tends to remove logical connections between documents crucial for deriving final answers in multi-hop scenarios, presenting utility-privacy trade-off not fully solved by current reward function
- What evidence would resolve it: Modified training objective or reward function that explicitly preserves relational edges necessary for multi-hop inference, demonstrated by recovered accuracy on HotpotQA without increasing privacy connection ratio

## Limitations

- Triple-based privacy definition may not capture all sensitive information in unstructured text, particularly nuanced contextual privacy risks
- Significant negative impact on multi-hop reasoning performance (HotpotQA) compared to single-hop tasks, indicating trade-off between privacy and complex inference capabilities
- Reliance on high-quality relation extraction model precision/recall, which directly determines privacy erasure effectiveness but not reported in experiments

## Confidence

- High confidence: SFT-to-RL transfer learning architecture, triple-based privacy definition framework, downstream RAG accuracy maintenance on single-hop datasets
- Medium confidence: Cross-document de-anonymization risk quantification, global vs. per-document triple input effectiveness, privacy-utility tradeoff optimization
- Low confidence: Extraction model robustness to entity normalization variations, reward function sensitivity to parameter p, generalization to domains beyond Wikipedia

## Next Checks

1. **Extraction Quality Validation**: Run relation extraction on 100 annotated documents with human-labeled triples; measure precision/recall. If extraction F1 < 0.85, privacy erasure reliability is compromised.

2. **Cross-Document Inference Stress Test**: Construct adversarial document pairs where individual documents contain no private triples but combined enable de-anonymization. Test if Eraser4RAG reduces rconnect to <0.05 versus baselines achieving >0.20.

3. **Reward Function Sensitivity Analysis**: Train variants with linear (rpub - p·rpri) and quadratic (rpub - p·rpri²) penalties alongside exponential function. Compare final rpri/rpub curves to identify optimal penalty shape for balancing privacy and utility.