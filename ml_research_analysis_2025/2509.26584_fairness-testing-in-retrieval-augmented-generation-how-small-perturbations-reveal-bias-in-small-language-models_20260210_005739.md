---
ver: rpa2
title: 'Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations
  Reveal Bias in Small Language Models'
arxiv_id: '2509.26584'
source_url: https://arxiv.org/abs/2509.26584
tags:
- fairness
- testing
- bias
- demographic
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a fairness testing methodology for Retrieval-Augmented
  Generation (RAG) systems using metamorphic testing with demographic perturbations.
  The approach evaluates three Small Language Models (SLMs) integrated into RAG pipelines,
  introducing 21 demographic variations to assess fairness in sentiment analysis.
---

# Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models

## Quick Facts
- arXiv ID: 2509.26584
- Source URL: https://arxiv.org/abs/2509.26584
- Authors: Matheus Vinicius da Silva de Oliveira; Jonathan de Andrade Silva; Awdren de Lima Fontao
- Reference count: 33
- Primary result: Up to one third of metamorphic relations fail under demographic perturbations in RAG systems

## Executive Summary
This study introduces a fairness testing methodology for Retrieval-Augmented Generation (RAG) systems using metamorphic testing with demographic perturbations. The approach evaluates three Small Language Models (SLMs) integrated into RAG pipelines, introducing 21 demographic variations to assess fairness in sentiment analysis. Results show that the retrieval component itself introduces significant bias, with racial cues being the predominant cause of violations. The findings demonstrate that fairness testing must extend beyond final outputs to include component-level evaluation of retrieval systems.

## Method Summary
The methodology employs metamorphic testing principles by creating perturbed inputs that should maintain semantic equivalence while varying demographic attributes. The study tests three SLMs integrated into RAG pipelines using 21 demographic variations including race, gender, and other attributes. The Retriever Robustness Score (RRS) quantifies retrieval instability, and fairness is assessed by comparing model outputs across perturbed versions of the same query. Sentiment analysis serves as the evaluation task, with metamorphic relations defined to check for consistent sentiment classification despite demographic variations in the input.

## Key Results
- Up to one third of metamorphic relations fail under demographic perturbations
- Racial cues are the predominant cause of fairness violations
- Retrieval component itself introduces significant bias beyond the language model outputs

## Why This Works (Mechanism)
The mechanism works by exploiting the semantic equivalence principle in metamorphic testing - when demographic attributes are varied but the core meaning remains constant, outputs should remain consistent. By systematically perturbing inputs with demographic cues, the methodology reveals where RAG systems break this equivalence, exposing bias in both retrieval and generation components. The approach effectively creates a stress test that highlights how small changes in demographic context can trigger disproportionate changes in system behavior.

## Foundational Learning

**Metamorphic Testing**: Testing technique that verifies system properties by checking relationships between multiple inputs and outputs rather than expected outputs directly. Needed to assess fairness without requiring ground truth for every demographic variation. Quick check: Verify that perturbations maintain semantic equivalence.

**Retrieval-Augmented Generation (RAG)**: System architecture combining information retrieval with language model generation. Needed to understand how bias can propagate through both retrieval and generation stages. Quick check: Map data flow from query through retrieval to final output.

**Small Language Models (SLMs)**: Compact language models designed for efficiency over absolute performance. Needed to understand the specific limitations and biases of resource-constrained models in RAG systems. Quick check: Compare SLM performance against larger models on same fairness metrics.

## Architecture Onboarding

**Component Map**: User Query -> Demographic Perturbation Engine -> Retriever -> Retrieved Documents -> Small Language Model -> Generated Output

**Critical Path**: Perturbation -> Retrieval -> Generation. Bias can enter at any stage, with retrieval often being the primary source of fairness violations.

**Design Tradeoffs**: Testing comprehensiveness versus computational cost - 21 perturbations per query provides good coverage but increases testing overhead. The choice of sentiment analysis as evaluation task balances simplicity with meaningful fairness assessment.

**Failure Signatures**: Inconsistent sentiment classification across demographic variants, disproportionate retrieval relevance scores for different demographic groups, and systematic preference for certain demographic attributes in generated responses.

**First Experiments**:
1. Run baseline sentiment classification on unperturbed queries to establish reference outputs
2. Apply demographic perturbations and verify semantic equivalence preservation
3. Measure RRS scores to quantify retrieval instability before examining generation outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on 21 demographic perturbations and three specific SLMs may limit generalizability
- Focus on sentiment analysis may not capture full complexity of bias in other RAG applications
- Perturbation methodology assumes realistic user behavior that may not reflect actual usage patterns

## Confidence
**High**: Core finding that retrieval components introduce significant bias is consistently observed across models and perturbation types

**Medium**: Specific failure rates and metamorphic relation violations may vary in production environments

**Low**: Universal applicability of RRS metric without further validation across diverse RAG architectures

## Next Checks
1. Replicate with larger and more diverse demographic perturbations including intersectional variations
2. Test methodology across different RAG applications beyond sentiment analysis
3. Conduct longitudinal study to assess temporal stability of identified bias patterns