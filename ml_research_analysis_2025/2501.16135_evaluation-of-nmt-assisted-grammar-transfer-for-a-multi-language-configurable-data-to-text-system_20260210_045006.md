---
ver: rpa2
title: Evaluation of NMT-Assisted Grammar Transfer for a Multi-Language Configurable
  Data-to-Text System
arxiv_id: '2501.16135'
source_url: https://arxiv.org/abs/2501.16135
tags:
- language
- grammar
- text
- system
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a rule-based data-to-text generation system
  enhanced with Neural Machine Translation (NMT) and grammar transfer for multilingual
  applications. The system translates grammatical configurations from a source language
  into target languages, enabling scalable multilingual text generation without human
  intervention for each text.
---

# Evaluation of NMT-Assisted Grammar Transfer for a Multi-Language Configurable Data-to-Text System

## Quick Facts
- arXiv ID: 2501.16135
- Source URL: https://arxiv.org/abs/2501.16135
- Reference count: 14
- Key outcome: 82% grammar unit transfer accuracy with NMT-assisted translation and spaCy dependency parsing

## Executive Summary
This study presents a rule-based data-to-text generation system enhanced with Neural Machine Translation (NMT) and grammar transfer for multilingual applications. The system translates grammatical configurations from a source language into target languages, enabling scalable multilingual text generation without human intervention for each text. A grammar dependency model facilitates cross-language grammar unit transfer using spaCy dependency parsing. Evaluation on the SportSett:Basketball dataset using post-editing by 13 translators showed that 82% of grammar units were correctly translated, with most errors requiring minor adjustments like case changes or compound noun head marking. German exhibited an 8% error rate for case changes, primarily from genitive to nominative. The system demonstrated grammatical correctness and efficiency, though further improvements in automatic transfer are suggested. Future work includes expanding participant numbers and testing grammar-rich source languages.

## Method Summary
The system uses a pipeline approach where document planning and microplanning occur in the source language, with grammar features specified in grammar units. These units are then translated via NMT, parsed with spaCy dependency parsing, and features are extracted and transferred to target languages. The surface realizer generates final text using these transferred configurations. Evaluation involved post-editing by 13 translators who identified errors and adjusted grammar unit settings, with changes tracked and categorized.

## Key Results
- 82% of grammar units were correctly translated across target languages
- German showed 8% error rate for case changes, primarily genitive-to-nominative for team names ending in 's'
- 19% average change rate across languages, with most errors requiring minor adjustments
- Most common German errors: case changes (8%), compound noun head marking (5%), determiner/preposition insertion (4%)

## Why This Works (Mechanism)

### Mechanism 1: NMT-Assisted Grammar Configuration Transfer
Translating grammatical configurations upstream, rather than translating generated output text, enables scalable multilingual NLG while preserving configurability and reducing hallucination risk. The system uses NMT to translate grammar units (containers holding settings like case, number, tense, gender) from source to target language. These translated configurations then feed the surface realizer, which generates text from source data without requiring human intervention per output.

### Mechanism 2: Dependency-Parse-Driven Feature Extraction
SpaCy dependency parsing with custom post-processing enables automatic extraction and transfer of linguistically-relevant features from translated grammar units. After NMT translates a grammar unit's text snippet, spaCy performs language-specific dependency parsing. Custom aggregations then extract features per part-of-speech (nouns: lemma, case, number, gender, preposition, adjectives, numerals, conjunctions, determiners; verbs: lemma, number, tense, person, gender; pronouns: lemma, case, number, gender, preposition, pronoun type).

### Mechanism 3: Post-Editing as Grammar Transfer Evaluation
Tracking human post-edit actions on automatically-transferred grammar units provides fine-grained diagnostic signal about specific transfer failures. Translators review source-target statement pairs, adjust grammar unit settings (not rendered text), and every edit is tagged with change categories. This isolates which grammatical features the automatic transfer mishandles.

## Foundational Learning

- **Surface Realization in Pipeline NLG**: Why needed here: The system's grammar units feed a surface realizer; understanding that realization is the final, automatic stage that converts abstract grammatical specifications to actual text clarifies why upstream grammar transfer matters. Quick check question: Can you explain why surface realization must be "completely automatic" in this architecture, and what would break if it required human input?

- **Dependency Parsing for Feature Extraction**: Why needed here: The entire grammar transfer mechanism depends on spaCy correctly identifying syntactic heads, dependencies, and morphological features in translated phrases. Quick check question: Given the German case error (genitive→nominative for team names ending in 's'), what dependency parse output would cause this misclassification?

- **Post-Editing vs. Output Quality Evaluation**: Why needed here: This study evaluates the *transfer mechanism* (grammar unit settings) via post-editing, not the final text quality directly; conflating these leads to misinterpreting results. Quick check question: If a translator adds grammar units that weren't necessary for grammatical correctness (as participant 9 did), does this indicate a system failure? Why or why not?

## Architecture Onboarding

- **Component map**: Document Planning → Microplanning → Grammar Unit Container → NMT Translation Layer → SpaCy Dependency Parser → Custom Post-Processor → Target Grammar Unit → Surface Realizer → Output Text

- **Critical path**: Source grammar unit → NMT translation → spaCy parse → feature extraction → target grammar unit → surface realization → output text. Errors in dependency parsing directly corrupt case/number/gender assignments.

- **Design tradeoffs**: Rule-based core + NMT-assisted transfer trades end-to-end neural fluency for hallucination control and auditability. One-time human review of translated configurations trades upfront cost for zero human-in-the-loop at generation time. Grammar unit abstraction trades representational completeness for cross-language transferability.

- **Failure signatures**: Case errors in German (8% of units): Often genitive→nominative misassignment when proper nouns end in 's' (e.g., "Denver Nuggets"). Compound noun head marking (5% German): Missing lexical information requires manual head specification (e.g., "Ergebnis" as head of "Double-Double-Ergebnis"). High inter-annotator variance: Some translators make minimal edits for grammaticality; others add non-essential units, making aggregate error rates noisy.

- **First 3 experiments**: 
  1. Reproduce the case error pattern: Run basketball team names ending in 's' through the German transfer pipeline; examine spaCy's parse output to confirm genitive misclassification hypothesis.
  2. Test compound noun head detection: Create German grammar units with multi-component compounds; measure head-marking edit frequency across different compound types.
  3. Quantify annotator variance: Have 3+ translators post-edit the same 20 grammar units; calculate per-category agreement to identify which error types are consistently detected vs. annotator-dependent.

## Open Questions the Paper Calls Out

### Open Question 1
How does system performance vary when a morphologically rich language (e.g., German) serves as the source language rather than English? The authors state in the Conclusion that they "are also interested in investigating translation performance when using grammar-rich languages such as German as a source language." This remains untested as the current study exclusively utilized English as the source language.

### Open Question 2
To what extent does a larger participant pool reduce the variance in post-editing behavior and subjective interpretation of grammatical correctness? With only 13 translators (and only 1 for several languages), it is unclear if the observed errors represent systemic failures or individual translator preferences. The Conclusion notes that "the limited number of participants hardly allows a profound quantitative evaluation."

### Open Question 3
Can heuristics or improved parsing logic be developed to automatically correct the specific case errors (e.g., genitive vs. nominative) identified in German proper nouns? The Results section notes that 8% of German grammar units required a case change, primarily because the dependency parser mistook team names ending in "s" (like "Denver Nuggets") for genitive nouns. The Conclusion suggests "improvements in automatic transfer are suggested."

## Limitations
- Small sample size of 13 translators introduces uncertainty in generalizability of error rate statistics
- High inter-annotator variability between translators makes aggregate error rates noisy
- German case error pattern (8% genitive-to-nominative misclassification) highlights systematic dependency parsing weakness
- Lack of automated grammar transfer evaluation metrics means all conclusions rely on subjective human judgment

## Confidence
- Grammar unit transfer accuracy claim (82%): Medium - based on small post-editing sample with high inter-annotator variance
- NMT-assisted grammar configuration transfer mechanism: High - architecture is clearly specified and logically sound
- Dependency parsing feature extraction: Medium - spaCy models are robust, but custom aggregation rules are underspecified

## Next Checks
1. Replicate the German case error pattern with 50+ proper noun examples ending in 's' to quantify misclassification rate and test spaCy dependency parse outputs
2. Conduct inter-annotator agreement study with 5+ translators post-editing identical grammar unit sets to measure variability in error detection
3. Implement automated grammar unit feature extraction comparison between source and target languages using a held-out test set