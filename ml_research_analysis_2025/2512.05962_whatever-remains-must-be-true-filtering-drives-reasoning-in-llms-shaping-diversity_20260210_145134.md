---
ver: rpa2
title: 'Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping
  Diversity'
arxiv_id: '2512.05962'
source_url: https://arxiv.org/abs/2512.05962
tags:
- base
- pass
- diversity
- grpo
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the loss of diversity observed in Large Language
  Models (LLMs) fine-tuned with Reinforcement Learning (RL), particularly in reasoning
  tasks. The authors argue that RL implicitly optimizes the mode-seeking Reverse KL
  divergence, causing models to concentrate on high-probability solutions while neglecting
  others, reducing output diversity.
---

# Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity

## Quick Facts
- arXiv ID: 2512.05962
- Source URL: https://arxiv.org/abs/2512.05962
- Reference count: 40
- Key outcome: α-DPG achieves state-of-the-art performance on the Lean theorem-proving benchmark, improving coverage-precision trade-offs by explicitly controlling the diversity-fidelity balance through α-divergence optimization

## Executive Summary
This paper addresses a critical limitation in reinforcement learning fine-tuning of LLMs for reasoning tasks: the loss of solution diversity when optimizing for correctness. The authors demonstrate that RL methods implicitly optimize Reverse KL divergence, which tends to concentrate probability mass on a single high-probability solution while neglecting alternatives. To address this, they propose Distributional Matching with Verifiable Rewards (DMVR), which explicitly defines a target distribution by filtering incorrect answers while preserving the relative probabilities of correct ones. The key innovation is α-DPG, which minimizes α-divergence to this target distribution, allowing fine-grained control over the precision-diversity trade-off by interpolating between mode-seeking (Reverse KL) and mass-covering (Forward KL) behaviors.

## Method Summary
The authors propose Distributional Matching with Verifiable Rewards (DMVR) to explicitly define a target distribution by filtering incorrect answers while preserving the relative probabilities of correct ones. They introduce α-DPG, which minimizes the α-divergence to this target, allowing control over the precision-diversity trade-off by interpolating between mode-seeking (Reverse KL) and mass-covering (Forward KL) behaviors. This approach addresses the implicit Reverse KL optimization in standard RL that causes models to concentrate on high-probability solutions while neglecting others, thereby reducing output diversity.

## Key Results
- α-DPG models achieve state-of-the-art performance along the coverage-precision Pareto frontier on Lean theorem-proving benchmark
- The method outperforms prior approaches in coverage, particularly at intermediate α values
- α-DPG demonstrates superior trade-offs between pass@1 (precision) and pass@256 (coverage) metrics

## Why This Works (Mechanism)
The mechanism exploits the fundamental difference between Reverse KL and Forward KL divergences. Reverse KL optimization (implicitly used in standard RL) is mode-seeking, causing the model to concentrate probability mass on the single highest-probability solution. In contrast, Forward KL is mass-covering, encouraging the model to assign non-zero probability to all modes in the target distribution. By using α-divergence with tunable α, the method can interpolate between these two extremes, allowing explicit control over the diversity-fidelity trade-off.

## Foundational Learning

**α-divergence**: A family of divergences parameterized by α that interpolates between Reverse KL (α→1) and Forward KL (α→-1). Why needed: Provides mathematical framework for controlling the precision-diversity trade-off. Quick check: Verify that α=1 recovers Reverse KL and α=-1 recovers Forward KL.

**Distributional Matching**: The process of aligning the model's output distribution with a target distribution. Why needed: Enables explicit optimization toward desired solution diversity rather than implicit Reverse KL behavior. Quick check: Confirm the target distribution preserves correct solution probabilities while filtering incorrect ones.

**Filtering without replacement**: Strategy for constructing target distributions by removing incorrect solutions while maintaining the relative probabilities of correct ones. Why needed: Ensures the target distribution reflects only valid solutions while preserving diversity among them. Quick check: Verify that filtering doesn't distort the relative probabilities of remaining correct solutions.

## Architecture Onboarding

Component map: Initial policy -> Filtering mechanism -> Target distribution construction -> α-DPG optimization -> Fine-tuned policy

Critical path: The key computational flow involves generating candidate solutions, filtering incorrect ones to construct the target distribution, then optimizing the policy to minimize α-divergence to this target using verifiable rewards.

Design tradeoffs: The method trades computational overhead of filtering and target distribution construction against improved diversity control. The choice of α represents a fundamental tradeoff between precision (higher α) and coverage (lower α).

Failure signatures: If α is too high, the model will exhibit mode-seeking behavior similar to standard RL, losing diversity benefits. If α is too low, the model may sacrifice precision for coverage. Poor filtering can lead to incorrect solutions contaminating the target distribution.

First experiments: 1) Verify Reverse KL behavior in baseline RL methods by measuring solution diversity, 2) Test α-DPG with different α values on a simple reasoning task to observe the precision-coverage tradeoff, 3) Validate the filtering mechanism by checking that relative probabilities of correct solutions are preserved in the target distribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is currently limited to a single mathematical reasoning domain (Lean theorem proving), constraining generalizability claims
- The filtering-based target distribution construction may not scale well to problems with larger candidate pools
- The practical sensitivity to α choice across different tasks remains to be fully characterized

## Confidence
- High confidence: The theoretical analysis of Reverse KL mode-seeking behavior and its role in diversity loss
- High confidence: The experimental demonstration of improved coverage-precision trade-offs on Lean
- Medium confidence: Generalizability to other reasoning domains beyond mathematical theorem proving
- Medium confidence: The scalability of the filtering-based target distribution construction method

## Next Checks
1. Test α-DPG on diverse reasoning tasks including code generation, logical reasoning, and multi-step problem solving to assess cross-domain effectiveness
2. Evaluate the method's performance when correct solutions are less distinct or when the candidate pool size is significantly larger
3. Conduct ablation studies varying the filtering threshold and replacement strategy to understand robustness to target distribution construction choices