---
ver: rpa2
title: 'SBSC: Step-By-Step Coding for Improving Mathematical Olympiad Performance'
arxiv_id: '2502.16666'
source_url: https://arxiv.org/abs/2502.16666
tags:
- sbsc
- step
- solve
- problem
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SBSC is a multi-turn math reasoning framework that solves Olympiad-level
  problems by generating sequences of programs, with each turn focusing on a sub-task
  and leveraging previous results. Experiments on AIME, AMC-12, MathOdyssey, and OlympiadBench
  show SBSC (greedy decoding) improves accuracy by 8-12.6% over prior methods like
  TIR-ToRA and PAL, and outperforms self-consistency results by 6.2-7.4%.
---

# SBSC: Step-By-Step Coding for Improving Mathematical Olympiad Performance

## Quick Facts
- arXiv ID: 2502.16666
- Source URL: https://arxiv.org/abs/2502.16666
- Authors: Kunal Singh; Ankan Biswas; Sayandeep Bhowmick; Pradeep Moturi; Siva Kishore Gollapalli
- Reference count: 40
- SBSC improves Olympiad math accuracy by 8-12.6% over prior methods with 3-7x higher cost

## Executive Summary
SBSC introduces a multi-turn reasoning framework that solves complex mathematical problems by generating sequences of programs, with each turn focusing on a sub-task and leveraging previous results. The approach uses 4-shot exemplars to guide the LLM through step-by-step decomposition, executing code at each stage and accumulating context. Experiments on AIME, AMC-12, MathOdyssey, and OlympiadBench show SBSC (greedy decoding) improves accuracy by 8-12.6% over prior methods like TIR-ToRA and PAL, and outperforms self-consistency results by 6.2-7.4%. The approach also outperforms on JEE-Bench and Omni-MATH. While SBSC is 3-7x costlier per question than baselines, it achieves strong pass@k scores approaching o1-level performance.

## Method Summary
SBSC uses few-shot prompting with 4 exemplars to teach LLMs a multi-turn program-aided reasoning format. At each turn, the model generates a sub-task and corresponding Python/SymPy program, executes it, and accumulates the output. The process continues until reaching a termination condition ("### END OF CODE") or hitting a 15-turn maximum. The framework maintains context across turns by concatenating previous programs and outputs, enabling dynamic discovery of solution paths based on intermediate results. SBSC incorporates error-recovery instructions to correct failed steps without restarting from the beginning.

## Key Results
- SBSC (greedy decoding) improves accuracy by 8-12.6% over TIR-ToRA and PAL on Olympiad benchmarks
- Outperforms self-consistency results by 6.2-7.4% across multiple datasets
- Achieves strong pass@k scores, approaching o1-level performance
- Geometry problems show significant underperformance (28% pass@25 vs 50%+ for other topics)
- Robust to exemplar choice and benefits from step-wise debugging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Granular sub-task decomposition reduces reasoning complexity by limiting problem scope per turn
- Mechanism: Each turn addresses one sub-task with a dedicated program, reducing cognitive load compared to solving the entire problem in a single code block. This allows more precise handling of constraints, edge cases, and simplifications that Olympiad-level problems require.
- Core assumption: Complex mathematical reasoning decomposes into independently verifiable sub-tasks with clean interfaces.
- Evidence anchors:
  - [abstract] "At each step/turn, by leveraging the code execution outputs and programs of previous steps, the model generates the next sub-task and the corresponding program to solve it."
  - [section 1.1] "Often, it is not feasible to solve a complex problem entirely using a single program block and as a result, these prompting strategies fail to systematically address each detailed step of the problem-solving process."
  - [corpus] Weak direct evidence; related benchmarks (RIMO, AMO-Bench) focus on evaluation difficulty, not decomposition mechanisms.
- Break condition: When sub-tasks are tightly coupled such that intermediate outputs don't meaningfully constrain subsequent steps.

### Mechanism 2
- Claim: Context accumulation across turns enables dynamic discovery of solution paths
- Mechanism: By carrying forward execution outputs (variables, computed results) from previous steps, the model can adaptively identify the next sub-task based on actual intermediate results rather than pre-planned decomposition. This allows course-correction when initial assumptions prove wrong.
- Core assumption: The underlying LLM has sufficient context window and instruction-following capability to maintain coherent state across 6-7 average turns.
- Evidence anchors:
  - [abstract] "This way, SBSC, sequentially navigates to reach the final answer."
  - [section 2] "The intermediate sub-task depends on the results of the previous turns and the question."
  - [corpus] No direct corpus evidence for dynamic discovery mechanism; neighboring papers focus on benchmark creation.
- Break condition: When context exceeds model capacity or when execution outputs are too large/complex to meaningfully incorporate.

### Mechanism 3
- Claim: Step-level error isolation improves recovery from failures
- Mechanism: When a sub-task's code fails, the model corrects only that step using the error message, without restarting from step 1. This contrasts with single-block approaches where errors often require complete re-solution attempts.
- Core assumption: Error messages provide actionable signals for localizing and fixing the specific sub-task error.
- Evidence anchors:
  - [section 5.3, Figure 4] Shows SBSC maintains accuracy even with multiple error steps while TIR-ToRA drops sharply.
  - [section 2, prompt instruction] "If the executed code snippet returns an error, use it to correct the current step's code snippet. DO NOT restart solving from Step 1."
  - [corpus] No corpus papers directly examine step-level error recovery in multi-turn reasoning.
- Break condition: When errors cascade across multiple interdependent steps, making local correction insufficient.

## Foundational Learning

- Concept: **In-Context Learning with Few-Shot Prompting**
  - Why needed here: SBSC relies entirely on 4-shot exemplars to teach the model the multi-turn reasoning format without fine-tuning. Understanding how exemplar design affects behavior is critical.
  - Quick check question: Can you explain why the paper uses 4-shot instead of 8-shot, and what happens when exemplars are removed?

- Concept: **Program-Aided Reasoning (PAL/POT paradigms)**
  - Why needed here: SBSC builds on PAL's insight that code execution handles computation more reliably than natural language reasoning. Understanding how to structure variable naming and comments within generated code matters for exemplar quality.
  - Quick check question: What are the two PAL-derived practices that the paper incorporates into SBSC exemplars?

- Concept: **Multi-Turn Conversation State Management**
  - Why needed here: SBSC requires maintaining and accumulating state (previous programs, outputs) across 6-7 turns on average. The context window grows linearly, affecting cost and potential for degradation.
  - Quick check question: How does token cost scale with turn number, and what optimization does the paper use?

## Architecture Onboarding

- Component map: LLM (G/Sonnet) -> Code Interpreter (Python/SymPy) -> Execution Output (oi)
- Critical path:
  1. Design 4-shot exemplars with sub-task decomposition, inline comments, and error-recovery instruction
  2. Initialize with system prompt + exemplars + question
  3. Loop: Generate sub-task → Generate program → Execute → Append output → Check for termination ("### END OF CODE")
  4. Maximum 15 turns (empirically, 99%+ problems complete within this)

- Design tradeoffs:
  - **Accuracy vs. Cost**: SBSC is 3-7x costlier per question than TIR-ToRA/PAL (Table 7-8). Use prompt caching to mitigate.
  - **Granularity vs. Coherence**: More granular steps improve error recovery but increase context load and potential for incoherence across turns.
  - **Exemplar specificity vs. generalization**: Exemplars tuned to AIME work on MathOdyssey/OlympiadBench but may underperform on significantly different problem distributions.

- Failure signatures:
  - **Infinite loops**: Model generates sub-tasks without progression toward "### END OF CODE" (mitigated by 15-turn cap)
  - **Context contamination**: Errors in early steps propagate through variable definitions in later code blocks
  - **Geometry blind spots**: Pass@k analysis shows geometry problems underperform significantly (Table 15: 28% vs 50%+ in other topics at pass@25)

- First 3 experiments:
  1. **Ablate comment removal**: Remove natural language comments from exemplars (Table 2 shows 8-point drop on AIME). This validates that code+comment structure matters.
  2. **Compare turn caps**: Run with max_turns=[4, 8, 15] on a held-out subset to understand early termination impact. The paper reports average 6-7 turns (Table 3).
  3. **Cross-domain exemplar transfer**: Test AIME-trained exemplars on a 20-question geometry subset to characterize the domain gap the paper identifies in pass@k analysis.

## Open Questions the Paper Calls Out

- None

## Limitations

- SBSC is 3-7x costlier per question than TIR-ToRA/PAL, raising practical deployment concerns
- Geometry problems show significant underperformance (28% pass@25 vs 50%+ for other topics)
- Evaluation focuses on quantitative accuracy without qualitative analysis of solution quality or educational value

## Confidence

- **High confidence**: The core mechanism of step-by-step program generation with error recovery is well-documented and the ablation studies provide robust evidence for its effectiveness. The performance improvements over baselines are consistently demonstrated across four different benchmark datasets.
- **Medium confidence**: The claim that SBSC approaches o1-level performance relies on relative comparisons rather than direct benchmarking against o1. The cost analysis is transparent but may underestimate real-world deployment costs including API rate limits and caching inefficiencies.
- **Low confidence**: The paper doesn't address whether the step-by-step approach produces mathematically sound reasoning versus pattern-matching solutions. The reliance on 4-shot exemplars without exploring exemplar sensitivity limits generalizability claims.

## Next Checks

1. **Geometry domain analysis**: Conduct a detailed error analysis on geometry problems to identify whether failures stem from visual reasoning limitations, spatial decomposition challenges, or insufficient geometric knowledge in the underlying LLM.

2. **Exemplar sensitivity testing**: Systematically vary exemplar quality, diversity, and domain alignment to quantify the impact on cross-domain generalization, particularly for the geometry performance gap.

3. **Cost-benefit scaling study**: Measure accuracy retention when reducing turn limits below the average 6-7 turns, and quantify the marginal accuracy gain per additional token cost to establish optimal deployment parameters.