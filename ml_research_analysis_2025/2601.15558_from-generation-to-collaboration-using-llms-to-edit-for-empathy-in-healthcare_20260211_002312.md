---
ver: rpa2
title: 'From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare'
arxiv_id: '2601.15558'
source_url: https://arxiv.org/abs/2601.15558
tags:
- empathy
- responses
- physician
- response
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses the challenge of balancing empathy and factual\
  \ accuracy in clinical communication by proposing a collaborative approach where\
  \ large language models (LLMs) act as empathy editors rather than autonomous generators.\
  \ Physicians\u2019 responses are edited to enhance empathetic tone while preserving\
  \ medical information."
---

# From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare

## Quick Facts
- arXiv ID: 2601.15558
- Source URL: https://arxiv.org/abs/2601.15558
- Reference count: 40
- Primary result: LLM editing of physician responses increases perceived empathy while maintaining factual accuracy (recall >90%, precision >91%) better than direct generation

## Executive Summary
This study addresses the challenge of balancing empathy and factual accuracy in clinical communication by proposing a collaborative approach where large language models (LLMs) act as empathy editors rather than autonomous generators. Physicians' responses are edited to enhance empathetic tone while preserving medical information. Novel quantitative metrics—Empathy Ranking Score and MedFactChecking Score—are introduced to assess emotional quality and factual integrity, respectively. Experimental results show that LLM-edited responses significantly increase perceived empathy while maintaining high factual accuracy compared to fully LLM-generated outputs, which suffer from factual degradation. The refined editing approach outperforms both direct generation and simple editing in preserving clinical facts.

## Method Summary
The study evaluates LLM editing of physician responses using two prompt templates (simple and refined) on 163 de-identified patient-physician message pairs from Mayo Clinic's prostate cancer portal. Empathy is assessed using a novel three-way comparison framework (3-EMRank) where LLM judges can select "both equally empathetic." Factual integrity is measured via MedFactChecking Score using bidirectional entailment between original and edited responses. The refined prompt adds behavioral constraints to prevent hallucinated content. Fact extraction uses Gemini-2.5-Flash with FActScore adaptation, while entailment checking uses a specialized FactEHR prompt.

## Key Results
- LLM-edited responses increase perceived empathy while maintaining high factual accuracy (recall >90%, precision >91%) compared to fully LLM-generated outputs
- Refined editing approach achieves near-perfect fact preservation (recall: 0.99, precision: 0.93) versus simple editing (recall: 0.95, precision: 0.89)
- Direct AI generation catastrophically fails on factual grounding (micro recall: 0.39) compared to editing approaches
- Three-way empathy comparison (3-EMRank) shows improved alignment (0.55-0.57) versus binary forced choice (0.23)

## Why This Works (Mechanism)

### Mechanism 1
Editing physician responses preserves factual grounding better than de novo generation because the LLM operates on an existing clinical information substrate rather than generating from parametric knowledge. The physician's original response provides a constrained search space for the LLM. Bidirectional entailment checking reveals that direct AI generation achieves only 0.39 micro recall versus 0.95+ for editing approaches, indicating that generation operates in a fundamentally different information space disconnected from patient-specific clinical facts.

### Mechanism 2
Explicit behavioral constraints in prompts systematically reduce hallucinated content while preserving empathetic enhancement capacity. The refined prompt adds four constraint categories: preserve factual accuracy with no inference, respect physician intent with no new recommendations, maintain emotional balance without false reassurance, and preserve structure. This constrains the model's generation to stylistic modification rather than content expansion. Results show refined editing achieves 0.99 recall/0.93 precision versus simple editing's 0.95/0.89.

### Mechanism 3
Three-way empathy comparison better aligns with human judgment than forced binary choice because empathy perception is inherently subjective and graded. The 3-EMRank allows "both responses are equally empathetic" as a valid judgment. Validation with prostate cancer patients shows alignment scores of 0.55-0.57 versus 0.23 for binary EM-Rank. This reduces forced-choice artifacts where models must select a winner between near-identical responses.

## Foundational Learning

- **Bidirectional Entailment Checking**
  - Why needed here: The MedFactChecking Score requires understanding that factual preservation has two directions—ensuring original facts appear in edited output (recall) and ensuring edited content is grounded in original (precision).
  - Quick check question: If a physician says "Your PSA is 4.2" and the edited response says "Your PSA of 4.2 is slightly elevated, which is common after surgery," which metric(s) would this affect?

- **Length-Hallucination Correlation**
  - Why needed here: Short physician responses trigger more content expansion by LLMs, with non-Gemini models showing 12-23 percentage point precision drops for short versus long responses. This informs deployment decisions about when editing is safe.
  - Quick check question: Why might a brief physician acknowledgment like "Looks good, continue current meds" be particularly vulnerable to hallucinated additions during empathy editing?

- **Empathy-Factuality Trade-off Curve**
  - Why needed here: Results show an inverse relationship—as empathy intensity increases from "Empathic" to "Extreme Empathic," precision drops from 0.91 to 0.78 while recall remains stable. Deployment requires selecting an operating point on this curve.
  - Quick check question: If a clinical deployment prioritizes safety over warmth, which empathy level should be selected, and what factual precision would you expect?

## Architecture Onboarding

- Component map:
  Input Layer -> Editing Module -> Empathy Evaluation -> Factuality Evaluation -> Output

- Critical path:
  1. De-identify patient-physician message pairs (manual in current study)
  2. Apply refined editing prompt to physician response
  3. Extract medical facts from both original and edited responses
  4. Compute bidirectional entailment (Fact-Recall and Fact-Precision)
  5. Run 3-EMRank comparison against baseline
  6. Flag responses with precision <0.90 for human review

- Design tradeoffs:
  - Gemini-2.5-Flash achieves best factuality (0.92 recall, 0.91 precision) but may have different empathy characteristics than open-source alternatives
  - Refined prompt sacrifices some empathy gain (ranked lower than simple prompt) for higher factual integrity
  - Three-way judging improves alignment but increases comparison complexity

- Failure signatures:
  - Short physician responses (<200 characters) with open-source models show 0.65 precision (Mistral)—flag for manual review
  - Responses flagged as "not preserved" may include semantically similar but differently phrased facts (72.7% confirmed as genuinely missing)
  - Expert-identified error patterns: unprompted follow-up recommendations (13 cases), clinical assumptions (11 cases), false assurance (4 cases)

- First 3 experiments:
  1. **Model Selection**: Compare Gemini-2.5-Flash vs. LLaMA-3.1-8B on factuality metrics using the refined prompt on a held-out test set; expect Gemini to show ~15 point F1 advantage.
  2. **Empathy Threshold Calibration**: Generate edited responses at three empathy levels (standard, high, extreme) and plot the precision degradation curve to identify safe operating threshold.
  3. **Length-Stratified Validation**: Partition test set by original response length (short/medium/long) and measure precision separately; implement length-aware flagging for responses under 231 characters.

## Open Questions the Paper Calls Out

- How does the empathy editing framework perform across diverse clinical specialties beyond urology?
- Can incorporating explicit clinical empathy definitions from validated psychometric instruments improve 3EM-Ranker reliability?
- What is the causal impact of LLM-edited empathetic responses on actual patient outcomes?
- How do patients from diverse demographic and educational backgrounds perceive AI-edited empathy?

## Limitations
- Single-center dataset from Mayo Clinic with prostate cancer patients may not generalize to other clinical contexts
- Judge model selection significantly impacts results, with Qwen3 (0.57) vs LLaMA-3.1 (0.55) showing 0.02 alignment difference
- Short physician responses (<200 characters) show precision drops to 0.65 with open-source models, representing accuracy risk

## Confidence
- High confidence in factual preservation mechanisms: bidirectional entailment framework provides robust quantitative evidence
- Medium confidence in empathy enhancement: LLM-as-judge scores may not fully capture human empathy perception in clinical contexts
- Low confidence in deployment readiness: system requires manual de-identification and shows performance variability

## Next Checks
1. Apply refined editing approach to multi-center dataset (e.g., MIMIC-III) to test generalizability across institutions and specialties
2. Conduct randomized controlled trial where human clinicians review LLM-edited responses versus original physician responses
3. Deploy system in controlled clinical environment with automated monitoring for hallucinated recommendations and clinical assumptions over time