---
ver: rpa2
title: Better Language Model Inversion by Compactly Representing Next-Token Distributions
arxiv_id: '2506.17090'
source_url: https://arxiv.org/abs/2506.17090
tags:
- language
- prompt
- pils
- should
- inverter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Language model inversion seeks to recover hidden prompts from\
  \ a language model\u2019s outputs, with implications for security and accountability\
  \ in model deployments. This work addresses the challenge of recovering hidden prompts\
  \ by leveraging next-token probabilities across multiple generation steps."
---

# Better Language Model Inversion by Compactly Representing Next-Token Distributions

## Quick Facts
- **arXiv ID:** 2506.17090
- **Source URL:** https://arxiv.org/abs/2506.17090
- **Reference count:** 40
- **Primary result:** PILS achieves 2-3.5x higher exact prompt recovery rates than prior state-of-the-art methods

## Executive Summary
This paper introduces PILS (Prompt Inversion from Logprob Sequences), a novel method for recovering hidden prompts from language model outputs by leveraging next-token probabilities across multiple generation steps. The key insight is that language model outputs occupy a low-dimensional subspace, allowing lossless compression of full next-token probability distributions using a linear map. This compression reduces representation size from vocabulary-sized vectors to the model's embedding size, enabling efficient use of multiple generation steps. PILS achieves massive gains over previous methods, with exact recovery rates 2-3.5x higher across test sets, and demonstrates strong generalization to longer sequences at test time.

## Method Summary
PILS recovers hidden prompts by gleaning clues from the model's next-token probabilities over multiple generation steps. The method compresses full next-token probability distributions into low-dimensional vectors using an additive log-ratio transform and selection of token indices, reducing representation from vocabulary-sized to embedding-sized. These compressed vectors from T generation steps are fed into a T5-base encoder-decoder model with a learned feed-forward adapter to reconstruct the original prompt. The method is trained end-to-end on pairs of (logprob_sequence, original_prompt) generated by the target LLM.

## Key Results
- PILS achieves exact recovery rates 2-3.5x higher than previous state-of-the-art methods
- Performance scales with generation steps: inverters trained on 16 steps improve by 5-27 points when evaluated on 32 steps
- Strong performance on recovering hidden system messages with 75-81% exact match
- Cross-family model transfer shows competitive results with text-based inversion methods

## Why This Works (Mechanism)

### Mechanism 1
The full next-token probability distribution from a language model can be losslessly compressed into a low-dimensional vector. The model's final hidden state $\boldsymbol{h} \in \mathbb{R}^D$ is projected to logits by the unembedding matrix $\boldsymbol{W}$, then transformed to probabilities via softmax. The composition of these operations results in the output logprobs living in a $D$-dimensional subspace. By applying an additive log-ratio (alr) transform and selecting a specific set of $D$ token indices, one can recover a vector $\text{alr}(\boldsymbol{p})_D$ that is a linear transformation of the original hidden state $\boldsymbol{h}$. This compact representation retains all information from $\boldsymbol{h}$.

### Mechanism 2
A hidden prompt is recoverable by aggregating clues revealed across multiple sequential generation steps. An autoregressive model conditions its output at each step on the entire preceding context, including the hidden prompt. Certain relationships between the prompt and the model's output may not manifest until later in the generation sequence. By using a sequence of compressed logprob vectors from $T$ steps as input, the inverter model learns to aggregate these temporally dispersed signals to reconstruct the prompt.

### Mechanism 3
A sequence-to-sequence model with a learned adapter can effectively learn the mapping from compressed logprob sequences to the original prompt. The core inverter is a T5-base encoder-decoder model. The encoder processes the sequence of compressed hidden-state vectors, $\boldsymbol{h}^{(1)}, \dots, \boldsymbol{h}^{(T)}$. A key architectural choice is a feed-forward adapter layer before the encoder, which projects the target model's embedding space ($D$) to the inverter's space ($D_{\text{invert}}$) and adds non-linearity. The model is trained end-to-end on pairs of (logprob_sequence, original_prompt) generated by the target LLM.

## Foundational Learning

**Concept: Softmax Bottleneck**
- Why needed here: This is the foundational theoretical insight of the paper. It explains why logprobs can be compressed. The softmax output over a vocabulary $V$ lies in a linear subspace whose rank is at most the embedding dimension $D$, which is typically much smaller than $V$.
- Quick check question: Given a model with embedding size $D$ and vocabulary $V$, what is the maximum rank of the matrix of all possible output log-probability vectors it can produce?

**Concept: Autoregressive Generation**
- Why needed here: The PILS method fundamentally relies on observing the model's behavior over time. Understanding how each new token is predicted based on all previous tokens is essential for grasping why a prompt's influence is spread across multiple steps.
- Quick check question: In greedy decoding, how is the token selected at each step $t$ from the probability distribution $P(token | prompt, generated\_tokens_{<t})$?

**Concept: Seq2Seq Models (Encoder-Decoder Architectures)**
- Why needed here: The inverter is a T5 model, which follows an encoder-decoder structure. One must understand that the encoder maps an input sequence to a latent representation, and the decoder generates an output sequence conditioned on that representation.
- Quick check question: In a standard encoder-decoder transformer, what type of attention mechanism allows the decoder to "look at" the encoder's output?

## Architecture Onboarding

**Component map:**
Data Generator -> Target LLM -> Full Logprobs (alr + selection) -> Compressed Vectors -> Adapter -> T5 Encoder -> T5 Decoder -> Recovered Prompt

**Critical path:**
Prompt -> Target LLM -> Full Logprobs (`alr` + selection) -> Compressed Vectors -> Adapter -> T5 Encoder -> T5 Decoder -> Recovered Prompt

**Design tradeoffs:**
- Generation Steps ($T$): More steps improve recovery but linearly increase API query costs and training data storage
- Token Selection: Using the first $D$ tokens is simpler but can cause rank deficiency. Using a random superset ($D+100$) adds robustness but requires a slightly more complex index selection
- Adapter Complexity: A simple linear adapter is more parameter-efficient but may lose information if $D > D_{\text{invert}}$. The chosen non-linear adapter preserves more information

**Failure signatures:**
- High Token F1, Low Exact Match: Indicates the inverter is recovering the semantic gist but failing on precise wording
- Performance Plateau: If increasing test steps ($T_{\text{test}}$) yields no improvement, the model has likely extracted all recoverable information from the prompt given its capacity
- Garbled Output: If the adapter's initialization is poor or the learning rate is too high, the inverter may output incoherent text, failing to learn the mapping from the compressed space

**First 3 experiments:**
1. Sanity Check (T=1): Train and evaluate a PILS model with $T=1$. Its performance should be comparable to the L2T baseline, validating the compression mechanism
2. Scaling Study: Train identical inverters with $T \in \{1, 8, 16, 32\}$. Plot exact match recovery vs. $T$ to confirm the primary hypothesis that more steps yield better recovery
3. Generalization Test: Take the model trained with $T=16$ and evaluate it on sequences of length $T \in \{16, 24, 32, 64\}$. The goal is to verify the key finding that performance improves even beyond the training step count

## Open Questions the Paper Calls Out

**Open Question 1:** Why does PILS's superior performance in non-transfer settings fail to materialize in cross-family model transfer, where text-based methods like O2P perform competitively? The proposed explanation (target specificity) remains speculative; the exact mechanism causing logprob-based inverters to overfit to specific model architectures is not empirically validated.

**Open Question 2:** What mechanism enables inverters trained on T generation steps to generalize and improve when evaluated on more than T steps, and does this depend on the inverter's pre-training architecture? This hypothesis is not tested against alternative architectures without relative position embeddings, leaving the root cause unclear.

**Open Question 3:** Would a large-scale, diverse, non-synthetic dataset of system prompts significantly improve system message inversion performance? Current experiments use small datasets (50-103 samples for fine-tuning), and the role of dataset quality versus quantity remains untested.

**Open Question 4:** Has PILS fully saturated the language model inversion task, and what performance ceiling remains with larger inverter models or more generation steps? Scaling experiments are limited to 32-64 steps and T5-base; the effect of larger models or more steps is unknown.

## Limitations

- Compression Robustness: The core compression mechanism relies on selecting D+100 token indices that yield a full-rank transformation, but the exact sensitivity to this choice is not explored
- Out-of-Distribution Generalization: Strong performance on unseen prompt distributions remains untested against highly adversarial prompts, different languages, or unusual formatting
- Target Model Dependency: Performance on significantly different architectures (e.g., decoder-only vs. encoder-decoder, or models with different embedding sizes) is not characterized

## Confidence

**High Confidence:** The core mathematical proof of the softmax bottleneck and the linear compression mechanism is sound. The empirical results showing PILS outperforms prior SOTA by 2-3.5x on exact recovery are robust and well-supported by the data.

**Medium Confidence:** The claim that multi-step generation provides additional information is well-supported by qualitative examples and the generalization results, but the exact nature of the information revealed at each step is not fully characterized.

**Medium Confidence:** The effectiveness of the T5-based Seq2Seq inverter is demonstrated, but the choice of architecture is not rigorously justified. It is possible that a different model family or a simpler approach could achieve comparable results.

## Next Checks

1. **Rank Deficiency Stress Test:** Systematically vary the number of selected token indices (D, D+10, D+50, D+200) and measure the impact on inversion performance to quantify sensitivity to compression choice.

2. **Adversarial Prompt Evaluation:** Create a benchmark of prompts specifically designed to be difficult to invert (e.g., prompts that are the negation of likely generations, or prompts that are highly specific jargon) to understand failure modes.

3. **Architecture Ablation Study:** Replace the T5-base inverter with a simpler model (e.g., a linear regression model or a small MLP) and compare performance to determine if T5 complexity is necessary.