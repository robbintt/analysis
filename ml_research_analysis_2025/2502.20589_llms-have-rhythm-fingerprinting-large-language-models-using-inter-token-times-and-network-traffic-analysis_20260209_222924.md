---
ver: rpa2
title: 'LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token Times
  and Network Traffic Analysis'
arxiv_id: '2502.20589'
source_url: https://arxiv.org/abs/2502.20589
tags:
- network
- different
- language
- data
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel passive fingerprinting technique
  that identifies language models by analyzing the temporal patterns of their token
  generation process, captured through inter-token times (ITTs) in network traffic.
  The method leverages the autoregressive nature of language models, where each token
  is generated sequentially, creating a unique rhythmic signature.
---

# LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token Times and Network Traffic Analysis

## Quick Facts
- arXiv ID: 2502.20589
- Source URL: https://arxiv.org/abs/2502.20589
- Reference count: 40
- Key outcome: Novel passive fingerprinting technique identifies language models by analyzing inter-token timing patterns in network traffic with high accuracy across diverse deployment scenarios

## Executive Summary
This paper introduces a novel passive fingerprinting technique that identifies language models by analyzing the temporal patterns of their token generation process, captured through inter-token times (ITTs) in network traffic. The method leverages the autoregressive nature of language models, where each token is generated sequentially, creating a unique rhythmic signature. A deep learning pipeline extracts 36 engineered features from network traffic and employs a hybrid BiLSTM-attention model to classify models. The technique is tested on 16 open-source small language models (SLMs) and 10 proprietary large language models (LLMs) across various deployment scenarios, including local, LAN, remote, and VPN. Results show high accuracy in identifying model families and variants, even under challenging network conditions, demonstrating the method's robustness and potential for real-world applications.

## Method Summary
The method extracts inter-token timing patterns from encrypted network traffic without accessing model internals. Network packets are captured and inter-arrival times computed, then processed through sliding windows to extract 36 engineered features including rate metrics, inter-arrival statistics, entropy measures, and correlation features. A hybrid BiLSTM-attention deep learning model classifies models from these feature sequences, trained with focal loss to handle class imbalance. The approach is validated across different deployment scenarios (local, LAN, remote, VPN) and shows robust performance despite network variability.

## Key Results
- High accuracy in identifying model families and variants from network traffic patterns
- Weighted F1 scores of 85% (different day), 74% (different network), 71% (VPN) demonstrate cross-condition generalization
- Distinct ITT profiles observed for different models on identical hardware, with larger models showing longer inter-token times
- 36 engineered features significantly outperform raw inter-arrival times for classification under realistic network conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Autoregressive language models produce distinctive inter-token timing patterns that persist as observable "rhythms" even after network transmission.
- **Mechanism:** Each token generation depends on all prior tokens; the computational path length varies with model architecture, parameter count, and hardware optimization. This creates consistent timing signatures characterized by latency baselines, periodicity, jitter patterns, and spike distributions.
- **Core assumption:** Model-specific timing patterns are sufficiently stable and distinguishable to survive packetization, encryption, and network variability.
- **Evidence anchors:**
  - [abstract]: "...generate text one token at a time based on all previously generated tokens, creating a unique temporal pattern–like a rhythm or heartbeat–that persists even when the output is streamed over a network."
  - [Section VI-A]: Figure 1 shows six SLMs on the same GPU with identical prompts exhibit distinct ITT profiles; Figure 5 shows mean ITT correlates with model size within families.
  - [corpus]: "Intrinsic Fingerprint of LLMs" supports the premise that LLMs have inherent identifiable characteristics, though focused on training dynamics rather than timing.
- **Break condition:** If adversaries intentionally introduce random delays or batch tokens with variable buffering at the server side, the timing signal could be degraded beyond recognition.

### Mechanism 2
- **Claim:** Raw inter-arrival times are insufficient for reliable classification under realistic network conditions; 36 engineered features are required to extract robust signatures.
- **Mechanism:** Sliding windows (0.5s, step 0.1s) compute rate metrics (burst rate, packet rate), inter-arrival statistics (mean, percentiles, entropy), pattern regularity measures (permutation entropy, timing regularity), and correlation features (size-time correlation). These aggregate higher-order patterns that survive network noise.
- **Core assumption:** The feature set captures model-intrinsic timing structure while filtering network-induced variability; the features generalize across different network paths.
- **Evidence anchors:**
  - [Section VI-A]: "Fig. 10 demonstrates that the DL model is overfitting the training data and fails to generalize... raw network traffic features are insufficient."
  - [Section V-D]: "Using an iterative empirical process... we extract a total of 36 engineered features... that focus on metrics revealing the model's token generation behavior."
  - [corpus]: Weak direct evidence for this specific feature set for LLMs; network traffic classification literature [36-38 cited] supports feature engineering approaches for encrypted traffic.
- **Break condition:** If network jitter variance exceeds the timing signal magnitude by orders of magnitude, no feature set may recover the fingerprint without per-network calibration.

### Mechanism 3
- **Claim:** A hybrid BiLSTM-attention architecture can classify models from feature sequences, generalizing across temporal and network variations.
- **Mechanism:** Three stacked BiLSTM blocks (128→64→32 units) capture bidirectional temporal dependencies; multi-head attention (8 heads, key_dim=128) focuses on discriminative timesteps; residual connections preserve gradients; focal loss handles class imbalance.
- **Core assumption:** The sequential dependencies in feature vectors are learnable and transfer across days, networks, and VPN routing without per-condition retraining.
- **Evidence anchors:**
  - [Section V-E]: Architecture diagram (Figure 4) and Algorithm 3 detail the BiLSTM-attention-residual design.
  - [Section VI-B]: Weighted F1 scores of 85% (different day), 74% (different network), 71% (VPN) demonstrate cross-condition generalization.
  - [corpus]: Prior work [30,31 cited] validates BiLSTM-attention for encrypted traffic classification, though not specifically for LLM timing.
- **Break condition:** If training data comes from only one network topology and test data has fundamentally different latency distributions (e.g., satellite links), the model may fail without domain adaptation.

## Foundational Learning

- **Concept: Autoregressive generation and tokenization**
  - **Why needed here:** The entire method hinges on understanding that LLMs generate tokens sequentially, with each prediction depending on prior context; the timing of each step reflects model computation.
  - **Quick check question:** Can you explain why a larger model (e.g., 9B vs 2B parameters) would produce longer inter-token times on the same hardware?

- **Concept: Network traffic analysis under encryption**
  - **Why needed here:** The method extracts features from packet inter-arrival times and sizes without decrypting payloads; understanding what information remains observable in encrypted streams is essential.
  - **Quick check question:** What two raw measurements can an observer extract from encrypted packet streams, and why is packetization a confounding factor?

- **Concept: BiLSTM with attention for sequential classification**
  - **Why needed here:** The classifier must learn temporal patterns in feature sequences; BiLSTM captures context in both directions, while attention highlights the most informative windows.
  - **Quick check question:** Why would a residual connection after the attention block help training stability in this architecture?

## Architecture Onboarding

- **Component map:** Data collection (tshark/Wireshark) -> Preprocessing (compute ITTs, sliding windows) -> Feature engineering (extract 36 features) -> Model (BiLSTM-Attention) -> Training (focal loss, Adam) -> Inference (sliding window classification)

- **Critical path:** Data collection quality → feature extraction correctness → model training convergence → cross-condition generalization. A bug in timestamp extraction or window alignment will cascade through the entire pipeline.

- **Design tradeoffs:**
  - **Window size (0.5s):** Larger windows smooth noise but reduce temporal resolution; smaller windows increase granularity but amplify noise. The paper empirically selected 0.5s.
  - **Feature set (36 features):** More features capture more signal but risk overfitting; fewer features may miss discriminative patterns. The paper iteratively selected these.
  - **Training data diversity:** Training on single network/day yields higher same-condition accuracy but poor generalization; multi-condition training may trade peak accuracy for robustness (not explicitly tested in paper).
  - **Assumption:** The paper trains and tests separately per scenario; a unified model trained on mixed conditions might behave differently.

- **Failure signatures:**
  - **Overfitting to training network:** High training accuracy, near-random validation on different network; seen with raw data (Figure 10).
  - **Confusion within model families:** Misclassification between Phi3 and Phi3.5 or ChatGPT-4 and ChatGPT-4o; indicates architectural similarity dominates timing differences.
  - **VPN degradation:** Recall drops for some models (e.g., ChatGPT-4o Mini) under VPN; indicates additional routing variability obscures timing patterns.

- **First 3 experiments:**
  1. **Local baseline replication:** Deploy 3–5 SLMs locally on GPU, collect ITTs using the Ollama `created_at` field, plot mean ITT and distribution per model. Verify that models show distinct timing profiles on identical prompts.
  2. **LAN feature extraction pipeline:** Set up client-server on same LAN, capture packets with `tshark`, implement the 36-feature extraction on sliding windows, train a simple classifier (e.g., random forest) to verify feature discriminability before deep learning.
  3. **Cross-day robustness test:** Collect proprietary LLM traffic on Day 1 for training, Day 2 for testing. Train the BiLSTM-attention model and report per-model precision/recall. Compare to paper's ~85% weighted F1 benchmark to validate implementation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this fingerprinting method withstand active adversarial obfuscation, such as the deliberate randomization of response timing?
- **Basis in paper:** [explicit] Section IV assumes an adversary is "incapable of obfuscating the LLM fingerprint" to preserve Quality of Service (QoS), but does not test this scenario.
- **Why unresolved:** The study relies on the assumption that attackers prioritize QoS over stealth, leaving the method's robustness against intentional timing noise unverified.
- **What evidence would resolve it:** Evaluating classification accuracy when artificial jitter or timing normalization is applied to the network traffic.

### Open Question 2
- **Question:** Can the proposed classifier generalize to identify models running on heterogeneous, unseen hardware configurations?
- **Basis in paper:** [inferred] Section VI-A notes hardware is a "fundamental significant factor" and shows distinct CPU/GPU profiles, suggesting the current model requires calibration for specific hardware.
- **Why unresolved:** It is unclear if a model trained on cloud GPU data can identify the same LLM running on a local CPU or different GPU architecture without retraining.
- **What evidence would resolve it:** Cross-hardware testing where the training set contains only one hardware type and the test set contains another.

### Open Question 3
- **Question:** Is the technique effective against LLMs that utilize server-side batching or non-streaming responses?
- **Basis in paper:** [inferred] Section VI-B relies specifically on models "configured to stream their responses," implying the method may fail if tokens are grouped or buffered before transmission.
- **Why unresolved:** If a provider delivers the complete response in one burst or uses continuous batching, the inter-token timing patterns would likely be destroyed.
- **What evidence would resolve it:** Testing the pipeline against APIs that provide responses in a single block or employ asynchronous batching strategies.

## Limitations
- Hardware dependency: ITTs vary significantly by hardware configuration, potentially entangling model fingerprinting with hardware fingerprinting
- Partial disclosure: Proprietary LLM dataset remains partially undisclosed, limiting reproducibility
- Unexplored alternatives: Ablation studies show engineered features outperform raw data but don't compare against simpler models
- No adversarial testing: Method's robustness against timing obfuscation techniques remains unverified

## Confidence

- **High confidence** in the core mechanism: The autoregressive generation process creates distinguishable timing patterns, supported by direct evidence showing different models exhibit unique ITT profiles on identical hardware and prompts.
- **Medium confidence** in feature engineering efficacy: While the 36-feature approach demonstrably improves generalization over raw data, the specific feature selection appears somewhat arbitrary and lacks comparison to alternative feature sets or simpler models.
- **Medium confidence** in cross-condition generalization: Weighted F1 scores of 85% (same network, different day), 74% (different network), and 71% (VPN) show reasonable robustness, but performance degrades predictably under network variability, and the model may not generalize to fundamentally different network conditions like satellite links.
- **Low confidence** in adversarial robustness: The paper does not evaluate whether timing signatures persist under deliberate obfuscation techniques such as variable response batching, artificial delays, or traffic shaping, leaving a significant gap in real-world applicability.

## Next Checks

1. **Hardware dependency validation**: Deploy the same set of SLMs across three different hardware configurations (high-end GPU, low-end GPU, CPU-only) and measure ITT distributions for identical prompts. Train and test classifiers within each hardware setup and across hardware boundaries to quantify the cross-platform performance degradation, confirming whether hardware fingerprinting becomes entangled with model fingerprinting.

2. **Adversarial robustness test**: Implement three basic obfuscation techniques—random delay injection (Gaussian noise added to token emission times), response batching (tokens grouped and sent together), and traffic padding (artificial packets to mask timing patterns). Apply these to captured LLM traffic and re-run the classification pipeline to measure accuracy degradation, determining whether timing patterns can be effectively obscured without breaking the model's functionality.

3. **Alternative model comparison**: Train and evaluate three different classification approaches on the same feature dataset: (a) the proposed BiLSTM-attention model, (b) a gradient-boosted decision tree (XGBoost), and (c) a simple logistic regression on aggregated features. Compare not just overall accuracy but also per-model precision/recall, training time, and inference latency to determine whether the complex deep learning approach provides meaningful advantages over simpler, more interpretable methods.