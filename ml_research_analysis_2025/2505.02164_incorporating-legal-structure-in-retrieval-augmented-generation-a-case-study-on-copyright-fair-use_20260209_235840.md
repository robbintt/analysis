---
ver: rpa2
title: 'Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study
  on Copyright Fair Use'
arxiv_id: '2505.02164'
source_url: https://arxiv.org/abs/2505.02164
tags:
- legal
- retrieval
- fair
- court
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a structured Retrieval-Augmented Generation
  (RAG) system for copyright fair use analysis that incorporates legal knowledge graphs
  and citation networks to improve retrieval quality and reasoning reliability. By
  modeling legal precedents at the statutory factor level and applying PageRank-based
  citation weighting, the system prioritizes doctrinally authoritative sources rather
  than relying solely on semantic similarity.
---

# Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use

## Quick Facts
- **arXiv ID**: 2505.02164
- **Source URL**: https://arxiv.org/abs/2505.02164
- **Reference count**: 40
- **Primary result**: Structured RAG achieves significantly higher PageRank scores (mean 0.213 vs 0.026) indicating improved retrieval of doctrinally relevant cases, though with slightly lower textual similarity (mean 0.521 vs 0.753).

## Executive Summary
This paper presents a structured Retrieval-Augmented Generation (RAG) system for copyright fair use analysis that incorporates legal knowledge graphs and citation networks to improve retrieval quality and reasoning reliability. By modeling legal precedents at the statutory factor level and applying PageRank-based citation weighting, the system prioritizes doctrinally authoritative sources rather than relying solely on semantic similarity. The method uses Chain-of-Thought reasoning and interleaved retrieval steps to better emulate legal reasoning processes. Preliminary testing shows the structured approach achieves significantly higher PageRank scores compared to standard RAG, indicating improved retrieval of doctrinally relevant cases, though with slightly lower textual similarity.

## Method Summary
The system uses a Neo4j knowledge graph to store legal opinions at the statutory factor level (Purpose, Nature, Amount, Market), with nodes representing cases, courts, and factor-specific paragraphs linked by citation and hierarchy relationships. PageRank is computed on both the citation network and court hierarchy to provide authority scores. Retrieval combines cosine similarity, citation authority, and court hierarchy using a convex combination with configurable weights. The LLM performs Chain-of-Thought factor-by-factor analysis before retrieval, and Gemini Flash 2.0 generates the final Fair Use analysis. The prototype was tested on 20 unresolved PACER complaints using 209 cases from 1976-2025.

## Key Results
- Structured RAG yields significantly higher PageRank scores (mean 0.213 vs 0.026) indicating improved retrieval of doctrinally relevant cases
- Textual similarity is somewhat lower in structured RAG (mean 0.521 vs 0.753), reflecting the tradeoff between authority and semantic relevance
- The system successfully retrieves more-cited precedents from higher courts, aligning with how legal authority is traditionally determined

## Why This Works (Mechanism)

### Mechanism 1: Statutory Factor-Level Retrieval Granularity
The system decomposes legal opinions into factor-specific chunks, enabling retrieval that aligns with how courts actually reason rather than retrieving whole documents based on surface similarity. By extracting verbatim paragraphs for each of the four Fair Use statutory factors, the system can retrieve the relevant "Purpose" analysis from a case even if that case's "Market" analysis discusses a different context.

### Mechanism 2: Citation-Network Authority Weighting via PageRank
Incorporating PageRank scores from the legal citation network shifts retrieval toward doctrinally authoritative precedents, even when they have lower semantic similarity to the query. The system computes PageRank over inter-opinion citations and court hierarchy, then combines these with textual similarity in a weighted ranking function.

### Mechanism 3: Interleaved Retrieval with Chain-of-Thought Factor Analysis
Having the LLM first analyze a dispute against the four Fair Use factors before retrieval anchors the search in doctrinal structure, potentially reducing sycophancy and improving relevance. The system performs factor-by-factor analysis to generate structured queries that guide subsequent retrieval, mapping layperson narratives onto legal issue frames.

## Foundational Learning

- **Knowledge Graphs with Typed Relationships**: Why needed: The system uses Neo4j to represent legal structure with nodes and edges. Quick check: Given a Case node with DECIDED_IN → Court and CITED → [Case1, Case2], how would you retrieve all opinions from courts that have appellate authority over the deciding court?
- **PageRank on Directed Graphs**: Why needed: Authority scoring depends on understanding that PageRank flows through citations. Quick check: If opinion A is cited by 100 district court opinions and opinion B is cited once by the Supreme Court, which likely has higher PageRank in a legal citation network?
- **Multi-Objective Retrieval Scoring (Convex Combination)**: Why needed: The ranking function combines three signals with learned or manually specified weights. Quick check: If w_text = 0.8 and w_cit = 0.1, what happens to retrieval when a landmark case uses unusual terminology not present in the query?

## Architecture Onboarding

- **Component map**: Neo4j Knowledge Graph -> Gecko Embeddings -> PageRank Computation -> Ranking Function -> Gemini Flash 2.0 LLM -> Retrieval Pipeline
- **Critical path**: 1) User submits complaint 2) LLM performs factor-by-factor analysis 3) Retrieve top-k chunks from Neo4j using combined scoring 4) Assemble retrieved passages as context 5) LLM generates structured Fair Use analysis with CoT reasoning
- **Design tradeoffs**: Higher w_text retrieves more readable but potentially less authoritative cases; higher w_cit/w_court retrieves authoritative cases that may be less factually similar
- **Failure signatures**: High-PageRank but irrelevant cases (w_cit too high), missing recent precedents (PageRank recency bias), hallucinated factor mappings, sycophantic outputs
- **First 3 experiments**: 1) Weight ablation study with varying (w_text, w_cit, w_court) combinations 2) Recency adjustment test boosting PageRank scores for cases from 2020-2025 3) Factor extraction quality audit sampling 20 opinions

## Open Questions the Paper Calls Out

### Open Question 1
What are the optimal weight configurations (w_text, w_cit, w_court) for the retrieval scoring function, and how does each component contribute to downstream Fair Use analysis quality? The paper plans systematic evaluation through ablation studies to assess individual contributions.

### Open Question 2
Does the tradeoff between higher doctrinal relevance (PageRank) and lower textual similarity in Structured RAG yield better or worse Fair Use analysis outcomes? The paper acknowledges this tradeoff might lead to worse analysis but hasn't directly evaluated analysis quality.

### Open Question 3
Can time-adjusted citation metrics be developed to better capture the emerging influence of recent cases like Warhol v. Goldsmith (2023)? The paper notes PageRank's recency bias as a limitation needing future work.

### Open Question 4
Can a classifier be trained to accurately route cases to appropriate legal doctrine experts when extending the system beyond Fair Use? The paper suggests this will require a routing mechanism for broader legal applications.

## Limitations
- Preliminary testing only on 20 unresolved complaints with no direct assessment of CoT reasoning quality or sycophancy reduction
- PageRank's recency bias means recent landmark cases (e.g., Warhol v. Goldsmith) receive low authority scores despite high doctrinal significance
- Ranking weights are manually specified without empirical optimization through ablation studies

## Confidence
- **High Confidence**: The structured RAG architecture (factor-level chunking, knowledge graph, PageRank weighting) is technically sound and well-described
- **Medium Confidence**: The mechanism by which citation-weighted retrieval improves doctrinal relevance is supported by preliminary results but lacks direct validation against attorney-labeled ground truth
- **Low Confidence**: Claims about CoT reasoning improving legal reasoning processes and reducing sycophancy are not empirically tested in this preliminary work

## Next Checks
1. **Attorney Validation Study**: Have copyright attorneys label top-10 retrieved cases for each complaint on doctrinal relevance and authority, comparing structured vs. standard RAG precision@k and recall@k
2. **Recent Case Retrieval Test**: Implement time-adjusted PageRank and measure whether the system now retrieves recent cases like Warhol v. Goldsmith when topically relevant
3. **Sycophancy Reduction Experiment**: Design test complaints with incorrect legal framing, run both systems, and have attorneys blind to system type rate whether outputs correct or reinforce the incorrect framing