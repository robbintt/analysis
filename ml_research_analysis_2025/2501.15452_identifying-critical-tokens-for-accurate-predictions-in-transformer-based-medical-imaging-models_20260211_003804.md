---
ver: rpa2
title: Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical
  Imaging Models
arxiv_id: '2501.15452'
source_url: https://arxiv.org/abs/2501.15452
tags:
- tokens
- token
- prediction
- vision
- insight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Token Insight, a method for identifying critical
  tokens in transformer-based medical imaging models that influence model predictions.
  The approach uses a principled token discarding mechanism, iteratively removing
  tokens and measuring their impact on prediction confidence until the prediction
  changes.
---

# Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models

## Quick Facts
- **arXiv ID:** 2501.15452
- **Source URL:** https://arxiv.org/abs/2501.15452
- **Reference count:** 30
- **Primary result:** Introduces Token Insight method to identify critical tokens in ViT models for colonic polyp detection, revealing both medically relevant and shortcut-learned features.

## Executive Summary
This paper introduces Token Insight, a method for identifying critical tokens in transformer-based medical imaging models that influence model predictions. The approach uses a principled token discarding mechanism, iteratively removing tokens and measuring their impact on prediction confidence until the prediction changes. The method was evaluated on colonic polyp identification using four different ViT-B/16 models (supervised, DINO, MAE, and scratch) trained on the CP-Child dataset. Results show that Token Insight effectively identifies medically relevant tokens while also revealing cases of shortcut learning where models rely on unintended signals.

## Method Summary
Token Insight is a token discarding mechanism that identifies which tokens are critical for a model's prediction by iteratively removing tokens and measuring their impact on prediction confidence. For each test image, the method computes the confidence for each remaining token when removed individually, then removes the token causing the largest confidence drop. This process repeats until the predicted class changes. The method was evaluated on binary classification of colonic polyps from colonoscopy images using ViT-B/16 models trained on the CP-CHILD dataset. Four model variants were compared: supervised/ImageNet pretrained, DINO pretrained, MAE pretrained, and training from scratch. The evaluation measured both classification accuracy and the effectiveness of Token Insight in identifying critical tokens.

## Key Results
- Token Insight successfully identifies medically relevant tokens for colonic polyp detection across all model variants
- Supervised and DINO models rely on fewer, more impactful tokens compared to MAE and scratch models
- The method reveals cases of shortcut learning where models focus on unintended signals rather than medically relevant features
- Classification accuracy exceeds 95% for all pretrained models, with scratch model achieving 95.3%

## Why This Works (Mechanism)
Token Insight works by systematically identifying which tokens contribute most to a model's prediction confidence. By iteratively removing the token that causes the largest drop in confidence and continuing until the prediction changes, the method reveals which tokens are essential for maintaining the original classification. This greedy approach effectively identifies the critical subset of tokens that the model relies on for its decision, while also exposing when the model has learned to focus on irrelevant features (shortcut learning).

## Foundational Learning
- **Token-based attention mechanisms**: Transformers process images as sequences of tokens (patches) with learned positional embeddings. Understanding how attention weights distribute across tokens is crucial for interpreting model decisions.
  - *Why needed*: Token Insight builds on the fundamental architecture of vision transformers where images are decomposed into patch-based tokens.
  - *Quick check*: Verify that ViT-B/16 produces 196 tokens from 224×224 images with 16×16 patches.

- **Greedy optimization algorithms**: The method uses a greedy search strategy to identify critical tokens by iteratively removing the most impactful token.
  - *Why needed*: Greedy approaches provide computationally tractable solutions to NP-hard subset selection problems.
  - *Quick check*: Confirm that removing tokens one-by-one until prediction changes is more efficient than exhaustive search.

- **Confidence-based evaluation metrics**: Token Insight measures prediction confidence changes rather than just accuracy, providing a more nuanced view of token importance.
  - *Why needed*: Confidence scores reveal the strength of a model's belief in its prediction, not just the final class.
  - *Quick check*: Verify that confidence is extracted from the correct class index and properly normalized.

## Architecture Onboarding

**Component Map:**
ViT-B/16 (224×224 input, 196 tokens) → Classification head (2-class) → Prediction confidence → Token Insight (iterative removal) → Critical tokens

**Critical Path:**
Input image → Patch tokenization (16×16) → Linear projection + positional embeddings → Transformer blocks → CLS token → Classification head → Prediction confidence → Token removal loop

**Design Tradeoffs:**
- Greedy vs. exhaustive search: O(N²) complexity vs. guaranteed optimal solution
- Token zeroing vs. sequence removal: Simpler implementation vs. maintaining proper attention mechanics
- Confidence threshold vs. argmax change: Continuous metric vs. discrete prediction change

**Failure Signatures:**
- All tokens equally important: Suggests model has learned redundant features or is near decision boundary
- No tokens identified as critical: Indicates model may not be using patch-based reasoning
- Inconsistent token removal order: Could indicate numerical instability or improper confidence measurement

**First Experiments:**
1. Verify token count: Confirm ViT-B/16 produces exactly 196 tokens from 224×224 input
2. Test single token removal: Remove one token and verify confidence changes appropriately
3. Validate stopping criterion: Ensure process stops when argmax prediction changes, not just confidence threshold

## Open Questions the Paper Calls Out
- **Theoretical bounds**: The authors plan to investigate theoretical bounds for how well the greedy algorithm captures the smallest set of tokens that influence the prediction. The greedy approach provides no guarantee of finding the minimal critical token set.

- **Computational efficiency**: The authors identify the need to reuse previously acquired information about token impact to avoid making a forward pass for each candidate token, as the current O(N²) complexity is prohibitive for large models.

- **Model selection implications**: The paper notes that depending on the medical imaging task, practitioners may prefer models with sparse token reliance (Supervised/DINO) or dense reliance (MAE/Scratch), but doesn't establish which behavior is clinically superior.

- **Shortcut learning metrics**: While Token Insight can visually identify shortcut learning cases, the authors don't provide quantitative metrics for "shortcut learning" prevalence that correlate with generalization performance.

## Limitations
- **Dataset accessibility**: CP-CHILD dataset requires direct contact with authors and no public repository is specified, limiting reproducibility.
- **Computational cost**: The O(N²) complexity of Token Insight makes it computationally expensive for large models or datasets.
- **Interpretation ambiguity**: Identifying critical tokens doesn't automatically confirm medical relevance versus model artifacts or shortcut features.

## Confidence

**High:** The general approach of iterative token removal and confidence monitoring is sound and reproducible with the given hyperparameters.

**Medium:** The reported accuracy figures are reasonable but sensitive to exact data splits and augmentation.

**Low:** The interpretation of "critical tokens" as medically meaningful is only indirectly validated and could conflate model shortcut features with genuine anatomical relevance.

## Next Checks

1. Request or simulate the CP-CHILD dataset and reproduce training curves for all four model variants to confirm accuracy ranges.

2. Implement Token Insight and run on a small subset of test images, logging the exact sequence of removed tokens and confidence drops; verify stopping occurs when prediction class changes.

3. Compare Token Insight results with standard attention-based interpretability methods (e.g., Grad-CAM, ViT attention maps) on the same images to assess overlap and divergence.