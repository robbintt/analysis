---
ver: rpa2
title: 'It''s All About the Confidence: An Unsupervised Approach for Multilingual
  Historical Entity Linking using Large Language Models'
arxiv_id: '2601.08500'
source_url: https://arxiv.org/abs/2601.08500
tags:
- entity
- candidate
- mhel-llamo
- entities
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MHEL-LLaMo, an unsupervised ensemble approach
  for multilingual historical entity linking that combines a bi-encoder for candidate
  retrieval with an LLM for NIL prediction and candidate selection. The system uses
  confidence scores from the bi-encoder to filter easy samples and apply the LLM only
  for hard cases, reducing computational costs while preventing hallucinations.
---

# It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models

## Quick Facts
- arXiv ID: 2601.08500
- Source URL: https://arxiv.org/abs/2601.08500
- Authors: Cristian Santini; Marieke Van Erp; Mehwish Alam
- Reference count: 24
- Primary result: Unsupervised ensemble approach achieves F1 up to 0.723 on English without fine-tuning

## Executive Summary
This paper introduces MHEL-LLaMo, an unsupervised ensemble method for multilingual historical entity linking that combines a bi-encoder for candidate retrieval with a large language model (LLM) for NIL prediction and candidate selection. The system uses confidence scores from the bi-encoder to filter easy samples and apply the LLM only for hard cases, reducing computational costs while preventing hallucinations. Evaluated on four benchmarks across six European languages from the 19th-20th centuries, MHEL-LLaMo achieves F1 scores up to 0.723 on English (HIPE-2020), 0.662 on French (NewsEye), and 0.698 on Italian (MHERCL), outperforming state-of-the-art models by up to 27% without requiring fine-tuning.

## Method Summary
MHEL-LLaMo uses BELA (a multilingual bi-encoder) to retrieve top-k candidate entities from Wikidata via FAISS k-NN search. The system applies an adaptive threshold to BELA's inner product scores: when scores exceed the threshold, the top candidate is accepted directly; otherwise, an instruction-tuned LLM performs NIL prediction and candidate selection via prompt chaining. The LLM chain first determines if any candidate matches the mention (yes/no), then selects the appropriate candidate if non-NIL. The approach requires no fine-tuning and uses language-specific models (Mistral-24B for most languages, Gemma-27B for Swedish, Poro-8B for Finnish).

## Key Results
- Achieves F1 scores up to 0.723 on English HIPE-2020, 0.662 on French NewsEye, and 0.698 on Italian MHERCL
- Outperforms state-of-the-art models by up to 27% without fine-tuning
- Adaptive threshold mechanism reduces computational costs by limiting LLM invocations to hard cases
- Prompt chaining improves NIL prediction accuracy on datasets with high NIL rates
- NIL prediction accuracy reaches 0.976 on HIPE-2020 and 0.963 on NewsEye

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Based Adaptive Routing
Using bi-encoder confidence scores to selectively invoke LLMs reduces computational cost while maintaining accuracy. BELA's inner product scores serve as a proxy for sample difficulty, with high scores accepted directly and low scores routed to the LLM. This mechanism assumes higher bi-encoder confidence correlates with correct predictions.

### Mechanism 2: Sequential Prompt Chaining for NIL + Selection
Explicitly separating NIL prediction from candidate selection via chained prompts improves performance on datasets with high NIL rates. First asking whether any candidate matches the mention (binary yes/no) before candidate selection reduces LLM bias toward selecting existing candidates.

### Mechanism 3: Metadata-Enriched Candidate Retrieval
Augmenting retrieved candidates with structured Wikidata metadata (label, description, date, type) improves LLM disambiguation accuracy. The lookup table provides language-specific metadata enabling the LLM to leverage temporal and type information during selection.

## Foundational Learning

- **Entity Linking Pipeline**: Mention detection → candidate retrieval → candidate ranking → NIL prediction. Understanding the full pipeline clarifies why confidence routing works. Quick check: Given "Paris" in historical French text, what three pieces of information would distinguish Paris, France from Paris, Texas?

- **Bi-Encoder Dense Retrieval**: Encodes queries and entities into shared embedding space; similarity computed via inner product/cosine distance; k-NN search retrieves nearest neighbors. BELA is the retrieval backbone; confidence scores derive from inner product magnitudes. Quick check: If bi-encoder produces 768-dim embeddings and KB contains 10M entities, what data structure enables sub-second k-NN retrieval?

- **Prompt Chaining**: Sequencing multiple LLM calls where each prompt conditions on prior outputs; enables task decomposition and error isolation. MHEL-LLaMo's NIL→selection chain explicitly separates two subtasks that implicit prompting conflates. Quick check: In a two-prompt chain where prompt A returns "yes/no" and prompt B only runs if A returns "yes," what failure mode does this prevent?

## Architecture Onboarding

- **Component map**: BELA bi-encoder (XLM-R-based, frozen) -> FAISS index of Wikidata entities -> SQLite lookup table -> Adaptive threshold module -> LLM router -> LLM chain (NIL prediction -> candidate selection) -> Wikidata ID or NIL output

- **Critical path**: Input (mention, context, language, document_date, genre) → BELA encodes → mention embedding → FAISS k-NN search → top-k candidates with inner product scores → lookup table enriches candidates with metadata → threshold check → if max(score) ≥ threshold → return top candidate; else → invoke LLM → LLM chain: NIL prediction prompt → (if non-NIL) → candidate selection prompt → Output: Wikidata ID or NIL

- **Design tradeoffs**: Vanilla (MHEL-LLaMo_van) vs. Threshold (MHEL-LLaMo_θ) - vanilla applies LLM to all samples; threshold reduces compute but risks bypassing correctable errors on long-tail entities. Chain vs. Single prompt - chain excels on high-NIL datasets; single prompt preferred on low-NIL domains. Block size (k) - larger k increases recall but adds LLM token cost.

- **Failure signatures**: Nordic languages show poor NIL recall (Swedish 0.184); Finnish F1=0.509; low-NIL domains generate excessive false positives when explicit NIL prediction is applied; KB staleness causes gold annotations to become invalid as Wikidata evolves.

- **First 3 experiments**: 1) BELA-only baseline - measure F1 and record confidence score distribution per language. 2) Threshold calibration - compute median confidence for correct predictions on dev set; validate correlation; select threshold per language. 3) Ablation: chain vs. single - compare F1, NIL precision, and NIL recall on high-NIL subset; quantify bias reduction.

## Open Questions the Paper Calls Out

- Can fine-tuning the bi-encoder candidate retrieval module specifically on historical data improve entity embeddings for MHEL tasks?
- Can supervised algorithms like Gradient Boosted Trees outperform the current inner-product confidence threshold in discriminating between easy and hard samples?
- Can parameter-efficient fine-tuning methods like LoRA reduce computational costs of the LLM component while maintaining performance on low-resource languages?
- Does poor performance on Nordic languages stem primarily from LLM's limited multilingual capabilities or bi-encoder's representation of morphologically complex historical text?

## Limitations

- Limited to 6 European languages from 19th-20th century texts; performance on non-European languages and ancient texts unknown
- LLM dependency remains despite threshold mechanism; performance varies significantly across language models
- Confidence score reliability breaks down for long-tail entities and low-NIL datasets where threshold-based routing underperforms
- Key implementation details underspecified (exact JSON prompt formats, BELA model download locations, threshold calibration procedures)

## Confidence

- **High Confidence**: Overall system architecture effectiveness, confidence-based routing for computational cost reduction, prompt chaining on high-NIL datasets
- **Medium Confidence**: Specific F1 scores (0.723 English, 0.662 French, 0.698 Italian) and improvement magnitude over state-of-the-art
- **Low Confidence**: Nordic language performance and universal applicability of threshold mechanism

## Next Checks

1. Evaluate MHEL-LLaMo on non-European historical languages (Chinese, Arabic, Latin/Greek) to assess KB and model dependence across different language families

2. Conduct systematic analysis of BELA confidence distributions across entity frequency tiers to test whether adaptive thresholds can be learned dynamically based on entity popularity

3. Create synthetic benchmarks with controlled KB incompleteness (removing 10-50% of entities) to measure performance degradation and metadata enrichment effectiveness across different entity types