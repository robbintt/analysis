---
ver: rpa2
title: Evaluating the stability of model explanations in instance-dependent cost-sensitive
  credit scoring
arxiv_id: '2509.01409'
source_url: https://arxiv.org/abs/2509.01409
tags:
- uni00000008
- uni00000013
- idcs
- uni00000018
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the stability of Local Interpretable Model-agnostic
  Explanations (LIME) and SHapley Additive exPlanations (SHAP) when applied to instance-dependent
  cost-sensitive (IDCS) credit scoring models. Using four open-source credit scoring
  datasets, the authors first assess the discriminatory power and cost-efficiency
  of IDCS classifiers, introducing a novel relative Average Expected Cost (relAEC)
  metric for cross-dataset comparability.
---

# Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring

## Quick Facts
- **arXiv ID:** 2509.01409
- **Source URL:** https://arxiv.org/abs/2509.01409
- **Reference count:** 6
- **Primary result:** IDCS classifiers improve cost-efficiency but produce significantly less stable explanations compared to traditional models, with instability increasing under higher class imbalance.

## Executive Summary
This study addresses the stability of Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) when applied to instance-dependent cost-sensitive (IDCS) credit scoring models. Using four open-source credit scoring datasets, the authors first assess the discriminatory power and cost-efficiency of IDCS classifiers, introducing a novel relative Average Expected Cost (relAEC) metric for cross-dataset comparability. They then investigate explanation stability under varying degrees of class imbalance through controlled resampling, measuring stability via Coefficient of Variation (CoV) and Sequential Rank Agreement (SRA). Results reveal that while IDCS classifiers improve cost-efficiency, they produce significantly less stable explanations compared to traditional models, with instability increasing under higher class imbalance. This highlights a critical trade-off between cost optimization and interpretability in credit scoring, underscoring the need to address stability issues in IDCS classifiers for practical deployment.

## Method Summary
The authors evaluate IDCS classifiers against traditional classifiers using four credit scoring datasets. They implement two types of IDCS models: csForest (with cost-sensitive splitting) and csBoost (with cost-sensitive boosting). The study introduces a relAEC metric to normalize cost savings across datasets. For explanation stability, they systematically vary class imbalance through resampling (1% to 30% default rates) while keeping test sets fixed. They compute SHAP and LIME explanations across 25 iterations for each imbalance level, then measure stability using Coefficient of Variation (CoV) for value consistency and Sequential Rank Agreement (SRA) for ranking consistency.

## Key Results
- IDCS classifiers achieve 10-20% better relAEC (cost-efficiency) compared to traditional models
- Explanation stability deteriorates significantly with increasing class imbalance, with IDCS models showing CoV values rising sharply at 1% imbalance
- SHAP and LIME explanations for IDCS models exhibit high SRA values, indicating unstable feature ranking across iterations
- The instability effect is more pronounced for IDCS classifiers than traditional models under the same class imbalance conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Instance-Dependent Cost-Sensitive (IDCS) classifiers improve financial efficiency (relAEC) at the expense of probability calibration, contributing to explanation instability.
- **Mechanism:** Traditional classifiers minimize Cross-Entropy (CE) to approximate $P(Y|X)$, producing well-calibrated probabilities. IDCS classifiers minimize the Average Expected Cost (AEC), which weights errors by financial loss rather than statistical accuracy. This optimization shifts the decision boundary to favor high-cost instances, distorting the probability density functions that post-hoc explainers like SHAP and LIME rely on to generate consistent local approximations.
- **Core assumption:** The paper assumes that minimizing AEC fundamentally alters the model's internal probability mapping compared to CE minimization.
- **Evidence anchors:**
  - [Abstract] "while IDCS classifiers improve cost-efficiency, they produce significantly less stable explanations"
  - [Section 4.1] "Another key difference between traditional and IDCS models is that the latter consistently exhibit poorer performance in terms of the Brier score... indicating poorer calibration."
  - [Corpus] Corpus papers discuss general fairness and explainability but do not address this specific cost-stability trade-off.
- **Break condition:** If a model is trained to minimize AEC but is heavily regularized or calibrated post-hoc (e.g., isotonic regression) specifically to repair Brier scores before explanation, the mechanism may weaken, though the paper notes calibration had limited effect on savings.

### Mechanism 2
- **Claim:** Injecting instance-dependent costs entangles feature importance with cost distributions, increasing noise in feature attributions.
- **Mechanism:** In standard learning, a feature's importance is derived from its correlation with the target $Y$. In IDCS, importance is weighted by the instance-specific cost $C$. If high-cost instances have unique feature characteristics (e.g., large loans have specific attributes), the model learns a decision boundary that is highly sensitive to these specific instances. When explainers perturb features, they encounter a "jagged" loss landscape where small changes trigger large cost-penalized shifts, resulting in high variance in feature importance rankings (SRA) across iterations.
- **Core assumption:** Assumption: The cost distribution is not uniformly distributed and correlates with specific feature patterns (e.g., loan amount), creating a heterogeneous feature-cost interaction.
- **Evidence anchors:**
  - [Section 5] "IDCS classifiers entangle features, labels, and costs... blurring the feature-label relationship."
  - [Section 5] "Perturbing features in SHAP and LIME thus captures not only the influence of x on Y but also the learned interaction with C."
  - [Corpus] Neighboring papers on "Fair and Explainable Credit-Scoring" mention evolving distributions but do not detail this cost entanglement mechanism.
- **Break condition:** If costs are homogeneous across instances (violating the "instance-dependent" premise), the mechanism reduces to standard class-weighted learning, and stability should align with traditional models.

### Mechanism 3
- **Claim:** Class imbalance amplifies explanation instability more severely in IDCS models than in traditional models.
- **Mechanism:** High class imbalance reduces the number of minority class samples. In IDCS, these few samples may carry disproportionately high costs (outliers). The model effectively overfits to the cost distribution of this scarce minority class. During explanation, the sensitivity to these few high-cost points creates "pockets" of instability where feature rankings fluctuate wildly depending on which high-cost instances are influential in a specific perturbation sample, leading to higher Coefficient of Variation (CoV).
- **Core assumption:** Assumption: The instability is driven by the interaction of scarcity (imbalance) and heterogeneity (variance in costs) in the minority class.
- **Evidence anchors:**
  - [Abstract] "instability increasing under higher class imbalance."
  - [Section 4.2.1] "CoV values rise sharply with increasing class imbalance... this effect is even more pronounced for IDCS classifiers."
  - [Corpus] "Implementation of an Asymmetric Adjusted Activation Function" addresses class imbalance but focuses on asymmetric costs rather than explanation stability.
- **Break condition:** If resampling techniques effectively decouple the cost distribution from the class distribution (e.g., by ensuring cost homogeneity within the resampled set), this amplification effect should theoretically diminish.

## Foundational Learning
- **Concept:** **Average Expected Cost (AEC) & relAEC**
  - **Why needed here:** Unlike standard accuracy or AUC, AEC is the specific objective function IDCS models optimize. Understanding relAEC (relative AEC) is critical because it normalizes cost savings against a "no-model" baseline, allowing comparison across datasets with different currency scales or loan amounts.
  - **Quick check question:** If a model predicts a probability of 0.8 for a defaulter (Loan $10k, LGD 0.75) and 0.2 for a non-defaulter (Opportunity Cost $1k), what is the contribution to the AEC?

- **Concept:** **Post-hoc Explanation Stability (CoV & SRA)**
  - **Why needed here:** This paper evaluates models not just on performance, but on the *reliability* of their explanations. The Coefficient of Variation (CoV) measures value consistency, while Sequential Rank Agreement (SRA) measures ranking consistency. High values for either indicate the model's reasoning is inconsistent or untrustworthy.
  - **Quick check question:** If an explanatory feature jumps from rank 1 to rank 10 across different training seeds, which stability metric (CoV or SRA) would primarily capture this discrepancy?

- **Concept:** **Cost Matrix Formulation (FN vs FP)**
  - **Why needed here:** The causal mechanism relies on how costs are defined. False Negatives (granting to defaulters) scale with Loan Amount $\times$ LGD. False Positives (denying good borrowers) scale with lost revenue + alternative investment risk. The asymmetry and variance of these costs drive the model's behavior.
  - **Quick check question:** Why does the paper fix the prior rates ($\pi_0, \pi_1$) for the alternative cost calculation ($C_{alt}$) rather than using the resampled training rates?

## Architecture Onboarding
- **Component map:** Credit Scoring Datasets -> Cost Engine -> Model Layer (Traditional vs IDCS) -> Explanation Layer (LIME/SHAP) -> Evaluation Layer (Performance + Stability metrics)
- **Critical path:**
  1. **Define Costs:** Calculate $C_i(0|1)$ and $C_i(1|0)$ for every instance $i$.
  2. **Resampling Loop:** Iterate through target default rates (1% to 30%). For each rate, resample training data while keeping test set fixed.
  3. **Training:** Fit IDCS models minimizing $\sum AEC$.
  4. **Explanation:** Compute SHAP/LIME values for the fixed test set.
  5. **Stability Check:** Aggregate feature importances across 25 iterations to compute CoV (variance of values) and SRA (variance of rankings).

- **Design tradeoffs:**
  - **Cost-Efficiency vs. Stability:** Choosing IDCS yields ~10-20% better relAEC but significantly higher CoV/SRA.
  - **csForest vs. csBoost:** csForest optimizes cost via splitting criteria, while csBoost optimizes AEC directly. csForest shows better AUC retention but worse relAEC logic.
  - **Resampling Strategy:** The paper fixes the cost priors ($\pi$) to real-world rates during resampling to prevent artificial inflation of costs in heavily downsampled minority classes.

- **Failure signatures:**
  - **High SRA/CoV:** Indicates that the model is learning spurious correlations between costs and features rather than robust signals.
  - **Diverging Brier Score:** If the Brier score is extremely high (>0.2), the probability estimates are likely too distorted for reliable SHAP/LIME interpretations.
  - **Cost Outliers:** Datasets with extreme cost ratios (e.g., CV > 0.8 in SGCS) show erratic stability behavior.

- **First 3 experiments:**
  1. **Cost Sensitivity Baseline:** Train one standard XGBoost and one csBoost on the GMSC dataset. Compare AUC vs. relAEC to confirm the trade-off exists in your implementation.
  2. **Stability Stress Test:** Fix a model (e.g., csLogit) and run the stability experiment at 1%, 10%, and 30% imbalance. Plot the CoV trend to verify it spikes at 1%.
  3. **Feature-Cost Correlation:** Analyze the "Cost Ratio" distribution (Fig 8) for your dataset. If the distribution is smooth (like GMSC), expect lower instability than a skewed distribution (like SGCS).

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the implementation of instance-based Loss Given Default (LGD) or Exposure at Default (EAD) in the cost matrix alter the stability of explanations compared to the fixed-cost matrix used in this study?
- Basis in paper: [explicit] The authors state that "modifying existing cost matrices using instance-based Loss Given Default (LGD) or Exposure at Default (EAD) implementations presents interesting future extensions."
- Why unresolved: This study relied on a specific cost matrix assuming a constant LGD parameter (0.75) to determine misclassification costs, leaving the impact of more complex, instance-based cost structures on explanation stability unknown.
- What evidence would resolve it: A comparative experiment evaluating the Coefficient of Variation (CoV) and Sequential Rank Agreement (SRA) of SHAP/LIME explanations when models are trained with instance-based LGD/EAD costs versus fixed costs.

### Open Question 2
- Question: Do instance-dependent cost-sensitive (IDCS) classifiers require larger datasets or less noisy cost distributions to maintain explanation stability at a given class imbalance rate?
- Basis in paper: [explicit] The conclusion suggests future work could "investigate whether IDCS learning requires larger datasets or less noisy cost distributions to maintain stability at a given imbalance rate."
- Why unresolved: While the study establishes that instability increases with class imbalance, it has not determined if data volume or cost noise reduction can mitigate this effect for IDCS models.
- What evidence would resolve it: Experiments measuring stability metrics across varying training set sizes and controlled levels of cost noise to identify thresholds where IDCS explanations stabilize.

### Open Question 3
- Question: Can stabilized explanation techniques, such as S-LIME or RankSHAP, effectively reduce the instability of explanations generated for IDCS classifiers?
- Basis in paper: [explicit] The authors propose "developing new IDCS models or XAI techniques that provide more stable explanations," specifically citing S-LIME and RankSHAP as recent approaches to investigate.
- Why unresolved: The current study used standard LIME and SHAP implementations, which were found to be significantly unstable for IDCS models; the efficacy of stabilized variants remains untested in this context.
- What evidence would resolve it: Benchmarking the stability (using CoV and SRA) of S-LIME and RankSHAP against standard SHAP and LIME when applied to IDCS classifiers under varying class imbalances.

## Limitations
- The instability findings may not generalize beyond the four credit scoring datasets used, particularly regarding different cost distributions and model architectures
- The paper assumes cost matrices are static and known, which may not hold in dynamic lending environments
- The stability metrics (CoV, SRA) capture variance but not bias, leaving open whether IDCS explanations systematically misrepresent feature importance

## Confidence
- **High:** The core claim of the cost-stability trade-off is supported by consistent relAEC improvements and rising instability metrics across multiple experiments and datasets
- **Medium:** Confidence in the mechanism linking class imbalance to instability, as the resampling design controls for many confounders but cannot fully isolate the cost-feature entanglement effect
- **Low:** Confidence in generalizability to other domains (e.g., insurance, healthcare) without further validation

## Next Checks
1. Test whether post-hoc probability calibration (e.g., Platt scaling) reduces explanation instability without sacrificing relAEC gains
2. Apply the same stability framework to non-financial IDCS problems (e.g., medical diagnosis with asymmetric treatment costs) to test mechanism generalizability
3. Measure explanation bias (e.g., feature attribution skew) alongside variance to determine if IDCS models produce systematically different explanations, not just unstable ones