---
ver: rpa2
title: 'Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher
  Data'
arxiv_id: '2510.03988'
source_url: https://arxiv.org/abs/2510.03988
tags:
- student
- data
- local
- global
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting the best responses
  from multiple teacher models for distilling reasoning capabilities into smaller
  student LLMs. While prior work focuses on prompt selection with single-teacher responses,
  this work explores the underexplored problem of response selection in multi-teacher
  settings.
---

# Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data

## Quick Facts
- arXiv ID: 2510.03988
- Source URL: https://arxiv.org/abs/2510.03988
- Reference count: 16
- Primary result: Local naturalness improves 32B student accuracy by 9.4 percentage points over global selection

## Executive Summary
This paper addresses the challenge of selecting optimal responses from multiple teacher models for reasoning distillation into smaller student LLMs. While prior work focuses on prompt selection, this research explores the underexplored problem of response selection in multi-teacher settings. The authors propose Local Naturalness, a method that evaluates responses based on student model log-probabilities over short sequential reasoning steps rather than global log-probabilities over entire responses. This approach addresses the limitation of global scores failing to correlate with downstream performance when responses come from multiple teachers, especially with long reasoning traces. Experiments demonstrate that Local Naturalness improves a 32B student model's accuracy on math benchmarks by 9.4 percentage points over global selection and surpasses the performance of training on the single best teacher's data.

## Method Summary
The method computes local log probabilities by averaging per-sentence log-probabilities, where each sentence is conditioned on at most k=4 preceding sentences. For each prompt, multiple teacher models generate responses using temperature 0.6 and top-p 0.95. The student model evaluates each response by splitting it into sentences and computing log-probabilities for each sentence conditioned on the prompt plus previous k sentences. Responses are selected based on highest average local score, and the student is fine-tuned using LLaMA-Factory with specified hyperparameters. The approach enables both teacher selection (by aggregating scores across prompts) and response selection (by picking the highest-scoring response per prompt across all teachers).

## Key Results
- Local naturalness improves 32B student accuracy by 9.4 percentage points over global selection on math benchmarks
- The method reliably identifies the most helpful teacher and consistently selects superior responses
- Training on locally-selected responses outperforms training on the single best teacher's data
- Average local log-probabilities from as few as 200 prompts were sufficient to reliably rank teacher models

## Why This Works (Mechanism)

### Mechanism 1
Student models trained on shorter contexts struggle with long-context consistency (10K+ tokens). Evaluating sentence-level probabilities within a limited context window captures step-level "naturalness" that better reflects what the student can actually learn, avoiding compounding errors from full-sequence evaluation. This assumes reasoning quality is compositional—good reasoning emerges from chaining accurate local inferences rather than global coherence alone.

### Mechanism 2
Aggregating local log-probabilities across prompts reliably identifies the most helpful teacher for a given student. Each teacher's data receives an average local naturalness score from the student's perspective, predicting which teacher's full dataset will yield best post-SFT performance. This assumes teacher helpfulness is student-specific—the same teacher isn't optimal for all students.

### Mechanism 3
Per-prompt response selection using local naturalness outperforms training on any single teacher's data. For each prompt, the response with highest local log-prob across all teachers is selected, cherry-picking the most learnable reasoning