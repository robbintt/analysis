---
ver: rpa2
title: 'MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language
  for Activities of Daily Living'
arxiv_id: '2504.20505'
source_url: https://arxiv.org/abs/2504.20505
tags:
- activity
- sensor
- dataset
- mural
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MuRAL, the first multi-resident ambient sensor
  dataset annotated with natural language, designed to enable large language models
  (LLMs) for human activity recognition (HAR) in smart homes. Unlike prior datasets,
  MuRAL includes over 21 hours of data from 21 sessions, with fine-grained action
  descriptions, resident identities, and high-level activity labels in realistic,
  dynamic multi-resident scenarios.
---

# MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living

## Quick Facts
- arXiv ID: 2504.20505
- Source URL: https://arxiv.org/abs/2504.20505
- Reference count: 26
- Primary result: First multi-resident ambient sensor dataset annotated with natural language for activity recognition in smart homes

## Executive Summary
MuRAL is the first multi-resident ambient sensor dataset annotated with natural language descriptions, designed to enable large language models (LLMs) for human activity recognition in smart homes. The dataset contains over 21 hours of data from 21 sessions with fine-grained action descriptions, resident identities, and high-level activity labels in realistic multi-resident scenarios. Unlike prior datasets focused on single residents or lacking natural language annotations, MuRAL provides the contextual richness needed for LLM-based reasoning about who did what and when in shared living spaces. The authors demonstrate that while LLMs can provide rich semantic interpretations of sensor data, they still struggle with multi-user ambiguity and under-specified sensor contexts, highlighting the need for improved sensor coverage and context reasoning for real-world HAR applications.

## Method Summary
The authors collected MuRAL using two residents in a smart home environment equipped with binary sensors (door, motion, item usage) and a video camera for annotation. Each session lasted approximately one hour and involved natural interactions between residents performing daily activities. The dataset includes sensor logs timestamped to the second, along with detailed natural language annotations describing actions (e.g., "Alice opened the refrigerator"), subject assignments (which resident performed each action), and high-level activity labels (e.g., "preparing dinner"). The authors used LLMs to benchmark three tasks: subject assignment (identifying which resident performed each action), action description (generating natural language descriptions from sensor sequences), and activity classification (categorizing sequences into predefined activities). Evaluation metrics included accuracy, cosine similarity for semantic similarity, and F1 scores across these tasks.

## Key Results
- Subject assignment accuracy reached 0.826 using LLM-based approaches
- Action description cosine similarity was approximately 0.56 between generated and ground truth descriptions
- Activity classification accuracy was low at 0.186, highlighting challenges in multi-resident scenarios
- LLMs showed strong ability to provide rich semantic interpretations but struggled with under-specified sensor contexts and multi-user ambiguity

## Why This Works (Mechanism)
MuRAL works by providing the contextual richness and multi-user complexity that single-resident datasets lack, enabling LLMs to reason about social interactions and shared activities in smart homes. The natural language annotations bridge the gap between low-level sensor events and high-level human activities, allowing LLMs to leverage their semantic understanding capabilities. The dataset's realistic scenarios with multiple residents create the ambiguity and complexity needed to test whether LLMs can handle real-world smart home environments where multiple people interact with shared spaces and resources.

## Foundational Learning
- **Multi-resident activity recognition**: Understanding activities involving multiple people is crucial for realistic smart home applications; quick check: can the system distinguish between joint activities and parallel individual activities?
- **Natural language sensor annotation**: Bridging sensor data and human-readable descriptions enables LLM reasoning; quick check: are annotations consistent and detailed enough for semantic understanding?
- **Context reasoning in sensor streams**: LLMs must infer missing context from sparse sensor data; quick check: does the system handle under-specified scenarios where not all actions are sensor-triggered?
- **Subject assignment in shared spaces**: Identifying which resident performed each action is fundamental for multi-user HAR; quick check: can the system disambiguate between residents when they're in the same location?
- **Semantic similarity evaluation**: Using cosine similarity to measure description quality provides quantitative assessment of LLM outputs; quick check: does the metric capture meaningful differences in description quality?

## Architecture Onboarding
**Component Map**: Sensors -> Sensor Stream -> LLM Reasoning Engine -> Subject Assignment/Action Description/Activity Classification
**Critical Path**: Sensor data collection → Natural language annotation → LLM processing → Performance evaluation
**Design Tradeoffs**: The dataset prioritizes realistic multi-resident scenarios over extensive sensor coverage, accepting sparse data in exchange for ecological validity. This creates challenges for LLM reasoning but better reflects real-world conditions.
**Failure Signatures**: Low activity classification accuracy (0.186) indicates that current approaches struggle when sensor coverage is insufficient to disambiguate multi-resident activities. Subject assignment failures occur when residents are in close proximity or share sensor triggers.
**First Experiments**: 1) Evaluate single-resident subset performance to establish baseline LLM capabilities, 2) Test different prompt engineering strategies for subject assignment to improve accuracy, 3) Analyze which sensor types contribute most to successful activity recognition in multi-resident scenarios.

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the scalability of LLM-based approaches to longer-duration scenarios, the impact of different sensor configurations on multi-resident recognition performance, and the potential for combining LLMs with traditional HAR techniques. The authors also note the need for more diverse multi-resident scenarios and larger datasets to train more robust models capable of handling the complexity of real-world smart homes.

## Limitations
- The dataset is relatively small with only 21 sessions totaling 21 hours, limiting training data for LLMs
- Sensor coverage is sparse, with many multi-resident activities lacking sufficient sensor triggers for complete context
- Annotation process relied on video review by research team members, introducing potential subjectivity
- Evaluation focuses primarily on in-domain performance without extensive cross-dataset validation

## Confidence
- **High Confidence**: The claim that MuRAL is the first multi-resident ambient sensor dataset with natural language annotations is well-supported
- **Medium Confidence**: LLM performance metrics on subject assignment and action description are reliable within the dataset but may not generalize to other scenarios
- **Low Confidence**: Activity classification results (accuracy 0.186) are particularly uncertain due to limited sensor coverage and high ambiguity in multi-resident scenarios

## Next Checks
1. **Cross-dataset generalization**: Evaluate MuRAL-trained LLMs on established single-resident datasets (e.g., CASAS) to assess transfer learning capabilities
2. **Sensor coverage optimization**: Conduct ablation studies removing different sensor types to determine which sensors are most critical for multi-resident activity disambiguation
3. **Long-duration scenario testing**: Extend evaluation to multi-hour continuous sessions to assess LLM performance on activity recognition over extended time periods and test for temporal consistency