---
ver: rpa2
title: 'BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research
  Agent'
arxiv_id: '2508.06600'
source_url: https://arxiv.org/abs/2508.06600
tags:
- search
- retrieval
- answer
- documents
- bm25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BrowseComp-Plus, a benchmark for evaluating
  deep-research agents that addresses fairness, transparency, and accessibility issues
  in existing benchmarks. It provides a fixed, human-verified corpus with supporting
  and challenging negative documents, enabling controlled experimentation.
---

# BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent

## Quick Facts
- arXiv ID: 2508.06600
- Source URL: https://arxiv.org/abs/2508.06600
- Reference count: 40
- Proprietary models like GPT-5 achieve 70.1% accuracy with Qwen3-Embedding-8B retrieval

## Executive Summary
BrowseComp-Plus introduces a controlled benchmark for evaluating deep-research agents, addressing fairness and transparency issues in existing benchmarks. The benchmark provides a fixed corpus with human-verified relevance labels, enabling researchers to disentangle retrieval effectiveness from reasoning capabilities. Evaluations show that stronger retrievers significantly improve both accuracy and efficiency, with proprietary models outperforming open-source alternatives primarily due to better tool-use orchestration rather than fundamental comprehension differences.

## Method Summary
The benchmark consists of 830 queries and 100,195 documents with human-verified evidence documents, gold documents, and hard negatives. Agents interact with retrievers (BM25 or neural embeddings) to search for relevant documents, then use LLMs to generate answers. Evaluation uses LLM-as-judge (gpt-4.1) for answer correctness and standard IR metrics for retrieval. The controlled corpus enables component-level analysis by providing complete evidence sets and challenging negatives for each query.

## Key Results
- GPT-5 achieves 70.1% accuracy with Qwen3-Embedding-8B retrieval
- Stronger retrievers reduce search calls by 1-3 while improving accuracy
- Open-source models lag significantly due to tool-use limitations, not comprehension
- Oracle retrieval shows corpus completeness with 93.49% accuracy for GPT-4.1

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Stronger neural retrievers improve deep-research agent accuracy while reducing search iterations.
- **Mechanism:** Higher-precision early retrieval surfaces relevant evidence faster, reducing the need for follow-up queries. Better retrievers return more informative documents that agents can cite and reason over effectively.
- **Core assumption:** Agent reasoning capability is not the primary bottleneck when retrieval quality is poor.
- **Evidence anchors:** [abstract] "Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls." [Section 4.7] "For most proprietary models, Qwen3-Embedding-8B reduces search calls by approximately 1–3 compared to BM25... better retrieval not only improves effectiveness (accuracy) but also efficiency (fewer tool calls)."

### Mechanism 2
- **Claim:** Fixed, human-verified corpora enable disentangled evaluation of retriever vs. agent contributions.
- **Mechanism:** By controlling the document collection and providing explicit relevance judgments (evidence documents, gold documents, hard negatives), researchers can measure retrieval effectiveness independently using standard IR metrics (Recall@k, nDCG@k) and isolate whether performance bottlenecks are in retrieval or reasoning.
- **Core assumption:** The curated corpus provides complete evidence for the reasoning chain required to answer each question.
- **Evidence anchors:** [abstract] "Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation." [Section 3.2.2] "Annotators are asked to: 1. Confirm that each clue is sufficiently justified by the supporting documents... 2. Determine whether the combination of clues and supporting evidence enables a human to answer the entirety of the question correctly."

### Mechanism 3
- **Claim:** Open-source models' primary limitation is interleaved reasoning-tool use, not document comprehension.
- **Mechanism:** When given all relevant documents directly (oracle setting), open-source models approach proprietary model performance (Qwen3-32B: 83.25% vs gpt-4.1: 93.49%). However, without oracle retrieval, open-source models make fewer search calls (1–2 vs 20+) and achieve lower accuracy, indicating weaker tool-use orchestration.
- **Core assumption:** Context window and reasoning over provided documents are sufficient for open-source models.
- **Evidence anchors:** [Section 4.6] "closed-source agents call the search tool more frequently than open-source models... OpenAI's gpt-5 and o3 issue an average of more than 20 search calls per query, while Qwen3-32B and SearchR1-32B make fewer than 2." [Section 4.8.1] "The effectiveness gap between Qwen3-32B and gpt-4.1 in this [oracle] setting is notably smaller than the gap observed in the non-oracle setting."

## Foundational Learning

- **Concept: Cranfield Evaluation Paradigm**
  - Why needed here: BrowseComp-Plus adopts this classical IR evaluation approach—fixed test collection, queries, and relevance judgments—to enable reproducible, component-level evaluation.
  - Quick check question: Can you explain why Recall@k and nDCG@k require pre-computed relevance judgments (qrel files)?

- **Concept: Dense vs. Sparse Retrieval**
  - Why needed here: The paper compares BM25 (sparse, lexical) against neural embedding retrievers (Qwen3-Embedding, ReasonIR). Understanding vector similarity search vs. term-frequency scoring is essential.
  - Quick check question: Why might BM25 benefit from a larger corpus (better IDF estimation) while dense retrievers show apparent degradation due to unjudged documents?

- **Concept: Test-Time Scaling in Agents**
  - Why needed here: Proprietary reasoning models (o3, gpt-5) use more search calls at higher reasoning effort settings, trading compute for accuracy. This is a key efficiency consideration.
  - Quick check question: What happens to search call count when reasoning effort increases from "low" to "high" in gpt-oss models?

## Architecture Onboarding

- **Component map:** Query → Retriever (BM25/Qwen3-Embedding) → Top-5 documents (truncated to 512 tokens) → Agent reasoning → Answer generation → LLM-as-judge evaluation
- **Critical path:** 1. Query → Retriever returns top-k documents (k=5, truncated to 512 tokens) 2. Agent performs iterative search planning and reasoning 3. Agent generates answer with citations 4. Judge compares extracted answer to ground truth
- **Design tradeoffs:** Corpus size (100K vs. web-scale): Smaller corpus enables controlled experiments but may not reflect real-world retrieval difficulty; Document truncation (512 tokens): Budget constraint; 86.5% of gold answers still retrievable; Tool access (search-only vs. search + get-document): Full-document reading helps strong models more (gpt-4.1: +8% accuracy)
- **Failure signatures:** Low search call count + low accuracy → Tool-use/orchestration failure (open-source models); High search calls + low accuracy → Retriever returning irrelevant documents (BM25 baseline); High recall + low accuracy → Agent reasoning failure over retrieved evidence; Oracle accuracy < 90% → Corpus completeness issue
- **First 3 experiments:** 1. Retriever ablation: Run same agent (e.g., gpt-4.1) with BM25 vs. Qwen3-Embedding-8B. Measure accuracy delta and search call reduction. 2. Oracle baseline: Provide all evidence documents directly to agent. If accuracy approaches 90%+, corpus is sufficient; bottleneck is retrieval. 3. Reasoning effort sweep: Test gpt-oss-120B at low/medium/high settings. Plot accuracy vs. search calls to characterize efficiency frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does retriever quality affect the learning dynamics of Deep-Research agents during training or reinforcement learning optimization?
- Basis in paper: [explicit] Section 5 states: "Understanding how retriever quality affects the learning dynamics of an agent remains an open question."
- Why unresolved: Current work only examines retriever effects at inference time, not during agent training where weaker retrievers may hinder policy learning.
- What evidence would resolve it: Training agents (e.g., Search-R1) with retrievers of varying quality and measuring convergence speed, final performance, and sample efficiency.

### Open Question 2
- Question: How well does an agent's tool-use capability generalize when switched from one retriever to another (out-of-distribution transfer)?
- Basis in paper: [explicit] Section 5 asks: "If an agent is optimized using a BM25 search tool, how well does its performance generalize when switched to an embedding-based search tool?"
- Why unresolved: Agents may overfit to specific retrieval characteristics during optimization, but cross-retriever generalization has not been tested.
- What evidence would resolve it: Train agents with BM25, evaluate with neural retrievers (and vice versa), comparing performance drops.

### Open Question 3
- Question: Can retrieval models be co-optimized with LLM agents to maximize end-to-end answer accuracy rather than traditional relevance metrics?
- Basis in paper: [explicit] Section 5 proposes: "Retrieval models could be co-optimized with the agent for achieving overall answer accuracy, rather than developed and evaluated in isolation."
- Why unresolved: Current retrievers optimize for human relevance judgments, not for downstream agent consumption and reasoning needs.
- What evidence would resolve it: Joint training experiments showing whether agent-aware retrievers outperform standard retrievers on final answer accuracy.

## Limitations
- Fixed 100K document corpus may not reflect real-world retrieval challenges with document freshness and scale
- Document truncation to 512 tokens impacts ~13.5% of gold documents and may affect reasoning for longer documents
- LLM-as-judge evaluation introduces potential biases and may not perfectly align with human judgment

## Confidence
- High confidence: Retrieval quality directly impacts both accuracy and efficiency (fewer search calls), as demonstrated across multiple models and retriever configurations
- Medium confidence: Fixed corpus with human-verified relevance enables fair comparison, though real-world generalization remains untested
- Medium confidence: Open-source models' primary limitation is tool-use orchestration rather than comprehension, based on oracle experiment evidence
- Low confidence: Absolute accuracy numbers for proprietary models (70.1% for GPT-5) due to unknown API configurations and reasoning effort settings

## Next Checks
1. **Retrieval effectiveness validation**: Run controlled ablation experiments comparing BM25 vs. Qwen3-Embedding-8B retrievers with the same agent to verify that retrieval improvements consistently reduce search calls while maintaining or improving accuracy
2. **Oracle completeness verification**: Test whether providing all evidence documents directly to open-source models (Qwen3-32B) achieves accuracy close to the reported 83.25%, confirming that tool-use limitations are the primary performance gap
3. **Context window stress test**: Systematically evaluate document truncation impact by comparing full-document vs. truncated-document retrieval for a subset of queries, measuring accuracy degradation and identifying failure patterns