---
ver: rpa2
title: 'Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory
  Replay and Concept-Basis Reconstruction'
arxiv_id: '2601.16034'
source_url: https://arxiv.org/abs/2601.16034
tags:
- refusal
- concept
- arxiv
- donor
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for transferring refusal behavior
  from one LLM to another without using target-side refusal supervision. The method,
  called Trajectory Replay via Concept-Basis Reconstruction, extracts a "refusal recipe"
  from a donor model by expressing the refusal direction as a weighted combination
  of concept atoms.
---

# Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction

## Quick Facts
- arXiv ID: 2601.16034
- Source URL: https://arxiv.org/abs/2601.16034
- Reference count: 40
- Key outcome: This paper proposes a framework for transferring refusal behavior from one LLM to another without using target-side refusal supervision. The method, called Trajectory Replay via Concept-Basis Reconstruction, extracts a "refusal recipe" from a donor model by expressing the refusal direction as a weighted combination of concept atoms. This recipe is then reconstructed in the target model's semantic space using layer alignment via dynamic time warping and a weight-SVD stability guard to prevent capability collapse. Evaluation across 8 model pairs, including dense-to-MoE and dense-to-reasoning transfers, shows consistent attenuation of refusal (e.g., from ~98% to ~1-2% refusal rate) while maintaining capability (e.g., ~70-81% accuracy on GSM8K and ~55-65% pass@1 on MBPP). The results support the hypothesis that refusal circuits are semantically universal across diverse LLM architectures and training regimes.

## Executive Summary
This paper introduces Trajectory Replay via Concept-Basis Reconstruction (TR-CBR), a white-box method for transferring refusal circuit ablation from a donor LLM to a target LLM without requiring target-side refusal supervision. The approach leverages the hypothesis that refusal directions decompose into model-agnostic coefficient mixtures over shared concept atoms that transfer across architectures. By reconstructing these directions in the target's semantic space using layer alignment and applying a weight-SVD stability guard to prevent capability collapse, the method achieves consistent refusal attenuation while preserving task performance across diverse model pairs. The results provide empirical support for semantic universality of refusal circuits across LLM architectures.

## Method Summary
The method extracts a "refusal recipe" from a donor model by expressing the refusal direction as a weighted combination of concept atoms. This recipe is reconstructed in the target model's semantic space using layer alignment via dynamic time warping and a weight-SVD stability guard to prevent capability collapse. The approach operates entirely on benign data for the target model (B0 budget constraint), making it applicable to safety-critical deployment scenarios. Evaluation across 8 model pairs shows consistent attenuation of refusal behavior while maintaining capability on standard benchmarks.

## Key Results
- Refusal attenuation from ~98% to ~1-2% across all 8 tested model pairs
- Capability preservation: GSM8K accuracy maintained at ~70-81%, MBPP pass@1 at ~55-65%
- Successful transfer across architectural divergences (dense-to-MoE, dense-to-reasoning)
- Weight-SVD guard prevents capability collapse (no-guard control shows catastrophic GSM8K drop of -24.1)

## Why This Works (Mechanism)

### Mechanism 1: Concept-Space Recipe Transfer
- **Claim:** Refusal directions decompose into model-agnostic coefficient mixtures over shared concept atoms that transfer across architectures.
- **Mechanism:** The donor refusal vector r_D is expressed as r_D ≈ A_D w via ridge regression. The same coefficient vector w reconstructs a functionally equivalent direction in the target: r_T ≈ A_T w. This works because concept atoms form a shared semantic basis spanning the refusal neighborhood.
- **Core assumption:** Locally linear mapping from latent semantic mixtures into residual-stream space (Appendix D); CAR spans the refusal subspace.
- **Evidence anchors:**
  - [abstract]: "reconstructing refusal directions using a shared 'recipe' of concept atoms"
  - [Section 4.1]: Eq. 2 formalizes semantic recipe invariance
  - [corpus]: Wang et al. (2025) "Refusal Direction is Universal Across Safety-Aligned Languages" supports cross-lingual universality (FMR 0.526)
- **Break condition:** If CAR atoms don't span the refusal neighborhood, or if target architecture implements non-linear semantic mappings, reconstruction fails.

### Mechanism 2: Geometric Layer Alignment
- **Claim:** Layer correspondence across models can be established via correlation structure of concept atoms, not anatomical position.
- **Mechanism:** Compute normalized Gram fingerprints G^(ℓ) from concept atom matrices. Use Dynamic Time Warping on distance matrix M_ij = 1 - cosine(G_D, G_T) to find monotonic layer mapping π. This preserves relative topological relationships between concepts across depths.
- **Core assumption:** Concept relationship geometry evolves similarly with depth across architectures.
- **Evidence anchors:**
  - [Section 5.1]: Formalizes Gram fingerprint computation (Eq. 3) and DTW alignment
  - [Figure 2]: Shows strong diagonal in layer alignment heatmap, implying preserved topological relationships
  - [corpus]: Kornblith et al. (2019) CKA/SVCCA methods motivate cross-network representation comparison
- **Break condition:** If target and donor have fundamentally different depth-wise semantic processing (e.g., early vs. late refusal emergence), alignment distorts transfer.

### Mechanism 3: Weight-SVD Stability Guard
- **Claim:** Capability collapse can be prevented by projecting intervention directions away from high-variance weight subspaces.
- **Mechanism:** Compute SVD of target weight matrix W = UΣV^T. Project intervention: r_safe = (I - V_{1:k} V_{1:k}^T) r, where V_{1:k} are top-k right singular vectors. This removes components aligned with principal weight-space directions hypothesized to encode core capabilities.
- **Core assumption:** Core capabilities (syntax, logic) inhabit high-variance weight subspace; refusal inhibition occupies lower-rank subspaces.
- **Evidence anchors:**
  - [Section 5.3]: Eq. 6 defines the guard; Overlap Energy metric quantifies interference risk
  - [Figure 5]: Shows guarded method maintains stability at high γ while unguarded collapses perplexity
  - [Table 3]: No-guard control causes catastrophic GSM8K drop (-24.1) despite stronger refusal attenuation
  - [corpus]: Zhang et al. (2024) AlphaEdit null-space constraints provide related approach
- **Break condition:** If capability-critical features inhabit low-variance subspaces (e.g., long-tail knowledge), guard may be insufficient.

## Foundational Learning

- **Contrastive Activation Directions:**
  - **Why needed here:** All atoms and refusal vectors are computed as mean activation differences between prompt sets (P⁺ vs P⁻).
  - **Quick check question:** Given two prompt sets, can you compute the direction r = μ(P⁺) - μ(P⁻) and explain what it captures?

- **Ridge Regression for Basis Decomposition:**
  - **Why needed here:** Used to express refusal vectors as weighted combinations of concept atoms (Eq. 4) and clean dirty directions.
  - **Quick check question:** Why does ridge regression (vs. OLS) help when decomposing directions over potentially correlated atoms?

- **Dynamic Time Warping:**
  - **Why needed here:** Aligns layers across models with different depths while enforcing monotonicity and continuity.
  - **Quick check question:** If donor has 32 layers and target has 24, how does DTW prevent invalid (non-monotonic) layer mappings?

## Architecture Onboarding

- **Component map:**
  1. **Concept Atom Registry (CAR):** 20 concept directions per layer, computed from contrastive prompt pairs
  2. **Gram Fingerprint Module:** Computes correlation structure G^(ℓ) = Â^T Â for layer alignment
  3. **DTW Layer Mapper:** Produces π: donor layers → target layers
  4. **Ridge Coefficient Encoder:** Solves w from donor: r_D ≈ A_D w
  5. **Target Reconstructor:** Computes r̃_T = A_T w at mapped layers
  6. **SVD Guard:** Projects r̃_T away from top-k weight singular vectors
  7. **Replay Engine:** Applies rank-one suppression W ← W - γ(Wr/||r||²)r^T

- **Critical path:** CAR construction (shared vocab) → Gram computation (both models) → DTW alignment → Donor coefficient extraction → Target reconstruction → SVD guard → Rank-one edit application. All target-side choices use only benign data (budget B₀).

- **Design tradeoffs:**
  - **CAR size (m=20):** Larger provides better span but increases computation; smaller risks misspecification
  - **Guard ratio ρ:** Higher prevents capability damage but may attenuate intervention effectiveness; selected per-target via benign perplexity drift
  - **Edit strength γ:** Fixed at 2.0 for robustness demonstration; sweeping requires benign drift as sole criterion

- **Failure signatures:**
  - **Misalignment failures:** Wrong-map control causes -12.5 GSM8K drop (Table 3)—edits hit wrong layers
  - **Subspace interference:** No-guard causes perplexity collapse (+1.02) with catastrophic capability loss
  - **CAR misspecification:** If refusal operates on features orthogonal to atoms, reconstruction fails silently
  - **Token-position sensitivity:** Paper uses final user prompt token; first response token may yield different results

- **First 3 experiments:**
  1. **Reproduce in-family transfer** (e.g., Qwen3-VL-2B → Qwen3-VL-8B) to validate pipeline with expected high spectral agreement (ρ > 0.85)
  2. **Run No-guard ablation** on a single pair to observe capability collapse firsthand; compare GSM8K before/after
  3. **Test CAR coverage sensitivity:** Remove 5 atoms from registry and measure transfer degradation; if minimal, atoms may be redundant

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can recipe-level transfer generalize to other alignment-relevant behaviors (e.g., sycophancy, hallucination suppression, persona adherence) while preserving the B0 budget constraint?
- **Basis in paper:** [explicit] "If refusal circuits are transferable semantic objects, then other alignment-relevant behaviors may also admit recipe-level transfer. A direct next step is to expand CAR coverage and test whether transfer generalizes beyond refusal directions to multi-behavior editing."
- **Why unresolved:** The paper only validates transfer for refusal circuits; other alignment behaviors may have different semantic structures or dimensionality.
- **What evidence would resolve it:** Successful cross-model transfer of non-refusal alignment behaviors using the same Trajectory Replay protocol with B0 compliance.

### Open Question 2
- **Question:** How can universality auditing be extended to closed-source proprietary models without weight-level access?
- **Basis in paper:** [explicit] "Future work should explore black-box approximations of these semantic fingerprints—potentially using steering vectors derived from API logprobs or contrastive decoding—to extend universality auditing to closed-source proprietary systems."
- **Why unresolved:** The method is strictly white-box, requiring direct weight and activation access.
- **What evidence would resolve it:** A black-box proxy for layer alignment and direction reconstruction that correlates with white-box transfer success rates.

### Open Question 3
- **Question:** Does the Weight-SVD stability guard adequately preserve capabilities in tasks relying on long-tail knowledge or rote memorization?
- **Basis in paper:** [inferred] The authors note the guard "may not generalize to tasks relying on long-tail knowledge or rote memorization, which can inhabit lower-rank subspaces."
- **Why unresolved:** Experiments only evaluated GSM8K and MBPP; the principal-component heuristic was not stress-tested on knowledge-intensive benchmarks.
- **What evidence would resolve it:** Transfer experiments on benchmarks like TriviaQA or MMLU showing whether capability preservation holds.

## Limitations
- CAR registry relies on manually constructed contrastive prompt sets with harmful-related atoms withheld from public release
- Weight-SVD guard assumes capability-critical features inhabit high-variance subspaces (may fail for long-tail knowledge)
- Cross-architectural layer alignment assumes monotonic evolution of concept relationships (may violate for models with different depth-wise processing)
- Success on 8 model pairs doesn't establish universality; failure cases remain unexplored

## Confidence
**High Confidence (4-5/5):**
- CAR construction methodology (contrastive activation differences, ridge regression)
- Weight-SVD guard implementation and its role in preventing capability collapse
- Quantitative results showing refusal attenuation across all tested pairs
- Layer alignment via DTW on Gram fingerprints as a valid cross-model comparison method

**Medium Confidence (2-3/5):**
- Semantic universality claim: While supported by consistent results, the underlying assumption that concept atoms form a shared basis across architectures needs more rigorous validation
- CAR atom relevance: Without access to harmful-related atoms, we cannot verify they span the refusal subspace
- Guard's sufficiency: The assumption about capability-critical features inhabiting high-variance subspaces is plausible but not proven

**Low Confidence (0-1/5):**
- Generalization to extreme architectural divergences (e.g., Mamba, RWKV, transformers with vastly different depth-width ratios)
- Performance on non-English languages despite Wang et al. (2025) showing FMR 0.526 cross-lingually
- Behavior when CAR atoms miss critical refusal features (silent failure mode)

## Next Checks
1. **CAR Coverage Sensitivity Analysis**: Systematically remove 5 random atoms from the registry and measure transfer degradation. If performance remains stable, atoms may be redundant; if degradation occurs, CAR completeness is critical.

2. **Architecture Stress Test**: Apply the method to extreme architectural divergences (e.g., dense-to-Mamba or transformer-to-RWKV) to identify the boundary of claimed universality. Monitor layer alignment quality and reconstruction fidelity.

3. **Failure Mode Exploration**: Intentionally misalign layers (swap π mapping) or remove the SVD guard to observe and characterize failure modes. Document capability collapse patterns and refusal attenuation limits.