---
ver: rpa2
title: 'Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted
  Motorsports'
arxiv_id: '2504.11717'
source_url: https://arxiv.org/abs/2504.11717
tags:
- safety
- uni00000011
- uni00000013
- hcsf
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-centered safety filter (HCSF) for
  shared autonomy that enhances system safety without compromising human agency. The
  method builds on a neural safety value function learned through black-box interactions
  and enforces safety via a novel state-action control barrier function (Q-CBF) that
  does not require system dynamics.
---

# Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports

## Quick Facts
- arXiv ID: 2504.11717
- Source URL: https://arxiv.org/abs/2504.11717
- Reference count: 40
- This paper introduces a human-centered safety filter (HCSF) that enhances system safety without compromising human agency, validated through a large-scale user study (83 participants) using Assetto Corsa racing simulator.

## Executive Summary
This paper presents a human-centered safety filter (HCSF) for shared autonomy that enhances system safety without compromising human agency. The method builds on a neural safety value function learned through black-box interactions and enforces safety via a novel state-action control barrier function (Q-CBF) that does not require system dynamics. Unlike conventional safety filters that abruptly override human inputs, HCSF smoothly modifies actions to maintain safety while preserving human control. The approach was validated in a large-scale user study (83 participants) using the high-fidelity Assetto Corsa racing simulator. Results show that HCSF significantly improves safety and user satisfaction compared to no assistance, and outperforms a conventional safety filter on human agency, comfort, and satisfaction while maintaining at least equal robustness. The method is fully model-free, scalable to high-dimensional systems, and the first safety filter applied to black-box shared autonomy with human operators.

## Method Summary
The HCSF learns a safety value function V(x) using SAC-based reinforcement learning on a black-box simulator (Assetto Corsa). The safety margin function g(x) is defined as the minimum distance to track boundary or nearest opponent. The filter enforces safety by solving an optimization problem that finds the action closest to the human's input while satisfying the Q-CBF constraint Q(x,u) ≥ γV(x), where γ=0.7. The method uses a warmup phase with pretrained nominal and overtaking policies, followed by initialization phases with adversarial/random/mixed sampling schemes to populate failure-prone states. During deployment, the filter samples 2000 candidate actions along a line from the human input to a safe alternative, selecting the minimum deviation that satisfies the safety constraint.

## Key Results
- HCSF significantly improves safety and user satisfaction compared to no assistance in a large-scale user study (83 participants)
- HCSF outperforms conventional safety filters on human agency, comfort, and satisfaction while maintaining at least equal robustness
- The method is fully model-free and scalable to high-dimensional systems, representing the first safety filter applied to black-box shared autonomy with human operators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing intervention magnitude preserves human agency while maintaining safety bounds.
- **Mechanism:** Instead of a binary "override" (switching to a fallback policy), the system solves a constrained optimization problem: `argmin ||u_human - u||^2` subject to `Q(x, u) >= gamma * V(x)`. This mathematically enforces safety using the smallest possible deviation from the human's intended control input.
- **Core assumption:** The divergence between the human's raw input and the filtered output correlates with the operator's perceived loss of agency.
- **Evidence anchors:**
  - [abstract] "HCSF's CBF-based interventions modify the human's actions minimally and smoothly..."
  - [section IV] "Our HCSF solves an OCP at each timestep to find a safe action that minimally deviates from the human control... while satisfying the Q-CBF constraint."
  - [corpus] "HumanAgencyBench" supports the general principle that measuring deviation from intent is a scalable proxy for agency evaluation.
- **Break condition:** If the optimization solver introduces latency exceeding the control loop frequency (e.g., >30Hz in high-speed racing), the system will lag, causing instability rather than assistance.

### Mechanism 2
- **Claim:** A state-action value function (Q-function) enables model-free safety enforcement by decoupling dynamics from the safety constraint.
- **Mechanism:** Traditional Control Barrier Functions (CBFs) require `hdot(x, u)`, which depends on knowing system dynamics `f(x, u)`. This method uses a learned Q-function to define the barrier constraint `Q(x, u) >= gamma * V(x)`. Since `Q` and `V` are neural networks learned from interaction, the constraint can be evaluated via simple forward passes without an explicit dynamics model.
- **Core assumption:** The learned Q-function approximates the true safety Bellman equation sufficiently well that the induced safe set `S*` is forward-invariant.
- **Evidence anchors:**
  - [abstract] "enforces a novel state-action control barrier function (Q-CBF) that does not require system dynamics."
  - [section IV] "The requirement (or lack thereof) of the system dynamics... [standard CBFs need] a priori knowledge... On the contrary, evaluating the original DCBF constraint... does not."
  - [corpus] "Deep QP Safety Filter" suggests a related trend in using learned Q-functions to construct data-driven safety layers.
- **Break condition:** If the Q-function exhibits high approximation error (e.g., in out-of-distribution states), the constraint `Q(x, u) >= 0` may falsely permit unsafe actions or forbid safe ones.

### Mechanism 3
- **Claim:** Curriculum-style "danger exposure" is required to learn robust safety filters in high-fidelity simulators.
- **Mechanism:** Racing simulators like Assetto Corsa rarely naturally produce the near-crash states necessary for learning safety boundaries. The authors use a "warmup and initialization" pipeline that deliberately injects adversarial actions (e.g., full throttle into turns) to force the agent into "failure-prone" regions, populating the replay buffer with the critical data needed to shape the safety value function.
- **Core assumption:** The safety filter generalizes from these induced failure modes to the diverse strategies of real human drivers.
- **Evidence anchors:**
  - [section V.D] "By deliberately inducing these 'dangerous' situations... our pipeline ensures the agent encounters a wide range of conditions where the safety filters must intervene effectively."
  - [corpus] Corpus evidence is weak for this specific training technique; related papers focus on agency evaluation rather than safety curriculum.
- **Break condition:** If the "warmup" policies are too conservative, or the "adversarial" distribution is too narrow, the Q-function will be undefined or inaccurate at the actual edges of human error.

## Foundational Learning

- **Concept: Hamilton-Jacobi (HJ) Reachability**
  - **Why needed here:** The paper grounds its definition of "safety" in the maximal safe set `S*`, derived from HJ reachability theory. You cannot understand the "safety value function" `V(x)` without this concept.
  - **Quick check question:** If `V(x)` is positive, does it mean the system is currently safe, or that it *can* avoid failure in the future?

- **Concept: Control Barrier Functions (CBFs)**
  - **Why needed here:** The paper modifies standard CBF logic (which uses dynamics) into Q-CBF (which uses value functions). Understanding the standard `hdot` constraint is necessary to appreciate the model-free contribution.
  - **Quick check question:** Does a standard CBF constraint `hdot >= -alpha * h` require knowledge of the system's state transition function?

- **Concept: Soft Actor-Critic (SAC)**
  - **Why needed here:** The system learns the Q-function using SAC, a model-free off-policy RL algorithm. Understanding "actor" (policy) and "critic" (value) separation is required to interpret the training loop.
  - **Quick check question:** Why is an "off-policy" algorithm useful when sampling dangerous states in a replay buffer?

## Architecture Onboarding

- **Component map:** State `x` (133-dim vector: speed, track distances, opponent info) -> Human Action `u_h` (steering/throttle/brake) -> Safety Critic `Q(x, u)` (estimates safety margin) + Safe Actor `u_safe` (fallback policy) -> Optimization Solver (projects `u_h` onto safe set defined by `Q(x, u) >= gamma * V(x)`) -> Action `u` (filtered output) -> Visual Cues (arrows on HUD indicating intervention magnitude)

- **Critical path:** The **Safety Critic training** is the primary bottleneck. You must run the Warmup/Initialization phases to collect data before standard RL fine-tuning begins.

- **Design tradeoffs:**
  - **Gamma (`gamma`):** The hyperparameter trading off "smoothness" vs "strict safety."
    - Low `gamma` (~0) -> Safe set must expand rapidly; interventions may be jerky/early.
    - High `gamma` (~0.9) -> Allows safety value to decay faster; interventions are smoother/more permissive, but risks cutting safety margins closer.
    - *Paper finding:* `gamma = 0.7` is the sweet spot for user satisfaction.

- **Failure signatures:**
  - **High "Input Modification" (I.M.):** The filter is constantly fighting the human. Likely causes: Miscalibrated `gamma` or mismatched intents (e.g., human trying to spin car, filter preventing it).
  - **Abrupt Jerk:** Optimization solver failing to find a "close" safe action, forcing a jump to the fallback policy `u_safe`.
  - **Collision:** Q-function approximation error; the agent thought the state was safe (`Q > 0`), but it wasn't.

- **First 3 experiments:**
  1.  **Overfit to Reference Path:** Train the Q-network *without* the warmup/adversarial initialization. Verify that the filter fails to recover from off-road states.
  2.  **Gamma Sweep:** Run the filter with `gamma = [0.0, 0.5, 0.9]` in a "mock" race. Plot the trade-off curve between average jerk (smoothness) and collision rate.
  3.  **Agency Check:** Implement a naive "Last-Resort" filter (hard switch). Compare subjective "loss of control" ratings between the naive filter and HCSF in a user study of 5 internal testers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the HCSF framework be extended to explicitly optimize for competitive task performance (e.g., lap times) alongside safety and agency?
- Basis in paper: [explicit] Section VII states the method "does not directly optimize for racing performance (e.g., lap times)," and Section VIII proposes extending the work to "performance-oriented tasks focusing on the interplay between safety and strategic decision-making."
- Why unresolved: The current optimization problem minimizes deviation from human input to preserve agency, which inherently prioritizes the human's strategy (even if suboptimal) over optimal task execution.
- What evidence would resolve it: A user study demonstrating that a modified HCSF significantly improves lap times or other performance metrics compared to unassisted driving, without increasing failure rates.

### Open Question 2
- Question: Does a secondary, proactive "coaching" layer mitigate the risk of operator over-reliance and skill degradation?
- Basis in paper: [explicit] Section VII notes that "prolonged exposure... may result in drivers becoming over-reliant" and suggests future work could add a layer to "coach" operators via multi-modal cues before physical intervention occurs.
- Why unresolved: Appendix D indicates that participants using safety filters showed potentially slower lap times in the final unassisted session compared to the control group, hinting at skill loss.
- What evidence would resolve it: A longitudinal study where participants using a coaching layer show better retention of unassisted driving skills in post-interaction sessions compared to those using only the reactive safety filter.

### Open Question 3
- Question: Do the visual intervention cues induce unintended behavioral changes or distinct confusion patterns in human operators?
- Basis in paper: [explicit] Section VII notes that "visual cues can help reduce confusion... they may also induce unintended behavioral changes that merit further investigation."
- Why unresolved: While the study measured overall satisfaction and agency, it did not analyze specific negative adaptations or reflexive responses triggered by the display of correction arrows.
- What evidence would resolve it: Ablation studies comparing driving behaviors with and without specific visual cues, identifying instances of over-correction, hesitation, or misinterpretation of the AI's intent.

## Limitations
- The HCSF's effectiveness hinges on the accuracy of the learned Q-function in out-of-distribution states, with no guarantee it generalizes to all human driving styles
- The method assumes safety can be evaluated via a scalar margin function (distance to track/opponents), which may not capture all failure modes in real racing
- The human study's ecological validity is limited - Assetto Corsa doesn't fully replicate the physical and cognitive demands of real motorsports
- The "human agency" metrics (I.M., comfort) are subjective and may not translate across different racing cultures or skill levels

## Confidence

- **High confidence:** The model-free Q-CBF formulation works as described and improves upon baseline filters on the tested metrics (agency, comfort, satisfaction). The mathematical framework is sound.
- **Medium confidence:** The warmup curriculum is necessary for learning robust safety boundaries. The gamma=0.7 hyperparameter is optimal for the tested user population.
- **Low confidence:** The HCSF will generalize to other racing tracks, vehicle dynamics, or non-racing applications without significant retraining. The subjective agency metrics capture the complete human experience.

## Next Checks
1. **Generalization test:** Deploy HCSF on a different racing track (e.g., Monaco street circuit) and vehicle type (e.g., Formula 1 car). Compare safety filter performance metrics and conduct a small user study to check for changes in agency satisfaction.
2. **Robustness to distribution shift:** Create a "stress test" suite with adversarial human inputs (deliberate spins, extreme off-track excursions) and measure the filter's failure rate and intervention magnitude. Compare against the original Assetto Corsa validation.
3. **Ablation on Q-function fidelity:** Retrain the safety critic with and without the adversarial warmup curriculum. Quantify the difference in safety margin accuracy by measuring the percentage of true failure states where Q(x,u) < 0.