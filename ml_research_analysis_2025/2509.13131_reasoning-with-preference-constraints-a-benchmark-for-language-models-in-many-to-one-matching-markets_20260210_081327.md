---
ver: rpa2
title: 'Reasoning with Preference Constraints: A Benchmark for Language Models in
  Many-to-One Matching Markets'
arxiv_id: '2509.13131'
source_url: https://arxiv.org/abs/2509.13131
tags:
- matching
- prompting
- reasoning
- llms
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new benchmark for evaluating large language\
  \ models (LLMs) on the College Admission Problem, a many-to-one matching problem\
  \ requiring reasoning under preferential and structural constraints. The benchmark\
  \ includes 369 instances with varying numbers of students (5\u201320), preference\
  \ types (complete, incomplete, flexible), and capacity settings (under-, exact-,\
  \ over-capacity)."
---

# Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets

## Quick Facts
- **arXiv ID**: 2509.13131
- **Source URL**: https://arxiv.org/abs/2509.13131
- **Reference count**: 40
- **Primary result**: Introduces a new benchmark evaluating LLMs on the College Admission Problem, showing reasoning models significantly outperform base models but struggle with scaling.

## Executive Summary
This paper introduces a new benchmark for evaluating large language models (LLMs) on the College Admission Problem, a many-to-one matching problem requiring reasoning under preferential and structural constraints. The benchmark includes 369 instances with varying numbers of students (5–20), preference types (complete, incomplete, flexible), and capacity settings (under-, exact-, over-capacity). Six open-weight LLMs—Llama, Mistral, Qwen, QwQ, and GPT-oss—were tested under eight prompting strategies (basic, role-based, in-context learning, chain-of-thought variants, and iterative prompting). Results show that while reasoning models (QwQ, GPT-oss) significantly outperform base models on feasibility, stability, and optimality, all models struggle as instance size increases. No single prompt strategy consistently delivers the best performance across all models. Iterative prompting does not guarantee monotonic improvement, suggesting that generating diverse outputs may be more beneficial than iterative self-correction in this context.

## Method Summary
The benchmark evaluates six open-weight LLMs on 369 synthetic instances of the College Admission Problem, varying student counts (5-20), preference completeness (complete/incomplete/flexible), and capacity settings (under/exact/over). Eight prompting strategies are tested including basic, role-based, in-context learning, chain-of-thought variants, and iterative prompting with up to 5 attempts. Model outputs are validated against ground-truth solutions from the Deferred Acceptance algorithm, measuring feasibility (capacity compliance), assignment stability, matching stability, and student-optimality. The evaluation framework includes a parser for matching formats and an evaluator comparing outputs to algorithm-generated solutions.

## Key Results
- Reasoning models (QwQ, GPT-oss) significantly outperform base models (Llama, Mistral, Qwen) on feasibility, stability, and optimality metrics
- Performance degrades as instance size increases, with reasoning models showing less abrupt drops
- No single prompt strategy consistently delivers optimal performance across all models
- Iterative prompting shows non-monotonic improvement, with "Best" attempt sometimes outperforming "Last"
- Models are more affected by student count than by preference type, even though both impact algorithm runtime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized reasoning training improves performance on constrained combinatorial optimization.
- Mechanism: Reasoning models (QwQ, GPT-oss) use reinforcement learning and supervised fine-tuning for step-by-step reasoning, enabling better tracking of capacity constraints, blocking pairs, and optimality conditions across iterations compared to base models that rely primarily on next-token prediction.
- Core assumption: The performance gap stems from reasoning-specific training rather than model scale alone.
- Evidence anchors:
  - [abstract] "reasoning LLMs, like QwQ and GPT-oss, significantly outperform traditional models such as Llama, Qwen or Mistral"
  - [Page 7] "reasoning LLMs' performance dominates that of the base LLMs, especially when dealing with more complex instances and metrics"
  - [corpus] Related work on LLM reasoning benchmarks (MATH, Olympiad-Bench) shows similar patterns but focuses on different problem classes
- Break condition: If scaling base model parameters alone closes the gap without reasoning-specific training, the mechanism would be scale-driven rather than training-paradigm-driven.

### Mechanism 2
- Claim: Performance degradation scales with solution space size rather than algorithmic complexity alone.
- Mechanism: Increasing student count expands the matching space multiplicatively; models lose track of constraint interactions across larger assignments, particularly for stability (detecting all blocking pairs requires checking O(|S|×|C|) pairs).
- Core assumption: The bottleneck is constraint-tracking under combinatorial explosion, not algorithm comprehension.
- Evidence anchors:
  - [Page 7] "performance decreases as instance size increases; however, advanced reasoning models exhibit a less abrupt drop in feasibility"
  - [Page 7-8] "the models are more significantly affected by the number of students than by the type of preferences, even though both factors impact the running time of the DA algorithm"
  - [corpus] Weak direct evidence; related matching market papers focus on algorithm design, not LLM scaling limits
- Break condition: If incomplete preferences (which reduce algorithm iterations) showed significant performance gains despite equal or larger student counts, complexity would be iteration-driven rather than space-driven.

### Mechanism 3
- Claim: Prompt strategy effectiveness depends on model reasoning maturity—instruction specificity inversely correlates with reasoning capability.
- Mechanism: Base models require detailed algorithmic guidance (ICL with steps, CoT pseudocode) to scaffold reasoning; reasoning models perform better with less prescriptive prompts (CoT unsupervised) that don't constrain their learned reasoning patterns.
- Core assumption: Over-specifying reasoning steps interferes with reasoning models' internalized procedures.
- Evidence anchors:
  - [Page 7] "for the base models, the best prompts, mainly the ICL ones and CoT with pseudocode, perform worst on every metric for reasoning models"
  - [Page 7] "there is no single golden rule that determines which prompts are associated with higher performance for all models"
  - [corpus] Prior work (Wei et al. 2022) shows CoT benefits, but model-specific effects remain underexplored
- Break condition: If a single prompt strategy consistently topped all models regardless of architecture, the mechanism would be task-specific rather than model-interaction-specific.

## Foundational Learning

- Concept: Many-to-one matching with preferences (Gale-Shapley Deferred Acceptance)
  - Why needed here: The benchmark evaluates whether models can produce feasible, stable, student-optimal matchings—the core output of DA algorithm.
  - Quick check question: Given 5 students and 3 colleges with capacities [2,1,2], can you manually trace two rounds of the student-proposing DA algorithm?

- Concept: Stability vs. optimality in matching markets
  - Why needed here: The paper evaluates four hierarchically harder metrics—feasibility, assignment stability, matching stability, optimality—each presupposing the previous.
  - Quick check question: If student s1 prefers c1 over their assigned c2, and c1 has an open slot, is this a blocking pair? What if c1 is full but prefers s1 to an admitted student?

- Concept: Prompting strategies (CoT, ICL, role-based, iterative)
  - Why needed here: Eight prompting strategies are tested; understanding when each helps is critical for deployment.
  - Quick check question: Why might "Think step-by-step" (CoT unsupervised) help reasoning models more than providing explicit DA pseudocode?

## Architecture Onboarding

- Component map: Instance generator -> Prompt template -> Model zoo -> Evaluator -> Iterative loop
- Critical path: 1. Parse instance → 2. Format with prompt strategy → 3. Generate model output → 4. Extract matching → 5. Validate format → 6. Compare to DA ground truth → 7. Compute 4 metrics
- Design tradeoffs:
  - Synthetic vs. real data: Controlled parameters vs. ecological validity
  - Instance size (5-20 students): Token limits constrain scale; reasoning models take 6-13x longer than base models
  - Feedback granularity: Current feedback reports metric counts; specific blocking pairs might improve correction but weren't tested
- Failure signatures:
  - Invalid outputs: Wrong entity names (e.g., "Alice" instead of "s1"), returning code instead of matching
  - Feasibility failures: Exceeding college capacities, multiply-assigned students
  - Stability failures: Blocking pairs where student prefers college and college has capacity or prefers student to current match
  - Optimality failures: Matching is stable but not student-optimal (wrong stable matching among multiple)
- First 3 experiments:
  1. Baseline survey: Run all 6 models on 369 instances with Basic prompt only; establish feasibility/stability/optimality floor for your infrastructure.
  2. Prompt sensitivity analysis: For your target model, test 3 prompts (ICL, CoT text, CoT pseudocode) on the 72 five-student instances; identify which strategy aligns with model type.
  3. Scaling pilot: Take the best prompt from #2, run on 10-student instances only; quantify the performance drop rate to predict viability at larger scales.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does providing fine-grained, specific feedback on constraint violations improve the monotonicity and success rate of iterative prompting?
- Basis in paper: [explicit] The authors explicitly propose "experiment[ing] with different feedback forms that vary in the level of detail... like specifying the exact unstable pairs."
- Why unresolved: Current iterative experiments showed non-monotonic performance using aggregate metrics; it remains unclear if detailed feedback aids correction or introduces confusion.
- What evidence would resolve it: Comparative trials measuring convergence rates when feedback identifies specific blocking pairs versus providing only high-level validity checks.

### Open Question 2
- Question: How does the interaction between instance size and preference type (incomplete vs. complete) affect model performance at larger scales?
- Basis in paper: [inferred] The paper notes that while student count significantly impacts performance, preference type does not at small scales, leading to a call for "more experiments... when scaling."
- Why unresolved: The limited scale (5–20 students) may not expose reasoning complexities associated with incomplete preference lists that appear in larger, real-world matching markets.
- What evidence would resolve it: Benchmarking results on instances with significantly more students (e.g., 50–100) to observe if the lack of performance difference between preference types persists.

### Open Question 3
- Question: Can LLMs act as interpreters to select the appropriate matching mechanism (e.g., DA vs. Top Trading Cycles) based on user priorities?
- Basis in paper: [explicit] The conclusion suggests LLMs could serve as "interpreters capable of suggesting appropriate approaches based on the context and users’ priorities" rather than just solvers.
- Why unresolved: The current work focused solely on the LLM's ability to execute the Deferred Acceptance algorithm, not its ability to reason about trade-offs between different algorithms.
- What evidence would resolve it: Experiments testing if LLMs can successfully map natural language user goals (e.g., "prioritize efficiency over stability") to the correct algorithmic choice.

## Limitations
- The observed performance differences between reasoning and base models may stem from architectural differences or specific training objectives, with incomplete ablation on model scale versus reasoning capability
- Synthetic benchmark lacks ecological validity of real-world matching markets involving strategic behavior, incomplete information, and negotiation
- Lack of released code and data prevents independent verification of the 369 instances and ground-truth solutions computed by the Deferred Acceptance algorithm

## Confidence

**High confidence**: The empirical finding that reasoning models significantly outperform base models on feasibility, stability, and optimality metrics is robust, supported by clear numerical differences across all instance types and the consistency of this pattern in Table 1 and Figure 2.

**Medium confidence**: The claim that performance degradation correlates with solution space size rather than algorithmic complexity is supported by the data but lacks direct experimental evidence isolating these factors. The observation that incomplete preferences don't significantly improve performance despite reducing algorithm iterations suggests space complexity dominates, but this mechanism isn't definitively proven.

**Low confidence**: The assertion that no single prompt strategy consistently delivers optimal performance across all models is based on aggregated statistics that may mask important interactions between model families and prompt types. The paper's analysis doesn't deeply explore whether certain reasoning models might benefit from specific prompt engineering that differs from base models.

## Next Checks
1. **Prompt strategy isolation test**: Run QwQ-32B on the 72 five-student instances using only three prompts (ICL, CoT text, CoT pseudocode) to determine if any single strategy consistently outperforms others, contradicting the paper's claim of model-specific optimality.

2. **Scale sensitivity analysis**: Generate 10 additional instances at 25 students (pushing token limits) using the same generation parameters to measure whether the observed performance cliff at 15-20 students is sharp or gradual, validating the space-complexity hypothesis.

3. **Reasoning capability ablation**: Test whether a 70B base model with supervised fine-tuning on structured reasoning tasks (distinct from the DA algorithm) can close the performance gap with QwQ-32B on 10-student instances, isolating whether reasoning-specific training or scale drives the advantage.