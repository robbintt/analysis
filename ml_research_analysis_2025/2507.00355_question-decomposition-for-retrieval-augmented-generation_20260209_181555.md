---
ver: rpa2
title: Question Decomposition for Retrieval-Augmented Generation
arxiv_id: '2507.00355'
source_url: https://arxiv.org/abs/2507.00355
tags:
- question
- retrieval
- query
- arxiv
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of answering multi-hop questions
  in retrieval-augmented generation (RAG) systems, where relevant information is scattered
  across multiple documents. The authors propose a pipeline that combines question
  decomposition and reranking to improve retrieval quality and answer accuracy.
---

# Question Decomposition for Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2507.00355
- Source URL: https://arxiv.org/abs/2507.00355
- Reference count: 6
- Primary result: +36.7% MRR@10 retrieval and +11.6% F1 answer accuracy over standard RAG

## Executive Summary
This paper addresses the challenge of answering multi-hop questions in retrieval-augmented generation (RAG) systems, where relevant information is scattered across multiple documents. The authors propose a pipeline that combines question decomposition and reranking to improve retrieval quality and answer accuracy. First, an LLM decomposes complex questions into simpler sub-questions, enabling more targeted retrieval. Retrieved documents from all sub-questions are merged and deduplicated, then reranked using a cross-encoder model to filter noise and promote relevance. The approach is evaluated on MultiHop-RAG and HotpotQA benchmarks, showing substantial gains over standard RAG baselines without requiring task-specific training or specialized indexing.

## Method Summary
The method uses a pipeline where an LLM first decomposes complex questions into 2-5 simpler sub-questions. Each sub-question is then used to retrieve documents using a bi-encoder model, with the top results merged and deduplicated. A cross-encoder reranker then re-scores this candidate pool against the original question to filter noise and promote relevance. Finally, an LLM generates answers using the reranked passages and original query. The approach is evaluated using MultiHop-RAG and HotpotQA benchmarks with standard RAG as baseline.

## Key Results
- +36.7% MRR@10 in retrieval quality compared to standard RAG
- +11.6% F1 in answer accuracy over standard RAG
- QD+RR pipeline improves retrieval recall by targeting semantically aligned sub-queries

## Why This Works (Mechanism)

### Mechanism 1: Semantic Granularity Alignment
Decomposing a complex, multi-hop query into simpler sub-questions improves retrieval recall by aligning the semantic granularity of the query with the granularity of individual documents in the corpus. A multi-hop query has a complex semantic representation, but by decomposing it into sub-questions, each sub-query's embedding is semantically closer to a specific, relevant document, increasing its retrieval score. This assumes the retriever's vector space encodes semantic similarity and that simpler, focused queries will have higher similarity scores with their target documents.

### Mechanism 2: Precision via Contextual Relevance Re-Scoring
Reranking the pooled documents from all sub-queries using the original, complex query restores precision by filtering out noise and promoting passages that are globally relevant to the user's ultimate intent. Question decomposition broadens the candidate pool, introducing irrelevant or partially relevant documents. A cross-encoder reranker takes the original query and each candidate document and performs a deep, joint attention-based comparison, assessing relevance based on the entire original query context. This assumes the cross-encoder model is sufficiently powerful and trained on data that generalizes to the domain of the user's query.

### Mechanism 3: Evidence Aggregation for Multi-Step Reasoning
Providing a merged set of relevant passages from various aspects of a complex query allows the generative LLM to perform reasoning across documents, effectively synthesizing an answer it could not derive from a single, noisy context. The final generation step receives a curated list of passages that collectively contain the necessary facts. The LLM's attention mechanism can then attend to tokens from multiple documents at once, identifying entities, comparing values, and synthesizing a final answer. This assumes the generative LLM has sufficient context window to attend to all the retrieved passages simultaneously and possesses the reasoning capabilities to synthesize an answer from them.

## Foundational Learning

- **Dense Retrieval vs. Cross-Encoder Re-ranking**
  - Why needed here: The paper's architecture relies on a two-stage retrieval process. Understanding that the first stage (bi-encoder) is fast but less precise, while the second stage (cross-encoder) is slow but highly precise, is fundamental to grasping the tradeoffs involved.
  - Quick check question: Why can't a cross-encoder be used for the initial retrieval over the entire corpus?

- **Multi-hop Question Answering**
  - Why needed here: This is the core problem the paper addresses. Understanding that these questions require reasoning across multiple documents (e.g., finding two values and comparing them) is essential for seeing why standard RAG fails.
  - Quick check question: Can you give an example of a multi-hop question that would fail with a simple keyword search?

- **The "Lost-in-the-Middle" Phenomenon in LLMs**
  - Why needed here: The discussion of evidence aggregation hinges on the LLM's ability to process multiple documents. Awareness of this limitation helps in understanding the importance of the reranking step to keep the context window focused.
  - Quick check question: If you provide an LLM with ten documents, where in the list is the model most likely to overlook a key piece of information?

## Architecture Onboarding

- Component map: User Query -> Question Decomposition (QD) Module -> Embedding Model -> Vector Store -> Candidate Pool -> Reranker Module -> Generator
- Critical path: The QD module's prompt is critical. It must instruct the LLM to create sub-questions that are searchable and cover the key aspects of the complex query. A bad prompt here cascades into poor retrieval. The retrieval pool size (k) must be balanced. Too low, and you miss evidence; too high, and the candidate pool becomes noisy and reranking becomes slow. The reranker must use the original query, not the sub-questions.
- Design tradeoffs: The central tradeoff is latency vs. quality. The QD+RR pipeline is significantly slower (approx. 19s/query) than naive RAG (0.03s). For real-time applications, this latency is often unacceptable. Caching sub-queries is a key mitigation. The authors tout the "no training required" benefit, making the system easy to deploy but potentially outperformed by a system with a fine-tuned decomposition model. The system is a "drop-in" solution, but performance is tied to the general-purpose LLMs and rerankers used.
- Failure signatures: Excessive latency with no gain if decomposed questions are too similar to the original. Single-hop degradation if decomposition introduces noise on simple queries. Reranker mismatch if the off-the-shelf reranker is not trained on data similar to your corpus.
- First 3 experiments:
  1. Baseline Replication: Implement the Naive RAG and RAG+QD+RR pipeline on a small sample of your own data. Measure end-to-end latency and answer quality to see if the paper's reported gains hold.
  2. Ablation on Sub-query Count: Vary the number of sub-questions generated (e.g., 2, 3, 5) and observe the impact on retrieval recall and answer quality.
  3. Reranker Ablation: Compare the performance of the QD+RR pipeline using a different off-the-shelf reranker or by skipping the reranking step entirely.

## Open Questions the Paper Calls Out

### Open Question 1
Can a query-complexity classifier effectively identify when to skip question decomposition to preserve efficiency and prevent noise in single-hop scenarios? The authors state employing QD only when a query is predicted to need multi-hop reasoning could preserve benefits while cutting overhead, noting in Limitations that decomposition can be counterproductive for already specific queries. This remains unresolved as the current system applies a fixed decomposition prompt to all queries. Evaluation of a pipeline with a pre-classification step that selectively applies decomposition would resolve this question.

### Open Question 2
Does generating a variable number of sub-queries based on predicted reasoning steps improve evidence alignment compared to the current fixed-budget approach? The ablation study reveals the LLM generates the maximum allowed sub-queries (5) in ~98% of cases despite most questions needing only 2-3 evidences; the authors note that "variable-size decomposition could better align with actual evidence needs." The current implementation relies on a fixed prompt limit, resulting in a mismatch between generated sub-queries and necessary reasoning steps. Experiments using prompts or models that output variable counts of sub-queries would resolve this question.

### Open Question 3
Would incorporating iterative retrieval, where initial evidence informs subsequent sub-query generation, significantly improve performance on complex tasks? The Limitations section notes the pipeline operates in a "single-shot manner" and lacks the ability to update sub-queries based on retrieved evidence, which limits adaptive multi-step reasoning. The current architecture retrieves documents for all sub-queries simultaneously without feedback loops, potentially missing information that requires sequential dependency. A comparative study between the proposed single-shot QD method and an iterative agent-based approach would resolve this question.

## Limitations
- The system's latency (19s/query) is significantly higher than naive RAG (0.03s), making it impractical for real-time applications without caching mechanisms.
- The fixed decomposition approach may introduce noise on single-hop queries where simpler retrieval would suffice.
- The effectiveness depends heavily on the quality of off-the-shelf components and may not generalize well to domain-specific corpora.

## Confidence
- **High confidence**: The mechanism of question decomposition improving recall by aligning semantic granularity is well-supported by the observed performance gains and the intuitive alignment between focused sub-queries and individual documents.
- **Medium confidence**: The effectiveness of cross-encoder reranking in filtering noise and restoring precision is supported by the reported metrics, but depends heavily on the quality of the initial candidate pool and the reranker's domain generalization.
- **Medium confidence**: The claim that providing multiple relevant passages enables better multi-step reasoning assumes the LLM has sufficient context window and reasoning capability, but doesn't explicitly address the "lost in the middle" problem or provide evidence about optimal context window management.

## Next Checks
1. **Decomposition Prompt Validation**: Test the QD module with 3-5 different decomposition prompts on a held-out set of multi-hop questions, measuring both retrieval recall and answer accuracy to identify the most effective prompt structure.
2. **Reranker Domain Adaptation**: Fine-tune the bge-reranker-large on a small sample of domain-specific question-passage pairs and compare performance against the zero-shot baseline to quantify the tradeoff between deployment simplicity and domain-specific performance.
3. **Latency-Quality Tradeoff Analysis**: Systematically measure the end-to-end latency and answer quality across different retrieval pool sizes (k=5, 10, 20) and sub-question counts (2-5) to identify the optimal configuration for different latency budgets.