---
ver: rpa2
title: Dropping Just a Handful of Preferences Can Change Top Large Language Model
  Rankings
arxiv_id: '2508.11847'
source_url: https://arxiv.org/abs/2508.11847
tags:
- arena
- data
- rankings
- chatbot
- dropping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to test the robustness of Bradley-Terry
  (BT) rankings to small worst-case data drops, using an AMIP-based influence approximation.
  Applied to multiple LLM evaluation platforms, the method shows that top model rankings
  can flip after removing as few as 2 preferences (0.003% of total).
---

# Dropping Just a Handful of Preferences Can Change Top Large Language Model Rankings
## Quick Facts
- **arXiv ID**: 2508.11847
- **Source URL**: https://arxiv.org/abs/2508.11847
- **Reference count**: 40
- **Primary result**: Top LLM rankings can flip after removing as few as 2 preferences (0.003% of total)

## Executive Summary
This paper introduces a method to test the robustness of Bradley-Terry (BT) rankings to small worst-case data drops, using an AMIP-based influence approximation. Applied to multiple LLM evaluation platforms, the method shows that top model rankings can flip after removing as few as 2 preferences (0.003% of total). This fragility persists even with bootstrap-based rankings and is similar for human- and LLM-judged systems, except for MT-bench, which is more robust due to expert annotation and discriminative prompts. The method identifies the specific dropped preferences driving flips, enabling inspection of these influential cases. The findings suggest leaderboard rankings are often unstable and warrant caution in interpretation.

## Method Summary
The paper proposes a method to assess the robustness of Bradley-Terry rankings to worst-case data drops. The approach uses an AMIP (Approximate Model Influence for Preferences) approximation to efficiently estimate the influence of removing individual preferences on model rankings. The method computes the difference in expected ranking loss between full and reduced datasets, identifying which preferences have the largest impact. For each model, it finds the most and least impactful preferences to remove, then simulates the ranking changes. The authors validate their approximation by comparing it to exact leave-one-out influence calculations on subsets of data. They apply this methodology across multiple LLM evaluation datasets including Chatbot Arena, MT-bench, and OpenCompass.

## Key Results
- Top model rankings can flip after removing as few as 2 preferences (0.003% of total)
- Ranking instability persists even with bootstrap-based rankings
- MT-bench shows greater robustness due to expert annotation and discriminative prompts
- The influence approximation method identifies specific dropped preferences driving ranking changes

## Why This Works (Mechanism)
The Bradley-Terry model estimates pairwise comparison probabilities between models, producing a global ranking. When individual preferences are removed, the model's parameters shift, potentially changing the relative ordering of models. The AMIP approximation captures how sensitive the ranking is to the removal of specific comparisons by estimating the change in expected ranking loss. Since rankings depend on the relative strengths of all models, removing even a few influential preferences can create cascading effects that flip top positions. The approximation works because it efficiently captures the first-order effect of preference removal on the likelihood function, avoiding computationally expensive leave-one-out calculations.

## Foundational Learning
- **Bradley-Terry model**: Pairwise comparison model that estimates the probability one item is preferred over another based on latent strengths - needed for understanding how pairwise preferences translate to global rankings, quick check: verify model produces valid probability distributions
- **Influence functions**: Mathematical tools for estimating how model parameters change when individual data points are removed - needed for approximating ranking sensitivity without full retraining, quick check: confirm influence estimates correlate with actual ranking changes
- **Bootstrapping**: Resampling technique for estimating uncertainty in model parameters - needed to assess whether ranking instability persists under different samples, quick check: compare bootstrap ranking distributions before and after preference removal
- **Expected ranking loss**: Metric measuring the expected error in predicted rankings - needed for quantifying the impact of data removal on ranking quality, quick check: verify loss increases monotonically with more preference removal
- **Pairwise preference aggregation**: Process of combining individual pairwise judgments into a coherent global ranking - needed to understand how local preferences determine global order, quick check: ensure aggregation method preserves transitivity properties
- **Worst-case analysis**: Approach focusing on most damaging scenarios rather than average behavior - needed to identify critical vulnerabilities in ranking systems, quick check: confirm worst-case drops actually cause ranking flips

## Architecture Onboarding
**Component map**: Preference data -> Bradley-Terry model fitting -> Ranking computation -> Influence approximation -> Worst-case preference identification -> Ranking stability analysis

**Critical path**: Preference data collection -> BT model estimation -> Ranking determination -> Influence function calculation -> Drop impact simulation -> Stability conclusion

**Design tradeoffs**: The paper trades computational efficiency (using AMIP approximation) for exactness (full leave-one-out computation), enabling analysis of large datasets while accepting some approximation error. This allows systematic exploration of ranking fragility that would be computationally prohibitive with exact methods.

**Failure signatures**: Ranking flips occur when influential preferences (often between closely matched models) are removed. The method identifies these by large influence scores. Failure modes include: (1) single preferences causing complete ranking reversals, (2) multiple small drops cumulatively causing instability, and (3) bootstrapping failing to capture worst-case scenarios.

**First experiments**: 1) Apply the method to a small synthetic dataset with known ground truth to verify correctness. 2) Compare AMIP approximations against exact leave-one-out calculations on a subset to quantify approximation error. 3) Test the method on a dataset with artificially introduced noisy preferences to validate sensitivity detection.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings rely on specific evaluation datasets and models tested, exact percentages may vary across different protocols
- Focus on pairwise comparison data may not extend directly to other evaluation paradigms like absolute scoring
- Influence approximation is novel but remains an approximation, exact computation is more precise but computationally prohibitive

## Confidence
- **Core finding (High)**: Ranking fragility to small data drops is systematically demonstrated across multiple datasets with bootstrap validation
- **Specific drop percentages (Medium)**: Exact percentages depend on dataset size and comparison density, may vary across different evaluation protocols
- **MT-bench robustness (Medium)**: Well-supported observation but may be specific to its particular annotation protocol and prompt design
- **Influence approximation accuracy (Medium)**: Novel method with clear trade-offs between accuracy and feasibility, approximation error is acknowledged

## Next Checks
1. Test the same methodology on additional LLM evaluation datasets beyond those studied, including datasets with different collection protocols and model sets
2. Apply the influence approximation method to a held-out test set to verify that identified influential preferences remain predictive of ranking changes
3. Compare the influence approximation results against exact influence calculations on a smaller subset of data to quantify approximation error