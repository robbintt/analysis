---
ver: rpa2
title: Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality
  Risk in Metastatic Castration-Resistant Prostate Cancer
arxiv_id: '2601.20046'
source_url: https://arxiv.org/abs/2601.20046
tags:
- clinical
- risk
- mortality
- cohort
- validation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a visit-level, 180-day mortality prediction
  model for metastatic castration-resistant prostate cancer (mCRPC) using routinely
  collected longitudinal clinical data. A Gated Recurrent Unit (GRU) architecture
  was trained and externally validated across two Phase III cohorts (n=526 and n=640),
  leveraging eight clinical variables including BMI, systolic blood pressure, and
  ECOG status.
---

# Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality Risk in Metastatic Castration-Resistant Prostate Cancer

## Quick Facts
- arXiv ID: 2601.20046
- Source URL: https://arxiv.org/abs/2601.20046
- Reference count: 20
- Achieved 0.89 AUC-ROC and 0.87 PR-AUC in external validation with 151-day median lead time for true positives

## Executive Summary
This work presents a visit-level, 180-day mortality prediction model for metastatic castration-resistant prostate cancer (mCRPC) using routinely collected longitudinal clinical data. A Gated Recurrent Unit (GRU) architecture was trained and externally validated across two Phase III cohorts (n=526 and n=640), leveraging eight clinical variables including BMI, systolic blood pressure, and ECOG status. At an 85% sensitivity threshold, the GRU achieved a C-index of 0.87, AUC-ROC of 0.89, and PR-AUC of 0.87 in external validation, with a calibration slope of 0.93 and intercept of 0.07. The model provided a median lead time of 151 days for true positive alerts and generated 18.3 alerts per 100 patient-visits, supporting proactive end-of-life care planning while maintaining clinical feasibility.

## Method Summary
The method uses a stacked GRU architecture (64→32 units) with batch normalization and dropout to process sequences of up to 12 visits per patient. Eight clinical features including age, vital signs, ECOG status, and drug dosage reductions are encoded using a temporal autoencoder (64-32-32-64) for missing value imputation. The model outputs visit-level risk scores using a Time-Distributed Dense layer with sigmoid activation. Training employs balanced batches with L2 regularization, and patient-level stratified 3-fold cross-validation selects thresholds to achieve ≥85% sensitivity. External validation was performed on a separate Phase III trial cohort using fixed thresholds from the development fold.

## Key Results
- GRU achieved 0.89 AUC-ROC, 0.87 PR-AUC, and 0.87 C-index in external validation
- Calibration slope of 0.93 and intercept of 0.07 indicate strong probability alignment
- Median lead time of 151 days for true positive alerts
- Generated 18.3 alerts per 100 patient-visits at 85% sensitivity threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GRU architecture captures cumulative physiological deterioration through sequential gating, enabling earlier detection of mortality risk transitions.
- Mechanism: GRU gates (update and reset) selectively retain or discard information from prior visits, allowing the model to build a compressed representation of patient trajectory. When late-stage frailty markers (BMI decline, hemodynamic instability) emerge, the hidden state accumulates these signals across visits rather than treating each encounter independently.
- Core assumption: Mortality risk in mCRPC manifests through gradual physiological decline that is detectable across sequential clinical encounters, not sudden unheralded events.
- Evidence anchors: [abstract] "median lead time of 151 days for true positive alerts"; [section II.A] "encoding the patient's cumulative physiological trajectory to produce a time-updated risk score"; [corpus] Transformer-based survival models similarly leverage temporal patient journeys for mortality prediction (arXiv:2503.12317), supporting the general principle that sequential architectures capture deterioration patterns.
- Break condition: If mortality events are predominantly driven by acute, unpredictable complications (e.g., sudden embolism) rather than gradual decline, temporal accumulation provides minimal signal.

### Mechanism 2
- Claim: BMI and systolic blood pressure serve as inexpensive proxies for systemic decline (cachexia, hemodynamic instability) that precede death in advanced cancer.
- Mechanism: Permutation importance analysis quantifies each feature's contribution to model performance. BMI (0.120 C-index degradation) and systolic BP (0.080) rank highest, suggesting the model relies heavily on metabolic and cardiovascular markers. In mCRPC, weight loss and blood pressure dysregulation correlate with disease burden and organ dysfunction.
- Core assumption: These routine clinical markers are informative because they reflect underlying pathophysiology, not because of spurious correlation with trial-specific protocols.
- Evidence anchors: [abstract] "permutation importance ranked BMI and systolic blood pressure as the strongest associations"; [section III.B.3] "reduced BMI and abnormal blood pressure can serve as inexpensive proxies for systemic decline that often precedes death"; [corpus] Related mortality prediction models in critical care similarly identify vital signs and metabolic markers as top predictors (arXiv:2403.06999, arXiv:2512.19716), though specific feature rankings vary by disease context.
- Break condition: If BMI/BP importance is driven by trial-specific measurement protocols or confounded by treatment arms, generalizability to routine clinical data degrades.

### Mechanism 3
- Claim: Calibration-prioritized model selection (choosing GRU over RSF) ensures predicted probabilities align with observed event rates, which is prerequisite for trustworthy clinical decision support.
- Mechanism: Despite RSF achieving marginally higher specificity (0.89 vs 0.73) and PPV (0.87 vs 0.74) in external validation, its calibration slope (1.34) indicated systematic overestimation of risk. The GRU's near-ideal calibration (slope 0.93, intercept 0.07) means a 60% predicted risk corresponds to approximately 60% observed mortality, enabling clinicians to trust the numeric output.
- Core assumption: Well-calibrated probability estimates are more clinically valuable than maximizing discrimination alone, particularly when thresholds guide intervention decisions.
- Evidence anchors: [abstract] "calibration slope of 0.93 and intercept of 0.07"; [section IV] "the GRU model demonstrated superior reliability... ensures that the predicted probabilities correspond closely to actual observed risks"; [corpus] Calibration as a critical evaluation metric is emphasized across medical ML literature, though few studies report it prominently; neighbor papers focus primarily on discrimination metrics.
- Break condition: If deployment context requires maximizing true negatives (e.g., resource-constrained settings prioritizing specificity), the RSF's higher PPV may outweigh calibration benefits.

## Foundational Learning

- Concept: **Gated Recurrent Units (GRU)**
  - Why needed here: The model processes variable-length visit sequences where each encounter contributes to an evolving risk estimate. Understanding how GRU gates (update gate z, reset gate r) control information flow explains why the model can maintain long-term dependencies without vanishing gradients.
  - Quick check question: Given a patient with 8 visits showing stable BMI, then sudden decline in visits 9-10, which gate mechanism allows the model to "pay more attention" to recent changes while retaining historical context?

- Concept: **Calibration in Clinical Prediction Models**
  - Why needed here: The paper explicitly selects GRU over RSF based on calibration despite RSF's higher specificity. Calibration slope (ideal = 1.0) and intercept (ideal = 0) quantify whether predicted probabilities match observed frequencies.
  - Quick check question: A model predicts 70% mortality risk for a patient subgroup, but only 45% actually die within 180 days. Is this miscalibration reflected in slope, intercept, or both?

- Concept: **Right-Censoring in Survival Analysis**
  - Why needed here: The paper excludes visits without 180-day outcome ascertainment from supervised training. Understanding why censored observations require special handling (vs. treating as "survived") prevents label contamination.
  - Quick check question: A patient's last visit occurs 90 days before study end, with no death recorded. Why is labeling this visit as y=0 (survived) problematic for a 180-day prediction window?

## Architecture Onboarding

- Component map:
  Input Layer: 8 features × T visits (T ≤ 12)
       ↓
  Temporal Autoencoder: [64-32-32-64] for missing value imputation
       ↓
  GRU Layer 1: 64 units + BatchNorm + Dropout(0.3)
       ↓
  GRU Layer 2: 32 units + BatchNorm + Dropout(0.2)
       ↓
  Time-Distributed Dense: 1 unit + Sigmoid activation
       ↓
  Output: Risk score S(x_t) ∈ [0,1] at each visit

- Critical path:
  1. **Data preprocessing pipeline** (highest risk of information leakage): Autoencoder weights and scaling parameters must be fit ONLY on training folds. The paper explicitly notes this constraint.
  2. **Threshold calibration**: Risk threshold θ is selected per dataset to achieve 85% sensitivity floor. In deployment, θ must be recalibrated on local data before prospective use.
  3. **Sequence construction**: Visits are ordered chronologically; the unidirectional GRU ensures prediction at time t depends only on visits 1...t (no future leakage).

- Design tradeoffs:
  - **Exclusion of right-censored visits** → Cleaner labels but reduced sample size and potential selection bias toward patients with complete follow-up
  - **Fixed 85% sensitivity threshold** → Guarantees clinical safety (few missed high-risk patients) at cost of higher false positive rate (18.3 alerts/100 visits)
  - **8-feature minimal set** → Enables EHR integration without specialized assays, but excludes potentially informative biomarkers (PSA, imaging)

- Failure signatures:
  - **Calibration drift under distribution shift**: If external cohort has different mortality prevalence (67% vs 14% in development), threshold recalibration is mandatory. Monitor calibration slope quarterly in deployment.
  - **Alert fatigue from high false positive burden**: Median TIW for false positives (59 days) vs true positives (151 days) suggests some alerts resolve; consider requiring consecutive high-risk scores before triggering clinical action.
  - **Imputation artifacts**: If missingness patterns change between trial and EHR data, autoencoder-based imputation may introduce systematic errors.

- First 3 experiments:
  1. **Ablation study on sequence length**: Retrain with max visits = 6, 8, 10, 12 to quantify how much historical context contributes to performance. Report C-index and calibration for each.
  2. **Threshold sensitivity analysis**: Generate ROC/PR curves and report performance at 80%, 85%, 90%, 95% sensitivity thresholds to help deployment teams select operating points based on local clinical constraints.
  3. **Subgroup calibration by ECOG status**: Stratify external validation by ECOG 0, 1, 2 to verify that calibration holds across functional status levels (paper notes ECOG 2 patients are underrepresented: 1.5% in development, 4.5% in validation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating this longitudinal alert system into electronic health records improve clinical outcomes in real-world settings?
- Basis in paper: [explicit] The authors state, "Future work should focus on integrating these longitudinal alerts into electronic health records to evaluate their impact on clinical outcomes in real-world settings."
- Why unresolved: The current study validated the model using retrospective Phase III clinical trial data, which differs significantly from live clinical workflows and general patient populations.
- What evidence would resolve it: A prospective implementation study measuring patient outcomes, such as quality of life or adherence to palliative care, in diverse, non-trial clinical environments.

### Open Question 2
- Question: Can a discrete-time survival training approach that retains censored visits improve model transportability compared to the current exclusion method?
- Basis in paper: [explicit] The authors note, "We plan to adopt discrete-time survival training based on censoring to keep censored visits and further improve transportability to real-world EHR settings."
- Why unresolved: The current model excluded right-censored cases to avoid ambiguous labels, potentially losing valuable information and limiting robustness in datasets with incomplete follow-up.
- What evidence would resolve it: A comparative analysis of model performance and calibration on real-world data between the current binary classification approach and the proposed discrete-time survival method.

### Open Question 3
- Question: Does the model maintain equitable performance across diverse racial and geographic subgroups?
- Basis in paper: [explicit] The authors state, "Given limited racial diversity... we cannot claim equitable performance across demographic groups. As part of our future plan, we will evaluate subgroup calibration... and validate across multiple sites."
- Why unresolved: The cohorts were predominantly White (82.3% and 90.6%), and performance variations across different ethnicities or regions remain unquantified.
- What evidence would resolve it: External validation on datasets with high representation of currently underrepresented groups, specifically analyzing calibration slopes and alert density by race and region.

## Limitations

- Model performance may degrade when applied to routine clinical data due to differences between trial populations and real-world EHR populations
- Limited racial diversity in validation cohorts (82.3% and 90.6% White) prevents claims about equitable performance across demographic groups
- Fixed 8-feature minimal set, while clinically feasible, may miss important prognostic signals available in modern EHRs

## Confidence

- **High confidence**: GRU architecture correctly captures sequential patient deterioration (supported by 151-day median lead time for true positives)
- **Medium confidence**: External validation results generalize to routine clinical settings (external cohort characteristics differ from typical EHR populations)
- **Medium confidence**: Calibration prioritization over pure discrimination is clinically appropriate (though RSF achieved better specificity)

## Next Checks

1. Re-calibrate the risk threshold on a local EHR cohort with different mortality prevalence to verify calibration stability
2. Test model performance when incorporating additional routinely available features (PSA trends, imaging findings) to assess potential performance ceiling
3. Conduct prospective deployment study measuring actual clinical workflow integration and alert response patterns in a palliative care setting