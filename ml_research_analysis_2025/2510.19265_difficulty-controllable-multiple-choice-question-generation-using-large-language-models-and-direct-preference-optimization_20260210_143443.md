---
ver: rpa2
title: Difficulty-Controllable Multiple-Choice Question Generation Using Large Language
  Models and Direct Preference Optimization
arxiv_id: '2510.19265'
source_url: https://arxiv.org/abs/2510.19265
tags:
- difficulty
- question
- questions
- reading
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a difficulty-controllable multiple-choice
  question generation method that leverages large language models (LLMs) and direct
  preference optimization (DPO) to improve the accuracy of difficulty control. The
  proposed approach addresses two key limitations in existing methods: the inability
  to directly generate multiple-choice questions and the lack of explicit optimization
  for difficulty control accuracy.'
---

# Difficulty-Controllable Multiple-Choice Question Generation Using Large Language Models and Direct Preference Optimization

## Quick Facts
- arXiv ID: 2510.19265
- Source URL: https://arxiv.org/abs/2510.19265
- Authors: Yuto Tomikawa; Masaki Uto
- Reference count: 40
- Primary result: DPO improves difficulty control accuracy compared to standard fine-tuning while maintaining question quality

## Executive Summary
This study introduces a difficulty-controllable multiple-choice question generation method that leverages large language models (LLMs) and direct preference optimization (DPO) to improve accuracy in difficulty control. The method addresses limitations in existing approaches by using DPO to explicitly optimize for difficulty alignment rather than just likelihood maximization. Experiments on the RACE dataset demonstrate that the DPO-enhanced model significantly improves difficulty control accuracy compared to standard fine-tuning, while maintaining high quality in terms of fluency, content relevance, and answerability.

## Method Summary
The proposed method uses Llama 3.1 as the base LLM with QLoRA adapters for efficient fine-tuning. It employs Item Response Theory (Rasch model) to quantify question difficulty using responses from 77 diverse QA systems as synthetic learners. The approach uses supervised fine-tuning followed by DPO training, where preference pairs are constructed based on difficulty control accuracy. The model generates questions and options tailored to specified difficulty levels, with difficulty quantified on a scale from -3 to +3.

## Key Results
- DPO significantly improves difficulty control accuracy compared to standard fine-tuning (lower MAE)
- The method maintains high quality metrics (fluency, content relevance, answerability)
- Generated questions have higher Fisher information, suggesting enhanced ability estimation accuracy
- Few-shot learning is insufficient for achieving difficulty control, highlighting advantages of the proposed method

## Why This Works (Mechanism)

### Mechanism 1: Preference Optimization Aligns Generation with Difficulty Targets
DPO improves difficulty control accuracy by explicitly optimizing for alignment between specified and realized difficulty. The sigmoid-weighted log-ratio objective increases likelihood of preferred outputs while regularizing toward the SFT reference. This directly minimizes the gap between target difficulty and generated difficulty, unlike likelihood-maximization alone.

### Mechanism 2: IRT-Based Difficulty Quantification via QA System Proxies
IRT with QA systems as synthetic learners produces difficulty estimates that correlate with reasoning complexity. The Rasch model defines the difficulty-ability relationship, enabling controllable generation. The 77 QA systems with varied architectures provide diverse response patterns for reliable difficulty calibration.

### Mechanism 3: Difficulty-Controllable Prompting with Fine-Tuned LLM
Fine-tuning an LLM with explicit difficulty conditioning enables controllable generation where few-shot prompting fails. Numerical difficulty tokens are learned as control codes that modulate internal generation strategies, establishing the mapping between target difficulty and question complexity.

## Foundational Learning

- **Concept: Item Response Theory (Rasch Model)**
  - Why needed: Provides mathematical foundation for quantifying difficulty on a scale interpretable relative to learner ability
  - Quick check: Given a question with difficulty b = 1.0, what is the probability a learner with ability θ = -1.0 answers correctly?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: Replaces RLHF's unstable reward-model training with a supervised objective
  - Quick check: Why does DPO require a reference model (ϕ_sft) in addition to the trainable model?

- **Concept: Fisher Information in IRT**
  - Why needed: Explains why improved difficulty control enhances ability estimation precision
  - Quick check: For a Rasch item with b = 0, at what θ value is Fisher information maximized?

## Architecture Onboarding

- **Component map**: RACE passages → QA system ensemble → IRT difficulty estimation → D_quadruples → SFT → DPO → Llama-3.1-8B with QLoRA
- **Critical path**: 1) Build 77 QA systems from 10 architectures with varied training fractions, 2) Collect binary responses and filter systems with <30% accuracy, 3) Estimate b_i via marginal MLE (Rasch), 4) SFT on difficulty-conditioned prompts, 5) Construct DPO pairs with random negatives, 6) DPO training with β-regularized objective
- **Design tradeoffs**: QA system diversity vs. noise (30% filter applied), random vs. difficulty-ranked negatives in DPO pairs, QLoRA memory efficiency vs. capacity limitations
- **Failure signatures**: Flat difficulty curve (SFT failed), high MAE with monotonic trend (calibration off), degraded fluency/answerability (over-optimized difficulty)
- **First 3 experiments**: 1) Validate QA-proxy assumption by comparing with human-labeled difficulty, 2) Ablate DPO pair construction with difficulty-ranked negatives, 3) Test cross-dataset generalization on different reading comprehension dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Difficulty estimation reliability remains uncertain without direct human validation
- Preference pair construction uses simple random sampling which may not provide strong signals
- Generalization beyond RACE dataset has not been verified
- Quality-accuracy trade-off not thoroughly explored

## Confidence
- **High Confidence**: DPO improves difficulty control accuracy compared to standard fine-tuning
- **Medium Confidence**: IRT-based difficulty estimation via QA systems produces meaningful scales
- **Medium Confidence**: Few-shot learning is insufficient for difficulty control within RACE dataset

## Next Checks
1. **Human Validation Study**: Compare IRT difficulty estimates from 77 QA systems against human expert ratings for a subset of generated questions to validate the QA-proxy approach.
2. **Cross-Dataset Transfer**: Apply trained DPO model to generate questions from a different reading comprehension dataset (e.g., SQuAD-MC) to evaluate generalization and maintain MAE performance.
3. **Ablation on Pair Construction**: Replace random negative sampling in DPO training with difficulty-ranked negatives based on estimation error, comparing MAE and difficulty control accuracy.