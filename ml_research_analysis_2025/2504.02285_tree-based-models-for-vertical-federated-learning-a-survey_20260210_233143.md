---
ver: rpa2
title: 'Tree-based Models for Vertical Federated Learning: A Survey'
arxiv_id: '2504.02285'
source_url: https://arxiv.org/abs/2504.02285
tags:
- data
- party
- tbms
- learning
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys tree-based models for vertical federated learning
  (VFL), categorizing them into feature-gathering and label-scattering models based
  on their communication and computation protocols. The study discusses their characteristics,
  privacy protection mechanisms, and applications.
---

# Tree-based Models for Vertical Federated Learning: A Survey

## Quick Facts
- arXiv ID: 2504.02285
- Source URL: https://arxiv.org/abs/2504.02285
- Reference count: 40
- Key outcome: Surveys tree-based VFL models, showing feature-gathering and label-scattering approaches achieve similar performance, with trade-offs in privacy, communication, and computation.

## Executive Summary
This survey paper systematically categorizes tree-based models for vertical federated learning (VFL) into feature-gathering and label-scattering protocols based on their communication and computation patterns. The study evaluates their privacy protection mechanisms, including differential privacy for feature-gathering and homomorphic encryption for label-scattering, and demonstrates through experiments on real datasets that both approaches achieve comparable model utility while facing distinct trade-offs in communication frequency and computational overhead.

## Method Summary
The paper surveys existing tree-based VFL approaches by analyzing their fundamental communication protocols: feature-gathering (where data parties share ordinal information about features with the label-owning task party) and label-scattering (where the task party shares gradients and the data parties compute local statistics). The survey evaluates privacy protection mechanisms (DP, HE, SS) and empirically compares the trade-offs between these approaches using standard datasets, focusing on accuracy, communication overhead, and computational costs.

## Key Results
- Both feature-gathering and label-scattering models achieve similar performance (AUC/MSE) on benchmark datasets
- Feature-gathering models require less frequent communication but suffer accuracy degradation under differential privacy
- Label-scattering models preserve accuracy better but incur higher communication frequency and computational costs due to homomorphic encryption
- The privacy-utility trade-off is empirically validated, with stronger privacy protection directly impacting model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Tree-based models in Vertical Federated Learning (VFL) can be trained by decomposing the optimal splitting rule calculation into distributed sub-tasks, categorized as either "feature-gathering" or "label-scattering."
- **Mechanism:** In **feature-gathering** protocols, data parties map features to ordinal numbers (or buckets) and send these to the task party (label owner), who computes the gain locally. In **label-scattering** protocols, the task party broadcasts gradients (label-related information) to data parties, who compute local statistics and return aggregated results.
- **Core assumption:** The intermediate information exchanged (ordinal numbers or gradients) is sufficient to approximate the optimal split without sharing raw feature values or raw labels directly.
- **Evidence anchors:**
  - [Section 3, Eq. 10 & 11]: Formalizes the decomposition of the splitting rule function $h(x,y)$ into sub-tasks $g(x)$ and $e(y)$.
  - [Section 3.1.1]: Describes the "pointer" mechanism where the task party identifies the split index and queries the data party for the actual value.
  - [corpus]: Related work like *PBM-VFL* supports the use of differential privacy mechanisms in this decomposition, though specific protocols vary.
- **Break condition:** If the communication channel leaks the mapping between "pointers" and specific feature values, or if the sample alignment (PSI) fails, the mechanism compromises privacy or utility.

### Mechanism 2
- **Claim:** Differential Privacy (DP) provides a mechanism for privacy protection in feature-gathering models by perturbing the ordinal numbers of feature values.
- **Mechanism:** Algorithms like **FederBoost** and **OpBoost** sort data into buckets and apply probabilistic mapping (perturbation) to the ordinal numbers before sharing. This obscures the exact value of a specific sample while preserving the statistical distribution necessary for split finding.
- **Core assumption:** The utility of the tree model is robust to noise in the relative ordering of feature values up to a specific privacy budget ($\epsilon$).
- **Evidence anchors:**
  - [Section 3.1.3]: Details how FederBoost uses bucketization and shuffling to protect ordinal numbers.
  - [Section 5.3.2, Fig 10 & 11]: Empirical results showing the trade-off where model utility (MSE/AUC) degrades as privacy protection strength increases (probability of staying in the correct bucket decreases).
  - [corpus]: *PBM-VFL* also suggests combining Secure Multi-Party Computation with DP, corroborating the viability of this mechanism.
- **Break condition:** If the privacy budget ($\epsilon$) is set too strictly (low $\epsilon$), the noise destroys the correlation between features and labels, rendering the model no better than random guessing.

### Mechanism 3
- **Claim:** Homomorphic Encryption (HE) and Secret Sharing (SS) enable label-scattering models to compute optimal splits on encrypted data, preventing the leakage of label-related gradients.
- **Mechanism:** The task party encrypts gradients (e.g., $g_i, h_i$ in XGBoost) using additive homomorphic encryption (e.g., Paillier). Data parties perform sorting and summation on the ciphertexts and return encrypted sums. The task party decrypts the result to find the maximum gain without exposing individual gradient values.
- **Core assumption:** The computational overhead of encryption/decryption and the communication overhead of transmitting ciphertexts are acceptable given the security requirements.
- **Evidence anchors:**
  - [Section 3.2.3]: Describes SecureBoost's use of HE to aggregate values in buckets without revealing individual $e(y)$ values.
  - [Section 5.3.3, Fig 12 & 13]: Demonstrates that communication overhead and time cost grow linearly or super-linearly with key size (security strength).
  - [corpus]: *Vertical Federated Learning in Practice* highlights that while encryption protects data, it introduces significant system complexity.
- **Break condition:** If the key size is insufficient, cryptographic security is compromised. If the system cannot handle the latency of encryption operations, the training process becomes impractically slow.

## Foundational Learning

- **Concept: Vertical vs. Horizontal Federated Learning**
  - **Why needed here:** The paper focuses exclusively on VFL where features are partitioned. Understanding this distinction is critical because the communication bottlenecks and privacy risks differ fundamentally from HFL (where samples are partitioned).
  - **Quick check question:** Do you understand why VFL requires Private Set Intersection (PSI) before training, while HFL typically does not?

- **Concept: Gradient Boosting Decision Trees (GBDT) & XGBoost**
  - **Why needed here:** The "Label-scattering" approach relies heavily on the mathematical properties of GBDT/XGBoost, specifically the use of first-order ($g_i$) and second-order ($h_i$) gradients to calculate split gain.
  - **Quick check question:** Can you explain why the label owner needs to compute and share gradients in a boosting scenario, whereas they might only share labels in a Random Forest?

- **Concept: The Privacy-Utility Trade-off**
  - **Why needed here:** The paper experimentally demonstrates that stronger privacy (via DP or HE) directly competes with model utility or system efficiency. A practitioner must balance these based on the application domain.
  - **Quick check question:** If you prioritize model accuracy above all else, which privacy mechanism (DP vs. HE) might you lean toward, accepting the associated costs?

## Architecture Onboarding

- **Component map:**
  - Data Party (Passive Party) -> Task Party (Active Party) -> Model Output
  - Coordinator (Optional) -> Key Management / PSI

- **Critical path:**
  1.  **Alignment:** Private Set Intersection (PSI) to align sample IDs across parties.
  2.  **Information Exchange:** Transmission of feature-ordinal-info (FG) or encrypted gradients (LS).
  3.  **Split Finding:** Calculation of optimal gain (local or federated).
  4.  **Tree Update:** Updating tree structure and indicator vectors.

- **Design tradeoffs:**
  - **Feature-Gathering:** Lower communication frequency (Table 4), but requires desensitization (DP) which hurts accuracy.
  - **Label-Scattering:** Potentially higher accuracy preservation (no noise), but 2-5x higher communication frequency and significant computational cost due to HE.

- **Failure signatures:**
  - **Stagnant Accuracy:** In FG models, often caused by excessive noise in ordinal numbers (low probability of staying in the correct bucket).
  - **Timeouts/High Latency:** In LS models, caused by large key sizes in HE or frequent communication rounds.
  - **Inference Errors:** Occurs if the "pointer" logic fails (Task party cannot map the split index back to the feature owner's value).

- **First 3 experiments:**
  1.  **Protocol Baseline:** Replicate Table 3 to compare the raw accuracy of FG vs. LS on a dataset (e.g., Adult or Credit) without privacy mechanisms to establish a performance baseline.
  2.  **Privacy Impact (FG):** Replicate Figure 10/11 to measure the drop in AUC/MSE as the noise level (probability of mapping to an incorrect bucket) increases in Feature-Gathering models.
  3.  **Efficiency Stress Test:** Replicate Figure 12/13 to measure the time/communication cost of Label-Scattering (HE) as the key size increases, determining the "breaking point" for your infrastructure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can tree-based VFL models effectively automate feature crossing to generate high-order features without compromising privacy or efficiency?
- Basis in paper: [Explicit] Section 6.2.2 states that participants lack knowledge of features owned by others, leading to missed opportunities for high-order features, and notes this is more challenging for TBMs than for neural networks.
- Why unresolved: Current protocols exchange gradients or ordinal numbers but lack efficient, privacy-preserving methods to search for and generate cross-party feature combinations.
- What evidence would resolve it: A VFL algorithm that integrates automated feature crossing and demonstrates improved accuracy over baseline models on datasets requiring high-order interactions.

### Open Question 2
- Question: How can non-overlapping samples be incorporated into the training process to broaden the applicability of tree-based VFL?
- Basis in paper: [Explicit] Section 6.2.4 notes that current training relies on overlapping samples, which limits applicability, and identifies designing mechanisms for non-overlapping samples as a valuable research direction.
- Why unresolved: VFL relies on aligned sample IDs for joint computation; utilizing unaligned data requires transferring knowledge across distinct sample spaces securely.
- What evidence would resolve it: A framework that successfully incorporates unaligned samples (e.g., via data augmentation) to improve model utility or robustness.

### Open Question 3
- Question: How can incentive mechanisms be designed to reward participants based on feature importance without biasing the model toward feature-rich parties?
- Basis in paper: [Explicit] Section 6.2.3 highlights the need for mechanisms to ensure models are not biased toward participants with more features and to reward those contributing important features.
- Why unresolved: Calculating fair contributions (e.g., Shapley values) in a distributed setting is computationally expensive and may leak information.
- What evidence would resolve it: A protocol that quantifies contribution and distributes rewards proportional to marginal utility without compromising privacy.

### Open Question 4
- Question: How can the communication overhead of label-scattering models be reduced to match the efficiency of feature-gathering models?
- Basis in paper: [Inferred] Section 5.3.1 empirically shows label-scattering models require "several times" more communication frequency than feature-gathering models; Section 6.2.1 explicitly lists reducing communication overhead as a critical area for improvement.
- Why unresolved: Label-scattering protocols require frequent synchronization for broadcasting label-related information and partitioning data at every node.
- What evidence would resolve it: An optimized protocol that reduces communication frequency or volume while maintaining model utility and privacy guarantees.

## Limitations
- Experimental validation is limited to relatively small datasets, raising questions about scalability to industrial-scale applications
- The privacy-utility trade-off is demonstrated empirically but not formally analyzed for worst-case scenarios
- The paper does not address robustness to data drift or non-IID feature distributions in practical deployments

## Confidence
- **High:** Core concepts of VFL and tree-based model decomposition
- **Medium:** Empirical results showing privacy-utility trade-off on benchmark datasets
- **Low:** Generalizability to large-scale, heterogeneous industrial applications and scalability challenges

## Next Checks
1. **Scalability Test:** Evaluate the communication and computational costs of feature-gathering and label-scattering models on large-scale datasets (e.g., millions of samples) to assess real-world feasibility.
2. **Robustness Analysis:** Test the models' performance under data drift or non-IID feature distributions to ensure reliability in dynamic environments.
3. **Privacy Leakage Assessment:** Conduct a formal privacy analysis (e.g., membership inference attacks) to validate the claimed privacy guarantees of differential privacy and homomorphic encryption mechanisms.