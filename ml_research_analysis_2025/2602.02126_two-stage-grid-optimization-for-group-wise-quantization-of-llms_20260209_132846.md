---
ver: rpa2
title: Two-Stage Grid Optimization for Group-wise Quantization of LLMs
arxiv_id: '2602.02126'
source_url: https://arxiv.org/abs/2602.02126
tags:
- gptq
- quantization
- group
- scales
- group-wise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses accuracy degradation in low-bit quantization
  of large language models by proposing a two-stage optimization framework for group-wise
  quantization. The first stage initializes group scales using input statistics to
  minimize group-wise reconstruction loss, while the second stage refines these scales
  using coordinate descent to minimize layer-wise reconstruction loss, explicitly
  accounting for inter-group correlations and quantization errors from preceding layers.
---

# Two-Stage Grid Optimization for Group-wise Quantization of LLMs

## Quick Facts
- arXiv ID: 2602.02126
- Source URL: https://arxiv.org/abs/2602.02126
- Reference count: 0
- Improves 2-bit quantization accuracy by 4-6% and preserves near-FP performance at 3-bit via two-stage scale optimization

## Executive Summary
This work addresses accuracy degradation in low-bit quantization of large language models by proposing a two-stage optimization framework for group-wise quantization. The first stage initializes group scales using input statistics to minimize group-wise reconstruction loss, while the second stage refines these scales using coordinate descent to minimize layer-wise reconstruction loss, explicitly accounting for inter-group correlations and quantization errors from preceding layers. Experiments on Llama2 and Llama3 models demonstrate consistent accuracy improvements over GPTQ, achieving 4-6% accuracy gains in 2-bit quantization and nearly preserving FP performance at 3-bit with negligible computational overhead.

## Method Summary
The method augments GPTQ quantization with two optimization stages: Stage 1 performs input-aware scale initialization using the diagonal Hessian blocks (H_{i,i}) to minimize group-wise reconstruction loss before GPTQ quantization; Stage 2 then refines these scales via coordinate descent on the full layer-wise objective, capturing inter-group correlations through off-diagonal Hessian blocks (H_{i,j}) while accounting for quantization errors propagated from preceding layers via the deviation correlation matrix R = E[ΔX X^T].

## Key Results
- 2-bit quantization achieves 4-6% accuracy gains over GPTQ baseline
- 3-bit quantization preserves near-FP performance with negligible accuracy drop
- Computational overhead remains minimal despite two-stage optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Initializing group scales using input-aware statistics before GPTQ reduces group-wise reconstruction error compared to weight-only scale selection.
- **Mechanism**: Standard GPTQ determines scales based solely on weight perturbation ‖Δw_i‖², ignoring input statistics X_i. Stage 1 minimizes the group-wise loss ‖Δw_i^T X_i‖² by using the diagonal Hessian block H_{i,i} = E[X_i X_i^T], extracting it from the precomputed Hessian H without additional computation. This aligns scale initialization with the actual reconstruction objective.
- **Core assumption**: The diagonal Hessian blocks H_{i,i} provide sufficient local information to meaningfully improve scale initialization over weight-only heuristics.
- **Evidence anchors**:
  - [abstract]: "In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics."
  - [section 3.1]: Formulates the optimization problem min_{s_i>0} (s_i·w_int,i − w_i)^T H_{i,i} (s_i·w_int,i − w_i).
  - [corpus]: RSQ (arXiv:2503.01820) similarly emphasizes input importance, suggesting input-aware quantization is a convergent direction.
- **Break condition**: If input distributions are near-uniform or calibration data is unrepresentative, H_{i,i} may not capture meaningful sensitivity, limiting gains.

### Mechanism 2
- **Claim**: Post-GPTQ scale refinement via coordinate descent on the full layer-wise objective captures inter-group correlations that independent scale optimization ignores.
- **Mechanism**: GPTQ assumes H=I (diagonal), reducing the loss to independent group terms. The full loss L(s) = Σ_{i,j} (s_i w_int,i − w_i)^T H_{i,j} (s_j w_int,j − w_j) includes off-diagonal blocks H_{i,j} (i≠j). Stage 2 freezes integer weights and iteratively updates one scale s_i at a time via the closed-form rule s*_i = s_i + (w_int,i^T H_{i,:}(w−q)) / (w_int,i^T H_{i,i} w_int,i), incorporating all cross-group terms.
- **Core assumption**: Inter-group correlations in the Hessian are strong enough that joint optimization meaningfully outperforms independent optimization.
- **Evidence anchors**:
  - [abstract]: "In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss."
  - [section 3.2]: Derives the closed-form update rule from ∂L/∂s_i = 0.
  - [corpus]: Weak direct evidence; neighbor papers focus on bit allocation or token importance, not inter-group scale coupling.
- **Break condition**: If groups are weakly correlated (H_{i,j} ≈ 0 for i≠j), coordinate descent provides negligible improvement over independent optimization.

### Mechanism 3
- **Claim**: Incorporating deviation correlation R = E[ΔX X^T] into scale updates for subsequent layers mitigates error accumulation from preceding quantization.
- **Mechanism**: After the first layer, inputs deviate from full-precision: ΔX = X − X̃. The loss becomes L(s) = (q−w)^T H (q−w) + 2w^T R (q−w) + c. The derivative ∂(w^T R (q−w))/∂s_i = w^T R_i w_int,i adds a correction term to the update rule, explicitly accounting for propagated quantization error.
- **Core assumption**: Accumulated input deviation ΔX is correlated with original inputs X in a way that can be captured by R, and this correlation is layer-stable during calibration.
- **Evidence anchors**:
  - [abstract]: "Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation."
  - [section 3.3]: Expands the loss to include the R term and derives the corrected update rule.
  - [corpus]: No direct corpus evidence for this specific mechanism.
- **Break condition**: If calibration sequences are too short or unrepresentative, R estimation may be noisy, reducing correction effectiveness.

## Foundational Learning

- **Concept: Hessian-based reconstruction loss (GPTQ framework)**
  - **Why needed here**: The paper's entire objective builds on GPTQ's formulation min_q (q−w)^T H (q−w) where H = E[XX^T]. Understanding this is essential to see why ignoring off-diagonal blocks is suboptimal.
  - **Quick check question**: Why does the Hessian H approximate output sensitivity to weight perturbations?

- **Concept: Group-wise vs. channel-wise quantization**
  - **Why needed here**: The paper targets group-wise quantization specifically. Grasping why a single scale per channel fails at INT2 (high intra-channel variance, outliers) motivates the problem.
  - **Quick check question**: What is the trade-off between group size and quantization accuracy?

- **Concept: Coordinate descent for coupled optimization**
  - **Why needed here**: Stage 2 uses coordinate descent to solve a coupled quadratic problem. Understanding why this works (convex, separable per variable) is key to implementation.
  - **Quick check question**: Why is coordinate descent efficient here compared to joint gradient descent?

## Architecture Onboarding

- **Component map**: Hessian precomputation -> Stage 1 initialization -> GPTQ quantization -> Stage 2 refinement -> Optimized scales and quantized weights

- **Critical path**:
  1. Hessian H must be correctly precomputed and block-extractable.
  2. Stage 1 grid search must use the correct clipping constraint when computing w_int,i.
  3. Stage 2 must freeze w_int and iterate over scales only.
  4. For layers 2+, R must be accumulated from prior-layer quantization errors.

- **Design tradeoffs**:
  - **Group size**: Smaller groups (e.g., 32 vs. 64) increase scale count, improving accuracy but raising dequantization overhead and memory.
  - **Grid search granularity**: Finer grid search in Stage 1 improves initialization but increases compute.
  - **Coordinate descent iterations**: More iterations reduce loss but with diminishing returns; paper suggests convergence is fast due to closed-form updates.

- **Failure signatures**:
  - **No improvement over GPTQ**: Likely H_{i,i} extraction bug or Stage 1 not running before GPTQ.
  - **Divergent scales in Stage 2**: Check that w_int is frozen and update rule denominators (w_int,i^T H_{i,i} w_int,i) are non-zero.
  - **Degraded accuracy in deeper layers**: R term may be missing or incorrectly computed; verify ΔX accumulation.

- **First 3 experiments**:
  1. **Ablation on Stage 1**: Run GPTQ alone vs. GPTQ + Stage 1 on Llama3-8B INT2 (group size 64). Measure Wiki2 PPL and verify the paper's reported drop (e.g., 15.07 → ~13.8).
  2. **Ablation on Stage 2**: Run GPTQ + Stage 2 (skip Stage 1) to isolate inter-group correlation benefits. Confirm PPL improvement (e.g., 15.07 → ~13.0).
  3. **Full pipeline on small model**: Apply both stages to Llama3.2-1B-Instruct INT2 (group size 64). Check if C4 PPL approaches ~149 (paper's result) vs. GPTQ's ~429. Verify runtime overhead is <30%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed two-stage optimization framework be effectively extended to weight-activation quantization, or does it strictly require activations to remain in full precision?
- Basis in paper: [explicit] The paper explicitly states in Section 4: "we perform weight-only quantization while keeping activations in FP," leaving the interaction with activation quantization unexplored.
- Why unresolved: The method relies on input statistics ($X$) and deviation terms ($\Delta X$) to optimize scales. Introducing activation quantization would alter the error distribution and reconstruction loss landscape, potentially violating the assumptions used to derive the closed-form update rules.
- What evidence would resolve it: Experimental results applying the method to weight-activation quantization settings (e.g., W4A8 or W4A4) on standard LLM benchmarks, measuring perplexity and accuracy retention.

### Open Question 2
- Question: Would a joint optimization of integer weights and group scales yield further accuracy improvements compared to the current sequential approach of freezing weights during scale refinement?
- Basis in paper: [inferred] In Section 3.1, the authors state they "freeze the integer weights obtained via GPTQ" during the second stage to enable efficient coordinate descent.
- Why unresolved: Freezing weights simplifies the optimization problem but restricts the solution space. Allowing weights to vary during scale refinement might minimize the layer-wise reconstruction loss further, though it would increase computational complexity.
- What evidence would resolve it: An ablation study comparing the current fixed-weight refinement against an iterative optimization loop where both weights and scales are updated, analyzing the trade-off between accuracy gains and runtime overhead.

### Open Question 3
- Question: Is the two-stage scale optimization effective for Mixture-of-Experts (MoE) architectures given their sparse activation patterns and distinct parameter distributions?
- Basis in paper: [inferred] Experimental validation is limited to dense Transformer models (Llama 2 and Llama 3), as shown in Tables 1 and 2.
- Why unresolved: MoE models possess routing mechanisms and sparse expert utilization which create highly non-uniform activation distributions ($H_{i,j}$). It is unclear if the group-wise scale initialization and refinement derived for dense models can handle the statistical instability of sparse, routed inputs.
- What evidence would resolve it: Evaluation of the method on MoE models (e.g., Mixtral or DeepSeek-MoE) using the specified group sizes (32, 64), reporting perplexity and zero-shot accuracy.

### Open Question 4
- Question: Can the coordinate descent-based refinement be successfully integrated with other PTQ methods like AWQ or QuaRot, or is it mathematically coupled to the GPTQ reconstruction objective?
- Basis in paper: [inferred] The paper frames the method as an augmentation for GPTQ ("Our method augments GPTQ") and derives update rules specifically based on the GPTQ reconstruction loss equation (3).
- Why unresolved: Other methods like AWQ optimize for different objectives (e.g., preserving salient weights) or use rotation (QuaRot). The closed-form update relies on the Hessian structure specific to the GPTQ framework, which may not align with the optimization landscapes of alternative algorithms.
- What evidence would resolve it: Applying the scale refinement stage to models quantized with AWQ or QuaRot and measuring if significant accuracy improvements are observed without destabilizing the base method.

## Limitations

- Input statistic dependency: Gains rely heavily on calibration data quality; poor calibration may limit improvements
- Unproven generality: Results limited to Llama2/Llama3; unknown performance on other model families or tasks
- Computational overhead: Stage 1 grid search and Stage 2 iterations add computation, though claimed negligible

## Confidence

- **High confidence** in mathematical formulation and derivation of both stages
- **Medium confidence** in claimed accuracy improvements based on limited model set
- **Low confidence** in practical deployment implications for trillion-parameter models

## Next Checks

1. **Ablation on Stage 1**: Run GPTQ alone vs. GPTQ + Stage 1 on Llama3-8B INT2 (group size 64). Measure Wiki2 PPL and verify the paper's reported drop (e.g., 15.07 → ~13.8).

2. **Ablation on Stage 2**: Run GPTQ + Stage 2 (skip Stage 1) to isolate inter-group correlation benefits. Confirm PPL improvement (e.g., 15.07 → ~13.0).

3. **Full pipeline on small model**: Apply both stages to Llama3.2-1B-Instruct INT2 (group size 64). Check if C4 PPL approaches ~149 (paper's result) vs. GPTQ's ~429. Verify runtime overhead is <30%.