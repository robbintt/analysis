---
ver: rpa2
title: Sequences of Logits Reveal the Low Rank Structure of Language Models
arxiv_id: '2510.24966'
source_url: https://arxiv.org/abs/2510.24966
tags:
- rank
- downsized
- matrix
- random
- logit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the low-rank structure of large language
  models (LLMs) through the extended logit matrix, which encodes the log-probabilities
  of token sequences. Empirical results show that for a wide range of modern LLMs,
  these matrices are well-approximated by low-rank matrices, with singular values
  decaying according to a power law.
---

# Sequences of Logits Reveal the Low Rank Structure of Language Models

## Quick Facts
- **arXiv ID**: 2510.24966
- **Source URL**: https://arxiv.org/abs/2510.24966
- **Reference count**: 40
- **Key outcome**: Extended logit matrices of LLMs are well-approximated by low-rank matrices with power-law decaying singular values, enabling generation via "Lingen" using only unrelated prompts

## Executive Summary
This paper investigates the low-rank structure of large language models by analyzing the extended logit matrix, which encodes the log-probabilities of token sequences. The authors empirically demonstrate that for a wide range of modern LLMs, these matrices exhibit low-rank structure with singular values decaying according to a power law. This structure emerges during pre-training and is absent in untrained models. The key innovation is the Lingen algorithm, which can generate coherent continuations of target prompts by only querying the model on unrelated or nonsensical prompts. This method achieves lower KL divergence to the true model than several strong baselines. Theoretically, the paper establishes an equivalence between low logit rank and a simple generative model called ISAN, providing insights into the computational complexity of learning from samples versus logit queries.

## Method Summary
The authors construct the extended logit matrix $L_M(H,F)$ by sampling histories $h$ from the dataset and futures $f$ from the dataset, then computing the mean-centered logits for the top-$k$ tokens following each future. For low-rank analysis, they compute the SVD of this matrix and measure the power-law decay of singular values. The Lingen algorithm works by first solving a linear regression to find weights $v$ such that $L_M(\{h_{targ}\}, F) \approx v^\top L_M(H, F)$, then generating token-by-token using $\text{softmax}(\sum_{h \in H} v_h \cdot L_M[\cdot | h \circ z_{1:t-1}])$. The analysis is performed on multiple models including OLMo-1b/7b, Llama-1b, Gemma-1b, and Mamba-1.4b, primarily using the wiki split of olmo-mix-1124.

## Key Results
- Extended logit matrices exhibit low-rank structure with singular values decaying as a power law with exponent $\alpha \approx 0.536$
- Linear relationships between histories transfer across different futures and across models
- Lingen achieves lower KL divergence than baselines including intermediate checkpoints and restricted-context models
- Low-rank structure is absent in untrained models but emerges during pre-training
- Theoretical equivalence established between low logit rank and time-varying ISAN models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extended logit matrices have approximately low rank, enabling compressed representations of histories
- **Mechanism:** For histories $h$ and futures $f$, $\log \Pr[f|h] \approx \langle \phi(h), \psi(f) \rangle$ where $\phi, \psi$ are low-dimensional embeddings
- **Core assumption:** Power-law decay of singular values continues as matrix dimensions scale
- **Evidence anchors:** Singular values decay with exponent $\alpha \approx 0.536$; if $\alpha > 0.5$, constant-rank approximation suffices
- **Break condition:** If $\alpha < 0.5$ at full scale, required rank grows with dimension

### Mechanism 2
- **Claim:** Linear relationships between histories transfer across different futures and across models
- **Mechanism:** The row kernel of $L_M(H,F)$ is approximately preserved when $F$ changes to nonsensical/random futures
- **Core assumption:** Column spaces of low-rank approximations should overlap significantly across conditions
- **Evidence anchors:** Principal angles between column spaces show cosines near 1, exceeding random baselines
- **Break condition:** Untrained model shows near-random overlap with trained models

### Mechanism 3
- **Claim:** Lingen generates coherent continuations for target prompts using only unrelated prompts
- **Mechanism:** Express target history $h_{targ}$ as linear combination $L_M(h_{targ}, F) \approx v^\top L_M(H,F)$ via regression
- **Core assumption:** Regression coefficients $v$ computed on one future distribution transfer to generated futures
- **Evidence anchors:** Lingen achieves lower KL divergence than baselines (intermediate checkpoints, restricted context)
- **Break condition:** If futures during generation diverge too far from regression distribution

## Foundational Learning

- **Concept:** Singular value decomposition and low-rank approximation
  - **Why needed here:** The entire framework rests on whether logit matrices are approximately low-rank
  - **Quick check question:** Given a matrix with singular values $[10, 3, 1, 0.1, 0.01]$, what rank-2 approximation error (Frobenius norm) would you expect?

- **Concept:** The softmax bottleneck (Yang et al., 2017)
  - **Why needed here:** This paper extends the single-token observation to sequences
  - **Quick check question:** Why does a vocabulary size of 50K with hidden dimension 4K create a rank constraint on next-token distributions?

- **Concept:** Linear dynamical systems and state-space models
  - **Why needed here:** The ISAN model is a time-varying LDS; the equivalence grounds empirical findings
  - **Quick check question:** In $x_t = A x_{t-1} + B u_t$, how does the dimension of $x$ relate to the rank of the observability matrix?

## Architecture Onboarding

- **Component map:**
  Extended Logit Matrix L_M(H,F) -> SVD, rank-r truncation -> Low-rank Approximation A -> row/column space analysis -> Linear Dependencies (kernels) -> regression to find v -> Lingen Generation Algorithm

- **Critical path:**
  1. Construct $L_{M,k}(H,F)$ by sampling histories/futures from dataset, computing logits for top-$k$ tokens per future
  2. Verify low-rank structure via singular value decay (check $\alpha > 0.5$)
  3. For generation: regress target row onto basis rows using one future set, apply coefficients to new futures

- **Design tradeoffs:**
  - $|H|, |F|$ size vs. memory: Full matrix has $|H| \times |F| \times |\Sigma|$ entries; use $L_{M,k}$ with $k=50$
  - Rank $r$ vs. KL divergence: Higher rank improves approximation but reduces compression benefit
  - In-distribution vs. out-of-distribution targets: Option 1 (wiki → wiki) outperforms Option 2 (wiki → nonsense)

- **Failure signatures:**
  - Power-law fit fails → matrix may not have exploitable structure (seen at training step 0)
  - Generated text derails after first token → using single-token logit matrix instead of extended
  - KL divergence exceeds baselines → linear combination doesn't transfer; check subspace overlap

- **First 3 experiments:**
  1. Replicate Figure 2 on your model: compute singular values of $L_{M,50}(H,F)$ for $|H|=|F|=1000$, fit power law exponent
  2. Test transfer: compute principal angles between low-rank approximations of $L_M(H,F)$ and $L_M(H,F')$ where $F'$ is token-shuffled
  3. Minimal Lingen: implement Algorithm 1 with $|H|=100$ basis histories, compare KL to true model at each token position

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How and why does the low-rank structure of the extended logit matrix emerge during the early stages of pre-training?
- **Basis in paper:** Conclusion and Section 3.1: "How and why does low-rank structure in the logit matrix emerge early in pre-training?"
- **Why unresolved:** The paper empirically observes that untrained models lack this structure and that it appears early in training, but it does not provide a mechanistic explanation for the origin of this phenomenon
- **What evidence would resolve it:** A theoretical framework or detailed empirical study tracking singular value decay from initialization through convergence

### Open Question 2
- **Question:** Can the Lingen generation procedure be used to reliably bypass safety guardrails (jailbreaks) in aligned language models?
- **Basis in paper:** Section 3.3 and Conclusion: "Can we use Lingen or some variation of it to bypass safety guardrails and generate responses to unsafe prompts?"
- **Why unresolved:** The paper demonstrates the ability to generate coherent text from unrelated/nonsensical prompts, suggesting a potential attack vector, but it does not validate this specifically against safety filters or harmful prompts
- **What evidence would resolve it:** Experiments applying Lingen to adversarial scenarios to measure success rates in eliciting harmful content

### Open Question 3
- **Question:** Can meaningful concepts and features be extracted from the representation space defined by the logit matrix's low-rank factors?
- **Basis in paper:** Conclusion: "The low rank structure allows us to represent each history as a vector... can we extract concepts and features in this representation space?"
- **Why unresolved:** While the paper suggests linear dependencies correspond to semantic relations, it does not formalize a method to map these dimensions to human-understandable concepts
- **What evidence would resolve it:** Identifying specific directions in the low-rank subspace that correspond to semantic attributes

### Open Question 4
- **Question:** Can theoretical guarantees for learning and representation be extended to models that are only approximately low-rank, as seen in practice?
- **Basis in paper:** Conclusion: "Can we extend our theoretical results to the case when the model is only approximately low-rank...?"
- **Why unresolved:** The theoretical results rely on exact low-rank assumptions, whereas empirical evidence shows singular values decay via a power law rather than truncating sharply
- **What evidence would resolve it:** Proving error bounds for learning ISANs under approximate low-rank conditions

## Limitations

- The empirical findings depend heavily on the choice of dataset and model size, with scaling to larger models or different domains potentially revealing different behavior
- The generalizability of the linear transfer property across semantic domains remains uncertain, particularly for specialized domains like code generation
- The Lingen generation algorithm's success depends critically on the assumption that regression coefficients transfer between distributions, with no theoretical guarantees about when this will fail

## Confidence

**High Confidence:**
- The extended logit matrix exhibits low-rank structure with power-law decaying singular values
- Low-rank structure is absent in untrained models but emerges during pre-training
- Linear dependencies in the logit matrix are consistent across models
- The Lingen algorithm achieves lower KL divergence than several strong baselines

**Medium Confidence:**
- The power-law exponent α > 0.5 will persist at larger scales
- Linear relationships transfer across different future distributions
- The low-rank property enables meaningful compression of model behavior

**Low Confidence:**
- The theoretical equivalence between low logit rank and ISAN models provides complete characterization
- The approach will generalize to all domains and model sizes
- The computational hardness results for sample-based learning directly inform the practical effectiveness of logit queries

## Next Checks

1. **Scaling Validation**: Replicate the singular value analysis for OLMo-7B on the full training corpus rather than just the wiki subset, measuring whether the power-law exponent α remains above 0.5 when matrix dimensions increase by 10x or 100x.

2. **Domain Transfer Test**: Apply Lingen to specialized domains (e.g., GitHub code, scientific papers, medical text) and measure whether the linear transfer property holds. Compare KL divergence performance across domains to assess generalizability limits.

3. **Break Condition Analysis**: Systematically vary the semantic distance between target prompts and basis histories in the Lingen algorithm, measuring at what point the linear approximation fails. This would quantify the limits of the transfer property and identify failure modes for practical deployment.