---
ver: rpa2
title: 'OKRA: an Explainable, Heterogeneous, Multi-Stakeholder Job Recommender System'
arxiv_id: '2504.07108'
source_url: https://arxiv.org/abs/2504.07108
tags:
- data
- fairness
- dataset
- embedding
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing explainable, fair,
  and accurate job recommender systems in a multi-stakeholder environment. The proposed
  Occupational Knowledge-based Recommender using Attention (OKRA) is a novel graph
  neural network model that leverages both structured and unstructured data to provide
  candidate- and company-side recommendations and explanations.
---

# OKRA: an Explainable, Heterogeneous, Multi-Stakeholder Job Recommender System

## Quick Facts
- arXiv ID: 2504.07108
- Source URL: https://arxiv.org/abs/2504.07108
- Reference count: 40
- Primary result: Novel graph neural network job recommender outperforming six baselines including transformers in nDCG

## Executive Summary
OKRA is a graph neural network model that leverages heterogeneous data to provide explainable job recommendations for both candidates and employers. The system constructs knowledge graphs from structured and unstructured data, then uses Graph Attention Networks to generate stakeholder-specific embeddings. OKRA outperforms six baseline models including state-of-the-art transformer approaches on nDCG metrics, while providing attention-based explanations. However, the model exhibits urban bias, showing lower performance for rural candidates, highlighting fairness challenges in job recommendation systems.

## Method Summary
OKRA processes job recommendation as a graph-ranking task using knowledge graphs constructed from structured (skills, licenses, work experience) and unstructured (CV/vacancy texts) data. The model uses e5-multilingual-small for text node embeddings, processes all nodes through GraphTransformers, then applies 4 GATv2 layers to generate separate candidate and company embeddings. These are combined via harmonic mean and optimized using LambdaRANK loss. The system extracts k-random walk sub-graphs (k=7) for each candidate-vacancy pair and evaluates performance using nDCG@3,5,10 metrics.

## Key Results
- OKRA outperforms six baselines including transformers on nDCG@3,5,10 for both proprietary Dutch and Zhaopin datasets
- Node- and edge-attention values serve as interpretable explanations for matching decisions
- All models including OKRA show urban bias, with rural candidates receiving lower recommendation scores
- Stakeholder-specific embeddings allow separate optimization for candidate satisfaction and company requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating heterogeneous data via a knowledge graph improves ranking accuracy over text-only baselines
- **Mechanism:** Converts tabular attributes and text into unified graph structure with relational node embeddings that capture semantic connections missed by text matching
- **Core assumption:** Structured metadata provides orthogonal information to textual semantic similarity
- **Evidence anchors:** Abstract states model "outperforms six baseline models"; Section 3.2 describes converting table names to edge types
- **Break condition:** If structured data is sparse or noisy, graph complexity may cause overfitting without accuracy gains

### Mechanism 2
- **Claim:** Stakeholder-specific embeddings satisfy conflicting goals (candidate satisfaction vs. company requirements) simultaneously
- **Mechanism:** Multi-head GATv2 layer creates distinct candidate-view and company-view embeddings, pooled separately and combined via harmonic mean
- **Core assumption:** Features determining "good match" differ depending on stakeholder perspective
- **Evidence anchors:** Abstract mentions "both candidate- and company-side recommendations"; Section 3.3 describes stakeholder-specific embedding component
- **Break condition:** If harmonic mean is too aggressive, it may suppress scores for technically qualified but under-skilled candidates

### Mechanism 3
- **Claim:** Node- and edge-level attention values function as inherent explanations for matching decisions
- **Mechanism:** GATv2 assigns importance weights to specific edges, which can be extracted to highlight driving features
- **Core assumption:** Higher attention weights correlate with human-interpretable justifications
- **Evidence anchors:** Abstract states "node- and edge-attention values can be used as explanations"; Section 2.2 discusses attention providing acceptable explainability
- **Break condition:** If attention sinks occur, explanations will be misleading or noisy

## Foundational Learning

- **Concept: Heterogeneous Graph Neural Networks (HGNNs)**
  - **Why needed here:** Model processes graph containing different node types (Candidate, Skill, Vacancy) rather than raw text
  - **Quick check question:** Can you explain how a GNN propagates information from a "Skill" node to a "Candidate" node and how that differs from a standard feed-forward network?

- **Concept: Learning to Rank (LambdaRank / nDCG)**
  - **Why needed here:** Paper optimizes directly for nDCG rather than binary cross-entropy, prioritizing order of top-k recommendations
  - **Quick check question:** Why would optimizing for accuracy fail to produce useful ranked job recommendations?

- **Concept: Attention Mechanisms in GNNs (GAT)**
  - **Why needed here:** Explainability claims rest entirely on attention coefficients
  - **Quick check question:** In an attention layer, does higher weight always imply causal importance or merely correlation within training set?

## Architecture Onboarding

- **Component map:** Input Knowledge Graph & Text sequences → Relational Node Embedding (e5 + GraphTransformer) → Stakeholder Embedding (4-head GATv2) → Prediction (Pooling → Linear → Harmonic Mean)

- **Critical path:** Creation of sub-graphs via random walks (k=7). If random walk fails to connect candidate to relevant skill due to path length or disconnected graph, model cannot predict match

- **Design tradeoffs:**
  - Context Window: Text truncated to 96 tokens for memory constraints, potentially losing deep employment history
  - Pooling: Mean-pooling found optimal but dilutes strong single-skill signals compared to max-pooling
  - Urban Bias: System inherits spatial biases from data; lacks specific de-biasing module to counter observed urban advantage

- **Failure signatures:**
  - Rural invisibility: Lower prediction scores for candidates with "rural" location nodes despite perfect skill matches
  - Attention Diffusion: If explanations are vague, attention heads may be averaging weights too evenly across sub-graph

- **First 3 experiments:**
  1. Ablation on Data Modality: Run OKRA using only text nodes vs. only structured nodes to quantify heterogeneous graph contribution
  2. Sub-graph Walk Length Sensitivity: Vary k (3, 7, 10) to determine if performance drops due to noise (long walks) or lack of context (short walks)
  3. Explainability Stress Test: Manually inspect attention weights for obvious matches (e.g., Python requirement matching Python skill node) to verify grounding

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do human stakeholders (job seekers, employers, recruiters) subjectively evaluate utility and trustworthiness of OKRA's explanations?
- **Basis in paper:** Authors state "we plan to perform a similar, large-scale user study to evaluate the explanations generated by OKRA by individuals from the three main stakeholder groups"
- **Why unresolved:** While model generates explanations via attention values, explainability is subjective and not yet validated by actual users
- **Evidence would resolve it:** Results from user study measuring decision-making confidence and satisfaction scores for explanations across all stakeholder groups

### Open Question 2
- **Question:** Can regional unfairness (urban vs. rural bias) be mitigated without significantly deteriorating ranking accuracy?
- **Basis in paper:** Authors note "Future work could look into different ways in which these types of unfairness can be negated, and how both types can be properly balanced"
- **Why unresolved:** Study found all models including OKRA exhibited urban bias, suggesting current optimization methods fail to ensure fairness
- **Evidence would resolve it:** Modified training objective or post-processing technique achieving performance disparity ($\Delta P$) and disparate visibility ($\Delta V$) near 0 while maintaining high nDCG

### Open Question 3
- **Question:** To what extent does varying weight of stakeholder-specific embeddings impact trade-off between candidate-side relevance and provider-side visibility?
- **Basis in paper:** Paper suggests "it could be useful to tweak how much each stakeholder's requirements weigh in the decision" rather than treating them equally
- **Why unresolved:** Current architecture uses harmonic mean assuming equal importance for candidates and companies
- **Evidence would resolve it:** Experiments comparing fixed harmonic means against tunable weighting parameters, measuring changes in provider-side exposure versus candidate-side ranking accuracy

## Limitations

- Proprietary dataset unavailability limits reproducibility to Zhaopin dataset only
- Fairness analysis focuses solely on urban vs. rural location bias, potentially missing other discrimination dimensions
- Explainability claims rely on attention weights without human validation studies to confirm meaningful interpretations
- Harmonic mean combination may suppress recommendations for candidates with slightly lower qualifications but genuine potential

## Confidence

- **High Confidence:** Architectural design using heterogeneous GNNs for multi-stakeholder recommendations is technically sound with well-supported performance improvements
- **Medium Confidence:** Explainability claims through attention mechanisms are plausible but not empirically validated with stakeholder studies
- **Medium Confidence:** Fairness findings regarding urban bias are based on available data but may not generalize to other fairness dimensions

## Next Checks

1. **Human Evaluation of Explanations:** Conduct user study with recruitment professionals to validate whether attention-based explanations are meaningful and actionable for decision-making

2. **Fairness Audit Across Multiple Dimensions:** Extend fairness analysis to include gender, age, and educational background disparities using counterfactual fairness testing to identify potential discrimination beyond location bias

3. **Generalization Testing:** Test OKRA's performance and fairness on additional job recommendation datasets from different regions and industries to assess robustness of urban bias finding and general applicability