---
ver: rpa2
title: 'Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for
  Question Answering in the Era of LLMs?'
arxiv_id: '2509.25107'
source_url: https://arxiv.org/abs/2509.25107
tags:
- extraction
- triples
- triple
- llama
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether knowledge extraction remains valuable
  for question answering over semi-structured web content in the era of large language
  models. The authors extend a QA benchmark with knowledge extraction annotations
  and evaluate commercial and open-source LLMs on both cleaned and whole webpages.
---

# Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?

## Quick Facts
- arXiv ID: 2509.25107
- Source URL: https://arxiv.org/abs/2509.25107
- Reference count: 40
- Primary result: Knowledge extraction improves LLM QA accuracy on semi-structured web content by up to 11%, with larger gains on noisy whole webpages than cleaned pages

## Executive Summary
This paper investigates whether knowledge extraction remains valuable for question answering over semi-structured web content in the era of large language models. The authors extend a QA benchmark with knowledge extraction annotations and evaluate commercial and open-source LLMs on both cleaned and whole webpages. They find that while LLMs achieve high QA accuracy (95%) on cleaned semi-structured content, their performance drops significantly on whole webpages (76%). Knowledge extraction via triple augmentation improves QA accuracy by up to 8% on cleaned pages and 11% on whole pages. Multi-task fine-tuning that teaches LLMs extraction capabilities yields comparable improvements. Script-based extraction, though less accurate, offers a low-cost alternative achieving 61% F1 on cleaned pages and providing a 5% QA accuracy boost for smaller models. The study concludes that knowledge extraction remains relevant and beneficial for LLM-based QA, particularly for smaller models and real-world web-scale applications.

## Method Summary
The study evaluates knowledge extraction's impact on QA over semi-structured web content using an extended WebSRC benchmark. They implement triple extraction (subject, predicate, object) via zero-shot/few-shot prompting, script-based generation, and fine-tuning approaches. QA performance is measured with LLM-based accuracy verification on cleaned (Trafilatura-processed) and whole HTML pages. The paper also explores multi-task fine-tuning where models learn both QA and extraction simultaneously. Scripts are generated by LLMs and executed to extract triples from webpages, offering a 3000x cost reduction compared to per-page LLM inference. The evaluation includes in-domain (same websites for train/eval) and out-of-domain (different websites) splits to test generalization.

## Key Results
- Knowledge extraction via triple augmentation improves QA accuracy by up to 8% on cleaned pages and 11% on whole pages
- Script-based extraction achieves 61% F1 on cleaned pages at 3000x lower inference cost than LLM-based extraction
- Multi-task fine-tuning on QA and extraction improves out-of-domain QA accuracy by 11-25% for smaller models

## Why This Works (Mechanism)

### Mechanism 1: Triple Augmentation Reduces QA Error on Noisy Content
Triple augmentation explicitly surfaces latent structure in semi-structured content, providing LLMs with pre-digested relational signals that bypass the need to parse HTML layouts or noisy DOM structures. This reduces reasoning load and attention dispersion over long, noisy contexts. The extracted triples are sufficiently accurate and relevant to the query, with larger gains on noisier whole webpages than cleaned pages.

### Mechanism 2: Multi-task Fine-tuning on Extraction Improves QA Generalization
Jointly fine-tuning LLMs on QA and triple extraction tasks improves out-of-domain QA accuracy by inducing structured representations of semi-structured content. This teaches models to recognize relational patterns (attribute-value, table row-column structure) that transfer to QA on unseen websites, acting as implicit schema learning.

### Mechanism 3: Script-based Extraction Trades Accuracy for 3000x Lower Inference Cost
LLM-generated extraction scripts achieve moderate extraction quality (61% F1 on cleaned pages) at a fraction of inference cost by generating executable Python parsers once per website. This amortizes LLM cost but sacrifices adaptability to page-level variation, working best when websites have relatively consistent DOM structures.

## Foundational Learning

- **Semi-structured data types**: Attribute-value pairs (vertical tables), horizontal web tables, and free-form layouts. Why needed: Extraction and QA difficulty vary significantly by layout (free-form lowest at 63% F1 out-of-domain). Quick check: Given an HTML table with columns [Product, Price, Stock], which semi-structured type is this? (Answer: Horizontal table)

- **Knowledge triples (OpenIE)**: (subject, predicate, object) tuples representing relational facts. Why needed: The paper's core intervention is augmenting QA with extracted triples; understanding triple structure is essential. Quick check: What triple would you extract from "AO 100A | Category | Criminal Forms"? (Answer: (AO 100A, Category, Criminal Forms))

- **Multi-task learning for LLMs**: Fine-tuning on multiple objectives simultaneously to improve generalization. Why needed: The paper shows QA + extraction multi-task training outperforms QA-only training. Quick check: Why might multi-task training help out-of-domain QA? (Answer: Learned structural priors transfer across websites)

## Architecture Onboarding

- **Component map**: HTML -> (cleaned via Trafilatura) OR flattened text -> Extraction path: Zero-shot/Few-shot LLM -> triples OR Script generator -> executable parser -> triples -> QA path: Question + reference (original OR augmented with triples) -> LLM -> answer -> Evaluation: LLM-based accuracy checker (Llama-70B as judge)

- **Critical path**: 1. Start with cleaned pages + few-shot extraction to validate pipeline 2. Add triple augmentation to QA inference 3. If compute-constrained, experiment with script-based extraction 4. If quality insufficient, fine-tune with multi-task (QA + extraction)

- **Design tradeoffs**: Accuracy vs. cost: Ground-truth triples > LLM extraction > script extraction; Speed vs. generalization: Scripts are fast but fail on whole pages; LLMs generalize better but cost ~3000x more; Model size: Smaller models (3B) benefit more from augmentation; larger models (70B+) near ceiling on cleaned data

- **Failure signatures**: Whole-page extraction collapses to 28% F1 (context window + noise); Script extraction fails on whole pages (DOM complexity); Larger models see no QA gain from noisy triples (distraction effect)

- **First 3 experiments**: 1. Baseline QA: Run zero-shot QA on cleaned pages with flattened text; measure Accuracy_LM (expect 73-97% depending on model size) 2. Triple augmentation (upper bound): Augment with ground-truth triples; measure lift (expect +8-13% for smaller models) 3. Cost-quality tradeoff: Compare few-shot LLM extraction vs. script-based extraction on cleaned pages; measure extraction F1 and QA lift (expect scripts at ~60-70% F1, +5% QA for 3B models)

## Open Questions the Paper Calls Out

### Open Question 1
Can script-based extraction methods be extended to achieve meaningful performance on whole webpages, where they currently fail? The paper explicitly states "Despite the potential, we have not seen success on whole webpages, where scripting is much harder." A modified script-generation approach achieving >40% F1 on whole webpages would demonstrate feasibility.

### Open Question 2
What techniques can mitigate noisy extracted triples degrading performance for larger LLMs on high-accuracy tasks? The paper notes that larger models don't benefit from script-based extractions because of potential distractions from wrongly extracted triples. A triple filtering or ranking method that improves larger model accuracy beyond baseline on cleaned pages would address this.

### Open Question 3
How can web-scale extraction cost be reduced while maintaining extraction quality above 50% F1? The paper states "The current quality of extraction at web scale remains sub-optimal" with fine-tuned LLMs achieving ~28% F1 on whole pages. A hybrid approach achieving >50% F1 on whole pages at <20% of fine-tuned LLM inference cost would be valuable.

## Limitations

- Script-based extraction has not succeeded on whole webpages due to DOM complexity and noise
- Larger models (70B) see diminishing returns from knowledge extraction due to performance ceilings
- Whole-page extraction quality remains limited at 28% F1 due to context window constraints and noise

## Confidence

**High Confidence**: Knowledge extraction improves QA accuracy on semi-structured content; Script-based extraction achieves 3000x cost reduction while maintaining moderate quality; Multi-task fine-tuning transfers structural knowledge to improve out-of-domain QA

**Medium Confidence**: Whole-page extraction quality limitation is fundamental rather than implementation-specific; Script extraction failure on whole pages is due to DOM complexity; Larger models don't benefit from triple augmentation due to performance ceiling

**Low Confidence**: Generalization of script-based approaches to arbitrary web domains; Long-term scalability of multi-task fine-tuning with increasing website diversity; Extrapolation of cost savings to production-scale deployments

## Next Checks

1. **Replicate the cost-accuracy tradeoff**: Run Llama-3B with script-based extraction on a small sample of SemiBench pages to verify the 3000x inference time reduction and ~5% QA accuracy improvement.

2. **Test whole-page extraction limits**: Attempt to extract triples from 10-20 representative whole webpages from the paper's corpus to verify the 28% F1 ceiling and understand whether this represents fundamental limitations or implementation gaps.

3. **Validate multi-task generalization**: Fine-tune a 3B model on 20 websites and test on 5 unseen websites with different layouts to verify the claimed out-of-domain QA improvements (72.6% â†’ 84.1%).