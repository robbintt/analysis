---
ver: rpa2
title: 'PHANTOM RECALL: When Familiar Puzzles Fool Smart Models'
arxiv_id: '2510.11812'
source_url: https://arxiv.org/abs/2510.11812
tags:
- reasoning
- premise
- conclusion
- error
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces PHANTOM RECALL, a benchmark of 25 classic\
  \ logic puzzles and 149 carefully designed perturbations to test whether LLMs truly\
  \ reason or just recall memorized solutions. When puzzles are slightly altered,\
  \ models\u2014including GPT, Gemini, and Claude\u2014often fail, revealing a failure\
  \ mode termed \"phantom recall\" where they confidently reproduce incorrect, memorized\
  \ answers."
---

# PHANTOM RECALL: When Familiar Puzzles Fool Smart Models

## Quick Facts
- **arXiv ID**: 2510.11812
- **Source URL**: https://arxiv.org/abs/2510.11812
- **Reference count**: 39
- **Primary result**: LLMs achieve near-perfect accuracy on classic logic puzzles but fail significantly when puzzles are slightly altered, revealing "phantom recall" where models reproduce memorized but incorrect solutions.

## Executive Summary
This paper introduces PHANTOM RECALL, a benchmark testing whether LLMs genuinely reason or merely recall memorized solutions. Using 25 classic logic puzzles and 149 carefully designed perturbations that preserve reasoning structure while altering answers, the authors demonstrate that models including GPT, Gemini, and Claude fail significantly on perturbed variants. The failure mode—termed "phantom recall"—occurs when models confidently reproduce incorrect, memorized answers despite clear evidence mismatches. The authors provide automated evaluation tools, a taxonomy of reasoning error categories, and a prompt-based mitigation framework, revealing substantial gaps between linguistic fluency and genuine logical reasoning.

## Method Summary
The authors constructed a benchmark of 25 classic logic puzzles with 149 perturbed variants that preserve reasoning structure while altering correct answers. They evaluated 11 LLMs (6 closed-source, 5 open-source) using zero-shot Chain-of-Thought prompting. An automated answer verification system achieved 96% human agreement for conceptual equivalence checking. A two-agent pipeline classified step-wise reasoning errors across five categories with premise/conclusion sub-categories, achieving 71% human coincidence. The mitigation framework used a three-phase protocol with prohibitory constraints, showing accuracy gains of 4.7-10.1 percentage points across models.

## Key Results
- Models achieve near-perfect performance on original puzzles but show 40-70% accuracy drops on perturbed variants
- Phantom recall dominates failure mode, where models reproduce memorized solutions to altered problems
- Error propagation through premise formation is the primary failure mechanism, with consequential errors comprising 35-38% of reasoning steps
- Structured prompting improves accuracy but doesn't close the robustness gap
- Qwen exhibits extreme over-elaboration, generating 51 average steps for incorrect answers versus 17 for correct ones

## Why This Works (Mechanism)

### Mechanism 1: Pattern Matching Overrides Re-Reasoning
When LLMs encounter puzzles resembling training data, cached solution templates compete with and often override fresh computation, causing systematic failures on perturbed variants. Retrieval of memorized reasoning chains is triggered by surface-level pattern matching to familiar puzzle structures. When perturbations alter constraints or numerical values, the model fails to suppress the cached response because contextual binding in LLMs is probabilistic rather than symbolic—pattern strength wins over instance-specific reasoning.

### Mechanism 2: Error Propagation Through Premise Formation
Reasoning failures concentrate at early premise-formation stages (evidence interpretation, assumption injection) and cascade through subsequent steps. Models make initial errors interpreting puzzle constraints—misrepresenting evidence, assuming unstated facts, or carrying forward prior mistakes. These compound because each step's conclusion becomes the next step's premise. Approximately 25-35% of reasoning steps exhibit deductive or compounded failures.

### Mechanism 3: Negative Constraints Prune Retrieval Competition
Explicit prohibitory instructions reduce phantom recall by constraining response space and enforcing verification checkpoints. The three-phase protocol (UNDERSTAND → SOLVE → VERIFY) combined with visual constraint markers (✗/✓) suppresses pattern-matching retrieval through forced re-reading, prohibition of external knowledge, and mandatory step-level verification before finalization. This shifts behavior from generative pattern completion to controlled constraint satisfaction.

## Foundational Learning

- **Concept: Benchmark Contamination**
  - Why needed here: The paper's central claim depends on distinguishing true reasoning from memorization; contamination inflates baseline performance on original puzzles
  - Quick check question: If a model scores 100% on original puzzles but 30% on perturbed variants with identical reasoning structure, what minimum contamination rate would explain this gap?

- **Concept: Logical Equivalence vs. Surface Perturbation**
  - Why needed here: Perturbation validity requires confirming that altered puzzles preserve reasoning structure while changing answers; invalid perturbations conflate difficulty with recall
  - Quick check question: Changing "wolf, goat, cabbage" to "cat, chicken, grain" in a river-crossing puzzle—does reasoning structure change? What if boat capacity changes from 1 to 2?

- **Concept: Cascading Error Attribution**
  - Why needed here: Error taxonomy distinguishes original failures from propagated ones; misattribution obscures intervention targets
  - Quick check question: If Step 3's premise derives from Step 2's incorrect conclusion, should Step 3's error be classified as "Cascading" or "Evidence Misrepresentation"?

## Architecture Onboarding

- **Component map:**
  Benchmark Generator: 25 base puzzles → 149 perturbations (preserve reasoning, alter solutions)
  Mirror-Image Converter: Open-ended → constrained response formats (43 puzzles)
  Answer Verification Auto-Evaluator: LLM-based conceptual equivalence checker (96% human agreement)
  Step Error Classification Auto-Evaluator: Two-agent pipeline (Deconstructor → Validator → Classifier)
  Mitigation Prompting System: Three-phase protocol with prohibitory constraints

- **Critical path:** Puzzle creation → Perturbation design → Zero-shot CoT inference → Answer verification → Chain decomposition → Error classification → Mitigation testing

- **Design tradeoffs:**
  Scale (25 puzzles) vs. depth (149 variants): Authors chose controlled analysis over breadth; limits generalization claims
  LLM-based evaluators vs. human annotation: Enables scale but introduces circularity risk
  Minimal perturbations: Test recall specifically but may miss broader robustness issues

- **Failure signatures:**
  Phantom Recall: Correct solution to original puzzle applied to perturbed variant
  Over-Elaboration: Long chains (Qwen: 51 avg steps for wrong answers) without convergence
  Evidence Misrepresentation: Fabricated constraints absent from puzzle statement
  Improper Candidate Elimination: Failure to discard invalid options

- **First 3 experiments:**
  1. Reproduce base result: Run LLaMA, Qwen, InternLM, Mistral, Phi on original vs. perturbed puzzles; expect 40-70% accuracy gap
  2. Validate auto-evaluator: Sample 20 chains, compare auto-classified errors against human annotation; expect ~70% agreement
  3. Ablate mitigation: Test full protocol vs. (a) constraints-only, (b) structure-only, (c) verification-only; identify driving components

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Can training-time interventions (e.g., counterfactual data augmentation) eliminate phantom recall more effectively than inference-time prompting?
  - Basis in paper: [explicit] The conclusion identifies "concrete targets for future training-time interventions," noting that prompting strategies left "robustness gaps."
  - Why unresolved: The authors evaluated existing models and applied prompting-based mitigation frameworks but did not train or fine-tune models on perturbed data.
  - What evidence would resolve it: Pre-training or fine-tuning models on datasets specifically designed with constraint perturbations, then testing for near-equal performance on base and variant puzzles.

- **Open Question 2**
  - Question: Do the "phantom recall" failure modes observed in logic puzzles persist across diverse, open-ended real-world reasoning tasks?
  - Basis in paper: [inferred] The Limitations section notes that the focus on logic puzzles "may not fully capture model behavior on open-ended reasoning or real-world problem-solving."
  - Why unresolved: The benchmark is restricted to rule-based puzzles with clear ground truth answers, leaving unstructured domains untested.
  - What evidence would resolve it: Extending the perturbation methodology to complex, open-ended tasks (e.g., commonsense reasoning or planning) to see if models exhibit similar memorization-over-reasoning behaviors.

- **Open Question 3**
  - Question: Is there a general prompting strategy that fully aligns model reasoning behavior across both original and perturbed puzzles?
  - Basis in paper: [explicit] The abstract asks: "Is there a general way of reformulating the prompt so that the models do better?"
  - Why unresolved: While the authors proposed a mitigation framework that improved accuracy, the conclusion notes that it "does not close the robustness gap."
  - What evidence would resolve it: A prompt structure that results in statistically insignificant performance differences between a model's accuracy on standard puzzles and their perturbed variants.

## Limitations
- Benchmark contamination risk: The 40-70% accuracy drop could reflect genuine reasoning limitations OR increased difficulty from unfamiliar puzzle variants
- Error taxonomy reliability: LLM-based classification introduces potential systematic biases in distinguishing cascading from compounding failures
- Generalization scope: Testing only 25 base puzzles across 11 models limits breadth and may not represent broader logical reasoning challenges

## Confidence
- **High Confidence**: The existence of substantial performance gaps between original and perturbed puzzles, and the documented presence of "phantom recall" behaviors
- **Medium Confidence**: The claim that error propagation through premise formation is the dominant failure mechanism
- **Low Confidence**: The assertion that negative constraints specifically drive mitigation effectiveness

## Next Checks
1. **Manual Error Annotation**: Have human experts independently classify reasoning errors in 50 randomly selected chains to verify the accuracy and consistency of the automated Step Error Classification pipeline
2. **Puzzle Domain Expansion**: Test the benchmark methodology on a distinct logical reasoning domain (e.g., spatial reasoning puzzles) to assess whether phantom recall patterns generalize
3. **Controlled Contamination Experiment**: Create synthetic datasets where models are explicitly trained on perturbed variants before testing on originals (and vice versa) to quantify the specific contribution of memorization versus reasoning limitations