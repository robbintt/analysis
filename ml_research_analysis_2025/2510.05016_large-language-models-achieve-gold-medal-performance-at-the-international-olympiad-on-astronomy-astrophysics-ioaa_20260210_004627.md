---
ver: rpa2
title: Large Language Models Achieve Gold Medal Performance at the International Olympiad
  on Astronomy & Astrophysics (IOAA)
arxiv_id: '2510.05016'
source_url: https://arxiv.org/abs/2510.05016
tags:
- ioaa
- problems
- data
- llms
- exams
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks five state-of-the-art large language models
  on International Olympiad on Astronomy and Astrophysics (IOAA) exams to evaluate
  their astronomical problem-solving capabilities. The IOAA exams are used as an ecologically
  valid benchmark because they test deep conceptual understanding, multi-step derivations,
  and multimodal analysis rather than simple knowledge recall.
---

# Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)

## Quick Facts
- arXiv ID: 2510.05016
- Source URL: https://arxiv.org/abs/2510.05016
- Reference count: 40
- Five state-of-the-art LLMs achieve gold medal performance on IOAA astronomy exams, with GPT-5 and Gemini 2.5 Pro scoring 84.2-85.6% on theory exams and GPT-5 achieving 88.5% on data analysis exams.

## Executive Summary
This paper benchmarks five frontier large language models (GPT-5, Gemini 2.5 Pro, OpenAI o3, Claude Opus 4.1, and Claude Sonnet 4) on International Olympiad on Astronomy and Astrophysics (IOAA) exams to evaluate their astronomical problem-solving capabilities. The IOAA exams serve as an ecologically valid benchmark because they test deep conceptual understanding, multi-step derivations, and multimodal analysis rather than simple knowledge recall. The results show GPT-5 and Gemini 2.5 Pro achieving gold medal performance in theory exams, while GPT-5 excels in data analysis with 88.5% average score. However, significant performance drops are observed in geometric reasoning, spatial visualization, and multimodal interpretation tasks.

## Method Summary
The study evaluates five SOTA LLMs on IOAA theory (49 problems) and data analysis (8 problems) exams from 2022-2025. Problems were sourced from official IOAA websites and provided in LaTeX format with embedded figures. A standardized prompt template was used across all models, including a reference sheet with 16 universal constants, 26 astronomical data values, and 6 calculus formulas. Responses were compiled from LaTeX to PDF and graded by two IOAA expert graders using official rubrics, with discrepancies resolved by consensus. The evaluation used single-pass inference per problem without retries to mirror exam conditions.

## Key Results
- GPT-5 and Gemini 2.5 Pro achieved gold medal performance in theory exams with average scores of 84.2% and 85.6%
- GPT-5 excelled in data analysis exams with an 88.5% average score, ranking in the top 10 among human participants
- Other models showed significant performance drops in data analysis (48-76%)
- Geometric and spatial reasoning errors were most prevalent, with 52-79% accuracy on Category I problems
- Error analysis revealed consistent weaknesses in conceptual reasoning, geometric reasoning, and spatial visualization

## Why This Works (Mechanism)

### Mechanism 1
High performance on Category II (Physics/Mathematics) problems stems from LLMs' capacity for symbolic manipulation and formulaic derivation chains. Problems requiring astrophysical calculations without geometric visualization map onto well-represented training distributions of mathematical reasoning (IMO, IPhO-style tasks). The paper shows 77–91% accuracy on Category II vs. 49–78% on Category I across models.

### Mechanism 2
GPT-5's data analysis advantage depends on superior multimodal integration—specifically plot reading and generation. Data analysis exams require extracting quantitative information from plots, generating tikz/pgfplots code, and iterating. GPT-5 achieved 88.5% while others dropped 10–15 percentage points from theory scores.

### Mechanism 3
Geometric/spatial reasoning failures arise from inability to maintain consistent 3D coordinate representations during chain-of-thought. LLMs "can only reason in natural language but not visualize or sketch spatial representation during thinking." Errors include writing spherical trigonometry equations that violate basic geometric principles.

## Foundational Learning

- Concept: **Spherical trigonometry and celestial coordinate systems**
  - Why needed here: Category I problems (37% of theory) require understanding great circle geometry, coordinate transformations, and timekeeping systems—identified as systematic failure points.
  - Quick check question: Given an eclipse geometry problem, can you identify whether Sun, Moon, and Earth centers are collinear?

- Concept: **Order-of-magnitude approximation in astrophysics**
  - Why needed here: Models show gaps in "physical intuition" for when approximations are valid; Appendix C.4 shows inappropriate application of Oort's approximation.
  - Quick check question: Given a Dyson sphere blocking all solar radiation, what is Earth's equilibrium temperature? (Hint: Is 0 K correct?)

- Concept: **Multimodal data extraction from scientific visualizations**
  - Why needed here: Data analysis problems require extracting values from light curves, spectra, and plots; GPT-5 succeeds where others fail.
  - Quick check question: Given a quasar image with scale bars, can you measure distances between reference marks with <10% error?

## Architecture Onboarding

- Component map: LaTeX extraction -> base64 image encoding -> reference sheet injection -> model inference -> LaTeX compilation -> PDF generation -> dual human grading
- Critical path: Problem categorization (Category I vs II) determines expected accuracy baseline -> Multimodal input quality (especially plot clarity) predicts data analysis success -> Derivation completeness (intermediate steps) separates medal-level from non-medal responses
- Design tradeoffs: Single-attempt evaluation mirrors exam conditions but underestimates retry-augmented performance; LaTeX output enables automated extraction but introduces compilation failures unrelated to reasoning; Excluding observation exams preserves ecological validity for LLMs but removes 150/600 points
- Failure signatures: Category I problems: Non-collinearity assumption failures, angle miscalculations, coordinate confusion; Data analysis: Plot reading errors (20–50% measurement error), plotting code failures; Approximation: Over-application of common formulas without validity checks
- First 3 experiments: 1) Add visual sketchpad tool and re-evaluate Category I problems; 2) Compare single-attempt vs. 3-retry performance on data analysis; 3) Fine-tune on synthesized astronomical VQA examples

## Open Questions the Paper Calls Out

### Open Question 1
Does augmenting LLMs with visual sketching capabilities significantly improve performance on astronomical problems requiring geometric and spatial reasoning (Category I)? The conclusion suggests "future work can implement visual sketchpad so that the models can imitate humans to visualize or draw spatial representations." This is unresolved because the error analysis identifies spatial visualization as a "consistent weakness" (52–79% accuracy) since LLMs currently "can only reason in natural language but not visualize."

### Open Question 2
Can fine-tuning on synthesized visual question-answering (QA) datasets at scale reduce failure rates in interpreting astronomical plots and images? The authors propose that given the vast amount of astronomical data, "we can synthesize visual question answering examples at scale to improve LLMs' multimodal understanding." This is unresolved because "plot and image reading" remains a major source of errors (Fig. 1b), and current models struggle to extract quantitative information from scientific visualizations.

### Open Question 3
What components are required for an "ecologically valid" benchmark that accurately predicts an LLM's capability to function as an autonomous research agent in astronomy? The authors state their "results call for ecologically valid, multimodal benchmarks on astronomical data analysis," noting that current evaluations may not fully capture real-world research necessities. This is unresolved because high scores on IOAA theory exams do not guarantee autonomy, as the exams omit observational components and the "critical gaps" in conceptual reasoning may hinder open-ended research tasks.

## Limitations
- GPT-5, Claude Opus 4.1, and Claude Sonnet 4 are not publicly available, making independent verification impossible
- The exclusion of observation exams (150/600 points) limits ecological validity for real-world astronomical problem-solving
- Single-attempt evaluation methodology may underestimate model performance compared to human participants who typically retry challenging problems

## Confidence

**High Confidence:** GPT-5 and Gemini 2.5 Pro achieving gold medal performance on theory exams (84.2-85.6% average scores) is well-supported by the data, with clear rankings among human participants and consistent expert grading.

**Medium Confidence:** The mechanism that geometric/spatial reasoning failures stem from inability to maintain 3D coordinate representations during chain-of-thought is plausible given the documented error patterns, though external visual tools as a potential mitigation remain underexplored in the corpus.

**Low Confidence:** The comparative performance between models on data analysis problems (48-76% range) may be affected by unreported API parameters and prompt engineering differences, as well as potential compilation issues with LaTeX output unrelated to reasoning capability.

## Next Checks

1. **External Visual Tool Integration Test:** Implement visual sketchpad tools for all models and re-evaluate Category I geometric problems to test whether spatial visualization deficits can be systematically mitigated.

2. **Multi-Attempt Performance Analysis:** Compare single-attempt vs. 3-retry performance on data analysis problems across all models to distinguish between multimodal interpretation errors and reasoning failures.

3. **Synthetic VQA Fine-tuning Experiment:** Create and fine-tune on synthesized astronomical visual question answering examples specifically targeting the identified error categories (geometric, conceptual, image reading) to measure potential performance improvements in Category I problems.