---
ver: rpa2
title: 'CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for
  CT Report Generation'
arxiv_id: '2601.11488'
source_url: https://arxiv.org/abs/2601.11488
tags:
- metrics
- reports
- clinical
- report
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CTest-Metric, the first unified framework
  to assess clinical validity of metrics for CT report generation. It evaluates eight
  widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore,
  GREEN Score, CRG) across seven LLMs on CT reports.
---

# CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation

## Quick Facts
- arXiv ID: 2601.11488
- Source URL: https://arxiv.org/abs/2601.11488
- Reference count: 0
- Key outcome: CTest-Metric framework reveals lexical NLG metrics are highly style-sensitive, BERTScore-F1 is least sensitive to factual errors, and GREEN Score best aligns with expert clinical judgments.

## Executive Summary
This paper introduces CTest-Metric, the first unified framework to assess clinical validity of metrics for CT report generation. It evaluates eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) across seven LLMs on CT reports. The framework tests Writing Style Generalizability (WSG), Synthetic Error Injection (SEI), and Metrics-vs-Expert Correlation (MvE). Results show that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman ~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. The work highlights the need for clinically grounded evaluation metrics and provides a systematic tool for their development.

## Method Summary
CTest-Metric framework evaluates 8 metrics (4 NLG: BLEU, ROUGE, METEOR, BERTScore-F1; 4 CE: F1-RadGraph, RaTEScore, GREEN, CRG) on CT reports from the CT-RATE dataset. The framework comprises three modules: WSG tests style sensitivity via LLaMA-3.1-8B-Instruct rephrasing, SEI tests error sensitivity via synthetic injection of laterality/negation errors, and MvE tests expert alignment by correlating metrics on high-disagreement cases (top 25 per LLM, 175 total) with 2 expert reviewers. RRG models are trained for 10 epochs on NVIDIA A100. The framework systematically identifies clinically valid metrics through these complementary assessments.

## Key Results
- Lexical NLG metrics show high sensitivity to stylistic variations (-48.82% to -1.16% score deviation in WSG)
- BERTScore-F1 is least sensitive to factual error injection (Δ(S_M, S_G) = -0.02) while GREEN Score is most sensitive (Δ(S_M, S_G) = -0.6053)
- GREEN Score best aligns with expert judgments (Spearman ρ = 0.7) while CRG shows negative correlation
- F1-RadGraph experiences most significant deviation from rephrasing (ranging from -42.29% to 68.95%)

## Why This Works (Mechanism)

### Mechanism 1: Style-Robustness Detection via Semantic Preservation Testing
- Claim: A clinically valid metric should produce stable scores across semantically equivalent rephrasings of the same report.
- Mechanism: The WSG module uses LLaMA-3.1-8B-Instruct with zero-shot prompting to rephrase predicted reports while preserving clinical outcomes. Metrics are applied to both original and rephrased versions; large score differences indicate inappropriate sensitivity to surface form rather than clinical content.
- Core assumption: The rephrasing LLM reliably preserves clinical semantics while altering writing style.
- Evidence anchors:
  - [abstract]: "lexical NLG metrics are highly sensitive to stylistic variations"
  - [section 4.2]: "NLG-based metrics were the most impacted... ranging from -48.82% to -1.16%... F1-RadGraph experienced the most significant deviation (ranging from -42.29% to 68.95%)"
  - [corpus]: Weak direct corpus support; related papers focus on generation rather than metric evaluation.
- Break condition: If rephrasing introduces unintended clinical changes, WSG conflation with SEI occurs.

### Mechanism 2: Graded Error Sensitivity via Synthetic Injection
- Claim: A clinically grounded metric must show monotonic degradation as factual error severity increases.
- Mechanism: SEI injects 1, 2, and multiple synthetic errors (laterality, negation) into ground truth reports at graded levels. Score deviation Δ(S_j, S_G) is computed at each level; clinically valid metrics should show progressive penalties.
- Core assumption: Synthetic error patterns (primarily laterality and negation) represent clinically significant error types.
- Evidence anchors:
  - [abstract]: "BERTScore-F1 is least sensitive to factual error injection"
  - [section 4.3]: "BERTScore-F1 with Δ(S_M, S_G) of -0.02... whereas the most affected metrics include the GREEN Score (Δ(S_M, S_G) = -0.6053)"
  - [corpus]: No direct corpus validation of synthetic error injection methodology.
- Break condition: If synthetic errors don't reflect real-world radiology errors, SEI results won't generalize to clinical deployment.

### Mechanism 3: Expert Alignment via Disagreement-Case Sampling
- Claim: Metric validity is established through correlation with clinician judgments on cases where automated metrics disagree.
- Mechanism: MvE computes per-patient scores across all 8 metrics, normalizes them, calculates standard deviation, and selects the top 25 highest-variance cases per model (175 total). Two clinical experts rate these; Spearman's ρ quantifies metric-expert alignment.
- Core assumption: High-disagreement cases are the most informative for distinguishing metric quality.
- Evidence anchors:
  - [abstract]: "GREEN Score aligns best with expert judgments (Spearman ~0.70), while CRG shows negative correlation"
  - [section 4.4]: "GREEN Score... with ρ = 0.7 followed by F1-RadGraph with ρ = 0.53... CRG... presented a negative correlation"
  - [corpus]: Related paper "ReEvalMed" similarly advocates aligning metrics with clinical judgment, providing indirect support.
- Break condition: If disagreement cases are unrepresentative of typical reports, correlation estimates may be biased.

## Foundational Learning

- Concept: **NLG vs. Clinical Efficacy (CE) Metrics**
  - Why needed here: The framework evaluates both types differently; understanding their design intent explains divergent behaviors (e.g., BLEU measures n-gram overlap, GREEN counts parsed errors).
  - Quick check question: Can you explain why a metric optimized for lexical overlap would fail on rephrased but clinically identical reports?

- Concept: **Spearman's Rank Correlation (ρ)**
  - Why needed here: MvE uses Spearman correlation to quantify metric-expert alignment; interpreting ρ values (0.7 = strong, 0.3 = weak, negative = inverse) is essential.
  - Quick check question: If a metric has ρ = -0.2 with expert ratings, what does that imply about its clinical utility?

- Concept: **Label-Level vs. Sentence-Level Evaluation**
  - Why needed here: CRG's robustness to rephrasing stems from its label-level design, while F1-RadGraph's sensitivity comes from sentence-level structure analysis.
  - Quick check question: Why would a label-presence metric be insensitive to negation errors if negation isn't explicitly modeled?

## Architecture Onboarding

- Component map: CT-RATE dataset -> CT-CLIP encoder -> 7 LLMs (DistilGPT, GPT2 variants, LLaMA variants, BioGPT-Large) -> 8 metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN, CRG) -> CTest-Metric (WSG -> SEI -> MvE)

- Critical path: Understand each metric's computation -> Run WSG to identify style-sensitive metrics -> Run SEI to assess error sensitivity -> Validate against expert ratings via MvE.

- Design tradeoffs:
  - WSG relies on single LLM for rephrasing; using different LLMs could yield different style variations.
  - SEI uses synthetic errors; real error distributions from production systems may differ.
  - MvE samples only high-disagreement cases; correlation on typical cases remains unknown.
  - Expert validation used 2 reviewers; broader inter-rater variability uncharacterized.

- Failure signatures:
  - Metric scores drop >30% on rephrasing -> style-sensitive, not clinically grounded.
  - Metric shows <5% degradation with multiple injected errors -> insensitive to factual errors.
  - Metric has negative correlation with expert ratings -> actively misleading for model selection.

- First 3 experiments:
  1. **Reproduce WSG on one metric**: Take BLEU, rephrase 50 reports using LLaMA-3.1-8B-Instruct, compute score difference distribution to verify reported sensitivity range.
  2. **Single-error SEI test**: Inject one laterality error into 100 ground truth reports, measure score drops across all 8 metrics to confirm BERTScore-F1's insensitivity and GREEN's sensitivity.
  3. **Expert correlation subset**: Randomly sample 30 disagreement cases, obtain expert ratings, compute Spearman ρ for GREEN and CRG to validate the 0.7 vs. negative correlation pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CTest-Metric's assessment generalize to other institutions, contrast phases, and non-chest body regions beyond the CT-RATE dataset?
- Basis in paper: [explicit] Authors state "broader generalization to other institutions, contrast phases, or body regions remains to be established" as a constraint.
- Why unresolved: All experiments were conducted on a single dataset (CT-RATE), which may have institution-specific reporting conventions, patient populations, and imaging protocols that limit generalizability.
- What evidence would resolve it: Application of CTest-Metric to external CT datasets from different institutions, with varying contrast phases (contrast-enhanced vs. non-contrast), and across different anatomical regions (abdominal, head/neck CT).

### Open Question 2
- Question: Why does CRG exhibit negative correlation with expert ratings despite being designed specifically for CT reports and showing strong robustness to stylistic variations?
- Basis in paper: [inferred] The paper reports this counterintuitive finding but does not fully explain the mechanism: "CRG remains robust to stylistic changes but correlates negatively with expert scores."
- Why unresolved: The paper attributes this to CRG's reliance on label-level information, but the specific factors driving the negative correlation (e.g., label selection, penalty weighting, distribution assumptions) remain unclear.
- What evidence would resolve it: Systematic ablation studies examining which CRG components drive the negative correlation, coupled with qualitative analysis of specific disagreement cases between CRG scores and expert judgments.

### Open Question 3
- Question: How does the fidelity of LLM-based synthetic error injection affect the validity of SEI assessments, particularly for error types beyond laterality and negation?
- Basis in paper: [explicit] Authors note "WSG and SEI depend on an LLM-based prompting, which primarily introduced laterality and negation errors, though the fidelity of these edits was not independently validated."
- Why unresolved: Without independent validation of whether injected errors accurately represent clinically realistic errors, SEI sensitivity measurements may not reflect real-world metric behavior.
- What evidence would resolve it: Expert review of synthetic error-injected reports to verify clinical plausibility, combined with expansion to additional error types (measurement inaccuracies, anatomical mislocalization, severity mischaracterization).

### Open Question 4
- Question: Would expert ratings and metric-expert correlations change substantially with a larger and more diverse panel of clinical experts?
- Basis in paper: [explicit] "Expert validation involved two reviewers, with a second opinion sought for ambiguous cases, which limits the diversity of assessment."
- Why unresolved: Radiology report quality assessment may vary based on subspecialty expertise, experience level, and institutional background; two reviewers cannot capture this variability.
- What evidence would resolve it: Replication of MvE analysis with expanded expert panels (e.g., 5-10 reviewers) spanning different subspecialties and experience levels, with inter-rater reliability analysis.

## Limitations
- The framework's validity rests on synthetic error patterns that may not reflect real-world radiology error distributions.
- Expert validation relied on only 2 reviewers rating 175 high-disagreement cases, leaving inter-rater reliability and typical-case correlation uncharacterized.
- The WSG module depends on a single LLM's rephrasing capability, and different models might produce varying style variations.

## Confidence
- WSG findings (style sensitivity of NLG metrics): **High** - results are directly computed from systematic rephrasing experiments
- SEI findings (BERTScore-F1 insensitivity, GREEN sensitivity): **Medium** - depends on synthetic error methodology that lacks real-world validation
- MvE findings (GREEN correlation ~0.70, CRG negative correlation): **Medium** - based on limited expert reviewers and selective high-disagreement sampling
- Overall framework utility: **Medium** - systematic approach demonstrated, but validation scope constrained

## Next Checks
1. Conduct real error injection using actual incorrect reports from clinical practice rather than synthetic errors to verify SEI findings generalize
2. Expand expert validation to 5+ reviewers rating 500+ randomly sampled cases to establish inter-rater reliability and typical-case correlations
3. Replicate WSG using 3 different LLMs for rephrasing to assess sensitivity to rephrasing model choice