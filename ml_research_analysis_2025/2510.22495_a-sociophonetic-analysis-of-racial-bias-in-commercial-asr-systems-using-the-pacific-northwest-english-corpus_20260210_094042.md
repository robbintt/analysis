---
ver: rpa2
title: A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the
  Pacific Northwest English Corpus
arxiv_id: '2510.22495'
source_url: https://arxiv.org/abs/2510.22495
tags:
- american
- across
- phonetic
- speech
- speakers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that systematic phonetic variation drives
  performance disparities in commercial ASR systems across multiple ethnic groups.
  Using the Pacific Northwest English corpus, we identified specific phonetic features,
  particularly resistance to the low-back merger and pre-nasal merger patterns, that
  consistently correlate with transcription errors, with the most pronounced effects
  observed for African American speakers across four major commercial systems.
---

# A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus

## Quick Facts
- arXiv ID: 2510.22495
- Source URL: https://arxiv.org/abs/2510.22495
- Reference count: 0
- Systematic phonetic variation drives ASR performance disparities across ethnic groups

## Executive Summary
This study identifies specific phonetic features that drive racial bias in commercial ASR systems using the Pacific Northwest English corpus. The analysis reveals that African American speakers experience higher transcription error rates, particularly for resistance to low-back merger and pre-nasal merger patterns. The research demonstrates that acoustic modeling—rather than language modeling—is the primary source of bias, with PER reductions of 42-54% confirming phonetic systematicity of errors.

## Method Summary
The study analyzes 16 speakers reading a 192-word list from the Pacific Northwest English corpus, using four commercial ASR systems. Manual phonetic transcriptions from Praat TextGrids are aligned with ASR outputs through the PNWEdict pronunciation dictionary. Errors are classified using a Phonetic Error Rate metric that maps orthographic outputs to canonical phonetic forms. Sociophonetic features are annotated and error co-occurrence patterns are analyzed, with manual acoustic validation of the heuristic approach.

## Key Results
- African American speakers show higher PER (6.67-9.57%) compared to Caucasian American speakers (4.22-5.77%) across four commercial ASR systems
- Resistance to low-back merger (-AO) and pre-nasal merger (IN) features consistently correlate with higher error rates
- PER reductions of 42-54% versus WER indicate errors are phonetically systematic rather than lexical or syntactic
- Manual validation confirms 85% of heuristic error classifications match acoustic evidence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Acoustic modeling—rather than language modeling—is the primary source of ASR bias across ethnic groups.
- **Mechanism:** Training data predominantly reflects mainstream vowel systems; speakers who maintain phonemic distinctions (e.g., /ɑ/–/ɔ/) produce formant patterns that fall outside learned acoustic distributions, triggering systematic misrecognition.
- **Core assumption:** Commercial ASR systems rely on implicit phonetic probability distributions shaped by training corpus demographics.
- **Evidence anchors:**
  - [abstract] "acoustic modeling of dialectal phonetic variation, rather than lexical or syntactic factors, remains a primary source of bias"
  - [Section 5.2] PER reductions of 42–54% vs. WER suggest errors are phonetically systematic, not random lexical failures
  - [corpus] Neighbor paper "Automatic Speech Recognition Biases in Newcastle English: an Error Analysis" confirms regional-dialect acoustic bias extends beyond racial categories
- **Break condition:** If language-model interventions (e.g., dialect-aware lexicons) alone eliminate disparities, acoustic modeling is not primary.

### Mechanism 2
- **Claim:** Specific sociophonetic variables—low-back merger resistance (-AO) and pre-nasal /ɪ/–/ɛ/ merger (IN)—disproportionately co-occur with transcription errors for African American speakers.
- **Mechanism:** ASR systems map acoustic input to canonical phoneme sequences; when speakers' realizations diverge from canonical forms (e.g., maintaining /ɔ/ where merged systems expect /ɑ/), the decoder substitutes phonetically similar but lexically incorrect items.
- **Core assumption:** The heuristic PER mapping (ASR orthographic output → PNWEdict canonical forms) reliably approximates the system's internal phonetic confusion.
- **Evidence anchors:**
  - [Section 5.3] African American speakers show 5.33 IN instances/speaker vs. 3.5 for Caucasian American; -AO errors occur at 5.8–7.9% normalized rate for African American vs. 0–5.2% for Caucasian American across systems
  - [Section 5.5] Manual inspection finds "caught→cot" (6 instances for African American speakers), consistent with low-back merger resistance
  - [corpus] No direct corpus evidence for the PNWEdict heuristic's validity beyond the 85% acoustic validation subsample
- **Break condition:** If acoustic analysis shows <50% of flagged errors exhibit the predicted phonetic features, the heuristic misattributes errors.

### Mechanism 3
- **Claim:** Phonetic Error Rate (PER) provides a more diagnostic evaluation metric than Word Error Rate (WER) for identifying sociophonetic sources of ASR failure.
- **Mechanism:** PER aligns phone-level transcriptions rather than word-level orthography, giving partial credit for near-phonetic matches and removing word-boundary penalties—revealing systematic acoustic confusions masked by WER.
- **Core assumption:** Phone-level alignment tools (sclite) and pronunciation dictionaries (PNWEdict) accurately capture the relevant phonetic contrasts.
- **Evidence anchors:**
  - [Section 4.4.2] PER formula: (S_phone + D_phone + I_phone) / N_phone
  - [Section 5.5] "85% of errors classified as phonetically-motivated showed acoustic patterns consistent with the hypothesized feature"
  - [corpus] Weak corpus validation—no external replication of PER methodology yet
- **Break condition:** If PER systematically underestimates or overestimates error rates for specific dialects due to dictionary coverage gaps, metric validity is compromised.

## Foundational Learning

- **Concept: Word Error Rate (WER)**
  - **Why needed here:** Standard ASR evaluation metric; baseline for measuring disparities
  - **Quick check question:** Given S=3 substitutions, D=2 deletions, I=1 insertion, N=50 reference words, compute WER.

- **Concept: Low-back merger**
  - **Why needed here:** Key phonetic feature differentiating dialects; central to observed error patterns
  - **Quick check question:** In a merged system, are "cot" and "caught" produced with the same or different vowel qualities?

- **Concept: Acoustic modeling vs. language modeling**
  - **Why needed here:** Paper claims acoustic modeling is bias locus; understanding this distinction is essential for targeted mitigation
  - **Quick check question:** Which component maps audio features to phoneme probabilities, and which assigns probability to word sequences?

## Architecture Onboarding

- **Component map:**
  Audio → Commercial ASR API → Orthographic hypothesis → PNWEdict → Phonetic hypothesis → sclite alignment → PER calculation → Sociophonetic error analysis

- **Critical path:**
  1. Extract manual phonetic transcriptions from TextGrids
  2. Generate hypothesis phonetic transcriptions by mapping ASR output through PNWEdict
  3. Compute phone-level edit distance via sclite
  4. Cross-reference error positions with sociophonetic annotation layers
  5. Validate subset via spectrogram analysis

- **Design tradeoffs:**
  - Controlled word-list task enables clean phonetic alignment but may underestimate spontaneous-speech error rates
  - Heuristic PER assumes ASR word choice reflects phonetic confusion; modern end-to-end systems may violate this assumption
  - PNWEdict improves regional coverage but inherits CMUdict gaps for underdocumented variants

- **Failure signatures:**
  - High WER but low PER → likely language-model or lexical issues, not acoustic
  - Disparities concentrated in specific phonetic contexts → acoustic-model underrepresentation
  - PER validation rate <70% → heuristic may be misattributing error sources

- **First 3 experiments:**
  1. Replicate PER analysis on conversational-speech (CS) subset to test whether error patterns intensify in naturalistic speech.
  2. Substitute CMUdict for PNWEdict to quantify the contribution of region-specific pronunciation coverage.
  3. Stratify by gender and age within ethnic groups to test for intersectional effects (limited by n=16 speaker sample size).

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (n=16 speakers) limits statistical power for intersectional analysis
- Controlled word-list paradigm may not reflect spontaneous speech error patterns
- PER heuristic relies on PNWEdict coverage and assumes acoustic confusion from orthographic choices

## Confidence
- **High confidence**: The demonstration that acoustic modeling is the primary bias locus (supported by 42-54% PER reductions across four systems)
- **Medium confidence**: The specific identification of -AO and IN as primary error-correlating features (based on consistent pattern across speakers but limited sample size)
- **Medium confidence**: The PER metric's diagnostic utility (validated at 85% but with heuristic assumptions)

## Next Checks
1. Cross-dialect validation: Apply PER methodology to speakers from dialect regions with different merger patterns to test generalization beyond Pacific Northwest vowel systems.

2. Model architecture ablation: Test whether hybrid ASR systems show stronger PER-WER divergence than end-to-end models, confirming traditional acoustic modeling as bias locus.

3. Pronunciation lexicon robustness: Systematically replace PNWEdict pronunciations with CMUdict equivalents to quantify region-specific dictionary coverage contribution to error patterns.