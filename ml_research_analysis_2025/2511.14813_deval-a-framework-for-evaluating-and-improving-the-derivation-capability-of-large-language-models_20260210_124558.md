---
ver: rpa2
title: 'DEVAL: A Framework for Evaluating and Improving the Derivation Capability
  of Large Language Models'
arxiv_id: '2511.14813'
source_url: https://arxiv.org/abs/2511.14813
tags:
- evaluation
- llms
- task
- reasoning
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal definition of Derivation Capability
  (DC) for Large Language Models (LLMs), capturing the ability to apply abstract transformation
  rules when inputs change. A systematic evaluation framework, DEVAL, is proposed
  to assess DC across seven tasks using five popular LLMs.
---

# DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models

## Quick Facts
- arXiv ID: 2511.14813
- Source URL: https://arxiv.org/abs/2511.14813
- Reference count: 40
- Key outcome: This paper introduces a formal definition of Derivation Capability (DC) for Large Language Models (LLMs), capturing the ability to apply abstract transformation rules when inputs change. A systematic evaluation framework, DEVAL, is proposed to assess DC across seven tasks using five popular LLMs. The evaluation reveals that mainstream models, including GPT-4o and Claude3.5, perform moderately in recognizing derivation relations but significantly struggle in applying them to problem-solving. To address this, a novel prompt engineering approach, Derivation Prompting (DP), is introduced, which explicitly guides LLMs to recognize and apply derivation rules. DP achieves an average improvement of 15.2% in DC performance, outperforming other prompt engineering methods. These findings highlight the limitations of current LLMs in abstract reasoning and demonstrate the effectiveness of DP in enhancing DC.

## Executive Summary
This paper proposes a systematic framework for evaluating and improving Large Language Models' (LLMs) Derivation Capability (DC)—the ability to apply abstract transformation rules when problem inputs change. The authors introduce DEVAL, a comprehensive evaluation framework that assesses DC across seven tasks using five popular LLMs. They find that while models can recognize derivation relations, they struggle to apply them in problem-solving. To address this, they propose Derivation Prompting (DP), a prompt engineering approach that guides LLMs through three explicit steps: identifying input changes, explaining expected output changes, and applying the transformation. DP achieves an average 15.2% improvement in DC performance, outperforming other prompt engineering methods.

## Method Summary
The DEVAL framework evaluates Derivation Capability by testing whether LLMs maintain logical consistency when inputs are transformed according to defined derivation relations. The method involves: (1) defining Transformation $T$ and Relation $R$ as formal derivation rules, (2) generating paired input cases $(x_1, x_2)$ where $x_2 = T(x_1)$, (3) having the LLM generate outputs $(y_1, y_2)$ for both inputs, and (4) evaluating whether $(y_1, y_2) \in R$. The paper evaluates five LLMs (GPT-3.5, GPT-4o, Claude3.5, Qwen, Kimi) across seven tasks using three derivation relation types: Identity (ID), General (GE), and Task-Specific (TS). Derivation Prompting (DP) is implemented as a three-step template: (1) explain what change has occurred in the input; (2) explain what change in output should result; (3) apply this output change to your original answer.

## Key Results
- Mainstream LLMs achieve only moderate performance in recognizing derivation relations but significantly underperform in applying them to problem-solving
- Derivation Prompting (DP) improves average DC performance by 15.2% across tasks compared to standard prompting
- The dominant failure mode is DR-Misapplied (54.57%), where models correctly identify transformation rules but fail to execute the final step
- Standard Supervised Fine-Tuning (SFT) improves ID robustness but underperforms DP for GE and TS transformations
- Manual rule construction remains necessary as LLMs struggle to autonomously generate formally correct derivation relations (only 38% validity rate)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing the reasoning process into explicit "Change-Map-Apply" steps via Derivation Prompting (DP) reduces the rate of logical consistency errors (specifically "DR-Misapplied" failures).
- **Mechanism:** The paper suggests that LLMs often identify transformation rules but fail to propagate them to the final token generation. DP forces the model to externalize intermediate states: (1) identifying the input transformation $T$, (2) defining the expected output transformation $R$, and (3) applying $R$. This likely reduces the cognitive load on the model's internal attention mechanism, preventing it from defaulting to surface-level pattern matching.
- **Core assumption:** The model possesses the latent capability to recognize the transformation but fails to execute it due to a lack of structural scaffolding in the prompt context.
- **Evidence anchors:**
  - [section 4]: "DP aims to mitigate this issue by explicitly guiding LLMs through three steps... forcing LLMs to internalize and apply the DR."
  - [figure 9]: Demonstrates a specific case where standard CoT fails to adjust a math result, but DP successfully identifies the added term $+x$ and adjusts the integral.
  - [corpus]: Corpus papers like *Atomic Thinking of LLMs* support the decoupling of reasoning steps, but do not specifically validate the DP mechanism.
- **Break condition:** This mechanism fails if the transformation rule requires domain knowledge completely absent from the model's pre-training data, or if the prompt context window is exhausted by the required explanation.

### Mechanism 2
- **Claim:** Evaluating "Derivation Relations" (DR) exposes reasoning deficits that static accuracy metrics miss by testing for structural homomorphism.
- **Mechanism:** Instead of evaluating $\hat{M}(x) = y$ (correctness), the framework evaluates $(\hat{M}(x_1), \hat{M}(x_2)) \in R$ given $(x_1, x_2) \in T$. This tests if the model's internal representation of the problem preserves the structural relationship defined by $T$. A failure here indicates the model is relying on heuristics that solve specific instances but violate the underlying abstract rules.
- **Core assumption:** The derivation relation $f \sim (T, R)$ is well-defined and the transformation $T$ isolates the specific reasoning capability being tested without introducing noise.
- **Evidence anchors:**
  - [section 2.A]: "We say $T$ and $R$ exhibit the Derivation Relation... if and only if $f$ is a homomorphism..."
  - [figure 2]: Shows GPT-4o failing to maintain consistency when options are shuffled (changing answer from 'C' to 'A' incorrectly), a failure invisible to single-turn evaluation.
  - [corpus]: Weak support; related papers discuss evaluation benchmarks but not the specific homomorphic testing logic.
- **Break condition:** If the "ground truth" transformation $R$ is ambiguous or non-deterministic (e.g., creative writing tasks), the evaluation metric (DCS) becomes unreliable.

### Mechanism 3
- **Claim:** Supervised Fine-Tuning (SFT) primarily improves "Identity" robustness (output invariance) but is less effective than prompting for general derivation logic.
- **Mechanism:** The paper observes that while SFT binds specific patterns (helping with robustness where output shouldn't change), it struggles to generalize abstract transformation rules. Prompting (DP) acts as a "runtime" guide that activates existing reasoning pathways, whereas SFT acts as a "compile-time" weight update that may overfit to surface patterns without learning the underlying operator.
- **Core assumption:** The failure in derivation is due to retrieval/execution friction rather than a fundamental lack of model capacity.
- **Evidence anchors:**
  - [section 5.C]: "SFT yields a substantial improvement on ID transformations... but underperforms compared to DP-prompted... in tasks that require output adaptation."
  - [figure 12]: Visual evidence showing SFT boosting ID performance to 87% but lagging behind DP in General (GE) and Task-Specific (TS) categories.
  - [corpus]: *Pat-DEVAL* suggests legal reasoning requires specific oversight, aligning with the finding that general SFT isn't enough for complex rule application.
- **Break condition:** If the model size is too small to contain the concept of the transformation at all, prompting may fail to elicit the behavior, necessitating SFT or architectural changes.

## Foundational Learning

- **Concept: Homomorphism**
  - **Why needed here:** The entire DEVAL framework is built on the mathematical definition of a homomorphism—a structure-preserving map between two sets. Understanding this is required to define valid $T$ (input changes) and $R$ (output changes).
  - **Quick check question:** If I double the input $x$ (Transformation $T$), and the function is $f(x) = x^2$, what is the relation $R$ between the original output and the new output? (Answer: The new output is 4 times the original, not 2 times).

- **Concept: Behavioral Testing (Metamorphic Testing)**
  - **Why needed here:** Traditional software testing checks if $f(x) = y$. Metamorphic testing checks if $f(T(x)) = R(f(x))$. This is the core evaluation paradigm of DEVAL.
  - **Quick check question:** Why is checking accuracy on a test set insufficient for evaluating reasoning? (Answer: A model can memorize answers or use spurious correlations without understanding the causal logic).

- **Concept: Error Attribution Taxonomy**
  - **Why needed here:** To debug LLM failures effectively, one must distinguish between "Unaware" (didn't see the change), "Mislocalized" (didn't know what to change), and "Misapplied" (knew what to do but failed execution).
  - **Quick check question:** A model sees $2+2=4$ and $2+3=4$. Is this DR-Unaware or DR-Misapplied? (Answer: DR-Misapplied, likely; or DR-Mislocalized depending on whether it recognized the input change).

## Architecture Onboarding

- **Component map:**
  1.  **Rule Formalizer:** Defines the "Ground Truth" logic (Transformation $T$ and Relation $R$).
  2.  **Data Constructor:** Generates paired inputs $(x_1, x_2)$ based on $T$.
  3.  **Inference Engine:** The target LLM running either standard prompting or Derivation Prompting (DP).
  4.  **Consistency Evaluator:** Computes the Derivation Capability Score (DCS) by checking if output pair $(y_1, y_2)$ satisfies $R$.

- **Critical path:** The definition of the Derivation Relation ($T, R$). If $R$ is defined incorrectly (e.g., assuming linearity in a non-linear problem), the entire evaluation is invalid. As noted in Section 5B, LLMs struggle to generate these rules autonomously, requiring human expertise here.

- **Design tradeoffs:**
  - **Manual vs. Auto-Generated Rules:** Manual rules (high validity, low scalability) vs. LLM-generated rules (low validity, high scalability). Section 5B shows LLM-generated rules have only ~38% formal correctness.
  - **SFT vs. DP:** SFT is expensive and improves mostly robustness (ID); DP is cheap and improves general derivation (GE/TS).

- **Failure signatures:**
  - **DR-Unaware (25%):** Model ignores input changes entirely (robustness failure).
  - **DR-Misapplied (55%):** Model identifies the change and the rule but fails to execute the final step (the dominant failure mode).

- **First 3 experiments:**
  1.  **Sanity Check (ID Type):** Implement the "Option Shuffling" test (Sec 3, Fig 2a) on your target model. If the model changes its answer when option labels (A,B,C) are shuffled, it lacks basic consistency.
  2.  **DP vs. CoT Baseline:** Run a simple math transformation task (e.g., "Calculate $f(x)$, then $f(x)+c$") comparing standard Chain-of-Thought against the 3-step Derivation Prompting template provided in Section 4.
  3.  **Rule Formalization Stress Test:** Attempt to automatically generate rules for a novel domain using GPT-4o. Manually validate the "Implementability" and "Correctness" to verify the 38% failure rate finding in Section 5B.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLMs be enhanced to autonomously generate high-quality, formally correct, and implementable Derivation Relations (DRs) to remove the need for manual rule construction?
- **Basis in paper:** [explicit] Section V.B concludes that while LLMs can generate surface-level descriptions, they lack formal correctness and implementability. The authors explicitly state, "Enhancing LLMs’ ability to autonomously synthesize abstract transformation rules remains a promising direction for future work."
- **Why unresolved:** Current LLMs (like GPT-4o) produce DRs that frequently lack domain depth (only 18.6% used domain knowledge) and formal validity, making them unreliable for direct deployment in evaluation frameworks.
- **What evidence would resolve it:** An automated pipeline where LLM-generated DRs achieve a formal correctness rate comparable to human experts (>90%) across diverse tasks without human intervention.

### Open Question 2
- **Question:** What symbolically grounded training approaches are required to enable LLMs to acquire Derivation Capability through fine-tuning, surpassing the limitations of standard Supervised Fine-Tuning (SFT)?
- **Basis in paper:** [explicit] Section V.C shows that standard SFT fails to improve performance on general and task-specific transformations compared to Derivation Prompting. The paper asserts that "standard SFT... does not substantially enhance LLMs’ ability to understand abstraction-based reasoning... More symbolically grounded approaches are needed."
- **Why unresolved:** Standard SFT binds data pairs but fails to instill the abstract reasoning structure required for Derivation Capability, particularly for non-identity transformations.
- **What evidence would resolve it:** A new fine-tuning methodology that results in statistically significant improvements in Derivation Capability Score (DCS) for General (GE) and Task-Specific (TS) relations, outperforming prompt-based methods.

### Open Question 3
- **Question:** How can hybrid interpretability methods (e.g., symbolic tracing or white-box analysis) be integrated to reliably attribute errors in Derivation Capability?
- **Basis in paper:** [explicit] Section VI ("Limitation") identifies "Attribution challenges" as a key limitation. The authors note that relying on LLM-based evaluation of CoT "lacks the reliability and credibility of a human-expert interpretability framework" and cannot fully reveal dependencies.
- **Why unresolved:** Current attribution relies on probabilistic LLM judgments which may be inconsistent, making it difficult to definitively distinguish between "DR-Unaware," "DR-Mislocalized," and "DR-Misapplied" error types.
- **What evidence would resolve it:** A framework utilizing symbolic tracing that provides deterministic verification of the causal link between reasoning steps and final answers, validated against a human-expert ground truth.

## Limitations
- The paper relies on human expertise for manually constructing Derivation Relations, as LLMs struggle to generate formally correct rules autonomously (only 38% validity rate)
- Attribution of failure modes relies on LLM-based evaluation of Chain-of-Thought reasoning, which lacks the reliability of human-expert interpretability frameworks
- Standard Supervised Fine-Tuning (SFT) shows limited effectiveness for improving general and task-specific derivation capabilities compared to prompt engineering approaches
- The evaluation framework requires well-defined transformation rules and relations, making it challenging to apply to domains with ambiguous or non-deterministic transformations

## Confidence
- Method Summary: High
- Key Results: High
- Why This Works (Mechanism): Medium
- Foundational Learning: High
- Architecture Onboarding: High
- Open Questions the Paper Calls Out: High
- Limitations: High

## Next Checks
1. Implement the "Option Shuffling" test (Sec 3, Fig 2a) on your target model to verify basic consistency capabilities
2. Run a controlled experiment comparing standard Chain-of-Thought against the 3-step Derivation Prompting template on a simple math transformation task
3. Attempt to automatically generate derivation rules for a novel domain using GPT-4o and manually validate their formal correctness and implementability to verify the 38% failure rate finding