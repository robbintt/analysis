---
ver: rpa2
title: 'AquaticCLIP: A Vision-Language Foundation Model for Underwater Scene Analysis'
arxiv_id: '2502.01785'
source_url: https://arxiv.org/abs/2502.01785
tags:
- image
- underwater
- dataset
- marine
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AquaticCLIP introduces a novel vision-language model tailored for
  underwater scene analysis, addressing the challenge of limited aquatic data for
  vision-language pre-training. The model leverages a 2-million image-text paired
  dataset curated from diverse online sources, enhanced with unsupervised text generation
  using MarineGPT at both image and instance levels.
---

# AquaticCLIP: A Vision-Language Foundation Model for Underwater Scene Analysis

## Quick Facts
- arXiv ID: 2502.01785
- Source URL: https://arxiv.org/abs/2502.01785
- Reference count: 40
- Outperforms existing methods on underwater vision-language tasks with up to 96.80% accuracy

## Executive Summary
AquaticCLIP introduces a novel vision-language model tailored for underwater scene analysis, addressing the challenge of limited aquatic data for vision-language pre-training. The model leverages a 2-million image-text paired dataset curated from diverse online sources, enhanced with unsupervised text generation using MarineGPT at both image and instance levels. AquaticCLIP employs a prompt-guided vision encoder and a vision-guided text encoder to align visual and textual representations through contrastive pre-training. Extensive experiments show that AquaticCLIP outperforms existing state-of-the-art methods in zero-shot and fine-tuned tasks across marine species classification, fine-grained fish and coral classification, object detection, and counting, achieving up to 96.80% accuracy in zero-shot coral classification and 93.40% in fine-grained fish classification.

## Method Summary
AquaticCLIP adapts the CLIP architecture with two key innovations: a Prompt-Guided Vision Encoder (PGVE) that uses learnable visual prompts to aggregate semantically relevant features, and a Vision-Guided Text Encoder (VGTE) that grounds textual concepts in visual features through cross-attention. The model is pre-trained on 2 million aquatic image-text pairs, with text descriptions generated by MarineGPT and filtered through a semantic cleaning module. The training objective is symmetric cross-modal contrastive loss that aligns the PGVE-augmented visual embeddings with the VGTE-augmented textual embeddings in a shared latent space.

## Key Results
- Achieves 96.80% accuracy in zero-shot coral classification on LSF dataset
- Outperforms state-of-the-art methods by 7.10% F1 score on fine-grained fish classification
- Demonstrates strong zero-shot transfer capability across multiple aquatic datasets including MAI, FishNet, and LSF

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learnable visual prompts improve feature aggregation by selectively focusing on semantically relevant regions while suppressing background noise.
- **Mechanism:** The Prompt-Guided Vision Encoder (PGVE) initializes learnable prompt vectors that act as queries in a cross-attention layer against patch embeddings, allowing weighted fusion of patches to filter out irrelevant visual features.
- **Core assumption:** Standard CLS token aggregation fails to capture fine-grained aquatic details due to complex backgrounds like turbidity and color casts.
- **Evidence anchors:** Performance degradation when PGVE is removed (AquaticCLIP5 vs. AquaticCLIP), related work acknowledging detection difficulty in aquatic domains.
- **Break condition:** If prompt count is too low (<5), model cannot capture instance diversity; if too high, redundancy introduces noise.

### Mechanism 2
- **Claim:** Injecting visual context into the text encoder improves semantic alignment by grounding textual concepts in specific visual features.
- **Mechanism:** The Vision-Guided Text Encoder (VGTE) uses text embeddings as queries to attend to a concatenation of patch features and learned visual prompts, modifying text representation based on actual visual content.
- **Core assumption:** Text modality often lacks specificity for fine-grained zero-shot alignment without visual grounding.
- **Evidence anchors:** Performance comparison between AquaticCLIP and AquaticCLIP4 shows consistent F1 score drop, though corpus evidence for this specific mechanism is weak.
- **Break condition:** If cross-attention over-smooths text features, it may lose distinct semantic meanings required for cross-modal retrieval.

### Mechanism 3
- **Claim:** Enriching dataset with unsupervised instance-level descriptions improves fine-grained classification more than image-level descriptions alone.
- **Mechanism:** Using object detector to crop instances and caption them with MarineGPT creates denser semantic space where specific features are explicitly paired with text.
- **Core assumption:** Ground truth descriptions are often holistic and fail to describe individual species within complex scenes.
- **Evidence anchors:** Removing instance-level text specifically hurts fine-grained datasets (FishNet, LSF) more than coarse datasets, validating need for synthetic enrichment.
- **Break condition:** If object detector hallucinates bounding boxes, generated text misaligns with image content, introducing noise.

## Foundational Learning

- **Concept: Vision-Language Pre-training (CLIP)**
  - **Why needed here:** AquaticCLIP is a direct modification of CLIP architecture; understanding contrastive loss alignment in shared latent space is essential.
  - **Quick check question:** Can you explain why contrastive loss (InfoNCE style) maximizes cosine similarity of positive pairs while minimizing it for negative pairs within a batch?

- **Concept: Cross-Attention in Transformers**
  - **Why needed here:** PGVE and VGTE innovations are implemented as cross-attention layers where one modality attends to another.
  - **Quick check question:** In PGVE mechanism, what serves as Query, Key, and Value, and how does this differ from self-attention?

- **Concept: Zero-Shot Classification**
  - **Why needed here:** Primary evaluation metric is zero-shot transfer; understanding how to form text prompts to query learned embedding space is critical.
  - **Quick check question:** How does model predict class label for image it has never seen during training?

## Architecture Onboarding

- **Component map:** Image -> Patches -> ViT -> PGVE -> Image Embedding; Text -> Tokens -> Transformer -> VGTE -> Text Embedding
- **Critical path:** Image passes through patches, ViT, and PGVE (aggregating features via prompts) to create image embedding; text passes through tokens, transformer, and VGTE (refining via visual context) to create text embedding; loss maximizes similarity between final embeddings.
- **Design tradeoffs:** Synthetic text quality vs. data scale tradeoff (mitigated by cleaning module); full backbone fine-tuning vs. adapter-only tuning tradeoff (freezing backbone performs significantly worse).
- **Failure signatures:** High F1 on coarse datasets but low F1 on fine-grained indicates TDCM failure to generate specific enough descriptions; degraded performance on "clear" water images suggests overfitting to challenging conditions.
- **First 3 experiments:** 1) Validate Text Cleaning Module by sampling 100 generated captions and manually inspecting keyword relevance; 2) Ablation study on prompt count (varying n_r âˆˆ {5, 10, 20, 30}) to verify optimal balance; 3) Compare Linear Probe vs. Zero-Shot performance to identify if alignment bottleneck exists.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AquaticCLIP performance degrade when teacher model (MarineGPT) generates hallucinated or erroneous descriptions that bypass semantic cleaning module?
- Basis in paper: Dataset construction relies entirely on MarineGPT for text generation, with cleaning module removing irrelevant keywords but not necessarily correcting semantic misclassifications.
- Why unresolved: Paper assumes MarineGPT provides reliable signal but does not quantify impact of "noisy positives" from teacher model errors.
- What evidence would resolve it: Analysis of model accuracy on curated subset where MarineGPT's generated descriptions are intentionally perturbed or incorrect.

### Open Question 2
- Question: Does fixed number of learnable visual prompts (n_r=20) create bottleneck in scenes with extreme biodiversity or high object density?
- Basis in paper: Ablation study identifies 20 prompts as optimal, but mechanism aggregates patch features into these fixed prompts.
- Why unresolved: Unclear if model can represent semantic granularity when number of distinct objects significantly exceeds fixed prompt count.
- What evidence would resolve it: Evaluation performance on high-density counting datasets where instance count per image consistently exceeds prompt count.

### Open Question 3
- Question: To what extent can AquaticCLIP's image encoder support temporal understanding in video-based tasks?
- Basis in paper: Dataset constructed by extracting frames every 50 seconds to ensure uniqueness, removing temporal redundancy and coherence.
- Why unresolved: While model excels at static analysis, suitability for video tasks requiring motion or behavioral context remains unverified.
- What evidence would resolve it: Fine-tuning AquaticCLIP vision encoder on underwater video action recognition or tracking benchmark.

## Limitations
- Reliance on synthetic text generation through MarineGPT introduces potential label noise that could propagate through training pipeline
- Dataset construction favors "challenging" underwater conditions, potentially limiting performance on high-quality imagery from controlled environments
- Model assumes fixed number of visual prompts (20), but sensitivity analysis for this hyperparameter is limited

## Confidence
**High Confidence:** Core claim that AquaticCLIP outperforms existing methods on tested datasets is well-supported by experimental results and systematic ablation studies.
**Medium Confidence:** Claim that learnable visual prompts specifically improve feature aggregation by suppressing background noise is plausible but not directly measured through attention visualization.
**Low Confidence:** Vision-Guided Text Encoder's contribution is weakest empirically supported claim, with performance difference relatively small and corpus lacking direct architectural equivalents.

## Next Checks
1. **Text Quality Validation:** Sample 200 generated captions from MarineGPT and have marine biology experts rate relevance and accuracy, comparing against human-annotated captions to quantify noise in synthetic text pipeline.
2. **Attention Mechanism Analysis:** Visualize attention weights in PGVE cross-attention layer for diverse underwater images, creating heatmaps to show which patches receive highest attention and correlate with semantic regions.
3. **Cross-Domain Transferability:** Test AquaticCLIP's zero-shot performance on non-aquatic vision-language datasets (CLIP benchmarks, COCO captions) to assess whether architectural modifications introduce domain-specific biases.