---
ver: rpa2
title: Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward
arxiv_id: '2510.24020'
source_url: https://arxiv.org/abs/2510.24020
tags:
- confidence
- answer
- questions
- reward
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of mitigating hallucinations
  in Large Language Models (LLMs) by training them to abstain from answering questions
  beyond their knowledge scope. The core method, Fine-grained Semantic Confidence
  Reward (FISCORE), employs a reinforcement learning framework that guides LLMs to
  abstain via sample-specific confidence.
---

# Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward

## Quick Facts
- **arXiv ID**: 2510.24020
- **Source URL**: https://arxiv.org/abs/2510.24020
- **Reference count**: 40
- **Primary result**: Introduces FISCORE, a reinforcement learning method that improves LLM reliability by training models to abstain from answering unknown questions using semantic clustering-based confidence rewards.

## Executive Summary
This paper addresses the challenge of mitigating hallucinations in Large Language Models (LLMs) by training them to abstain from answering questions beyond their knowledge scope. The core method, Fine-grained Semantic Confidence Reward (FISCORE), employs a reinforcement learning framework that guides LLMs to abstain via sample-specific confidence. Specifically, it generates multiple candidate answers, conducts semantic clustering, and trains the model to retain answers within high-confidence clusters while discarding those within low-confidence ones. The method also introduces a new metric, F1 rel, to evaluate the reliability of abstention fine-tuning tasks more comprehensively by balancing helpfulness and truthfulness. Experiments demonstrate that FISCORE significantly enhances reliability in both in-domain and out-of-distribution benchmarks, outperforming baselines in most cases.

## Method Summary
The FISCORE method trains LLMs to abstain from answering unknown questions through a reinforcement learning framework. The process begins by generating multiple candidate answers (G=10) for each prompt using the policy LLM. These responses are then clustered using a Natural Language Inference (NLI) model to group semantically equivalent answers. The size of each cluster serves as a proxy for the model's intrinsic confidence - larger clusters indicate higher consensus and confidence, while smaller clusters indicate uncertainty. The model is then trained using Group Relative Policy Optimization (GRPO) to align its verbalized confidence (expressed as "sure" or "unsure") with this intrinsic confidence level. The total reward function combines confidence alignment, accuracy on known questions, and format adherence to prevent reward hacking while maintaining helpfulness.

## Key Results
- FISCORE significantly improves F1_rel (harmonic mean of helpfulness and truthfulness) on Pararel dataset, achieving state-of-the-art performance.
- The method demonstrates strong generalization to out-of-distribution datasets like TriviaQA and NQ, with improvements of 3.45% and 1.32% respectively over SE-Tuning.
- Ablation studies confirm that both the confidence reward (Rc) and accuracy reward (Ra) are essential components, with accuracy reward preventing over-abstention and maintaining answer accuracy.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Clustering as a Confidence Proxy
The method assumes that the size of a semantic cluster derived from multiple generated answers serves as a reliable proxy for the model's intrinsic confidence. By sampling G responses and clustering them via Natural Language Inference (NLI), the method posits that large clusters indicate high consensus (confidence) while small or singleton clusters indicate divergence (uncertainty). The core assumption is that generative semantic consensus correlates strongly with factual correctness or knowledge boundaries.

### Mechanism 2: Fine-Grained Alignment via Reward Shaping
The reward function Rc grants a positive signal only if the model says "sure" when its answer falls in a large cluster (|Ci| ≥ τ) or "unsure" when in a small cluster (|Ci| < τ). This creates a direct gradient for self-awareness, teaching the model to associate internal state distribution (reflected by clusters) with explicit verbal tokens.

### Mechanism 3: Balancing Helpfulness and Truthfulness with Accuracy Rewards
Adding an auxiliary accuracy reward prevents the model from learning a "lazy" policy of constant abstention or confident guessing on wrong answers. The total reward Rtotal weights the confidence reward (wc) alongside an accuracy reward (wa), forcing the model to maintain high accuracy on known questions (F1ans) while optimizing abstention (F1abs).

## Foundational Learning

- **Concept**: Natural Language Inference (NLI) and Entailment
  - **Why needed here**: The architecture relies on an NLI model (DeBERTa) to determine if two generated answers are semantically equivalent (entailment) to form clusters.
  - **Quick check question**: Can you explain why checking for bidirectional entailment is stricter than simple string matching for grouping answers?

- **Concept**: Group Relative Policy Optimization (GRPO)
  - **Why needed here**: This RL algorithm samples a group of outputs per prompt to compute advantages, which is structurally necessary for the clustering-based confidence estimation.
  - **Quick check question**: How does GRPO differ from standard PPO in terms of how the advantage function is calculated relative to a group of samples?

- **Concept**: The Abstention Trade-off (Helpfulness vs. Truthfulness)
  - **Why needed here**: The paper introduces F1rel specifically to measure the harmonic mean of answering known questions and abstaining on unknown ones.
  - **Quick check question**: Why is accuracy alone a flawed metric for evaluating a model's ability to abstain from unknown questions?

## Architecture Onboarding

- **Component map**: Policy LLM -> NLI Module (DeBERTa) -> Reward Engine -> GRPO Trainer
- **Critical path**: Prompt → Sample G Rollouts → NLI Clustering → Determine Intrinsic Confidence (Cluster Size) → Compare with Verbalized Confidence → Calculate Reward → GRPO Update
- **Design tradeoffs**:
  - **Sampling Number (G)**: Higher G (e.g., 10) improves cluster stability but increases inference/compute cost linearly
  - **Threshold (τ)**: Setting τ = ⌈G/2⌉ is heuristic; lower thresholds encourage answering, higher thresholds encourage abstention
- **Failure signatures**:
  - **Reward Hacking**: Model generates uniform but incorrect answers that form a single large cluster, triggering "sure" rewards (mitigated by Ra)
  - **Over-abstention**: Model learns to say "unsure" on known questions if accuracy rewards are too weak (wa ≈ 0)
  - **Format Instability**: Model fails to generate valid <answer> tags, resulting in zero total reward
- **First 3 experiments**:
  1. **Hyperparameter Sweep**: Validate optimal G and wa on a validation split of Pararel to balance compute vs. F1rel
  2. **Baseline Comparison**: Run FISCORE vs. SE-Tuning on out-of-distribution (OOD) datasets (TriviaQA) to verify generalization claims
  3. **Cluster Analysis**: Visualize cluster sizes for known vs. unknown questions to empirically confirm if the "semantic consensus" assumption holds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the abstention threshold τ be automated or made adaptive to varying contexts instead of being manually set?
- **Basis in paper**: [explicit] Section F (Limitations) explicitly states, "we must manually set the threshold τ during GRPO training."
- **Why unresolved**: The current implementation relies on an empirical, static threshold (τ=5) to distinguish between high and low confidence clusters. A static threshold may not generalize well across different domains or varying levels of query difficulty without manual retuning.
- **What evidence would resolve it**: The development of a dynamic thresholding mechanism or a self-adaptive algorithm that adjusts τ based on the specific distribution of semantic clusters for a given input, demonstrating improved reliability without manual intervention.

### Open Question 2
- **Question**: How can the computational overhead of generating full answers for post-hoc abstention decisions be minimized?
- **Basis in paper**: [explicit] Section F (Limitations) notes that the method "requires more computation than directly abstaining" because it teaches LLMs to "first generate an answer and then express the abstention decision."
- **Why unresolved**: The current architecture requires the model to generate a complete answer before evaluating confidence, which is computationally wasteful for questions the model eventually decides to reject.
- **What evidence would resolve it**: An architectural modification or training objective that allows the model to predict the need for abstention earlier in the generation process (e.g., internal state monitoring) while maintaining the reliability achieved by FISCORE.

### Open Question 3
- **Question**: How can model reliability be evaluated in scenarios where the model's pre-existing knowledge scope is unknown?
- **Basis in paper**: [explicit] Section F (Limitations) states that the proposed metrics (F1ans, F1abs, F1rel) are limited because "we must know the knowledge that the model possesses before alignment."
- **Why unresolved**: The current evaluation framework relies on partitioning questions into "Known" and "Unknown" sets based on the performance of an initial LLM. This creates a dependency on the specific benchmark and the initial model's capabilities, making it difficult to assess reliability on novel datasets where the ground truth capability is undefined.
- **What evidence would resolve it**: A new evaluation metric that accurately correlates with reliability improvements without requiring a pre-defined partition of the model's knowledge boundaries (e.g., a reference-free reliability metric).

## Limitations
- The method relies heavily on the NLI model's ability to capture true semantic equivalence, which can struggle with nuanced cases like near-miss factual errors.
- The approach requires multiple forward passes (G=10) per inference, creating substantial computational overhead that scales linearly with G.
- Training assumes access to a clean dataset with known answer boundaries, which may not generalize to real-world scenarios where ground truth is noisy or incomplete.

## Confidence
- **High Confidence**: The core architecture (GRPO + semantic clustering + confidence alignment reward) is technically sound and experimental results show clear improvements over baselines on held-out datasets.
- **Medium Confidence**: The claim that FISCORE generalizes to out-of-distribution datasets is supported but not comprehensively validated.
- **Low Confidence**: The scalability claims are difficult to verify without runtime benchmarks; the paper states "linearly proportional" cost but doesn't provide wall-clock measurements.

## Next Checks
1. **NLI Robustness Test**: Generate synthetic datasets where multiple wrong answers cluster together (e.g., common misconceptions) to empirically verify if cluster size still correlates with correctness rather than just consensus.
2. **Cost-Benefit Analysis**: Measure actual inference latency with G=10 vs G=5 on a production-scale deployment to quantify the real-world overhead beyond the stated linear relationship.
3. **Transfer Learning Validation**: Fine-tune the same model on a different domain (e.g., medical QA) and test if the learned abstention behavior transfers without catastrophic forgetting of the semantic clustering mechanism.