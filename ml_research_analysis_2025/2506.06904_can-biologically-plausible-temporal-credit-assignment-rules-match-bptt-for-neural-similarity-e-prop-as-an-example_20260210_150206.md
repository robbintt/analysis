---
ver: rpa2
title: Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for
  Neural Similarity? E-prop as an Example
arxiv_id: '2506.06904'
source_url: https://arxiv.org/abs/2506.06904
tags:
- neural
- learning
- rules
- similarity
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines whether biologically plausible learning rules\
  \ can match Backpropagation Through Time (BPTT) in producing brain-like neural representations.\
  \ Using Procrustes analysis on primate neural datasets (Mante 2013 and Sussillo\
  \ 2015), we find that e-prop\u2014a gradient truncation-based, biologically plausible\
  \ learning rule\u2014achieves neural data similarity comparable to BPTT when matched\
  \ for task accuracy."
---

# Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example

## Quick Facts
- arXiv ID: 2506.06904
- Source URL: https://arxiv.org/abs/2506.06904
- Reference count: 40
- Key outcome: E-prop matches BPTT in neural similarity when matched for accuracy; initial weight gain has larger impact than learning rule choice

## Executive Summary
This study investigates whether biologically plausible learning rules can produce brain-like neural representations comparable to Backpropagation Through Time (BPTT). Using Procrustes analysis on primate neural datasets (Mante 2013 and Sussillo 2015), the authors find that e-prop—a gradient truncation-based, biologically plausible learning rule—achieves neural data similarity comparable to BPTT when matched for task accuracy. Critically, the study reveals that architectural choices and initialization parameters, particularly initial weight gain, have a larger impact on neural similarity than the choice of learning rule itself.

## Method Summary
The researchers compared RNNs trained with BPTT versus e-prop on two primate neural datasets. Models used leaky integration dynamics with retanh activation, Adam optimizer (lr=1e-3), and 200-400 hidden units. Neural similarity was quantified using Procrustes distance (via netrep library) between model hidden representations and trial-averaged neural responses. The study also examined weight eigenspectrums and dynamical properties post-training. Multiple seeds and noise levels were used, with e-prop implemented via gradient truncation using h.detach().

## Key Results
- E-prop achieves neural similarity comparable to BPTT when models are matched for task accuracy
- Initial weight gain parameter has the largest impact on final neural similarity across conditions
- Lower learning rates for BPTT increase similarity to e-prop models
- Post-training analyses reveal similar weight eigenspectrums and dynamical properties between BPTT and e-prop models at comparable accuracies

## Why This Works (Mechanism)
The study demonstrates that biologically plausible temporal credit assignment through gradient truncation can approximate the brain-like representations produced by exact BPTT gradients. By matching task performance, both learning rules converge to similar regions of the solution manifold, suggesting that the path taken to reach a solution matters less than the final accuracy and architectural constraints. The truncation in e-prop acts as a local approximation that preserves key dynamical properties while maintaining biological plausibility through local weight updates.

## Foundational Learning
- **Concept: Temporal Credit Assignment**
  - Why needed here: The core problem addressed is how a network assigns credit/blame for a final error to specific neural activations and weights that occurred at past time steps. BPTT solves this exactly but is biologically implausible; e-prop uses a truncated, local approximation.
  - Quick check question: Can you explain in one sentence why standard backpropagation is considered biologically implausible for temporal tasks? (Answer: It requires storing the entire history of activations and propagating error signals backwards in time through symmetric weights).

- **Concept: Three-Factor Learning Rule**
  - Why needed here: E-prop is formalized as a three-factor rule: pre-synaptic activity × post-synaptic eligibility trace × top-down instructive signal (e.g., neuromodulator conveying error/loss). This is the mathematical form of its biological plausibility claim.
  - Quick check question: What are the three factors in the e-prop weight update? (Answer: Pre-synaptic activity, post-synaptic eligibility trace, and a top-down learning signal/modulator).

- **Concept: Procrustes Distance**
  - Why needed here: This is the primary metric used to quantify "neural similarity." It measures the geometric distance between the manifold of a model's hidden representations and that of neural recordings after optimal rotation and scaling alignment.
  - Quick check question: What transformation is Procrustes analysis allowed to apply to align two representations before measuring their distance? (Answer: Orthogonal rotation and global scaling/stretching).

## Architecture Onboarding
- **Component map:**
  - Input Weights (W_x) -> Hidden State (h_t) -> Readout Weights (w) -> Output
  - Hidden State evolves via h_{t+1} = βh_t + (1-β)(W_h f(h_t) + W_x x_t)
  - Learning Rule computes weight updates (∆W) from error signal

- **Critical path:** The choice of Initial Weight Gain (g) has the most significant impact on final neural similarity. Set this parameter carefully. The Learning Rule (e-prop vs BPTT) is secondary, but e-prop is preferred for biological modeling. The Learning Rate affects the path and speed of convergence; lower rates for BPTT make it more similar to e-prop.

- **Design tradeoffs:**
  - E-prop vs BPTT: E-prop is local and biologically plausible but may require more training iterations to converge. BPTT is faster and exact but biologically implausible.
  - High vs Low Initial Gain: High gain can lead to more complex ("rich") dynamics but may be unstable. Low gain can lead to "lazy" learning with less expressive dynamics. Gain strongly biases the final solution manifold.
  - Sparsity/Dale's Law: Enforcing biological constraints like sparsity or excitatory/inhibitory separation (Dale's law) may affect performance and similarity, though the paper shows consistent trends under these constraints (Appendix Fig 7).

- **Failure signatures:**
  - E-prop fails to learn / converges to poor accuracy: The task may require temporal dependencies that are too long-range for the truncated gradient. Try a simpler task or consider rules with longer memory approximations.
  - Neural similarity is very low (high Procrustes distance) despite good accuracy: The task's solution manifold is large, and the model has converged to a non-brain-like region. Adjust initial gain or try a different architecture initialization. The paper notes that "not all learning rules are equally effective" and that older methods like node perturbation yield higher distances.

- **First 3 experiments:**
  1. Replicate core finding on a new task: Train RNNs on a temporal task (e.g., from Neurogym) using both BPTT and e-prop. Plot accuracy vs. Procrustes distance (after obtaining some neural proxy data or using a pre-existing dataset). Expect curves to overlap when matched for accuracy.
  2. Ablate initial gain: Fix the learning rule (e.g., e-prop) and train models with varying initial weight gains (g = 0.5, 1.0, 1.5). Plot the final Procrustes distance. Expect a significant variation, demonstrating the primacy of initialization.
  3. Vary BPTT learning rate: Train BPTT models with learning rates lr = 1e-3, 3e-4, 1e-4. Compute and plot the distance from these models to a fixed e-prop model. Expect the distance to decrease as the BPTT learning rate is lowered.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can incorporating neural data recorded during the learning process resolve the identifiability issues of distinguishing between different learning rules? The authors state that "incorporating neural data during learning — could help narrow the solution space, and we view this as a promising direction for future work."
- **Open Question 2:** Do biologically plausible rules maintain neural similarity to BPTT in tasks where they exhibit performance deficits compared to backpropagation? The authors note that "e-prop struggles on certain tasks (Liu et al., 2021), motivating future efforts to gather experimental data where bio-plausible rules diverge, enabling more targeted comparisons."
- **Open Question 3:** Can these findings regarding temporal credit assignment in recurrent networks be generalized to non-temporal domains such as vision? The authors identify that "extending these analyses to non-temporal domains — such as vision, where many recent bio-plausible rules are formulated — is a valuable direction for future work."

## Limitations
- The study relies on two primate datasets with limited condition diversity, creating potential generalizability concerns
- Sussillo 2015 dataset requires direct author correspondence for access, creating a reproducibility bottleneck
- The study doesn't systematically explore how learning rule effects vary across fundamentally different network architectures (e.g., LSTMs, transformers)

## Confidence
- **High Confidence**: The finding that e-prop achieves comparable neural similarity to BPTT when matched for accuracy is well-supported by consistent results across both datasets and multiple validation metrics
- **Medium Confidence**: The claim that initial weight gain has the largest impact on neural similarity is supported but could benefit from broader architectural exploration beyond the current RNN setup
- **Medium Confidence**: The assertion that biological plausibility doesn't necessarily compromise neural similarity representation quality is reasonable but limited by the specific datasets and tasks examined

## Next Checks
1. Test the BPTT vs e-prop comparison on a larger, more diverse neural dataset (e.g., Steinmetz et al. 2019) to assess generalizability across recording techniques and brain regions
2. Systematically vary network architecture (LSTM, transformer) while holding learning rules constant to quantify the relative contribution of architecture vs initialization vs learning rule
3. Conduct a parameter sensitivity analysis on initial weight gain across a wider range (g ∈ [0.1, 2.0]) with finer granularity to map the full landscape of its impact on neural similarity