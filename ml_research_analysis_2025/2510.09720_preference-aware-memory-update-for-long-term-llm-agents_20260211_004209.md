---
ver: rpa2
title: Preference-Aware Memory Update for Long-Term LLM Agents
arxiv_id: '2510.09720'
source_url: https://arxiv.org/abs/2510.09720
tags:
- memory
- preference
- user
- arxiv
- long-term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of dynamically updating user preference
  memory in long-term LLM agents, where existing systems fail to adaptively track
  evolving user behaviors during prolonged interactions. The authors propose a Preference-Aware
  Memory Update (PAMU) mechanism that combines sliding window averages (SW) and exponential
  moving averages (EMA) to create a fused preference representation capturing both
  short-term fluctuations and long-term user trends.
---

# Preference-Aware Memory Update for Long-Term LLM Agents

## Quick Facts
- **arXiv ID**: 2510.09720
- **Source URL**: https://arxiv.org/abs/2510.09720
- **Reference count**: 4
- **Primary result**: PAMU improves LLM output quality on long-term tasks by tracking user preference shifts via dual-scale temporal fusion.

## Executive Summary
This paper addresses the challenge of dynamically updating user preference memory in long-term LLM agents, where existing systems fail to adaptively track evolving user behaviors during prolonged interactions. The authors propose a Preference-Aware Memory Update (PAMU) mechanism that combines sliding window averages (SW) and exponential moving averages (EMA) to create a fused preference representation capturing both short-term fluctuations and long-term user trends. The system extracts multi-dimensional preference signals (tone, length, emotion, density, formality) from dialogue and uses deviation between SW and EMA to detect preference shifts. Evaluated on the LoCoMo dataset across five task scenarios, PAMU significantly improved output quality of LLM in five baseline memory systems, with statistically significant gains in F1 and BLEU-1 scores (e.g., improvements from 17.24/11.35 to 17.93/11.73 for single-hop tasks). The mechanism is model-agnostic, requiring no fine-tuning, and demonstrated effectiveness through ablation studies and controlled case studies showing real-time preference adaptation.

## Method Summary
PAMU extracts five-dimensional preference vectors from dialogue history using specialized classifiers (RoBERTa for tone, SKEP for emotion, OpenNRE for information density, formality classifier, and normalized token counts for length). It maintains dual representations: a Sliding Window (SW) average over W turns for short-term context and an Exponential Moving Average (EMA) with decay β for long-term trends. These are fused via weighted combination (λ·SW + (1-λ)·EMA) to create a stable yet responsive preference vector. A change detection signal computes the normalized deviation between SW and EMA, triggering preference prompt updates when exceeding threshold δ. The fused vector is discretized and formatted as natural language instructions injected into the LLM prompt. The system requires no fine-tuning of the base model and was tested with Qwen and LLaMA variants.

## Key Results
- PAMU improved F1 scores from 17.24 to 17.93 and BLEU-1 from 11.35 to 11.73 on single-hop tasks in LoCoMo dataset
- Statistically significant performance gains over five baseline memory systems across all task types
- Ablation studies confirmed the necessity of both dual-perspective modeling and change detection components
- Case studies demonstrated real-time adaptation to user preference shifts without requiring model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Dual-Scale Temporal Fusion
The system computes separate representations for immediate context (SW) and historical inertia (EMA), then fuses them to balance responsiveness and stability. This mathematical fusion of sliding window and exponential moving averages for preference vectors is not detailed in similar corpus neighbors, though the general principle of integrating trends and fluctuations in time-series is validated in traffic prediction literature.

### Mechanism 2: Divergence-Based Change Detection
The magnitude of deviation between short-term (SW) and long-term (EMA) estimates serves as a reliable signal for triggering memory updates. This divergence metric is formalized as an interpretable and actionable controller, though general alignment work supports the need for update triggers without validating this specific approach.

### Mechanism 3: Explicit Preference Prompting
Structured preference vectors are converted to natural language instructions and injected as prompts, effectively conditioning the LLM without requiring weight updates. This approach assumes sufficient instruction-following capabilities in the base model and aligns with other work emphasizing explicit user profiles for personalization.

## Foundational Learning

**Time-Series Smoothing (EMA vs. SMA)**
- Why needed here: The core logic relies on tuning β (decay) and W (window). Without understanding how EMA lags vs. how SMA drops data, you cannot debug the "change detection" signal.
- Quick check question: If a user changes style abruptly at turn t, which signal moves first: SW or EMA?

**Prompt Engineering & Discretization**
- Why needed here: You must map continuous scalar values (e.g., density 0.45) to tokens the LLM understands ("Moderate").
- Quick check question: How does the system handle a value exactly on the boundary of a discretization bin (e.g., 0.33)?

**Feature Extraction via Classification**
- Why needed here: The memory is only as good as the extracted signals (Tone, Formality). You need to understand the reliability of the underlying RoBERTa/OpenIE models.
- Quick check question: Does the "Tone Style" extractor output a single label or a probability distribution, and how does that affect the SW calculation?

## Architecture Onboarding

**Component map:**
Preference Extractor (RoBERTa/SKEP/OpenNRE) → Perception Module (SW/EMA logic) → Change Detector (C_t calculation) → Prompt Formatter (vector→text) → LLM (base model)

**Critical path:**
The Preference Extractor accuracy is the primary dependency; if tone/formality is misclassified, the update mechanism propagates error. The Prompt Formatter is the integration point with the LLM.

**Design tradeoffs:**
- Model-Agnostic vs. Latency: The system requires inference calls to multiple extractor models (RoBERTa, etc.) and the base LLM, increasing latency compared to end-to-end models.
- Generalization vs. Specificity: The 5 chosen dimensions (Tone, Length, etc.) are fixed. New user preferences (e.g., "use emojis") require code changes, not just learning.

**Failure signatures:**
- Oscillation: Output style fluctuates wildly between turns (likely λ or window W set too low).
- Stagnation: User changes intent, but the agent ignores it (EMA decay β too high or threshold δ too strict).
- Format Errors: LLM ignores instructions (Prompt template missing or LLM too small).

**First 3 experiments:**
1. Unit Test SW/EMA Logic: Feed synthetic data with a step-function change (e.g., 0,0,0,1,1,1) and verify the detection signal C_t peaks at the transition.
2. Prompt Ablation: Run the pipeline but strip the generated preference prompt; compare BLEU/F1 scores to quantify the specific contribution of the prompt injection.
3. Hyperparameter Sweep: Vary λ (fusion weight) to find the "elbow" where the model responds to change without becoming erratic.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and limitations, several remain unresolved regarding scalability to open-vocabulary preferences, the validity of F1/BLEU metrics for preference alignment, and sensitivity to hyperparameter choices.

## Limitations
- The empirical validation rests on a single-source benchmark (LoCoMo dataset) with no external validation on other long-term interaction corpora.
- Critical hyperparameters (W, β, λ, δ, ϵ, K) are not numerically specified, making exact replication challenging.
- The system's dependence on five predefined preference dimensions may limit adaptability to users with preferences outside these categories.

## Confidence
- **High Confidence**: The PAMU mechanism improves F1 and BLEU-1 scores over five baseline memory systems on the LoCoMo dataset. The dual-scale temporal fusion approach (SW+EMA) is mathematically sound and the ablation studies are properly controlled.
- **Medium Confidence**: The claim that PAMU works "without any model fine-tuning" assumes the base LLM has sufficient instruction-following capability. The effectiveness of divergence-based change detection depends heavily on threshold selection, which is not fully specified.
- **Low Confidence**: The generalizability claim across "various long-term LLM agents" extends beyond the tested memory systems. The discretization strategy for preference vectors may lose important nuance in user preferences.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary W, β, λ, and δ across multiple orders of magnitude to identify optimal ranges and stability boundaries. Test whether performance gains persist across different LLM sizes (1.5B to 30B parameters).
2. **Cross-Dataset Validation**: Evaluate PAMU on at least two additional long-term interaction datasets (e.g., from customer service or tutoring domains) to assess generalizability beyond LoCoMo. Compare against a competitive preference-aware baseline not tested in the original paper.
3. **Preference Dimension Ablation**: Remove individual preference dimensions (e.g., test without emotion or formality) to quantify their relative contribution to performance gains. Test whether the system can dynamically learn new preference dimensions from user feedback rather than requiring code changes.