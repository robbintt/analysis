---
ver: rpa2
title: 'OLMoASR: Open Models and Data for Training Robust Speech Recognition Models'
arxiv_id: '2508.20869'
source_url: https://arxiv.org/abs/2508.20869
tags:
- data
- whisper
- training
- speech
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OLMoASR, a new dataset and model suite for
  robust zero-shot speech recognition. The authors curated a large-scale dataset,
  OLMoASR-Mix, from 3M hours of audio and 17M transcripts, using text heuristics to
  filter low-quality or mistranscribed data, resulting in 1M high-quality hours.
---

# OLMoASR: Open Models and Data for Training Robust Speech Recognition Models

## Quick Facts
- arXiv ID: 2508.20869
- Source URL: https://arxiv.org/abs/2508.20869
- Reference count: 23
- Models achieve comparable performance to OpenAI's Whisper across short- and long-form benchmarks, with the medium-sized model reaching 12.8% and 11.0% WER on short and long-form tasks, matching Whisper-medium's 12.4% and 10.5% WER

## Executive Summary
This paper introduces OLMoASR, a new dataset and model suite for robust zero-shot speech recognition. The authors curated a large-scale dataset, OLMoASR-Mix, from 3M hours of audio and 17M transcripts, using text heuristics to filter low-quality or mistranscribed data, resulting in 1M high-quality hours. They trained models ranging from 39M to 1.5B parameters using the Whisper architecture and recipe. OLMoASR models achieve comparable performance to OpenAI's Whisper across short- and long-form benchmarks, with data curation, especially removing repeating lines, improving performance by up to 14.5% WER.

## Method Summary
OLMoASR introduces a data curation pipeline that filters 3M hours of raw audio-transcript pairs down to 1M high-quality hours using language alignment and text-based heuristics. The models use the Whisper architecture with five sizes (39M to 1.5B parameters) trained via AdamW with modified hyperparameters (batch 512, LR 1.5×10⁻³, warmup 1,049 steps). Training uses FlashAttention and dynamic loss scaling, with DDP+FP16 for smaller models and FSDP+bfloat16 for larger ones. Evaluation uses greedy decoding for short-form and beam search for long-form tasks across 21 benchmarks.

## Key Results
- OLMoASR-medium achieves 12.8% and 11.0% WER on short and long-form tasks, matching Whisper-medium's 12.4% and 10.5% WER
- Data curation pipeline removes 66% of data but improves performance by up to 14.5% WER
- Training on OLMoASR-Mix outperforms both academic datasets and weakly labeled web-scale data like YODAS

## Why This Works (Mechanism)
The paper demonstrates that aggressive data curation, particularly removing repeating lines and applying text-based quality heuristics, significantly improves model performance by eliminating low-quality or mistranscribed data. The Whisper architecture with FlashAttention enables efficient training at scale, while the zero-shot evaluation across diverse benchmarks shows robust generalization. The curated dataset's quality enables models to match Whisper's performance despite using open-source components.

## Foundational Learning

- **Concept**: **Zero-shot Generalization**
  - Why needed here: OLMoASR's primary goal is to be a robust model that works "out-of-the-box" on unseen datasets. Understanding this concept is crucial for interpreting all results.
  - Quick check question: Can you explain why a model trained on OLMoASR-Mix might perform better on a completely new benchmark than a model trained only on LibriSpeech?

- **Concept**: **Encoder-Decoder Architecture (Whisper-style)**
  - Why needed here: OLMoASR adopts the Whisper architecture. Knowing the roles of the encoder (processing audio) and decoder (generating text) is fundamental to understanding the model's function.
  - Quick check question: In the OLMoASR model, which part is responsible for transcribing the processed audio features into a sequence of tokens?

- **Concept**: **Data Curation and Filtering**
  - Why needed here: The paper's central contribution is its curation pipeline (removing bad data). Grasping this is key to understanding the paper's "why it works" argument.
  - Quick check question: Name one of the text-based heuristics used by OLMoASR to filter out low-quality training data and what type of problem it targets.

## Architecture Onboarding

- **Component map**: OLMoASR-Pool (3M hrs) → Curation Pipeline (Text heuristics, alignment) → OLMoASR-Mix (1M hrs) → Encoder-Decoder Transformer (Whisper architecture) with FlashAttention → Training (modified Whisper recipe) → Evaluation (short-form and long-form benchmarks)

- **Critical path**: 1. Data collection from OLMoASR-Pool. 2. Audio-text language alignment filtering. 3. Application of quality heuristics (repeat line removal, casing, WER-based filtering). 4. Model training on the final OLMoASR-Mix dataset. 5. Zero-shot evaluation on a suite of 21 benchmarks.

- **Design tradeoffs**: Data Quality vs. Quantity: The pipeline aggressively discards 66% of the data (from 3M to 1M hours) to gain quality. The tradeoff is potential loss of rare but useful signal versus improved learning efficiency. Scale vs. Capacity: The paper shows diminishing returns for a 74M model when scaling data beyond 220K hours. This highlights a tradeoff between dataset size and model parameter count.

- **Failure signatures**: No Quality Filtering Baseline: Very high WER (e.g., 37.2% avg short-form for tiny). Training on this indicates the curation pipeline is broken or bypassed. Domain Overfitting: Good performance on LibriSpeech but poor on CORAAL or CHiME-6 would indicate the training data lacks sufficient diversity. Instability: The paper notes using FSDP with bfloat16 was more stable than FP16 for larger models; divergence on larger runs may indicate numerical precision issues.

- **First 3 experiments**: 1. Baseline Validation: Reproduce the tiny.en model result on the "no quality filtering" subset and the OLMoASR-Mix to confirm the 14.4% WER improvement from repeat line removal. 2. Ablation on Filtering: Retrain a small model by removing only one filter at a time (e.g., only the casing filter, only the WER filter) to measure individual contribution to the final WER. 3. Generalization Test: Take a trained base.en or small.en model and evaluate it on a single, held-out OOD dataset (e.g., CORAAL) to verify zero-shot robustness claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the data scaling efficiency improve for the largest 1.5B parameter models, or is the utility of the OLMoASR-Mix dataset capped by the observed plateau in smaller models?
- Basis in paper: [inferred] Section 5.1 shows that increasing data by 20× for the 74M model yielded only a 2.1% WER reduction, leading the authors to speculate the model was "too small" to leverage the scale.
- Why unresolved: The scaling ablation was restricted to the 74M architecture; it remains unclear if the 1.5B model can effectively utilize the full 1M hours without saturating.
- What evidence would resolve it: Training the 1.5B parameter model on incremental subsets of OLMoASR-Mix to plot a compute-optimal scaling curve for the largest architecture.

### Open Question 2
- Question: To what extent can the performance gap between OLMoASR-large and Whisper-large be attributed to the lack of hyperparameter tuning rather than data composition?
- Basis in paper: [explicit] The authors state they "retain the same maximum learning rate for all scales and do not perform hyperparameter tuning," while noting the performance gap widens at the 769M and 1.5B scales.
- Why unresolved: The fixed training recipe may be suboptimal for larger model capacities, conflating data quality limitations with training dynamics issues.
- What evidence would resolve it: A sweep of learning rates and warmup schedules specifically for the large.en model to determine if the WER gap decreases without changing the dataset.

### Open Question 3
- Question: Does the aggressive filtering of "repeating lines" and specific casing patterns inadvertently remove valid data from niche domains or demographic groups?
- Basis in paper: [inferred] The curation pipeline removes up to 39.9% of data based on text heuristics like repeating lines (Section 2.2.2), validated primarily by aggregate WER improvements.
- Why unresolved: While aggregate performance improves, heuristic filters based on transcript formatting might systematically bias the dataset against specific sources (e.g., all-caps headlines) or speakers prone to repetition artifacts.
- What evidence would resolve it: A fine-grained error analysis comparing the "no quality filtering" baseline against the curated model across diverse accents, genders, and recording conditions to identify disparate performance drops.

## Limitations

- The exact composition of the OLMoASR-Pool dataset is not fully specified, making it difficult to assess training data representativeness
- The paper does not provide a direct comparison with Whisper models trained on the same curated data
- The long-form benchmark suite includes non-open datasets (BTV, TED) limiting reproducibility
- Potential biases in the data curation pipeline are not addressed, particularly regarding filtering of non-standard dialects or accents

## Confidence

- **High confidence**: Experimental results showing OLMoASR models achieving comparable WER to Whisper-medium on both short- and long-form benchmarks
- **Medium confidence**: Claim that data curation improves performance by up to 14.5% WER, though exact contribution of each filter is not fully isolated
- **Low confidence**: Assertion that OLMoASR models are "robust" to all forms of domain shift is not fully validated

## Next Checks

1. Reproduce the data curation pipeline: Reconstruct the OLMoASR-Mix dataset using the provided IDs and verify the impact of each filtering step on the final WER
2. Compare against Whisper on the same data: Train a Whisper model on the OLMoASR-Mix dataset and compare its zero-shot performance to OLMoASR models
3. Test robustness to domain shift: Evaluate OLMoASR models on a held-out dataset with significant domain shift to assess zero-shot generalization claims