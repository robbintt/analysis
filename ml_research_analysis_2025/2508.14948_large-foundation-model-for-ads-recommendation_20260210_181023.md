---
ver: rpa2
title: Large Foundation Model for Ads Recommendation
arxiv_id: '2508.14948'
source_url: https://arxiv.org/abs/2508.14948
tags:
- user
- downstream
- representations
- conference
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LFM4Ads, a large foundation model framework
  for ad recommendation that addresses limitations of existing methods by transferring
  user, item, and cross representations from upstream models. The framework extracts
  user representations (URs), item representations (IRs), and user-item cross representations
  (CRs) from a pre-trained foundation model, aggregates CRs into coarser forms using
  time-aware exponential moving average, and employs multi-granularity transfer mechanisms:
  non-linear adapters for feature-level transfer, isomorphic interaction modules for
  module-level transfer, and standalone retrieval for model-level transfer.'
---

# Large Foundation Model for Ads Recommendation

## Quick Facts
- **arXiv ID**: 2508.14948
- **Source URL**: https://arxiv.org/abs/2508.14948
- **Reference count**: 40
- **Primary result**: Achieved 2.45% GMV lift across Tencent's advertising platform through triple-representation transfer

## Executive Summary
This paper introduces LFM4Ads, a large foundation model framework for ad recommendation that addresses limitations of existing methods by transferring user, item, and cross representations from upstream models. The framework extracts user representations (URs), item representations (IRs), and user-item cross representations (CRs) from a pre-trained foundation model, aggregates CRs into coarser forms using time-aware exponential moving average, and employs multi-granularity transfer mechanisms. LFM4Ads has been deployed in Tencent's advertising platform since Q4 2024, processing tens of billions of daily samples with terabyte-scale parameters and achieving 10+ successful launches across various advertising scenarios.

## Method Summary
LFM4Ads implements a triple-representation transfer framework where representations are extracted from a pre-trained upstream model and transferred to downstream ad recommendation models. The system uses a triple-tower architecture (User, Item, Mix) with dual branches for content and ad optimization. Cross Representations are aggregated from sample-level to user/item-level using time-aware exponential moving average to address storage constraints. Transfer occurs through non-linear adapters for feature-level integration, isomorphic interaction modules for module-level transfer, and standalone retrieval for model-level transfer.

## Key Results
- Achieved 2.45% overall GMV lift across the entire Tencent advertising platform
- Demonstrated 10+ successful launches across Weixin Moments and Channels scenarios
- Showed that triple-representation transfer (UR+IR+CR) outperforms UR-only methods for cold-start items

## Why This Works (Mechanism)

### Mechanism 1: Triple-Representation Transfer (UR, IR, CR)
Transferring User (UR), Item (IR), and Cross (CR) representations outperforms transferring only User Representations, particularly for cold-start item scenarios. IRs capture multi-scenario exposure patterns critical for new items with sparse local data. CRs encode fine-grained user-item interactions (high-order correlations) that isolated embeddings miss. The system extracts these from a triple-tower architecture.

### Mechanism 2: Time-Aware Aggregation of Cross Representations
Raw sample-level Cross Representations (CRs) are too fine-grained and voluminous for direct transfer; aggregating them via time-aware exponential moving average (EMA) creates transferable coarse-grained signals. The system aggregates sample-level CR(u, i) into user-level CR(u) and item-level CR(i). The EMA updates weights based on time intervals since the last update (β(t)), adapting to activity levels.

### Mechanism 3: Non-Linear Adapter for Feature-Level Transfer
Directly using representations as features (linear fusion) fails due to the semantic gap between upstream and downstream spaces; a non-linear gating mechanism is required. Instead of linear projection, the framework uses a Hadamard product between downstream embeddings and a projected, non-linearly activated CR (EE' = EE ⊙ σ(CR · M)).

## Foundational Learning

- **Upstream-Downstream Paradigm (Transfer Learning)**: The core logic relies on decoupling a massive, slow "upstream" model from fast "downstream" models that serve real-time traffic. *Quick check*: Can you explain why we cannot simply train a 4TB model directly on the real-time serving path for pCTR?

- **Explicit vs. Implicit Feature Interaction**: The "Mix Tower" uses interaction layers (FM, CrossNet) to generate Cross Representations (CRs). Understanding the difference between explicit interaction (e.g., FM learning vector products) and implicit interaction (Deep Neural Networks) is required to select which layer's output to extract. *Quick check*: In the context of this paper, which layer (FM/CrossNet vs. DNN) was found to have the most transferable CRs?

- **Distribution Drift & Decay**: The aggregation mechanism relies on the assumption that old data is less relevant than new data. Understanding time-decay functions is essential for implementing the EMA updates for CRs. *Quick check*: How does the system behave differently when updating the CR for an "active" user vs. an "inactive" user?

## Architecture Onboarding

- **Component map**: Upstream LFM (Triple-tower with dual branches) -> Storage (Key-Value store) -> Downstream Models (pCTR/pCVR) -> Prediction

- **Critical path**: Data Ingest (80% Content + 20% Ad) -> Upstream LFM training -> Extraction (UR, IR, CR from penultimate MLP layer) -> Aggregation (Time-aware EMA) -> Serving (Downstream pulls UR/IR/CR -> applies Non-Linear Adapter -> prediction)

- **Design tradeoffs**: Dual Branch vs. Shared (dual branch necessary to decouple commercial intent from general content interest), Layer Selection (penultimate DNN layer outperformed raw embeddings or CrossNet layers), Granularity (Model-level transfer improved user experience but was revenue-neutral)

- **Failure signatures**: Semantic Gap (linear projection fails, AUC gains vanish), Negative Transfer (merging Content/Ad branches degrades performance), Latency/Storage (impractical to store sample-level CRs)

- **First 3 experiments**: 1) Dual-Branch Validation (shared vs. dual branch), 2) CR Layer Extraction (compare Embedding, CrossNet, and MLP layers), 3) Adapter vs. Linear (compare non-linear gating against simple concatenation)

## Open Questions the Paper Calls Out

- Why do cross representations (CRs) extracted from the penultimate MLP layer exhibit superior transferability compared to explicit interaction layers (e.g., Cross-Net) or the final output layer? The authors identify the penultimate layer as optimal empirically without providing theoretical justification.

- Can the Standalone Retrieval (model-level transfer) approach be optimized to deliver positive GMV lift rather than just neutral revenue with improved user experience? Current implementation improves engagement metrics but results in neutral GMV.

- Does the Time-aware Exponential Moving Average (EMA) suffice for aggregating Cross Representations, or would learned attention-based aggregation provide superior fidelity? EMA addresses storage constraints but may not capture varying importance of historical interactions as effectively as a learned mechanism.

## Limitations

- Massive computational requirements (4TB model with terabyte-scale parameters) create fundamental bottlenecks in real-time serving latency
- Effectiveness depends heavily on maintaining accurate, up-to-date aggregated representations in storage with potential staleness issues
- Dual-branch architecture increases model complexity and requires careful tuning to prevent negative transfer

## Confidence

**High Confidence**: Core claim that triple-representation transfer outperforms UR-only methods is well-supported by ablation studies and 2.45% GMV lift.

**Medium Confidence**: Time-aware aggregation mechanism's superiority is supported by ablation, but specific decay function parameters and optimality remain unclear.

**Low Confidence**: Assertion about penultimate MLP layer being optimal could not be fully verified as ablation comparing different layer selections is not explicitly detailed.

## Next Checks

1. Systematically extract CRs from multiple layers (Embedding, CrossNet, various MLP depths) in the Mix Tower and measure AUC impact on downstream tasks to confirm penultimate layer's superiority.

2. Design experiment specifically targeting new items with zero local impressions to measure whether IR transfer provides measurable lift compared to UR-only approaches in cold-start scenarios.

3. Measure end-to-end serving latency impact of retrieving aggregated CRs from storage versus computing them on-the-fly, and determine maximum acceptable staleness threshold for different user segments.