---
ver: rpa2
title: Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection
  in Cyber-Physical Systems
arxiv_id: '2508.09504'
source_url: https://arxiv.org/abs/2508.09504
tags:
- causal
- detection
- anomaly
- graph
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CGAD is a causal graph-based anomaly detection framework for cyber-physical
  systems, specifically water treatment networks. It employs a two-phase approach:
  causal profiling to learn Dynamic Bayesian Networks (DBNs) representing normal and
  attack states, and anomaly scoring via structural divergence using the Structural
  Hamming Distance (SHD) between causal graphs.'
---

# Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems

## Quick Facts
- arXiv ID: 2508.09504
- Source URL: https://arxiv.org/abs/2508.09504
- Reference count: 40
- Primary result: Achieves F1 scores of 0.7807 (SWaT), 0.8913 (WADI), 0.8297 (TE), and 0.8626 (SMD) with ROC-AUC >0.85

## Executive Summary
CGAD is a causal graph-based anomaly detection framework for cyber-physical systems, specifically water treatment networks. It employs a two-phase approach: causal profiling to learn Dynamic Bayesian Networks (DBNs) representing normal and attack states, and anomaly scoring via structural divergence using the Structural Hamming Distance (SHD) between causal graphs. The method demonstrates superior performance compared to baselines, achieving F1 scores of 0.7807 (SWaT), 0.8913 (WADI), 0.8297 (TE), and 0.8626 (SMD), with ROC-AUC scores exceeding 0.85 across all datasets. CGAD is robust to class imbalance, distribution shifts, and delayed attack effects, offering explainable and generalizable detection in complex time-series environments.

## Method Summary
The framework consists of two phases: (1) Causal Profiling - learn two DBNs using DYNOTEARS from CausalNex (G_Normal from normal data, G_Attack from attack data) with dataset-specific time-lags [4, 3, 4, 1 for SWaT/WADI/TE/SMD]; (2) Anomaly Scoring - for each test segment, infer graph G_T via DYNOTEARS, compute Structural Hamming Distance to both reference graphs, classify as Attack if SHD(G_T, G_Attack) < SHD(G_T, G_Normal). L1 regularization applied; Gaussian noise added to attack data.

## Key Results
- F1 scores: 0.7807 (SWaT), 0.8913 (WADI), 0.8297 (TE), 0.8626 (SMD)
- ROC-AUC scores exceed 0.85 across all datasets
- Robust to class imbalance (ratios up to 25:1 in SMD)
- Outperforms baselines in structural divergence-based detection

## Why This Works (Mechanism)

### Mechanism 1: Temporal Causal Structure Learning via Continuous Optimization
Modeling CPS as Dynamic Bayesian Networks captures time-lagged causal physics that static correlation graphs miss. DYNOTEARS enforces acyclicity while minimizing loss with L1 regularization to induce sparsity, yielding distinct DAGs for Normal and Attack states.

### Mechanism 2: Structural Divergence via Topological Comparison
Anomalies are detected by measuring the Structural Hamming Distance between test segment graphs and reference profiles. If SHD to Attack reference is smaller than to Normal reference, an attack is flagged.

### Mechanism 3: Supervised Causal Profiling for Generalization
Creating explicit graph profiles for known states allows the model to generalize to unseen attacks better than one-class anomaly detection, provided new attacks share structural hallmarks with known attack patterns.

## Foundational Learning

- **Dynamic Bayesian Networks (DBNs)**: Why needed - Standard BNs are static and cannot represent time-lagged flow through a plant. Quick check - Can you explain why a static DAG would fail to capture "delayed attack effects"?

- **Structural Hamming Distance (SHD)**: Why needed - You cannot simply subtract adjacency matrices because edge weights might scale differently. Quick check - If two graphs have identical edges but different edge weights, would SHD capture this difference?

- **Sparsity Regularization (L1 Norm)**: Why needed - Without L1 regularization, the graph would be a dense "hairball," making structural comparison impossible and computationally expensive. Quick check - What happens to the learned causal graph if λ is set too low?

## Architecture Onboarding

- **Component map**: Segmenter -> Structure Learner (DYNOTEARS) -> Reference Store -> Comparator -> Thresholding Agent
- **Critical path**: The time-lag parameter in DYNOTEARS - if shorter than system response time, spurious edges are learned; if too long, overfitting occurs
- **Design tradeoffs**: Interpretability vs. Noise Sensitivity (DAGs are explainable but sensitive to noise); Segment Size (smaller allows faster detection but insufficient for reliable inference)
- **Failure signatures**: Random Graph Drift (SHD to both references increases, causing erratic classification); High Latency (slower than simple baselines due to optimization per segment)
- **First 3 experiments**: 1) Hyperparameter Sensitivity (Time-Lag) - sweep lag parameter on SWaT/WADI; 2) Ablation on Structural Metrics - replace SHD with Jaccard Similarity; 3) Noise Injection Test - add Gaussian noise to attack training data and measure F1 drop

## Open Questions the Paper Calls Out
- How can CGAD be extended to support continuous or incremental causal learning that updates reference graphs in real-time to handle concept drift without significant downtime?
- Can the computational cost of segment-wise causal discovery during inference be reduced to support low-latency anomaly detection in large-scale systems?
- Does learning a single aggregate "Attack" causal graph limit the detection of novel, structurally distinct zero-day attacks compared to multi-class or ensemble causal models?

## Limitations
- Performance heavily dependent on dataset-specific hyperparameter tuning (time-lag, λ values)
- Assumes linear, sparse, acyclic relationships with Gaussian noise - may not hold for all CPS
- Computational cost of running DYNOTEARS for every segment may hinder real-time deployment

## Confidence
- **High Confidence**: Two-phase framework design and SHD metric usage
- **Medium Confidence**: Performance claims are dataset-specific and may not generalize without tuning
- **Low Confidence**: Exact preprocessing steps, noise regularization parameters, and edge cases are underspecified

## Next Checks
1. Hyperparameter Sensitivity Test - sweep time-lag and λ across all datasets to validate optimality
2. Ablation on Structural Metrics - replace SHD with Jaccard Similarity to confirm superiority
3. Noise Injection Robustness - systematically vary Gaussian noise levels in attack training data to measure F1 degradation