---
ver: rpa2
title: Rethinking Deep Alignment Through The Lens Of Incomplete Learning
arxiv_id: '2511.12155'
source_url: https://arxiv.org/abs/2511.12155
tags:
- safety
- alignment
- arxiv
- tokens
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that safety alignment in large language models
  exhibits position-dependent gradient weakening, creating incomplete learning in
  later response regions. The authors introduce base-favored tokens as computational
  indicators of undertrained distributional alignment and develop a targeted completion
  method that applies adaptive penalties and hybrid teacher distillation to address
  these regions.
---

# Rethinking Deep Alignment Through The Lens Of Incomplete Learning

## Quick Facts
- arXiv ID: 2511.12155
- Source URL: https://arxiv.org/abs/2511.12155
- Reference count: 39
- Primary result: Safety alignment exhibits position-dependent gradient weakening, causing incomplete learning in later response regions

## Executive Summary
This paper identifies a fundamental limitation in current safety alignment methods where gradient decay during autoregressive training causes incomplete distributional transformation, particularly affecting later token positions. The authors introduce base-favored tokens as computational indicators of undertrained alignment regions and develop a targeted completion method that applies adaptive penalties and hybrid teacher distillation. Experiments demonstrate dramatic improvements in adversarial robustness while preserving general capabilities, with attack success rates reduced by 48-98% across multiple model families.

## Method Summary
The approach detects base-favored tokens where base models assign higher probability than aligned models, indicating incomplete safety learning. These tokens receive L2 penalties through an adaptive completion loss that scales based on positional vulnerability. A hybrid teacher distillation framework combines aligned and base logits (λ=1.2) to preserve utility while strengthening safety. The method operates on mixed batches of harmful and utility data, applying targeted completion only to harmful contexts while maintaining general capabilities through the hybrid teacher.

## Key Results
- Adversarial attack success rates reduced by 48-98% across Llama and Qwen model families
- General capability preservation on MMLU, ARC-C, and GSM8K benchmarks
- Superior safety recovery after fine-tuning degradation compared to baseline methods
- Enhanced deliberative reasoning under adversarial conditions

## Why This Works (Mechanism)

### Mechanism 1: Gradient Concentration in Autoregressive Training
The autoregressive loss structure creates asymmetric gradient flow where early tokens receive stronger training signals through cumulative context contributions, while later tokens experience signal decay. This results in incomplete distributional transformation at later positions.

### Mechanism 2: Base-Favored Tokens as Functional Vulnerability Indicators
Tokens where π_base(v|x, y<t) > π_aligned(v|x, y<t) indicate incomplete safety learning and serve as computational targets for adversarial exploitation. These tokens show systematically higher counts in harmful contexts.

### Mechanism 3: Targeted L2 Completion as Distributional Correction
Adaptive L2 penalties on base-favored tokens complete the distributional transformation that standard alignment began but could not finish. Hybrid teacher distillation extrapolates aligned preferences while retaining base knowledge for utility preservation.

## Foundational Learning

- **Autoregressive Language Modeling Objective**: Understanding sequential factorization P(y|x) = ∏ P(y_t|x, y<t) is essential for grasping asymmetric gradient contributions across token positions. *Quick check: Why does the loss term at position t=5 contribute gradients to parameters affecting positions t=1,2,3,4, but the loss at t=1 does not directly receive gradients from t=5's loss term?*

- **Gradient Flow Through Attention Mechanisms**: Essential for understanding how "Context Dependency Asymmetry" causes early positions to receive cumulative gradient contributions. *Quick check: If early token hidden states h_i influence later predictions through attention, how does this create stronger parameter updates for early positions versus late positions?*

- **Knowledge Distillation with Temperature Scaling**: Required for understanding the hybrid teacher framework with τ=2.0. *Quick check: What effect does temperature τ > 1 have on the soft target distribution, and why might this benefit safety distillation?*

## Architecture Onboarding

- Component map: Training Data (harmful contexts + utility data) → Parallel Forward Passes: Base Model + Aligned Model → Base-Favored Token Detection: TopK(logits_base - logits_aligned, k=100) → Adaptive Penalty Scaling: α_adaptive = α_base · (1 + γ · risk_level) → Completion Loss: L2 penalty on detected tokens → Hybrid Teacher: λ·logits_aligned + (1-λ)·logits_base (λ=1.2) → Combined Objective: L_KL + α_adaptive · L_completion → Student Model Update (15 epochs, lr=1e-5)

- Critical path: Detection fidelity must correctly capture π_base > π_aligned; penalty calibration must balance suppression strength against utility preservation; teacher extrapolation must amplify safety signal without degrading downstream task performance.

- Design tradeoffs: Top-k threshold (k=100) balances token coverage against false positives; mixed batch ratio (α=0.4 harmful) balances safety signal against utility preservation; two-stage recovery protocol addresses fine-tuning artifacts at doubled training time.

- Failure signatures: Utility collapse (MMLU/ARC scores drop >3%) indicates over-penalization; persistent vulnerability (unchanged attack success rate) indicates detection failure; late-position KL decay persistence indicates completion loss not reaching later positions; Stage 1 misalignment in recovery indicates distributional realignment insufficient.

- First 3 experiments:
  1. Implement contrastive decoding (Eq. 7) with α=1.0 on Llama-3.1-8B-Instruct using 100 AdvBench prompts; expect prefill attack success drops from ~47% to <1%.
  2. Measure KL(π_aligned || π_base) across positions 1-20 before and after completion training on Llama-2-7B-Chat; expect pre-training shows decay (1.8→0.5) while post-training shows sustained KL (10.0→6.0).
  3. Train with k∈{50, 100, 200} on Qwen-2.5-7B-Instruct, evaluate on AdvBench; expect k=100 achieves ~44% ASR.

## Open Questions the Paper Calls Out

- **Scalability to larger models**: Does the framework remain effective when scaled to significantly larger model sizes (>70B parameters)? The conclusion explicitly states "Future work should scale these methods to larger models."

- **Impact on other safety dimensions**: How does completing distributional alignment affect other safety dimensions like deception or sycophancy? The conclusion directs future work to "examine their relationship to other safety dimensions."

- **Base model access requirements**: Can the method adapt to work without original base model weights? The methodology explicitly requires calculating logits_base for both teacher construction and token detection.

- **Source of enhanced reasoning**: Is the observed "enhanced deliberative reasoning" intrinsic to deep alignment or an artifact of using GSM8K (mathematical reasoning data) for utility preservation?

## Limitations

- Gradient concentration mechanism lacks direct empirical validation through gradient magnitude measurements
- Base-favored token detection framework lacks independent verification across diverse model families
- Generalizability to non-adversarial safety failures remains unclear as all evaluations focus on targeted attacks

## Confidence

- **High confidence**: Empirical attack success rate reductions (48-98%) and utility preservation metrics are directly measurable and reproducible
- **Medium confidence**: Position-dependent gradient weakening mechanism is theoretically sound but requires direct gradient measurement validation
- **Low confidence**: Claims about incomplete learning being primarily a positional gradient decay problem may oversimplify other contributing factors

## Next Checks

1. Measure and compare ∂logP(y_t|x, y<t)/∂θ magnitudes across early and late token positions during standard safety training versus completion training to validate the gradient concentration hypothesis

2. Apply the completion method to models from different families (e.g., Mistral, Gemma) and training protocols (SFT vs RLHF) to assess generalization beyond Llama and Qwen families

3. Test the method's effectiveness on naturally occurring harmful outputs and general safety benchmarks rather than only targeted adversarial attacks to verify broader safety improvement