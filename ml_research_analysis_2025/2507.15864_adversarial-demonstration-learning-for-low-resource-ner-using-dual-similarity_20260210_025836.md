---
ver: rpa2
title: Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity
arxiv_id: '2507.15864'
source_url: https://arxiv.org/abs/2507.15864
tags:
- demonstration
- similarity
- input
- examples
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of named entity recognition
  (NER) in low-resource scenarios, where limited labeled data makes training effective
  models difficult. The authors identify two key issues in existing demonstration-based
  learning approaches: first, reliance on semantic similarity alone for selecting
  demonstration examples, which overlooks feature similarity; second, the tendency
  of models to ignore demonstration examples during training.'
---

# Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity

## Quick Facts
- arXiv ID: 2507.15864
- Source URL: https://arxiv.org/abs/2507.15864
- Authors: Guowen Yuan; Tien-Hsuan Wu; Lianghao Xia; Ben Kao
- Reference count: 9
- Primary result: A DELL achieves 41.92% F1 on German legal dataset under 5-shot settings, surpassing DDSS at 35.73%

## Executive Summary
This paper addresses the challenge of named entity recognition (NER) in low-resource scenarios, where limited labeled data makes training effective models difficult. The authors identify two key issues in existing demonstration-based learning approaches: first, reliance on semantic similarity alone for selecting demonstration examples, which overlooks feature similarity; second, the tendency of models to ignore demonstration examples during training. To address these, the paper proposes Adversarial Demonstration Learning (ADL), which combines dual similarity—integrating both semantic and feature similarities—and adversarial training to force the model to pay attention to demonstrations. Experiments on three datasets (CoNLL03, German legal, and MIT restaurant reviews) under few-shot settings (5, 10, and 20 examples per entity type) show that the proposed method, A DELL , consistently outperforms existing approaches. For instance, on the German legal dataset, A DELL achieves a 41.92% F1 score under 5-shot settings, surpassing dynamic demonstration learning (DDSS) at 35.73%. Ablation studies confirm that both dual similarity and adversarial training contribute significantly to the improvement, demonstrating their complementary effectiveness.

## Method Summary
The proposed Adversarial Demonstration Learning (ADL) method consists of three main components: dual similarity demonstration selection, adversarial training, and ensemble voting. For demonstration selection, the method computes a weighted combination of semantic similarity (using SBERT embeddings) and feature similarity (predicted by a cross-encoder estimating Jaccard similarity between feature sets). The feature similarity predictor is trained on labeled pairs from the training set. Demonstrations are constructed by filtering candidates per entity type based on similarity thresholds and randomly sampling one per feature. The base model is StructShot with BERT encoder. During training, the method applies adversarial perturbations by randomly swapping entity labels or demonstration order, with a combined loss that balances main task, example permutation, and label permutation objectives. At inference, k demonstrations are generated per test instance and predictions are aggregated via majority voting.

## Key Results
- A DELL achieves 41.92% F1 on German legal dataset under 5-shot settings, surpassing DDSS at 35.73%
- On CoNLL03, A DELL reaches 64.9% F1 under 10-shot settings, outperforming DDSS at 44.2%
- Ablation studies show both dual similarity (γ=0.4) and adversarial training contribute significantly to performance gains
- Standard deviations of 5-10% F1 indicate substantial variance across runs

## Why This Works (Mechanism)

### Mechanism 1: Dual Similarity Improves Demonstration Selection
- Claim: Combining semantic similarity with feature similarity yields better demonstration examples than semantic similarity alone.
- Mechanism: The dual similarity function S(di, dj) = γ·Sfe(di, dj) + (1−γ)·Sse(di, dj) ranks candidate examples. Feature similarity Sfe is predicted by a cross-encoder trained to estimate Jaccard similarity of feature sets; semantic similarity Sse uses SBERT embeddings. Examples are selected per entity type, filtered by similarity threshold, then randomly sampled.
- Core assumption: Feature similarity and semantic similarity are complementary and largely uncorrelated—meaning semantic similarity alone fails to capture whether examples share relevant entity types.
- Evidence anchors:
  - [abstract]: "We show that feature similarity can provide significant performance improvement."
  - [section 4.4, Table 2]: Pearson correlation between Sse and feature Jaccard similarity is negative for GL dataset (-0.5351) and near-zero for CoNLL (-0.0519), supporting the claim that semantic similarity does not predict feature overlap.
  - [corpus]: Related papers on demonstration retrieval (e.g., "LLMs are Better Than You Think") also critique semantic-only selection, but direct empirical comparison to dual similarity is absent in the provided corpus.
- Break condition: If feature similarity predictor accuracy drops significantly (e.g., ranking accuracy <55%), the mechanism may degrade to random selection within feature categories.

### Mechanism 2: Adversarial Training Forces Demonstration Attention
- Claim: Models trained with standard demonstration learning under-attend to demonstration examples; adversarial perturbation of labels during training forces the model to rely on demonstrations.
- Mechanism: During training, label permutation swaps entity labels in demonstrations (e.g., [PER]↔[LOC]); example permutation shuffles demonstration order. The model must then reference the permuted demonstration to predict correctly under the perturbed labeling scheme. Loss combines main task (α), example permutation ((1−α)(1−β)), and label permutation ((1−α)β).
- Core assumption: The pre-trained language model's embeddings are dominated by prior knowledge, causing "demonstration blindness" unless explicitly countered.
- Evidence anchors:
  - [section 3.3]: "We observe that in traditional demonstration learning, the NER model trained may not pay much attention to the demonstrative examples."
  - [section 4.5, Figure 6]: Without ADL, swapping [PER]/[LOC] labels in demonstrations changes model output scores minimally (~0.37→0.35 for [PER] tokens). With ADL, the same swap causes large score shifts, indicating demonstration influence.
  - [corpus]: No direct corpus evidence supports or refutes this specific adversarial mechanism; related ICL papers do not address demonstration attentiveness.
- Break condition: If α (main task weight) is set too low, the model may overfit to arbitrary label permutations and fail to generalize to correct labels at inference.

### Mechanism 3: Ensemble Voting Reduces Variance
- Claim: Constructing k demonstrated inputs per test instance and aggregating via majority voting improves robustness.
- Mechanism: DI module generates k demonstrations with random sampling after filtering; each produces a prediction; final annotation is determined by majority vote across the k predictions.
- Core assumption: Demonstration quality varies due to the random sampling step; averaging reduces noise from occasional poor demonstrations.
- Evidence anchors:
  - [section 3.2]: "In testing/applying the model to an input di, we apply DI to di k times to form k demonstrated inputs. The predicted annotation of di is obtained via majority voting."
  - [corpus]: Ensemble strategies for ICL are mentioned in related work (e.g., "EL4NER: Ensemble Learning for NER"), supporting the general principle but not this specific implementation.
- Break condition: If k is too small (e.g., k=1), variance increases; if demonstrations are highly correlated, ensemble benefit diminishes.

## Foundational Learning

- **Concept: Named Entity Recognition (NER) with BIO/Markup Tagging**
  - Why needed here: The paper frames NER as assigning feature labels to token sequences. Understanding span-based labeling is prerequisite to grasping demonstration construction (examples show markup format like "Li Jie is [PER]").
  - Quick check question: Can you explain why NER is structured as sequence labeling rather than single-token classification?

- **Concept: In-Context Learning via Demonstrations**
  - Why needed here: The entire method builds on demonstration-based learning—concatenating labeled examples with input text to guide model predictions.
  - Quick check question: How does demonstration-based learning differ from standard fine-tuning in terms of how task knowledge is conveyed?

- **Concept: Adversarial Training Objective Design**
  - Why needed here: ADL introduces a multi-component loss with permutation-based adversarial terms. Understanding how to balance main task vs adversarial objectives via α and β is critical for implementation.
  - Quick check question: What happens to model behavior if the adversarial loss term dominates the main task loss?

## Architecture Onboarding

- **Component map:**
  - Input → Dual Similarity Scoring (Sfe + Sse) → Per-Feature Filtering + Random Sampling → Template Rendering → Demonstrated Input → StructShot (BERT encoder + classifier) → Main Task Loss + Adversarial Loss (example/label permutation) → Predictions → Ensemble Voting (k times) → Final Annotation

- **Critical path:**
  1. Train feature similarity predictor (cross-encoder on labeled pairs)
  2. Construct demo pool C from training set
  3. For each training instance: DI → demonstrated input → train with ADL loss
  4. For inference: repeat DI k times → aggregate

- **Design tradeoffs:**
  - **γ (feature vs semantic weight)**: Higher γ prioritizes feature similarity; paper uses grid search but optimal value may vary by dataset complexity.
  - **α (main task vs adversarial weight)**: Too high → model ignores demonstrations; too low → model overfits to arbitrary permutations.
  - **k (ensemble size at inference)**: Larger k reduces variance but increases latency.
  - **Random sampling vs top-k selection**: Paper uses random sampling after filtering to introduce diversity; deterministic top-k may reduce exploration.

- **Failure signatures:**
  - **Low feature predictor accuracy** (ranking accuracy <60%): Dual similarity degrades; check training data for feature diversity.
  - **No score shift under label permutation test** (Figure 6 pattern): ADL failed to induce attention; recheck α/β tuning.
  - **High variance across runs** (Table 1 std >5% F1): Likely due to random sampling; increase k or demo pool size.

- **First 3 experiments:**
  1. **Ablate feature similarity**: Set γ=0 (semantic-only) on validation set; compare F1 to full dual similarity. Expected drop of 2-5% F1 based on Table 1 (ADELL¬DS vs ADELL).
  2. **Label permutation sensitivity test**: Train with ADL, then at inference swap labels in demonstrations. Measure score shift magnitude. Should mirror Figure 6(b) pattern.
  3. **Feature predictor evaluation**: Compute ranking accuracy and Pearson correlation on held-out pairs. Target: ranking accuracy >70% for CoNLL, >80% for GL (per Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is ADELL's performance to the hyperparameters $\alpha$, $\beta$, and $\gamma$, and do optimal values transfer across different domains?
- Basis in paper: [inferred] The authors state in Sections 3.2 and 3.3 that the weighting factor $\gamma$ and the loss weights $\alpha$ and $\beta$ are "determined using grid search" or "adjusted," but they do not provide an analysis of the model's sensitivity to these parameters or if a single setting is robust across all three datasets (CoNLL, GL, MIT).
- Why unresolved: It is unclear if the improvements are dependent on extensive parameter tuning for each specific dataset, which would increase the operational cost in true low-resource environments.
- What evidence would resolve it: A sensitivity analysis plotting F1 scores against varying values of $\alpha$, $\beta$, and $\gamma$, and a cross-domain test using parameters tuned on a source domain applied directly to a target domain.

### Open Question 2
- Question: Can the proposed method generalize to languages with non-Latin scripts or agglutinative morphologies where pre-trained embedding quality is lower?
- Basis in paper: [inferred] The experiments are conducted solely on English (CoNLL, MIT) and German (GL), both Indo-European languages with high-quality BERT embeddings; the paper does not discuss the reliance of the "semantic similarity" component on the quality of these underlying pre-trained representations.
- Why unresolved: The dual similarity mechanism relies on a pre-trained BERT encoder for both semantic and feature similarity; if the underlying language model is weak (as is often the case in low-resource languages), the dual similarity metric may fail to select relevant demonstrations.
- What evidence would resolve it: Experimental results applying ADELL to low-resource NER datasets in languages such as Chinese, Arabic, or Finnish, comparing the performance against the English/German baselines.

### Open Question 3
- Question: Is the feature similarity predictor $S_{fe}$ effective in "zero-shot" scenarios where the training set contains no examples of the target entity type?
- Basis in paper: [inferred] Section 3.2 describes training the feature similarity predictor $M_c$ to estimate Jaccard similarity, but the experiments are limited to few-shot settings ($k \in \{5, 10, 20\}$); the paper does not address if the predictor can infer feature relevance for entirely unseen entity types during training.
- Why unresolved: If the feature predictor relies on learning from positive examples of specific features, it may fail to identify relevant demonstrations for novel entity types not present in the training data, limiting the method's applicability to strictly zero-shot learning.
- What evidence would resolve it: An ablation study or experiment setup where the model is evaluated on entity types that were entirely excluded from the training set, measuring the accuracy of the feature similarity predictor and the final NER performance.

## Limitations
- High variance across runs (5-10% F1 standard deviation) suggests results may be sensitive to random initialization and sampling
- Limited evaluation scope - only compares against StructShot baseline without including modern in-context learning approaches or other recent NER methods
- Feature similarity predictor performance varies dramatically across datasets (GL: 0.8135 correlation vs CoNLL: 0.3944), raising questions about reliability in low-resource settings

## Confidence
- **Dual similarity improves demonstration selection**: Medium - supported by negative/zero correlation results but lacks direct comparison to other retrieval methods
- **Adversarial training forces demonstration attention**: Medium - visualization evidence is compelling but mechanism explanation is limited
- **Overall method outperforms baselines**: Medium - controlled comparisons exist but lack broader benchmark coverage

## Next Checks
1. **Feature similarity predictor validation**: Compute ranking accuracy and Pearson correlation on held-out pairs for each dataset. Target: >0.75 for GL, >0.39 for CoNLL (per Table 2). Low accuracy indicates the dual similarity mechanism may be ineffective.

2. **Label permutation sensitivity test**: Train with ADL, then at inference swap labels in demonstrations. Measure score shift magnitude. Should mirror Figure 6(b) pattern showing large score shifts under ADL vs minimal shifts without it. No shift indicates adversarial training failed to induce attention.

3. **Cross-dataset generalization**: Apply the trained feature similarity predictor from CoNLL to GL and vice versa. Evaluate correlation drop. Significant performance degradation (>30% correlation drop) would suggest the predictor is overfit to domain-specific feature patterns.