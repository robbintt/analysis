---
ver: rpa2
title: 'M-IFEval: Multilingual Instruction-Following Evaluation'
arxiv_id: '2502.04688'
source_url: https://arxiv.org/abs/2502.04688
tags:
- language
- instruction
- instructions
- llms
- detectable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Instruction Following Evaluation (IFEval) benchmark only evaluates
  LLMs in English, limiting assessment of multilingual models. We propose the Multilingual
  Instruction Following Evaluation (M-IFEval) benchmark, expanding evaluation to French,
  Japanese, and Spanish with both general and language-specific instructions.
---

# M-IFEval: Multilingual Instruction-Following Evaluation

## Quick Facts
- arXiv ID: 2502.04688
- Source URL: https://arxiv.org/abs/2502.04688
- Reference count: 26
- Key outcome: M-IFEval benchmark reveals that while GPT4o leads on English IFEval, other models like o1 and Sonnet achieve higher scores on multilingual tasks across French, Japanese, and Spanish.

## Executive Summary
M-IFEval extends the English-only IFEval benchmark to evaluate instruction-following capabilities in French, Japanese, and Spanish. The benchmark includes 541 prompts with both general instructions (translated from IFEval) and language-specific constraints involving script restrictions, special character frequency, and cultural formatting. Evaluation uses objective, programmatic verification functions rather than subjective AI/human judgment. The study finds significant performance gaps on language-specific tasks, with models struggling particularly with character-level and script-based instructions (e.g., 0.0% success on Spanish ñ frequency control).

## Method Summary
The benchmark evaluates 8 state-of-the-art LLMs (GPT4o, o1, Claude variants, Qwen 2.5) on 115-235 prompts per language with 1-3 instructions each. Responses are generated using greedy decoding where possible. Evaluation uses deterministic verification functions for each instruction type (string matching, regex, script validation). Strict scoring requires all instructions in a prompt to pass. The methodology extends IFEval's objective approach while adding language-specific constraints like katakana avoidance in Japanese and accent control in French.

## Key Results
- GPT4o performs best on English IFEval but not on multilingual tasks
- Performance gap between top and bottom models is wider in M-IFEval (13.9-15.1 percentage points) than English (11.3 percentage points)
- LLMs achieve near-zero scores on language-specific tasks involving special character frequency control (0.0% for Spanish ñ frequency)
- Models struggle particularly with script-based instructions (14.3% for Japanese katakana avoidance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level language models struggle with character- and script-level instructions in non-English languages due to abstraction granularity in tokenization.
- Mechanism: Modern LLMs operate on subword tokens rather than individual characters. When instructions require precise character manipulation (e.g., "use 'ñ' exactly 5 times" or "avoid katakana"), the model must reason about token components it doesn't directly observe during training. This abstraction layer works adequately for English but breaks down for languages with complex script systems.
- Core assumption: The failure pattern on character-level instructions stems from architectural constraints of tokenization, not merely insufficient multilingual training data.
- Evidence anchors:
  - [abstract] "The models struggled particularly with script-based instructions such as restricting katakana usage in Japanese or controlling special character frequency."
  - [section 4] "The average scores for these three instruction types across all models was 60.2%, 14.3%, and 0.0%, respectively"
  - [section 5] "Experiments using a byte-level tokenizer (Xue et al., 2022) could possibly answer the question of why script or character based instructions are so hard to follow for modern token-level LLMs."

### Mechanism 2
- Claim: Cross-lingual instruction-following capability is not uniform across models; leading English performers do not guarantee leading multilingual performance.
- Mechanism: Different models receive different proportions of pre-training and fine-tuning data per language. Models may develop stronger representations for languages with more training exposure, enabling better instruction adherence. This creates a model-selection dependency: the optimal model varies by target language.
- Core assumption: Training data composition is the primary driver of cross-lingual performance differences, rather than architectural inductive biases.
- Evidence anchors:
  - [abstract] "While GPT4o performed best on English instructions, other models like o1 and Sonnet achieved higher scores on multilingual tasks."
  - [section 4] "o1 achieves a score on the Japanese benchmark of 84.8 while Sonnet achieves a score of 81.2" on shared instructions, but Sonnet outperforms on Japanese-specific instructions.
  - [section 5] "This could indicate that o1 has been trained on more Spanish and French data, or linguistically similar languages that confer cross lingual generalisation, while Sonnet may have been trained on more Japanese data."

### Mechanism 3
- Claim: Objective, programmatic evaluation functions enable reproducible instruction-following assessment without judge-model bias.
- Mechanism: M-IFEval extends IFEval's approach by implementing language-specific verification functions (string matching, regex-based checks, script validation) that deterministically evaluate compliance. This eliminates self-enhancement bias observed when using LLMs-as-judges.
- Core assumption: Programmatically verifiable instructions are representative of real-world instruction-following requirements; the subset of instructions amenable to objective checking captures meaningful capability variance.
- Evidence anchors:
  - [abstract] "IFEval... does this using objective criteria, offering a measure of LLM performance without subjective AI or human judgement."
  - [section 2] "However, their reliance on subjective AI scoring raises self-enhancement bias concerns, making them less suitable for model evaluation."
  - [section 3] "Each language lead then developed a function for each instruction that evaluates whether a response did or did not correctly follow the given instruction."

## Foundational Learning

- Concept: **Subword tokenization and its information bottlenecks**
  - Why needed here: Understanding why models fail on character-level instructions requires grasping how BPE/SentencePiece tokenizers compress text into variable-length tokens, obscuring character boundaries.
  - Quick check question: Given the token ["іnguіstіc"], can you determine how many "i" characters it contains without decomposing it? What does this imply for an LLM asked to count letter occurrences?

- Concept: **Cross-lingual transfer and positive transfer from related languages**
  - Why needed here: The paper hypothesizes that o1's strong French/Spanish performance may derive from training on linguistically similar languages; understanding transfer helps predict which language pairs will show spillover benefits.
  - Quick check question: If a model is heavily trained on Spanish, would you expect stronger instruction-following in Italian or in Japanese? Why?

- Concept: **Strict vs. loose evaluation metrics for instruction following**
  - Why needed here: The paper reports strict scores (all instructions must be satisfied) and loose scores (appendix); understanding this distinction is critical for interpreting benchmark results and setting deployment thresholds.
  - Quick check question: A model correctly follows 4 of 5 instructions in a prompt. Under strict evaluation, what is its score? Under loose evaluation averaging, what contribution does this make?

## Architecture Onboarding

- Component map: Prompt generation module -> Instruction taxonomy -> Verification functions -> Scoring layer
- Critical path:
  1. Extend instruction set → define language-specific constraints → implement verification functions
  2. Generate model responses with greedy decoding (temperature=0)
  3. Run verification → aggregate scores → analyze failure patterns by instruction group

- Design tradeoffs:
  - **Breadth vs. depth**: Only 3 languages covered; extending to low-resource languages would reveal larger performance gaps but requires native-speaker expertise.
  - **Objectivity vs. coverage**: Excludes instructions requiring semantic judgment (translation quality, fact-checking); trades realism for reproducibility.
  - **Prompt quantity**: Uneven prompt counts across languages (115 ES, 172 JA, 235 FR) may affect score reliability.

- Failure signatures:
  - **Character-frequency failures**: Models ignore or miscount target characters (ñ: 0.0% average success; katakana avoidance: 14.3%).
  - **Script-mixing failures**: Japanese models default to canonical orthography (using katakana for loanwords) despite explicit "no katakana" instructions.
  - **Accent-omission failures**: French models sometimes apply accents automatically even when instructed not to.

- First 3 experiments:
  1. **Tokenizer ablation**: Compare instruction-following accuracy on character-level tasks between the same model with BPE vs. byte-level tokenization (if feasible to reinitialize) to test Mechanism 1 directly.
  2. **Language-pair transfer matrix**: Evaluate all models on all three languages with both shared and language-specific instructions; compute transfer coefficients to quantify cross-lingual spillover.
  3. **Instruction difficulty calibration**: Have native speakers rate the difficulty of each language-specific instruction; correlate rated difficulty with model failure rates to distinguish model limitations from instruction inherent complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can byte-level tokenizers improve LLM performance on character-level and script-based constraints compared to standard sub-word tokenization?
- Basis in paper: [explicit] The authors state in Section 5, "Experiments using a byte-level tokenizer... could possibly answer the question of why script or character based instructions are so hard to follow for modern token-level LLMs."
- Why unresolved: Current sub-word tokenizers may obscure character-level information, making it difficult for models to count or restrict specific characters (e.g., "ñ" or katakana), but this hypothesis has not been empirically tested in this context.
- What evidence would resolve it: A comparative study evaluating the same model architecture on M-IFEval using both byte-level and standard tokenizers.

### Open Question 2
- Question: To what extent do specific mixtures of multilingual pre-training and fine-tuning data influence relative instruction-following performance across different languages?
- Basis in paper: [explicit] Section 5 proposes that future work should involve "experiments involving different mixes of multilingual pre-training data and fine-tuning data" to explain performance variances.
- Why unresolved: It is unclear if the superior performance of certain models (e.g., Sonnet in Japanese) is due to specific training data mixtures or architectural differences.
- What evidence would resolve it: Ablation studies on models trained with controlled variations in language data ratios, evaluated on the M-IFEval benchmark.

### Open Question 3
- Question: Is the performance gap between English and non-English instruction following significantly larger for low-resource languages than for the high-resource languages tested?
- Basis in paper: [explicit] Section 7 notes that the current evaluation was limited to relatively high-resource languages and suggests, "we may observe an even greater gap for low resource languages."
- Why unresolved: The current benchmark only covers French, Japanese, and Spanish, leaving the difficulty of script and character constraints in low-resource settings unknown.
- What evidence would resolve it: Extending the benchmark to include low-resource languages (e.g., Hausa Ajami script) and evaluating current state-of-the-art models.

## Limitations

- The evaluation covers only three high-resource languages (French, Japanese, Spanish), limiting generalizability to truly low-resource languages where tokenization challenges may be more severe
- The benchmark relies exclusively on programmatically verifiable instructions, excluding many real-world instruction-following tasks that require semantic judgment
- Training data composition assumptions for different models remain speculative without access to proprietary training corpus information

## Confidence

- **High**: The observation that GPT4o leads on English IFEval while other models (o1, Sonnet) perform better on multilingual tasks is well-supported by the reported scores
- **Medium**: The hypothesis that tokenization architecture causes character-level instruction failures is plausible but requires direct ablation testing to confirm
- **Medium**: The claim about objective evaluation eliminating self-enhancement bias is methodologically sound but assumes the excluded instruction types aren't critical for real-world capability

## Next Checks

1. **Tokenizer Ablation Test**: Run the same models with byte-level tokenization on character-frequency instructions (ñ count, katakana avoidance) to directly test whether token-level architecture causes the observed failures

2. **Training Data Correlation**: Obtain proxy measures of multilingual training data per model (e.g., from public reports, tokenizer vocabulary analysis) and correlate with instruction-following performance across the three languages to validate the data-composition hypothesis

3. **Human Evaluation Validation**: Select a subset of language-specific instructions and have native speakers judge both model outputs and the programmatic verification functions to ensure the objective metrics align with human notions of instruction-following success