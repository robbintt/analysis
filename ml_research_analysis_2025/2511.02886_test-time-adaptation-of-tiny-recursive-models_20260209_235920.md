---
ver: rpa2
title: Test-time Adaptation of Tiny Recursive Models
arxiv_id: '2511.02886'
source_url: https://arxiv.org/abs/2511.02886
tags:
- tasks
- training
- task
- evaluation
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that pre-trained tiny recursive models
  (TRMs) can be efficiently fine-tuned within ARC Prize 2025's compute constraints.
  A 7M parameter model was pre-trained on 1,280 public ARC tasks for 700k+ steps (48h
  on 4xH100 GPUs) achieving ~10% on public evaluation.
---

# Test-time Adaptation of Tiny Recursive Models

## Quick Facts
- arXiv ID: 2511.02886
- Source URL: https://arxiv.org/abs/2511.02886
- Authors: Ronan Killian McGovern
- Reference count: 26
- Pre-trained tiny recursive models can be efficiently fine-tuned within ARC Prize 2025's compute constraints, achieving 6.67% accuracy on semi-private tasks after 12.5k gradient steps

## Executive Summary
This paper demonstrates that pre-trained tiny recursive models (TRMs) can be efficiently fine-tuned within ARC Prize 2025's compute constraints. A 7M parameter model was pre-trained on 1,280 public ARC tasks for 700k+ steps (48h on 4xH100 GPUs) achieving ~10% on public evaluation. During competition, the same model was fine-tuned on unseen semi-private tasks in just 12,500 gradient steps, reaching 6.67% accuracy. Full fine-tuning outperformed LoRA and embedding-only approaches. While post-training acceleration is significant compared to training from scratch, the model still scores below its pre-trained performance on unseen tasks. The work shows that pre-training effectively shapes network parameters for efficient adaptation to new tasks within strict compute limits.

## Method Summary
The method involves pre-training a 7M parameter recursive transformer on 1,280 public ARC tasks, then fine-tuning on unseen semi-private tasks within competition constraints. Pre-training uses 700k+ optimizer steps with global batch size 768 on 4xH100 GPUs (~48 hours). Post-training fine-tuning runs for 12,500-15,000 gradient steps with batch size 384, using learning rates of 2e-4 (trunk) and 2e-2 (embeddings). The model employs full fine-tuning (not LoRA or embeddings-only) and uses majority voting over 256-512 augmented task variants at inference.

## Key Results
- Pre-trained 7M parameter model achieved ~10% accuracy on public ARC evaluation tasks
- Same model fine-tuned on semi-private tasks in 12,500 steps achieved 6.67% accuracy
- Full fine-tuning outperformed LoRA and embedding-only approaches
- Post-training adaptation required ~12.5k steps versus ~750k steps for training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-training significantly reduces the compute required to adapt to unseen tasks compared to training from scratch.
- **Mechanism:** Pre-training on public ARC tasks "pre-shapes" the network's weight landscape, allowing it to reach meaningful accuracy on new tasks in ~12.5k steps rather than the ~750k steps required for scratch training. This suggests the model learns transferable primitives or "concepts" during pre-training that accelerate convergence on semi-private tasks.
- **Core assumption:** Unseen competition tasks share underlying structural concepts or primitives with the public training distribution.
- **Evidence anchors:**
  - [abstract] "by starting from a tiny recursive model that has been pre-trained... one can efficiently fine-tune... within the allowed compute limits."
  - [section 4.2] "it is possible to pre-shape the neural network in a manner that accelerates tuning on unseen tasks."
  - [corpus] Neighbor 'Less is More' (arXiv:2510.04871) establishes that small recursive networks can learn reasoning behaviors on similar tasks, supporting the viability of the pre-training base.
- **Break condition:** If the target tasks are structurally disjoint from pre-training data (out-of-distribution), the "pre-shaping" may offer no acceleration benefit.

### Mechanism 2
- **Claim:** Full parameter fine-tuning (trunk + embeddings) is superior to adapter-based (LoRA) or embedding-only approaches for adapting TRMs.
- **Mechanism:** Effective adaptation requires updating the "program execution engine" (trunk), not just the "program selector" (embeddings). While embedding-only tuning attempts to map new tasks to existing trunk capabilities, the trunk itself appears to require modification to handle novel competition tasks. LoRA likely restricts the model's expressivity excessively for this low-parameter regime (7M).
- **Core assumption:** The pre-trained trunk is a "good initialization" but lacks the exact primitives needed for test tasks, requiring weight updates across the full network.
- **Evidence anchors:**
  - [abstract] "Full fine-tuning of both model trunk and task embeddings proved most effective, outperforming LoRA and embedding-only approaches."
  - [section 4.2] "LoRA fine-tuning... may reduce the ability of the model to adapt to the newly presented tasks."
  - [corpus] Limited direct support in provided corpus; evidence is primarily internal to this study.
- **Break condition:** If the trunk were significantly larger (e.g., 1B+ params), the "rigid trunk" assumption might break, potentially making LoRA or embedding tuning more viable to prevent overfitting.

### Mechanism 3
- **Claim:** Treating augmented task variants as independent tasks with unique embeddings acts as a form of regularization and latent program search.
- **Mechanism:** Rather than sharing an embedding across augmented views (e.g., rotations), assigning unique IDs forces the model to learn specific mappings for each variant. This appears to function like "Searching Latent Program Spaces" (SLPS), where the model searches for a specific latent vector (program) to execute the transformation, rather than relying on explicit encoding of symmetries.
- **Core assumption:** Forcing the model to distinguish variants via embeddings improves generalization more than explicitly hard-coding symmetry encodings (e.g., dihedral groups).
- **Evidence anchors:**
  - [section 4.5.2] "...performance of models trained with explicit encodings... was inferior to augmentation specific task embeddings."
  - [section 4.6] Draws explicit parallel to "Searching Latent Program Spaces," viewing the process as searching for a vector embedding describing the transformation.
  - [corpus] Neighbor 'Searching Latent Program Spaces' (implied in 4.6) supports the theoretical basis for this search mechanism.
- **Break condition:** If compute is extremely scarce, the inefficiency of re-learning symmetry mappings for every variant might outweigh the regularization benefits.

## Foundational Learning

- **Concept: Recursive Neural Networks (Weight Tying)**
  - **Why needed here:** The "Tiny Recursive Model" architecture relies on re-using the same neural network layer weights multiple times (recursion) to increase effective depth without increasing parameter count.
  - **Quick check question:** How does the effective depth of a 7M parameter recursive model compare to a standard feed-forward model of the same size?

- **Concept: Test-Time Compute / Adaptation**
  - **Why needed here:** The core constraint of the paper is adapting to *unseen* tasks within a fixed compute budget (12 hours on L4 GPUs) during the competition, rather than relying solely on frozen pre-trained weights.
  - **Quick check question:** Why does "pass@2" accuracy matter more than "pass@1000" in the context of the competition constraints?

- **Concept: Task and Augmentation Embeddings**
  - **Why needed here:** The model conditionally executes different reasoning paths based on learned embeddings. Understanding that *every* augmented variant gets its own embedding is critical to replicating the results.
  - **Quick check question:** In the TRM architecture, does a rotated version of a task share the same embedding ID as the original task?

## Architecture Onboarding

- **Component map:** Trunk -> Embeddings -> Heads
- **Critical path:**
  1. Load pre-trained checkpoint (7M trunk weights)
  2. Initialize *new* embeddings for competition tasks (initialized to mean of pre-trained embeddings)
  3. Run 12.5k optimizer steps on competition train pairs (Full Fine-Tuning)
  4. Inference: Run 256-512 augmented variants per task
  5. Aggregate results via majority voting

- **Design tradeoffs:**
  - **Explicit vs. Learned Encodings:** The paper attempted to replace learned augmentation embeddings with explicit feature encoding (e.g., flagging "is_rotated") to save parameters. This failed; learned unique embeddings performed better, suggesting implicit regularization is key.
  - **LoRA vs. Full Tuning:** Standard efficiency wisdom suggests LoRA for fine-tuning, but for this tiny model (7M), LoRA proved too restrictive. Full tuning was necessary to adapt the primitives.

- **Failure signatures:**
  - **Zero-shot / Frozen Trunk:** Scores near 0% on new tasks
  - **Embedding-Only Tuning:** Scores near 0% on held-out tasks (Section 2.4.2). The trunk cannot execute the new "programs" without weight updates
  - **Continued Pre-training with Re-init:** If embeddings are re-initialized and trained further without the original mapping, performance degrades (3.33% vs 6.67%)

- **First 3 experiments:**
  1. **Baseline Reproduction:** Load the provided ARC AGI II pre-trained checkpoint and run a post-training sweep (full fine-tuning vs. embedding-only) on a hold-out set of 10 public tasks to verify the 6.67% adaptation signal
  2. **Embedding Initialization Ablation:** Compare initializing new task embeddings to the *mean* of old embeddings vs. Gaussian noise. (Paper implies mean is better, but confirmation is valuable)
  3. **Cosine Similarity Check:** Measure the cosine similarity between learned embeddings of augmented variants (e.g., Task A vs. Task A-rotated) to verify if the model learns they are related (Section 4.5.1)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal model size (hidden dimension) for compute-efficient pre-training on a fixed number of ARC tasks, given the trade-off between training speed and inference time?
- **Basis in paper:** [explicit] The authors explicitly ask in Section 7 what size model is most compute-efficient, noting that Chinchilla laws suggest larger models might be more efficient, but their limited ablations were inconclusive.
- **Why unresolved:** Ablations varying the characteristic dimension (256 vs 512 vs 1024) were conducted, but the compute budget was insufficient to reach firm conclusions or observe clear scaling advantages.
- **What evidence would resolve it:** A comprehensive scaling law study measuring training and inference FLOPs against accuracy for various model widths on the ARC dataset.

### Open Question 2
- **Question:** Does adjusting the ratio of task embedding dimension to model trunk dimension improve the model's ability to capture similarities between task variants?
- **Basis in paper:** [explicit] Section 7 asks whether increasing or decreasing the embedding dimension (currently equal to the trunk dimension) improves performance, noting that current embeddings fail to show strong cosine similarity between variants.
- **Why unresolved:** The paper observes that explicit encoding failed and augmentation-specific embeddings show low similarity, but it remains unclear if the dimensionality is the limiting factor or if the architecture itself is the bottleneck.
- **What evidence would resolve it:** Ablation studies varying the embedding dimension independently of the trunk dimension, paired with analysis of the latent space structure of task variants.

### Open Question 3
- **Question:** Can the halting head be trained to effectively detect solved evaluation examples, or does the current failure indicate a requirement for greater effective model depth?
- **Basis in paper:** [explicit] Section 7 explicitly questions whether the halting signal's ineffectiveness on evaluation data can be fixed through training methods or if it implies the model needs more depth.
- **Why unresolved:** The halting head learns to identify solved training examples but generalizes poorly to evaluation examples, and the authors did not determine if this is a training issue or a capacity issue.
- **What evidence would resolve it:** Experiments regularizing the halting head during training or increasing the number of reasoning loops to test if deeper models allow the halting mechanism to generalize.

### Open Question 4
- **Question:** What diversity of pre-training tasks best shapes the network for adaptation to unseen tasks?
- **Basis in paper:** [explicit] Section 4.3 states it "remains unclear what diversity of pre-training tasks best shapes a network," noting that filtering for "hard" tasks resulted in worse performance than the original mixed dataset.
- **Why unresolved:** The heuristic that higher quality/harder data should yield better results failed in practice, suggesting unknown negative interactions between reduced data diversity and model adaptation.
- **What evidence would resolve it:** Controlled experiments comparing pre-training on datasets stratified by difficulty and diversity, followed by standardized post-training evaluations on held-out tasks.

## Limitations

- Reliance on a specific dataset split (ARC AGI II tasks) that is not publicly available in its entirety, making independent verification difficult
- While full fine-tuning outperformed adapter methods for this 7M parameter model, the optimal adaptation strategy for different model scales or task distributions is unclear
- The 6.67% accuracy, while demonstrating the adaptation mechanism works, remains relatively low compared to the pre-trained 10% on public tasks, suggesting significant room for improvement

## Confidence

- **High Confidence:** The core finding that pre-training enables efficient adaptation within compute constraints (Mechanism 1)
- **Medium Confidence:** The superiority of full fine-tuning over LoRA and embedding-only approaches (Mechanism 2)
- **Medium Confidence:** The assertion that learned augmentation embeddings outperform explicit encoding (Mechanism 3)

## Next Checks

1. **Embedding Initialization Ablation:** Compare initializing new task embeddings to the *mean* of old embeddings versus Gaussian noise to confirm the paper's implicit assumption about initialization strategy
2. **Cosine Similarity Analysis:** Measure the cosine similarity between learned embeddings of augmented variants (e.g., Task A vs. Task A-rotated) to verify whether the model learns to treat them as related, validating the implicit regularization hypothesis
3. **Out-of-Distribution Test:** Evaluate the adapted model on a completely different dataset (such as ARC-AGI-1 tasks) to assess whether the adaptation mechanism generalizes beyond the competition task distribution