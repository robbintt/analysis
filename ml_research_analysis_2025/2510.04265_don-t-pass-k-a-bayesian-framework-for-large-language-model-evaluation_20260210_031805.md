---
ver: rpa2
title: 'Don''t Pass@k: A Bayesian Framework for Large Language Model Evaluation'
arxiv_id: '2510.04265'
source_url: https://arxiv.org/abs/2510.04265
tags:
- arxiv
- convergence
- pass
- urlhttps
- aime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Bayesian evaluation framework that replaces
  the widely used Pass@k metric for assessing large language model (LLM) reasoning
  performance. The authors address the instability and unreliability of Pass@k, particularly
  when the number of trials is limited and computational resources are constrained.
---

# Don't Pass@k: A Bayesian Framework for Large Language Model Evaluation

## Quick Facts
- arXiv ID: 2510.04265
- Source URL: https://arxiv.org/abs/2510.04265
- Authors: Mohsen Hariri; Amirhossein Samandar; Michael Hinczewski; Vipin Chaudhary
- Reference count: 40
- Primary result: Bayesian evaluation framework achieves faster convergence and greater rank stability than Pass@k across four math reasoning benchmarks

## Executive Summary
This paper introduces a Bayesian evaluation framework that replaces the widely used Pass@k metric for assessing large language model (LLM) reasoning performance. The authors address the instability and unreliability of Pass@k, particularly when the number of trials is limited and computational resources are constrained. The core method involves modeling evaluation outcomes as categorical variables with a Dirichlet prior, yielding closed-form posterior means and credible intervals for any weighted rubric. This framework naturally extends to graded evaluations and allows incorporation of prior evidence when appropriate.

## Method Summary
The method models evaluation outcomes as categorical variables with a Dirichlet prior, enabling closed-form posterior means and credible intervals for any weighted rubric. The framework treats each question's outcomes as multinomial draws, with the Dirichlet prior conjugate to the multinomial likelihood. This yields analytic expressions for posterior mean and variance without Monte Carlo sampling. The approach naturally extends to graded evaluations and allows incorporation of prior evidence. Under uniform prior, Bayes@N rankings are mathematically order-equivalent to avg@N (Pass@1), explaining empirical robustness while adding principled uncertainty. The framework provides transparent decision rules for determining when observed performance differences are statistically meaningful versus noise.

## Key Results
- Bayesian method converges to correct rankings with fewer samples than Pass@k and its variants across four math reasoning benchmarks (AIME'24/25, HMMT'25, BrUMO'25)
- In simulations with known ground-truth success rates, Bayesian procedure consistently shows superior convergence rates compared to alternative evaluation methods
- Empirical results show that hundreds of trials may be required to distinguish models with very similar performance metrics (~1.3% gap)
- Informative priors from correlated model variants can accelerate ranking convergence, but require careful calibration to avoid prior dominance

## Why This Works (Mechanism)

### Mechanism 1
Modeling evaluation outcomes as categorical variables with a Dirichlet prior yields closed-form posterior means and credible intervals for any weighted rubric. Each question's outcomes follow a multinomial distribution over C+1 categories. The Dirichlet distribution is conjugate to the multinomial, so the posterior is also Dirichlet with parameters ν_α = n_α^0 + n_α (prior counts plus observed counts). This conjugacy yields analytic expressions for the posterior mean μ and variance σ² without Monte Carlo sampling. The core assumption is that outcomes per question are independent and identically distributed draws from a fixed categorical distribution, and the prior appropriately encodes available information.

### Mechanism 2
Under uniform prior, Bayes@N rankings are mathematically order-equivalent to avg@N (Pass@1) at all finite N, explaining empirical robustness while adding principled uncertainty. Equation 16-19 shows the Bayesian posterior mean μ relates to naive weighted average a via an affine transformation: μ = A + (N/(1+C+N))·a, where A is constant across models. Since the prefactor is positive, if μ > μ', then necessarily a > a'. Rankings are preserved exactly. This holds only when using uniform prior (D=0) and the same weight vector w across all compared models.

### Mechanism 3
Informative priors from correlated model variants can accelerate ranking convergence, but require careful calibration to avoid prior dominance. Prior data updates Dirichlet concentration parameters before observing any new trials. When the prior model's ranking correlates with the updated model (τ ≈ 0.88 in simulation), starting with D=1-4 prior trials yields higher initial Kendall's τ and faster convergence. However, excessive prior weight (D=8, 16) causes τ curves to dip below the uniform baseline as the prior overwhelms new evidence. The core assumption is that the prior model performance is positively correlated with the target model and the performance shift from original to updated is moderate.

## Foundational Learning

**Concept: Dirichlet distribution as conjugate prior**
Why needed here: The entire framework rests on Dirichlet-multinomial conjugacy enabling closed-form posteriors. Without understanding this, Algorithm 1 appears magical.
Quick check question: Can you explain why the posterior after observing counts n_αk is simply Dir(ν_α) where ν_αk = n_αk^0 + n_αk?

**Concept: Credible intervals vs. confidence intervals**
Why needed here: The paper's decision rule ("do not declare a winner when intervals overlap") relies on Bayesian credible intervals, not frequentist confidence intervals.
Quick check question: Given a 95% credible interval [0.42, 0.58] for model A's success rate, what statement can you make that you cannot make with a 95% confidence interval?

**Concept: Kendall's τ rank correlation**
Why needed here: Primary metric for quantifying convergence speed and comparing evaluation methods against the gold standard.
Quick check question: Why does τ_b handle ties differently than τ_a, and when would this matter in LLM evaluation?

## Architecture Onboarding

**Component map:**
Results matrix R (M×N) -> Category mapping function -> Category counts n_αk -> Prior counts n⁰_αk -> ν_αk parameters -> Algorithm 1 core computation -> Posterior mean μ and uncertainty σ

**Critical path:**
1. Run N inference trials per question → populate R matrix
2. Apply rubric rules to map each response to category k
3. Compute category counts n_αk for each question α
4. Add prior counts (if using): ν_αk = 1 + D-prior-counts + n_αk
5. Execute Algorithm 1: posterior mean μ and uncertainty σ via closed-form formulas
6. Compare models: declare difference only if credible intervals (e.g., μ ± 1.645σ for 95%) do not overlap

**Design tradeoffs:**
- Binary (C=1) vs. multi-category: More categories capture richer signals but require more trials per question for stable category probability estimates
- Uniform vs. informative prior: Informative priors accelerate convergence if correlated, but risk bias if prior model differs significantly
- N (trials per question): Higher N narrows CIs; paper shows distinguishing models with ~1.3% performance gap may require N≈200
- Rubric weights w: Directly encode evaluation priorities; different weights can reorder mid-tier models (Figure 8)

**Failure signatures:**
- Non-convergence within trial budget: True performance gap too small; must either run more trials or accept CI-based ties
- Prior-dominated posteriors (D >> N): Rankings reflect prior model, not current model being evaluated
- Excessive ties with CI: Either truly equivalent performance or insufficient trials; check z-scores to distinguish
- CI computation errors: Should never produce intervals outside [0,1] in binary case with proper Dirichlet prior—implementation bug if observed

**First 3 experiments:**
1. **Validate implementation with synthetic data:** Replicate Figure 2's biased-coin simulation (11 models, 30 questions, known ground-truth π_α). Verify your Algorithm 1 produces τ convergence curves matching the paper's Bayes@N line.
2. **Binary evaluation on your own models:** Select 3-5 LLMs, run N=80 trials on AIME'24/25-style math problems. Compare convergence speed (τ vs. N) for Pass@k vs. Bayes@N. Identify minimum N where Bayes@N reaches τ>0.95.
3. **Categorical rubric design:** Define a 4-category rubric (e.g., correct-and-boxed, correct-unboxed, wrong-with-reasoning, wrong-garbage). Choose weights w. Run Bayes@N on 2 models and verify: (a) different rubrics can flip rankings, (b) credible intervals remain valid across rubrics.

## Open Questions the Paper Calls Out

**Open Question 1:** What practical guidelines are required to determine the optimal strength of non-uniform priors (D) to accelerate convergence without introducing bias that degrades ranking accuracy? The paper demonstrates that while non-uniform priors can speed up convergence, setting the prior weight (D) too high (e.g., 8 or 16 trials worth of data) can lower Kendall's τ compared to a uniform prior by over-emphasizing outdated information. A systematic study analyzing the relationship between the magnitude of model update and the optimal D would be needed.

**Open Question 2:** How can the Bayesian evaluation framework be effectively adapted for subjective, preference-based tasks where labels come from noisy LLM-as-a-judge systems rather than deterministic ground truth? While the math supports categorical variables, the paper focuses on mathematical reasoning with binary/verifiable outcomes. Subjective domains introduce label noise (judge error) and bias, which the current framework assumes are fixed by the rubric definition. Empirical validation on a subjective benchmark showing that Bayes@N maintains rank stability and calibrated credible intervals even when the input labels are derived from noisy probabilistic judges would be needed.

**Open Question 3:** To what extent can expert-elicited or domain-conditioned priors improve evaluation efficiency compared to the historical data-based priors tested in the current study? The paper's experiments on non-uniform priors were limited to "stochastic perturbations" of synthetic models or previous model versions. It did not test priors derived from human intuition or specific domain constraints. Experiments comparing the sample efficiency of Bayes@N using expert-defined priors versus uniform or history-based priors on specialized benchmarks would be needed.

## Limitations

- Prior calibration remains an open problem; overly strong priors can degrade ranking accuracy
- Framework effectiveness heavily depends on rubric design and weight selection, with no guidance on rubric validation
- Distinguishing models with very similar performance may require hundreds of trials per question, creating practical computational constraints
- The framework has only been validated on mathematical reasoning tasks and may not generalize to other domains

## Confidence

**High Confidence:**
- The mathematical equivalence between Bayes@N (with uniform prior) and avg@N rankings
- The closed-form posterior computation via Dirichlet-multinomial conjugacy
- The demonstration that Bayes@N achieves faster convergence than Pass@k in the presented simulations

**Medium Confidence:**
- The generalizability of results across diverse model architectures
- The framework's robustness to rubric design variations
- The practical applicability for distinguishing closely-performing models

**Low Confidence:**
- The optimal method for prior calibration across arbitrary model comparisons
- The framework's performance on non-mathematical reasoning tasks
- The scalability to very large model fleets with limited computational resources

## Next Checks

1. **Cross-Domain Validation:** Apply the Bayesian framework to a non-mathematical reasoning task (e.g., commonsense reasoning or code generation) to test generalizability beyond the math benchmarks used in the paper. Compare convergence patterns and uncertainty quantification against the math domain results.

2. **Prior Calibration Study:** Design a systematic experiment varying the prior strength D across a spectrum of model similarity scenarios (from very similar to very different models). Quantify the impact of prior misspecification on ranking accuracy and develop guidelines for appropriate D selection based on model characteristics.

3. **Practical Scaling Analysis:** Evaluate the framework's performance on a larger scale with 50+ models and reduced trial budgets (N=20, 40). Measure the trade-off between evaluation cost and ranking reliability, and identify practical guidelines for when the Bayesian approach becomes more cost-effective than traditional methods.