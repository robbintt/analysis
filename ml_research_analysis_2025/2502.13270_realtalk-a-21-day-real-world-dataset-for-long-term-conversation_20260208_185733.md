---
ver: rpa2
title: 'REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation'
arxiv_id: '2502.13270'
source_url: https://arxiv.org/abs/2502.13270
tags:
- speaker
- conversation
- conversations
- each
- persona
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REALTALK introduces a 21-day real-world dialogue dataset to study
  long-term conversations, addressing the lack of authentic human interaction data
  in existing research. By comparing real dialogues with LLM-generated ones, the study
  finds that humans exhibit greater emotion diversity, lower empathy consistency,
  and higher variability in emotional intelligence (EI) attributes than LLMs.
---

# REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation

## Quick Facts
- arXiv ID: 2502.13270
- Source URL: https://arxiv.org/abs/2502.13270
- Reference count: 40
- Primary result: REALTALK reveals humans exhibit greater emotion diversity and lower empathy consistency than LLMs in long-term conversations

## Executive Summary
REALTALK introduces a 21-day real-world dialogue dataset to study long-term conversations, addressing the lack of authentic human interaction data in existing research. By comparing real dialogues with LLM-generated ones, the study finds that humans exhibit greater emotion diversity, lower empathy consistency, and higher variability in emotional intelligence (EI) attributes than LLMs. Two benchmarks—persona simulation and memory probing—are proposed to evaluate models' ability to replicate user styles and retain long-term context. Results show that fine-tuning on a specific user's chat history improves persona simulation, while LLMs struggle with memory-based reasoning tasks, especially in multi-hop questions. This work highlights the complexity of modeling human-like interactions and memory retention in AI systems.

## Method Summary
The study introduces REALTALK, a 21-day real-world dialogue dataset containing 10 conversations with extensive annotations. The research methodology involves two primary benchmark tasks: persona simulation (continuing conversation on behalf of a specific user given prior context) and memory probing (answering questions requiring long-term memory of past interactions). Evaluation uses ROUGE, BERTScore, and EI attribute matching for persona simulation, while memory probing employs partial exact match F1 and LLM-based accuracy. The study compares human-human conversations against LLM-generated dialogues and tests various memory architectures including full conversation context versus event-based compression.

## Key Results
- Humans exhibit greater emotion diversity and lower empathy consistency than LLMs in long-term conversations
- Fine-tuning on specific user chats improves persona emulation in stylistic EI attributes but not content similarity
- Event-based memory systems enhance efficiency but cause 41-46% performance drops on multi-hop reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Fine-Tuning Captures Stylistic Patterns, Not Content
Fine-tuning on a specific user's dialogue history improves persona emulation in stylistic EI attributes (emotion, sentiment, reflectiveness, grounding, empathy) but not in content similarity (lexical/semantic measures). The model learns distributional patterns of how a user expresses themselves—their typical emotional range, grounding frequency, and empathy style—rather than learning what topics they discuss or specific responses they would give. The paper shows fine-tuning improves message-level EI accuracy significantly (p < 0.02) while ROUGE/BERTScore remain unchanged.

### Mechanism 2: Context Window Alone Insufficient for Persona Grounding
Simply providing more conversation history to an LLM does not improve its ability to simulate a user's persona; performance saturates after ~3 sessions. LLMs process context as unstructured text and lack explicit mechanisms to extract and persist persona-relevant attributes. Without architectural support for persona state tracking, additional context adds noise rather than signal.

### Mechanism 3: Event-Based Memory Compression Trades Efficiency for Multi-Hop Reasoning
Condensing conversations into key events causes a 41-46% performance drop on multi-hop reasoning while improving efficiency. Multi-hop questions require "intermediary steps crucial for linking past and present context" which event extraction removes. Temporal and commonsense reasoning are less affected because they rely on retained event boundaries or external knowledge rather than implicit connections.

## Foundational Learning

- **Emotional Intelligence (EI) Taxonomy** (Self-awareness, Motivation, Empathy, Social Skills, Self-regulation)
  - Why needed here: The paper's evaluation framework treats EI as a multi-dimensional construct; understanding these components is required to interpret benchmark results and design improvements.
  - Quick check question: Can you explain why "excessive empathy" in LLMs is problematic if real humans show variable empathy levels?

- **Multi-hop vs. Temporal vs. Commonsense Reasoning in Memory**
  - Why needed here: The memory probing benchmark differentiates these three question types, and they show different failure patterns; architectural solutions may need to address them separately.
  - Quick check question: Why would event-based memory preserve temporal reasoning but degrade multi-hop reasoning?

- **Persona Consistency vs. Adaptability**
  - Why needed here: The paper reveals that humans adapt EI attributes to different partners; a "consistent persona" benchmark may be measuring the wrong target.
  - Quick check question: Should a persona simulation model produce consistent outputs across different conversational partners, or should it exhibit partner-dependent adaptation?

## Architecture Onboarding

- **Component map**: Data Layer (REALTALK conversations, QA annotations, event annotations) -> EI Evaluation Module (message-level classifiers) -> Persona Simulation Pipeline (context formatting → LLM inference → EI attribute comparison) -> Memory Probing Pipeline (full conversation/event list → Question answering → F1/accuracy) -> Fine-tuning Framework (per-speaker LoRA/Full fine-tuning on conversation history)

- **Critical path**: 1. Load and preprocess REALTALK conversations (concatenate consecutive messages, format sessions with dates) 2. Run EI attribute classifiers on ground truth to establish baseline distributions 3. Set up persona simulation baseline (zero-shot) and evaluate EI attribute matching 4. Fine-tune per-speaker models and compare stylistic EI accuracy 5. Run memory probing experiments with both full-context and event-based inputs

- **Design tradeoffs**:
  - Fine-tuning vs. RAG: Fine-tuning captures style but requires per-user training; RAG preserves content but context alone doesn't help persona simulation
  - Full context vs. Event memory: Full context retains multi-hop reasoning capability but is inefficient; event memory is efficient but loses implicit connections
  - Uniform EI target vs. Adaptive EI: Current LLMs default to uniformly high empathy; real humans vary—decide whether to model "ideal" or "realistic" EI

- **Failure signatures**:
  - Persona simulation outputs have correct semantics but wrong EI distribution (e.g., excessive empathy, low emotion diversity)
  - Memory probing fails on multi-hop questions despite correct retrieval of individual facts
  - Fine-tuned model reproduces content from training conversation inappropriately (overfitting)
  - Performance plateaus at 3 sessions of context regardless of model size

- **First 3 experiments**:
  1. Establish EI baseline: Run EI classifiers on REALTALK conversations; compare distributions to LOCOMO (synthetic) to confirm paper's findings on emotion diversity and empathy levels
  2. Zero-shot vs. Fine-tuned persona simulation: Compare message-level EI accuracy between generic LLM and per-speaker fine-tuned model; verify that style improves while content similarity stays constant
  3. Memory probing with ablated context: Test memory probing performance with (a) full conversation, (b) event-only, (c) events + 1 session of surrounding context to identify if partial context restoration mitigates multi-hop degradation

## Open Questions the Paper Calls Out

- **How can memory systems be designed to balance efficiency with multi-hop reasoning capability, given that event-based condensation causes 41-46% performance drops on multi-hop questions?**
  - Basis in paper: The authors state "Event-based memory (E) enhances efficiency but severely weakens memory probing performance, especially in multi-hop reasoning by causing a 41-46% performance drop" and note this "tradeoff mirrors real-world AI memory challenges."
  - Why unresolved: The paper identifies the tradeoff but does not propose or evaluate alternative memory architectures that might preserve intermediary context needed for multi-hop reasoning while maintaining efficiency.

- **What mechanisms beyond fine-tuning could enable LLMs to capture user-specific conversational styles from context alone?**
  - Basis in paper: The authors find "increasing conversation history does not improve message-level EI or exhibit a clear pattern" and "LLMs struggle to capture and replicate a speaker's style using context alone."
  - Why unresolved: The paper demonstrates that neither zero-shot prompting with longer context nor increasing training context improves persona simulation, but does not investigate retrieval-augmented or meta-learning approaches.

- **How do demographic factors (age, culture, native language) influence emotional intelligence patterns and persona consistency in long-term dialogues?**
  - Basis in paper: The limitations section states participants are "English speakers from the US, aged 18-25, limiting the dataset's representativeness" and "Future studies should include a more diverse demographic range."
  - Why unresolved: The narrow demographic scope prevents understanding of whether EI variability and persona adaptability findings generalize across populations.

## Limitations
- The fine-tuning experiments used unspecified hyperparameters and model architectures, making exact replication difficult without additional information
- EI attribute classification relies on gpt-4o-mini and RoBERTa models with implementation details that are not fully specified
- The relationship between conversational intimacy progression and EI attribute adaptation is inferred from correlations rather than causal experiments

## Confidence
- **High confidence**: Core findings about emotion diversity and empathy consistency differences between humans and LLMs are well-supported by comparative analysis with LOCOMO dataset
- **Medium confidence**: Persona simulation results showing fine-tuning improves stylistic EI but not content similarity, as methodology is clear but hyperparameters are unspecified
- **Medium confidence**: Memory probing results showing event-based compression degrades multi-hop reasoning, though exact implementation details of the memory systems are limited

## Next Checks
1. Replicate the EI baseline analysis by comparing REALTALK conversation distributions to LOCOMO synthetic data for emotion diversity and empathy metrics
2. Implement the zero-shot vs. fine-tuned persona simulation comparison using standardized hyperparameters to verify stylistic vs. content adaptation patterns
3. Conduct ablation studies on memory probing with partial context restoration (events + surrounding sessions) to quantify multi-hop reasoning degradation causes