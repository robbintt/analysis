---
ver: rpa2
title: 'Names Don''t Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning'
arxiv_id: '2601.23169'
source_url: https://arxiv.org/abs/2601.23169
tags:
- per-stream
- aggregated
- formula
- attention
- interchangeable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a symbol-invariant transformer architecture
  that achieves exact alpha-equivalence invariance through parallel embedding streams.
  Each interchangeable token receives its own stream where it is represented explicitly
  while others become placeholders, with shared attention parameters across streams
  enabling post-training vocabulary expansion.
---

# Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning

## Quick Facts
- arXiv ID: 2601.23169
- Source URL: https://arxiv.org/abs/2601.23169
- Authors: İlker Işık; Wenchao Li
- Reference count: 40
- Key outcome: Symbol-invariant transformer achieves exact alpha-equivalence invariance through parallel embedding streams, demonstrating state-of-the-art performance on LTL tasks and outperforming GPT-5.2

## Executive Summary
This paper introduces a symbol-invariant transformer architecture that achieves exact alpha-equivalence invariance through parallel embedding streams. Each interchangeable token receives its own stream where it is represented explicitly while others become placeholders, with shared attention parameters across streams enabling post-training vocabulary expansion. The architecture uses aggregated attention to fuse information across streams while preserving token-specific representations. Theoretical analysis proves the model is invariant to alpha-renaming by construction.

## Method Summary
The symbol-invariant transformer uses parallel embedding streams to achieve alpha-equivalence invariance. For each interchangeable token, the model creates a dedicated stream where that token is represented explicitly while all other interchangeable tokens become placeholders. All streams share the same attention parameters, enabling post-training vocabulary expansion. The architecture employs aggregated attention to combine information across streams while maintaining token-specific representations. This design ensures that the model's outputs are invariant to symbol renaming by construction.

## Key Results
- Achieves perfect alpha-covariance across all atomic proposition counts on LTL witness generation
- Outperforms GPT-5.2 on LTL tasks with "nearly instantaneous" performance (vs GPT's 37-second average per sample)
- Demonstrates state-of-the-art performance on propositional logic assignment prediction

## Why This Works (Mechanism)
The architecture works by creating multiple parallel embedding streams where each interchangeable token gets its own stream with explicit representation. Shared attention parameters across streams ensure consistent processing while allowing the model to handle post-training vocabulary expansion. The aggregated attention mechanism fuses information from all streams without losing token-specific details, maintaining the invariant property that output depends only on the structure of relationships between tokens, not their specific names.

## Foundational Learning
1. Alpha-equivalence invariance
   - Why needed: Ensures model outputs depend only on token relationships, not specific names
   - Quick check: Verify output consistency when renaming interchangeable symbols

2. Parallel embedding streams
   - Why needed: Allows each interchangeable token to be processed explicitly while maintaining shared parameters
   - Quick check: Confirm stream separation while observing parameter sharing

3. Aggregated attention mechanism
   - Why needed: Combines information across streams while preserving token-specific representations
   - Quick check: Validate that attention weights properly aggregate across streams

4. Post-training vocabulary expansion
   - Why needed: Enables model to handle new interchangeable tokens without retraining
   - Quick check: Test model performance with novel tokens after training

## Architecture Onboarding

**Component Map:** Token input -> Parallel Embedding Streams -> Shared Attention -> Aggregated Attention Fusion -> Output

**Critical Path:** Input tokens are routed to appropriate parallel streams, processed through shared attention mechanisms, then fused via aggregated attention to produce final output.

**Design Tradeoffs:** Linear complexity growth with interchangeable tokens enables exact invariance but may limit scalability; shared parameters enable vocabulary expansion but require careful attention parameter design.

**Failure Signatures:** Performance degradation with increasing interchangeable tokens; breakdown of invariance when token interchangeability assumptions are violated; inefficiency in handling large vocabularies.

**Three First Experiments:**
1. Test alpha-invariance by systematically renaming interchangeable tokens and measuring output consistency
2. Evaluate scalability by measuring runtime and memory usage as interchangeable token count increases
3. Validate post-training expansion by introducing novel interchangeable tokens and assessing performance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Computational complexity grows linearly with interchangeable token count, raising scalability concerns
- Theoretical invariance proof relies on idealized assumptions that may not hold in real-world noisy data
- Experimental validation limited to symbolic reasoning tasks, leaving generalizability to other domains uncertain

## Confidence
- Theoretical framework for alpha-invariance: High
- Empirical superiority claims over GPT-5.2: Medium (lacks detailed benchmarking)
- Cross-domain generalizability: Low (limited experimental scope)
- Scalability claims: Medium (theoretical concerns not fully addressed)

## Next Checks
1. Benchmark the symbol-invariant transformer on standard natural language understanding tasks to assess cross-domain generalizability
2. Conduct scalability experiments measuring runtime and memory usage as the number of interchangeable tokens increases from the current experimental range
3. Test robustness to noisy or partially structured inputs where token interchangeability assumptions break down