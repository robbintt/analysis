---
ver: rpa2
title: 'Don''t Walk the Line: Boundary Guidance for Filtered Generation'
arxiv_id: '2510.11834'
source_url: https://arxiv.org/abs/2510.11834
tags:
- safety
- arxiv
- reward
- training
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of misalignment between generative
  models and safety classifiers in compound AI systems, where models are often fine-tuned
  to reduce rejection probability but end up generating outputs near classifier decision
  boundaries, increasing both false positives and false negatives. The core method,
  Boundary Guidance, is a reinforcement learning fine-tuning approach that explicitly
  steers generation away from classifier decision boundaries rather than simply minimizing
  rejection probability.
---

# Don't Walk the Line: Boundary Guidance for Filtered Generation

## Quick Facts
- arXiv ID: 2510.11834
- Source URL: https://arxiv.org/abs/2510.11834
- Reference count: 18
- Primary result: Boundary Guidance achieves Pareto improvements in both helpfulness and harmlessness across model scales by steering generation away from safety classifier decision boundaries.

## Executive Summary
This paper addresses a fundamental problem in compound AI systems: fine-tuning generative models to minimize rejection probability often pushes them toward classifier decision boundaries, increasing both false positives and false negatives. The proposed solution, Boundary Guidance, uses reinforcement learning to explicitly steer generation away from ambiguous regions where safety classifiers are uncertain. The method combines helpfulness rewards from a reward model with boundary-avoiding signals from the safety classifier, creating an incentive for generating outputs that classifiers can evaluate with high confidence. Empirical results show significant improvements in harmlessness while maintaining or improving helpfulness, particularly for smaller models.

## Method Summary
Boundary Guidance fine-tunes instruction-tuned LLMs using GRPO with LoRA (r=16, α=32) to minimize generation near safety classifier decision boundaries. The reward function R(x,y) = u(x,y) + sign(t-0.5)×t combines helpfulness scores from a reward model with safety classifier confidence, where t is the probability of being "unsafe" from Meta-Llama-Guard-2-8B. Training uses 7,880 prompts (4,000 jailbreak, 3,000 Alpaca, 880 HarmfulQA) with 4-bit quantization and 8 generations per step. Evaluation employs GPT-4.1 as an LLM judge for helpfulness (1-4 scale) and harmfulness (0-3 scale).

## Key Results
- Boundary Guidance achieves Pareto improvements across model scales (0.5B to 14B parameters)
- Harmfulness decreases significantly (-0.09 to -0.17 points) while helpfulness is maintained or improved (+0.03 to +0.13 points)
- Smallest model (Qwen2.5-0.5B) shows largest absolute improvement in harmlessness
- Guard-only ablation causes near-universal refusals on 0.5B model (helpfulness drops 34%)
- Results demonstrate compound system optimization can achieve outcomes neither component could accomplish alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: System utility is minimized near classifier decision boundaries, creating a principled incentive to steer generation away from ambiguous regions.
- Mechanism: Expected utility follows a V-shaped function around threshold τ, decreasing for t < τ and increasing for t ≥ τ (Proposition 1). Outputs near the boundary maximize misclassification risk in both directions.
- Core assumption: Safety classifier confidence correlates with classification accuracy—outputs far from the boundary are easier to classify correctly.
- Evidence anchors:
  - [abstract] "this can be suboptimal: it often pushes the model toward producing samples near the classifier's decision boundary, increasing both false positives and false negatives"
  - [section 3] "expected utility is minimized in the neighborhood of the decision boundary τ"
  - [corpus] Weak direct support; related work on conformal novelty detection discusses FDR control at boundaries but in different context
- Break condition: If classifier confidence is poorly calibrated or adversarial inputs concentrate far from boundaries, the utility model breaks down.

### Mechanism 2
- Claim: Combining helpfulness rewards with boundary-avoiding safety signals yields Pareto improvements over optimizing either objective alone.
- Mechanism: The reward R(x,y) = u(x,y) + sign(t-0.5)×t creates gradient pressure toward unambiguous regions while maintaining utility. Ablations show safety-only optimization causes collapse to refusals on smaller models.
- Core assumption: The reward model u(x,y) and safety classifier t(x,y) provide complementary, non-redundant signals.
- Evidence anchors:
  - [section 4] "we use a continuous, boundary-avoiding reward" with the piecewise formulation
  - [section 6.2.1] Guard-only ablation: Qwen2.5-0.5B helpfulness drops from 2.13 to 1.40 (-34%) with near-universal refusals
  - [corpus] No directly comparable compound system optimization in neighbors
- Break condition: If reward model and classifier are adversarially correlated (e.g., helpful responses systematically near boundaries), the combined signal may not separate objectives.

### Mechanism 3
- Claim: Smaller models benefit more from Boundary Guidance because their base safety capabilities are weaker, leaving more room for improvement.
- Mechanism: Weaker models produce noisier safety-related outputs that scatter near boundaries; explicit steering provides disproportionately large gains.
- Core assumption: The base model has sufficient capacity to learn the boundary-avoiding policy without collapsing.
- Evidence anchors:
  - [section 6.1] "The largest overall improvement appears on the smallest model (Qwen2.5-0.5B)"
  - [section 7] "particularly pronounced benefits for smaller models where base safety capabilities are weaker"
  - [corpus] No scaling analysis in neighbor papers
- Break condition: Below some capacity threshold, models may fail to learn the dual-objective policy at all (as seen in guard-only ablation).

## Foundational Learning

- Concept: **KL-regularized reinforcement learning**
  - Why needed here: GRPO uses KL penalties to prevent policy from diverging too far from base during fine-tuning; critical for stability.
  - Quick check question: Can you explain why unregularized RL on classifier signals might cause catastrophic forgetting?

- Concept: **Classifier calibration and decision thresholds**
  - Why needed here: The boundary τ=0.5 is treated as calibrated probability; uncalibrated classifiers would misplace the boundary-avoidance region.
  - Quick check question: If a classifier outputs 0.7 for all inputs regardless of content, what happens to Boundary Guidance?

- Concept: **Pareto frontier in multi-objective optimization**
  - Why needed here: The paper claims Pareto improvements—understanding tradeoffs helps diagnose when improvements are real vs. measurement artifacts.
  - Quick check question: If helpfulness improves but harm stays constant (vs. improves), is that still Pareto improvement?

## Architecture Onboarding

- Component map: Policy model (Qwen2.5/Gemma) -> Safety classifier (Meta-Llama-Guard-2-8B) -> Reward model (Skywork-Reward-V2) -> GRPO with LoRA -> Updated policy weights

- Critical path: Prompt → Policy generates completion → Classifier computes t(x,y) → Reward model computes u(x,y) → Combined reward R → GRPO update → LoRA weight adjustment

- Design tradeoffs:
  - Including reward model: Required for small models, less critical for large (ablation shows 7B+ can work guard-only)
  - Prompt-aware vs. completion-only reward: Prompt-aware (ablation 6.2.2) degrades performance by training away refusal on unsafe prompts
  - Training data mix: 4000 jailbreak + 3000 benign + 880 harmful—imbalance may bias toward refusal behaviors

- Failure signatures:
  - **Collapse to universal refusal**: Seen in 0.5B guard-only; indicates model lacks capacity for dual-objective optimization
  - **Increased harmfulness**: Seen in prompt-aware ablation; reward accidentally incentivizes harmful outputs for easier filtering
  - **No improvement in largest models**: Qwen2.5-14B helpfulness regresses -0.05 (statistically insignificant, but worth monitoring)

- First 3 experiments:
  1. **Baseline replication**: Train Qwen2.5-7B with full Boundary Guidance reward on provided dataset; verify helpfulness and harm scores approximate Table 1 (Helpful: ~2.75, Harmful: ~0.04)
  2. **Guard-only ablation on your target model**: If deploying on smaller models, confirm reward model is necessary; if ≥7B, assess whether guard-only simplifies pipeline
  3. **Threshold sensitivity**: Vary τ from 0.3 to 0.7 to test whether the 0.5 threshold is optimal for your specific classifier—calibration may differ across guard models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Boundary Guidance be extended to handle multiple distinct safety dimensions with potentially correlated classifier outputs?
- Basis in paper: [explicit] Section 7.2 states "We assumed that safety is a binary category... In principle, it is possible to consider different notions of safety s_1(x,y), s_2(x,y), ..., s_k(x,y) with calibrated classifiers... A challenge... is that the probability for whether an output is unsafe depends also on the correlation structure of the calibrated estimates."
- Why unresolved: The decision-theoretic framework (Equation 1) assumes a single safety score; extending to multiple dimensions requires modeling correlations between classifier outputs, which current theory does not address.
- What evidence would resolve it: A formal extension of the utility function incorporating multiple safety dimensions, plus empirical validation showing improved performance on multi-category harm taxonomies.

### Open Question 2
- Question: Can welfare-aware filters that incorporate predicted user utility u(x,y) and over-blocking costs λ outperform safety-only filters in compound systems?
- Basis in paper: [explicit] Section 7.2 proposes "filters that take into account the predicted user utility u(x,y), the harm from not showing a prompt λ, and the safety classifier t(x,y)" to "move from filtering safe outputs to filtering outputs that are predicted to be negative welfare."
- Why unresolved: Current experiments only use safety classifiers; the theoretical framework introduces welfare parameters but does not empirically test welfare-aware filtering.
- What evidence would resolve it: Comparative experiments where filters incorporate reward model signals and calibrated over-blocking costs, measuring downstream user satisfaction and societal harm.

### Open Question 3
- Question: What is the minimum classifier accuracy required far from decision boundaries for Boundary Guidance to provide meaningful safety guarantees in high-stakes deployments?
- Basis in paper: [explicit] Section 7.1 states "Boundary Guidance relies on the fact that filters predict more accurately far from their decision boundary than close to it. The current classifiers might, in very high-stakes settings, not be sufficiently accurate in filtering generations even far from the decision boundary."
- Why unresolved: The method assumes a reliable relationship between classifier confidence and accuracy, but this relationship is not quantified or tested.
- What evidence would resolve it: Calibration analysis measuring classifier accuracy at different confidence thresholds, combined with failure mode analysis in high-stakes domains (medical, legal).

### Open Question 4
- Question: Why does the smallest model (0.5B parameters) collapse to universal refusals under guard-only training while larger models maintain helpfulness?
- Basis in paper: [inferred] Table 2 shows Qwen2.5-0.5B helpfulness drops from 2.13 to 1.40 (-34%) with guard-only training, while 7B+ models show no degradation. The paper states "inspection of rollouts shows the small model converging to near-universal refusals, pointing to insufficient capacity."
- Why unresolved: The relationship between model capacity and the ability to jointly optimize boundary avoidance and helpfulness is not characterized.
- What evidence would resolve it: Systematic experiments across model scales with guard-only training, analysis of learned representations, and identification of the capacity threshold where collapse occurs.

## Limitations
- Method relies on the assumption that safety classifier confidence correlates with classification accuracy far from decision boundaries, which may not hold in all settings.
- The approach requires a reward model for smaller models, adding complexity and potential brittleness to the system.
- No analysis of how classifier calibration affects performance—mis-calibrated classifiers could misplace the boundary-avoidance region.

## Confidence
- **High**: The theoretical foundation (Proposition 1) is sound and directly supports the main claim about utility minimization near boundaries.
- **High**: Empirical results show clear Pareto improvements with appropriate ablations demonstrating the necessity of both reward components.
- **Medium**: The paper provides limited analysis of why smaller models benefit more or the capacity threshold for collapse.
- **Low**: No systematic study of classifier calibration effects or robustness to different threshold choices.

## Next Checks
1. Replicate the 7B model training and verify helpfulness/harm scores match Table 1 (Helpful: ~2.75, Harmful: ~0.04) using GPT-4.1 evaluation.
2. Run guard-only ablation on your target model to determine whether reward model is necessary for your deployment scenario.
3. Test classifier threshold sensitivity by varying τ from 0.3 to 0.7 to assess calibration effects on your specific safety classifier.