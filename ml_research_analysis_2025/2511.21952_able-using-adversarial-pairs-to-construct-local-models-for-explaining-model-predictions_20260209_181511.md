---
ver: rpa2
title: 'ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model
  Predictions'
arxiv_id: '2511.21952'
source_url: https://arxiv.org/abs/2511.21952
tags:
- local
- able
- adversarial
- boundary
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ABLE, a novel approach to construct local models
  for explaining model predictions. ABLE addresses the limitations of existing methods
  like LIME, which suffer from instability and poor local fidelity due to random sampling.
---

# ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions

## Quick Facts
- arXiv ID: 2511.21952
- Source URL: https://arxiv.org/abs/2511.21952
- Reference count: 40
- ABLE achieves higher stability and fidelity than state-of-the-art methods, with highest fidelity in 16 out of 18 cases and best stability in 12 out of 18 cases

## Executive Summary
This paper introduces ABLE, a novel approach to construct local models for explaining model predictions that addresses the instability and poor local fidelity of existing methods like LIME. Instead of relying on random sampling, ABLE generates adversarial pairs that bracket the local decision boundary for the test instance. The method demonstrates significant improvements in both fidelity and stability across six UCI benchmark datasets and three deep neural network architectures, requiring only 300 queries compared to the 5000 points typically used by baselines.

## Method Summary
ABLE constructs local models by first generating neighborhood points around a test instance, then applying adversarial attacks to create pairs of points on opposite sides of the decision boundary. The method samples n=150 neighborhood points within radius r=0.2 via bounded Gaussian noise, applies forward attacks to flip labels and reverse attacks to return to original labels, and trains logistic regression on the resulting 300 adversarial points. The approach supports multiple attack methods including FGSM, PGD, etDeepFool, and HopSkipJump via the ART library.

## Key Results
- ABLE achieved highest fidelity in 16 out of 18 cases across six datasets and three model architectures
- Ranked best in stability in 12 out of 18 cases with Jaccard index improvements
- Requires only 300 queries versus 5000 for LIME, demonstrating high query efficiency
- Fidelity peaks at radius r=0.2 with R² up to 0.908, declining as radius increases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Generating adversarial pairs that straddle the decision boundary creates more informative training data for local models than random sampling
- **Mechanism**: For each neighborhood point D, a forward adversarial attack produces A (opposite class), then a reverse attack produces A' (same class as D). The pair (A, A') tightly brackets the boundary, providing balanced coverage of both sides
- **Core assumption**: Adversarial attacks find minimal perturbations that cross the true decision boundary, not spurious gradient directions
- **Evidence anchors**: Abstract states pairs "bracket the local decision boundary"; section 4 describes the two-step process creating pairs that "tightly bracket the target boundary"

### Mechanism 2
- **Claim**: Constraining neighborhood sampling to a small radius (r=0.2 optimal) produces higher-fidelity local approximations
- **Mechanism**: Sample noise z ~ N(0,I), rescale to ensure ||δ||₂ ≤ r, then add to x_test. This keeps neighborhood points in regions where linear approximation is valid
- **Core assumption**: The local decision boundary can be approximated linearly within small radius r
- **Evidence anchors**: Section 7.1 shows fidelity highest at r=0.2 achieving R² of 0.908, declining as r increases

### Mechanism 3
- **Claim**: Training linear models on adversarial pairs achieves higher fidelity with fewer queries (300 vs. 5000) than LIME's weighted random sampling
- **Mechanism**: Adversarial pairs lie near the boundary where predictions change, making each sample more informative for capturing the decision surface
- **Core assumption**: Points near the boundary are more valuable for learning decision boundaries than randomly distributed points
- **Evidence anchors**: Section 7.4 shows ABLE_FGSM requires only 300 queries compared to 5000 for baselines like LIME

## Foundational Learning

- **Concept: Adversarial Attacks (FGSM, PGD, DeepFool, HopSkipJump)**
  - Why needed here: ABLE requires generating minimal perturbations that flip predictions to bracket boundaries. Choice of attack affects both fidelity and runtime
  - Quick check question: Why does PGD typically find smaller perturbations than FGSM, and what is the computational tradeoff?

- **Concept: Local vs. Global Explanations**
  - Why needed here: ABLE is a local explanation method; understanding this scope clarifies why it explains individual predictions rather than overall model behavior
  - Quick check question: Why might a globally accurate feature importance ranking fail to explain an individual prediction?

- **Concept: Fidelity vs. Stability in Explanations**
  - Why needed here: ABLE claims simultaneous improvement in both; these are typically in tension
  - Quick check question: If you increase neighborhood radius r, would you expect local fidelity to increase or decrease, and why?

## Architecture Onboarding

- **Component map**: Neighborhood Generator -> Adversarial Attack Module -> Local Model Trainer
- **Critical path**: 1. Generate n neighborhood points around x_test with ||δ||₂ ≤ r; 2. For each point, generate adversarial pair via forward attack → A, reverse attack → A'; 3. Train linear model on 2n adversarial points
- **Design tradeoffs**: Smaller r (0.2) → higher fidelity but narrower explanation scope; FGSM → fastest (0.12-0.48s) but slightly lower fidelity than PGD/etDF; HSJ → black-box compatible but slower (2-4s)
- **Failure signatures**: TabNet on Adult/Car datasets showed lower fidelity than baselines; large r values degrade fidelity; linear surrogate cannot capture critical feature interactions
- **First 3 experiments**: 1. Replicate Table 3 sweep to confirm r=0.2, n=150 optimality; 2. Compare ABLE_HSJ vs. gradient-based variants on unavailable gradient model; 3. Test on Car Evaluation (4 classes) to confirm targeted reverse attacks work in multi-class settings

## Open Questions the Paper Calls Out
- **Open Question 1**: Can ABLE be effectively adapted for high-dimensional, unstructured data domains like images and text? The authors explicitly state this as future work, noting current validation only on six UCI tabular datasets.
- **Open Question 2**: Does integrating perceptual distance metrics (e.g., LPIPS, SSIM) into the perturbation process improve explanation quality for image data? The paper suggests applying perceptually motivated distance metrics for image data but hasn't tested this.
- **Open Question 3**: How can adversarial search techniques be optimized for complex models like TabNet where finding minimal perturbations is currently challenging? The authors observe performance drops on TabNet and call for developing more robust adversarial search techniques.

## Limitations
- Performance drops on complex models like TabNet suggest current attack methods struggle with attention-based architectures
- The method's generalizability to non-differentiable models (decision trees, random forests) remains unexplored
- The linear approximation assumption may break down for models with highly curved decision boundaries

## Confidence
- **High Confidence**: Experimental methodology is sound with proper statistical evaluation across multiple datasets and model architectures; fidelity and stability improvements are well-supported
- **Medium Confidence**: Mechanism explanation for why adversarial pairs improve local fidelity is theoretically plausible but lacks direct empirical validation of the "bracketing" claim
- **Low Confidence**: Generalizability to non-differentiable models and extremely high-dimensional data is not established

## Next Checks
1. **Attack Robustness Test**: Systematically measure the success rate of forward-reverse attack pairs across all datasets and model architectures, reporting the percentage of cases where either attack fails to converge within the budget
2. **Non-Differentiable Model Extension**: Implement ABLE_HSJ on a random forest or XGBoost model and compare its fidelity and stability against differentiable model results to validate black-box attack pathway
3. **Curvature Sensitivity Analysis**: For models with known high-curvature decision boundaries, systematically vary radius r and measure the rate of fidelity decay to empirically quantify the linear approximation assumption's limits