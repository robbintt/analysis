---
ver: rpa2
title: Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory
  Study on JPEG
arxiv_id: '2508.12833'
source_url: https://arxiv.org/abs/2508.12833
tags:
- data
- compression
- quality
- learning
- storage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically investigates the trade-off between data
  quantity and quality under storage constraints for on-device machine learning. Using
  CIFAR-10 and a simple CNN architecture, the study systematically varies both the
  amount of training data and its compression quality (JPEG).
---

# Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG

## Quick Facts
- arXiv ID: 2508.12833
- Source URL: https://arxiv.org/abs/2508.12833
- Reference count: 20
- Primary result: Optimal model accuracy under storage constraints requires a dynamic balance between data quantity and quality that varies with the storage budget

## Executive Summary
This paper empirically investigates the trade-off between data quantity and quality under storage constraints for on-device machine learning. Using CIFAR-10 and a simple CNN architecture, the study systematically varies both the amount of training data and its compression quality (JPEG). The results show that optimal model accuracy depends on a dynamic balance between data quantity and quality that varies with the storage budget, with intermediate compression levels sometimes outperforming both low and high quality data. Critically, the study reveals that individual data samples exhibit differential sensitivity to compression - some samples maintain performance even under heavy compression while others degrade significantly.

## Method Summary
The study uses CIFAR-10 with a 3-layer CNN trained for 30 epochs. Researchers systematically vary data quantity (10-100% of dataset) and JPEG compression quality (1-100%) to explore storage-constrained learning. For each storage budget, they evaluate multiple combinations of quantity and quality to identify optimal configurations. The analysis includes K-means clustering (K=5) on loss differences to categorize sample sensitivity to compression. Models are evaluated on both original and compressed test sets to characterize performance across different quality levels.

## Key Results
- Optimal model accuracy under storage constraints requires a dynamic balance between data quantity and quality that varies nonlinearly with the storage budget
- Individual data samples exhibit heterogeneous sensitivity to compression, with some maintaining performance under heavy compression while others degrade significantly
- Intermediate compression levels (e.g., quality 20-50%) can sometimes outperform both low and high quality data, depending on the storage budget

## Why This Works (Mechanism)

### Mechanism 1: Budget-Dependent Quantity-Quality Trade-off
- Claim: Optimal model accuracy under storage constraints requires a dynamic balance between data quantity and quality that varies nonlinearly with the storage budget.
- Core assumption: The relationship between storage allocation and model performance follows a non-monotonic curve that cannot be captured by fixed heuristics.
- Evidence anchors: Under tight budgets (<2000), increasing quantity tends to outperform prioritizing quality; optimal ratio shifts toward higher quality as budget increases.

### Mechanism 2: Compression-Induced Feature Shift
- Claim: Training with compressed data causes models to attend to different image regions compared to models trained on high-quality data.
- Core assumption: SHAP-based feature attribution differences reflect meaningful shifts in learned representations that generalize to test-time behavior.
- Evidence anchors: IoU decreases as quality drops, indicating a shift in relied-on evidence; low-quality features being inherently embedded within high-quality data.

### Mechanism 3: Per-Sample Differential Sensitivity
- Claim: Individual data samples exhibit heterogeneous sensitivity to compression, with some maintaining performance under heavy compression while others degrade significantly.
- Core assumption: Loss difference between compressed and uncompressed versions serves as a valid proxy for sample importance in adaptive compression policies.
- Evidence anchors: Cluster 5 demonstrates a unique inverse trend, with decreasing loss values as compression quality deteriorates.

## Foundational Learning

- Concept: **Storage budget as a first-class constraint**
  - Why needed here: Unlike compute or memory constraints commonly studied in on-device ML, storage constraints require joint optimization over both how many samples to keep and at what fidelity.
  - Quick check question: Can you explain why doubling storage budget doesn't necessarily mean you should double the number of training samples?

- Concept: **Lossy compression trade-offs (quality factor vs. file size)**
  - Why needed here: JPEG quality factor provides a controllable knob for trading fidelity against storage, but the relationship to downstream model performance is nonlinear and dataset-dependent.
  - Quick check question: If JPEG quality 10% produces 20KB images and quality 50% produces 100KB images, which should you choose for a 1GB budget if you have 50,000 training samples?

- Concept: **Sample-wise importance scoring (gradient-based, uncertainty-based, loss-based)**
  - Why needed here: The paper motivates adaptive compression policies that require estimating each sample's sensitivity or importance without exhaustively evaluating all compression levels.
  - Quick check question: What properties would a good "compression sensitivity proxy" need to have for real-time on-device use?

## Architecture Onboarding

- Component map: Data ingestion -> compression module (JPEG encoder with configurable quality) -> storage buffer (fixed capacity) -> training loop -> model -> sensitivity estimator (offline analysis component) -> compression policy allocator
- Critical path: 1) Define storage budget B, 2) For each candidate configuration (|D|, Q): estimate storage = |D| Ã— Q, 3) Train model on compressed subset, 4) Evaluate on uncompressed test set, 5) Identify optimal (|D|, Q) pair for given budget
- Design tradeoffs: More samples at lower quality vs. fewer samples at higher quality; uniform compression (simple, suboptimal) vs. per-sample adaptive compression (complex, potentially optimal); training overhead: compressed data retains more samples but requires decoding during training
- Failure signatures: Model trained only on high-quality data fails dramatically on compressed test inputs (R1 region behavior); model trained only on very low-quality data cannot leverage high-frequency features in high-quality test data; uniform dropping/compression underperforms budget-aware optimization by 5-10% accuracy
- First 3 experiments: 1) Reproduce Figure 1: For your dataset, sweep (|D|, Q) combinations and plot accuracy vs. storage usage to identify budget-dependent optimal regions, 2) Reproduce Figure 4: Train models at different quality levels and evaluate across test-time quality levels to characterize R1/R2/R3 regions for your domain, 3) Implement sensitivity clustering: Compute per-sample loss differences across compression levels and cluster samples to identify compression-robust vs. compression-sensitive cohorts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed quantity-quality trade-offs and differential sample sensitivity generalize to high-complexity datasets (e.g., ImageNet) and non-visual modalities?
- Basis in paper: The authors explicitly list the focus on CIFAR-10 and JPEG as a limitation, calling for future validation on diverse benchmarks (CIFAR-100, ImageNet) and modalities (audio/video).
- Why unresolved: The current study relies solely on a controlled, low-resolution benchmark, leaving the transferability of these storage-aware principles to higher-dimensional data unconfirmed.
- What evidence would resolve it: Empirical results from identical experiments on high-resolution datasets or audio streams showing similar non-uniform sensitivity patterns.

### Open Question 2
- Question: Can lightweight sensitivity proxies (e.g., uncertainty or gradient-based scores) accurately predict sample-wise compression impact for adaptive policies?
- Basis in paper: The discussion suggests employing "lightweight sensitivity proxies" to approximate performance impact, noting that "developing calibrated proxies... remains important future work."
- Why unresolved: The paper identifies sensitivity via post-hoc clustering analysis rather than providing a predictive mechanism usable during live training.
- What evidence would resolve it: A proposed proxy metric that correlates strongly with the empirical sensitivity clusters and enables efficient, knapsack-style data selection.

### Open Question 3
- Question: How can systems effectively mitigate the computational overhead of training on retained compressed data versus dropping it?
- Basis in paper: The authors acknowledge that retaining compressed data increases computational overhead and suggest exploring "hybrid policies" or "data consolidation" as future work.
- Why unresolved: The paper focuses on storage budgets; it does not model or optimize for the increased training time or energy costs associated with processing the extra data.
- What evidence would resolve it: A system design or algorithm that balances storage savings against training latency, demonstrating that the accuracy gains justify the computational cost.

## Limitations

- Architecture details for the CNN (layer sizes, filters, hyperparameters) are underspecified, requiring assumptions for reproduction
- The subset selection strategy (random vs. class-balanced) is not explicitly defined
- No validation of whether observed sample-wise sensitivity patterns generalize beyond CIFAR-10

## Confidence

- **High confidence:** The budget-dependent quantity-quality trade-off mechanism (Mechanism 1) is empirically demonstrated and theoretically sound
- **Medium confidence:** The compression-induced feature shift (Mechanism 2) is supported by SHAP analysis but requires further validation on different datasets
- **Low confidence:** The per-sample differential sensitivity characterization (Mechanism 3) appears novel but lacks comparison with alternative sensitivity estimation methods

## Next Checks

1. **Architecture sensitivity analysis:** Vary CNN architecture depth and complexity to test whether the quantity-quality trade-off pattern persists across model capacities
2. **Cross-dataset replication:** Apply the experimental protocol to a dataset with different characteristics (e.g., ImageNet-32 or TinyImageNet) to assess generalizability of the three-region performance pattern
3. **Alternative sensitivity proxies:** Compare the loss-difference-based sensitivity metric against gradient-based and uncertainty-based importance scores to validate the robustness of per-sample sensitivity clustering