---
ver: rpa2
title: Uncovering Fairness through Data Complexity as an Early Indicator
arxiv_id: '2504.05923'
source_url: https://arxiv.org/abs/2504.05923
tags:
- bias
- complexity
- metrics
- fairness
- differences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how differences in classification complexity
  between privileged and unprivileged groups influence fairness outcomes. Using synthetic
  datasets with various bias-generating mechanisms, we measure complexity differences
  between groups and their association with fairness metrics across multiple classifiers.
---

# Uncovering Fairness through Data Complexity as an Early Indicator

## Quick Facts
- arXiv ID: 2504.05923
- Source URL: https://arxiv.org/abs/2504.05923
- Reference count: 18
- Key outcome: This study explores how differences in classification complexity between privileged and unprivileged groups influence fairness outcomes. Using synthetic datasets with various bias-generating mechanisms, we measure complexity differences between groups and their association with fairness metrics across multiple classifiers. Association rule mining reveals that class imbalance differences (C2) strongly correlate with fairness disparities, particularly affecting Statistical Parity. Network density and decision boundary differences also emerge as key indicators of unfairness. These patterns hold in real-world datasets, validating that subgroup complexity differences can serve as early indicators of potential fairness challenges, guiding targeted interventions in machine learning pipelines.

## Executive Summary
This study establishes that differences in classification complexity between privileged and unprivileged groups serve as early indicators of fairness violations in machine learning. Through controlled synthetic experiments and real-world validation, the authors demonstrate that specific complexity differences—particularly class imbalance (C2), decision boundary density (N1), and network density—consistently predict fairness disparities across multiple classifiers. Association rule mining reveals these complexity differences appear as antecedents in rules predicting unfairness with high confidence, suggesting complexity metrics could guide targeted fairness interventions before model training.

## Method Summary
The methodology employs synthetic datasets generated via the Baumann et al. (2023) framework to control bias parameters, combined with 30 real-world datasets. For each dataset, protected attributes split data into privileged/unprivileged subgroups. The approach computes 14 classification complexity metrics per subgroup, calculates absolute differences (CMD), and trains three classifiers (Logistic Regression, Decision Tree, K-Nearest Neighbors) with 10-fold cross-validation. Fairness metrics (Statistical Parity, Equal Opportunity, Predictive Parity) are evaluated, and association rule mining (Apriori algorithm) identifies patterns linking CMDs to fairness violations, using thresholds of CMD > 0.1 and fairness outside [-0.1, 0.1].

## Key Results
- Class imbalance differences (C2) strongly correlate with Statistical Parity violations, appearing as antecedents in rules with 97% confidence across all classifiers
- Decision boundary complexity differences (N1) predict Equal Opportunity disparities, particularly for KNN and Logistic Regression (lift values 3.49 and 2.37)
- Network density differences correlate with Statistical Parity unfairness for neighborhood-based classifiers, with lift up to 2.14 in real-world validation
- Association rules show higher lift and confidence values in real-world datasets compared to synthetic ones, suggesting complexity differences may be stronger indicators in practical scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differences in class imbalance between privileged and unprivileged groups serve as early indicators of Statistical Parity violations.
- Mechanism: When one subgroup has a markedly different class distribution than another (quantified by C2 imbalance ratio), classifiers optimize for the dominant class pattern, systematically disadvantaging the subgroup with the minority distribution. The association rule mining revealed C2 differences appear as antecedents in rules predicting SP unfairness with 97% confidence across all three classifiers tested.
- Core assumption: Classifiers minimize global error, so subgroups with imbalanced class ratios receive suboptimal decision boundaries relative to their true distribution.
- Evidence anchors:
  - [abstract] "Association rule mining reveals that class imbalance differences (C2) strongly correlate with fairness disparities, particularly affecting Statistical Parity."
  - [Section 4.3] "Differences in class imbalance between the privileged and unprivileged groups (C2) are consistently associated with fairness disparities for all consequent... The strongest association is with SP for the three methods (DT, KN, and LR)."
  - [corpus] Weak direct corpus support; related work (IFFair) addresses reweighting for fairness but does not establish the C2-SP link directly.
- Break condition: If class imbalance differences exist but SP remains within fair bounds [−0.1, 0.1], the mechanism does not hold; the classifier may be robust to imbalance or other factors dominate.

### Mechanism 2
- Claim: Decision boundary complexity differences (fraction of borderline points, N1) between groups predict Equal Opportunity disparities, particularly for instance-based and linear classifiers.
- Mechanism: N1 measures the proportion of points lying near the decision boundary. When N1 differs substantially between groups, one group's instances are more frequently borderline—harder to classify correctly—leading to higher false negative rates for that group and thus EO violations. The association rules show N1 differences predict EO for K-Nearest Neighbors (lift 3.49) and Logistic Regression (lift 2.37).
- Core assumption: Borderline instances are more likely misclassified, and if borderline prevalence differs by group, error rates (specifically false negatives) will diverge.
- Evidence anchors:
  - [abstract] "Network density and decision boundary differences also emerge as key indicators of unfairness."
  - [Section 4.3] "Differences in the decision boundary cases (N1) show significant associations with SP and EO for the KN and LR algorithm."
  - [corpus] No direct corpus evidence on N1-EO linkage; this appears to be a novel contribution of the paper.
- Break condition: If N1 differences exist but EO is fair, the classifier may have sufficient margin or regularization to handle borderline cases equitably; or the positive class distribution compensates.

### Mechanism 3
- Claim: Network density differences between subgroups correlate with Statistical Parity unfairness, especially for neighborhood-based classifiers.
- Mechanism: Density captures how tightly packed instances are in feature space. Lower density (higher sparsity) within one subgroup means fewer nearby same-class neighbors for support, leading KNN-style classifiers to rely more on cross-class neighbors, increasing misclassification and favorable outcome disparities. Rules show density differences predict SP unfairness with lift up to 2.14 in real-world validation.
- Core assumption: K-Nearest Neighbors' predictions depend on local neighborhood composition; sparser groups have less reliable local class signals.
- Evidence anchors:
  - [Section 4.3] "The average density of network differences (density) also shows an association with SP for the KN algorithm."
  - [Section 5, Table 6] "Notably, the highest lift value (2.14) is observed in rules where density is the antecedent, particularly influencing SP in K-Nearest Neighbors (KN)."
  - [corpus] Weak corpus support; neighbor papers on fairness do not address density-fairness relationships.
- Break condition: If density differences exist but SP is fair, the classifier may use a sufficiently large k or distance weighting that mitigates sparsity effects.

## Foundational Learning

- Concept: Classification complexity metrics (per Lorena et al. 2019 taxonomy)
  - Why needed here: The entire methodology depends on computing complexity metrics (F1v, N1, N2, C2, density, etc.) for each subgroup and comparing them. Without understanding what these measure, results are uninterpretable.
  - Quick check question: Given a binary dataset split by protected attribute, which metrics quantify class overlap vs. class imbalance?

- Concept: Group fairness metrics (Statistical Parity, Equal Opportunity, Predictive Parity)
  - Why needed here: These are the outcome variables the paper aims to predict using complexity differences. SP tests independence (equal positive prediction rates), EO tests separation (equal true positive rates), PP tests sufficiency (equal precision).
  - Quick check question: If a classifier has equal TPR for both groups but different positive prediction rates, which metric(s) flag unfairness?

- Concept: Association rule mining (support, confidence, lift)
  - Why needed here: The paper uses Apriori-style rule mining to discover "complexity difference → unfairness" patterns. Understanding these metrics determines whether rules are meaningful or spurious.
  - Quick check question: A rule has lift=1.5 but support=0.05. Is this practically useful? Why or why not?

## Architecture Onboarding

- Component map:
  Data layer -> Complexity computation layer -> Fairness evaluation layer -> Rule mining layer
  Synthetic/real datasets -> 15 complexity metrics per subgroup -> 3 classifiers + fairness metrics -> Apriori association rules

- Critical path:
  1. Dataset ingestion → protected attribute split
  2. Parallel complexity metric computation per subgroup
  3. CMD calculation (absolute difference)
  4. Classifier training + fairness metric extraction
  5. Association rule mining on (CMD, fairness) pairs
  6. Validation on held-out real-world datasets

- Design tradeoffs:
  - Synthetic data enables controlled bias injection but may not capture real-world complexity combinations
  - Using absolute differences discards directionality (which group is harder); could mask asymmetric effects
  - Thresholds (CMD > 0.1, fairness outside [-0.1, 0.1]) are arbitrary but based on AIF360 conventions
  - Three classifiers cover different inductive biases but are not exhaustive

- Failure signatures:
  - All CMD values near zero but fairness violations present: bias is not complexity-driven (e.g., direct proxy use, label bias)
  - High CMD but fair outcomes: classifier is robust or complexity type is irrelevant to that classifier
  - Rules have high lift but very low support: patterns exist but are rare, not generalizable
  - Real-world validation fails: synthetic bias scenarios do not match real data distributions

- First 3 experiments:
  1. Replicate C2-SP association: Take a dataset with known class imbalance differences (e.g., Adult income by sex), compute C2 difference, train LR/DT/KNN, verify SP correlates with C2 magnitude.
  2. Test N1-EO on a different classifier: Add a non-linear classifier (e.g., Random Forest) to test whether N1 differences predict EO violations beyond KN/LR.
  3. Intervention test: Apply SMOTE or class reweighting to reduce C2 difference, retrain, and measure SP improvement—if C2 is causal, reducing it should reduce SP disparity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can interventions that specifically minimize complexity differences (e.g., resampling to fix imbalance or feature engineering to reduce overlap) establish a causal link to improved fairness outcomes?
- Basis in paper: [explicit] Section 6.5 states that while association rules identify patterns, "they do not establish strict causality," and suggests validating interventions by "re-running experiments after removing or mitigating complexity sources."
- Why unresolved: The current study identifies correlations (associations) between complexity differences and unfairness but does not experimentally confirm that reducing the complexity difference causes the model to become fairer.
- What evidence would resolve it: Experiments showing that selectively reducing specific complexity differences (like C2 or N1) leads to a statistically significant improvement in fairness metrics for the same model architecture.

### Open Question 2
- Question: Do the relationships between subgroup complexity differences and fairness metrics hold in settings with multiple protected attributes or continuous protected variables?
- Basis in paper: [explicit] Section 6.5 lists the limitation that the analysis "addressed binary protected attributes" and suggests future research could "investigate multiattribute fairness or continuous protected attributes."
- Why unresolved: The current methodology relies on a binary split (privileged vs. unprivileged) to calculate complexity differences; it is unclear how to define or measure these differences when groups intersect or when the protected attribute is continuous (e.g., age).
- What evidence would resolve it: An extension of the complexity-difference framework to intersectional groups (e.g., race and gender combined) or continuous variables, showing consistent associations with fairness metrics.

### Open Question 3
- Question: Do complexity metrics remain reliable indicators of unfairness in high-dimensional spaces or large-scale datasets?
- Basis in paper: [explicit] Section 6.5 notes that "some complexity metrics may become less reliable in high-dimensional spaces or large-scale data" and identifies extending these metrics as an important research path.
- Why unresolved: Many utilized metrics (e.g., neighborhood or network metrics) suffer from the curse of dimensionality (where distances become uniform), potentially rendering the identified association rules invalid for complex, modern datasets.
- What evidence would resolve it: Empirical testing of the derived association rules on high-dimensional datasets (e.g., image or genomics data) to see if high lift and confidence values are maintained.

## Limitations
- Synthetic datasets may not capture all real-world complexity combinations
- Using absolute differences discards directionality, potentially masking asymmetric effects
- Thresholds (CMD > 0.1, fairness outside [-0.1, 0.1]) are somewhat arbitrary despite AIF360 conventions

## Confidence
- High: C2-Statistical Parity mechanism (97% confidence rules across all classifiers)
- Medium: N1-Equal Opportunity link (novel contribution with strong lift values but no corpus precedent)
- Medium: Density-fairness relationship (empirical but not yet theoretically grounded)

## Next Checks
1. Test intervention causality by applying SMOTE or class reweighting to reduce C2 differences, retraining models, and measuring SP improvement.
2. Extend analysis to additional classifiers (e.g., Random Forest, SVM) to determine if complexity-fairness patterns generalize beyond the three tested.
3. Analyze directional complexity differences (privileged harder vs. unprivileged harder) to identify asymmetric fairness patterns that absolute differences may mask.