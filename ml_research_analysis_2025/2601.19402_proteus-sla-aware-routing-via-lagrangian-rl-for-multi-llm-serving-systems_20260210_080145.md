---
ver: rpa2
title: 'PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems'
arxiv_id: '2601.19402'
source_url: https://arxiv.org/abs/2601.19402
tags:
- accuracy
- cost
- routing
- policy
- proteus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROTEUS, a router that accepts accuracy targets
  as direct runtime input, enabling SLA-aware routing for multi-LLM serving systems.
  Unlike existing approaches that require offline parameter tuning, PROTEUS uses Lagrangian
  dual control with a learned dual variable to enforce accuracy constraints during
  training, allowing a single trained model to serve the full accuracy spectrum from
  0.85 to 0.95.
---

# PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems

## Quick Facts
- arXiv ID: 2601.19402
- Source URL: https://arxiv.org/abs/2601.19402
- Reference count: 2
- Primary result: 100% floor compliance on RouterBench and SPROUT with 89.8% cost savings vs best fixed model

## Executive Summary
PROTEUS introduces a Lagrangian reinforcement learning approach for SLA-aware routing in multi-LLM serving systems. Unlike existing routers that require offline parameter tuning, PROTEUS accepts accuracy targets τ as direct runtime input and enforces them during training via a learned dual variable λ. The method achieves perfect floor compliance while maintaining high runtime adaptability through τ-μ correlation of 0.97-0.98, enabling a single trained model to serve the full accuracy spectrum from 0.85 to 0.95.

## Method Summary
PROTEUS uses PPO with a τ-conditioned Beta policy outputting quality preference μ ∈ [0,1]. The Lagrangian dual variable λ tracks constraint violations during training: λ_{t+1} = [λ_t + η_λ·(τ - p̄_batch)]^+, with η_λ=0.4. Session-based training fixes τ per session for stable constraint learning. The routing score s_i = p_i(x) + μ·b_i - (1-μ)^γ·c_i uses learnable γ∈[2,8] for dataset-specific cost-quality tradeoffs. A DeBERTa-v3-small encoder produces query embeddings, while a separate performance prediction head estimates per-model accuracy p_i(x).

## Key Results
- 100% floor compliance on both RouterBench and SPROUT benchmarks
- τ-μ correlation of 0.973 (RouterBench) and 0.981 (SPROUT)
- Accuracy within 1.3% (RouterBench) and 4.6% (SPROUT) of oracle performance
- 89.8% cost savings versus best fixed model across the accuracy spectrum

## Why This Works (Mechanism)

### Mechanism 1: Lagrangian Dual Variable Enforcement
The learned Lagrangian dual variable λ enforces accuracy constraints during training by converting constraint violations into feedback signals that condition the policy network. During training, batch accuracy p̄_batch is compared against target τ. When p̄ < τ, λ increases via λ_{t+1} = [λ_t + η_λ·(τ - p̄_batch)]^+, penalizing cost-seeking behavior. This λ is injected into the policy network during training, teaching it to output higher μ (quality preference) when λ is high. By inference, the policy has learned the τ→μ mapping and λ is fixed at 1.0. The core assumption is that the constraint violation signal generalizes across the τ spectrum; the policy learns a continuous mapping rather than memorizing discrete operating points.

### Mechanism 2: Runtime τ Conditioning
Conditioning the policy on τ as a runtime input enables a single trained model to serve the full accuracy spectrum without retraining or parameter sweeping. The policy network π_θ(μ|x,τ) takes (query embedding, τ) as input and outputs quality preference μ ∈ [0,1]. This decouples the quality-cost tradeoff from model selection—the scoring function translates μ into model selection. High τ → high μ → expensive models; low τ → low μ → cheap models. The core assumption is that the relationship between τ and optimal μ is learnable and approximately monotonic across query types.

### Mechanism 3: Learnable Cost Sensitivity
The non-linear cost term (1-μ)^γ·c_i with learnable γ enables dataset-specific cost-quality tradeoffs that fixed linear weighting cannot achieve. The scoring function s_i = p_i(x) + μ·b_i - (1-μ)^γ·c_i uses γ ∈ [2,8] to control cost sensitivity. At γ=2, cost matters quadratically; at γ=8, cost matters only when μ≈0 (near-binary). This lets the system discover appropriate sensitivity rather than imposing a fixed tradeoff. The core assumption is that optimal cost sensitivity varies across model pools with different cost ranges.

## Foundational Learning

- **Concept: Lagrangian Duality for Constrained Optimization**
  - Why needed here: PROTEUS formulates routing as constrained RL where cost is minimized subject to accuracy ≥ τ. Understanding how dual variables convert hard constraints into penalty terms in the objective is essential.
  - Quick check question: If λ = 0 at convergence, what does that imply about constraint satisfaction?

- **Concept: Policy Gradient Methods (PPO)**
  - Why needed here: PROTEUS trains with PPO, which clips gradient updates to prevent destabilizing policy changes. Understanding why supervised learning is unsuitable (no ground-truth labels for "correct" routing) clarifies the RL choice.
  - Quick check question: Why does the paper use REINFORCE-style gradients rather than a critic for this single-step MDP?

- **Concept: Service Level Objectives (SLOs) vs. Parameters**
  - Why needed here: The core innovation is accepting τ (an outcome) as input rather than μ or α (parameters). Understanding the operational difference between specifying "I want 95% accuracy" vs. "set threshold to 0.7" is central.
  - Quick check question: Why is the relationship between parameters and outcomes described as "non-monotonic and dataset-dependent"?

## Architecture Onboarding

- **Component map:** Query → DeBERTa-v3-small → 256-dim embedding z → Policy Network (MLP) → μ → Scoring Function → argmax → Model selection
- **Critical path:** 1) Query x arrives with SLA-derived τ, 2) DeBERTa encodes x → z (adds ~2-9ms depending on batching), 3) Policy network takes (z, τ, λ=1.0 at inference) → sample μ from Beta, 4) Compute s_i for all K models, select argmax, 5) Route to selected model, return response
- **Design tradeoffs:** DeBERTa-v3-small vs. larger encoders: 5× faster than RoBERTa-base, sufficient for routing decisions, but may limit representation power for complex queries. Continuous μ vs. categorical over K models: Enables 1D exploration and smooth interpolation, but requires the scoring function to translate preference into selection. Session-based τ sampling vs. per-batch: More stable constraint signals, but slower coverage of the τ spectrum
- **Failure signatures:** Floor violations (accuracy < τ): Likely λ learning rate too low or insufficient training steps for dual convergence. Excessive overshoot (accuracy >> τ): λ may not be decreasing when p̄ > τ; check η_λ and [·]^+ implementation. High cost with moderate τ: γ may be too low (cost penalty too weak) or b_i values incorrectly learned. Poor τ-μ correlation: Policy network may not be properly conditioning on τ; verify τ is concatenated with embedding
- **First 3 experiments:** 1) **Dual variable convergence check:** Monitor λ over training for fixed τ sessions. λ should increase when p̄ < τ and decrease when p̄ > τ, stabilizing near constraint boundary. 2) **τ-μ correlation validation:** On held-out queries, plot τ vs. mean μ output. Expect near-linear relationship with r > 0.95; deviations indicate conditioning failure. 3) **Ablation on cost range:** Train on a synthetic model pool with 2× cost variation vs. 10× variation. Expect learnable γ to matter more in the latter case, matching the RouterBench vs. SPROUT finding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can routing jointly optimize model selection and reasoning token budgets for models like o1 and DeepSeek-R1?
- Basis in paper: Future Work states: "Reasoning-aware routing extends optimization to jointly select models and allocate reasoning budgets... The policy could output both μ (model preference) and a reasoning budget ρ."
- Why unresolved: Current scoring function (Equation 2) treats cost as static per query, but reasoning models have variable costs that depend on output length, creating a continuous optimization dimension alongside discrete model selection.
- What evidence would resolve it: A modified policy architecture outputting (μ, ρ) pairs, evaluated on reasoning model pools showing whether joint optimization outperforms sequential (select model, then set budget) approaches.

### Open Question 2
- Question: Can model costs be treated as runtime inputs rather than fixed training parameters without sacrificing target compliance?
- Basis in paper: Limitations section notes: "Model costs ci are fixed at training time. If API providers change pricing post-deployment... significant pricing changes would require retraining." Future Work proposes "conditioning the policy on a cost vector c alongside τ."
- Why unresolved: Cost-conditioning adds dimensionality that may disrupt the learned τ→μ mapping, and the policy must generalize to cost configurations unseen during training.
- What evidence would resolve it: Experiments where cost vectors vary at inference (simulating price changes) showing maintained floor compliance without retraining.

### Open Question 3
- Question: Can PROTEUS maintain target compliance under bandit feedback where only the selected model's outcome is observed?
- Basis in paper: Limitations: "PROTEUS trains on ground-truth correctness labels... but production systems lack such labels by default." Future Work proposes "bandit feedback formulations where only the selected model's outcome is observed."
- Why unresolved: The current Lagrangian constraint mechanism compares batch accuracy against τ, but bandit settings provide partial observations; counterfactual estimation introduces variance that may destabilize dual variable updates.
- What evidence would resolve it: Comparative experiments showing floor compliance rates under full-information vs. bandit feedback regimes, with analysis of convergence speed and constraint violation frequency.

## Limitations

- The learned γ parameter becomes suboptimal if model costs change significantly post-deployment, requiring retraining or cost renormalization.
- Session length for τ sampling is not specified, affecting the stability of constraint learning signals.
- The performance prediction head training procedure is underspecified (supervised pre-training, joint training, or specific loss function unclear).

## Confidence

- **High confidence**: Floor compliance (100% on both benchmarks), τ-μ correlation (0.97-0.98), cost savings (89.8% vs best fixed model) - these are direct empirical measurements with clear metrics.
- **Medium confidence**: The mechanism by which Lagrangian dual variable λ enforces constraints during training - while the mathematical formulation is sound, the practical stability and convergence properties across diverse τ distributions would benefit from additional ablation studies.
- **Medium confidence**: The claim that conditioning on τ as runtime input is the key differentiator from prior work - this is supported by the contrast with OmniRouter's poor floor compliance, but the paper doesn't isolate τ-conditioning from other architectural differences.

## Next Checks

1. **Dual variable stability test**: Run training with different η_λ values (0.1, 0.4, 1.0) and monitor λ convergence patterns. Verify that λ stabilizes near the constraint boundary without oscillations or divergence.

2. **Cost sensitivity ablation**: Train PROTEUS on model pools with varying cost ranges (2× vs 10×) and measure how learnable γ performance compares to fixed γ values. This would validate the paper's claim about dataset-specific cost-quality tradeoffs.

3. **τ distribution robustness**: Evaluate PROTEUS when τ values during inference follow a different distribution than training (e.g., only extreme values τ∈{0.85,0.95}). Measure floor compliance and accuracy-τ correlation to assess generalization limits.