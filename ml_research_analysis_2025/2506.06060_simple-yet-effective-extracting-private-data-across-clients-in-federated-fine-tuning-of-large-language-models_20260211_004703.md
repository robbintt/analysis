---
ver: rpa2
title: 'Simple Yet Effective: Extracting Private Data Across Clients in Federated
  Fine-Tuning of Large Language Models'
arxiv_id: '2506.06060'
source_url: https://arxiv.org/abs/2506.06060
tags:
- data
- federated
- prefix
- learning
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a realistic data extraction attack on federated\
  \ large language models (FedLLMs), where an attacker with access to only a single\
  \ client\u2019s data aims to extract previously unseen PII from other clients. Unlike\
  \ prior \"verbatim\" attacks, the method leverages contextual prefixes from the\
  \ attacker\u2019s own dataset to generalize across clients."
---

# Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models

## Quick Facts
- arXiv ID: 2506.06060
- Source URL: https://arxiv.org/abs/2506.06060
- Reference count: 40
- Key outcome: Attack achieves up to 56.57% coverage of victim-exclusive PII using only attacker's local prefixes

## Executive Summary
This paper demonstrates a realistic data extraction attack on federated large language models (FedLLMs) where an attacker with access to only a single client's data can extract previously unseen PII from other clients. Unlike prior verbatim attacks, the method leverages contextual prefixes from the attacker's own dataset to generalize across clients. Three simple yet effective attack strategies are introduced: PII-contextual prefix sampling, frequency-prioritized prefix sampling, and latent association fine-tuning. Experiments on a real-world legal dataset annotated with PII show that the attacks can achieve up to 56.57% coverage of victim-exclusive PII, with "Address," "Birthday," and "Name" being the most vulnerable types.

## Method Summary
The attack operates in a federated learning setting where multiple clients collaboratively train a global model without sharing raw data. The attacker extracts text segments immediately preceding PII instances in their own dataset as contextual prefixes. These prefixes serve as prompts to query the global model, exploiting the model's memorization of similar prefix-PII patterns from other clients during training. Three strategies are employed: extracting all contextual prefixes (Pc), frequency-prioritized sampling (Pf ≥σa), and latent association fine-tuning (LAFt) to strengthen prefix-PII associations. The method achieves cross-client PII extraction without requiring access to the victim's training data.

## Key Results
- Cross-client extraction achieves up to 56.57% coverage of victim-exclusive PII
- "Address," "Birthday," and "Name" are the most vulnerable PII types
- Simple PII masking reduces but does not eliminate extraction (2017 vs 2034 VxPII)
- Latent association fine-tuning increases coverage from 22.93% to 28.30% on Qwen1-8B

## Why This Works (Mechanism)

### Mechanism 1: PII-Contextual Prefix Sampling
The attacker extracts text segments immediately preceding PII instances in their own dataset. These prefixes serve as prompts to query the global model. Because the federated model has seen similar prefix-PII patterns from other clients during training, the model may complete the prompt with PII it memorized from those clients. Core assumption: Similar syntactic and semantic contexts precede PII across different clients in the same domain.

### Mechanism 2: Frequency-Prioritized Prefix Sampling
The attacker ranks all prefix substrings ending before PII instances by frequency. High-frequency prefixes are queried first under a fixed budget. The hypothesis is that frequent prefixes capture domain-specific patterns that are more likely to appear across clients. Core assumption: Frequency in the attacker's corpus correlates with cross-client generalizability.

### Mechanism 3: Latent Association Fine-Tuning
Construct a dataset by pairing frequent prefixes with known PII from the attacker's data. Fine-tune the model using causal language modeling to minimize the negative log-likelihood of PII tokens given the prefix. The fine-tuned model is then used for extraction. Core assumption: The model's latent ability to associate prefix distributions with PII distributions can be amplified via supervised fine-tuning.

## Foundational Learning

- **Federated Learning (FedAvg)**: Why needed: The attack operates in a federated setting where multiple clients collaboratively train a global model without sharing raw data. Understanding how local updates aggregate into a global model clarifies why PII from one client can leak to another through model weights. Quick check: Can you explain why FedAvg aggregation does not guarantee privacy, even though raw data is never transmitted?

- **LLM Memorization and Training Data Extraction**: Why needed: The attack exploits LLMs' tendency to memorize training data. Prior work on verbatim extraction assumes access to training prefixes; this paper generalizes to cross-client extraction using contextual prefixes. Quick check: What is the difference between verbatim extraction and the cross-client extraction proposed in this paper?

- **Personally Identifiable Information (PII) Taxonomy**: Why needed: The paper annotates PII across 7 major categories and 36 subcategories aligned with GDPR, CCPA, and CPIS. Understanding which PII types are most vulnerable (Address, Birthday, Name) informs both attack and defense strategies. Quick check: Why might complex PII types (e.g., Medication Record) be harder to extract than simple structured types (e.g., Birthday)?

## Architecture Onboarding

- **Component map**: Legal corpus (CAIL, CJRC, JEC-QA) with PII annotations -> FedAvg aggregation over 10 rounds -> LoRA-based parameter-efficient fine-tuning per client -> Attack layer: three strategies (Pc, Pf ≥σa, LAFt) -> Evaluation layer: CR, EF, VxPII metrics

- **Critical path**: 1) Preprocess and partition legal dataset with PII labels across clients; 2) Run federated fine-tuning to obtain global model θ; 3) Attacker (client 0) extracts prefixes Pc or Pf ≥σa from local data; 4) Optionally apply LAFt to obtain θ′; 5) Query θ or θ′ with prefixes; 6) Collect outputs Y; 7) Match outputs against victim-exclusive PII set using exact prefix matching

- **Design tradeoffs**: Larger prefix sets (Pf ≥1) achieve higher CR (up to 56.57%) but extremely low EF (0.01%); smaller sets (Pc) have lower CR (~22–29%) but higher EF (~0.19–0.24%); LAFt improves CR on Qwen1-8B but not consistently on Baichuan2-7B; simple PII masking reduces document frequency but leaves 2017 VxPII extractable

- **Failure signatures**: Very low efficiency (EF < 0.05%) with high prefix budget suggests diminishing returns; no extraction of complex PII types indicates attack requires structured, short-form PII; cross-client CR drops significantly if attacker and victim share few PII types

- **First 3 experiments**: 1) Baseline extraction with Pc: Designate client 0 as attacker, client 1 as victim, extract Pc with λ=50, query 15 times per prefix, generate 10 tokens, compute CR/EF/VxPII; 2) Frequency-prioritized sampling sweep: Construct Set(SUP(Pc)), sort by descending frequency, sweep prefix budget B exponentially, plot CR and EF against B; 3) LAFt augmentation: Select top-10k frequent prefixes and 10k random PII to construct Dft, fine-tune global model (1 epoch, LR=5e-5), re-run extraction with Pc and compare VxPII overlap

## Open Questions the Paper Calls Out

- **Can defense mechanisms beyond simple masking effectively mitigate cross-client PII extraction in FedLLMs without significantly degrading model utility?** The paper concludes with "the urgent need for stronger defense mechanisms" after showing that simple PII masking only slightly reduces extracted VxPII (2017 vs 2034).

- **Does pairing frequent prefixes with less frequent or underrepresented PII types during Latent Association Fine-tuning enable extraction of rarer PII categories?** Current LAFt uses frequent prefixes paired with randomly sampled PII, which tends to uncover more instances of already-well-extracted types.

- **To what extent does pretraining data contamination contribute to cross-client PII extraction, and can attribution methods distinguish memorization from pretraining versus federated fine-tuning?** The paper notes pretraining contamination is "difficult to eliminate" but the contribution remains ambiguous.

- **Do these attack methods generalize to non-legal domains and to multilingual or cross-lingual federated fine-tuning settings?** All experiments use Chinese legal datasets and Chinese-centric models, limiting claims about general applicability.

## Limitations
- The attack assumes prefix-PII associations are transferable across clients, which may not hold in domains with different document structures
- High coverage requires extremely low efficiency (0.01%), making the attack computationally prohibitive in practice
- LAFt effectiveness varies significantly across models without clear explanation for model-specific differences

## Confidence
- **High confidence**: Experimental methodology is sound with clear metric definitions and reproducible procedures; core finding that FedLLMs leak PII across clients is well-supported
- **Medium confidence**: Three attack mechanisms are theoretically justified and empirically validated on legal corpus, but cross-client prefix transferability assumptions require further validation on diverse datasets
- **Low confidence**: Discussion of defense mechanisms is brief and lacks quantitative evaluation of their effectiveness against proposed attacks

## Next Checks
1. **Domain Transferability Test**: Replicate the attack on non-legal datasets (e.g., MIMIC-III for healthcare, financial transaction records) with different document structures and PII distributions. Measure whether the same prefix-based extraction strategies achieve comparable coverage rates, particularly for complex PII types.

2. **Query Efficiency Analysis**: Systematically evaluate the marginal benefit of additional queries by computing the coverage gain per query as prefix budget increases. Determine whether there exists a practical query budget threshold beyond which efficiency becomes prohibitive.

3. **Model Architecture Dependency Investigation**: Conduct ablation studies comparing LAFt effectiveness across different model architectures (encoder-decoder vs. decoder-only, different parameter counts, different pretraining objectives). Investigate whether model-specific LAFt effects correlate with particular architectural features or memorization capacities.