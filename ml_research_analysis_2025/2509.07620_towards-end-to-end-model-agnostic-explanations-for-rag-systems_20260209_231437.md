---
ver: rpa2
title: Towards End-to-End Model-Agnostic Explanations for RAG Systems
arxiv_id: '2509.07620'
source_url: https://arxiv.org/abs/2509.07620
tags:
- explanations
- generator
- systems
- framework
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a model-agnostic framework for explaining Retrieval-Augmented
  Generation (RAG) systems through perturbation-based techniques. The approach provides
  explanations for both the retrieval and generation processes by decomposing inputs
  (documents or prompts) into features, perturbing them, and measuring their impact
  on model outputs.
---

# Towards End-to-End Model-Agnostic Explanations for RAG Systems

## Quick Facts
- **arXiv ID:** 2509.07620
- **Source URL:** https://arxiv.org/abs/2509.07620
- **Reference count:** 8
- **Key outcome:** Model-agnostic framework achieving 64.7% completeness for retriever explanations and 76.9% F1 score for generator explanations in RAG systems

## Executive Summary
This paper introduces a model-agnostic framework for explaining Retrieval-Augmented Generation (RAG) systems through perturbation-based techniques. The approach provides explanations for both retrieval and generation processes by decomposing inputs into features, perturbing them, and measuring their impact on model outputs. The framework uses word-level granularity for retriever explanations and sentence-level granularity for generator explanations, employing a "leave one feature out" perturbation strategy. User studies demonstrated that the framework can effectively highlight relevant context portions and achieved reasonable alignment with human annotations, though with slightly lower satisfaction scores compared to model-intrinsic approaches.

## Method Summary
The framework provides model-agnostic explanations for RAG systems by decomposing inputs (documents or prompts) into features and applying perturbation-based techniques. For the retriever component, the framework uses word-level granularity to identify important words by measuring the impact of removing each word on the similarity score. For the generator component, sentence-level granularity is employed to determine which sentences most influence the final answer. The perturbation strategy involves systematically removing individual features and measuring the resulting change in output quality. The framework is designed to work with both open-source and proprietary models, making it broadly applicable across different RAG implementations.

## Key Results
- Retriever explanations achieved 64.7% completeness score in user studies
- Generator explanations reached an F1 score of 76.9% against human annotations
- Models generating more accurate answers tended to focus on the most relevant context portions
- Model-agnostic explanations scored slightly lower than model-intrinsic approaches (3.42-3.45 vs 3.98-4.04 for completeness)

## Why This Works (Mechanism)
The framework leverages perturbation-based techniques to identify feature importance in RAG systems. By systematically removing individual words or sentences and measuring the impact on output quality, the approach can determine which features are most critical for the model's decision-making process. This works because the perturbation method captures the causal relationship between input features and model outputs, allowing for interpretable explanations without requiring access to model internals. The use of different granularities (word-level for retrieval, sentence-level for generation) aligns with the natural structure of these tasks and the type of information each component processes.

## Foundational Learning
- **RAG Systems**: Retrieval-augmented generation combines information retrieval with text generation to produce informed responses. Understanding RAG is essential as the framework specifically targets these hybrid architectures.
- **Perturbation-based Explanations**: These methods work by systematically modifying inputs and observing output changes to infer feature importance. They're needed here because they provide model-agnostic interpretability without requiring internal model access.
- **Granularity Levels**: The choice between word-level and sentence-level analysis affects explanation quality and computational efficiency. Quick check: Does the chosen granularity match the semantic unit of importance for each component?

## Architecture Onboarding

**Component Map**: User Query -> Retriever (documents) -> Generator -> Final Answer

**Critical Path**: The perturbation mechanism flows through both components independently: Retriever perturbation (word-level) → Relevance scoring → Generator perturbation (sentence-level) → Answer generation → Explanation output

**Design Tradeoffs**: Model-agnostic approach sacrifices some explanation fidelity for broad applicability across different models, including proprietary ones where model-intrinsic methods cannot be applied

**Failure Signatures**: Low completeness scores may indicate either poor feature decomposition or that the perturbation method fails to capture complex feature interactions in dense embedding spaces

**First Experiments**:
1. Run perturbation analysis on a simple RAG system with known ground truth relevance to validate the framework's accuracy
2. Compare word-level vs. phrase-level granularity for retriever explanations to test the sensitivity to granularity choice
3. Test the framework on a proprietary model to verify the claimed model-agnostic benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can better quantitative evaluation measures be developed for assessing explanation quality in RAG systems beyond completeness and F1 scores?
- Basis in paper: The authors state "We also aim to investigate better quantitative measures for evaluating explanations" in their conclusion.
- Why unresolved: Current metrics capture alignment with human annotations but may not fully reflect explanation utility for debugging, trust-building, or error detection in real deployments.
- What evidence would resolve it: A suite of validated metrics that correlate with downstream task outcomes and user decision-making quality.

### Open Question 2
- Question: Does word-level granularity fundamentally mismatch how dense embedding models encode semantic information in retrieval?
- Basis in paper: The paper acknowledges "word-level granularity for explaining the retrieval process overlooking how dense embedding models are otherwise trained" as a limitation.
- Why unresolved: Dense embeddings are typically trained on contextualized representations where individual words may not map cleanly to importance weights.
- What evidence would resolve it: Comparative studies showing whether phrase-level, sentence-level, or attention-based granularities yield higher completeness scores or better user alignment.

### Open Question 3
- Question: Can the performance gap between model-agnostic and model-intrinsic explanations be narrowed while maintaining framework flexibility?
- Basis in paper: The authors report lower user ratings for model-agnostic explanations but advocate for them due to flexibility with proprietary models.
- Why unresolved: The trade-off between explanation fidelity and model access remains unexplored; it's unclear if hybrid approaches could capture benefits of both paradigms.
- What evidence would resolve it: Studies testing whether incorporating lightweight model-accessible signals improves ratings without sacrificing broad applicability.

## Limitations
- User study evaluation involved only 20 participants rating explanations on a 5-point Likert scale for just 10 questions, limiting statistical power
- The framework focuses on a single dataset (NQ) and primarily the finance domain, raising questions about generalizability
- The perturbation-based approach may miss important synergistic effects between multiple features due to the "leave one feature out" strategy

## Confidence

| Claim | Confidence |
|-------|------------|
| Framework provides effective explanations for both retrieval and generation | Medium |
| Model-agnostic approach offers flexibility across different models | Medium |
| More accurate answers correlate with focus on relevant context portions | Medium |

## Next Checks

1. Conduct a larger-scale user study with diverse participants and multiple datasets to validate the framework's effectiveness across different domains and question types.
2. Compare the perturbation-based explanations against ground truth feature importance using synthetic datasets where the true feature relevance is known.
3. Test the framework's performance across multiple RAG architectures and model types (both open-source and proprietary) to empirically validate the claimed model-agnostic benefits and identify any limitations.