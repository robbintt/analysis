---
ver: rpa2
title: Compressing Deep Neural Networks Using Explainable AI
arxiv_id: '2507.05286'
source_url: https://arxiv.org/abs/2507.05286
tags:
- pruning
- compression
- accuracy
- importance
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of compressing deep neural networks
  (DNNs) to reduce memory footprint while maintaining accuracy for deployment on resource-constrained
  edge devices. The proposed approach uses a gradient-based explainable AI (XAI) method
  called Layer-wise Relevance Propagation (LRP) to compute importance scores for DNN
  parameters (weights).
---

# Compressing Deep Neural Networks Using Explainable AI

## Quick Facts
- arXiv ID: 2507.05286
- Source URL: https://arxiv.org/abs/2507.05286
- Reference count: 26
- Primary result: 64% model size reduction with 42% accuracy improvement over state-of-the-art XAI-based compression

## Executive Summary
This paper addresses deep neural network compression for resource-constrained edge devices by proposing a gradient-based XAI method using Layer-wise Relevance Propagation (LRP) to compute importance scores for DNN parameters. The approach combines pruning of weights with zero or negative importance scores and mixed-precision quantization guided by these scores, achieving significant size reduction while maintaining or improving accuracy compared to existing XAI-based methods.

## Method Summary
The proposed compression approach uses LRP to compute importance scores for DNN weights during a backward pass that redistributes output relevance layer-by-layer. After computing these scores, weights with zero or negative relevance are pruned. The remaining weights undergo mixed-precision quantization where per-layer median thresholds determine bit-width allocation - higher-scoring weights retain higher precision while lower-scoring weights are quantized to fewer bits. The method was evaluated on a 3-layer dense network with ReLU activations using a synthetic "multi" dataset.

## Key Results
- Achieved 64% model size reduction while improving accuracy by 42% compared to state-of-the-art XAI-based compression
- Pruning-only achieved 91% accuracy (vs 94.1% original) while reducing from 3000 to 2000 filters
- Mixed-precision quantization achieved 90.5% accuracy at 1.2 MB versus 90.1% at 0.82 MB for single-precision alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRP computes weight importance scores that identify which parameters contribute positively to network output
- Mechanism: During backward pass, LRP redistributes output relevance using conservation principle - what a neuron receives must be redistributed to lower layers with relevance score R_i←j = (a_i·w_ij / z_j) × R_j
- Core assumption: Gradient-based relevance propagation accurately reflects functional importance
- Evidence anchors: [abstract] importance scores computed using LRP; [section II.A] LRP based on conservation of flows; [corpus] Weak direct validation
- Break condition: If relevance scores correlate poorly with actual functional contribution

### Mechanism 2
- Claim: Weights with negative or zero relevance scores can be pruned without accuracy degradation
- Mechanism: Filters with R ≤ 0 are removed entirely, achieving 91% accuracy after pruning vs 94.1% original while reducing from 3000 to 2000 filters
- Core assumption: Negative scores indicate genuinely unimportant weights
- Evidence anchors: [abstract] parameters with negative/zero scores are pruned; [section II.B] filters with negative scores are not essential; [corpus] No direct validation
- Break condition: If negative scores result from insufficient sample diversity

### Mechanism 3
- Claim: Mixed-precision quantization guided by importance scores preserves accuracy better than uniform quantization
- Mechanism: After pruning, weights split by per-layer median threshold - higher-scoring weights retain higher bit-width (e.g., 16-bit), lower-scoring weights get reduced precision (e.g., 8-bit or 4-bit)
- Core assumption: Importance scores correlate with precision sensitivity
- Evidence anchors: [abstract] mixed-precision quantization applied based on scores; [Table I] pruning + MP Q achieves 1.2 MB at 90.5% accuracy; [corpus] SQS paper uses different approach
- Break condition: If precision sensitivity doesn't correlate with LRP scores

## Foundational Learning

- Concept: Layer-wise Relevance Propagation (LRP)
  - Why needed here: Core technique for computing all importance scores
  - Quick check question: Given a 3-layer network, can you trace how output relevance R_j propagates backward through activations and weights to compute R_i for input-layer weights?

- Concept: Mixed-Precision Quantization
  - Why needed here: Differentiator from prior XAI compression work with variable bit-width assignment
  - Quick check question: If a layer has importance scores ranging from 0.01 to 0.95 with median 0.3, which weights would you assign 16-bit vs 8-bit precision?

- Concept: Pruning Criteria Comparison (Magnitude vs XAI-based)
  - Why needed here: Understanding why magnitude pruning underperforms (71.49% accuracy) vs LRP-based (90.5%)
  - Quick check question: Why might a weight with small magnitude still be critical for correct classification?

## Architecture Onboarding

- Component map: Pre-trained FP32 model -> LRP Scoring Engine -> Pruning Module -> Threshold Calculator -> Mixed-Precision Quantizer -> Compressed Model
- Critical path: 1) Load pre-trained model, 2) Run forward pass on samples, 3) Execute LRP backward pass, 4) Prune R ≤ 0 weights, 5) Compute per-layer median, 6) Quantize based on score relative to threshold
- Design tradeoffs: Median vs other thresholds (tested weighted mean); bit-width combinations (4/8/16); sample diversity for LRP
- Failure signatures: Accuracy drops >5% after pruning; high variance across test runs; minimal size reduction (<30%)
- First 3 experiments: 1) Reproduce pruning-only baseline on 3-layer network with "multi" dataset, 2) Analyze per-layer score distributions before compression, 3) Compare mixed-precision vs uniform quantization with equivalent bit budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LRP-based importance metric be extended to prune or quantize structural components like entire channels or neurons?
- Basis in paper: [explicit] Conclusion proposes using algorithm for future work on other components
- Why unresolved: Current study focuses on filter/weight-level granularity
- What evidence would resolve it: Experimental results applying method to prune entire channels/neurons on standard benchmarks

### Open Question 2
- Question: Does compression technique maintain efficiency on complex architectures and large-scale datasets?
- Basis in paper: [inferred] Evaluation restricted to simple 3-layer dense network and toy dataset
- Why unresolved: Computational cost and relevance distribution may differ significantly in convolutional/attention layers
- What evidence would resolve it: Benchmarks on CIFAR-10 or ImageNet using architectures like VGG or BERT

### Open Question 3
- Question: Is median of importance scores optimal threshold for mixed-precision quantization?
- Basis in paper: [inferred] Median selected as "easily understood" and robust; weighted mean tested without comprehensive analysis
- Why unresolved: Static statistical measure may not capture distribution nuances in every layer
- What evidence would resolve it: Sensitivity analysis comparing median-based thresholds against learned/adaptive thresholds

## Limitations

- Lacks crucial implementation details including dataset generator code, specific LRP variant parameters, and pre-compression training protocol
- Reported improvements cannot be independently verified without these missing specifications
- Pruning criterion and median-based quantization thresholds appear empirically effective but lack principled justification for generalization
- Evaluation limited to simple 3-layer dense network and synthetic dataset, raising questions about applicability to complex architectures

## Confidence

- **High**: Basic mechanism of LRP relevance redistribution is well-established in XAI literature
- **Medium**: Effectiveness of importance-aware mixed-precision quantization is supported by results but lacks theoretical grounding
- **Low**: Generalization of negative-score pruning criterion across diverse architectures and datasets is unproven

## Next Checks

1. Test LRP score stability across different input sample sets to assess whether pruning decisions are sample-dependent artifacts
2. Compare accuracy-efficiency tradeoff curves of proposed method against simple magnitude-based pruning + uniform quantization at equivalent bit budgets
3. Apply the same compression pipeline to a standard benchmark (e.g., CIFAR-10 CNN) to evaluate transferability beyond toy dataset