---
ver: rpa2
title: 'Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability'
arxiv_id: '2509.23666'
source_url: https://arxiv.org/abs/2509.23666
tags:
- exit
- risk
- threshold
- should
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unreliable confidence-based
  early exits in deep neural networks, which can lead to overconfident but incorrect
  predictions, especially under distribution shifts. The authors propose a new framework
  called UAT that dynamically adapts exit thresholds during inference using a Multi-Armed
  Bandit approach.
---

# Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability

## Quick Facts
- **arXiv ID:** 2509.23666
- **Source URL:** https://arxiv.org/abs/2509.23666
- **Reference count:** 40
- **Primary result:** UAT improves early exit reliability by dynamically adapting thresholds via Multi-Armed Bandits, achieving 1.70–2.10× speedup with <2% accuracy drop.

## Executive Summary
This paper addresses the critical challenge of unreliable confidence-based early exits in deep neural networks, where overconfident but incorrect predictions are common under distribution shifts. The authors propose UAT, a framework that dynamically adapts exit thresholds during inference using a Multi-Armed Bandit approach. Instead of static validation-set thresholds, UAT learns to balance prediction accuracy and computational efficiency by maximizing a reward function that accounts for both prediction confidence and its reliability. A reliability score, trained offline, helps assess whether the model's confidence is trustworthy. The framework is validated on diverse tasks including vision-language understanding, text generation, and classification, demonstrating consistent improvements in speedup (1.70–2.10×) with minimal performance drop (<2%).

## Method Summary
UAT improves early exit reliability by replacing static confidence thresholds with dynamically learned thresholds via Multi-Armed Bandits. The framework consists of two phases: an offline training phase where a reliability function is learned alongside the base model using a modified loss, and an online inference phase where UCB selects thresholds to maximize a reward balancing calibrated confidence and computational cost. The reliability score helps filter out "overconfident but wrong" exits, while the MAB framework adapts to distribution shifts without requiring ground truth labels during inference.

## Key Results
- Achieves consistent speedup of 1.70–2.10× across vision-language, text generation, and classification tasks
- Maintains performance drop of less than 2% compared to full model inference
- Demonstrates adaptation to distribution shifts through dynamic threshold selection

## Why This Works (Mechanism)

### Mechanism 1: Reliability Score Filters Overconfident Errors
If the reliability function $g$ is trained effectively, it can decouple raw prediction confidence from actual correctness probability, filtering out "overconfident but wrong" early exits. A lightweight network (linear layer) is trained offline alongside the main model using a modified loss. This network takes the exit classifier's probability distribution and outputs a "reliability score" ($1 - C_g$) that approximates $p(y=\hat{y}|x, \hat{y})$—the probability that the prediction is correct given the model's output. By multiplying raw confidence $C_\tau$ by this reliability score, the effective confidence is dampened for samples where the model is likely hallucinating high certainty.

### Mechanism 2: Multi-Armed Bandit Adapts to Distribution Shifts
Treating exit thresholds as "arms" in a Multi-Armed Bandit (MAB) allows the system to adapt to distribution shifts without requiring ground truth labels during inference. Instead of a static threshold, UAT uses a UCB (Upper Confidence Bound) algorithm. The "arms" are candidate threshold values ($\Omega \subset [0,1]$). The "reward" is defined as the calibrated confidence minus a penalty for computational cost. The algorithm iteratively selects thresholds that maximize this reward, effectively learning the optimal point on the efficiency-accuracy frontier for the current data stream.

### Mechanism 3: Theoretical Regret Bounds Ensure Risk Control
Bounding the cumulative regret of the MAB policy translates directly to bounding the empirical risk of the early exit system relative to the full model. The authors prove Theorem 4.1, which links the regret $R(\pi, T)$ to the empirical risk $\hat{R}(\pi)$. By ensuring the regret grows sub-linearly (a property of UCB), the average performance loss (risk) converges to a bounded tolerance $\epsilon_d$. This provides a formal safety guarantee: the system is statistically unlikely to persistently select thresholds that yield unacceptably high error rates.

## Foundational Learning

- **Concept: Multi-Armed Bandits (UCB)**
  - **Why needed here:** UAT replaces static threshold heuristics with a bandit learner. You need to understand the exploration-exploitation trade-off to tune the exploration constant $\gamma$ and interpret why the threshold changes over time.
  - **Quick check question:** Can you explain why a UCB policy might select a threshold that has a lower empirical reward than the current best during inference?

- **Concept: Conformal Prediction / Risk Control**
  - **Why needed here:** The paper frames early exiting as a risk-control problem. Understanding how to set tolerance levels ($\epsilon$) and confidence levels ($\delta$) is critical for configuring the system for specific safety requirements.
  - **Quick check question:** If a user requires less than 1% error rate, how would you use Theorem 4.1 to constrain the search space of $\lambda$?

- **Concept: Calibration vs. Reliability**
  - **Why needed here:** The core insight is that raw softmax confidence is not reliable. Understanding calibration techniques (e.g., temperature scaling) provides context for why a learned "reliability function" $g$ is necessary and how it differs from simple calibration.
  - **Quick check question:** Why does the loss function in Eq. 3 require the regularizer $\Phi(c - \phi(g))$ to prevent the collapse of the reliability function?

## Architecture Onboarding

- **Component map:** Backbone (BERT/T5/BLIP-2) -> Exit Classifiers (ECs at intermediate layers) -> Reliability Head (shared linear layer) -> MAB Agent (maintains threshold statistics)

- **Critical path:**
  1. **Offline Phase:** Train Backbone + ECs + $g$ using the modified loss (Eq 3). Ensure $g$ learns to predict correctness.
  2. **Online Inference:** Input $x_t$ arrives. MAB Agent proposes threshold $\tau_t$. Forward pass reaches layer $i$.
  3. **Decision:** Check if $C^i_\tau (1 - C^i_g) \ge \tau_t$.
  4. **Feedback:** If exit, calculate reward $r_t$ and update MAB statistics for $\tau_t$.

- **Design tradeoffs:**
  - **Lambda ($\lambda$):** Controls the aggressiveness of early exiting. High $\lambda$ = more exits (faster but riskier). The authors suggest $\lambda = \epsilon / L$.
  - **Discretization ($\Omega$):** The paper uses 10 values (0.5–1.0). Finer granularity improves optimality but increases MAB convergence time (regret scales with arms).
  - **Exploration ($\gamma$):** High $\gamma$ slows convergence but finds better long-term thresholds; low $\gamma$ greedily exploits current knowledge.

- **Failure signatures:**
  - **Reliability Collapse:** $g$ outputs constant values, causing UAT to behave like standard confidence-based exiting (prone to overconfidence).
  - **Thrashing:** The MAB agent constantly switches thresholds, indicating $\gamma$ is too high or the data stream is too noisy/volatile.
  - **Under-exiting:** Thresholds drift too high (or $\lambda$ too low), resulting in near-zero speedup.

- **First 3 experiments:**
  1. **Sanity Check Reliability:** Train the modified loss on a validation set. Plot $C_g$ vs. actual accuracy to ensure the reliability head is discriminative (it should be low for wrong predictions).
  2. **Static vs. Dynamic:** Compare UAT against a fixed-threshold baseline on a dataset with a simulated distribution shift (e.g., change topics halfway through) to observe the adaptation threshold.
  3. **Hyperparameter Sweep $\lambda$:** Run UAT with varying $\lambda$ to plot the Pareto frontier of Risk vs. Speedup and verify alignment with Figure 2b in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What efficiency and accuracy gains are achievable if the UAT framework utilizes layer-specific dynamic thresholds rather than a single global threshold?
- **Basis in paper:** Section 6 (Conclusion) states, "It is interesting to quantify the gains achievable by allowing the threshold to adapt to each layer" and Section 7 (Limitations) notes there is "scope to have a local threshold."
- **Why unresolved:** The current implementation enforces a uniform threshold across all exits, which may force a compromise between the distinct confidence distributions of early and late layers.

### Open Question 2
- **Question:** How robust is the framework when the offline-trained reliability function $g$ fails to generalize to severe, unseen distribution shifts during inference?
- **Basis in paper:** Section 7 (Limitations) highlights that the method "relies on the quality of the learned confidence function $g$."
- **Why unresolved:** If the test distribution differs drastically from the training data used to learn $g$, the reliability score may become a misleading proxy for correctness, breaking the reward signal.

### Open Question 3
- **Question:** Does the calibration of the reliability score $(1-C_g^i)$ as an approximation for true class probability degrade at earlier exit layers compared to deeper layers?
- **Basis in paper:** Theorem 4.1 assumes $(1-C_g^i)$ approximates $p(y=\hat{y}|x, \hat{y})$ with high probability $(1-\delta_1)$, but empirical validity across varying depths is not explicitly verified.
- **Why unresolved:** If the approximation is poor for early exits (where feature representations are less mature), the MAB may incorrectly prioritize efficiency over correctness.

## Limitations
- The reliability function $g$ is trained offline and assumes the correlation between its output and actual correctness generalizes to test data, which may break down under severe distribution shifts
- Theoretical regret bounds assume i.i.d. samples and a finite, discrete threshold set, which may not hold in practice
- The framework adds complexity (shared reliability head, MAB agent) that must be justified by the performance gains

## Confidence

- **High:** The core mechanism of using a reliability score to modulate confidence before thresholding is technically sound and well-motivated by the overconfidence problem in EEDNNs
- **Medium:** The claim of consistent improvements across diverse tasks is supported by the ablation studies and figures, but the results are benchmark-specific
- **Medium:** The theoretical regret bounds linking to empirical risk are formally correct under stated assumptions, but the practical tightness of these bounds and their sensitivity to hyperparameters like $\gamma$ and $\lambda$ are not fully explored

## Next Checks

1. **Distribution Shift Robustness:** Evaluate UAT on a controlled distribution shift (e.g., change input domain or class balance mid-stream) and compare the reliability score's accuracy correlation before and after the shift
2. **Sensitivity to Discretization:** Run UAT with different numbers of threshold values (e.g., 5 vs. 20) and measure the impact on cumulative regret and final speedup/accuracy trade-off
3. **Reliability Head Generalization:** For a fixed dataset, train the reliability head on a subset of classes and test its performance on held-out classes to quantify how well it generalizes to unseen prediction scenarios