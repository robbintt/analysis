---
ver: rpa2
title: Can Time-Series Foundation Models Perform Building Energy Management Tasks?
arxiv_id: '2506.11250'
source_url: https://arxiv.org/abs/2506.11250
tags:
- tsfms
- data
- performance
- forecasting
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Time-series foundation models (TSFMs) have shown promise for generalizing
  across tasks in domains like building energy management, but their practical utility
  remains unclear. This paper evaluates their zero-shot performance on forecasting,
  thermal modeling with covariates, classification, and robustness under varying conditions.
---

# Can Time-Series Foundation Models Perform Building Energy Management Tasks?

## Quick Facts
- **arXiv ID**: 2506.11250
- **Source URL**: https://arxiv.org/abs/2506.11250
- **Reference count**: 3
- **Primary result**: TSFMs show promise for classification via zero-shot embeddings but underperform statistical models in building energy forecasting tasks.

## Executive Summary
Time-series foundation models (TSFMs) demonstrate strong capabilities for zero-shot representation learning, enabling competitive classification performance without dataset-specific training. However, their practical utility for building energy management tasks remains limited. When evaluated on forecasting, thermal modeling, and robustness, TSFMs trained predominantly on univariate data struggle with covariate integration and fail to generalize effectively to unseen data modalities. While they can match or slightly exceed statistical baselines on familiar tasks, their performance degrades significantly in complex, dynamic building environments where covariates and contextual information are critical.

## Method Summary
The study evaluates six pretrained TSFMs (Chronos, MOMENT, TimesFM, TimeGPT, LagLlama, Uni2TS) on zero-shot univariate forecasting, forecasting with covariates, and classification tasks in building energy management. Models are tested on UCI Electricity, Smart*, ecobee datasets for forecasting, and WHITED/BTS for classification. Statistical baselines (AutoARIMA, S-ARIMA) use test-time fitting on context windows. TSFMs' zero-shot representations are frozen and used for downstream classification with simple SVM/NN classifiers. Evaluation uses RMSE, DTW, and PRR metrics across multiple prediction horizons.

## Key Results
- TSFMs generate effective zero-shot representations for classification, outperforming supervised deep learning methods
- TSFMs perform well on datasets seen during pretraining but struggle with unseen data modalities
- Inclusion of covariates does not consistently improve TSFM forecasting accuracy
- TSFMs trained with NLL/CE loss preserve temporal patterns better than MSE-trained models

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot embeddings from pretrained TSFMs can support downstream classification without dataset-specific fine-tuning, outperforming supervised deep learning methods. Models like Chronos and MOMENT encode time-series into latent representations using transformers pretrained on diverse corpora. These embeddings capture universal temporal patterns that transfer across domains, allowing a simple classifier head (SVM or NN) to achieve competitive performance. The pretraining corpus must be sufficiently diverse to produce transferable representations, and the classification task must rely on temporal patterns rather than domain-specific semantics.

### Mechanism 2
Training objective (MSE vs. NLL/CE) fundamentally shapes what TSFMs learn—magnitude accuracy vs. distribution/pattern preservation. Models trained with MSE loss smooth predictions to minimize large deviations, suppressing peaks. Models trained with NLL or cross-entropy learn conditional probability distributions, better preserving temporal structure and peaks. The evaluation metric must align with the training objective, as users need to select metrics that reflect their actual task priorities.

### Mechanism 3
TSFMs trained predominantly on univariate forecasting cannot effectively leverage covariates, even when architectures nominally support multivariate input. Covariate integration methods (flattening sequences, residual forecasting) are architectural add-ons, not fundamental to the pretrained representations. The model's learned priors remain univariate, so covariates add noise rather than signal. This assumes covariates provide predictive signal beyond the target series' autocorrelation structure, but current TSFMs cannot learn cross-variate dependencies.

## Foundational Learning

- **Transformer architectures for time-series (encoder-only, decoder-only, encoder-decoder)**: TSFMs adopt different transformer variants with distinct trade-offs. MOMENT uses encoder-only for representation learning; Chronos uses encoder-decoder for probabilistic forecasting; TimesFM uses decoder-only. Architecture choice determines what tasks the model can support. Quick check: Can you explain why an encoder-only model might produce better embeddings while a decoder-only model might produce better autoregressive forecasts?

- **Test-time fitting vs. pretrained inference**: The paper uses statistical models (AutoARIMA, S-ARIMA) with test-time fitting—calibrating parameters only on the context window. This creates a critical comparison: models that adapt to local data vs. models that rely on learned priors. Quick check: Why might a statistical model with test-time fitting outperform a TSFM on short prediction horizons but lose on longer horizons?

- **Evaluation metrics beyond point-wise error (DTW, Peak Recall Rate)**: RMSE alone obscures whether models capture temporal patterns correctly. DTW tolerates phase shifts; PRR measures peak detection. Different applications require different metric priorities. Quick check: If a model achieves low RMSE but consistently misplaces peaks by 2 hours, which metric would reveal this failure mode?

## Architecture Onboarding

**Component map:**
Input → Patching/Tokenization → Transformer Backbone → Task Head → Output
         │                        │                      │
         ├─ MOMENT: Fixed patches ├─ Encoder-only       ├─ Forecasting head
         ├─ Chronos: Quantization ├─ Encoder-decoder    ├─ Classification head (SVM/NN)
         └─ TimesFM: Residual blks └─ Decoder-only       └─ Probabilistic output

**Critical path:**
1. Select model based on task: MOMENT/Chronos for classification embeddings; TimesFM/TimeGPT for zero-shot forecasting
2. Verify data familiarity: Has the model seen similar modalities during pretraining? (Table 2)
3. Choose evaluation metric aligned with operational requirements: RMSE for magnitude, DTW/PRR for pattern fidelity

**Design tradeoffs:**
- **MSE-trained models (MOMENT, TimesFM)**: Better RMSE, smoother predictions, may miss peaks
- **NLL/CE-trained models (Chronos, LagLlama, Uni2TS)**: Better pattern preservation, higher RMSE, capture peaks
- **Covariate support**: Only Uni2TS, TimeGPT, TimesFM support covariates, but performance gains are negligible

**Failure signatures:**
- TSFM produces smooth "average" predictions that miss peaks → MSE-trained model; consider NLL/CE alternatives
- TSFM forecasts degrade on unseen modalities (e.g., indoor temperature) → Model lacks modality familiarity; expect only marginal improvement over statistical baselines
- Covariate inclusion doesn't help or hurts performance → Expected behavior; current TSFMs cannot leverage covariates effectively

**First 3 experiments:**
1. **Baseline comparison**: Run TSFM (TimesFM or Chronos) against AutoARIMA with test-time fitting on your data. Compare RMSE at multiple horizons (4h, 12h, 24h). Expect TSFM to match or slightly exceed baselines only at longer horizons.
2. **Metric sensitivity**: Evaluate the same model using RMSE, DTW, and PRR. If rankings change significantly, your task requirements may favor a different training objective.
3. **Covariate ablation**: Test Uni2TS or TimeGPT with and without covariates. Expect minimal improvement; if improvement occurs, document conditions—this contradicts the paper's findings and warrants investigation.

## Open Questions the Paper Calls Out

### Open Question 1
How can TSFMs be redesigned to accept dual inputs (time-series data and natural language) to dynamically integrate contextual metadata for building energy management? The paper explicitly proposes "dual-input capability" to incorporate structured and unstructured information for context-aware predictions, noting current models miss auxiliary information. This remains unresolved because current zero-shot TSFMs rely exclusively on numerical time-series data, lacking mechanisms to process auxiliary metadata like occupancy or operational constraints. Evidence would require a TSFM architecture that processes text and time series simultaneously, demonstrating improved zero-shot performance on unseen domains compared to numerical-only baselines.

### Open Question 2
Can TSFMs transition from fixed-task predictors to task-agnostic systems capable of open-ended reasoning without fine-tuning? The paper suggests future TSFMs should employ "task-agnostic output mechanisms" or generate natural language outputs to unify analytical tasks like classification and anomaly detection. This remains unresolved because existing models require retraining or specific heads for different tasks, limiting their scalability and "foundation" status compared to LLMs. Evidence would require a unified model successfully performing forecasting, classification, and anomaly detection via a shared generative interface without architectural modifications or retraining.

### Open Question 3
What architectural modifications are required to ensure TSFMs effectively utilize covariates for thermal modeling? The paper demonstrates that covariates do not enhance TSFM performance, and highlights the need to capture contextual dependencies. This remains unresolved because the paper empirically shows current attention mechanisms fail to leverage covariate structures, often performing worse than univariate baselines, but offers no specific solution. Evidence would require a TSFM that significantly lowers RMSE in thermal modeling when provided with weather/HVAC data compared to its univariate counterpart and traditional baselines.

## Limitations
- TSFMs cannot effectively leverage covariates despite nominal architectural support
- Performance degrades significantly on data modalities not seen during pretraining
- Current evaluation lacks guidance on choosing metrics for specific operational contexts

## Confidence
- **High**: Zero-shot representations enable competitive classification without fine-tuning
- **Medium**: Training objective (MSE vs. NLL/CE) shapes prediction characteristics
- **Low**: TSFMs will consistently underperform statistical models on forecasting in complex BEM environments

## Next Checks
1. **Covariate ablation study**: Test UniCA and TFMAdapter adaptation methods on the same ecobee dataset used in this paper. Compare against raw TSFM performance to verify if proposed solutions address the covariate limitation.
2. **Metric prioritization experiment**: For a real BEM control scenario (e.g., peak demand avoidance), evaluate whether DTW/PRR-preferring models (Chronos, LagLlama) outperform RMSE-preferring models (TimesFM) in operational cost reduction.
3. **Pretraining diversity analysis**: Systematically vary the proportion of familiar vs. unfamiliar modalities in pretraining data. Measure zero-shot performance decay to establish quantitative thresholds for pretraining requirements.