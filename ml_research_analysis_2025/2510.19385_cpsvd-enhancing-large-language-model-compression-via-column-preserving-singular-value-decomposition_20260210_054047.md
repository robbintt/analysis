---
ver: rpa2
title: 'CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular
  Value Decomposition'
arxiv_id: '2510.19385'
source_url: https://arxiv.org/abs/2510.19385
tags:
- compression
- ratio
- duo-svd
- language
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Duo-SVD, a novel training-free framework
  for compressing large language models (LLMs) using Singular Value Decomposition
  (SVD). The key insight is that SVD approximation errors vary significantly across
  different columns and modules of a weight matrix, leading to suboptimal compression
  with uniform approaches.
---

# CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular Value Decomposition

## Quick Facts
- arXiv ID: 2510.19385
- Source URL: https://arxiv.org/abs/2510.19385
- Reference count: 40
- Primary result: Training-free SVD-based compression framework that achieves lower perplexity and higher accuracy than state-of-the-art SVD methods

## Executive Summary
This paper introduces Duo-SVD, a novel training-free framework for compressing large language models (LLMs) using Singular Value Decomposition (SVD). The key insight is that SVD approximation errors vary significantly across different columns and modules of a weight matrix, leading to suboptimal compression with uniform approaches. Duo-SVD addresses this through a dual-level optimization strategy: at the column level, it preserves high-sensitivity columns exactly while applying SVD only to low-sensitivity ones; at the module level, it formulates compression ratio allocation as a global constrained optimization problem based on perturbation-induced model deviation. Extensive experiments on LLaMA-2, Mistral, and LLaMA-3.1 models demonstrate that Duo-SVD consistently outperforms state-of-the-art SVD-based compression methods.

## Method Summary
Duo-SVD employs a two-stage framework for training-free LLM compression. The first stage, Module-Adaptive Allocation Strategy (MAAS), computes per-module sensitivity by measuring KL divergence between original and perturbed model outputs when each module is compressed independently. This sensitivity data feeds into a constrained optimization problem solved via dynamic programming to allocate compression ratios across modules while maintaining a target overall compression ratio. The second stage, Column-Preserving Strategy (CPS), performs SVD on each module's weight matrix but preserves high-sensitivity columns exactly rather than approximating them. Column importance is determined by reconstruction error when that column is zeroed out, and a ternary search finds the optimal number of preserved columns to balance compression ratio against performance. The method uses 256 calibration samples from WikiText2 for sensitivity computation and 32 samples for column error estimation.

## Key Results
- Achieves lower perplexity than state-of-the-art SVD methods (e.g., 5.12 vs. 6.32-7.84 for LLaMA-2 13B)
- Improves average accuracy on downstream tasks across MMLU, PIQA, WinoGrande, HellaSwag, ARC-e, ARC-c, and OpenbookQA
- Shows compatibility with quantization techniques for additional compression
- Provides practical inference speedups when optimized kernels are available

## Why This Works (Mechanism)
Duo-SVD works by recognizing that not all parameters contribute equally to model performance, and SVD approximation errors are not uniformly distributed. By preserving high-sensitivity columns exactly and applying SVD only to less critical components, the method minimizes information loss during compression. The module-level optimization ensures compression resources are allocated where they have the most impact, based on measured sensitivity to perturbation. This dual optimization strategy addresses the fundamental limitation of uniform SVD approaches that treat all parameters and modules equally.

## Foundational Learning
- **Whitening transformation**: Preprocessing step that decorrelates input features by transforming them to have identity covariance matrix. Why needed: Ensures columns are independent for meaningful individual importance assessment. Quick check: Verify covariance matrix of whitened data is approximately identity.
- **KL divergence sensitivity measurement**: Quantifies how much model output distribution changes when a module is perturbed. Why needed: Provides metric for allocating compression resources effectively across modules. Quick check: Compute KL divergence for perturbed vs original outputs and verify it increases with perturbation severity.
- **Dynamic programming for constrained optimization**: Solves the module compression ratio allocation as a multiple-choice knapsack problem. Why needed: Efficiently finds optimal compression ratio vector under budget constraints. Quick check: Verify optimal solution respects total compression ratio constraint and improves over greedy allocation.
- **Ternary search for unimodal functions**: Optimization technique for finding minimum of convex/concave functions. Why needed: Efficiently locates optimal number of preserved columns given parameter budget. Quick check: Plot F(c) function to verify unimodality before applying ternary search.
- **Column independence assumption**: Treats column importance as independent when selecting which to preserve. Why needed: Simplifies selection problem from combinatorial to greedy approach. Quick check: Compare greedy selection performance against exhaustive search on small matrices.

## Architecture Onboarding
- **Component map**: Calibration data -> Sensitivity computation (MAAS) -> Module ratio allocation -> Column error computation (CPS) -> Ternary search -> Dense+low-rank decomposition
- **Critical path**: Forward pass with hooks captures activations → Column error computation → Ternary search optimization → Matrix decomposition → Parameter allocation
- **Design tradeoffs**: Accuracy vs compression ratio (favors preservation of sensitive columns), computational efficiency vs optimality (greedy selection vs exhaustive), memory usage vs speed (dense vs low-rank computation)
- **Failure signatures**: Non-convex F(c) curves causing suboptimal ternary search results, excessive memory usage during sensitivity computation, degraded performance from incorrect column error computation
- **First experiments**: 1) Implement calibration data loading and forward hooks to capture input activations, 2) Implement column error computation and verify error ordering, 3) Test ternary search on a small weight matrix to validate optimal c* selection

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the strict convexity of the exact objective function F(c) be rigorously proven, rather than relying on the approximated upper bound and empirical validation?
- **Open Question 2**: To what extent can specialized hardware-aware kernels translate theoretical FLOPs reductions into proportional wall-clock speedups in production inference frameworks?
- **Open Question 3**: How does Duo-SVD compression affect safety alignment and bias amplification in compressed LLMs?
- **Open Question 4**: Does the Column Independence Assumption cause suboptimal column selection when strong inter-column correlations exist?

## Limitations
- No detailed ablation studies isolating impact of column-level vs. module-level optimizations
- Limited investigation of robustness to different calibration datasets or data distribution shifts
- Performance gains, while consistent, are relatively modest in some cases

## Confidence
- **High confidence** in the core technical contribution and general feasibility, given extensive experimental validation
- **Medium confidence** in claimed superiority over state-of-the-art methods, due to limited comparison with recent compression techniques
- **Low confidence** in practical deployment implications without detailed measurements on diverse hardware platforms

## Next Checks
1. Reproduce the column error computation and ternary search procedure on a small weight matrix to verify the F(c) unimodality assumption and optimal c* selection
2. Implement the MAAS sensitivity computation and validate that KL divergence estimates correlate with actual downstream performance degradation
3. Test the combined Duo-SVD approach on an additional model family (e.g., BLOOM or Falcon) to assess generalization beyond LLaMA/Mistral architectures