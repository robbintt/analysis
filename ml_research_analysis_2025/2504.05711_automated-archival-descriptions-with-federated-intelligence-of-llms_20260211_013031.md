---
ver: rpa2
title: Automated Archival Descriptions with Federated Intelligence of LLMs
arxiv_id: '2504.05711'
source_url: https://arxiv.org/abs/2504.05711
tags:
- metadata
- archival
- llms
- groppe
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of creating metadata descriptions
  for archival materials, which is tedious, error-prone, and requires specialized
  expertise. To automate this process, the authors propose an agentic AI-driven system
  that leverages multiple large language models (LLMs) and a federated optimization
  approach.
---

# Automated Archival Descriptions with Federated Intelligence of LLMs

## Quick Facts
- **arXiv ID:** 2504.05711
- **Source URL:** https://arxiv.org/abs/2504.05711
- **Reference count:** 32
- **Primary result:** Federated LLM approach achieves 0.90 metadata score, outperforming individual models (0.81-0.89)

## Executive Summary
This paper addresses the challenge of creating metadata descriptions for archival materials, which is tedious, error-prone, and requires specialized expertise. The authors propose an agentic AI-driven system that leverages multiple large language models (LLMs) and a federated optimization approach to automate this process. By synthesizing outputs from an ensemble of LLMs through a dedicated federating agent, the system achieves higher metadata quality (completeness and accuracy) than relying on a single model, with an average score of 0.90 compared to 0.81-0.89 for individual LLMs.

## Method Summary
The system uses LLM agents to generate metadata in parallel, a validator to ensure consistency with archival standards (ISAD(G)), and a federating agent to synthesize an optimal metadata description from the individual LLM outputs. The approach employs four different LLMs (Grok 3, GPT-4-turbo, DeepSeek-V3, Gemini 2.0 Flash) that generate parallel metadata sets, which are then analyzed and optimized by the Federator agent to create a superior synthesized record. An extensive evaluation on 22 real-world archival materials from the Deutscher Gewerkschaftsbund (DGB) archive demonstrates the feasibility of the approach.

## Key Results
- Federated approach achieves average metadata score of 0.90, outperforming individual LLMs (scores ranging from 0.81 to 0.89)
- The system demonstrates higher completeness and accuracy by adhering more strictly to archival standards
- LLM-as-Judge evaluation methodology provides more accurate assessment of semantic metadata quality than traditional metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthesizing outputs from an ensemble of LLMs via a federator agent yields higher metadata quality than relying on a single model.
- **Mechanism:** Multiple LLMs generate parallel metadata sets. A "Federator" agent analyzes these sets, identifies strengths (e.g., factual accuracy in one, better formatting in another), and performs element-by-element optimization to create a superior synthesized record.
- **Core assumption:** The Federator agent possesses sufficient reasoning capability to discern high-quality data segments from lower-quality ones and correct errors based on the source text.
- **Evidence anchors:** [abstract] "A federating agent to synthesize an optimal metadata description... outperforming individual LLMs." [section 3.4] Describes the strategy: "Select the best base... Enhance with details... Resolve discrepancies."

### Mechanism 2
- **Claim:** Iterative validation against a strict "Context" ensures structural consistency and adherence to archival standards.
- **Mechanism:** Instead of free-form generation, LLMs are constrained by a "Context" (schema, tags, examples). A "Validator" agent checks the output structure. If validation fails, the system rejects the output and forces the LLM to regenerate.
- **Core assumption:** The LLMs are capable of self-correction when provided with explicit error signals or validation failures.
- **Evidence anchors:** [abstract] "A validator to ensure consistency." [section 3.3] "We need a solution to ensure that metadata descriptions... are strictly consistent... We use a context that contains the necessary information."

### Mechanism 3
- **Claim:** Using an LLM as a "Judge" provides a more accurate assessment of semantic metadata quality than traditional string-matching metrics.
- **Mechanism:** Because archival metadata requires factual accuracy and intent equivalence, statistical overlap scores are insufficient. The system uses a high-capability LLM (Grok 3) to score outputs on a 0-1 scale based on completeness and standard adherence.
- **Core assumption:** The "Judge" LLM is reliably aligned with human archivist standards and does not exhibit bias toward specific output styles.
- **Evidence anchors:** [section 4.1] "Existing evaluation techniques... are not able to do this... so we also use LLMs as evaluators."

## Foundational Learning

- **Concept:** **ISAD(G) Standard (General International Standard Archival Description)**
  - **Why needed here:** The entire system is a translation engine from unstructured text into the structured ISAD(G) areas (Identity, Content, Access). You cannot debug the Validator or the Federator without knowing what a valid "Scope and Content" or "Extent" field looks like.
  - **Quick check question:** Can you distinguish between the "Identity Statement Area" and the "Content and Structure Area" in an archival finding aid?

- **Concept:** **Agentic Workflow (Instructor/Validator/Federator)**
  - **Why needed here:** The paper implements a pipeline where LLMs act as specific agents with roles (e.g., the Validator agent does not generate text; it checks it). Understanding role separation is critical to debugging the flow.
  - **Quick check question:** What is the specific role of the "Instructor" agent versus the "Federator" agent in this architecture?

- **Concept:** **Prompt Engineering for Structured Output (XML)**
  - **Why needed here:** The system relies on the LLM outputting valid XML. The "Context" mechanism (Section 3.3) is essentially advanced prompt engineering to enforce schema compliance.
  - **Quick check question:** How does providing "one-word tags" and "examples" in the context improve the consistency of LLM generation?

## Architecture Onboarding

- **Component map:** User Input -> Instructor Agent -> LLM Ensemble (Agents 1-4) -> Validator Agent -> Federator Agent -> Evaluator (Optional/Meta)
- **Critical path:** The **Federator Agent** logic. While the Ensemble generates the raw data, the quality gain (0.81 -> 0.90) comes from the Federator's ability to perform "element-by-element optimization" (Section 3.4). If this agent fails, the system is just a slow ensemble.
- **Design tradeoffs:**
  - **Latency vs. Quality:** Running 4 LLMs in parallel + a Federator + potential retries significantly increases latency and cost compared to a single LLM call.
  - **Strictness vs. Throughput:** The "Validator" ensures quality but could get stuck in loops (Section 3.3 mentions a "limited number of repetitions" to handle edge cases).
- **Failure signatures:**
  - **Repetition Loop:** The Validator repeatedly rejects an LLM output because the LLM cannot adhere to a specific complex XML schema constraint.
  - **Regression in Extent:** Section 4.3 notes the Federator sometimes scored lower on "Extent" than individual models because it added *more* detail (standard adherence) than the minimal ground truth.
- **First 3 experiments:**
  1. **Baseline Calibration:** Run the extraction on the provided 22 samples using *only* the best single model (LLM 1/Grok 3) to establish a baseline score.
  2. **Ablation of Federation:** Run the pipeline but use a simple "majority vote" or "first valid" logic instead of the LLM Federator to quantify the specific contribution of the synthesis mechanism.
  3. **Validation Stress Test:** Feed the system a non-standard document (e.g., a handwritten note or pure image) to test if the Instructor/Context mechanism gracefully handles documents that don't fit the text-extraction assumptions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance and latency of the federated approach scale when applied to significantly larger archival corpora?
- **Basis in paper:** [explicit] The conclusion explicitly states that "future work could investigate the scalability of this approach to larger archival datasets."
- **Why unresolved:** The current evaluation is limited to a small dataset of 22 representative archival units, leaving the system's behavior on bulk processing untested.
- **Evidence:** Benchmarking the framework on a full-scale digital repository containing thousands of diverse documents to measure throughput and error rates.

### Open Question 2
- **Question:** Does utilizing the same LLM for both federated synthesis and performance evaluation introduce a self-reinforcing bias?
- **Basis in paper:** [inferred] The methodology selects Grok 3 as both the Federator agent (Section 3.4) and the "LLM-as-a-Judge" evaluator (Section 4.1).
- **Why unresolved:** An LLM acting as its own judge may preferentially score outputs that align with its own internal reasoning or stylistic patterns over distinct but valid human-generated ground truth.
- **Evidence:** A comparative study where the generated metadata is evaluated by a neutral third-party LLM or a blind human expert panel.

### Open Question 3
- **Question:** To what extent does integrating human-in-the-loop feedback improve the refinement and accuracy of the automated descriptions?
- **Basis in paper:** [explicit] The conclusion lists "explore the integration of human-in-the-loop feedback to further refine the metadata generation process" as a direction for future work.
- **Why unresolved:** While the system automates extraction, the authors acknowledge that intellectual input remains essential for contextualizing documents, yet the current framework does not model this interaction.
- **Evidence:** User studies where professional archivists iteratively correct agent outputs, measuring the reduction in required manual effort over time.

## Limitations
- Evaluation based on small dataset of 22 archival units from single archive, limiting generalizability
- Exact prompts and context examples not fully specified, making exact reproduction difficult
- System latency and cost implications of running multiple LLMs not discussed

## Confidence

- **High Confidence:** The core mechanism of using a federating agent to synthesize outputs from multiple LLMs is technically sound and well-demonstrated through comparative scoring (0.81-0.89 vs 0.90)
- **Medium Confidence:** The iterative validation approach against ISAD(G) standards will improve metadata quality, though effectiveness depends on specific validation rules implemented
- **Medium Confidence:** The LLM-as-Judge evaluation methodology is appropriate for assessing semantic quality, but its reliability depends on judge model's alignment with human archivist standards

## Next Checks
1. **Dataset Diversity Test:** Apply the system to archival materials from different sources and document types to assess generalizability beyond the DGB archive
2. **Human-in-the-Loop Validation:** Have human archivists evaluate a subset of generated metadata to verify the LLM-as-Judge scores align with expert assessment
3. **Cost-Benefit Analysis:** Measure the actual latency and API costs of the federated approach versus simpler single-model solutions to quantify the tradeoff between quality gains and resource usage