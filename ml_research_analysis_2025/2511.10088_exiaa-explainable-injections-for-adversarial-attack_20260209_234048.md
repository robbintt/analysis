---
ver: rpa2
title: 'eXIAA: eXplainable Injections for Adversarial Attack'
arxiv_id: '2511.10088'
source_url: https://arxiv.org/abs/2511.10088
tags:
- image
- attack
- explanations
- original
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes eXIAA, a novel black-box, model-agnostic adversarial
  attack method that targets post-hoc explainability methods in image classification.
  The key idea is to generate adversarial perturbations by leveraging the explanations
  of images from the model's running-up (second-highest confidence) class, without
  requiring access to model weights or multiple attack steps.
---

# eXIAA: eXplainable Injections for Adversarial Attack

## Quick Facts
- arXiv ID: 2511.10088
- Source URL: https://arxiv.org/abs/2511.10088
- Reference count: 40
- Key outcome: Novel black-box, model-agnostic adversarial attack method targeting post-hoc explainability methods in image classification

## Executive Summary
This paper introduces eXIAA, a novel adversarial attack method that targets post-hoc explainability techniques in image classification models. The key innovation is leveraging explanations from the model's second-highest confidence class (running-up class) to generate adversarial perturbations that drastically alter feature attributions while maintaining visual similarity and original predictions. Unlike traditional adversarial attacks, eXIAA operates in a single-step, black-box manner without requiring access to model weights or multiple attack iterations. Experiments on ImageNet using ResNet-18 and ViT-B16 demonstrate that eXIAA can significantly modify explanations with minimal impact on classification confidence and high structural similarity.

## Method Summary
eXIAA generates adversarial perturbations by extracting GradCAM explanations from the running-up class and injecting them into the original image through weighted blending. The method identifies the second-highest confidence class from the model's prediction, generates its explanation map, and then combines this with the original image's explanation using a weighted factor. This injection process aims to maximize the absolute difference between original and attacked explanations while preserving the model's original prediction. The attack is model-agnostic and black-box, requiring only access to the model's predictions and the ability to generate explanations, making it applicable across different architectures without knowledge of internal parameters.

## Key Results
- eXIAA achieves over 100% increase in absolute difference for explanation changes in some settings
- Classification confidence drops by less than 5% on average while maintaining high structural similarity (SSIM ≈ 1)
- The attack demonstrates significant vulnerability in post-hoc explainability methods while preserving visual similarity and original predictions

## Why This Works (Mechanism)
The attack exploits the fact that post-hoc explainability methods like GradCAM generate explanations based on class-specific feature activations. By injecting feature attributions from the running-up class into the original image, eXIAA creates conflicting signals in the explanation space. The weighted blending technique ensures that the injected features are prominent enough to alter the explanation significantly but subtle enough to avoid changing the model's prediction or causing noticeable visual artifacts. This approach works because explainability methods are designed to highlight features important for classification, and by introducing features from a competing class, the explanation becomes misleading while the prediction remains stable.

## Foundational Learning

**GradCAM**: A gradient-based explainability method that generates visual explanations by computing weighted combinations of feature maps from the last convolutional layer. Why needed: eXIAA relies on GradCAM to extract explanation maps from both the predicted and running-up classes. Quick check: Verify that GradCAM produces heatmaps highlighting discriminative regions for each class.

**Running-up class identification**: The process of determining the second-highest confidence class from model predictions. Why needed: eXIAA uses explanations from this class as the source of injected features. Quick check: Confirm that the running-up class is correctly identified from softmax outputs.

**Weighted blending for perturbation**: A technique that combines two images or feature maps using a weighted factor to create smooth transitions. Why needed: eXIAA uses this to inject running-up class explanations while maintaining visual similarity. Quick check: Test that blending preserves image quality metrics like SSIM.

## Architecture Onboarding

**Component map**: Input image → Model prediction → Running-up class identification → GradCAM explanations (predicted + running-up) → Weighted blending → Adversarial image → Explanation comparison

**Critical path**: The core attack pipeline involves generating explanations for both the predicted and running-up classes, then performing weighted blending to create the adversarial example. The most computationally intensive step is GradCAM computation, but since eXIAA operates in single-step mode, this remains efficient.

**Design tradeoffs**: Single-step vs. iterative attacks (simplicity vs. potential effectiveness), GradCAM vs. other explainability methods (computational efficiency vs. explanation quality), and injection strength vs. stealthiness (perturbation magnitude vs. visual similarity).

**Failure signatures**: If the running-up class explanation is too similar to the predicted class explanation, injection may have minimal effect. If blending weights are too high, classification confidence may drop significantly or visual artifacts may appear.

**First experiments**:
1. Test eXIAA on a simple CNN with synthetic data to verify basic functionality
2. Apply the attack to ResNet-18 on a small subset of ImageNet to validate effectiveness
3. Compare GradCAM explanations before and after attack to measure explanation changes

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but implicit questions include: How effective is eXIAA across different explainability methods beyond GradCAM? What is the attack's performance in fully black-box settings where running-up class explanations are not directly accessible? How do these explanation manipulations affect human perception and trust in model explanations?

## Limitations
- Limited experimental scope focusing on only two model architectures (ResNet-18 and ViT-B16) and one dataset (ImageNet)
- Evaluation metrics may not fully capture practical implications of explanation manipulation in real-world applications
- Assumes access to explanation methods that can generate explanations for the running-up class, which may not be universally available

## Confidence

**High confidence**: The method's basic functionality and core experimental results are well-documented and reproducible
**Medium confidence**: The generalizability of results across different models and datasets
**Medium confidence**: The practical implications of explanation manipulation for safety-critical applications

## Next Checks

1. Test eXIAA across a broader range of model architectures (e.g., ConvNeXt, EfficientNet) and explainability methods (e.g., Integrated Gradients, LIME) to assess generalizability
2. Evaluate the attack's effectiveness in a fully black-box setting where running-up class explanations are not directly accessible
3. Conduct user studies to determine whether explanation changes induced by eXIAA are perceptually meaningful to human observers in practical scenarios