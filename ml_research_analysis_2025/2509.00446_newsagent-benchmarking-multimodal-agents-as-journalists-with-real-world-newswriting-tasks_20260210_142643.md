---
ver: rpa2
title: 'NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting
  Tasks'
arxiv_id: '2509.00446'
source_url: https://arxiv.org/abs/2509.00446
tags:
- news
- information
- step
- agents
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NEWSAGENT benchmarks autonomous agents in realistic journalistic
  workflows where agents must iteratively search, edit, and rephrase multimodal content
  to produce news articles under strict temporal constraints. The benchmark introduces
  a structured agentic pipeline with time-aware search and editing functions, using
  6,237 human-verified examples from real-world news articles.
---

# NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks

## Quick Facts
- arXiv ID: 2509.00446
- Source URL: https://arxiv.org/abs/2509.00446
- Reference count: 33
- Primary result: Agents can retrieve relevant information but struggle with planning and narrative integration, achieving F1 scores around 0.15–0.27 for search and edit functions

## Executive Summary
NEWSAGENT introduces a benchmark for evaluating autonomous agents on realistic journalistic workflows, where agents must iteratively search, edit, and rephrase multimodal content to produce news articles under strict temporal constraints. The benchmark features 6,237 human-verified examples from real-world news articles and introduces a structured agentic pipeline with time-aware search and editing functions. Evaluations across open- and closed-source models reveal that no single model dominates; reasoning capacity alone does not guarantee superior newswriting quality.

## Method Summary
The method uses a ReAct framework where agents act as journalists, receiving a news title, release date, and firsthand data (transcripts, descriptions, image captions). Agents iteratively Search for historical context using temporal filtering and semantic similarity, Edit a draft through Insert and Remove actions, and finally Rephrase the output. The benchmark uses 6,237 examples from BBC/APNews, with content formatted as JSON objects. Evaluation includes function-wise F1 scores for Search and Edit actions and end-to-end quality via a dimension-wise GPT-4 comparative assessment across six dimensions.

## Key Results
- Agents achieve F1 scores around 0.15–0.27 for search and edit functions, showing ability to retrieve relevant information but struggle with planning and narrative integration
- No single model dominates end-to-end assessments; open-source Qwen3-32B excels in journalistic style while closed-source GPT-4o leads in readability
- Reasoning capacity alone does not guarantee superior newswriting quality across the evaluated dimensions

## Why This Works (Mechanism)

### Mechanism 1: Time-aware search constrains information access
Time-aware search prevents information leakage by only retrieving content published before the simulated release date. The search function uses temporal filtering combined with semantic similarity (cosine > 0.7), returning top-k results that are strictly historical relative to the article's publication date. This mechanism enforces realistic journalistic constraints where future information is unavailable.

### Mechanism 2: Iterative perception-action loop enables progressive draft refinement
The perception-action loop with Search, Insert, and Remove actions allows agents to incrementally build articles through multiple iterations. At each iteration, the agent observes current draft state, task inputs, retrieved content, and operation messages, then selects an action to modify the draft until termination. This iterative approach enables progressive refinement rather than one-shot generation.

### Mechanism 3: Dimension-wise evaluation captures nuanced quality tradeoffs
Multi-dimensional GPT-4 evaluation reveals that overall quality doesn't correlate directly with factual alignment or human-like content selection. GPT-4 compares articles across six dimensions (Factuality, Logical Consistency, Importance, Readability, Objectivity, Journalistic Style), then synthesizes an overall judgment with chain-of-thought reasoning. This approach captures the multi-faceted nature of human preference for news articles.

## Foundational Learning

- Concept: ReAct framework (Reasoning + Acting)
  - Why needed here: The paper uses ReAct as the core agentic framework, where agents interleave chain-of-thought reasoning with explicit actions (Search, Insert, Remove)
  - Quick check question: Can you explain how ReAct differs from pure chain-of-thought prompting in terms of when actions are executed?

- Concept: Retrieval-Augmented Generation (RAG) with iterative refinement
  - Why needed here: NEWSAGENT extends beyond static RAG by requiring agents to actively discover and integrate information across multiple search iterations
  - Quick check question: How does iterative search differ from one-time retrieval, and what additional capabilities does it require from the agent?

- Concept: Embedding-based semantic similarity search
  - Why needed here: The search function uses cosine similarity with all-MiniLM-L6-v2 embeddings to retrieve relevant historical content
  - Quick check question: What are the tradeoffs between semantic similarity search and keyword-based search for journalistic content retrieval?

## Architecture Onboarding

- Component map:
  Input Layer (title, date, firsthand info) -> Historical Database (time-stamped objects) -> Search Engine (embeddings + cosine similarity + temporal filtering) -> Agent Loop (ReAct: Observation → Action Selection → State Update) -> Output Layer (draft → rephrasing → final article)

- Critical path:
  1. Agent receives title + date + firsthand data
  2. Agent issues Search queries to retrieve historical objects (temporal constraint enforced)
  3. Agent reviews retrieved content and decides to Insert or Remove objects from draft
  4. Loop continues (max 20 operations) until Terminate
  5. Draft is rephrased into final article
  6. Article evaluated via dimension-wise GPT-4 comparison

- Design tradeoffs:
  - 1-step vs 2-step execution: 1-step requires agent to specify query/content in same action; 2-step decomposes for reliability but increases search operations
  - Text-only representation: Converting multimodal content to text ensures broad model compatibility but loses native visual/audio information
  - Exact string matching for Insert/Remove: Prevents hallucination but may reject semantically equivalent content with minor text variations

- Failure signatures:
  - Zero Remove operations: All tested agents never self-corrected, indicating lack of self-evaluation signals in newswriting tasks
  - Low F1 alignment with human articles (0.15–0.27): Agents prioritize different information than human journalists, not necessarily indicating lower quality
  - Insert failures: Agents attempting to insert content not in retrieved set (1.46–5.09 failures on average depending on model/setting)
  - Over-exploration in 2-step mode: More searches without proportional increase in insertions

- First 3 experiments:
  1. Establish baseline with rule-based agent: Retrieve top-5 historical objects by title similarity (threshold 0.8), insert all without reasoning—this provides lower-bound performance
  2. Compare 1-step vs 2-step execution modes: Run same model (e.g., GPT-4o) in both modes to quantify precision-recall tradeoffs and operation efficiency
  3. Dimension-wise error analysis: Compare top closed-source (GPT-4o) vs top open-source (Qwen3-32B) outputs against human articles across all 6 evaluation dimensions to identify specific capability gaps

## Open Questions the Paper Calls Out

### Open Question 1
How can agent architectures be modified to induce self-correction behaviors (specifically the use of "Remove" actions) in open-ended tasks where explicit error signals are absent? The paper observes that agents never invoke the Remove operation, attributing this to the lack of clear failure feedback in journalistic workflows compared to reasoning benchmarks.

### Open Question 2
Does the direct processing of native multimodal inputs (raw images and audio) enhance narrative integration and search relevance compared to the text-only proxy approach used in the study? The current study converts all non-text modalities to textual descriptions to ensure broad model compatibility, leaving the potential added value of raw sensory data untested.

### Open Question 3
Can decomposing the journalistic workflow into specialized sub-agents (e.g., a planner, fact-checker, and editor) overcome the planning limitations observed in single-agent systems? The paper suggests that future work should explore frameworks like AutoGen or Tree-of-Thought to enable specialized agents to collaborate, thereby mimicking professional newsroom practices.

## Limitations

- Temporal filtering mechanism's effectiveness depends heavily on accurate metadata in the corpus, which wasn't independently verified
- The 0.7 cosine similarity threshold for search retrieval appears arbitrary and may not generalize across different domains or time periods
- Evaluation framework relies on GPT-4 judgments without establishing clear thresholds for what constitutes acceptable performance versus human-level quality

## Confidence

- High confidence: The core architecture (ReAct framework with iterative search and edit actions) is technically sound and reproducible
- Medium confidence: The experimental methodology and evaluation protocol are well-documented, though the GPT-4 evaluation introduces some opacity
- Low confidence: The claim that reasoning capacity alone doesn't guarantee newswriting quality requires more extensive cross-model validation

## Next Checks

1. **Temporal constraint validation**: Manually verify 50 random examples to confirm that retrieved historical content is correctly filtered by publication date relative to the target article
2. **Threshold sensitivity analysis**: Test search performance across multiple cosine similarity thresholds (0.5, 0.6, 0.7, 0.8) to establish the robustness of the 0.7 threshold
3. **Human evaluation benchmark**: Compare GPT-4 evaluation agreement rates against direct human assessment on a subset of 100 articles to quantify potential model bias in the evaluation framework