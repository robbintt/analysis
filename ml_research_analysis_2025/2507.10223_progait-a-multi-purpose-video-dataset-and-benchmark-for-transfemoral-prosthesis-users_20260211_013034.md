---
ver: rpa2
title: 'ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis
  Users'
arxiv_id: '2507.10223'
source_url: https://arxiv.org/abs/2507.10223
tags:
- gait
- dataset
- prosthetic
- video
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ProGait, a video dataset of transfemoral
  prosthesis users designed to improve vision-based gait analysis for this population.
  The dataset includes 412 video clips from four above-knee amputees testing different
  prosthetic legs under two scenarios: inside and outside parallel bars.'
---

# ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users

## Quick Facts
- arXiv ID: 2507.10223
- Source URL: https://arxiv.org/abs/2507.10223
- Reference count: 40
- Primary result: Introduces ProGait dataset of 412 video clips from 4 transfemoral prosthesis users, achieving 9-30% performance improvements in vision tasks through domain-specific fine-tuning.

## Executive Summary
This paper introduces ProGait, the first video dataset specifically designed for vision-based gait analysis of transfemoral prosthesis users. The dataset contains 412 video clips from four above-knee amputees testing different prosthetic legs under two scenarios: inside and outside parallel bars. The authors fine-tune baseline models (YOLO11 for VOS, RTMPose for HPE, LSTM for gait classification) and demonstrate significant performance improvements over pre-trained models—up to 9% in VOS mIoU and 10-30% in HPE AP. The work addresses a critical gap by enabling vision models to better detect and analyze prosthetic users, supporting clinical rehabilitation and prosthetic optimization.

## Method Summary
The ProGait dataset provides 412 video clips from four transfemoral prosthesis users walking with different prosthetic configurations. The authors establish a three-stage pipeline: YOLO11 fine-tuned for video object segmentation to isolate subjects, RTMPose fine-tuned for 2D human pose estimation using 23 COCO-WholeBody keypoints, and a custom LSTM classifier trained on lower-body keypoints for gait classification. During training, ground truth annotations are used as inputs for RTMPose and LSTM to prevent error propagation, though this creates a train-test distribution mismatch. The dataset includes detailed textual annotations describing gait deviations and corrective actions for clinical use.

## Key Results
- YOLO11-ProGait achieves 0.847 mIoU for segmentation vs. 0.784 for original YOLO11 (9% improvement)
- RTMPose-ProGait achieves 0.947 AP for pose estimation vs. 0.855 for original RTMPose (10% improvement)
- Sagittal view with lower-body keypoints achieves 0.826 Top-1 accuracy for gait classification, outperforming multi-view approaches
- Fine-tuned models show significantly better detection and analysis of prosthetic users compared to pre-trained models

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Fine-Tuning Improves Prosthesis Detection
Pre-trained models trained on able-bodied populations fail to recognize the unique visual appearance and movement patterns of prosthetic limbs. By exposing these models to prosthesis-specific training data, they learn to identify prosthetic components as part of the human body and distinguish users from healthcare staff. This domain adaptation is critical because prosthetic visual features are sufficiently distinct from able-bodied training data that fine-tuning is required.

### Mechanism 2: Sequential Pipeline Architecture Enables Multi-Task Gait Analysis
The cascaded pipeline (segmentation → pose estimation → classification) enables comprehensive gait analysis from video. Each stage provides cleaner input for the next: YOLO11 generates bounding boxes and segmentation masks to isolate the subject; RTMPose uses these masks to estimate keypoints; LSTM processes temporal pose sequences to classify gait patterns. Ground truth substitution during training prevents error propagation from affecting model learning.

### Mechanism 3: Sagittal View + Lower-Body Keypoints Sufficient for Gait Classification
Gait is fundamentally driven by lower-body dynamics, and sagittal view captures step length, timing, and knee flexion patterns more clearly than frontal view. Upper-body keypoints introduce noise without adding discriminative value for the 9 gait categories. The authors demonstrate that sagittal view alone with lower-body keypoints achieves higher classification accuracy than multi-view or full-body approaches.

## Foundational Learning

- **Transfer Learning / Domain Adaptation**: The paper's core contribution is adapting general vision models to the prosthesis domain. Without understanding transfer learning, the 9-30% improvements appear magical rather than methodologically grounded. Quick check: Can you explain why a model trained on COCO might fail to detect a prosthetic knee, and how fine-tuning on 70% of ProGait addresses this?

- **Object Keypoint Similarity (OKS) and AP Metrics**: The paper evaluates HPE using AP@[0.5, 0.95], which requires understanding OKS as a scale-normalized distance metric. Misinterpreting these metrics leads to over/underestimating model performance. Quick check: If two models have the same raw keypoint error but one is tested on larger subjects, which achieves higher AP and why?

- **Human-in-the-Loop Annotation Pipelines**: The annotation process uses SAM2 + manual correction, with iterative fine-tuning (RTMW) to reduce manual effort. Understanding this loop is critical for reproducing the dataset or extending it. Quick check: Why does the paper fine-tune RTMW on progressively larger frame subsets rather than manually correcting all frames upfront?

## Architecture Onboarding

- **Component map**:
```
Input Video (1920×1080, 30fps)
    ↓
[YOLO11-ProGait] → Bounding Boxes + Segmentation Masks
    ↓
[RTMPose-ProGait] → 23 Keypoints (body + feet)
    ↓
[LSTM Classifier, 128 hidden dim] → Gait Category (9 classes)

Parallel branch:
[Gait Analysis Annotations] → Textual descriptions (4 components per sample)
```

- **Critical path**: Segmentation quality is the bottleneck—if masks don't include prosthetic limbs, pose estimation fails on those regions. Lower-body keypoint accuracy (AP-leg = 0.918) directly determines classification performance. Subject split for test set ensures no data leakage.

- **Design tradeoffs**: Small subject pool (N=4) vs. diversity—each subject tests multiple prosthetic configurations, creating intra-subject variability but limiting inter-subject generalization. Ground truth during training vs. end-to-end pipeline—using GT inputs prevents error propagation but creates train-test distribution mismatch. 23 keypoints vs. 12 for classification—paper shows 12-keypoint model matches or exceeds 23-keypoint accuracy on sagittal view.

- **Failure signatures**: Occlusion by healthcare staff blocks segmentation; prosthetic leg tracking loss occurs despite 96% mIoU; multi-view confusion drops accuracy significantly when combining frontal + sagittal views.

- **First 3 experiments**:
1. Reproduce YOLO11-ProGait fine-tuning on the train split, evaluate mIoU on test split. Compare against zero-shot Grounded SAM2 with prompt "a human with prosthetic leg."
2. Ablate lower-body vs. full-body keypoints for LSTM classification. Confirm that 12-keypoint model matches or exceeds 23-keypoint accuracy on sagittal view.
3. Test cross-subject generalization: Train on 3 subjects, test on the 4th. Report per-subject AP and accuracy to identify whether certain prosthetic types or gait patterns are harder to generalize.

## Open Questions the Paper Calls Out

- **LLM Integration**: How can Large Language Models (LLMs) be effectively integrated with ProGait's textual annotations to automate clinical gait assessments and prosthesis optimization? The authors plan to leverage textual information alongside LLMs to provide quick assessments and reduce clinical workload.

- **Multi-View Fusion**: What multi-view fusion strategies are required to improve gait classification accuracy when combining frontal and sagittal video perspectives? The paper identifies that classification accuracy drops significantly (from 82.6% to 37.2%) when using both views simultaneously compared to sagittal view alone.

- **Broader Demographic Generalization**: Do the performance gains from fine-tuning on ProGait generalize to a broader demographic of prosthesis users and varied prosthetic hardware? The current dataset is limited to four subjects with specific characteristics (middle-aged/elderly, vascular issues).

## Limitations

- Small subject pool (N=4) limits population-level generalization and inter-subject variability assessment
- Controlled laboratory setting may not capture full range of real-world prosthetic usage patterns and environmental conditions
- Ground truth annotations during training create train-test distribution mismatch that may inflate performance estimates
- Limited demographic diversity (focus on middle-aged/elderly with vascular issues)

## Confidence

- **High Confidence**: Domain-specific fine-tuning improves prosthesis detection (supported by clear quantitative improvements: 9% VOS mIoU, 10-30% HPE AP gains)
- **Medium Confidence**: Sagittal view + lower-body keypoints are sufficient for gait classification (based on internal ablation studies, but limited subject diversity)
- **Low Confidence**: Pipeline error propagation is manageable (no direct evaluation of end-to-end performance with predicted vs. ground truth inputs)

## Next Checks

1. **Cross-subject generalization test**: Train on 3 subjects, test on the 4th to assess whether the model generalizes beyond individual movement patterns and prosthetic configurations.

2. **End-to-end pipeline evaluation**: Replace ground truth inputs in RTMPose and LSTM training with predicted outputs from YOLO11-ProGait to measure actual error propagation effects.

3. **Real-world deployment assessment**: Test the trained models on videos captured outside the laboratory setting (different lighting, backgrounds, occlusions) to evaluate robustness in clinical deployment scenarios.