---
ver: rpa2
title: Preliminary Quantitative Study on Explainability and Trust in AI Systems
arxiv_id: '2510.15769'
source_url: https://arxiv.org/abs/2510.15769
tags:
- trust
- explanations
- user
- explainability
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study quantitatively investigates how different explanation
  types affect user trust in AI systems. Using a loan approval simulation, 15 participants
  interacted with AI models under four explanation conditions (none, basic, contextual,
  and interactive counterfactual).
---

# Preliminary Quantitative Study on Explainability and Trust in AI Systems
## Quick Facts
- arXiv ID: 2510.15769
- Source URL: https://arxiv.org/abs/2510.15769
- Reference count: 10
- Primary result: Interactive explanations yield highest trust scores (M=4.22/5) compared to no explanation (M=2.98/5) in loan approval simulation

## Executive Summary
This study investigates how different AI explanation types affect user trust through a controlled loan approval simulation with 15 participants. Four explanation conditions were tested: no explanation, basic explanation, contextual explanation, and interactive counterfactual explanation. Results demonstrate that explanation design directly shapes trust outcomes, with interactive counterfactual explanations proving most effective. The study reveals that interactivity, not just accuracy, is the primary driver of trust enhancement, particularly benefiting novice users who showed greater understanding and perceived reliability.

## Method Summary
The study employed a within-subjects design where 15 participants interacted with AI loan approval models under four different explanation conditions. Participants engaged with a simulated loan approval system where they received either no explanation, basic explanations of decisions, contextual explanations with background information, or interactive counterfactual explanations allowing them to explore "what-if" scenarios. Trust was measured through self-reported surveys immediately after each interaction, assessing perceived reliability, understanding, and overall trust in the AI system.

## Key Results
- Interactive explanations achieved the highest trust scores (M=4.22/5) versus no explanation (M=2.98/5)
- Interactivity significantly enhanced perceived reliability and understanding compared to static explanations
- Novice users benefited most from interactive explanations, showing improved comprehension and trust
- Explanation design quality directly influenced trust outcomes more than model accuracy alone

## Why This Works (Mechanism)
The study demonstrates that human reasoning patterns align with interactive exploration of decision boundaries. When users can manipulate variables and observe counterfactual outcomes, they develop causal understanding that static explanations cannot provide. This active engagement mirrors how humans naturally learn and verify information, creating cognitive alignment between AI decision processes and human reasoning. The interactive counterfactual explanations allow users to test hypotheses about the system's decision logic, building confidence through self-discovery rather than passive acceptance of AI judgments.

## Foundational Learning
1. **Explanation-Trust Relationship**: Trust is not inherent to AI accuracy but is constructed through explanation quality - needed because trust drives adoption in high-stakes domains; quick check: measure trust across explanation quality variations
2. **Interactive Learning Theory**: Users build stronger mental models through active exploration versus passive consumption - needed because static explanations fail to address individual user needs; quick check: compare retention between interactive vs. static explanation groups
3. **Cognitive Load Management**: Interactive explanations must balance information depth with user cognitive capacity - needed because overwhelming users reduces trust; quick check: track task completion time and error rates across explanation types

## Architecture Onboarding
**Component Map**: User Interface -> Explanation Engine -> AI Model -> Decision Logic
**Critical Path**: User query → Explanation generation → Model inference → Decision output → Trust measurement
**Design Tradeoffs**: Rich interactivity improves trust but increases computational overhead and complexity
**Failure Signatures**: Users abandon interaction when explanations exceed 30 seconds generation time or require >3 clicks
**First 3 Experiments**:
1. A/B test different interaction depths (surface vs. deep exploration)
2. Measure trust decay over time with repeated interactions
3. Cross-validate trust scores with actual usage patterns in production

## Open Questions the Paper Calls Out
None

## Limitations
- Extremely small sample size (n=15) severely limits generalizability across populations
- Controlled simulation environment may not reflect real-world decision contexts with actual consequences
- Self-reported trust measures introduce potential response bias without behavioral validation
- Limited explanation type spectrum may not capture full range of practical explanation designs

## Confidence
- Trust score differences between explanation types: Medium confidence
- Interactivity as primary driver of trust: Medium confidence
- Novice user benefits: Low confidence

## Next Checks
1. Replicate with n≥100 participants across diverse demographic groups and professional backgrounds
2. Conduct field studies in actual loan application contexts rather than simulations
3. Implement longitudinal design tracking both stated trust and actual system usage over 3-6 months