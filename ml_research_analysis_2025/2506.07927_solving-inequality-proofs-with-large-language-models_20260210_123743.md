---
ver: rpa2
title: Solving Inequality Proofs with Large Language Models
arxiv_id: '2506.07927'
source_url: https://arxiv.org/abs/2506.07927
tags:
- inequality
- answer
- reasoning
- problem
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of inequality proving for large
  language models (LLMs), a task requiring advanced reasoning skills such as finding
  tight bounds and applying theorems. The authors propose an informal yet verifiable
  reformulation of inequality problems into two automatically checkable subtasks:
  bound estimation and relation prediction.'
---

# Solving Inequality Proofs with Large Language Models

## Quick Facts
- arXiv ID: 2506.07927
- Source URL: https://arxiv.org/abs/2506.07927
- Reference count: 40
- Large Language Models show high answer accuracy but fail step-wise verification on inequality proofs

## Executive Summary
This paper addresses the challenge of inequality proving for large language models (LLMs), a task requiring advanced reasoning skills such as finding tight bounds and applying theorems. The authors propose an informal yet verifiable reformulation of inequality problems into two automatically checkable subtasks: bound estimation and relation prediction. They introduce IneqMath, an expert-curated dataset of Olympiad-level inequality problems, featuring step-wise solutions and theorem annotations. To rigorously evaluate LLM performance, they develop a novel LLM-as-judge framework combining a final-answer judge with four step-wise judges targeting common reasoning flaws. Experiments with 29 leading LLMs reveal a critical gap: while models like o1 achieve high final-answer accuracy (e.g., 62.5%), their overall accuracy plummets to less than 10% under step-wise scrutiny, with some models experiencing drops of up to 65.5%.

## Method Summary
The paper introduces IneqMath, a dataset of 200 Olympiad-level inequality problems reformulated into two automatically verifiable subtasks: bound estimation (finding optimal constants) and relation prediction (determining inequality relations). The authors develop a five-judge LLM-as-judge framework for evaluation: a final answer judge plus four step-wise judges detecting common reasoning flaws (toy case overgeneralization, logical gaps, numerical approximation errors, and arithmetic mistakes). They evaluate 29 leading LLMs on this benchmark, finding that while top models achieve high final answer accuracy, their overall accuracy drops dramatically when step-wise verification is applied. The paper also explores theorem-guided reasoning and self-refinement as promising directions for improvement.

## Key Results
- o1 achieves 62.5% answer accuracy but only 8.0% overall accuracy under step-wise scrutiny
- Scaling model size or increasing test-time computation yields limited gains in rigorous proof construction
- Providing theorem hints improves performance for strong models like Gemini 2.5 Pro (+11%) but decreases it for weaker models
- Logical gaps are the most common failure mode (85% average failure rate)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing inequality proof problems into two checkable subtasks (bound estimation and relation prediction) preserves core reasoning challenges while enabling automatic verification.
- Mechanism: Rather than requiring formal proofs in systems like Lean/Isabelle, the task is reformulated into finding extremal constants or predicting inequality relations. These outputs can be automatically checked for equivalence, reducing evaluation overhead while retaining the need for theorem selection, symbolic manipulation, and bound discovery.
- Core assumption: The subtasks capture the essential reasoning skills of full inequality proving; solving them informally correlates with the ability to construct valid proofs.
- Evidence anchors:
  - [abstract] "recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction"
  - [section 2] Formal definitions of bound estimation (Π_bound) and relation prediction (Π_rel) problem instances.
  - [corpus] Related work on informal theorem proving (DeepTheorem) notes poor alignment between LLMs' informal knowledge and formal proof systems, supporting the need for alternative formulations.
- Break condition: If the subtasks do not require theorem-guided reasoning or tight bound discovery (e.g., problems solvable by pattern matching alone), the reformulation may fail to predict proof capability.

### Mechanism 2
- Claim: A multi-judge evaluation framework combining final-answer verification with four step-wise judges exposes reasoning flaws that single-metric evaluation misses.
- Mechanism: Five specialized LLM-based judges operate in sequence: Final Answer Judge (answer equivalence), Toy Case Judge (unjustified generalization from examples), Logical Gap Judge (missing derivations), Numerical Approximation Judge (non-rigorous substitutions), and Numerical Computation Judge (arithmetic errors). A solution must pass all judges to achieve "overall accuracy."
- Core assumption: The judges reliably detect the targeted error types, and the union of these checks captures most critical reasoning flaws in inequality proofs.
- Evidence anchors:
  - [abstract] "LLM-as-judge evaluation framework... combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws"
  - [section 4.3, Table 3] Judges show strong alignment with human annotations (average F1=0.93) on the development set.
  - [corpus] Related work on step-wise verification (Uncertainty-Aware Step-wise Verification) supports process-level evaluation for multi-step reasoning, though the specific judge categories here are novel.
- Break condition: If judges have systematic blind spots (e.g., subtle symbolic transformation errors) or disagree with expert evaluation on edge cases, overall accuracy may not reflect true proof soundness.

### Mechanism 3
- Claim: Theorem-guided reasoning and self-refinement can improve LLM performance on inequality proving, but effectiveness depends on model capability and relevance of provided theorems.
- Mechanism: (1) Retrieving or providing relevant theorems (e.g., from annotations) as hints during inference; (2) Self-critique loops where models generate and incorporate feedback on their own solutions. Stronger models (e.g., Gemini 2.5 Pro) benefit more from hints, while weaker models may be misled by irrelevant theorems.
- Core assumption: Models can effectively interpret and apply theorem hints, and critique feedback leads to meaningful refinement of reasoning steps.
- Evidence anchors:
  - [abstract] "promising research directions such as theorem-guided reasoning and self-refinement"
  - [section 5.4, Figure 9] Providing 1-2 frequent theorems improved overall accuracy for Gemini 2.5 Pro (+11%) but decreased it for weaker models like Grok 3 mini.
  - [section C.5, Figure 18] Self-critique improved answer accuracy for o3-mini and o4-mini by 2-5%.
  - [corpus] Related work on retrieval-augmented generation (LemmaHead) supports using external knowledge for proof generation, though domain-specific effectiveness varies.
- Break condition: If theorem retrieval is noisy or models cannot distinguish relevant from irrelevant theorems, hints may distract rather than assist. Similarly, if self-critique reinforces incorrect reasoning, refinement may not improve soundness.

## Foundational Learning
- Concept: Informal vs. formal mathematical reasoning
  - Why needed here: The paper deliberately targets informal proof capabilities of LLMs, contrasting with formal proof assistants like Lean/Isabelle. Understanding this distinction is crucial for interpreting the task formulation and evaluation philosophy.
  - Quick check question: Why might an LLM struggle with formal theorem proving even if it can solve the same problem informally in natural language?
- Concept: Step-wise verification and process supervision
  - Why needed here: The core contribution is a multi-judge framework that evaluates reasoning steps, not just final answers. This mirrors process reward models in related work and addresses the gap between answer correctness and proof soundness.
  - Quick check question: If a model achieves 70% answer accuracy but only 10% overall accuracy on IneqMath, what does this suggest about its reasoning process?
- Concept: Inequality proving techniques (e.g., AM-GM, Cauchy-Schwarz, convexity)
  - Why needed here: The IneqMath dataset includes 83 named theorems across 29 categories. Familiarity with common inequality strategies helps understand why theorem-guided reasoning is a promising direction and what makes these problems challenging for LLMs.
  - Quick check question: How might the AM-GM inequality be used to prove that a + b ≥ 2√ab for positive a, b?

## Architecture Onboarding
- Component map: IneqMath test set (200 problems) -> Task reformulation -> Model inference -> 5-judge evaluation -> Answer Acc/Step Acc/Overall Acc
- Critical path: Problem curation → Task reformulation → Model inference → Judge evaluation → Metric computation
- Design tradeoffs:
  - Scalability vs. rigor: LLM-as-judge is more scalable than human evaluation but less reliable for nuanced reasoning flaws
  - Informal vs. formal verification: Informal formulation allows natural language reasoning but lacks absolute correctness guarantees
  - Judge specialization vs. generality: Five specialized judges provide granularity but require maintaining multiple judge prompts and backends
- Failure signatures:
  - High Answer Acc, low Overall Acc: Indicates models find correct answers through flawed reasoning
  - Step-wise error clustering: Most failures from logical gaps (85% avg failure rate) and toy case reasoning (59.7%)
  - Theorem hint ineffectiveness: Weaker models show decreased performance with frequent theorem hints
- First 3 experiments:
  1. Reproduce baseline evaluation: Run the 29 LLMs on the IneqMath test set using provided prompts and judge pipeline
  2. Ablate judge components: Evaluate models with individual judges disabled to quantify each judge's contribution
  3. Test theorem retrieval strategies: Implement naive retrieval vs. semantic similarity-based retrieval

## Open Questions the Paper Calls Out
1. Can sophisticated theorem-retrieval mechanisms (e.g., RAG) reliably enhance LLM performance in inequality proving without causing distraction or misapplication in weaker models?
2. To what extent can critic-guided self-refinement consistently bridge the performance gap between final-answer accuracy and step-wise soundness?
3. How can LLM-as-judge frameworks be enhanced to detect nuanced logical fallacies, specifically unjustified "without loss of generality" (WLOG) assumptions?
4. What alternative training or architectural paradigms are required to surpass the "test-time computation saturation" observed in rigorous proof construction?

## Limitations
- The reformulation into checkable subtasks may not fully capture the complexity of constructing complete proofs
- The LLM-as-judge framework, while showing good alignment with human annotations, has limited error type coverage
- Theorem-guided reasoning experiments show inconsistent effectiveness across model capabilities
- The approach focuses on informal proof capabilities rather than formal verification

## Confidence
- High: Task reformulation and automatic verification mechanism
- Medium: LLM-as-judge evaluation framework reliability
- Medium: Theorem-guided reasoning effectiveness across model types

## Next Checks
1. Conduct ablation studies on the judge framework by testing models with individual judges disabled to quantify each judge's contribution to accuracy drops and identify potential blind spots.
2. Test alternative theorem retrieval strategies beyond the naive top-k frequent theorem approach, such as semantic similarity-based retrieval using training problem embeddings, to improve hint relevance.
3. Evaluate the framework on a subset of problems with complete formal proofs available to validate whether passing all judges correlates with proof soundness in formal systems.