---
ver: rpa2
title: 'Two Heads Are Better than One: Model-Weight and Latent-Space Analysis for
  Federated Learning on Non-iid Data against Poisoning Attacks'
arxiv_id: '2503.23288'
source_url: https://arxiv.org/abs/2503.23288
tags:
- geminiguard
- non-iid
- prob
- noise
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GeminiGuard is a novel defense against model poisoning attacks
  in federated learning under non-iid data conditions. It addresses the challenge
  that non-iid data distributions make benign and malicious model updates appear more
  similar, hindering effective attack detection.
---

# Two Heads Are Better than One: Model-Weight and Latent-Space Analysis for Federated Learning on Non-iid Data against Poisoning Attacks

## Quick Facts
- arXiv ID: 2503.23288
- Source URL: https://arxiv.org/abs/2503.23288
- Reference count: 32
- Primary result: Consistently outperforms 9 baselines across 5 non-iid scenarios, achieving <1% attack success rate under IBA attack on CIFAR-10

## Executive Summary
GeminiGuard is a novel defense against model poisoning attacks in federated learning under non-iid data conditions. It addresses the challenge that non-iid data distributions make benign and malicious model updates appear more similar, hindering effective attack detection. The key insight is that model-weight and latent-space analysis offer complementary perspectives for distinguishing malicious updates. GeminiGuard combines these approaches: a model-weight analysis module clusters updates based on weights, cosine similarity, and Euclidean distance; a latent-space analysis module computes trust scores using multi-layer activations. The method is lightweight, unsupervised, and designed for real-world deployment.

## Method Summary
GeminiGuard combines two complementary analysis modules to detect and filter malicious model updates in federated learning under non-iid data. The model-weight analysis module clusters updates based on weights, cosine similarity, and Euclidean distance, filtering highly suspicious updates before aggregation. The latent-space analysis module computes trust scores using multi-layer activations and Maximum Mean Discrepancy (MMD) to expose anomalies that weights alone miss. The sequential combination of weight-space filtering and latent-space scoring yields complementary coverage across attacks and non-iid types, maintaining high model accuracy while suppressing malicious influence.

## Key Results
- Achieves only 0.18% attack success rate under IBA attack on CIFAR-10 in quantity-based non-iid settings, while baseline defenses exceed 10%
- Consistently outperforms 9 baseline defenses across five non-iid scenarios
- Resilient against adaptive attacks and maintains high model accuracy across datasets and attack types
- Ablation studies show combining both analysis modules performs better than either alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive clustering on combined weight-metric features filters highly suspicious updates before aggregation.
- Mechanism: Construct a feature vector per update z_i = [w_i; ΣCosSim(w_i,w_j); ΣEucDist(w_i,w_j)], then apply K-Means with k* chosen via Silhouette Coefficient. Updates whose distance to the nearest centroid exceeds threshold τ are excluded.
- Core assumption: Malicious updates form outlying clusters or lie outside benign clusters under non-IID.
- Evidence anchors:
  - [section] Algorithm 1, Steps 7–9; Section III-B Step 2 defines z_i and filtering rule min_j||z_i-c_j|| ≤ τ.
  - [abstract] "model-weight analysis module clusters updates based on weights, cosine similarity and Euclidean distance."
  - [corpus] No directly comparable corpus neighbor validates this specific clustering pipeline.
- Break condition: If malicious updates closely mimic benign statistics (e.g., optimized adaptive updates), clustering may admit them, and the burden shifts to latent-space analysis.

### Mechanism 2
- Claim: Multi-layer latent-space distances via MMD expose anomalies that weights alone miss, especially under non-IID.
- Mechanism: Pass a small auxiliary dataset D_A through Θ_i = Θ + δ_i, extract concatenated multi-layer activations LSeq_i, compute pairwise MMD(LSeq_i, LSeq_j), average to obtain AvgDist_i, then derive trust score via softmax over exp(1/AvgDist_k/τ).
- Core assumption: Even when model weights are similar, backdoor/untargeted behaviors create distinguishable activation patterns at certain layers.
- Evidence anchors:
  - [section] Section III-B Steps 3–4; Eq. (1) defines MMD and Eq. (2) the trust score.
  - [abstract] "latent-space analysis module computes trust scores using multi-layer activations."
  - [corpus] Corpus neighbors do not evaluate latent-space MMD for FL poisoning; this is a gap in external validation.
- Break condition: If the auxiliary dataset is unrepresentative or the model architecture has minimal layer-wise divergence, trust scores may lack discriminative power.

### Mechanism 3
- Claim: Sequential combination of weight-space filtering and latent-space scoring yields complementary coverage across attacks and non-IID types.
- Mechanism: Filtering reduces the pool before expensive latent analysis; trust-weighted aggregation then suppresses residual malicious influence without discarding benign updates.
- Core assumption: The two perspectives have non-overlapping failure modes, so joint use improves robustness.
- Evidence anchors:
  - [section] Figure 4 shows layer-wise distance separability degrades under non-IID but remains useful; Table V shows GeminiGuard's ASR stays <1% across attacks and non-IID types where baselines exceed 10–20%.
  - [abstract] "model-weight and latent-space analysis are sufficiently different yet potentially complementary."
  - [corpus] The corpus neighbor "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning" suggests incentive-based robustness but does not validate complementarity of weight vs latent-space methods.
- Break condition: If an adaptive attacker jointly optimizes against both weight clustering and activation distances (Eq. 3), the complementarity advantage may shrink.

## Foundational Learning

- Concept: Non-IID data skew types (label vs feature vs quantity)
  - Why needed here: Defense behavior and benign/malicious overlap vary by non-IID type; understanding dir/prob/qty/noise/qs is essential for interpretation.
  - Quick check question: Can you explain why noise-based non-IID is often less disruptive than quantity-based skew?

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: Used to compare multi-layer activation distributions; intuition: smaller MMD ⇒ more similar latent behavior.
  - Quick check question: Given two sets of activations, would higher MMD correspond to higher or lower trust in GeminiGuard?

- Concept: Silhouette Coefficient for cluster selection
  - Why needed here: Determines k* without supervision; important to know when clustering may fail under high overlap.
  - Quick check question: What would a low average silhouette score suggest about benign/malicious separability in your current round?

## Architecture Onboarding

- Component map: Updates {δ_i} and auxiliary dataset D_A -> Model-weight module (weights, CosSim, EucDist, clustering, filtering) -> Latent-space module (LSeq_i, MMD, trust scores) -> Aggregation (softmax-weighted sum)
- Critical path: Local training -> upload δ_i -> clustering filter -> MMD trust scoring -> weighted aggregation
- Design tradeoffs:
  - More layers in LSeq increase discriminability but add compute
  - Larger D_A improves activation coverage but raises server cost
  - Higher τ admits more updates (potentially more false negatives); lower τ may drop benign updates
- Failure signatures:
  - High ASR despite low dropped-client ratio: Likely adaptive attack evading both modules
  - Sudden model accuracy drop on clean data: Over-aggressive filtering or unrepresentative D_A
  - Low silhouette scores across rounds: Benign/malicious clusters overlapping heavily
- First 3 experiments:
  1. Reproduce the CIFAR-10/IBA scenario under qty non-IID and sweep τ to observe ASR and accuracy tradeoffs
  2. Ablate layers used in LSeq (1 vs 2 vs 3) on a backdoor attack to validate Table VIII trends on a new dataset
  3. Run the adaptive attack formulation from Eq. (3) under a moderate non-IID (e.g., dir) and measure whether joint optimization increases ASR compared to non-adaptive attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is GeminiGuard computationally viable for Large Language Models (LLMs) and high-dimensional parameter spaces?
- Basis in paper: [inferred] The evaluation is restricted to CNNs and ResNet-50; calculating Maximum Mean Discrepancy (MMD) on multi-layer activations for models with billions of parameters may be infeasible.
- Why unresolved: The paper does not analyze memory or time complexity scaling relative to model parameter count.
- What evidence would resolve it: Evaluation on standard LLM benchmarks (e.g., LLaMA, GPT-based FL) measuring server-side memory usage and latency per round.

### Open Question 2
- Question: How does GeminiGuard perform when the server's auxiliary data exhibits a large semantic domain shift from client data?
- Basis in paper: [inferred] Table VI demonstrates robustness using *similar* datasets (e.g., CIFAR-100 for CIFAR-10) but does not test distinct domains (e.g., generic natural images vs. specialized medical imaging).
- Why unresolved: The latent-space analysis relies on activation patterns which may not correlate with client behavior if the input distributions are vastly different.
- What evidence would resolve it: Experiments using a generic auxiliary dataset (e.g., ImageNet) to defend a specialized FL task (e.g., Chest X-ray analysis) and measuring the resulting Attack Success Rate (ASR).

### Open Question 3
- Question: How sensitive is the defense performance to the temperature parameter $\tau$ and clustering threshold?
- Basis in paper: [inferred] Equation 2 introduces a temperature parameter for softmax scaling, yet the ablation study does not test the sensitivity of this hyperparameter across different datasets.
- Why unresolved: It is unclear if these values require manual tuning for every new task to maintain high model accuracy.
- What evidence would resolve it: Ablation studies plotting Model Accuracy and ASR against varying values of the temperature parameter $\tau$ and the clustering distance threshold.

## Limitations

- Non-IID Assumptions: Performance claims are based on non-iid data; under iid data, the complementarity between weight and latent analysis may weaken
- Auxiliary Dataset Representativeness: Trust scores depend on D_A being representative of benign behavior; unrepresentative data may produce unreliable scores
- Computational Overhead: Multi-layer activation extraction and pairwise MMD computation can be costly at scale; practicality for large models is not quantified

## Confidence

- High Confidence: The core design of combining weight-space clustering with latent-space MMD analysis is novel and well-motivated by the non-iid challenge
- Medium Confidence: Empirical results show strong performance against nine baselines, but reproducibility is limited by absence of public code
- Low Confidence: Claims about outperforming all baselines on every attack and non-iid type are difficult to verify without independent replication

## Next Checks

1. Reproduce CIFAR-10/IBA under qty non-IID: Sweep τ and measure ASR/accuracy tradeoffs to verify the main claim of <1% ASR under IBA attack
2. Ablate LSeq layers: Test the impact of using 1, 2, or 3 layers on a backdoor attack across a new dataset to validate Table VIII trends
3. Test adaptive attack (Eq. 3) under dir non-IID: Measure if joint optimization increases ASR compared to non-adaptive attacks, probing the limits of GeminiGuard's resilience