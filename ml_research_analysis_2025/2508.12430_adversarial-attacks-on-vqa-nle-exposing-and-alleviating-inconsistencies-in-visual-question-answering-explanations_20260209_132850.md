---
ver: rpa2
title: 'Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in
  Visual Question Answering Explanations'
arxiv_id: '2508.12430'
source_url: https://arxiv.org/abs/2508.12430
tags:
- question
- explanations
- attack
- knowledge
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the vulnerability of VQA-NLE models to
  adversarial attacks that induce inconsistent explanations. The authors propose two
  attack methods: a text-based attack leveraging BERT-Attack with synonym-based word
  substitution to perturb questions while maintaining semantic coherence, and a novel
  image-based attack that removes objects from images to generate adversarial examples.'
---

# Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations

## Quick Facts
- **arXiv ID**: 2508.12430
- **Source URL**: https://arxiv.org/abs/2508.12430
- **Reference count**: 15
- **Primary result**: Text and image-based adversarial attacks degrade explanation consistency in VQA-NLE models, with knowledge injection mitigating these inconsistencies

## Executive Summary
This paper investigates the vulnerability of VQA-NLE (Visual Question Answering with Natural Language Explanations) models to adversarial attacks that induce inconsistent explanations. The authors propose two attack methods: a text-based attack leveraging BERT-Attack with synonym-based word substitution to perturb questions while maintaining semantic coherence, and a novel image-based attack that removes objects from images to generate adversarial examples. They also introduce a knowledge-based mitigation strategy that appends external knowledge statements to questions to improve explanation consistency. Extensive experiments on VQA-X and A-OKVQA datasets demonstrate that their attacks effectively degrade explanation consistency, with BLEU-2 scores decreasing by up to 4% on VQA-X. The knowledge-based defense shows promise in alleviating these inconsistencies, highlighting the potential for improving VQA-NLE model robustness against adversarial attacks.

## Method Summary
The paper evaluates robustness of VQA-NLE models through text-based adversarial attacks using BERT-Attack with synonym substitution filtered by Universal Sentence Encoder (cosine similarity threshold Ïƒs=0.8), and image-based attacks that remove objects from images via diffusion-based inpainting. The mitigation strategy uses GPT-4o to generate knowledge statements (max 20 words) concatenated with questions, then fine-tunes the DistilGPT2 model until original accuracy is restored. The evaluation uses VQA-X (29k train, 1.4k val pairs) and A-OKVQA (17.1k train, 1.1k val rationales) datasets with metrics including BLEU-1 to BLEU-4, METEOR, ROUGE-L, BERTScore, and answer accuracy, plus Correctness/Detail/Context scores.

## Key Results
- Text-based attacks using BERT-Attack with synonym substitution effectively degrade explanation consistency, with BLEU-2 scores decreasing by up to 4% on VQA-X dataset
- Image-based attacks that remove objects present in explanations but absent from questions further degrade explanation quality by disrupting contextual grounding
- Knowledge-based mitigation strategy that appends external knowledge statements to questions significantly improves explanation consistency, reducing the effectiveness of adversarial attacks
- The proposed attacks expose vulnerabilities in VQA-NLE models' reliance on specific token patterns and scene descriptions rather than robust reasoning

## Why This Works (Mechanism)

### Mechanism 1: Synonym Substitution disrupting Token-level Anchors
- Claim: Replacing words with synonyms disrupts the model's reliance on specific token-pattern associations, causing inconsistent reasoning.
- Mechanism: VQA-NLE models often rely on shallow correlations between specific keywords and answer schemas. By using BERT-Attack to substitute tokens with high-perplexity but semantically equivalent synonyms, the attack breaks these brittle associations without changing the semantic intent.
- Core assumption: The victim model relies more on token frequency patterns than deep semantic grounding.
- Evidence anchors: [abstract] "...leverage an existing adversarial strategy to perturb questions..."; [Section 3.1] "...attack highlights cases where the model associates certain words or phrases with specific answers... revealing a dependency on dataset biases..."; [corpus] "QAVA: Query-Agnostic Visual Attack..." supports the vulnerability of VLMs to input perturbations.
- Break condition: If the model is robust to paraphrase or uses contextual embeddings that deeply integrate synonyms, the attack fails to degrade performance.

### Mechanism 2: Object Removal breaking Contextual Grounding
- Claim: Removing objects that appear in the natural explanation but not the question forces the model to "hallucinate" or switch logic, revealing a lack of holistic scene understanding.
- Mechanism: VQA-NLE models often justify answers by describing the general scene. By identifying objects present in the *explanation* set ($S_E$) but absent from the *question/answer* set ($S_{QA}$) and removing them (inpainting), the attack destroys the visual context the model usually relies on to generate its rationale, exposing that the explanation was merely a description of salient objects rather than a causal justification.
- Core assumption: The model generates explanations by describing visually salient objects rather than performing causal reasoning based solely on the question.
- Evidence anchors: [abstract] "...propose a novel strategy that minimally alters images to induce contradictory or spurious outputs."; [Section 3.2] "...$S_{candidate} := S_E \cap \{S_I \setminus S_{QA}\}$... target object... assumption is that a robust model should continue to generate explanations that accurately reflect the modified image content..."
- Break condition: If the model grounds explanations strictly in the question-relevant object regardless of other scene context, removing "irrelevant" objects will not degrade explanation quality.

### Mechanism 3: Knowledge Injection as Semantic Stabilization
- Claim: Appending external knowledge acts as a "semantic anchor," reducing the model's sensitivity to lexical variations in the question.
- Mechanism: By retrieving definitions or context (e.g., "Events are organized gatherings...") and concatenating it with the question, the model receives an explicit semantic bridge. This forces the cross-attention mechanism to ground the query in the provided definition rather than relying solely on the potentially ambiguous or perturbed tokens of the original question.
- Core assumption: The vision-language model can attend to and utilize the appended knowledge text effectively to override its internal biases.
- Evidence anchors: [abstract] "...mitigation method that leverages external knowledge to alleviate these inconsistencies..."; [Section 3.3] "...incorporating question-specific knowledge can help the model interpret synonymous words more faithfully..."
- Break condition: If the external knowledge is irrelevant, contradictory, or too lengthy, it may introduce noise rather than clarity.

## Foundational Learning

- **Concept: Multimodal Alignment (CLIP/ViT)**
  - Why needed here: The system relies on mapping image patches ($Z_I$) and text tokens ($Z_Q$) into a shared embedding space. Understanding how these modalities align is crucial to grasping why removing an object changes the text output.
  - Quick check question: How does the model handle an input where the image features (a dog) contradict the text features (prompt asking about a cat)?

- **Concept: Adversarial Transferability**
  - Why needed here: The text-attack uses BERT-Attack (a generic NLP attack). You need to understand that attacks trained on one model often work on another (transferability), which is why a general language attack disrupts a specific Vision-Language model.
  - Quick check question: Why would a perturbation designed to fool a BERT model also fool a DistilGPT2-based VQA model?

- **Concept: Seq2Seq Generation & Autoregression**
  - Why needed here: The explanation is generated token-by-token. The "inconsistency" implies that early tokens in the explanation sequence are being derailed by the adversarial input.
  - Quick check question: In the architecture diagram (Figure 1), what serves as the input to the decoder to start generating the explanation sequence?

## Architecture Onboarding

- **Component map**: COCO Image + Question -> CLIP ViT-B/16 (visual encoder) + DistilGPT2 (language decoder) -> Answer + Explanation
- **Critical path**:
  1. Input: COCO Image + Question
  2. Attack (Image): Map Q/A nouns to COCO classes -> Identify $S_{candidate}$ (objects in explanation but not Q/A) -> Inpaint/Remove -> Adversarial Image
  3. Attack (Text): Generate synonym candidates -> Filter by Universal Sentence Encoder (similarity > 0.8) -> Adversarial Question
  4. Defense: Feed Q to GPT-4o -> Get Knowledge $K$ -> Concatenate $Q + K$
  5. Inference: Feed Image + (Q or Q') + (K or Null) -> DistilGPT2 -> Answer + Explanation
- **Design tradeoffs**:
  - Semantic Coherence vs. Attack Success: The threshold $\sigma_s = 0.8$ is critical. Lower it, and you ruin the semantics (invalid attack). Raise it, and the perturbation might be too weak to fool the model.
  - Knowledge Quality: Relying on GPT-4o for "grounding" creates a dependency on an external black box. If the knowledge is hallucinated (e.g., "the dress" example), the defense fails.
- **Failure signatures**:
  - Circular Reasoning: Explanations like "to protect eyes because... wearing goggles" (as seen in Intro)
  - Hallucination: Explanation claims "black and white" for a color image (Figure 4) after object removal
  - Syntax Breakdown: The "Plural" baseline or attacks might cause the generator to output ungrammatical text if the perturbation disrupts the language model's flow significantly
- **First 3 experiments**:
  1. Baseline Vulnerability Scan: Run the "Plural" baseline (simple noun modification) vs. your proposed text-attack on a held-out set of VQA-X to verify that simple synonyms break the model more than grammatical number changes
  2. Ablation on Object Removal: Test if removing objects mentioned in the *question* ($S_{QA}$) vs. objects in the *explanation* ($S_E$) yields different degradation levels
  3. Knowledge Ablation: Replace GPT-4o knowledge with "dummy" or "noisy" text to verify that the defense works due to *semantic relevance* rather than just adding more input tokens

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are Large Vision-Language Models (LVLMs) such as LLaVA and Qwen-VL susceptible to the same explanation inconsistencies under text and image-based adversarial attacks?
- Basis in paper: [explicit] The authors state in the "Conclusion and Future Work" section: "For future work, we plan to extend our investigation to large vision-language models such as LLaVA and Qwen-VL."
- Why unresolved: The current study only evaluates the robustness of DistilGPT2-based models, leaving the vulnerabilities of larger, more complex architectures untested.
- What evidence would resolve it: Experimental results applying the proposed BERT-Attack and object removal strategies to LLaVA and Qwen-VL benchmarks.

### Open Question 2
- Question: Can chain-of-thought (CoT) prompting techniques effectively serve as a defense mechanism to improve explanation consistency under adversarial conditions?
- Basis in paper: [explicit] The authors explicitly list as future work an aim to "explore the effectiveness of prompting techniques, such as chain-of-thought reasoning, as a defense mechanism."
- Why unresolved: The current mitigation strategy relies solely on injecting external knowledge statements, and the efficacy of reasoning-based prompting remains unexplored.
- What evidence would resolve it: A comparative analysis measuring explanation consistency (e.g., BLEU scores) on adversarial samples when using CoT prompts versus the proposed knowledge-injection method.

### Open Question 3
- Question: How sensitive is the knowledge-based alleviation strategy to the relevance and accuracy of the retrieved external knowledge statements?
- Basis in paper: [explicit] The "Limitations" section notes that the method "depends on the question-related knowledge, which may not be effective in certain cases," specifically citing examples where retrieved knowledge is irrelevant.
- Why unresolved: The paper demonstrates successful alleviation but acknowledges failures when the knowledge generator produces unhelpful context, suggesting a dependency that hasn't been quantitatively bounded.
- What evidence would resolve it: An ablation study introducing varying levels of noise or irrelevance into the knowledge statements to observe the point at which the alleviation strategy fails or degrades performance.

### Open Question 4
- Question: Does applying simultaneous text-based and image-based perturbations lead to a compounding degradation in explanation consistency compared to single-modality attacks?
- Basis in paper: [inferred] The paper notes in a footnote that text and image manipulations are "primarily evaluate[d]... separately," but "in principle they can also be combined."
- Why unresolved: While the authors suggest the possibility, they do not provide experimental data on the synergistic effects of multi-modal attacks on model robustness.
- What evidence would resolve it: Experimental data showing consistency metrics for samples subjected to both synonym substitution and object removal simultaneously versus individually.

## Limitations
- The knowledge-based defense relies heavily on the quality and relevance of external knowledge from GPT-4o, which can hallucinate or produce irrelevant information that degrades performance
- The image-based attack assumes explanations primarily describe scene context rather than question-relevant details, which may not hold for all VQA-NLE architectures
- The proposed attacks may overestimate vulnerability if VQA-NLE models already focus explanations on question-relevant objects regardless of scene context

## Confidence
- **High Confidence**: The text-based attack methodology using BERT-Attack with semantic coherence preservation (cosine similarity > 0.8) is well-established and the experimental results showing BLEU-2 score decreases are reliable
- **Medium Confidence**: The image-based attack mechanism is novel and theoretically sound, but the assumption about explanation generation patterns may not hold universally across different VQA-NLE architectures
- **Medium Confidence**: The knowledge-based mitigation strategy shows empirical improvements but depends heavily on the quality of GPT-4o-generated knowledge, which varies substantially

## Next Checks
1. **Cross-dataset Generalization Test**: Evaluate the attack and defense mechanisms on additional VQA datasets beyond VQA-X and A-OKVQA to assess whether the observed inconsistencies are dataset-specific or represent broader architectural vulnerabilities

2. **Knowledge Quality A/B Test**: Systematically compare the knowledge-based defense against multiple knowledge sources (including manually curated knowledge, different language models, and random noise) to isolate whether improvements come from semantic relevance versus simple input augmentation

3. **Human Evaluation of Explanations**: Conduct human studies to verify that the measured "inconsistencies" identified by automated metrics (BLEU, METEOR, etc.) actually correspond to meaningful reasoning failures that humans would recognize as poor explanations