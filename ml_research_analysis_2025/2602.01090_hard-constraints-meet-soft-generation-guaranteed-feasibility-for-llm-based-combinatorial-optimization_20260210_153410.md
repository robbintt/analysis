---
ver: rpa2
title: 'Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based
  Combinatorial Optimization'
arxiv_id: '2602.01090'
source_url: https://arxiv.org/abs/2602.01090
tags:
- repair
- solution
- each
- constraints
- feasibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FALCON, a framework that guarantees 100%
  feasibility for large language model (LLM)-based combinatorial optimization by combining
  grammar-constrained decoding, feasibility repair, and adaptive sampling. The authors
  address the fundamental weakness of LLMs in satisfying hard constraints, which is
  critical for real-world deployment.
---

# Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization

## Quick Facts
- arXiv ID: 2602.01090
- Source URL: https://arxiv.org/abs/2602.01090
- Reference count: 40
- Primary result: FALCON achieves 100% feasibility while matching or exceeding state-of-the-art neural and LLM-based solvers across seven NP-hard CO problems

## Executive Summary
This paper introduces FALCON, a framework that guarantees 100% feasibility for large language model (LLM)-based combinatorial optimization by combining grammar-constrained decoding, feasibility repair, and adaptive sampling. The authors address the fundamental weakness of LLMs in satisfying hard constraints, which is critical for real-world deployment. FALCON ensures solution validity through three innovations: (i) grammar-constrained decoding enforces syntactic validity, (ii) a feasibility repair layer corrects semantic constraint violations, and (iii) adaptive Best-of-N sampling allocates inference compute efficiently. To train the underlying LLM, they introduce Best-anchored Objective-guided Preference Optimization (BOPO), which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, they prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

## Method Summary
FALCON is a two-stage pipeline: first, supervised fine-tuning (SFT) on expert solutions using LoRA adapters; second, refinement with BOPO that weights preference pairs by objective gaps. During inference, grammar-constrained decoding enforces syntactic validity by masking invalid tokens through a pushdown automaton, while a feasibility repair layer ensures semantic constraints are satisfied. Adaptive Best-of-N sampling balances solution quality and inference efficiency. The framework is evaluated on seven NP-hard CO problems (TSP, CVRP, OP, MIS, MVC, PFSP, JSSP) with synthetic data, achieving 100% feasibility rates and competitive optimality gaps relative to reference solvers like LKH-3 and Gurobi.

## Key Results
- FALCON achieves 100% feasibility rates across all seven NP-hard CO problems tested
- Solution quality matches or exceeds state-of-the-art neural and LLM-based solvers
- BOPO training improves solution quality and feasibility rates compared to GRPO
- Adaptive sampling reduces inference compute while maintaining solution quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grammar-constrained decoding enforces syntactic validity (format correctness) with probability one.
- Mechanism: A context-free grammar (CFG) is converted to a pushdown automaton (PDA). At each decoding step, the PDA state and stack determine which tokens lead to valid derivations; all others are masked (logits set to -∞), preventing syntactically invalid continuations.
- Core assumption: The grammar correctly specifies all valid output formats for the problem class and the PDA implementation correctly tracks state.
- Evidence anchors:
  - [abstract] "grammar-constrained decoding enforces syntactic validity"
  - [section] Definition 3.1 (CO Output Grammar), Definition 3.2 (Input-Dependent Grammar), Theorem 3.3 (Format Validity Guarantee), Algorithm 1.
  - [corpus] Related work on constrained text generation exists (e.g., "Soft-Radial Projection for Constrained End-to-End Learning"), but no direct external corroboration of this specific grammar-constrained decoding method.
- Break condition: If the grammar is incomplete or the PDA implementation is buggy, invalid tokens may not be correctly masked, leading to syntactically invalid outputs.

### Mechanism 2
- Claim: The feasibility repair layer guarantees 100% semantic feasibility by mapping any (possibly infeasible) solution to a feasible one, with bounded quality degradation.
- Mechanism: Problem-specific repair operators enforce semantic constraints (e.g., capacity, independence, precedence). Each operator satisfies: (1) Feasibility: maps any solution to feasible; (2) Idempotence: leaves already-feasible solutions unchanged; (3) Bounded Locality: modification distance is proportional to violation magnitude. The paper proves repair adds at most Lf·α·v(x) to the objective.
- Core assumption: The objective function is Lipschitz continuous with respect to the solution distance metric, and the repair operator's locality constant α is finite.
- Evidence anchors:
  - [abstract] "feasibility repair layer corrects semantic constraint violations"
  - [section] Definition 3.5 (Repair Operator), Theorem 3.6 (100% Feasibility Guarantee), Theorem 3.7 (Repair Quality Bound), Table 1, Appendix E.
  - [corpus] "FSNet: Feasibility-Seeking Neural Network for Constrained Optimization with Guarantees" suggests feasibility-seeking approaches, but no direct corroboration of these specific repair operators.
- Break condition: If the objective is not Lipschitz or the repair operator violates the locality bound, quality guarantees may not hold. If the repair operator has bugs, feasibility may not be guaranteed.

### Mechanism 3
- Claim: BOPO training provides dense, objective-guided supervision that improves solution quality and feasibility rates compared to GRPO.
- Mechanism: BOPO constructs preference pairs anchored to the best feasible solution in a batch, weighted by the objective gap (Δi). Larger gaps contribute more to the gradient. The paper proves O(1/√T) convergence under smoothness and bounded variance assumptions.
- Core assumption: The loss function is L-smooth, stochastic gradients have bounded variance, and objective-guided scaling factors w(Δ) are bounded.
- Evidence anchors:
  - [abstract] "Best-anchored Objective-guided Preference Optimization (BOPO)... weights preference pairs by their objective gap"
  - [section] Section 3.4, Definition 3.14 (BOPO Loss), Theorem 3.15 (BOPO Convergence), Figure 2, ablation study (Table 4, "w/o BOPO").
  - [corpus] Related work on preference optimization exists, but no direct external corroboration of BOPO's convergence or empirical performance.
- Break condition: If scaling factors w(Δ) are unbounded (e.g., near-zero average gaps), variance may explode. If the LLM lacks capacity or training distribution is narrow, generalization may fail.

## Foundational Learning

- Concept: Context-Free Grammars (CFGs) and Pushdown Automata (PDAs)
  - Why needed here: Grammar-constrained decoding relies on a CFG to define valid output formats and a PDA to track parse states during token generation.
  - Quick check question: Given a CFG with production S → "Route: [" NodeList "]", what tokens are valid immediately after "Route: ["?

- Concept: Combinatorial Optimization Problem Formulation (feasible region, constraints, objective)
  - Why needed here: The repair layer and BOPO training both depend on understanding the feasible region XCp and the objective fp.
  - Quick check question: For a CVRP with vehicle capacity Q and customer demands {qi}, what makes a solution infeasible?

- Concept: Preference Optimization and Convergence Rates
  - Why needed here: BOPO is a variant of preference optimization. Understanding the loss function, gradient structure, and convergence guarantees (O(1/√T)) is necessary to diagnose training issues.
  - Quick check question: In BOPO, if the best solution in a batch has objective 100 and another has objective 150, what is the objective gap Δ?

## Architecture Onboarding

- Component map:
  - Input Processing: Problem instance p → text description ϕ(p)
  - LLM Core: Base LLM (e.g., Qwen2.5-7B) with LoRA adapters, trained via SFT then BOPO
  - Grammar-Constrained Decoder: CFG → PDA; mask invalid tokens at each step
  - Feasibility Repair Layer: Problem-specific repair operators (Table 1)
  - Adaptive Sampler: Collect N samples (Nmin ≤ N ≤ Nmax); Bayesian confidence-based early stopping
  - Output: Best repaired feasible solution by objective fp

- Critical path:
  1. Training: SFT on expert solutions → BOPO refinement with repair layer
  2. Inference: Prompt → grammar-constrained decoding → repair each sample → adaptive selection → return best feasible solution

- Design tradeoffs:
  - Grammar complexity vs. flexibility: More expressive grammars are harder to debug; simpler grammars may limit coverage
  - Repair aggressiveness vs. quality: Greedy repair is fast and bounded but may be suboptimal; sophisticated repair could improve quality but increase complexity
  - Adaptive sampling thresholds: Lower τ increases efficiency but risks suboptimal solutions; higher τ improves quality but increases compute

- Failure signatures:
  - Syntactically invalid outputs: Grammar or PDA implementation incorrect/incomplete
  - Infeasible outputs after repair: Repair operator bugs or incomplete constraint coverage
  - Poor optimality gaps: BOPO training not converged, model under-capacity, or distribution mismatch
  - Excessive inference time: Adaptive sampler not triggering early termination (τ too high or confidence miscalibrated)

- First 3 experiments:
  1. Validate grammar-constrained decoding: Run inference on a small test set; verify 100% of raw (pre-repair) outputs are syntactically valid per the CFG
  2. Test repair operators: Generate solutions with deliberate violations (duplicates, capacity overflows); verify repair produces feasible solutions with bounded quality degradation
  3. Compare BOPO vs. GRPO: Train two models from the same SFT checkpoint with identical budgets; compare feasibility rates and optimality gaps on held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can problem-specific repair operators be learned automatically rather than manually designed, preserving the feasibility guarantee while reducing engineering effort?
- Basis in paper: [explicit] The authors state in Table 1 and Appendix E that repair operators require problem-specific design (e.g., "TSP repair per route + split overloaded routes" for CVRP, "Remove higher-degree vertex from each conflict" for MIS).
- Why unresolved: Manual design for each problem class limits scalability to new domains; the paper provides no mechanism for automatic repair operator discovery or learning.
- What evidence would resolve it: Demonstrating learned repair operators that satisfy Definition 3.5 properties (feasibility, idempotence, bounded locality) on novel problem types without manual specification.

### Open Question 2
- Question: How does FALCON scale to problem instances with 1000+ nodes or complex real-world constraint structures beyond the synthetic benchmarks (10–100 nodes) tested?
- Basis in paper: [inferred] Table 9 shows test instances only up to 100 nodes for routing and scheduling problems; Theorem 3.7's quality bound depends on violation magnitude v(x̂_p), which may grow with problem scale.
- Why unresolved: No empirical validation beyond moderate synthetic instances; repair operator complexity (e.g., O(n²) for TSP) may become prohibitive for large-scale industrial problems.
- What evidence would resolve it: Evaluation on large-scale benchmarks (e.g., TSPLIB instances with 1000+ cities, real-world logistics datasets) showing maintained 100% feasibility and competitive optimality gaps.

### Open Question 3
- Question: Can the grammar-constrained decoding and repair framework extend to combinatorial problems with continuous variables or mixed-integer formulations?
- Basis in paper: [inferred] The grammar formalism (Definition 3.1-3.2) generates discrete terminal symbols; all seven evaluated problems have purely discrete solution spaces (permutations, subsets, sequences).
- Why unresolved: Context-free grammars cannot naturally represent continuous decision variables; the repair layer assumes finite violation corrections.
- What evidence would resolve it: Extending FALCON to mixed-integer programming (MIP) or continuous optimization domains while preserving formal feasibility guarantees.

## Limitations
- Theoretical guarantees rely on assumptions about Lipschitz continuity and bounded variance that may not hold in practice
- Manual design of problem-specific grammars, PDAs, and repair operators limits scalability to new problem domains
- Computational overhead of grammar-constrained decoding and feasibility repair is not fully characterized
- Empirical evaluation limited to synthetic data distributions and moderate-sized instances (10-100 nodes)

## Confidence
- **High Confidence**: The empirical results demonstrating 100% feasibility rates and competitive optimality gaps across all 7 test problems are well-supported by the experimental section and tables
- **Medium Confidence**: The theoretical convergence proofs for BOPO and the repair quality bounds are mathematically sound given the stated assumptions, but practical tightness remains uncertain
- **Low Confidence**: The scalability claims and generalization to real-world problem distributions are not empirically validated; implementation complexity is acknowledged but not quantified

## Next Validation Checks
1. Evaluate FALCON on CO instances drawn from distributions significantly different from the training data to assess generalization limits
2. For problems where repair is frequently triggered, measure the actual objective degradation caused by repair and compare it to the theoretical bound
3. Implement FALCON for one CO problem (e.g., TSP) following the paper's specifications and attempt to reproduce the feasibility and optimality results on provided test instances