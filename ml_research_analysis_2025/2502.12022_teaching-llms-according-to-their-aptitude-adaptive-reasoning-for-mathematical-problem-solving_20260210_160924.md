---
ver: rpa2
title: 'Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical
  Problem Solving'
arxiv_id: '2502.12022'
source_url: https://arxiv.org/abs/2502.12022
tags:
- reasoning
- tata
- llms
- math
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TATA, a framework that enables large language
  models (LLMs) to adaptively choose between Chain-of-Thought (CoT) and Tool-Integrated
  Reasoning (TIR) for mathematical problem solving. The core idea is to tailor training
  data to each model's inherent aptitude through base-LLM-aware data selection during
  supervised fine-tuning.
---

# Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving

## Quick Facts
- **arXiv ID**: 2502.12022
- **Source URL**: https://arxiv.org/abs/2502.12022
- **Reference count**: 40
- **Primary result**: TATA framework enables LLMs to adaptively choose between Chain-of-Thought and Tool-Integrated Reasoning, achieving superior or comparable performance while reducing computational overhead

## Executive Summary
This paper introduces TATA, a framework that enables large language models to adaptively choose between Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR) for mathematical problem solving. The core innovation is base-LLM-aware data selection during supervised fine-tuning, which tailors training data to each model's unique aptitude. TATA constructs an anchor set to evaluate model performance with different reasoning strategies, then uses these evaluations to curate a personalized training dataset. Extensive experiments across six math reasoning benchmarks with both general-purpose and math-specialized LLMs demonstrate that TATA achieves superior or comparable performance to existing methods while significantly improving inference efficiency.

## Method Summary
TATA builds on existing math reasoning datasets (GSM8K and MATH) and augments them with DART-Math-Hard via rejection fine-tuning. The framework converts CoT solutions to TIR format using GPT-4o, then constructs an anchor set via K-means clustering on text embeddings. For each query, TATA computes one-shot performance scores for both CoT and TIR strategies on the anchor set. Data selection uses quantile thresholds on these score differences to determine which strategy to train for each query. Models are then fine-tuned using Adam optimizer (lr=2e-5, batch size=64, 3 epochs) on the curated dataset, enabling them to spontaneously select the appropriate reasoning strategy at inference time.

## Key Results
- TATA achieves superior or comparable performance to existing methods on six math reasoning benchmarks
- Significantly reduces computational overhead by minimizing unnecessary code executions (1.08-1.43 executions vs 2.6-3.2 for TIR-only)
- Aptitude-aware data selection is critical - models perform worse when trained on data selected by different base LLMs
- Improves inference efficiency without sacrificing the benefits of tool integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Base-LLM-aware data selection enables models to learn which reasoning strategy aligns with their inherent capabilities
- Mechanism: TATA evaluates each base LLM's performance on an anchor set using one-shot CoT vs. TIR prompts, then assigns each training query to the strategy that yields higher anchor accuracy for that specific model
- Core assumption: A model's one-shot performance differential predicts which training format will generalize better
- Evidence: Transferability experiments show Qwen2.5-0.5B performs worse when trained on data selected by LLaMA-3-8B vs. its own selected data (45.0% vs. 44.1%)

### Mechanism 2
- Claim: Implicit Instruction Tuning (IIT) through in-context learning provides a tractable proxy for estimating SFT effects
- Mechanism: The paper leverages that attention outputs decompose into test-input-only and demonstration-sample terms, where one-shot demonstrations implicitly adjust model behavior analogously to gradient-based fine-tuning
- Core assumption: ICL dynamics approximate the direction of SFT gradient updates for strategy preference learning
- Evidence: Formal derivation showing attention ≈ WV·Xtest·(WK·Xtest)^T·Q + WV·Xins·(WK·Xins)^T·Q, where the second term acts as implicit parameter adjustment

### Mechanism 3
- Claim: Training on aptitude-matched mixed-strategy data induces spontaneous strategy selection at inference without external routers
- Mechanism: By exposing the model to CoT examples for queries where it excels at CoT, and TIR examples where it excels at TIR, the model learns a conditional policy for problem-type specific strategy selection
- Core assumption: The model learns to associate query features with strategy effectiveness through supervised learning on curated examples
- Evidence: TATA models achieve intermediate code execution counts (1.08-1.43) indicating adaptive tool use compared to 0 for CoT-only and 2.6-3.2 for TIR-only

## Foundational Learning

- **Tool-Integrated Reasoning (TIR) Pipeline**
  - Why needed here: TATA assumes familiarity with how TIR interleaves natural language and code execution
  - Quick check question: Can you trace how a TIR model handles "Calculate √(2π × 7.3)" step-by-step, including when code executes?

- **Rejection Fine-Tuning (RFT)**
  - Why needed here: TATA builds on RFT-augmented datasets (DART-Math-Hard)
  - Quick check question: Given 10 sampled responses, how does RFT determine which to keep for training?

- **Quantile-Based Thresholding**
  - Why needed here: Data selection uses quantiles of (S^CoT - S^TIR) distributions to partition queries
  - Quick check question: If the 30th percentile of score differences is -0.15, what does selecting queries below this threshold mean for strategy preference?

## Architecture Onboarding

- **Component map**: D_orig → RFT augmentation (D_aug) → GPT-4o CoT-to-TIR conversion → Candidate set D with (query, CoT, TIR) triplets → Query embeddings → K-means clustering (k=100) → Cluster centers as D_anchor → Compute S_CoT and S_TIR scores → Apply quantile thresholds → Build D_SFT → Standard SFT

- **Critical path**: Anchor set quality directly affects score reliability → One-shot evaluation correctness determines selection accuracy → Quantile threshold tuning affects CoT/TIR balance

- **Design tradeoffs**: Anchor set size A=100 balances coverage vs. computation; A=200 shows diminishing returns; random vs. clustered anchor significantly impacts performance; naive inclusion of both strategies underperforms TATA

- **Failure signatures**: High code execution count with low accuracy indicates over-reliance on TIR; near-zero code executions on MATH suggests CoT-bias; transferability failure shows aptitude-specificity

- **First 3 experiments**: 1) Compare k-means vs random sampling anchor sets on validation accuracy; 2) Run TATA with threshold pairs [(20,50), (30,60), (40,70)] to verify robustness; 3) Train Qwen2.5-3B on data selected by LLaMA-3-8B to measure accuracy delta

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the TATA framework's aptitude-aware adaptive reasoning be effectively generalized to non-mathematical domains, such as logical deduction or scientific reasoning?
- Basis: Section 7 states extending adaptive tool use to "generalized reasoning scenarios" is promising for future research
- Why unresolved: TATA was validated exclusively on mathematical benchmarks; applicability to domains where computation is less central than logical structuring is unclear
- Evidence needed: Successful application to diverse reasoning benchmarks (e.g., Big-Bench Hard, ScienceWorld)

### Open Question 2
- Question: Does a fine-grained, step-level selection strategy between CoT and TIR offer significant advantages over instance-level selection?
- Basis: Section 7 identifies "investigating a more fine-grained, step-level selection strategy" as interesting direction
- Why unresolved: Current framework decides on a reasoning modality for entire problem instance; unknown if switching modalities mid-solution could improve efficiency
- Evidence needed: Modified training paradigm allowing dynamic modality switching within single solution trajectory

### Open Question 3
- Question: Can advanced reinforcement learning methods surpass the performance ceilings of Supervised Fine-Tuning in teaching models adaptive reasoning?
- Basis: Section 6.3 notes DPO yields only minor improvements over TATA; Section 7 suggests exploring RL or diverse data
- Why unresolved: Paper concludes SFT effectively elicits inherent capabilities but leaves open whether RL algorithms could further optimize decision boundaries
- Evidence needed: Experiments using policy gradient methods resulting in statistically significant gains over SFT-based TATA

## Limitations
- Framework's applicability limited to mathematical reasoning - generalizability to other domains untested
- Reliance on one-shot ICL as SFT proxy is theoretically grounded but empirically under-validated
- Effect size of aptitude-aware selection modest (45.0% vs 44.1%/44.6% performance degradation)

## Confidence

- **High Confidence**: Core empirical results showing TATA outperforms or matches baselines while reducing inference compute
- **Medium Confidence**: Claim that aptitude-aware data selection is "critical" - supported by transfer experiments but effect size modest
- **Medium Confidence**: Mechanism of implicit instruction tuning as SFT proxy - theoretical derivation provided but approximation validity unverified

## Next Checks
1. Apply TATA framework to non-mathematical reasoning tasks (e.g., code generation, commonsense reasoning) to test generalizability beyond math problems
2. Compare one-shot ICL scores against ground-truth SFT performance for subset of models and queries to quantify approximation error
3. Systematically investigate training dynamics when queries are poorly suited to either strategy to understand fallback mechanisms development