---
ver: rpa2
title: 'GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs
  in Domain-Specific Knowledge and Reasoning'
arxiv_id: '2505.22661'
source_url: https://arxiv.org/abs/2505.22661
tags:
- knowledge
- card
- evaluation
- reasoning
- guess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GUESS ARENA, an interactive framework for
  evaluating large language models (LLMs) in domain-specific knowledge and reasoning.
  Inspired by the "Guess Who I Am?" game, it uses multi-turn dialogue to assess how
  well models identify target cards from domain-specific decks, enabling dynamic,
  scalable, and interpretable evaluation.
---

# GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning

## Quick Facts
- **arXiv ID**: 2505.22661
- **Source URL**: https://arxiv.org/abs/2505.22661
- **Reference count**: 34
- **Primary result**: GUESS ARENA uses multi-turn dialogue games to evaluate LLM domain-specific reasoning and knowledge, showing higher adaptability and interpretability than static benchmarks.

## Executive Summary
GUESS ARENA introduces an interactive, self-adaptive framework for evaluating large language models' domain-specific knowledge and reasoning abilities. Inspired by the "Guess Who I Am?" game, it uses multi-turn dialogue where an evaluated model (Player) asks binary questions to identify a target card from a domain-specific deck, with a Judge model (GPT-4o) providing answers. The framework automatically constructs domain knowledge bases from user-provided documents and extracts relevant cards using adaptive algorithms, reducing manual effort. Experiments across five industries demonstrate GUESS ARENA's effectiveness in differentiating LLM performance in reasoning accuracy, efficiency, and knowledge applicability, while offering higher adaptability and interpretability compared to static benchmarks.

## Method Summary
The framework operates through a multi-stage pipeline: First, it ingests unstructured domain documents (PDF, HTML) and constructs a knowledge base using RAG extraction with GPT-4o, followed by semantic filtering and spectral clustering to generate domain-specific card decks. During evaluation, a Judge model answers binary questions about a target card while the Player model iteratively asks questions to identify it. The system scores performance using a composite metric combining accuracy (whether the target card is correctly identified), efficiency (number of turns taken), and knowledge applicability (how effectively domain knowledge is applied). The framework supports various prompt strategies, including step-by-step reasoning and knowledge injection, to enhance performance for weaker models.

## Key Results
- GUESS ARENA successfully differentiates LLM performance across five industries using composite scores combining accuracy, efficiency, and knowledge applicability
- The framework demonstrates higher adaptability and interpretability compared to static benchmarks by using dynamic multi-turn dialogue evaluation
- Prompt strategies like step-by-step reasoning and knowledge injection significantly enhance performance for weaker models
- The adaptive card extraction algorithm reduces manual effort while maintaining domain relevance and coverage

## Why This Works (Mechanism)
The framework works by transforming LLM evaluation into an interactive guessing game that reveals both knowledge depth and reasoning capability through multi-turn dialogue. The Judge model serves as a consistent oracle, while the Player's question-asking strategy demonstrates how well the model can navigate and apply domain knowledge. The composite scoring captures not just whether the model gets the right answer, but how efficiently it arrives there using relevant knowledge. The adaptive card extraction ensures the evaluation stays relevant to specific domains without requiring extensive manual curation, making the framework scalable across industries.

## Foundational Learning
- **RAG-based Knowledge Extraction**: Extracts relevant information from unstructured documents to build domain-specific knowledge bases; needed to automate knowledge base creation without manual curation; quick check: verify extracted passages contain relevant domain terms.
- **Spectral Clustering for Card Generation**: Groups semantically similar keywords into coherent cards; needed to organize extracted knowledge into meaningful evaluation units; quick check: examine cluster coherence by manually reviewing card contents.
- **Multi-turn Dialogue Evaluation**: Uses iterative question-answering to assess reasoning depth; needed to capture dynamic reasoning processes rather than just final answers; quick check: verify question diversity and progressive narrowing of candidate space.
- **Composite Scoring with Baselines**: Combines multiple metrics weighted equally with random baseline comparisons; needed to provide balanced evaluation across different aspects of performance; quick check: recalculate scores with different random baselines to test stability.

## Architecture Onboarding

**Component Map**: Documents -> RAG Extraction -> Semantic Filtering -> Spectral Clustering -> Card Deck -> Judge Model <- Player Model (multi-turn dialogue) -> Composite Scoring

**Critical Path**: The evaluation loop is the critical path: Player asks question → Judge answers → Player narrows candidates → Repeat until target identified or max turns reached → Calculate composite score

**Design Tradeoffs**: Uses GPT-4o as Judge for consistency but introduces potential hallucination bias; automates card generation to reduce manual effort but may miss nuanced domain distinctions; composite scoring provides balanced evaluation but requires careful baseline calibration.

**Failure Signatures**: Judge hallucination produces incorrect Yes/No answers affecting downstream reasoning; reasoning loops occur when models repeat questions or fail to narrow search space; efficiency degradation when models take excessive turns without progress.

**First Experiments**:
1. Run Basic prompt template against Judge for 30 rounds per domain using explicit keyword lists to validate core game mechanics
2. Test random baseline by having a uniform guesser play the game to establish $t_{rand}$ values
3. Compare composite scores across different LLM models to verify differentiation capability

## Open Questions the Paper Calls Out
None

## Limitations
- Raw source documents used for knowledge base construction are not publicly available, preventing full pipeline verification
- Judge model (GPT-4o) is treated as an oracle without human validation, raising potential hallucination concerns
- Specific values for random baseline turns ($t_{rand}$) and knowledge background injection prompts are not provided, limiting exact score reproduction

## Confidence
- **High**: Claims about game's ability to differentiate LLM performance using explicit keyword lists and Basic prompt template
- **Medium**: Claims about framework's adaptability and reduced manual effort (depend on unpublished document ingestion pipeline)
- **Medium**: Claims about prompt strategy impact (rely on unspecified prompt content)

## Next Checks
1. **Human validation of Judge responses**: Randomly sample 50 Judge answers across domains and verify correctness against card definitions using domain experts
2. **Full pipeline reproducibility**: Reconstruct knowledge base from publicly available documents (e.g., SEC filings for Finance, WHO guidelines for Healthcare) and compare resulting card sets to Appendix C.1 for overlap and coverage
3. **Random baseline estimation**: Run uniform random guesser across all domains to empirically estimate $t_{rand}$ and recalculate Efficiency and Knowledge Applicability scores for consistency with reported values