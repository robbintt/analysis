---
ver: rpa2
title: Enhancing Adversarial Transferability by Balancing Exploration and Exploitation
  with Gradient-Guided Sampling
arxiv_id: '2511.00411'
source_url: https://arxiv.org/abs/2511.00411
tags:
- adversarial
- sampling
- methods
- loss
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Gradient-Guided Sampling (GGS), an inner-iteration
  sampling strategy for adversarial attacks that balances exploration (cross-model
  generalization) and exploitation (attack potency). GGS guides sampling using gradients
  from the previous inner-iteration to achieve stable ascent toward flat loss regions
  with higher local maxima.
---

# Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling

## Quick Facts
- **arXiv ID:** 2511.00411
- **Source URL:** https://arxiv.org/abs/2511.00411
- **Reference count:** 40
- **Primary result:** Introduces Gradient-Guided Sampling (GGS) for adversarial attacks, achieving 82.08% untargeted and 17.67% targeted average transferability across 9 models.

## Executive Summary
This paper introduces Gradient-Guided Sampling (GGS), a novel inner-iteration sampling strategy for adversarial attacks that balances exploration and exploitation. GGS addresses the trade-off in existing methods by using gradients from the previous inner-iteration to guide sampling, achieving stable ascent toward flat loss regions with higher local maxima. Experiments demonstrate that GGS outperforms state-of-the-art methods, achieving 82.08% average untargeted attack success rate and 17.67% targeted success rate across nine diverse models. The method is also compatible with input transformation and inner-iteration sampling techniques, and shows strong performance on multimodal large language models and commercial cloud functions.

## Method Summary
The paper presents Gradient-Guided Sampling (GGS), an inner-iteration sampling strategy built on MI-FGSM for black-box transferable adversarial attacks. The core innovation is a sampling mechanism that balances exploration (cross-model generalization) and exploitation (attack potency) by using the gradient sign from the previous inner-iteration to guide the current sampling direction. Specifically, GGS computes an inner-iteration sampling point using $\hat{x}_i = x_{adv} + |\tilde{p}| \cdot \text{sign}(\tilde{g}_{i-1})$, where $\tilde{p}$ is uniform random noise and $\tilde{g}_{i-1}$ is the gradient from the previous inner-iteration. This creates a single-step dependency that stabilizes the ascent while maintaining exploration capabilities. The method uses standard MI-FGSM hyperparameters ($\epsilon = 16/255$, $T=10$, $N=20$, $\alpha = \epsilon/T$, $\zeta = 2.0 \times \epsilon$) and averages gradients across inner-iterations before updating the adversarial example.

## Key Results
- GGS achieves 82.08% average untargeted attack success rate and 17.67% targeted success rate across 9 diverse models.
- Outperforms state-of-the-art methods including MI-FGSM, DI-FGSM, and recent sampling-based approaches.
- Demonstrates compatibility with input transformations, improving transferability under JPEG compression and other perturbations.
- Shows strong performance on multimodal large language models and commercial cloud functions.

## Why This Works (Mechanism)
GGS works by balancing the exploration-exploitation trade-off in adversarial attacks. Traditional momentum-based methods over-prioritize exploitation by following previous gradients too aggressively, leading to local maxima. Random sampling methods over-prioritize exploration by adding noise without guidance, resulting in unstable ascents. GGS uses a single-step dependency: the gradient from iteration $i-1$ guides the sampling direction in iteration $i$. This creates a controlled exploration that maintains stability while seeking flatter loss regions with higher local maxima. The magnitude of noise comes from uniform random sampling, while the direction comes from the sign of the previous gradient, ensuring the attack explores around the current adversarial example in a way that balances generalization and potency.

## Foundational Learning
- **Black-box transferable attacks:** Adversarial attacks where the attacker has no access to the target model's parameters, requiring transferability across different architectures. *Why needed:* This is the fundamental problem GGS addresses, as it aims to create adversarial examples that fool multiple models.
- **Inner-iteration sampling:** The process of generating multiple perturbed samples within each outer iteration to compute gradient estimates. *Why needed:* GGS specifically improves this component by adding gradient-guided sampling to balance exploration and exploitation.
- **Momentum-based gradient updates:** Techniques like MI-FGSM that accumulate gradients over iterations to maintain attack direction. *Why needed:* GGS builds on MI-FGSM's framework while adding the novel sampling mechanism.
- **Exploration-exploitation trade-off:** The balance between finding new attack directions (exploration) and refining current successful directions (exploitation). *Why needed:* This is the central challenge GGS addresses, as existing methods over-prioritize one aspect.
- **Gradient sign-based perturbation:** Using the sign of gradients to determine perturbation direction, a common practice in adversarial attacks. *Why needed:* GGS specifically uses gradient signs from previous iterations to guide sampling.
- **Flat loss regions:** Areas in the loss landscape with low curvature that tend to have higher local maxima and better transferability. *Why needed:* GGS aims to ascend toward these regions for improved attack success.

## Architecture Onboarding

**Component map:** Input image → Surrogate model → GGS sampling loop → Gradient computation → Momentum update → Adversarial example → Target models

**Critical path:** Image → Surrogate model forward pass → GGS inner-iteration loop (20 steps) → Gradient averaging → Momentum update → Projection → Final adversarial example

**Design tradeoffs:** The single-step gradient dependency provides stability but may limit long-term exploration compared to multi-step methods. Fixed sampling radius simplifies implementation but may not be optimal for all model types.

**Failure signatures:** Low ASR indicates incorrect gradient sign usage (using current instead of previous gradient), exploding gradients suggest incorrect sampling radius bounds, and poor transferability may indicate over-reliance on exploitation rather than balanced exploration.

**First experiments:** 1) Implement GGS and test on ResNet50→DenseNet121 pair to verify directional sampling works, 2) Compare ASR with MI-FGSM baseline on same dataset split, 3) Test GGS under JPEG compression to verify compatibility gains.

## Open Questions the Paper Calls Out
- **Adapting GGS to non-gradient-averaging frameworks:** The authors aim to refine GGS to support non-gradient-averaging techniques like VMI-FGSM and RAP, as the current mechanism relies on gradient averaging which may not integrate with variance-tuning or reverse perturbation methods.
- **Impact of random gradient initialization:** The paper uses random initialization for the guiding gradient but doesn't analyze if this initial instability is necessary or if it could be minimized through zero-initialization or momentum to reach stable regions faster.
- **Adaptive sampling radius scheduling:** While the paper uses a fixed sampling radius, it shows different optimal values for normally trained versus adversarially trained models, suggesting a dynamic radius might better balance exploration needs across diverse architectures.

## Limitations
- Exact ImageNet subset and random seed not specified, which can cause ASR variance of ±2-3%.
- Preprocessing normalization details not fully clarified, potentially affecting reproducibility.
- Model weight versions (Torchvision vs. custom) not stated, which could impact attack success rates.

## Confidence
- **Core GGS method and ablation results:** High
- **Exact ASR numbers on full model suite:** Medium
- **Cross-modal and cloud model results:** Medium

## Next Checks
1. Implement GGS and run a pilot on a single surrogate/target pair (e.g., ResNet50→DenseNet121) to confirm directional gradient sampling works as intended.
2. Compare average ASR with MI-FGSM and other sampling baselines on the same dataset split to verify improvement magnitude.
3. Test GGS under input transformations (e.g., JPEG compression) to confirm compatibility gains reported in Table 4.