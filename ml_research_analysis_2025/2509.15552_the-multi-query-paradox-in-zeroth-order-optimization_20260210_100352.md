---
ver: rpa2
title: The Multi-Query Paradox in Zeroth-Order Optimization
arxiv_id: '2509.15552'
source_url: https://arxiv.org/abs/2509.15552
tags:
- query
- zo-align
- zo-avg
- optimization
- convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the fundamental trade-off in zeroth-order
  optimization between the quality of gradient estimates and the number of optimization
  iterations under a fixed query budget. It proposes two multi-query estimators: a
  simple averaging method (ZO-Avg) and a principled projection alignment method (ZO-Align)
  derived from local surrogate minimization.'
---

# The Multi-Query Paradox in Zeroth-Order Optimization

## Quick Facts
- arXiv ID: 2509.15552
- Source URL: https://arxiv.org/abs/2509.15552
- Reference count: 40
- Key outcome: Single-query per iteration is optimal for ZO-Avg, while multi-query is better for ZO-Align, resolving a fundamental trade-off in zeroth-order optimization under fixed query budgets

## Executive Summary
This paper addresses a fundamental question in zeroth-order optimization: how to optimally allocate a fixed query budget between gradient estimation quality and iteration count. The authors propose two multi-query estimators - ZO-Avg (simple averaging) and ZO-Align (projection alignment via local surrogate minimization) - and prove a dichotomy between them. For ZO-Avg, using exactly one query per iteration is always optimal under budget constraints, while for ZO-Align, using more queries generally improves convergence, with full-subspace estimation being optimal in strongly convex and stochastic settings. The theoretical findings are validated across multiple problem settings and include high-dimensional LLM fine-tuning tasks.

## Method Summary
The paper analyzes zeroth-order optimization under a fixed query budget, comparing two multi-query gradient estimation methods. ZO-Avg computes the average of directional derivatives scaled by query directions, while ZO-Align solves a local quadratic surrogate minimization to find optimal update coefficients within the query subspace. The key insight is that these methods exhibit fundamentally different query allocation efficiency: ZO-Avg suffers from diminishing returns when increasing queries per iteration, while ZO-Align leverages multi-query information to construct geometrically consistent updates. The analysis considers various settings including strongly convex, convex, non-convex, and stochastic optimization.

## Key Results
- For ZO-Avg, using more than one query per iteration is always query-inefficient under a fixed budget constraint
- For ZO-Align, multi-query estimation is generally superior, with full-subspace estimation being optimal in strongly convex and stochastic settings
- In high-dimensional regimes (d >> q), ZO-Align's advantage collapses due to near-orthogonality of random directions
- The dichotomy is validated experimentally across synthetic and real-world problems including LLM fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Under a fixed query budget, the standard averaging method (ZO-Avg) achieves optimal convergence by using exactly one query per iteration; allocating more queries to reduce variance degrades performance.
- **Mechanism**: The convergence rate depends on a product of contraction factors involving $q_t$ (queries per step). The function governing per-query efficiency is concave, meaning the marginal gain from reducing variance by adding queries is outweighed by the linear cost of consuming the budget, reducing the total number of iterations $T$.
- **Core assumption**: The query budget is the primary constraint; the objective function is $L$-smooth.
- **Evidence anchors**:
  - [abstract]: "For ZO-Avg, we prove that using more than one query per iteration is always query-inefficient."
  - [section 4.2.1]: "The function h(q) is concave... we gain more 'value per query' for smaller q."
  - [corpus]: General ZO optimization literature exists, but specific inefficiency of averaging under budget constraints is distinct to this paper.
- **Break condition**: If the budget constraint is removed or variance reduction has a super-linear benefit (which contradicts standard finite-difference properties).

### Mechanism 2
- **Claim**: The Projection Alignment method (ZO-Align) utilizes multi-query information to construct a geometrically consistent update, enabling larger query blocks ($q > 1$) to be beneficial in strongly convex settings.
- **Mechanism**: ZO-Align minimizes a local quadratic surrogate to find the optimal update within the query subspace. It effectively computes the orthogonal projection of the gradient onto that subspace. This alignment ensures the update direction matches all observed directional derivatives exactly, significantly lowering Mean Squared Error (MSE) compared to averaging.
- **Core assumption**: The local geometry of the function is well-approximated by the query subspace; $L$-smoothness holds.
- **Evidence anchors**:
  - [section 3.3]: "ZO-Align is the orthogonal projection of the true gradient onto the query subspace."
  - [section 3.4]: "For any $q > 1$, it is clear that $(d-q)/d < (d+1)/q$ [MSE comparison]."
  - [corpus]: [No direct corpus evidence supports this specific local surrogate derivation; assumption based on paper text.]
- **Break condition**: If the query directions are perfectly correlated (redundant) or the function landscape is highly non-smooth relative to the perturbation scale $\mu$.

### Mechanism 3
- **Claim**: In high-dimensional regimes ($d \gg q$), the theoretical advantage of ZO-Align collapses, causing its performance to resemble ZO-Avg.
- **Mechanism**: When dimensionality $d$ vastly exceeds query count $q$, the sampled random directions become nearly orthogonal with high probability. The Gram matrix $U^\top U$ concentrates around a scaled identity matrix, causing the projection correction $(U^\top U)^{-1}$ in ZO-Align to degenerate into a uniform scaling factor.
- **Core assumption**: Queries are sampled i.i.d. from an isotropic Gaussian distribution.
- **Evidence anchors**:
  - [section 5]: "When $d \gg q$ and $U^\top U \approx dI$, the correction degenerates to an almost uniform scaling."
  - [appendix A.1]: Concentration bounds for the empirical covariance matrix.
  - [corpus]: [Weak corpus signal for this specific dimensionality collapse mechanism.]
- **Break condition**: If the dimensionality $d$ is low, or if $q$ approaches $d$, where the directions are no longer near-orthogonal.

## Foundational Learning

- **Concept**: **Finite-Difference Gradient Estimation**
  - **Why needed here**: The core operation of the system is approximating $\nabla f(x)$ using only function values $f(x+\mu u)$. Understanding the bias-variance trade-off of the smoothing parameter $\mu$ is essential.
  - **Quick check question**: If $\mu$ is set too high, does the estimator approximate the gradient of $f$ or a smoothed version of $f$?

- **Concept**: **Strong Convexity and Smoothness**
  - **Why needed here**: The paper's proofs of the "dichotomy" rely on contraction factors derived from these properties (e.g., $\|\nabla f(x)\|^2 \ge 2\gamma(f(x)-f^*)$$).
  - **Quick check question**: Does a smaller condition number (ratio of $L/\gamma$) generally imply faster convergence for gradient descent?

- **Concept**: **Orthogonal Projection (Linear Algebra)**
  - **Why needed here**: ZO-Align is defined via the projection matrix $P_U = U(U^\top U)^{-1}U^\top$. Understanding that this operator projects a vector onto the column space of $U$ is required to grasp why ZO-Align is "geometrically consistent."
  - **Quick check question**: If vector $v$ is already in the column space of $U$, what is the result of $P_U v$?

## Architecture Onboarding

- **Component map**:
  1.  **Query Generator**: Samples direction matrix $U \in \mathbb{R}^{d \times q}$ from $\mathcal{N}(0, I)$.
  2.  **Evaluator**: Computes directional derivatives $\delta_i = (f(x+\mu u_i) - f(x))/\mu$.
  3.  **Aggregator**: Implements **ZO-Avg** (mean of $\delta_i u_i$) or **ZO-Align** (solves for coefficients $\beta$ in $\text{min}_\beta \|U\beta - \text{target}\|$).
  4.  **Optimizer**: Updates $x_{t+1} = x_t - \eta \hat{g}$.

- **Critical path**: The choice of **Aggregator** dictates the **Query Allocation Strategy**. If using ZO-Avg, restrict $q=1$. If using ZO-Align and dimension is moderate, maximize $q$.

- **Design tradeoffs**:
  - **Accuracy vs. Orthogonality**: In high dimensions ($d > 10,000$), calculating the exact inverse $(U^\top U)^{-1}$ for ZO-Align is computationally stable but unnecessary; the paper suggests a diagonal approximation is sufficient because the off-diagonals vanish.
  - **Step Size**: ZO-Avg requires a shrinking step size $\eta \propto \frac{q}{q+d}$, whereas ZO-Align allows a constant step size $\eta = 1/L$ (assuming $L$ is known or estimated).

- **Failure signatures**:
  - **Stagnation in ZO-Avg**: If $q > 1$ is used with ZO-Avg under a budget, performance degrades due to insufficient iterations.
  - **Oscillation in ZO-Align**: Occurs if $\mu$ is large relative to the curvature, violating the local surrogate assumption.
  - **Memory Overflow**: Explicitly forming $U$ for large $d$ (e.g., LLMs) is memory-intensive; directions should be generated on-the-fly or use implicit representations.

- **First 3 experiments**:
  1.  **Synthetic Quadratic (Low Dim)**: Verify the dichotomy. Run ZO-Avg ($q=1$ vs $q=10$) and ZO-Align ($q=1$ vs $q=10$) on a 100D quadratic function with fixed budget $K=1000$. Expect ZO-Avg ($q=1$) and ZO-Align ($q=10$) to win.
  2.  **High-Dimensional Sparsity**: Test a 10,000D problem. Compare exact ZO-Align vs. Diagonal-Approximation ZO-Align. Verify that performance is nearly identical.
  3.  **LLM Fine-Tuning (Budget Sweep)**: Fine-tune a small transformer (e.g., 60M params) on a classification task. Plot accuracy vs. total queries for ZO-Avg ($q=1$) vs. Diagonal ZO-Align ($q=10$).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the ZO-Align estimator be modified to retain its theoretical advantage over ZO-Avg in the over-parameterized regime ($d \gg q$) common in modern deep learning?
- **Basis in paper:** [inferred] Section 5 and 6.2 state that in high dimensions, the Gram matrix concentrates to a scaled identity, causing ZO-Align to degenerate into a rescaled ZO-Avg, thereby losing its geometric edge.
- **Why unresolved:** The paper characterizes the degeneration but does not propose a solution for maintaining multi-query efficiency in extremely high-dimensional spaces.
- **What evidence would resolve it:** A modified estimator or sampling strategy that achieves lower Mean Squared Error (MSE) or faster convergence than single-query ZO-Avg when $d$ is orders of magnitude larger than the query budget $q$.

### Open Question 2
- **Question:** Does the "single-query is optimal for ZO-Avg" dichotomy persist when using deterministic or orthogonal query directions rather than i.i.d. Gaussian samples?
- **Basis in paper:** [inferred] The theoretical proofs and the high-dimensional approximation (Appendix A) rely explicitly on the assumption that query directions are drawn i.i.d. from $\mathcal{N}(0, I)$.
- **Why unresolved:** Using orthogonal directions avoids the "redundant direction" issue in ZO-Avg, potentially altering the variance reduction dynamics and invalidating the single-query optimality result.
- **What evidence would resolve it:** Convergence rates for ZO-Avg and ZO-Align derived under a deterministic coordinate perturbation scheme or a rigid orthogonal basis.

### Open Question 3
- **Question:** How does a finite, non-infinitesimal smoothing parameter $\mu$ shift the optimal query allocation balance for ZO-Align?
- **Basis in paper:** [explicit] The authors state in Section 3.1 that "Throughout this work, we focus on an idealized regime in which $\mu \to 0$."
- **Why unresolved:** In practice, $\mu$ is finite, introducing bias into the gradient estimates; the interaction between this bias and the multi-query variance reduction could change the optimal allocation strategy.
- **What evidence would resolve it:** A convergence analysis including explicit $O(\mu)$ bias terms to determine if the "full-subspace" optimality for ZO-Align holds robustly for standard finite difference step sizes.

## Limitations
- Theoretical analysis assumes L-smoothness and strong convexity; performance in non-convex settings is validated only empirically
- The query budget model assumes equal cost per query, which may not hold in practice (e.g., caching effects)
- The paper focuses on isotropic Gaussian perturbations; performance under alternative noise distributions is unexplored

## Confidence
- **High confidence**: The dichotomy between ZO-Avg and ZO-Align in strongly convex settings, supported by both theoretical analysis and experiments
- **Medium confidence**: The assumption that local function geometry is well-approximated by query subspaces for ZO-Align, which could break down in highly non-smooth regimes
- **Low confidence**: The claim that high-dimensional LLM settings effectively reduce ZO-Align to ZO-Avg behavior, as empirical validation is limited to a single fine-tuning task

## Next Checks
1. Test the divergence between ZO-Avg and ZO-Align in strongly convex functions with varying condition numbers (L/Î³) to quantify when the projection alignment advantage is most pronounced
2. Experiment with structured query directions (e.g., using prior knowledge of function structure) rather than random Gaussian directions to see if the near-orthogonality collapse in high dimensions can be avoided
3. Compare against adaptive query allocation strategies that dynamically adjust q_t based on gradient history, testing whether the static allocation proposed here is truly optimal in practice