---
ver: rpa2
title: 'Evidence-Bound Autonomous Research (EviBound): A Governance Framework for
  Eliminating False Claims'
arxiv_id: '2511.05524'
source_url: https://arxiv.org/abs/2511.05524
tags:
- verification
- execution
- phase
- gate
- artifacts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Autonomous research agents hallucinate\u2014claiming tasks complete\
  \ without producing verifiable evidence. EviBound solves this with dual governance\
  \ gates that enforce machine-checkable evidence: an approval gate validates acceptance\
  \ criteria before execution, and a verification gate validates artifacts after execution\
  \ via MLflow API queries."
---

# Evidence-Bound Autonomous Research (EviBound): A Governance Framework for Eliminating False Claims

## Quick Facts
- arXiv ID: 2511.05524
- Source URL: https://arxiv.org/abs/2511.05524
- Authors: Ruiying Chen
- Reference count: 25
- Primary result: Achieves 0% hallucination rate versus 100% in prompt-only control

## Executive Summary
EviBound addresses the critical problem of autonomous research agents claiming task completion without verifiable evidence. The framework implements dual governance gates that enforce machine-checkable evidence through MLflow API queries. Pre-execution approval gates validate acceptance criteria schemas, while post-execution verification gates confirm artifact existence and metric compliance. Tested on 8 benchmark tasks, EviBound eliminates hallucinations while adding only ~8.3% execution overhead.

## Method Summary
The framework implements a three-phase pipeline: planning (4 agents generate tasks with acceptance criteria), execution (3 agents enforce dual gates), and reflection (patch generation for retries). The Approval Gate validates acceptance criteria schemas pre-execution with unanimous 3-agent approval (confidence ≥τ=0.7), rejecting placeholders like "<to be generated>". The Verification Gate queries MLflow API post-execution to confirm run_id existence, FINISHED status, artifact presence (with recursive path checking), and metric range compliance. Bounded retries (max 2 per phase) route failures to earliest repairable phase based on failure type.

## Key Results
- EviBound achieves 0% hallucination rate (7/8 verified, 1 correctly blocked) versus 100% hallucination for prompt-level only (0/8 verified) and 25% for verification-only (2/8 hallucinated)
- Framework adds only ~8.3% execution overhead while providing deterministic, reproducible verification
- Approval gate prevents wasted execution cycles by catching schema violations early (e.g., rejected T09 placeholder run_id)

## Why This Works (Mechanism)

### Mechanism 1: Schema Validation Prevents Malformed Contracts
The Approval Gate validates acceptance criteria schemas before code runs, catching structural violations proactively. Placeholders like "<to be generated>" are rejected outright, preventing execution without concrete evidence specification. This addresses the root cause of hallucinated contracts at the earliest possible point.

### Mechanism 2: API-Based Verification Binds Claims to Artifacts
The Verification Gate queries MLflow API to confirm run_id existence, FINISHED status, artifact presence, and metric compliance. This creates deterministic, tamper-resistant verification that only allows claims backed by queryable artifacts to propagate to reporting.

### Mechanism 3: Targeted Retry Routing Minimizes Wasted Cycles
Verification failures route by type to the earliest repairable phase: missing artifacts → evidence regeneration, runtime errors → execution retry, metric violations → contract refinement. This prevents wasteful full-cycle retries while allowing recovery from transient failures.

## Foundational Learning

**Machine-Checkable Evidence**
- Why needed: Distinguishes "agent claims success" from "artifacts provably exist"
- Quick check: Given claimed run_id = "abc123", what API call verifies it exists and finished?

**Consensus Thresholds (τ)**
- Why needed: Balances false positives (blocking valid tasks) vs. false negatives (allowing weak contracts)
- Quick check: If two agents approve with confidence 0.8 but one vetoes at 0.4, does task proceed?

**Bounded Retry Loops**
- Why needed: Prevents infinite cycles while allowing recovery from transient failures
- Quick check: A task fails verification 3 times in Phase 6. What happens next?

## Architecture Onboarding

**Component map**: Planning Team (4 agents) → pending_actions.json → Execution Team (3 agents) → dual gates → MLflow → Verification Gate → Reporting

**Critical path**:
1. Phase 3: Implementation proposals
2. Phase 4: Approval Gate (schema validation, unanimous consent)
3. Phase 5: Sandboxed execution with MLflow logging
4. Phase 6: Verification Gate (API queries, artifact checks)
5. Phase 7: Reporting (only verified results propagate)

**Design tradeoffs**:
- Overhead vs. Integrity: ~8.3% overhead buys 100pp hallucination reduction
- Proactive vs. Reactive: Approval gate adds upfront cost but prevents wasted execution
- MLflow Lock-in: Verification tied to MLflow API; adapting requires API translation layer

**Failure signatures**:
- Placeholder run_id: Approval gate rejects; fix by generating concrete run_id
- Missing artifacts: Routes to Phase 6.5; check mlflow.log_artifact() calls
- Metric mismatch: Routes to Phase 4.5; acceptance criteria may be over-specified
- MLflow unreachable: Hard failure; check network/server status

**First 3 experiments**:
1. Reproduce Baseline A behavior: Disable both gates on T01; observe 100% hallucination rate
2. Ablate approval gate: Run Baseline B (verification-only) on T06/T09; confirm 25% hallucination
3. Full EviBound on T13: Verify approval gate correctly blocks malformed contract

## Open Questions the Paper Calls Out

**Q1: Multi-model generalization**
Does the framework generalize beyond Claude 3.5 Sonnet? Paper states multi-model validation would strengthen architectural claims. Resolution requires replicating three-system comparison across at least 2-3 additional model families.

**Q2: Domain-specific evidence schemas**
Can domain-specific schemas be developed for non-ML research (theoretical proofs, systems benchmarking, NLP)? Current acceptance criteria assume MLflow artifacts. Resolution requires implementing and testing on 3+ non-ML domains.

**Q3: Adaptive confidence thresholds**
Can task-specific confidence thresholds be learned adaptively from historical data rather than manual setting? Current τ=0.7 was empirically set. Resolution requires training threshold calibration model on execution logs.

**Q4: Human-in-the-loop formalization**
How should human validation be formalized when retry budgets are exhausted? Human oversight is mentioned but not formalized. Resolution requires designing structured review protocol and measuring effort reduction.

## Limitations

**Implementation Dependencies**: Framework relies on MLflow API availability; failures during verification are not recoverable within the system.

**Agent Capability Assumptions**: Approval Gate assumes agents can produce schema-compliant contracts; limited evidence provided for consistent generation without human intervention.

**Schema Validation Boundaries**: Paper doesn't fully specify how Approval Gate programmatically distinguishes acceptable dynamic values from invalid placeholders during runtime.

## Confidence

**High**: Dual-gate architecture fundamentally addresses hallucination through verifiable evidence requirements; 0% hallucination empirically demonstrated.

**Medium**: ~8.3% overhead measurement well-documented; schema validation mechanism appears sound but lacks complete implementation details.

**Low**: Agent system coordination is described but not fully specified; critical implementation details like prompts and coordination logic are omitted.

## Next Checks

1. **Schema Validation Stress Test**: Run EviBound on tasks with intentionally malformed acceptance criteria to verify Approval Gate consistently rejects non-compliant contracts.

2. **MLflow Dependency Failure**: Simulate MLflow API unavailability during Phase 6 verification to confirm graceful failure and identify integration possibilities for alternative artifact stores.

3. **Agent Capability Boundaries**: Test whether agents can generate schema-compliant contracts with progressively complex acceptance criteria, measuring approval rates and identifying human intervention thresholds.