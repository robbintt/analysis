---
ver: rpa2
title: 'PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics
  Problem Solving'
arxiv_id: '2601.13453'
source_url: https://arxiv.org/abs/2601.13453
tags:
- scene
- visual
- manim
- code
- physics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhysicsSolutionAgent, an autonomous system
  that generates up to six-minute multimodal video explanations for physics problems
  using Manim animations. The approach combines a planner-coder architecture with
  GPT-5-mini, a RAG module for Manim documentation, and a screenshot-driven visual
  refinement loop using a vision-language model.
---

# PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving

## Quick Facts
- arXiv ID: 2601.13453
- Source URL: https://arxiv.org/abs/2601.13453
- Reference count: 40
- Generates up to 6-minute Manim animation videos explaining physics problems with 100% completion rate and 3.8/5 automated quality score

## Executive Summary
PhysicsSolutionAgent is an autonomous system that generates multimodal video explanations for physics problems using Manim animations. The system employs a planner-coder architecture with GPT-5-mini, a RAG module for Manim documentation, and a screenshot-driven visual refinement loop using a vision-language model. Tested on 32 physics problems, the agent achieves a 100% video completion rate with an average automated quality score of 3.8/5, though qualitative analysis reveals issues with redundancy, visual layout inconsistencies, and VLM interpretation errors.

## Method Summary
The system uses a 5-stage pipeline: solution generation, scene planning, RAG-enhanced code generation with ChromaDB, execution with 5-attempt error-correction, and screenshot-driven visual refinement via VLM. It employs a Planner-Coder architecture using GPT-5-mini and Manim CE v0.18.0 with manim-voiceover and Kokoro TTS. The approach combines automated reasoning for physics solutions with visual animation generation, using a combination of LLMs and VLMs to create educational content.

## Key Results
- 100% video completion rate across 32 physics problems
- Average automated quality score of 3.8/5 based on 15 quantitative parameters
- Overall Score formula: Solution Quality (5%) + Explanation Quality (10%) + Visual Quality (60%) + Error Penalty (25%)
- Recurring issues identified: minor redundancy in repeated visualization text, visual layout inconsistencies, and VLM interpretation errors

## Why This Works (Mechanism)
The system works by decomposing the complex task of video generation into manageable stages, using specialized models for different aspects: GPT-5-mini for planning and code generation, RAG for accurate Manim documentation lookup, and VLM for visual quality refinement. The error-correction loop with multiple execution attempts helps overcome Manim's known instability, while the screenshot-driven refinement provides feedback for iterative improvement.

## Foundational Learning

1. **Manim Animation Framework** - Why needed: Provides the core visualization engine for physics explanations; Quick check: Verify Manim v0.18.0 installation and basic animation rendering

2. **RAG (Retrieval-Augmented Generation)** - Why needed: Enables accurate lookup of Manim documentation to improve code generation; Quick check: Test ChromaDB vector search with sample Manim documentation queries

3. **Vision-Language Models** - Why needed: Analyzes screenshots to identify visual quality issues and guide refinement; Quick check: Validate VLM can correctly interpret basic Manim screenshots

4. **Error-Correction Loops** - Why needed: Handles Manim's instability through automated retries; Quick check: Confirm error-correction triggers appropriately on syntax errors

5. **Automated Quality Evaluation** - Why needed: Provides consistent assessment across multiple video quality dimensions; Quick check: Run evaluation on a simple test video to verify scoring

6. **Planner-Coder Architecture** - Why needed: Separates high-level planning from implementation details for better control; Quick check: Verify planner outputs coherent scene structure before code generation

## Architecture Onboarding

**Component Map:** Physics Problem -> Planner (GPT-5-mini) -> Solution Generator -> Scene Planner -> RAG + Manim Docs -> Code Generator -> Execution Engine -> VLM Refiner -> Final Video

**Critical Path:** Solution Generation → Scene Planning → Code Generation → Execution → Refinement (most failures occur in Code Generation or Execution)

**Design Tradeoffs:** Uses multiple specialized models (higher cost/complexity) vs. single model approach (lower quality); automated evaluation vs. human evaluation (faster but less nuanced); screenshot-based refinement vs. code-based (more direct but requires VLM reliability)

**Failure Signatures:** Manim syntax errors during execution, VLM misinterpretation of visual content, redundancy in narration, layout issues (overlapping elements), and failed execution after 5 retry attempts

**Three First Experiments:**
1. Test the complete pipeline on a simple physics problem to verify basic functionality
2. Run the error-correction loop with an intentionally broken Manim script to verify retry behavior
3. Execute the VLM refinement on a deliberately flawed screenshot to test visual analysis capability

## Open Questions the Paper Calls Out
None specified in the paper

## Limitations
- Unclear model specifications (GPT-5-mini and VLM not publicly identified)
- Evaluation dataset (32 physics problems) not publicly available for independent validation
- Recurring visual quality issues including redundancy, layout inconsistencies, and VLM interpretation errors

## Confidence

**High Confidence:** Automated evaluation methodology is well-specified with clear scoring formula and 15 quantitative parameters

**Medium Confidence:** Reported 100% completion rate and 3.8/5 quality score, though verification limited by unavailable model specifications and evaluation dataset

**Low Confidence:** Human evaluation results are not detailed enough to assess reliability

## Next Checks

1. **Model Substitution Validation:** Test the pipeline with GPT-4o and Claude as substitutes for "GPT-5-mini" to determine if the 100% completion rate is achievable with accessible models

2. **VLM Specification Test:** Implement the visual refinement loop using multiple VLMs (e.g., GPT-4V, Claude 3 Haiku with vision) to verify that screenshot-driven corrections are model-dependent and to identify the most effective approach

3. **Independent Evaluation:** Apply the automated evaluation framework to the same 32 physics problems using different implementations to verify the consistency of the 3.8/5 average quality score across different runs and model versions