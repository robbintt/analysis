---
ver: rpa2
title: 'Multimodal signal fusion for stress detection using deep neural networks:
  a novel approach for converting 1D signals to unified 2D images'
arxiv_id: '2509.13636'
source_url: https://arxiv.org/abs/2509.13636
tags:
- stress
- signals
- signal
- detection
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a novel approach to stress detection by converting\
  \ multimodal physiological signals\u2014photoplethysmography (PPG), galvanic skin\
  \ response (GSR), and acceleration (ACC)\u2014into unified 2D image matrices for\
  \ analysis using convolutional neural networks (CNNs). Unlike traditional methods\
  \ that process signals separately, this technique fuses them into structured images,\
  \ enabling CNNs to capture temporal and cross-signal dependencies more effectively."
---

# Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images

## Quick Facts
- arXiv ID: 2509.13636
- Source URL: https://arxiv.org/abs/2509.13636
- Reference count: 0
- Primary result: Achieved 95.86% test accuracy on WESAD dataset for stress detection

## Executive Summary
This study presents a novel approach to stress detection by converting multimodal physiological signals—photoplethysmography (PPG), galvanic skin response (GSR), and acceleration (ACC)—into unified 2D image matrices for analysis using convolutional neural networks (CNNs). Unlike traditional methods that process signals separately, this technique fuses them into structured images, enabling CNNs to capture temporal and cross-signal dependencies more effectively. Dynamic signal arrangement and a two-stage training strategy improve generalization and robustness. Evaluated on the WESAD dataset, the method achieves a test accuracy of 95.86%, outperforming existing approaches. The method enhances interpretability and serves as a form of data augmentation, demonstrating broad applicability for real-time health monitoring through wearable technologies.

## Method Summary
The method transforms three physiological signals (PPG, GSR, ACC) from the WESAD dataset into unified 2D image representations for CNN-based stress classification. Signals are detrended, segmented into 5-second windows, and converted to 32×32 matrices with resampling (sample repetition) to align temporal resolution. These matrices are resized to 128×128×3 RGB images using custom colorization. The signals are arranged in three different configurations (PEA, EAP, EPA) to create multiple image variants from the same data. A CNN with three convolutional blocks is trained in two stages: first on one arrangement to learn features, then on all arrangements with frozen early layers to improve generalization.

## Key Results
- Achieved 95.86% test accuracy on WESAD dataset for binary stress detection
- Outperformed existing approaches that process physiological signals separately
- Demonstrated effectiveness of signal permutation as structured data augmentation
- Validated subject-independent evaluation using S17 as test data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting multimodal 1D signals into a unified 2D image matrix enables a standard 2D CNN to learn cross-signal dependencies that independent 1D processing misses.
- Mechanism: The method stacks temporally aligned PPG, GSR, and ACC signals into the rows of a single image matrix. This transforms the problem from analyzing three separate time series into a single spatial recognition task. The 2D convolutional kernels slide across both time (horizontally) and modality (vertically), allowing the network to learn features that represent the relationship between signals (e.g., a spike in ACC occurring alongside a specific change in PPG).
- Core assumption: The spatial arrangement of signals in an image can preserve meaningful temporal correlations and inter-signal dependencies that are beneficial for classification.
- Evidence anchors:
  - [abstract] "...technique fuses them into structured image representations that enable CNNs to capture temporal and cross signal dependencies more effectively."
  - [PAGE 4] "...fusing PPG, GSR, and ACC (or other potentially informative signals) into a unified 2D image matrix, enabling CNNs to jointly learn inter-signal dependencies within a spatially structured representation."
  - [corpus] Evidence is weak or missing for this specific 2D stacking mechanism. Neighboring papers like `UniPhyNet` discuss multimodal fusion but often use more complex architectural blocks rather than this direct image conversion.
- Break condition: This mechanism fails if the forced alignment and sampling rate harmonization destroy critical non-linear temporal dynamics between the signals, or if the CNN's receptive field is too small to capture the relevant time scales.

### Mechanism 2
- Claim: Dynamically rearranging the order of signals in the 2D matrix acts as a form of structured data augmentation, improving model generalization.
- Mechanism: By permuting the signal rows (e.g., creating PEA, EAP, and EPA configurations from the same data), the method artificially expands the training set. This forces the CNN to learn features that are invariant to the absolute vertical position of a signal, preventing it from overfitting to a fixed layout and instead focusing on the signal patterns themselves.
- Core assumption: The predictive physiological patterns are largely invariant to their position in the input stack, and the model can learn to recognize them regardless of their "channel" location.
- Evidence anchors:
  - [abstract] "This image-based transformation... serves as a robust form of data augmentation."
  - [PAGE 7] "...rearranging signal sequences in the fused image matrix acts as structured data augmentation... encouraging the model to learn modality-invariant and inter-signal relationships rather than memorizing fixed layouts..."
  - [corpus] No direct evidence from corpus. This appears to be a novel technique specific to this paper's methodology.
- Break condition: This breaks if the CNN develops a strong positional bias it cannot overcome, leading to poor convergence when trained on mixed arrangements.

### Mechanism 3
- Claim: A two-stage training strategy with selective layer freezing enhances final model robustness and accuracy.
- Mechanism: The model is first trained to convergence on one signal arrangement (e.g., EAP) to learn robust low-level features (edges, textures). In the second stage, these early convolutional layers are frozen, and the model is further trained on the other arrangements (PEA, EPA). This preserves the general feature extractor while allowing the classifier to adapt to the variations introduced by different signal orderings, effectively combining the strengths of each layout.
- Core assumption: Low-level features learned from one signal arrangement are generalizable and beneficial for processing other arrangements, with only the final classification layers needing retraining.
- Evidence anchors:
  - [PAGE 8] "In Stage 1, we trained the CNN on the EAP arrangement... In Stage 2, we introduced additional arrangements... updating only the classifier layers while keeping the feature extractor fixed."
  - [PAGE 9] "...the combined training approach significantly outperforms individual dataset training."
  - [corpus] No direct evidence. This is a specific training methodology introduced by the authors.
- Break condition: This approach will fail if the initial training reaches a poor local minimum for the feature extractor that cannot be corrected by retraining only the classifier on subsequent arrangements.

## Foundational Learning

- Concept: **Convolutional Neural Networks (CNNs) on Non-Image Data**
  - Why needed here: The core innovation is repurposing a 2D CNN—designed for spatial data like photographs—for temporal physiological signals. Understanding how convolutions extract hierarchical features (edges -> shapes -> objects) is key.
  - Quick check question: If a 1D signal has a sudden spike, how might a 2D convolutional kernel "see" it differently when the signal is plotted as an image line compared to raw numerical processing?

- Concept: **Data Augmentation**
  - Why needed here: The paper explicitly introduces a novel form of augmentation: signal permutation. Grasping the general goal of augmentation (to expand the training distribution and prevent overfitting) is necessary to understand why this technique improves robustness.
  - Quick check question: Why does training a model on both original and horizontally flipped images improve its ability to recognize objects in new, unseen images?

- Concept: **Transfer Learning and Layer Freezing**
  - Why needed here: The two-stage training strategy is an application of transfer learning principles. The idea is that early layers learn general features (which can be frozen), while later layers learn task-specific features (which are fine-tuned).
  - Quick check question: In a CNN trained to classify animals, would the first convolutional layer learn features more like "edges and textures" or "fluffy tails and whiskers"?

## Architecture Onboarding

- Component map:
  1. Input Pipeline: A transformation module that takes raw PPG, GSR, and ACC signals, resamples them (using repetition) to align temporal resolution, normalizes them, and stacks them into a unified `[32, 32]` matrix per time window.
  2. Image Generator: A colorization module that converts the `[32, 32]` matrix into a `[128, 128, 3]` color image using a custom color map (not simple grayscale or RGB) to enhance feature contrast.
  3. CNN Core: A feature extraction module consisting of stacked `Conv2D + ReLU + MaxPooling` layers. This learns hierarchical spatial representations from the signal-derived images.
  4. Classifier Head: A prediction module containing `Dense (Fully Connected)` layers that ingest the flattened features and output a binary stress/no-stress classification.

- Critical path:
  1. Data Fidelity: The integrity of the signal-to-image conversion (alignment, resampling) is paramount. Any artifact introduced here will be learned as a false feature.
  2. Initial Training: The model must achieve strong convergence on the first arrangement (EAP) to build a reliable feature extractor.
  3. Strategic Freezing: The correct implementation of layer freezing is the critical step for the success of the multi-arrangement training strategy.

- Design tradeoffs:
  - Image Resolution: The input is resized to 128x128. A higher resolution could capture more detail but increases computational load. A lower resolution may lose subtle temporal features.
  - Colorization: The paper argues that a custom colorization outperforms grayscale. This adds complexity to the preprocessing pipeline but appears essential for achieving the highest accuracy.

- Failure signatures:
  - Loss Plateau in Stage 2: If the loss does not decrease when fine-tuning on new arrangements, the frozen features may be too specialized to the first arrangement. Consider unfreezing the top convolutional blocks.
  - Low Baseline Accuracy: If the model cannot achieve >90% accuracy on a single arrangement, the issue is likely in the base CNN architecture or data pipeline, not the fusion strategy.

- First 3 experiments:
  1. Reproduce Baseline: Implement the full pipeline for a single arrangement (e.g., PEA) and train the CNN from scratch. Verify you can achieve a test accuracy close to the reported ~93% on the WESAD dataset.
  2. Ablation on Arrangement: Train separate models on each of the three arrangements (PEA, EAP, EPA). Compare their individual performance to understand which signal orderings are inherently more informative.
  3. Two-Stage Training: Implement the full two-stage process: train on the best-performing arrangement, freeze layers, and fine-tune on the combined dataset of the other two. Compare the final accuracy to the best single-arrangement model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed image-based fusion method perform when validated on diverse, real-world datasets outside of the semi-controlled WESAD environment?
- Basis in paper: [explicit] The conclusion states that "broader validation on more diverse datasets" is an important direction for future work to verify the method's applicability beyond the specific conditions of the current study.
- Why unresolved: The study relies exclusively on the WESAD dataset, which, while multimodal, may not capture the full variability of signal noise and stress types present in completely unconstrained real-world settings.
- What evidence would resolve it: Successful evaluation (maintaining high accuracy and robustness) on additional public datasets or data collected from free-living, ambulatory subjects.

### Open Question 2
- Question: Can the proposed architecture be optimized into a sufficiently lightweight model for real-time deployment on resource-constrained wearable devices?
- Basis in paper: [explicit] The discussion acknowledges "increased computational complexity due to higher-dimensional image representations" and explicitly calls for "investigation into lightweight architectures for real-time deployment."
- Why unresolved: Converting 1D signals to 2D images introduces dimensionality that is computationally expensive for microcontrollers typically found in wearables, and the current study did not implement or test the model on such hardware.
- What evidence would resolve it: Demonstration of the model running on embedded hardware (e.g., a low-power microcontroller) with low latency and memory usage while retaining classification accuracy.

### Open Question 3
- Question: Does the sample repetition technique used to align temporal resolutions introduce artificial artifacts that distort the learning of true physiological dynamics?
- Basis in paper: [inferred] The method repeats lower-frequency signals (e.g., repeating GSR samples 8 times) to fill the 2D matrix (Section 4.1). While the authors claim this preserves features, they do not analyze if this upsampling creates step-like artifacts that the CNN might misinterpret as physiological features.
- Why unresolved: The paper does not provide an ablation study comparing this repetition method against other alignment techniques (e.g., interpolation) or analyze the frequency response of the generated images.
- What evidence would resolve it: A comparative analysis of model performance when trained on images generated via sample repetition versus linear interpolation or other resampling methods.

### Open Question 4
- Question: What specific cross-signal dependencies are captured by the CNN in the fused 2D representation, and are they truly superior to 1D temporal correlations?
- Basis in paper: [explicit] The authors claim the method enables CNNs to "capture temporal and cross-signal dependencies more effectively," but the conclusion lists "improved interpretability of learned features" as a remaining direction for future work.
- Why unresolved: While Grad-CAM or similar techniques could visualize "where" the network looks, the paper does not definitively prove that the network is learning *inter-signal* relationships rather than just robust individual features from the strongest signal (e.g., PPG).
- What evidence would resolve it: Saliency maps or feature visualization demonstrating that the CNN's decision relies on the spatial relationship between different signal rows (e.g., PPG vs. GSR alignment) rather than features isolated within single rows.

## Limitations
- Incomplete specification of signal-to-image conversion process, requiring significant assumptions during reproduction
- Custom colorization technique lacks precise mathematical definition, potentially introducing implementation variance
- Does not report per-subject performance or investigate individual physiological variability

## Confidence

- **High Confidence**: The CNN architecture specification and two-stage training strategy are clearly described and reproducible. The reported accuracy of 95.86% on WESAD is internally consistent with the methodology.
- **Medium Confidence**: The core innovation of converting multimodal signals to 2D images for CNN analysis is well-articulated, but the exact implementation details require interpretation. The data augmentation mechanism through signal permutation is logically sound but not empirically validated in isolation.
- **Low Confidence**: The exact signal alignment and resampling strategy across different sampling rates (64Hz PPG vs 4Hz GSR) is inferred rather than explicitly stated, which could significantly impact reproducibility.

## Next Checks

1. **Ablation on Signal Arrangement**: Train separate models on each of the three arrangements (PEA, EAP, EPA) and compare their individual performance to understand which signal orderings are inherently more informative.
2. **Baseline Comparison**: Implement the full pipeline for a single arrangement and train the CNN from scratch to verify achievement of baseline accuracy (~93%) before applying the two-stage strategy.
3. **Feature Invariance Test**: Systematically vary the colorization scheme and signal arrangement order to test whether the model truly learns invariant features or is overfitting to specific visual patterns.