---
ver: rpa2
title: 'EvolveSearch: An Iterative Self-Evolving Search Agent'
arxiv_id: '2505.22501'
source_url: https://arxiv.org/abs/2505.22501
tags:
- search
- tool
- data
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EvolveSearch addresses the challenge of training large language
  models for effective web search by introducing an iterative self-evolution framework
  that combines reinforcement learning (RL) with supervised fine-tuning (SFT). The
  method leverages high-reward rollouts from RL to generate training data, applying
  three filtering rules to ensure quality and diversity.
---

# EvolveSearch: An Iterative Self-Evolving Search Agent

## Quick Facts
- arXiv ID: 2505.22501
- Source URL: https://arxiv.org/abs/2505.22501
- Reference count: 23
- Key outcome: 4.7% average accuracy improvement over state-of-the-art methods on 7 multi-hop question-answering benchmarks

## Executive Summary
EvolveSearch introduces an iterative self-evolution framework that combines reinforcement learning (RL) with supervised fine-tuning (SFT) to train large language models for effective web search without requiring human-annotated reasoning data. The method alternates between RL exploration to discover high-reward rollouts and SFT optimization using filtered rollouts, progressively improving agentic web search capabilities. Extensive experiments demonstrate superior performance in both in-domain and out-of-domain scenarios, with each iteration contributing to performance gains while addressing the challenge of effective reward signal generation in complex search environments.

## Method Summary
EvolveSearch employs an iterative loop of RL exploration and rejection sampling fine-tuning (RSFT) using Qwen2.5-7B-Instruct as the base model. The process alternates between RL training with GRPO on current data partitions to generate rollouts, then filters these rollouts through three rules (high-reward, same-query deduplication, multi-call selection) before using the filtered data for SFT to create the next cold-start model. This cycle repeats across N=10 iterations using 80k training queries partitioned into 10 parts. The final model combines hybrid rewards (format compliance + answer correctness) evaluated by a 72B judge model, with filtering ensuring quality and diversity while eliminating the need for human-annotated reasoning data.

## Key Results
- Achieves 4.7% average accuracy improvement over state-of-the-art methods on seven multi-hop question-answering benchmarks
- Demonstrates consistent improvement across iterations, with each iteration contributing measurable gains
- Shows superior performance in both in-domain (NQ, TQ, HotpotQA, 2Wiki) and out-of-domain scenarios (Musique, Bamboogle, PopQA)
- Ablation studies confirm each filtering rule (HRS, SQD, MCS) contributes significantly to final performance

## Why This Works (Mechanism)

### Mechanism 1: Iterative RL-SFT Bootstrapping
Alternating RL exploration with SFT optimization creates progressive performance gains that neither method achieves alone. RL phase discovers high-reward rollouts through environment interaction, which are then filtered by quality/diversity rules and distilled into the base model via SFT, producing an improved cold-start policy for more effective RL exploration in the next iteration. This self-bootstrapping continues until performance plateaus.

### Mechanism 2: Hybrid Reward Signal
Combining format compliance with answer correctness creates a dense learning signal while preventing reward hacking. The format reward (Rf=1.0 if structure correct, else 0) gates answer evaluation, with answer reward (Ra=1.0 if correct via judge model), resulting in final reward R = 0.5*(Rf + Ra) when format valid, else 0. This ensures the model learns both proper reasoning structure and semantic correctness.

### Mechanism 3: Three-Rule Data Filtering
Quality and diversity filtering transforms noisy RL rollouts into effective SFT training data through three sequential rules: Rule 1 retains only rollouts with reward ≥ δ, Rule 2 keeps the most tool-intensive sample per query, and Rule 3 selects top-k samples with most tool calls across iterations. This process ensures the SFT phase receives high-quality, diverse, and multi-step reasoning trajectories that encode better patterns than short successful paths.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Core RL algorithm that avoids requiring a separate value model while maintaining stable policy updates through group-based advantage normalization. *Quick check: Can you explain why GRPO uses group-level reward normalization rather than individual sample rewards?*
- **ReAct-style Reasoning (Thought-Action-Observation)**: Defines the rollout structure that agents must learn—alternating internal reasoning with external tool calls and environment feedback. *Quick check: How does the observation at step t influence the thought at step t+1?*
- **Rejection Sampling Fine-Tuning**: Mechanism for converting RL exploration results into supervised training data by selecting high-quality samples matching specific criteria. *Quick check: Why might rejection sampling fail if the selection criteria don't align with task success metrics?*

## Architecture Onboarding

- **Component map**: Base Model (Qwen2.5-7B-Instruct) -> RL Training (GRPO, hybrid reward) -> Filtered Data Pool (3-rule filtering) -> SFT Phase (Zero-3 offload) -> Judge Model (Qwen2.5-72B-Instruct)
- **Critical path**: Initialize with raw QA pairs → RL training on i-th data partition produces rollouts → Filter rollouts through HRS→SQD→MCS, merge with existing data pool → SFT base model on filtered data → produces cold-start for next RL phase → Repeat N times; final model is RL model from last iteration
- **Design tradeoffs**: Data quantity vs. quality (paper finds quality filtering more important than raw sample count), exploration vs. exploitation (temperature=1.0 encourages exploration; filtering retains only successful paths), iteration count vs. compute (N=10 iterations used; unknown if more iterations yield diminishing returns)
- **Failure signatures**: RL converges prematurely without SFT bootstrap, judge model mislabels correct answers corrupting reward signal, filter rules too strict eliminating diverse reasoning paths, format reward too rigid preventing exploration
- **First 3 experiments**: 1) Ablation on filtering rules: Remove each rule individually and measure in-domain vs. out-of-domain performance drop, 2) Single-iteration baseline: Run RL-only without iterative SFT to quantify bootstrap benefit, 3) Reward sensitivity test: Swap judge model and verify consistent improvement trend over iterations

## Open Questions the Paper Calls Out
- **Question 1**: How does the EvolveSearch framework adapt to environments requiring integration of multiple diverse tools (e.g., code execution, calculators) alongside web search? *Basis: Limitations section explicitly states investigation of multiple tools is left for future research.*
- **Question 2**: Can a streaming or online rollout filtering system effectively reduce computational overhead associated with iterative collection and batch filtering? *Basis: Limitations section notes current reliance on batch filtering adds computation cost.*
- **Question 3**: Is the self-evolution process dependent on a teacher model significantly larger than the student agent? *Basis: Framework uses 72B judge for 7B agent; no evaluation with same-size or self-evaluation protocols provided.*

## Limitations
- Search environment specifics remain underspecified, including API details, snippet parsing, and result ranking mechanisms that could significantly impact reproducibility
- Judge model reliability represents a critical dependency with no validation of accuracy on edge cases across diverse question types
- Early iteration performance bottleneck could occur if initial rollouts rarely achieve the 0.7 reward threshold, potentially stalling the self-evolution cycle

## Confidence
- **High Confidence**: Iterative RL-SFT bootstrapping mechanism and progressive improvement claims, supported by ablation studies showing each iteration contributes gains
- **Medium Confidence**: Hybrid reward signal effectiveness, as design is sound but judge model performance remains unverified across diverse question types
- **Medium Confidence**: Three-rule filtering efficacy, with clear ablation results but unknown sensitivity to reward threshold tuning and potential early-data-starvation scenarios

## Next Checks
1. **Judge Model Robustness Test**: Evaluate the same hybrid reward framework with alternative judge models (DeepSeek-V3, GPT-4o, Grok-3) to verify consistent improvement trends and rule out judge-specific artifacts
2. **Early Iteration Data Flow Analysis**: Track the number and quality of rollouts passing the 0.7 reward threshold in Iterations 1-3 to detect potential data starvation and validate whether performance bootstrap actually occurs
3. **Reward Threshold Sensitivity**: Systematically vary the reward threshold (δ=0.5, 0.7, 0.9) and observe impacts on both data volume and final performance to determine optimal filtering balance