---
ver: rpa2
title: 'COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought
  Processes'
arxiv_id: '2510.14763'
source_url: https://arxiv.org/abs/2510.14763
tags:
- creative
- writing
- reasoning
- chinese
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COIG-Writer introduces a novel Chinese creative writing dataset
  with process supervision, capturing both outputs and their underlying thought processes.
  The dataset contains 1,665 expert-curated triplets across 51 genres, each including
  a reverse-engineered prompt, detailed creative reasoning, and final text.
---

# COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes

## Quick Facts
- arXiv ID: 2510.14763
- Source URL: https://arxiv.org/abs/2510.14763
- Reference count: 40
- Primary result: Process supervision with a 1:12 ratio of creative to general samples achieves 62.75% win rate in Chinese creative writing

## Executive Summary
COIG-Writer introduces a novel Chinese creative writing dataset with process supervision, capturing both outputs and their underlying thought processes. The dataset contains 1,665 expert-curated triplets across 51 genres, each including a reverse-engineered prompt, detailed creative reasoning, and final text. Experimental results demonstrate that process supervision significantly improves creative writing quality when properly stabilized with general data - achieving 62.75% win rate with a 1:12 ratio of creative to general samples. The study reveals that creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English), and that lexical diversity inversely correlates with creative quality, suggesting compensatory behavior for logical deficiencies.

## Method Summary
The study fine-tunes Qwen2.5-7B-Instruct using five configurations: MCW (1,665 COIG-Writer samples only), MCW+1k, MCW+5k, MCW+10k, and MG (20k general-purpose samples). Each configuration uses SFT for 3 epochs with AdamW optimizer (lr=2e-5), global batch size=32, and linear warmup over 10% of steps. Human pairwise comparison evaluates outputs on 557 test queries (204 Chinese, 353 English), measuring win rates, Type-Token Ratio, generation length, and cross-lingual contamination. The dataset was constructed by filtering 10,000 Chinese stories for creativity, then having human annotators reverse-engineer prompts and reasoning chains, validated across six dimensions.

## Key Results
- Process supervision achieves 62.75% win rate at optimal 1:12 ratio of creative to general samples
- Pure process supervision (MCW) degrades to 35.78% win rate, demonstrating need for stabilization
- Cross-lingual transfer fails with 89.26pp performance gap between Chinese and English
- TTR inversely correlates with creative quality, indicating compensatory behavior for logical deficiencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Creative writing quality emerges from the interaction of narrative logic and linguistic expression, not a single unified capability
- Mechanism: Process supervision provides narrative logic (structural planning, coherent plot development), while general data provides linguistic expression (natural phrasing, stylistic fluency). Neither alone suffices—pure process supervision produces logical but stilted text, while pure general data produces fluent but incoherent "beautiful nonsense."
- Evidence: "creative excellence emerges from the interaction of logical scaffolding and linguistic grounding"; MCW failure (35.78%) vs MG failure (fluent but poor performance)

### Mechanism 2
- Claim: Process supervision requires a minimum stabilization threshold of general data (~1:12 ratio) to be effective
- Mechanism: Without sufficient general data, specialized reasoning patterns destabilize base linguistic competence, leading to outputs that are well-structured but linguistically unnatural
- Evidence: "A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)"

### Mechanism 3
- Claim: Creative writing capabilities are culturally and linguistically bound at the reasoning level
- Mechanism: Reasoning processes embedded with Chinese narrative structures cause "contamination" when applied to English, generating Chinese text for English prompts and disrupting reasoning flows
- Evidence: "MCW produces Chinese text in 12.18% of English prompts... indicating that Chinese narrative structures... constitute incompatible features for English generation"

## Foundational Learning

- **Process Supervision**: Providing models with intermediate reasoning steps leading to an output, in addition to input-output pairs. Needed to understand COIG-Writer's core innovation of adding reasoning chains for creative writing. Quick check: Can you explain training on (prompt, story) vs. (prompt, reasoning process, story)?

- **Type-Token Ratio (TTR)**: A measure of lexical diversity calculated as unique words divided by total words. Critical for interpreting the "TTR paradox" where higher diversity inversely correlates with creative quality. Quick check: If a model generates text with very high TTR, what might the COIG-Writer paper suggest is the underlying issue?

- **Fine-Tuning Data Mixture Ratios**: Combining different datasets in specific proportions during supervised fine-tuning to balance model capabilities. Essential for understanding the central finding of the critical 1:12 ratio between creative and general data. Quick check: What is the likely consequence of fine-tuning with a 1:1 ratio of COIG-Writer to general data?

## Architecture Onboarding

- **Component map**: Source Text Collection -> LLM Filtering (Quality/Creativity) -> Human Annotators (Reverse-Engineer Prompt + Reasoning) -> Human-in-the-Loop Quality Validation (6 dimensions) -> COIG-Writer Dataset (1,665 triplets) -> Base LLM (Qwen2.5-7B-Instruct) + COIG-Writer + General Data (mixed ratios) -> SFT -> Specialized Creative Writing Model -> Human Preference Evaluation (pairwise) -> Win Rates & Analysis

- **Critical path**: 1) Data curation is the primary innovation - poor annotation quality propagates failure through entire system. 2) Ratio optimization is critical configuration - incorrect `D_CW : D_G` ratio is most likely cause of performance degradation. 3) Human pairwise comparison is ground truth - relying solely on automated metrics is a known failure mode for creative writing.

- **Design tradeoffs**: Quality vs. Scale (small dataset but high-quality for mechanism discovery vs. scale optimization), Stability vs. Specialization (pure process supervision produces logical but unnatural text vs. pure general data produces fluent but incoherent text), Monolingual Excellence vs. Cross-Lingual Transfer (high Chinese performance sacrifices English performance).

- **Failure signatures**: Low Win Rate (<40%) with High TTR indicates compensatory lexical diversity masking logical deficiencies; well-structured but stilted text indicates strong narrative logic but poor linguistic expression (insufficient general data); fluent but incoherent text indicates strong linguistic expression but poor narrative logic (insufficient process supervision); cross-lingual contamination (Chinese text for English prompts) indicates incompatible reasoning patterns.

- **First 3 experiments**: 1) Replicate ratio ablation study (1:1.2, 1:6, 1:12) on held-out data to confirm 1:12 threshold isn't an artifact. 2) Train models with poor ratio (1:1) and optimal 1:12 ratio, compute TTR for both to validate TTR paradox as diagnostic. 3) Evaluate best-performing model on English prompts from different domain to test broader "incompatible features" claim.

## Open Questions the Paper Calls Out

- Would scaling COIG-Writer beyond 1,665 samples further improve Chinese creative writing performance or enable cross-lingual transfer? (explicit: "Future work should investigate whether increasing COIG-Writer data beyond 1,665 samples could enhance Chinese performance or enable more effective cross-lingual transfer")

- Does the 1:12 optimal ratio remain stable across different dataset scales? (explicit: "Future work scaling the creative writing dataset alongside general data could further elucidate whether this ratio remains optimal across different dataset sizes")

- Does limited English effectiveness stem from Chinese-centric cultural framing or insufficient English creative writing examples? (explicit: "The current study cannot definitively distinguish whether the limited English effectiveness stems from the dataset's Chinese-centric cultural framing or simply from insufficient creative writing examples for English")

## Limitations

- Small dataset size (1,665 samples) limits generalizability and makes findings dependent on human judgment rather than scalable automated evaluation
- 1:12 stabilization ratio may be specific to Qwen2.5-7B-Instruct and particular general-purpose data composition, limiting generalizability
- Cross-lingual transfer failure lacks direct mechanistic explanation—could stem from incompatible narrative structures or data imbalance
- TTR paradox is correlational and requires further investigation to establish causation beyond this single experimental paradigm

## Confidence

- **High Confidence**: Basic experimental setup and ratio findings (62.75% win rate at 1:12, degradation below threshold)
- **Medium Confidence**: Two-component model of narrative logic + linguistic expression is plausible but not definitively proven as the only explanation
- **Low Confidence**: Claim that creative reasoning patterns are fundamentally culturally-bound with no cross-lingual transfer lacks mechanistic explanation

## Next Checks

1. **Ratio Stability Test**: Replicate `D_CW : D_G` ablation study on different base model (Llama-3) and general-purpose dataset (RedPajama) to determine if 1:12 ratio is model- and data-specific

2. **TTR Causation Study**: Conduct controlled experiment training models to maximize/minimize TTR through different data distributions, then measure actual creative quality via human evaluation to establish whether high TTR causes logical deficiency

3. **Cross-Lingual Mechanism Probe**: Create multilingual COIG-Writer with parallel Chinese-English reasoning chains and train models to determine whether transfer failure is due to incompatible reasoning patterns or lack of aligned training data