---
ver: rpa2
title: Addressing Tokenization Inconsistency in Steganography and Watermarking Based
  on Large Language Models
arxiv_id: '2508.20718'
source_url: https://arxiv.org/abs/2508.20718
tags:
- tokens
- token
- watermarking
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses tokenization inconsistency (TI) between sender
  and receiver in LLM-based steganography and watermarking, which undermines robustness.
  Inconsistent tokens were found to be infrequent and temporary.
---

# Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models

## Quick Facts
- arXiv ID: 2508.20718
- Source URL: https://arxiv.org/abs/2508.20718
- Reference count: 40
- Primary result: Two methods solve tokenization inconsistency (TI) in LLM-based steganography and watermarking, improving robustness while maintaining fluency.

## Executive Summary
This paper addresses tokenization inconsistency (TI) between sender and receiver in LLM-based steganography and watermarking, a problem that undermines robustness when detokenized text is retokenized differently. The authors empirically show that inconsistent tokens are both infrequent and temporary, enabling two tailored solutions: a stepwise verification method for steganography that removes candidate-level inconsistent tokens before encoding, and a post-hoc rollback method for watermarking that waits for temporary inconsistencies to resolve. Experiments across three language models (Llama-2-7b, Swallow-7b, Qwen2.5-7b) demonstrate significant improvements in fluency, imperceptibility, and anti-steganalysis compared to baselines.

## Method Summary
The paper proposes two methods to address tokenization inconsistency (TI) in LLM-based steganography and watermarking. For steganography, it introduces a stepwise verification method that filters out candidate-level inconsistent tokens before arithmetic coding, ensuring 100% extraction accuracy. For watermarking, it implements a post-hoc rollback method that observes token generation for a fixed period after detecting potential TI, rolling back only if the inconsistency persists. Both methods leverage the empirical findings that inconsistent tokens are infrequent and temporary. The methods are evaluated across three language models using multilingual C4 dataset prompts, with comprehensive testing against various attacks and baselines.

## Key Results
- Steganography: Stepwise verification reduced KL divergence by 47.86% and improved anti-steganalysis accuracy by 3.53% compared to baselines.
- Watermarking: Post-hoc rollback enhanced detectability and robustness against attacks while maintaining lower perplexity than baseline methods.
- Both methods successfully eliminated extraction errors while preserving generation fluency.

## Why This Works (Mechanism)

### Mechanism 1: Consistency via Candidate-Level Pre-filtering
- **Claim:** Filtering candidate-level inconsistent tokens before sampling ensures 100% extraction accuracy for steganography receivers.
- **Mechanism:** The system simulates the detokenize-retokenize pipeline for every token in the candidate pool at each generation step, removing any token that causes a mismatch before steganographic encoding.
- **Core assumption:** The infrequency of inconsistent tokens means their removal doesn't deplete the candidate pool, allowing encoding to proceed without failure.
- **Break condition:** If a specific prompt causes the candidate pool to consist majority of inconsistent tokens, the pool becomes too small for encoding.

### Mechanism 2: Temporariness-Based Rollback
- **Claim:** Waiting for a fixed observation window ($q$ tokens) after detecting TI allows distinguishing between temporary and stable inconsistencies, preserving fluency.
- **Mechanism:** Upon detecting a potential inconsistent token, the system generates $q$ more tokens. If the text naturally recovers to a consistent state, no action is taken; if TI persists, the generation rolls back to resample.
- **Core assumption:** A significant portion of inconsistent tokens are temporary and resolve naturally with context, allowing detection algorithms to tolerate short misalignment windows.
- **Break condition:** If the observation window $q$ is set too low for models with long-range dependencies, unnecessary rollbacks may degrade generation speed.

### Mechanism 3: Statistical Imperceptibility via Rarity
- **Claim:** Removing inconsistent tokens improves imperceptibility by creating minimal distortion since these tokens are statistically rare.
- **Mechanism:** Unlike previous methods that modify entire ambiguous groups or distributions significantly, this method targets only the specific inconsistent token subset, creating small KL divergence between cover and stego text distributions.
- **Core assumption:** The probability distribution of the candidate pool remains representative of the original language model distribution after inconsistent token removal.
- **Break condition:** If a specific tokenizer/model has high density of inconsistent tokens, filtering them would significantly shift the probability distribution and alert steganalyzers.

## Foundational Learning

- **Concept: Detokenization-Retokenization Pipeline**
  - **Why needed here:** TI arises because generated tokens must be converted to string (detokenize) for transmission and back to tokens (retokenize) for extraction, potentially changing token boundaries.
  - **Quick check question:** Does the tokenizer merge the subwords `_no` and `body` differently depending on whether they are generated sequentially or tokenized from the final string "nobody"?

- **Concept: Subword Tokenization (BPE)**
  - **Why needed here:** The mechanism relies on understanding how Byte Pair Encoding creates "glitch" or "partial" tokens that cause the inconsistencies the paper aims to fix.
  - **Quick check question:** Why might a tokenizer prefer a single token for a word during retokenization that it split into two tokens during generation?

- **Concept: Arithmetic Coding in Steganography**
  - **Why needed here:** The stepwise verification method filters the candidate pool used for arithmetic coding, requiring understanding how mapping bits to intervals works.
  - **Quick check question:** If you remove a token from the candidate pool, how does the arithmetic decoder adjust the interval mapping for the secret message?

## Architecture Onboarding

- **Component map:** Input (Prompt + Secret/Watermark Key) -> Generation Core (LLM produces Logits -> Softmax -> Consistency Filter) -> Filtered Candidate Pool -> Stego/Watermark Encoder (maps secret/key to token) -> Transmission Layer (Detokenize -> String -> Retokenize) -> Receiver/Decoder (reads token sequence -> decodes message)

- **Critical path:** The Consistency Filter (Algorithm 1). This component must run detokenize + retokenize checks on every candidate token before the selection step, and inefficient implementation will destroy generation latency.

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Steganography requires checking every candidate (linear complexity O(N) per step), which is slower than standard sampling but faster than quadratic disambiguation baselines. Watermarking uses "post-hoc" rollback to avoid per-token check cost.
  - **Robustness vs. Fluency:** Aggressive rollback (low $q$) ensures consistency but may disrupt semantic flow. High $q$ maintains flow but risks extraction errors if TI is stable.

- **Failure signatures:**
  - **Empty Pool Error:** In steganography, if filtering removes all valid candidates (extremely rare), the system must fallback to a non-SIT token, temporarily pausing embedding.
  - **Infinite Rollback:** In watermarking, if a prompt consistently generates a "stable" inconsistent token that the filter fails to handle or resampling repeatedly selects, the loop must have a max retry limit.

- **First 3 experiments:**
  1. **Baseline TI Rate:** Generate 1000 samples using Llama-2-7b and Qwen2.5-7b. Calculate Text-Level and Token-Level Inconsistency Rates to verify the "infrequency" claim.
  2. **Stepwise Filter Impact:** Implement Algorithm 1. Measure Bits-Per-Token (BPT) and Latency overhead. Confirm BPT remains stable while extraction error drops to 0%.
  3. **Rollback Window Sensitivity:** Implement post-hoc rollback with $q=2$ vs $q=10$. Measure PPL (fluency) and Watermark Strength to find optimal settings for specific models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a rigorous theoretical framework be developed to explain the causal mechanisms behind the emergence of inconsistent tokens?
- **Basis in paper:** The authors explicitly state in the Limitations section that "a rigorous theoretical account of how such tokens arise is still lacking, which presents a promising direction for future research."
- **Why unresolved:** The current study relies on empirical observations to identify characteristics like infrequency and temporariness but does not provide a mathematical or linguistic model predicting their occurrence.
- **What evidence would resolve it:** A formal model that predicts the probability of tokenization inconsistency based on tokenizer vocabulary structure and generation context, validated against empirical rates.

### Open Question 2
- **Question:** How can the stepwise verification method for steganography be adapted to maintain robustness against active attackers capable of modifying the transmitted text?
- **Basis in paper:** The Limitations section notes that the threat model assumes "the absence of an active attacker capable of modifying the stegotexts," implying this is an unaddressed vulnerability.
- **Why unresolved:** The proposed method guarantees 100% extraction correctness only in a passive transmission channel; active modifications would break the token synchronization required for verification.
- **What evidence would resolve it:** An extension of the method that integrates error correction or synchronization recovery mechanisms, demonstrating successful extraction rates under character or token substitution attacks.

### Open Question 3
- **Question:** Is there a theoretically grounded or adaptive method for determining the optimal observation period ($q$) in the post-hoc rollback method?
- **Basis in paper:** The authors set the observation period $q$ heuristically ($q=2$ for Llama-2, $q=10$ for Swallow/Qwen) based on aggregate empirical statistics rather than a dynamic calculation.
- **Why unresolved:** A static $q$ may not be optimal for all generation contexts; a dynamic method could reduce unnecessary waiting or missed rollbacks.
- **What evidence would resolve it:** An algorithm that dynamically adjusts $q$ based on real-time confidence scores or token history, showing improved perplexity or detectability over fixed-$q$ baselines.

## Limitations
- **Limited generalizability:** The study is limited to three specific language models and the C4 dataset, which may not represent all LLM architectures or real-world usage patterns.
- **Theoretical gaps:** The paper lacks a rigorous theoretical framework explaining why inconsistent tokens emerge, relying instead on empirical observations.
- **Security assumptions:** The threat model assumes passive transmission channels without active attackers capable of modifying the stegotexts, leaving this vulnerability unaddressed.

## Confidence

- **Confidence: Medium** - The infrequency and temporariness assumptions are empirically supported within tested models and dataset but may not generalize to all tokenizers or domains.
- **Confidence: Medium** - The rollback mechanism's observation window may need adjustment for different models or longer-range dependencies, as the choice of $q$ values appears somewhat arbitrary.
- **Confidence: Low** - The security implications of removing candidate-level inconsistent tokens are not fully explored, as this could create detectable patterns for sophisticated steganalysis.
- **Confidence: Medium** - Computational overhead of the stepwise verification method may impact real-time applications, though claimed to be faster than quadratic disambiguation baselines.

## Next Checks

1. **Cross-Tokenizer Validation:** Test the stepwise verification and rollback methods across different tokenizer architectures (e.g., SentencePiece, WordPiece) beyond BPE to verify infrequency and temporariness assumptions hold generally. Generate 1000 samples per tokenizer and calculate Text-Level and Token-Level Inconsistency Rates.

2. **Adaptive Window Size Analysis:** Implement an adaptive observation window that adjusts based on the model's inconsistency patterns rather than using fixed $q$ values. Test across different prompt types and domains to find optimal rollback thresholds that minimize both extraction errors and generation latency.

3. **Steganalysis Resilience Testing:** Design and implement advanced steganalysis attacks that specifically target the token filtering patterns created by the stepwise verification method. Test whether removing candidate-level inconsistent tokens creates detectable statistical signatures in the stego text distribution, particularly for models with high IT density or specific prompt types.