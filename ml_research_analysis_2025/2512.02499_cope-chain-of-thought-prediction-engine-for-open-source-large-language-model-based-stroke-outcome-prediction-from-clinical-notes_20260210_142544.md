---
ver: rpa2
title: 'COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model
  Based Stroke Outcome Prediction from Clinical Notes'
arxiv_id: '2512.02499'
source_url: https://arxiv.org/abs/2512.02499
tags:
- clinical
- prediction
- cope
- stroke
- outcome
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'COPE is a chain-of-thought reasoning-enhanced large language model
  framework that predicts 90-day functional outcomes after acute ischemic stroke from
  unstructured discharge summaries. Built on open-source LLaMA-3-8B models, it uses
  a two-step architecture: the first model generates clinical reasoning from the discharge
  summary, and the second produces a single modified Rankin Scale (mRS) score.'
---

# COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes

## Quick Facts
- arXiv ID: 2512.02499
- Source URL: https://arxiv.org/abs/2512.02499
- Reference count: 0
- COPE achieves MAE 1.01 and ±1 mRS accuracy 74.4% for stroke outcome prediction from discharge summaries using open-source LLMs.

## Executive Summary
COPE is a chain-of-thought reasoning-enhanced large language model framework that predicts 90-day functional outcomes after acute ischemic stroke from unstructured discharge summaries. Built on open-source LLaMA-3-8B models, it uses a two-step architecture: the first model generates clinical reasoning from the discharge summary, and the second produces a single modified Rankin Scale (mRS) score. Evaluated on 464 patients, COPE achieved a mean absolute error (MAE) of 1.01 (95% CI 0.92–1.11), accuracy within ±1 mRS point of 74.4% (69.9–78.8%), and exact accuracy of 32.8% (28.0–37.6%). Performance was comparable to the proprietary GPT-4.1 model and superior to ClinicalBERT, a structured-variable machine learning model, and a single-step LLM without reasoning. Subgroup analyses showed higher prediction error among EVT patients, those with longer discharge summaries, and patients over 80 years old. COPE demonstrates that open-source, reasoning-based LLMs can provide accurate, interpretable, and privacy-preserving stroke outcome prediction from unstructured clinical text.

## Method Summary
COPE uses a two-step pipeline with off-the-shelf LLaMA-3-8B-Instruct models without fine-tuning. The first model processes discharge summaries with a reasoning prompt to generate structured clinical rationale. The second model receives this reasoning and outputs a single integer mRS score (0-6). The framework was evaluated on 464 acute ischemic stroke patients from Stanford University Hospital, with 20% of cases used for prompt optimization and 80% held out for testing. Performance was measured using MAE, ±1 accuracy, and exact accuracy, with bootstrap resampling for confidence intervals.

## Key Results
- Achieved MAE of 1.01 (95% CI 0.92–1.11) for mRS prediction from discharge summaries
- Obtained 74.4% accuracy within ±1 mRS point and 32.8% exact accuracy
- Performance matched proprietary GPT-4.1 and exceeded ClinicalBERT and single-step LLM baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling clinical reasoning from score extraction improves output reliability and interpretability compared to single-step prediction
- Mechanism: First LLM generates unconstrained clinical rationale; second LLM receives only this reasoning and outputs a structured integer. This prevents format contamination where complex reasoning degrades output discipline.
- Core assumption: Single-step models struggle to simultaneously reason clinically and enforce rigid output constraints.
- Evidence anchors:
  - [abstract] "COPE uses a two-step CoT framework...the first generates clinical reasoning, and the second outputs an mRS prediction."
  - [page 12-13] "Single-step LLMs often produced free-text or mixed-format outputs...Paradoxically, more elaborate prompts, which often improve reasoning in other domains, degraded performance in this structured clinical prediction setting."
  - [corpus] Weak/no direct corpus evidence for this specific two-step decoupling mechanism in clinical prediction.

### Mechanism 2
- Claim: Zero-shot inference from pre-trained open-source LLMs can match proprietary model performance when task-specific architectural constraints are applied
- Mechanism: LLaMA-3-8B's pre-training implicitly encodes clinical knowledge; CoT prompting surfaces this without weight updates.
- Core assumption: The base model's pre-training distribution includes sufficient medical/clinical text to support stroke outcome reasoning.
- Evidence anchors:
  - [abstract] "Performance was comparable to the proprietary GPT-4.1 model and superior to ClinicalBERT, a structured-variable machine learning model, and a single-step LLM without reasoning."
  - [page 6] "Built with two off-the-shelf, open-source LLaMA-3-8B-Instruct models...no additional fine-tuning or weight updates were performed."
  - [corpus] Paper 91186 (temporal reasoning LLMs for clinical summarization) similarly evaluates zero-shot open-source LLMs on clinical tasks, providing indirect support.

### Mechanism 3
- Claim: Prompt engineering with domain expertise encoding (persona, scale definitions, reasoning instructions) is a primary performance driver
- Mechanism: Prompts inject clinical context—vascular neurologist persona, explicit mRS definitions, reasoning requirements—without modifying model weights.
- Core assumption: Prompt quality significantly impacts reasoning quality and final predictions; manual optimization is sufficient.
- Evidence anchors:
  - [page 6] "20% prompt-exploration subset, used to optimize prompts for both LLMs...final prompts were selected based on predictive accuracy."
  - [page 13] "The quality of prompt engineering likely influenced how the large language model made predictions...prompt design was largely manual and guided by iterative trial and error."
  - [corpus] Weak corpus evidence—no systematic prompt optimization comparisons identified.

## Foundational Learning

- Concept: **Modified Rankin Scale (mRS)**
  - Why needed here: This ordinal scale (0–6) is the prediction target; understanding its granularity is essential for interpreting MAE and ±1 accuracy.
  - Quick check question: What functional state distinguishes mRS=3 from mRS=4?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: COPE's core architectural principle—explicit intermediate reasoning before structured output.
  - Quick check question: In CoT, does the model output reasoning before or after the final answer?

- Concept: **Zero-Shot Inference**
  - Why needed here: COPE operates without fine-tuning; understanding what prior knowledge is assumed vs. what must be prompted.
  - Quick check question: What data would be required to convert this to a fine-tuned approach?

## Architecture Onboarding

- Component map:
  - De-identified discharge summary -> Reasoning LLM (LLaMA-3-8B-Instruct + Reasoning Prompt) -> Clinical rationale -> Extraction LLM (LLaMA-3-8B-Instruct + Extraction Prompt) -> mRS integer (0-6)

- Critical path:
  1. Data preprocessing (PHI removal, de-identification)
  2. Prompt selection/load (reasoning prompt → extraction prompt)
  3. Sequential inference: discharge summary → reasoning text → mRS integer
  4. Output validation (confirm integer in range 0–6)

- Design tradeoffs:
  - 8B vs. larger models (GPT-4.1): ~60–100× fewer parameters; lower compute/privacy cost vs. potential ceiling on performance
  - Two-step vs. single-step: Added inference overhead for interpretability and output reliability
  - Zero-shot vs. fine-tuning: Deployment simplicity vs. potential accuracy gains from domain adaptation

- Failure signatures:
  - MAE elevation in EVT subgroup (1.07 vs. 0.80 non-EVT)—likely case complexity and outcome heterogeneity
  - Performance degradation with longer notes (MAE rises across word-count quartiles)—noise dilutes signal
  - Elevated error in patients >80 years (MAE 1.32)—frailty/comorbidity increase outcome variability

- First 3 experiments:
  1. **Reproduce baseline:** Run COPE on the described test set (n=372) to verify MAE ~1.01 and ±1 ACC ~74%
  2. **Ablation (two-step vs. single-step):** Directly compare to isolate the contribution of reasoning decoupling
  3. **External validation:** Test on discharge summaries from a different institution to assess generalizability (not yet performed per paper limitations)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does COPE maintain performance parity with proprietary models when validated across multicenter datasets with diverse clinical workflows?
- Basis in paper: [explicit] The authors note that single-center evaluation limits generalizability and state, "Future multicenter validation will be essential to confirm external generalizability."
- Why unresolved: Current results are derived solely from the Stanford University Hospital registry, which may reflect specific documentation styles and patient populations.
- What evidence would resolve it: Prospective evaluation on external stroke registries from different geographical regions or hospital systems.

### Open Question 2
- Question: Does the chain-of-thought reasoning output generated by COPE improve clinical decision-making or trust compared to models providing only a numeric score?
- Basis in paper: [explicit] The Discussion states, "The study did not assess clinician interaction with the model... Evaluating usability and trust among care providers will be critical for future translation."
- Why unresolved: While the authors claim enhanced interpretability, no human-subject data currently exists to verify if clinicians find the reasoning valid or useful.
- What evidence would resolve it: User studies measuring physician prediction accuracy and reported trust levels when aided by COPE's reasoning versus standard predictors.

### Open Question 3
- Question: Can systematic or automated prompt optimization methods improve the robustness of COPE compared to the current manual trial-and-error approach?
- Basis in paper: [explicit] The authors acknowledge that "prompt design was largely manual" and suggest "Developing systematic or automated prompt optimization approaches may help reduce reliance on manual tuning."
- Why unresolved: Manual prompt engineering is brittle and may lead to overfitting on the exploration subset, risking performance drops on unseen data distributions.
- What evidence would resolve it: Comparative analysis of COPE's performance variance using automated prompt optimization algorithms versus manually curated prompts.

## Limitations
- Single-center evaluation on Stanford University Hospital data limits generalizability to other institutions and patient populations
- Manual prompt engineering approach lacks systematic optimization and may lead to overfitting on exploration subset
- Elevated prediction error in complex cases (EVT patients, elderly patients >80 years, longer discharge summaries)

## Confidence

- **High confidence**: The core architectural innovation (two-step CoT reasoning → extraction) and its superiority to single-step approaches is well-supported by direct comparisons within the study.
- **Medium confidence**: The claim that open-source models can match proprietary models is supported within this specific task and dataset, but generalizability to other clinical prediction tasks remains unproven.
- **Low confidence**: The long-term clinical utility and implementation feasibility in real-world settings remain speculative.

## Next Checks
1. **External validation**: Test COPE on discharge summaries from a different institution with distinct patient demographics and clinical practices to assess generalizability beyond the Stanford dataset.
2. **Subgroup performance analysis**: Conduct detailed error analysis on EVT patients and elderly populations to understand failure modes and identify potential model refinements for these high-error subgroups.
3. **Ablation study**: Systematically vary prompt engineering parameters (temperature, reasoning depth, persona specifications) to quantify their individual contributions to overall performance and establish more robust prompt design principles.