---
ver: rpa2
title: Comparative Study on the Discourse Meaning of Chinese and English Media in
  the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering
arxiv_id: '2504.18106'
source_url: https://arxiv.org/abs/2504.18106
tags:
- media
- chinese
- olympics
- opening
- ceremony
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study employs LDA topic modeling and LLM prompt engineering
  to analyze Chinese and English media coverage of the 2024 Paris Olympics. Chinese
  media focus on specific sports, Olympic spirit, doping controversies, and new technologies,
  while English media emphasize female athletes, medal wins, and eligibility disputes.
---

# Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering

## Quick Facts
- arXiv ID: 2504.18106
- Source URL: https://arxiv.org/abs/2504.18106
- Reference count: 40
- Primary result: Combined LDA topic modeling and LLM prompt engineering reveals distinct Chinese and English media discourse patterns around the 2024 Paris Olympics, with Chinese media focusing on specific sports and Olympic spirit while English media emphasize female athletes and medal wins.

## Executive Summary
This study employs Latent Dirichlet Allocation (LDA) topic modeling combined with Large Language Model (LLM) prompt engineering to analyze Chinese and English media coverage of the 2024 Paris Olympics. The research identifies distinct thematic focuses between the two language groups: Chinese media emphasize specific sports, Olympic spirit, doping controversies, and new technologies, while English media concentrate on female athletes, medal wins, and eligibility disputes. Both media types share common themes including the opening ceremony, athlete performance, and sponsorship brands. The methodology integrates macro-level topic extraction with micro-level discourse analysis through collocation patterns and semantic prosody classification, providing objective insights into how media construct discourse across languages.

## Method Summary
The study analyzes 715 Chinese and 499 English media reports from top-tier outlets using a two-stage approach. First, separate LDA models identify latent topics through unsupervised statistical co-occurrence analysis, with topic coherence optimization determining optimal topic counts (9 for Chinese, 12 for English, manually consolidated to 7 and 6). Second, LLM prompt engineering resolves interpretation challenges by retrieving keyword meanings from the model's knowledge base and generating human-interpretable topic descriptions through a weighted combination of keywords, retrieved meanings, and manual annotations. Collocation analysis of high-frequency keywords (node words) reveals systematic co-occurrence patterns that indicate semantic prosody—evaluative tendencies that signal media attitudes toward specific Olympic themes.

## Key Results
- Chinese media topics: specific sports coverage, Olympic spirit, doping controversies, new technologies, Chinese athlete performance
- English media topics: female athletes, medal wins, eligibility disputes, opening ceremony reactions, athlete achievements
- Semantic prosody differences: Chinese reports show positive prosody for opening ceremony and sportsmanship; English reports show positive prosody for female athletes but negative prosody for opening ceremony predictions and women's boxing controversies
- Common coverage: Both media types extensively cover the opening ceremony, athlete performance, and sponsorship brands

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LDA topic modeling combined with LLM prompt engineering enables objective extraction of coherent media discourse themes from multilingual corpora.
- Mechanism: LDA identifies latent topic clusters through unsupervised statistical co-occurrence of terms; LLM prompt engineering then resolves the coherence problem by retrieving keyword meanings from the model's knowledge base and generating human-interpretable topic descriptions. The formula combines weighted keywords, retrieved meanings, and manual annotations to produce discourse implications.
- Core assumption: LDA word clusters contain sufficient signal for meaningful topic reconstruction, and the LLM's internal knowledge base accurately contextualizes domain-specific terms (e.g., athlete names, Olympic terminology).
- Evidence anchors: LDA results show coherent topic clusters; LLM-generated descriptions align with manual annotations; topic coherence scores validate optimal topic counts.

### Mechanism 2
- Claim: Extended units of meaning analysis through collocation patterns reveals cross-linguistic differences in semantic prosody and media attitudes.
- Mechanism: Building on Sinclair's extended meaning unit theory, high-frequency keywords are analyzed for systematic co-occurrence patterns. The surrounding context reveals evaluative tendencies (semantic prosody)—positive, negative, or neutral—that indicate media stance.
- Core assumption: Collocational frequency correlates with deliberate discursive positioning, and translation-equivalent terms carry language-specific evaluative patterns.
- Evidence anchors: Detailed analysis of "opening ceremony" shows distinct collocation patterns between Chinese and English; semantic categorization reveals consistent evaluative differences across topics.

### Mechanism 3
- Claim: Topic number optimization through coherence scoring balances granularity with interpretability for comparative media analysis.
- Mechanism: The study experiments with topic counts from 2-20, using topic coherence as the evaluation metric. Optimal topics were 9 (Chinese) and 12 (English), subsequently manually reduced to 7 and 6 respectively based on semantic clustering and research relevance.
- Core assumption: Coherence scores capture human-perceived topic quality, and manual reduction preserves thematic distinctness without introducing researcher bias.
- Evidence anchors: Coherence scores peak at 9/12 topics; manual consolidation produces clearly differentiated, semantically coherent clusters without overlap.

## Foundational Learning

- Concept: **Latent Dirichlet Allocation (LDA)**
  - Why needed here: Core methodology for unsupervised topic extraction; understanding the three-layer Bayesian architecture (document-topic-word) is essential for interpreting topic tables
  - Quick check question: Can you explain why increasing topic numbers might improve coherence initially but degrade interpretability?

- Concept: **Semantic Prosody**
  - Why needed here: The analytical lens for detecting evaluative attitudes in collocation patterns; distinguishes this study from simple topic frequency analysis
  - Quick check question: Given the phrase "miss out on an Olympic gold medal," would you classify the semantic prosody of "gold medal" in this context as positive, negative, or neutral?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Referenced in the prompt engineering framework; underpins the multi-step LLM reasoning for topic interpretation
  - Quick check question: How does the four-step QLFR framework improve upon single-prompt classification?

## Architecture Onboarding

- Component map:
  Data Layer (Wisdom News Database + Factiva) -> Preprocessing Layer (LLM cleaning) -> Topic Extraction Layer (Separate LDA models) -> Interpretation Layer (LLM prompt engineering) -> Micro-Analysis Layer (Collocation patterns)

- Critical path:
  1. Corpus construction with keyword filters (≥2 occurrences)
  2. Preprocessing (lemmatization critical for Chinese tokenization accuracy)
  3. LDA training with coherence sweep (2-20 topics)
  4. Manual topic consolidation based on semantic overlap
  5. High-frequency keyword extraction per topic
  6. Collocation pattern identification (frequency-based)
  7. Semantic prosody annotation through extended context review

- Design tradeoffs:
  - Statistical vs. interpretive topic selection: Coherence scores alone produced 9/12 topics; manual consolidation reduced to 7/6 for thematic clarity—trades reproducibility for interpretability
  - Single-language vs. cross-linguistic analysis: Separate LDA models per language precludes direct topic alignment but respects language-specific term distributions
  - LLM interpretation: Relies on proprietary knowledge bases; transparency limited compared to rule-based topic labeling

- Failure signatures:
  - LDA topics with high-weight keywords that are stopwords or generic terms
  - Collocation patterns with <5 occurrences making prosody classification speculative
  - LLM-generated topic descriptions that contradict manual annotations
  - Coherence score variance >0.1 across repeated runs (indicates unstable topic structure)

- First 3 experiments:
  1. Baseline replication: Run LDA on the described corpus with 9/12 topics, compare coherence scores and top keywords against tables
  2. Prompt engineering ablation: Test the weighted formula with and without manual description component to measure human-in-the-loop contribution
  3. Semantic prosody validation: Select 50 instances each of "opening ceremony" from both corpora, have independent annotators classify prosody, compare against paper's categorization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the integration of LDA topic modeling with LLM prompt engineering be optimized to move beyond topic description to more nuanced discourse analysis?
- Basis in paper: The conclusion states, "Future work could enhance LDA integration with practical prompt engineering for more nuanced media discourse analysis."
- Why unresolved: The current study utilized the LLM primarily to interpret keywords and generate topic descriptions, stopping short of fully automating the discourse analysis.
- What evidence would resolve it: Developing prompt chains that successfully extract semantic prosody or framing directly from raw text without relying on intermediate manual keyword analysis.

### Open Question 2
- Question: To what extent does the choice of specific Large Language Model architectures (e.g., GPT vs. Llama) impact the reliability of the topic descriptions generated?
- Basis in paper: The paper proposes a general framework using "Large Language Models" but does not specify which model was used or evaluate the stability of the output across different models.
- Why unresolved: The methodology relies on an LLM to resolve the "incoherent" results of LDA, but the potential for model-specific hallucination or bias in the description phase is not assessed.
- What evidence would resolve it: A comparative study applying the same LDA keywords and prompt formulas to multiple different LLMs to measure consistency in the generated topic meanings.

### Open Question 3
- Question: Do the identified differences in semantic prosody (e.g., positive vs. negative views on the opening ceremony) persist across non-elite or social media sources?
- Basis in paper: The corpus is strictly limited to "ten top-tier Chinese media outlets" and "ten well-known English media outlets," excluding social media or local press.
- Why unresolved: The findings describe the discourse of "mainstream" authority, but it is unclear if these attitudes are reflected in broader public discourse or informal media.
- What evidence would resolve it: Replicating the study using a corpus of social media posts (e.g., Weibo, X/Twitter) from the same time period to see if the "positive" or "negative" prosodies align with elite media.

## Limitations

- The study relies on proprietary LLM prompt engineering for topic interpretation, introducing transparency gaps as specific manual descriptions used in the formula are not disclosed
- Cross-linguistic topic alignment is not attempted, preventing direct semantic comparison of equivalent themes across languages
- Semantic prosody classification relies on subjective human interpretation without reported inter-coder agreement statistics
- Corpus construction uses keyword filtering without specifying stopword lists or explaining how the threshold was determined

## Confidence

- **High confidence**: LDA topic extraction and coherence optimization (standard methodology with reproducible results)
- **Medium confidence**: Comparative theme identification (robust patterns clearly supported)
- **Medium confidence**: Semantic prosody analysis (methodologically valid but subjective interpretation without reliability metrics)
- **Low confidence**: LLM-generated discourse implications (proprietary knowledge base reliance with limited transparency)

## Next Checks

1. Replicate the LDA topic extraction with the described corpus, comparing coherence scores and top keywords against Tables IV/V to verify reproducibility
2. Conduct inter-coder reliability assessment on semantic prosody classification for 50 instances each of "opening ceremony" from both corpora
3. Test Equation (2) with and without the manual description component (Pi) to measure human-in-the-loop contribution to topic interpretability