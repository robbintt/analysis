---
ver: rpa2
title: 'HAMSA: Hijacking Aligned Compact Models via Stealthy Automation'
arxiv_id: '2508.16484'
source_url: https://arxiv.org/abs/2508.16484
tags:
- prompts
- jailbreak
- arxiv
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HAMSA introduces an automated red-teaming framework for generating
  stealthy jailbreak prompts against aligned compact LLMs, addressing the limitations
  of manual and low-quality automated methods. The core method employs a multi-stage
  evolutionary search with temperature-controlled variability to iteratively refine
  prompts while maintaining natural language fluency.
---

# HAMSA: Hijacking Aligned Compact Models via Stealthy Automation

## Quick Facts
- arXiv ID: 2508.16484
- Source URL: https://arxiv.org/abs/2508.16484
- Reference count: 40
- Primary result: Automated framework achieving 97% success rate on compact LLM jailbreaks via stealthy prompts

## Executive Summary
HAMSA introduces an automated red-teaming framework for generating stealthy jailbreak prompts against aligned compact LLMs, addressing limitations of manual and low-quality automated methods. The core method employs a multi-stage evolutionary search with temperature-controlled variability to iteratively refine prompts while maintaining natural language fluency. A Policy Puppetry Template disguises malicious instructions as benign policy files (XML, INI, JSON), and RAG-enhanced lifelong learning enables strategy transfer across prompts. Evaluated on English and Arabic datasets, HAMSA achieved success rates up to 97% with mean scores above 9.4, outperforming baselines. Notably, Arabic prompts showed higher adversarial potency (2.24 harmfulness score) than English (1.28), highlighting increased vulnerability in less-resourced dialects.

## Method Summary
HAMSA operates in two phases: WarmUp and Lifelong learning. WarmUp uses 20 seed prompts to populate a RAG library through evolutionary optimization with Policy Puppetry Templates. Lifelong learning applies retrieved strategies to 170 stratified prompts across 13 topics, iteratively refining attacks until achieving safety violation scores > 8.5. The framework uses temperature-controlled evolutionary search to balance effectiveness with fluency, wrapping malicious queries in XML/INI/JSON templates to bypass token-level filters. Attacker models (Qwen-7B, Mistral-7B, Vicuna-7B) generate prompts, victim model (GigaChatLite) receives attacks, and scorer LLM (GigaChat-MAX) evaluates responses on a 1-10 safety scale. The RAG library stores successful attack strategies with embeddings for retrieval-based transfer to new prompts.

## Key Results
- Achieved 97% absolute success rate on English dataset across 13 topics
- Arabic Darija prompts showed higher adversarial potency (2.24 harmfulness score) than English (1.28)
- Mean output quality scores exceeded 9.4 across evaluation metrics
- Required mean of 3.2 attack iterations to achieve success threshold

## Why This Works (Mechanism)

### Mechanism 1: Policy Puppetry Template Obfuscation
- Claim: Structural disguise of malicious instructions as configuration files bypasses token-level safety filters while preserving semantic content.
- Mechanism: Malicious queries $Q_i$ are wrapped in structured templates $\Pi$ (XML, INI, JSON), transforming combined query to $T_i = \langle\Pi(J_i), Q_i\rangle$. Safety classifiers trained on natural language patterns fail to recognize adversarial intent when presented in policy-file syntax, as these formats are statistically associated with benign configuration tasks during training.
- Core assumption: Safety classifiers rely on surface-level token patterns rather than deep semantic understanding of intent.
- Evidence anchors:
  - [abstract] "A Policy Puppetry Template disguises malicious instructions as benign policy files (XML, INI, JSON)"
  - [section 3] "The key idea of this template is reformulating prompts as proposal to complement social scene with various heroes, an environment, limitations, and to look like one of a few types of policy files"
  - [corpus] Neighbor paper "Graph of Attacks with Pruning" confirms stealthy prompt generation via structural obfuscation improves jailbreak success
- Break condition: If defenders implement semantic intent classifiers that parse structured formats and evaluate embedded instructions independently of syntax.

### Mechanism 2: Temperature-Controlled Evolutionary Search
- Claim: Population-based optimization with temperature-guided mutations balances adversarial effectiveness with natural language fluency.
- Mechanism: Candidate prompts undergo iterative refinement via mutation operators $\mu$ (paraphrasing, format obfuscation) and selection $\sigma$ guided by fitness scores $S(j_i) = -\log P(r|t_1, \ldots, t_m)$. Temperature-controlled variability controls exploration vs. exploitation, preventing degeneration into incoherent text that triggers perplexity-based filters.
- Core assumption: The fitness landscape contains discoverable optima where prompts simultaneously maximize harmful response probability and maintain fluency.
- Evidence anchors:
  - [abstract] "multi-stage evolutionary search with temperature-controlled variability to iteratively refine prompts while maintaining natural language fluency"
  - [section 3] "Iterative red-teaming can further be formalized as an evolutionary process: $J^{t+1} = \sigma(\mu(J^t), S)$"
  - [corpus] AutoDAN-related work (cited in paper) establishes hierarchical genetic algorithms for stealthy jailbreaks; HAMSA extends this with RAG
- Break condition: If temperature scheduling is poorly tuned—too high causes incoherence (detected by filters), too low causes premature convergence to weak attacks.

### Mechanism 3: RAG-Enhanced Strategy Transfer
- Claim: Retrieval-based strategy libraries enable cross-prompt generalization by reusing successful attack patterns from semantically similar queries.
- Mechanism: During WarmUp, successful attacks are stored with embeddings, summaries, and score differentials. In Lifelong phase, for new query $q$, top-K strategies are retrieved via cosine similarity $\text{sim}(r, r') = \langle\phi(r), \phi(r')\rangle / (\|\phi(r)\| \|\phi(r')\|)$. Algorithm 1 selects strategies based on score differential thresholds ($\Delta s_{max} > 5$ uses best strategy; $2 \leq \Delta s_{max} \leq 5$ combines moderate strategies).
- Core assumption: Semantic similarity of query contexts predicts transferability of attack strategies.
- Evidence anchors:
  - [abstract] "RAG-enhanced lifelong learning enables strategy transfer across prompts"
  - [section 3] "This design ensures that during Lifelong learning - where attacks leverage accumulated knowledge - early-stage performance on new prompts mirrors effectiveness observed later"
  - [corpus] Limited direct corpus evidence on RAG for jailbreak transfer; related work focuses on gradient-based or genetic methods without retrieval
- Break condition: If attack strategies are query-specific with low transferability, or if embedding similarity fails to capture attack-relevant features.

## Foundational Learning

- Concept: **Evolutionary/Genetic Optimization for Discrete Text**
  - Why needed here: HAMSA uses population-based search over discrete token sequences. Understanding mutation, crossover, selection pressure, and fitness landscapes is essential for diagnosing convergence issues.
  - Quick check question: Can you explain why temperature scaling in mutation prevents premature convergence in discrete search spaces?

- Concept: **Safety Alignment and Jailbreak Taxonomy**
  - Why needed here: The paper assumes familiarity with RLHF, instruction tuning, and why aligned models remain vulnerable. Understanding prefix injection, refusal suppression, and perplexity-based detection contextualizes why stealthy methods succeed.
  - Quick check question: Why do perplexity-based filters flag adversarial suffixes generated by gradient-based methods like GCG but not human-crafted prompts?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: HAMSA's Lifelong phase depends on embedding-based retrieval to select attack strategies. Understanding embedding spaces, cosine similarity, and retrieval thresholds is necessary for debugging strategy selection.
  - Quick check question: How does the embedding function $\phi(\cdot)$ affect which strategies are retrieved for semantically distinct but attack-similar queries?

## Architecture Onboarding

- Component map: Attacking LLM -> Victim LLM -> Scorer LLM -> RAG Library (bidirectional)
- Critical path:
  1. WarmUp: 20 seed prompts → Policy Puppetry wrapper → Victim response → Scorer evaluation → Successful attacks summarized and stored in RAG
  2. Lifelong: New query → Embed query → Retrieve top-K strategies → Apply selected strategies via Policy Puppetry → Iterate until score > 8.5 or max iterations
  3. Test: Frozen RAG library → No summarizer updates → Standardized attack execution
- Design tradeoffs:
  - RAG library size vs. retrieval precision: Larger library increases strategy diversity but may retrieve noisy matches
  - Temperature vs. stealth: Higher temperature explores more diverse attacks but risks incoherence detectable by perplexity filters
  - Score threshold vs. attack iterations: Lower termination threshold (e.g., 7.0) reduces iterations but may miss high-quality jailbreaks
  - WarmUp dataset size vs. coverage: Smaller WarmUp (20 prompts) is efficient but may miss topic-specific strategies
- Failure signatures:
  - Low success rate on specific topics: Check RAG library for topic coverage; may need targeted WarmUp expansion
  - High perplexity detections: Temperature too high; reduce mutation variability or add fluency constraint
  - Retrieved strategies irrelevant to query: Embedding function may not capture attack-relevant semantics; consider contrastive fine-tuning
  - Score differential $\Delta s_{max} < 2$ consistently: Strategy library exhausted; trigger NewStrategies() exploration mode
- First 3 experiments:
  1. Baseline reproduction: Run WarmUp (20 prompts) + Lifelong (170 prompts) on English dataset with Mistral-7B attacker; verify ~97% success rate on Fraud/Illegal Activity topics matches Table 2
  2. Ablation: Policy Puppetry removal: Disable XML/INI/JSON templates and use raw queries; measure success rate drop and perplexity detection increase to quantify obfuscation contribution
  3. Cross-lingual transfer test: Train RAG library on English WarmUp only, then test on Arabic Darija dataset; measure success rate gap to quantify language-specific strategy requirements (paper reports Arabic harmfulness 2.24 vs. English 1.28, suggesting partial transfer but increased vulnerability)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the higher adversarial potency observed in Arabic prompts primarily driven by insufficient safety alignment data in low-resource dialects or by the structural/morphological properties of the language itself?
- Basis: [inferred] The Discussion section hypothesizes these two factors but does not isolate them: "We hypothesize two contributing factors. Firstly, the reduced robustness... due to insufficient training data. Secondly, the structural properties of Arabic..."
- Why unresolved: The paper reports the phenomenon (Arabic harmfulness score 2.24 vs English 1.28) but does not perform ablation studies to decouple the linguistic features from the training data volume.
- What evidence would resolve it: A controlled ablation study comparing attacks on high-resource Arabic (Modern Standard) vs. dialectal Arabic, or a comparative analysis controlling for morphological complexity across different low-resource languages.

### Open Question 2
- Question: Can defensive mechanisms be designed to generalize effectively against adversarial attacks across diverse linguistic varieties without degrading model utility?
- Basis: [explicit] The Conclusion states: "Future work will focus on... designing defenses that generalize across diverse linguistic varieties."
- Why unresolved: The paper focuses on the attack methodology (HAMSA) and highlights vulnerabilities in specific dialects (Darija), but does not propose or test specific defensive countermeasures.
- What evidence would resolve it: A study evaluating existing defense strategies (e.g., perplexity filtering, guard models) on the curated Darija dataset, or the proposal of a new dialect-agnostic defense mechanism.

### Open Question 3
- Question: Does the HAMSA framework's high success rate transfer to larger, non-compact frontier models, or is it dependent on the specific alignment weaknesses of efficiency-oriented variants?
- Basis: [inferred] The scope is explicitly limited in the Introduction and Abstract to "aligned compact LLMs" and "efficiency-oriented variants," using 7B parameter models for evaluation.
- Why unresolved: The authors demonstrate success on 7B models (Mistral, Qwen, Vicuna) and GigaChatLite, but the behavior of larger, more robustly aligned models (e.g., GPT-4 or 70B+ variants) remains untested.
- What evidence would resolve it: Evaluation of HAMSA's attack success rate (ASR) against frontier models (e.g., GPT-4, Llama-3-70B) compared to the reported compact model baselines.

## Limitations
- Missing hyperparameter specifications for temperature-controlled evolutionary search, creating reproducibility uncertainty
- Limited validation of RAG strategy transfer effectiveness across semantically distinct prompts
- Smaller Arabic dataset (100 prompts) may not fully represent dialectal language vulnerabilities
- No evaluation against frontier models (70B+ parameters) to assess scalability limitations

## Confidence
- **High confidence**: Policy Puppetry Template mechanism for structural obfuscation is well-documented and theoretically sound. The claim that structured formats bypass token-level safety filters is supported by the core assumption that classifiers rely on surface patterns rather than deep semantic understanding.
- **Medium confidence**: Temperature-controlled evolutionary search produces effective jailbreaks, but confidence is limited by missing hyperparameter details. The fitness landscape assumption appears valid given reported success rates, but reproduction may require extensive parameter tuning.
- **Low confidence**: RAG-enhanced strategy transfer's effectiveness across semantically distinct prompts lacks direct validation. While the mechanism is described, the paper doesn't demonstrate that retrieved strategies maintain effectiveness when transferred beyond the training distribution.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary temperature ranges (e.g., [0.1, 0.5, 1.0, 2.0]) and population sizes (10, 50, 100) during evolutionary search to determine sensitivity of success rates to these parameters. Measure whether high success rates (97%) are robust or require precise tuning.
2. **Cross-Domain Transfer Validation**: Test strategy transfer from English WarmUp to Arabic test prompts and vice versa, measuring success rate decay. Additionally, test transfer between semantically related but distinct topics (e.g., "Fraud" → "Financial Crimes") to quantify semantic drift impact on strategy effectiveness.
3. **Semantic Intent Classifier Evaluation**: Implement a simple semantic intent classifier that parses Policy Puppetry Templates and evaluates embedded instructions independently of syntax. Measure whether this approach detects a significant portion of obfuscated attacks, directly testing the core assumption that surface-level token patterns are the primary defense mechanism.