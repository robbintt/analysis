---
ver: rpa2
title: 'MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval
  via Multi-Source Alignment'
arxiv_id: '2512.20950'
source_url: https://arxiv.org/abs/2512.20950
tags:
- retrieval
- post
- data
- english
- crosslingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TriAligner, a dual-encoder architecture with
  contrastive learning for multilingual and crosslingual fact-checked claim retrieval.
  The system incorporates native and English translations across modalities, uses
  data augmentation with large language models, and employs hard negative sampling
  to improve representation learning.
---

# MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment

## Quick Facts
- arXiv ID: 2512.20950
- Source URL: https://arxiv.org/abs/2512.20950
- Reference count: 14
- Primary result: TriAligner achieved 21st in monolingual track and 24th in crosslingual track of SemEval-2025 Task 7, demonstrating significant improvements over baseline models in multilingual fact-checked claim retrieval.

## Executive Summary
This paper introduces TriAligner, a dual-encoder architecture with contrastive learning for multilingual and crosslingual fact-checked claim retrieval. The system incorporates native and English translations across modalities, uses data augmentation with large language models, and employs hard negative sampling to improve representation learning. Evaluated on the MultiClaim dataset, TriAligner achieved significant improvements over baseline models in both monolingual and crosslingual settings, ranking 21st in the monolingual track and 24th in the crosslingual track of SemEval-2025 Task 7. The approach demonstrates enhanced retrieval accuracy and fact-checking performance through multi-source alignment and re-ranking with GPT-4o.

## Method Summary
TriAligner uses a dual-encoder architecture built on BGE-M3 embeddings to encode posts and fact-checked claims in four parallel streams (native and English versions for both). Three similarity matrices are computed from these embeddings and combined via learnable weighted coefficients. The model is trained using symmetric contrastive loss with hard negative sampling, and final retrieval is refined through GPT-4o re-ranking of top candidates. Data augmentation with GPT-4o was applied to English posts to expand training data.

## Key Results
- Achieved 21st rank in monolingual track and 24th rank in crosslingual track of SemEval-2025 Task 7
- Multi-source alignment (native + English + concatenated embeddings) significantly outperformed single-source approaches
- GPT-4o re-ranking improved performance by 5-10% in most languages, though degraded for Malay due to code-mixing issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source similarity aggregation (native + English + concatenated embeddings) improves retrieval over single-source approaches.
- Mechanism: Three separate similarity matrices are computed (native-to-native, English-to-English, concatenated-to-concatenated) and combined via learnable weighted coefficients (λ₁, λ₂, λ₃) in Equation 1. This allows the model to assign relative importance to each language source per language pair.
- Core assumption: Different languages benefit from different source weightings; English translations may compensate for weak multilingual encoder alignment in low-resource languages.
- Evidence anchors:
  - [section 3.2]: "We compute cosine similarity between embeddings to construct three similarity matrices... To compute the final similarity matrix, we apply Formula 1, where λ₁, λ₂, λ₃ are trainable coefficients"
  - [section 4.2]: "Both methods significantly enhance performance in crosslingual settings by leveraging both native and translated sources"
  - [corpus]: Weak/absent - neighbor papers describe similar retrieval pipelines but do not directly validate multi-source weighting specifically
- Break condition: If multilingual encoder alignment is already strong for all target languages, the marginal gain from English translation fusion diminishes; if translation quality is poor (e.g., code-mixed Malay), the weighted combination may introduce noise.

### Mechanism 2
- Claim: Contrastive loss with symmetric training encourages better separation of matching vs. non-matching post-claim pairs.
- Mechanism: The symmetric contrastive loss (Equation 3) computes row-wise and column-wise softmax probabilities, then maximizes the diagonal elements (true pairs) while minimizing off-diagonal elements (mismatched pairs). This bidirectional pressure is inspired by CLIP.
- Core assumption: The batch contains sufficient diversity for negative sampling; hard negatives improve the model's ability to distinguish semantically similar but unrelated pairs.
- Evidence anchors:
  - [section 3.2]: "We train the model using a symmetric contrastive loss applied to the final similarity matrix... This loss encourages true post-claim pairs (diagonal elements) to have higher similarity scores"
  - [section 3.1]: "These hard negatives help the model better distinguish between semantically similar but unrelated claims and posts"
  - [corpus]: [arXiv:2508.03475] reports using "bi-encoder model fine-tuned... for sentence similarity" in the same task, supporting contrastive alignment as a common strategy
- Break condition: If batch size is too small, negative sampling becomes insufficient; the paper notes "due to the large batch size, hard negative sampling did not yield significant performance improvements" - suggesting the baseline negatives were already adequate.

### Mechanism 3
- Claim: LLM-based re-ranking provides additional semantic filtering beyond neural retrieval similarity scores.
- Mechanism: GPT-4o receives the top 15 candidates and is prompted to return the top 10 most relevant fact-checks based on semantic relevance, not just embedding proximity. This introduces a reasoning step that can correct for embedding space misalignments.
- Core assumption: The LLM has sufficient multilingual understanding and can outperform embedding-based similarity for final ranking; the initial retrieval provides sufficiently good candidates for the LLM to reorder.
- Evidence anchors:
  - [section 3.2]: "In the final stage of our pipeline, we employ the GPT-4o model as a reranker to refine the ordering of candidate fact-checks"
  - [section 4.3]: "The LLM-based re-ranker also improves performance in both multilingual and crosslingual settings by approximately 5 to 10 percent"
  - [corpus]: [arXiv:2504.16627] describes a similar "two-stage strategy: a reliable baseline retrieval system using a fine-tuned embedding model and an LLM-based reranker"
- Break condition: If the initial retrieval misses the correct fact-check (outside top 15), re-ranking cannot recover it; if the LLM struggles with specific languages (e.g., Malay code-mixing), re-ranking may degrade results (observed in Table 2 for Malay).

## Foundational Learning

- Concept: **Dual-encoder architecture (CLIP-style alignment)**
  - Why needed here: Posts and claims are encoded independently into a shared embedding space; similarity is computed via cosine distance without cross-attention, enabling efficient retrieval at scale.
  - Quick check question: Can you explain why a dual-encoder is preferable to cross-attention for large-scale retrieval?

- Concept: **Contrastive learning with InfoNCE-style loss**
  - Why needed here: The symmetric contrastive loss pushes matching pairs together and non-matching pairs apart within a batch, learning better representations without explicit hard labels for similarity.
  - Quick check question: What happens to contrastive learning if all samples in a batch are semantically similar?

- Concept: **Hard negative sampling**
  - Why needed here: Random negatives may be trivially easy; hard negatives (semantically similar but incorrect) force the model to learn finer-grained distinctions.
  - Quick check question: How would you identify hard negatives from a pretrained embedding space?

## Architecture Onboarding

- Component map:
  Input preprocessing (GPT-4o data augmentation) -> BGE-M3 embeddings (native + English) -> Four parallel encoders (Linear(1024→256)→BatchNorm→ReLU→Dropout(0.2)→Linear(256→256)) -> Concatenation layer (Linear(512→256)→ReLU→Linear(256→256)) -> Three cosine similarity matrices -> Weighted combination (Equation 1) -> Symmetric contrastive loss (Equation 3) -> GPT-4o re-ranking (top 15→top 10)

- Critical path:
  1. BGE-M3 embeddings (1024-dim) for all posts and facts in both native and English
  2. Stage 1 encoders reduce to 256-dim per source
  3. Concatenation produces 512-dim, Stage 2 encoder produces final 256-dim
  4. Three similarity matrices computed and combined with learned λ weights
  5. Contrastive loss trains the full pipeline end-to-end

- Design tradeoffs:
  - Simple encoder architecture: Two-layer MLPs chosen to avoid overfitting given dataset size; not reported as a bottleneck
  - BGE-M3 vs. LaBSE: BGE-M3 chosen as primary backbone based on baseline comparison (Table 1)
  - Fixed 15→10 re-ranking: Re-ranker cannot improve metrics@20 since it only reorders within a fixed pool

- Failure signatures:
  - German language with augmentation: Data augmentation decreased performance across most metrics (Section 4.3) - suggests language-specific augmentation quality varies
  - Malay with re-ranking: Performance dropped with GPT-4o re-ranking, attributed to code-mixing and narrative-style differences
  - Thai crosslingual baseline: Large gap between English (0.857) and native (0.619) in Table 2 crosslingual setting - indicates weak multilingual alignment for low-resource languages

- First 3 experiments:
  1. Baseline validation: Run BGE-M3 and LaBSE on your data split with both native and English sources; confirm BGE-M3 outperforms LaBSE and identify which languages benefit most from English translation
  2. Ablation of multi-source vs. concatenation: Compare ConcatEnc (single concatenated matrix) vs. MultiSim (weighted combination without concatenation) to isolate the contribution of each component
  3. Re-ranking impact by language: Apply GPT-4o re-ranking to top 15 results and measure per-language improvement; flag languages where re-ranking degrades performance for further investigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does extending GPT-based data augmentation beyond English posts to native-language posts and fact-checked claims yield consistent performance gains across all languages?
- Basis in paper: [explicit] The authors state that "due to resource constraints, we were only able to apply data augmentation to English posts. Future work can extend this augmentation process to other sources (facts) and other languages."
- Why unresolved: The augmentation showed language-dependent effects, benefiting Arabic, Malay, and French but harming German performance; whether native-language augmentation helps or harms low-resource languages remains unknown.
- What evidence would resolve it: A controlled experiment applying the same LLM augmentation pipeline to native-language posts and facts across all 27 languages, comparing against the English-only augmentation baseline.

### Open Question 2
- Question: What causes the Malay language to exhibit performance degradation with LLM-based re-ranking when other languages benefit?
- Basis in paper: [inferred] The authors note that "the Malay language exhibited a notable decrease in performance with re-ranking" and hypothesize it may stem from "frequent code-mixing between Malay and English in social media posts, creating translation inconsistencies during cross-lingual re-ranking, or from the re-ranker's struggle with Malay's narrative-style debunking patterns."
- Why unresolved: Two competing hypotheses are offered without empirical validation; the mechanism underlying this anomaly is unidentified.
- What evidence would resolve it: Ablation experiments with Malay posts categorized by code-mixing frequency and debunking style, plus analysis of LLM attention patterns on Malay versus other languages.

### Open Question 3
- Question: Can hard negative sampling strategies be redesigned to provide significant benefits under large-batch training regimes?
- Basis in paper: [explicit] The authors report that "due to the large batch size, hard negative sampling did not yield significant performance improvements" despite its theoretical value for improving representation learning.
- Why unresolved: The interaction between batch size and hard negative effectiveness is not well understood in dense retrieval settings, and optimal sampling strategies for multilingual claim-post pairs remain unexplored.
- What evidence would resolve it: Systematic experiments varying hard negative ratios, sampling difficulty thresholds, and batch sizes, with analysis of gradient distributions across language pairs.

## Limitations

- Hyperparameter sensitivity: Initial values for trainable coefficients (λ₁, λ₂, λ₃) and scaling factors (S₁, S₂, S₃) are not specified, affecting reproducibility
- Resource constraints: Data augmentation limited to English posts creates potential training data imbalance across languages
- Marginal benefit: Hard negative sampling showed no significant improvement due to large batch size, suggesting baseline negatives were already adequate

## Confidence

- High confidence: Core retrieval architecture (dual-encoder with BGE-M3 backbone) and multi-source alignment effectiveness are well-supported by ablation studies
- Medium confidence: Symmetric contrastive loss mechanism is theoretically sound but hard negative sampling marginal benefit is questionable
- Medium confidence: LLM-based re-ranking shows consistent improvements (5-10%) across most languages, but degrades for Malay due to code-mixing issues

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the initial values of λ₁, λ₂, λ₃ and S₁, S₂, S₃ to determine their impact on retrieval performance across different languages
2. **Crosslingual alignment benchmarking**: For low-resource languages like Thai where native performance lags English significantly, test whether fine-tuning the multilingual encoder with targeted language pairs improves alignment
3. **Language-specific re-ranking evaluation**: For languages where GPT-4o re-ranking degrades performance (Malay), test alternative re-rankers or implement language-specific prompts to handle code-mixing and narrative-style differences