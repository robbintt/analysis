---
ver: rpa2
title: 'FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs'
arxiv_id: '2506.20810'
source_url: https://arxiv.org/abs/2506.20810
tags:
- lstm
- finn
- quantised
- hardware
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying recurrent neural
  networks, particularly LSTMs, on FPGAs for time-series applications like stock price
  prediction. Existing tools are limited to feed-forward networks, making LSTM acceleration
  difficult.
---

# FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs
## Quick Facts
- arXiv ID: 2506.20810
- Source URL: https://arxiv.org/abs/2506.20810
- Reference count: 27
- Key outcome: Extends FINN framework with ONNX Scan operator to enable LSTM acceleration on FPGAs with mixed-precision quantisation, achieving 4.3 ms inference latency on Zynq Ultrascale+ for ConvLSTM stock prediction

## Executive Summary
This paper addresses the challenge of deploying recurrent neural networks, particularly LSTMs, on FPGAs for time-series applications like stock price prediction. Existing tools are limited to feed-forward networks, making LSTM acceleration difficult. The authors extend the FINN framework by leveraging the ONNX Scan operator to model recurrent LSTM computations and enable mixed-precision quantisation. They develop new compiler transformations to map quantised LSTM graphs to hardware blocks and create reusable HLS layers for efficient hardware generation. The approach is validated on a Zynq Ultrascale+ FPGA with a ConvLSTM model for mid-price stock prediction using the FI-2010 dataset. The generated quantised accelerator achieves inference latency of 4.3 ms, consumes modest FPGA resources (49% LUTs, 13% FFs, 15% DSPs), and matches or outperforms state-of-the-art models with reduced precision. The method provides a scalable, resource-efficient path for RNN deployment on FPGAs.

## Method Summary
The authors extend the FINN framework by introducing support for the ONNX Scan operator, which enables the representation of recurrent LSTM computations. They develop compiler transformations that map quantised LSTM graphs to hardware blocks, allowing for mixed-precision quantisation. Reusable HLS layers are created for efficient hardware generation. The approach is validated using a ConvLSTM model for mid-price stock prediction on the FI-2010 dataset, implemented on a Zynq Ultrascale+ FPGA. The method achieves inference latency of 4.3 ms while consuming 49% LUTs, 13% FFs, and 15% DSPs, demonstrating resource efficiency and performance comparable to state-of-the-art models.

## Key Results
- Achieves 4.3 ms inference latency on Zynq Ultrascale+ FPGA for ConvLSTM stock prediction
- Consumes modest FPGA resources: 49% LUTs, 13% FFs, 15% DSPs
- Matches or outperforms state-of-the-art models with reduced precision quantisation
- Enables LSTM acceleration on FPGAs through ONNX Scan operator integration

## Why This Works (Mechanism)
The approach works by extending FINN with ONNX Scan operator support, which models recurrent LSTM computations. This enables mixed-precision quantisation and efficient hardware generation through compiler transformations and reusable HLS layers. The Scan operator allows expressing time-unrolled LSTM computations in a static graph, making them compatible with FINN's quantised hardware generation pipeline. The compiler transforms the quantised graph into hardware blocks that exploit the parallelism available in FPGAs while maintaining accuracy through adaptive precision allocation.

## Foundational Learning
- **ONNX Scan Operator**: Enables representation of recurrent computations in static computational graphs
  - Why needed: LSTMs require time-unrolled computation that traditional static graphs cannot express
  - Quick check: Can the operator correctly model multiple LSTM time steps in a single graph representation

- **Mixed-precision Quantisation**: Allocates different bit-widths to different parts of the neural network
  - Why needed: Balances accuracy requirements with hardware resource constraints
  - Quick check: Does precision reduction maintain accuracy while reducing resource utilization

- **HLS Layer Reuse**: Creates modular hardware components that can be instantiated multiple times
  - Why needed: Reduces development time and ensures consistent hardware generation
  - Quick check: Are generated modules functionally correct and resource-efficient when instantiated

- **Compiler Transformations**: Converts high-level ONNX graphs to hardware description language
  - Why needed: Bridges the gap between neural network models and FPGA implementations
  - Quick check: Does the transformation preserve computational semantics while optimizing for hardware

## Architecture Onboarding
- **Component Map**: ONNX Graph -> Compiler Transformations -> HLS Layers -> FPGA Hardware
- **Critical Path**: Graph representation through ONNX Scan → Compiler optimisation → HLS code generation → FPGA bitstream
- **Design Tradeoffs**: Mixed-precision offers accuracy-resource balance but adds complexity to compiler logic; ONNX Scan enables recurrence but may limit compatibility with non-Scan models
- **Failure Signatures**: Incorrect recurrence handling shows as temporal pattern errors; precision misconfiguration appears as accuracy degradation; HLS generation issues manifest as timing violations
- **First Experiments**:
  1. Validate ONNX Scan operator correctly models single LSTM time step computation
  2. Test mixed-precision quantisation on small LSTM with known accuracy thresholds
  3. Verify compiler transformation produces synthesizable HLS code for basic LSTM cell

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on ONNX Scan operator, potentially limiting compatibility with models not using this representation
- Focuses on a single ConvLSTM use case without broader RNN type coverage
- Resource utilization figures are reported for one FPGA family, limiting generalizability across hardware platforms

## Confidence
- High: The core technical contribution of extending FINN with ONNX Scan support for LSTM acceleration is well-supported by the described implementation and experimental results
- Medium: The resource efficiency claims are based on a single hardware platform and one application domain, which may not generalize
- Medium: The latency and accuracy comparisons are valid for the specific ConvLSTM stock prediction model but require additional validation for broader RNN types

## Next Checks
1. Test the framework on diverse LSTM architectures (stacked LSTMs, bidirectional LSTMs) beyond ConvLSTM to assess generality
2. Evaluate resource utilization and performance on alternative FPGA platforms (e.g., Intel Agilex, Xilinx Versal) to verify cross-platform portability
3. Conduct ablation studies on mixed-precision settings to quantify the trade-off between precision reduction and accuracy retention across different datasets and RNN applications