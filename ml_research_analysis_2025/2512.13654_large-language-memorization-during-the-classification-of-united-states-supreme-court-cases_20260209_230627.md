---
ver: rpa2
title: Large-Language Memorization During the Classification of United States Supreme
  Court Cases
arxiv_id: '2512.13654'
source_url: https://arxiv.org/abs/2512.13654
tags:
- legal
- classification
- cases
- court
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates four large language model (LLM) approaches
  for classifying U.S. Supreme Court cases into legal categories.
---

# Large-Language Memorization During the Classification of United States Supreme Court Cases

## Quick Facts
- arXiv ID: 2512.13654
- Source URL: https://arxiv.org/abs/2512.13654
- Authors: John E. Ortega; Dhruv D. Joshi; Matt P. Borkowski
- Reference count: 23
- Key outcome: DeepSeek achieves ~82% accuracy on 15-category and ~62% on 279-category SCOTUS case classification, outperforming BERT-based models by ~2 points

## Executive Summary
This paper evaluates four large language model approaches for classifying U.S. Supreme Court cases into legal categories, comparing BERT-based models with newer prompt-based LLMs. The authors find that DeepSeek, a prompt-based model with a 5,000+ token context window, achieves the highest accuracy—approximately 82% on broad categories and 62% on fine-grained categories. The study highlights the effectiveness of prompt-based inference and larger context windows for complex legal classification tasks, particularly when combined with log-smoothed weighted loss for handling class imbalance.

## Method Summary
The authors evaluate four model architectures on the Supreme Court Database (SCDB) for classifying cases into 15 broad or 279 fine-grained legal categories. They compare BERT-based approaches (with and without legal pretraining) using parameter-efficient fine-tuning, a decoder-based LLM (LLaMA 3) with various prompting strategies, and DeepSeek with advanced prompt-based classification. The evaluation employs chunking strategies to handle document length constraints, automated model selection, and retrieval-augmented classification techniques.

## Key Results
- DeepSeek achieves highest accuracy: ~82% on 15 categories, ~62% on 279 categories
- Legal-BERT outperforms standard BERT, especially on fine-grained tasks
- Prompt-based models with larger context windows (5,000+ tokens) outperform BERT's 512-token limit
- Log-smoothed weighted loss improves handling of class imbalance in high-cardinality classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based LLMs with larger context windows achieve higher classification accuracy than BERT-based models on long legal documents.
- Mechanism: DeepSeek processes up to 5,000 tokens per document versus BERT's 512-token limit, enabling semantic understanding across full opinions rather than truncated segments. The model leverages in-context learning through prompt design rather than gradient-based fine-tuning.
- Core assumption: The performance gain stems from context window expansion and prompt-based inference rather than parameter count alone.
- Evidence anchors:
  - [abstract] "prompt-based models with memories, such as DeepSeek... scoring about 2 points better than previous models not based on prompting"
  - [section 5] "Model 4, based on DeepSeek R1, is able to take advantage of context as much as 10 times larger than Models 1 and 2"
  - [corpus] Neighbor paper on LLM memorization patterns confirms that larger context windows correlate with reduced hallucination on classification tasks (FMR=0.56, limited direct citation linkage)
- Break condition: If documents average <512 tokens or GPU memory constrains context window usage, the advantage diminishes.

### Mechanism 2
- Claim: Domain-specific pre-training improves legal classification robustness, especially for fine-grained label spaces.
- Mechanism: Legal-BERT's pre-training on legal corpora provides better initialization for legal terminology and document structure, improving generalization when fine-tuned on SCDB.
- Core assumption: The pre-training corpus distribution aligns sufficiently with SCOTUS opinion characteristics.
- Evidence anchors:
  - [section 5] "LegalBERT consistently outperforms the standard BERT model across both coarse-grained (15 categories) and fine-grained (279 categories) legal classification tasks"
  - [section 5] "BERT's performance dropped significantly... LegalBERT, however, showed greater robustness"
  - [corpus] No direct corpus validation for this specific mechanism; assumption relies on paper's internal evidence
- Break condition: If target domain diverges significantly from pre-training corpus (e.g., non-US legal systems), gains may not transfer.

### Mechanism 3
- Claim: Log-smoothed weighted loss with classification head adaptation mitigates class imbalance in high-cardinality label spaces.
- Mechanism: PB3 assigns logarithmically smoothed weights inversely proportional to class frequency, with clipping for extreme outliers. This prevents majority class domination during training.
- Core assumption: Class frequency correlates with model difficulty; weighting improves minority class recall without degrading overall accuracy.
- Evidence anchors:
  - [section 3.2] "we apply logarithmic smoothing of those class weights during fine-tuning. Additionally, to address extreme outlier weights, we introduced a clipping mechanism"
  - [section 5] Model 4 with PB3 achieved 62% accuracy on 279 categories vs Legal-BERT's 62.4%—comparable despite larger label space complexity
  - [corpus] No external validation of this specific loss formulation in neighbor papers
- Break condition: If class distribution is relatively balanced, or if clipping threshold removes informative gradient signal, benefit is reduced.

## Foundational Learning

- Concept: **Transformer context windows and token limits**
  - Why needed here: BERT's 512-token limit requires chunking strategies (Stride-64, Concat-512) that may lose document-level context. Understanding this constraint explains why DeepSeek's 5,000+ token window improves performance.
  - Quick check question: Given a 6,960-token median document length, how many non-overlapping 512-token chunks are needed, and what semantic continuity is lost between chunks?

- Concept: **Parameter-efficient fine-tuning (LoRA) vs. prompt-based inference**
  - Why needed here: The paper compares LoRA adaptation (NPB1) against prompt-based approaches (PB1-PB3). Understanding the trade-off between gradient updates and in-context learning is essential for architecture selection.
  - Quick check question: Does LoRA modify the classification head? If not, what bottleneck does this create for 279-class tasks?

- Concept: **Class imbalance handling in multi-class classification**
  - Why needed here: SCDB has skewed class distributions. Log-smoothed weighted loss addresses this, but the mechanism requires understanding how softmax probability mass distributes across 279 labels.
  - Quick check question: With 279 classes and imbalanced data, what happens to softmax confidence when the majority class dominates training?

## Architecture Onboarding

- Component map: Input Document → Chunking Strategy → Model Backbone → Classification Approach → Output
- Critical path:
  1. Select model based on available GPU memory (BERT/Legal-BERT: 8GB sufficient; DeepSeek: requires larger allocation or API access)
  2. Choose chunking strategy aligned with model context window
  3. For BERT-based: Apply NPB2 (AutoModel) with Stride-64 for best results
  4. For DeepSeek: Use PB3 with log-smoothed weighted loss and 5,000-token context
  5. Evaluate on validation set before full SCDB deployment

- Design tradeoffs:
  - **BERT/Legal-BERT + NPB2**: Lower memory, faster inference, well-understood behavior; limited to 512 tokens, requires chunking
  - **DeepSeek + PB3**: Higher accuracy, longer context; requires more compute, prompt engineering overhead
  - **LLaMA 3 + PB1/PB2**: Moderate performance; memory issues reported (Ollama framework failures in paper)

- Failure signatures:
  - Accuracy drops sharply from 15 → 279 categories (BERT: 0.785 → 0.581): indicates classification head bottleneck
  - LoRA underperforms AutoModel on high-cardinality tasks: unadapted classification head lacks sufficient supervision
  - PB2 (RAG) underperforms PB1 (0.33 vs 0.55): retrieval introduces noise for structured prediction

- First 3 experiments:
  1. **Baseline replication**: Run Legal-BERT with NPB2 (AutoModel) on 15-category SCDB using Stride-64 chunking; target ~0.79 accuracy
  2. **Context window ablation**: Test DeepSeek PB3 with 512, 2,000, and 5,000 token windows to quantify context contribution
  3. **Class imbalance analysis**: Compare PB3 with and without log-smoothed weighted loss on 279-category task; measure per-class F1 for minority categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating graph structures that encode citation relationships between SCOTUS cases improve classification accuracy over text-only approaches?
- Basis in paper: [explicit] "We can use a graph structure to denote the relations between a given SCDB document with other documents cited in it. The final classification result... can be calculated based on some form of aggregation incorporating classification results of its references weighted by the graph structure."
- Why unresolved: The authors propose this as future work but did not implement or test it.
- What evidence would resolve it: Experiments comparing baseline text-only classification against graph-augmented classification using citation networks on the same SCDB dataset.

### Open Question 2
- Question: Would greedy chunk weighting strategies (prioritizing higher-performing document segments) improve BERT-based classification over uniform chunk processing?
- Basis in paper: [explicit] "Another area of future work can be applying greedy approaches... we can have a greedy summarization technique where in the final 512 token summary, we can include more number of tokens from the best performing 512 token chunk."
- Why unresolved: Proposed as future work; not implemented or evaluated in the study.
- What evidence would resolve it: Comparative experiments between standard chunking (Concat-512, Stride-64) and greedy-weighted variants on the 15- and 279-category tasks.

### Open Question 3
- Question: What causes the performance discrepancy between retrieval-augmented classification (PB2) and direct prompt-based classification (PB1), and can RAG be optimized for structured legal classification?
- Basis in paper: [inferred] The paper reports PB2 achieved only 0.33 accuracy versus 0.55 for PB1 and attributes this to "noise" from retrieval, but does not systematically investigate retrieval quality or alternative RAG configurations.
- Why unresolved: The analysis remains speculative; no ablation on retrieval components or document alignment was conducted.
- What evidence would resolve it: Ablation studies varying retrieval corpus, chunking strategy, and retrieval-to-generation coupling on the SCDB classification task.

### Open Question 4
- Question: Can larger decoder-based LLMs (e.g., LLaMA 3) match or exceed DeepSeek's performance on high-cardinality legal classification when computational constraints are removed?
- Basis in paper: [inferred] Model 3 (LLaMA 3) was "rendered unusable" for the 279-category task due to memory issues with the Ollama framework, leaving its potential performance unknown.
- Why unresolved: Hardware and framework limitations prevented evaluation, not model capability.
- What evidence would resolve it: Running LLaMA 3 experiments with sufficient GPU memory or optimized inference frameworks, comparing against DeepSeek on identical SCDB splits.

## Limitations

- Performance comparisons rely heavily on internal benchmarking against a single dataset (SCDB), limiting generalizability to other legal corpora or classification tasks.
- The reported GPU memory constraints for DeepSeek and LLaMA 3 models create practical deployment barriers not fully addressed in the analysis.
- The retrieval-augmented generation approach (PB2) underperforms expectations, suggesting that RAG architectures may introduce noise for structured prediction tasks despite theoretical advantages.

## Confidence

**High Confidence**: The finding that Legal-BERT outperforms standard BERT on both 15-category (0.79 vs 0.785) and 279-category tasks (0.624 vs 0.581) classification, and that DeepSeek achieves the highest overall accuracy (~0.82 on 15 categories, ~0.62 on 279 categories).

**Medium Confidence**: The claim that context window expansion from 512 to 5,000+ tokens drives the performance gain, as this relies on architectural assumptions rather than controlled ablation studies.

**Low Confidence**: The assertion that LoRA-based NPB1 underperforms due to an "unadapted classification head," as the paper does not directly test classification head modifications.

## Next Checks

1. **Context Window Ablation Study**: Systematically test DeepSeek PB3 performance across multiple context window sizes (512, 2,000, 5,000 tokens) on identical document subsets to quantify the exact contribution of context length versus model architecture to accuracy improvements.

2. **Classification Head Modification Experiment**: Test whether LoRA-based NPB1 performance improves when the classification head is explicitly fine-tuned for 279-class prediction, isolating whether the bottleneck is truly the frozen classification layer or other factors.

3. **Cross-Dataset Generalization Test**: Evaluate the best-performing models (DeepSeek PB3 and Legal-BERT NPB2) on at least one other legal classification dataset with different class distributions to assess whether the log-smoothed weighted loss and Legal-BERT pretraining provide domain transfer benefits beyond SCDB.