---
ver: rpa2
title: Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization
arxiv_id: '2505.17852'
source_url: https://arxiv.org/abs/2505.17852
tags:
- bptt
- cd-rge
- step
- memory
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that Zero-Order Optimization (ZOO) methods,\
  \ specifically Central-Difference Random-Vector Gradient Estimation (CD-RGE), can\
  \ effectively replace Backpropagation Through Time (BPTT) for training large Recurrent\
  \ Neural Networks (RNNs). By leveraging distributed computing and recent optimizations\
  \ like FlashRNN, the authors show that CD-RGE can train RNNs with up to 1 billion\
  \ parameters while using significantly less memory than BPTT\u2014reducing VRAM\
  \ requirements from linear scaling with context length to a constant."
---

# Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization

## Quick Facts
- arXiv ID: 2505.17852
- Source URL: https://arxiv.org/abs/2505.17852
- Authors: Francois Chaubard; Mykel Kochenderfer
- Reference count: 14
- This paper demonstrates that Zero-Order Optimization (ZOO) methods, specifically Central-Difference Random-Vector Gradient Estimation (CD-RGE), can effectively replace Backpropagation Through Time (BPTT) for training large Recurrent Neural Networks (RNNs).

## Executive Summary
This paper presents a method to train extremely large RNNs using Zero-Order Optimization (ZOO) instead of traditional Backpropagation Through Time (BPTT). By using Central-Difference Random-Vector Gradient Estimation (CD-RGE), the authors eliminate the need to store intermediate activations during training, reducing memory usage from linear scaling with sequence length to constant scaling. The approach enables training RNNs with up to 1 billion parameters while using significantly less memory than BPTT, achieving up to 19× faster convergence rates in overfitting experiments and matching or exceeding BPTT performance across transduction and language modeling tasks.

## Method Summary
The paper proposes using Central-Difference Random-Vector Gradient Estimation (CD-RGE) to train RNNs without BPTT. CD-RGE estimates gradients by evaluating the loss at perturbed weights (Θ ± εp) using forward passes only, avoiding the need to store intermediate activations. The method uses Rademacher probes (±1 entries) to generate perturbations and ties the learning rate η to the perturbation size ε. The authors implement this in a distributed setting using parameter servers and workers, leveraging FlashRNN kernels for LSTM efficiency. They demonstrate training RNNs from 100K to 1.1 billion parameters on tasks including synthetic transduction, COPY/REVERSE/ROLLING SUM, and Penn Treebank language modeling.

## Key Results
- CD-RGE reduces VRAM requirements from O(BCA) to O(B·a_max + |Θ|), enabling training of billion-parameter RNNs
- Achieves up to 19× faster convergence rates than BPTT in overfitting experiments
- Matches or exceeds BPTT performance across transduction and language modeling tasks
- Incurs a 2-3× increase in computation per step but achieves comparable wall-clock times through distributed implementation

## Why This Works (Mechanism)

### Mechanism 1: Memory Decoupling via Forward-Only Training
Replacing BPTT with CD-RGE removes the necessity to store intermediate activations, decoupling memory usage from sequence length. BPTT requires retaining the computational graph for every time step to compute gradients backwards, resulting in O(BCA) memory complexity. CD-RGE estimates gradients using only forward passes at perturbed weights, keeping the model in "inference mode" during training and requiring only O(B·a_max + |Θ|) memory.

### Mechanism 2: Implicit Regularization through Loss Smoothing
CD-RGE implicitly optimizes a smoothed surrogate of the loss function, acting as a regularizer to improve generalization. The finite-difference estimation over a distribution of perturbations is equivalent to convolving the original loss with the probe distribution, resulting in optimizing L_ε(Θ) = E_p[L(Θ + εp)]. This convolution smooths out sharp minima or spikes in the loss landscape, helping generalization and preventing overfitting to noise.

### Mechanism 3: Stable High-Dimensional Exploration
Constraining the step size η to equal the perturbation size ε stabilizes training in high-dimensional non-convex spaces by forcing exploration along the shell of a hypersphere. When using Rademacher probes, the perturbations effectively sample the shell of a sphere of radius ε√d. By stepping exactly the distance measured (η=ε), the update moves parameters to a previously evaluated lower-loss point, behaving like an ensemble search of successful perturbation directions.

## Foundational Learning

- **Concept: Finite-Difference Gradient Estimation**
  - Why needed here: This is the mathematical engine replacing backpropagation. Understanding the difference between Forward-Difference (biased, O(ε)) and Central-Difference (less biased, O(ε²)) explains why the paper relies on CD-RGE for precision.
  - Quick check question: Why does Central-Difference require double the forward passes of Forward-Difference, and why is it preferred here?

- **Concept: The Bias-Variance Trade-off in Zero-Order Optimization**
  - Why needed here: The method's success depends on balancing the perturbation count (n_pert) and size (ε). Low n_pert means high variance (noisy estimate); high ε means high bias (smoothing).
  - Quick check question: If your training loss is unstable, would you increase ε (to smooth/bias) or n_pert (to reduce variance)?

- **Concept: Rademacher Distribution**
  - Why needed here: The paper selects Rademacher vectors (±1 entries) over Gaussian or Uniform. This optimizes memory (bit storage) and ensures a constant norm for the probe vectors, standardizing the "step" scale.
  - Quick check question: Why is the constant norm of Rademacher vectors beneficial compared to the chi-squared distributed norms of Gaussian vectors?

## Architecture Onboarding

- **Component map:**
  Parameter Server (Rank 0) -> Workers (Ranks 1 to w) -> Parameter Server (Rank 0) -> Optimizer

- **Critical path:**
  1. Broadcast: Rank 0 sends model Θ and seeds S to all workers
  2. Probe & Forward: Workers reconstruct local probes p_i using seeds, apply ±ε, run inference, and output loss scalars
  3. Gather: Rank 0 collects losses
  4. Update: Rank 0 reconstructs probes locally (chunk by chunk to save memory), computes weighted update, and applies it to Θ

- **Design tradeoffs:**
  - Memory vs. Compute: Trade O(C) memory (BPTT) for O(n) compute (ZOO)
  - Precision vs. Speed: More perturbations (n_pert) → better gradient estimate but slower step
  - Communication vs. Storage: Probes are re-generated from seeds on the server to avoid transferring massive gradient vectors, trading small compute for bandwidth

- **Failure signatures:**
  - Divergence: ε is too large, causing the model to step out of the local basin
  - Stagnation: ε is too small or n_pert is too low; noise dominates the gradient estimate
  - OOM (Out of Memory): Batch size B or context length C is too large even for inference mode

- **First 3 experiments:**
  1. Overfitting Sanity Check: Train a small LSTM on a single batch of random data (Copy task, length 10). Confirm loss drops to near zero.
  2. Memory Scaling Profile: Train models of increasing size (1M, 10M, 100M) on a fixed context length. Plot VRAM usage against BPTT baselines.
  3. Hyperparameter Sweep (ε vs η): Run the Copy task with varying ε (e.g., 1e-5 to 0.1) while maintaining η = ε. Identify the optimal range.

## Open Questions the Paper Calls Out

- **Question:** Does CD-RGE enable RNNs to follow favorable power-law scaling curves on massive datasets (trillions of tokens) comparable to Transformers?
  - Basis in paper: [explicit] The authors state on page 9 that they "leave measuring of a more general scaling law for RNNs to future research" because Penn Treebank is too small to discern the generalization ability of larger RNNs.
  - Why unresolved: The experiments validate the method on small datasets (PTB) and synthetic tasks, but the performance trajectory at modern LLM data scales remains unverified.
  - What evidence would resolve it: Training curves and validation loss plots for billion-parameter RNNs trained on datasets like The Pile or Common Crawl.

- **Question:** How does CD-RGE compare to distributed BPTT specifically when utilizing macrobatching strategies to overcome memory limits?
  - Basis in paper: [explicit] Page 6 notes that while CD-RGE requires splitting batches for 1.1B models, the authors "save the analysis of distributed BPTT and splitting of macrobatches with CD-RGE to future research."
  - Why unresolved: The paper compares single-GPU CD-RGE against single-GPU BPTT, but did not benchmark against a BPTT implementation that uses macrobatching to offload memory.
  - What evidence would resolve it: A comparison of wall-clock time and convergence rates between CD-RGE and macrobatched BPTT on identical hardware clusters.

- **Question:** Can an adaptive strategy for the perturbation size (ε) and learning rate (η) be developed to navigate the trade-off between gradient variance and smoothing bias?
  - Basis in paper: [inferred] Page 4 discusses the sensitivity of the "critical" relationship between ε and η, noting that fixing them requires finding a "right region" to avoid instability or divergence.
  - Why unresolved: The method relies on a fixed ratio (η = ε) found via sweep; however, the optimal amount of loss-surface smoothing likely changes as the model moves from initialization to convergence.
  - What evidence would resolve it: Experiments showing that a dynamic schedule for ε improves final accuracy or convergence speed compared to the static approach.

## Limitations
- Gradient estimation fidelity depends heavily on perturbation size ε and count n_pert, with poor hyperparameter choices leading to noisy or biased updates
- Communication overhead in distributed setups may limit practical scaling beyond current benchmarks
- Results are strong on synthetic tasks and PTB, but performance on more complex, real-world sequential data remains untested

## Confidence
- **High Confidence:** Memory savings claims and the mechanism by which CD-RGE replaces BPTT's storage requirements are well-supported by theoretical analysis and experimental VRAM measurements
- **Medium Confidence:** Generalization improvements via implicit regularization are plausible given the smoothing effect, but the precise conditions under which this benefit manifests are not fully characterized
- **Low Confidence:** The claim of "19× faster convergence" is based on a single overfitting experiment; real-world convergence benefits may vary significantly with task and data distribution

## Next Checks
1. **Convergence Robustness Test:** Train the same LSTM model on PTB using multiple random seeds and varying ε/n_pert combinations. Quantify variance in convergence speed and final validation loss.
2. **Scaling Stress Test:** Benchmark VRAM and wall-clock time scaling laws for models from 10M to 1B parameters on sequences of length 1024+. Compare against BPTT baselines to confirm claimed efficiency gains hold at scale.
3. **Real-World Task Transfer:** Apply CD-RGE to a non-synthetic task (e.g., character-level language modeling on WikiText-103 or a speech recognition dataset). Evaluate whether the memory benefits translate to practical speedups and whether generalization holds.