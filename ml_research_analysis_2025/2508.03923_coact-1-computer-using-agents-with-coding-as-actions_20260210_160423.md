---
ver: rpa2
title: 'CoAct-1: Computer-using Agents with Coding as Actions'
arxiv_id: '2508.03923'
source_url: https://arxiv.org/abs/2508.03923
tags:
- coact-1
- agent
- agents
- coding
- openai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoAct-1 introduces a hybrid approach to computer automation by
  combining GUI-based control with direct programmatic execution. The system uses
  an Orchestrator to dynamically delegate tasks to either a GUI Operator for visual
  interactions or a Programmer for coding actions.
---

# CoAct-1: Computer-using Agents with Coding as Actions

## Quick Facts
- arXiv ID: 2508.03923
- Source URL: https://arxiv.org/abs/2508.03923
- Reference count: 7
- State-of-the-art success rate of 60.76% on OSWorld benchmark

## Executive Summary
CoAct-1 introduces a hybrid approach to computer automation by combining GUI-based control with direct programmatic execution. The system uses an Orchestrator to dynamically delegate tasks to either a GUI Operator for visual interactions or a Programmer for coding actions. This allows the agent to bypass inefficient GUI sequences for tasks like file management and data processing while still leveraging visual interaction when necessary. On the OSWorld benchmark, CoAct-1 achieves a state-of-the-art success rate of 60.76%, outperforming prior methods. It also improves efficiency, reducing the average number of steps per task to 10.15 compared to 15 for leading GUI agents. The results demonstrate that integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation.

## Method Summary
CoAct-1 is a multi-agent system built on the AG2 framework that dynamically routes subtasks between specialized agents. The Orchestrator (OpenAI o3 or o4-mini) decomposes user goals into subtasks and delegates them to either a Programmer (OpenAI o4-mini) for code execution or a GUI Operator (OpenAI CUA 4o) for visual interactions. The Programmer writes and iterates on Python/Bash scripts via a remote code interpreter, while the GUI Operator uses a VLM to perform mouse and keyboard actions through a GUI Action Interpreter. Each agent maintains isolated conversation histories to prevent cross-modal interference. The system operates within the OSWorld Linux environment with a maximum of 15 orchestration rounds, 20 programmer rounds, and 25 GUI operator steps.

## Key Results
- Achieves state-of-the-art success rate of 60.76% on OSWorld benchmark
- Reduces average steps per task from 15 to 10.15 compared to leading GUI agents
- Demonstrates effectiveness of hybrid GUI-programmatic approach for computer automation

## Why This Works (Mechanism)

### Mechanism 1: Step Compression Through Programmatic Actions
Replacing multi-step GUI sequences with single-shot code execution reduces cumulative error probability and improves task success. A Python or Bash script can perform file operations (e.g., batch image resizing, recursive directory search) that would require dozens of precise clicks and drags through GUI. Fewer atomic actions mean fewer opportunities for visual grounding errors or mis-clicks.

### Mechanism 2: Modal Routing Based on Subtask Affordance
Dynamic delegation to specialized agents based on subtask characteristics yields higher success than forcing all actions through a single modality. The Orchestrator classifies subtasks by required interaction type. Backend operations (file I/O, data processing, environment config) route to Programmer. Frontend operations (button clicks, menu navigation in applications without CLI) route to GUI Operator.

### Mechanism 3: Context Isolation Prevents Cross-Modal Interference
Maintaining separate conversation histories for each agent prevents irrelevant context from degrading specialist performance. The Programmer, Orchestrator, and GUI Operator each maintain isolated conversation buffers. Upon subtask completion, only a summary (not full history) returns to the Orchestrator.

## Foundational Learning

- **Concept: Error Accumulation in Long-Horizon Tasks**
  - Why needed here: CoAct-1's efficiency gains derive from reducing step count. Understanding why long action sequences fail helps justify the hybrid approach.
  - Quick check question: If each GUI action has a 3% error rate, what is the probability of successfully completing a 20-step task without any errors? (Answer: 0.97^20 ≈ 54%)

- **Concept: Programmatic vs. GUI Action Spaces**
  - Why needed here: The core innovation is treating coding as an "action" alongside clicks and keypresses. You must understand what operations are more natural in each space.
  - Quick check question: For "resize all images in a nested folder structure to 512x512," would you route to Programmer or GUI Operator? What factors inform this decision?

- **Concept: Multi-Agent Orchestration Patterns**
  - Why needed here: CoAct-1 uses a centralized Orchestrator with specialized workers. Understanding delegation, summary passing, and termination signals is essential for debugging.
  - Quick check question: What information must the Programmer return to the Orchestrator after completing a subtask? What information is intentionally excluded?

## Architecture Onboarding

- **Component map:**
  - User task + initial screenshot → Orchestrator
  - Orchestrator → Programmer OR GUI Operator
  - Programmer: iterates with Code Interpreter OR GUI Operator: iterates with GUI Action Interpreter
  - Execution agent → Orchestrator (summary + screenshot)
  - Orchestrator → next subtask or termination

- **Critical path:**
  1. User task + initial screenshot → Orchestrator
  2. Orchestrator decomposes into subtask, assigns to Programmer OR GUI Operator
  3. Programmer: iterates on code with Code Interpreter (max 20 rounds) OR GUI Operator: iterates on actions with GUI Action Interpreter (max 25 steps)
  4. Execution agent returns summary/message + screenshot to Orchestrator
  5. Orchestrator updates memory, decides next subtask or termination (max 15 rounds)
  6. Termination signal ends conversation

- **Design tradeoffs:**
  - Programmer is text-only LLM: Cannot process screenshots. Requires Orchestrator to explicitly provide environment context (open windows, file paths). Breaks when visual inspection is needed during code execution.
  - Separate histories reduce interference but lose cross-modal context: If the GUI Operator learns something relevant to the Programmer, it must be explicitly captured in the summary.
  - Max rounds (Programmer: 20, GUI: 25, Orchestrator: 15) bound total steps at 375: Intended to prevent infinite loops but may truncate complex tasks.

- **Failure signatures:**
  - High-level queries: Instructions requiring conceptual leaps beyond explicit keywords (e.g., "focus cursor on debug console" → must infer relation to "breakpoints"). Programmer searches literal keywords and fails.
  - Ambiguous queries: Scope ambiguity (workspace vs. global settings) leads to correct action in wrong context.
  - Misrouting: Orchestrator assigns GUI task to Programmer when no CLI exists, or vice versa.

- **First 3 experiments:**
  1. Ablate backbone models: Run CoAct-1 with o3 vs. o4-mini for Orchestrator and Programmer independently. Compare success rates to quantify which component benefits most from model capability.
  2. Step distribution analysis by domain: For each task category (Calc, VS Code, Multi-apps), histogram the GUI vs. coding action ratio. Verify that Programmer-heavy domains show larger improvements over baselines.
  3. Failure case clustering: Manually categorize failures by type (routing error, grounding error, ambiguity, capability limit). Identify whether errors concentrate in specific agents or task types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the agent improve semantic mapping from high-level natural language queries to implicit system settings or configuration concepts?
- Basis in paper: Section 4.6 details a failure case where the agent could not link "debug console" to the "focusEditorOnBrake" setting because the query lacked explicit keywords, noting the agent "failed to make the conceptual leap."
- Why unresolved: The current architecture appears to rely heavily on keyword matching rather than deep semantic reasoning about application domain logic.
- What evidence would resolve it: Success rate improvements on a specific benchmark of configuration tasks requiring indirect semantic mapping between user intent and non-obvious setting keys.

### Open Question 2
- Question: What mechanisms can effectively resolve task scope ambiguity (e.g., global vs. workspace) in user instructions without explicit clarification?
- Basis in paper: Section 4.6 highlights a failure where the agent modified workspace settings instead of global user settings for a request to "hide __pycache__," illustrating the challenge of disambiguating user intent.
- Why unresolved: The system currently lacks a robust ambiguity detection module or interactive clarification loop to distinguish between multiple valid interpretations of a task.
- What evidence would resolve it: Comparative performance metrics on tasks with intentional scope ambiguity, measuring success rates between default inference behaviors and interactive disambiguation approaches.

### Open Question 3
- Question: Does the strict separation of the Programmer (text-only) and GUI Operator (visual) create information bottlenecks for tasks requiring visual debugging?
- Basis in paper: Section 3.1 notes the Programmer is a language model with "limited ability to read and understand sequential images," relying on the Orchestrator to provide textual environment context.
- Why unresolved: It is unclear if textual summaries of visual state (e.g., "opened windows") result in information loss for programmatic tasks that might benefit from direct visual feedback (e.g., verifying a plot overlay).
- What evidence would resolve it: An ablation study comparing the current text-based Programmer inputs against a multimodal Programmer with direct screenshot access for coding subtasks.

## Limitations

- Hybrid approach assumes clean separation between GUI and programmatic tasks, but many workflows require interleaved visual verification during code execution
- Modal routing decisions by Orchestrator may struggle with ambiguous tasks where both approaches are viable
- Performance on proprietary applications without CLI interfaces remains untested

## Confidence

- **High confidence**: The efficiency gains (10.15 vs 15 steps) and improved success rate (60.76%) are directly measured from OSWorld benchmark results and are reproducible given access to the same evaluation framework.
- **Medium confidence**: The mechanism explanations (step compression, modal routing, context isolation) are well-supported by the paper's evidence and design choices, though some