---
ver: rpa2
title: 'Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision,
  and Reward'
arxiv_id: '2601.19055'
source_url: https://arxiv.org/abs/2601.19055
tags:
- user
- learning
- edits
- where
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a theoretical framework for fine-tuning\
  \ large language models (LLMs) using user-edit deployment data. The setup unifies\
  \ three feedback types\u2014supervised labels, preferences, and cost\u2014into a\
  \ single learning problem, reflecting how real users naturally provide feedback\
  \ by editing LLM responses."
---

# Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward

## Quick Facts
- arXiv ID: 2601.19055
- Source URL: https://arxiv.org/abs/2601.19055
- Authors: Dipendra Misra, Aldo Pacchiano, Ta-Chung Chi, Ge Gao
- Reference count: 40
- Primary result: Unifies supervised labels, preferences, and cost feedback from user edits into a single theoretical framework with provable sample complexity trade-offs.

## Executive Summary
This paper establishes a theoretical framework for fine-tuning large language models using user-edit deployment data. The authors demonstrate that user edits naturally encode three distinct feedback types—supervised labels (the edit itself), preferences (edit direction), and cost (edit distance)—that are typically studied separately. They analyze three basic learning algorithms (SFT, DPO, RL) with different sample complexity trade-offs depending on user distribution characteristics, and propose a late-ensembling bandit strategy that adaptively selects the best policy at test time.

## Method Summary
The method unifies three feedback types from user-edit data: supervised labels (user edits as training targets), preferences (indicating edit direction), and cost (edit distance as penalty). Three algorithms are analyzed: SFT (requires strong user convergence), DPO (robust to weak users), and RL (sample-efficient for simple costs). A late-ensembling bandit strategy combines these methods, selecting policies online via UCB to minimize cumulative edit cost. The framework is evaluated on email writing and summarization tasks using LLM-simulated users with strong/weak editing behaviors.

## Key Results
- User edits unify preferences, supervised labels, and cost feedback in a single theoretical framework.
- SFT, DPO, and RL have provably different sample complexity trade-offs depending on user behavior.
- Late-ensembling outperforms individual methods, achieving lowest worst-case suboptimality (0.1586) across user distributions.
- The method generalizes well to different user LLMs, maintaining performance gaps under distribution shift.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User edits simultaneously encode three distinct feedback types that are typically studied separately.
- Mechanism: Given a context x, agent response y, and user edit y', the data provides: (1) supervised label y' via standard next-token prediction, (2) preference signal that y' ≻ y via Bradley-Terry compatible distribution, and (3) cost signal via edit distance Δ_edit(y, y'). These feedback types have different sample complexity bounds depending on user distribution and model class.
- Core assumption: The balance equation holds: q(y'|x,y)/q(y|x,y') = π*_β(y'|x)/π*_β(y|x), which connects user edit behavior to the optimal policy distribution (Assumption 1).
- Evidence anchors:
  - [abstract]: "there emerges a unification of various feedback types namely preferences, supervised labels, and cost that are typically studied separately"
  - [section 4.1]: Lemma 1 proves the contraction property showing ||q∘π - π*_β||_TV ≤ (1-γ_min)||π - π*_β||_TV
  - [corpus]: Related work on span-level feedback (arxiv:2512.23693) explores fine-grained feedback but does not unify multiple feedback types within a single framework
- Break condition: If user edits are random or adversarial (no relationship to π*_β), the balance equation fails and theoretical guarantees collapse.

### Mechanism 2
- Claim: Different learning approaches (SFT vs DPO vs RL) have provably different trade-offs depending on user behavior characteristics.
- Mechanism: SFT performance depends on γ_min (user convergence rate to optimal), with error bound O(min{η_max·D(π_ref, π*_β), η̄_max·D^1/2}). DPO performance depends on preference concentrability C_PREF. RL performance depends on policy concentrability C̄*_ and cost function eluder dimension.
- Core assumption: Each algorithm's assumptions hold: realizability (π*_β ∈ Π), bounded density ratios, and concentrability conditions for DPO/RL.
- Evidence anchors:
  - [section 4.2]: Theorem 1 shows SFT requires large γ_min; Theorem 2 shows DPO requires small C_PREF; Theorem 3 shows RL requires low eluder dimension
  - [section 5]: SFT outperforms with strong user (high γ_min), DPO outperforms with weak user (low γ_min)
  - [corpus]: Related work on tangent space fine-tuning (arxiv:2602.01128) addresses multi-dimensional preferences but through directional optimization rather than algorithm selection
- Break condition: When γ_min → 0 (user rarely produces optimal responses) AND C_PREF → ∞ (poor coverage) AND cost function has high complexity, all individual methods fail.

### Mechanism 3
- Claim: Late-ensembling via UCB bandit selection adaptively chooses the best policy for the test-time user distribution without requiring prior knowledge.
- Mechanism: Train K policies offline using different feedback types. During online deployment, maintain empirical cost estimates C(π)/N(π) for each policy. Select π_t = argmin_π {C(π)/N(π) - α√(log(t)/N(π))} where the exploration bonus enables regret O(√T) relative to the best fixed policy.
- Core assumption: At least one policy in the ensemble performs reasonably well for the test distribution; the UCB parameter α is appropriately tuned.
- Evidence anchors:
  - [section 3.2]: Algorithm 1 describes the LateEnsemble procedure using UCB for policy selection
  - [section 5, Table 1]: LateEnsemble achieves lowest Max.SubOpt (0.1586) across all user/distribution combinations
  - [corpus]: T-POP (arxiv:2509.24696) addresses test-time personalization but requires online fine-tuning rather than bandit selection
- Break condition: If all K policies perform poorly (high edit cost) on the test distribution, ensembling cannot help; exploration overhead may slightly hurt early performance.

## Foundational Learning

- Concept: KL-regularized optimization and the optimal policy form
  - Why needed here: The paper competes against π*_β(y|x) ∝ π_ref(y|x)exp(-c(x,y)/β), which is the closed-form solution to the KL-constrained objective. Understanding why KL regularization appears is essential for interpreting the theoretical bounds.
  - Quick check question: Why does the optimal policy under KL regularization have an exponential form involving the cost function?

- Concept: Total variation distance vs KL divergence for measuring policy closeness
  - Why needed here: The contraction property (Lemma 1) uses TV distance, while the regularized objective uses KL divergence. The paper establishes connections between these metrics via Lemma 4-5.
  - Quick check question: What is the relationship between TV distance and unregularized vs regularized suboptimality?

- Concept: Multi-armed bandits and UCB algorithm
  - Why needed here: Late-ensembling frames policy selection as a bandit problem where each policy is an "arm" and edit cost is the "loss." UCB provides principled exploration-exploitation trade-off.
  - Quick check question: Why does the UCB formula include both an empirical cost term and an exploration bonus √(log(t)/N(π))?

## Architecture Onboarding

- Component map:
  - **Offline Phase**: User edit dataset D → Three parallel training pipelines → SFT policy π_SUP (Eq.3), DPO policy π_PREF (Eq.4), RL policy π_RL (Eq.6), Early ensemble π_EF (Eq.7)
  - **Online Phase**: Policy ensemble Ψ = [π_SUP, π_PREF, π_RL, π_EF] → UCB selector → Response generation → Edit cost observation → Update statistics C(π), N(π)

- Critical path:
  1. Data collection: Ensure deployment logs capture (context, response, edit) triples with proper tokenization for edit distance computation
  2. Cost model training: Train f̂ via square-loss regression if using RL approach (Eq.5); implement pessimistic cost function f̄ if theoretical guarantees needed
  3. Multi-policy training: Train all policies with identical train/validation splits; use validation loss for hyperparameter selection
  4. UCB integration: Initialize cost accumulators C(π)=0, N(π)=0; implement exploration bonus with tunable α parameter

- Design tradeoffs:
  - **Early vs Late ensembling**: Early (Eq.7) combines losses during training but requires hyperparameter λ tuning; Late selects at inference but adds exploration overhead
  - **SFT vs DPO vs RL**: SFT is simplest but requires high γ_min; DPO is more robust to weak users; RL is most sample-efficient for simple cost functions but requires cost model training
  - **Reference policy choice**: Using a stronger base model as π_ref improves coverage but may reduce diversity in edit patterns

- Failure signatures:
  - **Repetition loops**: DPO and EarlyEnsemble may generate repetitive text; implement post-generation trimming (200+ consecutive characters)
  - **Distribution mismatch**: If test user differs from training user, policies trained on strong users may underperform; LateEnsemble mitigates but requires sufficient exploration
  - **Hyperparameter sensitivity**: EarlyEnsemble performance varies significantly with λ; validation loss selection may not correlate with test edit cost

- First 3 experiments:
  1. **Baseline comparison on held-out data**: Train SFT, DPO, and EarlyEnsemble on the same dataset; evaluate average edit cost and Max.SubOpt across different user types (strong/weak). Expected: No single method dominates; ensemble approaches show lower worst-case suboptimality.
  2. **Ablation on ensemble composition**: Test LateEnsemble with different subsets of {SFT, DPO, RL, EarlyEnsemble} to identify which combinations provide robustness. Expected: Including diverse methods improves worst-case performance but may not improve best-case.
  3. **Transfer learning across user models**: Train policies on one LLM-simulated user (e.g., Qwen-32B), test on another (e.g., Llama-70B). Expected: LateEnsemble maintains performance gap over single methods; validates robustness to user distribution shift (see Table 2).

## Open Questions the Paper Calls Out

- Question: How do the proposed methods perform when trained and evaluated on real human edits rather than LLM-simulated users?
  - Basis in paper: [explicit] Conclusion states "evaluating these algorithms in a human study" as future work
  - Why unresolved: All experiments use LLM-simulated users with explicit preference strings; real users may edit differently than synthetic agents
  - What evidence would resolve it: Human study comparing cumulative edit costs across methods on email writing and summarization tasks

- Question: Can theoretical bounds be derived when the balance equation assumption (Assumption 1) is relaxed or violated?
  - Basis in paper: [explicit] Conclusion calls for "relaxing different assumptions"
  - Why unresolved: All theoretical results (Theorems 1-3) depend on the balance equation relating edit probabilities to the optimal policy
  - What evidence would resolve it: Bounds accommodating bounded violations of the balance equation, validated empirically with users who deviate from this assumption

- Question: Does the cost-based RL approach (ˆπRL from Equation 6) provide complementary benefits beyond SFT and DPO?
  - Basis in paper: [inferred] Section 5 states "We do not evaluate Equation 6 given challenges in implementing it"
  - Why unresolved: The pessimistic RL algorithm is theoretically analyzed and included in Algorithm 1, but excluded from empirical evaluation
  - What evidence would resolve it: Implementation and comparison of ˆπRL performance alongside other methods on the email and summarization benchmarks

- Question: Can optimal algorithms with provably minimal regret be derived for learning from user edits?
  - Basis in paper: [explicit] Conclusion calls for "deriving optimal algorithms"
  - Why unresolved: The paper provides upper bounds for individual methods and shows late-ensembling works empirically, but no lower bounds or optimality proofs
  - What evidence would resolve it: Information-theoretic lower bounds on achievable regret, or algorithms matching these bounds

## Limitations

- The theoretical framework relies heavily on the balance equation assumption (Assumption 1), which may not hold for real-world users who edit stochastically or adversarially.
- The analysis assumes a fixed user distribution throughout both offline and online phases, which may not reflect practical scenarios where user preferences evolve over time.
- The late-ensembling approach requires maintaining multiple trained policies and incurs exploration overhead during deployment, which may not be practical for resource-constrained applications.

## Confidence

- **High confidence**: The unification of feedback types (preferences, supervised labels, and cost) is theoretically sound and well-supported by the proofs. The empirical results showing late-ensembling's superior worst-case performance across user distributions are robust.
- **Medium confidence**: The sample complexity trade-offs between SFT, DPO, and RL depend on specific concentrability conditions that may not hold in practice. The choice of UCB parameter α=150 appears somewhat arbitrary without sensitivity analysis.
- **Low confidence**: The assumption that user edit distance directly corresponds to a well-defined cost function may not hold for all types of edits. The theoretical guarantees assume realizability (π*_β ∈ Π) which may not be satisfied for finite-capacity models.

## Next Checks

1. **Stress test Assumption 1**: Systematically vary the degree of randomness in user edits and measure how quickly the theoretical guarantees degrade. This would validate whether the balance equation is a reasonable approximation of real user behavior.

2. **Generalize beyond Levenshtein distance**: Implement alternative cost functions (semantic similarity, task-specific metrics) to test whether the theoretical framework extends beyond edit distance. This would validate the robustness of the RL approach to different cost definitions.

3. **Test dynamic user distributions**: Modify the online evaluation to include user preference drift over time. Measure whether late-ensembling maintains its advantage when the test-time user distribution changes during deployment.