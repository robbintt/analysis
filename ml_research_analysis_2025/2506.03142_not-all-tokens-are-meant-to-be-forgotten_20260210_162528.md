---
ver: rpa2
title: Not All Tokens Are Meant to Be Forgotten
arxiv_id: '2506.03142'
source_url: https://arxiv.org/abs/2506.03142
tags:
- unlearning
- forget
- information
- arxiv
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of catastrophic forgetting in large
  language model unlearning, where existing methods indiscriminately suppress all
  tokens in forget samples, leading to significant utility loss. The proposed Targeted
  Information Forgetting (TIF) framework differentiates between unwanted words (UW)
  and general words (GW) in forget samples using either generative or discriminative
  identifiers, then applies targeted preference optimization that unlearns only UW
  while preserving GW.
---

# Not All Tokens Are Meant to Be Forgotten

## Quick Facts
- arXiv ID: 2506.03142
- Source URL: https://arxiv.org/abs/2506.03142
- Authors: Xiangyu Zhou; Yao Qiang; Saleh Zare Zade; Douglas Zytko; Prashant Khanduri; Dongxiao Zhu
- Reference count: 13
- Primary result: TIF framework achieves SOTA unlearning effectiveness while preserving model utility, with over 85% utility and near-perfect forget quality on 1% and 5% forget sets, and maintaining 70% utility with highest forget quality on 10% forget set.

## Executive Summary
This paper addresses catastrophic forgetting in large language model unlearning by proposing the Targeted Information Forgetting (TIF) framework. Existing methods indiscriminately suppress all tokens in forget samples, leading to significant utility loss. TIF differentiates between unwanted words (UW) and general words (GW) in forget samples using either generative or discriminative identifiers, then applies targeted preference optimization that unlearns only UW while preserving GW. Extensive experiments on TOFU and MUSE benchmarks demonstrate that TIF achieves state-of-the-art unlearning effectiveness while preserving model utility.

## Method Summary
TIF operates in two stages: first, an information identifier (either generative ChatGPT-4 or discriminative DistilBERT) classifies tokens in forget samples as unwanted words (UW) or general words (GW). Second, the model undergoes targeted preference optimization (TPO) that combines Logit Preference Loss (LPL) on UW tokens and Preservation Loss (PL) on GW tokens. The framework trains with AdamW (lr=1e-5, batch=32) for 10 epochs, using β∈[0.1,0.5] for LPL strength and λ∈[0,0.01] for PL weight. The approach aims to selectively remove unwanted information while maintaining general knowledge through explicit preservation of GW.

## Key Results
- TPO-GPT variant achieves over 85% utility and near-perfect forget quality on 1% and 5% forget sets
- Maintains 70% utility with highest forget quality on the most challenging 10% forget set condition
- GPT-based identifier outperforms DistilBERT-based identifier in terms of forget-utility balance
- TIF significantly outperforms baseline unlearning methods across all benchmark conditions

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Information Differentiation
The framework distinguishes Unwanted Words (UW) from General Words (GW) in forget samples to reduce collateral utility loss during unlearning. An identifier labels each token as UW (specific private/copyrighted content) or GW (stop words, common phrases), and unlearning optimization targets only UW logits while preserving GW through explicit retention. This works because general words in forget samples overlap significantly with tokens in retain sets, and suppressing them causes unintended knowledge degradation.

### Mechanism 2: Preservation Loss for General Word Retention
Cross-entropy loss on GW during unlearning stabilizes loss values and slows utility degradation. PL explicitly optimizes the model to maintain low cross-entropy on GW while LPL suppresses UW. This counteracts the spillover effect where unlearning optimization inadvertently increases GW loss, as cross-entropy loss on GW correlates with downstream utility metrics.

### Mechanism 3: Logit Preference Loss for Targeted Logit Reduction
Directly reducing UW logits relative to the original model—rather than reducing output probability via softmax—preserves the broader logit distribution and reduces unintended forgetting. LPL maximizes the logit difference between original and unlearned models for UW tokens only, constraining changes to specific token logits rather than allowing other logits to inflate, which minimizes KL divergence between original and unlearned models on retain/general text.

## Foundational Learning

- **Negative Preference Optimization (NPO)**: Understanding NPO's bounded loss and reference model formulation is prerequisite to grasping LPL's modifications. Quick check: How does NPO prevent catastrophic collapse compared to gradient ascent?

- **Masked Language Modeling (MLM)**: The discriminative identifier uses DistilBERT's masked prediction to determine if a token is predictable from context (GW) or not (UW). Quick check: If DistilBERT predicts the masked token correctly, is it classified as UW or GW?

- **KL Divergence**: The paper uses KL divergence between original and unlearned models to diagnose over-forgetting. Understanding this metric is essential for interpreting Figure 8. Quick check: What does high KL divergence on the retain set indicate about unlearning quality?

## Architecture Onboarding

- **Component map**: Information Identifier (GPT/DistilBERT) → Original Model (frozen) → Unlearned Model (trainable) → TPO Loss (LPL on UW + PL on GW)

- **Critical path**: Preprocess forget samples through identifier to generate token masks → For each training step, compute LPL using original model logits on UW tokens → Compute PL on GW tokens → Backprop combined loss

- **Design tradeoffs**: Generative vs. discriminative identifier: GPT-based yields better forget-utility balance (higher Jaccard consistency ~0.88-0.91) but requires API access; DistilBERT is local and faster but less precise. β parameter controls LPL strength; paper finds 0.19-0.32 optimal across tasks. λ weight for PL: higher λ preserves more utility but may reduce forget quality.

- **Failure signatures**: Catastrophic collapse (model utility → 0); Under-forgetting (PrivLeak remains highly negative); Over-forgetting (high KL divergence on retain set); Identifier failure (inconsistent UW classification).

- **First 3 experiments**:
  1. Ablation on identifier type: Run TPO with GPT-based vs. DistilBERT-based identification on TOFU Forget05; compare forget quality and utility curves.
  2. PL weight sweep: Fix β=0.25, vary λ in [0, 0.005, 0.01, 0.02] on Forget10; plot utility vs. forget quality tradeoff.
  3. Scalability test: Apply TPO-GPT to Forget01, Forget05, Forget10 with identical hyperparameters; verify utility preservation scales with forget set size.

## Open Questions the Paper Calls Out

- Can the TIF framework be adapted to address knowledge-level unlearning where information is conceptually diffuse rather than localized to specific tokens? The paper explicitly calls for exploring techniques for knowledge-based identification to address more diffuse, knowledge-level unlearning tasks.

- How can the generative identification approach be effectively applied to datasets where sample lengths significantly exceed the context window limits of current LLMs? The paper notes that GPT models cannot process the MUSE Books dataset (~175k words) due to the 128k token limit.

- Does the Logit Preference Loss (LPL) maintain its utility-preserving properties when applied to models with significantly larger parameter counts or different architectures? Experiments are restricted to 3B and 7B parameter models, leaving scalability to larger architectures unverified.

## Limitations

- Identifier Generalization: The framework's performance critically depends on token-level UW/GW classification accuracy, which may not generalize across different domains, languages, or token types.

- Benchmark Representativeness: Both TOFU and MUSE benchmarks use relatively constrained text domains (synthetic biographies and published books/news), leaving effectiveness on diverse, real-world forgetting scenarios unproven.

- Logit Preference Assumption: The paper provides limited theoretical justification for why direct logit manipulation is superior to probability-space optimization, and lacks empirical comparisons to alternative preference formulations.

## Confidence

- **High Confidence**: TIF framework achieves superior forget-utility tradeoffs compared to baselines; combination of LPL and PL is more effective than either alone; GPT-based identifier outperforms DistilBERT-based identifier.

- **Medium Confidence**: Token-level differentiation is the primary driver of performance gains; optimal β range (0.19-0.32) generalizes across forget set sizes; LPL's logit-level targeting is more effective than probability-space approaches.

- **Low Confidence**: Theoretical superiority of LPL over alternative formulations; identifier performance would generalize to non-English text or code; framework scales effectively to billion-parameter models.

## Next Checks

1. **Cross-Domain Identifier Robustness**: Evaluate the discriminative identifier on non-English text (e.g., Chinese, Arabic) and code repositories. Measure UW/GW classification accuracy and downstream unlearning performance. If accuracy drops >15% or forget quality degrades significantly, the framework requires domain-specific identifier adaptation.

2. **Large-Scale Model Validation**: Apply TIF to a 70B parameter model (e.g., LLaMA-3 70B) using the same hyperparameters. Monitor GPU memory usage, training time, and whether the forget-utility tradeoff curve maintains its shape. If utility preservation drops >20% or training becomes unstable, investigate scaling-specific hyperparameter adjustments.

3. **Information Entanglement Stress Test**: Create synthetic forget samples where UW and GW tokens are semantically entangled (e.g., "The CEO [UW: John Smith] announced [GW: significant] profits"). Measure whether TIF can successfully separate and preserve GW while unlearning UW. If forget quality drops below 0.3 or utility drops below 60%, the token-splitting assumption needs refinement for complex information structures.