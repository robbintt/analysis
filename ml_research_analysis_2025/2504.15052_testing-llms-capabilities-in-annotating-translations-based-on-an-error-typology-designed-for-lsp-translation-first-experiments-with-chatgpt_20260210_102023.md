---
ver: rpa2
title: 'Testing LLMs'' Capabilities in Annotating Translations Based on an Error Typology
  Designed for LSP Translation: First Experiments with ChatGPT'
arxiv_id: '2504.15052'
source_url: https://arxiv.org/abs/2504.15052
tags:
- errors
- chatgpt
- error
- translation
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates the capability of large language models,
  specifically ChatGPT, to annotate translation errors using a customized error typology
  designed for specialized language translation. Two prompts were tested: a detailed
  prompt including full error definitions and a shorter prompt without definitions.'
---

# Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT

## Quick Facts
- arXiv ID: 2504.15052
- Source URL: https://arxiv.org/abs/2504.15052
- Reference count: 18
- Primary result: ChatGPT can identify translation errors without explicit definitions but benefits from detailed error definitions for accurate categorization, with significant performance degradation when self-assessing its own translations.

## Executive Summary
This study investigates whether large language models can effectively annotate translation errors using a customized error typology designed for specialized language translation (LSP). Using ChatGPT (GPT-4o), the researchers tested two prompt variants—one with detailed error definitions and one without—on translations from DeepL and ChatGPT's own outputs. Results show that while error detection is largely independent of explicit definitions, accurate categorization requires detailed instructions. The study also reveals significant limitations in LLM self-assessment capabilities, with performance deteriorating markedly when annotating its own translations. These findings highlight both the potential and constraints of LLMs in translation evaluation, particularly for specialized domains.

## Method Summary
The researchers used GPT-4o via ChatGPT to annotate translation errors in two sub-corpora: 35 English source texts (NLP research abstracts) translated by DeepL with 399 human-annotated errors, and 25 texts translated by ChatGPT with 193 human-annotated errors. Two prompt variants were tested: a "long prompt" containing full error definitions from a 50-page annotation manual, and a "short prompt" with only error type names and codes. The study employed sentence-level alignment and evaluated performance using macro-averaged precision, recall, and F1 scores, with error matching based on character overlap. A customized error typology derived from MQM with 39 error types across three categories was used.

## Key Results
- ChatGPT achieved F1 scores around 0.70 for error detection on DeepL translations regardless of prompt detail level
- Long prompt achieved 64.1% correct error labeling vs 46.9% for short prompt, while F1 remained similar (~0.70) for error detection
- Self-evaluation performance dropped significantly with F1 decreasing from 0.707 (DeepL evaluation) to 0.496 (self-evaluation), and false-error rate rising from 14.47% to 55.02%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing explicit error definitions in prompts improves LLM error categorization accuracy.
- Mechanism: Detailed definitions ground the model's classification decisions in domain-specific criteria rather than relying solely on parametric knowledge, reducing ambiguity in label assignment.
- Core assumption: The LLM can parse and apply lengthy definitional content during inference.
- Evidence anchors:
  - [abstract] "the degree of accuracy in error categorisation depends on the prompt's specific features and its level of detail"
  - [Section 5, Results] Long prompt achieved 64.1% correct labeling vs 46.9% for short prompt; F1 remained similar (~0.70) for error detection across both
  - [corpus] Weak direct corpus support; neighbor papers focus on multi-agent frameworks rather than prompt structure effects
- Break condition: If the annotation manual exceeds context window or if definitions conflict with model's internalized taxonomy, categorization may degrade.

### Mechanism 2
- Claim: LLM self-evaluation of own translations exhibits systematic underperformance compared to evaluation of external MT outputs.
- Mechanism: The model may lack sufficient "distance" from its own generation process to objectively identify errors, or may be implicitly biased toward viewing its outputs as correct.
- Core assumption: Self-assessment failure is not simply due to translation quality differences but reflects an evaluation-specific limitation.
- Evidence anchors:
  - [abstract] "ChatGPT's performance significantly deteriorated when annotating its own translations, indicating limitations in self-assessment"
  - [Section 5, Results] F1 dropped from 0.707 (DeepL evaluation with long prompt) to 0.496 (self-evaluation); false errors rose from 14.47% to 55.02%
  - [corpus] Not directly addressed in neighbor papers
- Break condition: If self-generated translations are genuinely higher quality with fewer detectable errors, the lower recall might reflect true difficulty rather than bias—though high false-error rate suggests otherwise.

### Mechanism 3
- Claim: Error detection capability is largely decoupled from explicit error definitions, operating from pre-trained linguistic knowledge.
- Mechanism: The model's ability to identify that something is wrong in a translation draws on general language competence acquired during training, while categorization requires explicit schema alignment.
- Core assumption: Detection and categorization are partially separable cognitive operations for LLMs.
- Evidence anchors:
  - [Section 5, Results] "removal of this information has only a (very) slight impact on the system's ability to identify errors"
  - [Section 6, Discussion] "ChatGPT is performing its annotations efficiently even without detailed instructions... relying more on its knowledge acquired during training"
  - [corpus] Neighbor paper "Remedy-R" suggests generative reasoning can support evaluation without explicit error annotations
- Break condition: For specialized domains with unfamiliar error types (e.g., domain-specific terminology errors), detection may also require explicit definitional support.

## Foundational Learning

- Concept: **MQM (Multidimensional Quality Metrics) framework**
  - Why needed here: The paper's error typology is derived from MQM; understanding its hierarchical structure (accuracy, fluency, style, terminology subcategories) is prerequisite to interpreting annotation results.
  - Quick check question: Can you distinguish between a "terminology error" and a "content transfer error" in the MQM schema?

- Concept: **Prompt chaining**
  - Why needed here: The study uses sequential prompts where outputs feed into subsequent inputs, enabling complex multi-step annotation without single-prompt overflow.
  - Quick check question: How would you structure a two-prompt chain where the first identifies error spans and the second categorizes them?

- Concept: **Macro-averaged precision/recall**
  - Why needed here: The paper reports document-level scores averaged across texts rather than micro-aggregated counts; this weights each document equally regardless of error density.
  - Quick check question: If one document has 50 errors and another has 5, does macro-averaging give them equal weight in the final score?

## Architecture Onboarding

- Component map:
  ```
  Source Text + MT Output → ChatGPT (GPT-4o) with Prompt → Error Annotations
                                              ↓
  Prompt = [Task instructions] + [Error typology ± definitions] + [50-page annotation manual attachment]
                                              ↓
  Evaluation: Compare LLM annotations vs. Expert human annotations → Precision/Recall/F1
  ```

- Critical path:
  1. Select MT system output to evaluate (DeepL vs. ChatGPT self-translation)
  2. Construct prompt with appropriate detail level (long vs. short)
  3. Execute annotation with sentence-level alignment (not document-level)
  4. Match predicted errors to reference errors using character overlap criterion
  5. Compute precision, recall, F1, and label accuracy per document

- Design tradeoffs:
  - Long prompt: Higher categorization accuracy, lower variability, more tokens
  - Short prompt: Similar detection, worse categorization, higher variance across documents
  - Sentence-level vs. document-level annotation: Reduces information density per prompt but may lose cross-sentence context

- Failure signatures:
  - High false-error rate (>50%) indicates over-annotation, typical in self-evaluation mode
  - Precision variance >0.5 across documents suggests inconsistent reliability for production use
  - Label accuracy <50% with short prompt indicates categorization failure

- First 3 experiments:
  1. Establish baseline with long prompt on DeepL outputs (35 documents); expect F1 ~0.70, label accuracy ~64%
  2. Ablate error definitions (short prompt on same 35 documents); expect similar detection, degraded categorization
  3. Test self-evaluation mode (long prompt on 25 ChatGPT translations); expect F1 drop to ~0.50 and false-error rate spike

## Open Questions the Paper Calls Out

- Can open-source LLMs provide translation error annotations of comparable or superior quality to ChatGPT while offering better reproducibility?
- Does the use of LLM-generated error annotations significantly reduce the cognitive effort required for teachers to evaluate translations?
- To what extent do LLM-generated annotations improve the quality of students' post-editing and translation learning?

## Limitations
- The study's error typology, while derived from MQM, was customized for LSP translation and may not generalize to broader domains.
- Self-evaluation performance showed high false-error rates (55.02%), suggesting systematic bias rather than true error identification difficulty.
- The corpus consists of technical NLP research abstracts, creating potential domain-specific biases in both error types and model performance.

## Confidence
- **High**: ChatGPT can detect translation errors without explicit definitions, with F1 scores around 0.70 for external MT outputs
- **Medium**: Detailed error definitions significantly improve categorization accuracy but not detection capability
- **Medium**: Self-evaluation mode shows systematic deterioration in performance, though the exact mechanism remains unclear
- **Low**: Claims about the superiority of long prompts for error categorization based on a single dataset and prompt variant comparison

## Next Checks
1. **Cross-domain validation**: Test the same prompt structure and error typology on non-technical domain texts (e.g., medical, legal, or business documents) to assess generalizability
2. **Open-source alternative comparison**: Evaluate Mistral, LLaMA, or other open models using identical prompts to establish whether the findings extend beyond proprietary systems
3. **Prompt refinement experiment**: Systematically vary prompt detail levels (beyond just long/short) and measure the relationship between definition specificity and categorization accuracy across different error types