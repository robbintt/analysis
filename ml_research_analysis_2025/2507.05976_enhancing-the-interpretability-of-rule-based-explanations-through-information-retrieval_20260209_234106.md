---
ver: rpa2
title: Enhancing the Interpretability of Rule-based Explanations through Information
  Retrieval
arxiv_id: '2507.05976'
source_url: https://arxiv.org/abs/2507.05976
tags:
- factors
- rules
- prediction
- attributes
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an attribution-based approach to enhance the
  interpretability of a rule-based AI model for predicting arm lymphedema risk after
  breast cancer radiotherapy. The method clusters model attributes into semantically
  coherent factors and computes their relevance using tf-idf metrics from Information
  Retrieval.
---

# Enhancing the Interpretability of Rule-based Explanations through Information Retrieval

## Quick Facts
- arXiv ID: 2507.05976
- Source URL: https://arxiv.org/abs/2507.05976
- Reference count: 33
- Primary result: Attribution-based IR approach improves rule-based explanation interpretability for lymphedema risk prediction

## Executive Summary
This paper addresses the challenge of interpreting rule-based AI models in clinical settings by introducing an attribution-based approach that leverages Information Retrieval techniques. The method clusters model attributes into semantically coherent factors and computes their relevance using tf-idf metrics, producing enhanced visualizations including radar graphs and factor lists. A user study comparing this approach against raw rule-based explanations demonstrated statistically significant improvements in both interpretability and usefulness ratings from both AI experts and non-AI experts. The work focuses specifically on predicting arm lymphedema risk after breast cancer radiotherapy, demonstrating practical clinical applicability.

## Method Summary
The proposed method clusters model attributes into semantically coherent factors and computes their relevance using tf-idf metrics from Information Retrieval. This attribution-based approach enhances rule-based explanations by creating more interpretable visualizations such as radar graphs and factor lists. The method processes model attributes to identify meaningful groupings and calculates factor relevance scores, transforming raw rule-based predictions into more comprehensible clinical insights. The approach was specifically applied to a rule-based AI model for predicting arm lymphedema risk following breast cancer radiotherapy.

## Key Results
- User study showed statistically significant improvements in interpretability ratings for enhanced explanations
- Usefulness scores were significantly higher for the proposed approach compared to raw rule-based explanations
- Both AI experts and non-AI experts rated the enhanced visualizations (radar graphs and factor lists) significantly higher than raw explanations

## Why This Works (Mechanism)
The method leverages tf-idf metrics from Information Retrieval to compute factor relevance scores, transforming raw attribute data into semantically meaningful groupings. By clustering model attributes into coherent factors, the approach creates a more intuitive representation of the model's decision-making process. The enhanced visualizations (radar graphs and factor lists) present complex medical relationships in a format that practitioners can more easily understand and apply to clinical decision-making. This combination of semantic clustering and IR-based relevance scoring bridges the gap between complex rule-based models and human interpretability requirements in medical contexts.

## Foundational Learning
- **tf-idf relevance scoring**: Needed to quantify the importance of different factors within the model's predictions; quick check involves verifying score distribution across factors
- **Semantic clustering of attributes**: Required to group related medical features into meaningful categories; quick check involves validating cluster coherence with domain expert feedback
- **Radar graph visualization**: Essential for presenting multi-factor relationships in an intuitive format; quick check involves testing readability across different factor counts
- **User perception measurement**: Necessary to evaluate interpretability improvements; quick check involves validating statistical significance of rating differences

## Architecture Onboarding
Component Map: Raw Rule-Based Model -> Attribute Clustering -> tf-idf Relevance Scoring -> Factor List Generation -> Radar Graph Visualization -> User Interface

Critical Path: The end-to-end pipeline processes model attributes through clustering, relevance scoring, and visualization generation to produce enhanced explanations for clinical use.

Design Tradeoffs: The approach balances computational efficiency with interpretability, choosing tf-idf scoring for its simplicity and effectiveness over more complex attribution methods, while accepting potential oversimplification of medical feature relationships.

Failure Signatures: Poor clustering quality manifests as semantically incoherent factor groups; incorrect tf-idf scoring appears as irrelevant factors being highlighted; visualization issues emerge as cluttered or unreadable radar graphs.

First Experiments:
1. Validate clustering quality by comparing generated factor groups against known medical taxonomies
2. Test tf-idf scoring stability across different dataset sizes and attribute distributions
3. Evaluate visualization clarity with varying numbers of factors and medical practitioners

## Open Questions the Paper Calls Out
None

## Limitations
- User study sample size of 25 participants limits generalizability across different clinical contexts
- tf-idf-based relevance computation assumes attribute independence within factors, which may not hold for medically correlated features
- Evaluation focuses solely on interpretability and usefulness perceptions without examining actual clinical decision-making impact

## Confidence
- High confidence in technical implementation of tf-idf scoring and visualization components
- Medium confidence in semantic clustering approach due to reliance on domain expertise
- Medium confidence in user study results given limited sample size and demographic diversity
- Low confidence in clinical utility claims without longitudinal decision-making studies

## Next Checks
1. Conduct a larger-scale user study (n>100) with diverse medical practitioners across multiple institutions to validate generalizability
2. Perform ablation studies comparing tf-idf relevance scoring against alternative attribution methods (SHAP, LIME) for medical rule-based models
3. Implement a longitudinal clinical trial measuring actual decision-making outcomes when using enhanced vs. raw rule-based explanations in lymphedema risk assessment