---
ver: rpa2
title: Training Language Models to Explain Their Own Computations
arxiv_id: '2511.08579'
source_url: https://arxiv.org/abs/2511.08579
tags:
- explainer
- llama-3
- activation
- training
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether language models can learn to accurately
  describe their own internal computations. The authors fine-tune language models
  to generate natural language explanations for three types of internal processes:
  feature descriptions (what inputs activate specific features), activation patching
  outcomes (how internal interventions affect predictions), and input ablation effects
  (which input tokens influence decisions).'
---

# Training Language Models to Explain Their Own Computations

## Quick Facts
- arXiv ID: 2511.08579
- Source URL: https://arxiv.org/abs/2511.08579
- Reference count: 40
- Models can learn to explain their own internal computations, with self-explanation outperforming other models even when those models are more capable.

## Executive Summary
This paper investigates whether language models can accurately describe their own internal computations through self-explanation. The authors fine-tune language models to generate natural language explanations for three types of internal processes: feature descriptions, activation patching outcomes, and input ablation effects. They find that models can indeed learn to self-explain with performance significantly exceeding zero-shot baselines, and more importantly, that models are substantially better at explaining their own computations than explaining other models—even when the explainer is more capable. This "privileged access" advantage enables data-efficient learning, with self-explanation achieving comparable results using only 0.8% of the training data required by alternative approaches.

## Method Summary
The paper uses existing interpretability techniques as ground truth to fine-tune language models to generate natural language descriptions of their internal representations and causal structure. For feature descriptions, the model takes SAE feature vectors as continuous token inputs at the embedding layer and learns to describe what inputs activate those features. For activation patching, the model learns to predict how internal interventions affect predictions. For input ablation, the model learns which input tokens influence decisions. The explainer model receives the internal representation as a continuous token input, allowing it to leverage the inherent structure and semantics of its own representations. This direct access to activation patterns enables the privileged access advantage where self-explanation outperforms cross-model explanation.

## Key Results
- Models trained to explain their own computations significantly outperform zero-shot baselines across all three tasks
- Self-explanation consistently outperforms cross-model explanation, even when the explainer is more capable
- Self-explanation achieves comparable performance using only 0.8% of the training data required by alternative approaches
- Activation similarity between explainer and target models predicts explanation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training models to produce explanations consistent with interpretability procedure outcomes enables them to generalize to novel queries about their own internal representations and causal structure.
- Mechanism: The paper uses ground-truth answers derived from mechanistic interpretability techniques as supervision. The explainer model is fine-tuned via cross-entropy loss to predict the natural language explanation given a question about the target model's internals. This forces the model to learn a mapping from internal state queries to the correct descriptive output.
- Core assumption: Existing interpretability techniques provide valid "ground truth" explanations that can serve as supervision.
- Evidence anchors: Abstract states they use existing interpretability techniques as ground truth; section 2.1 describes the cross-entropy fine-tuning procedure.
- Break condition: If interpretability procedures provide noisy or incorrect descriptions, the explainer will learn to replicate those errors.

### Mechanism 2
- Claim: Models exhibit "privileged access," meaning a model fine-tuned to explain its own computations outperforms other (even more capable) models fine-tuned to explain it.
- Mechanism: The explainer model receives the internal representation as a continuous token input at its embedding layer. This direct access to its own activation patterns allows the explainer to leverage the inherent structure and semantics of its own representations.
- Core assumption: The explainer can effectively condition on and decode information from its own raw activation vectors when presented as inputs.
- Evidence anchors: Abstract highlights the privileged access finding; section 2.2 describes the continuous vector input mechanism.
- Break condition: If internal representations are fundamentally opaque or the linear projection loses too much information, privileged access will not confer an advantage.

### Mechanism 3
- Claim: The degree of activation alignment between an explainer model and a target model predicts the quality of explanation.
- Mechanism: Models with more similar internal representations share a more isomorphic "semantic space," allowing the explainer to more accurately interpret the target's internal vectors.
- Core assumption: Activation similarity metrics capture meaningful alignment between semantic understanding.
- Evidence anchors: Section 3.4 discusses the correlation between activation similarity and explainer performance.
- Break condition: If models can have high activation similarity without shared semantic meaning, this relationship will not hold.

## Foundational Learning

- **Sparse Autoencoders (SAEs)**: Used to decompose dense, superposed internal activations into interpretable, monosemantic features. Quick check: Can you explain the goal of SAEs in mechanistic interpretability and why they are used instead of raw neuron activations?

- **Activation Patching**: The core causal intervention technique where activations are replaced between forward passes to test causal effects. Quick check: If patching an activation from prompt B into prompt A changes the model's prediction to match prompt B's output, what does that tell you about that activation?

- **Transformer Residual Stream**: The central "communication bus" where information is accumulated and read out by each layer. Quick check: In a Transformer, how does information from an earlier layer get passed to a later layer? What is the role of the residual connection?

## Architecture Onboarding

- **Component map**: Interpretability Procedure (T) -> Target Model (M) -> Explainer Model (E) with Linear Projection (Πℓ) -> Natural Language Explanation

- **Critical path**: 1) Run interpretability procedures on target model to create (question, continuous vector, ground-truth explanation) dataset. 2) Initialize projection layer if needed. 3) Fine-tune explainer and projection end-to-end to minimize cross-entropy loss. 4) Generate explanations for new queries.

- **Design tradeoffs**: Self-explanation vs. external explainer (data efficiency vs. generality), continuous vs. discrete input (more information vs. architectural complexity), projection pre-training (improved performance vs. added training step).

- **Failure signatures**: Low/random performance (poor ground-truth data or insufficient training), overtly general explanations (hallucinations instead of conditioning on activation), no privileged access advantage (poor alignment or insufficient model capacity).

- **First 3 experiments**: 1) Train Llama-3.1-8B explainer to explain itself on SAE feature description task and verify against baselines. 2) Train different model (Qwen3-8B or Llama-3.1-70B) to explain Llama-3.1-8B and compare performance. 3) Train self-explainer on logarithmically smaller subsets of training data and plot scaling curve.

## Open Questions the Paper Calls Out
- How do explainer model capacity, target model complexity, and task difficulty influence the emergence and magnitude of privileged access effects?
- Can truly joint training—where both the target model's behavior and its self-explanations co-evolve—maintain faithful introspection?
- Can the self-consistency objective be inverted to train models whose behavior matches normatively-specified explanations, enabling unsupervised alignment?

## Limitations
- The privileged access advantage may not extend to radically different architectures beyond closely related model families
- The approach inherits any biases or inaccuracies from the interpretability tools used to generate ground-truth labels
- The continuous vector input mechanism requires architectural modifications that may not scale cleanly to vastly different dimensionalities

## Confidence
- **High Confidence**: Models can learn to explain their own internal computations with performance exceeding zero-shot baselines
- **Medium Confidence**: Self-explanation outperforms cross-model explanation (tested within related model families)
- **Medium Confidence**: Data efficiency claim of 0.8% training data (based on limited experiments)

## Next Checks
1. Evaluate the self-explanation approach on a non-Transformer architecture (e.g., RWKV or Mamba) to determine if privileged access extends beyond tested model families
2. Systematically evaluate how noise or errors in ground-truth interpretability labels affect explanation quality by injecting controlled noise into supervision data
3. Test whether a model fine-tuned to explain one version of itself can successfully explain a different checkpoint of the same architecture without additional fine-tuning