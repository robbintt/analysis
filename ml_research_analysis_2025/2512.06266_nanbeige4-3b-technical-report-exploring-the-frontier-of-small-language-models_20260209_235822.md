---
ver: rpa2
title: 'Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models'
arxiv_id: '2512.06266'
source_url: https://arxiv.org/abs/2512.06266
tags:
- data
- reasoning
- training
- stage
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Nanbeige4-3B is a compact 3-billion-parameter language model that
  achieves state-of-the-art performance through a combination of high-quality pretraining
  on 23T tokens and advanced post-training techniques. The model employs a Fine-Grained
  Warmup-Stable-Decay scheduler during pretraining and a multi-stage post-training
  pipeline including cold-start SFT, deliberative generation refinement with CoT reconstruction,
  dual-level preference distillation, and multi-stage reinforcement learning.
---

# Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models

## Quick Facts
- arXiv ID: 2512.06266
- Source URL: https://arxiv.org/abs/2512.06266
- Authors: Chen Yang; Guangyue Peng; Jiaying Zhu; Ran Le; Ruixiang Feng; Tao Zhang; Wei Ruan; Xiaoqi Liu; Xiaoxue Cheng; Xiyun Xu; Yang Song; Yanzipeng Gao; Yiming Jia; Yun Xing; Yuntao Wen; Zekai Wang; Zhenwei An; Zhicong Sun; Zongchao Chen
- Reference count: 40
- 3B model achieving state-of-the-art performance on reasoning benchmarks while ranking among top-tier models on WritingBench

## Executive Summary
Nanbeige4-3B is a 3-billion-parameter language model that achieves state-of-the-art performance through high-quality pretraining on 23 trillion tokens and advanced post-training techniques. The model employs a Fine-Grained Warmup-Stable-Decay scheduler during pretraining and a multi-stage post-training pipeline including cold-start SFT, deliberative generation refinement with CoT reconstruction, dual-level preference distillation, and multi-stage reinforcement learning. Despite its small size, Nanbeige4-3B significantly outperforms much larger models like Qwen3-8B and Qwen3-14B on mathematical reasoning, scientific reasoning, tool use, and human preference alignment, while also ranking among top-tier models on the WritingBench leaderboard.

## Method Summary
The model uses hybrid data filtering (tagging + retrieval) with 20-dimensional quality scoring, followed by FG-WSD pretraining across four stages. Post-training includes cold-start SFT on 30M samples with 32K context, overall SFT with deliberative refinement and CoT reconstruction at 64K context, dual preference distillation from a teacher model, and multi-stage GRPO-based RL with verifiable rewards. On-policy data filtering (10%-90% pass rate) is applied before each RL stage to maximize learning efficiency.

## Key Results
- Achieves 90.4 on AIME 2024 (vs 76.0 for Qwen3-8B)
- Scores 82.2 on GPQA-Diamond (vs 62.0 for Qwen3-14B)
- Ranks 1st on WritingBench leaderboard
- Outperforms much larger models across multiple reasoning and alignment benchmarks

## Why This Works (Mechanism)

### Mechanism 1: FG-WSD Quality-Progressive Scheduling
FG-WSD progressively increases the proportion of higher-quality data in later stages, decoupling data ordering from learning rate changes and concentrating high-quality tokens when the model is most receptive. Evidence shows 1B model with FG-WSD achieves 34.3 vs 27.1 on GSM8k and 39.5 vs 34.5 on CMATH compared to vanilla WSD. Break condition: Poor data quality calibration diminishes curriculum benefits.

### Mechanism 2: Deliberative Generation Refinement with CoT Reconstruction
Multi-round critique-revision loop improves solutions using dynamic checklists and evaluators, then a chain-completion model generates CoT that logically leads to the refined solution. Evidence shows 16% improvement on Arena-Hard v2. Break condition: Hallucinated reasoning steps introduce noisy training signal.

### Mechanism 3: Dual Preference Distillation
Joint token-level distillation and sequence-level preference optimization trains student to match teacher token distributions on positives while learning from negatives, with DPO margin constraint enlarging positive-negative gap. Evidence shows ~8% improvement on AIME, ~10% on GPQA, ~30% on BFCL V4. Break condition: Weak signal if negative samples aren't significantly worse than positives.

### Mechanism 4: Multi-Stage RL with On-Policy Filtering
Each RL stage targets single domain (STEM, coding, human preference) with samples filtered by pass rates outside 10%-90% range. Evidence shows concentrated domain-specific training prevents imbalance observed when mixing data. Break condition: Aggressive filtering loses exposure to edge cases needed for robustness.

## Foundational Learning

- **WSD (Warmup-Stable-Decay) scheduling**: Why needed here: FG-WSD builds on vanilla WSD; understanding baseline clarifies what fine-grained variant adds. Quick check: Can you explain why WSD outperforms cosine decay when high-quality data is available for decay phase?

- **Direct Preference Optimization (DPO)**: Why needed here: DPD uses DPO as sequence-level regularizer; you need to understand how DPO shapes decision boundaries. Quick check: How does DPO differ from RLHF in terms of reward model requirements?

- **On-policy RL for language models**: Why needed here: Multi-stage RL uses GRPO with on-policy data filtering; understanding policy gradient basics is essential. Quick check: Why is filtering samples by pass rate (10%-90%) more effective than using all generated rollouts?

## Architecture Onboarding

- **Component map**: Hybrid data filtering → FG-WSD scheduler (Warmup → Diversity-Enriched Stable → High-Quality Stable → Decay) → Cold-start SFT → Overall SFT (deliberative refinement + CoT reconstruction) → DPD distillation → Multi-stage RL (STEM → Coding → Human Preference)

- **Critical path**: Pre-training data quality scoring → FG-WSD stage progression → Cold-start SFT scaling → DPD distillation (enables larger RL gains vs. SFT-only baseline)

- **Design tradeoffs**: Scaling SFT to 30M+ instructions continues improving reasoning benchmarks but requires substantial compute; smaller instruction sets may suffice for non-reasoning tasks. Multi-stage RL increases training complexity but prevents imbalance observed when mixing math and coding data. Token-level distillation on negative samples adds overhead but improves error recognition.

- **Failure signatures**: Base model underperforms on post-SFT eval → Check data quality calibration (content labels should outweigh format labels). RL stagnates on coding → May need domain-isolated stage; verify test function synthesis quality. CoT reconstruction produces inconsistent reasoning → Verify chain-completion model is trained on aligned solution-CoT pairs.

- **First 3 experiments**: 1) Replicate FG-WSD vs vanilla WSD comparison on 1B model with 1T tokens; verify reasoning benchmark improvements. 2) Ablate DPD components: train with token-level distillation only, DPO only, and joint; compare on AIME and Arena-Hard. 3) Test on-policy filtering thresholds (5%-95% vs 10%-90% vs 20%-80%) on single RL stage to find optimal difficulty band.

## Open Questions the Paper Calls Out

### Open Question 1
Can reasoning capabilities and training methodologies (FG-WSD, DPD) demonstrated on mathematical and scientific benchmarks effectively transfer to complex, long-horizon tasks of autonomous software engineering (SWE) and deep-research agents? Current evaluation limited to academic reasoning and short-horizon tool use, whereas SWE and deep-research require long-context coherence and iterative self-correction not captured in reported benchmarks.

### Open Question 2
Does FG-WSD scheduler maintain superiority over vanilla WSD when scaled to parameter counts significantly larger than 3B? Paper validates FG-WSD on 1B and applies to 3B, but provides no ablations or results for larger models (7B, 70B). Benefits of quality-progressive curriculum may saturate or behave differently as model capacity increases.

### Open Question 3
How sensitive is Dual Preference Distillation framework to capability gap between teacher and student models? Method relies on flagship proprietary teacher (Nanbeige3.5-Pro) to provide token-level supervision on student-generated negative samples, but impact of teacher's relative strength is not analyzed. Unclear if DPD remains effective if teacher is less capable or if student's negative samples are of extremely low quality.

## Limitations
- Architectural details (layers, hidden size, attention mechanisms) remain underspecified
- Quality tagging system for 20-dimensional rubric is vaguely described
- Nanbeige3.5-Pro teacher model used for DPD is not publicly available

## Confidence
- **High Confidence**: Overall post-training pipeline structure and its contribution to benchmark improvements
- **Medium Confidence**: Specific contribution of FG-WSD scheduling versus vanilla WSD
- **Low Confidence**: Exact replication of CoT reconstruction and deliberative refinement mechanisms

## Next Checks
1. Implement 20-dimensional quality tagging system using publicly available datasets and validate whether content-based scores correlate with downstream reasoning performance. Run FG-WSD vs vanilla WSD comparisons on 1B model.
2. Replace unavailable Nanbeige3.5-Pro with Qwen2.5-14B-Chat and DeepSeek-R1 distill for DPD. Measure token-level distillation effectiveness on AIME before and after adding DPO regularization.
3. Systematically vary pass-rate thresholds (5%-95%, 10%-90%, 20%-80%) on single GRPO stage using verifiable math problems. Track training stability and final performance on held-out reasoning benchmarks.