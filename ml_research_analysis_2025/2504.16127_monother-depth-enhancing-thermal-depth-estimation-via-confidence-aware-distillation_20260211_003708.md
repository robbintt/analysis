---
ver: rpa2
title: 'MonoTher-Depth: Enhancing Thermal Depth Estimation via Confidence-Aware Distillation'
arxiv_id: '2504.16127'
source_url: https://arxiv.org/abs/2504.16127
tags:
- depth
- thermal
- image
- distillation
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses monocular depth estimation from thermal images,
  which is challenging due to limited labeled data and poor generalization compared
  to RGB depth models. The authors propose MonoTher-Depth, a confidence-aware distillation
  framework that transfers knowledge from a pretrained RGB depth model to a thermal
  depth model.
---

# MonoTher-Depth: Enhancing Thermal Depth Estimation via Confidence-Aware Distillation

## Quick Facts
- arXiv ID: 2504.16127
- Source URL: https://arxiv.org/abs/2504.16127
- Reference count: 40
- One-line primary result: Confidence-aware distillation improves thermal depth estimation by 22.88% in absolute relative error for zero-shot generalization

## Executive Summary
This paper addresses monocular depth estimation from thermal images, which is challenging due to limited labeled data and poor generalization compared to RGB depth models. The authors propose MonoTher-Depth, a confidence-aware distillation framework that transfers knowledge from a pretrained RGB depth model to a thermal depth model. Their key innovation is using predicted confidence of the RGB model, based on cross-modal feature consistency and depth alignment, to selectively guide the thermal model during training. This approach eliminates the need for perfectly co-registered RGB-T pairs. Experiments show that their method significantly improves thermal depth estimation accuracy, reducing absolute relative error by 22.88% in new scenarios without labeled depth, compared to baseline methods without distillation.

## Method Summary
MonoTher-Depth is a confidence-aware distillation framework that transfers knowledge from a pretrained RGB depth model to a thermal depth model. The method uses a U-Net to predict per-pixel confidence of RGB depth predictions based on cross-modal feature consistency and depth alignment. This confidence weights the L1 consistency loss between RGB and warped thermal depth, allowing high-confidence RGB predictions to strongly influence thermal training while low-confidence regions are suppressed. The framework eliminates the need for perfectly co-registered RGB-T pairs by using depth-based warping with bilinear interpolation to align modalities spatially.

## Key Results
- Absolute relative error reduced by 22.88% in new scenarios without labeled depth
- Distillation without confidence degrades performance below no-distillation baseline (RMSE 4.469 vs 4.359)
- Thermal depth estimation provides advantages in adverse conditions like nighttime scenes

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Weighted Knowledge Transfer
Selective distillation based on predicted RGB model reliability improves thermal depth estimation compared to uniform transfer. A U-Net predicts per-pixel confidence $\hat{W}_r$ of RGB depth predictions by ingesting cross-modal metadata. This confidence then weights the L1 consistency loss between RGB and warped thermal depth, allowing high-confidence RGB predictions to strongly influence thermal training while low-confidence regions are suppressed. The confidence network is trained via negative log-likelihood with ground-truth depth when available.

### Mechanism 2: Cross-Modal Feature Consistency as Reliability Proxy
Cosine similarity between RGB and thermal feature embeddings serves as a proxy for RGB depth reliability. Features are extracted from the metric bins module's final layer for both modalities. After bilinear warping to align spatial coordinates, cosine distance $S_r = \langle F_r, f_{bilinear}(\hat{u}_rt, F_t)\rangle$ is computed. This similarity, along with its thermal-to-RGB counterpart, forms part of the metadata input to the confidence network. High feature similarity suggests the RGB model "sees" similar structure to thermal, increasing confidence.

### Mechanism 3: Sub-Pixel Spatial Alignment Without Co-Registration
Depth-based warping with bilinear interpolation enables distillation without perfectly co-registered RGB-Thermal pairs. Using predicted depth $\hat{D}_r$ and known camera intrinsics/extrinsics, RGB depth is warped to thermal image coordinates via $\hat{D}_{rt}, \hat{u}_{rt} = \pi(T_t^r \hat{D}_r \pi^{-1}(u_r, K_r), K_t)$. Bilinear interpolation samples warped depth at sub-pixel locations. The reverse transform produces $\breve{D}_{tr}$ for loss computation. This decouples distillation from strict hardware co-registration requirements.

## Foundational Learning

- **Knowledge Distillation**
  - Why needed here: The core strategy transfers learned priors from a large-scale RGB model (trained on 63.5M images) to a resource-constrained thermal domain. Understanding teacher-student architectures and loss weighting is essential.
  - Quick check question: Can you explain why stop-gradient ($sg(\cdot)$) is applied to the teacher's predictions in equation (6)?

- **Monocular Depth Estimation (MDE) Foundations**
  - Why needed here: The paper builds on DepthAnything with DPT decoder and metric bins. Familiarity with relative vs. metric depth, scale ambiguity, and SILOG loss is required to understand the training pipeline.
  - Quick check question: Why does SILOG loss (equation 7) use log-space differences rather than direct L1/L2 on depth values?

- **Cross-Modal Calibration and Projection**
  - Why needed here: The method requires understanding camera intrinsics ($K$), extrinsics ($T_t^r$), and projection functions ($\pi$) to warp depth between modalities. Without this, the spatial alignment mechanism is opaque.
  - Quick check question: Given a 3D point in RGB camera coordinates and transformation $T_t^r$, how would you project it into the thermal image plane?

## Architecture Onboarding

- **Component map**: RGB image -> DepthAnything (DinoV2 encoder + DPT decoder + metric bins) -> RGB depth $\hat{D}_r$ + features $F_r$ -> U-Net confidence network -> confidence $\hat{W}_r$ -> weighted consistency loss

- **Critical path**:
  1. Forward pass RGB image → RGB depth $\hat{D}_r$ + features $F_r$
  2. Forward pass thermal image → thermal depth $\hat{D}_t$ + features $F_t$
  3. Warp $\hat{D}_t$ to RGB frame → $\breve{D}_{tr}$ via equations (2)-(3)
  4. Compute metadata (6 components aligned to RGB frame)
  5. Confidence network → $\hat{W}_r$
  6. Compute losses: if GT depth available → $L_{silog}^r + L_{silog}^t + \alpha L_{cons} + \beta L_{nll} + \gamma L_{sm}$; if no GT → freeze RGB/confidence, train thermal with $L_{cons}$ only

- **Design tradeoffs**:
  - Confidence network complexity vs. distillation quality: Ablation (Table IV) shows removing confidence degrades results below no-distillation baseline; the 6-component metadata input is necessary.
  - Top-k filtering: Excluding top 20% residuals and using only top 80% feature similarity pixels trades coverage for robustness to outliers/occlusions.
  - Shared architecture vs. modality-specific: Using identical DepthAnything architecture for both modalities simplifies distillation but may not exploit thermal-specific features.

- **Failure signatures**:
  - Poor zero-shot indoor transfer: Models trained on outdoor MS² fail on ViViD++ indoor scenes despite fine-tuning—domain gap is too large (Section IV-G).
  - Degrading distillation without confidence: "No Conf." configuration produces RMSE 4.469, worse than "No Dist." at 4.359—blind distillation transfers RGB errors.
  - Thermal preprocessing issues: CLAHE provided no benefit; raw 16-bit normalization to 2nd/98th percentiles is sufficient.

- **First 3 experiments**:
  1. **Baseline comparison**: Train thermal-only DepthAnything on MS² training split; compare AbsRel, RMSE, and $\delta < 1.25$ against MonoTher-Depth to isolate distillation gains.
  2. **Confidence ablation**: Run three configurations—no distillation, distillation without confidence, full method—on MS² validation to reproduce Table IV and confirm confidence weighting is essential.
  3. **Zero-shot generalization test**: Load MS²-trained weights, evaluate on ViViD++ outdoor test (nighttime) without fine-tuning; compare RGB vs. thermal predictions to verify thermal advantage in adverse conditions (reproduce Table III top section).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the distillation framework be adapted to bridge the domain gap for indoor scenarios where the RGB teacher model produces poor initial predictions?
- Basis in paper: The authors state that their model struggles with indoor-to-outdoor generalization and that self-supervised fine-tuning fails to improve performance on the indoor ViViD++ scenarios due to poor RGB predictions.
- Why unresolved: The current confidence mechanism relies on the RGB teacher being somewhat reliable; if the teacher fails globally (as in indoor domains), the distillation signal degrades.
- What evidence would resolve it: Successful application of MonoTher-Depth to an indoor dataset (e.g., ViViD++ indoor split) showing improved metrics over the baseline, potentially via domain-adaptive confidence weighting.

### Open Question 2
- Question: To what extent does the performance depend on the specific architecture of the RGB teacher model (DepthAnything) versus the proposed distillation mechanism?
- Basis in paper: The paper utilizes DepthAnything as the teacher but does not ablate the choice of teacher model. It is unclear if the gains are specific to DepthAnything's priors or generalizable to other RGB foundation models.
- Why unresolved: Different RGB models may have different failure modes in adverse conditions (fog/smoke), which could alter the effectiveness of the confidence-aware distillation.
- What evidence would resolve it: A comparative study swapping the RGB teacher for other models (e.g., Metric3D, Marigold) while keeping the MonoTher-Depth distillation pipeline constant.

### Open Question 3
- Question: Is the confidence estimation robust to significant errors in the extrinsic calibration between the thermal and RGB cameras during the self-supervised fine-tuning phase?
- Basis in paper: The method assumes known camera extrinsics for sub-pixel warping (Eq. 1-3) to calculate feature consistency. While it claims to handle imperfect pairs, it relies on accurate geometric alignment for the metadata generation.
- Why unresolved: Errors in extrinsics could lead to misaligned feature maps and depth discrepancies, potentially causing the confidence network to incorrectly downweight valid teacher supervision.
- What evidence would resolve it: An analysis of performance degradation when synthetic noise is injected into the extrinsic transformation matrix $T_r^t$ during training.

## Limitations

- The method struggles with indoor-to-outdoor generalization due to large domain gaps between training and test environments
- Confidence network has limited capacity (4-layer U-Net) and may not capture complex confidence patterns across diverse environments
- Cross-modal feature consistency may fail when modalities capture fundamentally different information (e.g., thermal sees through fog while RGB sees only fog)

## Confidence

**High Confidence Claims**:
- The distillation framework with confidence weighting improves thermal depth estimation over baseline thermal-only training
- The method enables training without perfectly co-registered RGB-T pairs
- Thermal depth estimation provides advantages in adverse conditions like nighttime scenes

**Medium Confidence Claims**:
- Cross-modal feature consistency serves as a reliable proxy for RGB depth reliability
- The 6-component metadata input is optimal for confidence prediction
- Domain adaptation works for similar environments but fails for large domain gaps

**Low Confidence Claims**:
- Confidence network architecture (4-layer U-Net) is optimal
- CLAHE preprocessing provides no benefit
- Top-80% feature similarity filtering is optimal

## Next Checks

1. **Cross-Modal Consistency Under Adversarial Conditions**: Test the method on synthetic datasets where RGB and thermal modalities are deliberately misaligned or capture fundamentally different information (e.g., RGB sees fog while thermal sees through it). Measure whether the confidence network correctly identifies unreliable regions and whether distillation degrades performance.

2. **Confidence Network Architecture Sensitivity**: Systematically vary the confidence network architecture (depth, width, skip connections) and evaluate the impact on final thermal depth accuracy. Compare against simpler confidence estimation methods (e.g., direct L1/L2 error from RGB predictions) to validate the complexity of the U-Net approach.

3. **Zero-Shot Generalization Across Domain Gaps**: Beyond the reported outdoor-to-indoor failure, test the method's performance on intermediate domain gaps (e.g., different weather conditions, seasonal changes, urban-to-rural transitions). Quantify how confidence weighting performs compared to uniform distillation across these scenarios to validate the robustness of the reliability proxy.