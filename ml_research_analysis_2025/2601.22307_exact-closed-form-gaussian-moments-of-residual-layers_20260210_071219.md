---
ver: rpa2
title: Exact closed-form Gaussian moments of residual layers
arxiv_id: '2601.22307'
source_url: https://arxiv.org/abs/2601.22307
tags:
- unscented
- network
- activation
- weights
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper closes a long-standing gap by deriving exact closed-form\
  \ Gaussian moments for propagating uncertainty through neural network layers. The\
  \ authors achieve exact moment matching for probit, GeLU, ReLU, Heaviside, and sine\
  \ activation functions\u2014including residual connections\u2014by computing analytical\
  \ expressions for layer-wise mean and full covariance propagation."
---

# Exact closed-form Gaussian moments of residual layers

## Quick Facts
- arXiv ID: 2601.22307
- Source URL: https://arxiv.org/abs/2601.22307
- Reference count: 40
- Primary result: Exact closed-form Gaussian moment propagation through neural network layers

## Executive Summary
This paper solves the problem of propagating Gaussian uncertainty through neural network layers with exact closed-form expressions for mean and covariance. The authors derive analytical formulas for five activation functions (probit, GeLU, ReLU, Heaviside, sine) that enable precise moment matching at each layer. Their method achieves orders-of-magnitude improvements in KL divergence accuracy compared to linearization, mean-field, and unscented transforms, with up to millionfold better performance on random networks. The approach also delivers competitive results in practical applications like regression with input uncertainty and variational Bayesian inference.

## Method Summary
The method computes exact Gaussian moments by deriving closed-form expressions for three core functions (Mσ, Kσ, Lσ) that capture the mean, covariance between activations, and covariance between activation and linear term for each supported activation function. These are combined using a layer propagation formula that updates the mean and covariance matrix given layer weights and inputs. The network is processed layer-by-layer, with each layer's output approximated as a Gaussian with the computed moments. This chaining continues through all layers, maintaining full covariance matrices rather than assuming independence between neurons.

## Key Results
- Orders-of-magnitude KL divergence improvements (up to millionfold) over linearization, mean-field, and unscented transforms on random networks
- Competitive statistical calibration in regression with input uncertainty and variational Bayesian inference
- Theoretical error bounds derived using Wasserstein distance and second-order Poincaré inequalities
- Preliminary analysis of stochastic feedforward neurons with visualization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exact closed-form moment matching for specific activation functions is achievable by exploiting probabilistic representations and analytical properties of Gaussian integrals.
- Mechanism: For each supported activation function (probit, GeLU, ReLU, Heaviside, sine), the authors derive analytical expressions for the three core functions Mσ, Kσ, and Lσ that compute the mean, covariance between activations, and covariance between activation and linear term. For probit (Φ), they use an auxiliary standard Normal variable representation; for GeLU, they apply multivariate Stein's lemma and the Gaussian ODE ϕ′(x)+xϕ(x)=0; for ReLU and Heaviside, they take dominated limits of GeLU and probit respectively; for sine, they combine the characteristic function of the Normal distribution with trigonometric identities. These derivations avoid Riemann integrals by working with higher-level properties of Gaussian variables.
- Core assumption: The input to each layer is a multivariate Gaussian distribution (or can be approximated as such from the previous layer's moment-matched Gaussian).
- Evidence anchors:
  - [abstract]: "We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions"
  - [section 2.1 & appendices C–G]: Detailed derivations for each activation function, using probabilistic methods and properties like Stein's lemma and characteristic functions.
  - [corpus]: Related work on moment-matching and Gaussian inference (e.g., "EMPEROR: Efficient Moment-Preserving Representation of Distributions") supports the interest in moment-preserving representations, but does not provide these specific closed-form derivations.
- Break condition: If an activation function lacks such probabilistic structure or cannot be expressed as a limit/combination of the supported functions, the exact closed-form derivation does not apply.

### Mechanism 2
- Claim: Layer-by-layer Gaussian approximation with exact per-layer moments yields orders-of-magnitude better KL divergence compared to linearization, mean-field, or unscented transforms.
- Mechanism: The method chains the exact per-layer moment formulas through multiple layers, re-approximating the output of each layer as a Gaussian with the computed mean and covariance. Because each layer's moments are exact (for the supported activations and Gaussian input), the accumulated error stems only from the Gaussian approximation between layers, not from per-layer moment approximation. This contrasts with methods that approximate moments at each layer (linearization, unscented) or assume independence (mean-field), which introduce per-layer errors that compound.
- Core assumption: The layer-by-layer Gaussian approximation is reasonable; i.e., the output of a nonlinear layer given Gaussian input is not too non-Gaussian, or the network is not too deep/nonlinear in a way that strongly violates this.
- Evidence anchors:
  - [abstract]: "On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives."
  - [section 5.1 & appendix M]: Systematic evaluation across 114 test cases showing KL divergence reductions of 100× to 1,000,000× compared to baselines.
  - [corpus]: "Rethinking Approximate Gaussian Inference in Classification" discusses limitations of approximate Gaussian methods in classification, indirectly supporting the need for more accurate moment propagation.
- Break condition: If the network architecture or activation creates strongly non-Gaussian outputs (e.g., the adversarial Heaviside example in section 4 where the output is degenerate), the layer-by-layer Gaussian approximation can fail arbitrarily.

### Mechanism 3
- Claim: Theoretical error bounds can be derived using Wasserstein distance and second-order Poincaré inequalities, attributing error to Lipschitz constants of layers and non-normality induced by nonlinearity.
- Mechanism: The authors use a recursive triangle inequality to bound the Wasserstein distance between the true distribution Y0 and the moment-matched approximation Yana. The error at layer k is bounded by the product of the Lipschitz constant of layer k and the error at layer k-1, plus a term capturing the "non-normality" of the layer given Gaussian input. This non-normality term is bounded using a second-order Poincaré inequality, which depends on the second derivatives of the activation function.
- Core assumption: The activation functions have bounded first and second derivatives (or can be approximated by smooth functions for analysis), and the covariance matrices are well-conditioned.
- Evidence anchors:
  - [section 3 & appendix H]: Theoretical derivation of the error bound using Wasserstein distance and second-order Poincaré inequality.
  - [abstract]: "We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons."
  - [corpus]: "Relative Wasserstein Angle and the Problem of the W₂-Nearest Gaussian Distribution" is relevant to quantifying Gaussian approximations via Wasserstein geometry.
- Break condition: If the activation has unbounded second derivatives (e.g., ReLU at 0, though handled via limit), or if the network has degenerate covariances, the bounds may be loose or not applicable.

## Foundational Learning

- Concept: **Gaussian (Normal) Distribution and Its Moments**
  - Why needed here: The entire method propagates Gaussian distributions through layers by matching first and second moments (mean and covariance). Understanding multivariate Gaussians, covariance matrices, and how nonlinear transformations affect these moments is essential.
  - Quick check question: Given a random vector X ∼ N(μ, Σ) and a function Y = f(X), can you explain why E[Y] and Cov(Y) are generally not equal to f(μ) and ∇f(μ)Σ∇f(μ)ᵀ unless f is linear?

- Concept: **Moment Matching and KL Divergence**
  - Why needed here: The method is evaluated by comparing the KL divergence between the approximated Gaussian and the pseudo-true Gaussian (or true distribution). Moment matching here refers to choosing a Gaussian that has the same first two moments as the true distribution.
  - Quick check question: Why is KL divergence a natural metric for comparing a Gaussian approximation to a true distribution, and what does it mean for the approximation to have lower KL divergence?

- Concept: **Activation Functions and Their Properties**
  - Why needed here: The derivations exploit specific properties of probit (Φ), GeLU, ReLU, Heaviside, and sine functions. Understanding these functions, their derivatives, and their behavior under Gaussian expectations is crucial.
  - Quick check question: For the GeLU function σ(x)=xΦ(x), what is σ′(x), and why does the Gaussian ODE ϕ′(x)+xϕ(x)=0 appear in the moment derivations?

## Architecture Onboarding

- Component map: Core Moment Functions (Mσ, Kσ, Lσ) -> Layer Propagation -> Network Propagation -> Error Analysis Module -> Baseline Comparators
- Critical path:
  1. Implement the three core moment functions Mσ, Kσ, Lσ for each supported activation. This is the most mathematically intensive step; start with probit or sine which have relatively simple forms.
  2. Implement layer propagation using Lemma 2.4, carefully handling matrix operations for νij, τij, κij.
  3. Chain layers for network propagation, ensuring numerical stability (e.g., avoiding cancellation in bivariate normal CDF differences).
  4. Validate on single-layer networks with known analytical results before testing deep networks.
- Design tradeoffs:
  - **Activation support vs. generality**: Only five activation functions are supported; extending to others (e.g., logistic sigmoid) may require approximations.
  - **Exactness vs. computational cost**: Computing full covariances is O(n³) in hidden dimension due to matrix multiplications, though still faster than Monte Carlo for high dimensions.
  - **Numerical stability**: The bivariate normal CDF Φ₂ and its differences require careful quadrature (e.g., 10-point Gaussian quadrature on the correlation integral) to avoid cancellation errors for extreme arguments.
- Failure signatures:
  - **Mean-field failure**: If off-diagonal covariances are forced to zero, the method degenerates to mean-field, leading to underdispersion in correlated neuron outputs (see Example 3 in section 4).
  - **Linearization failure**: For large input variance, linearization gives arbitrarily wrong variance (see Example 1 with sin activation).
  - **Deep network non-normality**: In adversarial cases like Example 4 with Heaviside, the Gaussian approximation can be arbitrarily wrong if the network produces degenerate outputs.
- First 3 experiments:
  1. **Single-layer validation**: Propagate a Gaussian through a single layer with probit activation and compare mean/covariance to numerical integration (e.g., scipy.integrate). Verify exactness to machine precision.
  2. **Random network KL comparison**: Reproduce a subset of the experiments in section 5.1: generate a random 3-layer network with GeLU activation, input N(0, I), and compare KL divergence of the analytic method vs. linearization/mean-field against Monte Carlo ground truth.
  3. **Regression with input uncertainty**: Apply the method to the California Housing regression task (section 5.2) with noisy inputs. Compare coverage and interval width to the "certain" baseline that ignores input uncertainty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the preliminary analysis of stochastic feedforward neurons be expanded into a robust methodological framework for uncertainty propagation?
- Basis in paper: [explicit] Section 5.5 concludes by asking "Whether this line of inquiry deserves further methodological development we reserve for future work."
- Why unresolved: The paper provides only a "preliminary analysis" and a single empirical visualization (Fig 5.5) for stochastic activations, without rigorous derivation or broad testing.
- What evidence would resolve it: A theoretical derivation of moment propagation for the stochastic activation σ̃(x, U) and benchmarks comparing it to Monte Carlo sampling in deep networks.

### Open Question 2
- Question: Is it possible to derive exact closed-form Gaussian moments for the logistic sigmoid activation function?
- Basis in paper: [explicit] The paper states, "While we do not have exact integrals for the logistic sigmoid function, we do for Φ, which is a similarly shaped function."
- Why unresolved: The authors provide exact solutions for probit, GeLU, ReLU, Heaviside, and sine, but explicitly omit the standard logistic sigmoid due to mathematical difficulty.
- What evidence would resolve it: Analytical expressions for the functions Mσ, Kσ, Lσ specific to the logistic sigmoid, or a proof that such closed forms do not exist.

### Open Question 3
- Question: Can tighter theoretical error bounds be established for the Wasserstein distance between the true distribution and the Gaussian approximation?
- Basis in paper: [inferred] Appendix H derives a theoretical bound on the Wasserstein distance d_W(Y_0, Y_ana) but explicitly notes that "Even though this bound is loose, it lends attribution to the sources of error."
- Why unresolved: The existing bound relies on "crude approximation" involving supremum norms of activation derivatives, which may vastly overestimate the empirical error observed in the paper's experiments.
- What evidence would resolve it: A refined theorem reducing the dependence on the Lipschitz constants or covariance norms in the error bound, or tight bounds on the "non-normality" term.

## Limitations

- Only five activation functions are supported; extending to others requires finding similar probabilistic representations or accepting approximations
- Numerical stability challenges with bivariate normal CDF evaluations for extreme arguments despite proposed quadrature
- Deep network Gaussian approximation can break down arbitrarily for specific architectures or strongly non-Gaussian outputs

## Confidence

- **High confidence**: The closed-form derivations for the five supported activation functions are mathematically rigorous and can be verified independently. The layer-wise propagation formula (Lemma 2.4) is a straightforward application of moment calculations.
- **Medium confidence**: The claimed KL divergence improvements (up to millionfold) are based on systematic experiments across 114 test cases, but the exact numerical gains depend on implementation details of the numerical integration and the specific random seeds used.
- **Medium confidence**: The theoretical error bounds using Wasserstein distance and second-order Poincaré inequalities are derived under assumptions about bounded derivatives and well-conditioned covariances, which may not always hold in practice.

## Next Checks

1. **Numerical precision test**: Implement the core moment functions and verify that the output mean and covariance match high-precision numerical integration (e.g., using mpmath) for single-layer networks with extreme input variances.
2. **Activation extension test**: Attempt to derive the moment functions for a sixth activation (e.g., logistic sigmoid) using the same probabilistic techniques, and quantify the approximation error compared to numerical integration.
3. **Deep network stability test**: Evaluate the method on a deep network (e.g., 20+ layers) with GeLU activation and monitor for numerical instability, positive semi-definiteness violations, or KL divergence degradation as depth increases.