---
ver: rpa2
title: Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and
  LIME
arxiv_id: '2506.10154'
source_url: https://arxiv.org/abs/2506.10154
tags:
- text
- dataset
- emotions
- bangla
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines emotion analysis in Bangla social media comments
  using machine learning models including Linear SVM, KNN, Random Forest, and BiLSTM
  with TF-IDF n-gram features. The research explores PCA for dimensionality reduction
  and AdaBoost to enhance Decision Trees, alongside LIME for model interpretability.
---

# Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME

## Quick Facts
- arXiv ID: 2506.10154
- Source URL: https://arxiv.org/abs/2506.10154
- Reference count: 22
- Best performance: Decision Tree + AdaBoost with unigram TF-IDF (F1-score: 0.786)

## Executive Summary
This study examines emotion analysis in Bangla social media comments using machine learning models including Linear SVM, KNN, Random Forest, and BiLSTM with TF-IDF n-gram features. The research explores PCA for dimensionality reduction and AdaBoost to enhance Decision Trees, alongside LIME for model interpretability. Using 22,698 comments from the EmoNoBa dataset across 12 domains, the study achieves best performance with Decision Tree and AdaBoost (F1-score: 0.786), outperforming other models like Linear SVM (0.63) and BiLSTM (0.387). PCA generally reduced model performance. The work provides benchmarks for emotion detection in Bangla, highlighting challenges in low-resource language processing and suggesting future improvements using advanced models like BanglaBERT.

## Method Summary
The study preprocesses Bangla social media comments by removing punctuation, emojis, and collapsing whitespace, then applies TF-IDF vectorization (unigram/bigram/trigram) with optional PCA dimensionality reduction. Multiple classifiers are evaluated: Linear SVM, KNN, Random Forest, Decision Tree with AdaBoost (achieving best F1=0.786), and BiLSTM with Word2Vec embeddings (F1=0.387). The EmoNoBa dataset (22,698 samples, 6 emotion classes) is split 80/5/15 for training/validation/testing with stratified sampling. LIME is used for interpretability on AdaBoost predictions, and evaluation focuses on macro F1-score to address class imbalance.

## Key Results
- Decision Tree with AdaBoost achieves highest F1-score of 0.786 using unigram TF-IDF features
- PCA consistently degrades performance across all models (SVM unigram drops from 0.63 to 0.56)
- BiLSTM with Word2Vec embeddings performs worst (F1=0.387), significantly underperforming traditional ML models
- Unigram features outperform bigrams and trigrams for this task and dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdaBoost with Decision Tree achieves superior emotion classification by sequentially focusing on misclassified samples.
- Mechanism: AdaBoost iteratively reweights training instances—amplifying weights on hard-to-classify examples—then combines weak learners into a stronger ensemble, reducing variance and bias in emotion prediction boundaries.
- Core assumption: Hard instances contain learnable patterns rather than label noise; emotion categories in Bangla have separable decision boundaries that sequential correction can refine.
- Evidence anchors:
  - [abstract] "utilized a BiLSTM model and AdaBoost to improve decision trees... achieves best performance with Decision Tree and AdaBoost (F1-score: 0.786)"
  - [section] Table 3 shows AdaBoost improved macro F1 from 0.7799 to 0.7860 over base Decision Tree
  - [corpus] Corpus neighbors show transformer-based approaches (BanglaBERT, BERTurk) achieving similar or higher F1-scores (0.73-0.83), suggesting ensemble methods are competitive but not uniquely optimal
- Break condition: If training data contains high label noise (>15% annotation errors), AdaBoost may overfit noisy samples and degrade generalization.

### Mechanism 2
- Claim: Unigram TF-IDF features preserve discriminative emotion signals that higher-order n-grams and PCA dimensionality reduction tend to obscure.
- Mechanism: Unigram TF-IDF captures term importance without assuming word-order dependencies; bigrams/trigrams introduce sparsity and dilute signal for short social media comments; PCA's variance-preserving projection may discard emotion-specific low-variance features.
- Core assumption: Bangla emotion expression relies more on individual emotion words than syntactic patterns; short comment lengths (mean 11.7 words) limit bigram/trigram utility.
- Evidence anchors:
  - [abstract] "Linear SVM, KNN, and Random Forest with n-gram data from a TF-IDF vectorizer"
  - [section] Table 2 shows Linear SVM unigram F1=0.63 vs bigram F1=0.57 vs trigram F1=0.49; PCA reduced all model performance (e.g., SVM unigram dropped from 0.63 to 0.56)
  - [corpus] Corpus evidence on n-gram vs embedding tradeoffs is weak—neighbor papers focus on transformer embeddings rather than n-gram comparisons
- Break condition: For longer, syntactically complex texts (>50 words), unigrams lose discriminative power; PCA may help when feature dimensions exceed 10,000 with strong multicollinearity.

### Mechanism 3
- Claim: BiLSTM with Word2Vec embeddings underperforms due to insufficient training data and embedding quality for low-resource Bangla emotion semantics.
- Mechanism: BiLSTM models sequential context bidirectionally but require dense, high-quality word embeddings and large labeled datasets; Word2Vec trained on limited Bangla corpora may lack emotion-relevant semantic relationships.
- Core assumption: Pretrained Word2Vec embeddings capture insufficient emotion semantics; dataset size (22,698 samples) is inadequate for deep learning convergence without transfer learning.
- Evidence anchors:
  - [abstract] "BiLSTM with TF-IDF n-gram features... BiLSTM (0.387)"
  - [section] Table 4 shows BiLSTM F1=0.3869, worst among all models; Table 5 confirms it underperformed traditional methods
  - [corpus] Neighbor papers show BanglaBERT achieving F1=0.73-0.83 on same dataset, indicating pretrained transformers overcome low-resource limitations
- Break condition: With pretrained language model embeddings (BanglaBERT, mBERT) and 50,000+ labeled samples, BiLSTM or transformer architectures should outperform traditional ML.

## Foundational Learning

- Concept: TF-IDF Vectorization
  - Why needed here: Converts Bangla text into numerical features reflecting word importance across documents; foundation for all SVM, KNN, Random Forest experiments.
  - Quick check question: Given a comment "আমি খুব খুশি" appearing in 100 documents out of 10,000, and "খুশি" appearing in 500 documents, can you compute the TF-IDF weight for "খুশি"?

- Concept: Ensemble Boosting (AdaBoost)
  - Why needed here: Core mechanism for best-performing model; must understand sample reweighting and weak learner combination to interpret results.
  - Quick check question: After round 1, a sample has weight 0.3 and is misclassified by weak learner 1 with error rate 0.25. What happens to its weight in round 2?

- Concept: LIME (Local Interpretable Model-agnostic Explanations)
  - Why needed here: Paper uses LIME to explain AdaBoost predictions; essential for validating that model attends to emotion-relevant words.
  - Quick check question: LIME creates a local surrogate model around a single prediction. Why must this surrogate be simple (linear) rather than complex?

## Architecture Onboarding

- Component map: Preprocessing -> TF-IDF vectorization -> Classification -> LIME interpretation
- Critical path:
  1. Load EmoNoBa dataset (22,698 samples, 6 emotion classes)
  2. Preprocess text (punctuation/emoji removal)
  3. TF-IDF unigram vectorization (no PCA based on results)
  4. Train Decision Tree with AdaBoost (max_depth, n_estimators hyperparameters)
  5. Apply LIME to misclassified or low-confidence predictions
  6. Evaluate with macro F1-score (addresses class imbalance)

- Design tradeoffs:
  - Simplicity vs. performance: Decision Tree+AdaBoost (F1=0.786) outperforms BiLSTM (F1=0.387) with faster training
  - Interpretability vs. accuracy: LIME adds explainability but computational overhead per prediction
  - Feature complexity: Unigrams sacrifice context but improve generalization; bigrams/trigrams add sparsity without gains
  - Dimensionality: PCA reduces memory but degrades F1 by 7-11% across models

- Failure signatures:
  - BiLSTM F1 < 0.45: Check embedding quality, sequence length distribution, class balance
  - PCA consistently degrades performance: Feature variance distribution is emotion-relevant; avoid dimensionality reduction
  - Large gap between macro and micro F1: Class imbalance affecting minority emotions (Fear, Surprise have fewest samples)
  - LIME explanations show non-emotion words driving predictions: Feature extraction may be capturing dataset artifacts

- First 3 experiments:
  1. Replicate best configuration (Decision Tree + AdaBoost + unigram TF-IDF, no PCA) on EmoNoBa; validate F1 ≈ 0.78-0.79 with stratified 80/15/5 split.
  2. Ablation study: Compare unigram-only vs. unigram+bigram vs. unigram+bigram+trigram with same classifier to confirm n-gram degradation pattern.
  3. LIME analysis on 50 random misclassifications: Identify top 3 words per prediction; check if model attends to emotion-relevant Bangla terms vs. noise (usernames, numbers).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transformer-based language models like BanglaBERT significantly outperform traditional ML models for emotion detection in Bangla social media comments?
- Basis in paper: [explicit] The authors explicitly state in Section 6: "We intend to test more recent technologies, such as BERT or GPT, which may be more adept at comprehending context." They also note BanglaBERT achieved 0.8273 F1-score versus their best of 0.7860.
- Why unresolved: The authors did not implement or evaluate transformer-based architectures; their comparison relies only on reported results from prior work using the same dataset.
- What evidence would resolve it: A controlled experiment training and evaluating BanglaBERT or similar transformer models using identical preprocessing, splits, and metrics as this study.

### Open Question 2
- Question: How does expanding the data source beyond social media comments affect emotion classification performance for Bangla?
- Basis in paper: [explicit] The authors acknowledge in Section 6 that "only social media was used to collect the data, which does not adequately represent how Bangla is used in everyday life or in various contexts."
- Why unresolved: The study is restricted to the EmoNoBa dataset, sourced primarily from YouTube, Facebook, and Twitter.
- What evidence would resolve it: Experiments using a more diverse corpus including news articles, literature, and conversational transcripts, evaluated with the same classification pipeline.

### Open Question 3
- Question: Would balancing the dataset across emotion categories improve classifier performance on underrepresented emotions like Fear and Surprise?
- Basis in paper: [explicit] The authors state in Section 6: "In order to fairly represent all emotions, we also want to balance the data."
- Why unresolved: The dataset shows significant class imbalance (Joy highest, Fear lowest), but the authors did not apply resampling or reweighting techniques.
- What evidence would resolve it: A comparative study using oversampling, undersampling, or class-weighted loss on the current dataset and reporting per-class F1-scores.

### Open Question 4
- Question: What architectural or hyperparameter factors caused BiLSTM to underperform traditional models so substantially (F1-score of 0.3869 vs 0.7860)?
- Basis in paper: [inferred] The BiLSTM model performed dramatically worse than simpler models, but the paper provides no ablation study, hyperparameter analysis, or error diagnostics for this outcome.
- Why unresolved: The deep learning approach was tested with Word2Vec embeddings and a single architecture, without exploring alternatives or analyzing failure modes.
- What evidence would resolve it: Systematic experiments varying BiLSTM hyperparameters, embedding types, and training configurations, along with error analysis on misclassified instances.

## Limitations
- BiLSTM performance issues not fully explained; implementation details missing for deep learning approach
- Dataset size (22,698 samples) may limit generalizability and model convergence for complex architectures
- No evaluation of transformer-based models like BanglaBERT which show comparable or better performance in literature
- Domain bias from social media sources only; may not represent broader Bangla language usage

## Confidence

- **High Confidence**: Unigram TF-IDF outperforming higher-order n-grams and PCA's negative impact on model performance - supported by direct experimental comparisons in Tables 2 and 3.
- **Medium Confidence**: AdaBoost with Decision Tree being the optimal model - while results show F1=0.786, hyperparameter optimization and comparison with transformer baselines (BanglaBERT) could yield different conclusions.
- **Low Confidence**: BiLSTM underperability attributed solely to low-resource constraints - insufficient details on implementation prevent distinguishing between architecture choice, embedding quality, and training methodology issues.

## Next Checks
1. Replicate the best configuration (Decision Tree + AdaBoost + unigram TF-IDF) using the exact EmoNoBa dataset split and verify macro F1-score falls within the reported 0.78-0.79 range.

2. Compare AdaBoost Decision Tree against BanglaBERT on the same dataset using identical train/validation/test splits to determine if transformer approaches can match or exceed the reported F1=0.786.

3. Perform LIME analysis on 50 randomly selected misclassifications from the best model to verify that the top contributing words are emotion-relevant Bangla terms rather than dataset artifacts or noise features.