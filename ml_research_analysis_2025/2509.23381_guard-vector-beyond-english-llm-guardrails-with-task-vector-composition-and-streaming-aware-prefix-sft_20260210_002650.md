---
ver: rpa2
title: 'Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and
  Streaming-Aware Prefix SFT'
arxiv_id: '2509.23381'
source_url: https://arxiv.org/abs/2509.23381
tags:
- guard
- prefix
- evaluation
- streaming
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Guard Vector is a task-vector composition method that transfers
  safety behaviors from a guardrail model to a target language model without additional
  training or target language labels. It computes the parameter difference between
  a guardrail model and a same-architecture pretrained model, then composes this vector
  with a continual pretraining model in the target language to obtain a Target Guard
  Model (TGM).
---

# Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT

## Quick Facts
- **arXiv ID:** 2509.23381
- **Source URL:** https://arxiv.org/abs/2509.23381
- **Reference count:** 40
- **Primary result:** Enables cross-lingual safety transfer without additional training or target language labels

## Executive Summary
Guard Vector introduces a novel approach for transferring safety behaviors from English guardrail models to target language models through task-vector composition. The method computes parameter differences between guardrail and pretrained models, then composes these vectors with target language models to create Target Guard Models (TGMs). This approach achieves significant improvements in classification quality across standard safety benchmarks while reducing data and compute requirements. The method also introduces streaming-aware prefix SFT, which aligns behavior between streaming and full-text inputs, reducing latency through single-token classification while maintaining classification quality.

## Method Summary
Guard Vector works by extracting safety behaviors from a well-trained guardrail model through parameter difference computation. The method takes a pretrained model and its corresponding guardrail model, calculates the parameter difference vector that captures the safety behaviors, and then composes this vector with a continually pretrained target language model. This composition produces a Target Guard Model that inherits the safety behaviors without requiring additional safety training or target language safety labels. The streaming-aware prefix SFT component further enhances the method by training on streaming datasets to align the model's behavior between streaming (prefix) and full-text inputs, ensuring consistent performance in real-time applications.

## Key Results
- Achieves improved classification quality over established guardrails across standard safety suites
- Enables language extensibility to Chinese, Japanese, and Korean without additional training
- Demonstrates model portability across Llama and Gemma architectures
- Reduces latency through single-token classification while preserving streaming performance

## Why This Works (Mechanism)
The method leverages parameter difference vectors to capture and transfer safety behaviors from source to target models. By computing the difference between a guardrail model and its corresponding pretrained model, Guard Vector isolates the parameters that encode safety behaviors. This difference vector, when composed with a target language model that has undergone continual pretraining, effectively transfers these behaviors to the new language context. The streaming-aware prefix SFT component addresses the unique challenges of streaming inputs by ensuring consistent behavior between partial and complete text sequences.

## Foundational Learning
- **Parameter Difference Computation:** Calculates the vector representing safety behaviors learned during guardrail training - needed to isolate safety-specific parameters, quick check: verify parameter difference magnitude and distribution
- **Vector Composition:** Combines parameter difference vectors with target language models - needed to transfer safety behaviors, quick check: validate composition preserves base capabilities
- **Continual Pretraining:** Adapts pretrained models to target languages before safety transfer - needed to establish language-specific foundations, quick check: measure language alignment metrics
- **Streaming-Aware Training:** Aligns model behavior between streaming and full-text inputs - needed for real-time applications, quick check: compare prefix vs full-text outputs
- **Cross-Architecture Portability:** Transfers safety behaviors across different model architectures - needed for broad applicability, quick check: test on multiple architecture families

## Architecture Onboarding
**Component Map:** Pretrained Model -> Guardrail Model -> Parameter Difference Vector -> Target Language Model -> Target Guard Model -> Streaming-Aware Prefix SFT

**Critical Path:** The essential sequence involves computing the parameter difference between the guardrail and pretrained models, composing this vector with the target language model, and applying streaming-aware prefix SFT for real-time applications.

**Design Tradeoffs:** The method trades potential precision in safety behavior transfer for significantly reduced data and compute requirements. By leveraging existing guardrail models rather than training new safety classifiers, it achieves cross-lingual safety transfer without the need for extensive target language safety datasets.

**Failure Signatures:** The approach may fail when the source guardrail model's safety behaviors don't generalize well to the target language or cultural context. Poor performance can also occur if the parameter difference vector captures too much noise or if the continual pretraining of the target language model diverges significantly from the source model's domain.

**First 3 Experiments:**
1. Verify parameter difference computation between pretrained and guardrail models on a held-out safety dataset
2. Test vector composition with a simple target language model on bilingual safety classification
3. Evaluate streaming-aware prefix SFT impact on single-token classification latency and accuracy

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Requires a well-trained guardrail model for the target task, cannot bootstrap safety from scratch
- Effectiveness depends heavily on the quality and relevance of the source guardrail model
- Assumes parameter differences can be meaningfully composed across different training regimes

## Confidence
**Major Claim Clusters:**
- Guard Vector enables task-specific safety behaviors in target languages without additional training: High
- Streaming-aware prefix SFT preserves classification quality under streaming conditions: Medium
- Model portability across architectures (Llama and Gemma): Low

## Next Checks
1. Test Guard Vector method across a broader range of safety domains to assess generalizability of parameter difference transfer
2. Evaluate streaming-aware prefix SFT with diverse streaming patterns and input lengths for robustness in real-world deployment
3. Conduct ablation studies to quantify contributions of task-vector composition versus prefix SFT components separately