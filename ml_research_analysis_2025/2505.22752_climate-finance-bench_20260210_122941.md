---
ver: rpa2
title: Climate Finance Bench
arxiv_id: '2505.22752'
source_url: https://arxiv.org/abs/2505.22752
tags:
- retrieval
- question
- answer
- page
- climate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Climate Finance Bench introduces a benchmark dataset of 330 expert-annotated\
  \ question-answer pairs over 33 corporate sustainability reports, covering extraction,\
  \ numerical, and logical reasoning tasks. It compares RAG pipelines using minimal\
  \ dense retrieval, hybrid dense+BM25+reranking, and multiple LLMs, finding that\
  \ retrieval quality\u2014not model size\u2014is the primary bottleneck, with best\
  \ configurations achieving 62% accuracy."
---

# Climate Finance Bench

## Quick Facts
- **arXiv ID**: 2505.22752
- **Source URL**: https://arxiv.org/abs/2505.22752
- **Reference count**: 40
- **Primary result**: Hybrid retrieval (dense+BM25+reranking) achieves 62% accuracy on expert-annotated climate finance QA; quantization reduces emissions by ~75% with negligible accuracy loss.

## Executive Summary
Climate Finance Bench introduces a benchmark dataset of 330 expert-annotated question-answer pairs over 33 corporate sustainability reports, covering extraction, numerical, and logical reasoning tasks. It compares RAG pipelines using minimal dense retrieval, hybrid dense+BM25+reranking, and multiple LLMs, finding that retrieval quality—not model size—is the primary bottleneck, with best configurations achieving 62% accuracy. The study highlights environmental impact by quantifying per-query GHG emissions, showing that quantized local models reduce emissions by ~75% with negligible accuracy loss. This open dataset enables reproducible, resource-efficient climate-finance QA research.

## Method Summary
The benchmark evaluates RAG pipelines on 33 English sustainability reports using 330 expert-annotated QA pairs. Documents are chunked (2048 tokens, 10% overlap) and indexed with dense embeddings. Retrieval combines dense vector search, BM25 lexical matching, and cross-encoder reranking to select top-12 context chunks. Multiple LLMs (Claude 3.5, GPT-4o, quantized Llama 3.1-8B) generate answers, with a separate LLM-as-a-Judge scoring outputs on a 3-point scale. Environmental impact is measured via GHG emissions per query.

## Key Results
- Hybrid retrieval (dense+BM25+reranking) achieves 62.1% accuracy vs. 54.8% for minimal dense-only retrieval
- Proprietary models (GPT-4o, Claude) outperform quantized local models by ~10 percentage points
- 4-bit quantization reduces emissions by ~75% with negligible accuracy loss on tested models

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Retrieval Synergy
Combining dense vector retrieval with sparse lexical matching (BM25) and reranking improves answer accuracy for climate finance QA tasks. Dense retrieval captures semantic similarity (e.g., paraphrased concepts like "emissions reduction" vs "decarbonization"), while BM25 excels at exact matches for numeric values, units, and specific terminology often critical in sustainability reports. A cross-encoder reranker then jointly evaluates query-passage pairs to refine the top results before generation.

### Mechanism 2: Retrieval as the Performance Bottleneck
The primary factor limiting accuracy in RAG systems for this domain is the retriever's ability to surface the correct evidence, not the generative LLM's size or reasoning capability. Generative LLMs are conditioned only on the context provided by the retriever. If the retriever fails to fetch the passage containing the answer (a "retrieval miss"), the LLM cannot synthesize a correct response, regardless of its parametric knowledge or reasoning power.

### Mechanism 3: Quantization for Resource Efficiency
Applying 4-bit weight quantization to local LLMs significantly reduces computational resources (memory, energy) and associated carbon emissions with negligible impact on answer accuracy. Quantization reduces the precision of model weights (e.g., from 16-bit floating point to 4-bit integers), which lowers GPU memory requirements and accelerates inference.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: This is the core system architecture evaluated. Understanding how retrieval and generation are coupled is essential to grasp the results and limitations.
  - Quick check question: What are the two main stages of a RAG pipeline, and what does the LLM use as context for generation?

- **Concept: BM25 (Lexical Search) & Dense Retrieval**
  - Why needed here: The paper's key finding is that a *hybrid* retrieval approach is superior. You must understand the difference between keyword-based (BM25) and semantic (dense) search to appreciate why.
  - Quick check question: Which retrieval method is better at finding an exact number like "42.5 tCO2e", and which is better at finding the concept of "carbon footprint"?

- **Concept: Weight Quantization**
  - Why needed here: The study emphasizes environmental impact. You need to understand what quantization is to interpret the 75% emissions reduction claim and the trade-off with accuracy.
  - Quick check question: How does reducing the numerical precision of model weights affect the computational cost of running an LLM?

## Architecture Onboarding

- **Component map**: PDF Extraction & Chunking -> Vector Store & Index -> Retriever (Hybrid) -> Generator (LLM)
- **Critical path**: Document Ingestion: PDF → Cleaned/Merged Chunks → Retrieval: User Query → Hybrid Retrieval → Reranking → Top-12 Context Chunks → Generation: Context + Query → LLM → Final Answer
- **Design tradeoffs**:
  - Minimal vs. Hybrid Retrieval: Minimal (dense-only) is simpler but less accurate. Hybrid+reranking is more complex but yields higher accuracy.
  - Proprietary vs. Local/Quantized Models: Proprietary models offer higher performance but with higher cost and opaque carbon footprint. Local, quantized models offer dramatic cost and emissions savings with only a minor performance hit.
  - Chunking Strategy: Larger chunks (2048 tokens) maintain context but may include noise. Smaller chunks increase precision but risk splitting relevant information. The 10% overlap mitigates this.
- **Failure signatures**:
  - Retrieval Miss: The LLM answers "Not available in the retrieved information" or hallucinates because the correct passage was not in the top-12 chunks.
  - Parsing Noise: Using Docling for HTML conversion introduced noise, degrading performance.
  - Logical Reasoning Failure: The retriever finds individual facts but the LLM fails to correctly synthesize them for multi-hop logical questions.
- **First 3 experiments**:
  1. Establish a Minimal Baseline: Run the provided 330 questions using a minimal RAG pipeline (dense-only retrieval, simple LLM like Llama 3.1 8B) to get a baseline accuracy score.
  2. Ablate the Retrieval Pipeline: Incrementally add BM25, then the cross-encoder reranker to the retrieval step. Measure the change in accuracy at each step.
  3. Compare Quantized vs. Full-Precision Models: Select a local model (e.g., Llama 3.1 8B) and run inference in both full-precision and 4-bit quantized modes. Compare both the accuracy scores and the measured GHG emissions or GPU memory usage.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can type-aware or task-adaptive retrieval methods (e.g., Typed-RAG) break the 62–65% accuracy ceiling observed with hybrid retrieval?
- **Open Question 2**: Would GraphRAG improve performance on multi-hop logical reasoning questions that currently lag behind extraction and numerical tasks?
- **Open Question 3**: What post-processing techniques would enable Docling's structure-preserving conversion to improve—rather than degrade—retrieval performance?
- **Open Question 4**: How robust are current RAG pipelines to emerging-market, SME, and multilingual climate disclosures?

## Limitations

- The dataset focuses on large-cap firms from Europe and North America, limiting generalizability to emerging markets, SMEs, and non-English reports
- The benchmark emphasizes extraction and retrieval tasks, potentially underrepresenting complex reasoning scenarios
- Environmental impact analysis relies on reported estimates rather than direct measurements
- The LLM-as-a-Judge evaluation introduces subjectivity and potential bias

## Confidence

- **High Confidence**: Hybrid retrieval architecture demonstrably improves accuracy; quantization provides substantial emissions reduction with negligible accuracy loss; proprietary models outperform quantized local models
- **Medium Confidence**: Retrieval quality is the primary bottleneck rather than model size; 62% accuracy represents a meaningful performance ceiling; environmental impact quantification provides reasonable estimates
- **Low Confidence**: The specific cross-encoder model used for reranking is critical but unspecified; LLM-as-a-Judge evaluation would yield identical results if repeated; results generalize to sustainability reports from companies outside the tested GICS sectors

## Next Checks

1. **Retrieval Model Ablation**: Systematically test alternative cross-encoder reranker models (e.g., ms-marco-MiniLM, all-distilroberta-v1) and dense embedding models (e.g., bge-large, nomic-embed-text) to quantify their impact on the 62% accuracy ceiling and verify retrieval is truly the bottleneck.

2. **Document Source Variability**: Validate benchmark performance on sustainability reports from companies in different GICS sectors not represented in the original 33 (e.g., technology, healthcare) and reports following different disclosure frameworks (SASB vs. TCFD) to test generalizability.

3. **Extended Reasoning Evaluation**: Create and test a subset of multi-hop logical reasoning questions requiring synthesis across non-contiguous passages to determine if retrieval remains the bottleneck when tasks exceed simple extraction or single-passage reasoning.