---
ver: rpa2
title: 'YourBench: Easy Custom Evaluation Sets for Everyone'
arxiv_id: '2504.01833'
source_url: https://arxiv.org/abs/2504.01833
tags:
- question
- questions
- document
- answer
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'YourBench is a framework for automated, document-grounded LLM
  evaluation that addresses benchmark saturation and contamination by generating custom
  evaluation sets directly from user-provided documents. It uses a multi-stage pipeline:
  document ingestion and preprocessing, semantic chunking, LLM-based question-answer
  generation guided by principles of coverage, diversity, and answerability, citation
  validation, and semantic deduplication.'
---

# YourBench: Easy Custom Evaluation Sets for Everyone

## Quick Facts
- arXiv ID: 2504.01833
- Source URL: https://arxiv.org/abs/2504.01833
- Reference count: 40
- One-line primary result: Automated, document-grounded LLM evaluation framework that generates custom, up-to-date, and domain-specific benchmarks

## Executive Summary
YourBench is a framework for automated, document-grounded LLM evaluation that addresses benchmark saturation and contamination by generating custom evaluation sets directly from user-provided documents. It uses a multi-stage pipeline: document ingestion and preprocessing, semantic chunking, LLM-based question-answer generation guided by principles of coverage, diversity, and answerability, citation validation, and semantic deduplication. The framework was validated by replicating MMLU benchmark subsets from minimal Wikipedia text, achieving perfect relative model ranking preservation (Spearman ρ=1) while producing harder questions. Human evaluation showed approximately 85% question validity, and automated citation grounding performed well across 26 state-of-the-art models.

## Method Summary
YourBench implements a multi-stage pipeline for automated evaluation set generation. It ingests and preprocesses user-provided documents, then semantically chunks them into manageable units. The framework uses LLM-based question-answer generation guided by principles of coverage, diversity, and answerability, followed by citation validation to ensure factual grounding. Finally, semantic deduplication removes redundant questions. The system was validated by replicating MMLU benchmark subsets from minimal Wikipedia text, achieving perfect relative model ranking preservation while producing more challenging questions. The approach offers low-cost, domain-specific, and up-to-date evaluations, supported by the release of the YourBench library and the Tempora-0325 dataset of recent documents.

## Key Results
- Perfect relative model ranking preservation (Spearman ρ=1) when replicating MMLU subsets
- Generated questions were harder than original MMLU questions while maintaining ranking fidelity
- Human evaluation showed approximately 85% question validity rate
- Automated citation validation performed well across 26 state-of-the-art models

## Why This Works (Mechanism)
YourBench works by automating the entire evaluation set creation process while maintaining quality through multiple validation stages. The framework leverages LLM capabilities for question generation but controls for quality through citation validation and semantic deduplication. By grounding questions directly in user-provided documents, it avoids the saturation and contamination issues that plague static benchmarks. The semantic chunking ensures that questions are focused and answerable, while the multi-stage pipeline systematically addresses potential quality issues at each step. The approach balances automation with quality control, making custom evaluation accessible to users without requiring manual question curation.

## Foundational Learning
- **Semantic Chunking**: Breaking documents into meaningful units that can support focused questions
  - Why needed: Ensures questions are answerable and focused on specific document content
  - Quick check: Verify that each chunk contains sufficient context for a complete answer

- **Citation Validation**: Verifying that generated answers can be supported by the source document
  - Why needed: Prevents hallucination and ensures factual grounding
  - Quick check: Test with documents where some facts are intentionally omitted

- **Semantic Deduplication**: Identifying and removing semantically similar questions
  - Why needed: Prevents redundancy and ensures evaluation set diversity
  - Quick check: Verify that similar questions across different chunks are properly identified

## Architecture Onboarding

**Component Map**: Document Ingestion -> Preprocessing -> Semantic Chunking -> Question Generation -> Citation Validation -> Semantic Deduplication -> Final Evaluation Set

**Critical Path**: The question generation and citation validation stages form the critical path, as they directly impact the quality and validity of the final evaluation set. Failures in these stages can render the entire evaluation set unusable.

**Design Tradeoffs**: The framework trades manual curation quality for automation speed and domain flexibility. While this enables rapid creation of custom benchmarks, it introduces potential biases in question difficulty and coverage that require ongoing monitoring.

**Failure Signatures**: Poor citation validation rates indicate issues with either document quality or generation parameters. High semantic duplication rates suggest the chunking strategy needs adjustment. Low human validation rates may indicate problems with the generation prompt engineering or document selection.

**3 First Experiments**:
1. Test the framework with a single, well-structured Wikipedia article to verify basic functionality
2. Run citation validation on a known set of true/false statements to calibrate model performance
3. Generate questions from multiple document types (text, PDF, HTML) to test preprocessing robustness

## Open Questions the Paper Calls Out
Major uncertainties remain regarding the long-term effectiveness of automated question generation in diverse domains beyond the tested scientific and educational contexts. The framework's reliance on LLM-generated content introduces potential biases in question difficulty and domain coverage that were not fully characterized. While citation validation showed promise, the evaluation only tested 26 models, leaving open questions about performance across the broader landscape of state-of-the-art LLMs.

## Limitations
- Limited evaluation across only 26 state-of-the-art models for citation validation
- 85% human validation rate may be subject to author bias in evaluation
- Performance on highly specialized or technical domains remains unexplored
- Reliance on LLM-generated content introduces potential biases in question difficulty and domain coverage

## Confidence
- **High**: Core technical implementation of the YourBench pipeline including document processing, semantic chunking, and deduplication mechanisms
- **Medium**: Automated citation validation performance given the limited model coverage
- **Low**: Generalizability of question quality and difficulty assessments to novel domains and framework's robustness against adversarial document inputs

## Next Checks
1. External validation of question quality and difficulty through third-party human evaluation across multiple domain experts
2. Comprehensive benchmarking of citation validation across at least 50 diverse state-of-the-art LLMs, including both proprietary and open-source models
3. Systematic evaluation of YourBench-generated benchmarks on highly specialized technical domains (e.g., legal, medical, or engineering texts) to assess domain-transferability and identify potential generation biases