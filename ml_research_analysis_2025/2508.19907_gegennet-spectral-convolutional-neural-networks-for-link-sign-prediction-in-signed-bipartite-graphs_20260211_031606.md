---
ver: rpa2
title: 'GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction
  in Signed Bipartite Graphs'
arxiv_id: '2508.19907'
source_url: https://arxiv.org/abs/2508.19907
tags:
- spectral
- graph
- gegennet
- link
- signed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles link sign prediction in signed bipartite graphs
  (SBGs), a challenging problem involving predicting positive or negative links between
  two distinct node sets. Existing approaches often focus on unipartite graphs or
  neglect the unique bipartite structure and node heterogeneity.
---

# GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartive Graphs

## Quick Facts
- **arXiv ID:** 2508.19907
- **Source URL:** https://arxiv.org/abs/2508.19907
- **Reference count:** 40
- **Key outcome:** GegenNet achieves up to 4.28% improvement in AUC and 11.69% in F1-score for link sign prediction in signed bipartite graphs.

## Executive Summary
This paper addresses the challenge of predicting the sign of links in signed bipartite graphs (SBGs), where edges connect two distinct node sets and can be either positive or negative. The authors introduce GegenNet, a novel spectral convolutional neural network designed specifically for SBGs, overcoming limitations of existing methods that ignore bipartite structure or node heterogeneity. GegenNet leverages spectral feature initialization using Laplacian eigenvectors and singular value decomposition to capture structural semantics, employs a new Gegenbauer polynomial-based spectral filter that better fits link-sign signals across different spectral frequencies, and processes positive and negative edges in separate streams to learn sign-specific patterns. Extensive experiments on six real-world datasets demonstrate significant performance gains over 11 strong baselines.

## Method Summary
GegenNet uses a spectral graph convolution approach with three key innovations: (1) spectral feature initialization that combines Laplacian eigenvectors (capturing inter-partition relations) and top singular vectors of the normalized adjacency matrix (capturing intra-partition relations) to create meaningful node embeddings without random initialization; (2) a spectral filter based on Gegenbauer polynomials that replaces traditional monomial bases and better approximates the ideal link-sign curve across all spectral frequencies; and (3) sign-aware convolutional layers that apply separate Gegenbauer filters to positive and negative edges before fusing their representations. The model is trained using cross-entropy loss on an MLP decoder that takes concatenated final node embeddings to predict edge signs.

## Key Results
- GegenNet significantly outperforms 11 strong baselines on all six real-world datasets.
- Achieves up to 4.28% improvement in AUC and 11.69% improvement in F1-score compared to best competitors.
- Ablation studies confirm that spectral feature initialization is critical, with random initialization causing performance to drop to chance levels.
- Gegenbauer polynomial filters demonstrate superior curve-fitting ability compared to standard monomial-based filters like GCN and APPNP.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gegenbauer polynomial bases improve the model's ability to capture link formation patterns across different spectral frequencies.
- **Mechanism:** Standard graph neural networks rely on monomial bases (e.g., Λ^K) which struggle to fit the ideal curve of link existence signals, particularly in high- and low-frequency ranges. In SBGs, the relationship between spectral eigenvalues and link signs is non-linear and complex. Gegenbauer polynomials provide a more expressive function class that fits these spectral curves more closely than monomials, allowing the filter to retain informative signals at the spectrum's extremes.
- **Core assumption:** The underlying relationship between spectral eigenvalues and the probability of positive/negative link formation follows a trajectory better approximated by Gegenbauer polynomials than by Chebyshev polynomials or monomials.
- **Evidence anchors:** [Page 4, Figure 1] shows empirical "ideal" curves vs. GCN/APPNP fits, highlighting the failure of monomials in high/low-frequency areas; [Page 5, Section 4.1.3] defines the Gegenbauer polynomial basis J_k^α and plots its fit against the ideal; related papers discuss Laplacian spectral encodings but do not specifically validate Gegenbauer filters for signed graphs.
- **Break condition:** If the graph spectrum is relatively flat or if the link sign signal is concentrated strictly in the mid-frequency range, the additional complexity of Gegenbauer polynomials may yield diminishing returns over standard monomial filters.

### Mechanism 2
- **Claim:** Initializing node features via spectral decomposition captures structural semantics that random initialization destroys.
- **Mechanism:** GegenNet constructs features by solving two optimization problems: one for inter-partition proximity (minimizing distance for positive links, maximizing for negative) and one for intra-partition similarity. This is solved efficiently by setting features X as a weighted sum of the Laplacian eigenvectors (inter) and the top singular vectors of the normalized adjacency matrix (intra). This pre-loads the model with valid structural "position encodings."
- **Core assumption:** The structural position of a node relative to positive/negative communities (captured via Laplacian/SVD) is a more robust predictor of future links than external attributes or random embeddings, particularly in sparse graphs.
- **Evidence anchors:** [Page 6, Section 4.3] derives the objective functions for inter- and intra-partition relations (Eq. 13, 15) and the final feature construction X = μΦ + (1-μ)Ψ; [Page 7, Table 5] shows that replacing spectral initialization with random embeddings causes AUC to crash (e.g., Amazon drops from ~0.80 to 0.51); related work supports the utility of Laplacian spectral encodings for node identifiability.
- **Break condition:** If the graph is extremely dense and rich with external node attributes (e.g., text features), the spectral initialization might be redundant or constraining compared to learned attribute embeddings.

### Mechanism 3
- **Claim:** Separating positive and negative edges into distinct spectral convolution streams enables learning of sign-specific structural logics.
- **Mechanism:** The architecture applies separate Gegenbauer filters J_k^α(Â+) and J_k^α(Â-) to the node embeddings at every layer, rather than collapsing edges into a single matrix. These distinct embeddings are then concatenated and fused. Theoretical linearization analysis shows this results in a sophisticated combination of operators that can differentiate between the spectral signals of trust vs. distrust.
- **Core assumption:** Positive and negative links represent fundamentally different generative processes that should not be mixed in a single message-passing stream but rather processed in parallel and fused.
- **Evidence anchors:** [Page 5, Section 4.2] describes the sign-aware convolution layer (Eq. 9) using separate weights and filters for pos/neg; [Page 5, Section 4.2 Linearization Analysis] shows the final representation Z is a sum of 3^K terms involving products of distinct operators P ∈ {J(Â+), J(Â-), I}; related work on "Adversarial Signed Graph Learning" also treats sign preservation as critical.
- **Break condition:** If the ratio of negative links is vanishingly small (e.g., < 5%), the negative spectral stream may overfit to noise or fail to converge without specific regularization.

## Foundational Learning

- **Concept: Spectral Graph Theory & Filters**
  - **Why needed here:** The entire architecture is built on spectral convolution. You must understand that filtering in the "spectral domain" means multiplying eigenvalues Λ by a function (here, Gegenbauer polynomials) rather than just multiplying adjacency matrices.
  - **Quick check question:** Can you explain why standard GCN filters (monomials) might fail to capture "high-frequency" signals in a graph?

- **Concept: Orthogonal Polynomial Bases (Gegenbauer/Chebyshev)**
  - **Why needed here:** The core innovation is swapping the polynomial basis. Understanding that Chebyshev is a specific case of Gegenbauer (α=0) helps explain the flexibility the authors introduce.
  - **Quick check question:** If I set α=1.0 in a Gegenbauer polynomial, what standard polynomial does it resemble, and how does it differ from α=1.5?

- **Concept: Signed Bipartite Graph Constraints**
  - **Why needed here:** The model is not a general GNN. It explicitly handles the U × V structure (bipartite) and the E+/E- split (signed). Ignoring the bipartite constraint (e.g., adding U × U links) would violate the problem definition.
  - **Quick check question:** In a Signed Bipartite Graph, why can't we simply treat negative edges as positive edges with a weight of -1 in a single standard GNN layer?

## Architecture Onboarding

- **Component map:** Input graph → Spectral Feature Initialization (Laplacian SVD + SVD) → L layers of Sign-Aware Spectral Conv (Pos filter, Neg filter, Identity) → Fusion (Concat + Linear) → MLP Decoder
- **Critical path:** The Spectral Feature Initialization (Section 4.3) is the biggest deviation from standard pipelines. If you feed random tensors, the model effectively breaks (see Table 5).
- **Design tradeoffs:**
  - **Polynomial Order (k) vs. Smoothness:** Higher order k captures complex frequencies but risks overfitting.
  - **Alpha (α) Parameter:** Controls the shape of the filter. The paper finds α=1.5 often best, but α=0 (Chebyshev) is a strong backup.
  - **Complexity:** Requires eigen-decomposition during initialization (O(n³) or truncated approx), which is more expensive than random init but done only once.
- **Failure signatures:**
  - **Performance at Chance Level (~50%):** Check if spectral initialization was bypassed or if the positive/negative streams are accidentally merged.
  - **Over-smoothing:** If layers L are too deep (>6), distinct bipartite structure may wash out despite residual connections.
  - **Sparse Negative Links:** If |E-| is tiny, the negative stream may need regularization or higher weighting (δ) to learn effectively.
- **First 3 experiments:**
  1. **Ablation on Initialization:** Run GegenNet with Spectral Init vs. Random Init vs. SVD-only (no Laplacian) on a subset like 'Review' to validate the importance of the two spectral components.
  2. **Frequency Fitting:** Replicate Figure 1. Plot U^T Y U vs. Λ for a small dataset and overlay the Gegenbauer curve vs. a GCN curve to visualize the fitting advantage.
  3. **Hyperparameter μ Sweep:** Test the balance between Inter (Φ) and Intra (Ψ) features by varying μ from 0.1 to 0.9 to see which structural view dominates for your specific dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can GegenNet be extended to handle dynamic or multiplex bipartite networks to capture temporal evolution and multiple interaction types?
- **Basis in paper:** [explicit] The conclusion explicitly states, "Future work may explore extending GegenNet to dynamic or multiplex bipartite networks to handle temporal information and multiple interaction types."
- **Why unresolved:** The current architecture assumes a static graph structure with a single type of signed interaction and does not incorporate time-dependent feature updates or multiplex layers.
- **What evidence would resolve it:** A modified model architecture that successfully integrates time-domain spectral analysis or multiple graph views, demonstrating performance stability on time-series or multiplex SBG datasets.

### Open Question 2
- **Question:** Is there a theoretical relationship between the spectral properties of an SBG and the optimal parameter α for the Gegenbauer polynomial basis?
- **Basis in paper:** [inferred] Section 5.4 shows that model performance varies significantly with the choice of α (e.g., Chebyshev vs. Legendre vs. α=1.5) and dataset, but relies on empirical tuning rather than a theoretical selection criterion.
- **Why unresolved:** The paper demonstrates that the Gegenbauer basis fits spectral signals better than monomials but does not provide a theoretical formula for selecting the best α a priori for a given graph topology.
- **What evidence would resolve it:** A theoretical derivation linking graph eigenvalue distributions to the optimal α, or a self-adaptive mechanism that dynamically selects the basis without grid search.

### Open Question 3
- **Question:** How does the computational complexity of the spectral feature initialization scale with graph size compared to spatial message-passing methods?
- **Basis in paper:** [inferred] Section 4.3 proposes a spectral decomposition for feature initialization (X) involving eigenproblems. While described as "fast," the complexity of eigendecomposition is generally higher than spatial sampling methods, and specific scalability limits on massive graphs are not quantified.
- **Why unresolved:** The theoretical speedup relies on approximations (like randomized SVD) which are not deeply analyzed in terms of accuracy trade-offs against the full decomposition required for the "theoretically grounded" initialization.
- **What evidence would resolve it:** A rigorous complexity analysis and empirical runtime comparison on graphs with orders of magnitude more nodes (e.g., billions) to verify tractability.

## Limitations
- The spectral initialization may not generalize to graphs with rich external attributes where learned embeddings could outperform fixed spectral ones.
- The Gegenbauer polynomial advantage is demonstrated primarily through synthetic curve fitting; real-world validation of frequency-specific benefits is limited.
- The model's computational cost scales poorly with graph size due to spectral decomposition requirements, potentially limiting practical deployment on massive bipartite networks.

## Confidence
- **High confidence** in the empirical performance claims across all six datasets, given the systematic comparison with 11 baselines and statistically significant improvements.
- **Medium confidence** in the theoretical advantages of Gegenbauer polynomials, as the paper provides mathematical justification but limited ablation studies varying the polynomial order k or alpha.
- **Medium confidence** in the sign-aware architecture's contribution, since while the ablation shows it's better than merging edges, the relative importance versus spectral initialization isn't fully disentangled.

## Next Checks
1. **Cross-dataset generalization test:** Evaluate GegenNet on a new signed bipartite dataset (e.g., from different domain like citation or social networks) to verify the 4.28% AUC improvement holds beyond the six reported datasets.
2. **Frequency-specific ablation:** Systematically vary the Gegenbauer polynomial order k and alpha parameter across a wider range (not just 0, 0.5, 1.5) to identify optimal settings and confirm frequency-specific benefits claimed in the mechanism section.
3. **Scalability benchmark:** Measure runtime and memory usage on progressively larger bipartite graphs (synthetic scaling tests) to quantify the eigen-decomposition bottleneck and assess practical deployment limits.