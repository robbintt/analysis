---
ver: rpa2
title: Efficient and Scalable Agentic AI with Heterogeneous Systems
arxiv_id: '2507.19635'
source_url: https://arxiv.org/abs/2507.19635
tags:
- memory
- heterogeneous
- agentic
- agent
- workloads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a system design for dynamic orchestration of
  AI agent workloads on heterogeneous compute infrastructure spanning CPUs and accelerators
  from different vendors and performance tiers. The key contribution is a comprehensive
  approach that includes a framework for planning and optimizing agentic AI execution
  graphs using cost models accounting for compute, memory, and bandwidth constraints
  of different hardware; a MLIR-based representation and compilation system that can
  decompose AI agent execution graphs into granular operators and generate code for
  different hardware options; and a dynamic orchestration system that can place the
  granular components across heterogeneous compute infrastructure while meeting end-to-end
  SLA requirements.
---

# Efficient and Scalable Agentic AI with Heterogeneous Systems

## Quick Facts
- arXiv ID: 2507.19635
- Source URL: https://arxiv.org/abs/2507.19635
- Authors: Zain Asgar; Michelle Nguyen; Sachin Katti
- Reference count: 40
- Primary result: System design for dynamic orchestration of AI agent workloads on heterogeneous compute infrastructure

## Executive Summary
This paper presents a comprehensive system design for efficiently orchestrating AI agent workloads across heterogeneous compute infrastructure spanning CPUs and accelerators from different vendors and performance tiers. The framework addresses the challenge of optimizing agentic AI execution graphs by incorporating cost models that account for compute, memory, and bandwidth constraints of diverse hardware. The approach enables dynamic placement of granular components across heterogeneous infrastructure while meeting end-to-end SLA requirements, potentially delivering significant TCO benefits compared to homogeneous deployments.

## Method Summary
The system employs a three-part approach: (1) planning and optimization using cost models that account for compute, memory, and bandwidth constraints across different hardware options; (2) MLIR-based representation and compilation that decomposes AI agent execution graphs into granular operators and generates code for various hardware targets; and (3) dynamic orchestration that places these components across heterogeneous infrastructure to meet SLA requirements. The framework allows mixing older generation GPUs with newer accelerators to potentially extend infrastructure life while maintaining TCO benefits.

## Key Results
- Heterogeneous infrastructure can deliver significant TCO benefits compared to homogeneous deployments
- Combination of older generation GPUs with newer accelerators can deliver similar TCO as latest generation homogenous GPU infrastructure
- Dynamic orchestration successfully meets end-to-end SLA requirements across diverse hardware configurations

## Why This Works (Mechanism)
The system works by decomposing complex AI agent execution graphs into granular operators that can be individually scheduled and executed on appropriate hardware based on their specific requirements. The MLIR-based compilation creates hardware-agnostic representations that enable efficient code generation for different accelerator types, while the cost models incorporate real-time constraints to make optimal placement decisions. This fine-grained approach allows the system to exploit the complementary strengths of different hardware components rather than being constrained by the limitations of any single hardware type.

## Foundational Learning
- MLIR compilation framework: Needed to create hardware-agnostic representations and generate optimized code for diverse accelerators; quick check: verify MLIR's ability to handle custom AI agent operators
- Cost modeling for heterogeneous systems: Essential for making optimal placement decisions across different hardware with varying compute, memory, and bandwidth characteristics; quick check: validate cost model accuracy against actual hardware performance
- Dynamic orchestration with SLA constraints: Required to ensure end-to-end performance requirements are met while exploiting heterogeneous infrastructure; quick check: test orchestration under varying load and failure conditions

## Architecture Onboarding

Component Map: User Request -> Planner -> MLIR Compiler -> Cost Models -> Orchestrator -> Heterogeneous Hardware (CPUs/GPUs/Accelerators)

Critical Path: Request → Planning (cost model evaluation) → Compilation (MLIR IR generation) → Orchestration (placement decisions) → Execution

Design Tradeoffs: Granularity vs overhead in operator decomposition, accuracy vs complexity in cost models, latency vs throughput in orchestration decisions

Failure Signatures: Cost model inaccuracies leading to suboptimal placement, MLIR compilation failures for specific hardware targets, orchestration system overload causing SLA violations

First 3 Experiments:
1. Benchmark single AI agent execution across homogeneous vs heterogeneous configurations
2. Test MLIR compilation and code generation for representative operators on different hardware
3. Stress test the orchestration system under peak load conditions with varying SLA requirements

## Open Questions the Paper Calls Out
None provided in the input.

## Limitations
- Preliminary results may not scale to production workloads across diverse agentic AI applications
- TCO benefits depend on specific workload characteristics and may not generalize across all use cases
- The surprising finding about mixed-generation infrastructure equivalence requires validation across different hardware combinations and workloads

## Confidence
- High confidence: System design approach and framework components (planning, MLIR representation, orchestration)
- Medium confidence: Preliminary TCO results and their generalizability
- Low confidence: "Surprising finding" about mixed-generation infrastructure equivalence without detailed methodology

## Next Checks
1. Conduct controlled experiments comparing homogeneous vs heterogeneous deployments across multiple agentic AI workloads to verify the TCO claims
2. Analyze the MLIR-based compilation system's performance portability across different hardware architectures and operator types
3. Test the dynamic orchestration system's ability to meet SLA requirements under varying load conditions and failure scenarios