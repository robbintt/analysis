---
ver: rpa2
title: 'I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry
  for Authorship Attribution'
arxiv_id: '2506.17323'
source_url: https://arxiv.org/abs/2506.17323
tags:
- code
- attribution
- authorship
- token
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first systematic study of LLM authorship
  attribution for C code, addressing the challenge of identifying which model generated
  a given program. The authors propose CodeT5-Authorship, a custom encoder-only model
  derived from CodeT5, optimized for authorship classification using a two-layer classification
  head.
---

# I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution

## Quick Facts
- arXiv ID: 2506.17323
- Source URL: https://arxiv.org/abs/2506.17323
- Reference count: 40
- First systematic study showing >95% accuracy in attributing C code to specific LLMs using stylistic fingerprints

## Executive Summary
This paper presents the first systematic investigation into LLM authorship attribution for C code, demonstrating that code generated by different LLMs contains detectable stylistic fingerprints. The authors introduce CodeT360-Authorship, a custom encoder-only model optimized for authorship classification, and LLM-AuthorBench, a dataset of 32,000 compilable C programs from eight state-of-the-art LLMs. Their approach achieves 97.56% accuracy in binary classification tasks and 95.40% in multi-class attribution across five leading models, significantly outperforming traditional machine learning methods and off-the-shelf transformer models.

## Method Summary
The authors developed CodeT360-Authorship, a custom encoder-only model derived from CodeT360, optimized for authorship classification using a two-layer classification head. They created LLM-AuthorBench, a carefully curated dataset containing 32,000 compilable C programs generated by eight state-of-the-art LLMs. The methodology involves training the model on this dataset and evaluating its performance in both binary classification (distinguishing closely related models) and multi-class attribution tasks. The approach leverages the persistent stylistic patterns in LLM-generated code, demonstrating that these fingerprints remain detectable even across different LLM families.

## Key Results
- CodeT360-Authorship achieves 97.56% accuracy in binary classification distinguishing closely related models like GPT-4.1 vs. GPT-4o
- The model reaches 95.40% accuracy in multi-class attribution across five leading LLMs
- Outperforms traditional ML methods and off-the-shelf transformer models in authorship attribution tasks

## Why This Works (Mechanism)
The approach works because LLMs develop distinct stylistic patterns in their code generation that persist across different prompts and tasks. These patterns include syntactic preferences, variable naming conventions, structural choices, and coding patterns that are unique to each model family. Even closely related models like GPT-4.1 and GPT-4o maintain distinguishable signatures in their generated code. The encoder-only architecture of CodeT360-Authorship is particularly effective at capturing these subtle stylistic differences without being distracted by decoding considerations.

## Foundational Learning
- **Code Stylometry**: The analysis of programming style characteristics for authorship attribution. Why needed: Provides the theoretical foundation for detecting unique coding patterns. Quick check: Can human programmers be identified by their coding style?
- **Encoder-only transformer models**: Models that process input without generating output, optimized for understanding rather than generation. Why needed: More efficient for classification tasks where output generation isn't required. Quick check: Does the model architecture match the task requirements?
- **Compilable code datasets**: Curated collections of syntactically correct code samples. Why needed: Ensures the model learns from valid, functional code rather than noise. Quick check: Are all samples in the dataset actually compilable?
- **Binary vs. multi-class classification**: Different approaches to categorization problems with two vs. multiple classes. Why needed: Allows testing model performance at different difficulty levels. Quick check: How does performance degrade as class count increases?
- **Stylistic fingerprints**: Persistent patterns in generated content that remain consistent across different contexts. Why needed: Forms the basis for reliable authorship attribution. Quick check: Are these patterns stable across different prompts and tasks?
- **CodeT360 architecture**: A transformer-based model specifically designed for code understanding tasks. Why needed: Provides strong baseline performance for code-related tasks. Quick check: How does this compare to general-purpose language models?

## Architecture Onboarding

**Component Map**: LLM-AuthorBench Dataset -> CodeT360-Authorship Model -> Classification Head -> Attribution Result

**Critical Path**: Data Preprocessing -> Model Training -> Feature Extraction -> Classification Decision

**Design Tradeoffs**: The authors chose an encoder-only architecture over decoder models to optimize for classification accuracy rather than generation capability, trading potential generative flexibility for improved attribution performance.

**Failure Signatures**: The model may struggle with heavily edited code where original stylistic patterns are obscured, or when trained on insufficient data from specific model families. Performance could degrade when attempting attribution across very different programming languages.

**3 First Experiments**:
1. Train the model on the full LLM-AuthorBench dataset and measure baseline attribution accuracy
2. Test binary classification performance between closely related models (GPT-4.1 vs GPT-4o)
3. Evaluate cross-validation performance to assess model generalization across different data splits

## Open Questions the Paper Calls Out
The paper does not explicitly identify additional open questions beyond those addressed in the study.

## Limitations
- Dataset limited to C programs from eight specific LLMs, potentially limiting generalizability to other languages or newer model versions
- Does not address adversarial scenarios where models might be explicitly trained to mimic other styles
- Does not explore temporal stability of stylistic signatures across model updates or versions

## Confidence

**High confidence**: The core finding that LLM-generated code contains detectable stylistic patterns enabling authorship attribution

**Medium confidence**: Claims about specific accuracy rates and model comparisons, given the controlled experimental conditions

**Medium confidence**: The assertion that these patterns persist across model families, pending broader validation

## Next Checks

1. Test the model's performance on code that has undergone human editing or refactoring to assess robustness to post-generation modifications

2. Evaluate the approach on code from newer LLM versions and additional programming languages beyond C

3. Conduct a feature importance analysis to identify which code characteristics (syntactic patterns, variable naming, structural choices) most strongly contribute to authorship attribution