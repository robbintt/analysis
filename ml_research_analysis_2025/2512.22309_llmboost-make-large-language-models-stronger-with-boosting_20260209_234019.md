---
ver: rpa2
title: 'LLMBoost: Make Large Language Models Stronger with Boosting'
arxiv_id: '2512.22309'
source_url: https://arxiv.org/abs/2512.22309
tags:
- llmboost
- ensemble
- arxiv
- each
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLMBoost, a novel ensemble fine-tuning framework
  that leverages internal representations of large language models (LLMs) for hierarchical
  error correction. Unlike existing ensemble methods that treat models as black boxes
  and only combine final outputs, LLMBoost uses a cross-model attention mechanism
  enabling successor models to access and fuse hidden states from predecessors, facilitating
  precise error correction and knowledge transfer.
---

# LLMBoost: Make Large Language Models Stronger with Boosting

## Quick Facts
- arXiv ID: 2512.22309
- Source URL: https://arxiv.org/abs/2512.22309
- Reference count: 40
- Average accuracy improvement of 3.9% (maximum 6.6%) on reasoning tasks

## Executive Summary
This paper introduces LLMBoost, a novel ensemble fine-tuning framework that leverages internal representations of large language models (LLMs) for hierarchical error correction. Unlike existing ensemble methods that treat models as black boxes and only combine final outputs, LLMBoost uses a cross-model attention mechanism enabling successor models to access and fuse hidden states from predecessors, facilitating precise error correction and knowledge transfer. The framework employs a chain training paradigm with an error-suppression objective to progressively fine-tune connected models, and a near-parallel inference paradigm that pipelines hidden states across models to reduce latency. Theoretical analysis proves that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate consistent accuracy improvements averaging 3.9% (maximum 6.6%), with inference latency reduced by 47% compared to conventional sequential ensembles.

## Method Summary
LLMBoost introduces a hierarchical error-correction framework that treats LLMs as interconnected modules rather than independent agents. The core innovation is the cross-model attention mechanism, which allows successor models to access and fuse hidden states from their predecessors through residual addition at specific layers. The framework employs a two-stage chain training paradigm: first fine-tuning a base model, then training successor models using an error-suppression objective that learns to correct predecessor errors while preserving ground truth predictions. During inference, a near-parallel decoding approach pipelines hidden states across models using a shared thread-safe pool, achieving significant latency reduction compared to conventional sequential ensembles. The theoretical analysis proves monotonic improvement under bounded correction assumptions, while extensive experiments demonstrate consistent accuracy gains on commonsense and arithmetic reasoning benchmarks.

## Key Results
- Achieved average accuracy improvement of 3.9% across commonsense and arithmetic reasoning tasks
- Maximum accuracy improvement of 6.6% on specific benchmark datasets
- Reduced inference latency by 47% compared to conventional sequential ensemble methods
- Demonstrated theoretical guarantee of monotonic improvement under bounded correction assumptions

## Why This Works (Mechanism)
The framework works by enabling successor models to directly access and correct errors from predecessor models through hidden state fusion, rather than only combining final outputs. The cross-model attention mechanism injects predecessor hidden states into successor layers, providing rich error context for correction. The error-suppression objective explicitly trains successors to identify and correct specific error patterns from predecessors while maintaining ground truth predictions. The near-parallel inference pipeline reduces computational overhead by sharing intermediate representations across the model chain, making the ensemble approach more practical for real-world deployment.

## Foundational Learning
- **Cross-model attention**: Why needed - Enables successor models to access predecessor representations for error correction; Quick check - Verify hidden state dimensions match for addition
- **Error-suppression objective**: Why needed - Explicitly trains models to correct predecessor errors rather than just learning new patterns; Quick check - Monitor gradients to ensure error correction dominates
- **Near-parallel inference**: Why needed - Reduces computational overhead compared to sequential inference; Quick check - Measure latency reduction across different hardware configurations
- **Chain training paradigm**: Why needed - Enables progressive knowledge transfer and error correction; Quick check - Validate monotonic improvement across training stages
- **Thread-safe shared state pool**: Why needed - Facilitates efficient hidden state sharing during inference; Quick check - Test for deadlocks under high load conditions
- **Logits fusion with backward pass**: Why needed - Improves final prediction quality by combining intermediate outputs; Quick check - Compare accuracy with and without fusion

## Architecture Onboarding

Component map: Base Model → Successor Models → Near-Parallel Inference Pipeline

Critical path: Chain training → Cross-model attention fusion → Error-suppression learning → Logits fusion → Final prediction

Design tradeoffs: The framework trades increased training complexity and memory overhead for improved accuracy and reduced inference latency compared to traditional ensemble methods.

Failure signatures: Training divergence when error-suppression objective conflicts with primary task; Deadlocks in inference pipeline when hidden states are not properly synchronized; Performance plateaus when correction capacity is saturated.

First experiments: 1) Validate cross-model attention with homogeneous models; 2) Test error-suppression objective with simple error patterns; 3) Benchmark near-parallel inference latency on representative hardware

## Open Questions the Paper Calls Out
- How can LLMBoost be extended to support fully heterogeneous ensembles where models possess different tokenizers, hidden state dimensions, and architectural specifications?
- Can adaptive routing mechanisms be integrated into LLMBoost to dynamically select agents or reasoning paths based on task context to improve efficiency?
- How can the framework be modified to mitigate the diminishing returns observed when the number of sub-models exceeds three?
- Can LLMBoost be effectively adapted for multi-agent settings where different models specialize in distinct roles such as reasoning, planning, or tool usage?

## Limitations
- Currently fails with diverse model families due to structural mismatches in hidden state dimensions
- Performance plateaus or shows diminishing returns when ensemble size exceeds three models
- Requires careful hyperparameter tuning to balance error suppression with primary task learning
- Theoretical guarantees rely on bounded correction assumptions that may not hold in practice

## Confidence
- High confidence: Cross-model attention mechanism is clearly specified and theoretically sound; latency reduction claim is well-supported
- Medium confidence: Average accuracy improvements reported but depend on successful implementation and hyperparameter tuning
- Low confidence: Theoretical proof of monotonic improvement requires careful validation under real-world conditions

## Next Checks
1. Implement dimension alignment verification: Test cross-model attention with heterogeneous model pairs and verify hidden state projections maintain representational capacity
2. Stress-test the error-suppression objective: Conduct ablation studies varying α to determine stability thresholds and identify training divergence conditions
3. Benchmark thread synchronization overhead: Measure actual latency reduction across different hardware configurations to validate 47% improvement claim and identify bottlenecks