---
ver: rpa2
title: Evaluating the Sensitivity of LLMs to Prior Context
arxiv_id: '2506.00069'
source_url: https://arxiv.org/abs/2506.00069
tags:
- context
- performance
- claude
- gemini
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study reveals that large language models (LLMs) suffer significant\
  \ accuracy drops\u2014up to 73%\u2014when answering questions within multi-turn\
  \ interactions, even when prior context is unrelated to the task. Using novel benchmarks\
  \ built from the GPQA Diamond dataset and varied context types (free chat, same-domain,\
  \ and cross-domain multi-turn QA), the authors tested GPT, Claude, and Gemini models\
  \ across 4k to 64k token contexts."
---

# Evaluating the Sensitivity of LLMs to Prior Context

## Quick Facts
- arXiv ID: 2506.00069
- Source URL: https://arxiv.org/abs/2506.00069
- Reference count: 40
- Primary result: LLMs show significant accuracy drops (up to 73%) in multi-turn interactions even with unrelated prior context, with task repetition recovering up to 86% of lost performance.

## Executive Summary
This study reveals that large language models suffer significant accuracy degradation when answering questions within multi-turn interactions, even when prior context is unrelated to the task. Using novel benchmarks built from the GPQA Diamond dataset and varied context types (free chat, same-domain, and cross-domain multi-turn QA), the authors tested GPT, Claude, and Gemini models across 4k to 64k token contexts. Surprisingly, larger models were not necessarily more robust; in some cases, their performance dropped below smaller models. A key finding was that strategically repeating the task description before the query could recover up to 86% of the accuracy loss. These results highlight the critical need for context-aware LLM evaluation and prompt design to ensure reliable performance in real-world conversational settings.

## Method Summary
The study evaluated LLMs on GPQA Diamond questions (198 graduate-level STEM MCQs) preceded by varying prior context types and lengths. Models tested included GPT-4o, GPT-4o-mini, Claude 3 Sonnet, Claude 3 Haiku, Gemini 1.5 Pro, and Gemini 1.5 Flash. Prior context came from three sources: LMSYS-Chat-1M for free chat, MMLU STEM subjects for same-domain QA, and MMLU non-STEM subjects for cross-domain QA. Two task placements were tested: task-at-top (description only at first message) and task-repeated (description at start and before query). Context lengths ranged from 4k to 64k tokens, with accuracy measured across 3 seeds per condition.

## Key Results
- LLMs showed accuracy drops of up to 73% in multi-turn interactions even with unrelated prior context
- Larger models were not reliably more robust; some smaller models outperformed larger ones in cross-domain task-at-top scenarios
- Task repetition before queries recovered 70-86% of accuracy loss
- Cross-domain context caused 2-3× more degradation than same-domain context
- Performance degradation stabilized after 16k-32k tokens rather than continuing linearly

## Why This Works (Mechanism)

### Mechanism 1: Task Information Dilution in Long Contexts
The paper hypothesizes that as context length increases, the task description's influence on the model's attention degrades, causing formatting instruction adherence to break down. Qualitative analysis revealed formatting deviations (e.g., outputting LaTeX instead of specified format) as the primary error source in task-at-top scenarios, despite intervening examples demonstrating correct formatting. Repeating the task description appears to refocus the model's attention on key instructions.

### Mechanism 2: Cross-Domain Context Interference
Prior context from different knowledge domains causes more severe performance degradation than same-domain context, even when the target query is independent. The paper suggests that domain-specific activation patterns persist across turns and interfere with subsequent reasoning. Gemini Flash showed a 62% relative accuracy drop when comparing same-domain (0.340) to cross-domain (0.128) task-at-top at 64k context.

### Mechanism 3: Early-Stage Confidence Filtering
Performance degradation stabilizes after 4k-16k tokens because queries answered with high confidence remain stable, while lower-confidence queries are susceptible to even short prior context. The paper observes "early stabilisation"—accuracy drops sharply from no-context to 16k, then plateaus, suggesting a confidence threshold effect where marginal predictions flip easily but confident predictions resist context interference.

## Foundational Learning

- **Needle-in-a-Haystack vs. Multi-Turn Coherence**: The paper contrasts single-turn retrieval tasks with sustained multi-turn interactions. Confusing these paradigms leads to misapplying single-turn benchmarks (e.g., MMLU) to multi-turn scenarios. Quick check: Does the evaluation require the model to maintain instruction adherence across intervening turns, or only to locate information once?

- **Task-Relative Position Effects**: Where the task description sits (top vs. repeated) affects accuracy by up to 3.5×. Understanding position sensitivity is prerequisite to designing robust prompts. Quick check: In your prompt design, is the task instruction >10k tokens away from the query? If so, have you reiterated it?

- **Domain Continuity Bias**: Cross-domain context causes worse degradation. Engineers must recognize that even "irrelevant" prior turns can impair performance if they shift semantic domains. Quick check: Are prior conversation turns semantically related to the target task, or do they span unrelated topics (e.g., history → physics)?

## Architecture Onboarding

- **Component map**: Context Buffer -> Task Description Layer -> Target Query Engine -> Evaluation Module
- **Critical path**: Select context type and length → Inject task description at configured position → Append target query with shuffled choices → Extract model response and compute accuracy
- **Design tradeoffs**: Task-at-top only provides cleaner prompt structure but risks dilution at >16k tokens; Task-repeated recovers 70-86% of accuracy loss but adds token overhead
- **Failure signatures**: Format drift (model outputs LaTeX instead of specified MCQ format), domain contamination (2× worse accuracy in cross-domain vs same-domain), non-monotonic degradation (fluctuations at intermediate lengths)
- **First 3 experiments**: 1) Baseline calibration: Run no-context GPQA Diamond on your target model; 2) Task position A/B test: At 32k same-domain context, compare task-at-top vs task-repeated; 3) Domain sensitivity probe: At 16k context, compare same-domain vs cross-domain

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do chain-of-thought prompting, summarization, or selective memory retention mitigate the accuracy drops caused by extended prior context? The authors explicitly list these techniques as avenues for future research, but the current study primarily focused on "task-repeated" prompting as the sole mitigation strategy.

### Open Question 2
Does the sensitivity to prior context observed in STEM QA tasks generalize to coding contexts and non-English languages? The future work section suggests extending the study to "different languages and code context" to capture wider real-life scenarios, but experiments were restricted to English text and specific QA datasets.

### Open Question 3
What underlying mechanisms cause model performance degradation to plateau or stabilize after 16k to 32k tokens? The authors observe that accuracy drops often flatten after 16k and state that "more studies are needed to understand and explain this behaviour," though they hypothesize confidence levels without mechanistic proof.

## Limitations

- Task description prompt format is only loosely specified (referencing OpenAI's "simple-evals" framework without exact text), making faithful reproduction difficult
- Mechanisms are inferred from formatting errors rather than directly measured through attention weight analysis or latent representation probing
- Evaluation uses synthetic multi-turn contexts rather than naturally occurring conversations, potentially underestimating real-world degradation patterns

## Confidence

- **High Confidence**: Accuracy degradation occurs in multi-turn settings and is recoverable through task repetition
- **Medium Confidence**: Larger models are not necessarily more robust to context interference
- **Low Confidence**: Proposed mechanisms (task information dilution, domain continuity bias, early-stage confidence filtering) are speculative interpretations

## Next Checks

1. **Attention Pattern Validation**: Use attention visualization tools to directly measure whether task descriptions lose influence in long contexts, particularly comparing task-at-top vs. task-repeated conditions.

2. **Confidence-Stability Correlation**: Extract and analyze model log-probabilities for no-context predictions, then measure whether low-confidence predictions are indeed more susceptible to context interference by correlating confidence scores with accuracy drops across context lengths.

3. **Natural Conversation Stress Test**: Replace synthetic LMSYS and MMLU contexts with naturally occurring multi-turn conversations from sources like OpenWebText or Reddit, then measure if the same degradation patterns and recovery rates hold in more realistic scenarios.