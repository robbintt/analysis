---
ver: rpa2
title: 'Explainability Through Systematicity: The Hard Systematicity Challenge for
  Artificial Intelligence'
arxiv_id: '2507.22197'
source_url: https://arxiv.org/abs/2507.22197
tags:
- systematicity
- thought
- what
- which
- systematization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reframes explainability in AI as a broader ideal of
  systematicity. While existing work focuses on whether AI exhibits compositionality,
  the author argues for a richer conception: macrosystematicity, which involves striving
  towards consistency, coherence, comprehensiveness, and parsimonious principledness
  across an entire body of thought.'
---

# Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence

## Quick Facts
- arXiv ID: 2507.22197
- Source URL: https://arxiv.org/abs/2507.22197
- Reference count: 29
- Key outcome: Reframes AI explainability as a broader ideal of systematicity (macrosystematicity), requiring consistency, coherence, comprehensiveness, and principledness across an entire body of thought rather than just compositional structure.

## Executive Summary
This paper argues that explainability in AI is just one facet of a broader ideal: systematicity of thought. While existing work focuses on whether AI exhibits compositionality (microsystematicity), the author proposes a richer conception called macrosystematicity, which involves striving towards consistency, coherence, comprehensiveness, and parsimonious principledness across an entire body of thought. The paper distinguishes five functions of systematization (constitutive, hermeneutic, epistemological, critical, and didactic) and argues these rationales transfer from human to artificial cognition. The resulting "hard systematicity challenge" calls for building neural networks that exhibit macrosystematicity, not just microsystematicity, with systematicity needs dynamically calibrated based on model type, required function, and target human agents.

## Method Summary
The paper takes a conceptual and philosophical approach rather than empirical implementation. It develops a framework for understanding systematicity through five functions of systematization and proposes the 3C2P dimensions (Consistency, Coherence, Comprehensiveness, Principledness, Parsimony) as criteria for macrosystematicity. While the paper mentions potential technical approaches like fine-tuning for consistency, RAG, self-consistency verification loops, and latent space planning, no specific architectures, hyperparameters, or training procedures are provided. The work remains theoretical, calling for empirical validation and benchmark development rather than providing direct implementations.

## Key Results
- Reframes AI explainability as part of a broader systematicity ideal requiring system-wide integration rather than just compositional structure
- Identifies five distinct functions of systematization that generate different normative pressures on AI systems
- Proposes a dynamic understanding framework where systematicity requirements depend on model type, required function, and target agents
- Articulates the "hard systematicity challenge" requiring neural networks to exhibit macrosystematicity beyond Fodor's compositionality requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Interpretability and explainability of AI outputs depend on the capacity for systematic integration into a broader body of thought.
- **Mechanism:** Understanding a judgment requires placing it within "the space of reasons"—seeing what other thoughts it supports or conflicts with. The hermeneutic function of systematization renders thoughts intelligible through their inferential connections. Without this systematic integration, outputs remain isolated propositional moods rather than interpretable cognitive acts.
- **Core assumption:** Understanding is inferential; grasping a thought constitutively involves grasping its relations to other thoughts.
- **Evidence anchors:**
  - [abstract] "explainability is only one facet of a broader ideal... striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled"
  - [section 6] "We do not properly understand a judgment, a belief, or an assertion as long as we cannot see what other thoughts it rationally supports or conflicts with"
  - [corpus] Evidence weak; neighbor papers focus on domain-specific explainability needs rather than systematicity as a foundational mechanism.
- **Break condition:** If understanding can be achieved without grasping inferential relations (e.g., through direct familiarity or procedural knowledge), the mechanism weakens.

### Mechanism 2
- **Claim:** The five functions of systematization (constitutive, hermeneutic, epistemological, critical, didactic) generate differentiated normative pressures on AI systems.
- **Mechanism:** Each function creates a distinct rationale: (1) constitutive—maintaining interpretability as cognition; (2) hermeneutic—enabling understanding; (3) epistemological—providing acceptability criteria; (4) critical—enabling scrutiny and revision; (5) didactic—facilitating transmission. These functions transfer from human to artificial cognition because they address practical needs, not consciousness-dependent properties.
- **Core assumption:** The rationales for systematizing human thought apply analogously to artificial systems that produce propositional outputs.
- **Evidence anchors:**
  - [section 6] Full enumeration of five functions with transfer argument to AI
  - [section 19] "an LLM need not fully achieve systematic integration, but it must at least be interpretable as striving for it if the sentences it produces are to be interpretable as expressing propositional attitudes at all"
  - [corpus] Chalmers (2025) on propositional interpretability in AI provides supporting context.
- **Break condition:** If the functions are intrinsically tied to conscious experience or agency in ways that cannot apply to AI, the transfer fails.

### Mechanism 3
- **Claim:** The demand for systematicity should be dynamically calibrated based on three parameters: model type, function needed, and human agents involved.
- **Mechanism:** The three-parameter formula—[which model] × [which function] × [which agents]—generates context-sensitive requirements. Different functions prioritize different 3C2P dimensions: constitutive function foregrounds consistency; hermeneutic requires coherence and parsimony; epistemological demands comprehensive principledness; critical emphasizes demonstrable principledness; didactic benefits from parsimony and structure.
- **Core assumption:** Systematicity is not a binary property but a gradable, context-sensitive ideal that answers to practical needs.
- **Evidence anchors:**
  - [section 8] Full development of the dynamic understanding framework with the three-parameter formula
  - [section 28] "our sense of when and how AI models need to systematize should be guided by our sense of the functions that need to be discharged"
  - [corpus] No direct corpus evidence on dynamic calibration; this appears novel to the paper.
- **Break condition:** If systematicity functions are inseparable (cannot be prioritized independently), or if all AI contexts require maximal systematicity, the dynamic framework collapses to a single standard.

## Foundational Learning

- **Concept: Microsystematicity vs. Macrosystematicity**
  - Why needed here: The paper's core distinction; micro refers to compositional structure within individual thoughts (Fodor's focus), macro refers to integration across an entire body of propositions (3C2P dimensions).
  - Quick check question: Can you explain why consistency and coherence are macro-systematic properties rather than micro-systematic ones?

- **Concept: The 3C2P Dimensions**
  - Why needed here: These five dimensions operationalize macrosystematicity: Consistency (no contradictions), Coherence (rational interconnection), Comprehensiveness (no gaps), Principledness (subsumption under general laws), Parsimony (economy of principles).
  - Quick check question: Why might comprehensiveness and parsimony be in tension with each other?

- **Concept: Regulative Ideal**
  - Why needed here: Macrosystematicity is treated as a regulative ideal—a standard that guides practice even when never fully achieved, unlike a binary criterion that must be met.
  - Quick check question: How does treating systematicity as a regulative ideal rather than a requirement change how we evaluate AI systems?

## Architecture Onboarding

- **Component map:** Model Type → Required Function → Target Agents → 3C2P Calibration → Systematicity Requirements

- **Critical path:** Identify which function(s) dominate in a given use case → determine which 3C2P dimensions matter most → calibrate evaluation benchmarks accordingly. For example, a recidivism prediction model (epistemological + critical functions) needs strong principledness and consistency; a creative writing assistant (didactic function) needs minimal systematicity.

- **Design tradeoffs:** The 3C2P dimensions can be antagonistic—comprehensiveness may require more principles (reducing parsimony); principledness may introduce inconsistencies elsewhere. Section 16 notes that systematization involves "striking a reasonable balance between various dimensions."

- **Failure signatures:**
  - Inconsistent outputs across conversational threads (breaks constitutive function)
  - Explanations that multiply principles ad hoc ("If you don't like my principles, I have others")
  - Long-context incoherence where earlier commitments are abandoned
  - Outputs that cannot be situated within any rational inferential network

- **First 3 experiments:**
  1. **Cross-thread consistency probe:** Prompt the same model on related questions in separate threads; measure contradiction rate on factual and normative claims.
  2. **Principle proliferation test:** For a domain requiring explanations, count how many distinct principles are invoked across N queries; assess whether principles are reused or invented per query.
  3. **Function-specific benchmarking:** Design separate evaluations for each 3C2P dimension; identify which dimensions current models handle well vs. poorly across different function contexts (e.g., compare hermeneutic adequacy vs. critical adequacy).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can neural networks be trained to exhibit macrosystematicity (consistency, coherence, comprehensiveness, principledness, and parsimony) across their outputs, rather than merely microsystematicity?
- **Basis in paper:** [explicit] The paper formulates the "hard systematicity challenge": "the challenge of building neural networks that are sensitive to the structure of thought not merely in the narrow sense of being sensitive to how thoughts are composed of recombinable constituents, but in the broader and more demanding sense of striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled" (Section 7).
- **Why unresolved:** Current LLMs struggle with consistency and coherence, especially "once the amount of text exceeds their context window." The paper notes that "systematic integration is something that LLMs still struggle with" despite indirect incentives via SFT and RLHF.
- **What evidence would resolve it:** Demonstration of neural architectures or training methods that reliably produce outputs scoring high across all five dimensions of the 3C2P conception of macrosystematicity over extended interactions.

### Open Question 2
- **Question:** What benchmarks or evaluation frameworks can effectively measure macrosystematicity in AI models?
- **Basis in paper:** [inferred] The paper critiques that existing systematicity benchmarks (e.g., compositionality tests from Lake and Baroni 2023) target only microsystematicity. It explicitly calls for shifting attention to macrosystematicity but provides no standardized measurement approach.
- **Why unresolved:** The 3C2P dimensions (consistency, coherence, comprehensiveness, principledness, parsimony) are gradable properties requiring operationalization for empirical testing.
- **What evidence would resolve it:** Development and validation of a benchmark suite that quantifies performance on each dimension of macrosystematicity across diverse tasks.

### Open Question 3
- **Question:** Can LLMs learn to leverage the systematic integration of truths in their training data to overcome inconsistencies, incoherences, and lacunae, or must systematicity be imposed via external mechanisms (e.g., RAG, verification loops)?
- **Basis in paper:** [explicit] The paper poses this as an open possibility: "perhaps LLMs can learn to leverage the systematic integration of truths to overcome inconsistencies, incoherences, and lacunae in their training data—at least within domains in which the truth is systematic" (Section 7), citing Queloz (2025a).
- **Why unresolved:** It remains unclear whether statistical learning alone can internalize systematicity or whether hybrid approaches (RAG, self-consistency checks, latent space planning) are necessary.
- **What evidence would resolve it:** Comparative studies showing whether models trained on systematically structured data exhibit higher macrosystematicity than those trained on equivalent but unstructured data, controlling for other factors.

## Limitations
- No concrete benchmark datasets, prompts, or evaluation protocols provided for any 3C2P dimension
- Transfer argument from human to artificial cognition relies more on assertion than demonstration
- Dynamic calibration framework lacks operational guidance for resolving conflicts between dimensions
- 3C2P dimensions are described abstractly without threshold definitions for "adequate" systematicity

## Confidence
- **High confidence:** The conceptual distinction between microsystematicity (compositionality) and macrosystematicity (system-wide integration) is well-argued and represents a genuine advance in systematicity discourse. The five functions of systematization are clearly enumerated and their rationales well-developed.
- **Medium confidence:** The claim that interpretability requires systematic integration into a "space of reasons" is philosophically plausible but rests on contested assumptions about the nature of understanding. The transfer argument for applying human systematicity rationales to AI is reasonable but not conclusively demonstrated.
- **Low confidence:** The dynamic calibration framework, while promising, lacks operational guidance. The paper doesn't provide concrete methods for determining which systematicity dimensions matter most in specific applications or how to resolve conflicts between dimensions.

## Next Checks
1. **Cross-model systematicity benchmarking:** Test multiple LLM architectures on identical systematicity tasks to empirically validate whether some models exhibit stronger macro-systematic properties than others, and whether performance correlates with model scale or architecture type.

2. **Function-specific systematicity probes:** Design targeted evaluations for each systematicity function (constitutive, hermeneutic, epistemological, critical, didactic) to determine which functions current models handle well versus poorly, and whether these align with intended use cases.

3. **Dimension tradeoff analysis:** Systematically vary prompts to create systematicity tradeoffs (e.g., between comprehensiveness and parsimony) and measure how models navigate these tensions, providing empirical grounding for the claim that systematization involves "striking a reasonable balance."