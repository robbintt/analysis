---
ver: rpa2
title: 'ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management'
arxiv_id: '2512.19001'
source_url: https://arxiv.org/abs/2512.19001
tags:
- inventory
- management
- replenishment
- framework
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an OR-guided Pretrain-then-Reinforce framework
  for inventory management that integrates deep learning pretraining with reinforcement
  learning fine-tuning, guided by structured Operations Research logic. The framework
  uses a simulation-augmented OR model to generate reference decisions, which guide
  a deep learning model through pretraining and RL fine-tuning stages.
---

# ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management

## Quick Facts
- arXiv ID: 2512.19001
- Source URL: https://arxiv.org/abs/2512.19001
- Reference count: 25
- Primary result: 5.27-day reduction in inventory turnover, 2.29% increase in in-stock rates, 29.95% decrease in holding costs at JD.com

## Executive Summary
This paper introduces an OR-guided Pretrain-then-Reinforce framework for inventory management that integrates deep learning pretraining with reinforcement learning fine-tuning, guided by structured Operations Research logic. The framework uses a simulation-augmented OR model to generate reference decisions, which guide a deep learning model through pretraining and RL fine-tuning stages. Unlike conventional approaches that rely on massive model scaling, this lightweight, domain-informed design achieves superior performance. Field deployment at JD.com showed a 5.27-day reduction in inventory turnover, a 2.29% increase in in-stock rates, and a 29.95% decrease in holding costs. The method demonstrates strong adaptability, cost-efficiency, and transferability, offering a scalable and interpretable paradigm for large-scale e-commerce inventory management.

## Method Summary
The ORPR framework consists of three stages: (1) A trace-driven simulator replays historical demand under realistic constraints (lead times, review periods, lost sales), then an optimization model selects inventory-day targets that minimize holding cost subject to a stockout constraint—producing structured labels; (2) A Transformer+VAE-logits neural network is pretrained on abundant demand data and fine-tuned on OR labels; (3) The policy is refined via RLOO fine-tuning with a hybrid reward combining rule-based direction enforcement and Pareto-dominance simulation rewards, anchored by a KL penalty. The lightweight architecture (~0.93M parameters) substitutes for scale by leveraging domain-informed OR logic, achieving superior performance in field deployment.

## Key Results
- 5.27-day reduction in inventory turnover days at JD.com
- 2.29% increase in in-stock rates
- 29.95% decrease in holding costs
- Demonstrated adaptability to promotional periods through expert-guided secondary fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Simulation-Augmented OR Labels Provide High-Quality Behavioral Priors
OR-derived decisions enable sample-efficient learning by providing structured, economically-grounded supervision. A trace-driven simulator replays historical demand under realistic constraints (lead times, review periods, lost sales), then an optimization model selects inventory-day targets that minimize holding cost subject to a stockout constraint—producing labels that implicitly encode business logic. Core assumption: The simulation horizon and fidelity capture enough operational complexity that optimizing over simulation outputs generalizes to live conditions. Evidence: [Section 3.3.1] Optimization formulation minimizes inventory value under a lost-sales constraint; decisions expressed in inventory-days. Break condition: If simulator dynamics diverge from live environment (e.g., new fulfillment topology, policy change), labels may misalign and degrade guidance.

### Mechanism 2: RL Fine-Tuning Aligns Policy to OR Optimality While Permitting Exploration
Reinforcement learning acts as a deep alignment mechanism that internalizes OR decision patterns and can improve beyond them via exploration. The pre-trained DL policy is fine-tuned with a hybrid reward combining (i) a sign-aware rule loss to respect directionality of adjustments and (ii) a Pareto-dominance simulation reward to discover SKU-level improvements. A KL penalty anchors to the reference policy. Core assumption: The hybrid reward and KL constraint are sufficient to prevent catastrophic drift while allowing meaningful exploration within the action space. Evidence: [Section 3.2.1] Hybrid reward r_total = ω·r_rule + (1−ω)·r_sim; rule loss asymmetrically weights direction errors. Break condition: If β is too weak, policy may over-explore and diverge; if too strong, alignment may suppress beneficial specialization.

### Mechanism 3: Lightweight Model with Domain-Informed Design Substitutes for Scale
A compact Transformer+VAE architecture, when guided by OR logic, can match or exceed larger generic models. Three-stream Transformer encoder fuses sales, attributes, and decision-objective features; a VAE policy head preserves distributional richness for RL. Pre-training on abundant demand data before decision-head training reduces reliance on scarce decision labels. Core assumption: Demand-side pretraining transfers useful representations to the decision task, and OR labels are sufficiently informative to specialize the policy. Evidence: [Section 3.1.2, 3.4] Three-stage training; total ~0.93M parameters; contrast with scaling laws. Break condition: If feature streams are poorly aligned or OR labels are biased, compact capacity may underfit relative to a larger model.

## Foundational Learning

### Concept: Markov Decision Processes and Policy Gradient Basics
Why needed: RLOO fine-tunes a stochastic policy via gradient estimates; understanding baselines, KL penalties, and exploration is essential to diagnose instability. Quick check: Can you explain why a leave-one-out baseline reduces variance without introducing bias?

### Concept: Inventory Control Fundamentals (base-stock, holding/stockout costs, lead times)
Why needed: The OR model expresses decisions as inventory-days and trades off holding vs lost sales; intuitions about (s, S) and service-level constraints ground interpretation of rewards. Quick check: How does expressing decisions in inventory-days unify heterogeneous SKUs?

### Concept: Variational Autoencoders and Latent Policy Heads
Why needed: The policy head uses a VAE to model stochasticity and output logits for RL; ELBO training and latent spaces affect exploration. Quick check: What does the VAE-logits design preserve that a softmax output would suppress?

## Architecture Onboarding

### Component map:
Trace-driven simulator -> OR optimizer -> inventory-day labels -> Transformer encoder (sales + attributes + objectives) -> VAE decision head -> RLOO fine-tuning with hybrid reward and KL anchor

### Critical path:
1. Implement simulator and OR label generation; validate turnover calibration reproduces historical patterns.
2. Pre-train Transformer encoder + sales head on demand data; freeze, then train decision head with OR labels.
3. Run RLOO fine-tuning with β and ω tuned on held-out categories; monitor KL and Pareto-dominance rate.

### Design tradeoffs:
- Rule vs simulation reward weight (ω): Higher ω stabilizes alignment; lower ω enables discovery.
- KL penalty (β): Higher β anchors to OR; lower β allows policy deviation.
- Model size (~0.93M): Compact for speed/interpretability vs risk of underfitting heterogeneous categories.

### Failure signatures:
- Turnover calibration fails to match historical turnover → labels misrepresent preferences.
- RL reward plateaus without improvement → exploration insufficient or reward poorly shaped.
- KL divergence spikes → β too low or reward over-emphasizes outliers.
- Promotional periods understocked → OR guidance overly conservative; consider expert-guided secondary fine-tuning.

### First 3 experiments:
1. Offline backtest: Compare OR-only, DL-only, and DL+RL policies on historical data across ABC–XYZ segments; verify heterogeneity and turnover alignment.
2. Ablation on ω and β: Run RLOO with varying (ω, β); track total cost, KL, and Pareto-dominance rate to find stable region.
3. Promotional scenario test: Introduce expert labels for a known promotion; measure whether OR+expert RL reduces lost sales vs OR-only RL (as in Section 4.3.2).

## Open Questions the Paper Calls Out

### Open Question 1
Does the efficiency of the lightweight OR-guided framework hold when integrated with large foundation models? The conclusion states that future research involves "exploring the potential scaling law by replacing existing lightweight model components with advanced large models." The current work demonstrates "anti-scaling" success with a 0.93M parameter model, but it is unknown if this domain-informed architecture remains superior or stable when scaled to billion-parameter foundation models. Comparative experiments showing the framework's performance trajectory and computational cost when swapping the deep learning core for large time-series foundation models would resolve this.

### Open Question 2
Can the current concise OR formulation generalize to complex multi-echelon inventory systems? The authors identify "generalizing this paradigm to more complex inventory structures, such as multi-echelon systems," as a vital direction for future research. The deployed model uses a single-period, concise OR formulation augmented by simulation for specific SKU-DC pairs; multi-echelon networks involve inter-level dependencies and lead-time propagation that this simplified structure may not capture. Deployment of the framework in a multi-tier supply chain setting demonstrating maintained total system cost efficiency without structural modifications to the OR logic would resolve this.

### Open Question 3
Can the framework autonomously learn aggressive policies for major promotions without relying on expert-guided fine-tuning? The results (Table 2) indicate that "OR-only guided RL" underperformed during promotions due to conservative regression, necessitating a "secondary fine-tuning step using expert decision data" to correct the bias. The current design relies on human-in-the-loop data injection for tail events, leaving the model's capacity for fully autonomous adaptation to high-stakes demand shocks unsubstantiated. Ablation studies showing the RL agent successfully overriding conservative OR guidance during simulated demand spikes using only the standard reward function and without explicit expert labels would resolve this.

## Limitations
- Narrow internal validation scope: Only three SKUs tested, no comparative baselines against pure OR or heuristic methods
- Unclear operational scalability: Performance beyond small joint decision spaces unverified
- Limited generalizability: No cross-retailer transferability studies conducted

## Confidence
- Core mechanism (OR labels + RL fine-tuning): Medium - Strong logical design and field data support
- Anti-scaling claims: Low - Absence of comparative benchmarks with larger models
- Cost-efficiency arguments: Low - No variance estimates or statistical tests across multiple periods
- 29.95% holding-cost reduction: Medium - Promising but lacks statistical validation

## Next Checks
1. Offline simulation of ORPR vs. pure OR and pure RL baselines across multiple ABC-XYZ segments to quantify relative gains.
2. Sensitivity analysis of ω (rule reward weight) and β (KL anchor) to confirm stable operational region.
3. Deployment on a larger SKU set with cross-category coupling (n>3) to test scalability and joint decision coherence.