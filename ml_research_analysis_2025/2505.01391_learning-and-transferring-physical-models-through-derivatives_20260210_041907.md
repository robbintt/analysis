---
ver: rpa2
title: Learning and Transferring Physical Models through Derivatives
arxiv_id: '2505.01391'
source_url: https://arxiv.org/abs/2505.01391
tags:
- derl
- learning
- outl
- pinn
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Derivative Learning (DERL), a supervised
  approach for modeling physical systems by learning their partial derivatives. The
  key insight is that learning derivatives, along with initial and boundary conditions,
  is sufficient to fully capture a physical system's dynamics.
---

# Learning and Transferring Physical Models through Derivatives

## Quick Facts
- arXiv ID: 2505.01391
- Source URL: https://arxiv.org/abs/2505.01391
- Reference count: 40
- Key outcome: DERL learns physical systems by predicting derivatives, outperforming baselines on generalization and enabling transfer across models with minimal forgetting.

## Executive Summary
This paper introduces Derivative Learning (DERL), a supervised approach for modeling physical systems by learning their partial derivatives. The key insight is that learning derivatives, along with initial and boundary conditions, is sufficient to fully capture a physical system's dynamics. The method is evaluated on various ODEs and PDEs, consistently outperforming state-of-the-art approaches in generalizing to unseen initial conditions, parameters, and new domain regions. The authors extend DERL to enable knowledge transfer across physical models through a distillation protocol, demonstrating that physical knowledge can be effectively transferred and composed across models.

## Method Summary
DERL learns physical systems by predicting partial derivatives rather than function values. The method trains an MLP using a loss function that combines derivative prediction error, boundary condition adherence, and initial condition matching. The network's derivatives are computed via automatic differentiation. For parametric PDEs, parameters are included as network inputs. The authors prove theoretical guarantees showing convergence to the true solution when minimizing DERL's loss, even with empirical derivatives from finite differences. They extend the approach with a distillation protocol that transfers knowledge by matching higher-order derivatives between teacher and student models, enabling incremental learning across different domains and parameter ranges.

## Key Results
- DERL achieves lower L2 error than baselines on Allen-Cahn (0.006769 vs 0.01235) while maintaining superior PDE residuals
- Successfully transfers knowledge across physical models, extending working domains and parameter ranges with minimal forgetting
- Consistently outperforms state-of-the-art methods in generalizing to unseen initial conditions, parameters, and domain regions across multiple test cases

## Why This Works (Mechanism)
DERL works by learning the mathematical structure of physical systems through their derivatives rather than direct function approximation. By targeting derivatives explicitly, the network learns the underlying differential equations governing the system, which provides stronger inductive biases than learning only the function values. This approach inherently captures the local dynamics of the system, making it more robust to domain shifts and parameter changes. The theoretical guarantees ensure that minimizing the derivative prediction loss leads to convergence to the true physical solution, while the inclusion of boundary and initial conditions prevents trivial solutions.

## Foundational Learning
- **Automatic differentiation**: Needed to compute network derivatives efficiently for the loss function. Quick check: verify torch.func.jacrev produces correct Jacobian shapes.
- **Finite difference derivatives**: Used when analytical derivatives aren't available. Quick check: verify empirical derivatives converge to analytical ones as h→0.
- **Physics-informed training**: Combines data-driven learning with physical constraints. Quick check: monitor PDE residual during training.
- **Knowledge distillation**: Transfers learned representations between models. Quick check: verify student model matches teacher's derivative predictions.
- **Continual learning**: Enables sequential model updates without catastrophic forgetting. Quick check: measure performance on earlier tasks after sequential training.
- **Hyperparameter optimization**: Critical for balancing different loss terms. Quick check: perform grid search on λ weights for each experiment.

## Architecture Onboarding

**Component map:** MLP -> Automatic differentiation -> Loss function (derivative MSE + BC loss + IC loss) -> Adam/BFGS optimizer

**Critical path:** Collocation points → MLP prediction → Jacobian computation → Loss calculation → Parameter update

**Design tradeoffs:** Using derivatives directly vs. PDE residuals (DERL trades some physical constraint enforcement for better generalization), tanh vs. other activations (chosen for smoothness), BFGS vs. Adam (BFGS for smaller problems, Adam for larger ones).

**Failure signatures:** BC/IC weights too small → solution drifts from true solution; empirical derivatives with large h → noisy gradients; insufficient collocation points → poor coverage of domain.

**First experiments:**
1. Implement Allen-Cahn baseline with 4-layer MLP, 50 units, tanh activation
2. Compare DERL loss (derivative MSE + BC + IC) against OUTL baseline
3. Test finite difference derivative computation with varying h values

## Open Questions the Paper Calls Out
- Can advanced continual learning strategies effectively mitigate catastrophic forgetting in physical models trained incrementally over long sequences of tasks?
- Can DERL enable a compositional paradigm where multiple distinct expert models are integrated to solve complex, coupled physical systems?
- How does DERL compare to PINNs regarding robustness and error propagation when trained on data with significant measurement noise?

## Limitations
- Theoretical convergence assumes exact derivatives, but method relies on empirical derivatives with finite difference step size h
- PDE residual may be overly strict as a metric since some methods achieve similar prediction accuracy with higher residuals
- Absolute data requirements for achieving practical accuracy are not fully characterized

## Confidence
- High confidence: DERL consistently outperforms baselines on generalization to unseen initial conditions and parameters
- Medium confidence: Claims about superior physical consistency via PDE residuals
- Medium confidence: Claims about data efficiency improvements

## Next Checks
1. Systematically vary finite difference step size h and measure impact on convergence rate and final accuracy across different equations
2. Apply DERL-trained models to actual physical system control or prediction tasks beyond function approximation
3. Compare DERL against baselines using fixed computational budgets rather than fixed collocation points