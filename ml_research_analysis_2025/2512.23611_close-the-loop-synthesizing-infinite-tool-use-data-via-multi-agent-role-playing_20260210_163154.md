---
ver: rpa2
title: 'Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing'
arxiv_id: '2512.23611'
source_url: https://arxiv.org/abs/2512.23611
tags:
- uni00000013
- user
- tool
- uni00000048
- uni0000004f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfTool is a fully autonomous framework for generating high-quality
  tool-use data from raw API specifications. It employs a multi-agent role-playing
  system (User Simulator, Tool Agent, and Server Simulator) to synthesize diverse,
  verified interaction trajectories, addressing challenges of expensive human annotation,
  poor generalization, and quality ceilings in single-model synthesis.
---

# Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing

## Quick Facts
- **arXiv ID:** 2512.23611
- **Source URL:** https://arxiv.org/abs/2512.23611
- **Reference count:** 40
- **Primary result:** 32B model improves from 19.8% to 70.9% accuracy on Berkeley Function-Calling Leaderboard using fully synthetic data

## Executive Summary
InfTool is a fully autonomous framework that generates high-quality tool-use training data from raw API specifications without human annotation. It employs a three-agent multi-agent role-playing system (User Simulator, Tool Agent, Server Simulator) to synthesize diverse, verified interaction trajectories spanning single-turn calls to complex multi-step workflows. The framework uses iterative training via Group Relative Policy Optimization with gated rewards, creating a self-reinforcing loop that progressively improves model capabilities. Experiments show that InfTool improves a 32B base model from 19.8% to 70.9% accuracy on the Berkeley Function-Calling Leaderboard, surpassing models 10× larger and rivaling Claude-Opus, all from synthetic data without human annotation.

## Method Summary
InfTool starts with 17,713 raw APIs, clusters them into 3,059 MCP tools via semantic deduplication, then employs a multi-agent playground where User Simulator generates queries, Tool Agent resolves them via tool calls, and Server Simulator provides consistent responses. Trajectories are validated through self-reflection and voting, then used in a GRPO training loop with gated rewards (format + tool + teacher rewards, where reasoning rewards are only applied when tool execution succeeds). The framework iteratively identifies low-consistency samples as "hard" cases, generating new data targeting these regions to create a curriculum that progressively expands capabilities.

## Key Results
- 32B base model improves from 19.8% to 70.9% accuracy on Berkeley Function-Calling Leaderboard (+258%)
- Outperforms models 10× larger (175B) and rivals Claude-Opus
- Achieves these results using 100% synthetic data without human annotation
- Ablation shows 15.7-point drop without MCP Tree deduplication
- Demonstrates strong cross-domain generalization across 9+ domains

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Role-Playing for Verified Trajectory Synthesis
A three-agent system generates diverse, executable tool-use trajectories verified by successful completion rather than human review. The User Simulator creates realistic queries with structured information gaps, the Tool Agent attempts resolution via tool calls, and the Server Simulator provides consistent responses, creating a self-consistent simulation environment.

### Mechanism 2: Gated Reward Signal Binds Reasoning Quality to Execution Success
A composite reward function gates reasoning rewards behind correct tool execution, preventing models from learning elaborate but functionally useless reasoning patterns. The reward structure has three components, with the teacher/reasoning reward only applied when tool execution succeeds above threshold, creating strict dependency between improved reasoning and functional utility.

### Mechanism 3: Complexity-Adaptive Data Injection Targets Capability Frontiers
The framework iteratively identifies low-consistency samples and generates new data specifically targeting these "hard" regions, creating a curriculum that progressively expands model capabilities. At each iteration, samples showing high variance are classified as "Hard" and define the learning frontier, with new synthesis generating additional samples targeting these specific failure modes.

## Foundational Learning

### Concept: MCP (Model Context Protocol) and Structured Tool Definitions
**Why needed here:** InfTool builds directly on MCP as the interface specification for tools. Understanding how tools are defined (function identifier, JSON schema for arguments) is prerequisite to understanding how the framework generates valid calls and validates responses.

**Quick check question:** Can you explain why a strict JSON schema S_f for tool arguments enables automated validation that would be impossible with natural-language-only tool descriptions?

### Concept: Group Relative Policy Optimization (GRPO)
**Why needed here:** GRPO is the core RL algorithm. Unlike standard PPO, it uses group-relative advantages rather than a learned value function, reducing memory overhead. Understanding this helps explain why InfTool can iterate efficiently.

**Quick check question:** Why does eliminating the value network (critic) reduce memory overhead in policy optimization, and what tradeoff might this introduce?

### Concept: Simulation-to-Reality Gap in Synthetic Data
**Why needed here:** The paper explicitly acknowledges this limitation. Understanding why synthetic user simulators may not capture real user behavior (linguistic ambiguity, cognitive drift) is critical for assessing deployment readiness.

**Quick check question:** If a User Simulator is trained to be "ideally rational," what specific behaviors might it fail to exhibit that real users commonly demonstrate?

## Architecture Onboarding

### Component Map
Raw API Specs (17,713 candidates) → MCP Tree (semantic clustering, deduplication → 3,059 tools) → Multi-Agent Synthesis Loop (User Simulator ↔ Tool Agent ↔ Server Simulator) → Trajectories + Quality Validation → Self-Evolution Training Loop (Rollout → Consistency Scoring → Hard/Easy Split → GRPO with Gated Reward → Updated Model → New Synthesis Targeting Hard)

### Critical Path
1. **MCP Tree construction:** Must successfully cluster and deduplicate APIs, or downstream synthesis will be noisy. Paper shows ablation dropping from 70.9 to 55.2 without this.
2. **Cold-start SFT:** Initial model must achieve sufficient capability to participate in multi-agent synthesis. Paper uses Qwen2.5-32B base.
3. **Gated reward tuning:** Threshold τ and weight α must be calibrated or reasoning rewards never fire or dominate.

### Design Tradeoffs
- **Simulation fidelity vs. scale:** More constrained agents produce more valid trajectories but may lack diversity. Paper uses structured profiles (known_info, unknown_info) to balance this.
- **GRPO vs. PPO:** GRPO eliminates value network (lower memory) but may have higher variance estimates. Paper does not compare directly.
- **Rejection sampling vs. correction:** Paper corrects single-turn errors (87.65% success) but discards most multi-turn hallucinations (12.06% correction rate), suggesting multi-turn validation is a bottleneck.

### Failure Signatures
- **Low multi-turn yield:** If >80% of multi-turn trajectories are discarded, check Server Simulator context consistency.
- **Reward hacking:** If model learns to generate verbose reasoning without improving tool accuracy, check that gate threshold τ is not too low.
- **Domain overfitting:** If performance degrades on τ²-Bench OOD domains, User Simulator profiles may not cover the target distribution.

### First 3 Experiments
1. **Ablate MCP Tree:** Train on raw APIs without clustering. Expect ~15-point drop on BFCL (per table 3). Validates tool definition quality matters.
2. **Vary gate threshold τ:** Test τ ∈ {0.5, 0.7, 0.9} and measure both tool accuracy and reasoning length. Identify where reasoning rewards begin to correlate with execution.
3. **Cross-domain transfer:** Train on synthesized data from domains A, B, C; evaluate on held-out domain D. Compare to training on D directly. Quantifies simulation-to-reality gap for new domains.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** To what extent do synthetic user simulators fail to capture real-world linguistic ambiguity and cognitive drift, and can this simulation-to-reality gap be quantified with human user studies?
**Basis in paper:** Section 8 states synthetic user simulators exhibit idealized rationality that may not fully capture the linguistic ambiguity and cognitive drift typical of real-world human interactions.
**Why unresolved:** No human evaluation or user study was conducted; all evaluation uses automated benchmarks.
**What evidence would resolve it:** Comparative experiments where the same model trained on InfTool data is tested on real human-agent conversations, measuring success rates and failure modes specific to ambiguous or drifted user queries.

### Open Question 2
**Question:** How does self-reflection mechanism efficacy degrade as conversation length increases beyond the median 8-turn range observed in training data?
**Basis in paper:** Section 8 notes the efficacy of self-reflection degrades in extended multi-turn scenarios due to fundamental attention limitations; table 4 shows multi-turn hallucination correction success is only 12.06% vs 87.65% for single-turn.
**Why unresolved:** The paper reports aggregate statistics but does not analyze correction success as a function of conversation length or token count.
**What evidence would resolve it:** Controlled experiments varying conversation length (2, 5, 10, 15+ turns) with per-turn correction success rates and attention analysis.

### Open Question 3
**Question:** To what degree do simulated server responses faithfully reproduce actual API behaviors, and what is the impact of simulation fidelity on downstream model performance?
**Basis in paper:** Section 2.2 describes the Server Simulator generating responses via LLM prompts with no validation against real API executions.
**Why unresolved:** The framework never validates simulated tool outputs against real API calls during data generation.
**What evidence would resolve it:** A validation phase where simulated responses are compared to actual API calls for a held-out subset of tools, with correlation analysis between simulation fidelity and model performance on those tools.

## Limitations
- **Simulation-to-Reality Gap:** User Simulator may not capture real user behavior complexity, particularly ambiguity and cognitive drift, potentially limiting deployment performance.
- **Multi-turn Validation Bottleneck:** Only 12% of multi-turn trajectories are successfully corrected vs 87% for single-turn, suggesting Server Simulator struggles with extended contextual consistency.
- **Resource Intensity:** Framework requires substantial computational resources for iterative training, but detailed resource estimates are not provided.

## Confidence
- **High Confidence:** Core mechanism of multi-agent role-playing for synthetic data generation is well-supported by ablation studies showing 15.7-point drop without MCP Tree.
- **Medium Confidence:** Effectiveness of complexity-adaptive curriculum targeting "hard" samples is demonstrated through incremental performance gains but correlation between consistency scores and true capability gaps could be stronger.
- **Medium Confidence:** Comparison to state-of-the-art models (surpassing 10× larger models) is compelling but based on a single benchmark, suggesting potential overfitting to this specific evaluation.

## Next Checks
1. **Cross-Domain Transferability Test:** Train InfTool on synthesized data from three domains, then evaluate on held-out fourth domain. Compare performance against a model trained directly on real data from that domain to quantify the simulation-to-reality gap.
2. **Real User Query Benchmark:** Deploy the trained model on a held-out set of real user queries from production systems. Measure both accuracy and robustness to ambiguous or poorly specified requests to assess practical deployment readiness.
3. **Multi-turn Complexity Scaling:** Systematically evaluate model performance on multi-turn tool-use tasks of increasing complexity (3, 5, 7, 10+ rounds) to determine if the 12% correction bottleneck fundamentally limits the framework's ability to generate sophisticated workflows.