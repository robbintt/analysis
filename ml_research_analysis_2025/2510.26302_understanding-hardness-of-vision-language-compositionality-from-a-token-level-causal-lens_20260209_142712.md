---
ver: rpa2
title: Understanding Hardness of Vision-Language Compositionality from A Token-level
  Causal Lens
arxiv_id: '2510.26302'
source_url: https://arxiv.org/abs/2510.26302
tags:
- zinv
- text
- ninv
- arxiv
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes why CLIP models struggle with compositional\
  \ reasoning\u2014combining objects, attributes, and relations\u2014by introducing\
  \ a token-level causal representation learning framework. It extends block identifiability\
  \ to tokenized text, proving that CLIP's contrastive objective can recover the modal-invariant\
  \ latent variable at both sentence and token levels."
---

# Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens

## Quick Facts
- **arXiv ID**: 2510.26302
- **Source URL**: https://arxiv.org/abs/2510.26302
- **Reference count**: 40
- **Primary result**: CLIP models struggle with compositional reasoning due to "composition nonidentifiability"—pseudo-optimal text encoders can achieve perfect alignment yet fail to distinguish hard negatives created by swapping, replacing, or adding atomic concepts.

## Executive Summary
This paper analyzes why CLIP models struggle with compositional reasoning—combining objects, attributes, and relations—by introducing a token-level causal representation learning framework. It extends block identifiability to tokenized text, proving that CLIP's contrastive objective can recover the modal-invariant latent variable at both sentence and token levels. The core insight is "composition nonidentifiability": there exist pseudo-optimal text encoders that align perfectly with images but fail to distinguish correct captions from hard negatives created via SWAP, REPLACE, or ADD operations on atomic concepts. These encoders are provably insensitive to token permutations or substitutions, despite achieving the same training objective as true-optimal encoders. The analysis links language-side nonidentifiability to visual failures via the modality gap and shows how iterated composition operators compound hardness, motivating improved negative mining strategies. Empirically, the proposed token-aware algorithms replicate a large fraction of hard negatives used in benchmarks like ARO and VALSE, and iterative application of composition operators during training yields consistent gains over strong baselines on CC3M and CC12M datasets.

## Method Summary
The method extends block identifiability theory from sentence-level SCMs to token-aware SCMs, proving that CLIP's contrastive objective recovers modal-invariant latent variables under both frameworks. The core theoretical contribution shows the existence of pseudo-optimal text encoders that achieve perfect modal-invariant alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations over atomic concepts. This is operationalized through LLM-guided token permutation and rephrasing to generate hard negatives, which are then used in training with multi-call composition operators to compound difficulty. The approach is evaluated on compositionality benchmarks (ARO, VALSE, SugarCrepe) using CC3M and CC12M datasets, showing consistent improvements over strong baselines.

## Key Results
- Multi-call composition (applying two composition operators) yields consistent gains over strong baselines: +1.84 and +1.30 overall accuracy on CC3M for NegCLIP and TripletCLIP respectively
- Proposed token-aware algorithms replicate 60-80% of hard negatives in ARO and VALSE benchmarks, validating the theoretical framework
- Empirically demonstrates that CLIP's contrastive objective can recover modal-invariant latents but this recovery is insufficient to guarantee compositional sensitivity
- Shows how iterated composition operators compound hardness, motivating improved negative mining strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's contrastive objective recovers modal-invariant latent variables at token granularity, but this recovery is insufficient to guarantee compositional sensitivity.
- Mechanism: The framework extends block identifiability from sentence-level SCMs (Assumption 1) to token-aware SCMs (Assumption 4), where text is modeled as sequential token generation via z^(tex)_i ~ p(z^(tex)_i | z^inv, {z^(tex)_j}^{i-1}_{j=1}). Theorem 5 proves that optimal encoders f*, g* recover z^inv through invertible decomposition f* = h_f ∘ f^{-1}_{1:n_inv}, g* = h_g ∘ g^{-1}_{1:n_inv}, satisfying L_MMAlign → 0. However, token-level identifiability of z^inv does not imply identifiability of token order or composition structure.
- Core assumption: Token embeddings are generated by diffeomorphisms g_i with continuous, strictly positive densities; the modal-invariant component z^inv is shared across modalities while token-dependent partitions z^(tex)_i are sequentially conditioned.
- Evidence anchors:
  - [abstract]: "Our theory extends block identifiability to tokenized text, proving that CLIP's contrastive objective can recover the modal-invariant latent variable under both sentence-level and token-level SCMs."
  - [section 3]: Assumption 4 defines token-aware SCM with recursive sampling z^(tex)_i ~ p(z^(tex)_i | z^inv, {z^(tex)_j}^{i-1}_{j=1}); Theorem 5 formalizes the decomposition with invertible h_f, h_g.
  - [corpus]: Neighbor papers "DisCoCLIP" and "HiMo-CLIP" similarly address compositional failures by modifying encoder architectures, suggesting token-level structure is a recognized gap, though they do not provide causal identifiability proofs.
- Break condition: If token generation is not governed by smooth, invertible mappings (e.g., discrete tokenization without continuous latent space), or if z^inv does not dominate token-dependent partitions, the identifiability guarantees may not hold.

### Mechanism 2
- Claim: Composition nonidentifiability arises because pseudo-optimal encoders g** can achieve perfect modal-invariant alignment while remaining invariant to SWAP, REPLACE, and ADD operations on atomic concepts.
- Mechanism: Theorem 7 constructs g** from g* by defining set-valued inverses ĝ^{(k)}(X^{tex}) = ∩^{k}_{j=1} (g^{(k)})^{-1}(X̂(X^{tex}_{:,j})), where X̂ fixes the j-th column. This yields ĝ^{(k)}_{1:n_inv}(X^{tex}) = {z^inv} for all token permutations π, meaning g**(X^{tex}) = g**(X^{tex}_π). Theorems 8-9 extend this to replacements and additions: if embeddings X^{tex}_{:,j} and RF(X^{tex}_{:,j}) satisfy g**([..., X^{tex}_{:,j}, ...]) = g**([..., RF(X^{tex}_{:,j}), ...]), or if inserted tokens share z^inv, the encoder cannot distinguish hard negatives.
- Core assumption: The existence of token embeddings that are permutation-equivalent or rephrase-equivalent under the modal-invariant projection; for ADD, the intersection condition ∃ z*_inv ∈ (g*)^{-1}_{1:n_inv}(X^{base}) ∩ (g*)^{-1}_{1:n_inv}(X^{ADD}) ensures shared z^inv between base and augmented texts.
- Evidence anchors:
  - [abstract]: "We show the existence of pseudo-optimal text encoders that achieve perfect modal-invariant alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations over atomic concepts."
  - [section 4, Theorems 7-9]: Formal construction of g** and conditions for invariance to permutations (Eq. 10-11), replacements (Eq. 12-13), and additions (Eq. 16).
  - [corpus]: "SugarCrepe++" and "MAC" benchmarks confirm CLIP's vulnerability to adversarial compositional perturbations, empirically supporting the nonidentifiability prediction, though they lack formal existence proofs.
- Break condition: If token embeddings are sufficiently distinct such that no two permutations or replacements satisfy the conditioned modal-invariance equality, g** collapses to g* and compositional sensitivity is preserved. This requires strong token-level disentanglement not guaranteed by standard CLIP training.

### Mechanism 3
- Claim: Iterated composition operators (e.g., SWAP∘REPLACE) compound hardness by reducing the solution space of distinguishable compositions, motivating improved negative mining.
- Mechanism: The paper observes that each SWAP/REPLACE/ADD operation reduces the set of latent configurations consistent with both the original and hard-negative texts. Theorem proofs show that intersection constraints (e.g., Eq. 74, 81) shrink admissible z^inv values. Empirically, applying two composition calls during training (NegCLIP +MC, TripletCLIP +MC) yields gains of +1.84 and +1.30 overall accuracy on CC3M, with notable improvements on Replace-Object (83.66→84.86 on CC12M).
- Core assumption: The first composition call reduces the solution space sufficiently that a second call on a different atomic concept yields non-overlapping hard negatives; the training objective can exploit this increased difficulty.
- Evidence anchors:
  - [abstract]: "shows how iterated composition operators compound hardness, motivating improved negative mining strategies."
  - [section 5.3, Table 2]: Multi-call (MC) variants consistently outperform baselines; CC3M Overall improves from 57.18→59.02 (NegCLIP) and 63.49→64.79 (TripletCLIP).
  - [corpus]: "VideoComp" and related works explore compositional hardening through dataset design, but do not formalize the iterative operator compounding effect.
- Break condition: If the base text has few atomic concepts (k → 2), iterated operations may produce invalid or semantically identical texts, reducing the marginal benefit of additional composition calls.

## Foundational Learning

- **Concept**: Structural Causal Models (SCMs) and Block Identifiability
  - Why needed here: The paper's theoretical contribution rests on formalizing multimodal data generation via SCMs (Assumptions 1, 4) and proving that contrastive training recovers latent blocks z^inv. Without understanding SCM structure (endogenous/exogenous variables, structural equations) and identifiability conditions (diffeomorphisms, smooth densities), the main theorems are inaccessible.
  - Quick check question: Given an SCM x := f(z^inv, z^pr) with invertible f, under what conditions does observing x alone allow recovery of z^inv?

- **Concept**: Contrastive Learning and InfoNCE Objective
  - Why needed here: The paper links CLIP's training objective (Eq. 1) to modal-invariant alignment through Corollaries 3 and 6. Understanding how InfoNCE negative sampling and temperature scaling affect representation learning is essential to grasp why pseudo-optimal solutions exist.
  - Quick check question: In InfoNCE loss with K negative samples, what does the objective minimize as K → ∞, and how does this relate to entropy maximization?

- **Concept**: Token-level Representations and Composition Operators
  - Why needed here: The core empirical contribution operationalizes Theorems 7-9 through SWAP, REPLACE, ADD operators on token embeddings. Understanding how tokenization, embedding matrices X^{tex} ∈ ℝ^{d×k}, and positional structure interact is necessary to implement the hard negative generation algorithms.
  - Quick check question: For a caption "a white cat and a black dog," what token permutations would constitute SWAP-OBJ vs. SWAP-ATT hard negatives?

## Architecture Onboarding

- **Component map**:
  - **Assumption 4 SCM**: Defines the generative model—z^inv (modal-invariant), {z^(tex)_i} (token-dependent, recursively conditioned), z^(tex)_pr (text-private), z^(img)_dp/z^(img)_pr (image-dependent/private). Encoders f and {g_i} map these to observations.
  - **Theorem 5 decomposition**: Optimal encoders satisfy f* = h_f ∘ f^{-1}_{1:n_inv}, g* = h_g ∘ g^{-1}_{1:n_inv}, isolating z^inv. This is the "true-optimal" solution.
  - **Pseudo-optimal encoder g****: Constructed via set-valued inverses (Eq. 68, 84-85) that collapse token-specific variations while preserving z^inv projection. This is the "pseudo-optimal" solution that achieves alignment but fails composition.
  - **Hard negative generation (Algorithm 1)**: Implements SWAP/REPLACE/ADD via LLM-guided token permutation and rephrasing, filtered by CLIP score ranking.
  - **Multi-call composition**: Stacks two operators (e.g., REPLACE then SWAP) to generate harder negatives for training augmentation.

- **Critical path**:
  1. **Data generation assumption**: Ensure token-aware SCM (Assumption 4) holds for your text data—sequential dependency, continuous embeddings.
  2. **Encoder training**: Standard CLIP contrastive training yields f*, g* satisfying Theorem 5; no architectural change required.
  3. **Hard negative synthesis**: Use Algorithm 1 with LLM to generate SWAP/REPLACE/ADD candidates; rank by g*(X^{tex}) similarity.
  4. **Multi-call augmentation**: Apply second operator to first-call hard negatives; verify z^inv intersection condition (Eq. 83) holds for ADD.
  5. **Training with augmented negatives**: Add multi-call hard negatives to contrastive batches; monitor compositional benchmarks (ARO, VALSE, SugarCrepe).

- **Design tradeoffs**:
  - **LLM-based vs. rule-based generation**: LLM prompts (lines 1-3 in Algorithm 1) provide semantic diversity but introduce variability and cost. Rule-based permutations are deterministic but may produce nonsensical texts.
  - **Single-call vs. multi-call**: Multi-call yields harder negatives (+1-2% accuracy gains in Table 2) but increases computational overhead and may produce invalid texts for short captions.
  - **Full fine-tuning vs. efficient adaptation**: The paper applies multi-call to NegCLIP/TripletCLIP baselines; for production, consider LoRA or adapter-based integration to reduce retraining cost.

- **Failure signatures**:
  - **No accuracy gain from multi-call**: Indicates base text has few atomic concepts or second-call hard negatives are semantically equivalent; check token count and operator validity.
  - **LLM generates invalid permutations**: SWAP/REPLACE produces nonsensical captions (e.g., "cat a dog a"); filter with grammar checker or CLIP score threshold.
  - **ADD-condition violation**: Eq. 83 intersection fails because inserted tokens change z^inv; verify via g* embedding similarity before accepting ADD hard negatives.

- **First 3 experiments**:
  1. **Replicate ARO/VALSE overlap analysis**: Implement Algorithm 1 SWAP/REPLACE; use LLM-as-judge to categorize benchmark negatives into theorem categories; verify overlap percentages match Fig. 3-4 (~60-80% on ARO splits).
  2. **Ablation on composition operator types**: Train CLIP variants with single-operator negatives (SWAP-only, REPLACE-only, ADD-only) vs. multi-call; measure accuracy delta on CC3M across Replace/Swap/Add categories to isolate which operators benefit most from compounding.
  3. **Probe pseudo-optimal encoder existence**: For pretrained CLIP, compute A-distance between embeddings of original vs. SWAP/REPLACE/ADD texts before and after hard-negative training (per Section 5.2); expect near-zero distance without training (indicating g**) and positive distance with training.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can novel training objectives or architectural constraints be designed to theoretically guarantee convergence to the "true-optimal" encoder ($g^*$) while excluding the "pseudo-optimal" encoder ($g^{**}$) that fails compositionality?
  - **Basis in paper**: [explicit] The authors prove the existence of $g^{**}$ which achieves optimal alignment ($L \rightarrow 0$) despite failing to distinguish hard negatives (Theorems 7–9), noting "there are no evidences and solutions to identify which one in $g^*, g^{**}$ would be learned in practice."
  - **Why unresolved**: Since both encoders minimize the InfoNCE objective identically, standard contrastive training cannot differentiate between them; current solutions like negative mining are empirical mitigations rather than theoretical guarantees.
  - **What evidence would resolve it**: A modified loss function or regularization term proven to have a unique global minimum exclusively at $g^*$, or empirical convergence analysis showing a training method consistently avoids the permutation-insensitive properties of $g^{**}$.

- **Open Question 2**: How can the token-level causal framework be extended to rigorously define and identify causal variables within the visual modality, rather than relying on the modality gap to explain visual compositionality failures?
  - **Basis in paper**: [explicit] The paper states that because natural image generation differs from language, "it is impossible to derive the same causal analysis to explain the vision compositionality," forcing the authors to resort to the "constant modality gap phenomenon" for visual analysis.
  - **Why unresolved**: The current Structural Causal Model (SCM) assumes a sequential token generation process for text (Assumption 4), which does not map directly to the spatial/hierarchical generation of images, leaving visual failures explained only indirectly via alignment gaps.
  - **What evidence would resolve it**: A formalized SCM for the image generation process that allows for the derivation of "visual nonidentifiability" theorems analogous to the token-level Theorems 7–9.

- **Open Question 3**: What are the theoretical performance bounds and the point of diminishing returns when iteratively applying composition operators (SWAP/REPLACE/ADD) for hard negative mining?
  - **Basis in paper**: [inferred] The paper notes that iteratively calling operators "cannot be endless because each calling... will reduce the solution space," yet empirically only demonstrates gains from a "second calling" without analyzing the limits of this strategy.
  - **Why unresolved**: It is unclear if the "Multi-Calling" strategy eventually degrades semantic coherence or if the solution space collapse prevents further generalization gains beyond two iterations.
  - **What evidence would resolve it**: Theoretical bounds on the difficulty/identifiability of $N$-th order compositional negatives, and empirical training curves evaluating model robustness when trained with $N > 2$ iterative operations.

## Limitations

- The theoretical construction of pseudo-optimal encoders g** relies on strict assumptions about token embeddings being generated by diffeomorphisms with continuous densities and the existence of rephrase-equivalent or permutation-equivalent token sets. These assumptions are plausible but not empirically verified for real-world CLIP embeddings, which may have discrete, non-smooth structure.
- The proof that iterated composition operators compound hardness assumes that each operation reduces the solution space of admissible latent configurations. However, the rate of reduction and practical significance for real datasets are not quantified beyond the observed accuracy gains.
- The LLM-based hard negative generation introduces variability that may not fully capture the theoretical pseudo-optimal encoder behavior, as the LLM's rephrasing may introduce semantic shifts not accounted for in the theorems.

## Confidence

- **High Confidence**: The extension of block identifiability from sentence-level to token-level SCMs (Assumption 4, Theorem 5) is well-formalized and grounded in established causal inference theory. The decomposition of optimal encoders into modal-invariant and private components is mathematically sound.
- **Medium Confidence**: The construction of pseudo-optimal encoders g** (Theorems 7-9) is valid under the stated assumptions, but the practical prevalence of permutation-equivalent or rephrase-equivalent token embeddings in CLIP is uncertain and not directly measured.
- **Medium Confidence**: The empirical gains from multi-call composition (Table 2) are consistent across benchmarks, but the attribution to compounded hardness vs. increased negative diversity is not conclusively isolated.

## Next Checks

1. **Probe Pseudo-Optimal Encoder Existence**: For a pretrained CLIP model, compute the A-distance between embeddings of original and SWAP/REPLACE/ADD texts on a held-out compositional dataset. If the distance is near zero without hard-negative training, it suggests the presence of pseudo-optimal behavior.
2. **Quantify Solution Space Reduction**: For a sample of hard negatives generated by single-call vs. multi-call composition, measure the cosine similarity between their CLIP embeddings and the original. A significant drop for multi-call would support the hypothesis that iterated operations compound hardness.
3. **Ablate LLM Generation Variability**: Replace LLM-based rephrasing with rule-based synonym substitution for REPLACE operations. Compare the overlap with benchmark hard negatives and the accuracy gains from training with these rule-based negatives to assess the impact of LLM variability.