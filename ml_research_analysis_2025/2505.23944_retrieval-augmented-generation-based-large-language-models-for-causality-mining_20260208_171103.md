---
ver: rpa2
title: Retrieval Augmented Generation based Large Language Models for Causality Mining
arxiv_id: '2505.23944'
source_url: https://arxiv.org/abs/2505.23944
tags:
- causality
- causal
- pattern
- examples
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of causality mining using large
  language models (LLMs) by proposing retrieval-augmented generation (RAG) based dynamic
  prompting schemes. The authors construct a fewshot example repository indexed by
  causal connectives and develop two RAG approaches: Pattern RAG, which retrieves
  examples based on causal connective similarity, and kNN+Pattern RAG, which combines
  semantic similarity with pattern matching.'
---

# Retrieval Augmented Generation based Large Language Models for Causality Mining

## Quick Facts
- **arXiv ID**: 2505.23944
- **Source URL**: https://arxiv.org/abs/2505.23944
- **Reference count**: 25
- **Key outcome**: RAG-based dynamic prompting significantly improves LLM performance on causality detection and extraction tasks across three datasets.

## Executive Summary
This paper addresses causality mining using large language models (LLMs) by proposing retrieval-augmented generation (RAG) based dynamic prompting schemes. The authors construct a fewshot example repository indexed by causal connectives and develop two RAG approaches: Pattern RAG, which retrieves examples based on causal connective similarity, and kNN+Pattern RAG, which combines semantic similarity with pattern matching. Extensive experiments on three datasets (SemEval, ADE, and Li et al.) using five LLMs demonstrate that their RAG methods significantly outperform baseline approaches including zeroshot, random fewshot, and kNN RAG.

## Method Summary
The method constructs a Fewshot Example Repository with 2,365 causal sentences indexed by 1,394 unique causal connectives. Two RAG approaches are developed: Pattern RAG retrieves examples based on >90% causal connective similarity, while kNN+Pattern RAG combines semantic similarity (using text-embedding-ada-002) with pattern matching. The retrieved examples are concatenated with task-specific prompts and fed to LLMs for causality detection and extraction. The framework uses dynamic in-context learning where retrieved examples serve as fewshot demonstrations to guide the LLM's causal reasoning.

## Key Results
- kNN+Pattern RAG achieves up to 90% accuracy for causality detection, with F1 improvements of 2-7% over best baselines
- For causality extraction, methods achieve up to 91% accuracy with 4% F1 improvements
- Pattern RAG outperforms kNN RAG by leveraging syntactic pattern matching through causal connectives
- Simply increasing example quantity doesn't improve performance; quality of retrieved examples is critical

## Why This Works (Mechanism)

### Mechanism 1: Pattern-Based Retrieval via Causal Connectives
- Pattern RAG improves causality mining by matching causal connectives between input and retrieved examples
- GPT-3.5-turbo extracts causal connectives from sentences → sentences indexed by connectives in Fewshot Example DB → retrieval selects examples with >90% connective similarity → LLM aligns syntactic patterns to identify cause/effect phrases
- Core assumption: Explicit causal connectives (e.g., "caused by," "leads to") are reliable syntactic anchors that signal cause-effect structure
- Break condition: When sentences contain implicit causality without explicit connectives, or when connectives are ambiguous/multi-word with low similarity scores

### Mechanism 2: Hybrid Semantic-Syntactic Retrieval
- Combining kNN (semantic similarity) with Pattern RAG (syntactic matching) yields better performance than either alone
- kNN retrieves 10 semantically similar examples via text-embedding-ada-002 → Pattern RAG retrieves connective-matched examples → concatenate up to 20 examples → LLM leverages both sentence semantics and connective syntax
- Core assumption: Semantic neighbors provide domain context while pattern-matched examples provide structural guidance; together they offer complementary signals
- Break condition: When semantic neighbors lack causal structure, or when pattern matches are syntactically similar but semantically irrelevant

### Mechanism 3: Dynamic In-Context Learning with Structured Tagging
- Dynamically retrieved examples with XML-tagged cause/effect phrases enable LLMs to transfer causal reasoning patterns without fine-tuning
- Fewshot repository stores sentences with <cause>...</cause> and <effect>...</effect> tags → RAG selects relevant examples → LLM learns pattern alignment via in-context learning
- Core assumption: LLMs can generalize causal extraction patterns from structurally similar examples when properly formatted
- Break condition: When test instances have no matching connectives in repository, or when cause-effect structure differs significantly from example distribution

## Foundational Learning

- **Concept: Causal Connectives**
  - Why needed here: Pattern RAG depends entirely on identifying and matching causal connectives (e.g., "caused by," "induced," "leads to") to retrieve relevant examples
  - Quick check question: Given "High cholesterol contributes to heart disease," what is the causal connective, and how would Pattern RAG use it for retrieval?

- **Concept: Sequence Labeling for Cause-Effect Extraction**
  - Why needed here: Causality extraction is a token-level sequence labeling task; understanding phrase boundaries vs. single-word labels is critical for evaluation
  - Quick check question: If the ground truth labels "mishandling" as cause but the LLM outputs "mishandling of weapons," is this correct under the paper's evaluation metrics?

- **Concept: kNN Retrieval with Sentence Embeddings**
  - Why needed here: kNN RAG uses text-embedding-ada-002 for semantic similarity; understanding vector-space retrieval helps diagnose why kNN alone underperforms on pattern-sensitive tasks
  - Quick check question: Why might kNN retrieve sentences semantically similar to "earthquake damage" but fail to match the causal connective pattern?

## Architecture Onboarding

- **Component map:**
  Fewshot Example DB -> Causal Connective Extractor -> Pattern Matcher -> kNN Retriever -> Prompt Augmenter -> LLM Inference

- **Critical path:**
  Input sentence → Connective extraction (GPT-3.5) → Dual retrieval (Pattern RAG + kNN) → Prompt construction (20 examples max) → LLM inference → Cause/effect output with XML tags

- **Design tradeoffs:**
  - Repository size vs. coverage: Only 80 connectives have 5+ examples; rare connectives may return few/no matches
  - 10-example cap per connective: Redundancy limited; may miss diverse phrasings within same pattern
  - 20-example concatenation for kNN+Pattern: Higher token cost, potential context dilution
  - Single-word vs. phrase extraction: Datasets label single words; prompts request phrases—creates evaluation mismatch

- **Failure signatures:**
  - Pattern RAG returns <10 examples: Input connective has sparse coverage in repository
  - Low recall on Li et al. multi-cause-effect: Single-cause-effect examples don't transfer to complex scenarios
  - High precision, low recall: Overly conservative phrase boundaries (e.g., missing articles like "the foodborne illness")
  - kNN-only underperforms on ADE: Medical domain requires pattern matching over pure semantic similarity

- **First 3 experiments:**
  1. Ablation study: Compare Pattern RAG vs. kNN RAG vs. kNN+Pattern RAG on a held-out test set to quantify each component's contribution
  2. Connective coverage analysis: For your target domain, measure what percentage of sentences contain explicit causal connectives vs. implicit causality
  3. Error categorization on multi-cause-effect: Run kNN+Pattern RAG on Li et al. dataset, classify failures (phrase boundary errors, missing pairs, connective mismatches) to guide repository expansion

## Open Questions the Paper Calls Out

- **Open Question 1**: How can RAG-based dynamic prompting be adapted to effectively detect and extract inter-sentential causality relations?
  - Basis in paper: [explicit] The authors state in the Limitations section (Section 9) that the current approach focuses exclusively on intra-sentential causality and does not address inter-sentential relations
  - Why unresolved: Extending the method requires managing longer context windows and dependencies across sentence boundaries, which the current connective-indexed repository is not designed to handle
  - What evidence would resolve it: A modified RAG framework applied to a dataset containing cross-sentence causal links, demonstrating performance metrics comparable to the intra-sentential results reported in Tables 3 and 4

- **Open Question 2**: Do the proposed Pattern RAG and kNN+Pattern RAG methods generalize to languages with morphological or structural differences from English?
  - Basis in paper: [explicit] Section 9 explicitly notes that experiments were conducted only in English and that extending the approach requires an in-depth understanding of different language structures
  - Why unresolved: Causal connectives and sentence structures vary significantly across languages, potentially rendering the current English-centric connective indexing strategy ineffective without modification
  - What evidence would resolve it: Successful application of the methodology to a non-English causality dataset, showing that the combination of semantic and pattern-based retrieval maintains performance improvements over baselines

- **Open Question 3**: To what extent does performance degrade when processing causal sentences that lack explicit causal connectives?
  - Basis in paper: [inferred] The Pattern RAG method (Section 3.2) relies entirely on matching causal connectives to retrieve examples. While the authors note that kNN helps, they do not isolate performance on sentences with implicit causality where no connective exists
  - Why unresolved: If an input sentence lacks a clear connective, the pattern-matching component fails, forcing the system to rely solely on semantic similarity (kNN), which the authors note is often insufficient for precise phrase extraction
  - What evidence would resolve it: An ablation study on a specific subset of "implicit causality" sentences, comparing the F1 scores of Pattern RAG versus the kNN RAG baseline to quantify the reliance on explicit markers

## Limitations
- The method focuses exclusively on intra-sentential causality and does not address inter-sentential relations
- Experiments were conducted only in English, requiring adaptation for languages with different morphological or structural patterns
- Performance depends heavily on the presence of explicit causal connectives, with unclear behavior on implicit causality

## Confidence
- **kNN+Pattern RAG achieves up to 90% accuracy for causality detection**: High confidence
- **Pattern RAG outperforms kNN RAG by 2-7% F1 on causality detection**: High confidence
- **Hybrid retrieval provides complementary semantic and syntactic signals**: Medium confidence

## Next Checks
1. **Ablation study on retrieval components**: Run Pattern RAG, kNN RAG, and kNN+Pattern RAG on a held-out validation set to quantify individual and combined contributions to performance gains
2. **Connective coverage analysis**: Measure the percentage of test sentences containing explicit causal connectives present in the Fewshot Example DB, and analyze performance drop when connectives are absent or below threshold
3. **Error categorization on multi-cause-effect**: Manually review Li et al. extraction errors from kNN+Pattern RAG to classify failure modes (phrase boundary errors, missing pairs, connective mismatches) and assess whether repository expansion could address these gaps