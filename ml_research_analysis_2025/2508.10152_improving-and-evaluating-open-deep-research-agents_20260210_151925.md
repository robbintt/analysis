---
ver: rpa2
title: Improving and Evaluating Open Deep Research Agents
arxiv_id: '2508.10152'
source_url: https://arxiv.org/abs/2508.10152
tags:
- research
- deep
- browsecomp
- system
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper focuses on Deep Research Agents (DRAs), which autonomously
  search and utilize internet-based content to address user prompts. The authors adapt
  the challenging BrowseComp benchmark to compare the only open-source DRA, Open Deep
  Research (ODR), with proprietary systems.
---

# Improving and Evaluating Open Deep Research Agents

## Quick Facts
- arXiv ID: 2508.10152
- Source URL: https://arxiv.org/abs/2508.10152
- Reference count: 14
- Primary result: ODR+ achieves 10% accuracy on BC-Small benchmark, state-of-the-art among both open and closed-source DRAs

## Executive Summary
This paper addresses the challenge of building effective Deep Research Agents (DRAs) that can autonomously search the internet to answer complex multi-hop questions. The authors adapt the BrowseComp benchmark to create BrowseComp-Small (BC-Small), a computationally tractable subset suitable for academic evaluation. They benchmark existing systems and introduce ODR+, an improved open-source DRA that achieves state-of-the-art performance through three key innovations: question decomposition with constraint extraction, iterative search with retry-based URL selection, and structured output synthesis with confidence scoring.

## Method Summary
The ODR+ system implements a three-module architecture for autonomous research: (1) Question Decomposition extracts explicit constraints from user queries and generates targeted sub-questions; (2) Iterative Sub-Solution Search conducts repeated searches with retry mechanisms, selecting top-k frequent URLs and analyzing evidence to dynamically update the research plan; (3) Response Synthesis produces structured outputs with confidence scores based on constraint satisfaction. The system uses GPT-4o-mini for all LLM operations and FireCrawl for web scraping, with hyperparameters tuned on a 60-question training split. Evaluation uses the BrowseComp benchmark with semantic comparison via GPT-4o.

## Key Results
- ODR+ achieves 10% success rate on BC-Small test set (60 questions)
- Proprietary systems Claude-DeepResearch and Gemini-DeepResearch achieve 0% accuracy
- Ablation studies show all three improvements (decomposition, iterative planning, structured synthesis) contribute to ODR+'s success
- ODR+ outperforms previous open-source DRA (ODR) and closed-source alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question decomposition with constraint extraction improves multi-hop research accuracy.
- Mechanism: The system first extracts explicit identifying constraints (names, dates, locations) via prompt P1, then generates targeted sub-questions via P2 that preserve these constraints. This reduces search space ambiguity and enables focused evidence gathering per constraint.
- Core assumption: Complex queries can be decomposed into fact-based sub-questions whose answers collectively identify the target.
- Evidence anchors:
  - [abstract] "We introduce three strategic improvements to ODR, resulting in the ODR+ model, which achieves a state-of-the-art 10% success rate on BC-Small"
  - [section 6.4] Ablation shows removing sub-question decomposition drops accuracy from 25% to 5% on 20-question subset
  - [corpus] Related work (DRBench, ManuSearch) confirms multi-step decomposition as standard pattern; no direct evidence on constraint extraction specifically
- Break condition: Queries requiring holistic reasoning where constraints are interdependent and cannot be isolated.

### Mechanism 2
- Claim: Iterative search with retry-based URL selection and adaptive planning improves evidence quality.
- Mechanism: Each sub-question triggers N_query=3 repeated searches to mitigate ranking volatility; top-k=3 most frequent URLs across retries are selected. Evidence analysis (P4) evaluates constraint satisfaction and dynamically adds follow-up sub-questions. Loop continues until confidence threshold or limits (D_max=6 hops, T_max=210s).
- Core assumption: Search engine ranking variability averages out across retries; relevant pages appear consistently in top results.
- Evidence anchors:
  - [section 5.2] "We observed that web browsers returned a different page ranking each time the same query was submitted and therefore we submitted the same query N_query times"
  - [section 6.4] Removing iterative planning drops accuracy from 25% to 5%
  - [corpus] ManuSearch and DeepResearcher use similar iterative patterns; no controlled study on retry mechanism specifically
- Break condition: Queries where ground-truth sources rank poorly across all search variations; adversarial or niche topics.

### Mechanism 3
- Claim: Structured output synthesis with confidence scoring and validation improves answer reliability.
- Mechanism: Prompt P5 forces LLM to produce standardized format (Explanation, Exact Answer, Confidence) based only on retrieved evidence. Confidence computed as ratio of satisfied constraints to total constraints. Missing fields trigger fallback defaults ("Unknown", 10% confidence).
- Core assumption: Constraint satisfaction correlates with answer correctness; structured output reduces hallucination.
- Evidence anchors:
  - [abstract] "ablation studies indicating that all three of our improvements contributed to the success of ODR+"
  - [section 6.4] Removing structured synthesis drops accuracy from 25% to 0%
  - [corpus] No direct corpus evidence on structured output format for DRAs
- Break condition: Queries where correct answers require inference beyond explicit constraint matching.

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: BrowseComp questions require linking evidence across multiple sources; each hop connects one piece of evidence to another.
  - Quick check question: Can you trace how "actor born in Tennessee" → TV series cast list → cross-reference with "Caribbean immigrant actor"?

- Concept: Prompt engineering for structured output
  - Why needed here: ODR+ relies on five engineered prompts (P1-P5) producing JSON or standardized formats for downstream processing.
  - Quick check question: Given a user query, can you write a prompt that extracts constraints as a JSON list?

- Concept: State management in agent loops
  - Why needed here: ODR+ maintains research state S across iterations (findings, processed URLs, sub-questions, depth, time).
  - Quick check question: What state must persist between search iterations to avoid redundant work?

## Architecture Onboarding

- Component map:
  - Module 1 (Question Decomposition): extractConstraints(P1) → generateSubQuestions(P2) → S.subquestions queue
  - Module 2 (Iterative Search): webSearch(N_query=3 retries) → selectMostFrequent(k=3 URLs) → extractFromUrls(P3) → analyzeEvidence(P4) → update S.subquestions and S.subAnswers
  - Module 3 (Response Synthesis): synthesizeResponse(P5) → structuredResponse with validation

- Critical path: User query → constraint extraction → sub-question generation → [search → extract → analyze] loop → final synthesis. The loop is the bottleneck; each iteration adds ~30-40 seconds.

- Design tradeoffs:
  - D_max=6: Higher improves multi-hop coverage but increases runtime linearly
  - k=3 URLs: Higher improves recall but increases LLM API costs
  - N_query=3 retries: Higher reduces ranking variance but triples search API calls
  - T_max=210s: Lower reduces costs but truncates complex research paths

- Failure signatures:
  - 0% accuracy with free-text output: Structured synthesis disabled
  - 5% accuracy: Missing decomposition or iterative planning (search lacks direction or adaptivity)
  - Long runtime with low confidence: Search returning irrelevant URLs; check query quality
  - Malformed JSON in analysis: P4 prompt not producing valid structure; add format examples

- First 3 experiments:
  1. Reproduce ablation on 20-question subset: Disable each module and confirm accuracy drops match Table 3 (0%, 5%, 5%).
  2. Vary N_query (1, 3, 5) on 10 questions: Measure URL overlap rate and accuracy to validate retry mechanism assumption.
  3. Inspect failure cases where ODR+ returns "Unknown": Analyze whether evidence was missing or confidence scoring was too conservative.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ODR+ performance scale with increased hyperparameter settings (e.g., deeper search depths beyond D_max=6, longer time limits, or more search retries)?
- Basis in paper: [explicit] The authors state: "We note however that increasing these hyperparameter settings may likely improve system accuracy, at the cost of increased computational cost — we did not have the resources to investigate this potentiality."
- Why unresolved: Resource constraints prevented the authors from exploring the performance-compute tradeoff frontier.
- What evidence would resolve it: A systematic study varying D_max, T_max, k, and N_query while measuring both accuracy gains and computational costs.

### Open Question 2
- Question: Can the three proposed improvements (question decomposition, iterative planning, structured synthesis) transfer effectively to other open-source DRA architectures such as DeepResearcher or WebThinker?
- Basis in paper: [inferred] The authors acknowledge that DeepResearcher and WebThinker were released during their work but were "not straightforward to reproduce." The modular nature of ODR+ improvements suggests potential generalizability, but this remains untested.
- Why unresolved: Reproducibility issues and resource constraints prevented cross-architecture evaluation.
- What evidence would resolve it: Applying the three modules to alternative DRA architectures and measuring performance changes on BrowseComp-Small.

### Open Question 3
- Question: Would ODR+ maintain its advantage over proprietary systems if those systems were permitted training on BC-Small's training split, rather than evaluated zero-shot?
- Basis in paper: [explicit] The authors note: "ODR+ was developed using a separate 60-question training split, whereas ODR, Claude-DR, and Gemini-DR were evaluated zero-shot on the test set, introducing a potentially significant disadvantage for them."
- Why unresolved: Proprietary systems could not be tuned for custom benchmarks at the time of experimentation.
- What evidence would resolve it: Comparing all systems under matched conditions—either all zero-shot, or all with training-set access—on the same test split.

## Limitations

- The 10% accuracy rate, while state-of-the-art, remains far below human-level performance on the BC-Small benchmark
- Proprietary systems (Claude-DeepResearch, Gemini-DeepResearch) achieve 0% accuracy, suggesting either extreme question difficulty or potential issues with evaluation methodology
- Reliance on GPT-4o for evaluation introduces potential circularity, as the same model family used in ODR+ determines correctness
- Limited test data (60 questions) constrains statistical confidence in the 10% accuracy claim
- Full prompt specifications (P1-P5) and complete BC-Small dataset are not publicly available, limiting independent validation

## Confidence

- **High Confidence**: The three-module architecture design (question decomposition → iterative search → structured synthesis) is clearly specified and technically sound. The ablation study methodology (disabling each module independently) is methodologically valid.
- **Medium Confidence**: The 10% accuracy improvement claim is based on limited test data (60 questions) and evaluated by a model that may share biases with the system being tested. The claim that "all three improvements contributed" is supported by ablation but could mask interactions between modules.
- **Low Confidence**: The generalization of 10% BC-Small performance to broader DRA capabilities is uncertain given the extreme difficulty of the benchmark (0% for all proprietary systems) and the small sample size.

## Next Checks

1. **Prompt Transparency**: Request and test the complete P1-P5 prompt specifications on held-out questions to verify that constraint extraction and structured synthesis work as claimed.
2. **Evaluator Independence**: Run ODR+ responses through multiple independent evaluators (human judges, different LLMs) to assess whether the 10% accuracy claim holds across evaluation methods.
3. **Scaling Analysis**: Test ODR+ with varying D_max (3, 6, 9) and N_query (1, 3, 5) on a subset of 20 questions to quantify the marginal benefit of each hyperparameter and validate the search retry assumption.