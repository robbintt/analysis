---
ver: rpa2
title: Ontology Generation using Large Language Models
arxiv_id: '2503.05388'
source_url: https://arxiv.org/abs/2503.05388
tags:
- ontology
- generation
- llms
- prompting
- ontologies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two new prompting techniques, Memoryless CQbyCQ
  and Ontogenia, for automated ontology generation from competency questions and user
  stories using Large Language Models (LLMs). The authors conduct experiments on a
  benchmark dataset of ten ontologies with 100 competency questions and 29 user stories,
  comparing three LLMs (GPT-4, o1-preview, Llama-3.1-405B).
---

# Ontology Generation using Large Language Models

## Quick Facts
- **arXiv ID:** 2503.05388
- **Source URL:** https://arxiv.org/abs/2503.05388
- **Reference count:** 40
- **Primary result:** Two new prompting techniques (Memoryless CQbyCQ and Ontogenia) for automated ontology generation using LLMs, achieving 84-91% CQ modeling accuracy

## Executive Summary
This paper introduces two novel prompting techniques for automated ontology generation from competency questions and user stories using Large Language Models. The authors develop Memoryless CQbyCQ, which processes each competency question independently to reduce context distraction, and Ontogenia, which employs a five-step metacognitive prompting process with Ontology Design Patterns. Experiments on a benchmark dataset of ten ontologies with 100 competency questions and 29 user stories compare three LLMs (GPT-4, o1-preview, Llama-3.1-405B) across multiple evaluation dimensions including OOPS! ontology metrics, structural analysis of superfluous elements, and expert qualitative assessment.

## Method Summary
The method employs two prompting techniques for ontology generation: Memoryless CQbyCQ processes each competency question independently without access to previous outputs, reducing context size by approximately 60% to prevent LLM distraction, then merges results; Ontogenia uses a five-step metacognitive prompting approach combined with explicit Ontology Design Patterns to guide the LLM through requirement interpretation, reflection, and validation. The pipeline takes natural language user stories and competency questions as input, processes them through the selected prompting technique and LLM, and outputs OWL ontologies that are evaluated using automated tools (OOPS! for pitfalls) and human experts for qualitative assessment.

## Key Results
- o1-preview with Ontogenia produces ontologies meeting ontology engineers' requirements, significantly outperforming novice engineers in modeling ability
- GPT-4 with Memoryless CQbyCQ achieves comparable results to o1-preview while reducing context distraction
- Both techniques correctly model 84-91% of competency questions, though they generate some superfluous elements
- Complex competency questions involving reification and restrictions remain challenging for current prompting techniques

## Why This Works (Mechanism)

### Mechanism 1: Context Reduction via Sub-task Decomposition
- **Claim:** Isolating competency questions into independent generation tasks reduces model "distraction" and improves coverage of requirements compared to processing all CQs in a single context.
- **Mechanism:** By stripping the "memory" of previously generated axioms during the generation of new ones, the input context size is reduced by approximately 60%. This appears to prevent the LLM from getting lost in accumulated context, reducing "irrelevant or inconsistent solutions."
- **Core assumption:** The ontology elements required for a specific CQ are largely self-contained or easily mergeable, and overlap is easier to resolve than context-induced hallucinations.
- **Evidence anchors:**
  - [Section 4.2] states that Memoryless CQbyCQ reduces context size by ~60% because "long context can result in distraction of the LLM."
  - [Section 6] notes that Memoryless CQbyCQ performed better than the standard CQbyCQ technique, "possibly suggesting that using partially generated ontology models as data 'in memory'... actually may diminish their performance."
  - [Corpus] While the provided corpus focuses on CQ generation and evaluation, the general efficacy of decomposed prompting for complex tasks is supported by "Decomposed prompting: A modular approach for solving complex tasks" (cited in the paper).
- **Break condition:** This mechanism fails if the CQs are deeply interdependent, requiring shared complex context to model correctly, though the paper suggests merging outputs resolves minor overlaps.

### Mechanism 2: Metacognitive Scaffolding (Ontogenia)
- **Claim:** Structuring the prompt as a five-step introspective process (Metacognitive Prompting) combined with explicit Ontology Design Patterns (ODPs) enables models like o1-preview to outperform novice engineers.
- **Mechanism:** The Ontogenia technique forces the LLM to interpret requirements, reflect on rules/restrictions, and validate outputs with test instances before finalizing the OWL file. This mimics an expert's "System 2" reasoning, reducing structural flaws.
- **Core assumption:** The LLM possesses sufficient latent knowledge of logic and OWL syntax to execute the metacognitive steps correctly if guided.
- **Evidence anchors:**
  - [Section 4.2] details the mapping of the XD methodology to five Metacognitive Prompting (MP) steps.
  - [Abstract] concludes that "o1-preview with Ontogenia produces ontologies... significantly outperforming novice engineers in modelling ability."
  - [Corpus] The effectiveness of reasoning models in ontology tasks is supported by "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation" (ArXiv:2504.17402).
- **Break condition:** Fails when the required reasoning exceeds the model's capacity, specifically noted in the paper regarding "reification and restrictions."

### Mechanism 3: Multidimensional Evaluation Alignment
- **Claim:** Standard ontology metrics (OOPS!) are insufficient alone; a combination of structural analysis (superfluous elements) and expert qualitative assessment is required to verify usability.
- **Mechanism:** LLMs tend to generate "superfluous" elements (correct syntax but unnecessary for the CQ). A multidimensional check filters out models (like Llama) that score well on coverage but fail on conciseness.
- **Core assumption:** High coverage of CQs does not guarantee a usable ontology if the signal-to-noise ratio (superfluous elements) is too low.
- **Evidence anchors:**
  - [Section 7] discusses that despite high CQ modeling scores, "structural analysis reveals... the two prompting techniques generate more superfluous elements."
  - [Section 6] highlights that Llama generated "numerous superfluous elements... with a rate close to 40%," despite decent coverage.
  - [Corpus] "Large Language Models Assisting Ontology Evaluation" (ArXiv:2507.14552) reinforces the need for automated assistance in the costly evaluation process.
- **Break condition:** If an ontology use case prefers over-modeling (richness) over conciseness, penalizing superfluous elements becomes counter-productive.

## Foundational Learning

- **Concept: Competency Questions (CQs) & User Stories**
  - **Why needed here:** These are the input constraints for the LLM. You must understand that CQs are not just questions but validation constraints that map directly to SPARQL queries.
  - **Quick check question:** Can you distinguish between a "Validation CQ" (used for testing) and a general question, and identify if a CQ implies a simple Object Property or a complex Reification?

- **Concept: OWL Syntax & Turtle**
  - **Why needed here:** The output of the pipeline is an OWL file in Turtle syntax. You need to read the code, not just the natural language explanation, to identify "superfluous elements" or "inverse relationships."
  - **Quick check question:** Given the statement "Author wrote Book," can you write the Turtle triple for the object property `wrote` and its inverse `writtenBy`?

- **Concept: OOPS! (OntOlogy Pitfall Scanner)**
  - **Why needed here:** This is the automated gatekeeper for the architecture. You must understand why "multiple domains" is a critical pitfall (interpreted as intersection/AND logic) rather than a union (OR).
  - **Quick check question:** If an LLM defines a property with two separate `rdfs:domain` axioms, why does OOPS! flag this as a critical issue for reasoning?

## Architecture Onboarding

- **Component map:** User Stories + Competency Questions -> Prompting Strategy Selector (Memoryless vs. Ontogenia) -> LLM (GPT-4, o1-preview, or Llama) -> Post-Processor (OWL Merger + OOPS! Scanner) -> Evaluator (Human Expert + Structural Script)

- **Critical path:**
  1. **Prompt Engineering:** defining the persona and the steps (especially for Ontogenia)
  2. **Merging:** Handling the independent outputs of Memoryless CQbyCQ without creating conflicts
  3. **Filtration:** Removing superfluous elements identified by the structural analysis script

- **Design tradeoffs:**
  - **Memoryless CQbyCQ:** Lower context cost/lower distraction vs. higher post-processing merge effort
  - **Ontogenia:** Higher quality/reasoning (o1) vs. higher API cost and complexity in prompt design
  - **Model Choice:** Llama is open/cost-effective but generates 40% superfluous elements; o1-preview is expensive but rivals novices

- **Failure signatures:**
  - **P19 Pitfall:** "Multiple domains or ranges" (LLM lists domains separately rather than as an intersection class)
  - **Superfluous Clusters:** LLM generates `employedSince` AND `employmentStartDate` for the same relationship
  - **Hallucinated Namespaces:** Llama is cited as failing to declare namespaces correctly

- **First 3 experiments:**
  1. **Baseline Run:** Run the "Hospital" story through *Memoryless CQbyCQ* with GPT-4. Measure the percentage of CQs modelled and count the superfluous classes
  2. **Comparative Run:** Run the same story through *Ontogenia* with o1-preview. Compare the "reasoning explanation" provided by Ontogenia against the raw output of the first experiment
  3. **Stress Test:** Feed a "Complex CQ" (involving Reification) to Llama-3.1. Verify if the model creates a cycle in the class hierarchy (P06 pitfall) or fails to model entirely

## Open Questions the Paper Calls Out

- **Open Question 1:** What are the root causes of "superfluous elements" in LLM-generated ontologies, and how can their generation be prevented without sacrificing requirement coverage?
  - **Basis in paper:** [explicit] The authors state that the presence of these elements "raises questions about superfluous elements: their number, why they are generated by the LLM, and their consequences."
  - **Why unresolved:** While the paper identifies that techniques like Ontogenia generate more superfluous elements than the state-of-the-art, it only proposes manual post-processing as a mitigation rather than a solution to the generation cause.
  - **What evidence would resolve it:** A causal analysis linking specific prompt structures to superfluity rates, or a new prompting strategy that significantly reduces the rate of superfluous classes and properties while maintaining CQ coverage.

- **Open Question 2:** How can prompting techniques be advanced to reliably model complex requirements involving reification and restrictions, where current models struggle?
  - **Basis in paper:** [explicit] The paper highlights that results show "significantly lower scores for complex CQs" (specifically reification and restrictions) and lists this as a key modeling error in the abstract.
  - **Why unresolved:** Current prompting techniques (Memoryless CQbyCQ and Ontogenia) show a sharp drop in performance for these complex categories compared to simple property modeling.
  - **What evidence would resolve it:** A modified prompting technique or pipeline that achieves comparable F1-scores for reification and restriction tasks as for simple Object Property modeling.

- **Open Question 3:** How can the trade-off between reducing LLM context distraction and maintaining necessary history for integrated modeling be effectively managed?
  - **Basis in paper:** [inferred] The paper notes that removing memory (Memoryless CQbyCQ) improves results by reducing distraction, but explicitly states in the limitations that this approach is "not suitable for history-dependent modelling tasks."
  - **Why unresolved:** The paper demonstrates that full memory degrades performance (distraction) while no memory limits applicability (inability to handle dependencies), leaving the optimal balance undefined.
  - **What evidence would resolve it:** A context-management strategy that filters non-essential history, demonstrating superior performance on dependency-heavy ontologies compared to the Memoryless baseline.

## Limitations

- **Sample size constraint:** Evaluation is constrained by a small sample of only 10 ontologies, raising concerns about statistical significance and generalizability.
- **Data accessibility:** The dataset was intentionally created to avoid public leakage, limiting reproducibility and independent verification of results.
- **Expert subjectivity:** Qualitative assessment by a single expert introduces potential subjectivity in comparing performance against novice engineers.

## Confidence

- **High Confidence:** The fundamental mechanism of context reduction improving performance through Memoryless CQbyCQ (supported by 60% context size reduction and comparative performance metrics)
- **Medium Confidence:** The superiority of o1-preview with Ontogenia over novice engineers (based on qualitative assessment of a single expert)
- **Low Confidence:** The generalizability of results to different domains or larger ontology corpora (limited by sample size of 10 ontologies)

## Next Checks

1. **Replication on Open Dataset:** Apply both prompting techniques to the Wine ontology or another publicly available benchmark to verify generalizability beyond the restricted dataset.
2. **Expert Panel Validation:** Conduct a multi-expert qualitative assessment to reduce subjectivity in the comparison with novice engineers.
3. **Long-context Dependency Analysis:** Systematically test interdependent CQs requiring complex shared context to validate the claimed limits of the Memoryless approach.