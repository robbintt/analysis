---
ver: rpa2
title: 'MAD: Manifold Attracted Diffusion'
arxiv_id: '2509.24710'
source_url: https://arxiv.org/abs/2509.24710
tags:
- score
- data
- extended
- inference
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MAD (Manifold Attracted Diffusion), a method
  for generating clean samples from noisy data using score-based diffusion models.
  The core idea is to exploit the manifold hypothesis, interpreting noise as low-magnitude
  variations in off-manifold directions.
---

# MAD: Manifold Attracted Diffusion

## Quick Facts
- **arXiv ID:** 2509.24710
- **Source URL:** https://arxiv.org/abs/2509.24710
- **Reference count:** 35
- **Primary result:** MAD generates clean samples from noisy data by suppressing small off-manifold variations while preserving large on-manifold variations, achieving effective denoising without special training.

## Executive Summary
MAD (Manifold Attracted Diffusion) is a method for generating clean samples from noisy data using score-based diffusion models. The core insight is that noise manifests as low-magnitude variations in off-manifold directions, while meaningful signal occupies on-manifold directions with larger variance. MAD introduces an "extended score" that acts as a soft thresholding operator, reducing small variations to near zero while preserving larger on-manifold variations. The method modifies only the inference procedure without requiring special training, making it compatible with established frameworks and pretrained models. Experiments demonstrate successful noise suppression on synthetic data, FFHQ, AFHQv2, ImageNet, and EMPIAR-11618 cryo-EM data, with the method achieving approximately twice the computational cost of standard inference.

## Method Summary
MAD modifies the probability flow ODE of diffusion models by introducing an extended score that preferentially suppresses off-manifold variations. The extended score is computed using the standard pretrained score network and its finite difference derivative with respect to noise level. During inference, MAD solves for a time-dependent parameter γ(t) that controls the strength of manifold attraction, requiring two score network evaluations per step. The method uses hyperparameters (a,b,p,δ) to control the denoising strength and operates without retraining, making it compatible with existing pretrained diffusion models following the Karras framework.

## Key Results
- Successfully generates clean samples from noisy data by suppressing off-manifold noise while preserving on-manifold signal
- Demonstrates soft thresholding behavior on synthetic data, removing low-variance variations while keeping high-variance features
- Achieves effective denoising on real datasets including FFHQ, AFHQv2, ImageNet, and EMPIAR-11618 cryo-EM data
- Compatible with pretrained models without requiring special training, though with ~2× computational cost

## Why This Works (Mechanism)

### Mechanism 1: Extended Score as Soft Thresholding Operator
The extended score H₀ treats small-variance distributions differently than large-variance ones, creating a soft thresholding effect that suppresses noise. For Dirac deltas, H₀ converges to the mode at geometric rate during inference, while for larger variance distributions it behaves like the standard score. This exploits the separation between noise (small variance, off-manifold) and signal (larger variance, on-manifold) directions.

### Mechanism 2: Off-Manifold Noise Suppression via Geometric Separation
Under the manifold hypothesis, natural images occupy low-dimensional manifolds in high-dimensional ambient space. Noise in pixel space manifests as small-magnitude variations in off-manifold directions, which MAD suppresses through its thresholding behavior. The manifold dimension being much smaller than ambient dimension creates clear separation between on-manifold and off-manifold variance scales.

### Mechanism 3: Inference-Time Modification Without Retraining
MAD modifies only the inference ODE discretization, using the standard score network to approximate the extended score via finite differences. Since S(p_σ ∗ g_γ) = S(p_{√(σ²+γ)}), a network trained to approximate scores at all σ can approximate both the score and its derivative. The derivative is approximated via finite difference requiring two network evaluations per step.

## Foundational Learning

- **Probability Flow ODE Formulation of Diffusion Models**: MAD modifies the ODE dx_t = −σ̇(t)σ(t)S_{p_σ(t)}(x_t)dt; understanding this formulation is prerequisite to grasping where the extended score plugs in.
- **Manifold Hypothesis**: The entire theoretical motivation rests on data concentrating near low-dimensional manifolds; without this, the on/off-manifold distinction is meaningless.
- **Score Function and Denoiser Relationship**: MAD's implementation uses the denoiser parameterization D(x,σ) = σ²S_{p_σ}(x) + x common in practice; understanding this transformation is necessary for correct implementation.

## Architecture Onboarding

- **Component map**: Input (schedule, pretrained model, hyperparameters) -> Root-finding for γ_i -> Two score evaluations (s_i, es_i) -> Finite difference derivative (s'_i) -> Correction factor (m_i) -> Extended score update (x_{i+1})
- **Critical path**: Verify pretrained model follows Karras framework -> Implement root-finding for γ(t) equation -> Modify inference loop to add second score evaluation -> Tune hyperparameters on validation data
- **Design tradeoffs**: b parameter controls denoising strength (higher = stronger but risk oversmoothing), p parameter controls γ(t) decay rate, δ parameter controls derivative approximation accuracy, computational cost ~2× standard inference
- **Failure signatures**: Oversmoothing (reduce b or increase p), noise persistence (increase b or check model coverage), numerical instability (increase δ or check score smoothness), mode collapse (manifold assumption may not hold)
- **First 3 experiments**: 1) Train DDPM++ on noisy synthetic shapes, verify MAD recovers clean shapes, 2) FFHQ hyperparameter sweep with varying b while holding a=2.5, p=8 fixed, 3) Noise sensitivity test on clean pretrained model with varying artificial noise levels

## Open Questions the Paper Calls Out

### Open Question 1
Can an automatic and adaptive mechanism be developed for selecting the hyperparameters γ(t)? The Conclusion states that developing adaptive parameter selection for γ(t) would enhance the method's robustness and applicability. The current implementation relies on manual tuning of parameters (a, b) based on prior knowledge of the data, which varies significantly between datasets.

### Open Question 2
How can MAD be integrated with conditional diffusion models to solve general inverse problems? The Conclusion proposes extending MAD to solve inverse problems, such as Cryo-EM denoising, by integrating it with conditional diffusion frameworks like diffusion posterior sampling. The current work focuses on unconditional generation; the mathematical framework for conditioning the extended score on measurements has not been established.

### Open Question 3
Can theoretical analysis of the extended score improve its combination with noise injection or higher-order ODE solvers? The Conclusion suggests that further theoretical analysis could lead to improved incorporation into the inference procedure, specifically mentioning noise injection and higher-order discretization. The current method relies on a heuristic update rule derived from simplified settings, and its interaction with advanced solver techniques is undefined.

## Limitations

- Strong assumption of separation between on-manifold and off-manifold variance scales may not hold uniformly across all data types or noise regimes
- Limited quantitative validation with metrics beyond qualitative visual inspection
- Lack of systematic hyperparameter robustness analysis and guidelines for parameter selection across different domains

## Confidence

- **High confidence** in mathematical formulation and theoretical guarantees for Dirac mixtures
- **Medium confidence** in practical effectiveness for natural images based on qualitative results
- **Low confidence** in hyperparameter robustness and sensitivity across diverse datasets

## Next Checks

1. **Quantitative denoising benchmark:** Apply MAD to BSD500 with synthetic noise, measure PSNR/SSIM improvement vs. standard inference across noise levels
2. **Intrinsic dimension analysis:** Estimate intrinsic manifold dimension for FFHQ, AFHQv2, ImageNet, correlate with MAD's denoising effectiveness to validate geometric separation assumption
3. **Ablation study on hyperparameters:** Systematically vary b (0.1 to 100) and p (1 to 10) on FFHQ, measure reconstruction quality and diversity metrics (FID, LPIPS) to identify optimal ranges and failure modes