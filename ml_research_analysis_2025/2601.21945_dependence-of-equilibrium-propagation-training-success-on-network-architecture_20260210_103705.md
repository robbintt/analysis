---
ver: rpa2
title: Dependence of Equilibrium Propagation Training Success on Network Architecture
arxiv_id: '2601.21945'
source_url: https://arxiv.org/abs/2601.21945
tags:
- networks
- training
- lattice
- network
- lattices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how network architecture affects the performance
  of physics-based training via equilibrium propagation. The authors examine locally
  connected lattice architectures (e.g., square, triangular) compared to all-to-all
  and dense-layer networks, using XY models and standard benchmark tasks like XOR,
  Iris, and MNIST.
---

# Dependence of Equilibrium Propagation Training Success on Network Architecture

## Quick Facts
- arXiv ID: 2601.21945
- Source URL: https://arxiv.org/abs/2601.21945
- Authors: Qingshan Wang; Clara C. Wanjura; Florian Marquardt
- Reference count: 0
- One-line primary result: Sparse locally connected lattices with skip connections can achieve performance comparable to dense networks in equilibrium propagation training, though requiring more iterations.

## Executive Summary
This paper investigates how network architecture affects the performance of equilibrium propagation (EP), a physics-based training method for neuromorphic computing. The authors systematically compare locally connected lattice architectures (square, triangular) against all-to-all and dense-layer networks using XY models on benchmark tasks including XOR, Iris, and MNIST. They find that sparse lattices with only local connections can achieve performance comparable to dense networks when equipped with skip connections, though at the cost of requiring more training iterations. The study provides practical guidelines for scaling up EP-based systems by demonstrating that physically realizable architectures with local connectivity can support effective neuromorphic computing.

## Method Summary
The authors train XY models using equilibrium propagation on benchmark tasks, comparing locally connected lattice architectures (SQ, 3NSQ, P3NSQ, 4NSQ) against all-to-all and dense-layer networks. They use energy-based models where the energy function E = -ΣWij cos(ϕi - ϕj) - Σhi cos(ϕi - ψi) and cost function C = Σ(-log(1 + cos(ϕi - ϕτi))). Training proceeds through two phases: a free phase (β=0) where the system settles without nudging, and a nudged phase (β=0.1) where weak external forces are applied to outputs. Gradients are computed from the difference between these steady states, enabling parameter updates based on local configurations. The study examines XOR on 15×15 lattices, Iris with variable depth lattices, and MNIST with locally coupled lattice architectures.

## Key Results
- Sparse lattices with local connectivity can achieve performance comparable to dense networks when equipped with skip connections
- Skip connections within lattice architectures are critical for trainability in deeper networks and complex tasks
- Equilibrium propagation enables gradient-based training using only local information from two system states
- For MNIST, locally coupled lattice networks achieve up to 96.05% accuracy, outperforming dense-layer networks of similar parameter count

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sparse lattices with local connectivity can achieve performance comparable to dense networks, but require more training iterations.
- **Mechanism:** The network self-organizes into an "affected region" connecting inputs and outputs, where node responses to input perturbations are strong and couplings are significantly strengthened. A "marginal region" remains inactive. Longer-range connections (skip connections) facilitate this by preserving long-range responses.
- **Core assumption:** The observed self-organization into affected/marginal regions is a generalizable learning dynamic in these physical systems.
- **Evidence anchors:** [abstract] "Our results show that sparse networks with only local connections can achieve performance comparable to dense networks." [section] (Page 5) The lattice "separates into an affected region... and a marginal region."

### Mechanism 2
- **Claim:** Skip connections within the lattice architecture are critical for trainability in deeper networks and complex tasks.
- **Mechanism:** Skip connections provide alternative, shorter paths for information flow, analogous to residual connections in ResNets. This helps preserve information across depth and smoothens the loss landscape, mitigating the suppression of long-range responses by growing random bias fields.
- **Core assumption:** The beneficial effect of skip connections in smoothing the loss landscape, known from ANNs, applies directly to these energy-based physical networks.
- **Evidence anchors:** [section] (Page 5, Fig. 3 caption) "...underscoring the crucial role of skip connections in facilitating successful training."

### Mechanism 3
- **Claim:** Equilibrium Propagation (EP) enables gradient-based training using only local information derived from two system states.
- **Mechanism:** Gradients are approximated by the difference between two equilibrium states: a "free phase" (no nudging) and a "nudged phase" (weak external force applied to outputs). The change in local interactions between these phases provides the parameter update signal.
- **Core assumption:** The system can reliably reach equilibrium in both phases and the nudging force is small enough for the linear approximation to hold.
- **Evidence anchors:** [section] (Page 2) "The parameter gradient is then approximated by the difference between the two steady states... enabling parameter updates based on knowledge from local configurations."

## Foundational Learning

- **Concept:** Equilibrium Propagation (EP)
  - **Why needed here:** It is the core learning rule being analyzed. Understanding its two-phase (free and nudged) procedure is essential for interpreting all experimental results.
  - **Quick check question:** How does the system state in the nudged phase differ from the free phase to compute a gradient?

- **Concept:** XY Model and Energy-Based Models
  - **Why needed here:** This is the physical substrate of the neuromorphic computer. The energy function defines the node dynamics and the "equilibrium" sought during training.
  - **Quick check question:** What does the internal energy function E of the XY model depend on?

- **Concept:** Local Connectivity and Lattice Architectures
  - **Why needed here:** The paper's main contribution is analyzing how connectivity constraints (e.g., square vs. triangular lattices) affect the mechanisms of EP.
  - **Quick check question:** What are the key architectural differences between an SQ (square) and a 4NSQ (4-neighbor square with skip connections) lattice?

## Architecture Onboarding

- **Component map:** Lattice -> Node (Spin) -> Couplings (Wij) -> Input/Output Nodes -> Cost Function (C) -> EP Controller
- **Critical path:** Define Lattice -> Data Injection -> Free Phase -> Nudged Phase -> Update -> Repeat
- **Design tradeoffs:** Lattice Geometry (higher connectivity improves trainability but may be harder to fabricate), Window Size (larger windows capture more context but increase parameters), Parameter Count (dense networks learn faster but lattices match performance with fewer parameters)
- **Failure signatures:** XOR Failure on SQ Lattice (training loss plateaus, response localizes around inputs), Deep Lattice Degradation (performance drops with depth without skip connections), Multistability (system converges to different equilibria, causing noisy gradients)
- **First 3 experiments:**
  1. XOR on SQ vs. 4NSQ: Train both architectures on XOR task. Observe that 4NSQ succeeds while SQ fails, and visualize the "affected region" to understand spatial information flow.
  2. Iris with Variable Depth: Train 4NSQ lattices on Iris dataset with increasing depth (L). Measure point at which test accuracy declines compared to dense-layer network.
  3. MNIST with LCL vs. CNN-like: Train a Locally Coupled Lattice (LCL) network on MNIST and compare accuracy and stability against CNN-like architecture with weight sharing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can physically realizable architectures be designed to emulate the information-preserving properties of residual connections in deep networks?
- Basis in paper: [explicit] The authors state that identifying physical architectures analogous to residual networks "represents a promising direction for future work."
- Why unresolved: While skip connections in lattices (e.g., 4NSQ) improved performance, the specific design principles for implementing effective residual-like connections in physical substrates remain undefined.
- What evidence would resolve it: A demonstration of a specific, physically compliant lattice geometry that explicitly replicates the gradient flow or information preservation of ResNets, showing improved accuracy over standard lattices.

### Open Question 2
- Question: What modifications to the training protocol can reduce the significant convergence time required for locally connected lattices?
- Basis in paper: [inferred] The discussion notes that training a lattice requires roughly 20,000 epochs for XOR compared to 1,000 for an all-to-all network, identifying this inefficiency as a challenge.
- Why unresolved: The paper establishes that lattices can learn but does not propose methods to accelerate the dynamics or gradient flow to match the speed of dense networks.
- What evidence would resolve it: The development of an initialization scheme or dynamics modification that achieves equivalent accuracy on benchmarks (like XOR) in an order of magnitude fewer epochs.

### Open Question 3
- Question: Do the observed training dynamics, such as the separation into "affected" and "marginal" regions, generalize to other energy-based models beyond the XY model?
- Basis in paper: [inferred] The authors limit their analysis to the XY model but explicitly state they "expect many of our qualitative results to generalize to other models."
- Why unresolved: The specific response dynamics and coupling evolution were characterized only for the XY model; it is unproven whether other physical implementations (e.g., Ising, resistor networks) exhibit the same self-organization.
- What evidence would resolve it: A comparative study showing similar spatial response evolution and coupling localization in non-oscillator systems like Ising or memristor networks.

## Limitations
- Results focus primarily on 2D lattices, leaving open questions about extension to higher-dimensional or irregular connectivity patterns
- Computational cost of sparse lattices is significantly higher than dense networks, though not comprehensively quantified
- Limited testing on complex datasets beyond MNIST, raising questions about scalability to more challenging tasks
- Physical realizability concerns not addressed, including fabrication constraints, noise, and energy efficiency

## Confidence

- **High confidence**: Skip connections improve trainability in deeper equilibrium propagation networks; sparse lattices can match dense network performance on MNIST
- **Medium confidence**: Generalization of architectural principles to arbitrary tasks; "affected region" mechanism as fundamental property of EP training
- **Low confidence**: Physical realizability and practical advantages in actual neuromorphic hardware; theoretical foundation for spatial self-organization

## Next Checks

1. **Scalability test**: Train equilibrium propagation on CIFAR-10 using the best-performing lattice architecture from MNIST experiments, measuring both accuracy and training time compared to dense networks.

2. **Convergence analysis**: Systematically measure iterations required for different lattice geometries (SQ, 3NSQ, 4NSQ) to reach equilibrium during both free and nudged phases, and analyze how this scales with network depth.

3. **Noise robustness evaluation**: Introduce Gaussian noise to node states and couplings during training and testing to assess how local connectivity and equilibrium propagation dynamics affect network robustness compared to dense networks.