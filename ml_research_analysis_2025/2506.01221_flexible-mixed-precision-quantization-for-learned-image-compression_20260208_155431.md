---
ver: rpa2
title: Flexible Mixed Precision Quantization for Learned Image Compression
arxiv_id: '2506.01221'
source_url: https://arxiv.org/abs/2506.01221
tags:
- quantization
- fmpq
- size
- compression
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational and storage challenges of
  learned image compression (LIC) models by proposing a Flexible Mixed Precision Quantization
  (FMPQ) method. Unlike traditional fixed-precision quantization, FMPQ assigns different
  bit-widths to different layers of the LIC network based on the fractional change
  in rate-distortion loss (RD-loss), thereby optimizing resource utilization.
---

# Flexible Mixed Precision Quantization for Learned Image Compression

## Quick Facts
- **arXiv ID:** 2506.01221
- **Source URL:** https://arxiv.org/abs/2506.01221
- **Authors:** Md Adnan Faisal Hossain; Zhihao Duan; Fengqing Zhu
- **Reference count:** 30
- **Primary result:** FMPQ achieves up to 2.34% BD-Rate reduction over 8-bit quantization while maintaining similar model sizes.

## Executive Summary
This paper addresses the computational and storage challenges of learned image compression (LIC) models by proposing a Flexible Mixed Precision Quantization (FMPQ) method. Unlike traditional fixed-precision quantization, FMPQ assigns different bit-widths to different layers of the LIC network based on the fractional change in rate-distortion loss (RD-loss), thereby optimizing resource utilization. The method uses an adaptive search algorithm to efficiently determine the optimal bit-width distribution under a given model size constraint, significantly reducing the search time compared to exhaustive search methods. Experimental results on three widely-used LIC models demonstrate that FMPQ achieves superior rate-distortion performance while maintaining similar model sizes, offering a flexible trade-off between BD-Rate and model size compression.

## Method Summary
FMPQ works by first calculating the sensitivity of each layer to quantization using the fractional change in RD-loss when that layer is quantized in isolation. An adaptive search algorithm then finds the optimal bit-width distribution that meets a target model size constraint by iteratively adjusting a tolerance threshold. The quantized model is then fine-tuned using Quantization-Aware Training with the RD-loss function, which directly optimizes both rate and distortion rather than just reconstruction error. The method is evaluated on Scale Hyperprior, Mean Scale Hyperprior, and Cheng Anchor 2020 models using the COCO dataset for training and Kodak/Tecnick/Clic for evaluation.

## Key Results
- FMPQ achieves up to 2.34% BD-Rate reduction compared to 8-bit fixed precision quantization
- Adaptive search converges in 6-12 iterations versus 100s-900s for exhaustive search
- The method offers flexible trade-off between BD-Rate and model size, achieving up to 4× model size reduction with minimal performance loss

## Why This Works (Mechanism)

### Mechanism 1
Assigning bit-widths based on the fractional change in Rate-Distortion (RD) loss preserves compression performance better than uniform precision. The method calculates a sensitivity metric ζ_n(b) for each layer n by quantizing it in isolation and measuring the percentage increase in RD-loss. Layers that cause a large spike in RD-loss when quantized are assigned higher bit-widths, while robust layers receive lower bit-widths. This assumes that the sensitivity of a single layer to quantization is a proxy for its contribution to the overall network performance when the whole model is quantized.

### Mechanism 2
An adaptive search algorithm reduces the time-complexity of finding the optimal bit-width distribution compared to exhaustive search. The search targets a specific model size by tuning a tolerance threshold β, using variable step sizes to adjust β, taking larger steps when far from the target size and smaller steps as it approaches the constraint. This assumes the relationship between the tolerance threshold β and the resulting model size is monotonic and smooth enough for a gradient-free search to converge efficiently.

### Mechanism 3
Fine-tuning the quantized model using the RD-loss function directly (rather than reconstruction error) recovers accuracy lost during quantization. During Quantization-Aware Training, the algorithm updates both the network weights and the quantization parameters by backpropagating the RD-loss, employing a "leaky-clip" module to mitigate vanishing gradients caused by the clipping function. This assumes the gradients approximated via the straight-through estimator are sufficient to optimize the complex interplay between rate and distortion.

## Foundational Learning

- **Rate-Distortion (RD) Loss**
  - **Why needed here:** This is the objective function the paper optimizes. It balances the size of the compressed file (Rate) against the quality of the reconstructed image (Distortion).
  - **Quick check question:** If λ in the loss function L = Rate + λ · Distortion is increased, will the resulting image likely have higher or lower quality?

- **Mixed-Precision Quantization**
  - **Why needed here:** The core proposal of the paper. It involves using different data types (e.g., 4-bit, 8-bit) for different parts of the network to balance memory usage and accuracy.
  - **Quick check question:** Why would a uniform 8-bit quantization across all layers be considered "sub-optimal" resource utilization?

- **Quantization-Aware Training (QAT)**
  - **Why needed here:** The paper uses QAT to recover performance after quantization. It simulates the effects of low-precision arithmetic during the training phase itself.
  - **Quick check question:** How does QAT differ from Post-Training Quantization (PTQ) in terms of data requirements and computational cost?

## Architecture Onboarding

- **Component map:** Input pre-trained full-precision LIC model -> FMPQ Wrapper (Sensitivity Analyzer + Adaptive Search) -> QAT Engine (with leaky-clip) -> Output mixed-precision quantized model with bit-widths per layer

- **Critical path:** 1) Run calibration to determine per-layer sensitivity ζ_n for candidate bit-widths. 2) Execute Adaptive Search to find the bit-width distribution B_θ that satisfies CR_target. 3) Initialize QAT with B_θ and fine-tune on the dataset.

- **Design tradeoffs:** BD-Rate vs. Model Size: Lowering CR_target forces the search to accept higher tolerance β, degrading BD-Rate performance. Search Speed vs. Granularity: The adaptive factors (5x, 2x) determine how quickly the search converges versus how fine-grained the final size match is (±0.01).

- **Failure signatures:** Search Divergence: The adaptive search oscillates or fails to converge if the calibration dataset is not representative. Performance Collapse: QAT fails to recover accuracy if the initial bit-width assignment was too aggressive.

- **First 3 experiments:** 1) Sensitivity Validation: Visualize the distribution of bit-widths to confirm sensitive layers receive higher bit-widths. 2) Search Efficiency Benchmark: Compare wall-clock time of Adaptive Search against grid search for fixed CR_target. 3) Ablation on QAT Loss: Train quantized model using standard MSE vs. proposed RD-Loss to verify performance difference.

## Open Questions the Paper Calls Out
- Can FMPQ be effectively extended to LIC models with autoregressive structures?
- How does reliance on a small calibration dataset affect the robustness of bit-width assignment?
- Does mixed-precision approach yield tangible inference speedups on hardware without native support for arbitrary bit-widths?

## Limitations
- The per-layer sensitivity assumption may not hold for models with strong layer coupling or bypass connections.
- The adaptive search algorithm relies on a monotonic relationship between tolerance threshold and model size that may not always exist.
- The effectiveness of the leaky-clip module is critical but its exact implementation is not provided.

## Confidence
- **High Confidence:** Overall methodology of using RD-loss sensitivity to guide bit-width assignment is well-justified and supported by experimental results.
- **Medium Confidence:** Adaptive search algorithm significantly reduces search time compared to exhaustive search, though efficiency depends on hyperparameters.
- **Medium Confidence:** QAT with RD-loss optimization recovers accuracy better than minimizing quantization error alone, but lacks ablation study comparison.

## Next Checks
1. **Sensitivity Analysis Validation:** Re-run the per-layer sensitivity calculation on the Scale Hyperprior model and verify that sensitive layers are consistently assigned higher bit-widths across multiple random seeds.
2. **Search Efficiency Benchmark:** Implement a grid search baseline and compare its wall-clock time against the adaptive search for a target compression ratio of 0.75.
3. **QAT Loss Ablation:** Train two quantized models using the same bit-width distribution: one optimizing on RD-loss and one on standard MSE, then evaluate both on Kodak and Tecnick datasets.