---
ver: rpa2
title: 'Readability Measures and Automatic Text Simplification: In the Search of a
  Construct'
arxiv_id: '2511.09536'
source_url: https://arxiv.org/abs/2511.09536
tags:
- readability
- simplification
- coca
- text
- measures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Readability Measures and Automatic Text Simplification: In the Search of a Construct

## Quick Facts
- **arXiv ID:** 2511.09536
- **Source URL:** https://arxiv.org/abs/2511.09536
- **Reference count:** 0
- **Primary result:** Readability measures generally fail to correlate with human judgment and automatic metrics in ATS evaluation.

## Executive Summary
This study investigates the construct validity of readability measures in automatic text simplification (ATS) by correlating linguistic features with human judgment and automatic metrics across sentence- and document-level simplification datasets. The authors find that readability measures typically show weak correlations with both human evaluations and established ATS metrics like BLEU and SARI. A key finding is that computing readability features on the delta (difference between original and simplified text) yields higher correlations than computing features on simplified text alone, suggesting that human judgment of simplification inherently involves comparison to the source. The study reveals that sentence-level and document-level simplification evaluation activate different readability feature families, with lexical diversity measures (TTR variants) correlating better with sentence-level tasks. The authors conclude that the three evaluation paradigms (readability measures, automatic metrics, human judgment) form a "correlation triangle" with weak pairwise alignments, indicating an undefined construct in ATS.

## Method Summary
The authors computed 1,066 linguistic features using four TAA tools (TAALED for lexical diversity, TAALES for lexical sophistication, TAASSC for syntactic sophistication, TAACO for cohesion) plus traditional readability formulas (FRE, FKGL, Dale-Chall, etc.). These features were calculated in two modes: on simplified text only and on the delta (original - simplified). Pearson correlations were computed between feature sets and evaluation dimensions (human judgments of simplicity, fluency, meaning preservation; automatic metrics BLEU, SARI/D-SARI, BERTScore, SAMSA, LENS) across two datasets (SimplicityDA for sentence-level, D-Wikipedia for document-level). The study focused on identifying which features correlate with which evaluation criteria and whether delta computation yields stronger correlations than absolute feature values.

## Key Results
- Readability measures generally show weak correlations (< |0.3|) with both human judgment and automatic metrics across datasets.
- Delta-based feature computation yields higher correlations than features computed on simplified text alone, suggesting human judgment inherently compares output to source.
- TTR features correlate better with sentence-level simplification than document-level, despite frequent use for text complexity assessment.
- The three evaluation paradigms (readability measures, automatic metrics, human judgment) show low pairwise correlations, indicating an undefined construct in ATS.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Computing readability features on the delta (difference between original and simplified text) yields higher correlations with human judgment than computing features on simplified text alone.
- **Mechanism:** The delta computation captures *simplification as a transformation*, not just *simplicity as a property*. Human judges rate simplifications relative to the original, so delta-based measures better reflect the comparison process humans perform.
- **Core assumption:** Human judgment of simplification inherently involves comparing output to source, not evaluating output in isolation.
- **Evidence anchors:**
  - [abstract] "as the three different angles from which simplification can be assessed tend to exhibit rather low correlations with one another, there is a need for a clear definition of the construct in ATS."
  - [section 5.1] "all absolute values are higher when computed on the delta rather than on simplifications only... this suggests that while the coefficient values are low, the difference between simplicity and simplification has an effect on both humans and measures."
  - [corpus] Related work (DETECT, "Toward Human-Centered Readability Evaluation") similarly notes that surface-level metrics fail to account for human judgment processes.

### Mechanism 2
- **Claim:** Sentence-level and document-level simplification evaluation activate different readability feature families.
- **Mechanism:** Sentence simplification operates on constrained context where lexical diversity measures (TTR variants) capture meaningful variation. Document simplification involves coherence and length effects that overshadow lexical diversity signals, making word count and corpus-based frequency features more relevant.
- **Core assumption:** The unit of analysis (sentence vs. document) fundamentally changes what linguistic phenomena are diagnostic of simplification quality.
- **Evidence anchors:**
  - [section 5.1] "TTR features correlate better with sentence-level simplification than with document-level simplification... It appears quite surprising to see that TTR features correlate better with sentence-level simplification than with document-level simplification, as TTR it is frequently used for roughly assessing the complexity of a text."
  - [section 5.2] "COCA-based features are present in all criteria with D-wiki, while they are only present for delta-samsa with SimplicityDA."
  - [corpus] Weak/indirect—corpus papers focus on single evaluation contexts, not cross-level comparison.

### Mechanism 3
- **Claim:** The three evaluation paradigms (readability measures, automatic ATS metrics, human judgment) form a "correlation triangle" with weak pairwise alignments, indicating an undefined construct.
- **Mechanism:** Each paradigm measures a different latent variable: readability measures capture surface linguistic properties; automatic metrics capture n-gram/similarity overlap; humans judge meaning preservation, fluency, and perceived simplicity. Without a shared theoretical construct, these diverge.
- **Core assumption:** A well-defined task should show convergence across evaluation methods; low correlations indicate measurement mismatch or construct ambiguity.
- **Evidence anchors:**
  - [abstract] "as the three different angles from which simplification can be assessed tend to exhibit rather low correlations with one another, there is a need for a clear definition of the construct in ATS."
  - [section 6] "our findings shed light onto a lack in the ATS ecosystem: a well-defined construct."
  - [corpus] "Toward Human-Centered Readability Evaluation" explicitly states BLEU, FKGL, and SARI "fail to account" for human judgment; DETECT paper notes general-purpose metrics "insufficiently capture simplification quality."

## Foundational Learning

- **Concept: Construct Validity in NLP Tasks**
  - **Why needed here:** The paper's central argument is that ATS lacks a well-defined construct—researchers use different evaluation proxies without theoretical agreement on what "simplification" actually is.
  - **Quick check question:** If you replaced "simplification" with "paraphrasing" in the task definition, would your evaluation metrics change? If not, you may have a construct validity problem.

- **Concept: Delta-based vs. Absolute Feature Computation**
  - **Why needed here:** The paper shows that measuring change (delta) between source and output yields different correlation patterns than measuring output alone, which has implications for evaluation design.
  - **Quick check question:** For a text editing task, should you evaluate the edited text alone, or the difference between original and edited? What does each approach assume about your evaluation criteria?

- **Concept: Type-Token Ratio (TTR) and Lexical Diversity**
  - **Why needed here:** TTR variants appear frequently in the paper's correlation results, yet behave differently across sentence- vs. document-level tasks, indicating sensitivity to text length.
  - **Quick check question:** Why might TTR be problematic for comparing texts of different lengths? How do variants like root TTR or MTLD attempt to address this?

## Architecture Onboarding

- **Component map:** Original text + simplified text pairs (sentence or document level) -> Four TAA tool families (TAALED, TAALES, TAASSC, TAACO) + traditional formulas -> Feature extraction (1,066 features) -> Two computation modes (simple-only, delta) -> Pearson correlation analysis with human judgments and automatic metrics -> Ranked feature lists by correlation strength

- **Critical path:**
  1. Dataset selection (must have human judgments AND computable automatic metrics)
  2. Feature computation in both modes (simple-only, delta)
  3. Correlation matrix generation with significance filtering
  4. Cross-dataset comparison (sentence vs. document patterns)

- **Design tradeoffs:**
  - **Feature quantity vs. interpretability:** 1,066 features provide coverage but produce sparse meaningful signals; no guidance on feature selection
  - **Sentence vs. document evaluation:** Different datasets, metrics, and correlation patterns—cannot generalize across levels
  - **Delta computation assumption:** Assumes original text is appropriate baseline; may not hold for poorly written originals

- **Failure signatures:**
  - Correlations consistently < |0.3| across all feature-evaluation pairs (observed in this study)
  - Different feature families correlating with different evaluation dimensions (no convergent validity)
  - Sentence-level features failing to transfer to document-level (TTR pattern)
  - Human judgment criteria (fluency, simplicity, meaning) showing different feature correlation profiles

- **First 3 experiments:**
  1. **Replicate correlation analysis on a new language** using the same four tool families to test whether low-correlation pattern generalizes beyond English; expect similar weak correlations if construct problem is task-inherent.
  2. **Feature selection intervention:** Apply dimensionality reduction (PCA or feature importance from a trained model) to identify whether a subset of features yields higher correlations; if correlations remain low, this confirms construct mismatch rather than feature noise.
  3. **Human evaluation redesign:** Collect human ratings using both absolute (simplicity) and relative (simplification quality) scales on the same outputs to empirically test the delta mechanism—do delta features correlate better with relative ratings?

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the field of Automatic Text Simplification (ATS) establish a unified "construct" that aligns readability measures, automatic metrics, and human judgment?
- **Basis in paper:** [explicit] The authors conclude that low correlations across evaluation angles highlight "a need for a clear definition of the construct in ATS."
- **Why unresolved:** The study confirmed that standard readability formulas and complex linguistic features generally fail to correlate well with human judgment or automatic metrics (BLEU, SARI), leaving the field without a valid theoretical ground truth.
- **What evidence would resolve it:** The proposal and validation of a new evaluation framework where specific, selected readability features demonstrate high, statistically significant correlation with human assessments of simplicity and meaning preservation.

### Open Question 2
- **Question:** Why do lexical diversity measures (like Type-Token Ratio) correlate with evaluation metrics for sentence-level simplification but disappear in document-level simplification?
- **Basis in paper:** [inferred] The results show TTR features are prevalent in sentence-level correlations (SimplicityDA) but are "completely absent" for document-level correlations (D-Wikipedia), which the authors find "quite surprising."
- **Why unresolved:** The paper identifies this discrepancy but lacks the experimental scope to determine if this is due to dataset artifacts, the specific nature of document-level discourse, or the aggregation methods used for document metrics.
- **What evidence would resolve it:** A controlled analysis of lexical diversity features across varying text lengths and aggregation strategies to determine if the metric loses validity or if the human perception of complexity shifts at the document level.

### Open Question 3
- **Question:** Do the low correlations observed between readability measures and ATS evaluation metrics persist across languages other than English?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section: "These findings may vary on other corpora, other languages, and with other human annotators."
- **Why unresolved:** The study was restricted to English datasets (SimplicityDA and D-Wikipedia) and tools (e.g., COCA-based features), making it unclear if the disconnect between readability features and simplification scores is universal or language-specific.
- **What evidence would resolve it:** A replication of the correlation study on non-English ATS datasets (e.g., Spanish or German simplification corpora) using language-specific readability tools.

## Limitations
- Findings may not generalize beyond English-language datasets and tools (COCA-based features).
- Cross-dataset comparisons limited by different simplification tasks (sentence vs. document level) and evaluation frameworks.
- Delta computation assumes original text is valid baseline, which may not hold for low-quality sources.
- TAA tools provide extensive coverage but lack interpretability guidance for feature selection.

## Confidence
- **High confidence:** Weak correlations between evaluation paradigms are reproducible and consistent with prior work (DETECT, Toward Human-Centered Readability Evaluation).
- **Medium confidence:** Delta-based computation advantage over simple-only features is supported but requires further validation across diverse datasets and simplification types.
- **Medium confidence:** Differential behavior of TTR features between sentence and document levels is observed but may reflect dataset-specific properties rather than fundamental task differences.

## Next Checks
1. **Cross-linguistic replication:** Apply the same feature correlation analysis to German or Spanish simplification datasets to test whether weak correlations between evaluation paradigms are task-inherent rather than language-specific.
2. **Controlled construct manipulation:** Design an experiment where human raters evaluate the same simplifications using both absolute simplicity scales and relative simplification quality scales, then test whether delta features correlate better with relative judgments.
3. **Feature reduction intervention:** Apply PCA or feature importance ranking to identify whether a small subset of highly correlated features can achieve stronger convergent validity across evaluation paradigms, distinguishing construct ambiguity from feature noise.