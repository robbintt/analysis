---
ver: rpa2
title: 'Explainable AI for Clinical Outcome Prediction: A Survey of Clinician Perceptions
  and Preferences'
arxiv_id: '2502.20478'
source_url: https://arxiv.org/abs/2502.20478
tags:
- clinical
- methods
- participants
- patient
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study surveyed 32 clinical practitioners to understand their
  preferences for explainable AI (XAI) techniques when interpreting model predictions
  over text-based EHR data. Four XAI methods (LIME, attention-based span highlights,
  exemplar patient retrieval, and free-text rationales from LLMs) were implemented
  on an ICU admission note-based mortality prediction model.
---

# Explainable AI for Clinical Outcome Prediction: A Survey of Clinician Perceptions and Preferences

## Quick Facts
- arXiv ID: 2502.20478
- Source URL: https://arxiv.org/abs/2502.20478
- Authors: Jun Hou; Lucy Lu Wang
- Reference count: 36
- Primary result: Clinicians prefer free-text rationales from LLMs over attention-based, LIME, and exemplar retrieval explanations for text-based EHR mortality prediction

## Executive Summary
This study surveyed 32 clinical practitioners to understand their preferences for explainable AI (XAI) techniques when interpreting model predictions over text-based EHR data. Four XAI methods (LIME, attention-based span highlights, exemplar patient retrieval, and free-text rationales from LLMs) were implemented on an ICU admission note-based mortality prediction model. Participants preferred free-text rationales most, finding them most understandable and reasonable, while attention-based explanations were least preferred. Similar patient retrieval was valued for its evidence-based approach but ranked lowest overall. The study highlights the importance of creating both efficient, generalized tools and specialist-sensitive options tailored to varying clinical expertise levels.

## Method Summary
The researchers implemented four XAI methods on a UmlsBERT model fine-tuned for ICU admission note-based mortality prediction. These methods included LIME for feature attribution, attention-based span highlights, exemplar patient retrieval with NER highlighting, and GPT-4 generated free-text rationales. Thirty-two clinical practitioners participated in a survey where they evaluated each method's understandability, reasonableness, and trustworthiness using a 5-point Likert scale and ranked their preferences. The study used MIMIC-III data for the prediction task, achieving 87.86 micro-F1 and 66.43 macro-F1 scores.

## Key Results
- Free-text rationales from LLMs were most preferred for understandability and reasonableness
- Attention-based explanations were least preferred, with zero first-place rankings
- Similar patient retrieval was valued for evidence but ranked lowest overall
- Clinical workflow context significantly influenced method preferences (urgent vs. non-urgent settings)
- LIME explanations required threshold tuning to avoid noise and maintain clarity

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Alignment with Clinical Reasoning
- **Claim:** Free-text rationales are preferred because they mirror natural clinical communication patterns.
- **Mechanism:** Natural language explanations reduce cognitive translation burden by presenting information in the same format clinicians use for reasoning and handoffs. Participants described rationales as functioning "like a nurse's note for communicating with a doctor" in time-sensitive scenarios.
- **Core assumption:** Clinicians process narrative explanations faster than visual feature mappings because this matches their training and workflow.
- **Evidence anchors:**
  - [abstract] "Participants preferred free-text rationales most, finding them most understandable and reasonable"
  - [section] P27 mentioned generated content can function similarly to a nurse's note; P17 indicated rationales help explain outcomes to families in non-technical terms
  - [corpus] Limited direct corpus support; related papers focus on XAI evaluation frameworks rather than cognitive mechanisms
- **Break condition:** If rationales contain hallucinations or excessive jargon, understandability degrades (P4 noted jargon reduces clarity; P8 noted weak reasoning quality)

### Mechanism 2: Evidence-Grounded Trust Building
- **Claim:** Exemplar-based explanations build trust even when not preferred for efficiency.
- **Mechanism:** Similar patient retrieval provides verifiable, concrete evidence that clinicians can cross-reference against their experience. This mimics case-based reasoning common in medical training.
- **Core assumption:** Trust requires grounding in auditable evidence, not just plausible explanations.
- **Evidence anchors:**
  - [abstract] "Similar patient retrieval was valued for its evidence-based approach but ranked lowest overall"
  - [section] "Similar patient retrieval... demonstrated a greater ability to build trust than feature-based methods such as LIME"; participants valued historical cases as evidence
  - [corpus] "Bridging the Trust Gap" paper validates hybrid XAI combining fuzzy logic with SHAP for trust in resource-constrained settings
- **Break condition:** If retrieval quality is poor (non-similar patients) or NER highlighting is inconsistent (P27 noted shorthand/abbreviations cause confusion), trust benefit is lost

### Mechanism 3: Workflow-Context Specificity
- **Claim:** No single XAI method is optimal; preference depends on clinical workflow urgency and decision stage.
- **Mechanism:** Urgent settings (ICU, surgery) prioritize rapid visual scanning via highlights; less acute phases (post-surgical planning) reward detailed rationales for analysis.
- **Core assumption:** Time pressure mediates the acceptable tradeoff between explanation depth and speed.
- **Evidence anchors:**
  - [section] P17: "Highlight saves time and we need that. If we had more time... free text rationale gives more in-depth reasoning"
  - [section] Discussion notes XAI methods must be "tailored to specific clinical workflows"
  - [corpus] "Before the Clinic" paper emphasizes operable design principles aligned with clinical governance requirements
- **Break condition:** Applying detailed methods in urgent contexts creates workflow friction; applying simple highlights in planning contexts underutilizes clinician analytical capacity

## Foundational Learning

- **Concept: Post-hoc vs. Ante-hoc Explainability**
  - **Why needed here:** All four methods (LIME, attention, retrieval, LLM rationales) are post-hocâ€”they explain predictions after the model makes them. This introduces fidelity risk: the explanation model differs from the prediction model.
  - **Quick check question:** Can you articulate why LIME explanations might misrepresent what the UmlsBERT model actually attended to?

- **Concept: Clinical Decision Support System (CDSS) Integration Points**
  - **Why needed here:** XAI methods must fit into existing workflows (admission, diagnosis, treatment planning, discharge). The study reveals different preferences at different stages.
  - **Quick check question:** Which XAI method would you deploy for an ICU admission triage tool vs. a post-discharge follow-up prioritization system?

- **Concept: Fidelity vs. Interpretability Tradeoff**
  - **Why needed here:** Model-specific methods (attention) guarantee fidelity to model behavior but may highlight meaningless tokens ("of", "with"). Model-agnostic methods (LIME, retrieval, rationales) approximate behavior but may diverge from actual model reasoning.
  - **Quick check question:** Why did attention-based explanations receive zero first-place rankings despite being the most faithful to model internals?

## Architecture Onboarding

- **Component map:** Data preprocessing (admission note extraction) -> UmlsBERT encoding -> prediction -> parallel XAI generation -> visualization layer -> clinician interface
- **Critical path:** Data preprocessing (admission note extraction) -> UmlsBERT encoding -> prediction -> parallel XAI generation -> visualization layer -> clinician interface. The visualization formatting (color intensity, NER highlighting) directly impacted perceived understandability.
- **Design tradeoffs:**
  - **LIME:** High granularity (word-level) but requires threshold tuning (percentile variable) to avoid noise
  - **Attention:** Zero additional compute but highlights stopwords; single-color visualization limits interpretability
  - **Retrieval:** Evidence-grounded but requires labeled similarity data (unavailable) and retrieval quality tuning
  - **LLM rationales:** Most preferred but lacks grounded evidence; hallucination risk not systematically evaluated
- **Failure signatures:**
  - Attention highlighting stopwords ("of", "with") signaled low relevance to participants
  - Retrieval showing dissimilar chief complaints indicated poor embedding quality
  - Rationales with "weak reasoning" (e.g., medication allergy as mortality factor) eroded credibility
  - Visualizations requiring legend interpretation caused confusion
- **First 3 experiments:**
  1. **Hybrid XAI baseline:** Combine free-text rationales with retrieved exemplars to test whether evidence-grounding improves trust without sacrificing understandability (per Discussion recommendation)
  2. **Attention filtering A/B test:** Filter attention weights to exclude stopwords and function words; measure whether preference ranking improves
  3. **Workflow-stratified evaluation:** Recruit separate cohorts for urgent vs. non-urgent clinical roles; test hypothesis that preference distribution shifts by context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can clinicians effectively utilize XAI interfaces to identify hallucinations or errors in LLM-powered clinical decision support systems?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that they "plan to explore whether clinicians can effectively use XAI to identify hallucinations in LLM-powered decision support and mitigate the risks introduced by such systems."
- Why unresolved: The current study focused on clinician preferences and perceived utility of explanations for model predictions, but did not test the ability of these explanations to help users detect model errors or ungrounded reasoning.
- What evidence would resolve it: A user study design where clinicians are tasked with identifying specific hallucinated details in LLM outputs, comparing their detection rates when assisted by different XAI methods versus a control group.

### Open Question 2
- Question: How do clinician preferences for XAI methods shift when applied to multimodal data (structured vitals/labs combined with text) rather than text-only EHR data?
- Basis in paper: [explicit] The authors acknowledge they used only free-text admission notes and state, "Future work should explore multimodal outcome prediction models as well as other clinical predictive tasks."
- Why unresolved: The study's findings are specific to text-based inputs; however, participants emphasized that real-world ICU workflows rely heavily on both structured (vitals) and unstructured data, limiting the generalizability of current preference rankings.
- What evidence would resolve it: A replication of the survey methodology using a multimodal prediction model, requiring the adaptation of XAI methods (like LIME or Attention) to explain feature importance across both text and structured data types simultaneously.

### Open Question 3
- Question: Does combining free-text rationales with evidence-based exemplars (similar patient retrieval) improve clinician trust and perceived groundedness compared to rationales alone?
- Basis in paper: [inferred] In the Discussion, the authors suggest that "Combining free-text rationales with retrieved exemplars... could help address these issues" related to the lack of grounded evidence in free-text explanations.
- Why unresolved: Participants preferred free-text rationales but criticized their lack of evidence, whereas similar patient retrieval offered evidence but was ranked lowest in preference. It is unknown if a hybrid approach would yield a net positive utility.
- What evidence would resolve it: The development and evaluation of a hybrid XAI system that generates free-text explanations explicitly linked to retrieved similar patient cases, measuring if this improves "trust" and "reasonableness" scores over the baseline methods.

## Limitations
- Single-model setup (UmlsBERT on MIMIC-III) and specific prediction task (ICU mortality) limit generalizability
- Convenience sampling method and single-institution recruitment limit healthcare system generalizability
- Evaluation focused on understandability and perceived reasonableness rather than clinical actionability
- LLM-generated rationales were not systematically evaluated for hallucination rates or factual consistency

## Confidence
- **High Confidence**: Free-text rationales being most preferred for understandability and reasonableness; attention-based explanations being least preferred due to highlighting meaningless tokens
- **Medium Confidence**: The workflow-context specificity hypothesis (urgent vs. non-urgent settings preferring different methods); the evidence-grounded trust building mechanism for retrieval methods
- **Medium Confidence**: The cognitive alignment mechanism explaining why clinicians prefer narrative explanations that mirror clinical communication patterns

## Next Checks
1. **Hybrid XAI Effectiveness Test**: Implement and evaluate a combined approach pairing LLM rationales with retrieved similar patient cases to determine if grounding explanations in evidence improves trust without sacrificing understandability
2. **Attention Filtering Validation**: Conduct an A/B test comparing raw attention weights against filtered versions that exclude stopwords and function words, measuring changes in clinician preference rankings
3. **Workflow-Stratified Evaluation**: Recruit separate participant cohorts representing urgent care (ICU, emergency) versus non-urgent care (outpatient, chronic disease management) to test whether XAI method preferences shift predictably based on clinical context and time pressure