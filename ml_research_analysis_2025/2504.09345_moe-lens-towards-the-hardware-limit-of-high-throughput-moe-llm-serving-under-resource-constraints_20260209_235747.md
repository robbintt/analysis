---
ver: rpa2
title: 'MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under
  Resource Constraints'
arxiv_id: '2504.09345'
source_url: https://arxiv.org/abs/2504.09345
tags:
- cache
- memory
- throughput
- decode
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high-throughput inference
  for Mixture-of-Experts (MoE) Large Language Models (LLMs) in resource-constrained
  environments, where GPU memory is insufficient to hold the entire model. The key
  issue is effectively utilizing both CPU and GPU resources while managing the overhead
  of transferring model weights from CPU to GPU.
---

# MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints

## Quick Facts
- arXiv ID: 2504.09345
- Source URL: https://arxiv.org/abs/2504.09345
- Authors: Yichao Yuan; Lin Ma; Nishil Talati
- Reference count: 40
- Primary result: Achieves 4.6x average speedup (up to 25.5x) over MoE-Lightening for MoE LLM inference under GPU memory constraints

## Executive Summary
This paper addresses the challenge of high-throughput inference for Mixture-of-Experts (MoE) Large Language Models (LLMs) in resource-constrained environments, where GPU memory is insufficient to hold the entire model. The key issue is effectively utilizing both CPU and GPU resources while managing the overhead of transferring model weights from CPU to GPU. Prior approaches, including MoE-Lightening, have been limited by performance models that don't capture complex hardware interactions and system execution mechanisms.

The paper introduces MoE-Lens, a system designed to approach hardware limits for MoE LLM inference. It employs a two-stage holistic performance model: Stage 1 analyzes theoretical performance bounds based on CPU memory capacity, GPU compute power, and workload characteristics; Stage 2 captures system execution mechanisms like workload scheduling and paged KV cache effects to accurately predict achievable throughput. Informed by this model, MoE-Lens features a resource-aware scheduler for overlapping prefill and decode phases, an execution engine with a contiguous data mover for efficient weight transfers, and an optimized CPU-based attention implementation. Evaluated on diverse MoE models and datasets, MoE-Lens achieves an average 4.6x (up to 25.5x) speedup over the state-of-the-art MoE-Lightening, with the theoretical model predicting performance with an average 94% accuracy.

## Method Summary
MoE-Lens employs a two-stage performance model to optimize MoE LLM inference under GPU memory constraints. Stage 1 establishes theoretical throughput upper bounds by analyzing CPU memory capacity, GPU compute power, and workload characteristics. Stage 2 refines these predictions by modeling system execution mechanisms including workload scheduling and paged KV cache effects. The system implements a resource-aware scheduler that overlaps prefill and decode phases to maximize CPU memory utilization, an execution engine with a contiguous data mover that transfers model weights in 100MB packets to avoid IO stalls, and an optimized CPU-side attention implementation using hand-optimized SIMD intrinsics. The approach is validated across multiple MoE models (Mixtral-8x7B, Mixtral-8x22B, DBRX) and datasets, demonstrating significant speedups over prior state-of-the-art methods.

## Key Results
- Achieves 4.6x average speedup over MoE-Lightening, with up to 25.5x improvement on certain workloads
- Performance model predicts throughput with 94% average accuracy
- CPU memory capacity for KV cache storage, not PCIe bandwidth, is the primary limiting factor for GPU utilization
- Prefill-decode overlap improves CPU memory efficiency by a factor of (p+g)/(p+g/2)
- CPU memory bandwidth contention between attention computation and weight transfers becomes significant at 210GB+ KV cache sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CPU memory capacity for KV cache—not just PCIe bandwidth—fundamentally limits achievable GPU utilization in resource-constrained MoE inference.
- **Mechanism:** The number of parallel tokens required to saturate GPU compute (n ≥ C_GPU/B × N_e/N_k) translates to a KV cache size that often exceeds available CPU memory. With limited concurrent sequences, weight transfer overhead cannot be amortized, leaving GPU underutilized.
- **Core assumption:** The system operates in a regime where model size >> GPU memory, forcing weight streaming from CPU.
- **Evidence anchors:**
  - [abstract]: "CPU memory capacity, GPU compute power, workload characteristics... to understand the theoretical performance upper bound"
  - [Section 5.1, Table 2]: Quantifies KV cache sizes (614GB–2554GB) needed to saturate different GPUs; "CPU memory capacity for KV cache storage is a limiting factor"
  - [corpus]: Related work on expert offloading (Taming Latency-Memory Trade-Off) confirms memory capacity as a primary constraint, though corpus lacks direct validation of the PME formulation.
- **Break condition:** If GPU memory were sufficient to store all weights, or if PCIe bandwidth dropped to near-zero, this mechanism would no longer dominate.

### Mechanism 2
- **Claim:** Overlapping prefill and decode stages increases effective KV cache capacity and smooths resource utilization imbalance.
- **Mechanism:** Prefill tokens amortize memory cost across parallel computation (high memory efficiency), while decode tokens generate one-at-a-time (low efficiency). Overlapping allows early sequence completions to release memory, reducing peak consumption. The effective KV cache capacity expands by a factor of (p+g)/(p+g/2).
- **Core assumption:** Sequences have heterogeneous lengths and can be scheduled dynamically; preemption overhead is acceptable.
- **Evidence anchors:**
  - [abstract]: "resource-aware scheduler for prefill and decode phases, an execution engine that overlaps their computation"
  - [Section 5.4]: "prefill-decode overlap effectively improves the CPU memory usage... reduces the peak memory consumption"
  - [corpus]: RAPID-Serve and other systems use prefill-decode disaggregation, but corpus does not provide comparative data on memory efficiency gains specifically.
- **Break condition:** If all sequences had identical lengths and synchronized completion, overlap benefits would diminish.

### Mechanism 3
- **Claim:** VSLPipe's software-pipelined schedule with a dedicated Contiguous Data Mover maximizes CPU-GPU bandwidth utilization by hiding weight transfer latency behind computation.
- **Mechanism:** Weight transfers are partitioned into fine-grained packets (100MB) issued asynchronously, avoiding head-of-line blocking with compute-related transfers. CPU attention and GPU GEMM execute concurrently across partitioned token groups (α, β), with synchronization only at stage boundaries.
- **Core assumption:** CPU memory bandwidth can sustain concurrent attention computation and weight transfers without severe contention.
- **Evidence anchors:**
  - [abstract]: "efficient weight transfer, and optimized CPU-side attention execution"
  - [Section 6.5]: "eliminates IO stalls during weight transfers... prevents contention with other CPU-GPU transfers"
  - [corpus]: MoE-Gen uses module-based batching for single-GPU throughput; corpus does not provide direct validation of the packetized transfer strategy.
- **Break condition:** If CPU memory bandwidth were saturated by attention alone (observed at 210GB KV cache with long generation), weight transfers slow down, breaking the overlap assumption.

## Foundational Learning

- **Concept:** Mixture-of-Experts (MoE) sparsity and routing
  - **Why needed here:** Understanding that only top-k experts activate per token, yet all weights must reside in memory, is essential to grasping the memory-pressure problem.
  - **Quick check question:** For Mixtral-8x7B with N_e=8, N_k=2, what fraction of expert parameters are active per token?

- **Concept:** Prefill vs. decode stages in LLM inference
  - **Why needed here:** The paper's scheduler fundamentally exploits the different compute/memory characteristics of these stages.
  - **Quick check question:** Which stage is typically compute-bound, and which is memory-bound?

- **Concept:** Arithmetic intensity and the roofline model
  - **Why needed here:** The Stage 1 model uses intensity (I ≈ n × N_k/N_e) to derive GPU saturation conditions.
  - **Quick check question:** If intensity falls below C_GPU/B, what becomes the bottleneck?

## Architecture Onboarding

- **Component map:** Pipeline Profiler → Resource-Aware Scheduler (Prefill + Decode) → Execution Engine (VSLPipe) → Contiguous Data Mover → CPU Attention Kernel → GPU GEMM
- **Critical path:** Weight transfer (CPU→GPU) → GPU GEMM (MoE layers) → CPU attention (decode) → result return. If any stage exceeds δ = Model Size / B_IO, throughput degrades.
- **Design tradeoffs:**
  - Larger KV cache → more parallel tokens but higher CPU memory bandwidth pressure
  - Smaller weight packets → less head-of-line blocking but more overhead
  - Aggressive preemption → frees memory but incurs recomputation cost
- **Failure signatures:**
  - GPU utilization <50% with large KV cache → likely CPU memory bandwidth contention between attention and transfers
  - Frequent preemption cycles → KV cache undersized for generation length
  - Throughput plateau despite larger batches → hit GPU compute bound
- **First 3 experiments:**
  1. Profile baseline: Run Mixtral-8x7B on A40 with 70GB KV cache, measure GPU utilization and throughput; verify model prediction matches within 94%.
  2. Ablate prefill-decode overlap: Disable overlap, compare throughput at p=100, g=128; expect degradation proportional to PME loss.
  3. Stress CPU bandwidth: Run with 210GB KV cache and g=256; observe if attention time approaches IO time, indicating bandwidth saturation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the contention between CPU attention computation and CPU-GPU weight transfers be resolved when both compete for shared memory bandwidth in large KV cache scenarios?
- **Basis in paper:** [explicit] In Section 8.2, the authors observe: "When the KV cache is large, the CPU attention computation must scan through a substantial number of KV cache blocks, concurrent with CPU-GPU weight transfers for GPU computation. These two operations contend for shared CPU-side resources, particularly memory bandwidth." They note this slows weight transfers from ~5 to ~6 seconds.
- **Why unresolved:** The paper identifies the problem and observes its effect but does not propose or evaluate solutions to mitigate this bandwidth contention.
- **What evidence would resolve it:** A system design or scheduling strategy that decouples or prioritizes these competing bandwidth demands, with measurements showing reduced transfer latency under contention.

### Open Question 2
- **Question:** Can the MoE-Lens approach be extended to online, latency-sensitive serving scenarios while maintaining its throughput advantages?
- **Basis in paper:** [explicit] The paper states in Section 1: "Similar to prior works [9, 19, 38, 45], our focus is offline, batching processing inference tasks" and distinguishes this from "traditional LLM serving systems [26, 33, 36] optimized for latency-sensitive applications like chatbots and code completion, where low response time is critical."
- **Why unresolved:** The design explicitly trades off latency for throughput; the scheduling, pipelining, and preemption mechanisms may not meet the latency constraints of interactive applications.
- **What evidence would resolve it:** Adaptation of the prefill-decode overlap and resource-aware scheduling to latency-bounded workloads, with latency/throughput Pareto curves demonstrating viability.

### Open Question 3
- **Question:** How does MoE-Lens performance scale across diverse hardware configurations with different CPU-GPU bandwidths, CPU vector ISA capabilities, and memory channel counts?
- **Basis in paper:** [inferred] All experiments use a specific setup (dual Intel Platinum 8380 CPUs, NVIDIA A40 GPUs, 8 DDR4-3200 channels). Section 6.6 notes "throughput gain saturates beyond 20 threads, likely due to memory controller contention," indicating hardware-specific bottlenecks. The model in Section 5 uses hardware-specific parameters ($B_{IO}$, $C_{GPU}$) but generalization is untested.
- **Why unresolved:** The performance model is parameterized, but empirical validation on other platforms (e.g., AMD CPUs, consumer GPUs, different PCIe generations) is absent.
- **What evidence would resolve it:** Evaluation on at least two additional hardware platforms spanning different CPU/GPU vendors and bandwidth tiers, with analysis of model prediction accuracy.

### Open Question 4
- **Question:** What is the optimal packet size granularity for the Contiguous Data Mover under varying model sizes, PCIe generations, and concurrent transfer workloads?
- **Basis in paper:** [inferred] Section 6.5 states: "Empirically, a 100MB packet size strikes a good balance between transfer throughput and minimizing interference." This is a single empirical choice without systematic analysis of the design space or sensitivity to workload/hardware variations.
- **Why unresolved:** The 100MB value is justified only by empirical observation on one configuration; whether this generalizes or is Pareto-optimal remains unclear.
- **What evidence would resolve it:** A sensitivity study varying packet size across model sizes (47B–141B+ parameters) and PCIe bandwidths, measuring throughput and interference metrics.

## Limitations
- Performance model validation limited to specific hardware configurations without variance bounds or statistical analysis
- CPU memory bandwidth contention between attention computation and weight transfers not addressed with mitigation strategies
- Packet size optimization (100MB) based on empirical tuning without sensitivity analysis across diverse workloads
- Hand-optimized AVX512 intrinsics limit generalizability to non-Intel architectures
- No latency-sensitive serving evaluation despite throughput improvements

## Confidence
- **High Confidence:** The fundamental insight that CPU memory capacity for KV cache, not PCIe bandwidth, limits GPU utilization in resource-constrained MoE inference
- **Medium Confidence:** The PME's 94% accuracy claim and prefill-decode overlap mechanism, based on limited evaluation sets
- **Low Confidence:** The optimal packet size (100MB) for the contiguous data mover and hand-optimized CPU attention kernel performance

## Next Checks
1. **PME Robustness Validation:** Replicate the Stage 1 model on different GPU architectures (A100, MI300X) with varying memory bandwidths, measuring prediction accuracy across 5+ model sizes and datasets with mean absolute percentage error and confidence intervals.

2. **Memory Bandwidth Contention Threshold:** Systematically vary KV cache size and decode parallelism to identify exact thresholds where CPU memory bandwidth contention occurs, characterizing the relationship between cache size, generation length, and throughput degradation.

3. **Packet Size Sensitivity Analysis:** Implement the contiguous data mover with variable packet sizes (10MB-500MB) and measure weight transfer efficiency, CPU-GPU bandwidth utilization, and overall throughput to identify optimal packet size as a function of PCIe bandwidth and KV cache characteristics.