---
ver: rpa2
title: 'FedConv: A Learning-on-Model Paradigm for Heterogeneous Federated Clients'
arxiv_id: '2502.20639'
source_url: https://arxiv.org/abs/2502.20639
tags:
- uni00000013
- uni00000014
- clients
- uni00000048
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedConv addresses the problem of heterogeneous system resources
  in federated learning by introducing a novel learning-on-model paradigm that compresses
  global models via convolutional compression. Unlike traditional compression methods,
  the compressed models can be directly trained on clients without decompression.
---

# FedConv: A Learning-on-Model Paradigm for Heterogeneous Federated Clients

## Quick Facts
- **arXiv ID**: 2502.20639
- **Source URL**: https://arxiv.org/abs/2502.20639
- **Reference count**: 40
- **Primary result**: Outperforms state-of-the-art FL systems by 35% in accuracy, 33% in computation, and 25% in communication overhead

## Executive Summary
FedConv introduces a novel learning-on-model paradigm that addresses heterogeneous system resources in federated learning by compressing global models via convolutional compression. Unlike traditional compression methods, the compressed models can be directly trained on clients without decompression. The system uses transposed convolutional dilation to convert heterogeneous sub-models back to large models with unified size while retaining personalized information. Extensive experiments on six datasets demonstrate significant improvements in model accuracy and resource efficiency.

## Method Summary
FedConv addresses heterogeneous federated learning by treating model parameters as spatial data that can be compressed and reconstructed via convolution operations. The server compresses the global model using convolution layers fine-tuned on a small public dataset, creating sub-models that clients can train directly without decompression. After local training, the server dilates the sub-models back to global size using transposed convolution layers with client-specific parameters, then aggregates them using learnable weights optimized via Kullback-Leibler Divergence (KLD) similarity to the previous global model.

## Key Results
- Outperforms state-of-the-art FL systems by more than 35% on average in model accuracy
- Achieves 33% reduction in computation overhead and 25% reduction in communication overhead
- Demonstrates effectiveness across six diverse datasets including MNIST, CIFAR10, and WiAR

## Why This Works (Mechanism)

### Mechanism 1: Convolutional Compression (Learning-on-Model)
Applying convolution operations to global model parameters generates compressed sub-models that retain predictive capability without requiring decompression for training. The server treats global model parameters as spatial data, applies convolution layers iteratively fine-tuned on server-side dataset to minimize loss between compressed model output and ground truth. Core assumption: spatial distribution of neural network parameters contains learnable patterns that can be effectively compressed and reconstructed via convolution, analogous to image compression.

### Mechanism 2: Transposed Convolutional Dilation
Heterogeneous client models are unified into global model dimensions using transposed convolution, preserving personalized information learned from local non-IID data. After clients train compressed sub-models, the server applies Transposed Convolution layers to dilate sub-model parameters back to full global model size, using distinct TC parameters learned for each client to capture specific data distribution nuances.

### Mechanism 3: Weighted Aggregation via KLD
Direct averaging of dilated heterogeneous models degrades performance; learnable weights based on parameter similarity (KLD) are required to balance client contributions. The server assigns learnable weight vectors to each dilated model, optimized by minimizing prediction loss plus Kullback-Leibler Divergence term measuring similarity between dilated model and previous global model.

## Foundational Learning

- **Model Heterogeneity (System & Data)**: FedConv exists specifically because standard FL (FedAvg) forces weak devices to train large models (straving) or forces strong devices to train small models (under-utilization). *Quick check*: Can you explain why a "one-size-fits-all" model architecture fails in an environment with both high-end GPUs and embedded microcontrollers?

- **Convolution as Spatial Feature Extraction**: The core innovation treats weight matrices as images. You must understand that convolutions extract spatial hierarchies to grasp why the authors believe they can "compress" weights without the information loss typical of pruning. *Quick check*: If you have a 4D weight tensor $(C_{out}, C_{in}, K, K)$, how does reshaping it to a 2D matrix allow a 2D convolution to reduce its size?

- **Transposed Convolution (Deconvolution)**: This is the mathematical inverse of the compression step. Understanding stride and padding here is critical to ensuring the dilated output matches the global model dimensions exactly. *Quick check*: How does a transposed convolution differ from a standard upsampling (e.g., nearest neighbor) in terms of learning capacity?

## Architecture Onboarding

- **Component map**: Server Global Model → Compression Module (Conv layers, MLR activation) → TC Dilation Module (Transposed Conv layers) → Aggregator (Weighted Average). Client receives Compressed Sub-model → Local Training (Standard) → Uploads Sub-model.

- **Critical path**: 
  1. Initialization: Server pre-trains global model
  2. Compression Tuning: Server optimizes Conv parameters on public data (Eq. 1)
  3. Client Execution: Clients train sub-models locally (no knowledge of global size)
  4. Dilation Tuning: Server optimizes TC parameters to upscale client updates (Eq. 3)
  5. Aggregation: Server computes weighted average based on KLD optimization

- **Design tradeoffs**: 
  - Server Compute vs. Client Compute: FedConv shifts heavy lifting (compression/dilation tuning) to server. *Assumption: The server is resource-rich.*
  - Shrinkage Ratio (SR): Smaller SR saves client resources but risks dilation break condition (Fig 12 suggests threshold around 0.25-0.4)
  - Public Data Dependency: Requires small, representative server-side dataset. If data is unrepresentative, compression tuning may diverge.

- **Failure signatures**:
  - Imbalanced Parameter Distribution: If sub-model weights skew negative (Fig 6a), indicates failure in compression initialization or learning rate
  - Aggregation Divergence: If global accuracy oscillates wildly (Fig 7a), suggests weight normalization or learning rate scheduler is missing or misconfigured
  - Overfitting: If server-side tuning epochs exceed ≈20 (Fig 13b), compression parameters overfit server data, hurting client personalization

- **First 3 experiments**:
  1. Replication of Information Preservation: Compare FedConv compression vs. Pruning (Fig 2d) on simple CNN (e.g., MNIST) to verify Mutual Information is higher for Conv-compressed models
  2. Hyperparameter Sensitivity (Server Data): Vary server-side data size from 1% to 25% to confirm "turning point" for overfitting (Section 6.4.3)
  3. Real-world Heterogeneity Simulation: Configure 3 distinct client groups (High/Med/Low SR) and run FedConv vs. FedRolex on CIFAR-10 to verify accuracy gap closes as shown in Fig 9

## Open Questions the Paper Calls Out

### Open Question 1
Can the convolution-based compression and dilation mechanism be effectively adapted for non-CNN architectures like Transformers? The method applies convolution operations on parameter matrices (Section 4.1), relying on spatial locality assumptions that may not hold for attention-based or fully connected layers. The evaluation focuses solely on CNN architectures (ResNet, GoogLeNet), leaving applicability to other architectures untested.

### Open Question 2
How does the additional optimization overhead for compression and dilation impact server-side scalability? The server performs iterative fine-tuning of convolution and transposed convolution parameters for 20 epochs every round (Algorithm 1), yet the paper focuses exclusively on client efficiency (Section 6.3). The server-side computational bottleneck was not profiled against massive number of heterogeneous clients.

### Open Question 3
Can FedConv handle dynamic resource fluctuations on clients during training? Clients determine Shrinkage Ratios (SRs) based on resource profiles before training starts, and these ratios remain static throughout the process (Section 4.1). Real-world mobile resources fluctuate (e.g., battery drops, other app usage), but the framework lacks mechanism for dynamic SR adjustment.

## Limitations
- Core innovation lacks clear mathematical justification for why convolutional compression preserves more information than established methods like pruning or quantization
- Dependency on server-side public data for tuning compression and dilation parameters introduces potential bottleneck not thoroughly analyzed for scalability or privacy implications
- Theoretical foundation for why convolutional compression preserves more information than pruning lacks rigorous mathematical proof

## Confidence
- **High confidence**: Experimental results demonstrating superior accuracy and resource efficiency compared to baselines (FedAvg, FedRolex) are well-supported with multiple datasets and ablation studies
- **Medium confidence**: Architectural description of compression and dilation modules is detailed enough for implementation, though specific implementation details remain unclear
- **Low confidence**: Theoretical foundation for why convolutional compression preserves more information than pruning lacks rigorous mathematical proof

## Next Checks
1. **Theoretical validation**: Prove or disprove hypothesis that convolutional operations on weight matrices preserve more information than pruning by deriving information-theoretic bounds for both approaches
2. **Privacy analysis**: Quantify privacy implications of requiring server-side public data for parameter tuning, including potential membership inference risks and impact of data representativeness on compression quality
3. **Scalability testing**: Evaluate FedConv's performance when scaling to hundreds of clients with varying SR distributions, measuring how server-side tuning time scales and whether 5% public data requirement becomes prohibitive