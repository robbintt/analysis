---
ver: rpa2
title: Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented
  Generation?
arxiv_id: '2509.22325'
source_url: https://arxiv.org/abs/2509.22325
tags:
- query
- rewriting
- synthetic
- rewrites
- episode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether synthetic query rewrites can better
  capture user intent than human rewrites in retrieval-augmented generation (RAG)
  systems. The authors propose SynRewrite, which uses GPT-4o to generate synthetic
  query rewrites based on dialogue history, current queries, positive documents, and
  answers.
---

# Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?

## Quick Facts
- arXiv ID: 2509.22325
- Source URL: https://arxiv.org/abs/2509.22325
- Reference count: 13
- Synthetic query rewrites outperform human rewrites in RAG systems across multiple evaluation metrics

## Executive Summary
This paper investigates whether synthetic query rewrites can better capture user intent than human rewrites in retrieval-augmented generation (RAG) systems. The authors propose SynRewrite, a framework that leverages GPT-4o to generate synthetic query rewrites from dialogue history, current queries, positive documents, and answers. These synthetic rewrites are used to fine-tune a Flan-T5 model, which is further enhanced using reinforcement learning with Direct Preference Optimization (DPO). Experiments on TopiOCQA and QRECC datasets demonstrate that SynRewrite outperforms human rewrites in both retrieval (MRR@5) and generation (Rouge-1, Rouge-L, Bleu-4) tasks, achieving higher EM scores and better generalization.

## Method Summary
The SynRewrite approach uses GPT-4o to generate synthetic query rewrites based on dialogue history, current queries, positive documents, and answers. These synthetic rewrites are then used to fine-tune a Flan-T5 model. The fine-tuned model is further enhanced using reinforcement learning with Direct Preference Optimization (DPO) to improve its ability to capture user intent. The framework is evaluated on TopiOCQA and QRECC datasets, comparing performance against human rewrites across multiple RAG tasks and metrics.

## Key Results
- SynRewrite outperforms human rewrites in retrieval tasks with higher MRR@5 scores
- Synthetic rewrites achieve better generation performance with higher Rouge-1, Rouge-L, and Bleu-4 scores
- The approach demonstrates better generalization and higher EM scores compared to human annotations

## Why This Works (Mechanism)
The synthetic query rewrites capture user intent more effectively by leveraging the sophisticated understanding capabilities of GPT-4o, which can process dialogue context, relevant documents, and answers simultaneously. The fine-tuning of Flan-T5 on these high-quality synthetic rewrites, followed by reinforcement learning with DPO, allows the model to learn optimal rewriting strategies that maximize both retrieval and generation performance. This multi-stage approach creates a feedback loop where synthetic data generation and model optimization reinforce each other, resulting in query rewrites that better align with user intent than human annotations.

## Foundational Learning
1. Retrieval-Augmented Generation (RAG): Why needed - Combines information retrieval with text generation to provide more accurate and context-aware responses; Quick check - Verify the system retrieves relevant documents before generating answers
2. Query Rewriting: Why needed - Transforms ambiguous or incomplete queries into more specific versions that improve retrieval accuracy; Quick check - Test if rewritten queries consistently yield better retrieval results
3. Direct Preference Optimization (DPO): Why needed - Reinforcement learning technique that optimizes model outputs based on human preference data; Quick check - Ensure preference rankings are correctly implemented in the reward function
4. Flan-T5: Why needed - Pretrained encoder-decoder model suitable for fine-tuning on sequence-to-sequence tasks like query rewriting; Quick check - Confirm model capacity matches task complexity
5. Synthetic Data Generation: Why needed - Scales data creation without human annotation bottlenecks; Quick check - Validate synthetic data quality matches or exceeds human-generated data
6. Dialogue Context Processing: Why needed - Maintains conversation coherence across multiple turns; Quick check - Test model's ability to reference previous conversation turns

## Architecture Onboarding
Component map: GPT-4o -> Synthetic Rewrite Generator -> Flan-T5 -> DPO Trainer -> Enhanced Model
Critical path: Dialogue context + current query + positive documents + answers → GPT-4o synthetic rewrites → Flan-T5 fine-tuning → DPO reinforcement learning → Final model
Design tradeoffs: Synthetic data generation trades computational cost for scalability vs. human annotation accuracy; DPO optimization trades training time for improved alignment with user intent
Failure signatures: Poor retrieval performance indicates inadequate rewrite quality; Low generation scores suggest insufficient fine-tuning or suboptimal DPO parameters
First experiments: 1) Generate 100 synthetic rewrites and manually evaluate quality against human rewrites; 2) Fine-tune Flan-T5 on synthetic data and measure retrieval improvement on validation set; 3) Apply DPO and compare preference learning curves with and without synthetic data

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on GPT-4o introduces potential bias toward this specific model's capabilities and knowledge cutoff
- Evaluation focuses primarily on English-language datasets, limiting generalizability to other languages or domains
- Synthetic data generation assumes access to positive documents and answers, which may not reflect real-world scenarios without answer supervision

## Confidence
- Synthetic rewrites consistently outperform human annotations in RAG systems: High
- The proposed SynRewrite approach is more scalable than human annotation: High
- Performance gains generalize across different RAG tasks and metrics: Medium

## Next Checks
1. Conduct ablation studies removing GPT-4o's access to positive documents and answers during synthetic rewrite generation to assess real-world applicability
2. Test the approach on non-English datasets and domain-specific corpora to evaluate cross-lingual and domain generalization
3. Implement a cost-benefit analysis comparing human annotation time versus computational resources required for synthetic data generation at scale