---
ver: rpa2
title: A Champion-level Vision-based Reinforcement Learning Agent for Competitive
  Racing in Gran Turismo 7
arxiv_id: '2504.09021'
source_url: https://arxiv.org/abs/2504.09021
tags:
- agent
- racing
- learning
- track
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a vision-based autonomous racing agent for
  Gran Turismo 7 that achieves champion-level performance in competitive scenarios.
  The agent uses only ego-centric camera views and onboard sensor data, eliminating
  reliance on global features during inference.
---

# A Champion-level Vision-based Reinforcement Learning Agent for Competitive Racing in Gran Turismo 7

## Quick Facts
- arXiv ID: 2504.09021
- Source URL: https://arxiv.org/abs/2504.09021
- Authors: Hojoon Lee; Takuma Seno; Jun Jet Tai; Kaushik Subramanian; Kenta Kawamoto; Peter Stone; Peter R. Wurman
- Reference count: 40
- Primary result: Vision-based RL agent achieves champion-level performance in competitive Gran Turismo 7 racing

## Executive Summary
This paper presents a vision-based autonomous racing agent that achieves champion-level performance in competitive scenarios in Gran Turismo 7. The agent uses only ego-centric camera views and onboard sensor data, eliminating reliance on global features during inference. Through an asymmetric actor-critic architecture where the actor processes vision and proprioceptive data through a recurrent neural network while the critic leverages global features during training, the agent consistently outperforms GT7's built-in AI and human experts across three different racing scenarios.

## Method Summary
The method employs QR-SAC with an asymmetric actor-critic architecture. The actor receives 64×64 RGB images and 18-dimensional proprioceptive data, processes them through convolutional layers and a GRU with 512 hidden units, and outputs steering and throttle/brake commands. The critic, used only during training, additionally receives 177 3D track points and opponent grid data. The agent is trained using 20 distributed rollout workers on PS4 consoles, with network reinitialization at 2000 epochs to prevent early overfitting. Training involves a curriculum of increasing opponent density and BoP variation.

## Key Results
- Achieved highest winning margins across three tracks (Tokyo, Spa, Sarthe) compared to both built-in AI and human experts
- Ablation studies confirm the importance of the recurrent module for opponent tracking and asymmetric architecture for leveraging global information during training
- Demonstrates first vision-based autonomous racing agent to achieve champion-level performance in competitive racing scenarios

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Actor-Critic Privileged Learning
The actor learns robust policies from local sensors by receiving stable gradient signals from a critic with privileged global state access during training. The critic observes track geometry and opponent grid data alongside local features, providing precise value estimates while the actor—restricted to images and IMU data—learns to approximate this value function without direct global access.

### Mechanism 2: Recurrent Memory for Temporal Credit Assignment
The GRU-based recurrent module enables the agent to maintain implicit beliefs about off-screen opponents and track curvature through temporal integration. Hidden state ht−1 carries information across timesteps, allowing the agent to associate earlier visual cues with later reward outcomes.

### Mechanism 3: Primacy Bias Mitigation via Network Reinitialization
Reinitializing networks after the replay buffer fills prevents early overfitting to limited behavioral patterns. Early training data contains simpler scenarios, causing networks to overfit to these and develop brittle features. Reinitialization at 2,000 epochs forces relearning from richer data distribution.

## Foundational Learning

- **Partial Observability in POMDPs**: Why needed here: The agent never observes full state (opponents behind, track beyond ~6 seconds ahead). Must maintain beliefs over hidden variables.
  - Quick check question: Can you explain why a reactive policy (no memory) would fail when an opponent disappears behind the agent temporarily?

- **Distributional RL (Quantile Regression)**: Why needed here: QR-SAC models Q-value distributions rather than expectations, reducing overestimation bias in stochastic racing environments.
  - Quick check question: How does modeling return distributions (vs. expected returns) help when collision penalties create highly skewed reward distributions?

- **Asymmetric Actor-Critic Theory**: Why needed here: Understanding why privileged critic information helps training but isn't required at inference requires grasping value function approximation vs. policy representation.
  - Quick check question: If the critic sees opponent positions but the actor doesn't, how can the actor still learn to avoid collisions?

## Architecture Onboarding

- **Component map**: Image encoder (3 conv layers) -> 512-dim embedding -> Concatenate with proprioceptive -> GRU (512 hidden) -> 4 FC layers (2048 each) -> Action heads

- **Critical path**: 1) Image preprocessing (1920×1080 → 64×64, HUD/mirror masking) 2) Conv encoding + proprioceptive concatenation 3) GRU hidden state management (burn-in, storage, retrieval) 4) Action sampling (Gaussian with tanh bounds) 5) Critic quantile loss computation with global features

- **Design tradeoffs**: 64×64 resolution: Faster encoding but loses fine detail (e.g., distant opponent silhouettes); compensated by temporal integration. Sequence length 32 / burn-in 16: Balances memory requirements against temporal credit assignment range. Asymmetric architecture: Simpler deployment but requires simulation access to global features during training.

- **Failure signatures**: Symmetric actor-critic: Fails to achieve first place (ablation shows median finish ~8th position). No RNN: Complete failure—agent cannot overtake, finishes last. No augmentation: Higher variance across episodes, overfitting to specific visual patterns. No reinitialization: Premature convergence to suboptimal features.

- **First 3 experiments**: 1) Ablation baseline: Train symmetric variant (critic uses only local features) on Tokyo scenario for 5,000 epochs. Expected: median finish position 8-15, confirming privileged information necessity. 2) Hidden state dimensionality sweep: Compare 128-dim vs. 512-dim GRU on opponent tracking accuracy. Expected: 128-dim shows degraded performance in multi-opponent scenarios. 3) Reinitialization timing study: Test reinitialization at 1,000, 2,000 (default), and 4,000 epochs. Measure final winning margin and training stability. Expected: Earlier reinit underperforms (insufficient buffer diversity); later reinit may waste compute.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the agent generalize its racing performance to unseen vehicle models, track layouts, and dynamic weather conditions?
- Basis in paper: The Conclusion states that future work involves "enhancing the agent’s ability to generalize across different car models, tracks, and weather conditions," as the current experiments utilized a fixed car per scenario and controlled settings.
- Why unresolved: The training and evaluation protocols were restricted to three specific car-track scenarios with fixed weather, leaving the policy's robustness to new physical dynamics (cars) and visual domains (tracks/weather) untested.
- What evidence would resolve it: Evaluation of the trained agent on a suite of new tracks with different visual geometries and in weather conditions (e.g., rain, changing light) not present in the training distribution, without fine-tuning.

### Open Question 2
- Question: How does the agent perform in direct, competitive head-to-head races against top human drivers, distinct from overtaking built-in AI?
- Basis in paper: The Conclusion identifies "competitive head-to-head races with top human drivers" as a specific avenue for future work.
- Why unresolved: The current evaluation methodology measures performance by starting from the back of a grid against 19 Built-in AI (BIAI) agents to calculate "winning margin," rather than testing the agent's ability to defend position or react to aggressive, unpredictable human strategies over a full race distance.
- What evidence would resolve it: A study organizing direct wheel-to-wheel races between the vision-based agent and expert human drivers, analyzing defensive maneuvers and strategic interactions not present in the BIAI baseline.

### Open Question 3
- Question: Does the reliance on specific static visual cues (e.g., treelines, skylines) for track localization limit the agent's robustness in environments with sparse or ambiguous features?
- Basis in paper: The Visual Analysis section notes the agent prioritizes "stable environmental cues, such as the sky and treelines" over lane markings.
- Why unresolved: While effective in the high-fidelity GT7 scenarios tested, this dependency suggests a potential failure mode in environments with uniform backgrounds, heavy fog, or distinct topographies where such cues are absent or misleading.
- What evidence would resolve it: Ablation studies or evaluations in modified environments where high-level structural cues (skyline, horizon) are removed or randomized, testing if the agent can fall back on immediate track markers or proprioceptive data.

## Limitations

- Requires extensive computational resources: 20 PS4 consoles and ~5 days of training
- Evaluated only on GT7, limiting generalizability to other racing games or real-world applications
- Asymmetric architecture's effectiveness relies on the assumption that the actor can adequately approximate the critic's global state value function from partial observations

## Confidence

- **High Confidence**: The agent's ability to achieve champion-level performance in GT7 using only vision and proprioceptive data
- **Medium Confidence**: The specific mechanisms (asymmetric architecture, recurrent memory, network reinitialization) contributing to performance
- **Medium Confidence**: The claim of being the first vision-based agent to demonstrate champion-level performance in competitive racing scenarios

## Next Checks

1. **Architecture Sensitivity Analysis**: Systematically vary the sequence length, burn-in steps, and GRU hidden state dimensions to quantify their impact on overtaking success rate and lap times.

2. **Generalization Test**: Evaluate the pre-trained agent on previously unseen tracks in GT7 to assess its ability to generalize beyond the three tracks used in training.

3. **Opponent Density Stress Test**: Test the agent's performance in scenarios with varying numbers of opponents (beyond the 7-19 range in the paper) to identify potential limitations in opponent tracking or collision avoidance.