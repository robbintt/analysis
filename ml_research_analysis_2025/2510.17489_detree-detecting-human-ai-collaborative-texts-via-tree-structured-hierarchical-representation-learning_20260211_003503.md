---
ver: rpa2
title: 'DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical
  Representation Learning'
arxiv_id: '2510.17489'
source_url: https://arxiv.org/abs/2510.17489
tags:
- polish
- extend
- qwen2
- llama
- internlm2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DETree addresses the challenge of detecting human-AI collaborative
  text by modeling the hierarchical relationships among different text generation
  processes. The method constructs a Hierarchical Affinity Tree (HAT) from inter-class
  similarities and introduces a Tree-Structured Contrastive Loss (TSCL) to align text
  representations with this hierarchical structure.
---

# DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning

## Quick Facts
- arXiv ID: 2510.17489
- Source URL: https://arxiv.org/abs/2510.17489
- Authors: Yongxin He; Shan Zhang; Yixuan Cao; Lei Ma; Ping Luo
- Reference count: 40
- Key outcome: Achieves F1 scores up to 97.16 and AUROC improvement up to 15.55 in few-shot OOD scenarios via hierarchical tree-structured representation learning.

## Executive Summary
DETree addresses the challenge of detecting human-AI collaborative text by modeling the hierarchical relationships among different text generation processes. The method constructs a Hierarchical Affinity Tree (HAT) from inter-class similarities and introduces a Tree-Structured Contrastive Loss (TSCL) to align text representations with this hierarchical structure. A large-scale benchmark dataset (RealBench) is developed to simulate diverse human-AI collaboration patterns. DETree achieves state-of-the-art performance on hybrid text detection tasks, with F1 scores up to 97.16, and demonstrates strong generalization under out-of-distribution settings, improving AUROC by up to 15.55 in few-shot scenarios. The approach also shows robustness to adversarial perturbations and database compression, validating its practical deployability.

## Method Summary
DETree operates in two stages: first, it trains an encoder via Supervised Contrastive Learning (SCL) to generate high-quality text embeddings. It then computes inter-class similarities to construct a Hierarchical Affinity Tree (HAT) using agglomerative clustering and task-specific priors. In the second stage, the model is retrained with a Tree-Structured Contrastive Loss (TSCL) that enforces the hierarchical structure in the embedding space. For inference, DETree uses a retrieval-based approach with K-Nearest Neighbors (KNN) over a compressed database of text representations, enabling few-shot adaptation to out-of-distribution domains.

## Key Results
- Achieves F1 scores up to 97.16 on hybrid text detection tasks.
- Improves AUROC by up to 15.55 in few-shot out-of-distribution scenarios.
- Demonstrates robustness to adversarial perturbations and database compression.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring text generation classes as a **Hierarchical Affinity Tree (HAT)** allows the model to capture latent relationships between hybrid text types (e.g., AI-edited vs. human-edited) which flat classification ignores.
- **Mechanism:** An initial encoder is trained via Supervised Contrastive Learning (SCL) to generate embeddings. Inter-class similarity is computed (dot product of centroids). An agglomerative clustering algorithm constructs an initial tree, which is then reorganized based on task-specific priors (e.g., hybrids are closer to AI than human) to form the HAT.
- **Core assumption:** Texts generated by similar processes (e.g., "Llama3_polish_GPT-4o" and "Claude3.5_paraphrase_GPT-4o") cluster naturally in the representation space, and these clusters form a hierarchy rather than disjoint sets.
- **Evidence anchors:**
  - [abstract]: "...models the relationships among different processes as a Hierarchical Affinity Tree structure..."
  - [section 3.2]: "We propose a Hierarchical Affinity Tree Construction Algorithm to formalize and leverage latent relational structures among text categories."
  - [corpus]: Weak corpus support for the *specific tree mechanism*, though neighbors like "Fine-Grained Detection..." confirm the broader validity of moving beyond binary classification for mixed texts.
- **Break condition:** If the generation process leaves no distinct stylistic trace or if hybrid texts are statistically indistinguishable from pure human texts (violating the similarity assumption), the HAT structure will be noisy and uninformative.

### Mechanism 2
- **Claim:** Enforcing the hierarchical structure during training via **Tree-Structured Contrastive Loss (TSCL)** aligns the embedding space geometry with the class taxonomy, improving classification boundaries.
- **Mechanism:** TSCL modifies standard contrastive loss by defining positive and negative sets based on the Lowest Common Ancestor (LCA) depth in the HAT. It explicitly optimizes the inequality that samples closer in the tree hierarchy must be more similar in the embedding space than those farther apart.
- **Core assumption:** The similarity relationship between text classes is strictly hierarchical (transitive via LCA depth) rather than arbitrary or graph-based.
- **Evidence anchors:**
  - [abstract]: "...introduces a specialized loss function that aligns text representations with this tree."
  - [section 3.3]: "To encourage the embedding space to better align with the hierarchical structure modeled by the HAT... we adopt the following training objective [Equation 5]."
- **Break condition:** If the relationship between classes is better modeled as a graph (many-to-many) rather than a tree (one-to-many), enforcing strict hierarchical constraints could distort the embedding space.

### Mechanism 3
- **Claim:** **Retrieval-based few-shot adaptation** allows the system to generalize to Out-of-Distribution (OOD) domains by dynamically adjusting decision boundaries using a compact database, bypassing the need for weight updates.
- **Mechanism:** Instead of a parametric classifier head, inference uses K-Nearest Neighbors (KNN) over a database of text representations. For OOD scenarios, a small number of target-domain samples are injected into this database, effectively "moving" the decision boundary to fit the new distribution.
- **Core assumption:** The encoder's representations transfer sufficiently well to new domains that meaningful similarities can be retrieved with only a few examples (the manifold structure holds).
- **Evidence anchors:**
  - [abstract]: "...improving AUROC by up to 15.55 in few-shot scenarios."
  - [section 4.2]: "Under the few-shot setting, the inclusion of such samples improves classification performance... [Table 2]."
  - [corpus]: The neighbor "EditLens" discusses quantifying AI editing, aligning with the need for fine-grained OOD detection, but does not validate the retrieval mechanism.
- **Break condition:** If the new domain exhibits a significant domain shift that breaks the encoder's feature space (e.g., unseen languages or formats not in pre-training), KNN retrieval will fail regardless of few-shot samples.

## Foundational Learning

- **Concept: Supervised Contrastive Learning (SCL)**
  - **Why needed here:** SCL is the prerequisite step to generating the initial high-quality representations required to compute the similarity matrix for HAT construction.
  - **Quick check question:** How does the initial encoder treat different hybrid classes before the tree is constructed? (Answer: As independent classes to maximize separability).

- **Concept: Hierarchical Text Classification (HTC)**
  - **Why needed here:** DETree differs from standard HTC by *inferring* the hierarchy from data affinity rather than using a pre-defined rigid taxonomy.
  - **Quick check question:** Why does the paper reject standard HTC methods for this task? (Answer: Standard HTC relies on predefined paths which don't exist for dynamic human-AI collaboration modes).

- **Concept: K-Nearest Neighbors (KNN) Classification**
  - **Why needed here:** This is the inference engine. Understanding KNN is necessary to grasp how the "retrieval-based few-shot adaptation" works by simply adding neighbors.
  - **Quick check question:** How does K-means compression assist the KNN inference? (Answer: It replaces raw samples with cluster centroids to reduce storage cost and smooth decision boundaries).

## Architecture Onboarding

- **Component map:**
  1. **Input:** Text sequences (tokenized).
  2. **Encoder:** RoBERTa-large fine-tuned with LoRA (Low-Rank Adaptation).
  3. **Structure Module:** HAT Construction Algorithm (generates the tree offline).
  4. **Training Objective:** Tree-Structured Contrastive Loss (TSCL) + Virtual Class Prototypes.
  5. **Inference Engine:** Faiss-based Retrieval Database (compressed via K-means) + KNN voting.

- **Critical path:**
  The pipeline is sequential and dependent:
  `SCL Training` -> `Similarity Matrix Extraction` -> `HAT Construction` -> `TSCL Re-training`.
  *Note:* You cannot train with TSCL until the HAT is built, and you cannot build the HAT until the initial SCL encoder is trained.

- **Design tradeoffs:**
  - **Prior Selection:** Choosing where to place hybrid texts in the HAT (Prior 1: AI-like, Prior 2: Human-like, Prior 3: Independent). The paper shows Prior 1 (AI-like) performs best, suggesting AI stylistic signals dominate.
  - **Compression Ratio:** Using K-means to compress the retrieval database trades slight precision for massive efficiency gains and noise reduction (e.g., 10K representatives vs. 16M raw samples).

- **Failure signatures:**
  - **Shallow Tree:** If the HAT construction threshold is too high, the tree stops splitting early, resulting in "flat" clusters that lose fine-grained distinction.
  - **OOD Mismatch:** If the "Unseen" domain test set has no overlapping categories with the retrieval database, zero-shot performance drops sharply (requiring few-shot adaptation).

- **First 3 experiments:**
  1. **Visual Validation:** Run the HAT construction step on the validation set and visualize the tree structure (t-SNE/UMAP) to confirm that hybrid texts (e.g., `human_polish_Gemini1.5`) cluster closer to AI generations than human texts.
  2. **Loss Ablation:** Train two models—one with standard SCL and one with TSCL—on the same subset of RealBench. Compare their F1 scores on the "Hybrid Text Detection" task (Table 3) to quantify the value of the hierarchical constraint.
  3. **Retrieval Stress Test:** Take the trained model and evaluate it on a completely new dataset (not in RealBench) in "Zero-shot" mode vs. "Few-shot" mode (adding 5-10 samples) to verify the retrieval-based adaptation claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust is DETree against adversarial evasion achieved by fine-tuning the generator model to specifically bypass detection, rather than through post-hoc perturbations?
- **Basis in paper:** [explicit] The Limitations section states, "We have not explored adversarial evasion via model fine-tuning intended to bypass detection."
- **Why unresolved:** The paper evaluates robustness against perturbations (e.g., synonym swaps, paraphrasing) applied after generation, but it does not test scenarios where the generative model itself is optimized to produce text that lies outside the detection distribution.
- **Evidence:** An evaluation of DETree’s detection accuracy on text generated by models (e.g., GPT-4o, Llama 3) fine-tuned with a loss function that penalizes proximity to DETree's retrieval database embeddings.

### Open Question 2
- **Question:** Does the Hierarchical Affinity Tree (HAT) maintain structural validity and performance when scaling to texts generated by more than three sequential collaborators?
- **Basis in paper:** [explicit] The Limitations section notes, "extending the analysis to scenarios with more collaborators remains an interesting direction for future investigation."
- **Why unresolved:** The paper demonstrates success with three authors, showing the "first author" (base text) has the strongest signal. It is unclear if this signal degrades or becomes untraceable in the HAT structure as the number of sequential editing steps (human or AI) increases significantly.
- **Evidence:** Experiments measuring the violation of Theorem 3.1 (Hierarchical Similarity Constraint) and detection F1 scores on a new dataset constructed with 4, 5, or 10 sequential editing steps involving diverse LLMs.

### Open Question 3
- **Question:** Can DETree maintain high performance in entirely unseen, rare domains without requiring the current few-shot adaptation step to adjust decision boundaries?
- **Basis in paper:** [explicit] The Limitations section states, "when encountering entirely unseen and rare domains, the model still requires a small number of in-domain samples to effectively adjust the decision boundaries."
- **Why unresolved:** The current method relies on a retrieval-based adaptation paradigm that needs examples from the target domain. This constraint limits the system's deployability in strictly zero-shot settings where no labeled target data is available.
- **Evidence:** Evaluation of the model on highly specialized, out-of-distribution datasets (e.g., specific coding languages or niche medical sub-fields) using only the pre-trained RealBench database, without the retrieval-based adaptation phase.

## Limitations

- **Conceptual Scope:** The paper assumes that generation process leaves a recoverable stylistic trace that forms a strict hierarchical relationship. If AI editing is fully invisible to the model (e.g., perfect paraphrasing), or if hybrid texts represent true mixtures rather than linear blends, the HAT structure may not reflect the true relationship.
- **Dataset Generality:** RealBench is synthetically constructed from controlled augmentation pipelines. While it covers diverse base models and hybrid strategies, the text styles are constrained to known generation methods. Performance on truly unseen real-world hybrid texts remains untested.
- **Training Complexity:** The two-stage training (SCL → HAT → TSCL) requires substantial computational resources and careful hyperparameter tuning. The Virtual Class Prototypes mechanism is under-specified, making exact replication difficult.

## Confidence

**High Confidence:**
- **Mechanism 1 (HAT Construction):** Well-specified algorithm with clear implementation steps. The tree-building process is deterministic given the similarity matrix.
- **Mechanism 2 (TSCL Loss):** The mathematical formulation is complete and the inequality constraints are explicit. The core idea is sound.
- **Experiment 1 (F1 Scores):** Reported metrics are standard and the baseline comparisons are reasonable.

**Medium Confidence:**
- **Mechanism 3 (Few-shot Adaptation):** The retrieval-based approach is intuitive, but the exact implementation of Virtual Class Prototypes and the sensitivity to the number/quality of few-shot samples are unclear.
- **Experiment 2 (OOD Generalization):** Results are promising, but the definition of "Unseen" is tied to RealBench's synthetic construction. Real-world generalization is uncertain.
- **Experiment 3 (Robustness):** Tested against specific attack types, but the space of possible perturbations is vast. No claim of universal robustness.

**Low Confidence:**
- **Efficiency Claims:** The 15.55 AUROC improvement in few-shot scenarios is impressive but lacks comparison to parametric fine-tuning methods.
- **Ablation Studies:** While the paper includes ablations, the impact of individual hyperparameters (e.g., HAT depth, τ, k for KNN) is not fully explored.

## Next Checks

1. **Cross-Dataset Generalization:** Evaluate DETree on a held-out, real-world dataset of human-AI collaborative texts (e.g., arXiv papers with known AI editing) to test performance beyond synthetic hybrids.

2. **Robustness Expansion:** Test DETree against a broader range of adversarial attacks, including context-aware paraphrasing and watermark removal, to assess the limits of its resilience.

3. **Architectural Transfer:** Re-implement DETree with a different backbone (e.g., DeBERTa) and compare performance to isolate the contribution of the hierarchical method from the choice of encoder.