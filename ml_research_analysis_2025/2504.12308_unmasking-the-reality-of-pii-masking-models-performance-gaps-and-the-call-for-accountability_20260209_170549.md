---
ver: rpa2
title: 'Unmasking the Reality of PII Masking Models: Performance Gaps and the Call
  for Accountability'
arxiv_id: '2504.12308'
source_url: https://arxiv.org/abs/2504.12308
tags:
- masking
- text
- type
- entity
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the performance of three popular open-source\
  \ PII masking models\u2014Piiranha, Starpii, and Microsoft Presidio\u2014using a\
  \ newly curated dataset of 17K semi-synthetic sentences containing 16 types of PII.\
  \ The dataset was designed to test models across five NER detection dimensions and\
  \ one adversarial context, reflecting real-world complexities like ambiguity, phrasing\
  \ variability, syntax variations, and evolving entities."
---

# Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability

## Quick Facts
- arXiv ID: 2504.12308
- Source URL: https://arxiv.org/abs/2504.12308
- Reference count: 20
- Primary result: Three popular PII masking models failed to identify any PII in 28% of cases and misclassified PII in 67% of instances.

## Executive Summary
This study evaluates the performance of three popular open-source PII masking models—Piiranha, Starpii, and Microsoft Presidio—using a newly curated dataset of 17K semi-synthetic sentences containing 16 types of PII. The dataset was designed to test models across five NER detection dimensions and one adversarial context, reflecting real-world complexities like ambiguity, phrasing variability, syntax variations, and evolving entities. Results show that models failed to identify any PII in 28% of cases and misclassified PII in 67% of instances, with Starpii showing the highest non-identification rates. High misclassification rates were observed across all models, especially for emerging PII types like UPI IDs and international identifiers. The findings highlight significant gaps in current PII masking systems, including limited training data diversity and lack of robust evaluation metrics. The study calls for greater accountability, transparency in model disclosures, and improved adversarial testing to mitigate privacy risks in PII masking.

## Method Summary
The study evaluated three open-source PII masking models using a semi-synthetic dataset of 17K sentences containing 16 PII types across India, U.K., and U.S. jurisdictions. Researchers curated PII variations across syntax, linguistic, and representational categories, generated seed PII samples, and used GPT-4o-mini to create context paragraphs across five feature dimensions plus adversarial context. The dataset included Type 1 sentences with explicit PII references and Type 2 sentences without explicit references. Models were run on all texts, with predicted PII substrings and types extracted. Evaluation used regex matching for seed PII detection and classification accuracy for PII type identification.

## Key Results
- Models failed to identify any PII in 28% of test cases across all three models
- Misclassification rates reached 67% of instances where PII was detected but incorrectly classified
- Starpii showed the highest non-identification rates (up to 91% for Contextual Entity Disambiguation) and was trained primarily on code data
- Emerging PII types like UPI IDs and international identifiers had particularly high misclassification rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data distribution mismatch causes cascading detection failures on out-of-distribution PII.
- Mechanism: Models trained on narrow, geography-specific datasets (e.g., Western PII formats) fail when encountering novel entity types or regional variations not represented in training—evidenced by high non-identification for vehicle registration (61.4% Piiranha, 88.2% Starpii) and passport numbers (47.4% Piiranha, 77.7% Starpii).
- Core assumption: The performance gaps observed on semi-synthetic test data generalize to real-world deployments.
- Evidence anchors:
  - [abstract] "High misclassification rates were observed across all models, especially for emerging PII types like UPI IDs and international identifiers."
  - [section 4.1] "The high non-identification values for credit card numbers, National Identity Numbers... may be attributed to the lack of diversity in the respective training datasets and outdated representations."
  - [corpus] Related work on multilingual PII detection (RECAP framework) confirms detection challenges in low-resource languages due to limited annotated data.
- Break condition: If training data were expanded to include diverse jurisdictions, formats, and emerging entity types, detection rates would improve—but this remains untested.

### Mechanism 2
- Claim: Context-word dependency creates asymmetric performance between explicit and implicit PII references.
- Mechanism: NER models leverage surrounding context for entity disambiguation; when context is removed (Type 2 sentences without explicit PII type references), non-identification rates spike dramatically—Starpii jumps from 54% to 91% for Basic Entity Recognition.
- Core assumption: The Type 1/Type 2 experimental manipulation accurately captures real-world context variability.
- Evidence anchors:
  - [abstract] "Results show that models failed to identify any PII in 28% of cases... reflecting real-world complexities like ambiguity, phrasing variability, syntax variations."
  - [section 3.1] "Contextual Entity Disambiguation for both Type 1 and Type 2... signifies that when the entity may have multiple interpretations, the models fail with a high chance of omitting any possible predictions."
  - [corpus] PII-Bench evaluation confirms query-aware privacy protection systems struggle with context-dependent detection.
- Break condition: If models incorporated stronger local feature extraction (capitalization, punctuation, prefixes/suffixes independent of context), performance gaps between Type 1 and Type 2 would narrow.

### Mechanism 3
- Claim: Syntactic pattern overfitting causes systematic misclassification of emerging PII formats.
- Mechanism: Models relying on regex-based or pattern-matching approaches misclassify novel formats that superficially resemble known patterns—UPI IDs (e.g., abcd@okicici) misclassified as emails due to "@" symbol presence; credit cards outside 16-digit 4-4-4-4 format go undetected.
- Core assumption: Misclassification stems primarily from training data limitations rather than architectural constraints.
- Evidence anchors:
  - [abstract] "misclassified PII in 67% of instances... especially for emerging PII types like UPI IDs and international identifiers."
  - [section 1.3] "UPI... detected by NER as email ids possibly because they have '@' (common regular expression in emails) as part of them."
  - [corpus] Weak direct corpus evidence on regex overfitting specifically; related work on hybrid detection methods suggests combining regex with LLM-based approaches may mitigate this.
- Break condition: If training data included explicit negative examples (e.g., UPI IDs labeled as distinct from emails) and varied format representations, misclassification rates would decrease.

## Foundational Learning

- Concept: **Named Entity Recognition (NER) as Sequence Labeling**
  - Why needed here: Understanding that NER assigns labels token-by-token explains why boundary detection errors occur and why context matters for disambiguation.
  - Quick check question: Can you explain why a 10-digit alphanumeric string might be classified as a password, passport number, or driver's license depending on surrounding context?

- Concept: **Distribution Shift in ML Systems**
  - Why needed here: The core finding—that models trained on limited geographic/format distributions fail on broader inputs—is a distribution shift problem; recognizing this helps anticipate where failures will occur.
  - Quick check question: If a PII model was trained only on US data, what specific failure modes would you expect when deployed on Indian financial documents?

- Concept: **Precision vs. Recall Trade-offs in Privacy Systems**
  - Why needed here: High non-identification (28%) and high misclassification (67%) suggest models may be optimized for precision at the cost of recall—or vice versa; understanding this trade-off informs deployment decisions.
  - Quick check question: In a privacy masking system, would you prioritize minimizing false negatives (missed PII) or false positives (over-masking non-PII)? Why?

## Architecture Onboarding

- Component map:
  - PII Seed Generator -> Text Synthesis Layer (GPT-4o-mini) -> Type 1/Type 2 Split -> Model Under Test (Piiranha, Starpii, Microsoft Presidio) -> Evaluation Pipeline

- Critical path:
  1. PII variation curation → Seed sample generation → Text synthesis (30% rejection rate for missing seed PII)
  2. Type 1/Type 2 sentence generation → Model inference → Prediction extraction
  3. Non-identification analysis (empty output = failure) → Misclassification analysis (wrong label = failure)

- Design tradeoffs:
  - Semi-synthetic data enables controlled testing but may not capture all real-world noise patterns
  - Regex validation ensures seed PII presence but doesn't verify text quality beyond sample checks
  - GPT-4o-mini generation introduces potential instruction-following errors (~30% rejection rate acknowledged)

- Failure signatures:
  - **High non-identification + high misclassification**: Training data distribution mismatch (Starpii on non-code text)
  - **Type 1 >> Type 2 performance gap**: Over-reliance on context words; weak local feature extraction
  - **Consistent misclassification across entity types**: Pattern-matching overfit (UPI → email, varied credit card lengths → undetected)

- First 3 experiments:
  1. **Geographic ablation test**: Train a PII detection model on single-jurisdiction data; evaluate on multi-jurisdiction test set to quantify distribution shift impact directly.
  2. **Context perturbation study**: Systematically remove context words (Type 1 → Type 2 transition) while holding entity constant to measure context dependency coefficient per model architecture.
  3. **Format boundary testing**: For each PII type, test edge cases (minimum/maximum digit lengths, unusual delimiters, mixed scripts) to map failure boundaries and prioritize training data expansion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do PII masking models perform on code-embedded PII (e.g., email IDs split across JavaScript variables), and what dataset augmentations are needed to address this gap?
- Basis in paper: [explicit] Authors state they "have not considered abbreviations and PII within code" and recommend "compiling dataset for PII within code" as future work.
- Why unresolved: Current evaluation datasets focus on natural language text; code-embedded PII presents different structural challenges.
- What evidence would resolve it: Performance metrics from models tested on a curated dataset of code-embedded PII across programming languages.

### Open Question 2
- Question: Can an adversarial testing framework systematically identify failure modes in PII masking models across diverse linguistic contexts and perturbation types?
- Basis in paper: [explicit] Authors recommend "development of adversarial testing frameworks for PII masking" as future research direction.
- Why unresolved: Current adversarial testing is limited to a single dimension; no standardized framework exists for comprehensive adversarial evaluation.
- What evidence would resolve it: A validated framework that produces reproducible failure mode categorizations across multiple PII masking models.

### Open Question 3
- Question: What evaluation metrics beyond precision, recall, and F1 would better capture real-world PII masking effectiveness and downstream privacy risk?
- Basis in paper: [explicit] Authors state "there is a lack of consistent metrics for evaluating PII masking" and that standard metrics "may not capture the full picture of a model's effectiveness in practical applications."
- Why unresolved: Current metrics treat all PII types equally and don't reflect varying vulnerability levels or re-identification risk.
- What evidence would resolve it: Correlation between new metrics and actual privacy breach outcomes in deployment scenarios.

### Open Question 4
- Question: How does PII masking performance vary across demographic groups and naming conventions from underrepresented geographic regions?
- Basis in paper: [inferred] Authors note high non-identification for non-Western identifiers (UPI IDs, Aadhar) and cite research that "many name detection algorithms exhibit biases that can lead to unequal protection across different demographic groups."
- Why unresolved: Training data lacks geographic and demographic diversity; systematic bias audit not conducted.
- What evidence would resolve it: Stratified performance analysis across demographic groups and geographic regions on a diverse benchmark dataset.

## Limitations
- The study relies on semi-synthetic data generated by GPT-4o-mini, which may not fully capture real-world noise patterns and complexities
- The evaluation focuses on detection and classification accuracy without assessing downstream impacts on model utility
- The study does not investigate the root causes of misclassification at the architectural level—whether failures stem from training data limitations, model architecture constraints, or both

## Confidence

- **High Confidence**: The core finding that all three models exhibit significant non-identification (28%) and misclassification (67%) rates is well-supported by the experimental methodology and consistent across multiple PII types.
- **Medium Confidence**: The attribution of performance gaps primarily to training data limitations (rather than architectural constraints) is reasonable but not definitively proven.
- **Medium Confidence**: The conclusion that current evaluation metrics and adversarial testing are insufficient is supported by the experimental results but would benefit from direct comparison with existing benchmark methodologies.

## Next Checks
1. **Geographic ablation test**: Train a PII detection model on single-jurisdiction data (e.g., US-only) and evaluate on multi-jurisdiction test data to directly quantify distribution shift impact and validate the training data diversity hypothesis.
2. **Context dependency coefficient**: Systematically measure performance degradation when transitioning from Type 1 to Type 2 sentences while holding entity types constant, to establish the quantitative relationship between context dependency and detection accuracy per model architecture.
3. **Format boundary mapping**: Conduct systematic edge case testing for each PII type (minimum/maximum lengths, unusual delimiters, mixed scripts) to map failure boundaries and validate whether pattern-matching overfitting explains the observed misclassification patterns.