---
ver: rpa2
title: 'Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus
  on Korean Superstition'
arxiv_id: '2507.04014'
source_url: https://arxiv.org/abs/2507.04014
tags:
- korean
- cultural
- questions
- trap
- interpretation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Nunchi-Bench is a new benchmark for evaluating large language\
  \ models' cultural reasoning skills, with a focus on Korean superstitions. It includes\
  \ three types of tasks\u2014Multiple-Choice, Trap, and Interpretation questions\u2014\
  across 31 topics to assess factual knowledge, cultural advice, and situational interpretation."
---

# Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition

## Quick Facts
- **arXiv ID**: 2507.04014
- **Source URL**: https://arxiv.org/abs/2507.04014
- **Reference count**: 40
- **Key outcome**: Nunchi-Bench introduces a benchmark for evaluating LLMs' cultural reasoning on Korean superstitions, revealing models struggle to apply factual knowledge in practical scenarios despite recognizing facts.

## Executive Summary
Nunchi-Bench evaluates large language models' ability to reason about Korean cultural superstitions through three question types: Multiple-Choice (factual knowledge), Trap (culturally appropriate advice), and Interpretation (inferring cultural meaning). The benchmark tests models in both Korean and English, with and without explicit cultural framing, across 31 topics and 247 questions. Results show that while models generally recognize factual information, they struggle to apply it in practical scenarios. Explicit cultural cues significantly improve performance more than language alone, with Gemini 1.5 Pro and Claude 3 Opus performing best. The benchmark and leaderboard are publicly available to support further research in culturally adaptive AI systems.

## Method Summary
The benchmark uses 247 questions across 31 topics, with three question types: 31 Multiple-Choice questions testing factual knowledge, 92 Trap questions testing advisory appropriateness, and 124 Interpretation questions testing cultural inference. Questions exist in Korean and English versions, with Specified (explicit Korean context) and Neutral (no cultural framing) variants. GPT-4 Turbo evaluates responses using custom prompts, validated at 90% human alignment for Trap and 88.3% for Interpretation questions. Models are tested via zero-shot inference with greedy decoding. The benchmark includes a public leaderboard and is available on GitHub for community use.

## Key Results
- Models generally recognize factual information but struggle to apply it in practical scenarios, as evidenced by high MCQ scores paired with low Trap scores on the same topics.
- Explicit cultural framing in prompts improves model performance more effectively than prompt language alone, with English+Specified outperforming Korean+Neutral.
- The best-performing models are Gemini 1.5 Pro and Claude 3 Opus, while open-source models show mixed results, with those trained on Korean data performing better in Korean prompts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit cultural framing in prompts improves model performance more than prompt language alone.
- Mechanism: When cultural context is explicitly specified, models retrieve and apply relevant cultural knowledge more effectively, as measured by higher Score 2 counts on Trap and Interpretation questions.
- Core assumption: Models encode cultural knowledge but fail to activate it without explicit cues; cueing reduces retrieval ambiguity.
- Evidence anchors: [abstract] "Explicit cultural framing enhances performance more effectively than relying solely on the language of the prompt"; [section 3.3] "English+Specified exceeds Korean+Neutral, suggesting that contextual framing contributes more to reasoning performance than the language of the prompt itself."
- Break condition: If models were to develop robust implicit cultural detection from linguistic patterns alone, the Specified/Neutral gap would narrow.

### Mechanism 2
- Claim: Factual cultural knowledge does not reliably transfer to practical scenario reasoning.
- Mechanism: Models store cultural facts accessible via direct queries (MCQ) but fail to inhibit task-irrelevant associations when generating context-sensitive advice, leading to correct MCQ answers but incorrect Trap responses.
- Core assumption: Knowledge application requires suppressing default associations; this suppression is not triggered by standard instruction-following training.
- Evidence anchors: [abstract] "While models generally recognize factual information, they struggle to apply it in practical scenarios"; [section 3.5] "For ID 25...most models correctly select 'As poison' in MCQ. However, when presented with a practical scenario—such as serving apples to Korean relatives at night—models fail."
- Break condition: If reasoning-augmented training explicitly linked facts to advisory contexts, application gaps would shrink.

### Mechanism 3
- Claim: Korean-language prompts induce higher hallucination rates for non-Korean-optimized models.
- Mechanism: Models without high-quality Korean training data generate plausible but incorrect cultural references (-1 scores) more frequently in Korean prompts than in English prompts, suggesting language-specific data quality affects reasoning reliability.
- Core assumption: Hallucination rates reflect training data coverage; lower-quality Korean corpora increase spurious associations.
- Evidence anchors: [section 3.4] "In Trap questions, all models—except Claude 3 Sonnet and HyperClova-X—exhibit higher hallucination rates in the Korean versions compared to their English counterparts"; [section 3.4] "This accounts for the lower weighted scores observed in the Korean versions relative to English+Neutral."
- Break condition: If Korean training data quality improves or models better calibrate uncertainty in lower-resource languages, hallucination gaps would narrow.

## Foundational Learning

- **Cultural reasoning vs. factual recall**: The benchmark distinguishes between knowing facts (MCQ) and applying them in social scenarios (Trap/Interpretation). Understanding this distinction is critical for interpreting why MCQ scores don't predict Trap performance. Quick check: If a model answers "What does red ink symbolize?" correctly but advises using red ink for a Korean friend's cake, which capability failed—knowledge or application?

- **Contextual cueing in prompts**: The paper shows that explicitly marking cultural context (Specified vs. Neutral) affects performance independently of prompt language. This has direct implications for prompt engineering in multicultural applications. Quick check: You're designing a prompt for advising a user about gift-giving. Should you (a) write in the target culture's language, or (b) explicitly state the cultural background in any language?

- **LLM-as-evaluator scoring alignment**: The benchmark uses GPT-4 Turbo to score open-ended responses, achieving 90% alignment with humans on Trap questions after iterative prompt refinement. Understanding this methodology is essential for reproducing or extending the evaluation. Quick check: What validation step did the authors perform before trusting the LLM evaluator's scores?

## Architecture Onboarding

- **Component map**: Dataset (247 questions) -> Model inference (zero-shot, greedy decoding) -> LLM evaluator (GPT-4 Turbo) -> Score aggregation (weighted sums) -> Leaderboard
- **Critical path**: 1. Select superstitions via fill-in-the-blank quiz (>50% human accuracy threshold) 2. Generate questions via AI-human collaboration (MCQ) or manual design (Trap/Interpretation) 3. Validate relevance with 3 Korean annotators per question 4. Run inference on target models (zero-shot, greedy decoding) 5. Score MCQs via exact match; score Trap/Interpretation via LLM evaluator 6. Compute weighted sums and analyze score composition
- **Design tradeoffs**: Scope vs. generalizability (focusing on 31 Korean superstitions enables depth but limits cross-cultural claims); Automated vs. human evaluation (LLM evaluator scales but requires multi-phase alignment validation; no gold references are provided); Specified vs. Neutral framing (tests cultural cueing but may conflate explicitness with realism)
- **Failure signatures**: High MCQ + low Trap scores on same topic → knowledge-application gap; Higher -1 scores in Korean vs. English → language-specific hallucination; Neutral scores ≈ Specified scores → model not leveraging cultural cues
- **First 3 experiments**: 1. Run your model on all 4 versions (Korean/English × Specified/Neutral) to quantify cueing and language effects 2. Analyze per-topic MCQ-to-Trap score gaps to identify where knowledge fails to transfer 3. Sample 30 responses and compare your evaluator's scores against the paper's GPT-4 Turbo alignment methodology to validate reproducibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Nunchi-Bench framework generalize to other cultures beyond Korean superstitions?
- Basis in paper: [explicit] "Future work should extend this benchmark to diverse cultural contexts, ensuring that AI systems are not only multilingual but also culturally adaptive."
- Why unresolved: The benchmark focuses exclusively on Korean superstitions with 31 topics and 247 questions, limiting generalizability claims.
- What evidence would resolve it: Applying the same Trap and Interpretation question formats to superstitions from other cultures (e.g., Serbian flower-gifting customs) and comparing model performance patterns.

### Open Question 2
- Question: What specific components of Korean language training data improve cultural reasoning ability?
- Basis in paper: [inferred] The Discussion notes that HyperClova-X and EXAONE (Korean-focused models) perform better in Korean prompts, but "language-specific training boosts sensitivity... However, this does not necessarily improve performance, as seen in the weighted sum."
- Why unresolved: The paper identifies a correlation between Korean training and sensitivity but does not isolate which aspects of training (corpus composition, cultural annotations, instruction tuning) drive improvements.
- What evidence would resolve it: Ablation studies varying training data composition while controlling for model architecture and scale.

### Open Question 3
- Question: Why do explicit cultural cues improve performance more than prompt language alone?
- Basis in paper: [inferred] Results show "English+Specified exceeds Korean+Neutral, suggesting that contextual framing contributes more to reasoning performance than the language of the prompt itself," but the mechanism remains unexplained.
- Why unresolved: The paper documents this effect but does not investigate whether it stems from explicit cues triggering retrieval of cultural knowledge, reducing ambiguity, or other factors.
- What evidence would resolve it: Probing studies analyzing internal model activations when cultural cues are present versus absent, across different languages.

### Open Question 4
- Question: How reliably can LLM-based evaluators (like GPT-4 Turbo) assess cultural reasoning without gold reference responses?
- Basis in paper: [explicit] "Potential biases in the evaluator model and the absence of gold reference responses may still affect reliability. Future extensions could incorporate concise human-written references to improve robustness."
- Why unresolved: The current methodology achieved 88-90% alignment with human raters but relies on a single evaluator model without standardized reference answers.
- What evidence would resolve it: Comparing evaluations from multiple LLM judges against human annotations with gold references across diverse cultural scenarios.

## Limitations
- The benchmark's focus on Korean superstitions limits generalizability to broader cultural reasoning domains.
- Reliance on GPT-4 Turbo as both evaluator and gold standard introduces potential circularity in the evaluation methodology.
- Dataset construction depends on AI-human collaboration that may introduce systematic biases in question framing.

## Confidence

- **High confidence**: The knowledge-application gap finding (Mechanism 2) is well-supported by consistent MCQ-Trap score discrepancies across multiple topics and models.
- **Medium confidence**: The hallucination mechanism (Mechanism 3) is supported by observed score patterns but lacks direct evidence about training data composition.
- **Low confidence**: The claim that models "generally recognize factual information" in MCQ tasks may overstate performance, as the paper doesn't report baseline human performance or error analysis on incorrect MCQ responses.

## Next Checks

1. **Inter-annotator reliability audit**: Recompute human alignment metrics using individual human rater scores rather than aggregated references to verify the reported 90%/88.3% alignment rates.
2. **Cross-cultural generalization test**: Adapt 10 questions to analogous Western superstitions (e.g., breaking mirrors, walking under ladders) and rerun on the same models to test whether observed patterns transfer beyond Korean contexts.
3. **Hallucination root cause analysis**: Compare hallucination rates in Korean prompts against Korean-specific training data estimates from model cards/documentation to determine if observed patterns correlate with actual Korean data exposure.