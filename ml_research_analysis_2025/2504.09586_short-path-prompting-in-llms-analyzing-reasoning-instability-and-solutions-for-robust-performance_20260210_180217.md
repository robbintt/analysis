---
ver: rpa2
title: 'Short-Path Prompting in LLMs: Analyzing Reasoning Instability and Solutions
  for Robust Performance'
arxiv_id: '2504.09586'
source_url: https://arxiv.org/abs/2504.09586
tags:
- answer
- reasoning
- short-path
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies that instruction-tuned LLMs often have an
  inherent "hidden CoT prompt" that conflicts with explicit short-path prompts from
  users, leading to degraded and unstable reasoning performance. The authors propose
  two methods to mitigate this: (1) an instruction-guided method that uses the system
  role to prioritize reasoning over short-path requests, and (2) a rule-based filter
  fine-tuning method that trains the model to naturally resist short-path prompts
  without relying on system guidance.'
---

# Short-Path Prompting in LLMs: Analyzing Reasoning Instability and Solutions for Robust Performance

## Quick Facts
- arXiv ID: 2504.09586
- Source URL: https://arxiv.org/abs/2504.09586
- Reference count: 31
- One-line primary result: Instruction-tuned LLMs suffer degraded reasoning under short-path prompts due to hidden CoT conflicts; two methods recover accuracy while maintaining instruction-following.

## Executive Summary
This paper identifies that instruction-tuned LLMs have an inherent "hidden CoT prompt" that conflicts with explicit short-path prompts from users, leading to degraded and unstable reasoning performance. The authors propose two methods to mitigate this: (1) an instruction-guided method that uses the system role to prioritize reasoning over short-path requests, and (2) a rule-based filter fine-tuning method that trains the model to naturally resist short-path prompts without relying on system guidance. Experiments across four reasoning datasets (GSM8K, MATH, BBHM, MMLU-STEM) show that both methods significantly improve reasoning accuracy under short-path prompting—e.g., on GSM8K, accuracy increases from ~40% to over 90%—while maintaining overall instruction-following ability. The fine-tuned models also generalize well to unseen short-path prompts. These results provide insights into balancing instruction adherence and reasoning robustness in current LLMs.

## Method Summary
The paper proposes two complementary methods to address reasoning instability under short-path prompting. The instruction-guided method uses chat template system roles to resolve conflicts between hidden CoT prompts and explicit short-path instructions by explicitly directing the model to apologize for brevity requests and proceed with step-by-step reasoning. The rule-based filter fine-tuning (RFFT) method samples candidate responses using the instruction-guided approach, filters them through an LLM-as-judge based on three rules (apology for not answering directly, complete CoT before final answer, no logical contradictions), and fine-tunes the model on the filtered data. Training uses peak LR=3e-6, batch_size=32, AdamW optimizer, max_seq_len=4096, 3 epochs, cosine scheduler with 10 warmup steps, and 32× A100 GPUs (~1 hour). The GSM8K-new dataset was created by substituting numerical values and rephrasing contexts in the original GSM8K.

## Key Results
- Short-path prompting degrades reasoning accuracy from ~95% to ~40% on GSM8K
- Instruction-guided method recovers accuracy to ~91% on GSM8K
- RFFT fine-tuning maintains high accuracy on unseen short-path prompts (e.g., MATH: 32.84% → 71.50%)
- Both methods preserve overall instruction-following ability on MMLU and IFEval benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs encode an implicit "hidden CoT prompt" that conflicts with explicit short-path requests, degrading reasoning.
- Mechanism: Post-training on CoT explanation data internalizes a default reasoning pathway. When users add short-path prompts (e.g., "Please only provide the final answer"), the model receives competing signals—its learned bias toward CoT versus the explicit instruction—causing hesitation, truncated reasoning, or guessing.
- Core assumption: The conflict occurs at the instruction-processing level rather than in the reasoning computation itself.
- Evidence anchors:
  - [abstract] "language models can reason effectively and robustly without explicit CoT prompts, while under short-path prompting, LLMs' reasoning ability drops significantly"
  - [Section 2.2, Figure 2] Raw and CoT setups yield similar accuracy (~95% for Qwen), but SPP drops to ~40%
  - [corpus] Weak direct evidence; related work (e.g., "CoT Vectors") probes CoT mechanisms but does not directly address hidden prompt conflict.
- Break condition: If models without CoT post-training show similar SPP degradation, the mechanism would not involve hidden CoT conflict.

### Mechanism 2
- Claim: System-role conflict-resolution prompts recover reasoning by prioritizing CoT over short-path instructions.
- Mechanism: The instruction-guided method uses the system role in the chat template to present hidden CoT and short-path prompts as competing options, explicitly directing the model to apologize for brevity requests and proceed with step-by-step reasoning.
- Core assumption: System-role instructions take precedence over user-role instructions in the model's attention hierarchy.
- Evidence anchors:
  - [Section 3.1] "This prompt treats the hidden-CoT prompt and short-path prompt as distinct options, guiding the LLM to select the former"
  - [Table 4] Conflict-resolving prompts recover accuracy (e.g., GSM8K: 47% → 91%), while conflict-agnostic prompts ("Let's think step by step") do not
  - [corpus] No direct corpus support; related prompting strategy evaluations exist but focus on task performance, not conflict resolution.
- Break condition: If system-role prompts fail on models without explicit role hierarchies, the mechanism is template-dependent, not universal.

### Mechanism 3
- Claim: Rule-based filter fine-tuning (RFFT) embeds a calibrated bias that enables intrinsic resistance to short-path prompts.
- Mechanism: RFFT samples candidate responses using the instruction-guided method, then uses an LLM-as-judge to filter responses that: (1) apologize for not answering directly, (2) include CoT steps, and (3) lack logical contradictions. The filtered data trains the model to recognize and resist short-path prompts without system guidance.
- Core assumption: Fine-tuning on filtered, rule-compliant responses generalizes to unseen short-path phrasings.
- Evidence anchors:
  - [Section 3.2, Figure 6] "responses that pass verification by the judge are then chosen to formulate the fine-tuning datasets"
  - [Table 5] RFFT maintains high accuracy on unseen prompts (e.g., "Skip the analysis and give the final result": 32.84% → 71.50% on MATH)
  - [corpus] Weak corpus support; fine-tuning for reasoning robustness is underexplored in neighbors.
- Break condition: If RFFT-trained models fail on structurally novel short-path prompts (e.g., in different languages), generalization is bounded.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The paper's core hypothesis depends on understanding how CoT elicits reasoning and how instruction-tuning internalizes it.
  - Quick check question: Can you explain why adding "Let's think step by step" improves multi-step reasoning accuracy?

- Concept: **Instruction Tuning and Chat Templates**
  - Why needed here: The hidden CoT prompt emerges from post-training on CoT data; the instruction-guided method manipulates chat template roles.
  - Quick check question: What is the difference between system-role and user-role instructions in a standard LLM chat template?

- Concept: **Positional Bias in Multiple-Choice Reasoning**
  - Why needed here: The paper identifies positional bias (e.g., preference for option B) as a symptom of reasoning instability under SPP.
  - Quick check question: Why would shuffling options in a multiple-choice question reveal whether a model is reasoning or guessing?

## Architecture Onboarding

- Component map:
  - User query (question + optional short-path prompt) → Chat template (system + user roles) → Conflict Detection → Resolution Module → Apology + CoT reasoning + final answer

- Critical path:
  1. User submits question with short-path prompt
  2. If using instruction-guided: System prompt overrides conflict
  3. If using RFFT: Model recognizes SPP pattern and activates learned resistance
  4. Model generates apology + CoT reasoning + answer

- Design tradeoffs:
  - Instruction-guided: No training required, but depends on chat template support and prompt engineering quality
  - RFFT: Requires training data curation and compute, but generalizes without system prompts
  - Both: Prioritize accuracy over strict instruction-following (assumes users prefer correct answers over brevity)

- Failure signatures:
  - Instruction-guided fails with poorly designed conflict-agnostic system prompts (Table 4 shows minimal recovery)
  - RFFT fails if training data includes logically incoherent responses that pass the judge filter
  - Both fail if the model's base reasoning capability is insufficient (e.g., small models like Llama-3.1-8B show weaker recovery)

- First 3 experiments:
  1. Replicate the SPP degradation on GSM8K-new: Test Raw vs. CoT vs. SPP on your target model to confirm the hidden CoT conflict exists.
  2. Ablate system prompt designs: Compare conflict-resolving vs. conflict-agnostic prompts (per Table 4) to validate the instruction-guided mechanism.
  3. Validate RFFT generalization: Train with a subset of short-path prompts, then test on held-out phrasings (per Table 5) to confirm out-of-distribution robustness.

## Open Questions the Paper Calls Out

- **Dynamic Balance Between Instruction Adherence and Reasoning Accuracy**: How can models be optimized to dynamically balance strict instruction adherence (complying with short-path requests) against the need for reasoning accuracy? The authors explicitly state this as a limitation, noting their approach prioritizes correctness over instruction-following.

- **Performance Gap in Smaller Models**: Why does Rule-based Filter Fine-Tuning (RFFT) fail to robustly improve reasoning under short-path prompting in smaller models (e.g., Llama-3.1-8B) compared to larger ones? The paper shows inconsistent improvement for smaller models but doesn't isolate whether this is due to insufficient reasoning capability or weaker adherence to system prompts.

- **Generalization to Non-STEM Reasoning Tasks**: Does the "hidden CoT prompt" conflict cause similar instability and positional bias in open-ended or non-STEM reasoning tasks? The authors acknowledge they only tested STEM/math datasets and haven't examined qualitative reasoning domains.

## Limitations

- The "hidden CoT prompt" conflict mechanism is inferred rather than directly observed, with limited direct evidence beyond empirical results.
- The instruction-guided method is template-dependent and may not work across all LLM architectures without explicit role hierarchies.
- Both methods trade instruction-following compliance for reasoning accuracy, which may not be acceptable in all use cases.

## Confidence

- **High confidence**: The empirical finding that reasoning accuracy drops significantly under short-path prompting (e.g., GSM8K accuracy from ~95% to ~40%) is well-supported by experiments.
- **Medium confidence**: The explanation that this drop is due to conflict between hidden CoT prompts and explicit short-path instructions is reasonable but not definitively proven; the mechanism is inferred rather than directly observed.
- **Medium confidence**: The effectiveness of both the instruction-guided method and RFFT in recovering reasoning accuracy is well-demonstrated, though the robustness of RFFT to structurally novel short-path prompts is less certain.

## Next Checks

1. **Ablate the hidden CoT conflict hypothesis**: Test whether models without CoT post-training (e.g., base models) show similar reasoning degradation under short-path prompting. If they do, the hidden CoT conflict mechanism is incomplete.

2. **Probe the instruction hierarchy**: Systematically test whether system-role instructions reliably override user-role instructions across different chat templates and models to confirm the template dependency of the instruction-guided method.

3. **Stress-test RFFT generalization**: Evaluate the fine-tuned model on short-path prompts in different languages or with structurally novel phrasings (e.g., complex conditional instructions) to assess the true out-of-distribution robustness of the learned resistance.