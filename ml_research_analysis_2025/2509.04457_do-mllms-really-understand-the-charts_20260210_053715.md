---
ver: rpa2
title: Do MLLMs Really Understand the Charts?
arxiv_id: '2509.04457'
source_url: https://arxiv.org/abs/2509.04457
tags:
- chart
- reasoning
- answer
- visual
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChartVRBench, a benchmark designed to isolate
  and evaluate genuine visual reasoning in chart understanding by focusing on numerical
  estimation tasks without explicit annotations. The authors identify that current
  multimodal large language models (MLLMs) rely heavily on text recognition rather
  than visual reasoning when interpreting charts, leading to significant performance
  degradation on non-annotated data.
---

# Do MLLMs Really Understand the Charts?
## Quick Facts
- arXiv ID: 2509.04457
- Source URL: https://arxiv.org/abs/2509.04457
- Reference count: 40
- Key outcome: ChartVR-7B achieves 72.20% accuracy on ChartVRBench, outperforming both open-source baselines like Qwen2.5-VL-7B (61.39%) and powerful proprietary models such as Gemini-2.5-Flash (55.77%).

## Executive Summary
This paper introduces ChartVRBench, a benchmark designed to isolate and evaluate genuine visual reasoning in chart understanding by focusing on numerical estimation tasks without explicit annotations. The authors identify that current multimodal large language models (MLLMs) rely heavily on text recognition rather than visual reasoning when interpreting charts, leading to significant performance degradation on non-annotated data. To address this, they propose ChartVR, a model trained with a two-stage Visual Reasoning Reinforcement Finetuning (VR-RFT) strategy: first using supervised fine-tuning to teach structured reasoning chains, then employing reinforcement learning to refine numerical estimation accuracy.

## Method Summary
The authors propose ChartVR, a multimodal large language model enhanced for chart understanding through a two-stage Visual Reasoning Reinforcement Finetuning (VR-RFT) strategy. The first stage uses supervised fine-tuning to teach structured reasoning chains for interpreting charts, while the second stage employs reinforcement learning to refine numerical estimation accuracy. This approach aims to shift model behavior from text recognition to genuine visual reasoning. The benchmark ChartVRBench is specifically designed to evaluate this capability by focusing on numerical estimation tasks without explicit annotations, thereby isolating visual reasoning skills.

## Key Results
- ChartVR-7B achieves 72.20% accuracy on ChartVRBench, significantly outperforming Qwen2.5-VL-7B (61.39%) and Gemini-2.5-Flash (55.77%).
- The model demonstrates strong generalization to public chart reasoning benchmarks, indicating transferable visual reasoning skills.
- Current MLLMs show significant performance degradation on non-annotated data, confirming over-reliance on text recognition rather than visual reasoning.

## Why This Works (Mechanism)
The success of ChartVR stems from its targeted approach to overcoming the text recognition bias prevalent in current MLLMs. By designing ChartVRBench to focus on numerical estimation without explicit annotations, the benchmark forces models to rely on genuine visual reasoning rather than pattern matching or text extraction. The two-stage VR-RFT strategy first establishes structured reasoning capabilities through supervised fine-tuning, then refines these with reinforcement learning specifically optimized for numerical estimation accuracy. This combination addresses the core weakness of existing models while building robust visual reasoning skills that transfer beyond the training domain.

## Foundational Learning
- **Visual Reasoning**: Understanding charts requires interpreting visual elements (bars, lines, colors) to extract quantitative information without relying on text labels.
  - *Why needed*: Current MLLMs fail when explicit annotations are removed, indicating they don't truly understand visual data.
  - *Quick check*: Compare model performance on annotated vs non-annotated chart data.

- **Numerical Estimation from Visual Data**: Ability to accurately estimate values from chart elements without textual guidance.
  - *Why needed*: Real-world charts often lack explicit numerical labels, requiring estimation skills.
  - *Quick check*: Test model on charts with varying levels of numerical annotation.

- **Structured Reasoning Chains**: Breaking down chart interpretation into logical steps rather than direct answer prediction.
  - *Why needed*: Complex charts require multi-step reasoning that simple pattern matching cannot capture.
  - *Quick check*: Evaluate reasoning trace quality and logical consistency.

## Architecture Onboarding
- **Component Map**: ChartVRBench (evaluation) -> VR-RFT pipeline (training) -> ChartVR model (inference) -> Open-source/proprietary baselines (comparison)
- **Critical Path**: Supervised fine-tuning → Reinforcement learning refinement → Benchmark evaluation → Performance comparison
- **Design Tradeoffs**: Prioritizing visual reasoning over text recognition capabilities, accepting potential trade-offs in text-heavy tasks
- **Failure Signatures**: Performance collapse on non-annotated data, over-reliance on chart text elements, poor generalization to novel chart types
- **First Experiments**:
  1. Compare baseline MLLM performance on annotated vs non-annotated chart versions
  2. Test ChartVR on charts with gradually reduced annotation levels
  3. Evaluate cross-dataset generalization on public chart reasoning benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- The extent to which ChartVRBench eliminates text recognition bias remains uncertain, as some visual elements may still be interpreted through textual cues embedded in chart design.
- Transferability of visual reasoning skills to entirely novel chart types, real-world datasets, or complex multimodal contexts is not thoroughly validated.
- Reinforcement learning hyperparameters and potential overfitting to the benchmark's specific task distribution are not extensively discussed.

## Confidence
- **High Confidence**: The identification of text recognition over-reliance in current MLLMs for chart understanding is well-supported by comparative analysis between annotated and non-annotated data performance.
- **Medium Confidence**: The claim that ChartVR-7B outperforms both open-source and proprietary baselines on ChartVRBench is supported by reported metrics, though robustness across diverse scenarios requires further validation.
- **Low Confidence**: The assertion that visual reasoning skills learned are "foundational and transferable" is based on limited benchmark generalization and remains speculative without extensive testing across varied domains.

## Next Checks
1. Test ChartVR on additional, diverse chart reasoning datasets (e.g., InfoChartQA, Chart-HQA) to evaluate robustness and transferability beyond the benchmarks used in the paper.
2. Conduct detailed ablation studies to isolate the impact of supervised fine-tuning versus reinforcement learning in the VR-RFT pipeline, ensuring each stage contributes meaningfully to performance gains.
3. Deploy ChartVR in practical applications involving real-world charts (e.g., financial reports, scientific papers) to assess its ability to handle complex, multimodal contexts and noisy data.