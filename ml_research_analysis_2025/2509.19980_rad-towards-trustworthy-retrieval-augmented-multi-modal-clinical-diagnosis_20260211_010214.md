---
ver: rpa2
title: 'RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis'
arxiv_id: '2509.19980'
source_url: https://arxiv.org/abs/2509.19980
tags:
- value
- knowledge
- medical
- disease
- guideline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a retrieval-augmented framework for multimodal
  clinical diagnosis, RAD, to address the lack of explicit task-specific knowledge
  in current medical AI models. RAD retrieves and refines disease-centered guidelines
  from multiple medical sources and injects them into multimodal models to guide feature
  extraction, modality fusion, and decision-making.
---

# RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis

## Quick Facts
- **arXiv ID:** 2509.19980
- **Source URL:** https://arxiv.org/abs/2509.19980
- **Authors:** Haolin Li; Tianjie Dai; Zhe Chen; Siyuan Du; Jiangchao Yao; Ya Zhang; Yanfeng Wang
- **Reference count:** 40
- **Primary result:** State-of-the-art multi-modal clinical diagnosis using retrieval-augmented disease-centered guidelines, with average improvements of 3.09%, 2.24%, 2.49%, and 2.45% across MIMIC-ICD53, FairVLMed, SkinCAP, and NACC datasets.

## Executive Summary
This paper introduces RAD, a retrieval-augmented framework for multimodal clinical diagnosis that addresses the lack of explicit task-specific knowledge in current medical AI models. RAD retrieves and refines disease-centered guidelines from multiple medical sources and injects them into multimodal models to guide feature extraction, modality fusion, and decision-making. The framework employs a guideline-enhanced contrastive loss to align visual and textual features with disease-guideline prototypes and uses a dual decoder structure to steer cross-modal fusion using guidelines as queries. Experiments on four diverse medical datasets demonstrate state-of-the-art performance with significant improvements across all metrics, while also improving interpretability by focusing on abnormal regions and critical indicators for evidence-based diagnosis.

## Method Summary
RAD addresses multi-modal multi-label disease classification using Image (X-ray/MRI), Report, and EHR data by retrieving disease-centered guidelines from a knowledge corpus built from Wiki, PubMed, Guidelines, and Books. The method uses MedCPT to retrieve top-10 documents per disease, which are then refined using Qwen2.5-72B to create high-quality guideline embeddings. The architecture combines ResNet-50 for image processing and ClinicalBERT for text processing, with a Dual Transformer Decoder structure that uses both Guideline and Label embeddings as queries for cross-attention with fused Image/Text embeddings. The optimization employs a composite loss function combining BCE loss with Guideline-Enhanced Contrastive Loss (GECL), trained on hardware including a single NVIDIA A100. The framework demonstrates superior performance across four datasets while providing interpretable, evidence-based diagnoses.

## Key Results
- Achieves state-of-the-art performance with average improvements of 3.09%, 2.24%, 2.49%, and 2.45% across MIMIC-ICD53, FairVLMed, SkinCAP, and NACC datasets respectively
- Improves interpretability by focusing on abnormal regions and critical indicators, enabling evidence-based diagnosis
- Constructs MIMIC-ICD53, a new large-scale dataset with 51,830 samples across 53 classes by aligning MIMIC-CXR and MIMIC-IV with 3-day windows

## Why This Works (Mechanism)
RAD works by explicitly incorporating domain knowledge into the multimodal learning process through retrieval-augmented guidelines. The framework addresses the key limitation of current medical AI models that lack task-specific knowledge by retrieving disease-centered guidelines from multiple sources and refining them with large language models. These refined guidelines serve dual purposes: as prototypes for contrastive learning to align visual and textual features with disease-specific knowledge, and as queries in the dual decoder structure to guide cross-modal fusion. This approach ensures that the model's decision-making is grounded in established medical knowledge rather than purely data-driven patterns, leading to more trustworthy and interpretable diagnoses that focus on clinically relevant features.

## Foundational Learning
- **Multi-modal fusion:** Combining information from different data types (images, text, EHR) is essential for comprehensive clinical diagnosis; verify that modality encoders produce compatible feature representations
- **Contrastive learning:** Aligning features with disease prototypes improves semantic understanding; check that positive/negative pairs are properly constructed and that temperature parameter is well-tuned
- **Retrieval augmentation:** Incorporating external knowledge enhances model performance; validate that retrieved guidelines are relevant and that refinement process preserves critical information
- **Dual decoder architecture:** Using separate decoders for guidelines and labels enables targeted guidance; ensure that attention mechanisms properly weight different guidance sources
- **Medical knowledge integration:** Grounding AI decisions in established clinical guidelines improves trustworthiness; verify that guidelines are accurately represented and appropriately weighted in the loss function

## Architecture Onboarding

**Component Map:** Image/Text Encoders -> Dual Decoder (Guideline + Label Queries) -> Classification Output

**Critical Path:** Input data → Encoders (ResNet-50 + ClinicalBERT) → Fusion layer → Dual Decoder cross-attention → Classification head

**Design Tradeoffs:** The framework trades increased computational complexity (dual decoder, contrastive loss) for improved performance and interpretability; uses larger models (Qwen2.5-72B) for guideline refinement despite higher inference costs

**Failure Signatures:** Performance degradation when guidelines are noisy or irrelevant; training instability with improper contrastive loss parameters; modality misalignment when text encoder is not optimized for clinical language

**3 First Experiments:**
1. Verify baseline performance with standard multimodal fusion without retrieval augmentation
2. Test single decoder with only label queries to assess contribution of guideline guidance
3. Evaluate impact of guideline quality by comparing raw vs refined retrieved documents

## Open Questions the Paper Calls Out
None

## Limitations
- Several critical implementation details are underspecified including temperature parameter τ for GECL, negative sampling ratio r, and exact Dual Decoder architecture specifications
- The paper does not address potential biases in the medical knowledge corpus or handling of rare diseases with limited training examples
- While interpretability improvements are claimed, systematic evaluation using established metrics or human expert validation is not provided

## Confidence

**High Confidence:** The overall methodology of using retrieval-augmented frameworks with disease-centered guidelines for multimodal clinical diagnosis is sound and well-supported by experimental results showing state-of-the-art performance across four diverse datasets.

**Medium Confidence:** The specific architectural choices (ResNet-50 + ClinicalBERT, Dual Decoder structure) are reasonable but could benefit from more detailed specifications to ensure faithful reproduction.

**Low Confidence:** The exact implementation of the Guideline-Enhanced Contrastive Loss (GECL) and the Dual Transformer Decoder, particularly the temperature parameter and negative sampling ratio, are critical unknowns that could affect both performance and reproducibility.

## Next Checks
1. Reproduce ablation studies comparing ClinicalBERT vs BioClinicalBERT to confirm the importance of text encoder choice for text-heavy tasks
2. Conduct systematic evaluation of RAD's interpretability using established metrics (e.g., Grad-CAM, attention visualization) and compare with baseline models, ideally with medical domain expert input
3. Evaluate RAD's performance on rare disease cases or cases with limited training examples to assess robustness and generalization beyond common conditions