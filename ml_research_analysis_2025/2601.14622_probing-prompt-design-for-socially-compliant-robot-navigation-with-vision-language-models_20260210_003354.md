---
ver: rpa2
title: Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language
  Models
arxiv_id: '2601.14622'
source_url: https://arxiv.org/abs/2601.14622
tags:
- prompt
- navigation
- reasoning
- language
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates prompt design for socially compliant robot
  navigation using small vision language models (VLMs). The authors propose two dimensions
  of prompt design: system guidance (action-focused, reasoning-oriented, and perception-reasoning
  prompts) and motivational framing (competing against humans, other AI systems, or
  the model''s past self).'
---

# Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models

## Quick Facts
- arXiv ID: 2601.14622
- Source URL: https://arxiv.org/abs/2601.14622
- Reference count: 8
- Key outcome: Optimal prompt configurations differ by model training state—reasoning-oriented prompts with human competition for non-finetuned models, perception-reasoning prompts with self-competition for finetuned models

## Executive Summary
This paper investigates how prompt design affects socially compliant robot navigation performance using small vision language models (VLMs). The authors systematically vary system guidance types (action-focused, reasoning-oriented, perception-reasoning) and motivational framing (competition against humans, other AI systems, or the model's past self) across two navigation datasets. Their key finding is that prompt effectiveness depends critically on the model's training state: non-finetuned models benefit from human-referenced competition, while finetuned models perform best when competing against their own past performance. Importantly, the study reveals that system prompts act primarily as decision-level constraints rather than improving semantic understanding, with disproportionate improvements in action accuracy compared to perceptual or reasoning metrics.

## Method Summary
The study uses the TinyLLaVA framework with frozen SigLIP vision encoder and trainable projector/small LLM components. Two datasets are employed: SNEI (265 train/60 test samples) and MUSON (640 train/160 test samples), both containing egocentric images with conversational annotations about social navigation scenarios. Nine prompt variants are created by combining three system guidance types (A-series action-focused, R-series reasoning-oriented, PR-series perception-reasoning) with three motivational framings (competition against humans, other AI systems, or past self). Models are trained with DeepSpeed ZeRO Stage-3 using AdamW optimizer, cosine learning rate schedule with warmup, and FP16 precision. Performance is evaluated using action accuracy (primary metric) and semantic alignment metrics (BERT-F1, SBERT).

## Key Results
- Non-finetuned GPT-4o achieves best performance with reasoning-oriented prompts and competition against humans
- Finetuned small VLMs perform optimally with perception-reasoning prompts and competition against their past self
- System prompts can degrade performance below no-prompt baseline if poorly designed
- Prompt design produces disproportionate improvements in action accuracy versus semantic metrics, indicating decision-level rather than representational enhancement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** System prompts primarily function as decision-level constraints rather than representational enhancers.
- **Mechanism:** Prompts shape the final action selection process by constraining the output space toward socially compliant navigation decisions, rather than improving the model's underlying semantic understanding of scenes. This explains why prompts yield disproportionate improvements in action accuracy compared to semantic metrics like perception or prediction quality.
- **Core assumption:** Action selection and semantic understanding are at least partially separable cognitive processes in VLMs.
- **Evidence anchors:** Abstract states "our system prompts produce a disproportionately larger improvement in action accuracy"; Section 5.1 shows "substantially larger improvements in action accuracy than in the semantic quality"; corpus reference MAction-SocialNav finds reasoning-enhanced prompts improve action selection.

### Mechanism 2
- **Claim:** Optimal prompt configuration depends on model training state—non-finetuned models benefit from human-referenced competition, while finetuned models benefit from self-referenced competition.
- **Mechanism:** Non-finetuned models lack task-specific priors and benefit from external human benchmarks that ground social norms. Finetuned models have internalized task structure and benefit from self-improvement framing that leverages their existing knowledge without conflicting external references.
- **Core assumption:** Motivational framing interacts with the model's existing knowledge state; mismatched framing creates interference rather than guidance.
- **Evidence anchors:** Abstract states "For non-finetuned GPT-4o, competition against humans achieves the best performance... For finetuned models, competition against the model's past self yields the strongest results"; Section 5.1 shows "competition against the model's past self consistently yields the best performance [for finetuned models]."

### Mechanism 3
- **Claim:** Poorly designed system prompts can degrade performance below no-prompt baseline.
- **Mechanism:** System prompts that are misaligned with model capabilities or task structure introduce noise into the decision process, actively interfering with the model's pre-existing competencies rather than enhancing them.
- **Core assumption:** Models have baseline navigational reasoning from pretraining; prompts can either channel or disrupt this capacity.
- **Evidence anchors:** Abstract states "Poorly designed prompts can degrade performance below a no-prompt baseline"; Section 5.1 notes "A naive system prompt performs worse than the no–system-prompt baseline"; Section 1 mentions "Discrete prompts are often highly sensitive to wording and formatting."

## Foundational Learning

- **Concept: Vision-Language Model Architecture**
  - **Why needed here:** The TinyLLaVA framework uses a frozen vision encoder, trainable projector, and trainable language model. Understanding this separation is critical for knowing what finetuning modifies versus what prompts must compensate for.
  - **Quick check question:** Can you explain why the vision encoder remains frozen while the projector is trained, and what capability gap this creates that prompts might address?

- **Concept: Semantic vs. Decision-Level Metrics**
  - **Why needed here:** The paper's core claim hinges on distinguishing between semantic alignment (BERT-F1, SBERT for perception/prediction/reasoning) and action accuracy. Without this distinction, the mechanism claim is uninterpretable.
  - **Quick check question:** If a model improves from 0.45 to 0.50 in BERT-F1 but from 0.45 to 0.60 in action accuracy, what does this suggest about where the intervention is having its primary effect?

- **Concept: Prompt Sensitivity in Small Models**
  - **Why needed here:** The paper explicitly targets efficiency-oriented deployments using small VLMs, which exhibit weaker decision-making and higher prompt sensitivity than large models.
  - **Quick check question:** Why might a prompt that works well for GPT-4o fail or even degrade performance for a 1.1B parameter model fine-tuned on the same task?

## Architecture Onboarding

- **Component map:**
  RGB Image → Frozen Vision Encoder (SigLIP) → Trainable Multimodal Projector → Trainable Small Language Model (Phi-2 / Stablelm / TinyLlama) → Language Response containing perception, prediction, reasoning, action

- **Critical path:**
  1. Select base VLM and determine training state (finetuned vs. non-finetuned)
  2. Choose system guidance type based on model state: R-series for non-finetuned, PR-series for finetuned
  3. Choose motivational framing: human competition for non-finetuned, past-self competition for finetuned
  4. Apply discrete action constraint ("Move Forward", "Turn Left", etc.) for evaluation
  5. Evaluate using action accuracy (AA) as primary metric, not semantic similarity

- **Design tradeoffs:**
  - R-series prompts (reasoning-focused) vs. PR-series (perception-reasoning): PR provides more structure but may over-constrain smaller models
  - Human vs. past-self competition: Human provides external norm grounding; past-self avoids reference conflict but requires finetuning to have a meaningful "past"
  - Finetuning investment vs. prompt engineering: Finetuning improves semantics; prompts improve decisions—both may be needed

- **Failure signatures:**
  - Action accuracy drops below no-prompt baseline → Prompt design is misaligned; try simpler framing or different motivational reference
  - High semantic scores (BERT-F1, SBERT) but low AA after finetuning → Model understands scene but decision constraint is missing; add action-focused guidance
  - PR prompts underperform R prompts on finetuned models → Dataset may lack perceptual diversity; switch to R-series
  - Competition vs. AI systems consistently worst → This framing appears universally counterproductive; avoid

- **First 3 experiments:**
  1. **Baseline establishment:** Run no-prompt and naive-prompt conditions on your target VLM to confirm the degradation effect exists in your setup.
  2. **Guidance type ablation:** Test A-series, R-series, and PR-series prompts with fixed human-competition framing to isolate guidance effects.
  3. **Framing ablation:** With best guidance type from step 2, test all three motivational framings to validate the model-state-dependent pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the optimal prompt configurations generalize to larger and more diverse real-world social navigation datasets that include challenging conditions such as snowy weather, nighttime, or traffic signals?
- **Basis in paper:** The authors state "the datasets used in this study enable controlled evaluation, their limited size may not fully capture the diversity of real-world social navigation scenarios, such as snowy weather, nighttime conditions, or situations involving traffic signals."
- **Why unresolved:** The experiments were limited to SNEI (325 samples) and MUSON (800 samples), which may not represent the full complexity of real-world deployment.
- **What evidence would resolve it:** Evaluation on larger-scale datasets with diverse environmental conditions showing consistent prompt performance patterns.

### Open Question 2
- **Question:** Can adaptive or learned prompting mechanisms that respond to online environmental feedback outperform the fixed textual prompts studied here?
- **Basis in paper:** "We focus on text-based prompt design and do not explore adaptive or learned prompting mechanisms. Incorporating online prompt adaptation and reinforcement learning to respond to environmental feedback is an important direction for future research."
- **Why unresolved:** This study only examined static, handcrafted prompts across three guidance types and three motivational framing strategies.
- **What evidence would resolve it:** Comparative experiments showing dynamic prompt adaptation improving action accuracy over fixed prompts in changing environments.

### Open Question 3
- **Question:** Why does competition against other AI systems consistently perform worst across both finetuned and non-finetuned models?
- **Basis in paper:** The results show this pattern consistently (Table 1: A2, R2, PR2 underperform; Tables 2-3: similar trend), but the paper does not provide a mechanistic explanation for this finding.
- **Why unresolved:** The paper reports the empirical finding but does not investigate the underlying cognitive or computational reasons.
- **What evidence would resolve it:** Ablation studies analyzing model attention and reasoning patterns under different competitive framings to identify failure modes.

### Open Question 4
- **Question:** How do the findings transfer to continuous control settings with more complex embodied environments?
- **Basis in paper:** "Our experiments assume a constrained, discrete action space, which simplifies comparison but limits realism. Extending our analysis to continuous control and more complex embodied environments remains future work."
- **Why unresolved:** The discrete action space (6 commands) may not capture the nuance required for real-world robot navigation.
- **What evidence would resolve it:** Experiments on continuous action benchmarks showing similar prompt design effects on trajectory quality and social compliance metrics.

## Limitations
- Prompt effectiveness depends critically on exact wording and formatting, making replication challenging
- Limited to discrete action spaces, restricting real-world applicability to continuous control scenarios
- Does not explore adaptive or learned prompting mechanisms that could respond to environmental feedback

## Confidence

**High confidence:** System prompts can degrade performance below baseline; action accuracy improvements exceed semantic metric improvements
**Medium confidence:** Optimal prompt configuration depends on model training state; reasoning-oriented prompts work best for non-finetuned models
**Low confidence:** The specific mechanism by which motivational framing interacts with model knowledge state is theoretically plausible but requires direct validation

## Next Checks
1. **Mechanism validation:** Conduct controlled experiments varying only motivational framing while holding guidance type constant to isolate state-dependent effects
2. **Generalization testing:** Apply optimal prompt configurations to different VLM architectures (not just TinyLLaVA) to verify the framework's robustness
3. **Ablation studies:** Remove specific components of the PR-series prompts to determine which elements (perception, reasoning, or their combination) drive the performance gains