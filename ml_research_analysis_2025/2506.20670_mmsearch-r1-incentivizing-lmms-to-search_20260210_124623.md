---
ver: rpa2
title: 'MMSearch-R1: Incentivizing LMMs to Search'
arxiv_id: '2506.20670'
source_url: https://arxiv.org/abs/2506.20670
tags:
- search
- image
- answer
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMSearch-R1, a reinforcement learning framework
  that enables large multimodal models to perform on-demand, multi-turn search in
  real-world Internet environments. It integrates both image and text search tools
  and uses an outcome-based reward with a search penalty to guide models to invoke
  search only when necessary.
---

# MMSearch-R1: Incentivizing LMMs to Search

## Quick Facts
- arXiv ID: 2506.20670
- Source URL: https://arxiv.org/abs/2506.20670
- Authors: Jinming Wu; Zihao Deng; Wei Li; Yiding Liu; Bo You; Bo Li; Zejun Ma; Ziwei Liu
- Reference count: 40
- One-line primary result: MMSearch-R1-7B outperforms RAG-based baselines and matches 32B RAG models while reducing search calls by 30%+.

## Executive Summary
MMSearch-R1 introduces a reinforcement learning framework that enables large multimodal models to perform on-demand, multi-turn search in real-world Internet environments. By integrating image and text search tools with an outcome-based reward system that includes search penalties, the model learns to invoke search only when necessary. The authors construct a search-balanced dataset (FVQA) with both search-required and search-free samples, demonstrating that this balance is essential for efficient search behavior. Experiments show MMSearch-R1-7B outperforms RAG-based baselines of the same size and matches 32B RAG performance while significantly reducing search calls.

## Method Summary
The method uses GRPO (Group Relative Policy Optimization) via the veRL framework to train Qwen2.5-VL-7B to decide when to invoke image or text search tools. The reward function applies accuracy scores with a search penalty (0.1 factor) plus format compliance. Training uses a search-balanced dataset (~3,400 search-required and ~1,600 search-free samples) constructed through semi-automated pipeline. The model generates 8 rollouts per sample in multi-turn dialogues with search tools, with retrieved content masked during loss computation to focus learning on search strategy rather than specific retrieved content.

## Key Results
- MMSearch-R1-7B outperforms RAG-based baselines of the same size on knowledge-intensive and information-seeking VQA tasks
- Matches performance of 32B RAG-based model while reducing search calls by over 30%
- RL training improves ability to recognize knowledge boundaries and generate effective search queries
- Ablation studies show balanced data and search penalties are essential for efficient, on-demand search behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Outcome-based reward with search penalty shapes on-demand search behavior
- Mechanism: The reward function applies a penalty factor (0.1) to accuracy when search is performed, creating gradient toward using internal knowledge first
- Core assumption: Model can distinguish between questions answerable from internal knowledge vs. those requiring external information
- Evidence anchors: Abstract states "guided by an outcome-based reward with a search penalty"; page 4 describes penalty application; Figure 5(b) shows without penalty search ratio converges to 100%

### Mechanism 2
- Claim: Search-balanced dataset is essential for learning when to search
- Mechanism: Training on both search-required and search-free samples teaches model to recognize knowledge boundaries
- Core assumption: Model's pre-existing knowledge distribution can be estimated for constructing appropriate training samples
- Evidence anchors: Abstract states search-balanced subset "proves essential"; section 3.1 describes 3,400 search-required and ~1,600 search-free samples; Figure 5(b) shows removing data balancing leads to 100% search ratio

### Mechanism 3
- Claim: GRPO with masked tool outputs enables stable RL training with real-world search tools
- Mechanism: GRPO estimates baseline from group rewards without value function; tool outputs masked during loss computation
- Core assumption: Search tool provides sufficiently reliable results for learning meaningful search strategies
- Evidence anchors: Section 2.3 states retrieved content is masked during loss computation; section 2.1 describes GRPO baseline estimation; limitations section reports tool failure rates

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO) and GRPO variants
  - Why needed here: Training framework uses GRPO; understanding policy gradients, clip ratios, and KL constraints is essential for debugging
  - Quick check question: Can you explain why GRPO removes the need for a value function compared to standard PPO?

- Concept: Retrieval-Augmented Generation (RAG) pipelines
  - Why needed here: RAG is the baseline being improved upon; understanding its limitations clarifies motivation
  - Quick check question: What are two key differences between traditional RAG and on-demand search approach in MMSearch-R1?

- Concept: Multi-turn tool-augmented dialogue
  - Why needed here: Model engages in multi-turn interactions with search tools, producing structured outputs
  - Quick check question: How should tool-returned content be handled during loss computation, and why?

## Architecture Onboarding

- Component map: Policy Model (Qwen2.5-VL-7B) -> Multimodal Search Tools -> Reward Model -> GRPO Update -> Policy Model

- Critical path:
  1. Sample question from FVQA-train
  2. Generate 8 rollouts per sample via multi-turn dialogue with search tools
  3. Compute rewards (accuracy × search penalty + α × format)
  4. Update policy via GRPO objective (clip ratio ε=0.2, KL β=0.001)
  5. Repeat until convergence (~50 steps)

- Design tradeoffs:
  - Exact match vs. LLM-as-judge reward: EM is simple and scalable but may penalize semantically correct answers
  - Search penalty magnitude: Too high → under-search; too low → over-search; paper uses 0.1
  - Rollout count: 8 rollouts per sample balances variance reduction with compute cost

- Failure signatures:
  - Search ratio converging to 100%: Likely missing search penalty or unbalanced data
  - Low accuracy with high search ratio: Model not learning effective query generation
  - Format score consistently 0: Prompt compliance issue
  - Training instability: Check KL divergence coefficient, tool failure rates, or reward scaling

- First 3 experiments:
  1. Replicate ablation in Figure 5(b): Train without search penalty and without data balancing separately
  2. Vary search penalty factor (0.0, 0.1, 0.3, 0.5) on held-out validation set
  3. Test tool failure handling: Inject synthetic failures into text search pipeline

## Open Questions the Paper Calls Out

1. How can reward modeling be advanced to support complex or open-ended QA tasks beyond exact string matching?
   - Basis: Appendix H states current exact match design "may penalize answers that are semantically correct"
   - Why unresolved: Rule-based rewards lack semantic nuance while LLM-based rewards introduce bias and computational costs
   - What evidence would resolve it: A reward mechanism maintaining exact match scalability while capturing semantic correctness

2. How can image search tools be optimized to retrieve information based on localized visual content?
   - Basis: Appendix H identifies limitation where "image search currently requires submitting full image"
   - Why unresolved: Current pipeline forces processing entire image, potentially missing fine-grained details
   - What evidence would resolve it: Integration of region-aware search mechanism improving retrieval for localized queries

3. Does using semantic-level rewards consistently improve trade-off between accuracy and search efficiency?
   - Basis: Appendix G.2 shows GPT-4o rewards improve accuracy (59.5% vs. 55.7%) but increase search ratio (82.6% vs. 77.3%)
   - Why unresolved: Unclear if increased search frequency is necessary cost for higher accuracy or side effect
   - What evidence would resolve it: Experiments showing semantic reward setup achieving higher accuracy while maintaining or reducing search ratio

## Limitations
- Search-balanced dataset construction relies on model-based labeling which may introduce bias
- Text search pipeline depends on specific external components (SerpAPI, Jina Reader, Qwen3-32B) whose availability may vary
- FVQA dataset construction method may not capture full diversity of real-world multimodal search scenarios
- Reported results are benchmark-specific and may not generalize without retraining

## Confidence
- **High Confidence**: RL framework successfully improves search efficiency (30% reduction) while maintaining/increasing accuracy vs RAG baselines
- **Medium Confidence**: Claim that MMSearch-R1-7B matches 32B RAG performance may be sensitive to specific benchmarks
- **Low Confidence**: Semi-automated dataset construction's ability to generate representative queries across diverse domains hasn't been fully validated

## Next Checks
1. Evaluate MMSearch-R1 on held-out set of human-generated multimodal search queries to assess real-world robustness
2. Systematically vary search penalty factor (0.05, 0.1, 0.2, 0.3) across multiple benchmarks to map accuracy-search efficiency tradeoff curve
3. Inject controlled failures into text search pipeline (10-50% tool failure rate) to measure model robustness and identify failure propagation patterns