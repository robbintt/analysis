---
ver: rpa2
title: 'Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs'
arxiv_id: '2505.20155'
source_url: https://arxiv.org/abs/2505.20155
tags:
- uni00000013
- pruning
- uni00000051
- pangu
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pangu Light addresses the challenge of aggressively pruning large
  language models while preserving performance. The core method introduces weight
  re-initialization strategies (CLAP and SLNP) that stabilize models after joint width
  and depth pruning, combined with Post-RMSNorm absorption for efficient inference.
---

# Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs

## Quick Facts
- arXiv ID: 2505.20155
- Source URL: https://arxiv.org/abs/2505.20155
- Reference count: 29
- Primary result: 81.6 average score on reasoning benchmarks (vs 80.9 for Qwen3-32B) at 2.1x speedup while retaining 98.9% of original model performance

## Executive Summary
Pangu Light introduces a weight re-initialization framework for aggressively pruning large language models while maintaining performance. The approach combines Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP) to stabilize models after joint width and depth pruning, followed by Post-RMSNorm absorption for efficient inference. On Ascend NPUs, Pangu Light-32B achieves competitive reasoning performance while delivering 2.1x acceleration through structural pruning. The framework provides a superior accuracy-efficiency trade-off compared to existing pruning methods.

## Method Summary
Pangu Light uses structured pruning with importance scoring to remove channels, heads, FFN neurons, and layers based on calibration data. CLAP merges KV groups from pruned layers into retained layers to preserve attention information. SLNP rescales RMSNorm parameters after width pruning to prevent activation collapse. Post-RMSNorm absorption converts dynamic normalization to static scaling for inference acceleration. Recovery training uses 300B-token knowledge distillation from the original model.

## Key Results
- 81.6 average score on reasoning benchmarks vs 80.9 for Qwen3-32B on Ascend NPUs
- 2585 tokens/s throughput vs 2225 for Qwen3-32B at 2.1x acceleration
- 98.9% performance retention after aggressive pruning
- Up to 6% throughput gain from Post-RMSNorm absorption on 38B model

## Why This Works (Mechanism)

### Mechanism 1: Cross-Layer Attention Pruning (CLAP) for Depth Stabilization
When layer l+1 is pruned, CLAP transfers high-importance KV groups from both layer l and l+1 to layer l by ranking groups and merging top-K. This mitigates information loss during depth pruning by redistributing critical attention computations across consecutive layers.

### Mechanism 2: Stabilized LayerNorm Pruning (SLNP) for Width Stabilization
SLNP rescales RMSNorm affine parameters γ after channel pruning using correction scalar c_l = ||γ_orig||_2 / ||γ_pruned||_2. This prevents activation magnitude collapse by restoring the output scale disrupted by removing dimensions.

### Mechanism 3: Post-RMSNorm Absorption for Inference Acceleration
Post-RMSNorm absorption converts dynamic Post-RMSNorm operations into static channel-wise scaling by pre-computing average inverse scaling s̄_inv and fusing it into preceding projection matrices. This eliminates normalization overhead while maintaining accuracy within 0.9 points of full Sandwich-Norm.

## Foundational Learning

- **Structured vs. Unstructured Pruning**: Pangu Light uses structured pruning (removing entire channels, heads, layers) for hardware-speedup guarantees. Quick check: Can you explain why removing 50% of weights via unstructured pruning often yields minimal speedup on standard GPUs?

- **RMSNorm and Affine Parameters (γ)**: Both SLNP and Post-RMSNorm absorption manipulate learnable scale parameters γ that modulate normalized outputs. Quick check: Given RMSNorm(x) = γ ⊙ x / RMS(x), what happens to output magnitude if γ dimensions are removed without compensation?

- **Knowledge Distillation for Recovery Training**: The paper uses original model as teacher for 300B-token distillation to recover performance after pruning. Quick check: Why use full-logit distillation rather than hard-label fine-tuning for recovering a pruned LLM?

## Architecture Onboarding

- **Component map**: Importance Scoring Module → CLAP Module → SLNP Module → Post-RMSNorm Absorption Module → Recovery Training Pipeline

- **Critical path**: Collect calibration activations → compute importance scores → determine prune targets → apply CLAP (depth pruning) → apply SLNP (width pruning) → apply Post-RMSNorm absorption → run 300B-token distillation recovery

- **Design tradeoffs**: Aggressive multi-axis pruning yields higher speedup but increases recovery difficulty; Post-RMSNorm absorption trades architectural flexibility for inference speed; calibration dataset size affects importance score reliability

- **Failure signatures**: Accuracy collapse after pruning without CLAP/SLNP indicates destabilized attention or normalization; Post-absorption accuracy drops >1-2 points on out-of-distribution tasks suggests calibration mismatch; distillation loss plateaus indicates teacher-student logit misalignment

- **First 3 experiments**:
  1. Prune only width (no depth) on small model variant; compare activation-based scoring vs random baseline
  2. Remove one layer with and without CLAP; measure downstream task delta to isolate cross-layer transfer benefit
  3. Log ||γ_orig||_2 / ||γ_pruned||_2 ratios across layers after width pruning; verify clustering near expected retention ratio

## Open Questions the Paper Calls Out
- Can CLAP and SLNP techniques generalize to LLM architectures without Sandwich-Norm (e.g., Pre-RMSNorm or LayerNorm only)?
- Why does cross-layer parameter merging succeed for attention blocks but fail to yield improvements for FFN blocks?
- How sensitive are pruning importance metrics and re-initialization scalars to calibration dataset composition and size?

## Limitations
- Calibration dataset composition remains unspecified, creating uncertainty about importance scoring reliability
- Specific pruning thresholds for achieving target compression ratios are not provided
- Post-RMSNorm absorption assumes stable activation statistics at inference, which may not hold for out-of-distribution inputs

## Confidence
- **High Confidence**: Post-RMSNorm absorption methodology and knowledge distillation recovery training procedure
- **Medium Confidence**: CLAP and SLNP re-initialization mechanisms (show measurable improvements but rely on assumptions)
- **Low Confidence**: Overall performance claims due to unspecified calibration dataset and pruning ratios

## Next Checks
1. Measure cosine similarity between consecutive layer attention patterns to quantify cross-layer transfer foundation
2. Apply Post-RMSNorm absorption on models calibrated with different domain distributions and measure accuracy degradation on held-out domains
3. Visualize layer-wise importance score distributions across all layers to verify expected patterns