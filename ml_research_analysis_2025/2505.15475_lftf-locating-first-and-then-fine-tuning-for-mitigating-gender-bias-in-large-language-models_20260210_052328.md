---
ver: rpa2
title: 'LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large
  Language Models'
arxiv_id: '2505.15475'
source_url: https://arxiv.org/abs/2505.15475
tags:
- gender
- bias
- llms
- layer
- lftf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender bias in large language models (LLMs),
  particularly regarding profession-related associations. The authors propose two
  datasets, GenBiasEval and GenHintEval, along with corresponding evaluation metrics
  (AFGB-Score and UB-Score) to quantify gender bias.
---

# LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models

## Quick Facts
- arXiv ID: 2505.15475
- Source URL: https://arxiv.org/abs/2505.15475
- Reference count: 17
- Primary result: Reduces gender bias in LLMs while preserving general capabilities using targeted single-block fine-tuning

## Executive Summary
This paper introduces LFTF (Locating First and Then Fine-Tuning), an algorithm designed to mitigate gender bias in large language models, particularly regarding profession-related associations. The approach works by first identifying which transformer blocks are most responsible for gender bias using a Block Mitigating Importance Score (BMI), then fine-tuning only those blocks with a carefully designed loss function. Experiments demonstrate significant bias reduction (AFGB-Score dropping from ~0.26 to ~0.08) while maintaining general capabilities across multiple benchmarks, outperforming baseline methods like full-parameter fine-tuning and prompt-based approaches.

## Method Summary
The LFTF algorithm operates in two stages: first, it locates bias-relevant blocks by computing BMI scores that measure how much each block transforms hidden states when processing bias-eliciting prompts. The block with the highest BMI (typically the final layer) is identified as the primary source of gender bias. In the second stage, LFTF fine-tunes only this block using a dual-term loss function that simultaneously minimizes the probabilities of "he" and "she" pronouns on gender-eliciting templates. This targeted approach preserves general capabilities by limiting parameter updates to a small fraction of the model while effectively reducing gender bias.

## Key Results
- AFGB-Score reduced from 0.2618 to 0.0796 on Qwen2.5-7B and from 0.2604 to 0.0815 on Meta-Llama3-8B
- UB-Score maintained above 0.75 for both models, indicating preserved hint-following capability
- Outperforms baselines including full-parameter fine-tuning, LoRA, and prompt-based methods
- General capability retention: <1% change on 7 of 9 general benchmarks for Qwen2.5-7B-LFTF

## Why This Works (Mechanism)

### Mechanism 1: BMI-Based Block Localization Identifies Bias-Concentrated Parameters
The LFTF approach relies on the premise that gender bias in LLMs is not uniformly distributed but concentrated in specific transformer blocks. The BMI metric measures cosine similarity between input and output hidden states for each block when processing bias-eliciting prompts. Lower similarity (higher BMI) indicates the block is actively transforming representations in ways related to gender bias. The final layer consistently shows the highest BMI across tested models, suggesting bias emerges prominently in late-stage processing.

### Mechanism 2: Dual-Term Loss Function Enforces Gender Probability Equalization
The loss function L = P("he"|p,M) + P("she"|p,M) penalizes the model for favoring either pronoun. When one probability is higher, its corresponding term dominates the gradient, pushing the model to reduce that probability. This creates a self-balancing dynamic where the model converges toward equal probabilities (~0.5 each) rather than anti-bias (e.g., always predicting the opposite gender).

### Mechanism 3: Single-Block Fine-Tuning Preserves General Capabilities via Parameter Isolation
By updating only ~1/32 of parameters (for a 32-layer model), the majority of the model's weights remain unchanged. This preserves learned representations for tasks unrelated to gender while still allowing sufficient capacity to adjust bias-related processing in the targeted block.

## Foundational Learning

- **Concept: Functional Modularity in Transformers**
  - Why needed here: LFTF relies on the premise that specific transformer blocks specialize in certain functions. Without understanding that different layers handle different levels of abstraction, the localization approach makes little sense.
  - Quick check question: If you froze all layers except layer 15 in a 32-layer model, would you expect the model to retain most of its factual knowledge? Why or why not?

- **Concept: Probability Distributions over Vocabulary**
  - Why needed here: Both the evaluation metrics (AFGB-Score, UB-Score) and the loss function operate directly on the model's predicted probabilities for specific tokens. Understanding how logits → softmax → probabilities is essential for interpreting results.
  - Quick check question: If a model outputs logits of [2.0, 1.0, 0.5] for tokens ["he", "she", "they"], what are the corresponding probabilities? What would the LFTF loss be for this example?

- **Concept: Fine-Tuning vs. Full Parameter Training**
  - Why needed here: The paper explicitly compares LFTF (single-block) against FPFT (full-parameter). Understanding the trade-offs in parameter update scope is critical for evaluating why partial fine-tuning preserves general capabilities.
  - Quick check question: Why might full-parameter fine-tuning on a specialized dataset cause "catastrophic forgetting" of pre-trained knowledge, while single-block fine-tuning might not?

## Architecture Onboarding

- **Component map:**
  LFTF Pipeline:
  ├── Stage 1: Locating
  │   ├── GenBiasEval training samples (profession-based prompts)
  │   ├── Forward pass through all blocks
  │   ├── BMI computation per block: 1 - cos_sim(H_i, H_{i+1})
  │   └── Block ranking by BMI (descending)
  ├── Stage 2: Fine-Tuning
  │   ├── Select top-ranked block
  │   ├── Compute loss: L = P("he"|p,M) + P("she"|p,M)
  │   └── Backprop to update only selected block's ATT + MLP
  └── Evaluation
      ├── AFGB-Score (bias reduction): |P(he) - P(she)| averaged
      ├── UB-Score (hint-following): weighted consistency
      └── General benchmarks (MMLU, GSM8K, etc.)

- **Critical path:**
  1. Dataset quality → BMI accuracy → Correct block identification → Effective debiasing
  2. Loss function design → Convergence behavior → Balance between bias reduction and capability preservation
  3. Block selection granularity → Parameter isolation → Trade-off between intervention strength and side effects

- **Design tradeoffs:**
  - Single block vs. multiple blocks: Paper targets only the highest-BMI block; could extend to top-k blocks for potentially stronger debiasing but with higher interference risk
  - Loss function complexity: Current loss is minimal (2 terms); could add regularization for capability preservation or use contrastive pairs, but increases computational cost and hyperparameter sensitivity
  - Template scope: GenBiasEval uses profession-based templates; may not generalize to other bias domains without dataset extension

- **Failure signatures:**
  - Anti-bias collapse: If loss is asymmetric, model flips to opposite bias (model editing methods produce AFGB-Score ~0.96)
  - Hint-following degradation: Over-aggressive debiasing causes model to ignore explicit gender hints in prompts
  - Mathematical reasoning decline: Fine-tuning final layer may disrupt abstract reasoning; GSM8K drops 13% for Qwen2.5-7B-LFTF
  - BMI instability: If BMI values have high variance across random samples, block identification becomes unreliable

- **First 3 experiments:**
  1. BMI validation on held-out prompts: Compute BMI using GenBiasEval training split, then verify that the identified top block also shows highest BMI on test split
  2. Ablation on block position: Instead of top-BMI block, fine-tune (a) random block, (b) first block, (c) middle block. Compare debiasing effectiveness and capability preservation
  3. Cross-bias transfer: Train LFTF on gender bias, then evaluate on race/religion bias datasets. If BMI identifies gender-specific blocks, cross-bias transfer should be limited

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the LFTF methodology and the GenBiasEval dataset be adapted to address non-binary gender biases?
- Basis in paper: The authors explicitly state in the Limitations section that their dataset assumes gender is binary ("male" and "female") based on previous work, despite acknowledging non-binary identities.
- Why unresolved: The current dataset construction and metric definitions (AFGB-Score) rely entirely on calculating probability differences between "he" and "she" tokens, excluding other pronouns or gender identities.
- What evidence would resolve it: A modified version of GenBiasEval including gender-neutral or non-binary pronouns, and an updated LFTF loss function that minimizes bias across these additional categories without causing model collapse.

### Open Question 2
- Question: Does the LFTF algorithm's targeting of the final transformer block disproportionately degrade mathematical reasoning capabilities compared to other general tasks?
- Basis in paper: While the paper claims general capabilities are maintained, Table 5 shows a significant performance drop (approx. 13%) on the GSM8K mathematical reasoning benchmark, far exceeding the negligible drops in QA tasks.
- Why unresolved: The authors note the decline but do not investigate why the specific "key block" identified by the BMI metric (often the last layer) appears to store critical knowledge for mathematical reasoning that is disrupted during debiasing.
- What evidence would resolve it: An ablation study analyzing the representation of arithmetic facts in the high-BMI blocks, or a modified LFTF approach that excludes mathematical tokens from the fine-tuning loss.

### Open Question 3
- Question: Can the BMI metric and LFTF fine-tuning strategy be effectively generalized to mitigate other social biases, such as race or religion?
- Basis in paper: Appendix A.3 explicitly discusses the "Scalability of this Paper," proposing that the method could be extended to other biases by modifying the loss function terms and recalculate BMI.
- Why unresolved: The authors propose the theoretical approach but do not provide experimental validation to prove that bias "hotspots" for gender overlap with or function similarly to hotspots for racial or religious bias.
- What evidence would resolve it: Experimental results applying LFTF to established religious or racial bias benchmarks, demonstrating that the same blocks can be fine-tuned to mitigate these biases without interfering with the gender debiasing already performed.

## Limitations
- Evaluation restricted to profession-related gender bias using synthetic templates, limiting generalization to real-world text
- Significant mathematical reasoning decline (GSM8K drops 13%) suggests late-layer functionality beyond gender bias may be disrupted
- BMI-based localization assumes gender bias manifests as distinct hidden-state transformations, which may not hold for all bias types or model architectures

## Confidence
- **High confidence:** The empirical effectiveness of LFTF in reducing AFGB-Score (0.26→0.08) while preserving general capabilities is well-supported by controlled experiments
- **Medium confidence:** The BMI-based localization mechanism is plausible and shows empirical correlation with bias concentration, but lacks deeper mechanistic validation
- **Low confidence:** Claims about the approach's generalizability to other bias types and real-world text applications remain speculative

## Next Checks
1. **Cross-bias generalization test:** Apply LFTF (trained on gender bias) to explicitly constructed race and religion bias templates, measuring whether BMI identifies consistent blocks across bias types or shows domain-specific localization.

2. **Neuron-level mechanistic validation:** Use attribution techniques (Integrated Gradients, neuron ablation) to verify that the fine-tuned block's neurons directly mediate the reduced gender bias, establishing causal rather than correlational evidence.

3. **Long-context bias stability:** Evaluate bias reduction on longer documents (multiple paragraphs) where gender associations may emerge through discourse-level patterns, testing whether template-level debiasing generalizes to contextual reasoning.