---
ver: rpa2
title: Offline Preference Optimization via Maximum Marginal Likelihood Estimation
arxiv_id: '2510.22881'
source_url: https://arxiv.org/abs/2510.22881
tags:
- preference
- mmpo
- objective
- optimization
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMPO, a new preference optimization method
  for aligning large language models with human preferences. MMPO maximizes the marginal
  log-likelihood of preferred text outputs using a numerically stable log-sum-exp
  formulation, implicitly performing preference optimization without requiring an
  explicit reward model or entropy maximization.
---

# Offline Preference Optimization via Maximum Marginal Likelihood Estimation

## Quick Facts
- arXiv ID: 2510.22881
- Source URL: https://arxiv.org/abs/2510.22881
- Reference count: 15
- Primary result: MMPO achieves superior stability with respect to hyperparameter β compared to DPO and SimPO while maintaining competitive preference alignment and better preserving general language capabilities

## Executive Summary
This paper introduces MMPO (Maximum Marginal Likelihood Optimization), a novel preference optimization method for aligning large language models with human preferences. Unlike traditional approaches like DPO that rely on explicit reward models and entropy regularization, MMPO maximizes the marginal log-likelihood of preferred text outputs using a numerically stable log-sum-exp formulation. This implicitly performs preference optimization without requiring an explicit reward model or entropy maximization terms. The method demonstrates superior stability with respect to the hyperparameter β across models from 135M to 8B parameters, achieving competitive or superior preference alignment (winning 62-68% of comparisons on AlpacaEval-2) while better preserving general language capabilities (maintaining 34-46% accuracy on LM Harness tasks).

## Method Summary
MMPO optimizes the approximate marginal log-likelihood of preferred responses using a log-sum-exp operation over scores of chosen and rejected responses. The method normalizes rewards within each mini-batch to ensure consistent scaling, then computes the loss using both log-sum-exp and an auxiliary DPO-style logsigmoid term. This approach implicitly weights gradient updates based on the difference between chosen and rejected response scores through a sigmoid function, avoiding the need for explicit reward modeling or entropy regularization. The implementation is provided in PyTorch and tested on combined preference datasets including Orca DPO Pairs and UltraFeedback-Binarized.

## Key Results
- MMPO demonstrates superior stability across varying β values compared to DPO and SimPO baselines
- Achieves competitive preference alignment with 62-68% win rates on AlpacaEval-2 length-controlled evaluations
- Better preserves general language capabilities, maintaining 34-46% accuracy on LM Harness reasoning tasks while improving preference alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The MMPO objective implicitly performs preference optimization by generating a weighted gradient that naturally increases the likelihood of preferred responses relative to rejected ones, without explicit reward modeling.
- **Mechanism:** The method maximizes the approximate marginal log-likelihood using a log-sum-exp operation over scores $s_w$ (preferred) and $s_l$ (rejected). As derived in Theorem 3.2, the resulting gradient is a sum of two components: one increasing the likelihood of the chosen response weighted by $\sigma(s_w - s_l)$, and one increasing the likelihood of the rejected response weighted by $\sigma(s_l - s_w)$. When $s_w > s_l$, the gradient pushes the model primarily toward the preferred response.
- **Core assumption:** The score of the preferred response is generally higher than the rejected response ($s_w \ge s_l$) in the optimization landscape.
- **Evidence anchors:**
  - [section] Theorem 3.2 proves the gradient decomposition: $\nabla_w \theta = \sigma(s_w - s_l)\nabla_\theta \log \pi_\theta(z_w|x)$.
  - [section] Section 3.2 argues that the sigmoid function modulates the gradient update based on the score difference.
  - [corpus] Related work (e.g., *MaPPO*) supports viewing alignment as statistical estimation, but does not validate this specific gradient decomposition.
- **Break condition:** If $s_l \ge s_w$ consistently (e.g., due to mislabeled data or initialization issues), the weighting flips, potentially training the model to prefer rejected responses.

### Mechanism 2
- **Claim:** In-batch normalization of scores stabilizes training and reduces sensitivity to the hyperparameter $\beta$ compared to DPO.
- **Mechanism:** MMPO normalizes the calculated rewards across the mini-batch before computing the log-sum-exp. This prevents outliers with extreme log-probabilities from dominating the gradient, keeping the relative scores $s_w$ and $s_l$ on a consistent scale. This mimics advantage normalization in RL but applies it offline.
- **Core assumption:** The statistics of a mini-batch provide a sufficient proxy for normalizing instance-level rewards to prevent gradient explosion.
- **Evidence anchors:**
  - [section] Section 3.3 states that in-batch normalization "ensures the relative scores... are on a consistent scale."
  - [section] Figure 1 and Section 4.2.1 empirically show MMPO trajectories are clustered (stable) across varying $\beta$, unlike DPO.
  - [corpus] No direct external validation of this specific normalization technique was found in the provided corpus.
- **Break condition:** If batch size is too small (e.g., 1-2 samples), normalization statistics become noisy or undefined, leading to unstable training.

### Mechanism 3
- **Claim:** The MML formulation better preserves the base model's general language capabilities (e.g., reasoning) compared to contrastive losses like DPO.
- **Mechanism:** Standard DPO forces a hard margin between chosen and rejected log-probabilities, which can distort the model's distribution for non-preference tasks. MMPO optimizes a marginal likelihood (soft selection) combined with a sigmoid-weighted gradient, which appears to apply less aggressive pressure on the policy distribution, thereby preserving the "priors" learned during pre-training/SFT.
- **Core assumption:** The log-sum-exp objective acts as a "softer" constraint on the policy update than the logistic loss used in DPO.
- **Evidence anchors:**
  - [abstract] MMPO "better preserves the base model's general language capabilities."
  - [section] Section 4.2.1 shows MMPO maintains higher LM Harness accuracy (reasoning) while improving preference win-rates.
  - [corpus] *Alignment as Distribution Learning* supports the view that strict RLHF objectives can lead to degenerate solutions, aligning with the need for softer constraints.
- **Break condition:** If the learning rate is set too high, the preference signal (even if softer) may still overwrite the pre-trained weights, causing catastrophic forgetting of reasoning skills.

## Foundational Learning

- **Concept:** Maximum Marginal Likelihood (MML) & Log-Sum-Exp
  - **Why needed here:** MMPO recasts alignment not as reinforcement learning or classification, but as maximizing the likelihood of a target given a latent (hidden) variable—the generated text. You need to understand how log-sum-exp serves as a smooth maximum to approximate the sum over all possible latent generations.
  - **Quick check question:** How does the gradient of log(exp(s_w) + exp(s_l)) differ from max(s_w, s_l)?

- **Concept:** Policy Gradient vs. Direct Preference Optimization (DPO)
  - **Why needed here:** The paper positions itself as a simplification of RLHF (Policy Gradient) and a modification of DPO. You must understand what MMPO removes (explicit reward model, explicit KL constraint/entropy term) to see why the "implicit" weighting is novel.
  - **Quick check question:** Does MMPO require a separate value network or a reference model in the loss term? (Check Section 3.3/Listing 1 for the role of ref_chosen_logps).

- **Concept:** Gradient Weighting (Sigmoid)
  - **Why needed here:** The core theoretical contribution is that MMPO gradients are scaled by σ(s_w - s_l). Understanding how this scaling factor behaves (e.g., approaching 1 when the chosen is clearly better) explains why the method is stable.
  - **Quick check question:** If the model is untrained and s_w ≈ s_l, what is the magnitude of the weighting factor, and how does this affect the initial gradient updates?

## Architecture Onboarding

- **Component map:**
  - Input (x, z_w, z_l) -> Policy Model (π_θ) + Reference Model (π_sft) -> Score Calculation -> Normalized Scores -> MMPO Loss (log-sum-exp + logsigmoid)

- **Critical path:**
  1. **Forward Pass:** Compute log-probs for (z_w, z_l) under both Policy and Reference models.
  2. **Normalization:** Concatenate rewards for the whole batch, gather global min/max, and normalize scores (Listing 1, lines 18-26).
  3. **Loss Compute:** Apply -logsumexp (MML) and -logsigmoid (Auxiliary) to the normalized scores.

- **Design tradeoffs:**
  - **Auxiliary Loss:** The paper adds a DPO-style term (-logsigmoid) alongside the logsumexp. Ablation shows removing it slightly improves win-rates but hurts reasoning; keeping it balances the trade-off (Figure 3).
  - **In-batch Normalization:** Crucial for stability but requires synchronization across GPUs (see accelerator.gather in Listing 1), which adds communication overhead.

- **Failure signatures:**
  - **Entropy Collapse:** If the model becomes deterministic, scores saturate.
  - **Beta Sensitivity:** While MMPO is robust, extremely high β (e.g., >1.0) without normalization might still destabilize training.
  - **Capability Loss:** If learning rate is too aggressive, the "preference" signal overwrites "reasoning" priors (see DPO degradation in Figure 1).

- **First 3 experiments:**
  1. **Gradient Verification:** Implement the loss in isolation (Listing 1) on a tiny batch. Manually check if the gradient for the chosen token is positive and scaled by the sigmoid of the score difference, as per Theorem 3.2.
  2. **Beta Sensitivity Sweep:** Train a 135M parameter model using DPO and MMPO across β ∈ [0.01, 0.1, 0.5]. Plot win-rate vs. LM Harness accuracy to verify MMPO's trajectory clustering (reproduce Figure 1).
  3. **Ablation on Normalization:** Disable the accelerator.gather and batch normalization lines (use unnormalized rewards). Compare training loss curves to see if instability or gradient explosion occurs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can MMPO be effectively adapted for online preference optimization scenarios to compete with established on-policy methods like PPO or GRPO?
- **Basis in paper:** [explicit] The Conclusion states that exploring the application of MMPO to online scenarios and comparing it with online methods "represents another promising avenue for future research."
- **Why unresolved:** The current paper focuses exclusively on offline preference optimization using static datasets and does not evaluate dynamic reward functions or on-policy sampling.
- **What evidence would resolve it:** An implementation of MMPO capable of handling dynamic rewards and a comparative study against PPO or GRPO on an online alignment benchmark.

### Open Question 2
- **Question:** Does the stability and capability preservation of MMPO persist when scaling to models significantly larger than 8B parameters using full-parameter training?
- **Basis in paper:** [explicit] The Limitations section notes that testing on models exceeding 1B parameters required memory-efficiency techniques like LoRA and 8-bit optimizers, and investigating full-parameter training on larger models "remains an interesting direction."
- **Why unresolved:** The authors lacked the GPU resources to test full-parameter updates on larger models, leaving the scalability of the method's stability properties unproven.
- **What evidence would resolve it:** Experiments training models >8B parameters (e.g., 70B) with full-precision optimizers, demonstrating consistent stability across β and preservation of reasoning capabilities.

### Open Question 3
- **Question:** Is the poor performance of entropy maximization in the ablation study due to the incompatibility of the term with MMPO, or simply the specific token-level approximation used?
- **Basis in paper:** [inferred] The paper notes in Section 5 that adding entropy maximization degraded performance, but explicitly caveats that this "does not prove that the full KL-regularization used in RLHF is irrelevant," only that the specific approximation failed.
- **Why unresolved:** The paper only tested one specific, approximated implementation of the entropy term; the interaction between the theoretical KL term and the MMPO objective remains unclear.
- **What evidence would resolve it:** An ablation study using a different approximation or exact calculation for the entropy/KL term to determine if the negative result is fundamental to the objective or an artifact of the approximation.

## Limitations

- The theoretical justification for implicit preference optimization depends on the assumption that preferred responses consistently receive higher scores than rejected ones, which may fail with noisy data or poor initialization.
- The in-batch normalization technique lacks rigorous comparison to established RL normalization methods and may fail with very small batch sizes.
- Capability preservation is demonstrated using LM Harness benchmarks, which may not fully capture nuanced reasoning capabilities or domain-specific knowledge that could be lost during alignment.

## Confidence

- **High Confidence:** MMPO's superior stability with respect to β compared to DPO and SimPO (supported by Figure 1 and Section 4.2.1)
- **Medium Confidence:** MMPO's implicit preference optimization mechanism (theoretically derived in Theorem 3.2 but dependent on score assumptions)
- **Medium Confidence:** Capability preservation benefits (empirically demonstrated but potentially limited by benchmark scope)
- **Low Confidence:** The necessity and optimality of the auxiliary DPO-style loss term (ablation shows mixed results)

## Next Checks

1. **Score Distribution Analysis:** Conduct a comprehensive analysis of score distributions (s_w vs s_l) across training epochs and different model sizes to verify that the assumption s_w ≥ s_l holds consistently. Identify failure modes when this assumption breaks and quantify their impact on alignment quality.

2. **Normalization Robustness Test:** Systematically evaluate MMPO's performance across varying batch sizes (1, 8, 32, 128) and dataset heterogeneity levels to determine the minimum batch size required for effective normalization and identify conditions where in-batch normalization fails.

3. **Capability Transfer Study:** Design targeted experiments to test whether MMPO preserves specific reasoning capabilities (e.g., mathematical problem-solving, logical deduction) that may not be captured by LM Harness benchmarks, comparing against DPO under identical conditions.