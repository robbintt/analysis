---
ver: rpa2
title: '"Sometimes You Need Facts, and Sometimes a Hug": Understanding Older Adults''
  Preferences for Explanations in LLM-Based Conversational AI Systems'
arxiv_id: '2510.06697'
source_url: https://arxiv.org/abs/2510.06697
tags:
- explanations
- older
- adults
- systems
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores older adults' preferences for AI explanations
  in Conversational AI systems, focusing on how these preferences vary by task context
  and information source. Through a Speed Dating study with 23 older adults, the research
  examines reactions to AI explanations grounded in conversational history, environmental
  data, activity-based inferences, and internal system logic across routine reminders
  and emergency alerts.
---

# "Sometimes You Need Facts, and Sometimes a Hug": Understanding Older Adults' Preferences for Explanations in LLM-Based Conversational AI Systems

## Quick Facts
- arXiv ID: 2510.06697
- Source URL: https://arxiv.org/abs/2510.06697
- Reference count: 40
- Older adults prefer context-sensitive AI explanations, with conversational, warm explanations for routine tasks and direct, concise explanations for emergencies

## Executive Summary
This study investigates how older adults prefer AI explanations in conversational AI systems, revealing that explanation preferences are highly context-dependent. Through a Speed Dating study with 23 older adults, researchers found that users favor different explanation styles based on task risk and emotional stateâ€”warm and conversational for routine reminders, but direct and concise for emergencies. The research highlights the importance of grounding explanations in observable data (environmental sensors, conversational history) rather than abstract system logic, and treating explanations as interactive, multi-turn exchanges rather than static outputs.

## Method Summary
The study employed a Speed Dating methodology with 23 older adults, presenting 10 scenarios across two contexts (Routine Reminders and Emergency Alerts) and four information source types (Conversational History, Environmental Data, Activity-Based Inferences, and Internal System Logic). GPT-4.0 was prompted to generate context-appropriate explanations for each scenario, which were then manually verified for consistency. Participants evaluated explanations based on clarity, trustworthiness, and personal relevance, with preferences analyzed across different contexts and information sources.

## Key Results
- Explanation preferences shift dramatically with task risk: conversational, warm explanations preferred for routine tasks, while direct, concise explanations favored for emergencies
- Explanations grounded in environmental data and conversational history were most valued for their clarity and trustworthiness, while internal system logic explanations were viewed as ambiguous
- Participants perceived explanations as multi-turn conversational exchanges rather than static outputs, desiring progressive disclosure of information

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If explanation tone and verbosity are dynamically matched to task risk and user emotional state, then user acceptance and perceived empathy are likely to improve.
- **Mechanism:** Older adults evaluate explanatory value based on situational risk. In low-risk "routine" contexts, cognitive load is lower, allowing for "warm," conversational elaboration that builds companionship. In high-risk "emergency" contexts, cognitive load spikes, creating a preference for direct, concise, task-focused explanations to reduce decision latency.
- **Core assumption:** The system can accurately classify the context (routine vs. emergency) and infer the user's current emotional state (e.g., stress levels) before generating the explanation.
- **Evidence anchors:**
  - [abstract] "explanation needs shift with task risk and emotional state: conversational, warm explanations were preferred for routine reminders, while direct, concise explanations were favored for emergencies."
  - [section 5.1.1] Participants emphasized that verbose explanations in emergencies could downplay urgency ("If she's too sweet to me when my kitchen may be on fire, I'd be pretty mad").
  - [corpus] Neighbor papers support the general need for conversational robustness but lack specific data on the "risk/tone" trade-off found here.
- **Break condition:** If the system misclassifies a high-risk event as low-risk and delivers a "chatty" explanation, or if it fails to detect high stress and remains overly clinical.

### Mechanism 2
- **Claim:** If explanations are grounded in observable environmental data (sensors) or prior conversational history rather than abstract internal logic (confidence scores), then perceived trustworthiness and actionability increase.
- **Mechanism:** Older adults distrust abstract metrics ("I am 92% confident") because they lack context. Explanations based on environmental data (e.g., "smoke detector triggered") provide tangible evidence ("proof"), while conversational history (e.g., "you mentioned feeling sore yesterday") provides personal relevance and continuity, reinforcing the AI's role as a memory aid.
- **Core assumption:** The system has reliable access to real-time sensor data and a persistent, queryable memory of past interactions.
- **Evidence anchors:**
  - [abstract] "Explanations grounded in real-time environmental data and prior conversational history were most valued for their clarity, trustworthiness, and personal relevance."
  - [section 5.2.3] Participants expressed skepticism toward confidence scores, viewing them as "ambiguous" and lacking reasoning ("What does that even mean in this situation?").
  - [corpus] Evidence regarding specific information sources (sensors vs. scores) is weak in the provided neighbor snippets, which focus more on general health/reliability.
- **Break condition:** If sensor data is noisy or unavailable, forcing the system to rely on "internal logic" explanations, or if referencing history feels intrusive rather than supportive.

### Mechanism 3
- **Claim:** If explanations are treated as the start of a multi-turn negotiation rather than a static output, they better support progressive disclosure and user agency.
- **Mechanism:** Users have varying needs for depth (layering). A static "one-size-fits-all" explanation often causes information overload or insufficiency. By treating the explanation as a conversational opening, users can "peel the layers" of detail as needed, reducing cognitive burden and increasing the perceived interactivity of the AI.
- **Core assumption:** The LLM maintains state across the conversation to allow follow-up questions and can modulate detail levels dynamically.
- **Evidence anchors:**
  - [abstract] "Participants perceived explanations as multi-turn conversational exchanges rather than static outputs... and desired progressive disclosure of information."
  - [section 5.3.2] Participants envisioned a "conversational flowchart" where they control the depth of the explanation ("You can always ask for more, but you don't want her to assume you want everything").
  - [corpus] General corpus support exists for conversational agents, but not specifically for the "progressive disclosure" interaction pattern in this context.
- **Break condition:** If the interaction model is "one-shot" (Q&A) rather than conversational, preventing the user from drilling down into the "why."

## Foundational Learning

### Concept: Contextual Risk & Tone Alignment
- **Why needed here:** Standard "helpful assistant" personas (polite, verbose) fail in emergencies. Engineers must learn to decouple *politeness* from *helpfulness* in high-stakes domains.
- **Quick check question:** Should the system say "I'm sorry to bother you, but..." before alerting a user to a fire? (Answer: No).

### Concept: Grounding vs. Internal Logic
- **Why needed here:** Standard XAI often relies on probability or confidence scores. This demographic rejects that in favor of *evidence*. You must learn to map "confidence" to "observable facts" (e.g., sensors or history).
- **Quick check question:** An AI is 95% sure a user fell. Should it say "I am 95% confident" or "I detected a sudden impact and no movement for 2 minutes"?

### Concept: Progressive Disclosure (Layering)
- **Why needed here:** Engineers often build "complete" explanations. This user group prefers the "headline" first, with the option to ask "why?" later.
- **Quick check question:** Does the explanation architecture allow for a follow-up "Tell me more" turn, or does it dump all context in the first sentence?

## Architecture Onboarding

- **Component map:** Event Trigger -> Risk Classification -> Data Retrieval -> Prompt Engineering -> Output Generation
- **Critical path:**
  1. Event Trigger (e.g., reminder time, sensor alert)
  2. Risk Classification (Emergency vs. Routine)
  3. Data Retrieval (Fetch specific sensor reading OR relevant chat history)
  4. Prompt Engineering (Inject data + Tone instruction: "Direct/Facts" OR "Warm/Empathetic")
  5. Output Generation
- **Design tradeoffs:**
  - **Latency vs. Context:** Fetching conversational history adds latency. In emergencies, skip history and rely on sensor data for speed.
  - **Privacy vs. Continuity:** Referencing past conversations ("You said last week...") builds trust but raises privacy stakes.
- **Failure signatures:**
  - **The "Clueless Robot":** Using confidence scores ("I am 85% sure") without context (Section 5.2.3)
  - **The "Chatty First Responder":** Using warm, elaborate language during a fire/health alert (Section 5.1.1)
  - **The "Dead End":** Providing an explanation that doesn't allow a follow-up question, blocking progressive disclosure
- **First 3 experiments:**
  1. **Tone Calibration Test:** A/B test "Direct" vs. "Warm" explanations for the same emergency scenario to measure user preference/panic levels (Hypothesis: Direct wins in emergencies)
  2. **Source Trust Test:** Compare user trust ratings for "Confidence Score" explanations vs. "Sensor-Based" explanations for a routine reminder (Hypothesis: Sensor-based wins)
  3. **Interaction Mode Test:** Test a "Full Explanation" output vs. a "Headline + Drill-down" interaction flow to measure perceived cognitive load

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do explanation requirements and preferences shift as older adults' cognitive conditions and routines evolve over time?
- **Basis in paper:** [Explicit] The authors state in the Limitations section that "explanatory requirements shift as older adults' routines, environments, and cognitive conditions evolve," and suggest that "designing explanation systems that adapt over time represents another promising direction for future work."
- **Why unresolved:** The current study utilized a cross-sectional Speed Dating methodology, capturing preferences at a single point in time rather than tracking changes longitudinally as participants' cognitive states progressed.
- **What evidence would resolve it:** A longitudinal deployment study tracking how interaction metrics and subjective preferences for explanation tone and depth change as a user's Mild Cognitive Impairment (MCI) progresses.

### Open Question 2
- **Question:** How do older adults perceive and interact with alternative explanation techniques, such as contrastive or counterfactual explanations, compared to narrative approaches?
- **Basis in paper:** [Explicit] The authors note that their study operationalized explanations through "speculative storyboard-based vignettes" and suggest future work should "investigate other explanatory methods and techniques (e.g., contrastive, counterfactuals, part-based, etc.)".
- **Why unresolved:** This study focused exclusively on explanations grounded in four specific information sources (history, environmental data, etc.) formatted as natural language narratives, leaving other XAI techniques unexplored.
- **What evidence would resolve it:** A comparative user study where older adults interact with systems providing counterfactual explanations (e.g., "If you had done X, Y would not have happened") versus the narrative explanations tested here.

### Open Question 3
- **Question:** Under what specific social conditions is it appropriate for AI systems to share explanations with caregivers versus keeping them private to the older adult?
- **Basis in paper:** [Explicit] The authors state in the Discussion that "questions around what is explained, and more importantly, when, to whom, and under what social conditions necessitate further inquiry," specifically regarding the tension between collaborative awareness and autonomy.
- **Why unresolved:** While the study found that caregivers view explanations as "windows" into their loved one's life, it did not map the boundaries of when older adults feel this sharing is supportive versus intrusive.
- **What evidence would resolve it:** Empirical data from dyadic studies (older adult and caregiver) identifying specific "no-go zones" or contextual triggers where explanation sharing negatively impacts the older adult's sense of independence.

## Limitations
- Small sample size (n=23) and self-selected participants may not represent broader older adult populations
- Study relies on hypothetical scenarios rather than real-world behavioral observations
- Limited external validity to older adults with varying cognitive abilities and technological literacy

## Confidence

- **High confidence**: The finding that tone preferences shift dramatically between routine and emergency contexts (Section 5.1.1). This was consistently observed across participants and scenarios.
- **Medium confidence**: The preference for environmental/sensor data over internal system logic explanations. While clearly stated, this finding has less corpus support and depends on the specific implementation of confidence scores.
- **Medium confidence**: The multi-turn explanation model. While participants expressed this preference, it's unclear how this would function in practice with actual users versus hypothetical scenarios.

## Next Checks

1. **Real-world behavioral validation**: Deploy the explanation system in actual smart home environments with older adults over extended periods, measuring not just stated preferences but actual response times and task completion rates across different explanation types.

2. **Cognitive ability stratification**: Repeat the study with participants stratified by cognitive assessment scores to determine whether explanation preferences vary with cognitive capacity, particularly for the multi-turn progressive disclosure model.

3. **Emergency simulation test**: Conduct controlled emergency simulations (smoke alarm, fall detection) with different explanation types to measure actual stress responses and decision quality, rather than relying on hypothetical scenario responses.