---
ver: rpa2
title: 'AI-Driven Generation of Old English: A Framework for Low-Resource Languages'
arxiv_id: '2507.20111'
source_url: https://arxiv.org/abs/2507.20111
tags:
- english
- translation
- data
- texts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a framework for generating high-quality Old
  English texts using large language models, addressing the challenge of low-resource
  languages. The approach combines parameter-efficient fine-tuning, backtranslation,
  and a dual-agent pipeline for content generation and translation.
---

# AI-Driven Generation of Old English: A Framework for Low-Resource Languages

## Quick Facts
- arXiv ID: 2507.20111
- Source URL: https://arxiv.org/abs/2507.20111
- Reference count: 19
- Key outcome: English-to-Old English translation BLEU scores increase from 26 to over 65 using LoRA fine-tuning, backtranslation, and dual-agent pipeline

## Executive Summary
This study presents a framework for generating high-quality Old English texts using large language models, addressing the challenge of low-resource languages. The approach combines parameter-efficient fine-tuning, backtranslation, and a dual-agent pipeline for content generation and translation. Evaluation using automated metrics (BLEU, METEOR, CHRF) and expert human assessment demonstrates significant improvements over baseline models, with BLEU scores increasing from 26 to over 65 for English-to-Old English translation. The generated texts show high grammatical accuracy and stylistic fidelity, providing a scalable solution for expanding the Old English corpus and offering a blueprint for revitalizing other endangered languages through AI-driven synthetic data generation.

## Method Summary
The framework employs a two-phase training approach using Llama-8b as the base model. Phase 1 involves domain adaptation with LoRA on four auxiliary tasks (text completion, forward/back translation, crossed definition) to establish Old English linguistic foundations. Phase 2 uses backtranslation to generate synthetic parallel data, which is combined with authentic parallel corpora for task specialization. A dual-agent pipeline then generates new content: GPT-4o-mini creates Modern English prompts guided by authentic Old English samples, while the fine-tuned model translates these into Old English with few-shot context anchoring.

## Key Results
- BLEU scores increase from 26 (baseline) to over 65 after full pipeline implementation
- Expert linguistic evaluation shows average scores of 9.0+ for inflection, word order, and lexical choice
- Backtranslation provides ~6 point improvement across all metrics for direct translation
- Semantic coherence remains the lowest-scored criterion at 7.8/10

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task-similar continual pretraining with LoRA enables efficient domain transfer from Modern English to Old English with limited data.
- **Mechanism:** Four auxiliary tasks (text completion, forward/back translation, crossed definition) leverage the model's existing English proficiency to scaffold Old English acquisition, avoiding full-parameter retraining.
- **Core assumption:** Linguistic similarity between Modern and Old English provides transferable representations that survive low-rank adaptation.
- **Evidence anchors:**
  - [abstract] "combines parameter-efficient fine-tuning (Low-Rank Adaptation, LoRA)... with BLEU scores increasing from 26 to over 65"
  - [section: Model Training] "OldEnglishBase... demonstrated a significant improvement over the baseline model... effectively doubling them"
  - [corpus] Limited corpus support for this specific combination; neighbor papers address low-resource fine-tuning broadly (e.g., "Fine Tuning Methods for Low-resource Languages") but not the four-task formulation.
- **Break condition:** If target language lacks lexical/structural overlap with pretraining dominant language, task similarity assumption fails.

### Mechanism 2
- **Claim:** Backtranslation-generated synthetic parallel data breaks performance stagnation from limited authentic corpora.
- **Mechanism:** The English-centric base model produces reliable Modern English translations from Old English input; these reverse translations create synthetic parallel pairs that expand training diversity without manual annotation.
- **Core assumption:** Reverse translation quality exceeds forward translation quality when the model is English-dominant.
- **Evidence anchors:**
  - [abstract] "data augmentation via backtranslation"
  - [section: Phase 2] "Backtranslation works by taking monolingual Old English text... improvements of approximately 6 points across all metrics for direct translation"
  - [section: Results] "The refinement of this model through the technique of backtranslation proved to be key in overcoming the stagnation"
  - [corpus] Backtranslation is well-established in NMT literature; neighbor paper "MALT" addresses translation quality issues in low-resource Urdu but does not validate this specific application.
- **Break condition:** If synthetic pairs contain systematic errors, they propagate into model outputs (semantic coherence remained lowest-scored criterion at 7.8/10).

### Mechanism 3
- **Claim:** Separating content generation (Modern English) from translation (Old English) across specialized agents improves output diversity and stylistic fidelity.
- **Mechanism:** FragmentGen uses GPT-4o-mini to generate diverse Modern English prompts guided by authentic Old English samples; OldEnglishTranslator (the fine-tuned model) translates with few-shot context from the same samples, ensuring lexical and stylistic alignment.
- **Core assumption:** Context-anchored generation reduces out-of-vocabulary content and domain drift.
- **Evidence anchors:**
  - [abstract] "dual-agent pipeline that separates the tasks of content generation (in English) and translation (into Old English)"
  - [section: Synthetic Data Generation] "Randomly sampled fragments from the reference corpus are provided as contextual anchors... ensuring that the generated sentences remain stylistically consistent"
  - [section: Expert Linguistic Evaluation] Average scores of 9.0+ for inflection, word order, and lexical choice
  - [corpus] "Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent AI Systems" supports multi-agent translation architectures but does not validate this specific dual-agent design.
- **Break condition:** If sampled fragments lack genre diversity, generated content will reflect narrow stylistic range.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The paper uses LoRA to adapt an 8B-parameter model to Old English without full fine-tuning costs. Understanding rank selection and target modules is essential for reproducing results.
  - **Quick check question:** Can you explain why LoRA enables domain adaptation with fewer trainable parameters than full fine-tuning?

- **Concept: Backtranslation in Neural Machine Translation**
  - **Why needed here:** This mechanism drives the key performance gains from OldEnglishBase to OldEnglishRefined. Understanding its strengths and failure modes is critical.
  - **Quick check question:** Why does backtranslation help low-resource translation, and what quality risks does synthetic data introduce?

- **Concept: BLEU, CHRF, METEOR Metrics**
  - **Why needed here:** The paper reports score increases from 26 to 65+ BLEU. Interpreting these metrics and their limitations is necessary for evaluating claimed improvements.
  - **Quick check question:** What does a BLEU score of 65 indicate about n-gram overlap, and what aspects of translation quality does it not capture?

## Architecture Onboarding

- **Component map:**
  Llama-8b (base) → LoRA + 4-task domain adaptation → OldEnglishBase (vocabulary/grammar foundation) → Backtranslation + synthetic parallel data → OldEnglishRefined (translation-specialized) → Dual-agent pipeline: FragmentGen (GPT-4o-mini) → Modern English prompts → OldEnglishTranslator → Old English outputs

- **Critical path:** Domain adaptation → Backtranslation augmentation → Task-specialized fine-tuning → Dual-agent generation. Skipping domain adaptation yields baseline-level performance; skipping backtranslation causes stagnation after 1-3 epochs.

- **Design tradeoffs:**
  - LoRA efficiency vs. full fine-tuning expressiveness
  - Synthetic data scale vs. authenticity/fidelity risks
  - Greedy decoding reproducibility vs. output diversity
  - English-centric scaffolding vs. target-language purity

- **Failure signatures:**
  - Looped generation (repetitive phrases in outputs)
  - Non-translated segments (English text passes through unchanged)
  - Vocabulary hallucination (unattested word forms)
  - Co-reference tracking loss in complex syntax
  - Anachronistic concept retrofitting (modern terms in historical language)

- **First 3 experiments:**
  1. Establish baseline by evaluating raw Llama-8b on English→Old English translation using the DOEC test split; confirm reported ~26 BLEU and identify error patterns.
  2. Implement Phase 1 domain adaptation with the four-task formulation; validate that OldEnglishBase achieves ~60 BLEU before overfitting emerges (watch epoch 3-5 degradation).
  3. Generate synthetic parallel data via backtranslation on held-out Old English monolingual text; fine-tune to OldEnglishRefined and measure the ~6-point BLEU gain over OldEnglishBase.

## Open Questions the Paper Calls Out
None

## Limitations
- Incomplete technical specification with unspecified LoRA hyperparameters (rank, alpha, dropout, target modules) and learning rates
- Missing prompt templates for four auxiliary tasks and few-shot examples for dual-agent pipeline
- Undefined filtering criteria for removing low-quality synthetic samples from backtranslation pipeline
- All neighboring papers have zero citations, suggesting underdeveloped research area

## Confidence
**High Confidence (8-10/10):** Dual-agent pipeline architecture and overall framework design are clearly described and internally consistent. Performance gains from Phase 1 to Phase 2 are well-documented with specific BLEU improvements. Backtranslation use is grounded in established NMT literature.

**Medium Confidence (5-7/10):** Task-similar continual pretraining mechanism with LoRA is theoretically sound but lacks detailed validation. Specific contribution of each auxiliary task to overall improvement is not isolated or quantified.

**Low Confidence (2-4/10):** Reproducibility of exact implementation details is uncertain due to unspecified configuration parameters. Claim about linguistic similarity providing transferable representations is plausible but not empirically validated for this specific task combination.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary LoRA rank (8, 16, 32), learning rates (1e-4, 2e-4, 5e-4), and training epochs (1-5) to establish optimal configuration ranges and identify overfitting thresholds.

2. **Auxiliary Task Contribution Isolation:** Conduct ablation studies by removing each of the four auxiliary tasks individually during Phase 1 training to quantify their specific contributions to the final performance improvement.

3. **Synthetic Data Quality Assessment:** Implement comprehensive filtering criteria for backtranslation outputs (repetition detection, semantic coherence checks, domain consistency verification) and measure the impact on downstream translation quality.