---
ver: rpa2
title: Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation
arxiv_id: '2506.11380'
source_url: https://arxiv.org/abs/2506.11380
tags:
- visual
- plan
- step
- textual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal plan generation, where both textual
  and visual steps must be coherent and consistent. The authors propose an autoregressive
  framework that iteratively drafts text, generates images conditioned on the previous
  visual step, extracts structured visual information, and refines the text.
---

# Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation

## Quick Facts
- arXiv ID: 2506.11380
- Source URL: https://arxiv.org/abs/2506.11380
- Authors: Xiaoxin Lu; Ranran Haoran Zhang; Yusen Zhang; Rui Zhang
- Reference count: 32
- Key outcome: Autoregressive framework with iterative text drafting, image conditioning, visual extraction, and refinement improves multimodal plan quality across 1,100 tasks, showing gains in visual coherence (PPL ↓ 5.21→6.03), text-image alignment (CLIP ↑ 20.23→27.14), and human evaluation (77% win rate visual coherence, 81% win rate text-image alignment)

## Executive Summary
This paper addresses multimodal plan generation where both textual and visual steps must be coherent and consistent. The authors propose an autoregressive framework that iteratively drafts text, generates images conditioned on the previous visual step, extracts structured visual information, and refines the text. Experiments across 11 domains show their approach significantly improves visual coherence, text-image alignment, and overall plan quality compared to strong baselines.

## Method Summary
The framework generates multimodal plans through a 4-stage autoregressive loop per step: (1) LLM drafts step from goal plus history, (2) InstructPix2Pix generates image conditioned on draft plus previous image, (3) MLLM extracts pPDDL-structured visual information (objects, tools, action, goal), (4) LLM refines draft using extracted pPDDL. The approach was evaluated on 1,100 tasks from Instructables and wikiHow using three backbones (Mistral-7B, Gemini-1.5-flash, GPT-4o), showing consistent improvements in visual coherence (PPL), text-image alignment (CLIP), and human evaluation metrics.

## Key Results
- Visual coherence improved with PPL dropping from 6.03 to 5.21
- Text-image alignment improved with CLIP score increasing from 20.23 to 27.14
- Human evaluation showed 77% win rate for visual coherence and 81% win rate for text-image alignment
- Consistent performance gains across all three backbones (Mistral-7B, Gemini-1.5-flash, GPT-4o)

## Why This Works (Mechanism)

### Mechanism 1: Iterative Cross-Modal Feedback Loop
Alternating between text drafting and image-based refinement creates bidirectional consistency between modalities. The framework generates a text draft, produces an image conditioned on it, extracts structured information from that image, then refines the text based on extracted visual information. This creates a self-correcting loop where visual generation exposes inconsistencies in text, and text refinement grounds subsequent visual generation. Core assumption: extracted visual information accurately captures semantically relevant elements without significant information loss.

### Mechanism 2: Image-to-Image Conditioning for Visual Coherence
InstructPix2Pix generates each new image conditioned on both the text instruction AND the previous image. This preserves visual elements (object identity, scene context) across steps, as the model must explain the visual difference between frames rather than synthesize independently. Core assumption: the image editing model can accurately depict action-induced state transformations while maintaining object identity and scene consistency.

### Mechanism 3: Structured Visual Information Extraction (pPDDL)
Using a structured PDDL-like format (Objects, Tools, Action, Goal) for visual information extraction outperforms natural language descriptions by reducing noise and focusing refinement on actionable elements. Rather than generating verbose, unstructured captions, the structured format constrains extraction to four task-relevant categories. Core assumption: the four-category schema captures sufficient information for text refinement without losing critical semantic content.

## Foundational Learning

- **Concept: Autoregressive generation with cross-modal state**
  - Why needed here: The framework generates plans step-by-step where each step depends on accumulated textual AND visual history
  - Quick check question: If you remove the visual history from the input to step k, what specific failure mode would you expect?

- **Concept: Diffusion model conditioning mechanisms**
  - Why needed here: InstructPix2Pix conditions on both text AND reference image simultaneously
  - Quick check question: What happens to visual coherence if you replace InstructPix2Pix with standard Stable Diffusion (text-to-image only)?

- **Concept: Cross-modal alignment evaluation (CLIP, perplexity)**
  - Why needed here: The paper uses CLIP score for text-image alignment and perplexity for visual coherence
  - Quick check question: Why might a plan score well on CLIP but poorly on human evaluation for text-image alignment?

## Architecture Onboarding

- **Component map**: Goal (G) → [LLM: Draft Generator] → draft (d_k) → [InstructPix2Pix] → image (i_k) → [MLLM: Visual Extractor] → pPDDL (v_k) → [LLM: Text Refiner] → text (t_k)
- **Critical path**: Fine-tune InstructPix2Pix on wikiHow triplets (one-time, ~4 hours on 4x A100s) → For each plan step: Draft → Image → Extract → Refine → Text refinement quality depends entirely on extraction accuracy → Image coherence depends entirely on InstructPix2Pix conditioning
- **Design tradeoffs**: pPDDL vs. natural language descriptions (structured format reduces noise but may lose nuanced visual information), InstructPix2Pix vs. text-to-image (image conditioning maintains coherence but fails on major scene transitions), single-shot vs. iterative (iterative allows refinement but increases latency)
- **Failure signatures**: Visual coherence breakdown (container/object suddenly changes appearance → InstructPix2Pix conditioning failed), Text-image misalignment (image shows element not mentioned in text → extraction missed element or refinement ignored extraction), Compounding errors (plan quality degrades after step 3-4 → check extraction quality)
- **First 3 experiments**: 1) Run ablation removing the text refinement stage entirely to quantify refinement contribution, 2) Replace pPDDL with simpler 2-field format (Object, Action) on 20 tasks to test format optimality, 3) Select 15 tasks requiring scene transitions and measure InstructPix2Pix failure rates

## Open Questions the Paper Calls Out

### Open Question 1
How can visual coherence be evaluated using metrics that operate directly in the image space rather than relying on indirect text-based measurements? The authors state their text-based metric "may not fully capture the nuanced visual relationships" and explicitly list developing visual coherence metrics that function in the image space as future work.

### Open Question 2
Can image editing models be developed to successfully maintain coherence during significant workspace changes or scene transitions? The authors note their fine-tuned InstructPix2Pix model fails "when the textual instructions indicate significant workspace change" and suggest exploring image editing models that better fit the planning context.

### Open Question 3
How can the framework be adapted to effectively generate plans for high-complexity tasks (delicate spatial actions) and low-complexity abstract tasks? Section 6.6 demonstrates the framework performs best on "medium" complexity tasks, while "depicting abstract or delicate actions still exceeds the capacity of current models."

## Limitations
- Visual coherence breaks on large scene/workspace changes (acknowledged limitation of InstructPix2Pix)
- Framework performs best on medium complexity tasks, struggling with high-complexity spatial actions and low-complexity abstract tasks
- Evaluation relies on text-based metrics (CLIP, PPL) that may not fully capture nuanced visual relationships

## Confidence

- **High confidence**: Iterative refinement mechanism demonstrably improves multimodal consistency (supported by ablation W_DES showing 10-15 point CLIP drop without structured extraction)
- **Medium confidence**: Specific pPDDL format is optimal for extraction (only compared to natural language descriptions, not simpler structured alternatives)
- **Medium confidence**: InstructPix2Pix fine-tuning on wikiHow data significantly improves performance (no ablation comparing fine-tuned vs. pre-trained InstructPix2Pix)

## Next Checks

1. Run ablation removing the text refinement stage entirely to quantify its specific contribution to text-image alignment beyond the autoregressive structure
2. Replace pPDDL with a simpler 2-field structured format (Object, Action) on 20 tasks to test whether the 4-field schema is over-specified
3. Select 15 tasks requiring major scene transitions (e.g., "move from kitchen to garden") and measure InstructPix2Pix failure rates to characterize the acknowledged limitation's severity