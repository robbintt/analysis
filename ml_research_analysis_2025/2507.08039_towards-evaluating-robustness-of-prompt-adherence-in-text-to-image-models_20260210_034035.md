---
ver: rpa2
title: Towards Evaluating Robustness of Prompt Adherence in Text to Image Models
arxiv_id: '2507.08039'
source_url: https://arxiv.org/abs/2507.08039
tags:
- images
- image
- diffusion
- stable
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel evaluation framework for assessing
  prompt adherence in text-to-image models. The authors create a new 256x256 resolution
  dataset inspired by dSprites, containing simple geometric shapes (square, circle,
  triangle) with controlled factors of variation.
---

# Towards Evaluating Robustness of Prompt Adherence in Text to Image Models

## Quick Facts
- arXiv ID: 2507.08039
- Source URL: https://arxiv.org/abs/2507.08039
- Reference count: 40
- Primary result: Text-to-image models struggle with spatial positioning of simple geometric shapes despite high guidance scales

## Executive Summary
This paper introduces a novel evaluation framework for assessing prompt adherence in text-to-image models. The authors create a controlled 256x256 dataset with simple geometric shapes (square, circle, triangle) and controlled positioning factors. They employ a pipeline combining GPT-4o for image description generation and trained VAEs for reconstruction analysis to evaluate how well text-to-image models follow input prompts. The evaluation reveals that while models can generate correct shapes, they struggle significantly with spatial positioning, showing F1-scores below 0.5 for quadrant prediction across all tested models and resolutions.

## Method Summary
The authors create a 9,216-image test dataset inspired by dSprites, containing binary shapes (square, circle, triangle) at three scales with controlled positions. They train β-TCVAE models on the full dataset to learn the latent structure. The evaluation pipeline uses GPT-4o to generate text descriptions of ground truth images, then passes these descriptions through text-to-image models (Stable Diffusion 3 variants and Janus Pro models) to generate new images. These generated images are again described by GPT-4o, and the system compares the variation between initial and final descriptions using F1-scores for shape and quadrant prediction, along with VAE reconstruction loss to measure distributional drift.

## Key Results
- Models can generate correct shapes but fail significantly on spatial positioning tasks
- F1-scores for quadrant prediction remain below 0.5 across all models and resolutions
- Best performance: 0.41 F1 for Stable Diffusion and 0.5 for Janus Pro on quadrant prediction
- Shape prediction F1-scores degrade notably at lower resolutions (256x256), dropping to around 50% for Stable Diffusion models
- VAE reconstruction loss confirms generated images deviate from ground truth distribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The evaluation pipeline exposes prompt adherence degradation by measuring consistency between image-to-text and text-to-image transformations across multiple iterations.
- **Mechanism:** The method iterates: (1) Ground truth image → GPT-4o description → Text-to-Image model → generated image → GPT-4o description. By comparing first and final descriptions (shape and quadrant predictions via F1-score) and measuring VAE reconstruction loss, it quantifies information loss and faithfulness to factors of variation.
- **Core assumption:** GPT-4o provides accurate and consistent text representation of visual content, acting as reliable proxy for "what should be in the image."
- **Break condition:** The mechanism fails if GPT-4o is unreliable or inconsistent in describing simple binary images, introducing its own error into F1-scores independent of T2I model's performance.

### Mechanism 2
- **Claim:** Pre-trained VAEs serve as distributional probe to measure how far generated images have drifted from ground truth data manifold.
- **Mechanism:** The paper trains VAEs on full ground-truth dataset of binary shapes. When generated image passes through VAE encoder and decoder, reconstruction loss (MSE) is calculated. High reconstruction loss indicates image is "out-of-distribution"—it doesn't conform to simple, low-dimensional latent structure of ground truth images.
- **Core assumption:** VAEs have successfully learned low-dimensional latent structure of binary shapes dataset, and low reconstruction loss is reliable indicator that image belongs to this target distribution.
- **Break condition:** The mechanism fails if VAEs are not trained adequately or their latent space is not well-disentangled, making reconstruction loss noisy or misleading proxy for adherence to intended factors of variation.

### Mechanism 3
- **Claim:** Choice of minimal, low-dimensional dataset isolates failures in core prompt adherence from failures in handling high-dimensional complexity or noise.
- **Mechanism:** By using dataset with only two controlled factors of variation (geometric shape, spatial quadrant) on black background, authors remove confounding variables like complex textures, lighting, or ambiguous concepts. This creates "controlled experiment" where model's inability to follow prompt can be directly attributed to failure in mapping simple linguistic concepts to visual primitives and spatial relations.
- **Core assumption:** Prompt adherence failures on this simple dataset will generalize to or indicate fundamental weaknesses in more complex scenarios. Simplification does not remove core challenges of T2I alignment.
- **Break condition:** Mechanism's conclusions may not generalize if models' poor performance is specifically due to stark, binary nature of images, which may be out-of-distribution task compared to natural images they were trained on.

## Foundational Learning

- **Concept: Rectified Flow (in Stable Diffusion 3)**
  - **Why needed here:** The paper evaluates Stable Diffusion 3 variants, which use Rectified Flow instead of standard DDPM. Understanding this architecture is key to interpreting their performance and potential architectural reasons for observed failures.
  - **Quick check question:** What is the core difference in the diffusion process between a standard Denoising Diffusion Probabilistic Model (DDPM) and a Rectified Flow model?

- **Concept: Guidance Scale in Diffusion Models**
  - **Why needed here:** The paper explicitly uses high guidance scale (9.0) to enforce strict prompt adherence. This concept is central to understanding control variable and why reported failures are particularly significant.
  - **Quick check question:** How does increasing classifier-free guidance scale affect trade-off between prompt adherence and image diversity/quality in a diffusion model?

- **Concept: Disentanglement in VAEs**
  - **Why needed here:** The paper uses β-TCVAE and other VAE variants, trained on shapes dataset. Goal is to have latent space where each dimension corresponds to single factor of variation (e.g., shape, position).
  - **Quick check question:** In context of VAEs, what does "disentanglement" mean, and how does β-TCVAE's Total Correlation loss term promote it?

## Architecture Onboarding

- **Component Map:**
  Ground Truth Dataset -> GPT-4o Description Extraction -> Text-to-Image Model Generation -> GPT-4o Description Analysis -> F1-Score and VAE Loss Calculation

- **Critical Path:** The most critical path for reproducing core result is F1-score degradation loop.
  1. Sample image from test dataset
  2. Pass to GPT-4o API with specified system prompt to get initial description and shape/quadrant prediction
  3. Use generated description as prompt for Stable Diffusion 3 model (with guidance_scale=9.0)
  4. Pass generated image back to GPT-4o to get new predictions
  5. Calculate F1-score by comparing initial vs. final predictions for shape and quadrant across dataset

- **Design Tradeoffs:**
  - Using GPT-4o vs. smaller, local VLM: Authors chose GPT-4o for state-of-the-art capabilities, ensuring evaluation isn't limited by weak evaluator. This adds API cost and latency.
  - Simple binary dataset vs. complex natural images: Authors chose minimal dataset to isolate factors of variation and enable VAE-based analysis. This sacrifices ecological validity for experimental control and diagnostic clarity.
  - F1-Score vs. CLIPScore: F1-score on extracted attributes (shape, quadrant) provides more direct, interpretable measure of adherence to specific factors of variation than global embedding distance like CLIPScore.

- **Failure Signatures:**
  - High F1-Score Degradation: Drop in F1-score from ~1.0 (ground truth) to ~0.6 or lower across iterations. This indicates T2I model fails to reliably generate specified shape in specified quadrant.
  - High VAE Reconstruction Loss on Generated Images: Significant increase in loss compared to ground truth images. This indicates generated images are out-of-distribution, containing artifacts, incorrect shapes, or backgrounds that deviate from simple binary standard.
  - Blank Image Generation (SD3 Medium): This paper notes SD3 Medium sometimes generates blank images, which artificially lowers reconstruction loss and is critical failure mode to watch for.

- **First 3 Experiments:**
  1. Reproduce core F1-Score Degradation Result: Implement loop described in Critical Path for single model (e.g., Stable Diffusion 3 Medium) on small subset (e.g., 100 images) of test dataset. Verify F1-score drop.
  2. Validate VAE Fidelity: Train β-TCVAE on full shapes dataset and perform latent space traversal to qualitatively confirm disentanglement. Measure reconstruction loss on held-out test set from original dataset to establish baseline.
  3. Ablate on Guidance Scale: Repeat F1-Score experiment with different guidance scales (e.g., 1.0, 4.0, 7.5, 9.0) to empirically verify paper's claim that higher guidance does not resolve fundamental prompt adherence issues on this dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Which specific architectural components in Text-to-Image models are primary bottlenecks for spatial adherence and geometric prompt following?
- **Basis in paper:** [explicit] Conclusion states need for "deeper analysis of internal architecture of Text-to-Image models to see which areas are impacting these aspects of prompt adherence the most."
- **Why unresolved:** Study evaluates models as black boxes using output metrics (F1-score, reconstruction loss) but does not analyze internal mechanisms, such as cross-attention layers or positional encodings, that fail to bind location data to image.
- **What evidence would resolve it:** Ablation studies on attention mechanisms of models like MMDiT or Janus to identify which layers degrade spatial information during denoising or autoregressive generation process.

### Open Question 2
- **Question:** Does superior performance of Janus Pro over Stable Diffusion on this dataset stem from its autoregressive architecture or differences in training data?
- **Basis in paper:** [explicit] Authors observe that "Janus Pro models beat Stable Diffusion 3 variants," but note that "actual reason for this requires further investigation into architectures of the models."
- **Why unresolved:** Paper compares final image outputs but does not isolate cause of performance gap, leaving uncertainty about whether unified autoregressive approach is inherently better at spatial reasoning than rectified flow.
- **What evidence would resolve it:** Comparative analysis of attention maps or intermediate latent representations between architectures when processing identical spatial prompts.

### Open Question 3
- **Question:** Are instances where lighter models outperform heavier models in quadrant prediction statistically significant anomalies or consistent behaviors requiring architectural explanation?
- **Basis in paper:** [explicit] Results section notes that lighter models sometimes beat heavier ones, prompting authors to ask if "these patterns... have any concrete reasoning behind or they are just chance based for this particular dataset."
- **Why unresolved:** Evaluation relies on single test subset and aggregate F1-scores without statistical significance testing across multiple seeds or variations to confirm robustness of this counter-intuitive finding.
- **What evidence would resolve it:** Repeating evaluation across multiple random seeds and larger sample size to determine if performance inversion persists and warrants theoretical explanation.

## Limitations
- Evaluation framework heavily relies on GPT-4o's accuracy in describing simple binary images, introducing potential noise into F1-score metrics
- Dataset uses stark binary shapes that may be out-of-distribution for models trained on natural images, potentially underestimating real-world prompt adherence capabilities
- VAE reconstruction loss interpretation assumes well-disentangled latent spaces, but paper does not provide extensive validation of this assumption

## Confidence
- High confidence: The methodology for isolating spatial positioning failures using controlled factors of variation is sound and the results are reproducible
- Medium confidence: The GPT-4o-based description generation introduces uncertainty in F1-score measurements, though consistent pattern across models suggests reliability
- Low confidence: The generalizability of these specific shape and positioning failures to more complex, real-world prompt adherence scenarios

## Next Checks
1. Validate GPT-4o reliability: Manually verify a random sample of 100 image descriptions from GPT-4o to ensure shape and quadrant predictions are accurate and consistent with ground truth
2. Test on natural image dataset: Replicate evaluation pipeline on more complex dataset (e.g., COCO images) to determine if spatial positioning failures persist beyond binary shapes
3. Ablate guidance scale: Systematically test F1-score degradation at multiple guidance scales (1.0, 4.0, 7.5, 9.0) to confirm that higher guidance does not resolve fundamental prompt adherence issues