---
ver: rpa2
title: Adaptive Multi-Fidelity Reinforcement Learning for Variance Reduction in Engineering
  Design Optimization
arxiv_id: '2503.18229'
source_url: https://arxiv.org/abs/2503.18229
tags:
- fidelity
- design
- learning
- variance
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an adaptive multi-fidelity reinforcement
  learning framework to reduce variance in engineering design optimization by dynamically
  selecting and integrating heterogeneous low-fidelity models based on alignment with
  a high-fidelity policy. Unlike traditional hierarchical methods, the framework employs
  cosine similarity to measure policy alignment, ensuring correlated sampling and
  targeted learning that minimizes variance.
---

# Adaptive Multi-Fidelity Reinforcement Learning for Variance Reduction in Engineering Design Optimization

## Quick Facts
- arXiv ID: 2503.18229
- Source URL: https://arxiv.org/abs/2503.18229
- Reference count: 40
- One-line primary result: Adaptive multi-fidelity RL framework reduces variance in engineering design optimization by dynamically selecting low-fidelity models based on alignment with high-fidelity policy.

## Executive Summary
This paper introduces an adaptive multi-fidelity reinforcement learning framework that reduces variance in engineering design optimization by dynamically selecting and integrating heterogeneous low-fidelity models based on their alignment with a high-fidelity policy. Unlike traditional hierarchical methods, the framework employs cosine similarity to measure policy alignment, ensuring correlated sampling and targeted learning that minimizes variance. In an octocopter design case study, the adaptive approach demonstrated significantly lower variance and more consistent high-quality solutions compared to both hierarchical and single-fidelity baselines.

The method eliminates the need for manual tuning of model schedules by using a cosine annealing threshold that evolves from permissive to strict over training. This allows early computational savings through broad LF usage while ensuring convergence to HF-quality solutions in later stages. The framework's success depends on heterogeneous LF models with different error distributions across the design space, enabling selective combination that outperforms fixed hierarchies when LF models excel in different regions.

## Method Summary
The framework uses PPO to train separate RL agents for a high-fidelity flight dynamics simulator and two heterogeneous neural network surrogates. At each timestep, cosine similarity between LF and HF policy action means determines whether LF experience data is integrated into HF updates. An alignment threshold, scheduled from 90° to 0° using cosine annealing, controls model selection. The ε-greedy strategy chooses among aligned LF models or HF directly. Experience buffers store data per fidelity, with HF selectively incorporating aligned LF samples. The octocopter design task involves optimizing continuous arm length and discrete component choices to maximize trajectory tracking quality across ~10^6 configurations.

## Key Results
- Adaptive MFRL agent achieved significantly reduced variance (Figure 7) compared to hierarchical MFRL, HF-only, and LF-only baselines
- The framework demonstrated improved computational efficiency through dynamic model selection that minimized unnecessary HF evaluations
- Solution quality was more consistent across seeds with lower spread than all comparison methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive alignment-based model selection reduces variance in policy updates compared to fixed hierarchical schedules.
- Mechanism: At each training step, cosine similarity between the mean action distributions of each LF policy (π_LFi) and the HF policy (π_HF) is computed. LF experience data is integrated into HF policy updates only when alignment exceeds a threshold; otherwise, HF agent is used directly, preventing misaligned gradients from corrupting learning.
- Core assumption: LF policies that produce similar action distributions to the HF policy provide rewards that are correlated with HF rewards, and this correlation implies lower-variance gradient estimates.
- Evidence anchors:
  - [abstract] "low-fidelity policies and their experience data are adaptively used for efficient targeted learning, guided by their alignment with the high-fidelity policy...alignment, measured through cosine similarity of policy action means, ensures correlated sampling and systematically reduces the variance"
  - [section 3.1] "alignment is quantitatively assessed using the cosine similarity between the mean action distributions...In regions where LF policies closely align with the HF policy, the HF agent leverages the aligned LF agents' experience data for policy updates"
  - [corpus] Related work on control variate approaches for policy gradient estimation (arXiv:2503.05696) supports correlation-based variance reduction, though specific alignment metrics differ.
- Break condition: If LF and HF policies share action distributions but reward structures differ fundamentally (same action, different value), alignment no longer guarantees variance reduction.

### Mechanism 2
- Claim: Dynamic threshold scheduling enables early computational savings while ensuring convergence to HF-quality solutions.
- Mechanism: Alignment threshold evolves via cosine schedule from 90° (permissive) to 0° (strict) over training. Early training allows broad LF usage; later stages increasingly require exact alignment, forcing HF model usage for refinement.
- Core assumption: Early exploration benefits from cheap LF samples even with loose alignment, while late-stage policy refinement requires strict fidelity.
- Evidence anchors:
  - [section 3.1] "alignment threshold dynamically evolves during training using a cosine schedule, progressively adjusting from an initial broad alignment criterion (90 degrees) toward stricter alignment (0 degrees)"
  - [section 5, Figure 6] "Initially in regime R1, the LF2 model is heavily utilized...in regime R3, decrease in LF models coincides with increase in HF proportion...last regime R4 serves to increasingly refine learning of HF policy"
  - [corpus] Corpus evidence on dynamic scheduling is limited; no direct corroboration found.
- Break condition: If the task requires precise HF feedback even in early exploration (e.g., safety-critical constraints), loose thresholds may propagate fatal errors.

### Mechanism 3
- Claim: Heterogeneous non-hierarchical LF models can outperform fixed hierarchies when selectively combined based on local alignment.
- Mechanism: Multiple LF models with different error distributions across the design space are evaluated independently. The adaptive framework selects whichever LF model currently aligns with HF policy, bypassing rigid ordering constraints.
- Core assumption: Different LF models excel in different regions of the design space, and real-time alignment detection captures this variation better than predetermined schedules.
- Evidence anchors:
  - [abstract] "multiple heterogeneous, non-hierarchical low-fidelity models are dynamically leveraged...proposed approach substantially reduces variance...relative to traditional hierarchical multi-fidelity RL methods"
  - [section 5, Figure 7] "Adaptive MFRL agent uniquely demonstrates significantly reduced variance relative to all other agents...Hierarchical MFRL configurations indicate performance is sensitive to the ordering as well as proportions of model usage"
  - [corpus] "A Multi-Fidelity Control Variate Approach for Policy Gradient Estimation" (arXiv:2503.05696) corroborates that exploiting LF-HF correlation reduces variance, though does not address non-hierarchical ensembles.
- Break condition: If all LF models share systematic biases in the same regions, alignment-based selection offers no advantage over hierarchies.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: The framework uses PPO for policy training; understanding clipping, advantage estimation, and trust regions is essential to diagnose whether variance originates from the base algorithm vs. multi-fidelity integration.
  - Quick check question: Can you explain why PPO's clipped objective prevents destructively large policy updates?

- Concept: **Cosine Similarity in Policy Space**
  - Why needed here: Alignment is measured as cosine similarity of action mean vectors; misunderstanding this metric leads to incorrect threshold interpretation (degrees represent angular deviation, not probability).
  - Quick check question: Given two action mean vectors [1, 0] and [0.7, 0.7], what is the cosine similarity and corresponding angle?

- Concept: **Control Variates for Variance Reduction**
  - Why needed here: The paper positions itself against control variate methods; understanding the baseline helps contextualize why alignment-based selection is an alternative strategy.
  - Quick check question: In Monte Carlo estimation, how does a control variate reduce variance when the variate is correlated with the target?

## Architecture Onboarding

- Component map:
  HF Agent -> LF Agents -> Alignment Evaluator -> Threshold Scheduler -> Fidelity Selector -> Experience Buffer

- Critical path:
  1. Initialize all agents with random policies
  2. For each timestep: evaluate alignment → select fidelity → execute action → compute reward → store experience
  3. Update each agent's policy from its experience buffer using PPO
  4. At evaluation: run HF policy on seed designs, measure HF-only objective

- Design tradeoffs:
  - Computational cost vs. variance: Higher HF usage reduces variance but increases cost; adaptive framework navigates this automatically
  - Threshold aggressiveness: Faster annealing reduces early variance but increases computational burden
  - LF model diversity: More heterogeneous LF models provide better coverage but require more alignment checks

- Failure signatures:
  - All LF models ignored: Threshold too strict too early; check scheduler initialization
  - High variance persists: LF models may have correlated errors; verify heterogeneity in surrogate training data
  - Convergence to poor local optima: ε-greedy exploration may be insufficient; increase ε or extend training

- First 3 experiments:
  1. Baseline reproduction: Run HF-only PPO on octocopter task for 1200 episodes; confirm variance and solution quality match paper baseline (~0.65 mean quality, high spread)
  2. Alignment threshold sweep: Fix threshold at 30°, 60°, 90° constant (no scheduling); observe variance/cost tradeoff to validate dynamic scheduling rationale
  3. LF model ablation: Remove LF2 (better-aligned model) and run adaptive framework with only LF1; quantify performance degradation to assess robustness to LF quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative alignment metrics or dynamic threshold schedules provide greater variance reduction than the current cosine similarity and fixed cosine schedule approach?
- Basis in paper: [explicit] The authors explicitly propose exploring "alternative alignment metrics and threshold scheduling techniques to achieve greater variance reduction" in future research.
- Why unresolved: The current study limits its implementation to cosine similarity between policy action means and a specific cosine-based threshold schedule.
- What evidence would resolve it: Empirical results comparing convergence speed and solution variance using metrics such as KL-divergence or adaptive thresholding based on gradient noise.

### Open Question 2
- Question: Does incorporating cost-weighted selection probabilities during alignment improve the computational efficiency of the framework?
- Basis in paper: [explicit] The conclusion suggests "investigating cost-weighted model selection probabilities when multiple low-fidelity policies align with the high-fidelity policy."
- Why unresolved: The current $\epsilon$-greedy strategy selects aligned low-fidelity models without explicitly weighting them by their computational cost relative to their fidelity.
- What evidence would resolve it: A comparative analysis showing that cost-weighting allows the agent to achieve equivalent solution quality with a lower total simulation time budget.

### Open Question 3
- Question: Can integrating partial hierarchical structures into the adaptive framework enhance performance compared to a purely non-hierarchical approach?
- Basis in paper: [explicit] The authors state that "incorporating partial hierarchical structures within some fidelity levels may further enhance performance by combining the advantages of hierarchical and adaptive strategies."
- Why unresolved: The proposed framework was designed specifically to eliminate reliance on hierarchies, so hybrid structures remain untested.
- What evidence would resolve it: Benchmarking a hybrid adaptive-hierarchical agent against the purely adaptive agent on problems with known structured fidelity levels.

## Limitations

- The framework's success critically depends on the heterogeneity of LF models; performance may degrade if LF surrogates share systematic biases in the same design space regions.
- The specific task domain (aerospace design optimization) and computational budget assumptions may not translate directly to other engineering domains with different fidelity trade-offs.
- The mechanism linking alignment-based selection to variance reduction relies on assumptions about LF-HF reward correlation that aren't fully validated across different design space regions.

## Confidence

- **High confidence**: Variance reduction claims (Figure 7), computational efficiency improvements (Figure 6), and superiority over hierarchical baselines are well-supported by experimental data with clear statistical separation.
- **Medium confidence**: The mechanism linking alignment-based selection to variance reduction is theoretically sound but relies on assumptions about LF-HF reward correlation that aren't fully validated across different design space regions.
- **Medium confidence**: The dynamic threshold scheduling rationale is intuitive but lacks ablation studies proving it outperforms simpler fixed-threshold approaches.

## Next Checks

1. Perform LF model ablation studies systematically removing individual LF models to quantify the framework's robustness to LF quality heterogeneity.
2. Conduct cross-domain validation on a different engineering optimization problem (e.g., structural design) to assess generalizability beyond aerospace applications.
3. Implement a fixed-threshold baseline (multiple constant values) to isolate the contribution of dynamic scheduling versus the core alignment-based selection mechanism.