---
ver: rpa2
title: 'Dictionary Learning: The Complexity of Learning Sparse Superposed Features
  with Feedback'
arxiv_id: '2502.05407'
source_url: https://arxiv.org/abs/2502.05407
tags:
- feedback
- sparse
- learning
- feature
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of learning sparse superposed features
  (e.g., dictionaries) from feedback in the form of relative triplet comparisons.
  The core method involves an agent providing feedback on sparse combinations of atomic
  features, which a learner uses to recover the underlying feature matrix up to normal
  transformation.
---

# Dictionary Learning: The Complexity of Learning Sparse Superposed Features with Feedback

## Quick Facts
- arXiv ID: 2502.05407
- Source URL: https://arxiv.org/abs/2502.05407
- Reference count: 40
- Learns sparse superposed features from triplet comparisons with tight feedback complexity bounds

## Executive Summary
This paper establishes tight feedback complexity bounds for learning sparse superposed features (feature matrices) from relative triplet comparisons. The learner receives feedback comparing linear combinations of atomic features and must recover the underlying feature matrix up to normal transformation. The work proves fundamental limits for both constructive (where agents can construct activations) and distributional (where activations are sampled) settings, with tight bounds of Œò(r(r+1)/2 + (p-r)) for general activations and O(p¬≤(2/(p¬≤s) log 2/Œ¥)^(1/p¬≤)) for sparse activations. Experiments validate these bounds on feature recovery from Recursive Feature Machines and dictionary extraction from sparse autoencoders in large language models.

## Method Summary
The method reduces triplet comparisons to pairwise equality constraints, then solves for the feature matrix Œ¶ ‚àà Sym‚Çä(R^p√óp) up to normal transformation. For constructive settings, the learner builds a basis of orthogonal vectors and rank-1 matrices that span the version space of valid Œ¶. For distributional settings with sparse activations, the agent samples patterns that form a linearly independent basis. Large-scale optimization uses gradient descent with low-rank parameterization Œ¶ = UU·µÄ. The framework is validated on synthetic RFM data and SAE dictionaries from pretrained LLMs.

## Key Results
- Proves Œò(r(r+1)/2 + (p-r)) feedback complexity for low-rank feature matrices with constructive feedback
- Establishes O(p¬≤(2/(p¬≤s) log 2/Œ¥)^(1/p¬≤)) sample complexity for sparse sampled activations
- Experiments show PCC > 0.95 dictionary recovery from SAEs within 10% of theoretical bounds
- Validates theoretical bounds on RFM synthetic data with rank 4 features in 10-dimensional space

## Why This Works (Mechanism)

### Mechanism 1: Triplet-to-Pairwise Reduction
- **Claim:** Triplet comparisons can be reduced to pairwise equality constraints without loss of information for feature equivalence.
- **Mechanism:** Given triplet (x, y, z) where d(x,y) ‚â• d(x,z) relative to target Œ¶*, rescaling z ‚Üí ‚àö(1+Œª)z for appropriate Œª yields equality y'·µÄŒ¶*y' = z'·µÄŒ¶*z'. This transforms inequality constraints into linear equations in Sym(R^p√óp), where each pair (y,z) constrains Œ¶ to satisfy ‚ü®Œ¶, yy·µÄ - zz·µÄ‚ü© = 0.
- **Core assumption:** The learner is oblivious‚Äîrandomly selects any valid Œ¶ satisfying all constraints rather than optimizing.
- **Evidence anchors:** [abstract] "relative triplet comparisons...evaluates whether linear combinations...are more similar to...other combinations"; [section 4] Lemma 2 proves equivalence of triplet and pairwise formulations.

### Mechanism 2: Low-Rank Decomposition Strategy
- **Claim:** Low-rank feature matrices require Œò(r(r+1)/2 + (p-r)) feedback, exploiting the separation between eigenspace and nullspace.
- **Mechanism:** Decompose Œ¶* = Œ£·µ¢·µ£ Œª·µ¢u·µ¢u·µ¢·µÄ. Teaching proceeds in two phases: (1) nullspace vectors v·µ¢‚àànull(Œ¶*) via pairs (0, v·µ¢) forcing v·µ¢·µÄŒ¶v·µ¢=0; (2) kernel directions via linearly independent rank-1 matrices {u·µ¢u·µ¢·µÄ, (u·µ¢+u‚±º)(u·µ¢+u‚±º)·µÄ} that span O_Œ¶*.
- **Core assumption:** Œ¶* is symmetric PSD with known rank r; orthogonal Cholesky decompositions exist (Lemma 1).
- **Evidence anchors:** [abstract] "tight bounds when the agent is permitted to construct activations"; [section 4.2] Theorem 1 with proof in Appendices F-G.

### Mechanism 3: Sparse Sampling Complexity
- **Claim:** Sparse sampled activations require O(p¬≤(2/(p¬≤s) log 2/Œ¥)^(1/p¬≤)) samples, where s controls sparsity level.
- **Mechanism:** With sparse distributions (P(Œ±·µ¢=0)=p·µ¢), the probability of sampling patterns that form a linearly independent basis drops exponentially. The agent must receive enough samples to cover the "diagonal" pattern {Œª·µ¢e·µ¢} and "off-diagonal" patterns {(Œª·µ¢‚±ºe·µ¢ + Œª·µ¢‚±ºe‚±º)}. Hoeffding's inequality bounds the sample complexity.
- **Core assumption:** Activations sampled i.i.d. from Assumption 1 (sparse distribution); rescaling by the agent is permitted.
- **Evidence anchors:** [abstract] "strong upper bounds in sparse scenarios when the agent's feedback is limited to distributional information"; [section 5] Theorem 4 with proof in Appendix I.

## Foundational Learning

- **Concept:** Symmetric positive semi-definite (PSD) matrices and Cholesky decomposition
  - **Why needed here:** Feature matrices Œ¶ ‚àà Sym‚Çä(R^p√óp) are PSD; recovery is "up to normal transformation" meaning Œ¶ = UU·µÄ where U is only identifiable up to orthogonal transformation.
  - **Quick check question:** Given a PSD matrix Œ¶ of rank r < p, how many degrees of freedom does it have? (Answer: r(r+1)/2 + r(p-r) = r(r+1)/2 entries in the principal submatrix plus cross terms, but the paper uses r(r+1)/2 + (p-r) counting eigenspace and nullspace separately.)

- **Concept:** Version space learning and oblivious learners
  - **Why needed here:** The learner doesn't optimize‚Äîit randomly samples from the set of all Œ¶ satisfying constraints. Complexity is the minimum constraints to make this version space a singleton (up to scaling).
  - **Quick check question:** If feedback F constrains Œ¶ to satisfy 5 linear equations in Sym(R^4√ó4), what is the dimension of the version space? (Answer: dim(Sym(R^4√ó4))=10, so version space has dimension at most 10-5=5, assuming linear independence.)

- **Concept:** Dictionary learning and sparse coding fundamentals
  - **Why needed here:** The target Œ¶* = D·µÄD for dictionary D ‚àà R^(d√óp); this paper learns Œ¶* without observing D directly, only through activation comparisons.
  - **Quick check question:** Why is mutual incoherence (D's columns nearly orthogonal) helpful for dictionary identifiability? (Answer: Under strong incoherence, Œ¶* recovery implies D recovery up to rotation/sign per Lemma 1.)

## Architecture Onboarding

- **Component map:** [Agent] ‚Üí constructs/selects triplets (Œ±,Œ≤,Œ∂) ‚Üí [Triplet Resolver] ‚Üí pairs (y,z) + rescaling ‚Üí [Constraint Builder] ‚Üê stores {yy·µÄ - zz·µÄ} matrices ‚Üê [Feedback Processor] ‚Üí [Optimizer/Constraint Solver] ‚Üí outputs Œ¶ÃÇ ‚àà Sym‚Çä(R^p√óp) ‚Üí [Optional: Cholesky Decomposition] ‚Üí √õ such that Œ¶ÃÇ = √õ√õ·µÄ

- **Critical path:**
  1. Implement pairwise constraint accumulation (memory: O(p¬≤) per constraint, but can use batch gradient descent for large p)
  2. For constructive setting: explicitly build basis from Lemma 3 (eigendecomposition path) or Eq. 4 (sparse constructive path)
  3. For sampled setting: accumulate until design matrix M has full rank (check det(M) or condition number)
  4. Solve via convex optimization (cvxpy for small p) or gradient descent with regularization (Algorithm 3 for large p > 5000)

- **Design tradeoffs:**
  - **Eigendecomposition vs. Sparse Constructive:** Eigendecomposition exploits low rank (O(r¬≤) feedback) but requires knowing rank and eigenvectors; Sparse Constructive is rank-agnostic but requires O(p¬≤) feedback.
  - **Exact solver vs. GD:** Exact solvers give precise equivalence but O(p‚Å∂) complexity; GD scales but may converge to local minima (the paper uses Œª=10‚Åª‚Å¥ regularization and fixed U[0,0]=1 for identifiability).
  - **Sparsity level s:** Lower s reduces memory per activation but increases sample complexity dramatically (Theorem 4 shows exponential dependence on 1/p¬≤).

- **Failure signatures:**
  - **Rank underestimation:** If you assume rank r' < true rank r, version space contains infinitely many solutions; MSE plateaus above target.
  - **Insufficient samples (sparse setting):** PCC correlation improves slowly; at 1M feedback for p=512, PCC‚âà0.71 vs. 0.97 at 8M (Figure 3b).
  - **Numerical instability:** For p>5000, storing full Œ¶ is prohibitive; must use low-rank parameterization Œ¶=UU·µÄ. Gradient clipping is used to avoid explosion.

- **First 3 experiments:**
  1. **Synthetic RFM validation:** Train RFM on monomial regression (e.g., f*(z)=z‚ÇÄz‚ÇÅùüô(z‚ÇÖ>0)) with p=10, rank r=4-8. Test eigendecomposition feedback (Theorem 1 bound: 38 feedbacks) vs. random sampling (55 feedbacks). Verify MSE matches target Œ¶* at the bound.
  2. **Sparse sampling scaling:** Fix p=10, vary sparsity Œº‚àà{0.3,0.5,0.7,0.9}. Plot PCC vs. number of feedbacks. Confirm that higher Œº (more zeros) requires more samples per Theorem 4.
  3. **SAE dictionary recovery:** Load pre-trained SAE dictionary for Pythia-70M layer (p=512). Apply sparse constructive feedback (bound: 8.3M). Compare recovered Œ¶ÃÇ to ground truth using PCC. Target: PCC>0.95 within 10% excess of theoretical bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the underlying dictionary matrix $D$ be uniquely recovered (up to permutation and sign) without relying on strong mutual incoherence assumptions?
- Basis in paper: [explicit] Section 7.1 states that except under strong coherence assumptions (Lemma 1), "full recovery of the underlying dictionary remains open."
- Why unresolved: The current bounds only guarantee recovery up to normal transformation ($D^\top D$) for the general case, which leaves the specific basis vectors ambiguous.
- What evidence would resolve it: A proof of identifiability for $D$ under weaker conditions than strong incoherence, or a constructive algorithm that disentangles features in superposition without orthogonal rows.

### Open Question 2
- Question: What is the feedback complexity required to learn an $\epsilon$-accurate approximation of the feature matrix in Frobenius norm?
- Basis in paper: [explicit] Section 7.1 identifies relaxing exact feature equivalence to ask instead for an "Œµ-accurate approximation" as a "natural next step."
- Why unresolved: The current theoretical bounds (e.g., $\Theta(r(r+1)/2)$) are derived for exact recovery, whereas practical applications often tolerate small errors.
- What evidence would resolve it: Theoretical bounds on the number of triplet comparisons needed to ensure $\|\hat{\Phi} - \Phi^*\|_F \leq \epsilon$ for a learner.

### Open Question 3
- Question: Can the gap between the theoretical feedback complexity bounds and practical sample requirements be tightened by exploiting structural insights?
- Basis in paper: [explicit] Section 7.1 explicitly asks "whether the gap between these bounds and practical sample requirements can be tightened."
- Why unresolved: The paper establishes worst-case or average-case bounds that may be loose compared to the empirical performance observed in experiments with Recursive Feature Machines or LLMs.
- What evidence would resolve it: Refined theoretical analysis that reduces the complexity constants or asymptotic order to match the convergence rates observed in the empirical validation section.

## Limitations
- **Computational scalability**: Exact constraint solving grows as O(p^6), requiring low-rank approximations for large dictionaries
- **Distributional assumptions**: Sparse sampling bounds assume i.i.d. sampling from specific distributions that may not match real LLM activation patterns
- **Identifiability ambiguity**: Recovery is only up to normal transformation, with additional ambiguity for repeated eigenvalues

## Confidence
- **High confidence**: The core theoretical results (Theorems 1-4) are rigorously proven with explicit bounds matching lower bounds
- **Medium confidence**: Experimental validation on RFM synthetic data and SAE dictionaries shows alignment with theory
- **Low confidence**: The practical utility of recovered dictionaries for mechanistic interpretability is demonstrated through PCC metrics but not through downstream task performance

## Next Checks
1. **Rank estimation validation**: Test the framework's robustness when the true rank r is unknown or misestimated. Measure recovery quality as a function of assumed rank r' ‚â† r.
2. **Distribution mismatch stress test**: Evaluate sample complexity under activation distributions that deviate from Assumption 1 (e.g., non-sparse activations or correlated sparsity patterns common in LLMs).
3. **Downstream utility benchmark**: Apply recovered dictionaries to a concrete interpretability task (e.g., feature attribution or circuit analysis) and measure performance improvement relative to random or alternative baselines.