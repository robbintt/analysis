---
ver: rpa2
title: Robust Uncertainty Quantification for Self-Evolving Large Language Models via
  Continual Domain Pretraining
arxiv_id: '2510.22931'
source_url: https://arxiv.org/abs/2510.22931
tags:
- domain
- different
- prediction
- test
- ar-necp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel Conformal Prediction (CP) framework
  for self-evolving large language models (LLMs) under continual domain pretraining
  (CDP). It addresses two major challenges: distribution shift across domains and
  the need for adaptive rejection of unanswerable queries.'
---

# Robust Uncertainty Quantification for Self-Evolving Large Language Models via Continual Domain Pretraining

## Quick Facts
- arXiv ID: 2510.22931
- Source URL: https://arxiv.org/abs/2510.22931
- Reference count: 25
- Primary result: AR-NECP framework achieves target coverage across shifting distributions with smaller prediction sets than baseline methods

## Executive Summary
This paper addresses uncertainty quantification for large language models under continual domain pretraining (CDP), tackling distribution shift and unanswerable query rejection. The proposed AR-NECP framework combines a non-exchangeable conformal prediction module that reweights calibration data based on estimated test-domain distributions, with an adaptive rejection module that dynamically adjusts abstention thresholds. Experiments on TriviaQA, HotpotQA, and MMLU demonstrate robust coverage guarantees and improved efficiency over baseline methods across shifting domains.

## Method Summary
The AR-NECP framework operates by first estimating the test domain distribution through transformer-based clustering of calibration and test questions, then reweighting or resampling calibration data to match this estimated distribution. A label-conditional conformal prediction scheme uses normalized entropy scores to distinguish answerable from unanswerable questions, with adaptive thresholds determined through grid search to maintain target coverage while minimizing prediction set size. The method is validated on sequential fine-tuning across domains using Llama-3.1-8B, Mistral-7B, and Gemma-7B models.

## Key Results
- AR-NECP achieves target coverage across shifting distributions with mean relative errors in domain estimation ranging from 0.01-0.11
- Consistently produces smaller prediction sets than baseline methods, especially under high coverage requirements
- Maintains coverage guarantees even when calibration and test sets follow different domain distributions

## Why This Works (Mechanism)

### Mechanism 1
Reweighting or resampling calibration data based on estimated test-domain distributions restores approximate exchangeability, enabling valid coverage under domain shift. The framework clusters test and calibration questions using transformer embeddings, estimates the true domain proportions $\hat{n}^{test}$ via an invertible transition matrix $P$, then reweights calibration samples by $\hat{n}^{test}_k / \sum_k \hat{n}^{test}_k$ or resamples to match the estimated test distribution. This aligns the nonconformity score distribution between calibration and test sets.

### Mechanism 2
Label-conditional CP with adaptive rejection thresholds maintains target coverage while producing smaller prediction sets, especially under high coverage requirements. Separate error rates $\alpha_0$ (unanswerable) and $\alpha_1$ (answerable) are linked via Equation 12: $\alpha_1 = (1-r_{correct})(\alpha - \alpha_0) / (r_{correct}(1-\alpha))$. Grid search over $(\alpha_0, \alpha_1)$ minimizes expected prediction set size. Normalized entropy (NE) scores determine answerability; questions with $p_0(x_i) < q^0_{\alpha_0}$ and $p_1(x_i) > q^1_{\alpha_1}$ are rejected.

### Mechanism 3
Transformer-based clustering provides sufficiently accurate domain assignment to enable reliable estimation of test-domain proportions. Questions are encoded via a transformer encoder $T(\cdot)$, domain centroids $\tilde{c}_k$ are computed as $\ell_2$-normalized mean embeddings, and each question is assigned to the nearest centroid via cosine similarity. The clustering statistics from calibration data inform the transition matrix $P_{ij}$.

## Foundational Learning

- **Conformal Prediction (CP) Fundamentals**: Why needed here: The entire framework builds on CP's coverage guarantee $P(y_{n+1} \in C_\alpha(x_{n+1})) \geq 1 - \alpha$. Understanding nonconformity scores, quantile thresholds, and the exchangeability assumption is prerequisite. Quick check question: Given calibration scores $[0.1, 0.3, 0.5, 0.7]$ and $\alpha=0.2$, what quantile defines the prediction threshold?

- **Continual Domain Pretraining (CDP) and Catastrophic Forgetting**: Why needed here: The paper addresses coverage degradation when LLMs are sequentially fine-tuned across domains. Knowledge of how fine-tuning order affects model competence informs why adaptive rejection is necessary. Quick check question: If a model trained on domains A→B→C is tested on domain A data, what phenomenon might cause degraded performance?

- **Weighted Conformal Prediction under Covariate Shift**: Why needed here: The reweighting mechanism (Eq. 25) extends standard CP to handle $p_{test}(x) \neq p_{cal}(x)$. Prior work (Tibshirani et al., 2019) provides the theoretical foundation. Quick check question: If calibration data over-represents domain A relative to test data, should weights for domain A samples increase or decrease?

## Architecture Onboarding

- **Component map**: Domain centroid constructor -> Domain distribution estimator -> Calibration resampler/reweighter -> NE-based answerability scorer -> Adaptive rejection calibrator -> Inference engine

- **Critical path**: Domain centroid construction → Test distribution estimation → Calibration reweighting → Adaptive threshold calibration → Inference with rejection logic

- **Design tradeoffs**: Resampling vs. Reweighting: Resampling is stochastic (varies across runs) but conceptually simple; reweighting is deterministic and more stable when minority domains are underrepresented (Section 4.3). NE vs. MLP classifier for answerability: NE requires no training but may be noisier; MLP requires labeled training data from $D^{cluster}_{union}$ (Section 5.3, Table 1 shows NE generally outperforms MLP).

- **Failure signatures**: Coverage significantly below $1-\alpha$: Likely domain estimation error or severe within-domain non-I.I.D. data; check $\Delta_k$ values. Prediction sets excessively large: Grid search may have converged to suboptimal $(\alpha_0, \alpha_1)$ pairs; inspect $r_{correct}$ in calibration data. High rejection rate on answerable questions: NE threshold $q^1_{\alpha_1}$ too strict; verify NE distribution in calibration vs. test.

- **First 3 experiments**: 1. Baseline coverage under shift: Run standard CP (no reweighting, no rejection) on a test set dominated by one domain; confirm undercoverage when hard domain dominates (replicate Figure 2, "Basic" bar). 2. Domain estimation accuracy: On a held-out test set with known domain labels, compute $\hat{n}^{test}$ and report $\Delta_k$ per domain; verify errors are <0.15 (compare to Table 2). 3. Ablation on rejection thresholds: Fix reweighting, vary $(\alpha_0, \alpha_1)$ manually (no grid search), plot efficiency vs. coverage tradeoff; confirm grid search finds the Pareto-optimal point.

## Open Questions the Paper Calls Out

- Can calibration data sampled before fine-tuning be reused across multiple CDP stages by modeling the bias introduced by domain adaptation order? The authors state that fine-tuning across domains introduces non-exchangeability among models trained at different steps, making reuse infeasible without correction.

- How can the variance in the domain proportion estimator ($\hat{n}^{test}_k$) be controlled to ensure reliable coverage in finite-sample regimes? The authors acknowledge their estimated number of test questions per domain can still exhibit noticeable variance, and controlling this variance is beyond the scope of the paper.

- How does AR-NECP perform on generative tasks beyond QA, such as summarization, translation, or code generation, where prediction sets are harder to define? The method is evaluated only on QA datasets, and the clustering and scoring mechanisms rely on answer clustering via similarity metrics that may not transfer to structured or free-form generation tasks.

## Limitations

- Framework effectiveness depends on transformer embeddings providing clean separability between domains for reliable distribution estimation, which lacks theoretical guarantees
- Normalized entropy scores' ability to consistently distinguish answerable from unanswerable questions across diverse domains and model architectures lacks comprehensive validation
- Reliance on invertible transition matrices introduces brittleness when domains overlap significantly in embedding space

## Confidence

**High Confidence**: The basic CP framework and its coverage guarantees are well-established theoretically. The adaptive rejection mechanism's formulation and grid search approach are mathematically sound given the underlying assumptions.

**Medium Confidence**: The domain distribution estimation via transformer clustering shows reasonable empirical performance (Table 2 errors 0.01-0.11) but depends heavily on embedding quality and cluster separability, which varies by domain and dataset.

**Low Confidence**: The normalized entropy score's ability to consistently distinguish answerable from unanswerable questions across diverse domains and model architectures lacks comprehensive validation, particularly for specialized or technical domains.

## Next Checks

1. **Domain Estimation Robustness**: Systematically vary the number of sampled answers $M$ (from 10 to 1000) and clustering parameters to measure their impact on domain estimation accuracy and resulting coverage guarantees.

2. **NE Score Reliability**: Construct synthetic unanswerable questions by corrupting answerable ones (e.g., swapping entities, negating facts) and measure NE score separation; identify failure modes where NE misclassifies question types.

3. **Cross-Domain Transferability**: Apply the framework to a new domain pair (e.g., legal to medical) where domain embeddings are expected to overlap significantly, and measure degradation in both domain estimation and coverage guarantees.