---
ver: rpa2
title: 'Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent'
arxiv_id: '2512.00986'
source_url: https://arxiv.org/abs/2512.00986
tags:
- research
- uhpc
- performance
- retrieval
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Dr.Mi-Bench, a modular-integrated benchmark
  for evaluating scientific deep research (DR) agents. It addresses two key gaps in
  current DR evaluation: the narrow focus on retrieval over planning and reasoning,
  and the lack of benchmarks targeting academic/scientific domains.'
---

# Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent

## Quick Facts
- **arXiv ID:** 2512.00986
- **Source URL:** https://arxiv.org/abs/2512.00986
- **Reference count:** 40
- **Primary result:** Introduces Dr.Mi-Bench, a modular-integrated benchmark evaluating scientific deep research agents across planning, retrieval, and reasoning modules with 200 human-annotated instances.

## Executive Summary
This paper introduces Dr.Mi-Bench, a modular-integrated benchmark designed to evaluate scientific deep research (DR) agents. It addresses two key gaps in current DR evaluation: the narrow focus on retrieval over planning and reasoning, and the lack of benchmarks targeting academic/scientific domains. The benchmark uses 200 human-annotated instances across 10 scientific fields, evaluating agents across planning, retrieval, and reasoning modules via both end-to-end and isolated modes. Experimental results show uneven agent performance—specialized strengths but shared weaknesses in multi-source retrieval and cross-field consistency—while foundational LLMs exhibit strong reasoning potential but are bottlenecked by planning.

## Method Summary
Dr.Mi-Bench evaluates DR agents across three core modules: Planning (query decomposition into sub-tasks), Retrieval (evidence gathering from academic sources), and Reasoning (report synthesis). The benchmark uses 200 human-annotated instances from 10 academic domains, each including research queries, gold-standard sub-task plans (5-10 steps), gold citations (DOIs), and boolean diagnostic statements (15-20 per paper). Evaluation employs both end-to-end mode (autonomous) and isolated mode (ground-truth injected) using GPT-4o as a semantic judge. Metrics include Accuracy, Precision, Recall, and F1 for Planning and Reasoning; Jaccard Index for Retrieval; plus efficiency measures (latency, cost).

## Key Results
- Uneven agent performance across domains: agents show specialized strengths but shared weaknesses in multi-source retrieval and cross-field consistency
- Planning module identified as bottleneck: foundational LLMs exhibit strong reasoning potential but are limited by planning capabilities
- Retrieval challenges: agents struggle with matching against diverse academic sources using DOI and title-based matching rules

## Why This Works (Mechanism)
The benchmark's modular-integrated design allows for systematic evaluation of DR agents by isolating and measuring performance across planning, retrieval, and reasoning components. By using both end-to-end and isolated evaluation modes, it can diagnose whether performance bottlenecks are module-specific or systemic. The use of human-annotated gold standards with diagnostic statements provides a rigorous ground truth for semantic evaluation.

## Foundational Learning

### Deep Research Agents
**Why needed:** Understanding the architecture and capabilities of DR agents is essential for interpreting benchmark results and identifying performance bottlenecks.
**Quick check:** Can you explain how DR agents differ from standard language models in their approach to academic research tasks?

### Modular Evaluation
**Why needed:** Breaking down evaluation into planning, retrieval, and reasoning modules allows for targeted diagnosis of agent capabilities and weaknesses.
**Quick check:** How would you design an evaluation metric to compare predicted sub-tasks against gold-standard plans?

### Academic Citation Matching
**Why needed:** Accurate retrieval evaluation requires robust methods for matching retrieved papers against gold-standard citations across different identifier formats.
**Quick check:** What challenges arise when matching academic papers using DOIs versus title prefixes?

## Architecture Onboarding

### Component Map
Dr.Mi-Eval -> (Planning Evaluator, Retrieval Matcher, Reasoning Evaluator) -> Performance Metrics

### Critical Path
1. Query input → 2. Planning module output → 3. Retrieval module output → 4. Reasoning module output → 5. End-to-end evaluation
*Plus parallel isolated mode evaluation with ground-truth injection*

### Design Tradeoffs
- **End-to-end vs. Isolated modes:** End-to-end tests real-world capability but conflates module failures; isolated mode provides module-specific insights but may not reflect actual agent behavior
- **DOI vs. Title matching:** DOI provides exact matching but may miss variants; title prefix matching is more flexible but less precise

### Failure Signatures
- **Planning failures:** Low F1 scores with high variance across domains indicate poor query decomposition
- **Retrieval failures:** High false negatives with DOI matching suggest metadata normalization issues
- **Reasoning failures:** Low diagnostic accuracy despite high verbosity indicates hallucination

### Three First Experiments
1. Run a single agent on 5-10 annotated papers and compare end-to-end vs. isolated mode performance
2. Test the planning evaluator with predicted vs. gold sub-tasks to validate metric implementation
3. Evaluate retrieval matching accuracy using both DOI and title-based rules on a small dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset availability: The 200 annotated papers with complete gold standards are not directly provided in the paper
- Judge prompt specificity: Exact few-shot prompts for the "exhaustive pairwise comparison" in planning evaluation are not fully detailed
- Retrieval normalization: Exact algorithm for canonicalizing paper identifiers beyond the 20-character title rule is underspecified

## Confidence

### Confidence Labels for Major Claims
- **Benchmark Design Validity:** Medium - Modular approach is conceptually sound but depends on dataset quality
- **Agent Performance Results:** Medium - Clear patterns observed but based on proprietary dataset and configurations
- **Diagnostic Framework Utility:** Medium - Provides actionable insights but requires full benchmark access for validation

## Next Checks
1. **Data Availability Check:** Verify dataset release on official project page (https://github.com/DRBench/Dr.Mi-Bench) or contact authors for access to 200 annotated instances
2. **Implementation Fidelity Check:** Implement Dr.Mi-Eval pipeline using GPT-4o judge with Appendix prompts, test on manually-curated mini-benchmark (10-20 papers)
3. **Agent Comparison Check:** Run evaluation on two different DR agents (e.g., OpenAI and Gemini) to confirm reported performance patterns across modules