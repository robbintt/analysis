---
ver: rpa2
title: Descend or Rewind? Stochastic Gradient Descent Unlearning
arxiv_id: '2511.15983'
source_url: https://arxiv.org/abs/2511.15983
tags:
- unlearning
- convex
- lemma
- gradient
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves (\u03B5, \u03B4)-certified unlearning guarantees\
  \ for stochastic gradient descent versions of Descent-to-Delete (D2D) and Rewind-to-Delete\
  \ (R2D) algorithms across strongly convex, convex, and nonconvex loss functions.\
  \ The key insight is treating SGD unlearning as biased or disturbed gradient systems,\
  \ where the bias comes from the difference between gradients on retained and full\
  \ datasets."
---

# Descend or Rewind? Stochastic Gradient Descent Unlearning

## Quick Facts
- arXiv ID: 2511.15983
- Source URL: https://arxiv.org/abs/2511.15983
- Reference count: 40
- Primary result: (ε, δ)-certified unlearning guarantees for SGD-based D2D and R2D algorithms across convex and nonconvex loss functions

## Executive Summary
This paper establishes certified unlearning guarantees for stochastic gradient descent versions of Descent-to-Delete (D2D) and Rewind-to-Delete (R2D) algorithms. The key insight is treating SGD unlearning as biased gradient systems, where the bias comes from the difference between gradients on retained and full datasets. The analysis couples training and unlearning trajectories to obtain probabilistic sensitivity bounds, which are combined with a relaxed Gaussian mechanism that requires only first or second moment bounds instead of worst-case sensitivity.

The work proves that PSGD-R2D achieves unlearning on bounded domains with minimal assumptions, SGD-R2D works on unbounded domains under second-moment boundedness, and SGD-D2D provides the tightest δ-dependence for strongly convex functions. The analysis reveals that R2D always has computational advantage over retraining, while D2D provides tighter privacy guarantees when the unlearned data proportion is small.

## Method Summary
The paper analyzes two SGD-based unlearning algorithms: R2D (Rewind-to-Delete) and D2D (Descent-to-Delete). Both algorithms involve training on the full dataset for T iterations, then unlearning by running SGD on the retained dataset D'. R2D saves a checkpoint at iteration T-K and initializes unlearning from this checkpoint, while D2D initializes from the final weights θ_T. Both algorithms add calibrated Gaussian noise to the final weights to achieve certified unlearning. The sensitivity bounds are computed based on the function class (strongly convex, convex, or nonconvex) and domain (bounded or unbounded), using trajectory coupling arguments and relaxed Gaussian mechanism.

## Key Results
- PSGD-R2D achieves certified unlearning on bounded domains with minimal assumptions about the loss function
- SGD-R2D provides certified unlearning for unbounded domains under second-moment boundedness assumptions
- SGD-D2D achieves the tightest δ-dependence for strongly convex functions when the unlearning set is small
- R2D provides computational advantage over retraining, potentially infinite for strongly convex functions
- The relaxed Gaussian mechanism allows using first/second moment bounds instead of worst-case sensitivity

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Coupling of Trajectories
The unlearning result can be certified by bounding the distance to a hypothetical "retrained from scratch" model through optimal coupling of their randomness. Instead of analyzing the unlearning path in isolation, the analysis couples the random mini-batch sampling of the "unlearning" trajectory with the "retraining" trajectory. By maximizing the overlap of sampled batches, the divergence between the two trajectories is minimized, reducing the noise required to mask the difference.

### Mechanism 2: Relaxed Gaussian Mechanism
Certified unlearning can be achieved using only first or second moment bounds on the distance between trajectories, rather than worst-case sensitivity. Standard Differential Privacy requires worst-case sensitivity, but this work uses a "relaxed" mechanism where if the distance is bounded with high probability, one can apply the Gaussian mechanism with that bound and suffer only a degradation from δ to 2δ.

### Mechanism 3: Rewind-to-Delete (R2D) as Disturbance Reversal
For convex and nonconvex functions, "rewinding" to an earlier checkpoint is necessary for unlearning because the gradient system is not strictly contracting to a single point. R2D initializes unlearning from θ_{T-K} rather than the final θ_T. Since convex/nonconvex functions are "semi-contracting" or "expansive," disturbances accumulate over time. Rewinding effectively reverses the accumulation of the most recent disturbances, allowing the unlearning path to converge closer to the retraining path.

## Foundational Learning

- **Concept**: Differential Privacy (DP) and the Gaussian Mechanism
  - Why needed here: The definition of "certified unlearning" is structurally identical to DP (indistinguishability of outputs on adjacent datasets). You must understand (ε, δ) to calibrate the noise σ.
  - Quick check question: Can you explain why adding Gaussian noise with standard deviation σ ≈ (Σ√(2log(1.25/δ)))/(ε) makes two vectors indistinguishable?

- **Concept**: Biased Stochastic Gradient Descent (SGD) Convergence
  - Why needed here: The paper models training on the full dataset D as biased SGD with respect to the retained dataset D'. Understanding how the bias term d_t = ∇L_D - ∇L_{D'} affects convergence is central to Theorem 4 (SGD-D2D).
  - Quick check question: In biased SGD, does the noise variance or the bias magnitude primarily determine the radius of convergence to the stationary point?

- **Concept**: Strong Convexity vs. Nonconvexity in Optimization
  - Why needed here: The unlearning guarantees differ significantly based on the loss landscape. Strong convexity implies a unique global minimum (contraction), while nonconvexity implies potential expansion, dictating the choice between D2D and R2D.
  - Quick check question: Why does the existence of a unique global minimum (strong convexity) allow for "tighter" privacy bounds compared to nonconvex settings?

## Architecture Onboarding

- **Component map**: Training Loop (PSGD/SGD) -> Checkpoint Saver -> Unlearning Loop (P)SGD -> Noise Injector -> Sensitivity Calculator
- **Critical path**: The calculation of the noise scale σ. This depends on correctly identifying the function class and domain. Errors here result in either privacy leakage (under-noising) or model utility collapse (over-noising).
- **Design tradeoffs**: R2D vs. D2D: R2D is more general (works for nonconvex) and always provides computational advantage over retraining. D2D provides tighter privacy (δ-dependence) for strongly convex functions but requires the unlearning set to be small.
- **Failure signatures**: Unbounded gradients causing Σ to diverge on unbounded domains; D2D fails when m/n is too large; heavy tails breaking tail bounds in relaxed Gaussian mechanism.
- **First 3 experiments**: 1) Implement SGD-D2D and SGD-R2D on strongly convex loss (logistic regression) to verify D2D achieves lower noise σ. 2) Apply PSGD-R2D on nonconvex task (MLP/CNN) to verify unlearning succeeds while D2D fails to certify. 3) Implement SGD-R2D without projection and plot required noise σ against batch size b to verify theoretical scaling.

## Open Questions the Paper Calls Out
1. Can certified unlearning guarantees be extended to SGD using sampling without replacement?
2. What are the formal utility and generalization guarantees for the proposed SGD-R2D and SGD-D2D algorithms?
3. Can combining R2D with clipped or noisy gradient updates yield improved unlearning guarantees for nonconvex functions?

## Limitations
- The coupling argument critically assumes i.i.d. sampling with replacement and independence between randomization and data values
- The Relaxed Gaussian mechanism requires that failure probability bounds can be absorbed into the overall privacy budget (δ → 2δ)
- Second-moment boundedness assumptions are unverifiable without access to data, making bounds potentially overly conservative

## Confidence
- **High confidence**: Theoretical framework and proof techniques are sound
- **Medium confidence**: Practical applicability depends heavily on unverifiable assumptions about gradient bounds and data distributions
- **Medium confidence**: Computational advantages of R2D over retraining are proven but depend on checkpoint storage and specific choice of K

## Next Checks
1. Implement the optimal coupling for mini-batch sampling explicitly and measure the actual distance between coupled trajectories on real datasets to validate theoretical bounds
2. Test R2D/D2D algorithms with non-i.i.d. sampling strategies to identify failure modes beyond theoretical assumptions
3. Develop methods to estimate or bound the constants B, C from data rather than assuming they are known, and evaluate the impact on noise scale and utility