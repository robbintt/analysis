---
ver: rpa2
title: Memoryless Policy Iteration for Episodic POMDPs
arxiv_id: '2512.11082'
source_url: https://arxiv.org/abs/2512.11082
tags:
- policy
- iteration
- memoryless
- pomdps
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel family of policy iteration algorithms
  for computing deterministic memoryless policies in episodic POMDPs. The key idea
  is to alternate between single-stage output-based policy improvements and policy
  evaluations according to a prescribed periodic pattern, addressing the non-Markovian
  nature of the output process.
---

# Memoryless Policy Iteration for Episodic POMDPs

## Quick Facts
- arXiv ID: 2512.11082
- Source URL: https://arxiv.org/abs/2512.11082
- Reference count: 14
- The paper introduces a novel family of policy iteration algorithms for computing deterministic memoryless policies in episodic POMDPs, achieving significant computational speedups over policy-gradient baselines.

## Executive Summary
This paper addresses the challenge of finding deterministic memoryless policies for episodic Partially Observable Markov Decision Processes (POMDPs), where the output process is non-Markovian. The authors propose a periodic policy iteration scheme that alternates between single-stage output-based policy improvements and policy evaluations. By updating one stage at a time with immediate re-evaluation of affected quantities, the algorithm guarantees monotonic improvement in the non-Markovian output process. The approach identifies optimal periodic patterns that minimize computational cost and extends to a model-free variant that learns directly from data, showing significant speedups over existing methods in both model-based and model-free settings.

## Method Summary
The method computes deterministic memoryless policies by alternating between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern. The algorithm maintains two coupled quantities—state distributions μ_t (forward) and state-action values Q_t (backward). When improving policy at stage τ_ℓ, changes propagate forward through μ_{τ_ℓ+1}, ..., μ_T but affect Q values only for t < τ_ℓ. The periodic sequence τ_ℓ must be onto (covers all stages) and τ_{ℓ+1} ≠ τ_ℓ (no redundant consecutive updates). The authors prove monotonic improvement and convergence to locally optimal policies under these mild conditions. They further identify optimal periodic patterns that minimize computational cost through a forward-backward sweep and propose a model-free variant that learns directly from data using importance sampling to separate estimation of Q^π_t(s,a) (off-policy) and α_t(s|o) (importance-weighted on-policy).

## Key Results
- Proved monotonic improvement and convergence to locally optimal memoryless policies under mild conditions (onto periodic sequence with no consecutive duplicate stages)
- Identified forward-backward sweep as optimal periodic pattern minimizing computational cost for time-invariant dynamics
- Demonstrated significant computational speedups over policy-gradient baselines and specialized methods in both model-based and model-free settings
- State-informed model-free approach outperformed alternatives when state information was available during learning

## Why This Works (Mechanism)

### Mechanism 1: Single-Stage Policy Improvement with Periodic Scheduling
The algorithm guarantees monotonic improvement by updating one stage at a time with immediate re-evaluation of affected quantities. When improving policy at stage τ_ℓ, changes propagate forward through μ_{τ_ℓ+1}, ..., μ_T but affect Q values only for t < τ_ℓ. By updating at one stage per iteration and immediately re-evaluating dependent quantities before the next improvement, each step is guaranteed to improve or maintain L^π. This addresses the non-Markovian nature of the output process.

### Mechanism 2: Forward-Backward Sweep Minimizes Recomputation Cost
Alternating forward and backward sweeps (0→T-1, then T-1→0) achieves minimal computational cost per policy improvement under time-invariant dynamics. The forward-backward pattern ensures |τ_{ℓ+1} - τ_ℓ| = 1 for all ℓ, minimizing the computational index C = (w_μ + w_Q)/2 · V/M where V is total variation across one period M. After each forward-step improvement, only one μ_t needs recomputation; after each backward-step improvement, only one Q_t needs recomputation.

### Mechanism 3: State-Informed Model-Free Estimation via Importance Sampling
The state-informed model-free method separates estimation of Q^π_t(s,a) (off-policy) and α_t(s|o) (importance-weighted on-policy) to enable efficient multi-stage improvement from a single dataset. Q^π_t(s,a) is estimated via bootstrapping from off-policy data, while α_t(s|o) is estimated recursively using importance sampling that corrects for behavior policy deviations. This decomposition allows reusing data across forward/backward sweeps.

## Foundational Learning

- **Concept**: Partial observability breaks the Markov property in observation space
  - Why needed here: The fundamental challenge is that the output process {O_t} is non-Markovian even when the underlying state process {S_t} is Markovian. This prevents direct application of standard policy iteration.
  - Quick check question: Given a POMDP with states {s₁, s₂}, observations {o₁, o₂}, and Pr(O_t=o₁|S_t=s₁)=0.9, Pr(O_t=o₁|S_t=s₂)=0.1, does observing o₁ tell you the current state with certainty?

- **Concept**: Classical policy iteration for MDPs
  - Why needed here: This work extends policy iteration to POMDPs. Understanding the MDP case (simultaneous all-stage improvement, Q-value computation, greedy policy update) clarifies what must change when observations are partial.
  - Quick check question: In an MDP with 3 states and 2 actions, how many Q-values must be computed per policy evaluation step?

- **Concept**: Belief states vs. memoryless policies
  - Why needed here: Optimal POMDP solutions require belief-state policies (exponentially complex). Memoryless policies trade optimality for tractability by mapping observations directly to actions.
  - Quick check question: If a POMDP has |S|=100 states and |O|=10 observations, what is the dimension of the belief simplex versus the number of parameters in a deterministic memoryless policy?

## Architecture Onboarding

- **Component map**: π^0 (initial policy) -> Q values (backward pass) -> μ_t (forward pass) -> π_t (policy improvement) -> Q values (backward pass) -> convergence check

- **Critical path**:
  1. Initialize π⁰, compute initial Q values (full backward pass)
  2. Forward sweep: for t=0 to T-2, improve π_t, update μ_{t+1}
  3. Backward sweep: for t=T-1 to 1, improve π_t, update Q_{t-1} backward
  4. Check convergence (L^π unchanged over M consecutive improvements)
  5. Repeat until stable

- **Design tradeoffs**:
  - **Deterministic vs. stochastic policies**: Deterministic guarantees predictable behavior and convergence to local optima, but stochastic policies may outperform in some non-Markovian settings
  - **State-informed vs. observation-only**: State access during training enables efficient multi-stage updates; observation-only requires data recollection after each improvement
  - **Period choice**: Minimal-period forward-backward pattern is computationally optimal but may converge to different local optima than other schedules

- **Failure signatures**:
  - No improvement after M consecutive updates: algorithm has converged (expected)
  - Non-monotonic L^π values: check that re-evaluation occurs between improvements (bug in update ordering)
  - Observation-only method stalls: insufficient data coverage or exploration (increase ε or batch size)
  - State-informed method underperforms: check α_t estimation accuracy; intermediate ε values (0.3-0.7) often outperform extremes

- **First 3 experiments**:
  1. **Reproduce Figure 3**: Implement model-based policy iteration on small random POMDP (|S|=20, |A|=2, |O|=4, T=6). Verify monotonic convergence and compare iteration count to policy gradient. Expected: ~11 stage improvements to convergence.
  2. **Ablate periodic pattern**: Compare forward-backward sweep vs. forward-only sweep vs. random stage selection. Measure total computational operations and final L^π. Expected: forward-backward achieves lowest cost index C.
  3. **Model-free state-informed vs. baselines**: Implement Algorithm 2 with ε=0.5, batch size 5000 episodes. Compare against LSPI and REINFORCE on random POMDP (|S|=5, |A|=5, |O|=5, T=10). Expected: state-informed method reaches higher L^π faster.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the periodic policy iteration scheme be extended to infinite-horizon POMDPs while retaining monotonic improvement guarantees?
- Basis in paper: The conclusion states, "Directions for future research include extensions to infinite horizon..."
- Why unresolved: The current proofs of monotonic improvement and convergence (Theorem 1) rely on the finite episodic structure (horizon $T$) and backward recursion of value functions.
- What evidence would resolve it: A proof of convergence for average-reward or discounted infinite-horizon criteria, potentially requiring a different policy evaluation operator than the backward sweep.

### Open Question 2
- Question: How can the memoryless policy iteration framework be adapted for continuous state, action, and observation spaces?
- Basis in paper: The conclusion lists "extensions to... continuous state, action and observation spaces" as a direction for future research.
- Why unresolved: The algorithm currently relies on discrete summations over finite sets $\mathcal{S}, \mathcal{A}, \mathcal{O}$ (e.g., Eq. 3 and 6) and deterministic maximization over a discrete action set.
- What evidence would resolve it: A variant of the algorithm using function approximation (e.g., neural networks) for the Q-function and state distribution, along with convergence analysis.

### Open Question 3
- Question: Does the monotonic convergence property hold when using approximate policy iteration for large-scale problems?
- Basis in paper: The authors note that "extensions to approximate policy iteration methods will allow handling much larger problems."
- Why unresolved: The current theoretical guarantees assume exact policy evaluation; approximation errors in $\hat{Q}$ or $\hat{\alpha}$ could destabilize the periodic improvement schedule.
- What evidence would resolve it: Theoretical bounds on approximation errors that still ensure policy improvement, or empirical studies showing convergence in high-dimensional environments.

## Limitations

- The forward-backward pattern is proven optimal only for time-invariant problems; performance under time-varying dynamics remains unexplored
- The state-informed model-free approach requires access to true states during training, limiting applicability in many real-world settings
- The computational cost analysis assumes constant per-stage weights, which may not hold in practice

## Confidence

- **High confidence**: Monotonic improvement theorem (Theorem 1) and convergence to locally optimal policies; these follow directly from the proof structure
- **Medium confidence**: Computational optimality of forward-backward sweep (Theorem 2); while mathematically proven, practical benefits depend on accurate weight estimation
- **Low confidence**: Generalizability of results to large-scale problems and continuous observation spaces; experiments focus on discrete POMDPs with relatively small state spaces

## Next Checks

1. Test Algorithm 1 on time-varying POMDPs (e.g., seasonal dynamics) to evaluate forward-backward pattern performance when the time-invariance assumption breaks
2. Implement state-only model-free variant (no state information during training) to compare against state-informed approach and quantify the performance gap in real-world scenarios
3. Scale experiments to larger state spaces (|S| > 1000) and measure how computational advantages scale, particularly examining the relative cost of Q-value estimation versus policy improvement as problem size grows