---
ver: rpa2
title: 'FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity Analysis'
arxiv_id: '2503.10567'
source_url: https://arxiv.org/abs/2503.10567
tags:
- data
- clients
- fairness
- mislabeled
- rare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving fairness and robustness
  in federated learning (FL) when data is imbalanced and some clients have mislabeled
  data. Existing fair FL methods struggle with label noise, while robust FL techniques
  often compromise fairness.
---

# FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity Analysis

## Quick Facts
- **arXiv ID:** 2503.10567
- **Source URL:** https://arxiv.org/abs/2503.10567
- **Reference count:** 40
- **Primary result:** FedPCA achieves better worst-case and average accuracy than state-of-the-art fair and robust FL methods by jointly addressing label noise and data imbalance

## Executive Summary
This paper addresses the challenge of achieving fairness and robustness in federated learning when data is imbalanced and some clients have mislabeled data. Existing fair FL methods struggle with label noise, while robust FL techniques often compromise fairness. The authors propose performance-capacity analysis, which combines loss and a new feature dispersion score to differentiate mislabeled clients from those with rare data. Based on this analysis, they introduce FedPCA, an FL method that first identifies mislabeled clients using Gaussian Mixture Models, then applies fairness and robustness strategies in both global aggregation and local training by adjusting client weights and selectively using reliable data. Experiments on three datasets demonstrate that FedPCA effectively addresses the competition between fairness and robustness, achieving better worst-case and average accuracy compared to state-of-the-art methods.

## Method Summary
FedPCA operates by decoupling model performance (measured by loss) from model capacity (measured by feature dispersion score) to identify mislabeled clients. The server collects loss and dispersion metrics from all clients, fits a 3-component Gaussian Mixture Model to cluster clients into common, rare, and mislabeled groups, then applies weighted aggregation that prioritizes rare data for fairness while down-weighting mislabeled data for robustness. During local training, mislabeled clients either drop their entire dataset or use high-confidence sampling to filter out noisy labels. The method requires only minor modifications to standard FL frameworks and introduces two hyperparameters: the fairness weight $q$ and the confidence threshold $\tau$.

## Key Results
- FedPCA achieves 2-3% improvement in worst-case accuracy compared to state-of-the-art fair and robust FL methods
- The method maintains competitive average accuracy while improving fairness metrics
- Ablation studies confirm both the fairness component (dispersion weighting) and robustness component (noise filtering) are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Performance-Capacity Decoupling
The paper proposes that mislabeled clients and rare data clients, which exhibit similar high loss patterns, can be distinguished by analyzing the divergence between model performance (loss) and model capacity (feature dispersion). Rare data clients suffer from insufficient learning, resulting in high loss and low feature separation (low dispersion). In contrast, mislabeled clients suffer from label errors; the model learns to separate features well (high dispersion), but the labels contradict these features, resulting in high loss. By mapping clients onto a 2D plane of loss vs. dispersion, mislabeled clients appear as outliers in the "upper-right" region (high loss, high dispersion).

### Mechanism 2: Gaussian Mixture Model (GMM) Partitioning
A 3-component GMM applied to the loss-dispersion pairs reliably partitions the client population into common, rare, and mislabeled groups without prior knowledge of noise ratios. The algorithm identifies the centroid that deviates "upward and rightward" relative to the line connecting the other two centroids as the mislabeled cluster ($S_n$).

### Mechanism 3: Selective Aggregation and Training
Fairness and robustness are achieved simultaneously by down-weighting clients with low capacity/reliability (mislabeled) while up-weighting clients with low dispersion (rare/hard) in global aggregation. Aggregation weights are calculated as $w_{t,k} \propto \hat{N}_{t,k} \cdot r_{t,k} \cdot \exp(-q \cdot S_t)$. The term $\exp(-q \cdot S_t)$ assigns higher weights to clients with lower dispersion scores (rare data), enforcing fairness. The term $\hat{N}_{t,k} \cdot r_{t,k}$ scales by the volume of "reliable" data, enforcing robustness by suppressing mislabeled clients.

## Foundational Learning

- **Concept: Gaussian Mixture Models (GMM)**
  - Why needed here: To automatically cluster clients into three distinct groups (common, rare, mislabeled) based on the 2D metrics without manual thresholding.
  - Quick check question: Can you explain how Expectation-Maximization (EM) assigns a probability distribution to data points in 2D space, and why a 3-component model is chosen over 2 here?

- **Concept: Feature Dispersion / Representation Geometry**
  - Why needed here: This is the novel proxy for "model capacity." You must understand how inter-class distance in the feature space relates to classification difficulty (Bayes error bounds).
  - Quick check question: If features from different classes are highly overlapping in the latent space, would the dispersion score be high or low, and what does this imply about the model's ability to learn that data?

- **Concept: Federated Aggregation Weights**
  - Why needed here: The core intervention happens in the $w_{t,k}$ calculation. You need to understand how scalar weights linearly combine client model updates.
  - Quick check question: In standard FedAvg, weights are proportional to dataset size. How does multiplying by $\exp(-q \cdot Score)$ change the influence of a client with a very low dispersion score?

## Architecture Onboarding

- **Component map:** Clients -> LossModule, DispersionModule, Trainer -> Server -> MetricCollector, GMMClassifier, Aggregator -> Global Model
- **Critical path:** 
  1. Accurate calculation of the Feature Dispersion Score (Eq 4) on local data
  2. Server-side fitting of the GMM to ensure $S_n$ (mislabeled) is correctly identified as the upper-right cluster
  3. Smooth initialization (warm-up) to ensure features are meaningful before applying the logic
- **Design tradeoffs:**
  - FedPCA (D) vs. (HS): The (D) strategy (Drop) is safer but wastes potentially recoverable data. The (HS) strategy (High-Confidence Sampling) recovers data but risks "confirmation bias" if the model is confidently wrong
  - Hyperparameter $q$: Controls the strength of fairness. High $q$ forces the model to focus heavily on low-dispersion (rare) clients, potentially hurting average performance
- **Failure signatures:**
  - GMM Instability: If the global model drifts rapidly, the cluster centroids may shift, causing "flapping" where a client is treated as rare in round $t$ and mislabeled in $t+1$
  - Warm-up Collapse: If warm-up rounds are too few, features are random, making the dispersion score meaningless
- **First 3 experiments:**
  1. Visual Validation: Reproduce Fig 3. Scatter plot (Loss vs Dispersion) for clients. Verify that mislabeled clients actually occupy the upper-right quadrant
  2. Ablation on $q$: Run a sweep on $q$ (e.g., 0.0 to 5.0) on RSNA ICH to see the trade-off between Worst-Case ACC (Fairness) and Average ACC
  3. Component Isolation: Test FedPCA with only the robustness component (disable dispersion weighting) vs. only fairness (disable noise filtering) to confirm both are required for the result

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The feature dispersion metric's effectiveness as a proxy for model capacity requires stronger theoretical grounding
- The 2D loss-dispersion space may not always cleanly separate mislabeled from rare data clients, particularly when noise rates are high or data distributions are complex
- The choice of specific ResNet layer for feature extraction (only "middle-layer" specified) introduces potential variability in results

## Confidence
- **High:** The overall framework architecture (GMM clustering + weighted aggregation) is technically sound and well-implemented
- **Medium:** The feature dispersion metric's effectiveness as a proxy for model capacity requires stronger theoretical grounding
- **Medium:** The claim of achieving both fairness and robustness simultaneously is supported by experiments but may be sensitive to hyperparameter settings

## Next Checks
1. Visual Validation: Reproduce Figure 3 to verify that mislabeled clients consistently occupy the upper-right quadrant in the loss-dispersion space across different noise levels and datasets
2. Metric Sensitivity Analysis: Test FedPCA's performance when feature dispersion is calculated from different ResNet layers to assess robustness to architectural choices
3. Distribution Overlap Assessment: Systematically vary the ratio of mislabeled to rare clients and measure GMM classification accuracy to quantify the method's limitations under challenging conditions