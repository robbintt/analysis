---
ver: rpa2
title: Magistral
arxiv_id: '2506.10910'
source_url: https://arxiv.org/abs/2506.10910
tags:
- medium
- magistral
- training
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Magistral introduces a scalable RL pipeline for training reasoning
  models from scratch without distillation, achieving a 50% boost in AIME-24 accuracy
  over the base model. It employs an asynchronous distributed RL system with Group
  Relative Policy Optimization (GRPO), enhanced with modifications such as removing
  KL divergence, loss normalization, advantage normalization, and higher clipping
  thresholds for better exploration.
---

# Magistral

## Quick Facts
- arXiv ID: 2506.10910
- Source URL: https://arxiv.org/abs/2506.10910
- Reference count: 11
- Primary result: 50% boost in AIME-24 accuracy over base model using pure RL

## Executive Summary
Magistral introduces a scalable reinforcement learning (RL) pipeline for training reasoning models from scratch without distillation, achieving significant improvements in mathematical reasoning. The approach employs an asynchronous distributed RL system with Group Relative Policy Optimization (GRPO), enhanced with several modifications including KL divergence removal, loss normalization, advantage normalization, and higher clipping thresholds. A simple multilingual reward mechanism ensures reasoning occurs in the user's language while preserving multimodal reasoning, instruction following, and function calling capabilities.

## Method Summary
Magistral's approach trains reasoning models from scratch using a pure RL pipeline without distillation. The system uses an asynchronous distributed RL architecture with Group Relative Policy Optimization (GRPO), enhanced with modifications like removing KL divergence loss, implementing loss normalization, advantage normalization, and increasing clipping thresholds to encourage better exploration. A multilingual reward mechanism ensures reasoning happens in the user's language. The pipeline is scalable and trains models from scratch, with Magistral Medium reaching 90% accuracy on AIME-24 with majority voting, while Magistral Small is open-sourced under Apache 2.0.

## Key Results
- 50% boost in AIME-24 accuracy over base model
- Magistral Medium reaches 90% accuracy on AIME-24 with majority voting
- Pure RL training from scratch without distillation
- Preservation or improvement of multimodal reasoning, instruction following, and function calling capabilities

## Why This Works (Mechanism)
The RL pipeline's effectiveness stems from several key mechanisms. First, removing KL divergence loss allows the model to explore more freely without being constrained by the base model's distribution. Loss normalization and advantage normalization stabilize training and prevent gradient explosion. Higher clipping thresholds enable more aggressive exploration of the solution space. The asynchronous distributed system allows for more efficient parallel training. The multilingual reward mechanism ensures the model reasons in the user's language, improving accessibility and performance across different linguistic contexts.

## Foundational Learning

**Reinforcement Learning**: A machine learning paradigm where agents learn to make decisions by taking actions in an environment to maximize cumulative reward. Needed to enable models to learn reasoning strategies through trial and error rather than supervised learning. Quick check: Does the agent improve its performance on reasoning tasks over training iterations?

**Group Relative Policy Optimization (GRPO)**: An RL algorithm that normalizes advantages across a group of trajectories to reduce variance. Needed to stabilize training and make learning more sample-efficient. Quick check: Are advantage estimates properly normalized across trajectory batches?

**Asynchronous Distributed Training**: A training approach where multiple workers update a central model asynchronously. Needed to scale RL training across many GPUs and reduce wall-clock training time. Quick check: Does the asynchronous update scheme maintain training stability?

## Architecture Onboarding

**Component Map**: Data Pipeline -> RL Engine (GRPO) -> Model Parameters -> Reward Function -> Feedback Loop

**Critical Path**: User query → Model reasoning → Reward computation → Policy update → New model parameters

**Design Tradeoffs**: The pure RL approach sacrifices some supervised learning benefits (faster convergence, better sample efficiency) for greater flexibility and the ability to train from scratch. Asynchronous training improves speed but may introduce staleness. The multilingual reward mechanism adds complexity but improves accessibility.

**Failure Signatures**: 
- Poor reasoning performance may indicate insufficient exploration due to overly conservative clipping thresholds
- Training instability could result from improper advantage normalization
- Language-specific reasoning failures might indicate issues with the multilingual reward mechanism
- Degradation in non-reasoning capabilities suggests interference from RL training

**First Experiments**:
1. Measure AIME-24 accuracy improvement over baseline model
2. Test multilingual reasoning across different language datasets
3. Evaluate preservation of instruction following and function calling capabilities

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the methodology raises several implicit ones. How well does this approach generalize to non-mathematical reasoning tasks? What are the scaling limits of the asynchronous distributed system? How does the pure RL approach compare to hybrid methods that combine supervised and reinforcement learning? What is the optimal balance between exploration and exploitation in the clipping threshold settings?

## Limitations

- Lack of detailed ablation studies isolating the contribution of each RL modification
- Limited quantitative validation of preserved capabilities (multimodal reasoning, instruction following, function calling)
- No empirical testing of multilingual reward mechanism across diverse language datasets
- Scalability assertions not tested on larger model sizes or more complex reasoning tasks

## Confidence

**AIME-24 accuracy improvement**: Medium - Significant results reported but methodological details limited
**Preservation of other capabilities**: Medium - Claims made but lacking comprehensive quantitative validation
**Multilingual effectiveness**: Medium - Mechanism described but not thoroughly validated across languages
**Scalability claims**: Medium - System described but not empirically tested at larger scales

## Next Checks

1. Conduct ablation studies to isolate the impact of each RL modification (KL removal, loss normalization, advantage normalization, and higher clipping thresholds) on AIME-24 performance.

2. Validate the preservation of multimodal reasoning, instruction following, and function calling capabilities using standardized benchmarks and comparative analysis with baseline models.

3. Test the multilingual reward mechanism across diverse language datasets and user studies to assess its effectiveness and generalizability.