---
ver: rpa2
title: Single Image Reflection Removal via inter-layer Complementarity
arxiv_id: '2505.12641'
source_url: https://arxiv.org/abs/2505.12641
tags:
- reflection
- transmission
- prior
- layer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of single image reflection removal
  by proposing a novel dual-prior interaction framework that effectively models inter-layer
  complementarity. The core method introduces a lightweight Local Linear Correction
  Network (LLCN) that generates high-quality transmission priors through adaptive
  linear correction of the blended image, and a Dual-Stream Channel Reorganization
  Attention (DSCRAB) mechanism that achieves deep fusion of general and transmission
  priors for superior separation quality.
---

# Single Image Reflection Removal via inter-layer Complementarity

## Quick Facts
- arXiv ID: 2505.12641
- Source URL: https://arxiv.org/abs/2505.12641
- Reference count: 40
- Key outcome: State-of-the-art performance with PSNR 27.21 dB and SSIM 0.924 across five benchmark datasets

## Executive Summary
This paper introduces a novel dual-prior interaction framework for single image reflection removal that effectively models inter-layer complementarity. The core innovation is a lightweight Local Linear Correction Network (LLCN) that generates high-quality transmission priors through adaptive linear correction, combined with a Dual-Stream Channel Reorganization Attention (DSCRAB) mechanism for deep fusion of heterogeneous priors. The method achieves state-of-the-art performance while maintaining computational efficiency with only 131.54M parameters and 191.35G FLOPs.

## Method Summary
The method consists of two main components: (1) a Local Linear Correction Network that learns pixel-wise scaling and bias to generate transmission priors via T = S⊙I + B, and (2) a Dual-Prior Interaction Transformer with DSCRAB modules that fuse general and transmission priors through cross-stream channel reorganization and window-based attention. The approach uses a two-stage training strategy where LLCN is trained independently before being integrated with the DPIT for fine-tuning, allowing the transmission prior to provide explicit spatial constraints on the target layer.

## Key Results
- Achieves state-of-the-art PSNR of 27.21 dB and SSIM of 0.924 across five benchmark datasets
- Local linear correction (T=sI+b) outperforms end-to-end generation (T=fθ(I)) by 1.74 dB while using similar parameters
- DSCRAB improves fusion performance by 0.36 dB over simpler interaction modules
- Maintains computational efficiency with only 131.54M parameters and 191.35G FLOPs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining transmission prior generation to adaptive linear correction of the blended image produces higher-quality priors with fewer parameters than end-to-end generation.
- Mechanism: The LLCN learns pixel-wise scaling S and bias B to apply to input I via T = S⊙I + B, rather than directly regressing T. This "selection-rather-than-generation" approach reduces the learning objective from full reconstruction to information extraction.
- Core assumption: The blended image I contains sufficient information about the transmission layer that local linear transformations can recover it.
- Evidence anchors:
  - [abstract] "LLCN efficiently generates high-quality transmission priors with minimal parameters"
  - [Section III-B] "This parameterization simplifies the learning objective from complete transmission layer reconstruction to effective information extraction from the blended image"
  - [Table III] Local linear correction (T=sI+b) achieves 26.12 dB vs. end-to-end (T=fθ(I)) at 24.38 dB with similar parameters
- Break condition: If the transmission layer has been non-linearly distorted beyond what local affine transformations can correct (e.g., severe overexposure, complex scattering), this mechanism will underperform.

### Mechanism 2
- Claim: Cross-stream channel reorganization before attention computation enables deeper fusion of heterogeneous priors than standard dual-stream processing.
- Mechanism: DSCRAB splits each stream's channels, then cross-concatenates halves to form generation and exchange streams. Queries come from the generation stream; keys/values come from both streams. This forces explicit cross-prior interaction rather than parallel processing.
- Core assumption: General prior features and transmission prior features encode complementary information that benefits from channel-level mixing before attention.
- Evidence anchors:
  - [abstract] "Dual-Stream Channel Reorganization Attention mechanism that achieves deep fusion of general and transmission priors"
  - [Section III-C] Equations 11-16 detail the chunk, cross-concatenation, and dual-attention computation
  - [corpus] Weak direct evidence—neighbor papers focus on different fusion strategies; no direct comparison to channel reorganization available
- Break condition: If priors are misaligned (e.g., transmission prior is consistently wrong), channel reorganization may propagate errors rather than provide complementary constraints.

### Mechanism 3
- Claim: Introducing explicit transmission priors consistently improves separation quality across different dual-stream interaction architectures.
- Mechanism: The transmission prior provides direct spatial constraints on the target layer, complementing general semantic priors from pre-trained models that lack task-specific physical understanding.
- Core assumption: The LLCN-generated transmission prior is sufficiently accurate to provide useful guidance rather than noise.
- Evidence anchors:
  - [Table IV] Adding transmission prior improves all tested interaction modules: MuGI (+0.60 dB), DAIB (+0.41 dB), DSCRAB (+0.36 dB)
  - [Section I] "the transmission prior, as the most direct task-specific prior for the target transmission layer, has not been effectively modeled or fully utilized"
  - [corpus] Neighbor paper "SIRR-LMM" uses multimodal priors for reflection removal, supporting the value of task-specific prior information, though not transmission-specific
- Break condition: If the transmission prior generation fails (e.g., on extreme lighting conditions), downstream fusion may amplify artifacts rather than correct them.

## Foundational Learning

- Concept: Physical model of reflection (I = T + R and extensions)
  - Why needed here: The entire method is built on modeling the mixing process; LLCN uses T = S⊙I + B as an approximation
  - Quick check question: Can you explain why the linear additive model I = T + R is insufficient for real-world reflection, and how the paper's local linear model addresses this?

- Concept: Window-based attention in Vision Transformers
  - Why needed here: DSCRAB uses window partition and relative position biases; understanding window attention is essential for debugging the fusion mechanism
  - Quick check question: How does window attention differ from global attention, and what are the computational tradeoffs?

- Concept: Dual-stream architectures for image restoration
  - Why needed here: The paper explicitly positions itself as improving dual-stream designs; understanding prior work (YTMT, DSIT) clarifies what DSCRAB does differently
  - Quick check question: What is the key limitation of existing dual-stream architectures that this paper identifies, and how does channel reorganization address it?

## Architecture Onboarding

- Component map:
  - **LLCN** (99.44M params, 24.10G FLOPs): ConvNeXt-Base backbone → two parallel decoders → S (sigmoid, [0,1]) and B (tanh, [-1,1]) → prior = S⊙I + B
  - **General Prior Feature Extraction**: Pre-trained Swin Transformer → multi-scale features (F²g through F⁵g)
  - **Transmission Prior Feature Extraction (TPFEN)**: Dual-stream conv stems → MuGI blocks → multi-scale transmission features
  - **DPFEN (U-shaped)**: Same-layer fusion via DSCRAB at each scale → cross-layer fusion with upsampled features → final output via MuGI + Conv
  - **LRM module**: Estimates non-linear residual Φ for I = T + R + Φ(T,R) model

- Critical path:
  1. Input I → LLCN → T_prior (transmission prior image)
  2. I + T_prior → TPFEN → multi-scale transmission features
  3. I → Swin → multi-scale general features
  4. Both feature streams → DPFEN with DSCRAB fusion → T_hat, R_hat, Φ_hat

- Design tradeoffs:
  - LLCN uses ConvNeXt-Base (heavier backbone) but avoids end-to-end generation; this trades model size for simpler learning objective
  - DSCRAB adds computational overhead vs. simpler interaction modules (MLP, MuGI) but achieves better fusion; the ablation shows it's worth it
  - Two-stage training (LLCN first, then combined DPIT) decouples prior quality from fusion learning

- Failure signatures:
  - Transmission prior too weak: LLCN outputs near-uniform S≈1, B≈0 → T_prior ≈ I → no guidance benefit
  - Prior-feature misalignment: DSCRAB fusion produces ghosting or double edges in output
  - Over-smoothing: Perceptual loss weight too low → washed-out textures
  - Non-linear residual overfitting: Φ learns to encode transmission/reflection rather than true non-linear effects

- First 3 experiments:
  1. **LLCN isolation test**: Train LLCN alone, visualize S and B maps on sample images. If S is nearly uniform or B has high variance in non-reflective regions, the local linear model isn't learning meaningful corrections.
  2. **DSCRAB ablation with frozen LLCN**: Freeze LLCN weights, train DPIT with and without transmission prior (per Table IV). Confirm the ~0.36 dB gain holds in your setup before full training.
  3. **Cross-dataset sanity check**: Train on synthetic-only data, test on Real20. A large performance drop indicates the local linear model overfits to synthetic blending characteristics (γ₁, γ₂ ranges in Eq. 27).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the local linear correction framework be effectively extended to other low-level vision tasks such as deraining, dehazing, and deshadowing?
- Basis in paper: [explicit] The conclusion states: "Future research will unfold in two dimensions: on one hand, extending the local linear correction concept to low-level vision tasks such as deraining, dehazing, and deshadowing, exploring a lightweight prior generation framework combining physical constraints with pre-trained model finetuning."
- Why unresolved: The paper only validates the local linear correction model on reflection removal; the physical models for rain, haze, and shadows differ fundamentally from reflection superposition.
- What evidence would resolve it: Experiments applying LLCN-style correction to synthetic rainy, hazy, and shadowed datasets, comparing against task-specific architectures.

### Open Question 2
- Question: How can the dual-prior interaction mechanism be generalized to multi-prior collaborative scenarios with heterogeneous prior types?
- Basis in paper: [explicit] The conclusion states: "expanding the dual-prior interaction mechanism to multi-prior collaborative scenarios, constructing a unified interaction paradigm for image restoration tasks requiring fusion of multiple heterogeneous priors."
- Why unresolved: DSCRAB is designed for two streams (general and transmission priors); extending to three or more priors with different modalities (e.g., depth, polarization, language) requires architectural redesign.
- What evidence would resolve it: A unified framework demonstrating effective fusion of three or more heterogeneous priors with quantitative improvements over pairwise combinations.

### Open Question 3
- Question: Does end-to-end joint training of LLCN and DPIT improve performance compared to the current two-stage strategy?
- Basis in paper: [inferred] The training strategy separately optimizes LLCN (80 epochs) and DPIT (80 epochs), then fine-tunes combined (20 epochs). This decoupled approach may prevent gradient flow from the final separation objective back to the prior generator.
- Why unresolved: The ablation studies only vary the interaction module and prior inclusion, not the training paradigm itself.
- What evidence would resolve it: Comparative experiments with end-to-end training from initialization, measuring convergence speed and final PSNR/SSIM.

## Limitations
- The DSCRAB mechanism's effectiveness relies heavily on LLCN prior quality, but limited ablation shows LLCN performance in isolation
- Window size and partitioning strategy for DSCRAB are specified but not experimentally validated for different configurations
- Two-stage training assumes LLCN priors remain stable during joint fine-tuning, but this coupling is not explicitly tested

## Confidence

- **High**: LLCN outperforms end-to-end generation for transmission priors (direct PSNR comparison provided)
- **Medium**: DSCRAB provides meaningful improvement over simpler fusion modules (consistent gains across architectures but weaker direct ablation)
- **Medium**: Transmission priors consistently improve dual-stream architectures (averaged across modules but individual module variance not shown)

## Next Checks

1. **LLCN isolation validation**: Train LLCN independently and visualize S and B output distributions on both synthetic and real images to verify the local linear model learns meaningful corrections beyond uniform scaling.

2. **DSCRAB sensitivity analysis**: Vary window size (8×8, 16×16, 32×32) and channel chunk ratios to identify optimal configuration and test robustness to these hyperparameters.

3. **Prior quality degradation test**: Intentionally degrade LLCN outputs (add noise, constrain S to uniform values) and measure DPIT performance drop to quantify dependence on prior quality.