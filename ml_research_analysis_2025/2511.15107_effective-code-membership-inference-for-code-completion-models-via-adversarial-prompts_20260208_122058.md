---
ver: rpa2
title: Effective Code Membership Inference for Code Completion Models via Adversarial
  Prompts
arxiv_id: '2511.15107'
source_url: https://arxiv.org/abs/2511.15107
tags:
- uni00000013
- code
- uni00000011
- latexit
- uni0000001c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AdvPrompt-MIA, a novel membership inference
  attack method for code completion models that leverages semantics-preserving adversarial
  prompts to detect memorization. Unlike existing approaches that rely on expensive
  shadow models or simple output matching, AdvPrompt-MIA applies five types of code-specific
  perturbations (dead code insertion, variable renaming, debug statements, etc.) to
  generate 11 perturbed variants per input, then analyzes the model's behavioral consistency
  under these perturbations.
---

# Effective Code Membership Inference for Code Completion Models via Adversarial Prompts

## Quick Facts
- arXiv ID: 2511.15107
- Source URL: https://arxiv.org/abs/2511.15107
- Reference count: 40
- Primary result: Achieves AUC gains of up to 102% over state-of-the-art baselines using semantics-preserving adversarial prompts

## Executive Summary
This paper introduces AdvPrompt-MIA, a novel membership inference attack method for code completion models that leverages semantics-preserving adversarial prompts to detect memorization. Unlike existing approaches that rely on expensive shadow models or simple output matching, AdvPrompt-MIA applies five types of code-specific perturbations (dead code insertion, variable renaming, debug statements, etc.) to generate 11 perturbed variants per input, then analyzes the model's behavioral consistency under these perturbations. The method extracts feature vectors combining similarity and perplexity metrics from the original and perturbed outputs, and trains a deep learning classifier to distinguish member from non-member samples. Evaluated on Code Llama 7B across APPS and HumanEval benchmarks, AdvPrompt-MIA achieves AUC gains of up to 102% over state-of-the-art baselines, demonstrating strong generalizability across multiple code models and datasets.

## Method Summary
AdvPrompt-MIA generates 11 perturbed variants per input using five perturbation types (IDC, IRV, VR, IDP, IDL with 2/2/2/2/3 variants each). It extracts a 27-dimensional feature vector combining mean and standard deviation of cosine similarities and normalized perplexities across variants. A 3-layer MLP classifier (hidden=512) is trained to distinguish member from non-member samples. The method assumes partial knowledge (20% of training data) to train the classifier, and is evaluated on code completion models including Code Llama 7B, Deepseek-Coder 7B, StarCoder2 7B, Phi-2 2.7B, and WizardCoder 7B, fine-tuned on APPS and HumanEval datasets.

## Key Results
- Achieves AUC gains of up to 102% over state-of-the-art baselines
- Cross-dataset transferability: Classifier trained on APPS achieves 0.94 AUC on HumanEval
- Cross-model transferability: Training on Code Llama and testing on StarCoder2 yields >0.90 AUC
- Variance in similarity and perplexity scores across perturbations provides the strongest membership signal

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Induced Stability Differential
- **Claim:** If a code sample is a training set member, the model's output remains semantically consistent even when the input is modified with semantics-preserving perturbations; non-member outputs vary significantly.
- **Mechanism:** The method applies code transformations (e.g., inserting dead loops, renaming variables) that preserve functionality but alter token sequences. For memorized training data, the model has robust internal representations that "resist" these surface changes, whereas for unseen data, the perturbations disrupt the inference process, leading to higher variance in the output.
- **Core assumption:** Memorized code samples occupy a stable region in the model's latent space, causing the model to revert to the ground truth completion despite input noise, whereas generalization on non-members is more fragile to structural changes.
- **Evidence anchors:**
  - [Abstract]: Mentions leveraging "semantics-preserving adversarial prompts" to detect memorization.
  - [Section II-B]: Explicitly motivates the method with the observation that "membership signals are more robust under semantics-preserving perturbations."
  - [Corpus]: Related work (Neural Breadcrumbs) suggests MIAs often struggle with global signals; this mechanism relies on local behavioral consistency.
- **Break condition:** If the target model is robustly trained with heavy augmentation (specifically against these code transformations), the stability gap between members and non-members may vanish.

### Mechanism 2: Distributional Feature characterization
- **Claim:** Variance in similarity and perplexity scores across multiple perturbations provides a stronger membership signal than single-pass metrics.
- **Mechanism:** Instead of checking if one output matches the ground truth, the method constructs a 27-dimensional feature vector. This includes the *standard deviation* ($\sigma$) of similarities and perplexities across 11 perturbed variants. High variance implies non-membership (instability), while low variance implies membership.
- **Core assumption:** The *consistency* of the model's error/confidence pattern across a distribution of perturbations is a discriminative fingerprint of training data exposure.
- **Evidence anchors:**
  - [Section IV-B]: Defines the feature vector $\phi(x, y)$ including $\mu$ and $\sigma$ for both similarity and perplexity.
  - [Figure 8]: Ablation study showing that removing $\sigma(sim)$ (standard deviation of similarity) causes the largest performance drop.
- **Break condition:** If the perturbations are too weak to affect non-members (ceiling effect) or too strong that they break the semantic logic of members (floor effect), the distributional statistics will fail to separate classes.

### Mechanism 3: Partial Knowledge Transferability
- **Claim:** A classifier trained on feature vectors from one model/dataset can effectively infer membership for a different target model or dataset.
- **Mechanism:** Because the features capture fundamental behavioral properties (stability under perturbation) rather than model-specific weights, the decision boundary learned by the MLP is portable across different code LLMs (e.g., training on Code Llama, attacking DeepSeek).
- **Core assumption:** Different code LLMs exhibit similar "memorization signatures" (stability patterns) when exposed to training data, creating a shared feature space.
- **Evidence anchors:**
  - [Table III]: Shows cross-dataset results where a classifier trained on APPS achieves 0.94 AUC when evaluated on HumanEval (Code Llama).
  - [Figure 10]: Demonstrates cross-model transferability, e.g., training on Code Llama and testing on StarCoder2 yields >0.90 AUC.
- **Break condition:** If the target model has a fundamentally different architecture (e.g., non-Transformer) or tokenization strategy that responds erratically to the perturbations, the transferability will degrade.

## Foundational Learning

- **Concept: Membership Inference Attacks (MIA)**
  - **Why needed here:** The core objective is to determine if a specific code snippet was in the training set. Understanding MIA helps distinguish this from "extraction" (stealing the model) or "evasion" (breaking the model).
  - **Quick check question:** Is the goal to reconstruct the training data or to determine the *probability* that a specific known sample was seen during training?

- **Concept: Perplexity in Language Models**
  - **Why needed here:** The paper uses perplexity as a proxy for the model's "surprise" or confidence. Members typically yield lower perplexity (or stable relative perplexity) compared to non-members.
  - **Quick check question:** If a model assigns low perplexity to a code snippet, does it definitively mean it memorized it, or could it just be "common" code? (Hint: This ambiguity is why the paper combines perplexity with perturbations).

- **Concept: Code Semantic Equivalence**
  - **Why needed here:** The adversarial perturbations (dead code, renaming) must not change the program's actual function. If the perturbations break the code, the model's output changes would be due to syntax errors, not memorization.
  - **Quick check question:** Why is "Variable Renaming" considered a semantics-preserving transformation, whereas changing a `+` to `-` is not?

## Architecture Onboarding

- **Component map:** Perturbation Engine -> Victim Model Interface -> Feature Extractor -> Attack Classifier
- **Critical path:** The generation of the 11 perturbed variants is the bottleneck. Each input requires 12 total queries to the victim model (1 original + 11 perturbed). Latency is dominated by the victim model's inference speed.
- **Design tradeoffs:**
  - **Perturbation intensity:** Using all 4 types of Dead Control Branches (IDC) did not improve performance over using 2 (Page 4), suggesting redundancy vs. compute cost trade-offs.
  - **Black-box vs. Gray-box:** The method assumes "partial knowledge" (20% of training data) to train the classifier. In a true zero-knowledge setting, performance drops (Table III suggests cross-dataset transfer is possible but slightly weaker).
- **Failure signatures:**
  - **High FPR (False Positive Rate):** Occurs if the model generalizes so well to common code patterns that it produces stable, ground-truth-matching outputs for non-members.
  - **Low TPR on Large Models:** If the model is heavily regularized or rarely memorizes, the "stability gap" between members and non-members closes.
- **First 3 experiments:**
  1. **Sanity Check (Figure 1 Reproduction):** Pick a sample from APPS (member) and HumanEval (non-member). Apply a single perturbation (e.g., Variable Renaming). Manually inspect if the member output stays similar while the non-member output drifts.
  2. **Feature Ablation (RQ2):** Run the classifier using *only* the standard deviation features ($\sigma$) vs. *only* the mean features ($\mu$) to confirm that variance is the dominant signal.
  3. **Transfer Test (RQ4):** Train the MLP on Code Llama features and immediately test on DeepSeek-Coder features without retraining to verify the "behavioral signature" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive, model-specific perturbation optimization strategies further enhance the attack's transferability and accuracy?
- **Basis in paper:** [Explicit] In Section VI-D, the authors note that different models respond differently to specific perturbations (e.g., IDL) and explicitly identify "adaptive model-specific perturbation optimization as a promising direction for future work."
- **Why unresolved:** The current approach uses a fixed set of five perturbation types applied uniformly, but the study shows the impact of these perturbations varies significantly across target models.
- **What evidence would resolve it:** A study demonstrating a dynamic perturbation selection mechanism that consistently outperforms the static baseline across diverse model architectures.

### Open Question 2
- **Question:** How effective is the adversarial prompt approach when targeting the pre-training or post-training (RLHF/RLVR) phases of code LLMs?
- **Basis in paper:** [Explicit] Section VIII (Threats to Validity) states that while the current work targets fine-tuning, "Extending to pretraining or post-training (e.g., RLHF/RLVR) is important but non-trivial due to different training paradigms."
- **Why unresolved:** Memorization behaviors and the efficacy of perturbation-based signals may differ fundamentally between fine-tuning on specific tasks versus massive-scale pre-training or reinforcement learning alignment.
- **What evidence would resolve it:** Experimental results showing the method's AUC performance when applied to models at various pre-training checkpoints or after reinforcement learning alignment.

### Open Question 3
- **Question:** To what extent do standard defense mechanisms mitigate the effectiveness of semantics-preserving adversarial prompts?
- **Basis in paper:** [Explicit] Section VIII acknowledges that experiments were conducted on "unprotected models," whereas models with "defense mechanisms may behave differently."
- **Why unresolved:** It is unknown if privacy-enhancing technologies (like differential privacy) or output perturbation defenses would mask the behavioral consistency required for the attack to succeed.
- **What evidence would resolve it:** An evaluation of AdvPrompt-MIA’s performance against victim models specifically fine-tuned with defense mechanisms such as differential privacy or output filtering.

### Open Question 4
- **Question:** Does the choice of code embedding model (e.g., CodeBERT) limit the robustness or accuracy of the feature extraction process?
- **Basis in paper:** [Explicit] In Section VIII, the authors state: "We acknowledge that alternative code models or similarity metrics may affect the attack’s effectiveness and will explore them in future work."
- **Why unresolved:** The reliance on CodeBERT for vectorizing outputs introduces a dependency; different embeddings might capture semantic similarity or perplexity features more effectively.
- **What evidence would resolve it:** Comparative analysis replacing CodeBERT with alternative encoders (e.g., GraphCodeBERT, UniXcoder) to measure the impact on membership inference accuracy.

## Limitations
- The perturbation generation requires 12 queries per input (1 original + 11 variants), creating significant computational overhead
- The method assumes 20% known training data for classifier training, though cross-dataset transferability partially addresses this
- The perturbations are specifically designed for code completion tasks and may not generalize to other domains
- While transferability is demonstrated, the assumption that all code LLMs exhibit similar "memorization signatures" needs further validation

## Confidence
- **High Confidence:** The mechanism that semantics-preserving perturbations create differential stability patterns between members and non-members is well-supported by ablation studies showing σ(sim) is the most critical feature
- **Medium Confidence:** The claim of strong cross-model transferability is supported by results showing >0.90 AUC when training on Code Llama and testing on StarCoder2, but generalization across more diverse architectures remains untested
- **Medium Confidence:** The claim that the 27-dimensional feature vector is optimal is supported by ablation studies, but the specific choice of combining 11 perturbations with particular statistics appears somewhat arbitrary

## Next Checks
1. **Zero-Knowledge Scalability Test:** Evaluate AdvPrompt-MIA performance when no training data is available to the attacker (true black-box setting), measuring both accuracy degradation and computational cost per inference
2. **Perturbation Robustness Analysis:** Systematically vary perturbation intensity and type to identify breaking points where the stability differential between members and non-members disappears, particularly testing against models with different regularization strategies
3. **Cross-Domain Generalization:** Apply the AdvPrompt-MIA framework to non-code domains (e.g., natural language, image captioning) with domain-appropriate perturbations to test whether the stability-under-perturbation mechanism is universally applicable or code-specific