---
ver: rpa2
title: 'LILO: Bayesian Optimization with Interactive Natural Language Feedback'
arxiv_id: '2510.17671'
source_url: https://arxiv.org/abs/2510.17671
tags:
- utility
- feedback
- outcomes
- optimization
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LILO, a language-in-the-loop optimization
  framework that uses large language models to convert natural language feedback into
  scalar utilities for Bayesian optimization. Unlike traditional preference-based
  BO methods that rely on structured pairwise comparisons, LILO allows decision makers
  to provide rich, unstructured feedback in natural language, which the LLM translates
  into consistent utility signals.
---

# LILO: Bayesian Optimization with Interactive Natural Language Feedback

## Quick Facts
- **arXiv ID:** 2510.17671
- **Source URL:** https://arxiv.org/abs/2510.17671
- **Reference count:** 40
- **Primary result:** LILO outperforms conventional BO baselines and LLM-only optimizers on synthetic and real-world problems by converting natural language feedback into scalar utilities for Bayesian optimization.

## Executive Summary
LILO introduces a language-in-the-loop optimization framework that leverages large language models to translate natural language feedback into scalar utilities for Bayesian optimization. Unlike traditional preference-based BO methods that rely on structured pairwise comparisons, LILO allows decision makers to provide rich, unstructured feedback in natural language. The LLM translates this feedback into consistent utility signals while maintaining the sample efficiency and uncertainty quantification of standard BO. Experiments demonstrate LILO's superior performance, particularly in feedback-limited regimes, and its ability to incorporate prior knowledge expressed in natural language.

## Method Summary
LILO uses an LLM agent to interface with decision makers, converting natural language feedback into pairwise preference labels that train Gaussian Process surrogate models. The system generates candidates using Log Noisy Expected Improvement and selects feedback pairs using Expected Utility of the Best Option. This decouples understanding (LLM) from optimizing (BO), preserving theoretical guarantees while enabling intuitive interaction. The framework supports both pairwise and scalar feedback modes, with pairwise showing more reliable utility estimates. LILO also incorporates prior knowledge through LLM-based initialization when available.

## Key Results
- LILO outperforms both conventional BO baselines and LLM-only optimizers across synthetic and real-world optimization problems
- Pairwise comparisons provide more reliable utility estimates than direct scalar predictions when using LLMs as proxy humans
- Incorporating prior knowledge through LLM-based initialization substantially improves starting points and overall optimization performance
- LILO maintains sample efficiency and principled uncertainty quantification of standard BO while enabling natural language interaction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If an LLM translates unstructured natural language into pairwise preference labels, then the system captures richer information than scalar ratings alone, leading to higher quality utility models.
- **Mechanism:** The LLM ingests high-level goals and specific feedback, outputting discrete pairwise preferences for experimental outcomes. These labels serve as low-noise training data for a Pairwise Gaussian Process, avoiding optimization instability seen with direct continuous score predictions.
- **Core assumption:** LLMs are more consistent and reliable at relative comparisons than absolute estimation when acting as proxy humans.
- **Evidence anchors:** Abstract states LILO "turns varied types of textual feedback into consistent utility signals." Section 4.3.1 shows pairwise comparisons provide more reliable utility estimates than direct scalar predictions.
- **Break condition:** If LLM exhibits high inconsistency in pairwise judgments (violating transitivity), the GP surrogate model will fit a noisy or distorted utility landscape, causing optimization divergence.

### Mechanism 2
- **Claim:** If the LLM is restricted to a supporting role (labeling data) rather than directly controlling optimization, then the framework retains sample efficiency and theoretical guarantees of standard BO.
- **Mechanism:** The system decouples "understanding" (LLM) from "optimizing" (BO). The LLM generates data points which are fed into a GP surrogate. The acquisition function uses the GP's uncertainty estimates to select next candidates, preventing optimization drift common in LLM-only approaches.
- **Core assumption:** The uncertainty quantification provided by the GP (based on LLM labels) is a sufficiently accurate proxy for true uncertainty in the user's utility function.
- **Evidence anchors:** Abstract states "our method maintains the sample efficiency and principled uncertainty quantification of BO." Section 1 explains how LILO preserves BO's calibrated uncertainty and acquisition mechanisms.
- **Break condition:** If LLM acts as a bottleneck that injects systematic bias, the GP will confidently optimize for the wrong objective, violating the "principled uncertainty" claim.

### Mechanism 3
- **Claim:** If the system elicits feedback via agentic questioning, then it can uncover latent user constraints and prior knowledge that would otherwise require manual kernel engineering.
- **Mechanism:** The LILO agent generates questions based on observed outcomes and conversation history. This interactive loop allows users to correct the optimization path or provide domain-specific hints. The LLM incorporates these priors to warm-start candidate generation.
- **Core assumption:** Users possess (and can articulate) useful priors or constraints in natural language that the LLM can successfully parse and convert into search-space constraints or sampling distributions.
- **Evidence anchors:** Section 3.2 notes "A natural language interface offers a more intuitive way for decision makers to express their prior knowledge." Section 4.3.3 shows "Incorporating prior knowledge through LLM-based initialization substantially improves the starting point."
- **Break condition:** If users provide misleading or incorrect prior information, LLM-based initialization may aggressively bias the search towards a suboptimal region, causing BO to get stuck in a local optimum.

## Foundational Learning

- **Gaussian Processes (GPs) for Preference Learning:** LILO relies on GP to model latent utility function using pairwise preference labels rather than direct scalar observations. Understanding how GPs handle non-Gaussian likelihoods is critical.
  - *Quick check:* How does a GP update its posterior mean and variance when fed a binary "A is better than B" label instead of a scalar score?

- **In-Context Learning (ICL) Limitations:** The paper positions LILO as a solution to "diminishing returns" and "poor generalization" of LLMs when used as direct optimizers via ICL. Distinguishing ICL from fine-tuning explains why the hybrid approach is necessary.
  - *Quick check:* Why might an LLM fail to improve its optimization suggestions after seeing more than 20 examples purely through in-context learning?

- **Acquisition Functions (LogNEI vs. EUBO):** LILO uses LogNEI for candidate generation and EUBO for feedback acquisition. Understanding the trade-off (exploration vs. exploitation) is vital to grasping why the loop converges.
  - *Quick check:* Why is Noisy Expected Improvement (NEI) preferred over standard Expected Improvement (EI) when utility labels come from a potentially noisy LLM?

## Architecture Onboarding

- **Component map:** LLM Agent -> Question Generation/Answer Interpretation/Utility Labeling -> Surrogate Models (M_x, M_y) -> Acquisition Engine (LogNEI, EUBO) -> Experiment Manager (f(x)) -> Outcomes
- **Critical path:**
  1. Initialization: LLM asks questions → User Answers → LLM samples initial x (if prior exists)
  2. Experimentation: Black-box evaluates x → Outcomes y observed
  3. Labeling: LLM selects pairs → LLM labels preferences based on DM's feedback history
  4. Model Update: Fit/Update GPs (M_x, M_y) using new labels
  5. Optimization: Optimize acquisition function → Select next batch of x
- **Design tradeoffs:**
  - Pairwise vs. Scalar: Pairwise is more reliable but requires K LLM calls per iteration; scalar is constant cost
  - Batch Sizes (B_exp vs B_pf): Larger feedback batches reduce interaction rounds but increase cognitive load and LLM token costs
  - Simulation vs. Real Users: Uses LLM to simulate users for reproducibility; real deployment requires robust error handling
- **Failure signatures:**
  - Label Noise: GP variance collapses but mean function is wrong (LLM bias)
  - JSON Parsing Errors: LLM fails to format output; requires retry logic or grammar-constrained decoding
  - Stagnation: LLM-only baselines plateau early; if LILO stagnates, GP may not receive diverse enough labels
- **First 3 experiments:**
  1. Sanity Check (Synthetic): Replicate DTLZ2 + L1 experiment using fixed LLM to ensure GP fits LLM's pairwise labels correctly
  2. Ablation (Scalar vs. Pairwise): Compare GP performance when LLM provides direct scalar scores vs. pairwise preferences
  3. Robustness Test: Inject noise into "Simulated User" responses to test LILO convergence sensitivity to contradictory feedback

## Open Questions the Paper Calls Out

- **Real Human Evaluation:** How does LILO's performance and robustness change when interacting with real human decision makers compared to LLM-simulated agents? The paper notes evaluating performance with different natural language agents, "including but not limited to human decision makers," is an exciting area to understand generalization.

- **Calibration Strategies:** What calibration strategies or adaptive weighting schemes can effectively balance the LLM's informative world knowledge priors against specific preferences encoded in provided feedback? The paper suggests future work explore "calibration strategies or adaptive weighting schemes" to manage LLM bias toward internal world knowledge.

- **Hybrid Feedback Approaches:** Can hybrid approaches combining unstructured natural language feedback with structured quantitative feedback mitigate limitations of In-Context Learning in the utility estimation step? The discussion lists "Hybrid approaches that combine unstructured natural language with structured quantitative feedback" as a promising direction.

- **Model Scaling Effects:** How does the fidelity of the LLM utility estimator scale with model size or fine-tuning, and what is the resulting impact on optimization quality? The authors ask how "scaling, fine-tuning, or architectural advances further enhance optimization quality."

## Limitations

- LLM reliability in pairwise labeling is assumed but not fully validated against real human feedback; simulation uses another LLM as DM
- GP-based uncertainty quantification assumes LLM's pairwise labels accurately reflect true preference uncertainty, which may not hold if LLM exhibits systematic biases
- Prior knowledge incorporation mechanism is promising but effectiveness depends heavily on user ability to articulate useful priors in natural language

## Confidence

- **High:** Sample efficiency and theoretical guarantees (Mechanism 2) - well-established BO principles with clear experimental support
- **Medium:** Rich feedback capture through pairwise comparisons (Mechanism 1) - supported by preference learning literature but LLM-specific validation is indirect
- **Low:** Interactive prior knowledge elicitation (Mechanism 3) - innovative but lacks strong empirical validation beyond synthetic case

## Next Checks

1. Deploy LILO with real human decision makers on a real-world optimization problem (e.g., hyperparameter tuning) to validate simulation results
2. Systematically measure LLM pairwise labeling consistency by asking the same preference questions in different contexts across multiple runs
3. Conduct ablation studies testing LILO's performance when given intentionally misleading prior knowledge to assess robustness