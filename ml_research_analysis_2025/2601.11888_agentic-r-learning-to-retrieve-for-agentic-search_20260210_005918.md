---
ver: rpa2
title: 'Agentic-R: Learning to Retrieve for Agentic Search'
arxiv_id: '2601.11888'
source_url: https://arxiv.org/abs/2601.11888
tags:
- search
- agent
- answer
- passage
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Agentic-R introduces a retriever training framework tailored for\
  \ agentic search. Unlike traditional retrievers trained on static queries, Agentic-R\
  \ uses an iterative agent\u2013retriever optimization where the retriever is trained\
  \ on evolving, high-quality queries generated by the search agent."
---

# Agentic-R: Learning to Retrieve for Agentic Search

## Quick Facts
- arXiv ID: 2601.11888
- Source URL: https://arxiv.org/abs/2601.11888
- Reference count: 36
- Primary result: Agentic-R improves search accuracy and efficiency by training retrievers on agent-generated queries and modeling both local and global passage utility

## Executive Summary
Agentic-R introduces a novel retriever training framework designed specifically for agentic search systems. Unlike traditional retrievers trained on static queries, Agentic-R leverages an iterative optimization between the search agent and retriever, where the retriever learns from high-quality, agent-generated queries. The framework evaluates passage utility using both local relevance (query-passage fit) and global answer correctness (whether the passage leads to the correct final answer). Extensive experiments across seven benchmarks demonstrate that Agentic-R consistently outperforms strong baselines, improving both accuracy and search efficiency by reducing the number of required search turns.

## Method Summary
Agentic-R employs an iterative agent–retriever optimization framework where the retriever is trained on evolving, high-quality queries generated by the search agent. The key innovation is modeling passage utility using both local relevance (how well a passage matches the query) and global answer correctness (whether the passage ultimately leads to the right final answer). This dual-signal approach allows the retriever to learn which passages are most likely to contribute to correct answers, even if they don't perfectly match the immediate query. The training process involves multiple iterations where the agent generates queries, the retriever retrieves passages, and both components are updated based on answer verification feedback.

## Key Results
- Agentic-R consistently outperforms strong baselines across seven different benchmarks
- Improves both accuracy and search efficiency by reducing the number of required search turns
- Demonstrates effectiveness across different search agents, showing broad applicability

## Why This Works (Mechanism)
Agentic-R works by aligning retriever training with the actual behavior of agentic search systems. Traditional retrievers are trained to match queries with relevant passages, but in agentic search, the agent generates queries iteratively based on previous search results. By training on the agent's evolving queries, Agentic-R learns to retrieve passages that are useful in the context of the agent's search strategy. The dual utility modeling captures both immediate relevance and downstream impact on answer quality, allowing the retriever to prioritize passages that contribute to correct answers even if they don't perfectly match the current query.

## Foundational Learning

**Iterative Agent-Retriever Optimization**
- *Why needed*: Agentic search involves dynamic query generation based on retrieved information
- *Quick check*: Verify that the retriever's performance improves as the agent generates better queries over training iterations

**Dual Passage Utility Modeling**
- *Why needed*: Single relevance signals miss the connection between passages and final answer quality
- *Quick check*: Compare performance when using only local relevance vs. both signals to confirm the benefit of global utility

**Answer Verification Mechanism**
- *Why needed*: Global utility requires knowing whether retrieved passages lead to correct answers
- *Quick check*: Test framework performance with different answer verification methods to assess sensitivity

## Architecture Onboarding

**Component Map**
Search Agent -> Query Generator -> Retriever -> Passage Ranker -> Answer Verifier -> Agent & Retriever Update

**Critical Path**
Agent generates query → Retriever retrieves passages → Agent evaluates and selects → Answer verification → Joint optimization

**Design Tradeoffs**
- *Static vs. dynamic training*: Static training is faster but misses agent-specific query patterns
- *Local vs. global utility*: Local-only modeling is simpler but less effective for complex reasoning tasks
- *Answer verification quality*: High-quality verification enables better training but may be expensive or noisy

**Failure Signatures**
- Poor retriever performance if agent generates low-quality queries
- Suboptimal results without reliable answer verification
- Computational inefficiency due to iterative training process

**First Experiments**
1. Compare retrieval accuracy using agent-generated queries vs. static queries
2. Evaluate the impact of removing global utility signal on final answer quality
3. Test framework sensitivity to different search agent architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on quality of agent-generated queries
- Requires reliable answer verification mechanism for effective training
- Computationally expensive due to iterative agent–retriever training process

## Confidence

**Major Claims Confidence Assessment**

- **Claim:** Agentic-R consistently outperforms strong baselines across different search agents, improving both accuracy and search efficiency.
  - **Confidence: High** - Supported by extensive experiments on seven benchmarks with clear comparative results.

- **Claim:** The framework models passage utility using both local relevance and global answer correctness.
  - **Confidence: High** - Methodology explicitly incorporates both signals with well-defined distinction.

- **Claim:** Agentic-R reduces the number of required search turns while maintaining or improving accuracy.
  - **Confidence: Medium** - Efficiency gains are reported, but mechanisms and consistency across all benchmarks need deeper analysis.

## Next Checks

1. **Robustness to Agent Quality**: Test Agentic-R with search agents of varying quality to determine sensitivity to generated query quality.

2. **Cross-Domain Generalization**: Evaluate the framework on datasets from different domains (e.g., biomedical, legal) to assess generalizability beyond current benchmarks.

3. **Scalability and Computational Cost**: Measure computational overhead introduced by iterative training and compare with static retrieval methods in terms of training time and resource usage.