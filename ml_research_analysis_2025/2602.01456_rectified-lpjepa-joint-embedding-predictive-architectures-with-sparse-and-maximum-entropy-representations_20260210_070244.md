---
ver: rpa2
title: 'Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and
  Maximum-Entropy Representations'
arxiv_id: '2602.01456'
source_url: https://arxiv.org/abs/2602.01456
tags:
- rectified
- gaussian
- distribution
- generalized
- lpjepa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Rectified LpJEPA introduces a principled approach to learning\
  \ sparse and non-negative representations in self-supervised learning by aligning\
  \ features to a Rectified Generalized Gaussian (RGG) distribution via two-sample\
  \ sliced distribution matching. This design enables explicit control over expected\
  \ \u21130 sparsity through rectification while preserving maximum-entropy properties\
  \ under \u2113p norm constraints."
---

# Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations

## Quick Facts
- **arXiv ID:** 2602.01456
- **Source URL:** https://arxiv.org/abs/2602.01456
- **Reference count:** 40
- **Primary result:** A method for learning sparse and non-negative representations in self-supervised learning by aligning features to a Rectified Generalized Gaussian distribution, enabling explicit control over expected ℓ0 sparsity while preserving maximum-entropy properties.

## Executive Summary
Rectified LpJEPA introduces a principled approach to learning sparse and non-negative representations in self-supervised learning by aligning features to a Rectified Generalized Gaussian (RGG) distribution via two-sample sliced distribution matching. This design enables explicit control over expected ℓ0 sparsity through rectification while preserving maximum-entropy properties under ℓp norm constraints. The resulting method generalizes prior Gaussian-based JEPAs and offers controllable sparsity-performance tradeoffs, learning more statistically independent features with higher entropy. Empirical results across multiple image classification benchmarks show competitive downstream accuracy and dataset-adaptive sparsity patterns.

## Method Summary
Rectified LpJEPA builds on Joint-Embedding Predictive Architectures (JEPA) by adding Rectified Distribution Matching Regularization (RDMReg). The method trains an encoder and projector to minimize both an invariance loss (ℓ2 distance between rectified features from two augmented views) and a sliced Wasserstein distance between the rectified features and a Rectified Generalized Gaussian target distribution. The rectification is achieved via a ReLU activation at the projector output, which induces non-negativity and controllable sparsity. The RGG target distribution is parameterized by a mean shift µ that directly controls the expected fraction of non-zero entries through the CDF of the generalized Gaussian. The method uses random projections and sorting to efficiently compute the sliced Wasserstein distance in high dimensions without requiring closed-form densities.

## Key Results
- Achieves competitive downstream accuracy on CIFAR-100 and ImageNet-100 with controllable sparsity levels (3-95% non-zero features)
- Demonstrates dataset-adaptive sparsity patterns that correlate with downstream performance and feature independence
- Shows a sharp performance cliff when sparsity exceeds ~95%, highlighting a fundamental tradeoff
- Provides explicit control over expected ℓ0 norm through the mean shift parameter µ, with theoretical predictions matching empirical measurements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RDMReg induces controllable ℓ0 sparsity by matching features to a Rectified Generalized Gaussian distribution
- **Mechanism:** ReLU activation maps negative activations to zero. By matching feature distribution to an RGG target with shifted mean µ, expected fraction of nonzeros E[∥x∥₀]/d is determined by Φ(µ/σ). Decreasing µ pushes more probability mass below zero pre-rectification, increasing probability of zero outputs
- **Core assumption:** Encoder outputs are approximately generalized Gaussian before rectification
- **Evidence:** [abstract] RGG enables explicit control over expected ℓ₀ norm through rectification; [Section 3.5] Derivation of E[∥x∥₀] = d·Φ(µ/σ); [Figure 3b] Empirical ℓ₀ norms track theoretical predictions
- **Break condition:** If encoder outputs deviate significantly from generalized Gaussian (e.g., heavy tails, multimodal), sparsity control formula becomes unreliable

### Mechanism 2
- **Claim:** Sliced two-sample distribution matching via Cramér–Wold device enables tractable high-dimensional distribution alignment
- **Mechanism:** Cramér–Wold theorem states two random vectors are equal in distribution iff all 1D projections are. RDMReg samples random projection vectors c, computes projected marginals c^T z and c^T y, and minimizes sliced 2-Wasserstein distance by sorting and computing ℓ2 distance between sorted vectors
- **Core assumption:** Sufficient random projections approximate "all projections" well enough for convergence
- **Evidence:** [Section 4.1] Cramér–Wold device explanation; [Section 4.3] "RGG family is not preserved under linear projections"—necessitating two-sample matching; [Figure 13c] Performance stabilizes with ~100 projections regardless of dimension
- **Break condition:** If projection count is too low, or projections are adversarially chosen, distribution matching fails

### Mechanism 3
- **Claim:** Maximum-entropy preservation under ℓp norm constraints prevents collapse even in highly sparse regimes
- **Mechanism:** Truncated Generalized Gaussian TGN_p(µ,σ,(0,∞)) is maximum differential entropy distribution under expected ℓp norm constraints. RGG is mixture of TGN_p with Dirac at zero; its d(ξ)-dimensional entropy preserves entropy guarantee up to rescaling
- **Core assumption:** Entropy decomposition into marginal d(ξ)-entropies approximates joint entropy well when features are approximately independent
- **Evidence:** [Section 3.3] Maximum entropy characterization; [Section 3.6] d(ξ)-dimensional entropy formula; [Figure 4a] Entropy-sparsity Pareto frontier shows maintained entropy at moderate sparsity
- **Break condition:** If features become dependent despite regularization, marginal entropy overestimates joint entropy

## Foundational Learning

- **Concept:** Joint-Embedding Predictive Architectures (JEPA)
  - **Why needed:** Rectified LpJEPA builds on JEPA's view-invariance objective; understanding base architecture is prerequisite
  - **Quick check:** Can you explain why JEPA uses representation-space prediction instead of input-space reconstruction?

- **Concept:** ℓ0, ℓ1, and ℓp norms for sparsity
  - **Why needed:** Paper explicitly controls ℓ0 (count of nonzeros) via rectification, with ℓ1 as proxy metric; understanding these is essential
  - **Quick check:** Why is direct ℓ0 minimization NP-hard, and what relaxations are commonly used?

- **Concept:** Sliced Wasserstein Distance
  - **Why needed:** RDMReg instantiates distribution matching using sliced 2-Wasserstein distance; knowing how it works clarifies loss computation
  - **Quick check:** How does sliced Wasserstein distance approximate full Wasserstein distance, and why is sorting involved?

## Architecture Onboarding

- **Component map:** Input views → Encoder (ResNet/ViT) → Projector (3-layer MLP) → ReLU → Rectified features → Invariance loss and RDMReg
- **Critical path:**
  1. Sample two augmented views (x, x′) per image
  2. Pass through encoder → projector → ReLU to get z, z′
  3. Compute invariance loss: E[∥z − z′∥²₂]
  4. Sample y ~ RGG_p(µ, σ_GN) and N random projections c_i
  5. For each c_i: sort projections (Zc_i)↑ and (Yc_i)↑, compute (1/B)∥(Zc_i)↑ − (Yc_i)↑∥²₂
  6. Sum over projections and both views; combine with invariance loss (λ_dist = 125, λ_sim = 25)

- **Design tradeoffs:**
  - **µ (mean shift):** Controls sparsity; µ = 0 gives ~70% nonzero, µ = −3 gives <3% nonzero. Performance drops sharply only beyond ~95% zeros
  - **p (shape parameter):** p = 2 → Rectified Gaussian (less sparse), p = 1 → Rectified Laplace (more sparse). Paper finds p = 1 gives best sparsity-accuracy tradeoff
  - **σ (scale):** Default σ_GN ensures unit variance pre-rectification; σ_RGN ensures unit variance post-rectification but reduces sparsity. Paper recommends σ_GN
  - **Projection count N:** 8192 works well; performance is robust to lower counts (Figure 13c)

- **Failure signatures:**
  - **Collapse:** If σ → 0, variance vanishes and features collapse. Always enforce σ > ε
  - **No sparsity:** If µ is too positive or p is too large, features remain dense (m_ℓ₀ ≈ 1)
  - **Performance cliff:** If µ < −3, sparsity exceeds 95% and accuracy drops precipitously (Figure 3c)
  - **Slow convergence:** Random projections alone may be slow; incorporating covariance eigenvectors as projections accelerates convergence (Figure 14)

- **First 3 experiments:**
  1. **Baseline sanity check:** Train with µ = 0, p = 2, σ = σ_GN on CIFAR-100. Verify empirical ℓ₀ matches theoretical prediction (~73%) and accuracy is competitive with LeJEPA/VICReg
  2. **Sparsity sweep:** Vary µ ∈ {0, −1, −2, −3} with p = 1. Plot accuracy vs. ℓ₀ sparsity to reproduce Pareto frontier (Figure 3c). Identify sweet spot before performance cliff
  3. **Ablation on rectification:** Compare Rectified LpJEPA (ReLU on features, match to RGG) vs. LpJEPA (no ReLU, match to GG) vs. LeJEPA+post-hoc ReLU. Confirm rectification on both features and target is necessary (Figure 3a, Figure 11b)

## Open Questions the Paper Calls Out

- **Question:** How does replacing standard ReLU with alternative rectification non-linearities, such as RepReLU, impact sparsity-performance tradeoffs and optimization dynamics?
  - **Basis:** [explicit] Appendix H states authors use standard ReLU to "avoid extra hyperparameter tuning" and "defer detailed investigations on activation functions to future work"
  - **Why unresolved:** Paper restricts architecture to standard ReLU for simplicity, leaving potential benefits of differentiable rectification untested
  - **What evidence would resolve:** Comparative ablation study training Rectified LpJEPA with RepReLU and other variants, measuring convergence speed and final sparsity-accuracy metrics

- **Question:** What is the formal theoretical connection between implicit Non-Negative VCReg recovered by RDMReg and Non-Negative Matrix Factorization (NMF)?
  - **Basis:** [explicit] Appendix I mentions "duality in NMF" analogous to Gram-Covariance duality observed in contrastive learning, which authors "defer... to future work"
  - **Why unresolved:** While paper establishes RDMReg regularizes covariance of rectified features, deeper relationship to NMF decomposition properties is hypothesized but not formalized
  - **What evidence would resolve:** Theoretical derivation linking sliced Wasserstein matching of RGG distributions to NMF optimization landscape, supported by empirical analysis of feature orthogonality

- **Question:** Can observed variation in representation sparsity be utilized as reliable signal for detecting out-of-distribution data or misclassification errors in downstream transfer tasks?
  - **Basis:** [explicit] Appendix J.5 notes that while sparsity distributions differ between correct and incorrect samples in pretraining domain, "divergence is less prominent for downstream transfer tasks," and further investigation is deferred
  - **Why unresolved:** Utility of sparsity as OOD or uncertainty metric is suggested by dataset-adaptive sparsity patterns but shown to be ambiguous in transfer settings
  - **What evidence would resolve:** Analysis correlating sparsity levels with OOD detection scores (e.g., AUROC) and classification confidence on diverse set of unseen transfer domains

## Limitations

- **Distribution assumption uncertainty:** Primary uncertainty lies in assumption that encoder outputs are approximately generalized Gaussian before rectification; if this fails, theoretical sparsity control becomes unreliable
- **Independence approximation:** Maximum-entropy preservation claim relies on feature independence assumption; d(ξ) approximation needs more rigorous validation on real data
- **Dataset specificity:** Paper uses ImageNet-100 subset without specifying class selection, which may affect transfer performance comparisons
- **Performance cliff:** Sharp performance drop when sparsity exceeds ~95% suggests fundamental constraint on achievable sparsity

## Confidence

- **High:** Cramér–Wold sliced distribution matching mechanism and empirical validation are well-established and reproducible
- **Medium:** Maximum-entropy preservation claim relies on independence assumption and d(ξ) approximation needing more validation
- **Medium:** Sparsity control mechanism depends on generalized Gaussian assumption for encoder outputs that may not hold across all architectures and datasets

## Next Checks

1. **Distribution Validation:** Test whether encoder outputs from diverse architectures (ViT, ConvNeXt) follow generalized Gaussian distributions before rectification using Kolmogorov-Smirnov tests and Q-Q plots to quantify approximation error
2. **Independence Assessment:** Measure HSIC on learned features across different µ and p values; compare joint entropy estimates against sum of marginal d(ξ)-entropies to validate entropy decomposition assumption
3. **Robustness to Dataset Shift:** Evaluate Rectified LpJEPA on out-of-distribution datasets (ImageNet-R, ObjectNet) to test whether sparsity control and entropy preservation generalize beyond training distribution