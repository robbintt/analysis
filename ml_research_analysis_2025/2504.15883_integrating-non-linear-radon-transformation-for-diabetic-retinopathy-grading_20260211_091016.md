---
ver: rpa2
title: Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading
arxiv_id: '2504.15883'
source_url: https://arxiv.org/abs/2504.15883
tags:
- radfuse
- images
- image
- grading
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RadFuse, a novel multi-representation deep
  learning approach for diabetic retinopathy (DR) grading that integrates non-linear
  RadEx-transformed sinogram images with traditional fundus images. The study addresses
  the challenge of detecting subtle and complex retinal lesion patterns in DR, which
  are difficult for standard convolutional neural networks (CNNs) to capture due to
  their irregular shapes and distributions.
---

# Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading

## Quick Facts
- arXiv ID: 2504.15883
- Source URL: https://arxiv.org/abs/2504.15883
- Reference count: 40
- Five-stage severity grading achieves QWK 93.24%, accuracy 87.07%, F1 87.17%

## Executive Summary
This paper introduces RadFuse, a novel multi-representation deep learning approach for diabetic retinopathy (DR) grading that integrates non-linear RadEx-transformed sinogram images with traditional fundus images. The study addresses the challenge of detecting subtle and complex retinal lesion patterns in DR, which are difficult for standard convolutional neural networks (CNNs) to capture due to their irregular shapes and distributions. By combining spatial and transformed domain information through an optimized non-linear Radon transform (RadEx), the method enhances feature extraction for improved DR severity classification.

## Method Summary
The RadFuse framework integrates preprocessed fundus images with RadEx sinograms through horizontal concatenation to create a multi-representation input. The optimized RadEx transform uses parameterized exponential curves (z = M · (e^(c(p-q))-1)/(e^(c(b-q))+1)) with q values at intervals of M/50 and M/2 c values to ensure uniform pixel coverage. The fused 512×1024 input is processed by CNN architectures (ResNeXt-50, MobileNetV2, VGG19) with ImageNet pretraining. Training uses AdamW optimizer (lr=1e-4), batch size 16, cross-entropy loss, and 100 epochs with early stopping. Data augmentation (blurring, flips, rotation, sharpening, brightness/contrast) is applied to the concatenated composite image.

## Key Results
- Five-stage severity grading achieves QWK 93.24%, accuracy 87.07%, F1 87.17% on APTOS-2019
- Binary classification reaches 99.09% accuracy, 98.58% precision, 99.64% recall
- RadFuse consistently outperforms fundus-image-only models across all tested CNN architectures
- Optimized RadEx parameters improve sinogram coverage compared to naive implementations

## Why This Works (Mechanism)

### Mechanism 1
The non-linear RadEx transformation captures structural information (e.g., curved vessels, irregular lesions) that linear projection methods or standard spatial convolution might miss or homogenize. By mapping spatial pixels into a sinogram based on curved paths rather than straight lines, the transform creates a distinct feature space where the "curvature" and "distribution" of irregular DR lesions are highlighted differently than in the raw fundus image.

### Mechanism 2
Fusing the spatial domain (fundus image) with the transform domain (sinogram) enriches the feature set, allowing the classifier to resolve ambiguities between adjacent DR severity levels. The early fusion architecture horizontally concatenates the resized fundus image with the RadEx sinogram, forcing convolutional layers to learn joint features that correlate spatial location with transform-domain signatures.

### Mechanism 3
Optimizing the RadEx parameters (q for shift, c for curvature) to ensure uniform pixel coverage prevents the transform from "missing" sparsely distributed lesions in high-resolution images. The specific parameter selection strategy (q intervals of M/50, M/2 c values) ensures the projection curves densely cover the retina, preventing the "blank spots" seen in naive implementations.

## Foundational Learning

- **Concept: The Radon Transform and Sinograms**
  - Why needed here: Understanding that a standard Radon transform projects a 2D image along lines to create a 1D signal (or a 2D sinogram of intensity vs. angle) is essential to grasp what RadEx modifies (lines → curves).
  - Quick check question: If you take the Radon transform of a single white dot in a black image, what shape does it form in the resulting sinogram?

- **Concept: Early Fusion vs. Feature-Level Fusion**
  - Why needed here: The RadFuse architecture uses "early fusion" (concatenating inputs before the first layer). This differs from merging features deep inside the network.
  - Quick check question: When concatenating a fundus image and a sinogram along the width dimension, does the effective receptive field of the first convolutional layer change, or just the channel depth?

- **Concept: Class Imbalance in Medical Grading**
  - Why needed here: The paper uses Quadratic Weighted Kappa (QWK) and MCC because the dataset is imbalanced (e.g., fewer "Severe" cases than "Moderate").
  - Quick check question: In a dataset where 90% of images are "Healthy," a model that always predicts "Healthy" has 90% accuracy. What would its QWK score likely be?

## Architecture Onboarding

- **Component map:** Preprocessing -> RadEx Engine -> Fusion Layer -> CNN Backbone -> Classification Head
- **Critical path:** The RadEx Parameter Calculation (Algorithm 1). The values of c are derived via an inverse formula c = (1/(p-q)) log((M-z)/z). A bug in this logarithmic calculation or the range of z will result in NaN values or incomplete image coverage.
- **Design tradeoffs:** Dynamic vs. Static Augmentation - dynamic recalculation improves performance (QWK 85% → 86%) but increases training time 20x; default recommendation is Joint Augmentation for efficiency. Step Size (Δq): The authors chose M/50 as a balance; smaller steps increase coverage but exponentially increase transform calculation time.
- **Failure signatures:** Adjacent Class Confusion - the model struggles to distinguish "Moderate" from "Severe" (Table 3 & Confusion Matrices). Coverage Gaps - if visualization of the sinogram shows large black bars or checkerboards, the optimization parameters are likely mismatched to the image size M.
- **First 3 experiments:**
  1. Coverage Unit Test: Run Algorithm 1 on a blank 512×512 image of ones. The output sinogram should have no zeros/patterns indicating missed pixels.
  2. Ablation Run: Train the model on "Image-only" vs. "RadEx-only" vs. "RadFuse" on a small subset (e.g., 100 images) to verify the fusion is functioning.
  3. Parameter Sensitivity: Visualize sinograms for the same image using Δq = 10 vs. Δq = 100 to confirm the "optimal" setting captures more structural detail.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can alternative non-linear parametric curves within the RadEx framework improve the detection of intricate retinal features compared to the current logarithmic formulation? The study restricted its evaluation to the specific RadEx formulation defined in Equation 1; the potential of other curve geometries remains untested.

- **Open Question 2:** Can the RadEx calculation process be optimized to support dynamic augmentation during training without the prohibitive increase in computational cost? The current efficient implementation relies on static pre-generated sinograms, sacrificing potential accuracy gains due to time complexity.

- **Open Question 3:** To what extent would integrating advanced class imbalance handling techniques improve RadFuse's sensitivity for underrepresented classes like Severe and PDR? Despite high overall accuracy, the model shows lower recall for minority classes (e.g., 63.64% for Severe on APTOS), indicating that current data augmentation strategies alone are insufficient.

## Limitations

- The mathematical formulation of the RadEx transform appears incomplete or potentially malformed in the published text, creating reproducibility challenges
- Exact parameter selection for the Ben-Graham preprocessing pipeline (Gaussian blur kernel size, normalization constants) is unspecified, creating variability in baseline performance
- The claim of state-of-the-art performance on the APTOS-2019 dataset is difficult to verify due to inconsistent benchmarking practices across the DR grading literature

## Confidence

- **High Confidence:** The fundamental approach of combining spatial and transform-domain features is well-supported by experimental results showing consistent improvements across multiple architectures and datasets
- **Medium Confidence:** The optimization strategy for RadEx parameters is logically sound but lacks direct corpus validation; improvement over naive parameter selection is demonstrated but not extensively compared to alternative coverage strategies
- **Low Confidence:** The specific RadEx mathematical formulation and its claimed superiority over linear Radon or standard convolution cannot be independently verified without clarification of the equation notation

## Next Checks

1. **Equation Clarification:** Contact authors to verify the exact mathematical formulation of the RadEx transform equation (1) and confirm the parameter ranges for q and c values
2. **Preprocessing Standardization:** Implement the Ben-Graham preprocessing pipeline with multiple parameter configurations to establish baseline variability and determine optimal settings for this specific application
3. **Comparative Coverage Analysis:** Systematically evaluate sinogram quality using different parameter selection strategies (Algorithm 1 vs. uniform sampling vs. adaptive coverage) on a subset of images to quantify the impact of parameter optimization on lesion detection