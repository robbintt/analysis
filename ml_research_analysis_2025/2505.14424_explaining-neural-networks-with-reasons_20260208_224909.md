---
ver: rpa2
title: Explaining Neural Networks with Reasons
arxiv_id: '2505.14424'
source_url: https://arxiv.org/abs/2505.14424
tags:
- reasons
- neurons
- https
- neuron
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new interpretability method for neural networks
  based on a formal theory of reasons. It assigns each neuron a "reasons vector" that
  quantifies how strongly it speaks for various propositions.
---

# Explaining Neural Networks with Reasons

## Quick Facts
- arXiv ID: 2505.14424
- Source URL: https://arxiv.org/abs/2505.14424
- Reference count: 40
- Primary result: Novel interpretability method using "reasons vectors" that quantifies neuron-level support for propositions, showing faithfulness through interventions, correctness through clustering, and improved robustness/fairness when training with reasons-based loss.

## Executive Summary
This paper introduces a formal interpretability framework based on "reasons" that assigns each neuron a vector quantifying how strongly it speaks for various propositions across input situations. The method combines logical and Bayesian perspectives, accounts for polysemanticity, and applies uniformly across architectures and modalities. Experiments on LeNet for MNIST demonstrate the approach is scalable (requiring only forward passes), faithful (interventions based on reason vectors yield expected behavior), and correct (reason structure matches data structure). Training with a reasons-based loss improves robustness to adversarial attacks and fairness metrics while maintaining high accuracy, defying typical tradeoffs. Applied to a small LLM, the method successfully identifies sentiment-related neurons and allows controllable generation via interventions.

## Method Summary
The method defines propositions as sets of input-label pairs ("worlds") and computes a reasons vector for each neuron capturing its activation pattern across these worlds. Reason strength is calculated using a Bayesian update formula that measures how much a neuron's activation pattern shifts belief toward a proposition. For faithfulness validation, the paper uses causal interventions that suppress or amplify neurons based on their reason strengths. The method can be integrated into training via loss functions that encourage neurons to develop interpretable reason structures. Experiments demonstrate the approach on MNIST classification, fairness in income prediction, and sentiment analysis in a small LLM.

## Key Results
- LeNet interventions achieve 100% success rate for most digits when suppressing neurons speaking against a class or amplifying those speaking for it
- Reasons-based training with doxastic loss improves FGSM robustness from 62% to 78% at ε=0.15 while maintaining 99% accuracy
- PCA clustering of reason vectors shows correct hierarchical structure matching data distribution (Figure 4)
- Small LLM sentiment analysis identifies 93.2% of top-5 neurons for positive class as actually positive, with 97.9% intervention success rate

## Why This Works (Mechanism)

### Mechanism 1: Reasons Vector as Activation Trace Across Worlds
- Claim: A neuron's functional role can be captured by recording its activations across a representative sample of input situations ("possible worlds").
- Mechanism: For a set W of input-label pairs (worlds), each neuron u gets a reasons vector r_u ∈ ℝ^|W| where component k is the neuron's activation on input from world w_k. This vector serves as a signature of how the neuron responds across contexts.
- Core assumption: The sampled worlds are sufficiently representative of the task distribution to capture the neuron's conceptual role.
- Evidence anchors:
  - [Section 3, p.5]: "Given a neuron u of the neural network, its reasons vector r_u ∈ ℝ^2m has, as value at component k, the activation that neuron u has in the possible world w_k."
  - [Section 4.1, p.6]: Experiments use 1024 sampled worlds from MNIST test set.
  - [Corpus]: Sparse auto-encoders (SAEs) similarly decompose activations but require expensive training; this method uses raw activation traces directly.
- Break condition: If the world sample is too small or unrepresentative, the reasons vector will mischaracterize the neuron's role.

### Mechanism 2: Doxastic Reason Strength via Bayesian Update
- Claim: The strength with which a neuron speaks for a proposition can be computed as the log-odds ratio change from prior to posterior after updating with the neuron's reasons vector.
- Mechanism: Given prior belief b (uniform distribution over worlds), neuron u's reasons vector r_u, and proposition A ⊆ W, the strength is: D(r_u, A, b, W) = (1/2) log[(b∗r_u(A)/b∗r_u(A^c)) / (b(A)/b(A^c))]. This measures how much the neuron shifts belief toward A versus its complement.
- Core assumption: The update rule b∗x (softmax-like combination of prior and reasons) meaningfully models belief revision.
- Evidence anchors:
  - [Section 3, p.5]: Equation (2) defines the doxastic reason strength formula.
  - [Section 3, p.5]: "If b is the uniform measure, b∗x is the well-known softmax of x."
  - [Corpus]: Weak corpus support—this specific Bayesian-log-odds formulation appears novel; related work uses attention weights or gradient-based attribution instead.
- Break condition: If propositions are trivial (b(A) = 0 or 1), strength is undefined; if activations have extreme values, numerical instability may occur.

### Mechanism 3: Causal Intervention Validates Interpretation Faithfulness
- Claim: Neurons identified as strongly speaking for/against a proposition should causally influence the model's output accordingly.
- Mechanism: Two intervention protocols test this: (1) pos2neg—suppress neurons speaking against class d to flip predictions away from d; (2) neg2pos—amplify neurons speaking for class d to flip predictions toward d. Intervention formula: a' = m - c·a where m is mean activation, c is a scaling constant.
- Core assumption: The intervention formula meaningfully manipulates the neuron's semantic contribution rather than just breaking model computation.
- Evidence anchors:
  - [Section 4.1, p.7]: "It is a success if the model now predicts a digit different from d. Figure 3 (left) shows that, for all digits except 1, we have a 100% success rate."
  - [Section 4.3, p.11]: LLM sentiment intervention flips predictions in 97.9% (pos2neg) and 98.6% (neg2pos) of cases.
  - [Corpus]: Consistent with activation patching literature (Geiger et al., Meng et al. cited in paper).
- Break condition: If interventions cause degenerate outputs rather than semantically meaningful flips, faithfulness is not established.

## Foundational Learning

- Concept: **Propositions as sets of possible worlds**
  - Why needed here: The method identifies propositions with subsets of W (e.g., "input depicts digit 3" = {(x,3) ∈ W}). Understanding this set-theoretic framing is essential for computing reason strengths.
  - Quick check question: Given W = {(img1, 3), (img2, 7), (img3, 3)}, what is the proposition "digit is 3"?

- Concept: **Log-odds ratio and belief updating**
  - Why needed here: Reason strength is defined via log-odds changes. You need to understand why log(b(A)/b(A^c)) measures belief strength and how updates change it.
  - Quick check question: If prior odds of A are 1:1 and posterior odds after seeing evidence are 4:1, what is the log-odds change?

- Concept: **Activation patching / causal interventions**
  - Why needed here: The method validates interpretations by intervening on activations. You need to understand mean ablation and directional intervention.
  - Quick check question: Why might a' = m - 3a be better than a' = 0 for testing a neuron's role?

## Architecture Onboarding

- Component map:
  - World sampler -> Forward-pass collector -> Reasons vector store -> Strength calculator -> Intervention engine

- Critical path:
  1. Define task-appropriate worlds (input-label pairs for classification, prompts for LLMs)
  2. Sample representative W (balanced across classes, from held-out data)
  3. Run forward passes to populate reasons vectors
  4. Define propositions of interest (typically one per class/label)
  5. Compute strength profiles for all neurons
  6. Identify top-k neurons speaking for/against each proposition
  7. Validate via intervention experiments

- Design tradeoffs:
  - World set size: Larger W → more accurate reason vectors but higher compute (1024 used in experiments)
  - Test vs. train worlds: Test worlds give unbiased estimates; train worlds yield higher strengths but may overfit (Figure 6 shows qualitative similarity)
  - Intervention strength: Higher scaling constant c → stronger effect but risk of breaking model; values 3–7 used
  - Neuron group size: Intervening on top-5 to top-20 neurons balances specificity vs. effect magnitude

- Failure signatures:
  - All-zero or uniform reason vectors: Neuron not activating or saturated; exclude from analysis
  - High-strength neurons that don't respond to intervention: May indicate spurious correlation in world sample
  - Intervention causes degenerate output: Scaling constant too high or proposition poorly defined
  - Clusters don't form in correctness analysis: Model may not have learned structured representations (see conv1 layer in Figure 4)

- First 3 experiments:
  1. Sanity check on output layer: For a trained classifier, verify that output neuron d has highest positive strength for proposition "label = d" and negative strengths for other labels (Figure 2, right). This should always hold for a trained model.
  2. Layer-wise strength progression: Plot reason strengths across layers for a fixed proposition. Expect low strengths in early layers, increasing toward output (Figure 2, left). Failure indicates broken feature hierarchy.
  3. Minimal intervention test: Pick the neuron with highest strength for a proposition A. On 10 samples not in A, intervene to amplify that neuron. If 0/10 flip to predicting A, either the neuron is not causal or intervention formula needs adjustment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the network of reasons vectors be abstracted into a high-level, human-understandable network or identifiable circuits?
- Basis in paper: [explicit] The authors ask if one can "abstract from it a high-level and human-understandable network (analogous to [19]) or identify circuits (analogous to [41])" in Section 5.
- Why unresolved: The current work focuses on interpreting individual neurons and groups, but has not yet attempted to map the holistic "network of reasons" or circuit-level structure derived from these vectors.
- What evidence would resolve it: A demonstrated method to construct causal circuit diagrams or graphical models entirely from reasons vectors that successfully predict model behavior on complex tasks.

### Open Question 2
- Question: What theoretical guarantees on faithfulness and correctness can be derived for the reasons method under plausible assumptions?
- Basis in paper: [explicit] Section 5 explicitly poses the question: "Which theoretical guarantees on faithfulness and correctness can one derive under plausible assumptions?"
- Why unresolved: The paper currently relies on empirical validation (e.g., activation patching) rather than formal proofs to establish the method's fidelity.
- What evidence would resolve it: Formal theorems bounding the error rates of faithfulness and correctness metrics based on network architecture or sample size.

### Open Question 3
- Question: How does the reasons method perform on significantly larger state-of-the-art models and more complex reasoning tasks?
- Basis in paper: [inferred] The authors acknowledge in Section 5 that they "focused on breadth rather than depth" and tested only on LeNet and a 0.5B parameter LLM due to the method's novelty.
- Why unresolved: While the method is theoretically scalable (forward-pass only), it has not been empirically validated on frontier models or tasks requiring complex logic beyond sentiment analysis.
- What evidence would resolve it: Successful extraction of interpretable reason vectors from models with >7B parameters on benchmarks requiring multi-step reasoning.

### Open Question 4
- Question: What is the relationship between different reasons-based loss functions (e.g., doxastic vs. elementary) and the trade-offs between robustness, faithfulness, and correctness?
- Basis in paper: [inferred] Section 4.2 and Appendix B note a "nontrivial" interaction where the elementary loss improved faithfulness while the doxastic loss improved robustness, prompting the authors to call for "mapping the space of possible loss functions" in Section 5.
- Why unresolved: The paper presents two loss functions with different empirical outcomes but lacks a unified framework explaining why one improves certain interpretability metrics over others.
- What evidence would resolve it: A systematic ablation study varying loss components to map the Pareto frontier between accuracy, robustness, and interpretability metrics.

## Limitations

- The reasons vector approach requires forward passes through the entire network for all neurons, which scales quadratically with network width and is impractical for modern architectures.
- The method assumes propositions can be cleanly defined as sets of worlds, which may not capture complex or continuous concepts.
- Intervention success rates, while high, don't necessarily prove semantic understanding—the interventions could be exploiting statistical correlations rather than causal relationships.

## Confidence

- **Medium** for claims about interpretability faithfulness and scalability, as the MNIST experiments demonstrate consistent results but are limited to a single small architecture.
- **Low** for robustness and fairness claims, as the doxastic loss improvement of 8% on FGSM at ε=0.15 is promising but based on a single dataset and architecture without comparison to established adversarial training methods.

## Next Checks

1. Test the intervention protocol on a more complex architecture (e.g., ResNet-18) to verify scalability and whether high-level concepts require larger neuron groups than observed in LeNet.

2. Compare doxastic loss training against standard adversarial training across multiple datasets to determine if the improvement is specific to MNIST or generalizes.

3. Apply the method to a multi-modal model (e.g., CLIP) to test whether the reasons framework works across different input types and whether it can identify cross-modal neurons.