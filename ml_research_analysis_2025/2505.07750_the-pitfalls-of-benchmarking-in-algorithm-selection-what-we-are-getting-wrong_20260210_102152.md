---
ver: rpa2
title: 'The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong'
arxiv_id: '2505.07750'
source_url: https://arxiv.org/abs/2505.07750
tags:
- problem
- algorithm
- optimization
- features
- meta-models
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper exposes critical flaws in the methodology used to benchmark
  algorithm selection (AS) systems for continuous black-box optimization. The authors
  show that the common "leave-instance-out" (LIO) evaluation, used with the COCO benchmark,
  allows non-informative features and simple meta-models to achieve misleadingly high
  performance due to spurious correlations between features and algorithm performance
  within problem classes.
---

# The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong

## Quick Facts
- **arXiv ID**: 2505.07750
- **Source URL**: https://arxiv.org/abs/2505.07750
- **Reference count**: 40
- **Primary result**: Leave-instance-out evaluation inflates AS performance by rewarding spurious correlations; scale-sensitive metrics produce misleadingly low MSE without improving actual ranking accuracy.

## Executive Summary
This paper exposes critical flaws in how algorithm selection systems are benchmarked for continuous black-box optimization. The common "leave-instance-out" evaluation method, when applied to the COCO benchmark, allows non-informative features and simple meta-models to achieve misleadingly high performance due to spurious correlations between features and algorithm performance within problem classes. The authors also demonstrate that using scale-sensitive metrics like target precision for measuring optimization algorithm performance can produce overly optimistic results when training meta-models, as models may simply learn to predict based on problem scale rather than true optimization difficulty. Through controlled experiments, they show that LIO evaluation rewards features that correlate with problem class rather than genuine AS-relevant properties, and that scale-sensitive metrics can produce impressive-looking MSE scores that don't translate to actual ranking accuracy.

## Method Summary
The study uses the COCO benchmark (24 problem classes × 15 instances × 5 dimensions) and evaluates five optimization algorithms (GA, DE, PSO, ES, CMA-ES) with 30 runs each per instance, using a budget of 1000×D evaluations. Features extracted include 85 ELA features via pflacco, 85 non-informative features constructed using various aggregation and transformation functions on scaled fitness values, problem class identifiers, and problem scale measures. Meta-models are trained using Random Forest to predict either algorithm rankings or target precision. Two evaluation methodologies are compared: leave-instance-out (LIO) which splits instances within classes between train and test sets, and leave-problem-out (LPO) which withholds entire problem classes. Performance is measured using pairwise ranking error (PRE) for rankings and mean squared error (MSE) for precision predictions.

## Key Results
- Under LIO evaluation, non-informative features achieved PRE of ~0.13, nearly matching ELA features (0.1 PRE), demonstrating that LIO permits spurious correlations
- A meta-model using only problem class ID as feature achieved PRE of 0.05 under LIO, outperforming sophisticated ELA features
- Scale-sensitive features (f_scale = max-min) enabled rf-precision models to achieve orders-of-magnitude lower MSE than baselines, yet Friedman tests showed no significant PRE differences between scale-sensitive and scale-free models
- LPO evaluation revealed that all meta-models except random achieved comparable PRE (~0.2-0.6), exposing the generalization failure of LIO-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leave-instance-out (LIO) evaluation inflates perceived meta-model performance by rewarding spurious feature-class correlations rather than genuine algorithm selection capability.
- Mechanism: In the COCO benchmark, problem instances within the same class share highly similar properties and algorithm performance patterns. LIO splits instances of each class between train and test sets, allowing features to implicitly identify problem class membership. Meta-models then "solve" the easier classification task (which class is this?) rather than the actual AS task (which algorithm works best?).
- Core assumption: Algorithm performance differs between problem classes but is relatively consistent within instances of the same class.
- Evidence anchors:
  - [abstract]: "non-informative features and meta-models can achieve high accuracy, which should not be the case with a well-designed evaluation framework"
  - [Section 2.2]: The "class" meta-model using only the problem class ID as a feature achieved PRE of 0.05, outperforming ELA features (0.1 PRE) under LIO; "non-inf" features achieved ~0.13 PRE despite being deliberately non-informative for AS
  - [corpus]: Weak direct corpus support; related work on benchmarking pitfalls exists but doesn't address LIO specifically

### Mechanism 2
- Claim: Scale-sensitive performance metrics (e.g., target precision) allow meta-models to achieve impressive MSE scores without improving actual algorithm ranking accuracy.
- Mechanism: Target precision = f(x_best) - f(x_opt) correlates strongly with problem scale. A meta-model can achieve low MSE by learning scale relationships rather than genuine performance prediction. Since algorithm rankings are scale-independent, this apparent success doesn't translate to better AS capability.
- Core assumption: Objective functions in the benchmark suite have meaningfully different scales; algorithm rankings remain relatively stable under rescaling.
- Evidence anchors:
  - [abstract]: "scale-sensitive metrics can falsely present overly optimistic performance assessments of the meta-models"
  - [Section 3.2]: rf-precision model (single scale-correlated feature) achieved orders-of-magnitude lower MSE than baseline mean-precision; however, Friedman test showed no significant performance differences when evaluated via PRE (scale-free metric)
  - [corpus]: Time series forecasting literature [30] documents analogous scale-sensitivity issues with MAE

### Mechanism 3
- Claim: Leave-problem-out (LPO) evaluation exposes the generalization failure of meta-models trained under LIO assumptions.
- Mechanism: LPO withholds all instances of specific problem classes during training, forcing meta-models to generalize to genuinely unseen problem types. Features that merely encoded class membership under LIO become useless; only features capturing transferable problem properties retain predictive value.
- Core assumption: The 24 COCO problem classes represent meaningfully distinct optimization challenges with limited transferability.
- Evidence anchors:
  - [Section 2.2]: Under LPO evaluation, all meta-models except random achieved comparable PRE (~0.2-0.6), in stark contrast to LIO where class features achieved 0.05 PRE
  - [Section 2.3]: "LIO serves as a simpler stepping stone towards LPO is misguided... achieving strong performance on LIO may seem like progress, it does not inherently translate to improved LPO outcomes"
  - [corpus]: Corpus paper "Greedy Restart Schedules" (arXiv:2504.11440) addresses algorithm selection but doesn't engage with LIO/LPO distinction

## Foundational Learning

- Concept: Algorithm Selection (AS) pipeline
  - Why needed here: The entire paper critiques how we evaluate whether AS systems actually work; understanding the standard pipeline (benchmark → features → meta-model → evaluation) is prerequisite to seeing where it fails
  - Quick check question: Can you trace how an ELA feature becomes a prediction about which algorithm to use?

- Concept: Cross-validation design for hierarchical data
  - Why needed here: The LIO vs LPO distinction is fundamentally about how to handle data with grouped structure (instances nested within classes); incorrect splits create information leakage
  - Quick check question: Why does splitting instances within classes (LIO) not test generalization to new problem types?

- Concept: Scale-sensitive vs scale-free metrics
  - Why needed here: The paper's second critique depends on understanding why some metrics (target precision, MAE) change under rescaling while others (rankings, PRE) don't
  - Quick check question: If you double all objective function values, what happens to target precision vs algorithm rankings?

## Architecture Onboarding

- Component map: COCO benchmark (24 classes × 15 instances × 5D) → Feature extraction (ELA / non-informative / scale features from 250D LHS samples) → Meta-model training (Random Forest predicting ranks or precision) → Evaluation (PRE for ranking, MSE for precision)

- Critical path: Feature computation → Meta-model training on algorithm performance data → Performance evaluation via scale-free metrics on held-out problem classes

- Design tradeoffs:
  - LIO vs LPO: LIO provides larger training sets and easier evaluation but permits spurious correlations; LPO tests genuine generalization but reduces training diversity
  - Scale-sensitive vs scale-free targets: Precision targets provide richer signal but confound scale with difficulty; rankings discard magnitude information but isolate relative algorithm performance

- Failure signatures:
  - Non-informative features achieving near-ELA performance (indicates LIO leakage)
  - Large MSE improvement over baseline without corresponding PRE improvement (indicates scale-artifact learning)
  - Strong correlation between feature classification accuracy and AS performance under LIO (indicates class-memorization)

- First 3 experiments:
  1. Reproduce the "class" meta-model result: Train a meta-model using only problem class ID as feature under LIO; if PRE < 0.1, the benchmark methodology is compromised for AS evaluation
  2. Test scale-sensitivity: Train meta-models predicting target precision using only f_scale = max(Y) - min(Y); compare MSE reduction vs PRE improvement
  3. Compare LIO vs LPO systematically: For any proposed AS system, report both LIO and LPO results; large performance gaps indicate overfitting to class-specific patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can valid baseline models be constructed when using scale-sensitive performance metrics in algorithm selection?
- Basis in paper: [explicit] The authors state "To create a proper baseline for evaluating precision, the baseline must account for the problem's scale; otherwise, it is too weak to be useful," but provide no concrete solution.
- Why unresolved: The paper demonstrates the problem (scale-aware models outperform scale-agnostic baselines artificially) but does not propose a method for constructing baselines that properly account for scale.
- What evidence would resolve it: A formalized baseline construction method that normalizes or conditions on scale, validated across multiple benchmark suites showing no artificial advantage from scale-sensitive features.

### Open Question 2
- Question: Under what specific conditions can leave-instance-out (LIO) evaluation be considered valid for algorithm selection?
- Basis in paper: [explicit] The authors state "If LIO is used, the authors should clearly demonstrate why their problem set is unique and justifies the use of LIO methodology," implying some valid cases may exist.
- Why unresolved: The paper demonstrates LIO flaws on COCO but does not characterize the precise properties a benchmark must have for LIO to remain reliable.
- What evidence would resolve it: A theoretical or empirical framework defining minimum dissimilarity thresholds between instances within problem classes, validated on benchmarks meeting these criteria.

### Open Question 3
- Question: What benchmark suites or modifications to existing benchmarks would adequately support rigorous leave-problem-out (LPO) evaluation?
- Basis in paper: [explicit] The authors state "COCO, with its diverse yet limited set of problems, is not ideally suited as a benchmark for AS in the context of LPO due to insufficient problem quantity and dissimilarity among them."
- Why unresolved: The paper identifies the deficiency but does not propose concrete benchmark extensions or alternative suites suitable for LPO.
- What evidence would resolve it: Design and validation of an expanded benchmark with sufficient problem classes and inter-class diversity, demonstrating that LPO evaluation on this benchmark yields consistent, generalizable results.

## Limitations

- Analysis is confined to a single benchmark suite (COCO) with specific problem characteristics; results may not generalize to other optimization domains
- The non-informative feature construction, while deliberately arbitrary, may not span the full space of possible spurious correlations
- Conclusions about scale sensitivity depend on the specific portfolio of algorithms and their relative performance stability under rescaling

## Confidence

- High confidence: The existence of LIO methodology flaws and their mechanism (spurious class correlation) are well-demonstrated through controlled experiments
- Medium confidence: The recommendation to avoid LIO entirely is sound, though the severity may depend on the specific benchmark characteristics
- Medium confidence: Scale sensitivity findings are convincing but may require domain-specific validation for other optimization contexts

## Next Checks

1. Test whether LPO methodology reveals similar issues in other algorithm selection domains (e.g., machine learning algorithm selection, scheduling)
2. Systematically vary problem class similarity within the COCO benchmark to determine when LIO methodology becomes problematic
3. Investigate whether normalization or feature selection techniques can mitigate the spurious correlation problem without requiring complete LPO evaluation