---
ver: rpa2
title: 'Modeling Romanized Hindi and Bengali: Dataset Creation and Multilingual LLM
  Integration'
arxiv_id: '2511.22769'
source_url: https://arxiv.org/abs/2511.22769
tags:
- transliteration
- romanized
- hindi
- bengali
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndoTranslit, a large-scale dataset and multilingual
  transliteration model for Romanized Hindi and Bengali. The dataset contains over
  2.7 million aligned pairs, including both clean phonetic and noisy user-generated
  Romanized text, making it the largest of its kind for these languages.
---

# Modeling Romanized Hindi and Bengali: Dataset Creation and Multilingual LLM Integration

## Quick Facts
- **arXiv ID:** 2511.22769
- **Source URL:** https://arxiv.org/abs/2511.22769
- **Reference count:** 37
- **Primary result:** Introduces IndoTranslit, the largest dataset for Romanized Hindi and Bengali with 2.7M+ pairs, and a 60M-parameter Marian-based Seq2Seq model achieving strong transliteration performance.

## Executive Summary
This paper addresses the challenge of Romanized Hindi and Bengali transliteration by introducing IndoTranslit, a large-scale dataset containing over 2.7 million aligned pairs. The dataset uniquely combines clean phonetic transliterations with noisy user-generated variants, making it the largest of its kind for these languages. A compact Marian-based Seq2Seq LLM with 60M parameters is trained using a shared subword vocabulary to efficiently handle both languages. The model demonstrates strong performance on transliteration benchmarks, with BLEU scores of 77.57 (Hindi) and 77.82 (Bengali), and 73.15 in the multilingual setting, outperforming existing baselines while maintaining low memory requirements suitable for practical deployment.

## Method Summary
The authors create IndoTranslit by combining multiple data sources: ITRANS-based phonetic mappings for Hindi (1.66M pairs), rule-based Bengali transliterations (975k pairs), and LLM-generated noisy variants for Hindi (150k pairs). A 60M-parameter Marian-based Seq2Seq model is trained using a shared SentencePiece subword vocabulary of 32k tokens across both languages. Language-specific prefix tokens enable multilingual handling within a single model architecture. The model is trained with cross-entropy loss without label smoothing to preserve exact character mappings, optimized for low-resource deployment on consumer GPUs.

## Key Results
- IndoTranslit contains 2.7M+ aligned Romanized-native script pairs, the largest dataset for these languages
- The 60M-parameter model achieves BLEU scores of 77.57 (Hindi), 77.82 (Bengali), and 73.15 multilingual
- Character Error Rate (CER) remains low at 0.08-0.16 across languages, indicating high character-level accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A shared subword vocabulary across Hindi and Bengali improves phonetic alignment and reduces model size.
- **Mechanism:** SentencePiece trains a 32k-token vocabulary on both Romanized corpora simultaneously. Tied embeddings allow overlapping phonetic subwords (e.g., "sh", "aa") to share representations across languages, reducing parameters while preserving script-specific decoding via language prefix tokens.
- **Core assumption:** Romanized Hindi and Bengali share sufficient phonetic substructures that a joint vocabulary captures cross-lingual regularities without sacrificing script fidelity.
- **Evidence anchors:**
  - [abstract] "A compact Marian-based Seq2Seq LLM with 60M parameters is trained using a shared subword vocabulary to handle both languages efficiently."
  - [section IV.A] "We use a shared subword vocabulary vocabulary of size 32,000, trained via SentencePiece over both Hindi and Bengali Romanized corpora... not only reduces memory overhead through tied embeddings but also improves phonetic alignment between overlapping units."
  - [corpus] IndoNLP 2025 shared task (arXiv:2501.05816) addresses similar Romanized Indo-Aryan transliteration but relies on existing datasets; no direct evidence on shared vocabulary efficacy across Hindi-Bengali pairs.
- **Break condition:** If source scripts diverge significantly in phoneme-to-grapheme mappings (e.g., Dravidian languages), shared vocabulary may introduce ambiguity, degrading both languages.

### Mechanism 2
- **Claim:** Combining clean phonetic pairs with synthetic noisy variants improves robustness to informal Romanized text.
- **Mechanism:** The dataset mixes IAST-based canonical transliterations (1.66M Hindi pairs), rule-based mappings (975k Bengali pairs), and LLM-generated informal variants (150k Hindi). This exposes the model to both regular patterns and real-world spelling variations during training.
- **Core assumption:** Noisy synthetic data generated by Gemini-2.0 Flash Lite approximates the distribution of user-generated Romanized text on social media.
- **Evidence anchors:**
  - [abstract] "The dataset contains over 2.7 million aligned pairs, including both clean phonetic and noisy user-generated Romanized text."
  - [section III.A.1] "To introduce diversity in spelling, accent, and writing style, we generated an additional 150k Romanized samples using Gemini-2.0 Flash Lite... This introduced natural variation in vowel omission, consonant simplification, and non-standard spellings."
  - [corpus] Swa-bhasha Resource Hub (arXiv:2507.09245) similarly combines algorithmic and curated data for Sinhala transliteration, suggesting multi-source composition is a viable strategy, though cross-language generalization is unverified.
- **Break condition:** If synthetic noise distribution diverges from real user patterns (e.g., over-representing certain spelling errors), model may overfit to artificial variants and underperform on actual social media text.

### Mechanism 3
- **Claim:** Language-specific prefix tokens enable a single model to handle multilingual transliteration without architecture changes.
- **Mechanism:** The encoder receives input prefixed with a language identifier token (e.g., `<hi>` or `<bn>`). The shared transformer processes the sequence, and the decoder generates native-script output conditioned on this prefix, routing to the correct script without separate models.
- **Core assumption:** The prefix token provides sufficient signal for the model to disambiguate language-specific phoneme-to-grapheme mappings at inference time.
- **Evidence anchors:**
  - [abstract] "A compact Marian-based Seq2Seq LLM... capable of handling both Hindi and Bengali languages with a shared tokenizer."
  - [section IV.A] "A language-specific prefix token is used to distinguish tasks without altering the model structure."
  - [section V, Table IX] Multi-Trans configuration achieves BLEU 73.15, CER 0.08—competitive with single-language models, suggesting prefix-based routing is effective.
  - [corpus] No direct corpus evidence on prefix-token efficacy for Indo-Aryan transliteration; this remains an assumption grounded in the paper's reported results.
- **Break condition:** If Romanized input is heavily code-mixed (e.g., Hindi-Bengali-English in one sentence), a single prefix may not capture intra-sentence language switches, leading to incorrect script output.

## Foundational Learning

- **Concept:** Sequence-to-sequence (Seq2Seq) modeling with cross-entropy loss
  - **Why needed here:** Transliteration is framed as conditional generation where each target character is predicted autoregressively. Understanding how encoder-decoder attention aligns Romanized input to native script is essential for debugging alignment failures.
  - **Quick check question:** Given input "namaste" and target "नमस्ते", which decoder timestep should predict the first Devanagari character, and what encoder states should it attend to?

- **Concept:** Subword tokenization (SentencePiece/Byte-Pair Encoding)
  - **Why needed here:** The model uses a 32k shared vocabulary. Understanding how subwords split phonetic sequences affects both vocabulary coverage and out-of-vocabulary handling for rare Romanized spellings.
  - **Quick check question:** How would SentencePiece tokenize "kya hal hai" versus "kyaa haal hai"—would they share subword units?

- **Concept:** Character Error Rate (CER) vs. Word Error Rate (WER)
  - **Why needed here:** The paper reports both CER (0.08–0.16) and BLEU (73–78). CER captures fine-grained character-level accuracy critical for script conversion, while BLEU measures n-gram fluency. Knowing when each metric signals success or failure guides debugging.
  - **Quick check question:** If BLEU is high but CER is elevated, what type of errors is the model likely making—phonetic substitutions or word-order issues?

## Architecture Onboarding

- **Component map:** Input (Romanized string) → Tokenizer (SentencePiece, 32k vocab, shared) → Embedding layer (tied, 512-dim) → Encoder (6-layer transformer, 8-head attention, FFN 2048) → Decoder (6-layer transformer, cross-attention to encoder) → Output projection (512 → 32k vocab) → Native script string. Language prefix token (<hi> or <bn>) prepended at input to condition decoding.

- **Critical path:** Tokenizer vocabulary coverage → Encoder self-attention on phonetic subsequences → Decoder cross-attention alignment → Output projection softmax over native-script tokens. Failures typically originate in vocabulary gaps or attention misalignment.

- **Design tradeoffs:**
  - **60M parameters vs. larger models:** Chosen for low-resource deployment (RTX 4090, 16GB VRAM). Tradeoff: reduced capacity for rare phonetic patterns vs. faster inference and lower memory.
  - **No label smoothing:** Preserves exact one-to-one character mappings for transliteration fidelity. Tradeoff: may reduce robustness to ambiguous inputs compared to smoothed classification tasks.
  - **Shared vocabulary vs. language-specific:** Reduces parameters and enables cross-lingual transfer. Tradeoff: potential interference if phoneme mappings conflict across languages.

- **Failure signatures:**
  - **High CER on short inputs (<5 words):** Insufficient context for disambiguation; seen in Figure 4 where all models show elevated CER for short sequences.
  - **Code-mixed input degradation:** Model trained on language-prefixed single-language pairs may mishandle intra-sentence language switches (e.g., "aj ke class nai" in Table VIII).
  - **Vocabulary explosion on Bengali:** Bengali shows 4× higher vocabulary growth than Hindi (Figure 3); if subword coverage is insufficient, OOV tokens increase CER.

- **First 3 experiments:**
  1. **Baseline replication:** Train separate Hindi-only and Bengali-only models on IndoTranslit subsets. Compare BLEU/CER to multilingual model to isolate cross-lingual transfer effects.
  2. **Noise ablation:** Train on clean pairs only (remove Gemini-generated noisy data). Evaluate on held-out noisy test set to quantify synthetic noise contribution to robustness.
  3. **Prefix token stress test:** Feed code-mixed inputs (Hindi-Bengali-English) with each language prefix. Measure CER degradation and analyze attention patterns to identify where alignment fails.

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic noisy data may not accurately reflect real user-generated Romanized text patterns, potentially limiting real-world robustness
- The shared subword vocabulary approach could introduce cross-lingual interference if phonetic mappings conflict between Hindi and Bengali
- The model shows higher CER on short inputs (<5 words), suggesting limitations for practical use cases involving brief messages

## Confidence
**High Confidence Claims:**
- IndoTranslit is the largest available dataset for Romanized Hindi and Bengali transliteration (2.7M+ pairs)
- The Marian-based Seq2Seq model with 60M parameters achieves competitive performance on standard benchmarks
- Separate Hindi and Bengali models outperform the multilingual model on their respective languages (as expected)

**Medium Confidence Claims:**
- The shared subword vocabulary approach is more parameter-efficient while maintaining performance
- Synthetic noisy data improves robustness to informal Romanized text (based on model performance but not direct noisy-text evaluation)
- Language prefix tokens enable effective multilingual handling without architectural changes (assumed based on results)

**Low Confidence Claims:**
- The model generalizes to actual user-generated Romanized text beyond the synthetic variants in the dataset
- The 60M parameter model size represents the optimal tradeoff for this task
- The dataset composition fully captures the diversity of Romanized Hindi and Bengali usage

## Next Checks
1. **Real-world robustness test:** Evaluate the model on an external corpus of actual social media Romanized Hindi and Bengali text (e.g., Twitter/X posts, WhatsApp messages) to verify whether synthetic noisy training data translates to real-world robustness. Compare performance degradation against the synthetic-noise-ablated model.

2. **Language-specific ablation study:** Train and evaluate models with language-specific vocabularies (separate 32k vocabularies for Hindi and Bengali) to quantify the actual parameter savings and performance impact of the shared vocabulary approach. This would reveal whether the shared vocabulary introduces cross-lingual interference.

3. **Code-mixed input benchmark:** Create and evaluate on a code-mixed Romanized dataset containing Hindi-Bengali-English sequences without language prefixes to stress-test the prefix token mechanism. Measure CER degradation and analyze attention patterns to identify where the model fails to disambiguate language-specific phoneme-to-grapheme mappings.