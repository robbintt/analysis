---
ver: rpa2
title: Evaluation of retrieval-based QA on QUEST-LOFT
arxiv_id: '2511.06125'
source_url: https://arxiv.org/abs/2511.06125
tags:
- film
- match
- justified
- answer
- films
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive analysis of retrieval-augmented
  generation (RAG) performance on the QUEST-LOFT benchmark, which evaluates complex
  multi-hop question answering. The authors address poor performance on questions
  requiring information distributed across many documents by introducing structured
  output formats containing reasoning and evidence, combined with answer verification.
---

# Evaluation of retrieval-based QA on QUEST-LOFT

## Quick Facts
- **arXiv ID:** 2511.06125
- **Source URL:** https://arxiv.org/abs/2511.06125
- **Reference count:** 40
- **Key outcome:** RAG with structured outputs and verification significantly outperforms long-context approaches on QUEST-LOFT benchmark

## Executive Summary
This paper presents a comprehensive analysis of retrieval-augmented generation (RAG) performance on the QUEST-LOFT benchmark, which evaluates complex multi-hop question answering. The authors address poor performance on questions requiring information distributed across many documents by introducing structured output formats containing reasoning and evidence, combined with answer verification. Using Gemini 1.5 Pro, they achieve significant improvements: F1 score increases by 0.16 (from 0.65 to 0.81), accuracy by 0.16 (from 0.41 to 0.57), and subspan exact match by 0.11 (from 0.53 to 0.64) compared to baseline RAG methods.

## Method Summary
The authors evaluate retrieval-based question answering on QUEST-LOFT-128K, a benchmark requiring multi-answer responses with implicit set operations. They implement RAG using Gecko embeddings to retrieve top 40 documents from a 128K token corpus, then prompt Gemini 1.5 Pro with a structured "Justified QA" format requiring JSON output containing candidate answers, evidence snippets, reasoning, and final judgments. They compare this against Corpus-in-Context (CiC) approaches using the full 128K tokens and evaluate with set-based metrics including F1, precision, recall, accuracy, and subspan exact match.

## Key Results
- RAG with "Justified QA" structured outputs achieves 0.81 F1, 0.57 accuracy, and 0.64 subspan exact match on QUEST-LOFT
- Structured outputs provide +0.14 F1 improvement over standard list outputs
- RAG significantly outperforms CiC (128K context) on QUEST-LOFT, though baselines are close
- Answer verification provides only marginal gains (+0.02 F1) over well-designed initial prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured "Justified" output formats significantly enhance RAG performance on multi-hop questions compared to standard list outputs.
- **Mechanism:** The prompt forces the model to generate a JSON structure containing `candidate_answers`, `evidence_for`, and `reasoning` *before* finalizing the answer list. This effectively enforces a "recall-then-filter" strategy, where the model must first gather potential entities and then explicitly reason about their validity based on the text, similar to Chain-of-Thought but constrained to a verifiable schema.
- **Core assumption:** The instruction-tuned model is capable of adhering to a complex JSON schema while maintaining reasoning capabilities.
- **Evidence anchors:**
  - [abstract]: "...demonstrate that RAG can be optimized... when combined with a structured output format containing reasoning and evidence..."
  - [Section 4.2]: "The explicit structured reasoning elicited by the use of the Justified QA strategy significantly improves performance in the RAG setting (+0.14 F1)..."
  - [corpus]: "Stronger Baselines..." suggests simple RAG pipelines often underperform without optimization, reinforcing the need for structured generation.
- **Break condition:** Models that struggle with strict JSON adherence may fail to produce parsable outputs, or the "reasoning" tokens may consume context window without improving accuracy if the model is too weak.

### Mechanism 2
- **Claim:** Retrieval-Augmented Generation (RAG) outperforms Corpus-in-Context (CiC) when the retrieval set is sufficiently high-recall and the downstream prompt is optimized.
- **Mechanism:** By reducing the 128K token corpus to the top 40 documents via embedding similarity, the system presents the LLM with a focused information set. This appears to mitigate the "lost-in-the-middle" or attention dilution effects observed when forcing a model to process the entire 128K context window at once.
- **Core assumption:** The embedding model (e.g., Gecko) achieves near-perfect recall on the relevant documents within the top 40 results.
- **Evidence anchors:**
  - [abstract]: "...demonstrate that RAG can be optimized to significantly outperform long-context approaches..."
  - [Appendix A - Table 6]: Shows "Embedding-based retrieval" achieving 0.99 Recall@40, validating that the retriever successfully surfaces the necessary signal.
  - [Section 4.2]: Notes that while baselines are close, optimized RAG "significantly" outperforms optimized CiC.
- **Break condition:** If retrieval recall drops (e.g., larger corpora or subtler semantic matches), the QA model cannot recover missing documents, causing hard failures.

### Mechanism 3
- **Claim:** Self-verification provides marginal gains over a well-designed initial generation prompt.
- **Mechanism:** A secondary step verifies candidate answers against specific documents. However, because the "Justified QA" prompt already forces the model to output reasoning and evidence during generation, the verification step mostly duplicates this effort, resulting in diminishing returns.
- **Core assumption:** The model possesses the intrinsic capability to judge its own output accuracy without external ground truth.
- **Evidence anchors:**
  - [Section 4.2]: "...applying an additional verification step... improves performance slightly (+0.02 F1)..."
  - [Section 1]: "Reinforce the findings of prior research that most... benefits of self-verification can be equivalently achieved via improvements to the prompt..."
  - [corpus]: No direct corpus evidence found for the specific "diminishing returns" of verification vs. prompting in the provided neighbors, though general self-correction literature (e.g., "Does RAG Really Perform Bad...") focuses on retrieval quality.
- **Break condition:** Verification introduces latency and cost (additional LLM calls) that may not justify the <2% F1 improvement.

## Foundational Learning

- **Concept: Multi-Value QA & Set Operations**
  - **Why needed here:** The QUEST dataset requires identifying lists of entities (e.g., films) satisfying complex boolean criteria (AND/OR). Standard single-answer QA metrics (like simple EM) fail to capture partial credit.
  - **Quick check question:** If a model retrieves 3 out of 5 correct movies, should it score 0? How does F1 handle this?

- **Concept: Chain-of-Thought (CoT) in Structured Output**
  - **Why needed here:** The paper leverages JSON not just for formatting, but as a scaffold for reasoning. Understanding that `reasoning` fields function as explicit intermediate computation steps is key to the "Justified" strategy.
  - **Quick check question:** Why might forcing a model to write "evidence" text before the "final_judgment" improve the accuracy of the judgment?

- **Concept: Context Window vs. Effective Context**
  - **Why needed here:** The paper contrasts 128K context windows (CiC) with smaller retrieved context (RAG). One must understand that *fitting* text in a window does not guarantee the model *attends* to it effectively.
  - **Quick check question:** Why would a model perform worse when given *more* information (the full 128K corpus) compared to a filtered subset (Top 40 docs)?

## Architecture Onboarding

- **Component map:** Indexer -> Retriever -> Generator -> Parser -> (Optional Verifier)
- **Critical path:** The **Justified QA Prompt**. The paper demonstrates that changing the prompt from "List the IDs" to "Output a JSON with reasoning" yields the largest performance jump (+0.14 F1), far exceeding the switch from CiC to RAG or adding verification.
- **Design tradeoffs:**
  - **Gemini 1.5 Pro vs. Flash:** Pro can reason directly in JSON; Flash requires explicit natural language CoT ("Notes") *before* JSON to achieve comparable results.
  - **RAG vs. CiC:** RAG offers better performance at 128K; CiC is simpler (no retrieval step) but lags in accuracy.
- **Failure signatures:**
  - **JSON Syntax Errors:** The model drops fields or hallucinates non-existent IDs.
  - **Recall Collapse:** The retriever fails to surface the answer in Top 40 (rare in this 128K dataset, likely in larger corpora).
  - **Reasoning Distraction:** For simpler models (Flash), complex reasoning prompts may lower precision if not carefully tuned with "Notes" steps.
- **First 3 experiments:**
  1. **Baseline Check:** Implement "Retrieve-and-Read" (Top 40) with a generic prompt. Establish F1 baseline.
  2. **Prompt Optimization:** Implement the "Justified QA" JSON prompt. Compare F1 lift (expected +0.10 to +0.14).
  3. **Context Ablation:** Run the "Justified QA" prompt with all 128K tokens (CiC) vs. Top 40 (RAG). Verify if retrieval focus aids the specific model being used.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the performance gains of structured "Justified QA" be maintained while managing computational costs and latency as the number of questions and document volume scales up?
- **Basis in paper:** [explicit] Section 8 states, "One challenge for future research will be to find ways to maintain this top performance while managing costs as the number of questions and documents increase."
- **Why unresolved:** The experiments focused on a fixed corpus size (128K tokens) with a primary metric of accuracy/F1, without analyzing the trade-offs regarding inference speed or token usage in production-scale environments.
- **What evidence would resolve it:** Evaluation of latency and token consumption metrics on larger corpora (e.g., 1M tokens) comparing the multi-step Justified QA against simpler baselines.

### Open Question 2
- **Question:** Do the benefits of structured output prompting and verification transfer to QA tasks requiring aggregation of information across multiple documents per entity or long-form generation with fact attribution?
- **Basis in paper:** [explicit] Section 8 notes the authors are "interested in expanding our evaluation to a wider range of... benchmarks, including ones that require aggregation of information about the same entity from multiple different documents... or generation of long-form answers."
- **Why unresolved:** The QUEST-LOFT benchmark focuses on identifying multiple distinct entities using set operations, rather than synthesizing a single narrative from dispersed information.
- **What evidence would resolve it:** Application of the Justified QA framework on multi-hop reasoning benchmarks (e.g., HotpotQA) and long-form generation tasks.

### Open Question 3
- **Question:** Does the necessity of explicit natural language Chain-of-Thought (CoT) reasoning prior to structured JSON output depend strictly on model capability?
- **Basis in paper:** [inferred] Section 4.3 notes that Gemini 1.5 Flash showed significant improvement with CoT, while 1.5 Pro performed equally well or better without it.
- **Why unresolved:** The study compares only two model variations, leaving it unclear if this is a general scaling law or specific to the Gemini architecture.
- **What evidence would resolve it:** Ablation studies across a wider range of model sizes and families (e.g., 7B vs 70B parameters) measuring the delta between direct JSON generation and CoT-guided generation.

## Limitations
- Performance depends heavily on specific retrieval recall rates (0.99@40) that may not generalize to larger corpora
- Structured JSON format may be brittle across different model architectures and could fail if models struggle with strict schema adherence
- Results achieved using Gemini 1.5 Pro may not transfer to open-weight models with different reasoning capabilities

## Confidence
- **High Confidence:** The core finding that structured outputs with reasoning improve RAG performance on multi-hop questions is well-supported by experimental results and aligns with established Chain-of-Thought literature.
- **Medium Confidence:** The claim that RAG outperforms long-context approaches specifically on QUEST-LOFT may not generalize to all multi-hop QA tasks, as performance could shift with different corpus sizes or document distributions.
- **Low Confidence:** The assertion that verification provides only marginal gains (0.02 F1) may be dataset-specific and could vary significantly with different verification prompt designs or baseline model capabilities.

## Next Checks
1. **Retrieval Robustness Test:** Evaluate performance on a corpus where retrieval recall@40 drops to 0.80-0.90 to determine if the "Justified QA" strategy can compensate for retrieval failures.
2. **Model Architecture Transfer:** Implement the same structured output approach with an open-weight model (e.g., LLaMA-3) to test whether the performance gains are specific to Gemini 1.5 Pro or transfer across architectures.
3. **Verification Ablation Study:** Systematically vary the verification prompt structure and timing to quantify the actual contribution of verification versus structured generation, measuring both performance and computational cost trade-offs.