---
ver: rpa2
title: An Under-Explored Application for Explainable Multimodal Misogyny Detection
  in code-mixed Hindi-English
arxiv_id: '2601.08457'
source_url: https://arxiv.org/abs/2601.08457
tags:
- system
- speech
- misogyny
- hate
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an explainable multimodal web application for
  detecting misogyny in code-mixed Hindi-English text and memes using transformer-based
  models (mBERT, XLM-RoBERTa) and visual models (EfficientNet, ResNet). The system
  integrates LIME and SHAP for interpretability, providing feature importance scores
  for model predictions.
---

# An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English

## Quick Facts
- arXiv ID: 2601.08457
- Source URL: https://arxiv.org/abs/2601.08457
- Reference count: 0
- Primary result: Multimodal system achieves binary macro-F1 scores of 0.93 (text, mBERT), 0.72 (mBERT+EfficientNet), and 0.74 (mBERT+ResNet) on code-mixed Hindi-English misogyny detection

## Executive Summary
This study presents a web application for detecting and explaining misogyny in code-mixed Hindi-English text and memes. The system combines transformer-based models (mBERT, XLM-RoBERTa) for text analysis with CNN-based models (EfficientNet, ResNet) for visual analysis, achieving strong binary classification performance while providing interpretability through LIME and SHAP. Human evaluators assessed the application's usability, yielding moderate scores with users finding the system intuitive but noting dependability and efficiency issues.

## Method Summary
The system employs mBERT and XLM-RoBERTa for text-based misogyny detection on a dataset of approximately 4,193 code-mixed Hindi-English comments. For multimodal detection, it combines mBERT with EfficientNet or ResNet on a dataset of 4,218 memes (after filtering 836 Devanagari Hindi samples). LIME and SHAP are integrated for interpretability, providing feature importance scores and saliency maps separately for textual and visual inputs. The application is built using Flask and evaluated through user experience questionnaires (UEQ, CUQ).

## Key Results
- Binary classification macro-F1 scores: 0.93 (text, mBERT), 0.72 (mBERT+EfficientNet), 0.74 (mBERT+ResNet)
- Multilabel classification performance significantly lower: 0.27 (text, mBERT), 0.32 (mBERT+ResNet)
- User experience evaluation: mean score of 65.4/100 on UEQ and CUQ, with moderate usability but dependability and efficiency concerns

## Why This Works (Mechanism)

### Mechanism 1: Multilingual transformers capture code-mixed patterns
- Claim: mBERT/XLM-R handle syntactic irregularities and inconsistent spelling in code-mixed Hindi-English
- Evidence: mBERT achieves 0.93 binary macro-F1 on code-mixed dataset
- Core assumption: Shared cross-lingual representations transfer effectively to Hinglish
- Break condition: Performance degrades if test data contains significantly different transliteration patterns

### Mechanism 2: Multimodal fusion captures distributed misogyny signals
- Claim: Combining text and image features improves detection over unimodal approaches
- Evidence: Multimodal models achieve 0.72-0.74 binary F1 vs. 0.93 for text-only
- Core assumption: Misogynous intent distributes across both text and visual elements
- Break condition: OCR failures on stylized text break the text pipeline

### Mechanism 3: XAI provides interpretable feature attributions
- Claim: LIME and SHAP help users understand model decisions
- Evidence: Separate LIME/SHAP implemented for text and visual signals
- Core assumption: Users can meaningfully interpret feature importance plots
- Break condition: Explanations may be unstable under input perturbations

## Foundational Learning

- **Code-mixing and transliteration**: Hinglish combines Romanized Hindi with English, lacking standardized spelling. Standard NLP pipelines may fragment or misalign Hinglish tokens.
- **Transformer attention and subword tokenization**: mBERT/XLM-R use WordPiece/SentencePiece. SHAP highlighting "sagging" shows model associations vs. genuine causal features.
- **Multimodal fusion strategies**: Different approaches (early, late, cross-attention) have different failure modes. If image encoder fails but text works, should the system return a prediction?

## Architecture Onboarding

- **Component map**: Input layer -> OCR module -> Text encoder (mBERT/XLM-R) + Image encoder (EfficientNet/ResNet) -> Fusion/classification head -> XAI module (LIME/SHAP) -> Web framework (Flask) -> Feedback collection
- **Critical path**: User submits input -> If image: OCR extracts text -> Both processed -> Model inference -> XAI computation -> Results rendered
- **Design tradeoffs**: Model selection exposed to users (flexibility vs. cognitive load); Separate LIME/SHAP per modality (interpretability vs. computational cost); Weak annotation (faster data creation vs. label bias)
- **Failure signatures**: Low dependability score (0.625) due to response delays; Multilabel performance gap (binary 0.93 vs. multilabel 0.27); OCR errors requiring manual correction for ~20% of samples
- **First 3 experiments**:
  1. Latency profiling: Measure end-to-end inference time; identify bottleneck
  2. OCR robustness test: Run on held-out memes with varied text styles
  3. Explanation stability check: Perturb inputs and measure LIME/SHAP variance

## Open Questions the Paper Calls Out

- How does the analysis of user feedback forms quantify the subjective quality and interpretability of the provided LIME and SHAP explanations?
- Does optimizing the inference latency directly improve the low "dependability" and "efficiency" scores observed in the user experience evaluation?
- To what extent does the "weakly annotated" dataset introduce label bias or noise that limits the model's multi-label classification performance?

## Limitations

- Dataset quality concerns: Single annotator created weakly annotated dataset without inter-annotator agreement
- Multilabel performance gap: Binary F1 (0.93) vs. multilabel F1 (0.27) suggests annotation ambiguity
- OCR reliability unknown: Base accuracy on stylized meme text not characterized

## Confidence

- **High confidence**: Binary text classification performance, basic web application functionality, UEQ/CUQ methodology
- **Medium confidence**: Multimodal model architecture, explainability implementation quality, usability scores interpretation
- **Low confidence**: Dataset quality assessment, multilabel classification reliability, OCR pipeline robustness

## Next Checks

1. Re-annotate 100 random samples using multiple annotators and compute Cohen's kappa to establish ground truth quality
2. Run complete pipeline on diverse memes with varying text styles and measure text extraction accuracy
3. Generate 100 perturbations per test sample and compute variance in LIME/SHAP attributions to flag unstable explanations