---
ver: rpa2
title: 'MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture'
arxiv_id: '2512.19311'
source_url: https://arxiv.org/abs/2512.19311
tags:
- training
- mixflow
- guidance
- sampling
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MixFlow training is a novel method to alleviate exposure bias in
  diffusion models by leveraging slowed interpolation mixture during training. It
  is motivated by the Slow Flow phenomenon, where the ground-truth noisy data nearest
  to generated noisy data at a given sampling timestep corresponds to a higher-noise
  (slowed) timestep.
---

# MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture

## Quick Facts
- **arXiv ID:** 2512.19311
- **Source URL:** https://arxiv.org/abs/2512.19311
- **Reference count:** 40
- **Key outcome:** MixFlow training alleviates exposure bias in diffusion models through slowed interpolation mixture, achieving state-of-the-art gFID scores on ImageNet and text-to-image generation tasks.

## Executive Summary
MixFlow training introduces a novel approach to mitigating exposure bias in diffusion models by leveraging the Slow Flow phenomenon, where generated noisy data corresponds to higher-noise ground-truth timesteps. The method uses interpolations at slowed timesteps as training targets, sampling slowed timesteps from U[(1−γ)t, t] and training timesteps from Beta(2,1). Extensive experiments on class-conditional image generation (SiT, REPA, RAE) and text-to-image generation (Stable Diffusion 3.5) demonstrate significant improvements in generation quality, with state-of-the-art gFID scores achieved on ImageNet 256×256 and 512×512 resolutions.

## Method Summary
MixFlow training addresses exposure bias in diffusion models by exploiting the Slow Flow phenomenon, where the ground-truth noisy data nearest to generated data at timestep t actually corresponds to a higher-noise timestep. The method introduces slowed interpolation mixture during training, using interpolations at slowed timesteps as input for the prediction network. Training timesteps are sampled from Beta(2,1) distribution, while slowed timesteps are sampled from uniform distribution U[(1−γ)t, t]. This approach ensures uniform sampling across all timestep pairs and provides more informative training signals by aligning the ground-truth with the actual data distribution encountered during sampling.

## Key Results
- Post-training SiT-B with MixFlow improves gFID from 17.97 to 15.64 (without guidance) and from 4.46 to 3.91 (with guidance) on ImageNet 256×256
- MixFlow improves REPA-XL (gFID: 6.90→6.28 without guidance, 1.65→1.59 with guidance) and RAE-XL (gFID: 1.51→1.43 without guidance, 1.13→1.10 with guidance)
- MixFlow + RAE achieves state-of-the-art gFID of 1.43 (without guidance) and 1.10 (with guidance) on ImageNet 256×256, and 1.55 (without guidance) and 1.10 (with guidance) on 512×512
- MixFlow improves Stable Diffusion 3.5 across GenEval and T2I-CompBench metrics for text-to-image generation

## Why This Works (Mechanism)
MixFlow addresses exposure bias by recognizing that during sampling, generated noisy data at timestep t often corresponds to ground-truth data at a higher (slower) timestep. Traditional training uses the exact timestep t as the target, creating a mismatch between training and inference. MixFlow corrects this by using interpolations at slowed timesteps as training targets, sampled from U[(1−γ)t, t], while maintaining Beta(2,1) sampling for training timesteps. This alignment between training targets and the actual data distribution encountered during sampling provides more informative gradients and improves generation quality.

## Foundational Learning

**Diffusion Models**
*Why needed:* Core framework for understanding the problem MixFlow addresses
*Quick check:* Understanding of forward noising process and reverse denoising process

**Exposure Bias**
*Why needed:* Key problem MixFlow aims to solve
*Quick check:* Recognition that training and sampling distributions differ in diffusion models

**Slow Flow Phenomenon**
*Why needed:* Theoretical motivation for MixFlow's approach
*Quick check:* Understanding that generated noisy data corresponds to higher-noise ground-truth timesteps

**Beta Distribution Sampling**
*Why needed:* MixFlow uses Beta(2,1) for training timestep sampling
*Quick check:* Knowledge of Beta distribution properties and its use in diffusion models

**Interpolation in Latent Space**
*Why needed:* MixFlow uses interpolations at slowed timesteps as training targets
*Quick check:* Understanding of linear interpolation between latent representations

## Architecture Onboarding

**Component Map**
Prediction Network -> Slowed Timestep Sampling (U[(1−γ)t, t]) -> Interpolation -> Training Timestep Sampling (Beta(2,1)) -> Loss Computation

**Critical Path**
Training Timestep Sampling (Beta) -> Prediction Network -> Slowed Timestep Sampling (Uniform) -> Interpolation -> Loss Computation -> Parameter Update

**Design Tradeoffs**
MixFlow introduces additional computational overhead for computing interpolations at slowed timesteps, but provides more informative training signals. The hyperparameter γ controls the extent of timestep slowing and requires careful tuning. The method trades increased training complexity for improved generation quality and reduced exposure bias.

**Failure Signatures**
- Poor performance if γ is set too high, causing excessive timestep slowing
- Suboptimal results if training timesteps are not sampled from Beta(2,1)
- Degraded quality if interpolation is not performed correctly between slowed timesteps

**First Experiments**
1. Verify Slow Flow phenomenon exists in your specific model and dataset by comparing nearest neighbors in latent space
2. Implement basic MixFlow training with γ=0.5 and Beta(2,1) sampling, comparing gFID to baseline
3. Conduct ablation study removing MixFlow components to isolate contribution of slowed timestep interpolation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited analysis of Slow Flow phenomenon generalizability across diverse domains and model architectures
- Computational overhead from additional forward passes for interpolation targets, with unquantified impact on memory consumption and scalability
- Critical dependence on hyperparameter γ without systematic sensitivity analysis or theoretical grounding for optimal selection

## Confidence
- **High confidence:** Reproducibility of gFID improvements on ImageNet benchmarks and methodological soundness of interpolation approach
- **Medium confidence:** Causal relationship between MixFlow and exposure bias mitigation due to lack of ablation studies isolating this effect
- **Medium confidence:** State-of-the-art status given unspecified comparison methodology and hyperparameter tuning protocols for baselines

## Next Checks
1. Conduct controlled ablation studies comparing MixFlow against alternative exposure bias mitigation techniques to isolate its unique contribution
2. Evaluate MixFlow's performance on out-of-distribution datasets and non-natural image domains to test universality of Slow Flow phenomenon
3. Perform comprehensive hyperparameter sensitivity analysis for γ and slowed timestep sampling distribution to establish robust configuration guidelines