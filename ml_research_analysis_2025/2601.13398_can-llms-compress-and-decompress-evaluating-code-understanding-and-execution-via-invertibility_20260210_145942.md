---
ver: rpa2
title: Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution
  via Invertibility
arxiv_id: '2601.13398'
source_url: https://arxiv.org/abs/2601.13398
tags:
- output
- input
- code
- string
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current code LLMs achieve high accuracy on isolated forward execution
  tasks, but fail to maintain round-trip consistency when encoding and decoding inputs.
  We introduce RTCE, a benchmark that tests exact input reconstruction across four
  compression algorithms (LZW, AE, RLE, Huffman) using zero-shot, fine-tuned, and
  self-reflection prompting.
---

# Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility

## Quick Facts
- **arXiv ID**: 2601.13398
- **Source URL**: https://arxiv.org/abs/2601.13398
- **Reference count**: 40
- **Primary result**: Current code LLMs achieve high accuracy on isolated forward execution tasks, but fail to maintain round-trip consistency when encoding and decoding inputs.

## Executive Summary
This paper introduces RTCE, a benchmark that tests exact input reconstruction across four compression algorithms (LZW, AE, RLE, Huffman) using zero-shot, fine-tuned, and self-reflection prompting. Results show that even the strongest models plateau at moderate Pass@5 scores, with inversion tasks being the most challenging, revealing fundamental gaps in algorithmic reasoning and bidirectional understanding. The benchmark enforces a strict bijection constraint—dec(enc(x)) = x—requiring the model to internally maintain consistent forward and inverse mappings. When the model succeeds on forward tasks but fails on inversion, this indicates its forward correctness derived from surface pattern matching rather than mechanistic understanding of state transitions and control flow.

## Method Summary
The RTCE benchmark evaluates round-trip consistency of Code-LLMs through 4 tasks: (1) Output Prediction (X→z via encoder), (2) Input Prediction with Inversion (z→X' via inverted encoder), (3) Output Prediction with Inversion (X→z via inverted decoder), (4) Input Prediction (z→X' via decoder). Tests exact bijection fidelity across compression algorithms. 250 synthetic input samples across 4 families (PatternedString, StructuredLog, YAML-Like Config, Tabular) → 1000 total evaluation examples. Four algorithms: LZW, Arithmetic Encoding (AE), Run-Length Encoding (RLE), Huffman. Input lengths range from few characters to several hundred. Exact Match (EM), Edit Similarity (ES) via Levenshtein distance, Pass@5 (≥1 correct among 5 completions). Method includes zero-shot prompting with temperatures {0.2, 0.8}, multi-turn revision with 2 rounds, critique-edit cycles, and fine-tuning via LoRA (rank=8, all modules, LR=1e-4, 3 epochs, gradient accumulation=8, cutoff=2048).

## Key Results
- Current LLMs plateau at moderate Pass@5 scores even on forward tasks, with inversion tasks being the most challenging
- Performance hierarchy (RLE > LZW > AE > Huffman) maps to increasing state-tracking and compositional reasoning demands
- Self-reflection yields diminishing returns because models cannot self-correct errors they do not recognize as errors

## Why This Works (Mechanism)

### Mechanism 1: Round-Trip Bijection as a Diagnostic Probe
Round-trip consistency testing reveals reasoning gaps that single-direction I/O prediction benchmarks miss. The benchmark enforces a strict bijection constraint—dec(enc(x)) = x—requiring the model to internally maintain consistent forward and inverse mappings. When the model succeeds on forward tasks but fails on inversion, this indicates its forward correctness derived from surface pattern matching rather than mechanistic understanding of state transitions and control flow.

### Mechanism 2: Algorithmic Complexity Gradient Surfaces Capability Boundaries
Different compression paradigms stress distinct reasoning capabilities, creating a diagnostic gradient. RLE (run aggregation) tests local pattern detection and is most tractable for 7B+ models. LZW (dictionary-based) requires maintaining dynamic dictionary state across long spans. Huffman (prefix coding) demands hierarchical symbol-level reasoning. AE (arithmetic encoding) requires precise floating-point interval manipulation. The performance hierarchy (RLE > LZW > AE > Huffman) maps to increasing state-tracking and compositional reasoning demands.

### Mechanism 3: Self-Refinement Saturation Indicates Algorithmic Misunderstanding
Self-reflection yields diminishing returns because models cannot self-correct errors they do not recognize as errors. Multi-turn revision improves accuracy in round 1 but plateaus by round 2. This saturation occurs because the model's critique is bounded by its own parametric knowledge—if it misunderstands the algorithm's invariants, it cannot generate corrective feedback. The error is systemic, not superficial.

## Foundational Learning

- **Concept: Bijective Functions and Invertibility**
  - **Why needed here**: The entire benchmark hinges on the mathematical property that enc and dec form a bijection. Without this, "round-trip correctness" has no formal definition.
  - **Quick check question**: Given a function f(x) = 2x, what is its inverse, and why is it not bijective if the domain is integers?

- **Concept: State Tracking in Program Execution**
  - **Why needed here**: Compression algorithms like LZW and Huffman require maintaining evolving internal state (dictionaries, codebooks). Models fail when they cannot track state changes across long sequences.
  - **Quick check question**: Trace the LZW encoding of "ABAB" by hand—what entries are added to the dictionary at each step?

- **Concept: Exact Match vs. Semantic Equivalence**
  - **Why needed here**: Unlike code-to-NL round-trips that tolerate semantic equivalence, RTCE requires bit-exact reconstruction. This is stricter and eliminates ambiguity in evaluation.
  - **Quick check question**: Why would "aaabbbcc" → "a3b3c2" → "aaabbbcc" be easier to verify than code → description → code?

## Architecture Onboarding

- **Component map**: Input Generator (4 data families) → Reference Implementations (enc/dec for LZW, AE, RLE, Huffman) → Task Instantiator (4 task types) → Model Inference (vLLM, temperatures {0.2, 0.8}, n=5 completions) → Evaluator (Exact Match, Edit Similarity, Pass@5) → Optional: SFT on execution traces OR multi-turn revision loop

- **Critical path**: The inversion tasks (I/P Pred-I, O/P Pred-I) are the diagnostic core. If you only run forward tasks, you will overestimate model capability. Always include all four task types.

- **Design tradeoffs**: Synthetic vs. real-world data: Synthetic enables controlled difficulty gradients but may not generalize; paper argues compression is content-agnostic, so pattern distribution matters more than source. Execution-free vs. execution-based: Execution-free enables exact-match evaluation without sandbox overhead; execution-based would enable runtime error detection but introduces non-determinism. LoRA vs. full fine-tuning: LoRA is efficient but may not sufficiently alter internal representations for inverse reasoning.

- **Failure signatures**: High ES but near-zero EM: Model has correct intuition but makes surface errors (common in decoding). High forward accuracy but near-zero inverse accuracy: Model relies on pattern matching, not algorithmic understanding. Saturation after 1 revision round: Model lacks parametric knowledge to self-correct.

- **First 3 experiments**: 
  1. Baseline probe: Run all four task types on a 7B code-specialized model (e.g., Qwen2.5-Coder-7B) with temperature 0.2. Expect: RLE >> LZW > AE >> Huffman for inversion tasks.
  2. Ablation by input length: Filter dataset by input length bins (<50 chars, 50-150, >150). Plot Pass@5 vs. length. Hypothesis: Inversion accuracy degrades faster with length than forward accuracy.
  3. Error taxonomy: Manually inspect 20 failed inversion predictions. Categorize errors: state-tracking failure, arithmetic error, hallucinated symbols, format mismatch. This informs whether to target SFT data or architecture changes.

## Open Questions the Paper Calls Out

- **Cross-language generalization**: Do the findings regarding round-trip consistency failures in Python generalize to other programming languages with different syntax and execution models? The authors note in the Limitations section that "the current implementation focuses exclusively on Python, which may limit the generalizability of insights into model capabilities across other programming languages."

- **Broader code inversion**: How do Code-LLMs perform on other forms of code inversion, such as refactoring or decompilation, which fall outside the compression-decompression paradigm? The paper states that the RTCE design "centres around the compression-decompression paradigm, which may overlook other invertibility forms, such as refactoring, decompilation, or symbolic manipulation."

- **Runtime phenomena**: How does the inability to handle runtime phenomena (side effects, concurrency) contribute to the observed failures in round-trip consistency? The authors acknowledge that their "evaluation framework is static and execution-free, thus unable to assess how models handle runtime phenomena such as side effects, concurrency, exceptions, or external dependencies."

## Limitations
- **Synthetic data bias**: Exclusive use of synthetic inputs may underestimate model performance on real-world code patterns
- **Evaluation scope**: Benchmark focuses on four specific compression algorithms, which may not capture the full spectrum of algorithmic reasoning demands
- **Self-reflection saturation**: Observation relies on a single critique-edit cycle depth, underspecified mechanism

## Confidence
- **High**: Round-trip consistency gaps exist and are algorithmically meaningful (supported by consistent EM/ES disparities across multiple models and algorithms)
- **Medium**: Self-reflection has diminishing returns (supported by quantitative trends but limited qualitative analysis of error types)
- **Medium**: Performance hierarchy (RLE > LZW > AE > Huffman) reflects increasing reasoning complexity (plausible but not definitively proven—could also reflect task-specific familiarity)

## Next Checks
1. **Cross-dataset generalization**: Apply RTCE to a held-out set of real-world code snippets (e.g., from CodeNet or CodeSearchNet) to test whether synthetic-data performance patterns hold. Measure correlation between synthetic and real-world Pass@5 scores.

2. **Hybrid evaluation**: Introduce a minimal execution feedback loop (e.g., decode candidate output and check if it compiles or runs without error) to distinguish between syntactic format errors and semantic algorithmic errors. This would clarify whether saturation in self-reflection is due to lack of parametric knowledge or evaluation constraints.

3. **Algorithmic ablation**: Create simplified variants of Huffman encoding (e.g., fixed frequency distribution, shorter symbol tables) to isolate whether the difficulty stems from hierarchical reasoning per se or from specific implementation details like frequency dictionary management.