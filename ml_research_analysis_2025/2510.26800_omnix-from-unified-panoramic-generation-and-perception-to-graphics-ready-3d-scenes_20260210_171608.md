---
ver: rpa2
title: 'OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready
  3D Scenes'
arxiv_id: '2510.26800'
source_url: https://arxiv.org/abs/2510.26800
tags:
- panoramic
- perception
- generation
- omnix
- scenes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniX, a unified framework for panoramic
  perception, generation, and completion that repurposes pre-trained 2D flow matching
  models. The method addresses the challenge of constructing graphics-ready 3D scenes
  by integrating intrinsic property perception (albedo, roughness, metallic, normal,
  distance) with appearance generation.
---

# OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes

## Quick Facts
- arXiv ID: 2510.26800
- Source URL: https://arxiv.org/abs/2510.26800
- Reference count: 21
- Key outcome: OmniX achieves state-of-the-art panoramic intrinsic decomposition (17.755 PSNR for albedo) and geometry estimation (0.155 AbsRel for distance) by repurposing pre-trained 2D flow matching models.

## Executive Summary
OmniX introduces a unified framework for panoramic perception, generation, and completion that leverages pre-trained 2D flow matching models to construct graphics-ready 3D scenes. The method addresses the challenge of panoramic intrinsic decomposition by integrating intrinsic property perception (albedo, roughness, metallic, normal, distance) with appearance generation through a cross-modal adapter structure. OmniX employs a Separate-Adapter architecture that preserves pre-trained weight distributions while achieving superior performance across multiple vision tasks. The framework is trained on PanoX, a synthetic dataset containing 60,000 multimodal panoramas with dense geometry and PBR material annotations, enabling automatic construction of 3D scenes suitable for physically based rendering and physical simulation.

## Method Summary
OmniX repurposes the pre-trained FLUX.1-dev flow matching model through a cross-modal adapter architecture. The framework employs Separate-Adapter modules where each input type (RGB, mask, camera ray) and output type (depth, normal, albedo, roughness, metallic) receives its own lightweight LoRA adapter, allowing the DiT backbone to process each modality without interference. The method generalizes flow matching to accept multiple spatially-aligned condition inputs, enabling a single model to perform both generation and perception tasks. OmniX is trained on PanoX, a synthetic dataset with 60,000 panoramas from 8 diverse 3D scenes rendered in Unreal Engine 5, providing ground truth for dense prediction tasks where real panoramic training data is scarce.

## Key Results
- OmniX achieves 17.755 PSNR for albedo and 0.155 AbsRel for distance on panoramic intrinsic decomposition benchmarks
- Separate-Adapter architecture improves albedo PSNR by 40% (21.682 vs 15.294) compared to Shared-Branch approach
- Framework successfully generalizes to real-world panoramas, demonstrating excellent performance on in-the-wild images despite being trained on synthetic data

## Why This Works (Mechanism)

### Mechanism 1: Separate-Adapter Architecture Preserves Generative Priors
Assigning modality-specific LoRA adapters to different input/output types improves perception performance by preserving pre-trained weight distributions better than shared adapters. Each input and output modality receives its own lightweight LoRA adapter rather than sharing weights, allowing the DiT backbone to process each modality through learned transformations that don't interfere with each other.

### Mechanism 2: Unified Flow Matching Formulation Generalizes Generation to Perception
Extending the flow matching ODE framework to accept multiple spatially-aligned condition inputs enables a single generative model to perform both generation (RGB→panorama) and perception (RGB→X) tasks. The standard flow matching objective predicts velocity vectors from noise to target latent, generalized to handle multiple condition-output pairs within the same framework.

### Mechanism 3: Synthetic Panoramic Dataset with Dense Annotations Bridges Data Gap
Training on PanoX—a synthetic dataset with 60,000 panoramas covering indoor/outdoor scenes with geometry and PBR material annotations—enables the model to learn dense prediction tasks where real panoramic training data is scarce. Unreal Engine 5 renders pixel-aligned multimodal data from 8 diverse 3D scenes, providing ground truth for supervision that would be prohibitively expensive to annotate on real panoramas.

## Foundational Learning

- **Flow Matching Models (Rectified Flow)**
  - Why needed here: OmniX builds on FLUX.1-dev, a flow matching model that learns to predict velocity vectors between noise and data distributions via ODE integration. Understanding this is essential for grasping how the adapter modifications work.
  - Quick check question: Explain how flow matching differs from diffusion (DDPM) in terms of the training objective and sampling trajectory.

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: All adapters in OmniX are implemented as LoRAs, which add trainable low-rank matrices to frozen pre-trained weights. The Separate-Adapter architecture uses multiple LoRAs per task.
  - Quick check question: Given a weight matrix W ∈ R^(d×k), write the LoRA decomposition W' = W + BA where B ∈ R^(d×r), A ∈ R^(r×k). What is the parameter reduction ratio when r << min(d,k)?

- **PBR (Physically Based Rendering) Material Properties**
  - Why needed here: The perception tasks predict albedo (diffuse color), roughness (surface microfacet variance), metallic (conductivity), normal (surface orientation), and distance—all standard PBR inputs for graphics pipelines.
  - Quick check question: Why does metallic material prediction generalize poorly compared to other modalities? (Hint: Consider what visual cues distinguish metals and the sim-to-real gap.)

## Architecture Onboarding

- **Component map:**
  - Input panorama/image → VAE encoder → latent z₀ → DiT blocks with cross-modal attention → Output latent → VAE decoder → predicted modality map → (for 3D) Distance map → point cloud → mesh → other modalities → UV textures

- **Critical path:**
  1. Input panorama/image → VAE encoder → latent z₀
  2. Condition(s) → respective LoRA adapter → cross-modal attention
  3. DiT blocks process zₜ with condition features via flow matching
  4. Output latent → VAE decoder → predicted modality map
  5. For 3D: Distance map → point cloud → mesh; other modalities → UV textures

- **Design tradeoffs:**
  - **Shared-Branch vs. Shared-Adapter vs. Separate-Adapter:** Separate-Adapter performs best (21.682 vs. 15.294 PSNR for albedo) but requires more LoRA parameters per task. Shared-Branch is most parameter-efficient but sacrifices performance.
  - **Joint vs. independent modeling:** Joint geometry modeling (distance + normal) doesn't improve performance. Independent modeling per modality is currently optimal.
  - **Camera ray conditioning:** Improves normal estimation (19.917° vs. 20.578° mean error) but not other modalities—optional based on task priority.

- **Failure signatures:**
  - **Bumpy 3D surfaces:** Distance estimation not accurate enough (AbsRel 0.158 vs. MoGe's 0.106)—affects PBR rendering quality
  - **Metallic prediction generalization:** Poor transfer to real images due to limited panoramic PBR training data
  - **ERP seam discontinuity:** DiT has difficulty learning seam continuity; requires horizontal blending
  - **Slow inference:** Inherits flow matching inefficiency from FLUX.1-dev

- **First 3 experiments:**
  1. **Reproduce adapter ablation:** Train three variants (Shared-Branch, Shared-Adapter, Separate-Adapter) on PanoX-Train subset for OmniX-Pano2Albedo. Verify performance ranking matches Table 4.
  2. **In-the-wild generalization test:** Run all perception adapters on 10 diverse real panoramas from HDR360-UHD (held out from training). Qualitatively assess which modalities transfer best.
  3. **End-to-end 3D scene pipeline:** Input a single perspective image, generate panorama, predict all modalities, reconstruct mesh in Blender. Test PBR relighting with point light animation—identify artifacts from distance estimation errors.

## Open Questions the Paper Calls Out
- How can the training and inference efficiency of the OmniX framework be significantly improved while maintaining the quality of panoramic perception and generation? (The method inherits slow training and inference from pre-trained 2D flow matching models.)
- How can the accuracy of Euclidean distance prediction be refined to prevent "bumpy" surface artifacts in the reconstructed 3D meshes? (OmniX's distance prediction is not accurate enough, resulting in bumpy surfaces.)
- To what extent can pre-trained 2D generative image priors effectively inform PBR material estimation, particularly for metallic properties which currently suffer from poor generalization? (OmniX-Pano2Metallic performs poorly in generalization due to limited benefits of 2D priors for PBR material estimation.)
- Can a native architectural solution be developed for DiT-based models to handle ERP seam continuity, removing the reliance on horizontal blending techniques? (The DiT model struggles with seam continuity due to topological limitations of 2D position encoding.)

## Limitations
- **Sim-to-real generalization gap:** Metallic material prediction generalizes poorly to real images due to limited panoramic PBR training data
- **Distance estimation accuracy:** OmniX's distance prediction is not accurate enough, resulting in bumpy reconstructed 3D surfaces that affect PBR rendering quality
- **Inference efficiency:** Inherits slow training and inference efficiency from pre-trained 2D flow matching models

## Confidence

- **High confidence** in the adapter architecture's effectiveness for perception tasks (supported by ablation showing 40% albedo PSNR improvement)
- **Medium confidence** in unified flow matching formulation's ability to generalize generation to perception (mechanism plausible but limited direct evidence)
- **Medium confidence** in synthetic data's sufficiency for training (no real panoramic PBR data exists for comparison)

## Next Checks

1. Implement and benchmark all three adapter variants (Shared-Branch, Shared-Adapter, Separate-Adapter) on a PanoX-like synthetic dataset to verify the 40% performance gap
2. Test metallic material prediction generalization on 10 real-world panoramas to quantify the sim-to-real gap mentioned in limitations
3. Evaluate end-to-end 3D reconstruction quality by rendering the output under varying lighting conditions to identify artifacts from distance estimation errors