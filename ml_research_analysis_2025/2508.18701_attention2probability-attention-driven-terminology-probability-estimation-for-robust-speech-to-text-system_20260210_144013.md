---
ver: rpa2
title: 'Attention2Probability: Attention-Driven Terminology Probability Estimation
  for Robust Speech-to-Text System'
arxiv_id: '2508.18701'
source_url: https://arxiv.org/abs/2508.18701
tags:
- speech
- terms
- terminology
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Attention2Probability (A2P), a cross-attention-based
  method for terminology retrieval in speech-to-text tasks. A2P converts cross-attention
  weights between speech and terminology into presence probabilities, avoiding the
  high training costs and poor retrieval accuracy of VectorDB methods.
---

# Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System

## Quick Facts
- arXiv ID: 2508.18701
- Source URL: https://arxiv.org/abs/2508.18701
- Reference count: 12
- Primary result: Cross-attention-based terminology retrieval achieves 92.57% recall for Chinese speech, outperforming VectorDB semantic similarity methods

## Executive Summary
This paper introduces Attention2Probability (A2P), a cross-attention-based method for terminology retrieval in speech-to-text systems. A2P converts cross-attention weights between speech and terminology into presence probabilities, avoiding the high training costs and poor retrieval accuracy of VectorDB methods. The authors construct and release a new speech dataset with terminology using MegaTTS and open-source translation data. Experiments show A2P achieves superior recall rates while maintaining low latency, and improves terminology accuracy by 6-17% in downstream speech recognition and translation tasks.

## Method Summary
Attention2Probability (A2P) uses a frozen Qwen2-Audio-Instruction encoder and a trainable 80M-parameter cross-attention retriever to convert speech and terminology into presence probabilities. The method employs token-level pooling to aggregate subword attention into term-level estimates and uses a three-stage curriculum learning approach (word → phrase → real-term) to handle limited training data. Terms are retrieved via top-k selection and injected into prompts for speech language models, improving terminology accuracy in ASR and ST tasks.

## Key Results
- Achieves 92.57% recall for Chinese terminology retrieval vs. 69.98% for VectorDB
- Achieves 86.83% recall for English terminology retrieval vs. 45.60% for VectorDB
- Maintains 8.71ms latency per query, improving terminology accuracy by 6-17% in downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
Cross-attention weights between speech and terminology can be converted to presence probabilities, outperforming VectorDB semantic similarity. Multi-head cross-attention computes attention weights between speech features and term embeddings that directly indicate term presence probability rather than semantic relatedness. Core assumption: Cross-attention activations correlate with actual term occurrence in speech. Evidence: A2P significantly outperforms VectorDB; VectorDB identifies semantic similarity rather than occurrence probability. Break condition: If cross-attention reflects semantic association rather than phonetic/acoustic matching, recall may degrade for homophones.

### Mechanism 2
Token-level pooling with length normalization is necessary to aggregate subword attention into term-level probability estimates. Terms are tokenized into subwords, and pooling operation sums masked attention weights across tokens then normalizes by term length. Core assumption: Summing token-level attention preserves term integrity without losing fine-grained acoustic signals. Evidence: Ablating token-level pooling drastically reduces recall to below 1%. Break condition: If terms have highly variable token lengths or subword boundaries misalign with acoustic segments, pooling may introduce noise.

### Mechanism 3
Curriculum learning is essential when training data is limited; direct training on real terminology fails. Three-stage training progression: (1) Word-level → uniform length, reduced difficulty; (2) Phrase-level (1-4 words) → variable length exposure; (3) Real-term level → authentic terminology. Core assumption: The ~100k sample training corpus is below the scaling law threshold for direct training. Evidence: Directly training on real-term datasets yields no measurable performance. Break condition: If training data scales significantly, curriculum learning may become unnecessary overhead.

## Foundational Learning

- **Cross-Attention in Transformer Architectures**: Why needed: The entire A2P mechanism depends on understanding how query-key-value attention computes relevance scores between two sequences. Quick check: Can you explain why cross-attention differs from self-attention, and what the output represents?

- **Speech-Text Modality Alignment**: Why needed: A2P relies on Qwen2-Audio-Instruction's pre-aligned feature space; using SONAR features caused failure due to dimension mismatch. Quick check: Why would an audio encoder with ultra-high-dimensional features fail in a lightweight cross-attention retriever?

- **Curriculum Learning Strategy**: Why needed: Training stability depends on progressive difficulty; without this, the retriever cannot learn from limited terminology data. Quick check: What happens if you skip the word-level and phrase-level stages and train directly on real terminology?

## Architecture Onboarding

- **Component map**: Speech Input → Qwen2-Audio Encoder (frozen) → Speech Features (h_s) → Cross-Attention (trainable) → Token-Level Pooling + Sigmoid → Top-k Selection → Retrieved Terms → Prompt Construction → SLM (Qwen2-Audio)

- **Critical path**: 1. Audio encoder selection (Qwen2-Audio-Instruction required; SONAR fails on Chinese) 2. Token-level pooling implementation (ablation shows <1% recall without it) 3. Curriculum learning schedule (direct training yields 0% recall)

- **Design tradeoffs**: Accuracy vs. Latency: A2P achieves 86-92% recall but 8.71ms latency vs. VectorDB's 0.74ms. Training cost vs. Retrieval quality: No modal alignment training needed, but requires curriculum learning pipeline. Term bank size scalability: Recall degrades as term bank grows; VectorDB speed remains stable.

- **Failure signatures**: Using SONAR encoder: Chinese recall drops to 0%, English to 15-33%. Removing token-level pooling: Recall <1%. Skipping curriculum learning: No measurable performance. Large term banks with English: Performance degradation due to phonetic false recalls.

- **First 3 experiments**: 1. Encoder validation: Test A2P with Qwen2-Audio-Instruction vs. Qwen-Audio-Chat vs. SONAR on a small held-out set. Expect: Qwen2 > Qwen-Chat > SONAR. 2. Pooling ablation: Disable token-level pooling and measure recall drop. Expect: >90% relative degradation. 3. Curriculum learning sweep: Train with only word-level, only phrase-level, and full 3-stage curriculum. Compare recall curves. Expect: Full curriculum > phrase-only > word-only > direct training (0%).

## Open Questions the Paper Calls Out

### Open Question 1
How can Speech Large Language Models (SLMs) preserve baseline ASR/ST performance when processing multiple retrieved terms? Basis: The authors list this as a challenge, noting SLM performance doesn't strictly improve with increasing terminology volume. Unresolved because increasing intervention terms degraded performance, suggesting limitations in ICL capacity. Evidence: Training strategy or prompting method that maintains or improves baseline BLEU/WER scores as term count increases.

### Open Question 2
How can terminology accuracy in Speech Translation (ST) tasks be effectively increased? Basis: Authors explicitly ask this in conclusion. Unresolved because while A2P improves ST term accuracy by ~13%, it remains significantly lower than retrieval recall. Evidence: Intervention method achieving ST terminology accuracy comparable to high retriever recall rates.

### Open Question 3
Can A2P be optimized to achieve retrieval speeds comparable to VectorDB methods on large-scale terminology banks? Basis: Conclusion states A2P exhibits high latency compared to VectorDB (126.31ms vs. 0.91ms for 10k terms). Unresolved because Top-k selection creates O(N log K) bottleneck. Evidence: Architectural modification reducing A2P latency to near-real-time levels (<10ms) for databases exceeding 10k terms.

### Open Question 4
Is A2P universally applicable to other audio encoders, or is it over-reliant on Qwen2-Audio's specific alignment capabilities? Basis: Table 2 shows A2P performance collapses when switching encoder from Qwen2-Audio-Instruction to SONAR or Qwen-Audio-Chat. Unresolved because authors hypothesize lightweight retriever cannot handle SONAR's high-dimensional features. Evidence: Successful training on alternative audio encoders without requiring proportional scaling of retriever parameters.

## Limitations
- Theoretical foundation of cross-attention as probability estimator is unverified - paper demonstrates empirical superiority but lacks formal proof
- Heavy dependence on Qwen2-Audio-Instruction's pre-aligned feature space limits architectural flexibility
- Scalability analysis for larger term banks is speculative - performance degradation for English fusional morphology noted but not quantitatively analyzed

## Confidence

**High Confidence**: Empirical results showing A2P's superiority over VectorDB in recall rates (92.57% vs. 69.98% for Chinese, 86.83% vs. 45.60% for English) are well-documented with controlled experiments. Ablation studies demonstrating necessity of token-level pooling and curriculum learning are robust.

**Medium Confidence**: Claim that cross-attention weights represent presence probabilities rather than semantic similarity is supported by empirical results but lacks theoretical proof. Mechanism works in practice but underlying reason remains unclear.

**Low Confidence**: Scalability analysis for larger term banks is speculative. Paper notes performance degradation for English fusional morphology but does not provide quantitative analysis of how recall scales with term bank size.

## Next Checks

1. **Theoretical validation of probability estimation**: Design controlled experiment comparing cross-attention weights against ground-truth occurrence labels across diverse acoustic conditions. Measure correlation coefficients and test relationship for homophones, acoustically similar terms, and varying speech rates.

2. **Curriculum learning scaling threshold**: Systematically vary training dataset size from 10k to 1M samples while keeping curriculum structure constant. Plot recall curves to identify inflection point where curriculum learning becomes unnecessary.

3. **Cross-attention mechanism ablation**: Replace cross-attention layer with alternative attention mechanisms (self-attention, additive attention, or simple dot-product similarity) while keeping all other components constant. Compare recall rates to determine if simpler attention mechanisms could achieve similar performance with reduced computational overhead.