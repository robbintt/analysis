---
ver: rpa2
title: 'Fat-to-Thin Policy Optimization: Offline RL with Sparse Policies'
arxiv_id: '2501.14373'
source_url: https://arxiv.org/abs/2501.14373
tags:
- policy
- fttpo
- learning
- sparse
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fat-to-Thin Policy Optimization (FtTPO), the
  first offline reinforcement learning algorithm designed to handle sparse policies
  that assign zero probability to some actions. FtTPO addresses the challenge of out-of-support
  actions in offline learning by maintaining a fat (heavy-tailed) proposal policy
  to learn from the dataset and a thin (sparse) policy to interact with the environment.
---

# Fat-to-Thin Policy Optimization: Offline RL with Sparse Policies

## Quick Facts
- arXiv ID: 2501.14373
- Source URL: https://arxiv.org/abs/2501.14373
- Authors: Lingwei Zhu; Han Wang; Yukie Nagai
- Reference count: 38
- Primary result: Introduces the first offline RL algorithm using sparse policies to avoid out-of-support action problems, outperforming Gaussian-policy baselines on safety-critical and MuJoCo tasks.

## Executive Summary
Fat-to-Thin Policy Optimization (FtTPO) addresses the out-of-support action problem in offline reinforcement learning by introducing a two-policy architecture. The method uses a heavy-tailed proposal policy to learn from the dataset and transfer knowledge to a sparse actor policy that concentrates on a small band of actions. This design allows FtTPO to maintain beneficial stochasticity while avoiding dangerous extrapolation beyond the dataset's support. Experiments on a safety-critical treatment simulation and MuJoCo suite demonstrate that sparse policies can achieve superior performance compared to standard Gaussian policies.

## Method Summary
FtTPO is a two-stage offline RL algorithm that maintains a fat (heavy-tailed) proposal policy and a thin (sparse) actor policy, both instantiated as q-Gaussians. The proposal policy (q=2) learns from the dataset using forward KL divergence with q-exponential advantage weighting, while the actor policy (q=0) learns from the proposal via reverse KL. Before each actor update, the proposal's mean parameters are copied to the actor to initialize it in a high-value region. The method uses the Generalized Box-Müller Method for q-Gaussian sampling and filters low-advantage actions during proposal training using q-exponential weights with q<1.

## Key Results
- FtTPO outperforms popular offline algorithms using Gaussian policies by default on both safety-critical treatment simulation and MuJoCo suite
- Sparse policies achieve better performance by concentrating probability mass on a small band of actions rather than having overly large randomness
- Heavy-tailed q-Gaussian proposal policy outperforms standard Gaussian proposal in ablation studies
- Simple KL minimization for actor learning matches performance of more sophisticated SPOT actor learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A heavy-tailed proposal policy enables learning from offline datasets without out-of-support errors, then transfers knowledge to a sparse actor policy.
- Mechanism: The proposal policy π_φ uses q-Gaussian with q=2 (heavy-tailed, infinite support), allowing it to evaluate log-likelihood for all dataset actions. The actor policy π_θ uses q-Gaussian with q=0 (sparse, bounded support) and learns via reverse KL divergence by sampling actions from itself—avoiding out-of-support evaluations. Mean parameters are copied from proposal to actor before each update to initialize the actor in a high-value region.
- Core assumption: The behavior policy's support contains sufficient information about high-value actions that the sparse policy can retain after truncating the heavy tails.
- Evidence anchors: [abstract] "We maintain a fat (heavy-tailed) proposal policy that effectively learns from the dataset and injects knowledge to a thin (sparse) policy"
- Break condition: If dataset contains critical actions far from the mean that sparse support cannot cover, the thin policy may fail to learn optimal behavior.

### Mechanism 2
- Claim: q-exponential advantage weighting with q < 1 naturally filters out low-advantage actions during proposal policy training.
- Mechanism: The weighting function w(s,a) = exp_q((Q(s,a)-V(s))/τ) with q=0 truncates actions where advantage falls below -τ/(1-q). Unlike exponential weighting (which assigns non-zero weight to all actions), this creates sparse gradients that focus learning on beneficial actions while ignoring harmful ones.
- Core assumption: A sharp threshold on advantage values correctly separates "good" from "bad" actions without discarding useful exploration.
- Evidence anchors: [Section 4.3] "the q-exp weights will truncate actions with advantage Q(s,a) - V(s) < -τ/(1-q)"
- Break condition: If temperature τ is poorly tuned, the threshold may filter too aggressively (missing useful actions) or too weakly (retaining harmful actions).

### Mechanism 3
- Claim: Sparse policies with bounded stochasticity achieve better performance than infinite-support policies in safety-critical tasks by concentrating probability mass.
- Mechanism: The sparse q-Gaussian (q < 1) has bounded support, assigning strictly zero probability to actions outside its range. This forces the policy to allocate all probability mass within a controllable region. During deployment, actions are sampled only from this safe region, avoiding dangerous extrapolation while maintaining beneficial randomness within bounds.
- Core assumption: Optimal or near-optimal actions exist within a bounded region that can be identified and concentrated upon.
- Evidence anchors: [Section 5.1] "FtTPO managed to learn a sparse yet stochastic policy tightly concentrating around a small band of actions"
- Break condition: If the environment requires diverse actions across a wide range, sparse policies may lack sufficient exploration capacity.

## Foundational Learning

- Concept: **Forward vs Reverse KL Divergence**
  - Why needed here: The out-of-support problem occurs specifically with forward KL (sampling from data, evaluating under policy). Understanding this distinction is essential for grasping why the two-stage approach works.
  - Quick check question: If you minimize forward KL between a sparse policy and data distribution, what happens when data samples fall outside the policy support?

- Concept: **q-Exponential Family and Tsallis Statistics**
  - Why needed here: FtTPO instantiates both policies using q-Gaussians, which require understanding how the entropic index q controls support (sparse for q<1, Gaussian for q=1, heavy-tailed for q>1) and how to sample via the Generalized Box-Müller Method.
  - Quick check question: What range of q values produces sparse distributions with bounded support, and what happens at the boundary of that support?

- Concept: **Offline RL Distributional Shift Problem**
  - Why needed here: The fundamental challenge FtTPO addresses—policies can query actions not in the dataset, leading to unreliable Q-value estimates—provides context for why behavior-constrained learning matters.
  - Quick check question: Why does evaluating Q-values for out-of-distribution actions cause problems in offline RL, and how does FtTPO's approach differ from standard behavior regularization?

## Architecture Onboarding

- Component map:
  - Proposal Policy Network (π_φ) -> Actor Policy Network (π_θ) -> Q-Network (Q_ψ) -> V-Network (V_ζ) -> Sampling Module

- Critical path:
  1. Sample state s from dataset D and action a from behavior policy
  2. Compute Q_ψ(s,a) and V_ζ(s) for q-exponential weighting
  3. Update proposal policy φ by minimizing weighted negative log-likelihood
  4. Copy μ_φ → μ_θ (mean transfer)
  5. Sample actions â from actor policy π_θ
  6. Update actor policy θ by minimizing reverse KL divergence estimator

- Design tradeoffs:
  - KL estimator vs SPOT: Paper shows simple KL minimization matches performance of more sophisticated SPOT actor learning (Figure 6)
  - Heavy-tailed vs Gaussian proposal: Ablation shows heavy-tailed q-Gaussian (q=2) outperforms standard Gaussian proposal (FtTPO-SG)
  - Two networks vs single network: Training time approximately doubles vs baselines (~15 hours vs 6-8.5 hours for 1M steps)

- Failure signatures:
  - Out-of-support log-likelihood returns -∞: Indicates sparse policy evaluated on unsupported actions; verify you're sampling from π_θ (not dataset) for actor loss
  - Mode collapse to deterministic policy: May indicate mean transfer not occurring or learning rate too high; check copy operation and reduce learning rate
  - Slow learning with reverse KL alone: Confirmed in paper (Figure 1); two-stage approach is required, not just reverse KL
  - Excessive clipping at action boundaries: Sparse policy std clipped at action space bounds; if density concentrates at boundaries, may need different initialization

- First 3 experiments:
  1. Synthetic treatment environment (8-dim state, 1-dim action): 50 trajectories × 24 steps; verify sparse policy concentrates around optimal dosage without collapsing. Should achieve highest scores with tight action concentration visible in density plots.
  2. HalfCheetah Medium-Expert: Run 1M steps, compare FtTPO vs IQL vs SQL. Expect FtTPO to match or exceed baselines; visualize first action dimension evolution to confirm proposal locates high-reward region then actor concentrates.
  3. Ablation on proposal policy type: Compare FtTPO (q=2 proposal) vs FtTPO-SG (Gaussian proposal) on 3 MuJoCo environments. Expect heavy-tailed proposal to outperform Gaussian, validating the "fat" design choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do sparse policies impact the exploration-exploitation tradeoff in an online reinforcement learning setting?
- Basis in paper: [explicit] The conclusion states that while the paper focused on offline learning, it remains an open question what specific effects sparse policies bring to the "exploitation-exploration tradeoff in the online context."
- Why unresolved: FtTPO is designed for offline RL where exploration is effectively disabled; the behavioral implications of a finite support policy during online interaction remain unanalyzed.
- What evidence would resolve it: Empirical evaluations of FtTPO or similar sparse policies in online RL benchmarks, specifically measuring sample efficiency and coverage of the state-action space compared to Gaussian policies.

### Open Question 2
- Question: Can FtTPO effectively handle safety-critical tasks where safety constraints are separate from the reward function?
- Basis in paper: [explicit] The authors note in Section 5.1 that they assume safety is coded into the reward, but "extra care is required when reward and safety need to be considered separately," explicitly deferring this investigation to future work.
- Why unresolved: The current experiments rely on a simulation where maximum reward implies maximum safety, which does not reflect all real-world scenarios where safety is a hard constraint or cost function.
- What evidence would resolve it: Experiments on constrained RL benchmarks (e.g., Safety Gym) to verify if sparse policies reduce constraint violations compared to Gaussian policies when safety is not explicitly rewarded.

### Open Question 3
- Question: Can we theoretically quantify how exploitation capability is gained at the cost of exploration when truncating heavy tails?
- Basis in paper: [explicit] The conclusion identifies the need to "analyze how exploitation capability is gained at the cost of exploration by truncating the heavy tails."
- Why unresolved: The paper empirically demonstrates that sparse policies work but lacks a theoretical derivation of the efficiency trade-offs involved in removing distribution tails.
- What evidence would resolve it: A theoretical analysis formally relating the support size of the policy to the bias in gradient estimates or the convergence properties of the optimization process.

## Limitations
- Sparse policies may fail when optimal actions require diverse exploration across a wide range of the action space
- Limited experimental scope with only three MuJoCo tasks and one synthetic treatment environment tested
- Claims about natural filtering properties of q-exponential weights lack rigorous theoretical analysis of threshold selection

## Confidence
- **High confidence**: The mechanism for avoiding out-of-support errors through the two-policy architecture is sound and well-demonstrated. The q-Gaussian parameterization correctly implements the desired sparsity properties.
- **Medium confidence**: The empirical advantage over Gaussian baselines is convincing on tested tasks, but the results may not generalize to environments requiring broader action exploration or those with safety constraints that differ from the treatment simulation.
- **Low confidence**: Claims about the natural filtering properties of q-exponential weights and the general superiority of sparse policies across diverse domains are not adequately supported by current evidence.

## Next Checks
1. Test FtTPO on D4RL tasks requiring diverse exploration (e.g., Ant, Humanoid) to evaluate whether sparse policies can handle environments where optimal actions span a wide range.
2. Perform ablation studies varying the q parameter in the actor policy (0 < q < 1) to quantify the tradeoff between sparsity and exploration capacity, and identify the optimal q for different task types.
3. Evaluate FtTPO on safety-critical domains with different constraint structures (e.g., robotic manipulation with joint limits) to assess generalization of the safety benefits demonstrated in the treatment simulation.