---
ver: rpa2
title: Minimal-Edit Instruction Tuning for Low-Resource Indic GEC
arxiv_id: '2512.00219'
source_url: https://arxiv.org/abs/2512.00219
tags:
- error
- syntax
- hindi
- punctuation
- malayalam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address grammatical error correction for low-resource Indic
  languages (Hindi, Malayalam) under severe supervision constraints. Our method uses
  minimal-edit instruction tuning on a 12B Gemma 3 model, coupled with deterministic
  decoding and lightweight post-processing.
---

# Minimal-Edit Instruction Tuning for Low-Resource Indic GEC

## Quick Facts
- **arXiv ID**: 2512.00219
- **Source URL**: https://arxiv.org/abs/2512.00219
- **Reference count**: 28
- **Primary result**: Instruction-tuned Gemma 3 12B achieves 92.41 GLEU on Malayalam and 81.44 GLEU on Hindi under minimal-edit GEC in the BHASHA IndicGEC benchmark.

## Executive Summary
This work addresses grammatical error correction for low-resource Indic languages (Hindi, Malayalam) under severe supervision constraints. The method uses minimal-edit instruction tuning on a 12B Gemma 3 model, coupled with deterministic decoding and lightweight post-processing. Error-type analysis on training data informs fixed, language-specific prompts that prioritize punctuation, auxiliary, and morphology repairs while discouraging reordering. Evaluation under the BHASHA IndicGEC benchmark shows strong performance, demonstrating that classifier-guided prompt design, adapter-based instruction tuning, and conservative decoding offer a reproducible, efficient alternative to augmentation-heavy pipelines.

## Method Summary
The approach follows a two-stage pipeline: (1) instruction fine-tuning of Gemma 3 12B with PEFT/LoRA adapters on attention projections using Alpaca-style Alpaca-style formatting via Unsloth, and (2) inference with greedy decoding and a lightweight normalizer for whitespace, punctuation spacing, and prompt echo removal. Training data is minimal—600 Hindi and 300 Malayalam sentence pairs—so prompt design is derived from deterministic error classifiers to enforce minimal edits. The model is loaded with 4-bit quantization and trained with minimal-edit constraints (fewest changes, no paraphrasing, preserve numerals/named entities). Post-processing ensures outputs conform to standard punctuation and spacing rules.

## Key Results
- **Malayalam GLEU**: 92.41 (6th rank on BHASHA IndicGEC benchmark)
- **Hindi GLEU**: 81.44 (3rd rank on BHASHA IndicGEC benchmark)
- **Minimal-edit focus**: Prompts derived from error-type taxonomy successfully discourage paraphrasing and reordering, yielding high surface-level correction fidelity.

## Why This Works (Mechanism)
The method leverages minimal-edit instruction tuning to constrain the model to make the fewest necessary corrections, avoiding overcorrection and paraphrastic drift. By using classifier-informed prompts, the system explicitly guides the model toward specific error types (punctuation, auxiliaries, morphology) while discouraging others (word reordering). This targeted prompting, combined with deterministic greedy decoding and post-processing normalization, ensures reproducibility and efficiency without heavy data augmentation.

## Foundational Learning
- **Indic script normalization (NFKC)**: Ensures consistent Unicode representation across Hindi (Devanagari) and Malayalam (Malayalam script), preventing tokenization mismatches.
- **GLEU metric**: Corpus-level n-gram overlap scorer used for GEC; requires no variant tuning and is the official benchmark metric.
- **Minimal-edit prompting**: Instructs the model to make only necessary changes, avoiding paraphrastic drift; crucial for low-resource settings where overfitting is a risk.
- **PEFT/LoRA adapters**: Parameter-efficient fine-tuning on attention projections reduces compute and storage while preserving model capacity.
- **Alpaca-style instruction format**: Standardizes input-output pairs for instruction tuning, ensuring compatibility with Gemma 3's instruction-tuned weights.

## Architecture Onboarding

**Component map**: Dataset (CSV) -> Normalization (NFKC, whitespace, danda) -> Classifier error taxonomy -> Alpaca prompt templates -> Gemma 3 12B (4-bit, LoRA) -> Instruction tuning -> Greedy decoding -> Normalizer -> GLEU evaluation

**Critical path**: Dataset preprocessing → Classifier error taxonomy → Prompt design → Instruction tuning → Inference (greedy) → Post-processing → GLEU scoring

**Design tradeoffs**: Minimal-edit prompting vs. recall; 4-bit quantization vs. numerical precision; LoRA on attention projections vs. full fine-tuning; greedy decoding vs. beam search (trade accuracy for speed and reproducibility).

**Failure signatures**: Overcorrection (paraphrastic drift reducing GLEU); punctuation/whitespace mismatches causing spurious GLEU drops; token fragmentation for Indic scripts.

**First experiments**:
1. Test Alpaca prompt templates with a held-out dev set to confirm minimal-edit constraints are preserved.
2. Run inference with greedy decoding and measure GLEU sensitivity to minor post-processing changes.
3. Apply the pipeline to a held-out Indic language (e.g., Bengali) using its error-type classifier output to verify generalizability.

## Open Questions the Paper Calls Out
- Would integrating human judgments of meaning preservation reveal systematic blind spots in GLEU-based evaluation for conservative GEC systems?
- Can stronger morphosyntactic constraints (e.g., grammar-aware decoding or morphological taggers) further reduce overcorrection without sacrificing recall?
- Does classifier-informed prompt design overfit to development-set error distributions, degrading performance under distribution shift?
- Would multi-label error classification capturing cross-category dependencies improve prompt design over single-label assignment?

## Limitations
- GLEU scores are sensitive to exact prompt design and deterministic decoding configuration; without full prompt and LoRA hyperparameters, reproduction may yield different scores.
- The approach is benchmark-specific and has not been tested on other Indic languages or non-GEC grammatical tasks.
- No human evaluation of meaning preservation was conducted, so semantic fidelity is not independently verified.

## Confidence
- **High confidence**: GLEU evaluation setup (official BHASHA harness, corpus-level scoring, greedy decoding baseline) and preprocessing pipeline (Unicode NFKC, whitespace collapse, danda normalization).
- **Medium confidence**: Adapter-based instruction tuning with Alpaca-style formatting as a viable minimal-edit strategy, given the explicit two-stage pipeline and known use of LoRA on attention projections.
- **Low confidence**: Absolute GLEU scores (Malayalam 92.41, Hindi 81.44) without full prompt and training configuration disclosure; the minimal-edit claim hinges on prompt fidelity that is not fully reproducible from the paper alone.

## Next Checks
1. **Prompt and adapter audit**: Retrieve and test the exact Alpaca templates and LoRA hyperparameters (rank, alpha, target modules) against the published error-type taxonomy to confirm minimal-edit constraints are preserved.
2. **Score stability under perturbation**: Run inference with slight prompt or decoding variations (e.g., top-k=1 vs greedy, minor punctuation spacing changes) to quantify GLEU sensitivity and confirm the reported results are not artifacts of a narrow configuration.
3. **Cross-lingual minimal-edit transfer**: Apply the same pipeline to a held-out Indic language (e.g., Bengali) using its error-type classifier output to verify that minimal-edit instruction tuning generalizes beyond the two benchmarked languages.