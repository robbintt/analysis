---
ver: rpa2
title: 'SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification'
arxiv_id: '2504.14223'
source_url: https://arxiv.org/abs/2504.14223
tags:
- language
- text
- simplification
- plain
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents SimplifyMyText, the first LLM-based system
  for transforming complex text into plain language tailored for specific audiences.
  The system accepts multiple input formats (typed text, PDFs, Word documents) and
  offers five audience-specific simplification options: Scientists and Researchers,
  Students and Academics, Industry Professionals, Journalists and Media Professionals,
  and General Public.'
---

# SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification

## Quick Facts
- arXiv ID: 2504.14223
- Source URL: https://arxiv.org/abs/2504.14223
- Reference count: 25
- Primary result: First LLM-based system for audience-specific plain language text simplification using zero-shot learning

## Executive Summary
SimplifyMyText is a web-based platform that transforms complex text into plain language tailored for specific audiences. The system accepts multiple input formats including typed text, PDFs, and Word documents, offering five audience-specific simplification options: Scientists and Researchers, Students and Academics, Industry Professionals, Journalists and Media Professionals, and General Public. It leverages GPT-4o and Llama-3.1 through zero-shot learning without requiring task-specific training. The platform includes expert mode for advanced editing with synonym/definition lookup and adjustable complexity levels, plus a rating feature for continuous improvement. Evaluation on the PKWP dataset using BLEU, SARI, and Flesch-Kincaid metrics shows consistent performance across both models, demonstrating effective simplification for diverse audiences.

## Method Summary
The system employs zero-shot learning with GPT-4o (default) and Llama-3.1 70B models to perform text simplification. It uses prompt engineering conditioned on specific audience personas to generate lexically and structurally distinct outputs. The React frontend handles user interaction and file inputs, while the Node.js backend manages API routing and file parsing. Expert mode provides interactive complexity calibration through synonym/definition lookup and adjustable complexity levels. The system was evaluated on the PKWP dataset using automated metrics including BLEU (0.414-0.509 range), SARI (38.97-41.27), and Flesch-Kincaid readability scores (66.03-75.20).

## Key Results
- GPT-4o achieves higher Flesch-Kincaid Ease scores (75.20 max) while Llama-3.1 outperforms in BLEU (0.509 max) and SARI (41.27 max) metrics
- All five audience categories produce statistically distinct outputs across readability metrics
- The system successfully handles multiple input formats including PDFs and Word documents
- Expert mode provides granular control over simplification complexity with real-time editing capabilities

## Why This Works (Mechanism)

### Mechanism 1: Audience-Conditioned Zero-Shot Prompting
The system injects a "target audience" variable into the LLM context, leveraging the model's pre-training data to adjust vocabulary density and sentence complexity. This approach generates lexically and structurally distinct outputs for different personas without task-specific fine-tuning.

### Mechanism 2: Trade-off Management via Model Selection
Selecting between GPT-4o and Llama-3.1 allows users to navigate the readability-accuracy trade-off. GPT-4o prioritizes Flesch-Kincaid Ease for maximum readability, while Llama-3.1 emphasizes BLEU/SARI scores for better semantic preservation.

### Mechanism 3: Interactive Complexity Calibration (Expert Mode)
Expert mode transforms the system from a single-pass generator to a recursive editor, allowing users to verify and adjust LLM outputs through synonym/definition lookup and complexity level adjustments.

## Foundational Learning

- **Concept: Zero-Shot Generalization**
  - Why needed: System relies on model's ability to follow simplification instructions without specific training examples for different audience personas
  - Quick check: If you change the prompt to "Explain like I'm five," does the model restrict vocabulary based on general knowledge of child development, or does it require specific fine-tuning examples?

- **Concept: Flesch-Kincaid Readability Formula**
  - Why needed: Primary metric cited to prove system effectiveness, calculating readability based on syllables per word and words per sentence
  - Quick check: If a text has high FK Ease (easy) but uses incorrect metaphors, is it actually "plain language"? (The metric captures syntax, not semantic clarity)

- **Concept: BLEU vs. SARI in Text Simplification**
  - Why needed: Evaluation shows divergence between these metrics - BLEU measures similarity to reference, while SARI measures simplification operations
  - Quick check: Why might a high BLEU score be bad for simplification? (Copying complex source text word-for-word yields perfect BLEU but zero simplification)

## Architecture Onboarding

- **Component map:** React.js frontend -> Node.js backend -> GPT-4o/Llama-3.1 inference engine -> Prompt orchestrator
- **Critical path:** Input Parsing (PDF/Word → Plain Text) → Prompt Construction (System Prompt + Audience Persona + Input Text) → Model Inference (Completion call) → Post-processing (Rendering interactive UI for word-lookup)
- **Design tradeoffs:** Closed vs. Open Source (GPT-4o for better readability vs. Llama-3.1 for data privacy), Automatic vs. Manual (default automatic vs. expert mode for control)
- **Failure signatures:** Format loss from PDF parsing, metric gaming by chopping sentences without simplifying vocabulary, hallucination inserting non-existent information
- **First 3 experiments:** 1) Persona Divergence Test: Run identical text through all 5 audience settings and plot FK-Grade distribution; 2) Metric Correlation Check: Manually evaluate 20 random outputs and correlate with BLEU/SARI scores; 3) Latency Profiling: Measure end-to-end latency for GPT-4o vs. Llama-3.1 on 1000-word document

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can reinforcement learning from user feedback improve the quality and adaptability of the simplification system compared to the current zero-shot implementation?
- Basis in paper: [explicit] Authors state they "plan to leverage user feedback through reinforcement learning to further improve the quality and adaptability of text simplification."
- Why unresolved: Current system relies solely on zero-shot learning; proposed reinforcement learning is a future objective rather than tested feature
- What evidence would resolve it: Comparative evaluation showing performance metrics and user satisfaction ratings before and after implementing reinforcement learning loop

### Open Question 2
- Question: How well do the reported automated metric scores (BLEU, SARI, Flesch-Kincaid) correlate with human judgments of readability and appropriateness for specific target audiences?
- Basis in paper: [inferred] Evaluation relies strictly on automated metrics, but paper emphasizes goal of "social inclusion" and serving diverse audiences whose comprehension needs are nuanced and difficult to capture algorithmically
- Why unresolved: High scores on standard metrics don't necessarily guarantee text is "plain" or accessible to humans with low literacy, nor verify perfect meaning preservation for specific professional audiences
- What evidence would resolve it: User study involving human participants from target demographics assessing comprehensibility and utility of simplified texts

### Open Question 3
- Question: Does the performance observed on the Wikipedia-based PKWP dataset generalize to specialized, high-stakes domains such as healthcare, law, or government administration?
- Basis in paper: [inferred] Introduction highlights need for plain language in sectors like healthcare and government, but evaluation is restricted to PKWP dataset derived from Wikipedia articles
- Why unresolved: LLMs may exhibit different hallucination rates or simplification capabilities when handling complex jargon in legal or medical documents compared to general knowledge content
- What evidence would resolve it: Benchmarking system on domain-specific corpus (e.g., medical discharge summaries or legal contracts) to assess if readability and meaning retention remain consistent

## Limitations

- System effectiveness critically depends on undisclosed prompt engineering quality for each audience persona
- Zero-shot approach may fail on highly technical or domain-specific content where pre-trained models lack contextual understanding
- Performance metrics from PKWP dataset (Wikipedia-based) may not generalize to specialized domains like healthcare or legal content

## Confidence

- **High Confidence:** System architecture (React + Node.js + GPT-4o/Llama-3.1) and basic evaluation methodology are well-specified and reproducible
- **Medium Confidence:** Reported performance metrics are plausible given model capabilities, but exact reproduction requires undisclosed prompt templates
- **Low Confidence:** Claim that five distinct audience personas produce meaningfully different outputs assumes model's internal representations for these personas are sufficiently distinct, which may not hold for all content types

## Next Checks

1. **Persona Differentiation Test:** Run identical complex text through all five audience settings and statistically analyze FK-Grade distribution to verify persona-based output separation
2. **Human Evaluation Correlation:** Manually assess 20 random outputs for semantic fidelity and correlate with BLEU/SARI scores to determine which metric better predicts human judgment quality
3. **Cross-Dataset Validation:** Test the system on an independent text simplification dataset (e.g., Newsela) to verify generalizability beyond the PKWP dataset used in evaluation