---
ver: rpa2
title: Selective Adversarial Attacks on LLM Benchmarks
arxiv_id: '2510.13570'
source_url: https://arxiv.org/abs/2510.13570
tags:
- arxiv
- attacks
- target
- adversarial
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of selective adversarial attacks
  on large language model (LLM) benchmarks, where the goal is to degrade performance
  of a target model while preserving accuracy of reference models. Using the MMLU
  benchmark, the authors formalize selectivity, apply standard attacks (BAE, DeepWordBug)
  from TextAttack under a custom constraint, and propose a surrogate-model pipeline
  to generate perturbations without accessing target internals.
---

# Selective Adversarial Attacks on LLM Benchmarks

## Quick Facts
- arXiv ID: 2510.13570
- Source URL: https://arxiv.org/abs/2510.13570
- Reference count: 34
- Key outcome: Selective adversarial attacks can degrade target LLM accuracy while preserving reference models, challenging benchmark reliability.

## Executive Summary
This paper introduces selective adversarial attacks on LLM benchmarks, where the goal is to degrade performance of a target model while preserving accuracy of reference models. Using the MMLU benchmark, the authors formalize selectivity, apply standard attacks (BAE, DeepWordBug) from TextAttack under a custom constraint, and propose a surrogate-model pipeline to generate perturbations without accessing target internals. Empirically, they show that word-level attacks are more effective than character-level, with Qwen models exhibiting higher sensitivity. Selective attacks exist and can materially alter model rankings, challenging the reliability of leaderboard-driven evaluation.

## Method Summary
The authors introduce selective adversarial attacks targeting specific LLM models while preserving performance on reference models. They use TextAttack to implement BAE (word-level) and DeepWordBug (character-level) attacks, adding a custom constraint that requires the target model to fail while reference models succeed. For gray-box settings, they propose a surrogate model trained via DPO on a preference dataset of selective vs non-selective paraphrases. The method is evaluated on MMLU benchmark with Qwen2-7B, Llama-3.1-8B, and Mistral-7B models.

## Key Results
- Word-level BAE attacks achieve higher selectivity than character-level DeepWordBug attacks
- Qwen2-7B is most sensitive to selective attacks, showing >0.10 selectivity gap
- Surrogate models can generate selective attacks without access to target internals (Δ=-0.10)
- Selective attacks can materially alter model rankings on benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Constraint-Guided Greedy Search
Imposing a multi-model constraint during adversarial search forces perturbations to be selective, significantly widening the performance gap between target and reference models. Standard attacks use greedy search to maximize target loss, but this paper introduces a custom constraint that validates candidate edits only if the target model misclassifies while reference models maintain correct predictions.

### Mechanism 2: Surrogate Preference Optimization (DPO)
A surrogate model can learn to generate selective perturbations without accessing target internals by optimizing a preference dataset ranked by "selectivity score." The authors generate paraphrases and rank them using a cost function that minimizes target score while maximizing reference stability, then apply DPO to train a generator to prefer selective paraphrases.

### Mechanism 3: Differential Sensitivity to Word-Level Perturbation
Word-level substitutions produce higher selectivity than character-level noise because modern LLMs possess robustness to typos but diverge in handling semantic context. Character-level attacks rely on visual or tokenization noise, which most modern LLMs handle similarly via robust tokenizers or subword normalization.

## Foundational Learning

- **Concept: TextAttack Recipes & Constraints**
  - Why needed here: The paper builds its entire experimental protocol by extending the TextAttack framework. Understanding how "recipes" (combinations of search methods and transformations) interact with "constraints" (pre/post conditions) is required to implement the custom selective constraint.
  - Quick check question: How does adding a constraint that checks multiple models simultaneously change the complexity of the greedy search?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: The paper uses DPO to train the surrogate generator. Unlike Supervised Fine-Tuning (SFT) which clones successful attacks, DPO optimizes the relative quality of attacks, which is crucial for maximizing the "gap" (selectivity) rather than just minimizing target accuracy.
  - Quick check question: Why might DPO preserve non-target model performance better than SFT when training a surrogate?

- **Concept: Transferability & Surrogate Models**
  - Why needed here: The surrogate pipeline is designed to perform "gray-box" attacks (no access to target internals). Understanding transferability—the degree to which an attack generated on one model affects another—is the core theoretical justification for why a surrogate can attack a target.
  - Quick check question: If the surrogate model is too similar to the reference models, how might that impact the selectivity of the generated attacks?

## Architecture Onboarding

- **Component map:**
  Target/Reference Models -> Attack Engine (TextAttack) -> Custom Constraint Module -> Scoring Function -> Surrogate Generator (DPO-trained) -> Output Paraphrases

- **Critical path:**
  1. Input: Standard MMLU question
  2. Generation: Surrogate proposes N paraphrases
  3. Filtering: Apply semantic constraints (consistency with original)
  4. Scoring: Run Target and References on paraphrases; compute Selectivity Score
  5. Selection: If valid selective sample found -> Attack Success. If not -> Select highest score for DPO feedback

- **Design tradeoffs:**
  - White-box (Constraint) vs. Gray-box (Surrogate): Constraint method is highly effective but requires target access; Surrogate method offers lower but significant degradation without target access
  - SFT vs. DPO: SFT converges faster but plateaus; DPO provides cleaner divergence (better selectivity) but requires careful tuning

- **Failure signatures:**
  - Semantic Drift: Generated paraphrases change the question meaning, invalidating the benchmark
  - Collateral Degradation: The attack hurts the reference models as much as the target
  - Plateauing: Surrogate fails to find new selective perturbations after 1-2 iterations

- **First 3 experiments:**
  1. Reproduce BAE + Custom Constraint: Set up TextAttack with BAE recipe on Qwen-7B (Target) and Llama-3 (Reference). Add constraint checking Llama's accuracy. Verify if Qwen drops >10% while Llama stays within ±2%.
  2. Assess Attack Granularity: Run the same setup with DeepWordBug (Character). Confirm hypothesis that character-level noise fails to achieve selectivity.
  3. Surrogate Generalization: Train surrogate on MMLU subset (e.g., STEM) and test on held-out subset (e.g., Humanities) to verify selectivity skill generalizes across domains.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can selective adversarial attacks be reliably constructed in purely black-box settings with only query access, without target model internals or surrogate training on the same architecture?
- **Open Question 2**: What specific architectural or training-data properties explain why Qwen models exhibit significantly higher sensitivity to selective word-level perturbations compared to Llama and Mistral families?
- **Open Question 3**: Can explicit defense mechanisms—such as adversarial training or input normalization—mitigate selective attacks without degrading clean benchmark performance?
- **Open Question 4**: Do selective adversarial attacks transfer across benchmarks beyond MMLU (e.g., reasoning, mathematical, or domain-specific evaluations)?

## Limitations
- Results may not generalize to proprietary models or real-world black-box settings
- Selectivity depends heavily on model architecture and training data differences
- The paper focuses on attack feasibility without exploring comprehensive defense mechanisms
- Limited to MMLU benchmark; generalizability to other domains remains unclear

## Confidence
- **Methodology**: High - well-defined framework with reproducible TextAttack recipes
- **Results**: Medium - promising selectivity results but limited to specific model families
- **Generalizability**: Low - limited to MMLU benchmark and open-weight models
- **Impact**: High - demonstrates significant implications for benchmark reliability

## Next Checks
1. Verify custom constraint implementation correctly filters perturbations that affect reference models
2. Compare selectivity gap between word-level and character-level attacks on same target model
3. Test surrogate model generalization by training on one MMLU subset and evaluating on another