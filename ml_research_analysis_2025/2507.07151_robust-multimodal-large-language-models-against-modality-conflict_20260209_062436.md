---
ver: rpa2
title: Robust Multimodal Large Language Models Against Modality Conflict
arxiv_id: '2507.07151'
source_url: https://arxiv.org/abs/2507.07151
tags:
- conflict
- image
- modality
- question
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates hallucinations in multimodal large language
  models (MLLMs) caused by modality conflicts, where visual and textual inputs contradict
  each other. The authors formally define three types of modality conflicts (object,
  attribute, and relationship) and construct the Multimodal Modality Conflict (MMMC)
  dataset to simulate these scenarios.
---

# Robust Multimodal Large Language Models Against Modality Conflict

## Quick Facts
- arXiv ID: 2507.07151
- Source URL: https://arxiv.org/abs/2507.07151
- Reference count: 24
- This paper investigates hallucinations in multimodal large language models (MLLMs) caused by modality conflicts, where visual and textual inputs contradict each other.

## Executive Summary
This paper investigates hallucinations in multimodal large language models (MLLMs) caused by modality conflicts, where visual and textual inputs contradict each other. The authors formally define three types of modality conflicts (object, attribute, and relationship) and construct the Multimodal Modality Conflict (MMMC) dataset to simulate these scenarios. They propose three methods to address this issue: prompt engineering, supervised fine-tuning, and reinforcement learning. Experimental results on MMMC show that all prevalent MLLMs exhibit high hallucination rates (>40%) when faced with modality conflicts. Among the proposed methods, reinforcement learning achieves the best performance in mitigating hallucinations, while supervised fine-tuning shows promising and stable results.

## Method Summary
The authors construct the MMMC dataset with 20K image-question-answer triples by extracting objects, attributes, and relationships from Visual Genome images using GPT-4o-mini, then generating counterfactual questions that create modality conflicts. They propose three methods to address hallucinations: (1) Prompt engineering adds a verification instruction to check if the image contains mentioned information before answering, (2) Supervised fine-tuning minimizes negative log-likelihood of ground-truth rejection responses on the MMMC dataset using LoRA, and (3) Reinforcement learning uses REINFORCE++ with a binary reward from Llama-3.3-70B-Instruct judge based on semantic consistency with ground-truth answers.

## Key Results
- All tested MLLMs exhibit high hallucination rates (>40%) under modality conflict scenarios
- Reinforcement learning achieves the best performance in reducing hallucinations but shows high variance
- Supervised fine-tuning shows stable and promising performance while maintaining better alignment with original tasks
- Prompt engineering effectiveness varies significantly across model architectures and sizes

## Why This Works (Mechanism)

### Mechanism 1
Reinforcement learning reduces hallucination rates under modality conflict by sampling diverse responses from the model itself and optimizing against a semantic-consistency reward signal. RL formulates conditional generation as an MDP where the model explores the response space, receives +1/-1 rewards based on semantic consistency with ground truth (judged by an LLM), and updates parameters via policy gradient (REINFORCE++). This exploration exposes the model to more diverse conflict patterns than static SFT data.

### Mechanism 2
Supervised fine-tuning on conflict-aware data teaches the model to recognize and explicitly reject inconsistent premises by aligning predictions with ground-truth rejection responses. SFT minimizes negative log-likelihood of ground-truth answers on MMMC, which includes responses like "The image does not contain a blue trolley." The model learns to map (conflicting image, question) pairs to rejection responses through gradient descent on the language modeling objective.

### Mechanism 3
Prompt engineering can trigger conflict-checking behavior in instruction-following models, but effectiveness is architecture-dependent and unstable across model families. The prompt "Please check if the image contains mentioned information and answer the question: T" prepends a verification instruction. For models with strong instruction-following, this shifts attention to cross-modal verification before generation.

## Foundational Learning

- **Modality conflict (object/attribute/relationship)**: Why needed here - This is the core problem definition. Without understanding that conflicts occur at three distinct levels—objects not present, attributes mismatched, or relationships contradicted—you cannot interpret the MMMC dataset construction or the failure modes of different models. Quick check: Given an image of a red car on a street and the question "What color is the blue car?", identify which conflict type this represents and what the correct response should be.

- **Hallucination rate vs. semantic consistency**: Why needed here - The paper evaluates methods using both Hallu-Rate (binary: did the model hallucinate?) and LLM-Judge (0-4 quality score). Understanding this distinction is critical for interpreting Table 2 results—e.g., RL reduces Hallu-Rate dramatically but shows high variance. Quick check: If a model responds "The image shows no blue trolley" when asked "What is behind the blue trolley?", should this count as a hallucination? Explain why or why not.

- **Alignment tax in multimodal fine-tuning**: Why needed here - Section 4.3 and Figure 3 show that SFT and RL can degrade performance on original vision-language tasks. Understanding this tradeoff is essential for practical deployment—you must verify that robustness gains don't come at unacceptable costs to general capability. Quick check: After fine-tuning on MMMC, a model's MMBench score drops from 62.89 to 58.49. Is this acceptable? What additional information do you need to decide?

## Architecture Onboarding

- **Component map**: Base question sampling -> Key components detection (LLM extracts objects/attributes/relationships) -> Components substitution (LLM generates counterfactual questions) -> Answer generation (LLM produces rejection responses without seeing the image) -> Human verification -> Data storage

- **Critical path**: Data quality is the bottleneck: The authors use GPT-4o-mini for construction and human annotators for verification (section 2.2). Garbage-in-garbage-out applies—if substitution logic is flawed, all methods train on wrong labels. Reward model accuracy gates RL: The Llama-3.3-70B-Instruct judge determines rewards. If it misclassifies semantic consistency, RL optimizes the wrong objective. Foundation model selection determines method viability: InstructBLIP fails with both SFT and RL (Table 2, Figure 3)—architecture matters more than method choice for some models.

- **Design tradeoffs**: RL vs. SFT: RL achieves lower Hallu-Rate (e.g., LLaVA-NeXT-7B: 31.27% vs. 42.83%) but has high variance and training instability. SFT is stable but may overfit. Training data scale: RL uses only 1K samples (vs. 10K for SFT) due to collapse risk beyond that. This limits RL's exposure to conflict diversity. LoRA vs. full fine-tuning: All experiments use LoRA, trading maximum performance for efficiency and reduced alignment tax.

- **Failure signatures**: RL collapse: InstructBLIP-7B response length jumps to maximum at ~300 episodes with repetitive, irrelevant text (section 4.3, Appendix D). Monitor response length and KL divergence. SFT overfitting: ROUGE-L may increase while Hallu-Rate doesn't improve (e.g., Qwen2-VL-Instruct-7B SFT ROUGE-L=28.60 but Hallu-Rate only drops from 47.95% to 32.02% with GPT judge). Prompt engineering backfire: Smaller models (Qwen2-VL-Instruct-2B) show increased Hallu-Rate with prompts, suggesting the instruction adds confusion rather than guidance.

- **First 3 experiments**: 1) Baseline diagnostic: Run your target MLLM on the MMMC test split (2K samples) with no modifications. Report Hallu-Rate via GPT-4o judge and identify which conflict type (object/attribute/relationship) has highest failure rate using Tables 4-6 as reference. 2) Prompt engineering ablation: Test 3 prompt variants: (a) paper's verification prompt, (b) explicit conflict-detection prompt ("If the question assumes something not in the image, state that"), (c) chain-of-thought prompt ("First list what you see, then answer"). Compare stability across model sizes. 3) SFT with alignment tax monitoring: Fine-tune on MMMC with the paper's hyperparameters. Before/after, evaluate on 3 general benchmarks (e.g., MMBench, MMStar, HallusionBench) to quantify alignment tax. If tax >10% on any benchmark, reduce training epochs or LoRA rank.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can MLLMs be improved to specifically handle relationship conflicts, which currently pose a "critical" challenge compared to object or attribute conflicts? Basis in paper: Section 4.3 states that relationship-conflict types "pose a significant challenge" and require critical enhancement beyond current RL/SFT methods.

- **Open Question 2**: How can the inherent instability and model collapse observed during reinforcement learning for conflict mitigation be resolved? Basis in paper: Section 4.3 highlights that RL training is "unstable," causing InstructBLIP-7B to collapse into repetitive loops (Figure 4).

- **Open Question 3**: Can more diverse, non-synthetic training data overcome the local optima observed in current conflict training? Basis in paper: Section 4.3 speculates models may need "more diverse and informative data" as performance plateaus despite increased training episodes.

## Limitations

- Reinforcement learning provides the strongest hallucination reduction but suffers from high variance and potential instability, with InstructBLIP-7B showing complete training collapse at approximately 300 episodes.

- The efficacy of supervised fine-tuning is architecture-dependent—while stable, it can cause significant performance degradation on standard vision-language tasks (alignment tax), particularly for Qwen2-VL-Instruct-7B.

- Prompt engineering shows inconsistent results across model families, with smaller models like Qwen2-VL-Instruct-2B experiencing increased hallucination rates when verification prompts are applied.

## Confidence

**High Confidence**: The observation that prevalent MLLMs exhibit high hallucination rates (>40%) under modality conflict is well-supported by the experimental results across multiple model families.

**Medium Confidence**: The superiority of reinforcement learning over other methods is demonstrated but tempered by high variance and training instability.

**Low Confidence**: The claim that reinforcement learning "learns more diverse and robust answers" is weakly supported by the corpus—HalluShift++ addresses hierarchical hallucinations but not modality conflicts specifically.

## Next Checks

1. **Reward Model Validation**: Test the Llama-3.3-70B-Instruct judge's consistency by having it evaluate the same response-answer pairs multiple times. Calculate inter-rater reliability to ensure the binary reward signal is stable and not the source of RL variance.

2. **Cross-Dataset Generalization**: Apply the best-performing fine-tuned model (RL or SFT) to HalluShift++ or SegSub datasets that evaluate different hallucination types. Measure performance degradation or improvement to assess whether MMMC fine-tuning generalizes beyond its specific conflict types.

3. **Architecture Ablation Study**: Systematically test each method (prompt, SFT, RL) on a broader range of MLLM architectures including BLIP-2 variants, Gemini series, and GPT-4o. This would validate whether the observed architecture-dependent failures are systematic or specific to the tested models.