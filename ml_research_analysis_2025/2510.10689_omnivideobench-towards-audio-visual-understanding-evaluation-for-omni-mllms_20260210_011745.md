---
ver: rpa2
title: 'OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs'
arxiv_id: '2510.10689'
source_url: https://arxiv.org/abs/2510.10689
tags:
- reasoning
- wang
- video
- zhang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniVideoBench is a new benchmark designed to evaluate how well
  multimodal large language models (MLLMs) understand and reason using both audio
  and visual information together. Existing benchmarks often focus on short clips
  or only one modality, missing the challenge of combining audio and vision in a logically
  consistent way.
---

# OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs

## Quick Facts
- **arXiv ID:** 2510.10689
- **Source URL:** https://arxiv.org/abs/2510.10689
- **Reference count:** 15
- **Key outcome:** New benchmark exposing significant gaps in MLLM audio-visual integration performance compared to humans and closed-source models

## Executive Summary
OmniVideoBench introduces a comprehensive benchmark for evaluating multimodal large language models' ability to reason using both audio and visual information together. Current benchmarks often focus on short clips or single modalities, missing the challenge of combining audio and vision in a logically consistent way. OmniVideoBench addresses this by using 628 real-world videos (ranging from seconds to 30 minutes) and creating 1,000 high-quality question-answer pairs with detailed step-by-step reasoning traces. The benchmark covers 13 types of reasoning tasks including temporal reasoning, spatial localization, counting, causal inference, and summarization. Evaluations reveal that current MLLMs, including leading open-source models, perform significantly worse than humans and lag far behind closed-source models, especially on tasks requiring audio-visual integration or music understanding.

## Method Summary
The benchmark was developed through a rigorous multi-stage pipeline. First, 628 real-world videos were collected from YouTube, covering diverse content types including travel, sports, news, and documentaries. Each video ranges from seconds to 30 minutes in length. Next, 1,000 question-answer pairs were created with detailed reasoning traces for each, covering 13 reasoning types such as temporal reasoning, spatial localization, counting, causal inference, and summarization. The questions were designed to require both audio and visual information for accurate answering. Finally, the benchmark was evaluated across 8 representative MLLMs (including both open-source and closed-source models) to assess their audio-visual understanding capabilities. Human evaluation was also conducted for comparison.

## Key Results
- Current MLLMs perform significantly worse than humans on audio-visual reasoning tasks
- Leading open-source models lag far behind closed-source models, especially on audio-visual integration and music understanding tasks
- Performance gaps are most pronounced in tasks requiring temporal reasoning, spatial localization, and causal inference
- The benchmark successfully exposes limitations in existing models' ability to integrate audio and visual information logically

## Why This Works (Mechanism)
OmniVideoBench works by providing a realistic test environment that requires genuine audio-visual integration rather than superficial pattern matching. The use of long-form, real-world videos (up to 30 minutes) forces models to maintain context and track information across extended periods. The 13 diverse reasoning tasks ensure comprehensive evaluation of different cognitive capabilities. The detailed reasoning traces provide transparency into model decision-making processes, revealing where and why models fail. By using real-world content rather than synthetic or short clips, the benchmark better reflects practical deployment scenarios where audio-visual integration is essential.

## Foundational Learning

**Multimodal reasoning** - The ability to combine information from different sensory modalities (audio and visual) to draw conclusions. Needed because real-world understanding requires integrating multiple information streams. Quick check: Can the model answer questions that require information from both audio and visual channels simultaneously?

**Temporal reasoning** - Understanding and reasoning about events across time, including cause-effect relationships and event sequences. Needed because videos are inherently temporal sequences requiring understanding of temporal dependencies. Quick check: Can the model correctly order events or identify temporal relationships in video content?

**Spatial localization** - The ability to identify and reason about spatial relationships, object positions, and movements within visual scenes. Needed because many real-world tasks require understanding where things are and how they move. Quick check: Can the model accurately describe object locations and movements within video frames?

**Causal inference** - Understanding cause-and-effect relationships between events and actions. Needed because real-world reasoning requires understanding why things happen, not just what happens. Quick check: Can the model identify causal relationships between audio cues and visual events?

## Architecture Onboarding

**Component map:** Video input → Audio-visual feature extraction → Multimodal fusion → Reasoning module → Answer generation

**Critical path:** The reasoning module is the most critical component, as it must integrate audio-visual features and apply appropriate reasoning strategies for each of the 13 task types.

**Design tradeoffs:** The benchmark prioritizes real-world relevance and comprehensive coverage over computational efficiency, using long videos that require substantial processing resources but provide more realistic test scenarios.

**Failure signatures:** Models typically fail on tasks requiring: (1) integration of audio and visual information, (2) understanding music or sound patterns, (3) temporal reasoning across long time spans, (4) spatial localization with complex scenes.

**First experiments:** 1) Test model performance on single-modality vs. audio-visual questions to measure integration capability. 2) Evaluate model performance across different video lengths to assess temporal reasoning. 3) Compare model performance on simple vs. complex spatial reasoning tasks.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The benchmark focuses on specific 13 reasoning tasks, which may not capture all aspects of real-world audio-visual understanding
- Performance comparisons are limited to the specific evaluation methodology used, which may not generalize to all deployment scenarios
- The claim that existing benchmarks "overlook" synergistic audio-visual understanding is somewhat overstated, as recent works like Daily-Omni and AMUSE do address similar integration challenges
- The evaluation uses a specific set of 8 MLLMs, which may not represent the full diversity of available models

## Confidence

**Benchmark design and dataset quality:** High - The methodology for creating high-quality question-answer pairs with detailed reasoning traces is rigorous and well-documented.

**Performance gap findings:** High - The evaluation methodology is comprehensive and the performance differences are substantial and clearly demonstrated.

**Claims about existing benchmark limitations:** Medium - While the benchmark does expose limitations in existing approaches, the claim that existing benchmarks "overlook" audio-visual integration is somewhat overstated given recent related work.

**Generalizability of results to all MLLM applications:** Medium - The results are specific to the 13 reasoning tasks defined in the benchmark and may not generalize to all real-world scenarios requiring audio-visual understanding.

## Next Checks

1. Test whether models that perform well on OmniVideoBench also show improved performance on real-world applications requiring audio-visual integration, such as video understanding assistants or surveillance systems.

2. Evaluate whether performance improvements on OmniVideoBench translate to gains on other multimodal reasoning benchmarks like Daily-Omni or AMUSE to assess generalizability.

3. Assess model robustness by introducing controlled audio-visual conflicts or noise to determine whether failures are due to integration limitations or superficial pattern matching.