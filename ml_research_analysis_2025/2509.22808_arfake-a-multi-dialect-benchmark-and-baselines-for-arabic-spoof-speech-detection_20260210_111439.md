---
ver: rpa2
title: 'ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection'
arxiv_id: '2509.22808'
source_url: https://arxiv.org/abs/2509.22808
tags:
- speech
- dataset
- arabic
- audio
- spoofed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ArFake, the first multi-dialect Arabic spoof-speech
  detection benchmark. The authors synthesized speech in eight Arabic dialects using
  four TTS models (FishSpeech, XTTS-v2, ArTST, VITS) from the Casablanca corpus.
---

# ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection

## Quick Facts
- **arXiv ID**: 2509.22808
- **Source URL**: https://arxiv.org/abs/2509.22808
- **Reference count**: 27
- **Primary result**: Introduces ArFake, the first multi-dialect Arabic spoof-speech detection benchmark with eight dialects synthesized using four TTS models

## Executive Summary
This paper presents ArFake, a pioneering multi-dialect Arabic spoof-speech detection benchmark designed to address the growing need for robust anti-spoofing systems in Arabic. The authors constructed a comprehensive dataset by synthesizing speech in eight Arabic dialects using four different TTS models (FishSpeech, XTTS-v2, ArTST, VITS) from the Casablanca corpus. They evaluated both traditional machine learning approaches using MFCC features and modern embedding-based neural models, with particular focus on their performance across different dialect regions and TTS models. The benchmark demonstrates that embedding-based approaches, especially Whisper-large, significantly outperform traditional ML methods in detecting spoofed speech.

## Method Summary
The authors developed ArFake by first collecting and preparing the Casablanca corpus containing Arabic speech across eight dialect regions. They then generated spoofed speech using four TTS models: FishSpeech, XTTS-v2, ArTST, and VITS. For evaluation, they employed both classical machine learning classifiers (trained on MFCC features) and embedding-based neural models. The realism of generated speech was assessed through Mean Opinion Score (MOS) and Automatic Speech Recognition (ASR) Word Error Rate (WER) metrics. The final benchmark combines data from FishSpeech, XTTS-v2, and ArTST to maximize generalizability while maintaining high-quality spoofed samples.

## Key Results
- FishSpeech generated the most challenging and realistic spoofed speech samples
- A combined dataset using FishSpeech, XTTS-v2, and ArTST was constructed for better generalizability
- Embedding models, particularly Whisper-large, achieved superior performance with an EER of 4.88% on the ArFake test set
- Strong performance was maintained on unseen VITS samples, demonstrating good generalization capabilities

## Why This Works (Mechanism)
The success of the ArFake benchmark stems from its comprehensive coverage of Arabic dialects and multiple TTS synthesis approaches, creating a diverse and challenging evaluation environment. The combination of human perception metrics (MOS) with objective ASR WER provides a balanced assessment of spoof speech realism. Embedding-based models excel because they capture rich acoustic and linguistic patterns that distinguish genuine from synthetic speech, while traditional MFCC-based approaches struggle with the nuanced characteristics of modern TTS outputs.

## Foundational Learning
1. **MFCC (Mel-Frequency Cepstral Coefficients)**: Spectral feature representation for speech; needed because it's a standard baseline for speech processing tasks; quick check: visualize MFCC spectrograms of real vs spoofed samples
2. **EER (Equal Error Rate)**: Point where false acceptance rate equals false rejection rate; needed as standard metric for biometric/spoof detection; quick check: plot ROC curve and identify EER point
3. **MOS (Mean Opinion Score)**: Human perceptual quality rating; needed to capture subjective realism that automatic metrics might miss; quick check: calculate inter-rater reliability among human evaluators
4. **TTS Models**: Text-to-speech synthesis systems; needed to generate diverse spoofed speech samples; quick check: compare spectrogram characteristics across different TTS outputs
5. **Dialect Classification**: Regional speech variation categorization; needed because Arabic has significant dialectal diversity; quick check: verify dialect labels using speaker geolocation or self-report

## Architecture Onboarding

**Component Map:**
Data Preparation -> TTS Synthesis -> Feature Extraction -> Classification Models -> Evaluation Metrics

**Critical Path:**
Casablanca corpus → Dialect-specific text → Four TTS models → Spoofed speech generation → MFCC/Embedding extraction → Classifier training → EER calculation

**Design Tradeoffs:**
The authors balanced dataset diversity against synthesis quality by combining three high-performing TTS models rather than using all four. This approach maximized coverage while maintaining realistic spoof samples, though it potentially introduced sampling bias toward certain synthesis characteristics.

**Failure Signatures:**
Poor performance on unseen TTS models, degradation across dialect boundaries, high WER with low EER (or vice versa) indicating metric misalignment, and systematic errors on specific dialect regions suggest model limitations in generalization.

**First Experiments:**
1. Train baseline SVM classifier on MFCC features and measure EER across all dialect regions
2. Fine-tune Whisper-large on the combined FishSpeech+XTTS-v2+ArTST dataset and evaluate on held-out VITS samples
3. Conduct ablation study removing individual dialect regions to identify which contribute most to detection difficulty

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation focuses primarily on speaker verification metrics (EER) and ASR WER, with limited exploration of other potential failure modes
- Benchmark construction relies heavily on synthetic data generation, which may not fully represent real-world spoofing attempts
- Performance metrics are based on controlled test conditions using synthesized speech, limiting generalizability to unknown TTS models

## Confidence
- Benchmark construction and dataset characteristics: High
- TTS model performance comparisons: Medium
- Embedding model superiority claims: Medium
- Real-world applicability: Low

## Next Checks
1. Test the benchmark's robustness against additional TTS models not included in the training set, particularly newer or domain-specific synthesis approaches
2. Evaluate detection performance on real spoofed speech instances captured from actual attack scenarios, rather than purely synthetic data
3. Conduct adversarial testing with carefully crafted spoofed samples designed to exploit potential weaknesses in the detection models