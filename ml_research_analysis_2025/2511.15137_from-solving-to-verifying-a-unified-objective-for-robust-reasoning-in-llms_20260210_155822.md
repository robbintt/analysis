---
ver: rpa2
title: 'From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs'
arxiv_id: '2511.15137'
source_url: https://arxiv.org/abs/2511.15137
tags:
- arxiv
- verification
- reasoning
- preprint
- self-verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRPO-Verif enhances self-verification in large language models
  by integrating solution generation and verification into a unified reinforcement
  learning objective. The method uses an auxiliary loss weighted by a hyperparameter
  to incorporate verification rewards alongside solution rewards during training.
---

# From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs

## Quick Facts
- arXiv ID: 2511.15137
- Source URL: https://arxiv.org/abs/2511.15137
- Authors: Xiaoxuan Wang; Bo Liu; Song Jiang; Jingzhou Liu; Jingyuan Qi; Xia Chen; Baosheng He
- Reference count: 6
- Key outcome: GRPO-Verif improves self-verification accuracy from 32.9% to 37.1% on benchmarks while maintaining comparable solution accuracy (38.5% vs 38.4%)

## Executive Summary
GRPO-Verif introduces a unified reinforcement learning objective that jointly optimizes solution generation and self-verification for large language models. The method uses an auxiliary verification loss weighted by hyperparameter α to incorporate verification rewards alongside solution rewards during training. Experiments on Qwen2.5-3B demonstrate that explicit self-verification strengthens verification capability without compromising reasoning performance, addressing the question of whether enhanced verification can improve reasoning without degradation.

## Method Summary
GRPO-Verif modifies the GRPO algorithm by combining solution generation and verification into a single training objective. The method samples n solutions per question, computes solution rewards, then generates verification responses conditioned on both question and solution. Both solution and verification rewards are normalized using group statistics to compute advantages without requiring a critic network. The joint loss combines solution and verification terms with weight α, enabling simultaneous optimization of both tasks through shared model parameters.

## Key Results
- Verification accuracy improves from 32.9% to 37.1% on benchmarks
- Solution accuracy maintained at comparable levels (38.5% vs 38.4%)
- Qwen2.5-3B trained on 6k problems from DAPO-Math-17k
- Evaluated on AMC23, MATH, Minerva_Math, and OlympiadBench benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Joint optimization of solution generation and verification creates complementary learning signals that improve verification without degrading solution quality. GRPO-Verif combines two objectives—solution accuracy and verification accuracy—into a single loss function weighted by hyperparameter α (set to 0.2). The model receives gradients from both tasks simultaneously, encouraging representations that support both solving and verifying. Core assumption: Solution quality and verification capability share transferable reasoning representations rather than competing for model capacity.

### Mechanism 2
Group-normalized advantages enable critic-free RL training while maintaining stable gradient estimates for both solution and verification branches. Instead of learning a value function (critic), GRPO-Verif normalizes rewards within each group of n sampled solutions/verifications. Advantages are computed as A = (R - mean(R_group)) / std(R_group), providing baseline-adjusted signals without additional model parameters. Core assumption: Within-group reward statistics provide sufficiently accurate baselines for advantage estimation across diverse problem types.

### Mechanism 3
Conditioning verification on question-solution pairs forces the model to learn error detection by comparing proposed solutions against problem constraints. Verification responses v^(i) are generated from π(·|q, y^(i)), meaning the model sees both the original question and candidate solution before producing a PASS/FAIL judgment. This creates a supervised signal for detecting incorrect reasoning. Core assumption: The model can learn generalizable error patterns from binary verification rewards without explicit error annotation.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO) fundamentals**
  - Why needed here: GRPO is a variant of PPO; understanding clipping, probability ratios r_t(θ), and KL penalties is essential for debugging training dynamics.
  - Quick check question: Can you explain why PPO clips the probability ratio and what happens when the clipping threshold is too narrow?

- Concept: **Advantage functions and baselines**
  - Why needed here: The core innovation uses group-normalized advantages; without this, you cannot diagnose why certain solutions receive higher gradient signals.
  - Quick check question: If all solutions in a group receive identical rewards, what happens to the advantages and resulting gradients?

- Concept: **Multi-task learning and loss weighting**
  - Why needed here: GRPO-Verif balances two objectives via α; improper weighting causes task competition or dominance.
  - Quick check question: Given solution accuracy 38.5% and verification accuracy 37.1%, should α be increased or decreased to further prioritize verification?

## Architecture Onboarding

- Component map:
  - Solution generation branch -> Sample n solutions per question, compute solution rewards, normalize advantages
  - Verification generation branch -> For each solution, generate verification response, compute verification rewards, normalize advantages separately
  - Joint loss combiner -> Sums solution loss + α × verification loss, applies PPO clipping
  - Policy updater -> Single backward pass updates shared model parameters from combined loss

- Critical path:
  1. Sample question batch from training set
  2. Generate n=8 solutions per question using current policy
  3. Evaluate solution correctness → compute solution advantages
  4. For each solution, generate verification response
  5. Evaluate verification correctness → compute verification advantages
  6. Compute joint loss and update policy

- Design tradeoffs:
  - Group size n=8: Larger n improves advantage estimate stability but increases compute linearly
  - α=0.2: Higher values prioritize verification accuracy but risk solution degradation; paper shows this is a viable balance point
  - β=0 (no KL penalty): Reduces compute but may allow larger policy deviations from reference

- Failure signatures:
  - Verification accuracy plateaus while solution accuracy drops → α too high
  - Both accuracies oscillate wildly → learning rate too high or group size too small
  - Verification always predicts PASS or FAIL → reward imbalance or template ambiguity
  - Training loss diverges → check for NaN rewards from malformed verification outputs

- First 3 experiments:
  1. Sweep α values [0.0, 0.1, 0.2, 0.5, 1.0] on a 1k problem subset to find the verification/solution tradeoff curve before full training
  2. Ablate verification conditioning: Compare full GRPO-Verif against verification conditioned only on solution (no question) to confirm question-solution interaction is necessary
  3. Group size sensitivity: Test n=[4, 8, 16] to quantify advantage stability vs. compute tradeoff, measuring variance in per-epoch accuracy improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can improved self-verification explicitly enhance reasoning performance (solution accuracy), rather than merely maintaining it?
- Basis in paper: The introduction asks whether enhancing self-verification "can further improve reasoning performance." Results show verification accuracy improved (32.9%→37.1%) but solution accuracy remained comparable (38.4%→38.5%), leaving the question of whether verification gains can transfer to better solving partially unanswered.
- Why unresolved: The unified objective improved verification without degrading solutions, but did not demonstrate positive transfer to reasoning.
- What evidence would resolve it: Experiments showing GRPO-Verif yields statistically significant gains in solution accuracy over GRPO baseline on the same benchmarks.

### Open Question 2
- Question: How does the verification weight α affect the trade-off between verification capability and solution generation quality?
- Basis in paper: The hyperparameter α is set to 0.2 without ablation study. Given that verification and solution objectives may compete, the sensitivity of this weighting is unknown.
- Why unresolved: No experiments varying α are reported; optimal balancing may differ by model scale or task domain.
- What evidence would resolve it: A systematic ablation across α values (e.g., 0.0 to 1.0) reporting both verification and solution accuracy curves.

### Open Question 3
- Question: Does GRPO-Verif generalize to larger model scales and different architectures beyond Qwen2.5-3B?
- Basis in paper: All experiments use a single 3B parameter model. The interaction between model capacity and unified verification training is unstudied.
- Why unresolved: Smaller models may benefit differently from auxiliary verification signals than larger models with emergent verification abilities.
- What evidence would resolve it: Evaluations on 7B, 14B, and 70B variants across at least two model families.

### Open Question 4
- Question: Can the computational overhead of verification generation be reduced while retaining performance gains?
- Basis in paper: Appendix B states: "A current limitation of GRPO-Verif is the added computational overhead introduced by generating and training on verification responses."
- Why unresolved: Each training step requires n additional verification generations; no efficiency optimizations are explored.
- What evidence would resolve it: Comparison of alternative strategies (e.g., fewer verification samples, lightweight verification heads, or off-policy verification reuse) against the current approach.

## Limitations

- The 4.2 percentage point improvement in verification accuracy may not be substantial enough to warrant the additional complexity over simpler verification approaches.
- The analysis focuses on a single model size (Qwen2.5-3B) and a single dataset (DAPO-Math-17k), limiting generalizability to other domains and model scales.
- The computational overhead of generating n verification responses per training step is significant and not addressed by efficiency optimizations.

## Confidence

- **High confidence**: The core mechanism of combining solution and verification objectives is technically sound and well-implemented. The group-normalized advantage computation follows established RL principles. The experimental setup (model choice, evaluation benchmarks) is clearly specified.
- **Medium confidence**: The claimed improvement in verification accuracy while maintaining solution accuracy is supported by reported numbers, but the practical significance of a 4.2 percentage point gain requires further validation. The choice of α=0.2 appears reasonable but lacks systematic justification.
- **Low confidence**: The generalizability of results to other model sizes, domains beyond mathematics, and different reward structures remains untested. The long-term stability of the approach and its behavior with different group sizes or alternative verification templates is unclear.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically sweep α values across [0.0, 0.1, 0.2, 0.5, 1.0] and n values across [4, 8, 16] to quantify the tradeoff between verification improvement and solution degradation, measuring variance in per-epoch accuracy improvements and identifying optimal configurations for different problem difficulty distributions.

2. **Ablation on verification conditioning**: Compare full GRPO-Verif against two variants: (a) verification conditioned only on solution (no question) and (b) solution-conditioned verification with shuffled question-solution pairs, to confirm that the question-solution interaction is necessary for learning genuine error detection rather than superficial patterns.

3. **Generalization test**: Evaluate GRPO-Verif-trained models on out-of-distribution mathematical problems (e.g., physics problems, logical puzzles) and non-mathematical reasoning tasks to assess whether verification capability transfers beyond the training domain, measuring both solution and verification accuracy to detect potential negative transfer.