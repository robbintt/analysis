---
ver: rpa2
title: Population synthesis with geographic coordinates
arxiv_id: '2510.09669'
source_url: https://arxiv.org/abs/2510.09669
tags:
- data
- synthetic
- spatial
- geographic
- coordinates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating synthetic populations
  with explicit geographic coordinates, a crucial requirement for applications like
  flood risk modeling, epidemic spread, and urban planning. The key difficulty is
  that geographic coordinates exhibit non-regular distributions with large empty spaces
  and highly uneven densities, making traditional population synthesis methods ineffective.
---

# Population synthesis with geographic coordinates

## Quick Facts
- **arXiv ID:** 2510.09669
- **Source URL:** https://arxiv.org/abs/2510.09669
- **Reference count:** 40
- **Primary result:** NF+VAE generates synthetic populations with geographic coordinates achieving SWD of 0.022 (vs 0.095 for VAE alone) while protecting privacy

## Executive Summary
This paper addresses the challenge of generating synthetic populations with explicit geographic coordinates, crucial for applications like flood risk modeling and urban planning. The key difficulty is that geographic coordinates exhibit non-regular distributions with large empty spaces and highly uneven densities, making traditional population synthesis methods ineffective. The proposed NF+VAE approach combines Normalizing Flows to map geographic coordinates into a regular latent space, followed by a Variational Autoencoder to learn joint distributions of spatial and non-spatial features.

The method was evaluated on 121 datasets including Italian mortgages and Airbnb listings across 15 cities. NF+VAE outperformed benchmarks on fidelity (sliced-Wasserstein distance for geographic coordinates: 0.022 vs 0.095 for VAE alone), utility (R² for price prediction: 0.196 vs 0.163 for global shuffle), and privacy (membership inference AUC-ROC: 0.5 vs 0.7 for local shuffle). The approach successfully generates geolocated synthetic populations that maintain statistical properties of real data while protecting individual privacy.

## Method Summary
The NF+VAE method employs a two-stage approach: first, a Neural Spline Flow (NSF) is trained to transform geographic coordinates (latitude/longitude) into a regular 2D Gaussian latent space, addressing the non-uniform density issues of real geographic distributions. Second, a VAE is trained on the combined latent coordinates and other features with a weighted loss function prioritizing geographic reconstruction. Synthetic data generation involves sampling from the Gaussian latent space, decoding through the VAE, and applying the inverse NSF transformation to obtain realistic geographic coordinates. The method uses weighted loss terms with α_GEO > α_R > α_KL to prioritize spatial fidelity while maintaining feature utility and privacy through VAE compression.

## Key Results
- NF+VAE achieved sliced-Wasserstein distance of 0.022 for geographic coordinates versus 0.095 for VAE alone
- Utility measured by hedonic regression R² was 0.196 for NF+VAE versus 0.163 for global shuffle
- Privacy protection showed membership inference AUC-ROC of 0.5 (random guessing) versus 0.7 for local shuffle methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping irregular geographic coordinates to a regular latent space via Normalizing Flows (NF) prevents the generative model from failing due to sparse or uneven spatial densities.
- **Mechanism:** Geographic data often contains large voids (e.g., oceans, parks) and high-density clusters (city centers). NF learns an invertible transformation that maps these complex distributions to a standard Gaussian. This "unfolds" the tangled geometry, allowing the subsequent generative model to sample from a uniform, mathematically tractable space rather than raw, jagged coordinates.
- **Core assumption:** The spatial distribution is learnable via bijective transformations and does not contain topological holes that cannot be mapped to a simply connected Gaussian space.
- **Evidence anchors:**
  - [Abstract] "latitude and longitude differ from other continuous variables... NF... map[s] geographic coordinates into a more regular latent space."
  - [Results] "The methods that use NF achieve performances comparable to Local shuffle (median $d_F^G$: 0.022 for NF+VAE... vs 0.095 for VAE alone)."
  - [Corpus] Weak direct evidence for NF specifically; neighbors focus on GANs (WGAN) or Diffusion, implying this is a novel architectural choice for this specific problem.
- **Break condition:** If the geographic region is fragmented into disjoint islands without a shared boundary representation in the flow, the single continuous latent space may generate invalid coordinates in the "gaps" between islands.

### Mechanism 2
- **Claim:** Sequential training (NF first, then VAE) captures spatial autocorrelation better than joint modeling of raw coordinates.
- **Mechanism:** By training the VAE on the *latent* coordinates (output of NF) alongside housing features, the model learns the joint probability P(Features | Location). Since the NF has already "smoothed" the location data, the VAE's encoder can efficiently compress regional patterns into its latent vector without fighting the irregularity of the raw map.
- **Core assumption:** Spatial autocorrelation exists and is transferable; features in close proximity are more similar than distant ones (Tobler's law).
- **Evidence anchors:**
  - [Section 3.2] "NF+VAE... captures correlations between spatial and non-spatial variables... [exploiting] spatial autocorrelations."
  - [Results] "VAE-based methods capture spatial dependencies better than copula-based methods (median $d_F^S$: 0.028 for NF+VAE vs 0.080 for NF+copula)."
  - [Corpus] [13689] supports the general need for generative models to capture spatial structure in population synthesis.
- **Break condition:** If the relationship between location and attributes is purely random (no spatial autocorrelation), the VAE will fail to learn a meaningful signal and may overfit to noise.

### Mechanism 3
- **Claim:** Information loss in the VAE bottleneck acts as a privacy filter, preventing exact replication of sensitive records.
- **Mechanism:** The VAE compresses high-dimensional input data into a lower-dimensional latent code and forces it to approximate a Gaussian distribution. This irreversible compression ensures that while the *statistical* properties are preserved, the specific "fingerprint" of any individual record is blurred, making membership inference difficult.
- **Core assumption:** The dimensionality reduction is sufficiently small to destroy unique identifiers but large enough to maintain utility.
- **Evidence anchors:**
  - [Abstract] "...ensuring privacy."
  - [Results] "Privacy (membership inference AUC-ROC: 0.5 vs 0.7 for local shuffle)... synthetic population does not replicate individual records."
  - [Corpus] [18] (cited in paper) provides the theoretical basis for synthetic data privacy risks and defenses.
- **Break condition:** If the dataset contains extreme outliers (unique combinations not easily compressed), the decoder might memorize these specific instances, leaking privacy.

## Foundational Learning

- **Concept: Change of Variables (Normalizing Flows)**
  - **Why needed here:** You cannot effectively sample or reconstruct from a "Swiss cheese" distribution (cities with holes). You must understand how transforming a variable x to z affects the probability density p(x) via the Jacobian determinant.
  - **Quick check question:** If I map a uniform distribution through an exponential function, where does the density get "squashed" and where does it expand?

- **Concept: Spatial Autocorrelation (Moran's I)**
  - **Why needed here:** This is the metric the paper uses to prove the model works. You must understand that valid synthetic data must respect that nearby things are related (e.g., price clusters).
  - **Quick check question:** Would you expect the Moran's I for "house price" in a city to be positive, negative, or zero?

- **Concept: The ELBO (Evidence Lower Bound)**
  - **Why needed here:** The VAE optimizes a trade-off between reconstruction accuracy (fidelity) and KL-divergence (regularization). Understanding this explains why the model generates *similar* but not *identical* data.
  - **Quick check question:** If the KL-divergence term in the loss is weighted too high, what happens to the reconstruction fidelity?

## Architecture Onboarding

- **Component map:** Input Dataset (Lat, Lon, Features) -> Spatial Transformer (NF) -> Gaussian Latent Space -> Feature Combiner -> VAE Encoder -> Latent Space -> VAE Decoder -> Combined Features -> Inverse Transformer (NF) -> Output Synthetic Dataset (Lat, Lon, Features)

- **Critical path:** The training of the NF is sequential and independent of the VAE. If the NF fails to converge to a smooth Gaussian latent space, the VAE will receive garbled location inputs and the entire pipeline will fail.

- **Design tradeoffs:**
  - **Loss Weighting (α_GEO vs α_R):** The paper sets α_GEO > α_R. This prioritizes getting the house location correct over getting the exact feature values right, which is logical for spatial modeling but may reduce attribute precision.
  - **Latent Dimensionality (k):** A smaller k increases privacy (more blurring) but decreases utility (loss of subtle feature correlations).

- **Failure signatures:**
  - **"Scatterplot effect":** Synthetic coordinates appear as a uniform cloud over the map (NF failed to capture density; model defaults to mean).
  - **"Mode Collapse":** All synthetic houses generate at the city center or a single coordinate (NF variance explosion or VAE posterior collapse).
  - **Privacy Leak:** Specific outliers (e.g., a mansion in a slum) appear identically in the synthetic data (VAE reconstruction loss too low/high capacity).

- **First 3 experiments:**
  1. **Ablation (Visual):** Train *without* NF (just VAE on raw coords). Visualize the output map. Expect a "fog" over the bounding box rather than city streets.
  2. **Fidelity Test:** Calculate Sliced-Wasserstein distance between real and synthetic coordinates. Target < 0.03 (per paper results).
  3. **Autocorrelation Check:** Plot "Garage Presence" on the synthetic map. Verify it appears in suburbs/outskirts, not the city center, matching the input spatial logic.

## Open Questions the Paper Calls Out

- **Question 1:** Can the NF+VAE framework be successfully integrated with differential privacy mechanisms to provide formal privacy guarantees without significantly degrading the fidelity of geographic coordinates?
  - **Basis in paper:** [explicit] The authors state in the Limitations section: "Another possibility to address privacy is to use generative models relying on differential privacy... We leave the comparison of NF+VAE with such approaches to future work."
  - **Why unresolved:** The current study relies on the information bottleneck of the VAE for heuristic privacy, but it does not implement or test against formal differential privacy (DP) standards.
  - **What evidence would resolve it:** An implementation of the NF+VAE training loop using DP-SGD (Differentially Private Stochastic Gradient Descent), evaluated on the trade-off between the privacy budget (ε) and the sliced-Wasserstein distance (d^F_G).

- **Question 2:** Does the synthetic population generated by NF+VAE maintain the same utility when used to initialize Agent-Based Models (ABMs) as the real population?
  - **Basis in paper:** [explicit] The authors note in the Limitations regarding utility metrics: "In the case of ABMs, a reasonable utility measure could be comparing output of the ABM simulations when using the real or synthetic populations."
  - **Why unresolved:** The paper currently evaluates utility using a static hedonic regression model (predicting house prices), rather than testing the synthetic data in dynamic simulations (e.g., flood risk or epidemic spread).
  - **What evidence would resolve it:** Running the specific flood-risk ABM mentioned in the introduction with both real and synthetic input data, then comparing the statistical properties of the simulation outputs (e.g., aggregate demand shifts or price fluctuations).

- **Question 3:** How does NF+VAE compare against Generative Adversarial Networks (GANs) or Bayesian Networks in terms of fidelity and spatial autocorrelation?
  - **Basis in paper:** [explicit] The authors acknowledge in the Limitations: "The inclusions of additional baselines, such as Combinatorial Optimization, Generative Adversarial Networks (GANs), or Bayesian Networks, would provide a more comprehensive assessment."
  - **Why unresolved:** The study only benchmarks against copula-based methods, VAEs, and shuffle techniques, leaving the performance relative to other deep generative models (GANs) unknown.
  - **What evidence would resolve it:** A comparative benchmarking study applying standard tabular GANs (e.g., CTGAN) to the data_isp dataset and comparing the results to NF+VAE using the fidelity (d^F_G, d^F_S) and privacy (ρ_P) metrics defined in the paper.

## Limitations

- The paper does not implement or compare against formal differential privacy mechanisms, relying instead on information-theoretic privacy assumptions from VAE compression
- Architectural details remain unspecified (VAE depth, latent dimension, NF bin count, loss weight magnitudes), preventing exact reproduction
- The method assumes continuous spatial distributions and may struggle with highly fragmented regions (islands without connecting geography)

## Confidence

- **High confidence:** The core mechanism of using NF to transform geographic coordinates into a regular latent space is sound and well-supported by the results (SWD of 0.022 vs 0.095 for VAE alone)
- **Medium confidence:** The sequential training approach (NF then VAE) effectively captures spatial autocorrelation, though the exact architectural choices remain unspecified
- **Medium confidence:** The privacy benefits from VAE compression are theoretically justified but not validated against state-of-the-art privacy attacks

## Next Checks

1. **Ablation study:** Train VAE directly on raw coordinates (without NF) and compare spatial distribution fidelity - expect "fog effect" over map boundaries rather than realistic city patterns

2. **Privacy stress test:** Generate synthetic data from datasets with extreme outliers (unique attribute combinations) and test for exact replication using record linkage attacks

3. **Geographic robustness:** Apply method to highly fragmented regions (e.g., Mediterranean islands) and evaluate whether synthetic coordinates respect valid geographic boundaries or generate invalid locations in ocean spaces