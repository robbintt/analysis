---
ver: rpa2
title: Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual
  Cooking
arxiv_id: '2507.03330'
source_url: https://arxiv.org/abs/2507.03330
tags:
- cooking
- recipe
- object
- status
- oscar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OSCAR, a technical pipeline for recipe progress
  tracking in non-visual cooking, leveraging object status recognition. OSCAR integrates
  recipe parsing, object status extraction, visual alignment with cooking steps, and
  time-causal modeling to support real-time step tracking.
---

# Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking

## Quick Facts
- arXiv ID: 2507.03330
- Source URL: https://arxiv.org/abs/2507.03330
- Reference count: 40
- One-line primary result: Object status recognition improves recipe step prediction accuracy by over 20% in non-visual cooking scenarios.

## Executive Summary
This paper presents OSCAR, a technical pipeline for recipe progress tracking in non-visual cooking, leveraging object status recognition. OSCAR integrates recipe parsing, object status extraction, visual alignment with cooking steps, and time-causal modeling to support real-time step tracking. Evaluated on 173 instructional videos and a real-world dataset of 12 non-visual cooking sessions recorded by BLV individuals, OSCAR demonstrates that object status consistently improves step prediction accuracy across vision-language models (VLMs). Specifically, OSCAR improved accuracy by over 20% compared to baseline methods, with CLIP accuracy increasing from 41.7% to 68.0% and SigLIP from 62.2% to 82.8%. The study also identified key factors affecting performance in real-world conditions, such as implicit tasks, lighting, occlusion, and camera placement. The contributions include the OSCAR pipeline, a real-world annotated non-visual cooking dataset, and design insights for future context-aware assistive cooking systems.

## Method Summary
The OSCAR pipeline processes cooking videos to predict recipe steps by combining object status recognition with time-causal filtering. First, GPT-4o extracts recipe steps and object status pairs (verb+noun) from recipe text. OpenCV extracts video frames (5 per step segment, blur-filtered). CLIP or SigLIP computes similarity between frames and both recipe steps and object statuses. Similarities are averaged, and a time-causal model prevents backward step predictions. The step with the highest combined similarity is output as the current step.

## Key Results
- OSCAR improved step prediction accuracy by over 20% compared to baseline methods across all test scenarios.
- CLIP accuracy increased from 41.7% to 68.0%, and SigLIP from 62.2% to 82.8% when using object status recognition.
- Real-world evaluation on BLV cooking sessions revealed key challenges including implicit tasks, lighting, occlusion, and camera placement affecting performance.

## Why This Works (Mechanism)

### Mechanism 1: Object Status Alignment (Verb+Noun Grounding)
The system extracts [verb] + [noun] pairs (Object Status) from recipe text using GPT-4o. It then computes similarity scores between video frames and these specific status descriptions using VLMs (CLIP/SigLIP). This bypasses the ambiguity of generic step text by looking for the visual evidence of the transformation. Fails if visual occlusion or lighting prevents the VLM from resolving texture/state details.

### Mechanism 2: Time-Causal Filtering
A post-processing layer maintains a history log and prevents the prediction of "earlier" steps unless valid. It filters out "noise" where a user touches a previously prepared ingredient which might otherwise trigger a false prediction of that previous step. Fails in recipes with heavy concurrency or intentional regression.

### Mechanism 3: Tolerance to Non-Visual Adaptations
By anchoring predictions on the state of the object (e.g., "spread beans") rather than the specific tool-action pair, the system tolerates idiosyncratic strategies used by BLV individuals, such as using fingers instead of tools. Fails if the adaptive strategy alters the visual signature significantly.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) - CLIP/SigLIP**
  - Why needed here: These models are the "eyes" of the system. Understanding that they project text and images into a shared vector space to calculate "similarity" is essential for understanding how "chopped carrots" (text) matches a video frame (image).
  - Quick check question: If a VLM has never seen "watermelon rind," can it correctly identify it?

- **Concept: Temporal/Sequential Logic in State Machines**
  - Why needed here: The Time-Causal Model acts as a state machine. You need to understand how enforcing transition rules (State A must precede State B) reduces noise in a continuous signal stream.
  - Quick check question: Why would a system predict Step 3 when the user is actually on Step 4 if they are interacting with an object from Step 3?

- **Concept: Non-Visual "Exploratory" Behaviors**
  - Why needed here: Unlike standard computer vision tasks, BLV cooking involves "checking" via touch. A system designer must differentiate between "interaction to transform" (doing) and "interaction to sense" (checking).
  - Quick check question: How does the system interpret a user holding a knife but not cutting? (Is it a false positive?).

## Architecture Onboarding

- **Component map:** Input (Recipe Text + Video Stream) -> Parser (GPT-4o) -> Visual Encoder (CLIP/SigLIP) -> Similarity Engine -> Time-Causal Model -> Output (Current Step, Completed Steps, Missing Steps)

- **Critical path:** The alignment accuracy between the Visual Frame and the Object Status Text Embedding. If the VLM cannot distinguish "saut√©ed onions" from "raw onions," the pipeline fails.

- **Design tradeoffs:**
  - Frame Sampling: High frequency (real-time latency, noise) vs. Low frequency (missing transient states)
  - VLM Choice: CLIP (faster/lower accuracy) vs. SigLIP (better in clutter/low res per this paper)
  - Strictness of Time-Causal: Strict (might miss user corrections/returns) vs. Loose (more false positives from re-checking)

- **Failure signatures:**
  - "Phantom Regression": User touches a finished ingredient, and the system jumps back to the previous step
  - "Stuck State": Lighting or occlusion prevents the VLM from seeing the state change, keeping the system on the current step
  - "Implicit Task" Noise: User spends 2 minutes cleaning or finding a bin; system prediction confidence drops or fluctuates wildly

- **First 3 experiments:**
  1. Static Image Alignment Test: Take a "chopped onion" image. Compare similarity scores for "Step: Chop onions" vs "Status: Chopped onions" to verify the Object Status signal is stronger.
  2. Time-Causal Stress Test: Feed the system a video stream with oscillating predictions (1-2-1-2) due to a hand blocking the view. Verify the Time-Causal Model locks to "2".
  3. Lighting Robustness Test: Run the pipeline on the Non-Visual Cooking Dataset (V1-V12) specifically looking for performance drops in videos flagged as having "suboptimal lighting" (e.g., V6).

## Open Questions the Paper Calls Out

### Open Question 1
How do blind and low vision (BLV) users naturally interact with real-time, context-aware progress tracking systems during cooking? The study evaluated technical prediction accuracy on datasets but did not deploy a user-facing system; therefore, user interaction patterns, feedback preferences, and the impact on cooking confidence remain unknown. A user study involving an interactive, deployable prototype of OSCAR used by BLV participants in their own kitchens would resolve this.

### Open Question 2
What interaction methods effectively allow BLV cooks to maintain optimal camera framing without interrupting their workflow? The real-world dataset revealed that chest-mounted cameras often failed to capture the active workspace, but the study did not test solutions for this hardware/interaction limitation. An evaluation of assistive framing interfaces (audio or haptic guidance) that measure reduction in framing errors and user cognitive load during cooking tasks would resolve this.

### Open Question 3
How can computational models distinguish between non-transformative exploratory actions (e.g., safety checks) and actual step progression? Current vision-language models (VLMs) rely on frame-by-frame classification, causing them to misclassify repetitive tactile verification (feeling ingredients) as distinct cooking steps. A modified temporal model that successfully filters out exploratory motion patterns to maintain stable step predictions during non-visual object interactions would resolve this.

## Limitations
- The effectiveness of object status recognition depends heavily on the VLM's ability to resolve fine-grained visual differences, which may not hold across diverse real-world conditions.
- The time-causal model's behavior in edge cases (e.g., intentional step regression, skipped steps, or parallel tasks) is not fully explored.
- The reliance on GPT-4o for object status extraction introduces a black-box dependency without validation of consistency and accuracy.

## Confidence

- **High Confidence:** The OSCAR pipeline architecture is well-defined, and the core mechanism of using object status for visual alignment is supported by experimental results showing significant accuracy improvements over baselines.
- **Medium Confidence:** The improvement in step prediction accuracy (20% gain) is demonstrated on the test datasets, but the robustness of this performance across diverse cooking styles, lighting conditions, and user adaptations requires further validation.
- **Low Confidence:** The generalization of object status recognition to states not well-represented in training data (e.g., unique cuts, textures, or non-standard tools) is not explicitly tested.

## Next Checks

1. **Robustness to Visual Ambiguity:** Test the system's performance on a dataset with varied lighting, occlusion, and camera angles, specifically focusing on frames with low VLM confidence scores to identify failure modes.
2. **Object Status Extraction Consistency:** Conduct an ablation study where the object status extraction step is manually verified or replaced with a rule-based system to quantify the impact of GPT-4o's performance on the overall accuracy.
3. **Time-Causal Model Edge Cases:** Evaluate the system on recipes with parallel steps, intentional regressions (e.g., "return to the pan"), or skipped steps to assess how well the time-causal model handles non-linear workflows.