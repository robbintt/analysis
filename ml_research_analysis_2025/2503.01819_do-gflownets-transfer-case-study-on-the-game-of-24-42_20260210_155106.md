---
ver: rpa2
title: Do GFlowNets Transfer? Case Study on the Game of 24/42
arxiv_id: '2503.01819'
source_url: https://arxiv.org/abs/2503.01819
tags:
- llama
- game
- fine-tuned
- gflownet
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates GFlowNets' ability to transfer learning from
  the Game of 24 to the Game of 42 across different temperature settings and decoding
  strategies. Fine-tuned GFlowNets outperform pre-trained LLaMA models on in-distribution
  tasks (Game of 24) across all configurations, with optimal parameters identified
  as top-k=10, min-p=0.05, and top-p=0.85.
---

# Do GFlowNets Transfer? Case Study on the Game of 24/42

## Quick Facts
- arXiv ID: 2503.01819
- Source URL: https://arxiv.org/abs/2503.01819
- Authors: Adesh Gupta; Abhinav Kumar; Mansi Gupta; Paras Chopra
- Reference count: 40
- Primary result: Fine-tuned GFlowNets outperform pre-trained LLaMA models on in-distribution tasks but show limited zero-shot transferability to related tasks

## Executive Summary
This paper evaluates GFlowNets' ability to transfer learning from the Game of 24 to the Game of 42 across different temperature settings and decoding strategies. Fine-tuned GFlowNets outperform pre-trained LLaMA models on in-distribution tasks (Game of 24) across all configurations, with optimal parameters identified as top-k=10, min-p=0.05, and top-p=0.85. However, GFlowNets show limited zero-shot transferability to the closely related Game of 42, struggling to maintain both accuracy and solution diversity. Temperature settings exhibit the most significant impact on performance, with higher temperatures (0.7-1.1) facilitating better exploration. The study highlights GFlowNets' sensitivity to hyperparameter configurations and reveals key limitations in their cross-task generalization compared to instruction-tuned models.

## Method Summary
The study fine-tunes LLaMA-1B/3B/8B models using GFlowNets with trajectory balance objective on the Game of 24 task, then evaluates zero-shot transfer to the Game of 42. Training uses 20 examples (10 easy, 10 hard) from LLM-reasoner dataset. Models are evaluated on 50-problem test sets for each game, with subsets categorized by solution diversity (1-2 solutions for accuracy, ≥7 solutions for diversity). The GFlowNet reward function assigns 100 for success and 0.001 otherwise. Decoding strategies include top-k (5, 10, 15), min-p (0.05, 0.10, 0.15), and top-p (0.85, 0.90, 0.95) with temperature settings of 0.3, 0.7, and 1.1.

## Key Results
- Fine-tuned GFlowNets achieve higher success rates than pre-trained LLaMA on Game of 24 across all configurations
- Optimal decoding parameters identified: top-k=10, min-p=0.05, top-p=0.85
- Higher temperatures (0.7-1.1) yield better performance by facilitating more exploration
- GFlowNets show limited zero-shot transferability to Game of 42, maintaining neither accuracy nor diversity
- Transfer performance remains below pre-trained LLaMA baselines on out-of-distribution tasks

## Why This Works (Mechanism)

### Mechanism 1: Flow-Matching Objective for Diverse Sampling
- Claim: GFlowNets train a forward policy P_F to sample trajectories proportional to reward, enabling diverse valid solutions rather than single high-reward outputs.
- Mechanism: The trajectory balance objective forces the learned flow F(s) at terminal states to match R(x), distributing probability mass across all high-reward states rather than concentrating on the maximum.
- Core assumption: The reward function correctly identifies all valid solutions, and the DAG structure captures reachable states.

### Mechanism 2: Temperature Scaling as Exploration Control
- Claim: Higher temperatures (0.7–1.1) flatten token distributions, enabling broader trajectory exploration critical for discovering diverse solutions.
- Mechanism: Temperature divides logits before softmax; higher values reduce probability concentration, allowing lower-probability but valid paths to be sampled.
- Core assumption: The model has learned enough structure that higher-probability tokens are not exclusively correct.

### Mechanism 3: Task-Specific Flow Overfitting Limits Transfer
- Claim: GFlowNet fine-tuning optimizes flow for the training distribution's reward landscape, but this specialization does not generalize to related tasks with different target rewards.
- Mechanism: The forward policy learns state-transition probabilities conditioned on trajectories that reach the specific reward (24); changing the target (42) requires re-learning edge flows for a different terminal state distribution.
- Core assumption: Transfer requires shared structure in trajectory space; target-value change fundamentally alters which paths are rewarded.

## Foundational Learning

- **Concept: Markov Decision Processes and Flow Networks**
  - Why needed here: GFlowNets frame generation as trajectories over a DAG; understanding state transitions and flow conservation is prerequisite.
  - Quick check question: Can you explain why flow must satisfy F(s)·P_F(s'|s) = F(s')·P_B(s|s')?

- **Concept: Reward Shaping and Sparse Rewards**
  - Why needed here: The paper uses binary reward (100 for success, 0.001 otherwise); understanding how sparse rewards affect credit assignment is critical.
  - Quick check question: Why might a sparse reward of 100 only at termination cause learning instability?

- **Concept: Autoregressive Decoding Strategies**
  - Why needed here: Top-k, top-p, and min-p sampling directly interact with GFlowNet policy outputs.
  - Quick check question: How does top-k=10 differ from top-p=0.85 in terms of candidate set size variability?

## Architecture Onboarding

- **Component map:** LLaMA backbone (1B/3B/8B) -> Token logits -> Temperature scaling -> Decoding strategy (top-k/min-p/top-p) -> Autoregressive trajectory generation -> Reward evaluation (target match) -> GFlowNet loss (trajectory balance) -> Backprop to policy

- **Critical path:**
  1. Sample 20 trajectories per problem instance
  2. Evaluate each trajectory: does it reach target (24 or 42)?
  3. Compute trajectory balance loss using rewards
  4. Update forward policy P_F

- **Design tradeoffs:**
  - Higher temperature -> more exploration but risk of incoherence
  - Small training set (20 examples) -> fast iteration but limited distribution coverage
  - Top-k vs. top-p: top-k is more stable (paper finding), top-p adapts to distribution shape

- **Failure signatures:**
  - SR near zero on transfer task (Game of 42) despite high SR on training task -> flow overfitting
  - TC/SR ≈ 1.0 -> no diversity gain despite GFlowNet training
  - Performance collapse at temperature < 0.3 -> insufficient exploration

- **First 3 experiments:**
  1. Replicate Game of 24 fine-tuning with top-k=10, temp=0.7 on LLaMA-3B; verify SR matches paper (~0.40)
  2. Ablate temperature: run same setup at temp=0.3, 0.7, 1.1; confirm diversity (TC/SR) increases with temperature
  3. Zero-shot transfer test: evaluate same model on Game of 42 without fine-tuning; expect SR drop to near pre-trained baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does scaling model size beyond 8B parameters significantly improve GFlowNets' zero-shot transfer capabilities?
- Basis in paper: [explicit] The conclusion states that "Computational limitations constrain these findings, as experiments focused on small and medium-sized LLMs," and suggests expanding to larger models could offer deeper insights.
- Why unresolved: The study only evaluated LLaMA 1B, 3B, and 8B models; it is unknown if the observed transfer failure is an inherent limitation of GFlowNets or a capacity issue.
- What evidence would resolve it: Replicating the Game of 24/42 transfer experiment on larger models (e.g., 70B+) to observe if the performance gap between in-distribution and out-of-distribution tasks narrows.

### Open Question 2
- Question: How can GFlowNet training objectives or reward structures be modified to improve generalization to out-of-distribution tasks?
- Basis in paper: [explicit] The abstract highlights the "need for future research in improved transfer learning capabilities" based on the finding that GFlowNets struggle to maintain diversity and accuracy during transfer.
- Why unresolved: The current trajectory balance objective successfully optimizes for in-distribution diversity but fails to capture the underlying arithmetic logic required for the transfer to Game of 42.
- What evidence would resolve it: A comparative study of new GFlowNet objectives or reward shaping techniques that result in statistically significant success rates on the Game of 42 without explicit training on it.

### Open Question 3
- Question: Is the failure to transfer primarily due to the small training dataset size (20 examples) rather than the GFlowNet architecture itself?
- Basis in paper: [inferred] The methodology notes the use of only "20 examples (10 easy and 10 hard) for training."
- Why unresolved: While the authors attribute poor transfer to generalization limitations, the extremely low number of training samples may have resulted in overfitting to the Game of 24, limiting the model's ability to abstract the arithmetic rules needed for Game 42.
- What evidence would resolve it: Ablation studies showing the relationship between training set volume and transfer success rates.

## Limitations
- The study identifies significant transfer limitations but does not provide mechanistic explanations for why fine-tuned models fail on the closely related Game of 42 task
- The small training dataset (20 examples) raises questions about whether results generalize to larger-scale GFlowNet training regimes
- The paper does not compare GFlowNet transfer performance against alternative fine-tuning approaches beyond LLaMA

## Confidence
- High confidence: In-distribution performance improvements with optimal decoding parameters (top-k=10, min-p=0.05, top-p=0.85)
- Medium confidence: Temperature's role in facilitating exploration (0.7-1.1 optimal range)
- Medium confidence: Limited zero-shot transferability to Game of 42
- Low confidence: Explanations for why transfer fails and whether GFlowNets fundamentally cannot transfer or require different training approaches

## Next Checks
1. **Ablation study on temperature**: Run Game of 24 fine-tuning with fixed optimal decoding parameters but systematically vary temperature (0.3, 0.5, 0.7, 0.9, 1.1, 1.3) to isolate temperature's independent effect on SR and TC

2. **Intermediate state analysis**: Track and visualize the trajectory states that GFlowNets generate during Game of 24 training versus Game of 42 inference to identify whether structural mismatches in intermediate state distributions explain transfer failure

3. **Transfer learning variants**: Implement and test alternative transfer approaches including multi-task training (24 and 42 simultaneously), curriculum learning (gradual target increase), and parameter-efficient fine-tuning methods to determine if GFlowNets can be adapted for better cross-task generalization