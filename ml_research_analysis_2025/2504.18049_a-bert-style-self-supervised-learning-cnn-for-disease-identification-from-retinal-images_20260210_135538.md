---
ver: rpa2
title: A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal
  Images
arxiv_id: '2504.18049'
source_url: https://arxiv.org/abs/2504.18049
tags:
- images
- learning
- self-supervised
- image
- cnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a BERT-style self-supervised learning approach
  using a lightweight CNN (nn-MobileNet) for disease identification from retinal images.
  Unlike Vision Transformers (ViTs), which require large labeled datasets and high
  computational power, this method leverages unlabeled retinal fundus images from
  the UK Biobank (178,803 images) for pre-training.
---

# A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images

## Quick Facts
- arXiv ID: 2504.18049
- Source URL: https://arxiv.org/abs/2504.18049
- Authors: Xin Li; Wenhui Zhu; Peijie Qiu; Oana M. Dumitrascu; Amal Youssef; Yalin Wang
- Reference count: 33
- Key result: Achieves 99.33% accuracy for Alzheimer's disease identification and 99.38% accuracy for Parkinson's disease identification from retinal images using BERT-style self-supervised pre-training on a lightweight CNN.

## Executive Summary
This paper introduces a BERT-style self-supervised learning approach using a lightweight CNN (nn-MobileNet) for disease identification from retinal images. Unlike Vision Transformers (ViTs), which require large labeled datasets and high computational power, this method leverages unlabeled retinal fundus images from the UK Biobank (178,803 images) for pre-training. The approach employs sparse convolution and hierarchical feature maps to overcome limitations of CNNs in handling masked regions during self-supervised learning. Experimental results show significant performance improvements in downstream tasks, including Alzheimer's disease (AD) and Parkinson's disease (PD) identification, achieving 99.33% accuracy and 99.38% accuracy, respectively. The method also demonstrated enhanced performance on the MICCAI Myopic Maculopathy Analysis Challenge (MMAC) dataset, improving kappa score by 0.232 and weighted F1 score by 0.0412. This work highlights the potential of CNNs in medical imaging with limited labeled data, offering a scalable and efficient alternative to ViTs.

## Method Summary
The method uses nn-MobileNet as the backbone encoder, modified with ILRB blocks and specific channel configurations. It employs BERT-style masked autoencoding with 60% masking ratio and 7×7 patches on 224×224 images. Sparse convolution ensures convolution operations are conducted exclusively on non-masked pixels. Hierarchical feature maps at five resolutions (H/2, H/4, H/8, H/16, H/32) are generated and used for multi-scale reconstruction via a UNet-style decoder with 4 upsampling blocks. The model is pre-trained on 178,803 unlabeled retinal fundus images from UK Biobank, then fine-tuned on small labeled datasets for downstream tasks (AD: 362 images, PD: 415 images) using 5-fold stratified cross-validation with 80/20 train/validation split and heavy data augmentation.

## Key Results
- Achieved 99.33% accuracy for Alzheimer's disease identification and 99.38% accuracy for Parkinson's disease identification from retinal images.
- Improved kappa score by 0.232 and weighted F1 score by 0.0412 on the MICCAI Myopic Maculopathy Analysis Challenge (MMAC) dataset.
- Demonstrated superior performance compared to standard ImageNet pre-training and ViT-based approaches while using significantly fewer training images.

## Why This Works (Mechanism)

### Mechanism 1
Sparse convolution enables CNNs to perform BERT-style masked autoencoding without the architectural inconsistencies that plague standard convolutions on masked inputs. Standard convolutions process all pixels in a sliding window, including masked regions, which introduces noise and inconsistent gradients. Sparse convolution explicitly tracks which pixels are masked and restricts convolution operations to only non-masked (visible) regions. This preserves the spatial consistency needed for reconstruction-based pre-training, allowing the encoder to build representations from genuine image content rather than artifacts of the masking process.

### Mechanism 2
Hierarchical multi-scale feature map reconstruction provides richer supervisory signal than single-resolution reconstruction, improving transfer to downstream tasks with varying spatial requirements. The encoder generates feature maps at five resolutions, and the decoder receives masked features at each resolution and progressively reconstructs via upsampling blocks with skip connections. This multi-scale supervision forces intermediate representations to capture both fine-grained details and global structure, which different downstream tasks may require differently.

### Mechanism 3
Pre-training on large-scale domain-matched unlabeled data (UK Biobank fundus images) transfers more efficiently to medical downstream tasks than either supervised ImageNet pre-training or larger-scale but domain-mismatched SSL. Because these images share the same modality, anatomical structure, and imaging artifacts as downstream tasks, the learned features capture domain-relevant invariances rather than generic edges/textures from ImageNet. When fine-tuned on small labeled sets, the model requires fewer gradient updates to reach high performance.

## Foundational Learning

- **Concept:** Masked Autoencoding (MAE) / BERT-style pre-training
  - **Why needed here:** Understanding how masking + reconstruction creates self-supervision is essential for diagnosing pre-training failures.
  - **Quick check question:** Can you explain why predicting masked pixels from visible pixels teaches the model about image structure without any labels?

- **Concept:** Sparse Convolution / Submanifold Sparse Convolution
  - **Why needed here:** Implementing this method requires understanding how sparse convolution differs from dense convolution—specifically, how to maintain a sparse data structure and avoid converting masked regions to zeros.
  - **Quick check question:** In a 3×3 convolution kernel centered on a visible pixel, how should the output change if 4 of the9 input pixels are masked? What if they're simply set to zero instead?

- **Concept:** Transfer Learning and Fine-tuning Protocols
  - **Why needed here:** The paper's value proposition hinges on effective transfer from pre-training to small downstream datasets.
  - **Quick check question:** If downstream fine-tuning accuracy is *worse* than training from scratch, what are three hypotheses you would test first?

## Architecture Onboarding

- **Component map:**
  Input (224×224 RGB) -> [Random Masking 60%] -> [Sparse Convolution Wrapper] -> Encoder: nn-MobileNet -> Feature maps S_i (5 resolutions) -> Decoder: UNet-style -> Loss: MSE(visible regions)

- **Critical path:**
  1. Mask generation: Must produce consistent 7×7 grid masks aligned with downsampling ratio (32).
  2. Sparse convolution integration: The sparse conv library must correctly handle the mask and not process masked voxels.
  3. Hierarchical mask propagation: Mask must be correctly downsampled to match each feature map resolution.
  4. Pre-training → fine-tuning handoff: Encoder weights must be correctly extracted and loaded into a new classification model.

- **Design tradeoffs:**
  - Masking ratio (60%): Higher ratios may provide stronger regularization but risk insufficient context for reconstruction.
  - nn-MobileNet vs. larger backbone: nn-MobileNet is chosen for efficiency, but larger CNNs may capture more complex features.
  - Decoder depth (4 blocks): Deeper decoder improves reconstruction quality but may overfit to pre-training task.
  - Pre-training epochs: Insufficient pre-training underfits; excessive pre-training wastes compute.

- **Failure signatures:**
  - Reconstruction loss doesn't decrease: Check sparse convolution implementation; may be processing masked regions.
  - Fine-tuning accuracy = random chance: Check weight loading; encoder may not be receiving pre-trained weights.
  - Fine-tuning worse than training from scratch: Pre-training may have collapsed to trivial solution.
  - OOM during pre-training: Sparse conv implementations may have overhead. Reduce batch size.
  - Large gap between train/val accuracy during fine-tuning: Overfitting on small downstream dataset.

- **First 3 experiments:**
  1. Validate sparse convolution correctness: Pre-train for 10 epochs on a small subset. Visualize reconstructions. Check that masked regions are being predicted.
  2. Ablate pre-training benefit: Train nn-MobileNet from scratch on downstream task vs. load pre-trained weights and fine-tune. Compare accuracy and convergence speed.
  3. Domain sensitivity test: Pre-train on UK Biobank fundus, then fine-tune on MMAC dataset vs. a non-retinal medical imaging dataset. Quantify transfer gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can this BERT-style self-supervised learning framework be effectively adapted for 3D or cross-sectional medical imaging modalities, specifically Optical Coherence Tomography (OCT) and Magnetic Resonance Imaging (MRI)?
- Basis in paper: The conclusion states: "In our future work, we aim to further investigate the advantages of CNN within the realm of self-supervised learning, extending their application to a wider array of medical imaging analyses, including optical coherence tomography (OCT) and magnetic resonance imaging (MRI)."
- Why unresolved: The current study validates the method exclusively on 2D retinal fundus images, and it is unclear if the sparse convolution strategy handles the depth information and different noise profiles inherent in OCT and MRI data effectively.
- What evidence would resolve it: Successful implementation and performance benchmarking of the sparse convolution pre-training strategy on standard OCT and MRI datasets, demonstrating comparable or superior results to current supervised baselines.

### Open Question 2
How does the pre-training efficiency and final accuracy of this lightweight CNN approach scale compared to Vision Transformers (ViT) when utilizing datasets significantly larger than the UK Biobank (e.g., >1.6M images)?
- Basis in paper: The paper compares its results against RETFound (trained on 1.6M images) while using only 178,803 images, suggesting CNNs are more data-efficient.
- Why unresolved: The experimental comparison uses different dataset sizes for the CNN and the ViT competitor, leaving the comparative scaling laws untested.
- What evidence would resolve it: A comparative study pre-training both the proposed nn-MobileNet and a ViT baseline on the identical large-scale dataset to directly compare performance ceilings and computational costs.

### Open Question 3
To what extent is the specific masking ratio (60%) and mask size dependent on the downsampling ratio of the nn-MobileNet architecture, and are these parameters optimal for other CNN backbones?
- Basis in paper: The method section specifies a fixed masking ratio of 60% and a mask size calculated based on the network's downsampling ratio, but provides no ablation study on how sensitive the reconstruction task is to these specific hyperparameters.
- Why unresolved: Without ablation studies, it is unclear if these parameters are heuristically chosen for this specific architecture or if they represent a generalizable principle for sparse CNN pre-training.
- What evidence would resolve it: Ablation experiments varying the masking ratio and mask sizes across different downstream tasks to identify the optimal configuration and sensitivity of the model.

## Limitations

- Sparse convolution implementation details are not fully specified; correctness depends on third-party libraries which may vary in behavior.
- Pre-training duration and exact hyperparameters (learning rate, batch size) are unspecified, making replication sensitivity unclear.
- Transfer performance depends on domain alignment between UK Biobank and downstream datasets, which is not quantitatively validated beyond the paper's results.

## Confidence

- Mechanism 1 (Sparse convolution for masked reconstruction): Medium - theoretically sound but implementation-dependent
- Mechanism 2 (Hierarchical multi-scale supervision): Medium - architectural rationale is clear, but empirical necessity unproven
- Mechanism 3 (Domain-specific pre-training benefit): Medium - supported by comparison to RETFound but lacks ablation on domain mismatch

## Next Checks

1. **Sparse convolution correctness:** Visualize pre-training reconstructions after 10 epochs to confirm masked regions are being predicted (not artifacts from processing masked pixels). Check gradient flow through sparse conv layers.
2. **Pre-training necessity:** Train identical nn-MobileNet architecture from scratch on AD/PD tasks vs. with pre-trained weights. Compare accuracy, AUC, and convergence curves to quantify transfer benefit.
3. **Domain sensitivity:** Pre-train on UK Biobank fundus images, then fine-tune on MMAC dataset (domain-matched) vs. non-retinal medical imaging dataset (domain-mismatched). Measure transfer gap to validate domain-specific pre-training hypothesis.