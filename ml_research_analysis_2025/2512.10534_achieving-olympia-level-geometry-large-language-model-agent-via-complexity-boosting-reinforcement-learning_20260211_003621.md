---
ver: rpa2
title: Achieving Olympia-Level Geometry Large Language Model Agent via Complexity
  Boosting Reinforcement Learning
arxiv_id: '2512.10534'
source_url: https://arxiv.org/abs/2512.10534
tags:
- geometry
- agent
- learning
- data
- interngeometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InternGeometry, the first large language
  model agent to achieve medalist-level performance on International Mathematical
  Olympiad (IMO) geometry problems. The agent overcomes weak auxiliary-construction
  heuristics through iterative proposition and construction proposals, verification
  via a symbolic engine, and reflection on feedback.
---

# Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.10534
- Source URL: https://arxiv.org/abs/2512.10534
- Reference count: 40
- InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding average gold medalist score

## Executive Summary
InternGeometry is the first large language model agent to achieve medalist-level performance on International Mathematical Olympiad (IMO) geometry problems. Built on InternThinker-32B, the agent uses a Complexity-Boosting Reinforcement Learning (CBRL) framework to gradually increase training difficulty. It achieves remarkable data efficiency, solving 44 of 50 IMO problems using only 13K training examples - just 0.004% of AlphaGeometry 2's data.

The system overcomes weak auxiliary-construction heuristics through iterative proposition and construction proposals, verification via a symbolic engine, and reflection on feedback. A dynamic memory mechanism enables over 200 interactions per problem. Notably, InternGeometry can propose novel auxiliary constructions not found in human solutions, suggesting potential for creative problem-solving beyond human methods.

## Method Summary
InternGeometry combines an LLM-based geometry reasoning agent with a symbolic deduction engine through an iterative feedback loop. The agent generates propositions and construction proposals, which are verified by the symbolic engine. Failed attempts trigger reflection and refinement, with a dynamic memory mechanism tracking the problem-solving process across 200+ interactions. The Complexity-Boosting Reinforcement Learning framework progressively increases problem difficulty during training, improving efficiency. The system is built on InternThinker-32B and trained on just 13K examples, achieving performance exceeding average gold medalist scores on IMO geometry problems from 2000-2024.

## Key Results
- Solves 44 of 50 IMO geometry problems (2000-2024), exceeding average gold medalist score of 40.9
- Achieves this performance using only 13K training examples (0.004% of AlphaGeometry 2's data)
- Proposes novel auxiliary constructions not found in human solutions
- Enables over 200 interactions per problem through dynamic memory mechanism

## Why This Works (Mechanism)
The system's effectiveness stems from its iterative refinement process that combines LLM creativity with symbolic verification. The agent proposes geometric constructions and propositions, which are rigorously checked by the symbolic engine. When verification fails, the system reflects on the feedback and generates improved proposals. This creates a powerful loop where the LLM explores the solution space while the symbolic engine ensures mathematical validity. The Complexity-Boosting Reinforcement Learning framework further enhances this by gradually exposing the model to more difficult problems, preventing it from plateauing on simpler cases and building robust problem-solving capabilities across the difficulty spectrum.

## Foundational Learning
- Symbolic geometry verification - needed to ensure mathematical correctness of proposed solutions; quick check: verify engine can prove standard geometric theorems
- Iterative feedback loops - needed to refine proposals through successive approximations; quick check: measure improvement in proposal quality over interaction cycles
- Dynamic memory mechanisms - needed to track complex problem-solving states across 200+ interactions; quick check: verify memory retention of failed attempts prevents repetition
- Complexity boosting in RL - needed to prevent early overfitting on simple problems; quick check: compare learning curves with and without complexity boosting
- Auxiliary construction generation - needed to identify useful intermediate steps; quick check: measure success rate of generated constructions leading to solutions

## Architecture Onboarding

**Component Map:**
InternThinker-32B (LLM) -> Proposition Generator -> Construction Generator -> Symbolic Engine (verifier) -> Reflection Module -> Dynamic Memory -> InternThinker-32B (feedback loop)

**Critical Path:**
Construction proposal → Symbolic verification → Feedback → Reflection → Updated proposal

**Design Tradeoffs:**
- LLM-based reasoning vs pure symbolic methods: trades computational efficiency for creative exploration
- Limited training data vs large-scale pretraining: achieves high performance with 0.004% of competitor's data
- Iterative approach vs direct solving: enables complex problem decomposition at cost of computational overhead

**Failure Signatures:**
- Getting stuck in local optima of construction space
- Memory overflow from 200+ interaction tracking
- Symbolic engine timeouts on complex verification tasks
- Overfitting to training problem patterns

**First Experiments:**
1. Run single IMO problem through full pipeline to verify end-to-end functionality
2. Measure symbolic engine verification time across problem difficulty spectrum
3. Test model with disabled reflection module to quantify its contribution

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance comparison uses average gold medalist scores rather than individual problem accuracy rates
- Heavy reliance on symbolic engine raises questions about genuine LLM understanding versus interface to formal verification
- Computational overhead from 200+ interactions per problem may limit practical deployment
- Novel construction claims need independent verification for mathematical validity

## Confidence

**Major Claim Confidence:**
- IMO performance claims: Medium
- Novel construction generation: Medium
- Training efficiency claims: High
- Symbolic engine integration effectiveness: High

## Next Checks
1. Conduct ablation studies removing the symbolic engine to quantify its contribution to final performance versus the LLM's own reasoning capabilities
2. Test the model on geometry problems outside the IMO dataset to assess generalization to novel problem types and difficulty levels
3. Implement human expert review of claimed novel auxiliary constructions to verify mathematical validity and novelty independent of automated verification