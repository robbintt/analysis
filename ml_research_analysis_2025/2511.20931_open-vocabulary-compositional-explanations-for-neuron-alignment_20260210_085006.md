---
ver: rpa2
title: Open Vocabulary Compositional Explanations for Neuron Alignment
arxiv_id: '2511.20931'
source_url: https://arxiv.org/abs/2511.20931
tags:
- explanations
- concepts
- concept
- segmentation
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the dependency of compositional explanations
  on human-annotated datasets by introducing a framework for open vocabulary compositional
  explanations in the vision domain. The framework uses open vocabulary semantic segmentation
  models to generate segmentation masks for arbitrary concepts without requiring manual
  annotations.
---

# Open Vocabulary Compositional Explanations for Neuron Alignment

## Quick Facts
- arXiv ID: 2511.20931
- Source URL: https://arxiv.org/abs/2511.20931
- Authors: Biagio La Rosa; Leilani H. Gilpin
- Reference count: 40
- Key outcome: Framework achieves comparable or better performance than previous methods on human-annotated datasets while offering greater flexibility and better results on datasets without human annotations.

## Executive Summary
This paper introduces a framework for generating compositional explanations of neuron activations in vision models using open vocabulary semantic segmentation. The approach eliminates dependency on human-annotated datasets by leveraging pre-trained segmentation models to generate concept masks for arbitrary user-specified concepts. The framework consists of three steps: specifying concepts, generating segmentation masks using open vocabulary models, and deriving compositional explanations through beam search optimization.

## Method Summary
The framework generates open vocabulary compositional explanations through a three-step process: (1) specifying arbitrary concepts organized into disjoint subsets representing different granularity levels, (2) using open vocabulary semantic segmentation models (specifically CAT-Seg) to generate pixel-level masks for each concept without manual annotation, and (3) applying a beam search algorithm guided by the MMESH spatial heuristic to find logical formulas (AND, OR, AND NOT combinations) that maximize intersection over union (IoU) with neuron activation clusters. The method is training-free and works with any probed vision model and concept set.

## Key Results
- Achieves IoU=0.212 on ADE20K validation set compared to human-annotated baseline of 0.219
- Outperforms previous methods on datasets without human annotations (e.g., CUB dataset)
- Enables explanations at varying levels of granularity through concept subset separation

## Why This Works (Mechanism)

### Mechanism 1
Open vocabulary segmentation models can substitute for human-annotated masks without significant degradation in explanation quality. The framework leverages pre-trained models (e.g., CAT-Seg) that encode visual-linguistic alignments from large-scale training to generate pixel-level concept masks for arbitrary user-specified concepts, enabling IoU-based alignment computation without manual annotation.

### Mechanism 2
Beam search guided by the MMESH spatial heuristic efficiently identifies compositional explanations maximizing neuron-concept alignment. The algorithm iteratively expands candidate labels using logical operators, with MMESH estimating IoU using bounding/inscribed boxes to prune candidates below the current beam minimum before computing exact IoU.

### Mechanism 3
Separating concepts into disjoint subsets enables multi-granularity explanations and mitigates intra-subset granularity conflicts. Each concept subset represents a granularity level (e.g., species, parts, colors), allowing the algorithm to compute separate mask sets per subset and select across subsets during beam search.

## Foundational Learning

- **Concept: Semantic Segmentation and IoU Metric**
  - Why needed here: The framework's core operation is generating pixel-level masks and measuring their overlap with neuron activations.
  - Quick check question: Given two binary masks of size H×W, can you compute IoU as intersection/union?

- **Concept: Compositional/Logical Explanations**
  - Why needed here: Explanations are expressed as propositional formulas (e.g., "(Cat AND White) OR Dog") requiring understanding of AND, OR, AND NOT semantics.
  - Quick check question: If mask_A covers pixels {1,2,3} and mask_B covers {2,3,4}, what pixels does "A AND NOT B" cover?

- **Concept: Beam Search with Heuristics**
  - Why needed here: The explanation algorithm uses beam search (width=5, max length=3) guided by MMESH estimates to avoid exponential search.
  - Quick check question: In beam search with width 3, if candidates have scores [0.8, 0.7, 0.6, 0.5], which survive to the next iteration?

## Architecture Onboarding

- **Component map:** Concept Set Specification -> Mask Generator -> Binarization Module -> Activation Extractor -> Compositional Explanation Engine
- **Critical path:** Concept set → Mask generation (per subset) → Binarization → Activation extraction → Beam search. Mask generation is the bottleneck; must re-run for each modified subset.
- **Design tradeoffs:** Memory vs. concept count (more concepts increase output tensor size), granularity vs. completeness (specific concepts reduce hallucinations but require more complete subsets), beam width vs. optimality (larger beam increases chance of finding optimal explanation but slows computation)
- **Failure signatures:** High ActCov + low DetAcc (explanations too general), degenerate explanations in low clusters (algorithm converges on unrelated concepts), mask hallucinations (concept set incomplete for dataset)
- **First 3 experiments:** (1) Baseline comparison on human-annotated data: Run framework on Ade20K with Places365 probed model; compare IoU, ActCov, DetAcc against [31]. (2) No-annotation scenario: Probe CUB-trained model using CUB dataset without human masks. Verify framework outperforms closed-vocabulary [3] and cross-dataset [31] baselines. (3) Granularity ablation: For a single neuron, generate explanations using single-granularity subsets vs. multi-granularity combination.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance critically depends on the quality and coverage of the open vocabulary segmentation model
- Beam search heuristic provides no guarantees of finding globally optimal explanations
- Multi-granularity approach assumes concept hierarchies align with the segmentation model's internal representations

## Confidence
- **High Confidence:** The framework's ability to generate compositional explanations using open vocabulary masks
- **Medium Confidence:** Claims about flexibility and performance on non-human-annotated datasets
- **Medium Confidence:** The effectiveness of multi-granularity explanations in providing complementary insights

## Next Checks
1. Cross-dataset generalization test: Evaluate the framework on a third dataset (e.g., COCO-Stuff) with and without human annotations to verify robustness across different visual domains and concept distributions.

2. Segmentation model ablation study: Compare explanation quality using different open vocabulary segmentation models (CAT-Seg vs. alternatives like PICa or Detic) to isolate the impact of segmentation quality on explanation performance.

3. Optimality verification: Implement an exhaustive search baseline for small concept sets to measure the frequency and magnitude of sub-optimal solutions produced by the MMESH-guided beam search heuristic.