---
ver: rpa2
title: Beyond Line-Level Filtering for the Pretraining Corpora of LLMs
arxiv_id: '2510.24139'
source_url: https://arxiv.org/abs/2510.24139
tags:
- table
- data
- lines
- korean
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper revisits conventional line-level filtering techniques\
  \ for large language model pretraining corpora, highlighting that basic rules such\
  \ as deduplication and trailing-punctuation filtering can inadvertently remove structurally\
  \ important content. The authors propose two enhanced methods\u2014pattern-aware\
  \ line-level deduplication (PLD) and pattern-aware trailing-punctuation filtering\
  \ (PTF)\u2014which consider the sequential distribution of lines within documents\
  \ rather than applying isolated heuristics."
---

# Beyond Line-Level Filtering for the Pretraining Corpora of LLMs

## Quick Facts
- arXiv ID: 2510.24139
- Source URL: https://arxiv.org/abs/2510.24139
- Authors: Chanwoo Park; Suyoung Park; Yelim Ahn; Jongmin Kim; Jongyeon Park; Jaejin Lee
- Reference count: 40
- Primary result: Pattern-aware line-level filtering improves downstream task performance by preserving document structure

## Executive Summary
This paper revisits conventional line-level filtering techniques for large language model pretraining corpora, highlighting that basic rules such as deduplication and trailing-punctuation filtering can inadvertently remove structurally important content. The authors propose two enhanced methods—pattern-aware line-level deduplication (PLD) and pattern-aware trailing-punctuation filtering (PTF)—which consider the sequential distribution of lines within documents rather than applying isolated heuristics. PLD categorizes lines by document frequency to retain distinctive sequences while filtering repetitive or boilerplate text, and PTF allows short non-punctuated line runs between punctuated ones. Experiments training small models (1B parameters) in English and Korean show that these pattern-aware approaches consistently improve downstream performance, notably boosting accuracy on multiple-choice benchmarks and significantly enhancing generative question-answering results on SQuAD v1 and KorQuAD v1.

## Method Summary
The authors propose two pattern-aware line-level filtering methods for pretraining corpora. Pattern-aware line-level deduplication (PLD) categorizes lines by document frequency using SHA1 hashes and retains sequences matching specific regex patterns, preserving distinctive content while filtering repetitive boilerplate. Pattern-aware trailing-punctuation filtering (PTF) categorizes lines by punctuation type and allows short non-punctuated line runs between punctuated ones, preserving dialogue or list-like structures. The methods were evaluated by training 1.3B–1.5B parameter decoder-only models on CommonCrawl WET data (30B tokens) for English and Korean, comparing downstream performance on multiple-choice benchmarks (HellaSwag, ARC-Easy, PIQA, SciQ, CommonsenseQA, SocialIQA, OpenbookQA for English; KoBEST-Hellaswag/COPA, SNU-Ko-ARC-Easy/LAMBADA for Korean) and generative QA tasks (SQuAD v1, KorQuAD v1).

## Key Results
- Pattern-aware line-level filtering consistently improves downstream task performance across multiple benchmarks
- PLD and PTF combined yield significant improvements on multiple-choice tasks (HellaSwag, ARC-Easy, PIQA, SciQ)
- PTF substantially enhances generative question-answering performance on SQuAD v1 and KorQuAD v1
- The methods show robust performance across both English and Korean languages

## Why This Works (Mechanism)
The paper demonstrates that conventional line-level filtering techniques can inadvertently remove structurally important content by treating lines in isolation. By considering document-level patterns and sequential distributions of lines, the proposed methods preserve distinctive sequences while filtering repetitive or boilerplate text. The pattern-aware approach maintains the coherence and structure of documents, which is crucial for training language models that can understand context and generate coherent responses.

## Foundational Learning
- Document frequency categorization: Lines are classified as Red/Yellow/Green based on how many documents they appear in, helping distinguish unique content from boilerplate
  - Why needed: To identify and preserve distinctive content while filtering repetitive text
  - Quick check: Verify that unique lines (Green) appear in fewer documents than boilerplate (Red)

- Regex-based sequence patterns: Specific patterns like `g{2,}`, `g{2,}(y+g{1,})+g`, and `g+` define which line sequences to retain
  - Why needed: To preserve structurally meaningful document patterns
  - Quick check: Test regex patterns on sample categorized sequences to ensure correct matching

- Punctuation-aware filtering: Different treatment of lines based on trailing punctuation (., ?, !, ", ')
  - Why needed: To maintain dialogue, lists, and other structured text formats
  - Quick check: Verify that punctuation types are correctly identified and categorized

## Architecture Onboarding

**Component Map**
Data Preparation -> Line Frequency Counting -> PLD Filtering -> PTF Filtering -> Training Data

**Critical Path**
The critical path is the sequence from data preparation through both filtering stages to final training data, as errors in early stages propagate downstream.

**Design Tradeoffs**
The paper trades computational complexity for data quality, using pattern-aware methods that require additional processing but preserve document structure better than simple line-level heuristics.

**Failure Signatures**
- Incorrect frequency counts leading to wrong line categorization
- Regex patterns not matching intended sequences due to encoding issues
- Punctuation detection failures causing loss of structured content

**First 3 Experiments**
1. Validate the line frequency counting and categorization system on a small sample corpus
2. Test PLD regex patterns on categorized sequences to verify correct retention logic
3. Evaluate PTF punctuation categorization on sample text with various punctuation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pattern-aware line filtering generalize effectively to languages beyond English and Korean?
- Basis in paper: "Its applicability to other languages remains to be explored."
- Why unresolved: The study only evaluated English and Korean; languages with different writing systems (e.g., Chinese, Arabic) or morphological characteristics may exhibit different frequency distributions and punctuation patterns.
- What evidence would resolve it: Ablation studies training models on corpora processed with PLD and PTF for at least 3-5 additional languages from diverse language families, measuring downstream task performance.

### Open Question 2
- Question: How do PLD and PTF interact with other filtering steps in the data construction pipeline?
- Basis in paper: "We do not investigate the impact of other steps in the data construction pipeline, such as model-based filtering or document-level deduplication."
- Why unresolved: The paper isolates line-level filtering effects but real pipelines combine multiple techniques; interactions could amplify or diminish benefits.
- What evidence would resolve it: Systematic ablation experiments combining pattern-aware filtering with model-based quality classifiers and document-level deduplication, measuring both individual and combined effects.

### Open Question 3
- Question: Can the trade-off between multiple-choice task gains and generative QA losses from PTF be mitigated?
- Basis in paper: The authors observe that "applying PTF harms the performance of short-form generative question answering tasks" while improving multiple-choice benchmarks.
- Why unresolved: The current k parameter cannot simultaneously optimize both task types, suggesting a fundamental tension in what content is preserved.
- What evidence would resolve it: Experiments with task-adaptive filtering (different k values for different training stages) or hybrid approaches that selectively apply PTF based on document-level characteristics.

### Open Question 4
- Question: Are the optimal threshold parameters (r, g for PLD; k for PTF) transferable across languages with similar characteristics?
- Basis in paper: "It is uncertain whether parameters tuned for one language are optimal for all languages."
- Why unresolved: English uses r=1000, g=1 while Korean uses r=50, g=3; the relationship between language properties and optimal thresholds is unclear.
- What evidence would resolve it: Cross-lingual transfer experiments testing whether parameters derived from one language improve another, plus analysis correlating linguistic features (e.g., morphological complexity, corpus size) with optimal thresholds.

## Limitations
- The study relies on small models (1.3B–1.5B parameters) and short pretraining regimes (30B tokens), limiting generalizability to larger-scale settings
- Hyperparameter choices for document-frequency thresholds (r/g) and trailing-punctuation thresholds (k) are empirically derived but not extensively validated across different domains or languages
- There is ambiguity around model architecture details, shard selection, and sampling seeds, which are essential for faithful reproduction

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Pattern-aware filtering improves downstream task performance | High |
| The methods are effective for English and Korean | High |
| Results generalize to larger models and longer training | Medium |
| Optimal parameters transfer across languages | Low |

## Next Checks

1. Reproduce the pattern-aware filtering pipeline (PLD/PTF) on a held-out subset of the pretraining corpus, validating the regex-based retention logic and frequency categorization.

2. Train two 1B-parameter models (one with standard line-level filtering, one with PLD/PTF) for 30B tokens, ensuring identical architecture and training setup, and compare downstream task performance.

3. Perform ablation studies varying the document-frequency threshold (r/g) and punctuation-run threshold (k) to assess sensitivity and optimal settings for different languages.