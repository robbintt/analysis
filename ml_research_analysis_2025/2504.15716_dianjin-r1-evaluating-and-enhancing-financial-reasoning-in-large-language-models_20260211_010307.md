---
ver: rpa2
title: 'DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language
  Models'
arxiv_id: '2504.15716'
source_url: https://arxiv.org/abs/2504.15716
tags:
- reasoning
- answer
- financial
- cflue
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DianJin-R1, a reasoning-enhanced framework
  for improving financial reasoning in large language models. It combines structured
  supervision with reinforcement learning, using a high-quality dataset (DianJin-R1-Data)
  built from CFLUE, FinQA, and a proprietary compliance corpus.
---

# DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2504.15716
- Source URL: https://arxiv.org/abs/2504.15716
- Reference count: 12
- DianJin-R1-32B achieves 86.74% on CFLUE and 96.00% on CCC benchmarks

## Executive Summary
DianJin-R1 introduces a reasoning-enhanced framework for improving financial reasoning in large language models through structured supervision and reinforcement learning. The framework uses a high-quality dataset (DianJin-R1-Data) built from CFLUE, FinQA, and proprietary compliance data, with reasoning traces verified by GPT-4o. DianJin-R1 models are fine-tuned to generate both reasoning steps and final answers, then refined using Group Relative Policy Optimization with dual reward signals. The resulting models significantly outperform base models on financial reasoning tasks, with DianJin-R1-32B achieving state-of-the-art results on multiple benchmarks.

## Method Summary
DianJin-R1 combines supervised fine-tuning (SFT) with reinforcement learning (RL) to enhance financial reasoning capabilities. The SFT stage trains Qwen2.5 models on a dataset constructed from CFLUE, FinQA, and compliance corpora, with reasoning traces generated by DeepSeek-R1 and verified by GPT-4o. The RL stage employs Group Relative Policy Optimization (GRPO) with dual rewards: one encouraging structured output format (single <think/> and <answer/> blocks) and another rewarding answer correctness. The framework includes a multi-agent workflow for compliance checking that synthesizes procedural reasoning traces, which are then distilled into single-model inference.

## Key Results
- DianJin-R1-32B achieves 86.74% accuracy on CFLUE benchmark
- DianJin-R1-32B achieves 96.00% accuracy on CCC compliance benchmark
- Single-call reasoning models match or surpass multi-agent systems on CCC tasks
- Models show significant improvements over base Qwen2.5 models across all five evaluated benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Structured Reasoning Supervision
High-quality reasoning traces transfer domain-specific problem patterns through explicit chain-of-thought annotations. DeepSeek-R1 generates reasoning steps that are verified by GPT-4o for answer correctness and reasoning consistency. This structured supervision teaches the model domain-appropriate decomposition strategies for financial tasks. The approach assumes verification via GPT-4o ensures reasoning quality, though errors in the verifier may propagate as noisy supervision.

### Mechanism 2: Dual-Reward Reinforcement Learning
GRPO uses group-relative advantages with format and accuracy rewards to shape both structural compliance and answer correctness. Format reward enforces structured output schema with single <think/> and <answer/> blocks, while accuracy reward reinforces correct final answers. The binary reward system is simple to implement but may not capture partial progress in multi-step reasoning. This mechanism assumes binary rewards sufficiently capture quality for the financial reasoning tasks.

### Mechanism 3: Multi-Agent Workflow Synthesis
For CCC compliance checking, specialized agents evaluate individual condition nodes in a decision workflow. Intermediate CoTs are merged into unified reasoning traces, providing structured procedural supervision. The workflow structure encodes domain expertise, which is then distilled into single-model inference. This approach assumes the workflow correctly encodes domain knowledge and that the distillation process preserves critical reasoning patterns.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**: Essential for understanding the <think/> tag structure and reasoning verification process. Quick check: Given a financial question requiring multi-step calculation, can you write out each reasoning step before the final answer?
- **Policy Gradient Methods in RL**: Necessary to understand GRPO's optimization dynamics and reward shaping. Quick check: Explain how a binary reward signal propagates through a policy gradient update and why sparse rewards can cause learning instability.
- **Knowledge Distillation from Multi-Agent Systems**: Important for understanding how procedural reasoning is compressed from multiple agents into single-model inference. Quick check: Why might a single model fail to replicate multi-agent reasoning even when trained on merged outputs?

## Architecture Onboarding

- **Component map**: CFLUE/FinQA filtering -> DeepSeek-R1 reasoning generation -> GPT-4o verification -> retry/discard; CCC -> multi-agent workflow execution -> intermediate CoT merge -> SFT stage -> RL stage with GRPO -> evaluation on five benchmarks
- **Critical path**: Data quality filtering determines training signal quality → verify filtering thresholds on sample data; GPT-4o verification alignment → spot-check verified samples for reasoning quality; SFT convergence before RL → monitor loss and sample outputs at checkpoints; RL reward calibration → ensure format reward is achievable; check accuracy reward distribution
- **Design tradeoffs**: CFLUE-centric training provides large scale but may underweight FinQA numerical patterns and CCC procedural logic; binary rewards are simple but may not capture partial progress; single-call inference offers efficiency but may have accuracy ceiling compared to multi-agent systems
- **Failure signatures**: Low format reward during RL indicates model not learning output schema → check SFT quality, increase format reward weight; high accuracy reward but low benchmark performance suggests reward gaming or distribution shift; CCC performance degradation indicates workflow edge cases not covered → augment with adversarial examples; FinQA stagnation may indicate limited cross-lingual transfer
- **First 3 experiments**: Ablate SFT data sources to quantify contribution per domain; test reward sensitivity by varying format vs. accuracy weights; extend RL to English samples to measure cross-lingual transfer impact

## Open Questions the Paper Calls Out

1. Does incorporating English-language reinforcement learning data (e.g., from FinQA) improve cross-lingual transfer and performance on English financial benchmarks?
2. Does including hard non-reasoning cases from GCFLUEOE, GFinQA, and GCCC in the RL stage further enhance reasoning capabilities?
3. Can tool-augmented reasoning (e.g., calculators, retrieval systems, rule engines) during inference improve precision on numerical and compliance-heavy financial tasks?
4. Would extending accuracy rewards beyond multiple-choice questions (to open-ended and dialogue-based tasks) improve performance on non-MCQ financial benchmarks?

## Limitations

- GRPO implementation details are unspecified, making exact reproduction difficult
- GPT-4o serves as both verifier and evaluator, creating potential evaluation bias
- CFLUE-centric training data may cause underperformance on FinQA and CCC tasks
- Binary reward structure may be overly coarse for complex financial reasoning tasks

## Confidence

- **High Confidence**: CFLUE benchmark results and multi-agent workflow effectiveness for CCC compliance checking
- **Medium Confidence**: FinQA performance claims given CFLUE-centric training and cross-lingual transfer limitations
- **Low Confidence**: GRPO optimization dynamics and verification error impact on reasoning quality

## Next Checks

1. **GRPO Hyperparameter Sensitivity**: Systematically vary KL penalty coefficients and clip ranges to identify optimal settings and verify reported training stability
2. **Cross-Verification Benchmark**: Evaluate DianJin-R1 models using an alternative verifier (e.g., Claude-3 or GPT-4o-mini) to test for evaluation bias in reported accuracy scores
3. **Ablation Study on Training Data**: Train separate models on CFLUE-only, CFLUE+FinQA, and CFLUE+CCC subsets to quantify domain contribution and identify performance bottlenecks