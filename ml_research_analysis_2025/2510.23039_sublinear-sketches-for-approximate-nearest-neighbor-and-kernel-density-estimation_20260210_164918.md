---
ver: rpa2
title: Sublinear Sketches for Approximate Nearest Neighbor and Kernel Density Estimation
arxiv_id: '2510.23039'
source_url: https://arxiv.org/abs/2510.23039
tags:
- data
- size
- approximate
- points
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces new sublinear sketching algorithms for Approximate\
  \ Nearest Neighbor (ANN) and Approximate Kernel Density Estimation (A-KDE) on streaming\
  \ data. For ANN, under Poisson-distributed data, the authors show that only a sublinear\
  \ fraction of the stream (O(n^{1-\u03B7})) needs to be stored to maintain (c,r)-ANN\
  \ guarantees."
---

# Sublinear Sketches for Approximate Nearest Neighbor and Kernel Density Estimation

## Quick Facts
- arXiv ID: 2510.23039
- Source URL: https://arxiv.org/abs/2510.23039
- Reference count: 15
- This work introduces new sublinear sketching algorithms for Approximate Nearest Neighbor (ANN) and Approximate Kernel Density Estimation (A-KDE) on streaming data.

## Executive Summary
This paper introduces two new sublinear sketching algorithms for streaming data: one for Approximate Nearest Neighbor (S-ANN) and one for sliding-window Approximate Kernel Density Estimation (SW-AKDE). For ANN, the authors show that under Poisson-distributed data, only a sublinear fraction of the stream needs to be stored to maintain (c,r)-ANN guarantees. For A-KDE, they propose the first sliding-window sketch by integrating RACE with Exponential Histograms, enabling accurate density estimation with efficient data expiration. Experiments on real and synthetic datasets confirm the algorithms are lightweight, achieve low error, and scale well compared to baselines like Johnson-Lindenstrauss projections and RACE.

## Method Summary
The paper proposes two main algorithms. For ANN, the S-ANN algorithm uses uniform sampling to store a sublinear O(n^(1-η)) fraction of the stream in L LSH hash tables. For a query, it retrieves candidates from the tables, deduplicates them, and returns the closest point within radius cr. For A-KDE, the SW-AKDE algorithm modifies the RACE sketch by placing an Exponential Histogram (EH) in each of its L x W cells. Updates involve hashing a data point with L functions and adding a 1 to the corresponding EH. A query hashes the query point and averages the count estimates from the EHs at the corresponding cells. Both algorithms support batch queries and are designed for streaming data with the ANN version also supporting deletions in the Turnstile model.

## Key Results
- S-ANN achieves comparable or better accuracy than Johnson-Lindenstrauss projections while using less memory, with sublinear sketches possible for η≥0.5.
- SW-AKDE achieves comparable accuracy to RACE in sliding-window settings while efficiently handling data expiration.
- Both algorithms scale well and achieve low error on real and synthetic datasets.

## Why This Works (Mechanism)
S-ANN works by leveraging the probabilistic guarantees of LSH and uniform sampling. By storing only a sublinear fraction of the stream, it ensures that if a near neighbor exists in the full stream, it is likely to be stored with high probability. SW-AKDE works by combining the locality-sensitive hashing of RACE with the sliding-window guarantees of Exponential Histograms, allowing it to maintain accurate density estimates while efficiently expiring old data.

## Foundational Learning
- **p-stable LSH**: A family of hash functions where the hash value is a linear combination of the input coordinates with p-stable random variables. This property ensures that the collision probability between two points depends only on their distance, which is crucial for the LSH guarantees in S-ANN.
- **Exponential Histograms (EH)**: A data structure that maintains an approximate count of the number of 1's in the last N elements of a stream. It does this by storing O(log N) buckets with exponentially increasing sizes, which allows it to provide a 1±ε approximation with O(1/ε log N) space.
- **RACE Sketch**: A locality-sensitive hashing scheme that uses multiple hash tables to estimate the similarity between points. It works by hashing a point with multiple functions and incrementing counters at the hashed locations. The similarity is then estimated by comparing the counters.
- **Poisson Point Process**: A probability distribution over point patterns where points are independently and uniformly distributed in space. This is the assumed data distribution for the theoretical guarantees of S-ANN.
- **Turnstile Model**: A model of streaming data where updates can be both insertions and deletions. S-ANN extends to this model with the assumption that an adversary deletes at most d points from any r-ball.

## Architecture Onboarding
- **Component Map**: Stream -> S-ANN (LSH + Sampling) -> ANN Candidates -> Nearest Neighbor; Stream -> SW-AKDE (RACE + EH) -> Density Estimate
- **Critical Path**: For S-ANN: Point insertion -> LSH hashing -> Uniform sampling -> Hash table storage. For SW-AKDE: Point update -> LSH hashing -> EH update -> Density query.
- **Design Tradeoffs**: S-ANN trades off accuracy for space by storing only a sublinear fraction of the stream. SW-AKDE trades off query time for space by using multiple EHs to maintain the sliding window.
- **Failure Signatures**: For S-ANN, if recall is zero, it likely means the sampling parameter η is too large. For SW-AKDE, if the density estimate is inaccurate, it likely means the EHs are not correctly expiring old data.
- **First Experiments**: 1. Implement and unit-test the EH data structure on a controlled stream. 2. Implement S-ANN with a simple LSH family and verify the number of stored points matches the expected O(n^(1-η)) bound. 3. Implement SW-AKDE and compare its density estimates to the true values on a small synthetic dataset.

## Open Questions the Paper Calls Out
1. Can the sublinear sketch guarantees for ANN be extended to data distributions other than the Poisson point process?
2. How can the sliding-window size N be selected optimally or adapted dynamically based on the evolving data distribution?
3. Can data-aware Locality Sensitive Hashing (LSH) variants be utilized to improve the space-time trade-offs of these streaming sketches?
4. Can the ANN guarantees in the Turnstile model be maintained under less restrictive deletion constraints?

## Limitations
- The theoretical guarantees for S-ANN rely on the assumption that the data follows a Poisson point process.
- The optimal window size N for SW-AKDE is not known and may depend on the data distribution.
- The algorithms rely on standard LSH families which are data-oblivious; data-aware hashing could theoretically improve performance.

## Confidence
- **High**: Sublinear space bounds (O(n^{1-η}) for S-ANN and O(RW · 1/√{1+ε}−1 · log²N for SW-AKDE) and general algorithmic approach.
- **Medium**: Empirical results, as the exact LSH and EH parameter settings are not provided.

## Next Checks
1. Implement unit tests for the Exponential Histogram to verify it correctly estimates counts in a sliding window and expires old data.
2. For S-ANN, log the actual number of points stored during experiments and verify it matches the expected O(n^(1-η)) bound.
3. Run S-ANN with a range of η values and confirm recall degrades gracefully as η increases, as predicted by theory.