---
ver: rpa2
title: Relating System Safety and Machine Learnt Model Performance
arxiv_id: '2507.20135'
source_url: https://arxiv.org/abs/2507.20135
tags:
- performance
- safety
- detection
- requirements
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an initial method to relate system safety objectives,
  expressed as quantitative safety objectives (QSOs), to machine learnt model (MLM)
  performance requirements and metrics. Using an aircraft emergency braking system
  (AEBS) with a deep neural network for runway sign detection as an example, the authors
  develop a binomial model abstraction for the machine learnt component (MLC) behavior,
  deriving safety-related performance requirements from the allocated QSO.
---

# Relating System Safety and Machine Learnt Model Performance

## Quick Facts
- **arXiv ID**: 2507.20135
- **Source URL**: https://arxiv.org/abs/2507.20135
- **Reference count**: 26
- **Primary result**: Presents a method to derive machine learnt model performance requirements from system safety objectives using a binomial process abstraction.

## Executive Summary
This paper introduces a novel method to systematically relate system safety objectives, expressed as Quantitative Safety Objectives (QSOs), to specific performance requirements for machine learnt models (MLMs). Using an aircraft emergency braking system with a deep neural network for runway sign detection as a case study, the authors develop a binomial model abstraction for the MLM's behavior. The method derives concrete safety-related requirements including per-image detection probabilities, confirmation thresholds, and generalization error bounds. It also provides sample size estimates for testing based on statistical learning theory. The approach represents the first systematic connection between system-level safety and MLM performance metrics.

## Method Summary
The method models the MLC's behavior as a binomial process where detection events are treated as Bernoulli trials. Starting from a system-level QSO, the method uses fault tree analysis to allocate safety budgets to the MLC. It then inverts the binomial cumulative distribution function to derive maximum allowable per-image miss probabilities and confirmation thresholds that satisfy the QSO. The method also establishes the mathematical equivalence between the MLC's failure probability and its generalization error under zero-one loss. Sample size requirements for verification are calculated using Hoeffding's inequality to bound the generalization gap.

## Key Results
- Derives a per-image miss probability requirement of 0.1 and confirmation threshold of 6/12 detections to meet QSO of 2×10⁻⁴
- Establishes equivalence between MLM failure probability and generalization error (0.124)
- Provides sample size estimate of 26,393 samples for testing with tolerance 0.012 and confidence 0.001
- Demonstrates systematic connection between system safety and MLM performance for the first time

## Why This Works (Mechanism)

### Mechanism 1
System-level safety objectives (QSOs) can be decomposed into specific model performance requirements by modeling the MLC's behavior as a binomial process. The method aggregates per-image predictions into a detection vector and treats confirmation as a logic gate requiring k successes out of n Bernoulli trials. By inverting the Binomial CDF, one can solve for the maximum allowable per-image miss probability required to ensure system-level failure probability remains below QSO. Core assumption: per-image detection events are Independent and Identically Distributed (IID) Bernoulli trials.

### Mechanism 2
The probability of an MLC failure is mathematically equivalent to the model's generalization error under zero-one loss. The paper formally defines failure probability as the expectation of an indicator function for incorrect responses over input space distribution. Theorem 1 proves this is identical to Population Risk (generalization error) for binary classification. This allows safety requirements to translate directly into statistical learning bounds. Core assumption: binary loss function with well-defined correct responses.

### Mechanism 3
Required sample sizes for safety verification can be derived by bounding the generalization gap using Hoeffding's inequality relative to safety budget. Because true generalization error is unknown, the method calculates minimum number of independent test samples needed to ensure empirical risk is statistically close to true risk. Tolerance for this bound is derived as a fraction of margin between required generalization error and operational performance. Core assumption: sufficient independence in test samples to satisfy statistical inequalities.

## Foundational Learning

- **Quantitative Safety Objectives (QSO) & Fault Tree Analysis (FTA)**: This is the top of the derivation process. You cannot derive model requirements without understanding system-level probability budget (e.g., 10⁻³ per flight hour) and how it allocates down to MLC via logic gates in fault tree. Quick check: If system QSO is 10⁻⁵ and MLC is in series with hardware component with failure rate 10⁻⁶, what is approximate maximum failure probability allowed for MLC?

- **Generalization Error (Population Risk) vs. Empirical Risk**: Core mechanism relies on equating safety failure with generalization error. Engineers must distinguish between error observed on finite test set (Empirical Risk) and theoretical error over all possible inputs (Population Risk) to understand why large sample sizes are mandated. Quick check: Why is achieving 1% error rate on 100-image test set insufficient proof of 10⁻⁴ safety requirement?

- **The IID Assumption in Time-Series Data**: Paper's binomial model assumes independence between frames. In reality, video frames are highly correlated. Understanding this tension is critical for valid verification. Quick check: If car detects stop sign in frame t, is it more, less, or equally likely to detect it in frame t+1 compared to random frame from different video? How does this affect independence assumption?

## Architecture Onboarding

- **Component map**: Video Camera (Environmental Scene) -> Signal Conditioning -> MLSD (DNN) -> Detection Confirmation (K-of-M logic) -> Emergency Braking Controller
- **Critical path**: System QSO → FTA Budget Allocation → Binomial Probability Calculation → Per-Image Miss Probability → Generalization Error Requirement
- **Design tradeoffs**: Increasing detection vector size (n) or adjusting confirmation threshold (x_min) allows system to tolerate higher per-image error rate (p_miss), potentially reducing model training difficulty at cost of latency. Assumption vs. Conservatism: IID assumption is technically violated by video data; engineer must decide if "random abstraction" provides sufficiently conservative bound.
- **Failure signatures**: Systematic Non-Detection (consistent misses on specific sign types leading to confirmation failure), Marginal Performance (operating near threshold x_min, statistically increasing probability of crossing failure boundary).
- **First 3 experiments**: 1) Parameter Sensitivity Sweep: Plot Pr(T=0) vs. Per-Image Miss Probability for various confirmation thresholds to visualize safety compliance zone. 2) Sample Size Calculator: Implement Eq. (10) to calculate required test set sizes for different tolerance levels. 3) Correlation Impact Study: Simulate detection vectors with varying temporal correlation to observe how binomial probability prediction diverges from actual failure rates.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can generalization error formulation be extended to explicitly account for robustness-related failures? The paper proposes partitioning generalization error into standard generalization and robustness components but leaves robustness metric definition for future work. Requires formalized metric relating relative frequency of abnormal inputs to model failures.

- **Open Question 2**: How can method be adapted to derive requirements for per-image false positives and false classifications? Current binomial model restricts scope to false negatives when object is present, ignoring safety impacts of false alarms or misclassifications. Requires multinomial distribution constraining false positive rates to meet safety objectives.

- **Open Question 3**: Can alternative statistical bounds reduce pessimistic sample size estimates required for verification? Current method yields very large sample size requirements (>26,000 samples) which may be impractical. Hoeffding's inequality provides "pessimistic worst-case" bound; methods like Normal distribution approximation could provide more favorable estimates.

## Limitations
- Independence Assumption: Binomial model assumes IID Bernoulli trials for per-image detections, technically violated by temporally correlated video frames.
- Data Distribution Specificity: All derived requirements valid only for specific input distribution defined by Operational Design Domain; performance in out-of-distribution scenarios not addressed.
- Model Architecture Independent: Method provides requirements for MLC but does not prescribe or validate any specific DNN architecture, dataset, or training procedure.

## Confidence
- **High Confidence**: Mathematical derivation linking system-level QSOs to MLC performance via binomial CDF inversion and equivalence proof between failure probability and generalization error are sound within stated assumptions.
- **Medium Confidence**: Application of Hoeffding's inequality for sample size estimation is statistically valid, but practical choice of tolerance (ε=0.012) and safety margin parameters lacks empirical justification.
- **Low Confidence**: Practical validity of treating correlated video frames as IID Bernoulli trials is asserted but not empirically validated; most significant threat to method's real-world applicability.

## Next Checks
1. **Correlation Impact Study**: Simulate detection vectors with controlled temporal correlation (ρ from 0 to 0.9) and measure divergence between predicted binomial failure probabilities and actual failure rates.
2. **Distribution Drift Testing**: Train model on source dataset, evaluate p_miss on same-distribution test set, then evaluate on slightly shifted distribution (different lighting). Measure degradation to understand sensitivity to distribution shifts.
3. **Margin Sensitivity Analysis**: Systematically vary safety margin parameter (SM) and tolerance (ε) in sample size calculation to determine conservatism of current choices and impact on required test dataset size.