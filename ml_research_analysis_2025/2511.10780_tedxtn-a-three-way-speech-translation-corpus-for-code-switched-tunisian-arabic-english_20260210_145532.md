---
ver: rpa2
title: 'TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic
  - English'
arxiv_id: '2511.10780'
source_url: https://arxiv.org/abs/2511.10780
tags:
- speech
- arabic
- translation
- corpus
- tunisian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TEDxTN, the first publicly available Tunisian
  Arabic to English speech translation dataset. The dataset consists of 108 TEDx talks
  representing 25 hours of speech with code-switching from over 11 different regions
  of Tunisia.
---

# TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English

## Quick Facts
- arXiv ID: 2511.10780
- Source URL: https://arxiv.org/abs/2511.10780
- Reference count: 9
- First open-source speech translation corpus for code-switched Tunisian Arabic-English

## Executive Summary
This paper introduces TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. The corpus consists of 108 TEDx talks representing 25 hours of speech with code-switching from over 11 different regions of Tunisia. The authors collected, segmented, transcribed, and translated the talks following internally developed annotation guidelines. They report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. The best model achieves 21.37% WER and 25.68 BLEU scores on the transcription and translation tasks of the TEDxTN test set, respectively. The corpus is the first open-source and publicly available speech translation corpus of code-switching Tunisian dialect, and the authors believe it is a valuable resource for further research on the natural language processing of Tunisian Dialect.

## Method Summary
The TEDxTN corpus contains 25 hours of Tunisian Arabic speech from 108 TEDx talks, manually segmented into 17,278 segments with 3-way alignments (speech, Tunisian transcription, English translation). The authors collected audio from YouTube, transcribed and translated it following script-based annotation guidelines, and split the data into 97/5/6 talks for training/validation/testing. They fine-tuned multiple pre-trained models including Whisper-large-v3, w2v-BERT-2.0T, XLSR, and MMS using SpeechBrain toolkit for 80 epochs with Adam optimizer. ASR models used CTC heads while ST models used encoder-decoder architectures. Evaluation used WER/CER for ASR and TrueCased BLEU (no punctuation) for ST.

## Key Results
- Best ASR model (w2v-BERT-2.0T) achieves 21.37% WER on test set
- Best ST model (Whisper-large-v3) achieves 25.68 BLEU on test set
- Zero-shot Whisper-large-v3 performs poorly (94% WER), demonstrating need for fine-tuning
- Encoder-to-NLLB-decoder ST models fail (<5 BLEU), requiring further investigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning pre-trained multilingual speech models on domain-specific data yields substantial performance gains over zero-shot inference for low-resource dialects.
- Mechanism: Transfer learning leverages acoustic and linguistic representations learned from high-resource languages, adapting them to the specific phonetic patterns, vocabulary, and code-switching behaviors of Tunisian Arabic through gradient updates on targeted data.
- Core assumption: Pre-trained models have sufficient exposure to related linguistic features (Arabic phonemes, Romance language patterns) that transfer to the target dialect.
- Evidence anchors:
  - [abstract] "We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models"
  - [section 5.2, Table 6] Whisper-large-v3 zero-shot achieves 94.00% WER on test set; after fine-tuning, reduces to 25.19% WER—a ~69 percentage point improvement
  - [corpus] Limited corpus evidence on transfer dynamics; paper does not ablate which pre-training features contribute most to gains
- Break condition: If the pre-training corpus has minimal Arabic dialect coverage or if code-switching patterns differ fundamentally from training distribution, fine-tuning gains may saturate early.

### Mechanism 2
- Claim: End-to-end (E2E) speech translation is advantageous for dialectal languages lacking standardized orthography.
- Mechanism: E2E models map audio directly to target-language text without requiring intermediate transcription, circumventing error propagation from cascaded ASR→MT systems and eliminating dependency on inconsistent orthographic conventions.
- Core assumption: The model can learn implicit acoustic-to-semantic mappings without explicit source-text supervision.
- Evidence anchors:
  - [section 1] "It turns out that this approach is well suited for speech translation from spoken languages characterized by the lack of a standardized orthographic convention, which is the case for multiple low-resourced languages across the world including all Arabic dialects"
  - [section 1] E2E models "generate translation directly from speech in the source language without relying on its transcription as an intermediate representation"
  - [corpus] No direct comparison to cascaded systems in this paper; mechanism inferred from stated rationale, not empirically validated here
- Break condition: If parallel ST training data is extremely limited (<10 hours), cascaded systems with separate ASR and MT may achieve better performance through modular specialization.

### Mechanism 3
- Claim: Explicit annotation guidelines enforcing script-based language boundaries improve model learning on code-switched speech.
- Mechanism: Requiring Arabic script for native words and Roman script for foreign words creates consistent visual cues that help models distinguish language switches during training, particularly for intra-word code-switching where Arabic clitics attach to foreign stems.
- Core assumption: Annotators can consistently apply script-switching rules and the visual distinction translates to learnable model signals.
- Evidence anchors:
  - [section 4.2] "We have chosen to follow the CODA* design principles to develop our annotation guidelines"
  - [section 4.2, Table 2] Examples show underlined non-Tunisian phrases; rule 3 specifies "Arabic clitics and affixes are written in Arabic script, and French or English words are written in Roman script"
  - [corpus] CMI score of 21.50% and 53.70% of segments containing foreign words suggest annotation captures substantial code-switching; no ablation on alternative annotation schemes
- Break condition: If code-switching is primarily phonological rather than lexical (e.g., pronunciation adaptation without loanwords), script-based annotation may miss critical variation.

## Foundational Learning

- **Concept: Self-supervised speech representations (wav2vec 2.0 / XLS-R / w2v-BERT)**
  - Why needed here: The paper uses w2v-BERT-2.0T, XLSR, and MMS as pre-trained encoders. Understanding how these models learn universal acoustic features from unlabeled audio explains why they transfer to unseen dialects.
  - Quick check question: Can you explain why masking spans of audio and predicting quantized tokens teaches phonetically meaningful representations?

- **Concept: CTC (Connectionist Temporal Classification) loss**
  - Why needed here: MMS, XLSR, and w2v-BERT models use CTC heads for transcription. CTC handles the alignment problem between variable-length audio and text without frame-level labels.
  - Quick check question: Why does CTC require a blank token, and what happens to the loss if the same label appears in consecutive frames?

- **Concept: Code-Mixing Index (CMI)**
  - Why needed here: The paper uses CMI to quantify code-switching intensity (21.50% overall, 33.09% in test set). Understanding CMI helps you compare datasets and anticipate model difficulty.
  - Quick check question: If an utterance has 10 words—7 Arabic, 3 English—what is its CMI score?

## Architecture Onboarding

- **Component map:**
  Audio (16kHz WAV) -> Pre-trained Encoder (Whisper / w2v-BERT-2.0T / XLSR / MMS) -> Task Head (CTC or decoder) -> Output (Transcription or Translation)

- **Critical path:**
  1. Data preparation: Download audio from YouTube URLs, align with transcript/translation segments
  2. Tokenization: Whisper uses its subword tokenizer; CTC models require custom character/subword vocabularies
  3. Fine-tuning: 80 epochs with Adam optimizer, learning rate following SpeechBrain recipes
  4. Evaluation: WER/CER for ASR; BLEU (TrueCased, no punctuation) for ST

- **Design tradeoffs:**
  - **Model size vs. data scale:** w2v-BERT-2.0T (590M params) outperforms Whisper-large-v3 (1.5B params) on ASR despite being 3× smaller—suggesting encoder quality matters more than scale for this task
  - **Zero-shot vs. fine-tuning:** Zero-shot Whisper-large-v3 (94% WER) is unusable; fine-tuning is mandatory for this dialect
  - **ASR vs. ST optimization:** The paper reports separate best models for each task; a unified model may sacrifice performance for convenience

- **Failure signatures:**
  - **High WER on Tunisian-only segments:** Table 9 shows 24.16% WER on TUN vs. 13.93% on FOR—model struggles more with dialect than foreign words
  - **Script confusion:** Table 8 shows model predicting Arabic script where reference uses Latin (e.g., "fumoir" → Arabic transliteration)—inconsistent script normalization
  - **NLLB decoder failure:** Encoder-to-NLLB-decoder ST models achieved <5 BLEU; cause unclear, requires investigation

- **First 3 experiments:**
  1. **Reproduce w2v-BERT-2.0T ASR baseline** using provided SpeechBrain recipe; validate 21.37% test WER. This confirms environment correctness.
  2. **Ablate training data size** by fine-tuning on 50%, 25%, 10% of training set to characterize data efficiency curve. Paper does not report this.
  3. **Test script-normalized evaluation** by post-processing predictions to canonicalize script choices (all foreign words → Latin); measure WER impact. This isolates script-confusion errors from genuine recognition errors.

## Open Questions the Paper Calls Out

- **Question:** Can the TEDxTN corpus be effectively utilized for NLP tasks other than Speech Recognition and Translation?
  - **Basis in paper:** [explicit] Section 6 states, "For future work, we plan to... use it for other NLP tasks."
  - **Why unresolved:** The paper only establishes baselines for ASR and AST; the utility of the three-way parallel data (Speech, Tunisian, English) for tasks like summary generation or sentiment analysis remains untested.
  - **What evidence would resolve it:** Benchmarks or baseline results for downstream NLP tasks using the textual components of the TEDxTN dataset.

## Limitations

- **Limited dataset size:** 25 hours of speech limits generalizability of results and may not capture full dialectal variation
- **No cascaded system comparison:** E2E models are claimed to be advantageous for dialectal languages but not empirically compared to cascaded ASR→MT approaches
- **Script normalization issues:** Inconsistent script choices between model predictions and reference transcriptions may artificially inflate WER scores

## Confidence

- **High confidence**: The dataset creation methodology and baseline ASR results are reproducible and well-documented. The improvement from zero-shot to fine-tuned models (69 percentage points WER reduction) is substantial and clearly demonstrated.
- **Medium confidence**: The claim that E2E models are "well suited" for dialectal languages without standardized orthography is supported by theoretical reasoning but lacks empirical comparison to cascaded systems. The script-based annotation guidelines' effectiveness for code-switching is inferred from design rationale rather than ablation studies.
- **Low confidence**: The Speech Translation results (25.68 BLEU) and the failure of encoder-to-NLLB-decoder models lack sufficient diagnostic information to understand root causes or compare against alternative approaches.

## Next Checks

1. Conduct controlled ablation experiments varying script normalization schemes to isolate script-confusion errors from genuine recognition errors, determining whether current WER scores reflect true acoustic modeling performance.
2. Evaluate model performance on regional subsets (TUN vs FOR segments) to quantify dialect-specific versus foreign-word recognition challenges, validating the claim that dialect variation poses the primary difficulty.
3. Implement and compare cascaded ASR→MT pipelines against the reported E2E models to empirically test whether E2E architectures actually outperform modular approaches for this low-resource, code-switched scenario.