---
ver: rpa2
title: 'LIMI: Less is More for Agency'
arxiv_id: '2509.17567'
source_url: https://arxiv.org/abs/2509.17567
tags:
- data
- agentic
- task
- limi
- subtask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LIMI demonstrates that sophisticated agentic AI capabilities can
  emerge from minimal training data through strategic curation rather than dataset
  scale. Using only 78 carefully designed training samples focused on collaborative
  software development and research workflows, LIMI achieves 73.5% performance on
  comprehensive agency benchmarks, dramatically outperforming state-of-the-art models
  trained on orders of magnitude more data: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1
  (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).'
---

# LIMI: Less is More for Agency

## Quick Facts
- **arXiv ID:** 2509.17567
- **Source URL:** https://arxiv.org/abs/2509.17567
- **Reference count:** 40
- **Key result:** LIMI achieves 73.5% agency benchmark performance using only 78 training samples, outperforming models trained on 10,000+ samples

## Executive Summary
LIMI demonstrates that sophisticated agentic AI capabilities can emerge from minimal training data through strategic curation rather than dataset scale. Using only 78 carefully designed training samples focused on collaborative software development and research workflows, LIMI achieves 73.5% performance on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models trained on orders of magnitude more data. This establishes the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations. The approach combines novel agentic query synthesis, systematic trajectory collection capturing complete interaction sequences, and targeted fine-tuning that captures sophisticated reasoning, tool utilization, and collaborative problem-solving patterns.

## Method Summary
LIMI's methodology centers on three core innovations: (1) Agentic Query Synthesis that generates realistic, contextually appropriate queries through an iterative refinement process combining automated generation with human expert review, (2) Systematic Trajectory Collection that captures complete interaction sequences rather than isolated demonstrations, and (3) Targeted Fine-Tuning that focuses on agency-specific behaviors including tool utilization, multi-step reasoning, and collaborative problem-solving. The training data consists of 78 samples specifically curated for collaborative software development and scientific research workflows, representing domains that collectively span the majority of knowledge work scenarios. The approach leverages GPT-5 as a teacher model alongside human PhD annotators to generate high-quality agentic trajectories that capture sophisticated reasoning patterns and tool interactions.

## Key Results
- LIMI achieves 73.5% performance on comprehensive agency benchmarks
- Outperforms Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%)
- Demonstrates 53.7% improvement over models trained on 10,000 samples with 128x fewer samples
- Effectiveness extends across generalization benchmarks spanning tool use, coding, and scientific computing domains

## Why This Works (Mechanism)
LIMI's success stems from the Agency Efficiency Principle, which posits that machine autonomy emerges from strategic curation of high-quality agentic demonstrations rather than data abundance. The methodology captures complete interaction trajectories that include not just final outputs but the reasoning process, tool selection decisions, and collaborative problem-solving patterns. By focusing on domains that represent the majority of knowledge work scenarios, LIMI's training data captures the essential patterns of human-AI collaboration in software development and research contexts. The iterative refinement process for query synthesis ensures that training samples are both realistic and challenging, forcing the model to learn sophisticated reasoning rather than simple pattern matching.

## Foundational Learning

**Agentic Query Synthesis**
- *Why needed:* Traditional data collection methods generate low-quality, unrealistic queries that don't capture the complexity of real-world agency tasks
- *Quick check:* Evaluate synthetic queries against human-generated queries for realism and task complexity

**Systematic Trajectory Collection**
- *Why needed:* Isolated demonstrations miss the temporal dependencies and reasoning patterns essential for agency
- *Quick check:* Verify that collected trajectories include complete interaction sequences from problem understanding to final solution

**Agency Efficiency Principle**
- *Why needed:* Challenges the conventional wisdom that more data always leads to better performance, particularly for specialized capabilities like agency
- *Quick check:* Compare performance curves across different sample sizes to identify the optimal efficiency point

## Architecture Onboarding

**Component Map:** Query Synthesis -> Trajectory Collection -> Fine-tuning Pipeline -> Agency Benchmark Evaluation

**Critical Path:** The fine-tuning stage is most critical, as it transforms the carefully curated trajectories into the model's learned agency capabilities. This stage requires precise hyperparameter tuning to capture the sophisticated reasoning patterns embedded in the training data.

**Design Tradeoffs:** LIMI prioritizes data quality over quantity, accepting the higher cost of expert annotation and teacher model usage to achieve superior performance with minimal samples. This tradeoff sacrifices scalability for effectiveness.

**Failure Signatures:** Poor query synthesis leads to unrealistic training data that doesn't generalize to real-world tasks. Incomplete trajectory collection misses crucial reasoning steps, resulting in models that can't handle complex, multi-step agency tasks. Overfitting to the 78 samples can occur if fine-tuning isn't properly regularized.

**First Experiments:**
1. Validate query synthesis by having human experts rate synthetic queries versus real user queries
2. Test trajectory completeness by having independent reviewers reconstruct the reasoning process from collected demonstrations
3. Evaluate fine-tuning effectiveness by comparing model performance on held-out samples from the training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Agency Efficiency Principle generalize to non-knowledge-work domains, such as embodied robotics or multimodal creative tasks?
- Basis in paper: [explicit] The authors state their approach focuses on "collaborative software development and scientific research workflowsâ€”domains that collectively span the majority of knowledge work scenarios" (Page 2), leaving physical or non-textual agency unaddressed.
- Why unresolved: While the paper validates the principle for coding and research, it remains unclear if "strategic curation" works when the action space involves physical manipulation or continuous control rather than discrete tool calls.
- What evidence would resolve it: Experiments applying LIMI's data curation strategy to robotic manipulation benchmarks or multimodal creative generation tasks.

### Open Question 2
- Question: To what extent is LIMI's performance dependent on the specific reasoning capabilities of the teacher model (GPT-5) used for trajectory collection?
- Basis in paper: [inferred] The methodology specifies that "Four PhD student annotators... work alongside GPT-5 as the agentic model" to generate the 78 training trajectories (Page 6).
- Why unresolved: It is undetermined if the success stems from the curation strategy itself or the high-quality, "expert-level" trajectories that only a frontier model like GPT-5 could produce.
- What evidence would resolve it: An ablation study generating trajectories with weaker or differently-architected models, followed by the same fine-tuning procedure.

### Open Question 3
- Question: What is the theoretical lower bound for sample efficiency before performance collapses?
- Basis in paper: [inferred] The paper demonstrates success with 78 samples but does not test the "floor" of this efficiency principle (Page 8).
- Why unresolved: It is possible that 78 samples is still above the critical mass required for agency emergence; understanding the minimum viable dataset is crucial for the "Less is More" claim.
- What evidence would resolve it: A scaling curve analysis testing performance with drastically smaller datasets (e.g., 10, 20, 40 samples).

## Limitations
- Evaluation lacks external validation on real-world deployment scenarios beyond controlled benchmarks
- Performance comparisons don't control for architectural differences, training duration, or other confounding variables
- Implementation details for systematic collection methodology remain sparse
- Generalizability claims based on benchmark performance rather than field deployment data

## Confidence
**High Confidence:** The core finding that LIMI achieves 73.5% on agency benchmarks compared to 24.1%, 11.9%, 27.5%, and 45.1% for competing models. The statistical comparisons between LIMI and the 10,000-sample baseline (53.7% improvement) are internally consistent and methodologically sound within the benchmark framework.

**Medium Confidence:** The generalization claims across domains. While benchmark results support this, the transfer to actual collaborative software development and research workflows remains unproven without field studies.

**Low Confidence:** The Agency Efficiency Principle as a universal claim. The strategic curation approach shows promise for this specific use case but lacks evidence for broader applicability across different agentic AI domains.

## Next Checks
1. Deploy LIMI in actual collaborative software development environments with human teams over 2-3 month periods to measure real-world performance versus controlled benchmarks
2. Conduct ablation studies varying sample count systematically (e.g., 78, 200, 500, 1000, 10,000) while holding other variables constant to isolate the data efficiency claim
3. Test LIMI on external agency benchmarks not developed by the research team to validate the 73.5% performance claim and assess potential benchmark-specific overfitting