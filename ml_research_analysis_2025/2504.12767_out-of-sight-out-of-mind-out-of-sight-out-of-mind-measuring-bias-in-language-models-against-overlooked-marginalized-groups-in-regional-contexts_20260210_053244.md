---
ver: rpa2
title: 'Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language
  Models Against Overlooked Marginalized Groups in Regional Contexts'
arxiv_id: '2504.12767'
source_url: https://arxiv.org/abs/2504.12767
tags:
- bias
- groups
- arabic
- marginalized
- against
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies offensive stereotyping bias (SOS) in language
  models (LMs) against overlooked marginalized groups across 25 countries, including
  Egypt, Germany, the UK, and the US, using 23 LMs and 270 marginalized groups across
  six sensitive attributes. The authors create datasets in three languages (Modern
  Standard Arabic, Egyptian Arabic, English, and German) to measure SOS bias in MLMs,
  generative models, and instruction-following models.
---

# Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts

## Quick Facts
- **arXiv ID**: 2504.12767
- **Source URL**: https://arxiv.org/abs/2504.12767
- **Reference count**: 40
- **One-line primary result**: Study finds higher SOS bias in low-resource languages and dialects, with intersectional bias particularly severe against non-binary, LGBTQIA+, and Black women.

## Executive Summary
This paper measures offensive stereotyping bias (SOS) in language models against 270 overlooked marginalized groups across 25 countries using three LM types: MLMs, generative models, and instruction-following models. The study creates datasets in English, German, and Arabic (MSA and Egyptian dialect) to evaluate bias using three metrics: a new SOS_MLM metric for MLMs, the HurtLex-based HONEST score for generative models, and Rectified F1 for instruction-following models. Results show significantly higher bias in low-resource languages and dialects, particularly against intersectional identities, highlighting the need for better bias evaluation metrics for generative models in low-resource languages.

## Method Summary
The study constructs datasets by filling 74 toxic/non-toxic templates with identity terms collected from Minority Rights Group International, UNHCR, and other sources. Three evaluation approaches are used: for MLMs, the SOS_MLM metric calculates pseudo-log-likelihood differences between toxic and non-toxic sentences; for generative models, the HONEST score measures probability of generating HurtLex words in sentence completions; for instruction-following models, the Rectified F1 score penalizes standard F1 by hallucination rate. The analysis covers 23 models across English, German, and Arabic (MSA and Egyptian dialect) for 270 marginalized versus 60 dominant groups.

## Key Results
- SOS bias is significantly higher in low-resource languages and dialects (Arabic/Egyptian) compared to high-resource languages (English/German)
- Intersectional bias is particularly severe against non-binary, LGBTQIA+, and Black women identities
- Arabic MLMs show high SOS bias against both marginalized and dominant groups, unlike other language models
- Instruction-following models exhibit higher hallucination rates when processing Egyptian Arabic compared to MSA
- Current bias evaluation metrics have significant limitations, especially for generative models in low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrasting pseudo-log-likelihoods between toxic and non-toxic sentence templates reveals offensive stereotyping bias in MLMs.
- **Mechanism**: The SOS_MLM metric calculates the probability P(U|M, θ) for toxic versus non-toxic sentence variants. If the model consistently assigns higher likelihood to toxic versions containing specific identities, it indicates learned associations between those identities and pejorative language.
- **Core assumption**: The syntactic structure of templates isolates identity terms as the primary variable influencing probability scores.
- **Evidence anchors**: [Section 4.3] describes the probability estimation method; [Corpus] lacks specific evidence for this contrastive method.

### Mechanism 2
- **Claim**: Pre-training on translated Western news sources propagates negative stereotypes of dominant groups into Arabic language models.
- **Mechanism**: High SOS bias in Arabic MLMs against both marginalized and dominant groups (e.g., "Sunni", "Arab") is attributed to training on translations of Western media that historically frame Arabs/Muslims negatively.
- **Core assumption**: Negative associations stem from translated corpora rather than local language use patterns.
- **Evidence anchors**: [Section 5.2] hypothesizes this translation-artifact mechanism; [Corpus] provides weak support focused on general social bias.

### Mechanism 3
- **Claim**: Dialectal resource gaps exacerbate bias measurement and model performance degradation in low-resource languages.
- **Mechanism**: Higher hallucination rates in instruction-following models and higher SOS bias scores in MLMs for Egyptian dialect versus MSA indicate insufficient dialectal representation, leading to out-of-distribution failures.
- **Core assumption**: Higher bias scores reflect true representation quality failures rather than measurement artifacts.
- **Evidence anchors**: [Abstract] notes significant bias differences between Egyptian Arabic and MSA; [Section 5.1] speculates about dialectal data collection issues.

## Foundational Learning

- **Concept**: Socio-technical Bias (vs. Data Bias)
  - **Why needed here**: The paper defines SOS bias as reflecting structural inequalities in training data that result in harmful model behaviors, moving beyond simple data imbalance.
  - **Quick check question**: Does a high SOS score imply the training data had more toxic words, or that the model learned a structural association between an identity and toxicity?

- **Concept**: The HONEST Metric
  - **Why needed here**: This metric measures the probability of model completions containing words from the HurtLex lexicon for generative models.
  - **Quick check question**: Why would a low HONEST score in a low-resource language (like Arabic) be misleading regarding the model's actual safety?

- **Concept**: Rectified F1 Score
  - **Why needed here**: This metric penalizes standard F1 based on hallucination rate, which is critical for evaluating instruction-following models on low-resource languages.
  - **Quick check question**: If a model answers "I don't know" to every prompt, what would the Rectified F1 score approach?

## Architecture Onboarding

- **Component map**: 270 Identity Terms (Marginalized/Dominant) → Template Instantiation (SOS Dataset / HONEST) → 23 LMs (MLMs, IFMs, Generative) → SOS_MLM metric / HurtLex matching / Rectified F1

- **Critical path**: 
  1. Accurately defining "marginalized" vs. "dominant" per region is prerequisite for all analysis
  2. Converting identities into toxic/non-toxic templates
  3. Running specific log-likelihood (MLM) or generation (Gen) tasks

- **Design tradeoffs**:
  - SOS dataset uses fixed templates (high control, lower naturalness) vs. HONEST allows open completion (higher naturalness, but relies on incomplete lexicons)
  - Multilingual models offer coverage but showed significantly higher hallucination rates in Arabic dialects compared to high-resource languages

- **Failure signatures**:
  - The "Low Score" Trap: Low HONEST scores in Arabic/German may indicate HurtLex lexicon gaps rather than low bias
  - The "Translation" Effect: High bias against dominant identities (e.g., "Arab" in Arabic models) signals training on translated Western data
  - Hallucination Spikes: Sudden increase in non-Yes/No answers indicates instruction language is out-of-distribution

- **First 3 experiments**:
  1. Run SOS_MLM metric on target MLM using both MSA and Egyptian Arabic templates to quantify performance gap
  2. Generate 100 completions for sensitive identity in low-resource language and manually check if HONEST metric misses toxic outputs due to missing HurtLex entries
  3. Prompt IFM with classification instructions in English vs. low-resource dialect and measure drop in Rectified F1

## Open Questions the Paper Calls Out

- **Question**: How does offensive stereotyping bias against overlooked marginalized groups propagate to specific downstream NLP tasks?
  - **Basis in paper**: [explicit] Authors state studying discrimination in downstream tasks is future work
  - **Why unresolved**: Study focused on intrinsic model bias rather than impact on application-level performance
  - **What evidence would resolve it**: Evaluating LM performance on downstream tasks using the created dataset of 270 overlooked marginalized groups

- **Question**: How can bias evaluation metrics be developed for low-resource languages to overcome HurtLex limitations?
  - **Basis in paper**: [inferred] HONEST scores for Arabic were artificially low due to HurtLex lexicon gaps
  - **Why unresolved**: Current metrics rely on lexical matching that fails to capture full spectrum of offensive language in low-resource languages
  - **What evidence would resolve it**: Creating a new metric or expanded lexicon for Arabic dialects showing higher correlation with human toxicity judgments

- **Question**: Does training on authentic local data rather than translated Western media eliminate bias against dominant groups in Arabic LMs?
  - **Basis in paper**: [inferred] Authors speculate high bias against "Sunni" and "Arab" stems from translated Western media training data
  - **Why unresolved**: Paper identifies correlation and hypothesizes cause but lacks direct experimental evidence
  - **What evidence would resolve it**: Comparative analysis of Arabic LMs trained on local media versus translated Western corpora

## Limitations

- Metric limitations: SOS_MLM assumes templates effectively isolate identity terms; HurtLex lexicon coverage gaps may underestimate bias in Arabic and German
- Resource disparity uncertainty: Higher bias scores for Arabic dialects may reflect genuine gaps or measurement artifacts
- Identity classification: Distinction between "marginalized" and "dominant" relies on external sources that may not capture local dynamics
- Model version uncertainty: Uses both open and closed models without specifying exact versions or training dates

## Confidence

**High Confidence**: Finding that intersectional bias is particularly severe against non-binary, LGBTQIA+, and Black women identities
**Medium Confidence**: Observation that low-resource languages show higher bias scores, though specific causes remain uncertain
**Medium Confidence**: Hypothesis that Arabic model bias against dominant groups stems from translated Western media training data
**Low Confidence**: Specific numerical comparisons between language pairs should be interpreted cautiously due to metric limitations

## Next Checks

1. **Lexicon Coverage Validation**: Generate 100 random completions for marginalized identities in Arabic using a generative model, manually code toxic outputs not captured by HurtLex, and compare missed toxic proportion to HONEST score to estimate measurement gap

2. **Translation Artifact Test**: Obtain training corpus statistics for Arabic MLMs and analyze proportion of translated versus native Arabic content, then correlate with SOS bias scores against dominant groups to test translation hypothesis

3. **Dialect Template Naturalness**: Conduct human evaluation where native Egyptian Arabic speakers rate naturalness of SOS templates versus MSA templates, then compare ratings with measured bias scores to determine if template quality differences explain performance gap