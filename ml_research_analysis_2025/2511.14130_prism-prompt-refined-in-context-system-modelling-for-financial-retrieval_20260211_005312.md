---
ver: rpa2
title: 'PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval'
arxiv_id: '2511.14130'
source_url: https://arxiv.org/abs/2511.14130
tags:
- prompt
- chunk
- financial
- document
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRISM is a training-free framework for financial information retrieval
  that integrates refined system prompting, in-context learning (ICL), and a lightweight
  multi-agent system to rank documents and text chunks. Prompt engineering enhances
  reasoning quality through structured, domain-informed instructions, while ICL augments
  the system with semantically relevant few-shot examples retrieved via embedding-based
  search.
---

# PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval

## Quick Facts
- arXiv ID: 2511.14130
- Source URL: https://arxiv.org/abs/2511.14130
- Reference count: 40
- Key outcome: Non-agentic configuration using P4 prompt and document-level ICL achieves NDCG@5 of 0.71818 on restricted validation split

## Executive Summary
PRISM is a training-free framework for financial information retrieval that combines refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. The system uses domain-informed prompt engineering with structured reasoning scaffolds to improve LLM ranking accuracy, while ICL provides semantically relevant few-shot examples through embedding-based search. Experiments demonstrate that the non-agentic configuration outperforms multi-agent approaches, achieving high reproducibility with low variance and efficient production-scale deployment potential.

## Method Summary
PRISM implements a two-stage financial information retrieval system where documents are ranked across five SEC document types before chunks within the top document are ranked. The framework uses P4 prompt templates with Chain-of-Thought, ReAct, and Tree-of-Thought reasoning scaffolds, FAISS vector store with text-embedding-3 for 5-shot ICL retrieval, and LangGraph-based multi-agent decomposition. The best configuration uses non-agentic workflow with document-level ICL only, avoiding context overload from chunk-level examples. Models evaluated include GPT-4o-mini, GPT-4.1, GPT-5-mini, and GPT-5.

## Key Results
- Non-agentic configuration achieves NDCG@5 of 0.71818 on restricted validation split
- Document-level ICL improves performance by +0.036 compared to no ICL
- Multi-agent system underperforms non-agentic approach by 0.05-0.08 NDCG
- Narrow confidence intervals (0.0015-0.0027) indicate high reproducibility

## Why This Works (Mechanism)

### Mechanism 1: Domain-informed prompt engineering
Structured reasoning scaffolds in P4 prompt improve LLM ranking accuracy by forcing explicit relevance reasoning. Domain priors provide corpus-informed guidance that constrains search space. Performance degrades when prompts become overly verbose or contain conflicting instructions.

### Mechanism 2: Document-level in-context learning
Semantically-scoped ICL at document level only enhances retrieval by providing task-relevant exemplars without fragmenting attention. FAISS + text-embedding-3 retrieves top-k semantically similar query-document pairs. Performance degrades with too many examples or semantically distant exemplars.

### Mechanism 3: Multi-agent decomposition trade-offs
Multi-agent decomposition introduces coordination overhead that outweighs theoretical benefits of role specialization in this domain. Sequential agent dependencies cause error propagation, and prompt-architecture incompatibility disrupts inter-agent communication. Specialized agents underperform monolithic reasoning when agent graph depth exceeds model capacity.

## Foundational Learning

### Concept 1: NDCG@5
Why needed: Sole evaluation metric for interpreting all ablation results and understanding ranking order importance.
Quick check: If 5 relevant items are ranked in positions 3-7 instead of 1-5, will NDCG@5 be higher or lower? Why?

### Concept 2: In-Context Learning vs. Fine-Tuning
Why needed: PRISM's "training-free" value proposition depends on understanding how ICL provides task adaptation without gradient updates.
Quick check: If ICL examples have incorrect label mappings but correct input distribution and output format, will performance degrade significantly?

### Concept 3: Listwise Ranking vs. Pointwise/Pairwise
Why needed: PRISM uses listwise ranking, and understanding trade-offs explains prompt design choices and token efficiency.
Quick check: Why might listwise ranking struggle with 100+ candidate chunks, and how does PRISM's chunking/filtering strategy address this?

## Architecture Onboarding

### Component Map:
Query → Document ranking (P4 prompt + optional ICL) → Top document chunks → Chunk ranking (P4 prompt) → Top-k ranked chunks

### Critical Path:
1. Query → Document ranking (8-10s latency, 2-3K tokens)
2. Top document → Chunk extraction
3. Chunks → Chunk ranking (130-160s latency, ~100K tokens)
4. Return top-k ranked chunks

### Design Tradeoffs:

| Decision | Option A | Option B | Winner | Reason |
|----------|----------|----------|--------|--------|
| Workflow | Non-agentic | Multi-agent | Non-agentic | +0.05-0.08 NDCG, no coordination overhead |
| ICL Scope | Document-only | Document + Chunk | Document-only | Avoids context overload, +0.036 NDCG |
| Model | GPT-5-mini | GPT-5 | GPT-5 (if budget allows) | +0.014 NDCG but 3x cost |
| Prompt | P1 (simple) | P4 (structured) | P4 | +0.02-0.03 NDCG from reasoning scaffolds |
| ICL Shots | 5 examples | 10-15 examples | 5 | More shots cause degradation |

### Failure Signatures:
1. MAS + Complex Prompt: P4 prompt in agentic workflow → 0.55757. Constraint-heavy prompts disrupt agent communication.
2. Dual-Level ICL: ICL at both document and chunk levels → context overload, performance drop.
3. Small Model + Deep Graph: GPT-4o-mini in A2 architecture → 0.51518. Error propagation exceeds model's reasoning capacity.

### First 3 Experiments:
1. Establish baseline: Run P4 prompt + GPT-5-mini, non-agentic, no ICL. Expected: ~0.688-0.690 NDCG@5.
2. Validate ICL gain: Add document-level ICL-5/TE3-S. Expected: ~0.02-0.03 improvement.
3. Characterize MAS failure: Replicate Run 28 (P1 + GPT-5-mini + A4 + 5-shot ICL). Expected: ~0.627 NDCG, confirming agentic underperformance.

## Open Questions the Paper Calls Out

### Open Question 1
Can adaptive, LLM-assisted chunking strategies improve ranking performance compared to fixed segmentation? The limitations section proposes an "adaptive, LLM-assisted chunking strategy" where boundaries are determined by content density rather than fixed lengths. This is unresolved because current non-agentic workflows use fixed splits which may not preserve contextual coherence for information-dense financial chunks.

### Open Question 2
Does hybrid retrieval integrating lexical methods (BM25) with dense embeddings outperform the current L2-distance approach? The authors identify reliance on simple L2-distance lookup as a limitation and suggest integrating BM25 via Reciprocal Rank Fusion. This is unresolved because purely dense methods may overlook domain-specific financial terminology that lexical methods capture effectively.

### Open Question 3
Can expressive aggregation mechanisms like Tree-of-Thought (ToT) consensus overcome the performance saturation bottleneck in Multi-Agent Systems? The paper hypothesizes that "Tree-of-Thought consensus or probabilistic voting" may allow larger models to leverage MAS better than current topologies. This is unresolved because current MAS designs exhibited a saturation point that limited performance gains from superior models like GPT-5.

## Limitations

- Multi-agent system consistently underperforms non-agentic baseline, suggesting architectural assumptions about role specialization may not translate to financial retrieval
- P4 prompt's complex reasoning scaffolds that benefit non-agentic workflows appear to conflict with agent communication protocols
- Single validation split and absence of cross-validation limits generalizability claims despite narrow confidence intervals

## Confidence

- **High Confidence:** Non-agentic workflow superiority over multi-agent system (consistently demonstrated across 10+ ablation runs with clear performance gaps)
- **Medium Confidence:** P4 prompt superiority and document-only ICL benefits (supported by ablation results but dependent on specific FinAgentBench dataset characteristics)
- **Medium Confidence:** NDCG@5 as appropriate metric for financial retrieval (standard in information retrieval but may not capture domain-specific nuances)
- **Low Confidence:** Production efficiency claims (based on single-run timings without load testing or cost modeling under realistic query distributions)

## Next Checks

1. **Cross-validation replication:** Test PRISM performance across multiple FinAgentBench validation splits to verify that the 0.71818 NDCG@5 result is not an artifact of a particularly favorable validation subset.

2. **Prompt-architecture interaction analysis:** Systematically vary P4 prompt complexity in both non-agentic and MAS workflows to isolate whether prompt verbosity or specific reasoning scaffold components cause MAS degradation.

3. **Embedding retrieval quality audit:** Measure semantic similarity between retrieved ICL examples and target queries using multiple embedding models to validate that FAISS retrieval is capturing relevant exemplars rather than superficial keyword matches.