---
ver: rpa2
title: 'Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic
  Analysis and Proximal Algorithms'
arxiv_id: '2511.21115'
source_url: https://arxiv.org/abs/2511.21115
tags:
- function
- holds
- definition
- theorem
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of estimating partial linear models
  (PLMs) using deep neural networks (DNNs) under least absolute deviation (LAD) regression.
  The key innovation is to parametrize the nonparametric component of PLMs with sparse
  DNNs and to introduce a nonconvex, nonsmooth penalty for robustness.
---

# Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms

## Quick Facts
- **arXiv ID**: 2511.21115
- **Source URL**: https://arxiv.org/abs/2511.21115
- **Reference count**: 40
- **Key outcome**: This paper tackles the challenge of estimating partial linear models (PLMs) using deep neural networks (DNNs) under least absolute deviation (LAD) regression. The key innovation is to parametrize the nonparametric component of PLMs with sparse DNNs and to introduce a nonconvex, nonsmooth penalty for robustness. The authors establish consistency, convergence rates, and asymptotic normality of the proposed estimator, while addressing the additional complexities introduced by network expansion, nonsmooth penalties, and the need for infinite-dimensional variational analysis. They further analyze the oracle problem and its continuous relaxation, deriving closed-form proximal updates for the relaxed formulation. This leads to a significant computational advantage, trading off statistical accuracy for tractability. Finally, they prove global convergence of a proximal stochastic subgradient method for both formulations, using tools from differential geometry and optimization theory.

## Executive Summary
This paper addresses the estimation of partial linear models (PLMs) with nonparametric components represented by deep neural networks (DNNs), using least absolute deviation (LAD) regression for robustness. The authors propose a penalized LAD estimator with nonconvex, nonsmooth regularization to achieve both sparsity and smoothness in the DNN representation. They establish theoretical guarantees including consistency, convergence rates, and asymptotic normality, while also analyzing the computational tractability of the problem through continuous relaxation and proximal algorithms. The work bridges statistical theory and computational optimization, providing a comprehensive framework for robust, scalable PLM estimation with DNNs.

## Method Summary
The method estimates partial linear models Y = β₀ᵀX + g₀(Z) + ε using penalized LAD regression, where the nonparametric component g₀ is approximated by a sparse DNN that expands with sample size N. The objective is to minimize (1/N)Σ|Yᵢ − βᵀXᵢ − g(Zᵢ)| + λₙJₙ,ₘ(β, g), where Jₙ,ₘ is a nonconvex, nonsmooth penalty. The authors analyze two proximal stochastic subgradient methods: one for a continuous relaxation of the ℓ₀ penalty using a smooth approximation, and another for the exact ℓ₀ formulation. The DNN architecture expands with N (depth L=O(log N), width qₖ≲N, sparsity s=O(Nrₙ²log N)), and ReLU activation is used. The method provides theoretical guarantees for consistency, convergence rates, and asymptotic normality, while also establishing convergence of the optimization algorithms to Clarke-critical points.

## Key Results
- Establishes consistency and Oₚ(rₙlog²N + λₙ) convergence rate for the penalized LAD estimator, where rₙ is the DNN approximation rate
- Proves asymptotic normality √N(β̂ₙ − β₀) ⇝ N(0, Σ₂⁻¹Σ₁Σ₂⁻¹) under Lipschitz error assumptions
- Analyzes two proximal stochastic subgradient methods, proving convergence to Clarke-critical points for both exact ℓ₀ and continuous relaxation formulations
- Derives closed-form proximal updates for the relaxed formulation, achieving O(H) computational cost versus O(H log H) for exact ℓ₀ projection
- Establishes epi-convergence of continuous approximations to the original problem, ensuring statistical fidelity of the relaxation

## Why This Works (Mechanism)

### Mechanism 1: LAD Robustness via Median-Unbiased Estimation
LAD regression provides robustness to outliers and heavy-tailed error distributions compared to least squares estimation. The absolute value loss |Y - β^T X - g(Z)| grows linearly with residual magnitude rather than quadratically, limiting the influence of extreme observations. Combined with the assumption P(ε ≤ 0) = 1/2, this yields median-unbiased estimation. The core assumption is that the error distribution has bounded conditional density f_ε(·|v) with 1/c_0 < f_ε(t|v) < c_0 for small t, ensuring the loss landscape has sufficient curvature near the optimum.

### Mechanism 2: DNN Approximation via Expanding Sparse Architecture
Sparse DNNs with ReLU activation can approximate Hölder-smooth functions g_0 at rate r_N ≈ N^{-ζ} where ζ depends on intrinsic dimension and smoothness. The network architecture M(s, L, q, A) expands with sample size: depth L = O(log N), sparsity s = O(N r_N² log N), width q_k ≲ N. This controls the covering number log N(ε, M_C^N, ||·||_∞) ≲ (s+1) log(H/ε), enabling empirical process convergence despite growing parameter space. The core assumption is that the true function g_0 is composite Hölder smooth with intrinsic dimension d̄_k less than effective smoothness 2γ̄_k.

### Mechanism 3: Proximal Subgradient Convergence via Definable Structure
Proximal stochastic subgradient methods converge to Clarke-critical points for both the exact ℓ₀ formulation and its continuous relaxation. The objective functions are definable in an o-minimal structure, which implies the Weak Sard Property and allows the use of differential geometric tools. Combined with the chain rule for locally Lipschitz definable functions, the Lyapunov framework guarantees cluster points satisfy 0 ∈ ∂^C f(x) + N_X(x). The core assumption is that the regularization term is lower semi-continuous, separable, and sequentially normally epi-compact.

## Foundational Learning

- **Concept: Subdifferentials and Clarke Subgradients**
  - **Why needed here**: The LAD loss |·| is nonsmooth at zero; the regularization may be nonconvex. Classical gradients don't exist, so you need Mordukhovich/Clarke subdifferentials to derive optimality conditions and prove convergence.
  - **Quick check question**: For f(x) = |x| at x = 0, what is ∂^C f(0)? (Answer: [-1, 1])

- **Concept: Covering Numbers and Empirical Processes**
  - **Why needed here**: The DNN architecture grows with N, so the function class F_N has covering number approaching infinity. You need entropy bounds to prove uniform convergence of empirical losses and establish consistency.
  - **Quick check question**: If log N(ε, F, L²) ≲ s log(1/ε), how does the covering entropy integral J[](δ) behave? (Answer: J[](δ) ≲ δ√(s log(1/δ)))

- **Concept: Epi-Convergence and Variational Analysis**
  - **Why needed here**: The continuous approximation L_{N,σ_k} epi-converges to the original problem L_N. This ensures minimizers of approximations converge to minimizers of the target, bridging computational tractability and statistical fidelity.
  - **Quick check question**: What does epi-convergence of f_k → f guarantee about lim_k(argmin f_k)? (Answer: limsup_k(argmin f_k) ⊆ argmin f)

## Architecture Onboarding

- **Component map**: Data layer (X ∈ R^d, Z ∈ R^l, Y) -> Linear head (β^T X) -> DNN nonparametric branch (g(W; Z) ∈ M(s, L, q, A)) -> Loss layer (LAD loss + penalty) -> Optimizer (proximal stochastic subgradient)

- **Critical path**:
  1. Initialize (β_0, W_0) within bounds; warm-start g-branch via pretraining on residuals from β-only fit
  2. Set hyperparameters: λ_N = o(1), σ (relaxation sharpness), {γ_k} (sparsity weights), {α_k} (step sizes satisfying ∑α_k = ∞, ∑α_k² < ∞)
  3. Per iteration: sample mini-batch, compute subgradient of |·| via sign*(·), apply proximal update (Algorithm 49 or 51)
  4. Monitor: objective value convergence, sparsity level ∑||W_k||_0, validation LAD loss

- **Design tradeoffs**:
  - **Continuous approximation vs. Exact ℓ₀**: Relaxed formulation has O(H) proximal cost (box projection); exact requires O(H log H) sorting for top-s selection. Trade statistical fidelity for computational speed.
  - **Network expansion rate**: Faster growth (larger s, q_k) improves approximation r_N but increases covering entropy, requiring more samples. Assumption A4 specifies the admissible range.
  - **Regularization strength λ_N**: Larger λ_N improves sparsity/smoothness but adds bias; must satisfy λ_N(||∂_g J_{N,2}||* + ||∂_β J_{N,1}||) = o_p(1/√N).

- **Failure signatures**:
  - **Non-degenerate β variance**: If Σ_2 = E[f_ε(0|V){X - φ*(Z)}{X - φ*(Z)}^T] is ill-conditioned, asymptotic normality fails; check condition number during fitting
  - **Diverging covering number**: If width/sparsity grows faster than Assumption A4, empirical process term dominates and consistency breaks; monitor log(covering number) / √N
  - **Stuck at non-critical point**: If stochastic gradients are biased or definability fails, subgradient method may not converge; verify unbiasedness E[ζ|β, W] ∈ ∂^C G

- **First 3 experiments**:
  1. **Sanity check on synthetic PLM**: Generate Y = β_0^T X + sin(2πZ_1) + ε with ε ~ Laplace(0, 1); fit with N = 5000, L = 3, s = 50; verify ||β̂ - β_0|| = O(N^{-1/2}) and ||ĝ - g_0||_{L2} ≈ N^{-1/3} (Hölder γ = 1)
  2. **Ablation: LAD vs. LSE under contamination**: Add 5% outliers (ε ~ Cauchy); compare β̂_LAD vs. β̂_LSE bias and variance; expect LAD robustness to manifest as lower bias
  3. **Exact vs. relaxed tradeoff curve**: For fixed N, vary σ ∈ {0.01, 0.1, 1.0}; plot (computation time, validation LAD loss, sparsity); expect relaxed to be 5-10× faster with <5% loss degradation for moderate σ

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do the iterates produced by the proximal stochastic subgradient method converge to a global minimum or a local minimum that retains the asymptotic normality properties established for the ideal estimator?
- **Basis in paper**: [inferred] The paper establishes asymptotic normality for the global minimizer $\hat{\beta}_N$ (Theorem 3) but proves algorithmic convergence only to a Clarke stationary point (Theorem 5 and 6).
- **Why unresolved**: There is no guarantee that the Clarke stationary points found by the optimization algorithms correspond to the global minimizers required for the statistical guarantees.
- **What evidence would resolve it**: A proof showing that the LAD-DNN objective satisfies the strict saddle property or that all local minima are global within the defined function class.

### Open Question 2
- **Question**: Can the specific "mild assumptions" referenced in Remark 3 be explicitly verified for the penalized LAD-DNN objective to guarantee convergence to a local minimum?
- **Basis in paper**: [explicit] Remark 3 states that under "additional mild assumptions," the algorithm escapes strict saddles, referring the reader to [37] for details without verifying them for the specific LAD-DNN context.
- **Why unresolved**: The paper defers the verification of these conditions, leaving the gap between theoretical convergence to critical points and the stronger result of convergence to local minima unbridged.
- **What evidence would resolve it**: Explicit verification that the penalized LAD objective function satisfies the required definability or geometrical conditions cited in the reference [37].

### Open Question 3
- **Question**: Can the asymptotic analysis be extended to Partial Linear Models with dependent data (e.g., time series or spatial data)?
- **Basis in paper**: [inferred] The paper assumes i.i.d. observations in Eq (1), despite citing literature on dependent data PLMs (e.g., Brown 2024) in the introduction as motivation.
- **Why unresolved**: The covering number arguments and empirical process theory used to prove consistency (Theorem 1) rely heavily on the independence of samples.
- **What evidence would resolve it**: Deriving convergence rates and asymptotic normality under mixing conditions (e.g., β-mixing) or specific dependence structures.

## Limitations
- The theoretical results are limited to specific settings: sub-Gaussian design for consistency and Lipschitz errors for asymptotic normality, which may not cover all practical scenarios.
- The computational advantages of the continuous relaxation come at the cost of statistical accuracy, and the tradeoff may not be optimal for all datasets.
- The analysis relies heavily on the DNN's ability to expand appropriately with sample size, which may be challenging to implement in practice and could lead to overfitting if not carefully controlled.
- The convergence guarantees for the proximal stochastic subgradient method depend on strict assumptions about the sparsity pattern and the learning rate schedule, which may be difficult to verify empirically.

## Confidence
- **High confidence**: The theoretical consistency and convergence rate results under sub-Gaussian design and the Hölder smoothness assumptions for the DNN approximation. The proofs are rigorous and follow established techniques in empirical process theory and nonparametric estimation.
- **Medium confidence**: The asymptotic normality of the estimator under Lipschitz errors and the effectiveness of the continuous relaxation in practice. While the theoretical foundations are solid, empirical validation in diverse settings is limited.
- **Low confidence**: The practical implementation of the exact ℓ₀ projection and the definability assumptions for the proximal stochastic subgradient method. These claims are more theoretical and may not directly translate to efficient algorithms without further refinement.

## Next Checks
1. **Empirical evaluation of continuous vs. exact ℓ₀**: Implement both formulations on a high-dimensional dataset with known sparsity structure (e.g., genomic data). Measure the tradeoff between computational efficiency and statistical accuracy, focusing on the relaxation parameter σ and its impact on the sparsity pattern.
2. **Robustness to non-standard error distributions**: Test the LAD estimator on datasets with heavy-tailed or skewed error distributions (e.g., income data). Compare its performance to least squares estimation in terms of bias and variance, particularly in the presence of outliers.
3. **Scalability of DNN expansion**: Apply the method to a large-scale dataset (e.g., image data) with a high-dimensional Z. Monitor the growth of the network architecture (depth, width, sparsity) and its impact on the covering number and convergence rate. Verify that the expansion remains within the bounds specified by Assumption A4.