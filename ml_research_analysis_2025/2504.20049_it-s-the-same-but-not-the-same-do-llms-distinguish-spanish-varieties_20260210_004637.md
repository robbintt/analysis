---
ver: rpa2
title: 'It''s the same but not the same: Do LLMs distinguish Spanish varieties?'
arxiv_id: '2504.20049'
source_url: https://arxiv.org/abs/2504.20049
tags:
- modelos
- xico
- espa
- morfosintaxis
- para
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates nine large language models (LLMs) on their
  ability to identify seven Spanish varieties (Andean, Antillean, Continental Caribbean,
  Chilean, Peninsular, Mexican and Central American, and Rioplatense) using a multiple-choice
  test with 30 questions. The results show that Peninsular Spanish is best recognized
  by all models, while others struggle, particularly with lexical variation.
---

# It's the same but not the same: Do LLMs distinguish Spanish varieties?

## Quick Facts
- **arXiv ID**: 2504.20049
- **Source URL**: https://arxiv.org/abs/2504.20049
- **Reference count**: 0
- **Primary result**: Nine LLMs evaluated on 7 Spanish varieties; all models perform best on Peninsular Spanish, with GPT-4o as the only model achieving high scores across multiple varieties, revealing strong training bias toward Peninsular Spanish.

## Executive Summary
This study evaluates nine large language models on their ability to identify seven Spanish varieties (Andean, Antillean, Continental Caribbean, Chilean, Peninsular, Mexican and Central American, and Rioplatense) using a multiple-choice test with 30 questions. The results show that Peninsular Spanish is best recognized by all models, while others struggle, particularly with lexical variation. GPT-4o outperforms all other models, being the only one to achieve high scores across multiple varieties. The findings reveal a strong bias toward Peninsular Spanish in LLM training data, highlighting the need for more representative models to address linguistic inequality and digital linguistic bias.

## Method Summary
The paper evaluates 9 LLMs using a 30-question multiple-choice test (20 morphosyntactic, 10 lexical) across 7 Spanish varieties. Models are prompted with regional identity roles ("Eres de [región]...") and select answers via log-probability maximization at temperature=0. Scoring uses a weighted penalty formula based on question difficulty. The test is validated against Spanish corpora (CORPES XXI, CREA, VARILEX) and covers 7 varieties defined by dialectology literature. Models include GPT-4o, GPT-4o-mini, and several open models (Gemma, Mistral, Llama variants, Occiglot, Yi).

## Key Results
- All models perform best on Peninsular Spanish, indicating training data bias
- GPT-4o is the only model to achieve high scores across multiple varieties
- Lexical questions show significantly higher error rates than morphosyntactic ones, especially in smaller models
- Strong correlation (0.94) between CEREAL corpus representation and model performance
- Mexican and Central American variety shows consistently low recognition across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model performance on Spanish varieties correlates with training corpus representation
- Mechanism: Training data composition shapes dialectal competence. When corpora over-represent specific varieties (e.g., Peninsular Spanish from National Library of Spain sources), models develop disproportionate sensitivity to those dialectal features while under-learning others.
- Core assumption: The statistical relationship between corpus frequency and model scores reflects causal training dynamics rather than coincidental correlation.
- Evidence anchors:
  - [abstract]: "The Peninsular Spanish variety is the best identified by all models, suggesting a training bias."
  - [section 4]: "Se observa una fuerte relación que se plasma en un alto coeficiente de correlación de 0.94 entre las dos variables" (comparing scores vs. CEREAL word counts)
  - [corpus]: Related work "La Leaderboard" addresses Spanish variety evaluation but corpus lacks direct validation of the training-corpus mechanism
- Break condition: If post-training alignment (RLHF, instruction tuning) disproportionately emphasizes certain varieties, the correlation would reflect alignment rather than pretraining composition.

### Mechanism 2
- Claim: Morphosyntactic features are more reliably acquired than lexical ones in smaller models
- Mechanism: Lexical variation requires memorizing specific word-region mappings with lower statistical regularity, while morphosyntactic patterns (e.g., voseo verb forms) offer more predictable structural cues. Smaller capacity models prioritize higher-frequency structural patterns.
- Core assumption: The error pattern asymmetry (lexical > morphosyntactic errors) reflects differential learnability rather than test design artifact.
- Evidence anchors:
  - [abstract]: Notes GPT-4o as only model capable of recognizing variability
  - [section 4]: "los errores más frecuentes se dieron fundamentalmente en las preguntas de léxico, especialmente en modelos con menor capacidad"
  - [corpus]: Limited direct validation in neighbors; mechanism remains model-internal hypothesis
- Break condition: If lexical questions in the test were systematically more difficult by design (lower baseline probability), the asymmetry would reflect test construction rather than model learning dynamics.

### Mechanism 3
- Claim: Prompting with regional identity alone is insufficient to activate dialectal competence
- Mechanism: Role-based prompting ("Eres de Centroamérica...") triggers surface-level style adjustments but cannot retrieve unlearned dialectal patterns. Models cannot generate what their weights do not encode.
- Core assumption: The prompting strategy effectively communicates regional context but fails due to knowledge gaps, not prompt insensitivity.
- Evidence anchors:
  - [section 2]: Describes prompting protocol with regional role assignment
  - [section 3]: Documents systematic failures despite clear role prompts (e.g., 100% of models failed Q25 on Andean lexical item "habas")
  - [corpus]: Neighbor "Using Contextually Aligned Online Reviews" addresses variety evaluation but doesn't test prompting mechanisms
- Break condition: If alternative prompting strategies (few-shot examples, chain-of-thought) significantly improve scores, the barrier would be prompt engineering rather than knowledge representation.

## Foundational Learning

- Concept: **Diatopic variation**
  - Why needed here: The paper evaluates whether models distinguish regional linguistic differences; understanding that Spanish has systematic variation across geography is prerequisite to interpreting results.
  - Quick check question: Can you explain why "levantarse" vs. "pararse" for "to stand up" is a dialectal rather than individual choice?

- Concept: **Training corpus composition bias**
  - Why needed here: The paper attributes performance gaps to imbalanced training data; understanding how corpora like CEREAL or OSCAR are collected explains why Peninsular Spanish dominates.
  - Quick check question: Why would web-scraped Spanish corpora over-represent Spain-based sources even though Mexico has more Spanish speakers?

- Concept: **Log-probability as model confidence**
  - Why needed here: The methodology uses log-probabilities to select model responses; understanding this evaluation approach is necessary to interpret the scoring system.
  - Quick check question: Why might log-probability selection at temperature=0 be more reliable for evaluation than sampling-based generation?

## Architecture Onboarding

- Component map:
  - **Evaluation harness**: 30-question multiple-choice test (20 morphosyntactic, 10 lexical) with region-specific role prompts
  - **Model interface**: OpenAI API (GPT models) vs. local inference via HuggingFace weights (open models)
  - **Scoring system**: Binary correctness (1/0) with penalty weighting based on question difficulty
  - **Response selection**: Argmax over log-probabilities at temperature=0

- Critical path:
  1. Define variety classification scheme (7 varieties from dialectology literature)
  2. Construct questions with variety-specific correct answers validated against corpora (CORPES XXI, CREA, VARILEX)
  3. Format prompts with regional identity role assignment
  4. Extract log-probabilities for each option, select highest
  5. Compute scores with penalty formula, aggregate by variety and model

- Design tradeoffs:
  - **Breadth vs. depth**: 30 questions across 7 varieties enables broad comparison but limits statistical power per variety
  - **Standard vs. colloquial**: Using "standard" variety forms improves consistency but may miss common colloquial usage
  - **Single-prompt vs. few-shot**: Single role-prompt approach is controlled but may under-elicit model knowledge

- Failure signatures:
  - **Uniform selection bias**: Model consistently selecting Peninsular forms regardless of prompted region indicates variety-blind behavior
  - **Lexical collapse**: 70-100% lexical error rates (seen in smaller models) suggest vocabulary never acquired
  - **Morphosyntactic detection with lexical failure**: Recognizing voseo but failing regional vocabulary (e.g., Mistral on Rioplatense) indicates partial acquisition

- First 3 experiments:
  1. Reproduce scoring on GPT-4o-mini and one open model (e.g., Gemma-2-9b-it) with temperature=0 on the 30-question test; verify log-probability extraction works correctly for both API and local models
  2. Ablate the role prompt: run same questions without regional identity framing to measure how much the prompt actually shifts responses vs. baseline
  3. Extend to few-shot: provide 3 example Q&A pairs in the target variety before test questions to test whether knowledge exists but isn't activated by zero-shot prompting

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LLM performance on Spanish dialectal identification change when evaluated using generative tasks rather than multiple-choice log-probability estimation?
- **Basis in paper:** [inferred] The methodology relies exclusively on multiple-choice questions and log-probability rankings to assess "knowledge," leaving the models' ability to actively produce authentic dialectal text untested.
- **Why unresolved:** The current study measures passive recognition (selecting the correct option) rather than active production, which is a distinct capability required for real-world applications like localization.
- **What evidence would resolve it:** A comparative study evaluating the same models on open-ended generation tasks, graded by native speakers or distinct evaluation metrics.

### Open Question 2
- **Question:** Can models distinguish between individual national dialects within the "Mexican and Central American" macro-area, or does the observed low performance mask even greater confusion at the country level?
- **Basis in paper:** [inferred] The authors acknowledge limitations in their classification, noting they normalized varieties and grouped "Mexican and Central American" because the "spectrum of varieties of Spanish is too wide."
- **Why unresolved:** The study treats Mexico and Central America as a single block, so it is unknown if the models recognize specific features of Guatemalan or Costa Rican Spanish, or if they fail uniformly across the entire region.
- **What evidence would resolve it:** A fine-grained evaluation testing the models specifically on linguistic features unique to individual nations within the Central American group.

### Open Question 3
- **Question:** Can targeted fine-tuning on low-resource dialects (e.g., Andean, Antillean) close the performance gap with Peninsular Spanish without inducing catastrophic forgetting?
- **Basis in paper:** [explicit] The authors explicitly call for the need to "continue evaluating models" to discern those representative of diversity, and identify a strong correlation (0.94) between training data presence in CEREAL and high performance.
- **Why unresolved:** While the study identifies a "Digital Linguistic Bias" caused by skewed training data, it does not test methods to correct this imbalance in smaller or open-source models.
- **What evidence would resolve it:** Experiments fine-tuning underperforming models (like Llama variants) on curated corpora of Antillean or Andean Spanish to see if scores improve to match Peninsular levels.

## Limitations

- Scoring formula uses a penalty mechanism based on question difficulty, but exact weighting parameters are truncated in the paper and must be retrieved from the Zenodo repository
- The prompt template is only partially shown (two examples), raising uncertainty about exact phrasing consistency across all varieties
- The mechanism linking corpus frequency to model performance is supported by correlation but not causal validation - post-training alignment could explain the bias

## Confidence

- **High**: GPT-4o outperforming other models, Peninsular Spanish being best recognized, lexical errors exceeding morphosyntactic errors in smaller models
- **Medium**: The claim that training corpus bias explains performance gaps (correlation is strong but causal mechanism untested)
- **Low**: The effectiveness of role-based prompting strategy (tested against no prompting, but not against few-shot or chain-of-thought alternatives)

## Next Checks

1. Replicate the exact penalty formula from Zenodo and verify that the reported scores can be reproduced on at least two models (one API, one local)
2. Run an ablation test comparing role-prompted vs. no-prompt conditions to quantify how much the regional identity framing actually shifts model outputs
3. Extend the evaluation with few-shot prompting (3 example Q&A pairs per variety) to test whether knowledge exists but isn't activated by zero-shot prompting