---
ver: rpa2
title: Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting
arxiv_id: '2508.00523'
source_url: https://arxiv.org/abs/2508.00523
tags:
- regret
- bound
- online
- delay
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies online nonsubmodular optimization with delayed\
  \ feedback in the bandit setting, where the loss function is \u03B1-weakly DR-submodular\
  \ and \u03B2-weakly DR-supermodular. Previous work established an (\u03B1,\u03B2\
  )-regret bound of O(nd^{1/3}T^{2/3}), but this bound depends on the maximum delay\
  \ and couples the effects of delays and bandit feedback."
---

# Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting

## Quick Facts
- **arXiv ID:** 2508.00523
- **Source URL:** https://arxiv.org/abs/2508.00523
- **Authors:** Sifan Yang; Yuanyu Wan; Lijun Zhang
- **Reference count:** 13
- **Primary Result:** Two algorithms DBGD-NF (O(n d̄^{1/3}T^{2/3}) average delay bound) and BDBGD-NF (O(n(T^{2/3} + √(dT))) decoupled bound) for online nonsubmodular optimization with delayed feedback.

## Executive Summary
This paper addresses online nonsubmodular optimization with delayed feedback in the bandit setting, where the loss function is neither submodular nor supermodular. Previous work achieved an (α,β)-regret bound of O(nd^{1/3}T^{2/3}) that depends on the maximum delay. The authors propose two algorithms: DBGD-NF, which achieves a better O(n d̄^{1/3}T^{2/3}) regret bound using all available estimated gradients and scaling with average delay; and BDBGD-NF, which employs a blocking update mechanism to decouple the joint effect of delays and bandit feedback, achieving an O(n(T^{2/3} + √(dT))) regret bound. Experiments on structured sparse learning demonstrate the superiority of the proposed methods.

## Method Summary
The paper proposes two algorithms for online nonsubmodular optimization with delayed feedback. DBGD-NF uses a one-point gradient estimator and updates the decision using all available estimated gradients in each round, achieving an O(n d̄^{1/3}T^{2/3}) regret bound that scales with average delay rather than maximum delay. BDBGD-NF extends this with a blocking update mechanism that decouples the joint effect of delays and bandit feedback, resulting in an O(n(T^{2/3} + √(dT))) regret bound. Both algorithms operate on the Lovász extension of the discrete nonsubmodular function, enabling gradient-based updates in the continuous domain. The paper also develops DOGD-NF for the full-information setting with an O(√(n d̄T)) regret bound.

## Key Results
- DBGD-NF achieves O(n d̄^{1/3}T^{2/3}) regret bound using all available gradients, improving over previous bounds dependent on maximum delay
- BDBGD-NF achieves O(n(T^{2/3} + √(dT))) regret bound by decoupling delays from bandit feedback effects
- DOGD-NF for full-information setting achieves O(√(n d̄T)) regret bound
- Experiments on structured sparse learning demonstrate superior performance over baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Utilizing all available delayed gradients improves the regret bound's dependence on delay from maximum delay (d) to average delay (d̄).
- **Mechanism:** DBGD-NF aggregates and uses all received gradients {ĝ_k | k ∈ F_t} at each update step t, rather than waiting for the oldest outstanding gradient. This smoothing effect makes the regret bound scale with d̄ = (1/T)∑_{t=1}^T d_t instead of worst-case d.
- **Core assumption:** Delays d_t are arbitrary but finite; loss function is α-weakly DR-submodular and β-weakly DR-supermodular.
- **Evidence anchors:** Abstract states "utilizes all the available estimated gradients in each round to update the decision. It achieves a better O(n d̄^{1/3}T^{2/3}) regret bound..."; Section 5 demonstrates this mechanism.

### Mechanism 2
- **Claim:** A blocking update mechanism can decouple the joint effect of delays and bandit feedback.
- **Mechanism:** BDBGD-NF divides T rounds into blocks of size K. Updates occur only at block ends using gradients from completed blocks. By choosing K = T^{1/3}, the variance is controlled and delay term appears additively as √(dT) instead of multiplicatively.
- **Core assumption:** Maximum delay d is known or bounded to set learning rate and block size.
- **Evidence anchors:** Abstract mentions "extend DBGD-NF by employing a blocking update mechanism to decouple the joint effect of the delays and bandit feedback, which enjoys an O(n(T^{2/3} + √(dT))) regret bound."

### Mechanism 3
- **Claim:** The Lovász extension and convex relaxation enable gradient-based methods for discrete nonsubmodular problems.
- **Mechanism:** The discrete nonsubmodular function is mapped to continuous domain via Lovász extension f_L(·). For nonsubmodular functions, the subgradient of f_L serves as approximate subgradient for convex closure f_C, enabling gradient descent updates.
- **Core assumption:** Nonsubmodular function can be decomposed as f(S) := f̄(S) - f̲(S), where f̄ is weakly DR-submodular and f̲ is weakly DR-supermodular.
- **Evidence anchors:** Section 3 introduces Lovász extension and convex relation; Lemma 1 demonstrates approximation between f_L and f_C.

## Foundational Learning

- **Concept: Online Nonsubmodular Optimization**
  - **Why needed here:** Core problem class differs from standard OCO because loss functions are discrete set functions lacking diminishing returns property of submodularity.
  - **Quick check question:** Can you explain how a nonsubmodular function differs from a submodular one, and why standard convexity assumption used in OCO doesn't directly apply?

- **Concept: Delayed Feedback in Bandits**
  - **Why needed here:** Key constraint addressed - learner must make decisions without immediate knowledge of incurred loss, common in distributed systems.
  - **Quick check question:** How does receiving loss value f_t(S_t) at round t+d_t instead of round t fundamentally change online learning algorithm and its analysis?

- **Concept: One-Point Gradient Estimator**
  - **Why needed here:** In bandit setting, only scalar loss value is observed, not gradient. This technique constructs unbiased gradient estimate from single function evaluation.
  - **Quick check question:** How can single function value f_t(x) be used to estimate gradient ∇f_t(x), and what is trade-off (e.g., in variance) compared to multi-point estimator?

## Architecture Onboarding

- **Component map:** Input Layer -> Delay Handler (buffer/pool P_i) -> Gradient Estimator (one-point) -> Decision Core (Non-blocking DBGD-NF or Blocking BDBGD-NF) -> Sampling Layer

- **Critical path:**
  1. Initialize x_1
  2. Sort x_t and sample set S_t
  3. Receive delayed loss values, compute estimated gradients ĝ_k, store in buffer
  4. Identify available gradients and perform projected gradient descent update on x_t (DBGD-NF) or y_m (BDBGD-NF)

- **Design tradeoffs:**
  - Average vs. Maximum Delay: DBGD-NF achieves bound dependent on average delay d̄ but couples delay and bandit effects; BDBGD-NF decouples effects but bound depends on maximum delay d
  - Update Frequency: Blocking updates reduce variance but update less frequently; non-blocking uses all information immediately but has higher variance

- **Failure signatures:**
  - Exploding regret: If delays are highly irregular and maximum delay d >> d̄^{2/3}T^{1/3}, DBGD-NF bound may be inferior to BDBGD-NF
  - Stalled learning: If delay d_t exceeds block size K in BDBGD-NF, gradients may arrive too late for their block to be used

- **First 3 experiments:**
  1. Baseline Comparison on Synthetic Data: Implement structured sparse learning with linear regression task; compare DOGD-NF, DOAGD, DBGD-NF, BDBGD-NF, and DBAGD under uniform random delays; plot regret vs iterations for different maximum delays
  2. Irregular Delay Stress Test: Create delay distribution with mostly small delays but few extremely large ones; test robustness claim; expect DBGD-NF to perform well if average delay is low
  3. Block Size Sensitivity Analysis: For BDBGD-NF, vary block size K away from theoretical optimum T^{1/3}; measure how regret changes to validate theoretical trade-off

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a blocking-based algorithm be developed that achieves a decoupled regret bound dependent on the average delay d̄ rather than the maximum delay d?
  - **Basis:** Conclusion states BDBGD-NF regret bound depends on maximum delay, and authors plan to "investigate how to develop a decoupled algorithm to obtain the regret bound that relies on the average delay"
  - **Why unresolved:** Current blocking mechanism successfully decouples feedback effects but introduces dependence on worst-case delay d rather than average d̄
  - **What evidence would resolve it:** Algorithm utilizing blocking update that proves regret bound of O(n(T^{2/3} + √(d̄T))) or similar

- **Open Question 2:** How can online nonsubmodular optimization be extended to non-stationary environments where data distribution or optimal solution may change over time?
  - **Basis:** Authors explicitly list "online nonsubmodular optimization in the non-stationary environments" as future work direction
  - **Why unresolved:** Current paper assumes stationary setting where goal is to minimize static regret against single best-fixed decision in hindsight
  - **What evidence would resolve it:** Modification of proposed algorithms that provides guarantees for dynamic regret or tracking regret in non-stationary settings

- **Open Question 3:** Is it possible to achieve best-of-both-worlds: algorithm that simultaneously decouples joint effect of delays and bandit feedback and relies on average delay?
  - **Basis:** Paper presents DBGD-NF (average delay, coupled effects) and BDBGD-NF (max delay, decoupled effects) as separate contributions with distinct advantages; unified method not established
  - **Why unresolved:** BDBGD-NF is advantageous only when d = o(d̄^{2/3}T^{1/3}), suggesting gap where unified algorithm might outperform both in general delay settings
  - **What evidence would resolve it:** Single algorithmic framework that maintains decoupling property while adapting to average delay irregularities without relying on maximum delay d

## Limitations

- Theoretical contributions rely on specific structural assumptions about nonsubmodular functions (α-weakly DR-submodular and β-weakly DR-supermodular decomposition)
- Experimental validation limited to synthetic structured sparse learning tasks; effectiveness on real-world nonsubmodular optimization problems remains unclear
- Performance gap between proposed algorithms and baselines could be influenced by specific hyperparameter tuning strategy

## Confidence

- **Mechanism 1 (DBGD-NF's average delay bound):** High confidence - Theoretical derivation is rigorous and core insight is clearly explained and experimentally validated
- **Mechanism 2 (BDBGD-NF's decoupled bound):** Medium confidence - Theoretical analysis is sound but assumption that maximum delay d is known for setting block size K may be restrictive; experimental validation is limited
- **Mechanism 3 (Lovász extension for nonsubmodular functions):** High confidence - Builds on established submodular optimization theory with clear extensions to nonsubmodular case

## Next Checks

1. **Real-World Dataset Validation:** Apply proposed algorithms to real-world nonsubmodular optimization problem such as feature selection for non-submodular objective or influence maximization in social networks with delayed feedback; compare performance against state-of-the-art methods

2. **Robustness to Unknown Delays:** Modify BDBGD-NF to handle scenarios where maximum delay d is unknown; implement adaptive mechanism to estimate d online and adjust block size K accordingly; evaluate algorithm's performance under varying delay distributions

3. **Variance Analysis of Gradient Estimators:** Conduct theoretical and empirical analysis of variance of one-point gradient estimator used in bandit setting; quantify how variance scales with exploration parameter μ and dimension n; investigate techniques to reduce variance without compromising unbiasedness of estimator