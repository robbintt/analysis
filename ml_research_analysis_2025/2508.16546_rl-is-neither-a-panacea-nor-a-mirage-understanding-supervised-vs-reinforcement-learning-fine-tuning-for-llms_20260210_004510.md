---
ver: rpa2
title: 'RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement
  Learning Fine-Tuning for LLMs'
arxiv_id: '2508.16546'
source_url: https://arxiv.org/abs/2508.16546
tags:
- singular
- fine-tuning
- performance
- arxiv
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the dynamics between supervised fine-tuning
  (SFT) and reinforcement learning (RL) in post-training large language models, using
  a card game benchmark and spectral analysis. The authors find that RL primarily
  restores generalization lost during SFT rather than introducing new capabilities,
  with effectiveness depending on the severity of SFT-induced overfitting.
---

# RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs

## Quick Facts
- arXiv ID: 2508.16546
- Source URL: https://arxiv.org/abs/2508.16546
- Authors: Hangzhan Jin; Sicheng Lv; Sifan Wu; Mohammad Hamdaqa
- Reference count: 17
- Primary result: RL primarily restores generalization lost during SFT rather than introducing new capabilities, with effectiveness depending on SFT-induced overfitting severity.

## Executive Summary
This study investigates the dynamics between supervised fine-tuning (SFT) and reinforcement learning (RL) in post-training large language models. Using a card game benchmark and spectral analysis, the authors find that RL primarily restores out-of-distribution (OOD) generalization lost during SFT rather than introducing new capabilities. Through singular value decomposition analysis, they show that OOD performance degradation and recovery correlate with rotations of singular vectors rather than changes in singular value magnitudes. Critically, low-rank and shallow-layer recovery—restoring directions of the top 20% singular values or first 25% of layers—recovers 70-80% of OOD performance, offering an efficient alternative to costly RL fine-tuning.

## Method Summary
The study employs a two-stage fine-tuning pipeline on Llama-3.2-11B and Qwen-2.5-7B models using the GeneralPoints card game benchmark with OOD variants. First, supervised fine-tuning (SFT) is performed with learning rate 1e-6 and batch size 64. Then, proximal policy optimization (PPO) reinforcement learning fine-tuning starts from the SFT checkpoint. The researchers conduct SVD spectral analysis on Q/K/V projection matrices and embed_tokens/lm_head matrices, tracking how singular vectors rotate during each stage. They also test UV restoration by replacing post-SFT singular vectors with pre-trained directions to quantify OOD recovery.

## Key Results
- RL-FT can restore much of the OOD performance loss from SFT, but effectiveness depends on SFT severity
- OOD performance changes correlate with rotations of singular vectors rather than changes in singular value magnitudes
- Restoring top 20% singular values or first 25% of layers recovers 70-80% of OOD performance
- RL primarily counteracts SFT-induced directional drift rather than finding fundamentally new solutions

## Why This Works (Mechanism)

### Mechanism 1: Restorative RL vs. SFT Drift
Reinforcement learning fine-tuning primarily restores out-of-distribution (OOD) generalization lost during supervised fine-tuning (SFT), rather than creating new capabilities, provided SFT has not induced severe overfitting. SFT causes the model's policy to drift away from the base initialization to maximize likelihood on specific data, degrading OOD performance. RL-FT (specifically PPO with KL constraints) counteracts this directional drift, pulling the representation back toward a generalizable state while maintaining task-specific ID performance.

### Mechanism 2: Rotational Dynamics of Singular Vectors
Performance shifts in fine-tuning are driven by the rotation of singular vectors (changes in direction) rather than changes in singular value magnitudes. The optimization process (gradient descent with weight decay) prefers to modify weights via orthogonal rotations because the penalty cost is O(η²), whereas changing singular values costs O(η). Both SFT and RL exploit this "gauge" to reorient the feature space while keeping the spectrum (intrinsic capacity) intact.

### Mechanism 3: Targeted Subspace Recovery
Efficient recovery of OOD performance is possible by restoring the singular vector directions of only the top-ranked components (top 20% singular values) or the shallow layers (first 25%). General reasoning capabilities are largely encoded in the top singular directions and the outer layers of the transformer. Intermediate layers and lower-rank directions are repurposed for task-specific specialization during SFT. Restoring directions in the general-encoding subspaces recovers OOD capability without erasing the specialized knowledge.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: This is the primary diagnostic tool used in the paper. Understanding that M = UΣV^T decomposes a matrix into directions (U, V) and magnitudes (Σ) is required to interpret why "rotations" affect performance while "values" remain stable.
  - Quick check question: If a weight matrix W undergoes a rotation W → WR, how do its singular values change? (Answer: They stay the same).

- **Concept: Catastrophic Forgetting**
  - Why needed here: The paper frames the OOD performance drop during SFT as a form of forgetting. Understanding the trade-off between learning a new distribution (ID) and retaining pre-trained knowledge (OOD) is central.
  - Quick check question: As ID loss monotonically decreases during SFT, what typically happens to OOD loss? (Answer: It initially drops, then rises/erodes).

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: The paper uses PPO for the RL stage. Knowing that PPO uses a clipped surrogate objective and often a KL-divergence penalty helps explain why it constrains the policy and acts as a "restorative" force rather than an unconstrained explorer.
  - Quick check question: Does PPO encourage the policy to stay close to the reference policy? (Answer: Yes, typically via KL penalties or clipping).

## Architecture Onboarding

- **Component map:** SFT (full-parameter fine-tuning on target task) -> RL (PPO fine-tuning starting from SFT checkpoint) -> Diagnostics (SVD spectral analysis on Self-Attention and MLP matrices)

- **Critical path:** SFT Early Stopping -> RL Recovery. The model must not pass the "severe overfitting" threshold during SFT, or the subsequent RL stage will fail to recover OOD performance.

- **Design tradeoffs:**
  - ID vs. OOD: Maximizing ID accuracy requires prolonged SFT, which degrades OOD. RL recovers OOD but sacrifices some peak ID accuracy.
  - Cost vs. Accuracy: Full RL is expensive. "Low-rank UV merging" (restoring directions mathematically) is cheap and recovers ~80% of the benefit.

- **Failure signatures:**
  - Irreversible Drift: OOD accuracy flatlines during RL even after significant training steps. This indicates SFT was run for too long.
  - Intermediate Layer Clash: Attempting to restore directions in intermediate layers (e.g., layers 10-20) destroys ID performance without gaining OOD.

- **First 3 experiments:**
  1. UV Merging: Take a post-SFT checkpoint, compute SVD, and replace the U, V matrices of the top 20% singular values with those from the pre-trained base model. Evaluate OOD recovery.
  2. SFT Duration Sweep: Train SFT checkpoints at varying lengths (e.g., 20%, 50%, 100%, 150% of standard epochs), then run RL. Plot "RL Recovery Success" vs. "SFT Overfitting Severity."
  3. Layer-wise Reset: For a fixed post-SFT model, restore singular vector directions for distinct layer groups (Head, Intermediate, Tail) and measure the impact on ID vs. OOD accuracy to verify the spatial separation of general vs. specialized knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
Why do SFT and RL fine-tuning converge on similar singular vector rotation profiles despite having distinct optimization objectives? The authors state that the "exact nature of the rotation pattern shared by SFT and RL remains unresolved" and identifying the cause is an open question. A theoretical framework linking gradient dynamics in SFT and RL to a shared rotational invariance would resolve this.

### Open Question 2
What specific role do the head and tail singular values play in the deterioration or recovery of out-of-distribution (OOD) generalization? The authors note they plan to "isolate the head and tail singular values to ascertain their exact role" in future work. Ablation studies that selectively freeze or modify the head and tail singular values during training would prove causality.

### Open Question 3
Does the "restoration" mechanism of RL fine-tuning generalize to complex domains like advanced mathematics and code generation? The conclusion lists applying this methodology to "advanced math and code-generation tasks" as necessary future work. Replicating the spectral analysis and low-rank recovery experiments on models fine-tuned for complex coding or mathematical reasoning benchmarks would validate universality.

### Open Question 4
What is the precise threshold at which SFT-induced overfitting becomes irreversible by subsequent RL fine-tuning? The authors observe that if SFT is "prolonged beyond a threshold," RL cannot fully recover OOD performance, but they do not define this boundary quantitatively. A systematic study mapping SFT duration and spectral distance against RL recovery success rates would identify the exact "point of no return."

## Limitations

- Core claims rely heavily on controlled experimental settings with synthetic OOD variants in a card game domain, limiting generalizability to real-world tasks
- UV merging approach requires precise selection of singular vector thresholds (top 20%) and layer boundaries (first 25%) that may vary across different model architectures and tasks
- The mechanism explaining why low-rank directions encode general reasoning while higher-rank directions encode task-specific knowledge is proposed but not rigorously proven

## Confidence

- **High Confidence:** The empirical observation that RL can recover OOD performance lost during SFT when applied at appropriate checkpoints. The spectral analysis showing correlations between vector rotations and performance changes.
- **Medium Confidence:** The claim that RL primarily restores rather than creates new capabilities, and that the effectiveness depends on SFT severity. The specific effectiveness percentages (70-80% recovery) from low-rank restoration.
- **Low Confidence:** The proposed mechanism explaining why low-rank directions encode general reasoning and higher-rank directions encode task-specific knowledge. The assertion that these findings generalize to real-world tasks beyond synthetic card game benchmarks.

## Next Checks

1. **Cross-task validation:** Test the SFT→RL recovery pipeline and UV merging approach on diverse real-world tasks (e.g., summarization, code generation, medical diagnosis) with naturally occurring OOD variants to assess generalizability beyond synthetic card game benchmarks.

2. **Temporal stability analysis:** Track OOD recovery over extended RL training periods (beyond the reported 5K steps) to determine whether restored performance is stable or degrades over time, and whether additional RL steps provide diminishing returns.

3. **Architecture scaling study:** Apply the same methodology to larger models (13B-70B parameters) to verify whether the spectral patterns (low-rank encoding general reasoning) and recovery effectiveness percentages scale consistently with model size.