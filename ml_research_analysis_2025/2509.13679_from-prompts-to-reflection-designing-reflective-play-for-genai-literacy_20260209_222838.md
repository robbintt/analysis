---
ver: rpa2
title: 'From Prompts to Reflection: Designing Reflective Play for GenAI Literacy'
arxiv_id: '2509.13679'
source_url: https://arxiv.org/abs/2509.13679
tags:
- genai
- players
- game
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ImaginAItion is a multiplayer party game that promotes reflective
  understanding of generative AI through constrained prompting and peer comparison.
  In ten sessions with 30 adults, gameplay led to 73.8% of participants achieving
  calibrated understandings of model behaviors.
---

# From Prompts to Reflection: Designing Reflective Play for GenAI Literacy

## Quick Facts
- arXiv ID: 2509.13679
- Source URL: https://arxiv.org/abs/2509.13679
- Authors: Qianou Ma; Megan Chai; Yike Tan; Jihun Choi; Jini Kim; Erik Harpstead; Geoff Kauffman; Tongshuang Wu
- Reference count: 40
- Primary result: 73.8% of participants achieved calibrated understanding of GenAI model behaviors through gameplay

## Executive Summary
ImaginAItion is a multiplayer party game designed to improve generative AI literacy through constrained prompting and peer comparison. The game successfully engaged 30 adults in reflective understanding of model behaviors, with 73.8% achieving calibrated understanding of model defaults and biases. Players developed strategies to mitigate model defaults, recognized human-AI perceptual mismatches, and balanced model consistency with unpredictability. The study demonstrates that embedding reflection into engaging gameplay mechanics can effectively support critical GenAI literacy education.

## Method Summary
The study involved 10 sessions with 30 adults playing ImaginAItion, a multiplayer web game where players compete to create the most similar image to a given prompt within a 70-second time limit. Players submit prompts constrained to 50 tokens and vote on which peer's image most closely matches the original prompt. The game includes 6 rounds with prompts spanning categories like demographic bias, spatial reasoning, and text rendering. Pre- and post-surveys measured changes in understanding, with qualitative analysis of reflection outcomes coded into categories like "enlightened," "confirmed," and "misaligned."

## Key Results
- 73.8% of participants achieved calibrated understanding of model behaviors
- Players developed strategies to mitigate model defaults and recognized human-AI perceptual mismatches
- Group composition and repeated exposure significantly influenced reflection quality
- The game successfully supported critical GenAI literacy through engaging gameplay mechanics

## Why This Works (Mechanism)

### Mechanism 1: Constrained Prompting for Hypothesis Testing
Constraining prompt length forces players to externalize mental models of GenAI defaults. The brevity reward creates tradeoffs between specification and omission, transforming abstract beliefs into testable hypotheses visible in the prompt itself.

### Mechanism 2: Multi-Level Structured Contrast
Comparing outputs at multiple levels (self vs. AI, self vs. peers, peers vs. peers) surfaces misalignments that single-player use would miss. The reveal phase creates three simultaneous comparison axes that produce cognitive dissonance when expectations fail.

### Mechanism 3: Repeated Exposure with Category Variation
Exposing players to multiple categories of model behavior across rounds enables pattern recognition beyond single-instance learning. Six rounds spanning different categories help players detect recurring default behaviors while avoiding overgeneralization from single examples.

## Foundational Learning

- **Concept:** Under-specification vs. over-specification tradeoffs
  - **Why needed here:** Players must understand that omitting details invites model defaults while over-specifying risks unintended interpretations.
  - **Quick check question:** Can you predict what visual elements the model will "fill in" if you only prompt "a CEO"?

- **Concept:** Human-AI perception gaps
  - **Why needed here:** The game reveals that humans and models prioritize different prompt aspects.
  - **Quick check question:** When you see an AI-generated image that differs from your intent, how do you determine whether the fault lies in your prompt or the model's interpretation?

- **Concept:** Probabilistic output behavior
  - **Why needed here:** Understanding that identical prompts can yield different outputs is essential for interpreting inconsistency.
  - **Quick check question:** If you submit the exact same prompt twice and get different results, what model characteristic does this demonstrate?

## Architecture Onboarding

- **Component map:** Frontend (React/Vite) -> WebSocket -> FastAPI Backend -> OpenAI gpt-image-1 API -> Game State Manager (Python dataclasses) -> Structured Logging (prompts, images, votes, timestamps)

- **Critical path:** Player submits prompt -> token counting (tiktoken, gpt-4o encoding, 300ms debouncing) -> prompt passes unchanged to gpt-image-1 API (~30s generation) -> async processing via ThreadPoolExecutor -> results broadcast via Socket.IO to all players (<1s latency) -> vote aggregation -> reveal phase with token overlap highlighting -> quick-draw panel enables 2 retry attempts per player

- **Design tradeoffs:** Chose gpt-image-1 (no temperature/seed control) to match real-world API constraints but limits experimental rigor for consistency testing; breach penalty creates incentive for brevity but may encourage overly risky minimal prompts; six rounds balances exposure time vs. pattern recognition opportunities

- **Failure signatures:** High "Uncorrected" outcomes in realism/co-occurrence categories suggest insufficient scaffolding for hypothesis formation; homogeneous groups showing fewer bias discussions indicate need for diversity-aware matching; experts showing more "Confirmed" outcomes suggest game may not challenge entrenched beliefs

- **First 3 experiments:**
  1. A/B test prompt category frequency: Run sessions where bias categories appear 3x each vs. once each to measure calibration rate differences
  2. Group composition manipulation: Systematically vary group diversity and measure reflection outcome distributions
  3. Longitudinal transfer test: Survey participants 2 weeks post-gameplay to assess persistence of prompting strategies

## Open Questions the Paper Calls Out

1. Does participation in ImaginAItion lead to sustained, long-term changes in prompting strategies and GenAI beliefs outside of the game context?

2. How does group composition (specifically demographic diversity and expertise mix) causally influence the depth of reflection on model bias?

3. Can dynamic, adaptive game mechanics improve the coverage of the "unlimited AI hypothesis space" compared to static prompt pools?

## Limitations

- Temporal model drift may invalidate the specific biases the game is designed to surface
- Social learning dependency means reflection outcomes heavily depend on group composition quality
- Limited evidence about whether reflection translates to improved real-world GenAI usage

## Confidence

- **High Confidence:** The game successfully engages players in reflective discourse about model behaviors
- **Medium Confidence:** The three proposed mechanisms effectively explain observed outcomes
- **Low Confidence:** The intervention's effectiveness will persist with future model versions or transfer to other GenAI literacy contexts

## Next Checks

1. **Model Version Replication Test:** Run the exact same game protocol with current OpenAI image generation API to quantify how many original biases remain detectable.

2. **Longitudinal Transfer Assessment:** Conduct follow-up surveys 2-4 weeks after gameplay to assess whether participants' prompting strategies persist in actual GenAI usage.

3. **Mechanistic Isolation Experiment:** Design A/B conditions that isolate each proposed mechanism to quantify the relative contribution of social learning versus prompting constraints to reflection outcomes.