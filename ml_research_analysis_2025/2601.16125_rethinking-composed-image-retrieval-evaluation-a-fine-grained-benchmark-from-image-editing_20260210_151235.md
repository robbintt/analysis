---
ver: rpa2
title: 'Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from
  Image Editing'
arxiv_id: '2601.16125'
source_url: https://arxiv.org/abs/2601.16125
tags:
- image
- query
- categories
- edir
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for fine-grained evaluation in composed
  image retrieval (CIR), where current benchmarks lack detailed category coverage
  and sufficient query scale. The authors propose a novel data synthesis pipeline
  that leverages image editing to systematically construct queries across a comprehensive
  taxonomy of 15 subcategories spanning five main categories.
---

# Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing

## Quick Facts
- arXiv ID: 2601.16125
- Source URL: https://arxiv.org/abs/2601.16125
- Reference count: 40
- Key outcome: Novel synthetic benchmark (EDIR) of 5,000 queries and 178,645 images reveals significant gaps in state-of-the-art multimodal models' compositional reasoning capabilities.

## Executive Summary
Composed Image Retrieval (CIR) requires models to combine a reference image with a modification text to find a target image. Current benchmarks lack fine-grained category coverage and sufficient query scale, making it difficult to diagnose model failures. This paper introduces EDIR, a synthetic benchmark created through an image editing pipeline that systematically constructs queries across 15 subcategories spanning five main categories. Evaluation of 13 multimodal embedding models reveals that even state-of-the-art models struggle with compositional reasoning, particularly in categories requiring spatial understanding, counting, and negation.

## Method Summary
The authors develop a data synthesis pipeline that inverts the standard annotation process by generating target images from modification texts rather than annotating existing pairs. Using Qwen2.5-VL-32B-Instruct for seed filtering and instruction generation, Qwen-Image-Edit-2509 for image editing, and Qwen3-32B for query rewriting, they create 5,000 high-quality query triplets across 15 subcategories. The pipeline employs a two-stage MLLM filtering process to ensure quality. For in-domain training experiments, they fine-tune Qwen2.5-VL-7B-Instruct with LoRA on 225K triplets using specified hyperparameters (batch size 128, 2,500 steps, LR 3e-5, weight decay 0.01, InfoNCE loss with temperature 0.03).

## Key Results
- State-of-the-art MLLM embedders achieve only ~36% Recall@1 on EDIR, with significant performance gaps across subcategories.
- Categories requiring spatial reasoning, counting, and negation show particular difficulty, suggesting intrinsic model limitations.
- In-domain training improves performance on data-solvable categories (Color, Material) but not on categories requiring compositional reasoning (Count, Spatial).
- The benchmark successfully distinguishes between data-solvable challenges and intrinsic architectural limitations.

## Why This Works (Mechanism)

### Mechanism 1: Causally Inverted Data Synthesis
Reversing the standard annotation pipeline (generating the target image from the modification text rather than annotating existing pairs) ensures precise alignment between the query and the visual change. The pipeline initiates with a textual edit instruction ($T_{edit}$) and uses an image editing model to synthesize the target image ($I_t$). This programmatically guarantees that the specific attribute or object change requested in the text is present in the target, eliminating "post-hoc" labeling noise.

### Mechanism 2: Context-Constrained Hard Negative Mining
Forcing the model to distinguish between a target and hard negatives that share a "base" visual context exposes failures in fine-grained compositional reasoning. The pipeline generates sets of images using composite instructions $\{a, b, c, d\}$. The "base" modifications $\{a, b\}$ create a shared visual context among candidates, while "distinctive" modifications $\{c, d\}$ differentiate them. This prevents models from using simple global feature matching.

### Mechanism 3: Solvability Separation via In-Domain Training
Performance gaps on EDIR distinguish between "data-solvable" challenges (lack of specific training examples) and "intrinsic" architectural limitations (failure to reason). By training a model on the synthesized data, the authors observed that categories like Color and Material improved drastically (data-solvable), while Count and Spatial remained difficult, suggesting the latter requires logical mechanisms missing from current architectures.

## Foundational Learning

- **Concept: Composed Image Retrieval (CIR)** - Why needed: This is the core task definition. Unlike text-to-image retrieval, CIR requires a model to fuse a reference image ($I_r$) with a modification text ($T_m$) to find a target ($I_t$). Quick check: If a user provides an image of a red car and the text "make it blue," should the model retrieve images of red cars or blue cars?

- **Concept: Modality Bias (Text-Only Shortcuts)** - Why needed: The paper identifies that existing models often succeed by ignoring the image and just searching for text keywords. EDIR is designed to break this shortcut. Quick check: In a benchmark where models perform better with "text-only" input than "image+text," what fundamental failure mode is being exposed?

- **Concept: MLLM (Multimodal Large Language Model) Embeddings** - Why needed: The paper evaluates the shift from CLIP-based models to MLLM-based embedders. Understanding how LLMs process visual tokens as "text" is crucial for interpreting the results. Quick check: Why might an MLLM struggle with "Spatial" or "Count" queries even if it can generate accurate captions for images?

## Architecture Onboarding

- **Component map:** Source Image ($I_r$) -> MLLM (Generates Edit Instructions) -> Image Editor (Generates $I_t$) -> Query Rewriter (LLM) -> Quality Filter (MLLM) -> Benchmark Triplets

- **Critical path:** The success of the EDIR benchmark relies on the Query Rewrite and Filtering stage. Raw edit instructions (e.g., "add a hat") must be converted into natural user queries (e.g., "I want to see this person with a hat") and verified to match the visual edit.

- **Design tradeoffs:** Synthetic vs. Real (trading authenticity for precise control and scale: 5,000 queries vs. CIRCO's 1,000); Complexity (limiting "Complex" queries to 3 constraints to keep the task solvable/diagnostic).

- **Failure signatures:** High Text-Only Score (indicates benchmark has modality bias); Low "Remove" Score (indicates model cannot "un-see" or subtract visual features); Low Correlation with CIRCO (indicates model is exploiting biases in older benchmark).

- **First 3 experiments:**
  1. Baseline Evaluation: Run standard MLLM embedders on EDIR to establish the "capability gap" (~36% Recall@1 for MLLMs).
  2. Modality Ablation: Evaluate models on EDIR using text-only input to check for reliance on linguistic shortcuts.
  3. In-Domain Fine-tuning: Train a lightweight adaptor on a subset of EDIR data to validate the "solvability" of specific categories like Texture vs. Spatial.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the data synthesis pipeline be optimized for computational efficiency to support massive-scale pre-training rather than solely benchmark construction? (The pipeline is "computationally expensive," making "large-scale data generation a challenge.")

- **Open Question 2:** To what extent does model performance degrade when handling complex queries with four or more interdependent conditions compared to the three-condition limit used in EDIR? (The current benchmark caps complexity at three composite instructions.)

- **Open Question 3:** What specific architectural modifications are required to resolve the "intrinsic" model limitations in compositional reasoning (e.g., spatial, count) and negation handling? (Failures in these categories represent "intrinsic model weaknesses" not easily resolved by data alone.)

## Limitations

- Data Synthesis Fidelity: The synthetic nature introduces uncertainty about whether generated target images truly represent realistic user scenarios, potentially inflating difficulty estimates for certain categories.

- MLLM Filtering Reliability: The two-stage filtering process depends entirely on MLLM judgment, with uncertainty about whether it can reliably distinguish acceptable from unacceptable queries.

- Category Granularity: Boundaries between some categories (e.g., Shape vs. Size, Texture vs. Material) may be ambiguous, potentially affecting evaluation consistency.

## Confidence

- **High Confidence:** The systematic approach to data synthesis and comprehensive evaluation of 13 models across 15 subcategories.
- **Medium Confidence:** The interpretation of category-specific performance gaps, particularly attributing them to architectural limitations vs. data scarcity.
- **Low Confidence:** The generalizability of findings to real-world user queries, since all queries are synthetically generated.

## Next Checks

1. **Cross-Model Consistency Test:** Evaluate the same EDIR benchmark using different MLLM image editors (e.g., GPT-4V, Gemini) to verify that difficulty patterns are consistent across synthesis methods.

2. **Human Evaluation Benchmark:** Conduct a small-scale human study where annotators rate the quality and realism of randomly sampled EDIR queries and their corresponding target images.

3. **Domain Transfer Experiment:** Test whether models trained on EDIR data show improved performance on real CIR benchmarks like CIRCO, measuring if fine-grained training generalizes beyond the synthetic domain.