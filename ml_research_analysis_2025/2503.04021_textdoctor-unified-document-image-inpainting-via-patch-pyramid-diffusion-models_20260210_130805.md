---
ver: rpa2
title: 'TextDoctor: Unified Document Image Inpainting via Patch Pyramid Diffusion
  Models'
arxiv_id: '2503.04021'
source_url: https://arxiv.org/abs/2503.04021
tags:
- textdoctor
- image
- document
- patch
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TextDoctor, a unified document image inpainting
  method designed to restore high-resolution text documents affected by degradation,
  scanning issues, or interference. The method is inspired by human reading behavior,
  first restoring fundamental text elements from image patches, then applying diffusion
  models to the full document.
---

# TextDoctor: Unified Document Image Inpainting via Patch Pyramid Diffusion Models

## Quick Facts
- **arXiv ID:** 2503.04021
- **Source URL:** https://arxiv.org/abs/2503.04021
- **Reference count:** 40
- **Primary result:** Outperforms state-of-the-art document inpainting methods on 7 public datasets with superior PSNR, SSIM, FID-crop, U-IDS-crop, and text recognition accuracy.

## Executive Summary
TextDoctor introduces a unified document image inpainting method that restores high-resolution text documents affected by degradation, scanning issues, or interference. The method is inspired by human reading behavior, first restoring fundamental text elements from image patches, then applying diffusion models to the full document. To address challenges of generalization across document types and high memory usage, TextDoctor employs a structure pyramid prediction and patch pyramid diffusion model. These techniques use multiscale inputs and pyramid patches to improve global and local text restoration quality. Experiments on seven public datasets show that TextDoctor significantly outperforms state-of-the-art methods, achieving higher pixel-level and perceptual-level similarity, and superior text recognition accuracy. Notably, TextDoctor generalizes well across different document types without additional fine-tuning, and efficiently processes high-resolution images without out-of-memory issues.

## Method Summary
TextDoctor uses a two-stage approach: first, a structure prediction model predicts binary maps of text regions from masked image patches; second, a patch pyramid diffusion model restores the full document conditioned on these predicted structures. The method employs multiscale inputs and pyramid patches to balance global and local restoration quality. Training uses 118,478 text image patches from TII-ST and TII-HT datasets, with the structure model trained for 50 epochs and the diffusion model for 1M iterations. Inference uses patch-based structure pyramid prediction and conditional patch pyramid denoising with patch sizes 128² and 256², achieving efficient processing of ultra-high-resolution documents up to 8K without memory issues.

## Key Results
- Achieves state-of-the-art performance on seven public datasets (FUNSD, BCSD, Roboflow subsets) with higher PSNR, SSIM, FID-crop, and U-IDS-crop scores than existing methods
- Demonstrates superior text recognition accuracy using CRNN, MORAN, and PaddleOCR models
- Successfully generalizes across different document types without additional fine-tuning
- Efficiently processes ultra-high-resolution images (up to 8K) without out-of-memory issues, with processing time of approximately 110 seconds for 8K images

## Why This Works (Mechanism)
The method's effectiveness stems from its human-reading-inspired two-stage approach that first restores fundamental text structures before applying diffusion models to the full document. The structure prediction stage provides a semantic prior that guides the diffusion model to focus on text restoration rather than general image completion. The patch pyramid approach allows the model to capture both global document context and local text details by processing multiple scales. This hierarchical processing, combined with the conditional diffusion that uses predicted structure maps as guidance, enables TextDoctor to achieve superior restoration quality while maintaining computational efficiency for high-resolution documents.

## Foundational Learning
- **Structure Prediction:** Predicts binary maps indicating text regions from masked patches; needed to provide semantic guidance for the diffusion model; quick check: verify predicted structure maps align with actual text locations in validation set
- **Patch Pyramid Diffusion:** Applies diffusion models at multiple scales using pyramid patches; needed to balance global context and local detail restoration; quick check: compare results using single vs. pyramid patch sizes
- **Conditional Diffusion:** Uses predicted structure maps as conditioning for the denoising process; needed to focus restoration on text regions; quick check: test diffusion model performance with and without structure conditioning
- **Content and Style Losses:** Uses CRNN features and Gram matrices for perceptual similarity; needed to ensure restored text maintains visual and semantic consistency; quick check: verify content/style loss contributions during training
- **Multiscale Processing:** Processes images at multiple resolutions through upsampling and pyramid structures; needed to handle both global document layout and local character details; quick check: test restoration quality at different upsampling factors
- **Patch-based Inference:** Processes high-resolution images in overlapping patches to manage memory; needed to enable ultra-high-resolution processing without OOM; quick check: verify seamless merging of overlapping patch outputs

## Architecture Onboarding

**Component Map:** Input Image -> Structure Prediction Model -> Binary Structure Maps -> Patch Pyramid Diffusion Model -> Restored Document

**Critical Path:** Masked image patches → Structure Prediction (Stage 1) → Predicted structure maps → Conditional Patch Pyramid Diffusion (Stage 2) → Merged output

**Design Tradeoffs:** The two-stage decoupled approach provides computational efficiency and modularity but may propagate errors from structure prediction to the diffusion stage. The patch pyramid approach balances global and local restoration quality but introduces complexity in patch merging and may create seam artifacts if not properly implemented.

**Failure Signatures:** Seam artifacts from improper overlap averaging during patch merging; out-of-memory errors when processing extremely large images without proper cache management; poor text restoration when masks cover entire words or sentences; digit reconstruction failures with minimal contextual information.

**First Experiments:** 1) Test structure prediction model on validation patches to verify binary map accuracy; 2) Run conditional diffusion on small patches with ground-truth structure maps to isolate diffusion performance; 3) Verify patch merging function R(·) produces seamless outputs with proper overlap averaging.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can document inpainting models be adapted to reconstruct semantic content (entire words or sentences) when large masks eliminate the local context necessary for texture generation?
- **Basis in paper:** The authors state in the Conclusion that the method "struggles with large masks, such as those covering entire words or sentences."
- **Why unresolved:** The current diffusion approach relies heavily on surrounding pixel context to generate text; when a mask covers a whole semantic unit, the model lacks the priors to hallucinate the correct missing characters.
- **What evidence would resolve it:** A modified architecture or loss function that incorporates higher-level semantic understanding or external language models to generate text for fully masked regions, validated by word-level accuracy metrics on datasets with sentence-level masking.

### Open Question 2
- **Question:** What specific architectural or data augmentation strategies can improve the reconstruction of numerical digits in documents with minimal contextual information?
- **Basis in paper:** The authors explicitly note in the Limitations section that "TextDoctor may not accurately reconstruct missing numbers with minimal contextual information."
- **Why unresolved:** Numbers often lack the redundant spatial features of letters, making them harder to infer from structure alone without distinct visual priors or strict logical constraints.
- **What evidence would resolve it:** Demonstrated improvements in digit-level accuracy on financial or tabular datasets (like BCSD) where numbers are heavily degraded or masked, potentially using supervised contrastive learning on digit classes.

### Open Question 3
- **Question:** How can the inference pipeline be optimized to maintain high visual fidelity while reducing the computational latency associated with processing ultra-high-resolution (e.g., 8K) documents?
- **Basis in paper:** The paper concludes that while ultra-high-resolution processing is supported, "it costs more computational resources," and suggests "using more mini-batch patches in parallel" as a future direction.
- **Why unresolved:** While patch-based processing manages memory (OOM), the sequential or partially parallel nature of the diffusion denoising steps results in high latency (e.g., ~110s for 8K images).
- **What evidence would resolve it:** Implementation of a parallelized inference scheduler or a distilled diffusion model that reduces the number of denoising steps without sacrificing the pixel-level PSNR/SSIM scores reported in the paper.

### Open Question 4
- **Question:** Does decoupling the structure prediction model from the inpainting diffusion model limit the upper bound of restoration quality compared to a jointly optimized end-to-end system?
- **Basis in paper:** The paper treats structure prediction ($f_{\bar{\theta}}$) and inpainting ($g_{\theta}$) as independent training stages. However, the authors note that "performance... is influenced by the structure prediction" and suggest "a more sophisticated model can further improve performance."
- **Why unresolved:** Errors in the first stage (structure prediction) propagate to the second stage (diffusion) with no mechanism for correction, suggesting a potential ceiling on performance.
- **What evidence would resolve it:** An ablation study comparing the current pipeline against a version where the structure prediction gradients are influenced by the final inpainting loss, showing whether joint optimization improves the FID and Char Acc metrics.

## Limitations
- Struggles with large masks covering entire words or sentences due to lack of contextual information for text generation
- May not accurately reconstruct missing numbers with minimal contextual information, particularly in tabular or financial documents
- Requires external architectural details from cited works (AOT, GRIG, VM-UNet, Mamba) for faithful reproduction, as specific layer configurations are not fully detailed

## Confidence

**High Confidence:** Experimental methodology, dataset preparation, and quantitative results (PSNR/SSIM/FID comparisons) are clearly specified and reproducible.

**Medium Confidence:** The two-stage training procedure is well-documented, but exact architectural details for the diffusion model's time embedding integration require external references.

**Medium Confidence:** Generalization claims are supported by ablation studies, but real-world performance on extremely degraded documents or non-text documents remains untested.

## Next Checks
1. Verify the exact CRNN layer indices used for content/style feature extraction in the structure prediction loss function
2. Implement the patch merging function $R(\cdot)$ with proper overlap averaging to prevent seam artifacts in high-resolution outputs
3. Test the OOM boundary by processing images at 8K resolution with varying batch sizes and cache flushing strategies during patch-based inference