---
ver: rpa2
title: Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion
  Detection Systems
arxiv_id: '2510.19761'
source_url: https://arxiv.org/abs/2510.19761
tags:
- adversarial
- network
- attacks
- nids
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how increasing the depth of deep neural
  networks affects their robustness against adversarial attacks in network intrusion
  detection systems (NIDS) compared to computer vision. We train five progressively
  deeper fully-connected neural networks on the CSE-CIC-IDS2018 dataset for NIDS and
  MNIST for computer vision, then evaluate their performance under FGSM adversarial
  attacks.
---

# Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion Detection Systems

## Quick Facts
- arXiv ID: 2510.19761
- Source URL: https://arxiv.org/abs/2510.19761
- Reference count: 22
- This study shows that deeper NIDS models are significantly more vulnerable to adversarial attacks than shallower ones, while in computer vision, depth has minimal impact on robustness.

## Executive Summary
This study investigates how deep neural network depth affects adversarial robustness in network intrusion detection systems compared to computer vision. Through systematic experiments on the CSE-CIC-IDS2018 dataset and MNIST, the research reveals that deeper NIDS models experience dramatically reduced robustness under FGSM attacks, with accuracy dropping from 0.999 at epsilon=0 to 0.849 at epsilon=0.1 for the deepest model. In contrast, computer vision models maintain consistent performance across depths, with accuracy remaining above 0.734 even under strong attacks. The findings challenge the assumption that deeper models are always better and highlight the need for domain-specific architecture design in security-critical ML applications.

## Method Summary
The study trains five progressively deeper fully-connected neural networks on both the CSE-CIC-IDS2018 dataset for NIDS (78 features) and MNIST for computer vision (784 features). Models range from shallow (one hidden layer) to deep (five hidden layers) architectures. After preprocessing the NIDS data through standardization, feature selection, and correlation-based elimination, the researchers evaluate clean performance and then apply FGSM adversarial attacks at varying perturbation magnitudes (epsilon values from 0.0 to 0.1). Performance metrics including precision, recall, F1-score, and accuracy are measured across both domains to compare depth effects on robustness.

## Key Results
- In NIDS, deeper networks show significantly reduced robustness, with accuracy dropping from 0.999 at epsilon=0 to 0.849 at epsilon=0.1 for the deepest model, while shallower models maintain better resilience.
- Computer vision models show consistent performance across depths, with accuracy remaining above 0.734 even under strong attacks.
- The study demonstrates that in NIDS, increased model complexity amplifies vulnerability due to constrained feature space, while in high-dimensional image domains, attack effectiveness depends primarily on perturbation magnitude rather than network depth.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In the NIDS domain, increasing DNN depth degrades adversarial robustness, whereas in computer vision, depth has a more modest impact on robustness under FGSM attacks.
- **Mechanism:** Deeper networks in a constrained, low-dimensional feature space (NIDS) exhibit higher model complexity, which amplifies sensitivity to gradient-based perturbations and expands the attack surface. In high-dimensional domains (computer vision), the primary driver of attack success is perturbation magnitude (epsilon), not network depth.
- **Core assumption:** The observed robustness differences are primarily driven by input dimensionality and model complexity, not by dataset-specific artifacts or training procedures.
- **Evidence anchors:**
  - [abstract] "Our experimental results reveal that in the NIDS domain, adding more layers does not necessarily improve their performance, yet it may actually significantly degrade their robustness against adversarial attacks. Conversely, in the computer vision domain, adding more layers exhibits a more modest impact on robustness."
  - [section V.C] "In the NIDS domain... increasing the number of parameters and layers rapidly expands the attack surface, making deeper NIDS models more vulnerable to adversarial perturbations."
  - [section V.C] "In image processing (MNIST)... the most influential parameter for robustness is ε, not network depth."
- **Break condition:** If deeper NIDS models were shown to maintain or improve robustness under FGSM attacks, or if epsilon were not the dominant factor in computer vision robustness, this mechanism would not hold.

### Mechanism 2
- **Claim:** Shallow NIDS models exhibit higher resilience to FGSM attacks due to limited attack space in constrained feature dimensions.
- **Mechanism:** With fewer features (78 in CSE-CIC-IDS2018 after preprocessing), shallow models have fewer parameters and a smaller attack surface, making gradient-based perturbations less effective at finding adversarial directions that cause misclassification.
- **Core assumption:** The feature space size and model parameter count directly constrain the effectiveness of gradient-based attacks in NIDS.
- **Evidence anchors:**
  - [section V.C] "The feature space in the NIDS domain (CSE-CIC-IDS2018) is smaller, meaning shallow models naturally act as a line of defense against adversarial attacks, often showing strong resilience even at higher ε values due to the limited attack space."
  - [Table II] Model 1 (shallowest) maintains accuracy of 0.939 at ε=0.1, while Model 5 (deepest) drops to 0.800 at the same epsilon.
  - [corpus] No direct corpus evidence on shallow model resilience; related papers focus on defense frameworks rather than depth effects.
- **Break condition:** If shallow NIDS models were found to be equally or more vulnerable than deeper models under FGSM attacks, this mechanism would be invalidated.

### Mechanism 3
- **Claim:** Adversarial robustness in NIDS is influenced by both perturbation magnitude (epsilon) and model complexity (depth/parameters), while in computer vision, robustness depends primarily on epsilon.
- **Mechanism:** The heterogeneous, structured nature of NIDS features means deeper networks can learn complex, exploitable representations that increase vulnerability. In contrast, high-dimensional image data already provides a large attack surface even for shallow models, making depth a secondary factor.
- **Core assumption:** Feature structure (heterogeneous/structured vs. homogeneous pixels) and dimensionality determine how model complexity affects adversarial vulnerability.
- **Evidence anchors:**
  - [section V.C] "In a nutshell, unlike the computer vision domain... where ε is the most influential parameter for robustness, in the NIDS domain, it is more complicated, and in addition to ε, model parameters and neural network depth play important roles."
  - [section VI] "In the NIDS domain, increasing model depth can unintentionally make models more susceptible to adversarial perturbations... Conversely, in high-dimensional, structured domains like image recognition... the effect of depth is minimal."
  - [corpus] Weak direct evidence; related work discusses adversarial robustness in NIDS but does not directly address the depth-robustness tradeoff.
- **Break condition:** If future work shows that depth has a comparable effect on robustness in both domains, or that structured features do not affect vulnerability, this mechanism would require revision.

## Foundational Learning

- **Concept:** **Fast Gradient Sign Method (FGSM)**
  - **Why needed here:** FGSM is the attack method used throughout the experiments to evaluate adversarial robustness across models and domains. Understanding how FGSM generates perturbations via the gradient of the loss function is essential to interpret the robustness results.
  - **Quick check question:** Given an input sample, what are the three components FGSM uses to compute the adversarial perturbation?

- **Concept:** **Input dimensionality and constrained feature spaces**
  - **Why needed here:** The paper's core finding hinges on the difference between low-dimensional, constrained NIDS features and high-dimensional image pixels. Understanding how dimensionality affects attack surface and model sensitivity is critical for applying these results.
  - **Quick check question:** Why would a model with 78 input features behave differently under adversarial attack compared to a model with 784 input features?

- **Concept:** **Model complexity vs. robustness tradeoff**
  - **Why needed here:** The paper argues that increased model complexity (via depth) can degrade robustness in NIDS, challenging the assumption that deeper models are always better. Recognizing this tradeoff is necessary for principled architecture design in security applications.
  - **Quick check question:** In what scenario might adding more layers to a neural network reduce its security rather than improve it?

## Architecture Onboarding

- **Component map:**
  - Input layer: 78 features (NIDS) or 784 pixels (MNIST)
  - Hidden layers: 1–5 fully-connected layers with widths [64], [64, 128], [64, 128, 128], [64, 128, 512, 128], [64, 128, 512, 128, 64]
  - Output layer: 1 neuron (binary NIDS classification) or 10 neurons (MNIST multi-class)
  - Attack module: FGSM with variable epsilon (0.0–0.1)

- **Critical path:**
  1. Data preprocessing (standardization, feature selection for NIDS; normalization for MNIST)
  2. Model training across five depth variants
  3. Clean performance evaluation (precision, recall, F1, accuracy)
  4. FGSM attack generation at increasing epsilon values
  5. Robustness evaluation and cross-domain comparison

- **Design tradeoffs:**
  - **Shallow vs. deep NIDS models:** Shallow models trade potential representational capacity for higher adversarial robustness; deep models may achieve similar clean accuracy but degrade faster under attack.
  - **Epsilon selection:** Larger epsilon increases attack effectiveness but may violate problem-space constraints in NIDS (e.g., protocol validity).
  - **Feature space reduction:** May improve robustness but risks losing discriminative information for intrusion detection.

- **Failure signatures:**
  - **Deep NIDS model under attack:** Rapid accuracy drop (e.g., from 0.999 at ε=0 to 0.849 at ε=0.1) with recall collapsing faster than precision.
  - **Shallow NIDS model under attack:** Graceful degradation, maintaining >0.9 accuracy even at moderate epsilon.
  - **MNIST models (any depth):** Gradual performance decline with increasing epsilon, minimal depth-dependent differences.

- **First 3 experiments:**
  1. **Replicate depth-robustness comparison:** Train the five architectures on CSE-CIC-IDS2018 and MNIST, evaluate under FGSM at ε ∈ {0.0, 0.02, 0.04, 0.06, 0.08, 0.1}. Confirm the domain-specific depth effects.
  2. **Feature dimensionality ablation:** Reduce NIDS feature count (e.g., to 40, 20) and re-evaluate robustness across depths to test whether constrained feature space drives the observed vulnerability.
  3. **Alternative attack evaluation:** Apply a different gradient-based attack (e.g., PGD) to assess whether the depth-robustness relationship generalizes beyond FGSM.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What defense mechanisms can effectively balance predictive accuracy and adversarial resilience in NIDS without sacrificing the detection capabilities that deeper networks might provide?
- **Basis in paper:** [explicit] The conclusion states: "Future work should investigate tailored defenses that balance predictive accuracy and adversarial resilience, especially in structured-data settings like NIDS."
- **Why unresolved:** Current defense strategies are largely borrowed from computer vision; this paper demonstrates that NIDS has fundamentally different robustness characteristics requiring domain-specific approaches.
- **What evidence would resolve it:** Development of NIDS-specific defense methods (e.g., constrained perturbation training, protocol-aware regularization) evaluated across multiple depths showing maintained accuracy under adversarial conditions.

### Open Question 2
- **Question:** Does the inverse relationship between depth and robustness in NIDS persist under stronger adversarial attacks beyond FGSM, such as PGD, C&W, or adaptive attacks?
- **Basis in paper:** [inferred] The study only evaluates FGSM attacks. Stronger iterative or optimization-based attacks may exhibit different depth-robustness dynamics in the constrained NIDS feature space.
- **Why unresolved:** FGSM is a single-step attack; multi-step attacks could exploit the expanded attack surface of deeper networks differently, potentially amplifying or diminishing the observed effect.
- **What evidence would resolve it:** Systematic evaluation of NIDS models across depths using diverse attack methodologies (PGD, AutoAttack, boundary attacks) with consistent perturbation budgets.

### Open Question 3
- **Question:** At what depth threshold does the robustness degradation in NIDS become significant, and does this threshold depend on the specific dataset or feature dimensionality?
- **Basis in paper:** [inferred] The study tests only 1-5 hidden layers on a single NIDS dataset with 78 features. The optimal depth-robustness tradeoff may vary with different feature spaces.
- **Why unresolved:** The paper attributes vulnerability to "constrained feature space" but does not systematically vary input dimensionality to isolate this factor from other dataset characteristics.
- **What evidence would resolve it:** Experiments across multiple NIDS datasets with varying feature dimensions, systematically mapping depth thresholds where robustness degradation accelerates.

## Limitations

- The study relies on a single attack method (FGSM) and dataset (CSE-CIC-IDS2018), limiting generalizability to other threat models and network environments.
- Training hyperparameters and specific preprocessing thresholds are underspecified, making exact reproduction challenging.
- No analysis of attack transferability or real-world threat models that might exploit depth-related vulnerabilities differently.

## Confidence

- **Mechanism 1:** Medium - Results are consistent with the hypothesis but preprocessing pipeline details are incomplete
- **Mechanism 2:** Medium - Clear empirical pattern but lacks supporting corpus evidence on shallow model resilience
- **Mechanism 3:** Medium - Cross-domain comparison is robust but requires validation with additional attacks and datasets

## Next Checks

1. **Replicate with stronger attacks:** Apply PGD or Carlini-Wagner attacks to CSE-CIC-IDS2018 models to confirm the depth-robustness relationship extends beyond FGSM.
2. **Feature space ablation:** Systematically vary NIDS feature count (e.g., 40, 20 features) and re-evaluate robustness across depths to test whether constrained feature space drives the observed vulnerability.
3. **Defense mechanism evaluation:** Implement adversarial training or regularization (e.g., weight decay, dropout) to assess if depth-induced vulnerability can be mitigated while maintaining detection accuracy.