---
ver: rpa2
title: 'ContourDiff: Unpaired Medical Image Translation with Structural Consistency'
arxiv_id: '2403.10786'
source_url: https://arxiv.org/abs/2403.10786
tags:
- translation
- image
- images
- contourdiff
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ContourDiff proposes a novel diffusion-based approach for unpaired
  medical image translation that uses domain-invariant anatomical contour representations
  as spatial constraints. The method trains a diffusion model on output-domain images
  conditioned on their contours and adjacent slices, then generates input-domain images
  in the output-domain appearance by conditioning on input contours and previously
  generated adjacent slices.
---

# ContourDiff: Unpaired Medical Image Translation with Structural Consistency

## Quick Facts
- **arXiv ID:** 2403.10786
- **Source URL:** https://arxiv.org/abs/2403.10786
- **Reference count:** 20
- **Primary result:** Zero-shot CT-to-MRI translation with significant DSC improvements (15.1-23.4%) over competing methods

## Executive Summary
ContourDiff introduces a diffusion-based approach for unpaired medical image translation that preserves anatomical fidelity by using domain-invariant contour representations as spatial constraints. The method trains a diffusion model exclusively on output-domain images (e.g., MRI) conditioned on their Canny edge contours and adjacent slices, then generates input-domain images in the output-domain appearance by conditioning on input contours and previously generated adjacent slices. This enables zero-shot translation between modalities with severe structural biases without requiring any input-domain information during training. Evaluated on CT-to-MRI translation for lumbar spine and hip-and-thigh regions, ContourDiff significantly outperforms competing methods in segmentation accuracy, edge alignment, and image quality metrics.

## Method Summary
ContourDiff is a diffusion-based unpaired image translation method that operates exclusively on output-domain data during training. The model uses a 6-stage UNet to predict noise in a DDPM framework, but conditions generation on three input channels: the noisy target image, a Canny edge contour map, and an adjacent slice from the same volume. During training, the model learns to fill in output-domain texture (e.g., MRI appearance) strictly within the bounds of provided contours. At inference, it generates input-domain images (e.g., CT) in the output-domain appearance by conditioning on input contours and previously generated adjacent slices, creating a causal chain for 3D consistency. The method requires no input-domain information during training, enabling zero-shot capability for translating between structurally similar domains.

## Key Results
- **Significant segmentation improvements:** 15.1% DSC improvement on lumbar dataset, 7.5% on lumbar-SPIDER, and 23.4% on hip-and-thigh datasets
- **Superior edge alignment:** HD95 reductions of 8.287-8.966 compared to competing methods
- **Better image quality:** Superior FID/KID scores across all tested datasets
- **Zero-shot capability:** Successfully translates T2 MRI to T1 MRI without retraining, using model trained for CT-to-MRI

## Why This Works (Mechanism)

### Mechanism 1: Contour as a Domain-Invariant Structural Anchor
The method uses Canny edge detection to extract anatomical contours that serve as domain-invariant structural anchors. By training the diffusion model to fill output-domain texture within contour bounds, it decouples structure from modality-specific texture. This forces the model to preserve input anatomy regardless of domain differences. The core assumption is that Canny edge detection captures relevant anatomical boundaries in both domains despite contrast differences.

### Mechanism 2: Spatially Coherent Guided Diffusion (SCGD) for 3D Consistency
SCGD conditions slice generation on adjacent slices to enforce volumetric consistency. During training, a third input channel provides either the previous or next slice with probability P_adj. This creates a causal chain where slice N guides slice N+1 generation, preventing checkerboard artifacts common in slice-by-slice synthesis. The structural changes between adjacent slices are assumed to be small and predictable.

### Mechanism 3: Zero-Shot Generalization via Conditional Independence
By training exclusively on output-domain data conditioned on structural contours, the model avoids learning input-domain distributions. This theoretically allows translation from any domain that shares contour definitions with the training output domain. The contour extraction method must produce statistically similar maps across modalities for this to work.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** ContourDiff replaces GANs with diffusion backbone. Understanding how models predict noise (ε) at timestep t is essential to grasp how contour constraints are applied during reverse process.
  - **Quick check question:** Can you explain why a diffusion model predicts noise rather than the clean image directly?

- **Concept: Spatial Conditioning / Concatenation**
  - **Why needed here:** The core architectural change is adding contour as input channel. This differs from cross-attention or adaptive normalization used in other conditional models.
  - **Quick check question:** How does concatenating a contour map to input tensor of a UNet differ mechanistically from using it as an attention mask?

- **Concept: Canny Edge Detection Parameters**
  - **Why needed here:** The method relies on "correct" contours. Hysteresis thresholds determine what counts as an edge, directly impacting structural bias preservation.
  - **Quick check question:** If you lower Canny threshold too much on noisy CT, what happens to resulting contour map, and how might this affect diffusion model?

## Architecture Onboarding

- **Component map:** Raw Image -> Artifact Filter (multiotsu → binary_erosion → remove_small_objects) -> Canny Contour Extraction -> 3-Channel Input [Noisy Image, Contour, Adjacent Slice] -> 6-Stage UNet -> Noise Prediction

- **Critical path:**
  1. Filter raw input to remove non-anatomical artifacts (tables, labels)
  2. Generate structural guide using Canny edge detection
  3. For 3D volume, generate n=16 candidates for first slice, pick lowest mean intensity
  4. Run sequential diffusion for slice i, conditioning on slice i-1

- **Design tradeoffs:**
  - Pixel Space vs. Latent Space: Operates in pixel space to preserve precise contour alignment. Tradeoff: Higher computational cost vs. Latent Diffusion, but necessary to prevent downsampling from erasing fine edge details.
  - P_adj (Adjacent Slice Ratio): Lower values (e.g., 0.2) force reliance on contours (better anatomy); higher values might improve texture consistency but risk anatomical drift. Paper recommends ≤ 0.5.

- **Failure signatures:**
  - "Overly Bright Backgrounds": Caused by stochastic instability in initial slice. Fix: Increase number of candidates n for initial slice selection
  - Anatomical Drift: Generated structures slowly shift or disappear in later slices. Fix: Ensure P_adj is not too high; check Canny thresholds on problematic slices

- **First 3 experiments:**
  1. Contour Ablation: Train with empty contour map (all zeros). Compare DSC against full model to quantify contour constraint value
  2. SCGD Ablation: Set P_adj = 0 and compare 3D consistency against P_adj = 0.2
  3. Robustness Check: Add Gaussian noise (15dB SNR) to input CT and visually inspect resulting contour map. Verify if model still produces anatomically plausible outputs

## Open Questions the Paper Calls Out

- **Question 1:** How can "distortion-aware guidance" be integrated into the ContourDiff framework to balance strict structural fidelity with preservation of diagnostically relevant geometric information, such as susceptibility-related MRI distortions?
  - **Basis:** Authors note in Conclusion that "strictly enforcing pixel-wise contour alignment could risk suppressing such meaningful distortions"
  - **Why unresolved:** Current method treats geometric distortions (which carry diagnostic information) as noise to be removed
  - **What evidence would resolve it:** Modified guidance mechanism that retains susceptibility artifacts while maintaining high DSC scores

- **Question 2:** Can automatic threshold selection based on simple image statistics replace manual tuning of Canny edge detection parameters without degrading performance?
  - **Basis:** Conclusion states practical deployment "may further benefit from automatic threshold selection"
  - **Why unresolved:** Current implementation requires manual setting of thresholds based on representative subsets
  - **What evidence would resolve it:** Adaptive thresholding algorithm achieving equivalent DSC/ASSD to manually tuned baseline

- **Question 3:** Can contour-guidance mechanism be effectively adapted for Latent Diffusion Models (LDMs) to improve computational efficiency without losing pixel-level anatomical precision?
  - **Basis:** Section 2.2 justifies operating in image space to avoid "loss of fine detail" in conditioning maps due to downsampling
  - **Why unresolved:** Image-space diffusion ensures precise contour adherence but is computationally heavier than LDMs
  - **What evidence would resolve it:** Latent-based variant maintaining comparable Edge HD95 scores while reducing inference time

## Limitations

- **Contour dependency:** Method relies heavily on Canny edge detection, which may fail with severe noise or artifacts in input domains (e.g., metal artifacts in CT)
- **Anatomical drift:** Error accumulation in sequential slice generation could degrade fidelity in later slices of 3D volumes
- **Zero-shot generalization:** While theoretically elegant, performance on more divergent domain pairs beyond T2→T1 MRI remains unknown

## Confidence

- **High Confidence:** Contour conditioning mechanism improves anatomical preservation (supported by ablation studies showing DSC gains)
- **Medium Confidence:** SCGD effectively prevents 3D discontinuities (visual inspection suggests improvement, but quantitative metrics are limited)
- **Medium Confidence:** Zero-shot capability works for T2→T1 translation (demonstrated in Table 6, but generalization to other domain pairs unverified)

## Next Checks

1. Test robustness by adding synthetic noise/artifacts to CT inputs and measuring degradation in contour alignment and segmentation performance
2. Evaluate anatomical drift quantitatively by tracking segmentation accuracy across sequential slices in generated volumes
3. Test zero-shot generalization by attempting translation between other modality pairs (e.g., T1→T2 MRI) without retraining