---
ver: rpa2
title: Agent-Enhanced Large Language Models for Researching Political Institutions
arxiv_id: '2503.13524'
source_url: https://arxiv.org/abs/2503.13524
tags:
- data
- congress
- policy
- agent
- bill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CongressRA, a large language model (LLM)
  agent designed to streamline political science research on the U.S. Congress.
---

# Agent-Enhanced Large Language Models for Researching Political Institutions

## Quick Facts
- arXiv ID: 2503.13524
- Source URL: https://arxiv.org/abs/2503.13524
- Authors: Joseph R. Loffredo; Suyeol Yun
- Reference count: 40
- One-line primary result: CongressRA LLM agent automates Binder's (1999) legislative gridlock measurement with greater efficiency than manual methods.

## Executive Summary
This paper introduces CongressRA, an LLM agent that combines retrieval-augmented generation with specialized tools to streamline political science research on the U.S. Congress. By integrating SQL databases for structured data, vector databases for semantic search, and API access to congressional records, CongressRA demonstrates how LLM agents can reduce labor and complexity in empirical research tasks. The agent successfully replicates Binder's gridlock methodology, identifying policy issues from news articles, matching them to legislation, and calculating gridlock scores across multiple Congresses with greater efficiency than traditional approaches.

## Method Summary
CongressRA employs an agentic RAG framework that integrates external data sources through function calling capabilities. The system combines SQL databases for structured legislative data, vector databases for semantic search of news articles and bill summaries, and API access to congressional records. The workflow follows a three-step process: (1) semantic search of news articles to identify salient policy issues and cluster them, (2) semantic search to match identified issues to relevant legislation, and (3) SQL/API lookup to verify enactment status. The agent dynamically chains these function calls to automate the entire gridlock measurement process from raw data to final score calculation.

## Key Results
- Successfully replicates Binder's (1999) legislative gridlock measurement methodology using automated LLM agents
- Demonstrates 3-step workflow: article clustering → bill matching → status verification for calculating gridlock scores
- Identifies need for manual threshold review despite automation, highlighting current limitations in autonomous parameter calibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agentic Retrieval-Augmented Generation (RAG) reduces hallucinations by grounding LLM reasoning in external, verifiable data sources.
- Mechanism: The system decouples reasoning (LLM) from knowledge storage, using "action-calling" to query SQL databases, vector databases, or APIs instead of relying on static pre-trained weights.
- Core assumption: External data sources (congress.gov, Voteview) are accurate, up-to-date, and accessible via defined interfaces.
- Evidence anchors: [abstract] "Agentic RAG... equips LLMs with action-calling capabilities for interaction with external knowledge bases"; [section] contrasts traditional LLMs' "static, pre-trained knowledge bases" with Agentic RAG's ability to "autonomously determine the most relevant sources to query."
- Break condition: If retrieval tools fail to access databases or return irrelevant chunks, the LLM will likely hallucinate despite the RAG architecture.

### Mechanism 2
- Claim: Modular function chaining allows LLMs to automate multi-step empirical workflows that exceed single-prompt complexity.
- Mechanism: The agent decomposes high-level prompts into discrete tool calls, maintaining context across steps to synthesize final results.
- Core assumption: The LLM possesses sufficient reasoning capability to correctly order dependent sub-tasks and interpret intermediate function call outputs.
- Evidence anchors: [abstract] Highlights "integrating retrieval-augmented generation (RAG) with specialized tools for data retrieval, preprocessing, and analysis"; [section] states, "LLM agents can dynamically chain function calls together and accomplish multi-step tasks without user intervention."
- Break condition: If prompt complexity exceeds model's context window or function call limits (e.g., OpenAI's 5-call limit), the workflow will stall or produce incomplete results.

### Mechanism 3
- Claim: Hybrid storage architectures (Vector + SQL) enable semantic flexibility while maintaining structural precision.
- Mechanism: Unstructured data (news, bill summaries) is stored as embeddings for semantic search, while structured data (roll calls, member scores) is stored in SQL for exact querying.
- Core assumption: Vector embeddings accurately capture semantic meaning of political text, and SQL schemas are robust enough to answer structured queries about legislative status.
- Evidence anchors: [abstract] The agent combines "SQL databases for structured data" and "vector databases for semantic search"; [section] details design choice: "Structured Data Retrieval with SQL" for exact outputs vs. "Semantic Search with Vector Databases" for context.
- Break condition: Semantic drift in vector search or schema rigidity in SQL results in data mismatch errors.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper explicitly contrasts "Agentic RAG" with fine-tuning. Understanding how RAG injects external context into the prompt window is crucial to seeing why this reduces the "static knowledge" problem.
  - Quick check question: How does injecting retrieved text into a prompt context differ from retraining the model on that text?

- Concept: **Vector Embeddings & Cosine Similarity**
  - Why needed here: The "Gridlock" application relies on matching news articles to bill summaries using semantic similarity. Understanding that text is converted to vectors and compared mathematically is crucial for debugging retrieval quality.
  - Quick check question: Why would a keyword search fail to match the concept "fiscal showdown" to a bill summary that only uses the phrase "debt limit increase"?

- Concept: **Function Calling / Tool Use**
  - Why needed here: The core of the "Agent" is the ability to execute code (functions) rather than just generate text. Understanding how an LLM outputs structured data to trigger an API call is essential.
  - Quick check question: In the CongressRA architecture, does the LLM write the SQL query itself, or does it call a predefined function that constructs the query?

## Architecture Onboarding

- Component map:
  - Core LLM (e.g., GPT-4) -> Orchestrator (LangChain/OpenAI Assistant API) -> Tools (get_bill_details, search_article_archive, get_nominate_score) -> Data Stores (SQL Database, Vector Database)

- Critical path:
  1. **Prompt Ingestion:** User submits complex query (e.g., "Calculate gridlock for 116th Congress")
  2. **Tool Selection:** Agent identifies need to find "salient issues" first, selects `search_article_archive`
  3. **Iterative Retrieval:** Agent retrieves articles, clusters them (internal reasoning), generates search terms for `search_bill_summaries`
  4. **Verification:** Agent takes bill IDs and uses `get_bill_actions` to check enactment status
  5. **Synthesis:** Agent calculates the ratio (Gridlock Score) and generates response

- Design tradeoffs:
  - **Freshness vs. Complexity:** Using APIs (congress.gov) ensures live data but adds latency and rate-limit risks compared to static SQL dumps
  - **Granularity vs. Token Limits:** Storing full text in vector DBs allows deep search but hits token limits during retrieval (Page 18 notes limiting retrieval to top 200 articles to manage tokens)
  - **Autonomy vs. Accuracy:** Giving agent discretion to set "similarity thresholds" can lead to false positives (Page 18 notes agent struggled to filter unrelated bills without manual review)

- Failure signatures:
  - **Thresholding Errors:** Agent retrieves bills with cosine similarity of 0.35 when valid cutoff was 0.40, leading to irrelevant legislation inclusion
  - **Context Overflow:** Attempting to process full text of 500 bills in single prompt crashes context window
  - **Tool Looping:** Agent repeatedly tries same SQL query with slight variations due to misunderstanding "null" result

- First 3 experiments:
  1. **Single-Step Retrieval:** Ask CongressRA to retrieve details for known bill (e.g., "H.R. 1, 117th Congress") to verify API tool connectivity and parsing
  2. **Semantic Search Test:** Submit vague query ("articles about voting rights") to vector database tool and inspect top 5 results for semantic relevance
  3. **Mini-Workflow Replication:** Attempt to replicate gridlock measurement for single Congress (e.g., 113th) to test function chaining logic before scaling to multi-Congress analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM agents autonomously calibrate semantic similarity thresholds to filter irrelevant results without requiring manual review by researchers?
- Basis in paper: [explicit] Authors state, "we found that the agent did not always set satisfactory thresholds to filter out unrelated bills. To maintain the accuracy of the measure, we still needed to manually review lists of retrieved bills."
- Why unresolved: Paper demonstrates current agent's failure to self-regulate this parameter effectively, offering no technical solution for automating threshold validation.
- What evidence would resolve it: Experiment showing agent dynamically adjusting thresholds to match human-judged relevance scores with high precision and recall.

### Open Question 2
- Question: Do gridlock scores generated by LLM agents correlate highly with human-coded measures across historical datasets?
- Basis in paper: [inferred] Authors clarify their generated values are "intended to highlight the scalability... rather than be a replication and reanalysis of Binder's findings," implying automated measurement validity remains untested.
- Why unresolved: Paper focuses on process replication and efficiency rather than output validation against gold-standard dataset.
- What evidence would resolve it: Statistical correlation analysis comparing CongressRA's gridlock scores to Binder's (1999) scores for overlapping historical period (1947-1996).

### Open Question 3
- Question: How can the opaque reasoning processes of the core LLM be standardized to ensure reproducible function-calling behavior?
- Basis in paper: [explicit] Authors note that "the actual LLM that is the core of the agentic framework still maintains its own reasoning capacity," creating transparency issue distinct from defined functions.
- Why unresolved: While predefined tools are transparent, model's internal decision-making logic regarding when and how to use them remains a "black box."
- What evidence would resolve it: Demonstrating identical queries yield identical decision paths and function calls across multiple trials, potentially using fixed seeds or open-weight models.

## Limitations

- **Unknown Implementation Details:** Specific system prompts, similarity thresholds, and agent decision logic are not provided, creating significant barriers to faithful reproduction
- **External Dependencies:** System performance depends on external APIs (congress.gov) and vector database services, introducing potential failure points beyond authors' control
- **Manual Oversight Required:** Agent currently requires manual review of threshold parameters and retrieved results to maintain accuracy, limiting full automation

## Confidence

- **High Confidence:** Core architecture of Agentic RAG combining SQL and vector databases is clearly specified and theoretically sound
- **Medium Confidence:** Three-step workflow (article clustering → bill matching → status verification) is described, but implementation details remain underspecified
- **Low Confidence:** Paper provides insufficient detail about prompt engineering, similarity thresholds, and agent decision-making logic to guarantee reproducible results

## Next Checks

1. Replicate single-Congress gridlock calculation (e.g., 113th) to test whether three-step workflow produces comparable results to Binder's methodology
2. Conduct sensitivity analysis by varying semantic similarity threshold (0.30-0.50) to assess impact on gridlock score stability
3. Test system's performance on simpler task (e.g., retrieving bill status for known issue) to isolate whether failures stem from RAG architecture or specific implementation choices