---
ver: rpa2
title: 'MINT: Multi-Vector Search Index Tuning'
arxiv_id: '2504.20018'
source_url: https://arxiv.org/abs/2504.20018
tags:
- query
- search
- index
- indexes
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework for index tuning in multi-vector
  search systems. It addresses the challenge of selecting efficient indexes for multi-vector
  queries across different features/modalities.
---

# MINT: Multi-Vector Search Index Tuning

## Quick Facts
- **arXiv ID:** 2504.20018
- **Source URL:** https://arxiv.org/abs/2504.20018
- **Reference count:** 40
- **Primary result:** Achieves 2.1× to 8.3× speedup in multi-vector search with recall constraints

## Executive Summary
MINT addresses the challenge of selecting optimal indexes for multi-vector search queries across different features/modalities. The framework includes a query planner that uses cost and recall estimators to find the best query plans, and a configuration searcher that uses beam search to select optimal index configurations. Experiments show that MINT achieves significant speedup compared to baselines while maintaining recall constraints.

## Method Summary
The framework trains linear cost models and logarithmic recall models on sampled data, then uses dynamic programming or search to find optimal query plans for specific configurations. A beam search algorithm explores the space of possible index configurations to find the best setup that minimizes probability-weighted latency while satisfying storage and recall constraints.

## Key Results
- Achieves 2.1× to 8.3× speedup compared to baseline methods
- Maintains recall constraints (e.g., 90% or 97% recall@100)
- Successfully selects multi-column indexes over single-column ones to reduce latency
- Outperforms exhaustive search while exploring fewer configurations

## Why This Works (Mechanism)
The approach works by accurately estimating the cost and recall of different query plans using learned models, then systematically exploring the configuration space to find optimal index setups. The combination of dynamic programming for query planning and beam search for configuration selection enables efficient exploration of a large search space.

## Foundational Learning

**Linear cost models** - Predict the number of distance computations needed for a query. Needed to estimate query latency without running expensive queries. Quick check: Verify linearity holds for ek ≥ 100 on sampled data.

**Logarithmic recall models** - Estimate the recall achieved by retrieving k items. Needed to predict recall without exhaustive evaluation. Quick check: Plot actual vs predicted recall to verify model accuracy.

**Beam search configuration** - Explores index configurations while maintaining diversity. Needed to balance exploration vs exploitation in large configuration spaces. Quick check: Verify selected configurations improve over random search.

## Architecture Onboarding

**Component map:** Data → Estimators → Query Planner → Configuration Searcher → Optimized Configuration

**Critical path:** Sample data → Train estimators → Plan queries → Search configurations → Deploy indexes

**Design tradeoffs:** Simple linear/log models vs complex ML models for speed vs accuracy; beam search vs exhaustive search for efficiency vs completeness.

**Failure signatures:** Estimator inaccuracy leads to suboptimal ek selection; beam search may miss optimal configurations with small beam width.

**First experiments:**
1. Validate estimator accuracy by comparing predicted vs actual numDist/recall on holdout sample
2. Test query planner on fixed configuration to verify plan quality
3. Run configuration searcher on simple workload to verify multi-column index selection

## Open Questions the Paper Calls Out

**Open Question 1:** Can the framework be applied to quantization-based or tree-based indexes without significant modification? The paper plans to extend Mint to other index types like IVF-PQ, but current estimators are designed for graph-based indexes.

**Open Question 2:** How much performance gain can be achieved by replacing heuristic models with learned ML-based estimators? The paper suggests developing more accurate estimators could yield additional performance improvements.

**Open Question 3:** How sensitive is query planner quality to sample size k' used in dynamic programming? The current implementation uses a small constant (5), but the impact on plan quality for complex queries is unexplored.

## Limitations

- Beam width parameter for configuration search is not explicitly stated in experiments
- Exact latency numbers depend on specific C++ index implementations not fully detailed
- Query workloads are semi-synthetic with unspecified seeds, making exact reproduction difficult

## Confidence

- **High confidence** in overall framework design and methodology
- **Medium confidence** in reproducing exact latency improvements due to implementation specifics
- **Medium confidence** in generalizability across different datasets and workloads

## Next Checks

1. Verify estimator accuracy by comparing predicted vs actual numDist and recall on holdout sample
2. Profile query planner runtime to confirm Algorithm 2 (DP) is used when |X| ≥ 4
3. Test configuration searcher on "Naïve" workload to verify multi-column index selection