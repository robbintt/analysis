---
ver: rpa2
title: 'SCALA: Split Federated Learning with Concatenated Activations and Logit Adjustments'
arxiv_id: '2405.04875'
source_url: https://arxiv.org/abs/2405.04875
tags:
- label
- learning
- local
- client
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes SCALA (Split Federated Learning with Concatenated
  Activations and Logit Adjustments) to address label distribution skew in split federated
  learning (SFL). SCALA tackles two types of skew: local (data heterogeneity across
  clients) and global (due to partial client participation).'
---

# SCALA: Split Federated Learning with Concatenated Activations and Logit Adjustments

## Quick Facts
- arXiv ID: 2405.04875
- Source URL: https://arxiv.org/abs/2405.04875
- Authors: Jiarong Yang; Yuan Liu
- Reference count: 40
- One-line primary result: Achieves up to 91.25% accuracy on CIFAR-10 under severe label skew via concatenated activations and logit adjustments

## Executive Summary
SCALA addresses label distribution skew in split federated learning by combining concatenated activations and logit adjustments. The method tackles both local skew (data heterogeneity across clients) and global skew (partial participation) through a two-pronged approach: centrally training the server-side model on concatenated client activations to recover missing classes, and applying logit adjustments to loss functions to handle long-tailed label distributions. Theoretical analysis shows sublinear convergence with reduced gradient dissimilarity, while experiments demonstrate significant accuracy improvements over baseline methods.

## Method Summary
SCALA modifies the standard split federated learning architecture by centrally training the server-side model on concatenated activations from participating clients, forming a virtual mini-batch that spans all observed classes. The method applies logit adjustments to both server and client loss functions, using log P(y) to equalize training signals across classes. The split point between client and server models is moved deeper into the network to reduce gradient dissimilarity caused by local label distribution skew. The training involves synchronized local iterations where clients compute activations, upload them to the server, and receive backpropagated gradients with client-specific logit adjustments.

## Key Results
- Achieves up to 91.25% accuracy on CIFAR-10 with ResNet-18 under severe label skew (α=2, β=0.05)
- Reduces gradient dissimilarity through deeper server-side splits, with accuracy decreasing monotonically as split point moves from S1 to S4
- Effectively mitigates both local and global skew, with participation ratio ρ=50% yielding 81.99% accuracy on CIFAR-100 compared to 74.67% at ρ=5%
- Sublinear convergence proven theoretically with gradient dissimilarity bounded by N_c and N_s terms

## Why This Works (Mechanism)

### Mechanism 1: Concatenated Activations for Missing Class Recovery
The server receives activations {A_k} from clients and concatenates them as A = ∪_k A_k, training w_s centrally on this combined batch. This creates a mini-batch whose class support is the union of all participating clients' classes, allowing the server to learn representations for classes absent from any single client. The core assumption is that client-side models produce sufficiently aligned feature representations for coherent joint training. Break condition occurs when extreme data heterogeneity causes feature drift, making concatenated activations incompatible.

### Mechanism 2: Logit Adjustment for Long-Tailed Label Distributions
The method adjusts pre-softmax logits by adding log P(y) to equalize training signals across classes. For high-frequency classes, log P(y) is large (less negative), reducing their effective logit margin; for low-frequency classes, the adjustment is smaller, amplifying their gradient signal. The server estimates the concatenated label distribution P_s(y) from participating clients each round. Break condition occurs when P_s(y) estimates are highly inaccurate due to extreme participation bias, mis-calibrating the loss.

### Mechanism 3: Gradient Dissimilarity Reduction via Server-Side Depth
Increasing the number of server-side model layers N_s reduces gradient dissimilarity caused by local label distribution skew. Client-side layers contribute the N_c/ρ√T · √I·κ²_max term to convergence error, where κ²_max bounds gradient dissimilarity. Moving more layers to the server removes them from this error term. Experiments confirm deeper server-side splits yield higher accuracy, with accuracy dropping ~10-15% from S1 to S4.

## Foundational Learning

- **Concept: Split Federated Learning (SFL) Architecture**
  - Why needed here: SCALA modifies the standard SFL training loop; understanding the baseline forward-backward flow across the cut layer is prerequisite to grasping where concatenation and logit adjustment inject.
  - Quick check question: In standard SFL, does the server-side model update once per global iteration or once per local iteration? How does SCALA differ?

- **Concept: Label Distribution Skew Taxonomy (Local vs. Global)**
  - Why needed here: The paper explicitly separates local skew (data heterogeneity across clients, causing missing classes) from global skew (partial participation, causing long-tailed concatenated distributions); each mechanism targets a different skew type.
  - Quick check question: If 10 clients each have data from only 2 of 10 classes, and all 10 clients participate every round, which skew type is present? What if only 2 clients participate and they share the same 2 classes?

- **Concept: Balanced Error vs. Misclassification Error**
  - Why needed here: Logit adjustment optimizes for balanced error (average per-class accuracy) rather than standard cross-entropy's frequency-weighted error; this explains the trade-off where high-frequency class accuracy may drop.
  - Quick check question: A model achieves 99% accuracy on class A (90% of data) and 50% on class B (10% of data). What is the standard accuracy? What is the balanced accuracy? Which does logit adjustment target?

## Architecture Onboarding

- **Component map:**
  - Client-side model (w_c) -> Server-side model (w_s) -> Classifier head
  - Client processes raw data -> outputs activations
  - Server receives concatenated activations -> outputs predictions
  - Logit adjustment module -> computes P_s(y) from Y, adjusts loss
  - Aggregation module -> FedAvg-weighted averaging of client-side models

- **Critical path:**
  1. Server selects subset C_t, broadcasts w_c^t and per-client batch sizes B_k
  2. Each client k samples minibatch D̃_k, computes A_k = h(w_c; D̃_k), uploads A_k + Y_k
  3. Server concatenates activations, computes P_s(y), updates w_s using logit-adjusted loss — every local iteration
  4. Server computes backpropagated gradients G_k with client-specific logit adjustment, sends to clients
  5. Clients update w_c,k via chain rule: w_c,k^{i+1} = w_c,k^i - η · G_k · ∇h(w_c,k^i)
  6. After I local iterations, server aggregates client-side models → w_c^{t+1}

- **Design tradeoffs:**
  - **Split point depth**: S1 (server-heavy) → higher accuracy, more server compute; S4 (client-heavy) → lower accuracy, less server load. Fig. 7 shows ~10-15% accuracy drop from S1 to S4.
  - **Participation ratio ρ**: Higher ρ improves label coverage in concatenated batch but increases communication. Table 2 shows ρ=5% → 74.67% vs ρ=50% → 81.99% under β=0.05.
  - **Local iterations I**: More iterations reduce communication rounds but risk divergence. Table 5: I=10 → 80.33%, 5.07MB vs I=30 → 81.39%, 15.08MB per global iteration.

- **Failure signatures:**
  - **Persistent missing classes**: If a class never appears in any participating client across all rounds, concatenation cannot recover it. Manifests as near-zero per-class accuracy in Fig. 5.
  - **Gradient inconsistency explosion**: When client-side models diverge (ν² bound violated), training oscillates or converges to poor local minima.
  - **Logit overcorrection**: Excessive adjustment can collapse high-frequency class accuracy; monitor per-class metrics, not just overall accuracy.

- **First 3 experiments:**
  1. **Split point ablation**: On CIFAR-10 with AlexNet, test split points S1–S4 under α=2 quantity skew. Confirm accuracy decreases as N_c increases (replicate Fig. 7a). Validates gradient dissimilarity mechanism.
  2. **Mechanism ablation**: Compare SplitFedV1 (baseline), CA-SFL (concatenation only), LLA-SFL (local logit only), SCALA (full) on CIFAR-10 under β=0.05. Replicate Fig. 4b to isolate each component's contribution.
  3. **Participation robustness**: Sweep ρ ∈ {5%, 10%, 20%, 50%} on CIFAR-100 with ResNet-18 under β=0.05. Replicate Table 4 pattern to validate global skew mitigation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can activation compression be integrated into SCALA to reduce communication overhead without degrading the efficacy of concatenated activations?
- Basis in paper: [explicit] The "Limitations" section identifies the reliance on frequent activation exchange causing "substantial communication overhead" and suggests designing efficient variants via compression as future work.
- Why unresolved: Compression introduces noise that may distort the feature representations, potentially breaking the central server's ability to correct missing classes through concatenation.
- What evidence would resolve it: An ablation study applying quantization or sparsification to the smashed data, measuring the trade-off between communication cost and accuracy on non-IID datasets.

### Open Question 2
- Question: How can privacy-preserving mechanisms secure the required transmission of labels and activations without disrupting SCALA's convergence guarantees?
- Basis in paper: [explicit] The paper notes that "SCALA requires the transmission of labels, which poses a risk of privacy leakage," and identifies incorporating privacy mechanisms as a necessary research direction.
- Why unresolved: Techniques like differential privacy add noise that may conflict with the precise logit adjustments required to handle the long-tailed distribution of partial participation.
- What evidence would resolve it: Integration of a privacy mechanism (e.g., DP or encryption) into SCALA with a theoretical and empirical analysis of the privacy-utility trade-off.

### Open Question 3
- Question: How does SCALA perform in asynchronous environments where clients have heterogeneous availability and computational speeds?
- Basis in paper: [inferred] The method assumes synchronous execution where "all participating clients synchronously execute local iterations," despite acknowledging that clients have heterogeneous capabilities.
- Why unresolved: Asynchrony results in stale or incomplete concatenated batches at the server, potentially invalidating the estimated global label distribution used for logit adjustment.
- What evidence would resolve it: Experimental results evaluating SCALA under asynchronous client scheduling with variable delays and stragglers.

## Limitations
- The theoretical convergence proof relies on assumptions of bounded gradient dissimilarity and client model alignment that may break under extreme data heterogeneity.
- The effectiveness of concatenated activations depends critically on client-side models producing compatible feature representations, which is not validated in the corpus and could fail when clients have vastly different data distributions.
- The logit adjustment mechanism assumes accurate estimation of the concatenated label distribution P_s(y), but the paper does not quantify estimation error under severe participation skew (low ρ).

## Confidence

- **High Confidence**: The gradient dissimilarity reduction mechanism (split point depth effects) is well-supported by both theory and experimental evidence (Fig. 7). The overall training pipeline and architecture are clearly specified.
- **Medium Confidence**: The concatenation mechanism's effectiveness is theoretically sound but lacks corpus validation for the specific use case. The logit adjustment's impact on balanced error is mathematically proven but not empirically isolated from other factors.
- **Low Confidence**: The scalability of the approach to real-world datasets with hundreds of classes and thousands of clients is not demonstrated. The communication overhead trade-offs under varying participation ratios need more thorough analysis.

## Next Checks

1. **Feature alignment stress test**: Evaluate SCALA's performance when client-side models are deliberately trained with different initialization seeds or architectures to quantify the breaking point of the concatenation mechanism.
2. **Estimation error analysis**: Measure the impact of inaccurate P_s(y) estimation by injecting noise into the concatenated label distribution and tracking accuracy degradation across different skew levels.
3. **Communication overhead benchmarking**: Compare the actual bandwidth usage of SCALA against baseline methods across varying participation ratios and local iteration counts to validate the claimed efficiency gains.