---
ver: rpa2
title: 'CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture
  Search'
arxiv_id: '2509.25862'
source_url: https://arxiv.org/abs/2509.25862
tags:
- hardware
- search
- accuracy
- neural
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CIMNAS addresses the challenge of co-optimizing neural network
  models and Compute-In-Memory (CIM) hardware for energy-efficient AI processing.
  Manual tuning is impractical due to the vast number of interdependent software and
  hardware parameters.
---

# CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search

## Quick Facts
- arXiv ID: 2509.25862
- Source URL: https://arxiv.org/abs/2509.25862
- Authors: Olga Krestinskaya; Mohammed E. Fouda; Ahmed Eltawil; Khaled N. Salama
- Reference count: 40
- Primary result: Joint search of neural network and CIM hardware achieves 90.1×-104.5× EDAP reduction vs baselines

## Executive Summary
CIMNAS addresses the challenge of co-optimizing neural network models and Compute-In-Memory (CIM) hardware for energy-efficient AI processing. Manual tuning is impractical due to the vast number of interdependent software and hardware parameters. The framework introduces a joint search approach that simultaneously optimizes neural network architectures, quantization policies, and CIM hardware parameters across device-, circuit-, and architecture-levels.

The method employs a genetic algorithm to explore a massive search space of 9.9×10^85 possible configurations, using a pre-trained supernetwork-based accuracy predictor to efficiently evaluate performance. Hardware metrics are assessed using the CiMLoop simulator, which leverages histograms of quantized layer inputs and weights for fast, accurate estimation. Evaluated on ImageNet using MobileNet, CIMNAS achieves significant improvements in energy efficiency, area efficiency, and EDAP while maintaining competitive accuracy.

## Method Summary
CIMNAS is a joint optimization framework that co-searches neural network architectures, quantization policies, and CIM hardware parameters. The framework uses a genetic algorithm to explore a search space of 9.9×10^85 configurations, evaluating candidates through a pre-trained supernetwork-based accuracy predictor and the CiMLoop simulator. The search simultaneously considers software parameters (layers, kernels), quantization policies (bit-widths), and hardware parameters (device/circuit/architecture levels). The optimization targets a multi-objective cost function that minimizes EDAP while maintaining accuracy and adhering to area constraints.

## Key Results
- Achieves 90.1× to 104.5× reduction in energy-delay-area product (EDAP) compared to baseline models
- Improves energy efficiency (TOPS/W) by 4.68× to 4.82× while maintaining 73.81% accuracy
- Enhances area efficiency (TOPS/mm²) by 11.3× to 12.78×
- Demonstrates robustness across different models, hardware configurations, and technology nodes (up to 819.5× EDAP reduction on SRAM-based ResNet50)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly searching software, quantization, and hardware parameters avoids the sub-optimal local minima inherent in sequential, multi-stage optimization.
- Mechanism: The framework uses an evolutionary algorithm to explore a massive search space of $9.9 \times 10^{85}$ configurations in parallel. By treating all parameters as a single genome, crossover and mutation operations can simultaneously adjust, for example, a neural network's layer count and the CIM architecture's crossbar size, discovering non-obvious synergies.
- Core assumption: The primary bottleneck in CIM design is the complex, non-linear interdependence between software and hardware parameters, which cannot be decomposed into independent sub-problems without performance loss.
- Evidence anchors:
  - [abstract] "CIMNAS simultaneously searches across software parameters, quantization policies, and a broad range of hardware parameters... incorporating device-, circuit-, and architecture-level co-optimizations."
  - [PAGE 2, Section I] "...CIMNAS avoids sub-optimal solutions caused by independent or sequential tuning and mitigates the risk of local minima."
  - [PAGE 4, Section III-A] "...most parameters influence all performance metrics, making joint optimization essential to avoid local minima."
- Break condition: If the search space were nearly decomposable (i.e., software and hardware parameters were largely independent), a faster, sequential search would find the global optimum.

### Mechanism 2
- Claim: The efficiency of the search is maintained by replacing costly model fine-tuning and simulation with a supernetwork-based accuracy predictor and a histogram-driven hardware simulator.
- Mechanism:
    1.  **Accuracy:** Instead of retraining each candidate, a lightweight predictor, pre-trained on a full-precision "Once-for-all" supernetwork, estimates the performance of a quantized model.
    2.  **Hardware:** Instead of full, cycle-accurate simulation, the CiMLoop simulator uses pre-computed histograms of quantized layer inputs and weights. This allows for rapid, data-dependent estimates of energy, delay, and area.
- Core assumption: The relative ranking of candidate designs provided by the fast predictor and simulator is sufficiently consistent with their true performance, allowing the evolutionary algorithm to converge correctly.
- Evidence anchors:
  - [abstract] "...using a pre-trained supernetwork-based accuracy predictor to efficiently evaluate performance. Hardware metrics are assessed using the CiMLoop simulator, which leverages histograms..."
  - [PAGE 6, Section III-C1] "...it takes just 5 GPU seconds per network, directly fine-tuning each quantized design would require at least 4-5 GPU hours."
  - [PAGE 6, Section III-C2] "CiMLoop leverages histograms... This approach significantly reduces simulation time... Each design evaluation takes about 30 seconds on a 64-core CPU..."
- Break condition: If the accuracy predictor's error rate is high enough to frequently misrank candidates, or if the simulator's simplifications fail to capture critical performance bottlenecks, the search will converge on sub-optimal designs.

### Mechanism 3
- Claim: The search is guided towards practical, fabricable designs by incorporating a multi-objective cost function that explicitly penalizes large on-chip area.
- Mechanism: The optimization objective is not just accuracy, but a function $f_\alpha$ that minimizes Energy $\times$ Delay $\times$ Area / Accuracy. This is combined with a hard constraint $A_\alpha \le A_{constr}$, which discards any design exceeding a specified area (e.g., 800mm²).
- Core assumption: The primary trade-off in CIM design is between efficiency (EDAP) and accuracy, and that unconstrained optimization will naturally tend towards impractically large, energy-efficient designs.
- Evidence anchors:
  - [PAGE 5, Section III-B2] "...unconstrained optimization often results in CIM designs with infeasibly large on-chip areas. To address this, CIMNAS focuses on area-constrained optimization..."
  - [PAGE 5, Section III-B2] Eq. 1: $f_\alpha = f(E_\alpha, D_\alpha, A_\alpha, Acc_\alpha)$ s.t. $A_\alpha \le A_{constr}$
  - [PAGE 7, Section IV-A] "Hardware evaluations were performed... with an area constraint of $A_{constr} = 800mm^2$, reflecting a reasonable die size for single-chip fabrication."
- Break condition: If the best performance can be achieved within a small area, or if the area penalty is too weak, the search may still generate designs that are not cost-effective to manufacture.

## Foundational Learning

- **Concept: Compute-in-Memory (CIM) Architecture Hierarchy**
  - Why needed here: The search space is built on a specific hardware hierarchy (device $\to$ crossbar macro $\to$ tile $\to$ chip). Understanding this is essential to interpreting the optimized parameters.
  - Quick check question: Can you diagram the hierarchical relationship between a single RRAM cell, a crossbar macro, a tile, and the global buffer?

- **Concept: Evolutionary Algorithms (EA) for NAS**
  - Why needed here: This is the core search engine. Understanding concepts like population, generations, crossover, and mutation is necessary to tune the search.
  - Quick check question: In a genetic algorithm, what is the role of the "crossover probability" ($P_c$) and "mutation probability" ($P_m$) in balancing exploration vs. exploitation?

- **Concept: Once-for-All (OFA) Supernetwork & Accuracy Predictors**
  - Why needed here: The entire search efficiency relies on avoiding costly retraining. One must understand what a supernetwork is and how an accuracy predictor is trained from it.
  - Quick check question: Instead of training a new model for each candidate, what single, large network is trained in advance to enable efficient evaluation?

## Architecture Onboarding

- **Component map:** Software search space ($S_M$) -> Quantization search space ($S_Q$) -> Hardware search space ($S_H$) -> Area constraint filter -> CiMLoop simulator -> Accuracy predictor -> Objective function calculator -> Genetic algorithm population

- **Critical path:** Sampling a candidate $\to$ Filtering by area $\to$ Generating quantized histograms $\to$ Running simulator & predictor $\to$ Calculating objective score $\to$ Ranking & evolving the population

- **Design tradeoffs:**
  - **Search Time vs. Accuracy:** The predictor is fast but an approximation. Final fine-tuning is needed for selected top candidates.
  - **Exploration vs. Exploitation:** Controlled by EA hyperparameters ($P_c, P_m, \eta_c, \eta_m$). High mutation = more exploration but slower convergence.
  - **Objective Complexity:** A simple EDAP/Accuracy objective is stable but may not capture all design priorities. The priority-based function offers fine-grained control at the cost of more tuning.

- **Failure signatures:**
  - **Convergence to local minima:** Often caused by an insufficiently diverse initial population or poor EA hyperparameter tuning.
  - **Accuracy degradation:** Can happen if the accuracy predictor is not well-calibrated or if the quantization search space is too aggressive.
  - **Impractical designs:** If the area constraint is poorly defined or if the search overfits to the simulator's model.

- **First 3 experiments:**
  1.  **Baseline Verification:** Run a search on a simple, well-known network (e.g., MobileNetV2) with a fixed hardware configuration to validate the accuracy predictor and basic EA loop.
  2.  **Hardware Sweep:** Fix the model and run a search only over the hardware space ($S_H$) to understand the sensitivity of EDAP to parameters like crossbar size and tile count.
  3.  **Full Joint Search:** Execute the full CIMNAS pipeline on a target application, comparing the top designs against a two-stage (software-then-hardware) baseline to quantify the gains from joint optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prediction models be developed to adapt CIMNAS search results to new hardware technologies or technology nodes without rerunning the entire search process?
- Basis in paper: [explicit] The conclusion states the authors plan to "develop prediction models to adapt search results to different hardware technologies, eliminating the need to rerun the search when migrating to new hardware and a new technology node."
- Why unresolved: Currently, migrating to a new node (e.g., from 32nm to 7nm) or technology requires re-running the genetic algorithm and simulations because the hardware-performance relationships are re-evaluated from scratch.
- What evidence would resolve it: A transfer learning mechanism or predictor that maps optimization results from one node to another while maintaining EDAP efficiency and accuracy without re-simulation.

### Open Question 2
- Question: Can the CIMNAS framework be effectively extended to support workloads beyond image classification, such as object detection or natural language processing?
- Basis in paper: [explicit] The conclusion notes: "As part of future work, we aim to extend the algorithm to support a broader range of workloads and tasks beyond image classification."
- Why unresolved: The current study validates the framework exclusively on ImageNet using CNNs (MobileNet, ResNet). Different tasks involve varying layer types, memory access patterns, and accuracy metrics that may not be captured by the current accuracy predictor or hardware mapper.
- What evidence would resolve it: Successful optimization of a non-classification task (e.g., Object Detection on COCO) demonstrating similar EDAP reductions and accuracy retention compared to baseline models.

### Open Question 3
- Question: How do device non-idealities and noise variations influence the optimal neural architecture and hardware configurations selected by CIMNAS?
- Basis in paper: [inferred] Section III-A states that while parameters like bits per cell impact accuracy, "the topic [of device non-idealities and noise variations] is beyond the scope of this work."
- Why unresolved: The current accuracy predictor relies on quantized models but does not appear to explicitly model physical hardware noise or non-idealities in the search loop, potentially leading to optimistic accuracy estimates for the selected hardware configurations.
- What evidence would resolve it: An experiment where noise models are integrated into the fitness evaluation, showing whether the "optimal" architectures change significantly compared to the noise-free baseline.

## Limitations

- The framework's impressive gains are currently validated through simulation rather than fabricated hardware, leaving uncertainty about real-world performance
- The accuracy predictor may not generalize well to architectures discovered by joint search that differ significantly from the OFA supernetwork training distribution
- The search assumes perfect knowledge of hardware characteristics and does not explicitly model device non-idealities or noise variations that could impact actual performance

## Confidence

- **High:** The core mechanism of joint optimization avoiding local minima is well-established in multi-objective optimization theory
- **Medium:** The search efficiency approach (accuracy predictor + CiMLoop simulator) is validated but relies on simplifications that may miss edge cases
- **Low:** The practical hardware results (area constraints, TOPS/W improvements) are simulated metrics requiring fabrication and benchmarking for confirmation

## Next Checks

1. **Accuracy Predictor Validation:** Generate 50-100 candidate architectures from the joint search, fine-tune them for 5-10 epochs, and compare the true ImageNet accuracy against the predictor's estimates. Report the mean absolute error and correlation coefficient.

2. **Cross-Technology Robustness:** Apply CIMNAS to a different CIM technology (e.g., SRAM-based arrays) with a modified hardware search space. Compare the EDAP/Acc Pareto front to the RRAM results to verify the framework's adaptability.

3. **End-to-End Co-Design Benchmark:** Implement a "sequential" baseline where a NAS tool (e.g., OFA) is run first to find an optimal software model, then a hardware design is optimized for that fixed model. Compare the final EDAP and accuracy of the sequential design to the top CIMNAS design to isolate the benefit of joint optimization.