---
ver: rpa2
title: 'Bridging Text and Video Generation: A Survey'
arxiv_id: '2510.04999'
source_url: https://arxiv.org/abs/2510.04999
tags:
- video
- arxiv
- generation
- preprint
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of text-to-video (T2V)
  generation models, covering their evolution from early GANs and VAEs to modern diffusion-based
  architectures. It systematically reviews model methodologies, training datasets,
  and configurations, including hardware specifications and hyperparameters.
---

# Bridging Text and Video Generation: A Survey

## Quick Facts
- arXiv ID: 2510.04999
- Source URL: https://arxiv.org/abs/2510.04999
- Reference count: 40
- One-line primary result: Comprehensive survey of text-to-video (T2V) generation models, covering evolution, architectures, training, and evaluation

## Executive Summary
This survey comprehensively reviews text-to-video generation models, tracing their evolution from early GANs and VAEs to modern diffusion-based architectures. It systematically covers model methodologies, training datasets, and configurations, including hardware specifications and hyperparameters. The paper also outlines standard evaluation metrics and benchmarks, discusses human evaluation approaches, and introduces emerging evaluation frameworks like VBench for more perception-aligned assessment. Key challenges such as alignment, long-range coherence, and computational efficiency are identified, along with future research directions including dataset enrichment, architectural optimization, and potential applications across education, accessibility, marketing, and cultural preservation.

## Method Summary
The survey synthesizes existing literature on T2V generation without presenting original experimental results. It summarizes model architectures, training configurations (e.g., CogVideo on A100 x 32, Batch 256, LR 1e-4), and evaluation protocols. For reproduction, a representative diffusion-based model like Latent-Shift could be implemented by extending a pretrained T2I latent diffusion model with temporal attention layers, training on video-text pairs from an alternative dataset (e.g., Pexels or Kinetics-600), and using Adam optimizer with learning rates between 1e-5 to 1e-4. However, exact architectural details and data preprocessing steps are not fully specified, requiring substitutions and assumptions.

## Key Results
- T2V models evolved from GANs/VAEs to diffusion-based architectures, with most modern models leveraging pretrained T2I diffusion models as starting points.
- Temporal coherence remains a major challenge, with flickering, object morphing, and motion freezing being common failure modes.
- Evaluation metrics like FVD, CLIP-SIM, and emerging frameworks like VBench are used, but no standardized metric fully captures human perception of video quality.

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Decomposition for Temporal Coherence
- Separating content (static) and motion (dynamic) representations in latent space appears to improve temporal consistency across frames.
- MoCoGAN decomposes latent space into content subspace (constant) and motion subspace (frame-specific via RNN); VideoFusion emits base noise (shared) and residual noise (frame-specific).
- Core assumption: Temporal artifacts arise from treating each frame independently; shared representations enforce consistency.
- Evidence: [section 3.1.1] MoCoGAN uses decomposed GAN architecture; [section 3.3.2] VideoFusion uses hierarchical noise decomposition.
- Break condition: Decomposition may fail when scenes require abrupt transitions or multiple interacting objects with independent motion patterns.

### Mechanism 2: Pseudo-3D Extensions for Spatiotemporal Processing
- Extending 2D image models with inflated temporal operations (pseudo-3D convolutions, temporal attention) allows leveraging pretrained T2I knowledge while adding temporal modeling.
- 2D kernels are expanded to include temporal dimensions; temporal shift modules move features across time without adding parameters; temporal attention layers capture long-range dependencies.
- Core assumption: Pretrained T2I models encode useful visual priors that transfer to video with minimal modification.
- Evidence: [section 3.3.1] Make-A-Video uses pseudo-3D convolution and attention layers; [section 3.3.3] Latent-Shift uses temporal shift module.
- Break condition: Pure inflation approaches may struggle with long-term dependencies beyond local temporal windows; memory scales linearly with sequence length.

### Mechanism 3: Hierarchical Generation for Long-Form Video
- Multi-stage generation (keyframes → interpolation → super-resolution) enables longer, higher-quality videos by decomposing the problem hierarchically.
- Generate sparse keyframes first, then interpolate intermediate frames, then upsample spatially—each stage handles a narrower aspect of the full problem.
- Core assumption: Temporal structure can be planned at low resolution/low framerate before detail is added.
- Evidence: [section 3.3.5] LaVie uses Base T2V model → Temporal Interpolation → Video Super-Resolution; [section 3.3.11] Pyramidal Flow uses spatial and temporal pyramids.
- Break condition: Error accumulation across stages; keyframe planning may miss fine-grained motion that cannot be recovered by interpolation.

## Foundational Learning

- Concept: **Denoising Diffusion Probabilistic Models (DDPMs)**
  - Why needed here: Most modern T2V models are diffusion-based; understanding forward/reverse processes, noise schedules, and conditioning (classifier-free guidance) is prerequisite.
  - Quick check question: Can you explain why classifier-free guidance adjusts predictions as `(1+w)ε_θ(x_t,t|y) - w·ε_θ(x_t,t)` and what `w` controls?

- Concept: **Variational Autoencoders (VAEs) and VQ-VAEs**
  - Why needed here: Latent diffusion models require understanding how VAEs compress video to latent space; discrete tokenization (VQ-VAE) underpins transformer-based approaches.
  - Quick check question: What does the KL divergence term in the ELBO objective regularize, and why does this matter for sampling from the prior?

- Concept: **Temporal Attention and Positional Encodings**
  - Why needed here: Capturing long-range temporal dependencies requires understanding how attention extends across time; 3D positional encodings (RoPE, 3D-RoPE) are common in DiT architectures.
  - Quick check question: How does 3D shifted window attention (CogVideoX) differ from full temporal attention in computational cost and receptive field?

## Architecture Onboarding

- Component map: VAE/VQ-VAE compresses video to latents -> Denoising backbone (U-Net or DiT) with temporal layers -> Conditioning (text encoder + cross-attention) -> VAE decode (optional super-resolution)

- Critical path:
  1. Pretrain or load T2I latent diffusion model
  2. Inflate with temporal layers (initialize temporal weights to near-zero or copy from spatial)
  3. Train on video-text pairs with joint image+video objective to prevent catastrophic forgetting
  4. Add interpolation/super-resolution stages if cascaded generation needed

- Design tradeoffs:
  - **U-Net vs. DiT backbone**: U-Net better studied for diffusion; DiT scales better but requires more data/compute
  - **Full attention vs. windowed attention**: Full captures global dependencies but O(T²); windowed efficient but limited receptive field
  - **Latent vs. pixel diffusion**: Latent dramatically reduces compute but may lose fine detail; pixel-space preserves detail but is expensive
  - **Training-free vs. trained temporal layers**: Training-free (Free-Bloom, FIFO-Diffusion) enables zero-shot but quality gap vs. trained

- Failure signatures:
  - **Temporal flickering**: Background/objects jitter between frames; indicates weak temporal attention or insufficient shared latent structure
  - **Object morphing/drifting**: Identity changes over time; indicates missing reference-frame conditioning or weak spatial attention
  - **Motion freezing**: Static output despite motion prompt; indicates temporal modules not activated or text-motion misalignment
  - **Video-text misalignment**: Generated content doesn't match prompt; indicates weak cross-attention or inadequate text encoder

- First 3 experiments:
  1. **Sanity check**: Load pretrained T2I LDM, generate single frames from prompts—verify spatial quality before adding temporal complexity
  2. **Temporal inflation ablation**: Add temporal layers with different initialization strategies (zero, copy-spatial, small random); compare FVD on a held-out video set
  3. **Guidance scale sweep**: Test classifier-free guidance scales (w=1.5, 3.0, 7.5, 15) and evaluate tradeoff between text-alignment (CLIP-SIM) and diversity/artifacts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large-scale, copyright-free synthetic video datasets generated from game engines (e.g., Unity, Unreal) effectively replace real-world video datasets for training T2V models while maintaining or improving generation quality?
- Basis in paper: [explicit] Section 6.1 explicitly proposes synthesizing datasets using game engines to overcome copyright, scale, and quality limitations of existing datasets like WebVid-10M and HowTo100M, which have become inaccessible due to legal restrictions.
- Why unresolved: No systematic comparison exists between models trained on synthetic game-engine data versus real-world data across standard benchmarks (UCF-101, MSR-VTT, Kinetics-600).
- What evidence would resolve it: A controlled study training identical T2V architectures on synthetic vs. real datasets, evaluated on FVD, CLIP-SIM, and VBench dimensions.

### Open Question 2
- Question: What architectural innovations are required to generate videos longer than a few seconds while maintaining temporal coherence, spatial consistency, and physical plausibility?
- Basis in paper: [explicit] Section 6.2 states that "models tend to produce only short output videos" and "most generated videos suffer from temporal and spatial incoherence: objects often shift, appear, or disappear unnaturally."
- Why unresolved: Current approaches like FIFO-Diffusion and Pyramidal Flow address long-form generation but trade off quality, and no unified solution achieves both long duration and high coherence.
- What evidence would resolve it: New architectures producing videos exceeding 30-60 seconds with measurable temporal consistency (e.g., VBench motion smoothness, subject consistency scores above 0.9).

### Open Question 3
- Question: How can evaluation metrics be standardized to capture human perception of video quality across semantic alignment, motion realism, and aesthetic quality dimensions?
- Basis in paper: [explicit] Section 5.3 states that conventional metrics "fundamentally cannot capture key qualitative aspects that match human perception" and are "not able to quantify key human-centered characteristics such as identity preservation, motion fluidity, and temporal stability."
- Why unresolved: VBench is proposed as a solution but adoption is not yet universal; correlation between VBench's 16 dimensions and human preference across diverse applications remains under-validated.
- What evidence would resolve it: Large-scale human evaluation studies correlating VBench dimension scores with human rankings across multiple T2V models and prompt categories.

## Limitations

- The survey synthesizes existing literature rather than presenting original experimental results, making independent verification difficult.
- Specific architectural details, hyperparameters, and exact training procedures for individual models are often not fully specified.
- Dataset availability issues (e.g., inactive WebVid-10M links) mean replication may require substituting datasets, introducing variability in performance comparisons.

## Confidence

- **High**: The historical progression from GANs/VAEs to diffusion-based models is well-established and clearly described.
- **Medium**: The identified mechanisms (latent space decomposition, pseudo-3D extensions, hierarchical generation) are plausible and supported by references, but their relative effectiveness depends on specific implementations and benchmarks.
- **Low**: Quantitative performance comparisons across models are uncertain due to potential inconsistencies in evaluation settings and data.

## Next Checks

1. **Dataset Substitution Impact**: Replicate a diffusion-based T2V model (e.g., Latent-Shift) using an alternative dataset (e.g., Pexels or Kinetics-600) and compare FVD/CLIP-SIM scores to published results.
2. **Temporal Module Ablation**: Train a base diffusion model with and without temporal attention/shift layers; measure the impact on temporal consistency metrics (FVD, user study scores).
3. **Guidance Scale Sensitivity**: Sweep classifier-free guidance scales (w=1.5, 3.0, 7.5, 15) on a held-out video set and quantify the tradeoff between text alignment (CLIP-SIM) and motion realism (user ratings).