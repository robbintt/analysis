---
ver: rpa2
title: 'FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference
  Benchmarking'
arxiv_id: '2504.16188'
source_url: https://arxiv.org/abs/2504.16188
tags:
- financial
- premise
- hypothesis
- language
- finnli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FinNLI, a benchmark dataset for financial
  Natural Language Inference (NLI) across multiple financial text genres. The dataset
  addresses the gap in domain-specific NLI resources by sampling premises from real-world
  financial documents and generating hypotheses using LLMs, with expert annotation
  ensuring high-quality test data.
---

# FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking

## Quick Facts
- arXiv ID: 2504.16188
- Source URL: https://arxiv.org/abs/2504.16188
- Reference count: 40
- Key outcome: Introduces FinNLI benchmark dataset for financial NLI across multiple genres; shows domain shift significantly degrades general-domain NLI performance

## Executive Summary
This paper introduces FinNLI, a benchmark dataset for financial Natural Language Inference (NLI) across multiple financial text genres. The dataset addresses the gap in domain-specific NLI resources by sampling premises from real-world financial documents and generating hypotheses using LLMs, with expert annotation ensuring high-quality test data. Evaluations show that domain shift significantly impacts performance, with the highest Macro F1 scores being 74.57% for fine-tuned PLMs and 78.62% for LLMs, indicating the dataset's difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. The results highlight substantial room for improvement in financial NLI, underscoring the need for better models capable of handling financial reasoning and domain-specific language.

## Method Summary
The authors create FinNLI by sampling premises from SEC Filings, Annual Reports, and Earnings Calls, then generate hypotheses using LLMs with conditional prompting based on roles and writing styles. Expert annotators validate the test set labels. The dataset includes 21,304 pairs with 3,304 expert-annotated test instances. Z-filtering is applied to reduce spurious correlations, and iterative prompt refinement is used based on NLI model feedback and expert annotations. Models are evaluated using Macro F1 due to class imbalance in the test set.

## Key Results
- General-domain models achieve ~70% accuracy on FinNLI vs. ~90% on MNLI, demonstrating significant domain shift
- Highest Macro F1 scores: 74.57% for fine-tuned PLMs, 78.62% for LLMs
- Instruction-tuned financial LLMs (FinMA) underperform general models (Llama-2-7B), with FinMA 7B at 23.20% vs. Llama-2-7B-chat at 34.87%
- Neutral-to-entailment confusion accounts for 56% of errors from Llama 3.1 70B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Real-world financial premises combined with synthetic LLM-generated hypotheses produce challenging NLI examples that expose model weaknesses on domain-specific reasoning.
- **Mechanism:** Premises sampled from SEC Filings, Annual Reports, and Earnings Calls capture authentic financial language complexity (formal terminology, numerical data, colloquial speech patterns). LLMs generate hypotheses conditioned on roles and writing styles, creating diverse reasoning challenges. Expert annotation validates quality.
- **Core assumption:** LLMs can generate sufficiently diverse and challenging hypotheses when properly prompted, and experts can reliably identify incorrect synthetic labels.
- **Evidence anchors:**
  - [abstract] "FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance."
  - [Section 3.2] "To encourage stylistic variations, we use conditional prompting with role-playing and treat writing style as an attribute dimension."
  - [Section 4.1] "The overall agreement rate for the FinNLI dataset is noticeably lower than the 84.97% agreement reported for general-domain synthetic NLI datasets such as GNLI."
- **Break condition:** If premise sampling fails to capture financial language diversity (incomplete sentences, tables), or if LLMs generate hypotheses with artifacts that survive z-filtering, dataset quality degrades.

### Mechanism 2
- **Claim:** Z-filtering reduces spurious correlations that models would otherwise exploit as shortcuts.
- **Mechanism:** The z-filtering algorithm identifies task-independent features (unigrams, bigrams, lexical overlap, financial term density) highly correlated with labels and removes samples contributing to high spurious correlations. Lower z-scores post-filtering indicate reduced exploitability.
- **Core assumption:** High z-scores between task-independent features and labels indicate harmful artifacts rather than legitimate domain patterns.
- **Evidence anchors:**
  - [Section 3.3] "Z-scores and top-k features before and after applying z-filtering are included in Appendix C showing the effectiveness of this our approach in minimizing spurious correlations."
  - [Table 8] "The unigram might is highly correlated with the neutral class (z-statistic = 56.75) but reduces to 25.82 after filtering."
  - [Table 9] Shows performance drop after z-filtering, indicating removed instances were those models classified correctly via biases.
- **Break condition:** If filtering is too aggressive (low batch size, high k), too many samples are removed; if too lenient, artifacts persist.

### Mechanism 3
- **Claim:** Iterative prompt refinement using feedback from general-domain NLI models and expert annotators improves synthetic data quality.
- **Mechanism:** General-domain NLI model predictions identify challenging instances (correct vs. incorrect predictions). Expert annotation feedback reveals label inconsistencies and reasoning gaps. Prompts are refined to include instructions for mathematical, temporal, and financial reasoning patterns.
- **Core assumption:** Disagreement between NLI model predictions and generated labels indicates genuinely challenging examples rather than low-quality generations.
- **Evidence anchors:**
  - [Section 3.4] "14.27% of the examples where the general-domain NLI model assigns a similar label to the generated label are relabelled by annotators, compared to 46.88% for instances with differing labels."
  - [Section 4.1] "After refining the prompt, the second round (R2) showed improvement, with 76.47% out of 595 instances displaying label agreement."
  - [corpus] Related NLI datasets (SciNLI, MSciNLI) use rule-based generation; corpus evidence on iterative refinement with experts is weak outside this paper.
- **Break condition:** If prompt refinements overfit to specific model quirks rather than improving genuine reasoning quality, generalization suffers.

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - Why needed here: Core task definition—determining entailment, contradiction, or neutral relationships between premise-hypothesis pairs underpins all evaluation methodology.
  - Quick check question: Given premise "Revenue increased 15%" and hypothesis "The company grew," what label applies and why?

- **Concept: Domain Shift in NLP**
  - Why needed here: Explains why general-domain models (trained on MNLI/SNLI) drop from ~90% to ~70% accuracy on financial text.
  - Quick check question: Why would a model trained on Wikipedia struggle with SEC filing language?

- **Concept: Spurious Correlations in Datasets**
  - Why needed here: Motivates z-filtering; models exploit superficial features (e.g., "might" → neutral) instead of learning true reasoning.
  - Quick check question: If a model achieves 90% accuracy by detecting the word "may" predicts "neutral," is it solving NLI?

## Architecture Onboarding

- **Component map:** Premise Sampler → Hypothesis Generator (LLM + role/style prompts) → Z-Filter → NLI Model Feedback Loop → Expert Annotators → Evaluator
- **Critical path:** Premise preprocessing quality → hypothesis diversity → z-filtering effectiveness → expert annotation consistency → benchmark validity
- **Design tradeoffs:**
  - GPT-4 vs. Llama generation: GPT-4 produces harder examples but licensing restricts training use
  - Aggressive vs. conservative z-filtering: More filtering reduces artifacts but shrinks dataset
  - Expert annotation rounds: More rounds improve quality but increase cost
- **Failure signatures:**
  - Low inter-annotator agreement (Fleiss-κ < 0.7) indicates ambiguous instructions or poor premise quality
  - Model performance plateaus below 80% despite fine-tuning suggests dataset contains genuinely hard or ambiguous cases
  - Instruction-tuned financial LLMs underperform base models → indicates overfitting to training tasks (FinMA: 23-29% vs. Llama-2-7B: 35%)
- **First 3 experiments:**
  1. **Baseline replication:** Run DeBERTa-V3-NLI and BART-Large-MNLI zero-shot on FinNLI test set to confirm domain shift (expect ~70% accuracy vs. ~90% on MNLI).
  2. **Fine-tuning ablation:** Fine-tune RoBERTa-Base and RoBERTa-Large on FinNLI training set; compare Macro F1 to verify that larger models gain more (expect ~70% vs. ~75%).
  3. **Prompt sensitivity test:** Evaluate Llama-3.1-8B under Zero-Shot, Zero-Shot-AG, Few-Shot-AG, and CoT prompting to confirm small models benefit from CoT (expect ~7% improvement).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do instruction-tuned financial LLMs exhibit significantly lower generalizability and performance on FinNLI compared to their general-domain base models?
- **Basis in paper:** [explicit] The authors state in the Abstract and Results that it is "Surprising" that these models perform poorly, "suggesting limited generalizability." They speculate this could be due to "limited training sets, suboptimal training parameters, or training schedules," but do not isolate the cause.
- **Why unresolved:** The paper identifies the performance drop (e.g., FinMA 7B performing worse than Llama-2-7B-chat) but only offers hypotheses for the degradation without conducting ablation studies to confirm the root cause.
- **What evidence would resolve it:** Ablation studies varying the composition and scale of instruction-tuning data (specifically the inclusion or exclusion of NLI-style tasks) to see if the degradation is due to data imbalance or catastrophic forgetting.

### Open Question 2
- **Question:** Can FinNLI serve as an effective intermediate task to enhance performance on downstream financial applications like risk assessment or automated reporting?
- **Basis in paper:** [explicit] In the Limitations and Conclusion sections, the authors state: "Future work will explore... using FinNLI as an intermediate task to enhance performance on related tasks."
- **Why unresolved:** The current study focuses exclusively on evaluating FinNLI as a standalone benchmark target rather than as a training resource for transfer learning to other financial tasks.
- **What evidence would resolve it:** Experiments that pre-train or fine-tune financial models on FinNLI and subsequently measure performance improvements on distinct downstream tasks such as financial sentiment analysis or question answering.

### Open Question 3
- **Question:** Would implementing advanced setups, such as generating explanation rationales or probability distributions over label space, mitigate the high rate of neutral-to-entailment confusion?
- **Basis in paper:** [explicit] The authors note in the Limitations section: "Our NLI task setup employs a single gold label classification, and we have yet to explore more advanced setups, such as those involving explanation rationales or generating probability distributions..."
- **Why unresolved:** The current evaluation relies on single-label hard classification, which fails to capture model uncertainty or reasoning paths, particularly in the "ambiguous" instances identified in the error analysis.
- **What evidence would resolve it:** A comparative evaluation where models are prompted with Chain-of-Thought to produce rationales, or calibrated to output probability distributions, to see if these methods reduce errors on low-confidence/ambiguous instances.

## Limitations
- Dataset availability: The FinNLI dataset is not publicly accessible and requires direct contact with authors for access, limiting independent verification of results
- Generalizability of synthetic data quality: While z-filtering reduces spurious correlations, the effectiveness of LLM-generated hypotheses depends heavily on prompt engineering quality that may not transfer to other domains
- Instruction-tuned model underperformance: The finding that financial LLMs (FinMA) underperform general models (Llama-2-7B) raises questions about whether this reflects genuine limitations or artifacts of the FinMA training process

## Confidence
- **High Confidence:** Domain shift impact (general models dropping from ~90% to ~70% on financial text), Macro F1 metric appropriateness for imbalanced test set
- **Medium Confidence:** Effectiveness of z-filtering in reducing spurious correlations (supported by statistical evidence but depends on implementation details)
- **Medium Confidence:** Iterative prompt refinement improving data quality (shown through inter-annotator agreement improvements but relies on subjective expert judgment)

## Next Checks
1. **Zero-shot baseline replication:** Run DeBERTa-V3-NLI and BART-Large-MNLI on FinNLI test set to independently verify the ~70% accuracy domain shift effect
2. **Prompt sensitivity analysis:** Systematically test Llama-3.1-8B across all prompting strategies (Zero-Shot, Zero-Shot-AG, Few-Shot-AG, CoT) to confirm the ~7% CoT improvement
3. **Artifact analysis:** Conduct targeted evaluation on the 16 z-filtered samples to determine whether they represent genuinely ambiguous cases or systematic weaknesses in the filtering approach