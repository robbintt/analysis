---
ver: rpa2
title: 'M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking
  Dataset'
arxiv_id: '2510.23508'
source_url: https://arxiv.org/abs/2510.23508
tags:
- image
- claim
- claims
- images
- location
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces M4FC, a large-scale, real-world dataset
  for multimodal fact-checking, addressing limitations in existing datasets such as
  limited size, language coverage, and task diversity. M4FC includes 4,982 images
  and 6,980 claims across ten languages, covering six tasks: visual claim extraction,
  claimant intent prediction, fake image detection, image contextualization, location
  verification, and verdict prediction.'
---

# M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset

## Quick Facts
- **arXiv ID**: 2510.23508
- **Source URL**: https://arxiv.org/abs/2510.23508
- **Reference count**: 40
- **Primary result**: M4FC introduces 4,982 images and 6,980 claims across 10 languages for multimodal fact-checking, with proprietary models outperforming open-weight alternatives in multilingual settings.

## Executive Summary
This paper introduces M4FC, a large-scale, real-world dataset for multimodal fact-checking, addressing limitations in existing datasets such as limited size, language coverage, and task diversity. M4FC includes 4,982 images and 6,980 claims across ten languages, covering six tasks: visual claim extraction, claimant intent prediction, fake image detection, image contextualization, location verification, and verdict prediction. The dataset was collected from 22 fact-checking organizations across 17 countries, ensuring broad cultural and geographic diversity. Two new tasks—visual claim extraction and location verification—are introduced. Baseline experiments using various multimodal large language models (MLLMs) and specialized detectors show that M4FC is challenging for current models, with proprietary models like Gemini-1.5-Flash generally outperforming open-weight alternatives, especially in multilingual settings. The study also demonstrates that intermediate tasks significantly impact verdict prediction performance, with location verification and image contextualization providing complementary benefits. Overall, M4FC offers a comprehensive resource for advancing multimodal fact-checking research and improving automated misinformation detection.

## Method Summary
M4FC collects 4,982 images and 6,980 claims from 22 fact-checking organizations across 10 languages (Arabic, Dutch, English, French, German, Portuguese, Spanish, Tamil, Telugu, Turkish). The dataset covers six multimodal fact-checking tasks: Visual Claim Extraction (VCE), Claimant Intent Prediction (CIP), Fake Image Detection (FD), Image Contextualization (IC), Location Verification (LV), and Verdict Prediction (VP). Zero-shot evaluation uses temperature=0 with MLLMs (InternVL2.5-8B, Qwen2.5-VL-7B, Llama-3.2-11B-Vision) and proprietary models (GPT4o-mini, Gemini-1.5-Flash). Evidence retrieval includes Google RIS for web pages, OpenStreetMap for maps, and ESRI satellite imagery at zoom level 15. Fake detectors are fine-tuned with Adam (lr=2e-5, 10 epochs). The temporal split uses 60%/10%/30% with cutoffs at 2022-09-22, 2023-01-18, and 2024-09-01.

## Key Results
- Proprietary models (Gemini-1.5-Flash, GPT4o-mini) outperform open-weight alternatives, especially in multilingual settings
- Combining intermediate tasks (particularly location verification and image contextualization) with web evidence improves verdict prediction F1 to 89.16
- Image contextualization provides the largest performance boost, improving F1 by 16.96 percentage points
- Open-weight models show dramatic performance drops on non-English languages (e.g., Llama-3.2-11B: 42.57 F1 on Arabic vs. 60.68 on English)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Combining intermediate tasks (particularly location verification and image contextualization) with web evidence improves verdict prediction performance in multimodal fact-checking.
- **Mechanism**: A pipeline architecture decomposes fact-checking into sequential sub-tasks (visual claim extraction, claimant intent prediction, fake image detection, image contextualization, location verification, verdict prediction). Intermediate tasks accumulate complementary evidence about an image's provenance, context, and authenticity, which informs the final verdict. Location verification provides geospatial constraints, while image contextualization supplies temporal and entity-level information.
- **Core assumption**: Human fact-checking workflows can be decomposed into learnable sub-tasks whose outputs provide cumulative, non-redundant evidence for verdict prediction.
- **Evidence anchors**:
  - [Key outcome]: "intermediate tasks significantly impact verdict prediction performance, with location verification and image contextualization providing complementary benefits."
  - [Results, Table 6]: Combining ground truth intermediate tasks and web evidence achieves the highest F1 (89.16 for GPT4o-mini), demonstrating task complementarity.
  - [Results, Page 7]: "image contextualization yields the largest performance boost, improving F1 by 16.96 percentage points" when ground truth is available.
- **Break condition**: Pipeline performance degrades when early tasks (e.g., visual claim extraction) produce incorrect outputs that propagate errors downstream. The paper notes: "Replacing ground truth claims with extracted ones substantially decreases F1, showing that inaccuracies in extracted claims harm downstream verdict prediction."

### Mechanism 2
- **Claim**: Proprietary MLLMs (Gemini-1.5-Flash, GPT4o-mini) exhibit greater multilingual robustness in verdict prediction compared to open-weight alternatives, particularly when leveraging retrieved evidence.
- **Mechanism**: Large-scale proprietary models possess broader multilingual pre-training coverage and more sophisticated cross-lingual transfer capabilities, enabling them to maintain performance consistency across languages. Retrieved web evidence acts as an external knowledge buffer that compensates for linguistic knowledge gaps, especially benefiting models with weaker multilingual capabilities.
- **Core assumption**: Multilingual robustness stems from both model architecture/training data diversity and the ability to effectively integrate external evidence in lower-resource language contexts.
- **Evidence anchors**:
  - [Abstract]: "proprietary models like Gemini-1.5-Flash generally outperforming open-weight alternatives, especially in multilingual settings."
  - [Results, Figure 7]: Proprietary models show more consistent F1 scores across all 10 languages, while open-weight models like Llama-3.2-11B show dramatic performance drops on Arabic, Turkish, and Spanish.
  - [Results, Page 7]: "Across all systems, providing RIS evidence consistently improves results, and the effect is most pronounced for models struggling with multilingual inputs."
- **Break condition**: Multilingual robustness fails when language-specific cultural nuances or context (e.g., sarcasm detection in Tamil posts, political references in Turkish claims) are not captured in pre-training data or evidence retrieval. The paper notes visual claim extraction failures due to "model's inability to capture sarcasm in social media posts."

### Mechanism 3
- **Claim**: Multi-modal evidence types (textual web pages, maps, satellite imagery) provide complementary verification signals, with their relative utility varying by task and model architecture.
- **Mechanism**: Different evidence modalities excel at verifying different aspects of claims. Textual web evidence provides contextual provenance information (date, source, event), while geospatial evidence (maps, satellite views) validates location-based claims through cross-view matching. Models vary in their ability to process and integrate these modalities.
- **Core assumption**: Effective multimodal fact-checking requires models capable of reasoning across heterogeneous evidence types and knowing when each type is most relevant.
- **Evidence anchors**:
  - [Tasks, Page 3-4]: Location verification introduces "maps and satellite images" as previously overlooked evidence types in multimodal AFC research.
  - [Results, Table 5]: Location verification performance varies by modality—Qwen2.5-VL-7B performs best with satellite imagery (65.19 F1), while GPT4o-mini is the only model to benefit from combining map and satellite views.
  - [Results, Page 7]: "RIS constitutes the most useful source of evidence" for verdict prediction, with other evidence types (web search, image search, geolocation) in DEFAME providing "limited impact."
- **Break condition**: Evidence modality utility breaks down when: (1) models cannot effectively cross-reference ground-level images with aerial views (common failure in location verification), (2) satellite imagery zoom levels are inappropriate for the verification task, or (3) models over-rely on textual evidence while ignoring visual-geospatial signals.

## Foundational Learning

- **Concept: Multimodal Pipeline Architecture for Verification**
  - **Why needed here**: M4FC decomposes fact-checking into six sequential tasks, each building on previous outputs. Understanding pipeline design is essential to grasp how intermediate task outputs affect downstream verdict accuracy and how errors propagate.
  - **Quick check question**: If visual claim extraction misidentifies the claim text, how would this affect claimant intent prediction and ultimately verdict prediction?

- **Concept: Cross-Lingual Transfer in Vision-Language Models**
  - **Why needed here**: The dataset spans 10 languages with significant performance disparities between proprietary and open-weight models. Understanding cross-lingual transfer explains why some models maintain performance across languages while others fail on specific linguistic/cultural contexts.
  - **Quick check question**: Why might a model perform well on Telugu (a lower-resource language) but poorly on Spanish (a higher-resource language), as shown in Figure 7?

- **Concept: Evidence Fusion and Modal Complementarity**
  - **Why needed here**: The paper introduces novel evidence types (maps, satellite images) alongside traditional web text. Understanding how different evidence modalities contribute uniquely to verification tasks is crucial for designing effective fact-checking systems.
  - **Quick check question**: For an out-of-context image claiming to show a political rally in Brazil, which evidence type would be more useful: textual web pages about the event, or satellite imagery of the claimed location?

## Architecture Onboarding

- **Component map**: The M4FC architecture consists of six task modules: (1) Visual Claim Extraction (input: social media screenshot → output: English claim), (2) Claimant Intent Prediction (input: image + claim → output: intent text), (3) Fake Image Detection (input: image → output: authentic/manipulated label), (4) Image Contextualization (input: image + claim + RIS evidence → output: provenance metadata), (5) Location Verification (input: image + map/satellite views → output: location match boolean), (6) Verdict Prediction (input: image + claim + intermediate outputs + evidence → output: true/false). Evidence retrieval (Google RIS for web pages, OpenStreetMap/ESRI for geospatial data) feeds into multiple tasks.

- **Critical path**: Visual Claim Extraction → Claimant Intent Prediction + Fake Image Detection → Image Contextualization + Location Verification (parallel) → Verdict Prediction. The most performance-sensitive nodes are Image Contextualization (provides largest F1 boost when accurate) and Evidence Retrieval (web evidence has strongest impact on verdict accuracy).

- **Design tradeoffs**:
  1. **Ground truth vs. predicted intermediate outputs**: Using predicted outputs creates an end-to-end system but introduces error propagation; ground truth outputs provide performance upper bounds but aren't available at inference time.
  2. **Evidence modality selection**: Textual RIS evidence provides broad utility but may miss geospatial verification needs; satellite/maps are task-specific but require outdoor scene applicability and appropriate zoom levels.
  3. **Model selection**: Proprietary models offer multilingual robustness and stronger intermediate task performance; open-weight models provide reproducibility but struggle with multilingual claims and certain tasks.

- **Failure signatures**:
  1. **Pipeline error cascade**: If visual claim extraction fails (e.g., misinterprets sarcasm, misses embedded text), subsequent tasks operate on incorrect inputs, degrading verdict accuracy.
  2. **Evidence integration failure**: Models may retrieve relevant evidence but fail to integrate it effectively (e.g., location verification F1 drops when using aerial views vs. text for some models).
  3. **Multilingual breakdown**: Open-weight models show dramatic performance variance across languages (e.g., Llama-3.2-11B: 42.57 F1 on Arabic vs. 60.68 on English), indicating insufficient cross-lingual transfer.

- **First 3 experiments**:
  1. **Establish baselines on individual tasks**: Run all six tasks with provided baseline models (InternVL2.5-8B, Qwen2.5-VL-7B, Llama-3.2-11B-Vision, GPT4o-mini, Gemini-1.5-Flash) using the M4FC test set. Focus on identifying which tasks each model handles well/poorly and characterize error types (claim extraction errors, sarcasm misinterpretation, cross-view matching failures).
  2. **Analyze intermediate task combinations for verdict prediction**: Systematically test how different combinations of predicted intermediate task outputs (with and without web evidence) affect verdict F1. Start with the combinations in Table 6 (e.g., VCE+CIP+FD vs. VCE+IC+LV) and extend to novel combinations. Measure the performance gap between using predicted vs. ground truth intermediate outputs.
  3. **Evaluate multilingual robustness across evidence conditions**: For verdict prediction, test all models on the multilingual balanced setting with and without RIS evidence. Disaggregate results by language to identify: (a) languages where evidence retrieval provides the largest boost, (b) languages where open-weight models fail most severely, (c) whether performance correlates with language resource level or cultural specificity.

## Open Questions the Paper Calls Out

- **How to optimally combine intermediate tasks**: The paper states that their results on combining intermediate tasks "open the door for future work on how to best implement these combinations, in terms of accuracy, interpretability, and computational efficiency" (Page 8).

- **Scaling location verification**: The authors identify that "scaling the dataset for this task with real-world cases is not a viable option. Instead, future works should consider creating larger synthetic datasets" (Page 9).

## Limitations

- **Small location verification dataset**: Only 195 real-world instances limit robust model training and evaluation for this task.
- **Error propagation in pipeline**: Using predicted intermediate outputs substantially decreases performance, but systematic error propagation analysis is lacking.
- **API dependence**: Proprietary model performance may mask computational efficiency differences due to API access limitations.

## Confidence

- **High Confidence**: Overall dataset construction methodology, task decomposition, baseline results showing proprietary model superiority in multilingual settings, and evidence showing task complementarity (particularly IC + LV boosting VP performance).
- **Medium Confidence**: Specific performance metrics for individual tasks (particularly location verification and image contextualization) due to small sample sizes and high variance across models, and analysis of error types based on limited manual inspection.
- **Low Confidence**: Absolute performance gains from different evidence combinations due to zero-shot nature of evaluation and lack of comparison with fine-tuned baselines on individual tasks.

## Next Checks

1. **Error Propagation Analysis**: Systematically measure how errors in visual claim extraction cascade through the pipeline by injecting controlled noise at different stages and measuring downstream effects on verdict prediction accuracy.

2. **Temporal Generalization Test**: Train models on data before 2022-09-22 and test exclusively on post-2024-09-01 data to evaluate how well the pipeline generalizes to new misinformation patterns and whether intermediate task performance degrades over time.

3. **Multilingual Robustness Stress Test**: Create a subset of M4FC with claims from cultures/regions with minimal representation in model pre-training data (e.g., Tamil, Telugu) and evaluate whether evidence retrieval can compensate for linguistic knowledge gaps, comparing performance with and without RIS evidence.