---
ver: rpa2
title: 'Speechless: Speech Instruction Training Without Speech for Low Resource Languages'
arxiv_id: '2505.17417'
source_url: https://arxiv.org/abs/2505.17417
tags:
- speech
- speechless
- data
- training
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Speechless, a novel approach for generating
  synthetic training data for speech language models without relying on traditional
  text-to-speech systems. The method uses a quantized Whisper encoder to generate
  semantic speech tokens, bypassing the need for waveform generation entirely.
---

# Speechless: Speech Instruction Training Without Speech for Low Resource Languages

## Quick Facts
- arXiv ID: 2505.17417
- Source URL: https://arxiv.org/abs/2505.17417
- Reference count: 0
- Primary result: Competitive WER (2.08-4.21% on Librispeech, 2.69-7.08% on Vietnamese Common Voice) without TTS systems

## Executive Summary
This paper introduces Speechless, a novel approach for generating synthetic training data for speech language models without relying on traditional text-to-speech systems. The method uses a quantized Whisper encoder to generate semantic speech tokens, bypassing the need for waveform generation entirely. This approach is particularly valuable for low-resource languages where TTS models may be limited or unavailable. The method involves three main stages: training a residual vector quantizer to align semantic and text representations, developing Speechless to generate semantic tokens from text, and using Speechless to generate synthetic data for fine-tuning large language models on speech understanding tasks.

## Method Summary
Speechless generates synthetic speech tokens from text by leveraging a quantized Whisper encoder and a decoder-only language model. The approach involves three stages: (1) training a residual vector quantizer (RVQ) with expanded codebooks to discretize Whisper encoder outputs, (2) training a 1B parameter decoder-only model (Speechless) to translate text instructions into semantic tokens that align with Whisper's latent space, and (3) using Speechless to generate synthetic instruction data for fine-tuning target LLMs with expanded vocabularies. At inference, real speech is processed by the frozen Whisper encoder and RVQ, producing tokens the fine-tuned LLM can understand. The method achieves competitive ASR performance across multiple domains while avoiding the computational and data requirements of TTS systems.

## Key Results
- WER of 2.08-4.21% on Librispeech clean test set
- WER of 2.69-7.08% on Vietnamese Common Voice test set
- Competitive performance across various ASR settings without requiring TTS systems
- Enables effective speech instruction tuning of LLMs while maintaining instruction-following capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-to-semantic token mapping can substitute for text-to-speech generation when training speech-aware LLMs.
- Mechanism: A decoder-only language model (Speechless, 1B parameters) learns to translate text tokens into discrete semantic tokens derived from Whisper's encoder representations. Since these semantic tokens occupy the same latent space as real speech encoded by Whisper, the downstream LLM trained on synthetic tokens generalizes to actual spoken input at inference.
- Core assumption: The semantic tokens preserve sufficient linguistic information for instruction-following tasks without acoustic detail.
- Evidence anchors:
  - [abstract] "generating semantic speech tokens aligned with a pre-trained Whisper encoder... enabling an LLM to be fine-tuned on text instructions while maintaining the ability to understand spoken instructions during inference"
  - [section 2.2] "Speechless functions similarly to a machine translation model. It translates text-based instructions into a sequence of semantic tokens that is close to what would be generated by the Whisper Encoder"
  - [corpus] Related work on synthetic voice for African ASR (arXiv:2507.17578) shows TTS-based synthesis can improve low-resource ASR, but Speechless eliminates TTS dependency entirely—limited direct corpus comparison available.
- Break condition: If semantic tokens lose task-critical prosodic or paralinguistic information required for instruction understanding, performance would degrade on nuanced commands.

### Mechanism 2
- Claim: Residual Vector Quantization (RVQ) with expanded codebooks preserves semantic content while discretizing Whisper encoder outputs.
- Mechanism: RVQ iteratively refines representations through multiple codebooks—capturing both coarse semantic features and fine-grained nuances. Expanding codebook size from 512 to 2048 entries (initialized by duplicating original weights with Kaiming noise) improves utilization for low-resource languages.
- Core assumption: The quantization error introduced by RVQ does not destroy instruction-relevant semantic information.
- Evidence anchors:
  - [section 2.1] "RVQ achieves this through an iterative refinement process: First, it creates a coarse representation... then progressively refines this representation by quantizing the residual errors"
  - [section 4.1] "once quantized, the Whisper encoder's performance declines in both noisy and multilingual settings. We posit that this is primarily due to information being lost during the residual vector quantization operation"
  - [corpus] No direct corpus evidence on RVQ codebook scaling for low-resource speech; assumption remains unvalidated externally.
- Break condition: If codebook capacity is insufficient for target language phonology, quantization collapse or poor reconstruction would occur.

### Mechanism 3
- Claim: Frozen Whisper encoder at inference provides noise robustness and speaker invariance without requiring diverse synthetic speakers during training.
- Mechanism: Since Whisper was pre-trained on large-scale weakly supervised data, its encoder already generalizes across speakers and noise conditions. Speechless only needs to generate semantically aligned tokens—the encoder handles acoustic variability at inference.
- Core assumption: Whisper's pre-trained robustness transfers to the quantized token space used during LLM fine-tuning.
- Evidence anchors:
  - [section 1] "leveraging the Whisper encoder's inherent noise robustness and speaker invariance at inference time, our approach avoids the need for speaker diversity"
  - [section 4.1] "with added noise (VBD noisy), the Whisper encoder starts to generate tokens that show poorer WER in comparison [to Speechless]"
  - [corpus] Whisper fine-tuning for low-resource languages (arXiv:2503.18485) confirms Whisper struggles with underrepresented languages but benefits from targeted fine-tuning—supports frozen-encoder limitations.
- Break condition: If domain shift between Whisper's pre-training data and target low-resource language is too large, encoder representations may be insufficiently discriminative.

## Foundational Learning

- Concept: **Early-fusion speech-language models**
  - Why needed here: Speechless targets early-fusion architectures where speech representations feed directly into the LLM, bypassing cascaded ASR→LLM pipelines. Understanding this distinction is critical for grasping why semantic token alignment matters.
  - Quick check question: Can you explain why early-fusion reduces latency compared to cascaded ASR+LLM systems?

- Concept: **Residual Vector Quantization (RVQ)**
  - Why needed here: Stage 1 relies on RVQ to discretize continuous Whisper encoder outputs. Understanding iterative residual coding is essential for diagnosing codebook utilization and quantization error issues.
  - Quick check question: How does RVQ differ from single-codebook VQ in terms of reconstruction fidelity and computational cost?

- Concept: **Catastrophic forgetting in multimodal fine-tuning**
  - Why needed here: Table 3 shows MMLU/VMLU degradation after speech instruction tuning, attributed to catastrophic forgetting. Recognizing this tradeoff informs decisions about joint text-speech training.
  - Quick check question: What techniques could mitigate catastrophic forgetting when adding speech modalities to a text-only LLM?

## Architecture Onboarding

- Component map:
  Whisper Medium Encoder -> RVQ Quantizer -> Speechless LM (1B) -> Target LLM (LLaMA 3.2 1B/3B)

- Critical path:
  1. Stage 1: Train RVQ quantizer on ASR data (ViVoice + LibriTTS-R)
  2. Stage 2: Train Speechless on text→semantic-token pairs using transcriptions as input
  3. Stage 3: Generate synthetic instruction data via Speechless; fine-tune target LLM with expanded tokenizer
  4. Inference: Real speech -> Whisper Encoder -> RVQ -> LLM

- Design tradeoffs:
  - **Codebook size vs. utilization**: Quadrupling to 2048 entries improves capacity but requires careful initialization (duplicate + noise) to avoid poor utilization
  - **Model size vs. inference cost**: Speechless is 1B parameters but offline-only; target LLM size determines serving cost
  - **Noise robustness vs. clean training**: Training on clean speech (ViVoice, LibriTTS-R) increases noise sensitivity—explicitly noted as a limitation

- Failure signatures:
  - High WER on noisy test sets (VBD noisy: 6.17→15.76 CER after quantization) indicates quantizer information loss
  - Text benchmark degradation (MMLU: 69.40→62.27) signals catastrophic forgetting from speech token expansion
  - Poor codebook utilization (if initialization fails) leads to collapsed representations

- First 3 experiments:
  1. **Validate quantizer reconstruction**: Pass ASR test set through Whisper->RVQ->Whisper decoder; compare WER to unquantized baseline to isolate quantization loss.
  2. **Ablate Speechless quality**: Generate semantic tokens from held-out text via Speechless; decode through Whisper decoder and measure WER against ground truth transcriptions.
  3. **Probe noise robustness**: Evaluate full pipeline (Speechless-trained LLM with real noisy speech input) on VBDemand noisy split; compare to cascaded Whisper+LLaMA baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating noisy audio data during the quantizer training phase improve robustness without degrading clean performance?
- Basis in paper: [explicit] Authors state future work will focus on "enhancing noise robustness" and attribute current sensitivity to the use of clean-only training data in Stage 1.
- Why unresolved: The current codebooks were derived exclusively from clean speech, leading to performance drops on noisy datasets like VBDemand.
- What evidence would resolve it: Improved WER on VBDemand noisy test sets after retraining Stage 1 with noise augmentation.

### Open Question 2
- Question: Can mitigation strategies reduce the observed catastrophic forgetting of text capabilities during speech instruction tuning?
- Basis in paper: [explicit] The paper notes "performance degradation in text-based benchmarks suggests potential issues with catastrophic forgetting."
- Why unresolved: The current fine-tuning process causes a drop in MMLU (69.40 to 62.27) and VMLU scores compared to the base model.
- What evidence would resolve it: Benchmarking MMLU/VMLU scores of models fine-tuned with techniques like replay or LoRA against the current baseline.

### Open Question 3
- Question: How does the method perform on languages with significantly fewer ASR resources than Vietnamese (868k samples)?
- Basis in paper: [explicit] Future work involves "expanding applicability to a broader range of languages and dialects."
- Why unresolved: Vietnamese, while low-resource compared to English, still utilized a substantial dataset (868k utterances) for training the quantizer.
- What evidence would resolve it: Evaluating the pipeline on languages with limited data (e.g., <100 hours) to determine the minimum data requirement.

## Limitations

- Quantization degrades performance in noisy settings (VBD noisy WER increases from 6.17 to 15.76)
- Catastrophic forgetting observed in text benchmarks (MMLU drops from 69.40 to 62.27)
- Offline-only Speechless generation requirement limits deployment flexibility

## Confidence

- **High Confidence**: WER results on clean speech benchmarks (LibriSpeech, Vietnamese Common Voice) demonstrate the method's viability for low-resource ASR without TTS systems
- **Medium Confidence**: The semantic token generation mechanism via Speechless effectively substitutes for text-to-speech in instruction tuning, though noise robustness remains questionable
- **Medium Confidence**: RVQ with expanded codebooks improves representation capacity, but optimal initialization and utilization require further validation
- **Low Confidence**: Catastrophic forgetting mitigation strategies and their impact on long-term multimodal capability are not fully addressed

## Next Checks

1. **Quantization fidelity test**: Pass held-out ASR test set through Whisper->RVQ->Whisper decoder pipeline and measure WER degradation compared to unquantized baseline to isolate information loss from the quantization process

2. **Cross-language generalization**: Apply the full pipeline to a third low-resource language (e.g., Swahili or Hausa) to verify the approach extends beyond the Vietnamese-English pair and assess performance without language-specific fine-tuning of the Whisper encoder

3. **Noise robustness evaluation**: Train the RVQ quantizer on noisy ASR datasets (including VBDemand and artificially corrupted speech) and measure the impact on downstream WER for real-world noisy speech conditions, comparing against the current clean-training baseline