---
ver: rpa2
title: 'MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf'
arxiv_id: '2502.04376'
source_url: https://arxiv.org/abs/2502.04376
tags:
- meeting
- information
- response
- transcript
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a prototype LLM-powered meeting delegate
  system designed to represent individuals in meetings. The system uses real-time
  transcript analysis to determine when and how to engage in conversations, focusing
  on the participant role rather than facilitator functions.
---

# MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf

## Quick Facts
- **arXiv ID:** 2502.04376
- **Source URL:** https://arxiv.org/abs/2502.04376
- **Reference count:** 28
- **Primary result:** A benchmark dataset and evaluation framework for LLM-powered meeting delegate systems, showing ~60% recall in addressing key points with significant variance across models and scenarios.

## Executive Summary
This paper introduces a prototype system where LLMs can attend meetings on behalf of users, focusing on the participant role (vs. facilitator). The system uses real-time transcript analysis and in-context learning to decide when and how to engage, guided by a carefully designed prompt. To evaluate performance, the authors create a benchmark from real meeting transcripts covering four scenarios: Explicit Cue, Implicit Cue, Chime In, and Keep Silence. Testing across multiple LLMs shows GPT-4/4o maintain balanced engagement, while Gemini 1.5 Pro is more cautious and Gemini 1.5 Flash/Llama3 models are more active. Overall, ~60% of responses address at least one key point from ground-truth responses, demonstrating promise while highlighting areas for improvement.

## Method Summary
The system uses a Meeting Engagement module that processes real-time transcripts with an LLM via in-context learning. For each utterance, the LLM decides whether to respond or remain silent based on a prompt including persona definition, meeting context, user intents, and detailed behavioral guidelines. The benchmark dataset is derived from the ELITR Minuting Corpus (61 English meetings), with test cases manually verified and expanded via synthetic scenarios (mismatched names, phonetic errors). Evaluation uses GPT-4 for recall and GPT-4 Turbo for attribution, measuring Response/Silence rates, loose/strict recall, and attribution types (Expected Response, Contextual Info, Previous Transcript, Hallucination).

## Key Results
- GPT-4/4o models maintain balanced response/silence rates across scenarios; Gemini 1.5 Pro is cautious, Gemini 1.5 Flash/Llama3 models are more active.
- ~60% of responses address at least one key point from ground-truth responses (loose recall).
- Attribution analysis shows 10-30% of responses are repetitive (Previous Transcript), ~5% are hallucinations, and the rest draw from expected response or contextual info.
- Performance degrades significantly under phonetic transcription errors (e.g., GPT-4o response rate drops from 94.3% to 68% with noisy names).

## Why This Works (Mechanism)

### Mechanism 1: Real-time Transcript Analysis for Engagement Decision
The system decides when to engage by continuously monitoring the meeting transcript for explicit cues, implicit cues, or opportunities to chime in. The Meeting Engagement module evaluates each new utterance using an LLM with in-context learning. The LLM processes the ongoing transcript, user intents, and background information to determine if a response is warranted and what to say.

### Mechanism 2: In-context Learning with Meeting-specific Prompts
The system uses carefully designed prompts to guide the LLM to behave as a delegate, balancing responsiveness with restraint. The prompt includes persona definition, meeting context, intents, background, and detailed guidelines for judging when and how to speak. This steers the LLM without fine-tuning.

### Mechanism 3: Tolerance for Transcription Errors via Phonetic Matching
The system can handle mis-transcribed names or words by using phonetic similarity and context to identify cues. The prompt explicitly instructs the LLM to consider phonetic similarities, especially for names, and to use attendee lists and context to disambiguate.

## Foundational Learning

- **Concept: In-context learning**
  - Why needed here: The system relies on prompts to make the LLM behave as a delegate without fine-tuning.
  - Quick check question: Can you explain how the prompt in Table 14 steers the model’s behavior for engagement decisions?

- **Concept: Meeting roles (participant vs. facilitator)**
  - Why needed here: The system focuses on the participant role, which differs from facilitator systems in prior work.
  - Quick check question: What distinguishes the participant role from the facilitator role in the context of meeting delegates?

- **Concept: Evaluation metrics for generative agents**
  - Why needed here: Understanding recall, attribution, and response/silence rates is essential to interpret benchmark results.
  - Quick check question: How does the “loose recall” metric differ from “strict recall,” and what does a 60% loose recall rate indicate about system performance?

## Architecture Onboarding

- **Component map:** Information Gathering -> Meeting Engagement -> Voice Generation
- **Critical path:** Transcript acquisition → prompt construction (transcript + intents + background) → LLM inference for engagement decision → response generation → TTS output.
- **Design tradeoffs:**
  - Latency vs. accuracy: Streaming APIs reduce latency but may increase error rates in complex reasoning.
  - Active vs. cautious engagement: Model choice (e.g., GPT-4o vs. Gemini 1.5 Pro) shifts response/silence balance.
  - Context window vs. completeness: Long meetings may require truncation or summarization, risking information loss.
- **Failure signatures:**
  - Irrelevant or repetitive responses (high “Previous Transcript” attribution)
  - Failure to recognize cues in noisy transcripts (Noisy Name ablation)
  - Over-engagement in “Keep Silence” scenarios (low Silence Rate)
- **First 3 experiments:**
  1. Replicate benchmark evaluation on a subset of the dataset to understand model-specific response/silence tradeoffs.
  2. Inject synthetic transcription errors (e.g., phonetic name substitutions) to test tolerance limits.
  3. Modify the prompt to reduce “Previous Transcript” attribution and measure impact on response relevance.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can a meeting delegate system effectively transition to fully autonomous operation (Phase III) while maintaining strict privacy safeguards against over-sharing?
  - Basis in paper: Section 6.1 explicitly outlines a "Phase III (Delegate)" goal where the system acts fully autonomously, but notes that current limitations require starting at "Phase I (Execute)" with user-defined boundaries due to privacy risks.

- **Open Question 2:** Can specific fine-tuning or architectural improvements mitigate the performance degradation caused by phonetic errors in name transcription?
  - Basis in paper: The ablation study in Section 5 shows that performance drops significantly (e.g., GPT-4o response rate falls from 94.3% to 68%) when names are replaced by phonetically similar words, highlighting a fragility in current models.

- **Open Question 3:** To what extent does incorporating multimodal inputs (e.g., tone, pitch, latency) via technologies like the Realtime API improve the accuracy of engagement timing and turn-taking?
  - Basis in paper: The Limitations section and Section 3 explicitly suggest that advancements like OpenAI’s Realtime API, which supports direct voice input/output, could enhance relevance and performance by capturing speed and tone.

## Limitations

- Performance drops significantly under transcription errors (e.g., noisy name ablation), indicating limited robustness.
- The benchmark dataset, though derived from real meetings, is still limited in size and scenario coverage.
- Evaluation methodology and metrics (especially recall and attribution) would benefit from external validation.

## Confidence

- Confidence in the core claim—that LLMs can act as meeting delegates—is **Medium**: supported by quantitative results but constrained by dataset and scenario limitations.
- Confidence in the engagement decision mechanism is **High**: well-documented and replicable.
- Confidence in the prompt-based approach to guide LLM behavior is **Medium**: results vary significantly across models, suggesting prompt robustness is not fully established.

## Next Checks

1. Test system performance on a broader range of meeting types (e.g., technical discussions, brainstorming sessions) to assess generalizability.
2. Evaluate end-to-end latency from transcript update to response generation under realistic streaming conditions.
3. Conduct user studies to measure perceived appropriateness and helpfulness of delegate responses in live meetings.