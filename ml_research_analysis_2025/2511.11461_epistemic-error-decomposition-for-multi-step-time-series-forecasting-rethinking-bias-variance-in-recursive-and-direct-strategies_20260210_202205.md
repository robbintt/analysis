---
ver: rpa2
title: 'Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking
  Bias-Variance in Recursive and Direct Strategies'
arxiv_id: '2511.11461'
source_url: https://arxiv.org/abs/2511.11461
tags:
- recursive
- noise
- direct
- variance
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the common belief that recursive strategies
  in multi-step time series forecasting always have high bias and low variance, while
  direct strategies have low bias and high variance. The authors provide a theoretical
  analysis that decomposes the forecast error into three parts: irreducible noise,
  structural bias (approximation gap), and estimation variance.'
---

# Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies

## Quick Facts
- arXiv ID: 2511.11461
- Source URL: https://arxiv.org/abs/2511.11461
- Reference count: 34
- One-line primary result: Recursive strategies can have lower bias and higher variance than direct strategies for nonlinear predictors, challenging conventional bias-variance assumptions.

## Executive Summary
This paper challenges the conventional wisdom that recursive strategies in multi-step time series forecasting always have high bias and low variance, while direct strategies have low bias and high variance. Through theoretical analysis, the authors decompose forecast error into three components: irreducible noise, structural bias (approximation gap), and estimation variance. They demonstrate that for nonlinear predictors, recursive composition can actually reduce structural bias by expanding model expressivity, while the estimation variance can be amplified by a Jacobian-based factor. Empirical results with MLPs on the ETTm1 dataset confirm these findings, showing that recursive models can exhibit both lower bias and higher variance than direct models.

## Method Summary
The paper analyzes multi-step time series forecasting using a theoretical framework that decomposes forecast error into irreducible noise, structural bias, and estimation variance. The authors compare recursive strategies (training one-step models and iterating) versus direct strategies (training separate h-step ahead models) through a composition map analysis. They derive closed-form expressions for variance amplification using the delta method and analyze noise regimes to determine when each strategy performs better. Experiments use bottlenecked MLPs (width=2) trained on ETTm1 dataset with 50 random seeds across varying training sample sizes.

## Key Results
- Recursive strategies can have lower structural bias than direct strategies for nonlinear predictors due to composition-induced expressivity expansion
- Estimation variance of recursive strategies equals one-step variance multiplied by a Jacobian-based amplification factor T_h
- Process noise versus measurement noise regimes determine optimal strategy choice
- Empirical results show recursive models achieve lower bias but higher variance than direct models on ETTm1 dataset

## Why This Works (Mechanism)

### Mechanism 1: Nonlinear Composition Expands Expressivity
- Claim: For nonlinear predictors, recursive composition can reduce structural bias compared to direct strategies
- Mechanism: The composition map A: θ → α_h transforms a d-dimensional parameter space into a higher-dimensional effective parameter space. For example, a 3D bilinear one-step model composes into a 5D polynomial space at horizon h=2, accessing function families direct models cannot.
- Core assumption: The true data-generating process requires expressivity closer to the composed manifold than the direct subspace
- Evidence anchors: Shows recursive bilinear predictor spans 5D space while direct spans 3D subspace; empirical validation shows recursive models systematically closer to task parameters
- Break condition: When task manifold is closer to direct subspace than composed manifold

### Mechanism 2: Jacobian-Driven Variance Amplification
- Claim: Recursive estimation variance equals one-step variance scaled by a Jacobian-derived amplification factor T_h
- Mechanism: Parameter uncertainty propagates through composition via the delta method. The covariance transforms as Σ_αh ≈ J_h Σ_θ J_h^T, where J_h is the Jacobian of the composition map
- Core assumption: Taylor expansion remainder terms are negligible—requires modest local curvature and small estimation noise
- Evidence anchors: Formal derivation showing EV^rec(h) ≈ T_h · EV₁-step; shows amplification factor can be greater or less than 1 depending on geometry
- Break condition: High estimator bias or large parameter uncertainty where higher-order terms dominate

### Mechanism 3: Noise Regime Governs Strategy Selection
- Claim: The relative magnitudes of process noise (σ_s) and measurement noise (σ_e) determine which strategy has lower total error
- Mechanism: Process noise accumulates through recursion while measurement noise affects one-step estimator bias. When σ_e dominates, recursive estimators become biased, favoring direct methods
- Core assumption: DGP follows latent AR process with additive measurement noise; noise statistics are separable
- Evidence anchors: Derives closed-form aleatoric floors showing σ_ε,2 can be less than σ_ε,1 depending on noise parameters
- Break condition: When measurement noise exceeds threshold causing OLS bias that invalidates delta-method approximations

## Foundational Learning

- Concept: **Bias-Variance Decomposition**
  - Why needed here: The paper decomposes MSE = σ²_noise + G^H + EV^H, separating irreducible aleatoric noise from epistemic structural gap and estimation variance
  - Quick check question: Given MSE = 0.5 on training data and 0.8 on test data, with irreducible noise σ² = 0.3, what are the structural gap and estimation variance?

- Concept: **Recursive vs Direct Multi-step Forecasting**
  - Why needed here: Understanding that recursive strategies train one-step model and iterate h times, while direct strategies train separate h-ahead models independently
  - Quick check question: For horizon h=3, how many model applications does a recursive strategy require vs. a direct strategy at inference time?

- Concept: **Delta Method for Variance Propagation**
  - Why needed here: The Jacobian-based variance amplification mechanism relies on applying the delta method to transform parameter covariance through nonlinear composition maps
  - Quick check question: If X has variance Var(X) = σ² and Y = aX + b, what is Var(Y)? How does this generalize to Y = g(X) for nonlinear g?

- Concept: **Function Composition and Jacobians**
  - Why needed here: Understanding how composition map T_h: θ → α_h(θ) transforms parameters and how chain rule produces Jacobian J_h
  - Quick check question: Given f(x) = x² and g(y) = 3y, what is the Jacobian of g ∘ f at x = 2?

## Architecture Onboarding

- Component map: One-Step Model f̂₁(θ) → Composition Map T_h → Effective h-Step Model f̂_h(α_h) → Direct Model ĝ_h → Test Predictions → Compare MSE

- Critical path:
  1. Characterize noise regime: Estimate σ_s and σ_e from data
  2. Assess model nonlinearity: Linear predictors have G^Δ = 0; nonlinear predictors require checking which manifold is closer to task
  3. Estimate variance amplification: Compute or approximate Jacobian norm to predict T_h
  4. Select strategy: High σ_e/σ_s ratio → favor direct; Low σ_e/σ_s + high nonlinearity → favor recursive

- Design tradeoffs:
  - **Expressivity vs. Stability**: Recursive composition expands accessible function space (can reduce bias) but amplifies parameter uncertainty through Jacobian (increases variance)
  - **Horizon length**: Longer horizons increase both expressivity gains and variance amplification
  - **Model capacity**: Paper uses bottlenecked MLP (width=2) to isolate mechanism; higher capacity may mask these effects
  - **Training data size**: Variance terms scale as O(1/N); at large N, structural gap dominates, favoring recursive if G^rec < G^dir

- Failure signatures:
  - Recursive strategy shows diverging test variance across seeds while direct remains stable → Jacobian amplification dominant
  - Recursive strategy has higher bias than direct despite nonlinearity → task manifold lies closer to direct subspace
  - Theory-predicted EV doesn't match empirical EV → estimator bias from measurement noise breaking delta-method assumptions
  - Direct strategy outperforms at all horizons → likely high measurement noise regime

- First 3 experiments:
  1. **Noise regime characterization on your dataset**: Estimate process vs. measurement noise ratio using auxiliary variables or holdout analysis
  2. **Learning curve comparison**: Train both recursive and direct models across varying sample sizes, plot ρ_MSE = MSE_rec/MSE_dir and ρ_Var = Var_rec/Var_dir ratios
  3. **Jacobian norm estimation**: For your trained recursive model, compute ||J_h||_F at validation points to identify variance amplification zones

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the recursive composition-induced reduction in structural bias persist in deeper, state-of-the-art architectures like Transformers or LSTMs?
- Basis in paper: The conclusion and limitations explicitly state that future work should "extend the analysis to deeper or more expressive models" as experiments were limited to shallow MLPs
- Why unresolved: Theoretical analysis suggests composition alters expressivity, but this was only validated on bottlenecked, single-layer networks; complex weight spaces may behave differently
- What evidence would resolve it: Empirical replication using deep Transformer or recurrent architectures on standard forecasting benchmarks

### Open Question 2
- Question: How can the composition-Jacobian norm be reliably estimated for nonlinear predictors to quantify uncertainty amplification?
- Basis in paper: The authors explicitly request future work to "quantify this link by estimating composition-Jacobian norms across architectures and horizons"
- Why unresolved: While the theory posits a Jacobian-derived amplification factor T_h, the paper notes the empirical study "do[es] not yet isolate an exact form" of this mechanism for nonlinear models
- What evidence would resolve it: A method to approximate J_h for MLPs, demonstrating high correlation between theoretical amplification factor and observed estimation variance

### Open Question 3
- Question: Can the characterization of process and measurement noise regimes be operationalized for automatic model selection?
- Basis in paper: The paper lists "Leveraging these diagnostics as priors for automatic strategy or hyperparameter selection" as a "promising next step"
- Why unresolved: The paper provides theoretical "actionable guidance" but does not implement or test an automated selection protocol
- What evidence would resolve it: An adaptive system that estimates σ_s/σ_e from input data and dynamically switches between recursive and direct strategies

## Limitations
- Analysis relies on delta-method approximations that break down with significant measurement noise or large parameter uncertainty
- Experiments limited to bottlenecked models (width=2) and relatively short horizons (h=2)
- Empirical validation restricted to one dataset (ETT1m), leaving uncertainty about generalization to different domains
- Theory assumes separable noise statistics and specific DGP structures that may not hold in practice

## Confidence

- **High Confidence**: The decomposition of forecast error into irreducible noise, structural bias, and estimation variance is theoretically sound and aligns with established statistical learning principles
- **Medium Confidence**: The Jacobian-based variance amplification mechanism is mathematically rigorous, but practical magnitude depends on difficult-to-estimate local geometry and parameter uncertainty
- **Low Confidence**: Specific numerical predictions (e.g., exact ρ_MSE and ρ_Var values across sample sizes) may not transfer to different datasets or model architectures

## Next Checks

1. **Noise Regime Characterization**: Estimate process vs. measurement noise ratios on your target dataset using auxiliary variables or holdout analysis

2. **Learning Curve Analysis**: Train both recursive and direct models across a range of sample sizes (e.g., [500, 1000, 2500, 5000, 7500, 10000]), plot ρ_MSE and ρ_Var ratios

3. **Jacobian Norm Estimation**: For your recursive model, compute ||J_h||_F at validation points to identify variance amplification zones