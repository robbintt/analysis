---
ver: rpa2
title: 'CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified
  State Exam'
arxiv_id: '2507.22958'
source_url: https://arxiv.org/abs/2507.22958
tags:
- solution
- score
- answer
- correct
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the EGE-Math Solutions Assessment Benchmark,
  the first dataset designed to evaluate Vision-Language Models on their ability to
  grade handwritten mathematical solutions according to human rubrics rather than
  solving problems themselves. The benchmark uses 122 scanned student solutions from
  the Russian Unified State Exam (EGE), each with expert-assigned scores and detailed
  grading criteria.
---

# CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam

## Quick Facts
- arXiv ID: 2507.22958
- Source URL: https://arxiv.org/abs/2507.22958
- Authors: Ruslan Khrulev
- Reference count: 40
- Primary result: OpenAI o4-mini achieved 56.56% accuracy in grading handwritten EGE math solutions

## Executive Summary
This paper introduces the EGE-Math Solutions Assessment Benchmark, the first dataset designed to evaluate Vision-Language Models on their ability to grade handwritten mathematical solutions according to human rubrics rather than solving problems themselves. The benchmark uses 122 scanned student solutions from the Russian Unified State Exam (EGE), each with expert-assigned scores and detailed grading criteria. Seven state-of-the-art VLMs were evaluated in three modes: without answer, with answer, and with reference solution. OpenAI o4-mini achieved the highest accuracy of 56.56% in the With Answer mode, with a quality score of 78.17%, demonstrating that current models can partially emulate expert grading but still fall short of human performance. The study highlights the challenges of interpreting handwriting, applying nuanced grading rubrics, and identifying errors, offering a foundation for developing AI-assisted educational assessment tools.

## Method Summary
The study evaluated seven Vision-Language Models on grading handwritten mathematical solutions from the Russian Unified State Exam (EGE). The benchmark dataset consists of 122 scanned student solutions from Tasks 13-19, each paired with expert-assigned scores, reference solutions, and detailed grading rubrics. Models were tested in three zero-shot modes: Without Answer (image and problem statement only), With Answer (image, problem, and student's answer), and With True Solution (all previous plus reference solution). Evaluation metrics included Accuracy (exact score match percentage), Quality Score (penalizing distance from true score), and Average Score Distance. The experimental setup required API access to various VLMs with structured output prompts for consistent score extraction.

## Key Results
- OpenAI o4-mini achieved the highest accuracy of 56.56% in the With Answer mode
- Quality Score of 78.17% indicates moderate alignment with expert grading
- Models performed best on algebraic tasks (Tasks 13, 15, 16, 19) and struggled with geometric tasks (Tasks 14, 17)
- Providing the student's answer improved performance compared to image-only input

## Why This Works (Mechanism)
Current VLMs can partially emulate expert grading by combining visual perception of handwritten mathematics with natural language reasoning about solution quality. The benchmark isolates grading capability from problem-solving ability, revealing that models can apply rubric criteria when provided sufficient context. The improvement from With Answer mode suggests models can leverage student responses to assess solution completeness and correctness, though they struggle with handwriting interpretation and nuanced rubric application.

## Foundational Learning
- Vision-Language Model (VLM) fundamentals: Understanding how VLMs integrate visual and textual processing for multimodal tasks is crucial for interpreting model behavior on handwritten mathematical content.
- Educational assessment rubrics: Knowledge of how expert graders apply criteria to mathematical solutions explains the grading challenge and evaluation metrics.
- Handwritten Mathematical Expression Recognition (HMER): Understanding OCR limitations for mathematical handwriting is essential for diagnosing error sources in grading accuracy.
- Zero-shot evaluation methodology: Familiarity with this approach explains why models were not fine-tuned on the specific task.
- Quality Score metric: Understanding this distance-based metric is crucial for interpreting model performance beyond simple accuracy.

## Architecture Onboarding
### Component Map
- Scanned images -> VLM visual encoder -> OCR extraction -> Text embedding -> Reasoning module -> Score prediction
- Problem statement -> Text embedding -> Context integration -> Rubric application -> Score prediction
- Reference solution -> Text embedding -> Comparative analysis -> Score prediction

### Critical Path
The critical path involves accurate handwriting recognition followed by rubric-based reasoning. Models must first correctly interpret the handwritten mathematical content, then apply grading criteria to assess solution quality against expert standards.

### Design Tradeoffs
- Zero-shot vs. fine-tuning: The study chose zero-shot prompting for broader applicability but may have sacrificed performance that fine-tuning could provide
- VLM selection: Included smaller, more accessible models rather than frontier models due to computational constraints
- Single exam focus: Provides deep evaluation on one standardized test but limits generalizability

### Failure Signatures
- Geometric task underperformance indicates visual-spatial reasoning limitations
- Handwriting interpretation errors suggest OCR accuracy issues
- Inconsistent rubric application reveals reasoning variability across similar solution types

### 3 First Experiments
1. Compare OCR accuracy across models on a subset of handwriting samples to isolate transcription errors
2. Test model performance on typed mathematical expressions versus handwritten to measure handwriting impact
3. Evaluate model consistency by regrading the same solutions multiple times with different prompts

## Open Questions the Paper Calls Out
- Can hybrid architectures combining general VLM perception with specialized Handwritten Mathematical Expression Recognition (HMER) models reduce error propagation and improve grading accuracy compared to end-to-end VLMs alone?
- How can VLMs be improved to perform robust comparative analysis between student solutions and reference solutions when the solution paths differ significantly?
- To what extent would fine-tuning VLMs on curriculum-specific assessment tasks improve their alignment with expert grading rubrics compared to zero-shot prompting?
- How do frontier models (e.g., OpenAI o3, Google Gemini 2.5 Pro) perform on this handwritten mathematical assessment task compared to the smaller models evaluated?

## Limitations
- The benchmark's narrow scope of 122 samples from a single exam limits generalizability to broader mathematical assessment contexts
- Handwriting interpretation remains a fundamental challenge, particularly for geometric constructions and messy script
- Rubric application introduces subjectivity even among expert graders, creating an imperfect ground truth
- Only smaller or mid-tier models were evaluated; the upper bound of current VLM capability on this task is unknown

## Confidence
- **High Confidence**: The benchmark's methodology for evaluating VLMs on grading rather than solving is sound and fills a genuine gap in existing evaluation frameworks
- **Medium Confidence**: The relative performance rankings among VLMs are likely accurate, though absolute accuracy figures may vary with different grading thresholds or rubric interpretations
- **Low Confidence**: Claims about specific model capabilities for educational assessment tools should be treated cautiously given the small sample size and single-exam context

## Next Checks
1. Expand evaluation to include additional mathematical exams and problem types beyond the Russian EGE to assess generalizability across different educational contexts and handwriting styles
2. Conduct human inter-rater reliability studies to establish baseline agreement rates and determine whether current VLM performance approaches human-level consistency
3. Test the benchmark with newer VLMs and updated models to establish whether the performance gap relative to human grading is narrowing over time