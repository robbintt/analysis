---
ver: rpa2
title: 'SGMem: Sentence Graph Memory for Long-Term Conversational Agents'
arxiv_id: '2509.21212'
source_url: https://arxiv.org/abs/2509.21212
tags:
- memory
- answer
- sgmem
- conv-47
- f4ea84fb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of managing long-term conversational
  memory in dialogue agents, where memory fragmentation occurs across raw dialogue
  and generated memory (summaries, facts, insights). The proposed SGMem (Sentence
  Graph Memory) framework organizes dialogue into sentence-level graphs within chunked
  units, explicitly modeling semantic associations across turns, rounds, and sessions.
---

# SGMem: Sentence Graph Memory for Long-Term Conversational Agents

## Quick Facts
- arXiv ID: 2509.21212
- Source URL: https://arxiv.org/abs/2509.21212
- Reference count: 22
- Primary result: SGMem achieves accuracy of 0.700/0.730 on LongMemEval and 0.526/0.532 on LoCoMo (Top-5/Top-10), outperforming strong baselines

## Executive Summary
SGMem (Sentence Graph Memory) addresses memory fragmentation in long-term conversational agents by organizing dialogue into sentence-level graphs within chunked units. The framework explicitly models semantic associations across turns, rounds, and sessions, combining retrieved raw dialogue with generated memory (summaries, facts, insights). By using sentence-level granularity and multi-hop graph traversal, SGMem provides coherent context for long-term question answering. Experiments demonstrate significant improvements over baselines on both LongMemEval and LoCoMo datasets, particularly for multi-session and temporal reasoning queries.

## Method Summary
SGMem represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. The method parses dialogues hierarchically, generates summaries/facts/insights via LLM, encodes all memory units with Sentence-BERT, and builds a sentence graph with membership and similarity edges. Retrieval combines vector search with h-hop graph traversal, ranking chunks by aggregate similarity and assembling top-K chunks with generated memory for LLM response generation. The framework uses 7 vector index tables (sessions, rounds, turns, sentences, summaries, facts, insights) and stores the sentence graph in Neo4j.

## Key Results
- SGMem achieves 0.700/0.730 accuracy on LongMemEval (Top-5/Top-10) and 0.526/0.532 on LoCoMo
- Outperforms strong baselines across all granularities and memory types
- Demonstrates effectiveness in reducing memory fragmentation and improving retrieval coherence, especially for multi-session and temporal reasoning queries
- h=1 hop traversal consistently outperforms h=0 (pure vector search) and h≥2 (noisy expansions)

## Why This Works (Mechanism)

### Mechanism 1
Sentence-level granularity reduces memory fragmentation better than turn/round/session-level retrieval. Sentences serve as atomic semantic units, enabling fine-grained evidence retrieval that would be buried in larger chunks. The KNN graph links semantically similar sentences across session boundaries, enabling cross-context associations.

### Mechanism 2
Multi-hop graph traversal recovers context that pure vector similarity misses. Initial vector retrieval identifies seed sentences, which are expanded via h-hop traversal on the sentence graph. These sentences are mapped back to parent chunks, which are then ranked by aggregate similarity, bridging fragmented evidence scattered across multiple chunks.

### Mechanism 3
Combining raw dialogue chunks with generated memory (summaries, facts, insights) outperforms either alone. Raw dialogue preserves fidelity and context; generated memory compresses and abstracts. SGMem retrieves both via separate index tables and aggregates them, with the sentence graph linking raw sentences to generated memory implicitly through shared chunk membership and similarity edges.

## Foundational Learning

- **Vector similarity search (dense retrieval)**: SGMem uses Sentence-BERT embeddings to index all memory units and retrieve candidates via cosine similarity. Quick check: Given a query embedding q and document embedding d, how is cosine similarity computed? What does a score of 1.0 mean?

- **k-nearest-neighbor graphs**: The sentence graph G is constructed by linking each sentence to its k most similar neighbors, enabling multi-hop traversal. Quick check: If k=3 and you have 100 sentences, what is the maximum number of edges in the KNN graph? Is it symmetric by default?

- **RAG context assembly**: SGMem aggregates retrieved chunks, summaries, facts, and insights into a single context window for the LLM. Quick check: If you retrieve 5 sessions (~500 tokens each), 3 summaries (~100 tokens each), and 10 facts (~20 tokens each), what is the total context size? How does this compare to a 128K token limit?

## Architecture Onboarding

- **Component map**: Input Processing (NLTK sentence segmentation + LLM generation) -> Indexing (7 vector index tables) -> Graph Construction (chunk-sentence membership + KNN similarity edges) -> Storage (ElasticSearch + Neo4j) -> Retrieval (vector search + h-hop traversal) -> Generation (LLM with assembled context)

- **Critical path**: 1) New dialogue arrives → segment into sentences → embed → add to vector index; 2) Periodically generate summaries/facts/insights → embed → add to vector index; 3) Query arrives → vector retrieval across 7 tables → seed sentences identified; 4) Graph traversal from seeds (h hops) → collect neighbor sentences; 5) Map sentences to parent chunks → rank chunks by aggregate score; 6) Assemble top-K chunks + generated memory → LLM generates response

- **Design tradeoffs**: Chunk granularity (sessions capture more context but are noisier; turns are precise but fragmented); h-hop depth (h=1 balances recall and precision; h=0 is pure vector search; h≥2 adds noise); KNN size k (larger k increases connectivity but may link irrelevant sentences); Retriever choice (Dense achieves higher peak accuracy but requires tuning; BM25 is more robust)

- **Failure signatures**: Temporal inconsistency (outdated facts cause conflicting answers); Context overflow (too many nodes/chunks exceed LLM window); Graph sparsity (high γ threshold prevents edge formation); Hallucinated facts (LLM-generated content introduces errors)

- **First 3 experiments**: 1) Baseline comparison on LongMemEval: Implement RAG-SMFI and compare against SGMem-SMFI, measuring accuracy gap and analyzing query type benefits; 2) Hyperparameter sweep (h, k, n): Run ablations on LongMemEval with h∈{0,1,2}, k∈{1,3,5}, n∈{5,10,15}, identifying optimal configuration under 8K token limit; 3) Error analysis on LoCoMo temporal queries: Sample 50 temporal reasoning questions where SGMem succeeds and RAG fails, inspecting retrieved sentences to verify h-hop traversal surfaces correct sessions

## Open Questions the Paper Calls Out

- **Hallucination detection**: How can SGMem be augmented to detect and mitigate hallucinations or factual inconsistencies present in LLM-generated memories? The authors state SGMem does not yet address hallucinations or factual inconsistencies in generated content.

- **Scalability optimization**: Can the sentence-graph construction and maintenance process be optimized to handle extremely long dialogue histories efficiently? The paper notes SGMem has not been optimized for efficiency at scale and may incur computational overhead for very large histories.

- **Real-world dynamics**: How effective is SGMem in dynamic, real-world settings involving multimodal inputs or streaming dialogue updates? The authors acknowledge current benchmarks may not capture real-world conversational dynamics such as multimodal contexts or streaming updates.

## Limitations

- Hallucination propagation: The framework relies on LLM-generated summaries, facts, and insights that may contain hallucinations, which directly propagate into final responses without verification mechanisms.

- Granularity mapping ambiguity: The paper doesn't explicitly specify which granularity (sessions/rounds/turns) maps to each variant (SGMem-TF vs SGMem-SF vs SGMem-SMFI), creating potential reproducibility issues.

- Temporal reasoning fragility: Multi-hop traversal can add noise, especially in LoCoMo's longer conversations, with optimal k=1 suggesting the sentence graph becomes less reliable with longer, noisier histories.

## Confidence

- **High Confidence**: Sentence-level retrieval reduces memory fragmentation compared to coarser granularities; combining raw dialogue with generated memory improves performance; h=1 hop traversal outperforms h=0 on both datasets.

- **Medium Confidence**: The sentence graph's k-NN construction effectively bridges semantic gaps across session boundaries; the specific k=3 (LongMemEval) and k=1 (LoCoMo) hyperparameters are optimal; context aggregation formula produces coherent evidence.

- **Low Confidence**: Generated memory (summaries/facts/insights) consistently provides factual accuracy; the framework scales to real-world production use without context overflow; the LLM-as-a-Judge evaluation perfectly captures human judgment quality.

## Next Checks

1. **Hallucination analysis**: Sample 50 generated summaries/facts/insights from the test set and verify factual consistency against original dialogue. Measure hallucination rate and assess impact on final QA accuracy.

2. **Temporal reasoning ablation**: For 30 temporal queries where SGMem succeeds but RAG fails, trace the exact sentence graph traversal path. Verify that h-hop expansion specifically surfaces the temporally relevant session rather than relying on pure vector similarity.

3. **Context overflow stress test**: Systematically increase n (max nodes) and K (top-k chunks) parameters while monitoring token counts. Identify the exact point where context exceeds LLM window and measure accuracy degradation to quantify robustness limits.