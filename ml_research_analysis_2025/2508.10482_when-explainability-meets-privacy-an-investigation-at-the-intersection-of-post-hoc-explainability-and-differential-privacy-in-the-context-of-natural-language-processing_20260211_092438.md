---
ver: rpa2
title: 'When Explainability Meets Privacy: An Investigation at the Intersection of
  Post-hoc Explainability and Differential Privacy in the Context of Natural Language
  Processing'
arxiv_id: '2508.10482'
source_url: https://arxiv.org/abs/2508.10482
tags:
- privacy
- explainability
- methods
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the privacy-explainability trade-off in
  NLP by applying differentially private text rewriting methods to datasets before
  training models and evaluating the impact on post-hoc explainability. Three DP methods
  (TEM, DP-Prompt, DP-BART) and four explainers (Gradient, IG, LIME, SHAP) are used
  across three datasets (SST2, AG News, Trustpilot) with five PLMs.
---

# When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing

## Quick Facts
- arXiv ID: 2508.10482
- Source URL: https://arxiv.org/abs/2508.10482
- Reference count: 23
- Primary result: Privacy typically reduces explainability, but certain configurations (LIME/SHAP, AG News task) achieve strong performance even under tight privacy budgets, suggesting privacy and explainability can coexist.

## Executive Summary
This study investigates the privacy-explainability trade-off in NLP by applying differentially private text rewriting methods to datasets before training models and evaluating the impact on post-hoc explainability. Three DP methods (TEM, DP-Prompt, DP-BART) and four explainers (Gradient, IG, LIME, SHAP) are used across three datasets (SST2, AG News, Trustpilot) with five PLMs. Results show that LIME and SHAP generally maintain better explainability under privacy constraints, with AG News showing greater resilience than SST2. While privacy typically reduces explainability, certain configurations achieve strong performance even under tight privacy budgets, suggesting privacy and explainability can coexist. The study provides practical recommendations for balancing privacy, utility, and explainability in NLP applications.

## Method Summary
The method applies three differentially private text rewriting techniques (TEM word-level, DP-Prompt with FLAN-T5, DP-BART document-level) at various epsilon values to three datasets (SST2, AG News, Trustpilot). Five pre-trained language models (BERT-Base/Large, RoBERTa-Base/Large, DeBERTa-Base) are fine-tuned on the privatized datasets. Four post-hoc explainers (Gradient, Integrated Gradients, LIME, SHAP) are then applied to generate explanations. Explainability is measured via faithfulness metrics (AOPC comprehensiveness/sufficiency and soft variants), combined into a composite score with utility (F1) weighted by α parameter.

## Key Results
- LIME and SHAP maintain better explainability under privacy constraints compared to gradient-based methods
- AG News topic classification shows greater resilience to privacy degradation than SST2 sentiment analysis
- Base models consistently outperform large models in the privacy-explainability trade-off
- Composite scores decrease as model size increases, suggesting smaller models may be preferable under privacy constraints

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Based Explainer Resilience
DP rewriting introduces noise by substituting words or perturbing embeddings. Gradient-based methods compute derivatives directly from input features to outputs, making them highly sensitive to local noise and the specific artifacts of privatized tokens. In contrast, LIME and SHAP use local sampling and surrogate models, which may average out the variance introduced by DP noise, yielding more stable importance scores.

### Mechanism 2: Task Semantic Density
In topic classification (AG News), the semantic signal is distributed across many contextually aligned keywords; replacing a few via DP rewriting retains enough signal for the model and explainer. However, in sentiment analysis (SST2), the signal often relies on subtle, sparse polarity cues; destroying these cues creates a sharp drop in both utility and the explainer's ability to identify faithful features.

### Mechanism 3: Model Capacity Regularization
Large models have the capacity to overfit to the idiosyncratic noise patterns or artifacts of DP-rewritten text. While they might fit the training distribution, they fail to generalize to non-private test distributions, degrading explanation faithfulness. Base models, having lower capacity, are constrained from memorizing this noise, effectively acting as a regularizer.

## Foundational Learning

**Concept: Local Differential Privacy (LDP) in NLP**
Why needed here: To understand the input-level guarantees. Unlike global DP (protecting dataset participation), LDP protects the content of a specific text input by making it indistinguishable from neighbors before it reaches the model.
Quick check question: How does the mechanism of TEM (word-level) differ from DP-BART (document-level) in terms of where the noise is injected?

**Concept: Faithfulness vs. Plausibility in Explainability**
Why needed here: The paper measures "explainability" via faithfulness metrics (AOPC—how much removing tokens changes the prediction), not human plausibility. Understanding this distinction is critical to interpreting the "composite score."
Quick check question: If a model's explanation is "faithful" but the model accuracy is low, what does that imply about the model's decision process?

**Concept: The Composite Score (α weighting)**
Why needed here: The authors introduce a composite score CS(m,α) = α*F1 + (1-α)*Explainability. This allows engineers to tune the system based on whether they prioritize privacy/utility (high α) or interpretability (low α).
Quick check question: If you are building a medical diagnosis tool where a wrong explanation is dangerous, should you set α to 0.25 or 0.75?

## Architecture Onboarding

**Component map:**
Raw Text (SST2/AG News/Trustpilot) → Privacy Rewriter (TEM / DP-Prompt / DP-BART) → Private Corpus → Model Backbone (BERT/RoBERTa/DeBERTa) → Explainer Module (LIME/SHAP/Gradient) → Evaluator: Composite Score Calculation

**Critical path:**
1. Privatization: The choice of ε (privacy budget) and DP method sets the "hard limit" on available semantic signal.
2. Explainer Selection: Choosing Gradient/IG over LIME/SHAP on high-noise data (low ε) will likely result in failed explainability metrics.

**Design tradeoffs:**
- Utility vs. Explainability (α): You must explicitly define the weight α. A high utility focus allows you to use stricter privacy budgets (lower ε) while still "passing" the composite score, but explanation quality will degrade.
- Granularity vs. Coherence: Word-level DP (TEM) is computationally cheaper but destroys coherence; Generative DP (DP-BART) preserves coherence but requires massive ε values (e.g., 500+) to function.

**Failure signatures:**
- Gradient Flatlining: If Gradient or IG return near-zero or highly variant scores, the privacy budget (ε) is likely too tight for these methods.
- Large Model Instability: If BERT-Large shows high standard deviation and lower scores than BERT-Base, the model is overfitting to privatization artifacts.

**First 3 experiments:**
1. Reproduce the Explainer Gap: Run the pipeline on SST2 using TEM (ε=1) with both Gradient and LIME to confirm that LIME outperforms Gradient in the composite score.
2. Identify the "Sweet Spot": On AG News, sweep DP-Prompt temperatures (T=1.25 to 1.75) with α=0.5 to find the configuration where privacy and explainability coexist (Composite Score > Baseline).
3. Verify Model Size Effect: Compare BERT-Base vs. BERT-Large on the Trustpilot dataset to validate the finding that base models offer better trade-offs in this constrained setting.

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameter sensitivity: Results may be highly sensitive to specific fine-tuning hyperparameters which are not disclosed.
- DP mechanism fidelity: The actual implementation of TEM, DP-Prompt, and DP-BART is not fully specified.
- Generalizability to other tasks: Results are based on sentiment and topic classification; performance on other NLP tasks may differ.

## Confidence
- High Confidence: The observation that perturbation-based explainers (LIME/SHAP) outperform gradient-based methods (Gradient/IG) under DP constraints.
- Medium Confidence: The claim that task semantic density influences privacy-explainability trade-offs.
- Medium Confidence: The model capacity regularization effect (base models outperforming large models).

## Next Checks
1. **Hyperparameter ablation study**: Reproduce key results (SST2 TEM ε=1, AG News DP-Prompt T=1.5) while varying learning rates (1e-5, 2e-5, 3e-5) and batch sizes (8, 16, 32) to quantify sensitivity to training configurations.

2. **Extended task evaluation**: Apply the same pipeline to a different NLP task family (e.g., natural language inference or question answering) to test whether the perturbation-based explainer advantage generalizes beyond sentiment and topic classification.

3. **Human evaluation pilot**: Conduct a small-scale user study (n=20-30) comparing explanations from Gradient vs. LIME on privatized vs. non-privatized models for SST2, measuring both faithfulness (via AOPC) and plausibility (via Likert-scale ratings) to bridge the current measurement gap.