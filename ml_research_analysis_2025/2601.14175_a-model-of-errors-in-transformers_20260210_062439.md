---
ver: rpa2
title: A model of errors in transformers
arxiv_id: '2601.14175'
source_url: https://arxiv.org/abs/2601.14175
tags:
- output
- accuracy
- errors
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the error rates of large language models (LLMs)
  on deterministic tasks involving repetitive processing of tokens from a small set.
  The authors propose that errors arise when small errors in the attention mechanism
  accumulate across the context and cross a threshold, leading to incorrect predictions.
---

# A model of errors in transformers

## Quick Facts
- arXiv ID: 2601.14175
- Source URL: https://arxiv.org/abs/2601.14175
- Authors: Suvrat Raju; Praneeth Netrapalli
- Reference count: 40
- Primary result: Errors in LLMs on repetitive deterministic tasks accumulate from attention noise and follow a two-parameter gamma-function formula

## Executive Summary
This paper proposes a quantitative model for errors in large language models when performing deterministic tasks involving repetitive token processing. The authors observe that small errors in the attention mechanism accumulate across the context and, when they cross a threshold, lead to incorrect predictions. Based on this insight, they derive a two-parameter formula relating accuracy to task complexity that fits empirical data remarkably well across multiple models and tasks. The work suggests that errors on long repetitive tasks result from attention noise accumulation rather than collapse of reasoning or inability to express compositional functions.

## Method Summary
The authors test their error accumulation model across 8 different deterministic tasks (list reversal, nested linear transformations, dynamic programming, tower of Hanoi, decimal/binary addition, and multiplication) using approximately 200,000 prompts across 3 state-of-the-art models (Gemini 2.5 Flash, Gemini 2.5 Pro, and DeepSeek R1). They measure accuracy as a function of task complexity and compare it against their proposed formula: accuracy = 1/Γ(q/2) × γ(q/2, q/2rc²α), where r is a noise rate, q is the number of plausible error directions, c is task complexity, and α is a scaling parameter. The model provides excellent agreement with empirical accuracy in most cases, with O(1) values of q that can be interpreted in terms of plausible error directions.

## Key Results
- Empirical accuracy across all tested tasks and models follows the proposed two-parameter gamma-function formula with excellent quantitative agreement
- The noise rate parameter r and error directions parameter q can be interpreted meaningfully in terms of the model's attention mechanism and task structure
- Errors can be reduced by modifying prompts to help the model focus attention more effectively on relevant tokens
- The model works across different task types (reversal, transformation, dynamic programming, recursion) and different state-of-the-art models

## Why This Works (Mechanism)
The proposed mechanism suggests that errors arise from the accumulation of small noise perturbations in the attention mechanism as the model processes tokens sequentially. When these accumulated errors cross a critical threshold, the model's predictions become incorrect. This accumulation process is modeled using incomplete gamma functions, which naturally capture the probabilistic nature of error propagation through repeated attention operations.

## Foundational Learning
- Attention mechanism noise: Small perturbations in attention weights accumulate across processing steps, eventually causing prediction errors
- Incomplete gamma function: Mathematical framework for modeling cumulative probability distributions of error accumulation
- Task complexity scaling: Relationship between the number of processing steps and the likelihood of error threshold crossing
- Attention-focused prompting: Prompt engineering techniques that help models allocate attention more effectively to relevant tokens

## Architecture Onboarding
- Component map: Attention mechanism -> Token processing -> Context accumulation -> Prediction output
- Critical path: Input tokens → Attention weights → Weighted representations → Next token prediction
- Design tradeoffs: Model accuracy vs. context length vs. attention precision in deterministic tasks
- Failure signatures: Accuracy degradation follows gamma-function decay as task complexity increases
- First experiments: 1) Measure accuracy decay on list reversal tasks of increasing length, 2) Test prompt modifications to improve attention focus, 3) Vary noise injection levels to validate the accumulation model

## Open Questions the Paper Calls Out
None

## Limitations
- The model focuses exclusively on deterministic, repetitive token processing tasks and may not generalize to more complex reasoning tasks
- The formula's two-parameter nature (noise rate r and error directions q) may risk overfitting given the large number of tasks tested
- The relationship between task complexity c and the model's internal representations remains phenomenological rather than mechanistic

## Confidence
- High confidence: Empirical observation that errors accumulate in deterministic repetitive tasks and can be reduced through attention-focused prompt engineering
- Medium confidence: Interpretation that errors fundamentally arise from attention noise accumulation rather than reasoning collapse or compositional limitations
- Medium confidence: Specific mathematical form of the error rate formula and assignment of universal scaling parameters

## Next Checks
1. Test the error accumulation model on non-deterministic tasks involving probabilistic reasoning, creative generation, or tasks where multiple correct outputs exist for a single input
2. Apply the model to smaller language models (below 1B parameters) and examine whether the relationship between accuracy, complexity, and error accumulation holds across different model scales
3. Conduct ablation studies where specific attention heads are disabled or modified to directly measure their contribution to error accumulation rates