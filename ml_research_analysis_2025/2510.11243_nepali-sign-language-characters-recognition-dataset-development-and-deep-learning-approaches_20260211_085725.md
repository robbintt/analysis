---
ver: rpa2
title: 'Nepali Sign Language Characters Recognition: Dataset Development and Deep
  Learning Approaches'
arxiv_id: '2510.11243'
source_url: https://arxiv.org/abs/2510.11243
tags:
- sign
- language
- recognition
- classification
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed the first benchmark dataset for Nepali Sign
  Language (NSL), consisting of 36 gesture classes with 1,500 samples per class, totaling
  54,000 images with plain and random backgrounds. The dataset was designed to capture
  structural and visual features of NSL gestures.
---

# Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches

## Quick Facts
- **arXiv ID:** 2510.11243
- **Source URL:** https://arxiv.org/abs/2510.11243
- **Reference count:** 10
- **Primary result:** First benchmark dataset for Nepali Sign Language with 54,000 images, achieving 90.45% accuracy using MobileNetV2

## Executive Summary
This study presents the first benchmark dataset for Nepali Sign Language (NSL) recognition, addressing the critical gap in resources for this underexplored sign language. The researchers developed a comprehensive dataset containing 36 gesture classes with 1,500 samples per class, totaling 54,000 images captured against both plain and random backgrounds. Using transfer learning approaches with pre-trained CNN architectures, the study demonstrates that deep learning models can effectively recognize NSL gestures, with MobileNetV2 achieving 90.45% accuracy and ResNet50 reaching 88.78%.

The work establishes a foundation for future research in low-resource sign language recognition by providing both the dataset and validated deep learning methodologies. The study highlights the effectiveness of transfer learning for sign language recognition tasks, particularly in settings where large amounts of domain-specific training data are not available. The results suggest that convolutional neural networks can successfully capture the structural and visual features of NSL gestures, opening pathways for developing assistive technologies for the Nepali deaf community.

## Method Summary
The researchers employed transfer learning and fine-tuning techniques on pre-trained CNN architectures to recognize Nepali Sign Language gestures. They utilized MobileNetV2 and ResNet50 as the primary model architectures, leveraging their pre-trained weights on large-scale image datasets to initialize the learning process. The models were then fine-tuned on the newly developed NSL dataset, allowing them to adapt to the specific characteristics of sign language gestures. The dataset was carefully constructed to capture both structural features (hand shapes and positions) and visual features (skin tones, backgrounds, and lighting conditions) of NSL gestures.

## Key Results
- MobileNetV2 achieved 90.45% classification accuracy on the NSL dataset
- ResNet50 achieved 88.78% classification accuracy on the same dataset
- The dataset contains 54,000 images across 36 gesture classes with 1,500 samples per class
- Transfer learning demonstrated effectiveness in low-resource sign language recognition

## Why This Works (Mechanism)
The success of the approach stems from the combination of a well-structured dataset and the power of transfer learning. By using pre-trained CNN models, the system leverages learned features from general image recognition tasks that are transferable to sign language recognition. The dataset's design, which includes both plain and random backgrounds, helps the models learn robust features that generalize across different visual conditions. The convolutional layers effectively capture spatial hierarchies in hand gestures, while the fine-tuning process adapts these general features to the specific characteristics of NSL gestures.

## Foundational Learning

**Transfer Learning**
*Why needed:* Enables effective learning with limited data by leveraging knowledge from pre-trained models
*Quick check:* Verify pre-trained model weights are from a sufficiently large and diverse dataset

**Convolutional Neural Networks**
*Why needed:* Specialized architecture for processing spatial hierarchies in images, particularly effective for hand gesture recognition
*Quick check:* Confirm convolutional layers properly capture spatial relationships in gesture images

**Sign Language Recognition**
*Why needed:* Understanding the specific challenges of gesture-based communication including temporal and spatial features
*Quick check:* Validate dataset covers key gesture variations and edge cases

**Data Augmentation**
*Why needed:* Increases model robustness and generalization by introducing variations during training
*Quick check:* Ensure augmentation techniques preserve essential gesture features while adding diversity

## Architecture Onboarding

**Component Map:**
Dataset (NSL images) -> Pre-trained CNN (MobileNetV2/ResNet50) -> Transfer Learning + Fine-tuning -> Classification Layer -> Output (gesture class)

**Critical Path:**
Data preprocessing and augmentation -> Transfer learning initialization -> Fine-tuning on NSL dataset -> Validation and testing -> Performance evaluation

**Design Tradeoffs:**
MobileNetV2 offers faster inference and lower computational requirements but may sacrifice some accuracy compared to ResNet50, which provides higher accuracy but requires more computational resources. The choice between them depends on the deployment context and resource constraints.

**Failure Signatures:**
- Poor performance on gestures with similar hand configurations
- Reduced accuracy with varying lighting conditions or occlusions
- Difficulty generalizing to signers with different skin tones
- Sensitivity to background complexity beyond what was included in training

**First Experiments:**
1. Evaluate model performance on a held-out test set with varying background complexities
2. Test model robustness across different lighting conditions and skin tones
3. Assess cross-validation performance using k-fold validation on the dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset limited to only 36 gesture classes, potentially missing broader NSL vocabulary
- High accuracy may be influenced by controlled image capture conditions
- Does not address real-world challenges like varying lighting, different skin tones, or occlusions
- Limited testing on dynamic, real-world environments and signer variations

## Confidence
- Dataset and deep learning approaches effectively capture NSL gesture features: **Medium** (limited scope to specific gestures and conditions)
- Transfer learning effectiveness for low-resource sign language recognition: **High** (aligns with established practices)

## Next Checks
1. Expand the dataset to include a more comprehensive range of NSL gestures and real-world variations in lighting, backgrounds, and user demographics
2. Conduct cross-validation with signers of different ages, genders, and skin tones to assess model robustness
3. Test the system in dynamic, real-world environments to evaluate practical performance under real conditions