---
ver: rpa2
title: Dynamic Classifier-Free Diffusion Guidance via Online Feedback
arxiv_id: '2509.16131'
source_url: https://arxiv.org/abs/2509.16131
tags:
- guidance
- dynamic
- prompt
- alignment
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamic framework for adjusting the classifier-free
  guidance (CFG) scale during diffusion sampling, based on online feedback from small
  latent-space evaluators. Unlike static or heuristic schedules, the method uses greedy
  search to select optimal CFG values per timestep, guided by evaluators for alignment,
  visual quality, and specialized capabilities (text rendering, numerical reasoning).
---

# Dynamic Classifier-Free Diffusion Guidance via Online Feedback

## Quick Facts
- **arXiv ID**: 2509.16131
- **Source URL**: https://arxiv.org/abs/2509.16131
- **Reference count**: 16
- **Primary result**: Dynamic CFG selection improves alignment and visual quality with up to 53.8% human preference win-rate over Imagen 3 baseline

## Executive Summary
This paper introduces a dynamic framework for adjusting the classifier-free guidance (CFG) scale during diffusion sampling, based on online feedback from small latent-space evaluators. Unlike static or heuristic schedules, the method uses greedy search to select optimal CFG values per timestep, guided by evaluators for alignment, visual quality, and specialized capabilities (text rendering, numerical reasoning). Experiments on LDM and Imagen 3 show consistent improvements in both alignment and visual quality, with up to 53.8% human preference win-rate over the Imagen 3 baseline. The method is lightweight, adding only ~1% computational overhead, and demonstrates robustness across model families and prompt types, validating that optimal guidance is inherently dynamic and prompt-dependent.

## Method Summary
The method implements dynamic CFG scale selection via greedy search over a discrete set of values {1, 3, 7.5, 11, 15} at each denoising timestep. Small latent-space evaluators (based on CLIP and reward models) score each CFG candidate's output in latent space before decoding. The best-scoring CFG value is selected for that timestep. Three evaluator types are used: a latent CLIP for alignment (fine-tuned on WebLI with noise augmentation), a discriminator-based reward model for human preference, and specialized evaluators for text rendering and numerical reasoning. Adaptive weighting combines multiple evaluators based on their relative performance across timesteps. The approach adds minimal overhead (~1%) and requires only black-box access to the diffusion model.

## Key Results
- Dynamic CFG selection consistently improves Gecko alignment scores across LDM variants and StableDiffusion
- Visual quality improvements measured by FID on MS-COCO, with best results using adaptive evaluator weighting
- Up to 53.8% human preference win-rate over Imagen 3 baseline when transferring to the proprietary model's latent space
- Robust performance across diverse prompt sets (Gecko, MS-COCO, GenAI-Bench) and model families

## Why This Works (Mechanism)
The method works because optimal CFG scale is not static but depends on both the prompt content and the current denoising stage. Early timesteps benefit from higher CFG for strong signal injection, while later timesteps require lower CFG to preserve fine details. By evaluating candidates in latent space before full decoding, the approach avoids expensive full-image scoring while maintaining sensitivity to both alignment and visual quality. The greedy search is computationally feasible because it reuses the unconditional and conditional predictions across all CFG candidates.

## Foundational Learning
- **Classifier-free guidance (CFG)**: A technique to steer diffusion model outputs toward conditional generation by interpolating between conditional and unconditional predictions. Needed because static CFG values often represent suboptimal tradeoffs between alignment and visual quality.
- **Latent diffusion models (LDMs)**: Diffusion models operating in compressed latent space rather than pixel space. Quick check: Verify VAE decoder is properly conditioned on latent inputs during evaluation.
- **CLIP embeddings**: Multimodal embeddings mapping images and text to a shared space. Quick check: Ensure latent CLIP is fine-tuned with appropriate noise augmentation matching the target diffusion model.
- **Greedy search**: Iterative selection of locally optimal choices. Quick check: Confirm all CFG candidates reuse the same unconditional/conditional predictions to minimize computation.
- **Evaluator weighting**: Combining multiple scoring functions with importance weights. Quick check: Verify adaptive weights are computed correctly to prevent numerical instability.

## Architecture Onboarding

**Component Map:** Prompt → LDM → CFG candidates (s_t ∈ {1,3,7.5,11,15}) → Latent evaluators → Best CFG selection → LDM step → Output

**Critical Path:** At each timestep: (1) generate unconditional and conditional predictions, (2) compute CFG candidates, (3) score with evaluators, (4) select best CFG, (5) apply to denoising step

**Design Tradeoffs:** Discrete search over 5 CFG values balances coverage and computation vs. continuous optimization; latent evaluation reduces cost vs. pixel-space scoring but requires careful evaluator training

**Failure Signatures:** Evaluator instability early in sampling (expected, use time-weighted loss); inappropriate CFG selection for specialized domains (requires domain-specific evaluators)

**First Experiments:** 1) Train latent alignment evaluator on WebLI with noise augmentation; 2) Implement greedy CFG selection on a simple prompt set; 3) Test adaptive vs. fixed evaluator weighting on MS-COCO

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the online feedback framework be extended to dynamically optimize other inference parameters beyond the CFG scale, such as noise schedules or sampler hyperparameters?
- **Basis in paper**: [explicit] The conclusion states the framework "can be expanded to perform inference-time search beyond the CFG schedule."
- **Why unresolved**: The current implementation focuses exclusively on selecting the CFG scale $s_t$ and does not adjust other sampling parameters.
- **What evidence would resolve it**: Experiments applying the greedy search mechanism to parameters like time-step skipping or stochasticity (eta).

### Open Question 2
- **Question**: Does replacing the discrete greedy search with a continuous optimization method further improve generation fidelity?
- **Basis in paper**: [inferred] The method restricts the search to a discrete set of 5 values ($S=\{1, 3, 7.5, 11, 15\}$), potentially missing optimal intermediate values.
- **Why unresolved**: The paper evaluates the proposed discrete search but does not compare it against continuous optimization strategies like gradient descent.
- **What evidence would resolve it**: A comparative study measuring performance differences between discrete greedy search and continuous optimization over the same evaluator rewards.

### Open Question 3
- **Question**: How effectively does the framework scale to highly specialized domains where training data for latent evaluators is scarce?
- **Basis in paper**: [explicit] The authors note the approach can be "extended to more specialized skills given appropriate evaluators" but require specific fine-tuning data.
- **Why unresolved**: The paper tests general skills (text, counting) with available data, leaving low-resource domains unexplored.
- **What evidence would resolve it**: Application of the method to niche domains (e.g., medical imaging) using limited evaluator training sets.

## Limitations
- Effectiveness depends heavily on quality and diversity of evaluator training data, with potential performance degradation on out-of-distribution prompts
- Transfer to Imagen 3 relies on proprietary latent space and VAE, limiting verification of cross-model generalization
- Human preference data collection methodology is not fully specified, raising reproducibility concerns

## Confidence
- **High Confidence**: Dynamic CFG selection consistently improves Gecko alignment scores across multiple prompt sets and models
- **Medium Confidence**: Visual quality improvements (FID) are demonstrated but depend on specific evaluator weighting schemes
- **Medium Confidence**: 53.8% human preference win-rate over Imagen 3 baseline is compelling but based on single proprietary model transfer

## Next Checks
1. Test dynamic CFG with evaluators trained on open-source latent diffusion models (e.g., SDXL) to verify cross-model robustness without proprietary Imagen 3
2. Conduct ablation studies on evaluator weighting schemes (linear vs adaptive) to isolate contribution of weighting from CFG selection
3. Evaluate performance on out-of-distribution prompts (e.g., abstract art, specialized technical domains) to assess generalization limits