---
ver: rpa2
title: A Multimodal, Multitask System for Generating E Commerce Text Listings from
  Images
arxiv_id: '2510.21835'
source_url: https://arxiv.org/abs/2510.21835
tags:
- price
- attributes
- text
- attribute
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-task, hierarchical architecture for
  generating e-commerce product listings from images, addressing the issues of factual
  inconsistency in direct vision-language models and architectural fragmentation in
  siloed approaches. The core idea is to first predict structured product attributes
  (e.g., color, hemline, neck style) and price jointly using a vision encoder, then
  use these predicted attributes to guide a text decoder via a prompt, ensuring factual
  consistency.
---

# A Multimodal, Multitask System for Generating E Commerce Text Listings from Images

## Quick Facts
- arXiv ID: 2510.21835
- Source URL: https://arxiv.org/abs/2510.21835
- Reference count: 30
- Primary result: Hierarchical multimodal model reduces hallucinations by 44.5% (12.7%→7.1%) vs. non-hierarchical ablation

## Executive Summary
This paper proposes a hierarchical, multi-task architecture for generating e-commerce product listings from images. The key innovation is predicting structured product attributes and price jointly using a vision encoder, then using these predictions to condition a text decoder via a prompt. This approach ensures factual consistency while maintaining fluency. The model achieves a 44.5% relative reduction in hallucination rate compared to direct vision-language models, with improved attribute classification and price regression performance, though with a slight trade-off in text fluency.

## Method Summary
The system uses a ViT-Base/16 encoder to extract visual features, which are then processed by two MLP heads for attribute classification (12 attributes) and price regression. A projection layer transforms the visual embedding to 512 dimensions, which is concatenated with a prompt token encoding predicted attributes. This combined representation conditions a T5-Small decoder to generate product listings. The model is trained end-to-end with a weighted joint loss (0.4·L_attr + 0.1·L_price + 0.5·L_text) using Adam optimizer (LR=1e-4, batch=16) for 10 epochs. Inference first predicts attributes, constructs a detailed prompt, then generates text with beam=4.

## Key Results
- 44.5% relative reduction in hallucination rate (12.7%→7.1%) compared to non-hierarchical ablation
- Multi-task learning improves attribute F1 from 0.317 to 0.338 (6.6% gain) and price R² from 0.460 to 0.476 (3.6% gain)
- 3.5× faster inference due to constrained beam search from hierarchical prompting
- Trade-off: 3.5% lower ROUGE-L score (0.403 vs 0.429) when using hierarchical approach

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Generation for Hallucination Reduction
Structured attribute prediction before text generation reduces factual hallucinations by constraining the decoder's output space. The model first predicts discrete attributes via classification heads, then embeds these predictions into a prompt that conditions the text decoder. This creates a "self-contained, verifiable knowledge source" that anchors generation. If attribute prediction accuracy degrades significantly (e.g., F1 < 0.15), grounding becomes counterproductive—the prompt will inject errors.

### Mechanism 2: Multi-Task Learning for Holistic Visual Representation
Jointly training attribute classification and price regression on a shared vision encoder improves both tasks by capturing latent visual cues. Backpropagation through a combined loss forces the ViT encoder to learn features useful for both tasks—e.g., craftsmanship and material quality signals that inform price but aren't explicit attribute labels. If tasks are unrelated (e.g., adding a task with no visual basis), gradient interference could degrade both.

### Mechanism 3: Grounded Prompting as Regularization
Providing predicted attributes as an explicit prompt acts as a strong regularizer, trading fluency (ROUGE-L) for factual consistency. The prompt constrains beam search by explicitly specifying content to include, reducing the decoder's "creative" search space where hallucinations emerge. If the decoder is scaled significantly (e.g., GPT-4 class), it may override prompt constraints or generate more fluently but still hallucinate.

## Foundational Learning

- **Concept: Vision Transformer (ViT) patch embeddings**
  - Why needed here: The encoder processes images as 16×16 patches with self-attention. Understanding pooled representation vs. patch-level outputs is critical for attaching task heads.
  - Quick check question: Where does the attribute classification head attach—to patch tokens or the [CLS] token?

- **Concept: Cross-entropy vs. MSE loss scale balancing**
  - Why needed here: The joint loss combines classification (cross-entropy), regression (MSE), and text generation losses with different magnitudes. Improper weighting causes one head to dominate.
  - Quick check question: If price loss is 10× larger than attribute loss, what happens to gradient flow?

- **Concept: Beam search and output constraints**
  - Why needed here: Hierarchical prompting reduces latency (3.5× faster) by constraining the decoder's search space. Understanding beam search mechanics explains this efficiency gain.
  - Quick check question: How does a more constrained prompt affect beam search convergence?

## Architecture Onboarding

- **Component map:** ViT-Base/16 (86M params) → pooled embedding (768-dim) → MLP heads (attribute classifier + price regressor) → projection (768→512) → T5-Small decoder (60M params) → generated text

- **Critical path:** Image → ViT patches → pooled embedding → MLP heads → attribute logits + price → predicted attributes → formatted prompt string → prompt tokens + projected visual embedding → T5 decoder → generated text

- **Design tradeoffs:**
  - Fluency vs. Factual Consistency: Hierarchical grounding reduces hallucinations but lowers ROUGE-L by ~3.5%
  - Latency vs. Model Size: Adding MLP heads (+0.4M params) actually reduces inference time by 3.5× by constraining beam search
  - Attribute Coverage vs. Learnability: 12 attributes with 399 possible labels creates long-tail classification challenges (see Appendix C: Fabric F1=0.185)

- **Failure signatures:**
  - High hallucination + low attribute F1: Encoder-attribute pathway broken; check loss weights (α may be too low)
  - Coherent but wrong facts: Decoder ignoring prompt; check prompt formatting and tokenization
  - Gibberish output (like GIT-base): Decoder undertrained; pre-training on text corpus matters (T5-Small outperforms GIT-base despite fewer params)

- **First 3 experiments:**
  1. **Ablate hierarchy:** Run inference with empty prompt (only visual embedding) to reproduce the 12.7% → 7.1% hallucination reduction claim.
  2. **Siloed vs. MTL:** Train two separate ViT models (price-only, attributes-only) and compare R2/F1 to joint training.
  3. **Loss weight sweep:** Test {α=0.5, β=0, γ=0.5} to verify the 67% hallucination spike when attributes are removed from the loss.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does incorporating non-visual features (brand embeddings, seller metadata) alongside the learned Holistic Visual Representation significantly improve price prediction accuracy beyond the current ~50% R² ceiling?
- **Basis in paper:** Section 8.1 states that the "brand confounder" is a primary driver of price that the visual-only model cannot access, proposing multimodal feature fusion as future work.
- **Why unresolved:** The paper deliberately excluded brand from the dataset to test visual-only price prediction, leaving the fusion approach untested.
- **What evidence would resolve it:** Train a variant concatenating the ViT image embedding with categorical brand/seller embeddings before the price head; measure R² improvement on held-out test data.

### Open Question 2
- **Question:** Does the internal hallucination metric (text-vs-predicted-attributes consistency) correlate with external human judgments of factual accuracy against the actual product image?
- **Basis in paper:** Section 8.4 notes that the hallucination metric "does not... guarantee external consistency (text vs. the ground-truth image). A model could be perfectly consistent with its own incorrect predictions."
- **Why unresolved:** The study only measured internal consistency, not external validation against source images.
- **What evidence would resolve it:** Large-scale human annotation study comparing model outputs to original images; compute correlation between internal hallucination rate and human-rated factual accuracy.

### Open Question 3
- **Question:** Would Retrieval-Augmented Generation (RAG), injecting real-time market context (trends, competitor descriptions, reviews), improve the commercial relevance of generated listings while preserving the 44.5% hallucination reduction?
- **Basis in paper:** Section 8.3 proposes RAG as a "transformative next step" to ground outputs in live market context beyond static visual evidence.
- **Why unresolved:** The current model operates as a self-contained system without external knowledge retrieval.
- **What evidence would resolve it:** Implement RAG pipeline retrieving relevant market documents before generation; compare hallucination rates and commercial metrics against baseline.

### Open Question 4
- **Question:** Does scaling the text decoder from T5-Small to a larger LLM recover the 3.5% ROUGE-L fluency deficit while maintaining the hierarchical approach's hallucination reduction?
- **Basis in paper:** Section 8.3 identifies T5-Small's limited fluency as a constraint, proposing larger LLMs as future work to enhance text sophistication.
- **Why unresolved:** The paper only tested T5-Small (60M parameters); the trade-off between factual grounding and fluency at larger scales remains unknown.
- **What evidence would resolve it:** Replace T5-Small with T5-Base/Large or comparable LLM; measure hallucination rate and ROUGE-L on identical test set.

## Limitations

- **Domain specificity:** The dataset is limited to women's clothing from Myntra, raising questions about generalization to other e-commerce categories or platforms
- **Proprietary metric:** The hallucination metric relies on an internally defined contradiction detection method that isn't fully specified, making independent validation challenging
- **Price prediction ceiling:** The model achieves only ~50% R² for price prediction due to inability to capture brand effects from visual features alone

## Confidence

- **High confidence:** Hierarchical architecture reduces hallucinations (directly measured, ablation shows clear effect)
- **Medium confidence:** Multi-task learning improves both attribute F1 and price R² (metrics show gains, but could be dataset-specific)
- **Medium confidence:** Prompt-based grounding trades fluency for factual consistency (ROUGE-L drop is measurable, but qualitative impact unclear)
- **Low confidence:** Generalization to other e-commerce domains (no cross-domain validation performed)

## Next Checks

1. **Cross-category generalization test:** Apply the trained model to a held-out category (e.g., men's clothing or electronics) from the same platform and measure performance degradation in attribute F1, price R², and hallucination rate.

2. **Independent hallucination detection:** Implement an open-source contradiction detection method (e.g., zero-shot classification with textual entailment) and verify the 44.5% hallucination reduction claim using this alternative metric.

3. **Prompt template ablation study:** Systematically vary prompt complexity (number of attributes included, formatting) to quantify the relationship between prompt informativeness and both hallucination rate and ROUGE-L score.