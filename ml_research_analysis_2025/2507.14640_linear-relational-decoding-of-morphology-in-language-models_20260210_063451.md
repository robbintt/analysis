---
ver: rpa2
title: Linear Relational Decoding of Morphology in Language Models
arxiv_id: '2507.14640'
source_url: https://arxiv.org/abs/2507.14640
tags:
- linear
- relations
- affine
- object
- relational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that linear transformations can faithfully
  reproduce final-layer object predictions for morphological relations in language
  models, achieving 90% accuracy. By applying Jacobian-based approximations to transformer
  outputs, the authors show that a simple multiplicative operator is nearly as effective
  as affine methods for morphology, suggesting that these relations are sparsely encoded
  as truly linear in latent space.
---

# Linear Relational Decoding of Morphology in Language Models

## Quick Facts
- **arXiv ID**: 2507.14640
- **Source URL**: https://arxiv.org/abs/2507.14640
- **Reference count**: 5
- **Key outcome**: Linear transformations reproduce final-layer object predictions for morphological relations with 90% accuracy across multiple languages and models

## Executive Summary
This paper demonstrates that linear transformations derived from model Jacobians can faithfully approximate final-layer object predictions for morphological relations in language models. By computing the mean Jacobian between middle-layer subject representations and final-layer object predictions, the authors show that a simple multiplicative operator achieves 90% faithfulness for morphology, while semantic relations require affine (multiplicative plus additive) approximations. The findings suggest certain conceptual relationships in language models are sparsely encoded as truly linear in latent space, providing evidence that these relations are readily interpretable from intermediate representations.

## Method Summary
The method computes Jacobian matrices between middle-layer subject states and final-layer object predictions to derive linear approximations of subject-object relationships. For each relation, Jacobians are averaged across training subjects to obtain a mean transformation matrix, with an optional bias term capturing additive residuals. These approximations are then applied to held-out subjects and decoded via the model's final layer to predict objects. The approach is tested on GPT-J and Llama-7b across 8 languages using the BATS dataset, with faithfulness measured as top-1 token accuracy.

## Key Results
- Linear LRE achieves 90% faithfulness on morphological relations, outperforming bias-only methods
- Affine LRE (linear + bias) achieves 95% faithfulness on morphology, while bias alone only reaches 27%
- For semantic/encyclopedic relations, bias contributes significantly (up to 45% faithfulness), while linear alone achieves only 19%
- Cross-layer subject-to-object projection works effectively when subject states are taken from middle layers (3-9 for GPT-J, 4-16 for Llama-7b)

## Why This Works (Mechanism)

### Mechanism 1: Linear (Multiplicative) Relational Encoding for Morphology
The Jacobian ∂F_r/∂s captures how small perturbations to subject representations propagate to object predictions. For morphology, averaging this Jacobian across subjects yields a matrix that generalizes to unseen subjects—indicating the underlying relation is encoded as a near-linear map in residual space. This suggests morphological relations are sparsely encoded as truly linear transformations in transformer latent space.

### Mechanism 2: Affine Approximation for Non-Morphological Relations
Semantic and encyclopedic relations require both multiplicative (W_r) and additive (b_r) terms, with the bias term capturing relation-specific offsets not expressible by pure linear transformation. The additive component dominates for relations like [things - color], suggesting these relations rely more on directional offsets than structured transformation.

### Mechanism 3: Cross-Layer Subject-to-Object Projection
Subject token representations at middle layers contain sufficient information to predict final-layer object states through a learned linear map. The Jacobian captures how this enriched state should be projected across remaining layers, with β scaling compensating for layer normalization effects that distort magnitude between layers.

## Foundational Learning

- **Jacobian Matrix as Local Linear Approximation**: Needed because the entire method relies on computing ∂F_r/∂s to approximate how subject state changes affect object predictions. Quick check: Given f(x) = x³, what is the Jacobian (derivative) at x=2, and what does it predict for f(2.01)?

- **Affine vs. Linear Transformations**: Needed because the paper distinguishes between pure linear (W·s) and affine (W·s + b) approximators. Understanding why adding a bias term helps for some relations but not others is central to the findings. Quick check: Can a pure linear transformation (no bias) map the point (0,0) to any point other than (0,0)? What does this imply for relations where the "origin" subject has no meaningful object?

- **Residual Stream and Layer Normalization in Transformers**: Needed because the β scaling parameter exists specifically because layer norm distorts magnitude across layers. Understanding residual stream accumulation helps explain why middle-layer states can predict final outputs. Quick check: If each transformer layer adds its output to the residual stream, why might information from layer 5 still be recoverable at layer 27?

## Architecture Onboarding

- **Component map**: Subject Hidden State (s) -> Jacobian (W_r) -> Bias (b_r) -> Decoder Head (D) -> Output Token
- **Critical path**: 1) Select relation category and collect subject-object pairs, 2) Format pairs into ICL prompts (8 examples), 3) Forward pass to extract subject hidden state s at specified layer, 4) Compute Jacobian W_r via gradient of final object state w.r.t. s, 5) Average Jacobians and compute bias across training subjects, 6) Apply approximation (Linear: W_r·s, Affine: β·W_r·s + b_r) to held-out subjects, 7) Decode approximated object state via decoder head D, 8) Compare top-1 predicted token against ground truth object
- **Design tradeoffs**: Subject layer selection (earlier layers may lack enrichment; later layers may have lost subject-specific information), number of ICL examples (more examples may stabilize Jacobian computation but risk contamination), β tuning (essential for affine method, optimal ~7 found but may not transfer)
- **Failure signatures**: Low faithfulness with high model accuracy (approximation method fails, not the model), Linear outperforming affine on non-morphological relations (unexpected, verify bias computation), high variance across subject layers (suggests relation encoding is not consistently localized), stemmed forms predicted instead of full forms (vocabulary/tokenization issues)
- **First 3 experiments**: 1) Reproduce single relation: Implement Linear LRE for [noun_plural] on GPT-J using layers 3-9, 2) Ablate bias term: Compare Linear vs. Affine vs. Bias-only across morphology and semantic relations, 3) Cross-model transfer: Extract Jacobians from GPT-J and apply to Llama-7b subject states

## Open Questions the Paper Calls Out
- **Does high faithfulness in linear relational decoding imply causal efficacy for intervening on model behavior?**: The paper did not investigate whether the faithfulness of the Jacobian approximation is associated with causality. High reconstruction accuracy does not guarantee the identified linear direction is the actual mechanism the model uses.
- **Does the sparse linear encoding of morphological relations persist in significantly larger language models?**: Experiments were limited to GPT-J and Llama-7b due to hardware constraints, leaving the scalability of the findings uncertain.
- **Is the linear decoding mechanism utilized by the model during standard generation, or is it specific to the explicit in-context learning prompts used for elicitation?**: The methodology relies on ICL prompts to define relations; the model might encode morphology differently when processing natural text without such explicit framing.

## Limitations
- The mechanistic explanation for why morphology specifically admits pure linear approximation remains correlational rather than causal
- The assumption that subject-object functions are locally linear for morphology needs validation beyond faithfulness metrics
- The paper does not address potential confounds from in-context learning examples influencing the Jacobian computation

## Confidence

- **High Confidence**: The empirical finding that Linear LRE achieves ~90% faithfulness on morphological relations is well-supported by multiple datasets, languages, and models
- **Medium Confidence**: The distinction between linear dominance in morphology versus affine necessity in encyclopedic relations is observed consistently but lacks mechanistic explanation
- **Low Confidence**: The interpretation that β scaling compensates specifically for layer normalization magnitude mismatch is asserted but not directly tested

## Next Checks

1. **Second-Order Term Analysis**: Compute and evaluate the contribution of Hessian terms (∂²F_r/∂s²) to the object prediction. If these terms are truly negligible for morphology (<5% faithfulness improvement when included), the linear approximation claim gains causal support.

2. **Randomized Model Controls**: Apply the same LRE methodology to models with randomly initialized weights. If faithfulness remains high (>70%) for morphological relations even in untrained models, this would suggest the observed linearity is an architectural artifact rather than learned semantic encoding.

3. **Ablation of ICL Examples**: Repeat the Jacobian computation using only 1-2 ICL examples instead of 8, and measure faithfulness degradation. If performance remains stable (>80% of original), this suggests the linear mechanism is robust to example count and not overfitting to specific in-context demonstrations.