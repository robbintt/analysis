---
ver: rpa2
title: 'CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search
  via Query Reformulation and Rank Fusion'
arxiv_id: '2509.15588'
source_url: https://arxiv.org/abs/2509.15588
tags:
- query
- retrieval
- conversational
- rewriting
- interactive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the CFDA & CLIP submission to TREC iKAT 2025,
  focusing on improving conversational search through query reformulation and rank
  fusion. The authors explore both interactive and offline submission tasks, proposing
  pipelines that combine Best-of-N selection and Reciprocal Rank Fusion (RRF) strategies.
---

# CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion

## Quick Facts
- arXiv ID: 2509.15588
- Source URL: https://arxiv.org/abs/2509.15588
- Reference count: 28
- Primary result: Fusion-based methods achieved the strongest performance, with RRF-first + DeBERTaV3 reaching 0.4425 nDCG@10 on iKAT-2024.

## Executive Summary
This paper presents the CFDA & CLIP submission to TREC iKAT 2025, focusing on improving conversational search through query reformulation and rank fusion. The authors explore both interactive and offline submission tasks, proposing pipelines that combine Best-of-N selection and Reciprocal Rank Fusion (RRF) strategies. Their systems leverage LLM4CS and CHIQ-AD for context-aware query rewriting, followed by SPLADE retrieval and neural reranking. Results demonstrate that reranking and fusion improve robustness and effectiveness, with fusion-based methods achieving the strongest performance. The study also highlights trade-offs between effectiveness and efficiency, particularly in latency-sensitive interactive settings.

## Method Summary
The CFDA & CLIP system employs a multi-stage pipeline for conversational search. First, query rewriting is performed using LLM4CS and CHIQ-AD models to convert incomplete conversational utterances into self-contained queries. Next, SPLADE sparse retrieval is applied to the rewritten queries. The system then applies either Best-of-N selection or Reciprocal Rank Fusion (RRF) to combine results from multiple rewrites. Finally, neural reranking is performed using DeBERTaV3 or BGE cross-encoders to refine the final ranking. For the offline track, the system also incorporates PTKB classification and passage summarization. The approach was evaluated on iKAT 2023 and 2024 datasets using ClueWeb22 as the retrieval corpus.

## Key Results
- RRF-first + DeBERTaV3 achieved 0.4425 nDCG@10 on iKAT-2024, outperforming rerank-first approaches.
- Query rewriting improved retrieval over raw conversational utterances, with LLM4CS and CHIQ-AD outperforming SPLADE-only on iKAT-2024.
- Fusion-based methods demonstrated better robustness to ambiguous queries compared to single-query approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-aware query rewriting improves retrieval over raw conversational utterances by resolving ellipsis and coreference.
- Mechanism: LLM4CS uses chain-of-thought prompting to generate self-contained queries from dialogue history; CHIQ-AD enhances history via disambiguation and expansion before rewriting.
- Core assumption: Rewritten queries preserve user intent while making implicit context explicit for standard retrievers.
- Evidence anchors:
  - [abstract] "We explored query rewriting and retrieval fusion as core strategies."
  - [section] Table 1 shows LLM4CS (0.3230 nDCG@10) and CHIQ-AD (0.3221) outperform SPLADE-only (0.2974) on iKAT-2024.
  - [corpus] Related work on adaptive personalized CIR confirms context integration improves retrieval (FMR=0.540, arXiv:2508.08634).
- Break condition: If dialogue history exceeds context window or contains contradictory turns, rewriting may drop critical signals (see CHIQ-FT 512-token limit failure in Table 2: 0.1286 nDCG@10).

### Mechanism 2
- Claim: Fusion of multiple query rewrites via RRF before neural reranking improves robustness to ambiguity.
- Mechanism: Multiple rewrites retrieve separate candidate lists; RRF merges rankings based on reciprocal positions; the unified list is then reranked.
- Core assumption: Different rewrites capture complementary aspects of user intent; fusion diversifies retrieval before precision-focused reranking.
- Evidence anchors:
  - [abstract] "fusion-based methods achieving the strongest performance"
  - [section] Table 3: RRF first + DeBERTaV3 achieves 0.4425 nDCG@10 vs. rerank first (0.4275) on iKAT-2024.
  - [corpus] Evidence on fusion in conversational settings is limited in neighbors; no direct corpus comparison available.
- Break condition: If rewrites are semantically redundant or all miss key intent, fusion amplifies noise without gain.

### Mechanism 3
- Claim: Neural cross-encoder reranking refines initial sparse retrieval by modeling fine-grained query-passage interactions.
- Mechanism: SPLADE provides lexical expansion for recall; DeBERTaV3/BGE cross-encoders re-score top-k passages with deep interaction modeling.
- Core assumption: Cross-encoders capture relevance signals that sparse representations miss.
- Evidence anchors:
  - [abstract] "Results demonstrate that reranking and fusion improve robustness and effectiveness"
  - [section] Table 2: DeBERTaV3 improves CHIQ-AD from 0.3221 to 0.4112 (+27.5% relative gain in nDCG@10).
  - [corpus] Efficient conversational search via dense retrieval (arXiv:2504.21507) notes latency-effectiveness trade-offs, supporting reranking as a precision stage.
- Break condition: If top-k from first-stage retrieval misses relevant passages entirely, reranking cannot recover them.

## Foundational Learning

- Concept: Conversational Query Reformulation (CQR)
  - Why needed here: User utterances in multi-turn dialogues are incomplete; CQR converts them to self-contained queries for off-the-shelf retrievers.
  - Quick check question: Given "What about their budget?" and previous turn discussing "European vacation," what should the rewritten query contain?

- Concept: Reciprocal Rank Fusion (RRF)
  - Why needed here: Combines ranked lists from multiple rewrites without requiring score normalization.
  - Quick check question: If Document A ranks 1st in list-1 and 5th in list-2, while Document B ranks 3rd in both, which has higher RRF score with k=60?

- Concept: Cross-Encoder Reranking
  - Why needed here: Bi-encoders (SPLADE) are fast but approximate; cross-encoders jointly encode query-passage pairs for higher precision at inference cost.
  - Quick check question: Why apply cross-encoders only to top-k candidates rather than the full corpus?

## Architecture Onboarding

- Component map:
  Input -> Query Rewriting -> Retrieval -> Fusion (optional) -> Reranking -> Response Generation

- Critical path:
  Interactive: Rewriting -> (Best-of-N selection or parallel retrieval) -> RRF -> Rerank -> Response (latency-sensitive)
  Offline: Rewriting -> Best-of-N + RRF -> Rerank -> PTKB classification + passage summarization -> Response

- Design tradeoffs:
  Best-of-N vs. RRF: Best-of-N selects single best query (lower latency); RRF fuses multiple (higher effectiveness, more compute).
  RRF before vs. after reranking: Before exposes reranker to diverse candidates (better per Table 3); after is computationally cheaper.
  Fine-tuned (CHIQ-FT) vs. prompt-based: FT is faster but limited by 512-token context (fails on long dialogues).

- Failure signatures:
  CHIQ-FT nDCG@10 = 0.1286 (Table 2): Context truncation causes incomplete rewrites.
  Reranking without fusion: Lower robustness to ambiguous queries (single rewrite may miss intent).
  High N_candidates (>10): Marginal gains with linear cost increase (Figure 2).

- First 3 experiments:
  1. Replicate Table 2 baseline: Compare SPLADE-only vs. + CHIQ-AD vs. + LLM4CS vs. + DeBERTaV3 reranking on iKAT-2024.
  2. Ablate RRF ordering: Run RRF-first vs. rerank-first with DeBERTaV3; expect ~0.015 nDCG@10 difference per Table 3.
  3. Measure latency: Profile end-to-end interactive pipeline with N_candidates=5 vs. 10; identify bottleneck (likely LLM rewriting or cross-encoder reranking).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive system effectively balance efficiency and effectiveness by dynamically selecting between single-query retrieval and multi-query Reciprocal Rank Fusion (RRF) based on dialogue context?
- Basis in paper: [explicit] Section 6 states future work involves systems that could "dynamically decide between single-query retrieval and multi-query RRF based on query complexity or dialogue context."
- Why unresolved: Current implementations apply these strategies statically, potentially wasting resources on simple queries.
- What evidence would resolve it: A study measuring latency vs. retrieval accuracy for a dynamic switch mechanism compared to static fusion.

### Open Question 2
- Question: How does deeper integration of the Personal Text Knowledge Base (PTKB) via entity linking and knowledge-grounded rewriting impact retrieval and response generation performance?
- Basis in paper: [explicit] Section 6 notes "Current usage of the PTKB is limited" and suggests exploring "entity linking and knowledge-grounded rewriting."
- Why unresolved: The paper establishes that PTKB usage was minimal, leaving the potential benefits of deep integration unverified.
- What evidence would resolve it: Experimental results comparing baseline PTKB usage against an entity-linked, knowledge-grounded approach in the pipeline.

### Open Question 3
- Question: Can specialized summarization models (e.g., PEGASUS, BART) outperform standard LLM generation in the response synthesis stage for conversational search?
- Basis in paper: [explicit] Section 6 proposes that the response generation stage could benefit from "more specialized summarization models" tailored for long-form passages.
- Why unresolved: The current work used a general pipeline without testing specific summarization architectures.
- What evidence would resolve it: Comparative evaluation of response quality using specialized summarizers versus the current generation method.

## Limitations
- Unknown hyperparameters: RRF's k value, SPLADE model version, and DeBERTaV3 checkpoint are unspecified, potentially affecting reproducibility.
- Context truncation risk: CHIQ-FT's poor performance (0.1286 nDCG@10) due to 512-token limits suggests rewrite models may drop critical signals in long dialogues.
- Latency-effectiveness trade-off: Best-of-N selection reduces computation but sacrifices the robustness gains seen with RRF fusion (0.4425 vs. 0.4275 nDCG@10).

## Confidence

- High confidence: Query rewriting improves retrieval (Table 1 shows 8-9% gains over SPLADE-only).
- Medium confidence: Fusion before reranking is superior to reranking first (Table 3, 0.015 nDCG@10 difference), though limited ablation.
- Low confidence: PTKB integration effectivenessâ€”integration method is only briefly mentioned, with no ablation or performance impact shown.

## Next Checks

1. Ablation study: Test RRF-first vs. rerank-first with DeBERTaV3 on iKAT-2024 to confirm the 0.015 nDCG@10 difference.
2. Latency profiling: Measure end-to-end interactive pipeline latency with N_candidates=5 vs. 10; identify bottlenecks (LLM rewriting vs. cross-encoder reranking).
3. Rewrite quality analysis: Inspect failed cases from CHIQ-FT to quantify information loss from context truncation.