---
ver: rpa2
title: 'Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied
  Task Planning'
arxiv_id: '2510.21302'
source_url: https://arxiv.org/abs/2510.21302
tags:
- code
- task
- probe
- main
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents NESYRO, a neuro-symbolic framework that improves
  reliability of LLM-generated code for embodied task planning in dynamic, partially
  observable environments. The core idea is to combine symbolic verification (ensuring
  code correctness against task specifications) with interactive validation (grounding
  each skill via environment exploration).
---

# Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning

## Quick Facts
- **arXiv ID**: 2510.21302
- **Source URL**: https://arxiv.org/abs/2510.21302
- **Reference count**: 40
- **Key outcome**: NESYRO improves LLM-generated code reliability for embodied task planning via neuro-symbolic verification and interactive validation, achieving 46.2% higher success rates than baseline Code-as-Policies.

## Executive Summary
This paper presents NESYRO, a neuro-symbolic framework that addresses the reliability problem of LLM-generated code for embodied task planning in dynamic, partially observable environments. The framework combines symbolic verification (using SMT solvers to check code against task specifications) with interactive validation (using neuro-symbolic confidence scores to ground each skill via environment exploration). When confidence in a skill falls below threshold, NESYRO generates safe probe code to acquire missing observations, then recursively refines the policy. Evaluation on RLBench and real-world tasks shows NESYRO improves task success rate by 46.2% over Code-as-Policies and achieves over 86.8% executability of task-relevant actions.

## Method Summary
NESYRO operates through a verification-validation-execution loop. First, an LLM generates a task specification and policy code, which is verified by an SMT solver (Z3) for logical consistency. For each skill in the verified policy, a neuro-symbolic confidence score (NeSyConf) is computed by multiplying Common Sense Confidence (CSC) from the LLM with Logic Confidence (LC) from a PDDL planner. If NeSyConf falls below a threshold, the framework generates and executes safe probe code to acquire missing observations, then recursively refines the policy. The grounded policy is then executed in the environment using either RLBench APIs or MoveIt for real-world applications.

## Key Results
- NESYRO achieves 46.2% higher task success rates compared to Code-as-Policies across various observability levels
- The framework attains over 86.8% executability of task-relevant actions in RLBench simulations
- Real-world experiments show reduction of irreversible actions from 29 (CaP) to 7 when using NESYRO
- NESYRO demonstrates robust performance across high, low, stochastic, and complete observability levels, particularly excelling in long-horizon tasks requiring multiple exploration steps

## Why This Works (Mechanism)

### Mechanism 1: Neuro-Symbolic Verification with Iterative Refinement
The framework uses an SMT solver (Z3) to statically verify that generated policy code satisfies the task specification before execution. When verification fails, structured feedback identifies violations, prompting the LLM to revise only the unvalidated portions of code. This catches logical inconsistencies early, reducing compile errors and task failures. The core assumption is that the symbolic task specification accurately captures task constraints, and verification criteria align with real-world executability.

### Mechanism 2: Neuro-Symbolic Confidence Score (NeSyConf) for Skill Validation
NeSyConf multiplies Common Sense Confidence (CSC) from an LLM with Logic Confidence (LC) from a symbolic planner to yield a more reliable signal for skill grounding. CSC is computed from token-level perplexity using demonstrations and domain knowledge, while LC is binary (1 if PDDL planner finds a valid plan including the skill, else 0). Both components contribute comparably to overall performance, as shown by ablation studies where removing either component drops success rates by approximately 20%.

### Mechanism 3: Recursive Safe Probe Generation for Missing Observations
When NeSyConf falls below threshold, NESYRO generates and executes verified "probe" code to acquire missing observations without risking irreversible task actions. The probe policy undergoes the same verification and validation cycle, executes to gather new observations, and then the main policy is refined at the failing skill step using updated observations. This recursive structure generates a policy tree rooted at the main policy, where each probe functions as a subroutine.

## Foundational Learning

- **Partially Observable Markov Decision Processes (POMDPs)**: Understanding POMDPs is essential because the problem is explicitly formulated as a POMDP, requiring knowledge of how partial observability affects planning and why exploration is needed. Quick check: Can you explain why a policy in a POMDP must balance exploration and exploitation differently than in a fully observable MDP?

- **SMT Solving and Z3 Basics**: The verification phase uses Z3 to check code against specifications, so understanding constraint satisfaction helps debug verification failures and interpret feedback. Quick check: Given a simple constraint (e.g., "x > 5 AND x < 3"), can you identify whether it is satisfiable and what Z3 would return?

- **PDDL Planning and Fast Downward**: The validation phase uses a PDDL planner (Fast Downward) to compute Logic Confidence, so understanding action preconditions and effects is essential for diagnosing LC = 0 cases. Quick check: If a PDDL action "open_drawer" has precondition "(and (unlocked ?d) (empty ?d))" but the current state only has "(unlocked drawer1)", will the planner find a valid plan including this action?

## Architecture Onboarding

- **Component map**: Instruction g + initial observation o≤0 → Φveri → (Tspec, πmain) → Ψveri → verified πmain → for each skill fn: compute CSC (Φvali) and LC (Ψvali) → NeSyConf = CSC × LC → if NeSyConf < ϵ: generate πprobe (recursive) → else: advance to fn+1 → all skills validated → execute grounded πmain

- **Critical path**: 1) Instruction and observation fed to verification LLM to generate task specification and policy code. 2) SMT solver verifies code against specification, providing feedback on failures. 3) For each skill, parallel computation of CSC and LC. 4) NeSyConf calculation and threshold comparison. 5) Low confidence triggers recursive probe generation and execution. 6) Policy refinement using acquired observations. 7) Execution of fully validated policy.

- **Design tradeoffs**: The current implementation uses binary LC (0 or 1) for simplicity, though probabilistic PDDL could provide more nuanced feasibility assessment. Smaller LLM models (≥1B parameters) degrade CSC accuracy, with 3B+ parameters being sufficient. Threshold calibration uses conservative lower quartile estimates from probe trials to reduce false positives but may increase probe overhead.

- **Failure signatures**: Compile errors after verification indicate SMT solver passed syntactically invalid code. NeSyConf stuck at 0 suggests LC = 0 due to missing preconditions in domain knowledge. Infinite recursion occurs when probes generate additional probes without acquiring useful observations. Irreversible actions during probe execution indicate probe code not truly safe.

- **First 3 experiments**: 1) Reproduce Table 2 by running NESYRO vs. CaP on "Object Relocation" under "High Incompleteness" (10 trials), expecting SR improvement from ~25% to ~70%. 2) Ablate LC or CSC by disabling one component on a long-horizon task to quantify individual contribution to success rates. 3) Calibrate threshold ε by running 5 probe trials on a fixed skill, computing lower quartile of successful probe scores as candidate threshold, then validating on held-out tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would replacing the binary logic confidence (LC) with probabilistic reasoning (e.g., probabilistic PDDL) affect task success rates in environments with stochastic state transitions?
- Basis in paper: The authors state "the current implementation of NESYRO employs a binary LC and predefined domain knowledge, which limits its generality in real-world applications. Future work will address this limitation by incorporating probabilistic and temporal reasoning, such as probabilistic PDDL."
- Why unresolved: The current LC returns only 0 or 1, which cannot capture partial satisfiability or probabilistic preconditions common in real-world settings.
- What evidence would resolve it: Comparative experiments showing success rates under stochastic transitions using probabilistic PDDL versus binary LC.

### Open Question 2
- Question: Can NESYRO's validation phase be extended to handle skills not explicitly defined in the domain knowledge, and what mechanisms would enable such generalization?
- Basis in paper: The authors plan to "relax these assumptions and explore the framework's applicability to more diverse and dynamic domains by extending validation to skills that are not explicitly defined in the domain knowledge."
- Why unresolved: The current framework requires predefined skill definitions with explicit preconditions and effects for symbolic validation.
- What evidence would resolve it: Experiments on held-out skills showing successful validation and grounding without prior domain encoding.

### Open Question 3
- Question: How does the performance of NESYRO scale with increasing task horizon and action space complexity beyond the 7-skill long-horizon tasks tested?
- Basis in paper: The paper evaluates long-horizon tasks with at most 7 skills. Recursive probing generates policy trees that could grow exponentially with task complexity, but scalability limits are not analyzed.
- Why unresolved: No experiments address whether the recursive V&V process remains tractable for significantly longer task sequences or larger action spaces.
- What evidence would resolve it: Experiments on tasks with 10-20+ skill steps reporting computational cost and success rates.

### Open Question 4
- Question: Is the multiplicative combination of CSC and LC (NeSyConf = CSC × LC) optimal, or would alternative integration strategies better leverage their complementary strengths?
- Basis in paper: The paper ablates LC and CSC independently (Table 5), showing both contribute, but does not explore alternative fusion strategies beyond multiplication.
- Why unresolved: The design choice is motivated by analogy to SayCan but not systematically compared against alternatives like weighted sums or learned integration.
- What evidence would resolve it: Ablation experiments comparing multiplicative, additive, and learned combination strategies across task types.

## Limitations
- **Domain Knowledge Dependency**: Framework performance heavily relies on completeness and accuracy of PDDL domain knowledge, with no quantification of sensitivity to missing or incorrect definitions.
- **Probe Safety Assumptions**: While claiming probes are "safe," the definition of safety is not rigorously validated, and probe actions could cause subtle state changes affecting subsequent execution.
- **Threshold Calibration Stability**: The threshold ε is calibrated using probe trials on a subset of skills, but stability across different tasks, environments, or robot platforms is not tested.

## Confidence

- **High Confidence**: The core mechanism of combining symbolic verification (Ψveri) with neuro-symbolic confidence (NeSyConf) is well-supported by ablation studies (Table 5) and execution metrics (Table 3), with robust improvements in executability (86.8%) and reduction in irreversible actions.

- **Medium Confidence**: The specific implementation of NeSyConf (CSC × LC) is novel, but the binary nature of LC is a simplification acknowledged as future work, with corpus evidence suggesting alternative probabilistic approaches exist.

- **Low Confidence**: The probe safety guarantees and threshold calibration method are asserted but not empirically validated across diverse scenarios, with the assumption that probe actions are always reversible or non-destructive being a critical unverified claim.

## Next Checks

1. **Domain Knowledge Stress Test**: Systematically remove or corrupt PDDL domain definitions for a subset of skills; measure how LC degradation affects probe frequency and overall task success to quantify framework sensitivity to domain knowledge completeness.

2. **Probe Safety Audit**: Instrument the probe execution phase to log all state changes (even subtle ones) and analyze whether any probes cause unintended consequences in subsequent task execution; compare irreversible action counts with and without probe logging enabled.

3. **Threshold Transferability Test**: Calibrate ε on one task/environment, then apply it to a structurally different task without re-calibration; measure change in probe overhead and task success rate to assess threshold stability across domains.