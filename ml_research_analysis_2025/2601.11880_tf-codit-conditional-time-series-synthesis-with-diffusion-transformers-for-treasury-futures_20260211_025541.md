---
ver: rpa2
title: 'TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for
  Treasury Futures'
arxiv_id: '2601.11880'
source_url: https://arxiv.org/abs/2601.11880
tags:
- market
- data
- price
- time
- futures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces TF-CoDiT, the first LLM-DiT framework for
  conditioned time-series generation of treasury futures. To overcome the challenges
  of low-data regimes and complex market dependencies, it adapts standard DiT by transforming
  multi-channel 1-D time series into Discrete Wavelet Transform coefficient matrices,
  which are then encoded into a latent variable via a U-shape VAE.
---

# TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures

## Quick Facts
- **arXiv ID:** 2601.11880
- **Source URL:** https://arxiv.org/abs/2601.11880
- **Reference count:** 33
- **Primary result:** First LLM-DiT framework for conditioned treasury futures time-series synthesis, achieving 0.433 MSE and 0.453 MAE with 13.4% MSE and 12.8% MAE improvements over previous approaches

## Executive Summary
This work introduces TF-CoDiT, the first LLM-DiT framework for conditioned time-series generation of treasury futures. To overcome the challenges of low-data regimes and complex market dependencies, it adapts standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform coefficient matrices, which are then encoded into a latent variable via a U-shape VAE. A Financial Market Attribute Protocol (FinMAP) is proposed to standardize market descriptions and generate structured prompts covering 17 daily or 23 periodic economic indicators from 7 or 8 perspectives. Experiments on four treasury futures contracts from 2015 to 2025 show TF-CoDiT achieves at most 0.433 MSE and 0.453 MAE on ground-truth data, outperforming previous approaches with 13.4% MSE and 12.8% MAE improvements on month-level tasks, and demonstrating robustness across contracts and durations.

## Method Summary
TF-CoDiT transforms multi-channel 1-D treasury futures time series into 2D wavelet coefficient matrices via Discrete Wavelet Transform, then encodes these coefficients into a latent variable using a U-shape VAE with Latent Query Attention. An LLM backbone (Gemma-2B) is adapted to perform conditional diffusion in the latent space, guided by structured prompts from the Financial Market Attribute Protocol (FinMAP) covering 17 daily or 23 periodic economic indicators. The generated latents are decoded back to wavelet coefficients and reconstructed into time series via inverse DWT. The model is trained in two stages: first the U-VAE on DWT coefficients, then the DiT on the latent diffusion task with frozen LLM weights.

## Key Results
- Achieves at most 0.433 MSE and 0.453 MAE on ground-truth treasury futures data
- Outperforms previous approaches with 13.4% MSE and 12.8% MAE improvements on month-level tasks
- Demonstrates robustness across four treasury futures contracts (TS, TF, T, TL) from 2015 to 2025
- U-VAE with Latent Query Attention outperforms Conv-VAE on 3 of 4 multivariate time series datasets
- FinMAP structured prompts improve performance by 13.1% MSE compared to free-form prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting 1-D time series to 2D wavelet coefficient matrices enables the DiT to capture multi-scale volatility patterns more effectively than raw time-domain data.
- **Mechanism:** The Discrete Wavelet Transform (DWT) decomposes each channel into approximation coefficients (low-frequency trends) and detail coefficients (high-frequency shocks), creating a 3D tensor W ∈ R^(C×Γ×T). This transforms time series generation into a text-to-image synthesis problem, allowing reuse of proven DiT architectures from vision domains.
- **Core assumption:** The energy distribution of financial signals across frequency bands contains more predictive structure than point-in-time values, and this structure can be inverted losslessly via IDWT.
- **Evidence anchors:**
  - [abstract] "adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices"
  - [Section 3.2] "This transformation enables TF-CoDiT to learn from the informative 'energy distribution' of time-series data, instead of information-less point-in-time values"
  - [corpus] Weak direct evidence. Related TimeBridge paper mentions diffusion priors for time series but uses different approach. No corpus paper validates wavelet+DiT combination specifically.
- **Break condition:** If wavelet decomposition loses phase information critical to price dynamics, or if the Haar wavelet (used here) is too simple for complex volatility patterns, reconstruction errors will compound.

### Mechanism 2
- **Claim:** The U-shape VAE with Latent Query Attention (LQA) captures cross-channel dependencies (e.g., correlations between opening/high/low/close prices and trading volume) that standard convolutional VAEs miss.
- **Mechanism:** The encoder uses hierarchical attention with learnable queries Q^(ℓ) that progressively reduce channel dimensions (C_ℓ = C/r), forcing the model to distill global dependencies into compact hidden states. The deepest layer H^(L) ∈ R^(1×d) is parameterized into latent z₀. Shared queries between encoder/decoder enable selective information retrieval during reconstruction.
- **Core assumption:** Cross-channel correlations in treasury futures are hierarchical and can be compressed through learned query mechanisms without losing critical inter-variable dynamics.
- **Evidence anchors:**
  - [abstract] "A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable"
  - [Section 3.4] "LQA is implemented at each layer... This hierarchical attention mechanism forces the model to distill global dependencies into an increasingly compact hidden space"
  - [Section 5, Figure 6] Shows U-VAE outperforming Conv-VAE on 3 of 4 multivariate time series datasets (ETT, WTH, QLIB), with MIMIC as exception
  - [corpus] U-DiT Policy paper validates U-shaped DiT for multi-modal dependencies in robotics, but domain differs significantly
- **Break condition:** If treasury futures channels have weak or non-hierarchical correlations, LQA may introduce unnecessary complexity. The MIMIC dataset failure suggests effectiveness varies by domain.

### Mechanism 3
- **Claim:** The FinMAP protocol provides structured semantic grounding that enables the LLM to learn mappings between macro-economic narratives and price movements, reducing the information bottleneck of free-form text.
- **Mechanism:** FinMAP defines a hierarchical taxonomy: daily snapshots (17 indicators across 7 categories like Liquidity, Sentiment, Rates Bonds) and periodic narratives (23 indicators across 8 dimensions). Prompts are constructed through temporal aggregation (8-day prompts from daily FinMAPs, longer horizons via recursive aggregation), ensuring consistent "macro-narrative" across durations.
- **Core assumption:** The 17/23 indicators cover the essential drivers of treasury futures movements, and the hierarchical aggregation preserves causal relationships between market conditions and price dynamics.
- **Evidence anchors:**
  - [abstract] "To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP)"
  - [Section 3.5] "FinMAP's taxonomy is organized into a hierarchical architecture encompassing two distinct temporal resolutions"
  - [Table 4] Ablation shows removing daily FinMAP increases MSE from 0.277 to 0.448 for 8-day TS generation; full FinMAP beats partial variants
  - [corpus] No corpus validation of FinMAP or similar structured financial prompt systems. Evidence is entirely intra-paper.
- **Break condition:** If key economic drivers are omitted from the 17/23 indicators, or if the hierarchical aggregation loses temporal ordering of events, the model may learn spurious correlations.

## Foundational Learning

- **Concept: Discrete Wavelet Transform (DWT) for time-frequency analysis**
  - Why needed here: The entire architecture assumes DWT decomposes signals into meaningful frequency bands. Without understanding approximation vs. detail coefficients, wavelet scales, and invertibility, you cannot debug reconstruction errors or choose alternative wavelets.
  - Quick check question: Given a 64-step price sequence decomposed to level J=3, what are the shapes of approximation coefficient a₃ and detail coefficients d₁, d₂, d₃?

- **Concept: Variational Autoencoders with hierarchical latent spaces**
  - Why needed here: The U-VAE uses a non-standard architecture with learned queries and channel-wise encoding. Understanding ELBO, KL divergence, and reconstruction losses (L1 vs. MSE) is critical for training stability and diagnosing mode collapse.
  - Quick check question: Why might L1 reconstruction loss preserve high-frequency "edges" better than MSE for heavy-tailed financial data?

- **Concept: Latent Diffusion Models with cross-attention conditioning**
  - Why needed here: TF-CoDiT uses a FuseDiT backbone where text and latent tokens share self-attention with asymmetric masking. Understanding the forward noising process, denoising objective L_diff, and how conditioning enters the architecture is essential for implementation.
  - Quick check question: In the asymmetric mask M, why are text tokens causally masked while latent tokens receive full attention?

## Architecture Onboarding

- **Component map:**
  Input (Prompt + Time Series) → Signal Transform: DWT (1D → 2D) → U-VAE Encoder (LQA-based) → LLM Backbone (Gemma-2B, frozen) → U-VAE Decoder → Signal Reconstruction: IDWT → Output (synthesized time series)

- **Critical path:**
  1. **Data preparation:** Convert raw TF OHLCV+settlement+open interest to normalized form (stratified normalization per Section A.5). Construct FinMAP prompts via multi-agent labeling pipeline.
  2. **U-VAE training:** Train on DWT coefficients first (72,448 sliding window samples). Use AdamW, lr=0.001, L1 loss. Validate reconstruction before proceeding.
  3. **TF-CoDiT training:** Freeze Gemma-2B, train only embedding/output layers. Use mixed contract/duration batches. Warmup 5%, lr=0.0005.
  4. **Inference:** DPM-Solver for denoising, then VAE decode + IDWT.

- **Design tradeoffs:**
  - **Haar wavelet vs. Daubechies:** Haar is simplest but may not capture smooth trends well. Paper doesn't justify choice or compare alternatives.
  - **L1 vs. MSE in VAE:** L1 preserves high-frequency detail better for heavy-tailed data (validated in Figure 5), but may increase training instability.
  - **Frozen LLM vs. fine-tuning:** Freezing reduces compute but limits adaptation to financial domain. The 13.4% MSE improvement suggests it works, but "black swan" generalization remains a limitation (Section 6).
  - **FinMAP vs. free-form prompts:** Structured prompts improve performance (Table 4), but require domain expertise to define and maintain. Abstraction may miss novel market dynamics.

- **Failure signatures:**
  - **Blurred reconstructions:** If VAE outputs overly smooth wavelet coefficients, check if L1 loss is actually used (MSE causes blurring per Section 5).
  - **Channel correlation loss:** If generated OHLC shows unrealistic relationships (e.g., close > high), U-VAE encoder may be under-capacity or LQA reduction ratio too aggressive.
  - **Long-horizon drift:** For 64+ day generation, cumulative error in Figure 4 shows variance explosion. Consider shorter horizons or hierarchical generation.
  - **Prompt misalignment:** If generated series contradicts FinMAP conditions, check token truncation for context length or verify aggregation logic.

- **First 3 experiments:**
  1. **VAE reconstruction sanity check:** Train U-VAE on single contract (e.g., T), visualize DWT coefficient reconstruction errors. Compare L1 vs. MSE loss on held-out samples. Target: <0.05 MAE on approximation coefficients.
  2. **Ablation on FinMAP granularity:** Train TF-CoDiT with (a) full FinMAP, (b) daily only, (c) free-form prompts on 8-day generation. Replicate Table 4 to validate semantic grounding hypothesis before full training.
  3. **Cross-contract transfer:** Train on TF, T, TL; test on TS (smallest dataset, 655 samples). If performance degrades severely compared to in-distribution, the model may be overfitting to contract-specific patterns rather than learning general TF dynamics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-modal conditioners, such as synchronous time series of commodity prices or foreign exchange rates, be integrated into the TF-CoDiT framework to enhance synthesis fidelity?
- Basis in paper: [explicit] The "Limitations" section notes that the current reliance on textual narratives is a constraint because text is discrete and information-sparse compared to continuous cross-asset signals. The authors identify incorporating multi-modal conditioners as a "promising but unexplored avenue."
- Why unresolved: The current architecture (inspired by FuseDiT) and FinMAP protocol are designed exclusively for text-based conditioning.
- What evidence would resolve it: Implementation of a multi-modal conditioning mechanism (e.g., incorporating gold/oil prices) that yields statistically significant improvements in MSE/MAE over the text-only baseline.

### Open Question 2
- Question: Can the framework be adapted to effectively generalize to "black-swan" events or rare market regimes that are absent from the training distribution?
- Basis in paper: [explicit] The authors explicitly state that the pre-trained parameters adapted from text-to-image models lack intrinsic alignment with complex financial theories, which hinders generalization to rare events.
- Why unresolved: The model currently adapts a general-purpose DiT backbone which lacks the domain-specific priors necessary to synthesize plausible trajectories for tail-risk events not heavily observed in the training data.
- What evidence would resolve it: Successful synthesis of realistic market trajectories under stress-test scenarios (e.g., financial crises) that are under-represented in the 2015–2025 dataset.

### Open Question 3
- Question: What standardized, external benchmarks are required to quantitatively assess the semantic quality and coverage of financial taxonomies like FinMAP?
- Basis in paper: [explicit] The authors identify the "Evaluation of FinMAP" as a limitation, noting that current evaluations are largely intrinsic (ablation studies) and lack an external, standardized benchmark for financial description protocols.
- Why unresolved: There is currently no objective metric or dataset to score the semantic richness and accuracy of structured financial prompts independent of the generation task.
- What evidence would resolve it: The development and adoption of a quantitative benchmark that correlates prompt quality with downstream synthesis fidelity across multiple models.

## Limitations
- **Proprietary Data Dependence:** Performance relies on access to Chinese market data and proprietary labeled market reviews
- **Architecture Specificity:** U-VAE's LQA and FuseDiT's asymmetric masking are custom designs without ablation studies on alternatives
- **Domain Generalization:** Limited robustness to "black swan" events and regime shifts outside 2015-2025 training window

## Confidence
- **High:** U-VAE's superiority over Conv-VAE on multivariate time series (Figure 6) with multiple datasets
- **Medium:** FinMAP ablation (Table 4) shows structured prompts beat free-form text, but schema not validated against alternatives
- **Low:** Overall TF-CoDiT performance (0.433 MSE) impressive but lacks external benchmarks or comparison to non-DiT methods

## Next Checks
1. **Cross-Window Robustness:** Train TF-CoDiT on 2015-2020, test on 2021-2025 to check generalization beyond training economic regime
2. **Wavelet Ablation:** Replace Haar with Daubechies-4 or Symlets in both VAE and DiT; compare reconstruction quality and generation fidelity
3. **Prompt-Free Generation:** Generate unconditioned samples (no FinMAP) and measure MSE degradation to quantify semantic grounding value-add