---
ver: rpa2
title: 'Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented
  Generation'
arxiv_id: '2508.08816'
source_url: https://arxiv.org/abs/2508.08816
tags:
- mrag
- search
- planning
- retrieval
- remplan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: E-Agent addresses the limitations of existing multimodal retrieval-augmented
  generation (mRAG) systems by introducing a dynamic planning framework that reduces
  redundant searches and improves accuracy. The approach combines a mRAG planner that
  performs contextual analysis of both text and images in a single forward pass, and
  a task executor that implements optimized workflows.
---

# Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2508.08816
- Source URL: https://arxiv.org/abs/2508.08816
- Authors: Yuechen Wang; Yuming Qiao; Dan Meng; Jun Yang; Haonan Lu; Zhenyu Yang; Xudong Zhang
- Reference count: 35
- Primary result: E-Agent achieves 13% accuracy gains over state-of-the-art mRAG methods while reducing redundant searches by 37%

## Executive Summary
E-Agent addresses the limitations of existing multimodal retrieval-augmented generation (mRAG) systems by introducing a dynamic planning framework that reduces redundant searches and improves accuracy. The approach combines a mRAG planner that performs contextual analysis of both text and images in a single forward pass, and a task executor that implements optimized workflows. The planner dynamically orchestrates multimodal tools based on contextual reasoning, while the executor uses tool-aware sequencing to implement the generated plans. The framework introduces RemPlan, the first comprehensive benchmark for evaluating dynamic mRAG planning capabilities, featuring diverse question types requiring different retrieval strategies.

## Method Summary
E-Agent implements a one-time mRAG planning strategy using an 8B parameter model (InternVL2-8B) to analyze image-query pairs and generate complete execution plans in a single forward pass. The task executor, powered by a 72B parameter MLLM (Qwen2-VL-72B), implements the plan using four specialized tools: Image Search, Requery, Text Search, and Response. The system is fine-tuned on 10K training samples and evaluated on RemPlan and other multimodal benchmarks. The planner distinguishes between retrieval-dependent and retrieval-independent queries, suppressing unnecessary tool use for "Fundamental" questions while orchestrating appropriate tools for others.

## Key Results
- Achieves 13% accuracy gains over state-of-the-art mRAG methods
- Reduces redundant searches by 37% through dynamic planning
- Operates with an 8B parameter model for improved computational efficiency
- Introduces RemPlan benchmark with 200 image-question pairs for evaluating mRAG planning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Single-Pass Deterministic Planning
The E-Agent planner analyzes the image and query simultaneously to generate a complete execution plan upfront, eliminating the need for iterative replanning. This contrasts with methods like OmniSearch that replan after every step, reducing latency and error propagation.

### Mechanism 2: Context-Aware Tool Orchestration
The planner jointly encodes visual and textual inputs to categorize queries into four types, suppressing tool use for "Fundamental" types while orchestrating specific tools for others. This prevents unnecessary noise from irrelevant searches.

### Mechanism 3: Asymmetric Model Sizing
The framework offloads complex reasoning to a large MLLM (72B) while restricting planning logic to a smaller, fine-tuned model (8B). This optimizes the efficiency-accuracy trade-off by using the 8B model specifically for structured planning outputs.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Understanding why LLMs need RAG is crucial - standard LLMs have static knowledge that becomes outdated. Quick check: Can you explain why a standard LLM might fail to answer "Who won the 2024 election?" without RAG?

- **Multimodal Large Language Models (MLLMs)**: The executor relies on MLLMs to process images and text simultaneously. Understanding how vision encoders map images to text embedding space is crucial for debugging "Visual-Recognition" failures. Quick check: How does an MLLM handle an input image differently than a text description of that image?

- **Supervised Fine-Tuning (SFT) for Tool Use**: The planner is trained to output specific tool schemas rather than prompted. Understanding how to format datasets (Image + Query -> Plan String) is required to reproduce or modify the planner. Quick check: What data format is required to train a model to output a function call instead of natural language?

## Architecture Onboarding

- **Component map**: Planner (InternVL2-8B) -> Executor -> Tools (Image Search, Requery, Text Search, Response)
- **Critical path**: User Query + Image → Planner Inference (8B) → Structured Plan → Tool Loop → Final Generation (72B)
- **Design tradeoffs**: Efficiency vs. Multi-hop (one-time plan is faster but brittle), Noise Reduction vs. Recall (strict classification avoids noise but risks hallucination)
- **Failure signatures**: Image Search Hallucination, Silent Knowledge Gaps, Requery Drift
- **First 3 experiments**: Verify Planner Accuracy (compare predicted plans against ground-truth), Ablate Tooling (run with only Response tool), Stress Test "Type 1" (verify planner suppresses redundant searching)

## Open Questions the Paper Calls Out

### Open Question 1
Can hierarchical planning architectures with intermediate validation checkpoints improve E-Agent's performance on complex multi-hop reasoning tasks? The authors identify the "one-shot planning mechanism" as a limitation lacking intermediate verification steps.

### Open Question 2
How can adaptive toolkit management mechanisms be designed to incorporate emerging multimodal interfaces without manual updates? The framework's dependence on predefined toolkits necessitates periodic updates, limiting long-term adaptability.

### Open Question 3
Does integrating dynamic reflection modules reduce failure rates caused by incorrect retrieval results? Case analysis reveals incorrect search results lead to wrong answers because the executor cannot adjust the plan.

## Limitations

- Single-pass planning strategy uncertainty for complex multi-hop reasoning tasks
- Asymmetric model sizing represents untested architectural choice without direct comparisons
- RemPlan benchmark covers only 200 image-question pairs, potentially limiting generalizability

## Confidence

**High Confidence**: The claim that E-Agent achieves 13% accuracy gains over state-of-the-art mRAG methods is supported by quantitative results across multiple benchmarks.

**Medium Confidence**: The mechanism of reducing redundant searches by 37% is well-supported by efficiency metrics, though absolute numbers depend on baseline comparisons.

**Low Confidence**: The scalability and robustness of the single-pass planning approach for complex, real-world scenarios remains speculative, as evaluation focuses on curated benchmark questions.

## Next Checks

1. **Multi-hop Reasoning Stress Test**: Evaluate E-Agent on complex, real-world scenarios requiring iterative refinement to validate the brittleness claim.

2. **Planner Generalization Study**: Test the 8B planner on out-of-distribution data with different visual styles or question types not represented in RemPlan to assess overfitting risks.

3. **Ablation of Asymmetric Sizing**: Compare E-Agent performance with alternative configurations (e.g., 72B planner/72B executor, 8B planner/8B executor) to quantify the actual contribution of the asymmetric architecture.