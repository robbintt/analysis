---
ver: rpa2
title: Estimating forest carbon stocks from high-resolution remote sensing imagery
  by reducing domain shift with style transfer
arxiv_id: '2502.00784'
source_url: https://arxiv.org/abs/2502.00784
tags:
- carbon
- image
- data
- estimation
- forest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of accurately estimating forest
  carbon stocks from high-resolution remote sensing imagery, particularly in regions
  with complex topography and cloud cover. The authors propose the MSwin-Pix2Pix model,
  which combines style transfer using Swin Transformer and deep learning techniques
  to reduce domain shift between images captured at different times and locations.
---

# Estimating forest carbon stocks from high-resolution remote sensing imagery by reducing domain shift with style transfer

## Quick Facts
- **arXiv ID**: 2502.00784
- **Source URL**: https://arxiv.org/abs/2502.00784
- **Authors**: Zhenyu Yu; Jinnian Wang
- **Reference count**: 15
- **Primary result**: MSwin-Pix2Pix model achieves MAE of 16.2891, RMSE of 29.3763, R² of 0.7105, and SSIM of 0.7510 for forest carbon stock estimation

## Executive Summary
This study addresses the challenge of accurately estimating forest carbon stocks from high-resolution remote sensing imagery, particularly in regions with complex topography and cloud cover. The authors propose the MSwin-Pix2Pix model, which combines style transfer using Swin Transformer and deep learning techniques to reduce domain shift between images captured at different times and locations. The model incorporates a median filter and mask to improve accuracy and stability. Results show that MSwin-Pix2Pix outperforms other models, achieving an MAE of 16.2891, RMSE of 29.3763, R² of 0.7105, and SSIM of 0.7510. Analysis of carbon stock trends in Huize County, China, from 2005 to 2020 reveals an overall increasing trend, with 44.04% of the area experiencing an increase and 10.22% a decrease.

## Method Summary
The MSwin-Pix2Pix model uses a cGAN framework with a Mask SUNet generator that incorporates Swin Transformer blocks to extract global features through attention mechanisms. The method includes style transfer to align multi-temporal satellite images and reduce domain shift, a median filter to remove noise, and a vegetation mask to improve accuracy. The model is trained using a combination of adversarial loss and pixel-wise losses, with validation through 5-fold cross-validation.

## Key Results
- MSwin-Pix2Pix achieves superior performance with MAE of 16.2891, RMSE of 29.3763, R² of 0.7105, and SSIM of 0.7510
- The model outperforms traditional methods including OLS, GWR, RF, SVR, CNN, and standard Pix2Pix
- Carbon stock analysis from 2005-2020 shows an overall increasing trend, with 44.04% of area increasing and 10.22% decreasing

## Why This Works (Mechanism)

### Mechanism 1: Domain Shift Reduction via Style Transfer
The model performs image-to-image translation using a conditional GAN with a Swin Transformer-based generator, mapping images from different temporal domains to a consistent style. This effectively normalizes for sensor, lighting, and atmospheric variations.

### Mechanism 2: Global Feature Extraction with Swin Transformer
The Swin Transformer blocks use Shifted Window Multi-Head Self-Attention to capture long-range dependencies and multi-scale features, improving the model's ability to understand complex forest structures.

### Mechanism 3: Target Area Refinement via Mask and Median Filter
An NDVI-based mask identifies forest areas and filters predictions for non-vegetated regions, while a median filter removes high-intensity speckle noise near boundaries, reducing overall estimation error.

## Foundational Learning

- **Concept: Generative Adversarial Networks (GANs) and Conditional GANs (cGANs)**
  - Why needed: The core of the Pix2Pix and Swin-Pix2Pix models is a cGAN
  - Quick check: In a cGAN for image translation, what is the primary difference in the discriminator's input compared to a traditional GAN?

- **Concept: Attention Mechanisms and Vision Transformers (ViT)**
  - Why needed: The paper replaces CNN layers with Swin Transformer blocks
  - Quick check: How does the 'Shifted Window' approach in the Swin Transformer differ from the global self-attention used in the original Vision Transformer (ViT), and what problem does it solve?

- **Concept: Image-to-Image Translation**
  - Why needed: The problem is framed as translating a remote sensing image into a carbon stock map
  - Quick check: In this paper's context, what serves as the 'content' image and what serves as the 'style' image in the style transfer process?

## Architecture Onboarding

- **Component map**: Input images -> Feature extraction (50 bands) -> Swin-Pix2Pix style transfer (optional) -> MSwin-Pix2Pix generator (Mask SUNet) -> PatchGAN discriminator -> Median filter + Mask post-processing -> Carbon stock estimation

- **Critical path**: Data alignment → Style transfer training → Estimation training → Inference with post-processing

- **Design tradeoffs**: Swin Transformer vs. CNN (global vs. local features), PatchGAN vs. full-image discriminator (high-frequency details vs. global consistency), L2 + SmoothL1 vs. L1 loss (estimation vs. style transfer)

- **Failure signatures**: Blurry generated images (L1 loss dominance), high boundary noise (mask threshold issues), checkerboard artifacts (up-sampling problems)

- **First 3 experiments**:
  1. Implement Swin-Pix2Pix to convert Landsat images to GF-1 style using L1 + cGAN loss
  2. Train carbon estimation model in three configurations: standard Pix2Pix, Swin-Pix2Pix, full MSwin-Pix2Pix
  3. Train full MSwin-Pix2Pix model using different loss combinations (L1 only, L2 only, L2+SmoothL1)

## Open Questions the Paper Calls Out

### Open Question 1
Can the MSwin-Pix2Pix model generalize effectively to forest ecosystems with different structural complexities and species compositions than the pure Pinus forests of Huize County?

### Open Question 2
How does the MSwin-Pix2Pix optical imagery approach compare directly to LiDAR-based estimations in terms of accuracy and error distribution?

### Open Question 3
Does the style transfer process, which aligns historical images to a 2020 reference style, inadvertently suppress inter-annual phenological variations or disturbance signals?

## Limitations
- The model's performance depends heavily on the quality and representativeness of the style transfer training data
- The fixed NDVI threshold (M - 2σ) may not be optimal for regions with different vegetation density
- High computational cost of Swin Transformer blocks compared to CNNs could limit scalability

## Confidence

- **High Confidence**: Observed performance improvement over baseline models given specified architecture and loss functions
- **Medium Confidence**: Mechanism by which style transfer reduces domain shift, dependent on generalization of learned style mappings
- **Low Confidence**: Long-term stability and generalizability to regions with vastly different topography and vegetation types

## Next Checks

1. **Ablation Study on Mask Threshold**: Systematically vary the NDVI mask threshold to determine sensitivity and optimal value for different forest types

2. **Out-of-Distribution Test**: Evaluate the trained MSwin-Pix2Pix model on a separate dataset from a different geographic region with distinct topography and cloud patterns

3. **Computational Efficiency Analysis**: Benchmark training and inference times against baseline CNN model to quantify computational tradeoff for performance gain