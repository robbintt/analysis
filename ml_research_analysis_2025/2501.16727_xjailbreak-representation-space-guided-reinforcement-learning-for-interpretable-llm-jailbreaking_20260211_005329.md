---
ver: rpa2
title: 'xJailbreak: Representation Space Guided Reinforcement Learning for Interpretable
  LLM Jailbreaking'
arxiv_id: '2501.16727'
source_url: https://arxiv.org/abs/2501.16727
tags:
- prompt
- prompts
- intent
- jailbreak
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces xJailbreak, a novel reinforcement learning-based
  black-box jailbreaking method that improves interpretability by optimizing prompt
  generation through embedding proximity analysis between benign and malicious prompts.
  The approach employs representation space guidance as a critical component of the
  reward function, ensuring rewritten prompts closely align with the original intent
  while enhancing attack effectiveness.
---

# xJailbreak: Representation Space Guided Reinforcement Learning for Interpretable LLM Jailbreaking

## Quick Facts
- arXiv ID: 2501.16727
- Source URL: https://arxiv.org/abs/2501.16727
- Authors: Sunbowen Lee; Shiwen Ni; Chi Wei; Shuaimin Li; Liyang Fan; Ahmadreza Argha; Hamid Alinejad-Rokny; Ruifeng Xu; Yicheng Gong; Min Yang
- Reference count: 29
- One-line primary result: Novel RL-based black-box jailbreaking method achieving up to 80% attack success rate on multiple prominent LLMs while preserving semantic intent through embedding proximity analysis.

## Executive Summary
xJailbreak introduces a novel reinforcement learning-based black-box jailbreaking method that optimizes prompt generation by analyzing embedding proximity between benign and malicious prompts. The approach employs representation space guidance as a critical component of the reward function, ensuring rewritten prompts closely align with the original intent while enhancing attack effectiveness. The method achieves state-of-the-art performance on multiple prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, and GPT-4o-0806, with attack success rates reaching up to 80% in some cases. The paper also introduces a comprehensive evaluation framework incorporating keywords, intent matching, and answer validation to provide a more rigorous assessment of jailbreak success.

## Method Summary
xJailbreak formulates LLM jailbreaking as a Markov Decision Process where the state is the prompt embedding (4096-dim from Llama3-8B-Instruct), actions are template selections (10 options), and rewards combine a "borderline score" (embedding proximity to benign prompts) with an "intent score" (semantic preservation). The RL agent uses Proximal Policy Optimization (PPO) with a 4096-1024-10 MLP architecture, trained with γ=0.9 and α=0.2. The method employs a Helper-LLM (Llama3-8B-Instruct-Jailbroken) for rewriting prompts and computing intent scores, while a Repr-LLM extracts embeddings for the representation space guidance. The training data consists of 80 malicious prompts from AdvBench, with evaluation on 100 test prompts from MaliciousInstruct.

## Key Results
- Achieves 80% attack success rate on Qwen2.5-7B-Instruct and 75% on GPT-4o-0806
- Maintains intent preservation with scores consistently above 0.9 across all tested models
- Outperforms existing methods including AutoDAN, DAN, and RL-JACK on multiple benchmarks
- Demonstrates geometric interpretability through embedding space visualization showing clear separation between benign and malicious prompts

## Why This Works (Mechanism)

### Mechanism 1: Representation Space Boundary Crossing
The method exploits geometric properties of the model's latent space by calculating a "borderline score" that measures the projection of a prompt's embedding onto a vector connecting malicious and benign prompt centroids. This incentivizes rewriting prompts to reside within the semantic neighborhood of benign prompts, reducing the likelihood of triggering safety refusals. The approach assumes safety alignment is spatially localized in embedding space rather than being an intrinsic property of semantic content.

### Mechanism 2: Intent Preservation via Dual-Reward Shaping
A composite reward function balances geometric optimization with semantic fidelity by weighting the borderline score with an intent score computed by a Helper-LLM. The intent score rates similarity between original and rewritten prompts (-1, 0, 1), penalizing drift from the original malicious goal. This prevents the agent from discovering jailbreaks that lose their harmful intent while achieving technical success.

### Mechanism 3: Short-Horizon Policy Optimization (Gamma Tuning)
The method uses γ=0.9 instead of standard 0.99 to prioritize immediate successful mutations over theoretical future gains. This treats jailbreaking as a near-greedy search problem rather than a multi-step strategic game, appropriate for the dense state space of prompt embeddings where jailbreak-able configurations can be found quickly.

## Foundational Learning

- **Markov Decision Process (MDP) Formulation for Text**
  - Why needed: Converts static text into dynamic state (embedding vector) and action (template selection) for RL application
  - Quick check: How does the system represent the "state" of a prompt to the RL agent? (Answer: The embedding vector of the current prompt)

- **Projection and Vector Arithmetic in Latent Space**
  - Why needed: The borderline score relies on projecting a prompt onto a vector connecting benign and malicious centroids
  - Quick check: If a prompt's embedding is exactly at the midpoint between the benign center and malicious center, what is its borderline distance? (Answer: 0)

- **Proximal Policy Optimization (PPO)**
  - Why needed: Specific RL algorithm used for stability in high-dimensional embedding space
  - Quick check: Why does the paper use PPO instead of standard policy gradient? (Answer: For stability in the high-dimensional embedding space)

## Architecture Onboarding

- **Component map:** Input Malicious Prompt -> Embed via Repr-LLM -> State Vector -> RL Agent selects Template ID -> Helper-LLM rewrites prompt using Template -> Repr-LLM embeds new prompt -> Calculate Borderline Score -> Victim-LLM receives new prompt -> Generate output -> Check output for refusal and validity -> Helper-LLM compares New vs Old (Intent) -> Aggregate Reward -> Train PPO Agent

- **Critical path:** The pipeline processes prompts through embedding extraction, template selection, rewriting, geometric reward calculation, victim model interaction, and semantic validation, with the RL agent learning to optimize this sequence for successful jailbreaks.

- **Design tradeoffs:** Uses jailbroken Helper-LLM for rewriting pipeline (safety-aligned model would refuse), sets α=0.2 to balance geometric and semantic rewards, employs short-horizon RL (γ=0.9) for near-greedy search.

- **Failure signatures:** High Borderline Score with Low Validity indicates benign prompt but ineffective template; Intent Score Collapse suggests nonsense generation; Random Walk indicates poor convergence with high γ.

- **First 3 experiments:** 1) Reproduce embedding visualization to verify linear separability of benign/malicious prompts, 2) Run α ablation sweep on validation set to confirm optimal weighting, 3) Log template selection frequency and test top templates in isolation to distinguish RL learning from heuristic exploitation.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the borderline score be normalized to eliminate sensitivity associated with the weight parameter α? The authors identify difficulties in normalizing the borderline score and suggest future research explore robust scaling methods.

- **Open Question 2:** Does integrating dynamic template generation improve attack success rates compared to fixed rewriting templates? The paper notes that not optimizing template generation suggests integrating advancements from peer research could enhance performance.

- **Open Question 3:** Is reliance on a pre-existing jailbroken model for embedding guidance strictly necessary, or can standard aligned models suffice? The paper uses a model modified to be highly responsive to malicious prompts for generating the representation space but doesn't test alternatives.

## Limitations
- Reliance on jailbroken Helper-LLM for rewriting pipeline, which may not be available in production environments
- Embedding-space separation assumption that benign and malicious prompts form distinct clusters may fail for models with different safety alignment training
- Short-horizon RL approach may miss complex multi-step jailbreak strategies requiring longer planning horizons

## Confidence
- **High confidence** in empirical performance metrics (ASR, intent preservation) across multiple open and closed-source models
- **Medium confidence** in theoretical generalizability of representation-space guidance mechanism
- **Medium confidence** in interpretability claims since representation-space guidance offers geometric intuition but actual decision boundaries remain opaque
- **High confidence** in intent preservation mechanism through ablation studies, but depends critically on Helper-LLM's ability to judge semantic similarity

## Next Checks
1. Test representation-space separation assumption on safety-aligned models with different training protocols (constitutional AI vs RLHF)
2. Evaluate performance when Helper-LLM is replaced with a safety-aligned model that may refuse rewriting requests
3. Compare short-horizon (γ=0.9) vs long-horizon (γ=0.99) performance on jailbreaks requiring sequential semantic transformations