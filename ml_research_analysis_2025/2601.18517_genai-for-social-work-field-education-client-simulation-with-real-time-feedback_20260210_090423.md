---
ver: rpa2
title: 'GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback'
arxiv_id: '2601.18517'
source_url: https://arxiv.org/abs/2601.18517
tags:
- social
- client
- skills
- work
- counseling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SWITCH is a chatbot system for social work field education that
  integrates client simulation, real-time counseling skill classification, and Motivational
  Interviewing (MI) progression. It addresses the challenge of providing timely, objective
  feedback in field training, where instructor availability is limited.
---

# GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback

## Quick Facts
- arXiv ID: 2601.18517
- Source URL: https://arxiv.org/abs/2601.18517
- Authors: James Sungarda; Hongkai Liu; Zilong Zhou; Tien-Hsuan Wu; Johnson Chun-Sing Cheung; Ben Kao
- Reference count: 35
- Primary result: SWITCH achieves 0.98-0.99 accuracy in counseling skill classification using fine-tuned BERT, significantly outperforming baseline methods.

## Executive Summary
SWITCH is a chatbot system for social work field education that integrates client simulation, real-time counseling skill classification, and Motivational Interviewing (MI) progression. It addresses the challenge of providing timely, objective feedback in field training, where instructor availability is limited. The system uses a cognitively grounded client profile with static and dynamic fields to simulate realistic client behavior. Counseling skills are classified using both fine-tuned BERT and in-context learning with retrieval over annotated transcripts, achieving accuracy of 0.98-0.99 and 0.92-0.94 respectively, significantly outperforming the baseline (0.65). The MI controller uses skill scores and contextual analysis to manage stage progression. SWITCH provides a scalable, low-cost solution that complements field education and allows supervisors to focus on higher-level mentorship.

## Method Summary
The system implements two approaches for counseling skill classification: (1) Fine-tuning bert-large-uncased with focal loss (α=0.25, γ=2.0) on 4,734 annotated counselor-client utterance pairs, using 80/10/10 train/val/test split; (2) In-context learning with gpt-4o-mini, retrieving 8 similar examples using BM25, MiniLM, or BGE-M3, then prompting with conversation context and skill definitions. The cognitive model maintains separate static (background, beliefs) and dynamic (emotions, automatic thoughts, openness) fields, regenerating dynamic fields before each response. MI progression is controlled by a weighted skill score (Equation 1) and an LLM-based controller that evaluates stage completion using chain-of-thought reasoning and an internal rewards/costs table.

## Key Results
- Counseling skill classification accuracy: 0.98-0.99 using fine-tuned BERT, 0.92-0.94 using ICL with retrieval, versus baseline of 0.65
- The cognitively grounded client profile with static and dynamic fields enables realistic, adaptive client simulation that evolves during sessions
- Integration of real-time skill classification with MI stage progression control provides objective, scalable field training feedback

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A cognitively grounded client profile with separate static and dynamic fields enables realistic, adaptive client simulation that evolves during sessions.
- **Mechanism:** Static fields (background, core beliefs, coping strategies) provide stable identity anchoring. Dynamic fields (emotions, automatic thoughts, openness, behaviors) are regenerated by the LLM before each response, ensuring the client's reply reflects their current psychological state.
- **Core assumption:** LLMs can generate psychologically coherent internal state updates that produce behavior consistent with both the persona and the therapeutic context.
- **Evidence anchors:** The abstract states SWITCH uses static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness). Table IV demonstrates the cognitive model structure with explicit static/dynamic separation; Section III-C states these dynamic fields are generated before the output message is generated. PATIENT-ψ (neighbor paper) previously validated that inserting cognitive fields into LLM system prompts improves simulation fidelity; SWITCH extends this with dynamic field updates per turn.
- **Break condition:** If dynamic fields are generated after the response (rather than before), or if fields become inconsistent across turns, the client will behave incoherently.

### Mechanism 2
- **Claim:** Retrieval-augmented in-context learning (ICL) over annotated counseling transcripts substantially improves skill classification accuracy over zero-shot prompting.
- **Mechanism:** Rather than prompting an LLM with only skill definitions, SWITCH retrieves 8 similar annotated utterances from a demonstration pool using dense/sparse retrieval (BGE-M3, MiniLM, BM25). These examples provide task-specific patterns that the LLM can analogize from, reducing ambiguity in skill identification.
- **Core assumption:** Annotated transcripts contain sufficient signal that similar utterances will exhibit similar skill patterns, and retrieval can surface relevant exemplars.
- **Evidence anchors:** The abstract states counseling skills are classified using in-context learning with retrieval over annotated transcripts, achieving accuracy of 0.92-0.94 significantly outperforming the baseline (0.65). Table VI shows baseline accuracy 0.6487-0.6520 vs. ICL 0.9286-0.9462; Section IV-A details the 8-example retrieval approach. OnCoCo 1.0 (neighbor paper) provides a public dataset for fine-grained counseling message classification, supporting transcript-based approaches—but the corpus does not directly validate ICL over retrieval for this specific task.
- **Break condition:** If the demonstration pool lacks coverage for certain skills (e.g., rare skills like "immediacy" at 1.34%), retrieval will surface irrelevant examples, and accuracy will degrade.

### Mechanism 3
- **Claim:** Combining a weighted skill score with an LLM-based MI controller enables principled stage progression control.
- **Mechanism:** A skill score (Equation 1) aggregates weighted frequencies of skills used, with stage-specific weights (early-stage skills weighted 2 in early stages, reversed in later stages). When the score exceeds a threshold (0.4 or 0.6), an LLM controller evaluates whether MI stage goals are met using chain-of-thought reasoning and an internal "rewards/costs of change" table modeled on ambivalence theory.
- **Core assumption:** The skill score formula correlates with therapeutic progress, and an LLM can reliably evaluate MI stage completion given structured prompts.
- **Evidence anchors:** The abstract states the MI controller uses skill scores and contextual analysis to manage stage progression. Section III-A describes Equation 1, threshold activation, and the MI controller's use of chain-of-thought and the rewards/costs table inspired by ambivalence theory. Yang et al. (neighbor paper) implemented MI principles for consistent client simulation but lacked explicit progression control; SWITCH extends this with a controller mechanism, but corpus evidence for LLM-based stage evaluation is limited.
- **Break condition:** If skill weights don't align with actual MI stage relevance, or if the controller is called prematurely/over-cautiously, progression will be erratic or stuck.

## Foundational Learning

- **Concept:** Motivational Interviewing (MI) stages of change
  - **Why needed here:** The entire progression system is built on MI's stage model (pre-contemplation → contemplation → preparation). Understanding how client ambivalence, resistance, and readiness manifest in each stage is essential for designing stage info and progression logic.
  - **Quick check question:** Can you describe how a client in "pre-contemplation" typically responds to change suggestions, versus one in "contemplation"?

- **Concept:** Multi-label text classification with class imbalance
  - **Why needed here:** Each utterance can contain multiple skills (avg. 3.06), and skill frequencies vary dramatically (active listening 14.34% vs. normalizing 0.59%). Focal loss and threshold optimization are used to address this.
  - **Quick check question:** Why would focal loss (with γ=2.0) help when training on imbalanced skill data?

- **Concept:** Retrieval-augmented in-context learning
  - **Why needed here:** The skill classifier uses ICL with retrieved examples rather than fine-tuning alone. Understanding how retrieval surface relevant demonstrations clarifies why this hybrid approach works.
  - **Quick check question:** How does providing 8 similar annotated examples in the LLM context improve classification over definitions alone?

## Architecture Onboarding

- **Component map:**
  User utterance → Skill Classification Module (BERT or ICL with retrieval) → skill labels
  Skill labels → Skill Score Calculator (Equation 1, stage-specific weights)
  If threshold met → MI Controller LLM (chain-of-thought + rewards/costs table) → stage progression decision
  Client Profile (static + dynamic fields) + Stage Info → Response Generation LLM → client response + updated dynamic fields

- **Critical path:**
  User message → Skill classification → Score accumulation → Threshold check → (if met) MI controller evaluation → (if progressed) stage info update → Response generation with refreshed cognitive model

- **Design tradeoffs:**
  - **BERT vs. ICL:** BERT achieves higher accuracy (0.98–0.99) but requires fine-tuning and labeled data. ICL (0.92–0.94) is no-training but depends on retrieval quality and demonstration pool coverage.
  - **Threshold strategy:** Joint optimization across classes outperformed static or independent thresholds for BERT (Table VI).
  - **Retrieval method:** BGE-M3 slightly outperformed BM25 and MiniLM in ICL experiments, but differences were small.

- **Failure signatures:**
  - **Inconsistent client persona:** Dynamic fields not regenerated before response, or state drifting across turns.
  - **Stuck or erratic progression:** Skill thresholds mis-calibrated, or MI controller making inconsistent decisions.
  - **Rare skill under-detection:** Class imbalance in training data causes low recall for skills like "immediacy" (F1=0.00 in Table VII for BERT).
  - **Robotic client responses:** Missing or incomplete stage info in prompts.

- **First 3 experiments:**
  1. **Replicate skill classification ablation:** Run baseline (zero-shot), ICL with different retrievers, and BERT on a held-out test set; compare accuracy and per-skill F1 to validate Table VI/VII.
  2. **Cognitive model consistency test:** Conduct multi-turn simulated sessions; verify dynamic fields are updated per turn and remain coherent with the persona.
  3. **Progression calibration:** Simulate conversations designed to trigger (or not trigger) stage transitions; validate that the controller's decisions align with MI theory and that thresholds are appropriately sensitive.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can personalized feedback be generated based on the longitudinal skill usage data collected during training sessions?
- **Basis in paper:** Section VII states future work should use the data of social work skills used by the trainee to generate "useful and personalized feedback either after each individual interaction, or when the training session is done."
- **Why unresolved:** The current system focuses on real-time classification and progression control but does not yet synthesize the session data into explanatory feedback for the trainee.
- **What evidence would resolve it:** A module that generates post-session reports and a user study validating the helpfulness of the automated feedback.

### Open Question 2
- **Question:** Can the system be extended to support voice and video-based interactions?
- **Basis in paper:** Section VII notes that the current solution "leaves room for future implementations of SWITCH in voice and video based systems."
- **Why unresolved:** The current architecture is designed for text-based chat; voice and video require integration with additional modalities (e.g., speech-to-text, facial expression analysis).
- **What evidence would resolve it:** A prototype implementation demonstrating real-time skill classification from audio-visual streams.

### Open Question 3
- **Question:** How can classification performance be improved for rare counseling skills (e.g., Immediacy, Normalizing) given the severe class imbalance in the training data?
- **Basis in paper:** Section IV and Table VII show F1-scores of 0.0 for Immediacy and low scores for other rare skills, noting the "pronounced imbalance" and suggesting a "more balanced dataset" is needed.
- **Why unresolved:** The current fine-tuned BERT and ICL approaches struggle to generalize to skills with very few training examples (e.g., <100 instances).
- **What evidence would resolve it:** Experiments utilizing data augmentation or few-shot learning techniques that yield significantly higher F1-scores for the minority classes.

### Open Question 4
- **Question:** Does using SWITCH result in measurable improvements in counseling competence compared to traditional peer role-play?
- **Basis in paper:** While Section VI describes deployment in a course, the paper evaluates technical accuracy (classification) but does not present data on student learning outcomes or skill retention.
- **Why unresolved:** The system's educational efficacy is assumed based on the pedagogical framework (MI), but it lacks quantitative validation of learning gains.
- **What evidence would resolve it:** A controlled study comparing objective skill assessments of students trained via SWITCH versus those trained via traditional methods.

## Limitations

- The system's effectiveness depends heavily on the quality and representativeness of the annotated counseling dataset, which is not publicly available and may limit generalizability.
- The MI controller's progression logic, while theoretically grounded, lacks direct empirical validation against real counseling outcomes.
- The dynamic cognitive model's ability to maintain coherent client personas across extended sessions has not been rigorously tested beyond the reported evaluations.

## Confidence

- **High Confidence:** Skill classification accuracy (0.98-0.99 for BERT, 0.92-0.94 for ICL) is well-supported by experimental results and ablation studies.
- **Medium Confidence:** The cognitive model's ability to generate realistic client behavior is theoretically sound but lacks extensive empirical validation.
- **Medium Confidence:** MI stage progression control shows promise but has limited corpus evidence for the LLM controller's reliability in complex counseling scenarios.

## Next Checks

1. **Dataset Generalizability Test:** Evaluate SWITCH's skill classification and progression logic on an independent counseling dataset to assess performance beyond the original training corpus.
2. **Extended Session Coherence:** Conduct multi-session simulations (3+ sessions) to verify the cognitive model maintains consistent client personas and that progression feels natural over time.
3. **MI Theory Alignment Audit:** Have counseling experts review 50+ generated session transcripts to assess whether stage transitions and client responses align with established MI principles and therapeutic best practices.