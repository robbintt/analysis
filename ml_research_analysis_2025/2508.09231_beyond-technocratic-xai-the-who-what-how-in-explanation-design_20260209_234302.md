---
ver: rpa2
title: 'Beyond Technocratic XAI: The Who, What & How in Explanation Design'
arxiv_id: '2508.09231'
source_url: https://arxiv.org/abs/2508.09231
tags:
- explanation
- design
- explanations
- methods
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a three-part framework for designing explainable
  AI (XAI) systems that emphasizes stakeholder needs, explanation content, and delivery
  methods. The framework addresses the gap between technical XAI methods and their
  practical application by treating explanation as a sociotechnical design process.
---

# Beyond Technocratic XAI: The Who, What & How in Explanation Design

## Quick Facts
- arXiv ID: 2508.09231
- Source URL: https://arxiv.org/abs/2508.09231
- Reference count: 38
- One-line primary result: Introduces a three-part framework for designing explainable AI systems that emphasizes stakeholder needs, explanation content, and delivery methods.

## Executive Summary
This paper addresses the gap between technical XAI methods and their practical application by introducing a three-part framework for designing explainable AI systems. The framework treats explanation as a sociotechnical design process, emphasizing that effective explanations require understanding who needs them, what they need explained, and how that explanation should be delivered. The work goes beyond technocratic approaches by incorporating ethical considerations including epistemic inequality, social inequity, and accountability. A healthcare case study demonstrates how this framework guides explanation design, showing that effective XAI requires balancing technical accuracy with accessibility, ethical reflection, and real-world constraints.

## Method Summary
The framework consists of three sequential steps: (1) **Who** - classifying stakeholders along dimensions of domain knowledge, technical knowledge, and goals; (2) **What** - selecting explanation content based on scope (local/global), focus (behavioral/mechanistic), specificity (model-specific/agnostic), and operational cost; and (3) **How** - choosing delivery formats (numerical, visual, textual, or interactive) matched to stakeholder needs. The paper proposes a four-axis classification system for XAI methods and emphasizes that explanation delivery is fundamentally a design problem that shapes epistemic access. The healthcare case study applies this framework to a chronic heart failure readmission prediction model, selecting SHAP for local, behavioral, model-agnostic explanations delivered through visual and textual formats suitable for clinicians.

## Key Results
- Proposes a three-part framework (Who-What-How) that treats explanation design as a sociotechnical process rather than a purely technical problem
- Identifies four critical axes for method selection: scope, focus, specificity, and operational cost
- Highlights ethical risks including epistemic inequality, social inequity, and accountability gaps in current XAI approaches
- Demonstrates through case study how framework guides selection of SHAP method with visual/textual delivery for healthcare clinicians
- Calls for human-centered, context-aware approaches that support both understanding and accountability

## Why This Works (Mechanism)

### Mechanism 1
Matching explanation design to stakeholder characteristics improves explanation utility and reduces misinterpretation. By classifying stakeholders along dimensions of domain knowledge, technical knowledge, and goals, explanation designers can select methods that align with cognitive capacity and decision-making needs. This reduces the gap between technical output and user understanding.

### Mechanism 2
Structuring explanation content decisions along four axes (scope, focus, specificity, cost) enables systematic method selection. Rather than choosing XAI methods ad hoc, designers evaluate each option against these dimensions, creating a decision boundary for method appropriateness.

### Mechanism 3
Delivery format choice shapes epistemic access and can reinforce or mitigate power asymmetries. Numerical formats privilege statistically fluent users; visual formats enable quick pattern recognition; textual formats improve accessibility for non-experts but risk oversimplification; interactive formats enable exploration but require more user effort.

## Foundational Learning

- **Concept**: Post-hoc vs intrinsic explainability
  - **Why needed here**: The framework focuses on post-hoc methods applied to black-box models; understanding this distinction clarifies why operational cost and model specificity matter.
  - **Quick check question**: Can you explain why LIME requires multiple model queries while gradient-based methods reuse model internals?

- **Concept**: Feature attribution vs counterfactual explanations
  - **Why needed here**: These represent different answers to "What" gets explained—whether you explain which features mattered or what would need to change for a different outcome.
  - **Quick check question**: For a loan rejection, what would a feature attribution show versus a counterfactual explanation?

- **Concept**: Epistemic injustice in AI systems
  - **Why needed here**: The framework explicitly addresses how explanation design can exclude certain stakeholders from meaningful understanding; this concept frames the ethical dimension.
  - **Quick check question**: Who in a healthcare AI system typically receives richer explanations—clinicians or patients—and what does this imply about knowledge distribution?

## Architecture Onboarding

- **Component map**: Stakeholder Classifier -> Explanation Selector -> Delivery Renderer -> Ethics Check
- **Critical path**: WHO (identify stakeholder) -> WHAT (select content and method) -> HOW (choose delivery format) -> Ethics reflection at each stage
- **Design tradeoffs**: Faithfulness vs accessibility, cost vs depth, model access vs generality
- **Failure signatures**: Explanations ignored/misunderstood (mismatched WHO), technically correct but not actionable (mismatched WHAT scope), overwhelm/mislead (mismatched HOW format), legitimize bias without contestation (missing ethics check)
- **First 3 experiments**:
  1. **Stakeholder mapping exercise**: For your deployment context, enumerate all stakeholders and classify them using Table 1 dimensions. Identify any stakeholders who occupy multiple roles.
  2. **Method comparison on single prediction**: Apply two methods from different categories in Table 2 (e.g., SHAP and counterfactuals) to the same model prediction. Document what each reveals and hides.
  3. **Format A/B test**: Take one explanation and render it in two formats (e.g., numerical SHAP values vs textual summary). Test comprehension with users from different stakeholder groups.

## Open Questions the Paper Calls Out

### Open Question 1
How can empirical validation of XAI frameworks move beyond traditional metrics (satisfaction, trust) to capture unintended consequences like cognitive overload or institutional drift? The paper states empirical validation "must go beyond traditional metrics" and engage with "lived experiences" to identify which groups are confused or excluded.

### Open Question 2
How can design toolkits operationalize ethical robustness (e.g., flagging epistemic injustice) without reducing participatory design to a "token gesture"? The paper calls for "design tools and workflows that are both actionable and ethically robust," specifically warning against simplification that conceals harm.

### Open Question 3
How do material resource constraints and institutional power dynamics impact the distribution of explanation quality across different organizations? The paper asks researchers to explore how explanation quality is distributed across institutions and to investigate "why those constraints exist."

## Limitations
- The framework lacks empirical validation; the healthcare case study is described as a "thought experiment" without implementation details or user testing results
- Critical assumptions about stakeholder categorization and method selection criteria remain theoretical
- The four-axis classification system for XAI methods is proposed without systematic evaluation of whether these axes capture all relevant constraints
- Does not address how to handle cases where users occupy multiple roles simultaneously or where stakeholder groups have highly heterogeneous intra-group needs

## Confidence
- **High Confidence**: The recognition that explanation design is a sociotechnical process requiring attention to stakeholder needs, content selection, and delivery methods is well-established in the literature and logically sound.
- **Medium Confidence**: The specific three-part framework (WHO-WHAT-HOW) provides a useful organizational structure, but its superiority over existing approaches lacks empirical comparison.
- **Low Confidence**: The four-axis method classification and stakeholder categorization scheme require field testing to validate their practical utility and generalizability across domains.

## Next Checks
1. **Empirical Framework Testing**: Implement the framework for a real AI deployment and conduct controlled user studies comparing explanations designed using this framework versus ad hoc approaches. Measure comprehension, trust calibration, and decision quality across different stakeholder groups.

2. **Stakeholder Classification Validation**: Survey practitioners from multiple AI deployment contexts to assess whether the Developer/Operator/Validator/Subject categorization captures the full range of real-world stakeholder needs. Identify cases where users occupy multiple roles or where new stakeholder types emerge.

3. **Method Selection Matrix Evaluation**: Systematically evaluate whether the four-axis classification (scope, focus, specificity, cost) accurately predicts method performance across diverse deployment scenarios. Test edge cases where methods fall into multiple categories or where constraints interact in complex ways.