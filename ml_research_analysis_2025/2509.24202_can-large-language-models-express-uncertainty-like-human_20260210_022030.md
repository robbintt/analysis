---
ver: rpa2
title: Can Large Language Models Express Uncertainty Like Human?
arxiv_id: '2509.24202'
source_url: https://arxiv.org/abs/2509.24202
tags:
- answer
- confidence
- question
- predicted
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a lightweight, human-aligned approach to
  LLM uncertainty estimation through linguistic confidence (LC), where models express
  uncertainty using hedging language rather than numerical scores. To enable systematic
  evaluation, the authors construct a large-scale human-annotated dataset of hedging
  expressions mapped to confidence scores, and develop a near-zero-cost DistilRoBERTa-based
  mapper for efficient conversion.
---

# Can Large Language Models Express Uncertainty Like Human?
## Quick Facts
- arXiv ID: 2509.24206
- Source URL: https://arxiv.org/abs/2509.24206
- Reference count: 40
- Key outcome: Introduces linguistic confidence (LC) for LLM uncertainty estimation using hedging language, showing LC+ prompting and fine-tuning approaches achieve competitive calibration and discriminability on QA benchmarks

## Executive Summary
This paper introduces a novel approach to uncertainty estimation in large language models through linguistic confidence (LC), where models express uncertainty using hedging language rather than numerical scores. The authors develop a comprehensive framework including a human-annotated dataset mapping hedging expressions to confidence scores, a DistilRoBERTa-based mapper for efficient conversion, and evaluation across modern LLMs on QA benchmarks. Their experiments demonstrate that while vanilla LC performs poorly, carefully designed prompting strategies (LC+) achieve competitive calibration performance comparable to strong baselines like semantic uncertainty.

## Method Summary
The authors propose a lightweight, human-aligned approach to LLM uncertainty estimation through linguistic confidence (LC), where models express uncertainty using hedging language rather than numerical scores. They construct a large-scale human-annotated dataset of hedging expressions mapped to confidence scores and develop a near-zero-cost DistilRoBERTa-based mapper for efficient conversion. The study conducts comprehensive evaluation of LC across modern LLMs on QA benchmarks (SimpleQA, NQ-Open, PopQA), showing that vanilla LC performs poorly but carefully designed prompting (LC+) achieves competitive calibration and discriminability. To further improve reliability, they propose a fine-tuning framework that leverages semantic uncertainty as supervision, training models to express uncertainty more faithfully. Experiments on Qwen3-8B demonstrate consistent improvements in both calibration and discriminability, outperforming baselines like verbalized numerical confidence on NQ-Open.

## Key Results
- LC+ prompting strategy achieves competitive calibration performance comparable to semantic uncertainty baseline
- Fine-tuning framework leveraging semantic uncertainty supervision improves both calibration and discriminability on Qwen3-8B
- DistilRoBERTa mapper enables near-zero-cost conversion between hedging expressions and confidence scores
- Linguistic confidence approach demonstrates scalability and user-friendliness compared to numerical confidence methods

## Why This Works (Mechanism)
Linguistic confidence leverages humans' natural ability to interpret hedging language, making uncertainty communication more intuitive and accessible than numerical scores. The framework works by first establishing a mapping between hedging expressions and confidence scores through human annotation, then using this mapping to convert between linguistic and numerical uncertainty representations. The effectiveness stems from aligning LLM uncertainty expression with human communication patterns, while the DistilRoBERTa mapper provides efficient conversion without requiring extensive computational resources. The LC+ prompting strategy optimizes how uncertainty is elicited, and the fine-tuning framework reinforces faithful uncertainty expression through semantic uncertainty supervision.

## Foundational Learning
**Hedging Language**: Linguistic expressions that convey uncertainty (e.g., "I think," "maybe," "possibly") - why needed: Forms the basis of linguistic confidence representation; quick check: Verify model can generate and recognize common hedging expressions
**Confidence Calibration**: Alignment between predicted confidence and actual accuracy - why needed: Ensures uncertainty estimates are meaningful and reliable; quick check: Compare predicted confidence distribution against actual error rates
**Semantic Uncertainty**: Uncertainty expressed through meaning rather than explicit confidence scores - why needed: Serves as supervision signal for fine-tuning and baseline comparison; quick check: Validate semantic uncertainty correlates with actual model uncertainty
**DistilRoBERTa Mapper**: Compressed transformer model for mapping hedging to confidence scores - why needed: Enables efficient conversion between linguistic and numerical uncertainty representations; quick check: Measure mapping accuracy and inference speed
**Prompt Engineering**: Strategic design of prompts to elicit desired model behaviors - why needed: Critical for effective LC+ prompting strategy; quick check: Test different prompt templates for hedging expression generation
**Calibration Metrics**: Quantitative measures of uncertainty quality (e.g., Expected Calibration Error) - why needed: Provides objective evaluation of uncertainty estimation performance; quick check: Calculate multiple calibration metrics across different confidence thresholds

## Architecture Onboarding
Component map: User Query -> LC+ Prompting -> LLM Response -> Hedging Extraction -> DistilRoBERTa Mapper -> Confidence Score -> Calibration Evaluation

Critical path: The core workflow flows from user queries through LC+ prompting to generate LLM responses containing hedging expressions, which are then processed by the DistilRoBERTa mapper to produce numerical confidence scores for calibration evaluation.

Design tradeoffs: The framework trades some precision of numerical confidence for the accessibility and scalability of linguistic confidence. Using DistilRoBERTa provides efficiency but may sacrifice some mapping accuracy compared to larger models. The reliance on human-annotated data for mapping introduces potential bias and scalability limitations.

Failure signatures: Poor hedging-to-confidence mapping quality will manifest as miscalibrated confidence scores. Ineffective LC+ prompting will result in insufficient hedging expression generation. The approach may fail when models cannot generate appropriate hedging language or when hedging expressions don't correlate well with actual uncertainty.

First experiments: 1) Test DistilRoBERTa mapper accuracy on held-out hedging expressions; 2) Evaluate LC+ prompting effectiveness across different LLM architectures; 3) Compare calibration performance of LC vs. semantic uncertainty on NQ-Open benchmark.

## Open Questions the Paper Calls Out
The paper identifies several key open questions including extending LC to reasoning and multimodal tasks, investigating cross-linguistic robustness of hedging expressions, exploring the impact of model size on LC effectiveness, and determining optimal strategies for generating high-quality semantic uncertainty supervision signals for fine-tuning.

## Limitations
- Human-annotated hedging-to-confidence mapping may not generalize well to diverse domains or languages
- LC+ prompting effectiveness primarily evaluated on specific QA benchmarks, limiting conclusions about task robustness
- Semantic uncertainty supervision quality and scalability in real-world applications remains unclear
- Study focuses on Qwen3-8B, limiting conclusions about model size scaling effects

## Confidence
- High: DistilRoBERTa mapper effectiveness and LC+ prompting strategy performance on tested benchmarks
- Medium: Generalizability of LC to other domains and languages given limited scope of human annotations
- Low: Effectiveness of LC in reasoning and multimodal tasks without empirical validation

## Next Checks
1) Evaluate LC performance on non-QA tasks such as code generation or medical diagnosis to assess domain generalizability
2) Test the approach with multilingual hedging expressions to verify cross-linguistic robustness
3) Conduct ablation studies on the quality and source of semantic uncertainty signals used for fine-tuning to determine their impact on calibration performance