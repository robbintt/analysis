---
ver: rpa2
title: Evaluating Parameter Efficient Methods for RLVR
arxiv_id: '2512.23165'
source_url: https://arxiv.org/abs/2512.23165
tags:
- lora
- methods
- learning
- reasoning
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work conducts the first comprehensive evaluation of Parameter-Efficient
  Fine-Tuning (PEFT) methods under Reinforcement Learning with Verifiable Rewards
  (RLVR), aiming to identify the optimal PEFT architecture for RLVR. The study systematically
  benchmarks over 12 PEFT methods across the DeepSeek-R1-Distill model families on
  mathematical reasoning tasks.
---

# Evaluating Parameter Efficient Methods for RLVR

## Quick Facts
- arXiv ID: 2512.23165
- Source URL: https://arxiv.org/abs/2512.23165
- Authors: Qingyu Yin; Yulun Wu; Zhennan Shen; Sunbowen Li; Zhilin Wang; Yanshu Li; Chak Tou Leong; Jiale Kang; Jinjin Gu
- Reference count: 6
- This work conducts the first comprehensive evaluation of Parameter-Efficient Fine-Tuning (PEFT) methods under Reinforcement Learning with Verifiable Rewards (RLVR), systematically benchmarking over 12 PEFT methods across DeepSeek-R1-Distill model families on mathematical reasoning tasks.

## Executive Summary
This paper presents the first comprehensive evaluation of Parameter-Efficient Fine-Tuning (PEFT) methods specifically for Reinforcement Learning with Verifiable Rewards (RLVR). The study benchmarks over 12 PEFT approaches across DeepSeek-R1-Distill model families on mathematical reasoning tasks, revealing critical insights about which architectural variants succeed or fail in RLVR settings. The findings demonstrate that structural PEFT variants consistently outperform standard LoRA, while SVD-based initialization strategies and extreme parameter reduction methods suffer from fundamental limitations in RLVR's sparse reward optimization landscape.

## Method Summary
The study evaluates PEFT methods using DeepSeek-R1-Distill-Qwen-1.5B and 7B models trained with DAPO algorithm on DAPO-Math-17k-Processed dataset. Training uses 8 rollouts per prompt with binary reward verification, rank-32 adapters with alpha-64 scaling, and targets all linear modules {q,k,v,o,gate,up,down}proj. Evaluation occurs on MATH-500, AIME24/25, AMC23, Minerva, and HMMT datasets. The paper systematically compares structural variants (DoRA, MiSS, AdaLoRA), initialization strategies (PiSSA, MiLoRA), and efficiency-focused methods (LoRA-FA, VeRA, IA3) against standard LoRA and full fine-tuning baselines.

## Key Results
- Structural variants DoRA, MiSS, and AdaLoRA consistently outperform standard LoRA (46.6% vs 42.5% average accuracy)
- SVD-based initialization methods PiSSA and MiLoRA suffer spectral collapse due to misalignment with RLVR's off-principal update dynamics
- Extreme parameter reduction methods like VeRA and Rank-1 adapters severely bottleneck reasoning capacity, indicating a minimum expressivity threshold
- DoRA achieves 46.6% average accuracy, surpassing both standard LoRA (42.5%) and full-parameter fine-tuning (44.9%)

## Why This Works (Mechanism)

### Mechanism 1
Structural PEFT variants (DoRA, MiSS, AdaLoRA) outperform standard LoRA for RLVR by decoupling optimization dynamics. DoRA separates magnitude and direction updates (y = m·Wx/||W||), allowing independent scaling of each component. This mitigates the "rigid low-rank constraint" that limits standard LoRA when facing complex policy shifts required by RL. AdaLoRA adapts rank allocation dynamically; MiSS uses shard-sharing structures. The architectural flexibility of structural variants aligns with RLVR's sparse, exploratory optimization landscape better than LoRA's fixed additive formulation.

### Mechanism 2
SVD-based initialization (PiSSA, MiLoRA) fails in RLVR due to spectral misalignment with RL's "off-principal" update dynamics. RLVR updates localize to low-curvature, non-principal subspaces to preserve pre-trained spectral geometry. PiSSA explicitly restricts updates to principal subspace (U[:r], V[:r]), creating structural conflict → collapse (0.2% accuracy). MiLoRA initializes in off-principal space but fails differently: negligible initialization magnitude (σ_tail→0) means gradient flow dominates, projecting updates back onto principal components despite intent.

### Mechanism 3
RLVR requires a minimum expressivity threshold; extreme parameter reduction creates information bottlenecks that prevent reasoning acquisition. RLVR relies on sparse 1-bit reward signals. Unlike SFT's dense teacher-forcing, this demands sufficient trainable parameter space to reorient reasoning circuits. Vector-only updates (VeRA, IA3) or LayerNorm tuning cannot represent the complex policy shifts needed. Moderate reduction (LoRA-FA, freezing A matrix) works because full low-rank structure remains.

## Foundational Learning

- **Low-Rank Adaptation (LoRA) fundamentals**: All evaluated methods build on or modify LoRA's core decomposition (ΔW = BA). Understanding rank, alpha scaling, and A/B initialization is prerequisite. *Quick check*: Can you explain why LoRA initializes B=0 and A=random, and how rank-32 with alpha-64 scales the update?

- **GRPO/DAPO optimization objectives**: The paper uses DAPO as the default RLVR algorithm. Understanding group-based advantage estimation, clip-higher, and dynamic sampling clarifies why certain PEFT structures align better. *Quick check*: How does GRPO estimate advantages without a critic, and why does DAPO add clip-higher?

- **Singular Value Decomposition and principal components**: The spectral analysis of why PiSSA/MiLoRA fail requires understanding what principal vs. tail singular components represent in weight matrices. *Quick check*: If you initialize adapters on U[:r] vs U[r:], what subspaces of the original weight matrix are you targeting?

## Architecture Onboarding

- **Component map**: Baseline layer (Standard LoRA, Full-Parameter Fine-Tuning) -> Structural adapters (DoRA, MiSS, AdaLoRA) -> Initialization variants (PiSSA, MiLoRA, LoRA+, rsLoRA) -> Efficiency adapters (LoRA-FA, VeRA) -> Alternative mechanisms (IA3, LayerNorm Tuning)

- **Critical path**: Start with DoRA on DeepSeek-R1-Distill-Qwen-1.5B with DAPO. Target all linear modules {q,k,v,o,gate,up,down}proj. Use rank=32, alpha=64, dropout=0.05. Generate G=8 rollouts per prompt with binary reward verification. Scale to 7B only after validating on 1.5B.

- **Design tradeoffs**: DoRA: Best performance (~46.6%) but slightly more compute than LoRA due to magnitude computation. LoRA+: Strong alternative (55.0% on 7B), simpler than DoRA, requires learning rate tuning. LoRA-FA: Good efficiency/performance balance if memory-constrained. Avoid: PiSSA, MiLoRA (spectral collapse), VeRA/IA3/Rank-1 (expressivity bottleneck).

- **Failure signatures**: Spectral collapse: Accuracy drops to near-zero within first 100 steps → check if using SVD-based initialization. Expressivity bottleneck: Training progresses but plateaus well below baseline (40-42% range) → check if using vector-only updates. Training instability with MiLoRA: Reward increases initially then crashes → initialization magnitude issue.

- **First 3 experiments**: 1) Replicate DoRA vs LoRA comparison on 1.5B with DAPO on DAPO-Math-17k-Processed, validate on MATH-500 and AIME24. 2) Ablate LoRA rank (1, 8, 16, 32) to confirm expressivity floor exists in your specific setup. 3) Test LoRA+ with learning rate ratio tuning as a simpler alternative to DoRA before committing to architectural changes.

## Open Questions the Paper Calls Out

- **What are the precise mathematical mechanisms governing spectral evolution in PEFT-RL?**: The authors state the "precise mathematical reasons remain to be fully elucidated" regarding why specific structural biases align with RLVR while SVD-based initializations fail. Current conclusions are empirical observations of spectral collapse, lacking a grounded theory explaining why optimization trajectories favor magnitude-direction decoupling over principal component updates.

- **Do these PEFT-RL findings hold for "R1-Zero-like" cold-start paradigms and prolonged training?**: Section 4 notes plans to test "R1-Zero-like cold-start paradigms" and "prolonged training steps" to verify stability outside current short-horizon settings. Current evaluations rely on SFT-warm-started models; it is unclear if parameter-efficient methods can drive reasoning emergence from base models or sustain optimization over longer horizons.

- **What are the numerical stability risks when merging PEFT adapters for deployment?**: Section 4 identifies "numerical stability of weight merging" and "inconsistencies between training and inference" as critical, overlooked engineering hurdles. The study focuses on training benchmarks, leaving the practical transition—merging adapter weights into base models—as an unsolved challenge that may hide performance variances.

## Limitations

- The evaluation focuses exclusively on mathematical reasoning tasks with binary reward verification, leaving unclear whether findings generalize to open-ended or continuous-reward RLVR scenarios
- Only DeepSeek-R1-Distill-Qwen models (1.5B, 7B) were tested, raising questions about scalability to larger models (>70B) where optimization dynamics may differ
- The DAPO algorithm is used as the default RLVR optimizer, but the performance landscape for PEFT methods may shift with alternative algorithms like PPO or DQN

## Confidence

- **High confidence**: DoRA, MiSS, and AdaLoRA consistently outperform standard LoRA (46.6% vs 42.5%) based on direct ablation experiments across multiple datasets
- **Medium confidence**: SVD-based initialization failure due to spectral collapse is well-supported by both empirical accuracy drops and theoretical analysis of off-principal update dynamics, but this mechanism needs validation across different RLVR algorithms
- **Medium confidence**: Expressivity threshold findings are supported by systematic rank ablation studies, though the minimum threshold may be task-dependent rather than universal

## Next Checks

1. Test DoRA and MiLoRA performance on a non-mathematical RLVR task (e.g., code generation with continuous rewards) to verify generalizability beyond binary verification
2. Scale DoRA to 70B parameter models and compare with standard LoRA to validate that structural advantages persist at scale
3. Replace DAPO with PPO or DQN and retrain DoRA vs PiSSA to determine whether SVD-based failure is algorithm-specific or fundamental to RLVR optimization