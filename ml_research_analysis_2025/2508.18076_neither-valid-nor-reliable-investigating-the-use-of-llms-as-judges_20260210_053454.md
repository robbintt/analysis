---
ver: rpa2
title: Neither Valid nor Reliable? Investigating the Use of LLMs as Judges
arxiv_id: '2508.18076'
source_url: https://arxiv.org/abs/2508.18076
tags:
- evaluation
- lljs
- https
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper critically examines the validity and reliability of
  using large language models (LLMs) as judges for natural language generation (NLG)
  evaluation. It identifies four key assumptions: LLMs as human proxies, capable evaluators,
  scalable evaluators, and cost-effective evaluators.'
---

# Neither Valid nor Reliable? Investigating the Use of LLMs as Judges
## Quick Facts
- arXiv ID: 2508.18076
- Source URL: https://arxiv.org/abs/2508.18076
- Reference count: 40
- This paper critically examines the validity and reliability of using large language models (LLMs) as judges for natural language generation (NLG) evaluation.

## Executive Summary
This paper critically examines the validity and reliability of using large language models (LLMs) as judges for natural language generation (NLG) evaluation. It identifies four key assumptions: LLMs as human proxies, capable evaluators, scalable evaluators, and cost-effective evaluators. The authors argue that current enthusiasm for LLMs as judges is premature, as adoption has outpaced rigorous scrutiny of their limitations. Through analysis of three applications—text summarization, data annotation, and safety alignment—they highlight challenges including inconsistent human judgment collection, instruction adherence issues, data contamination, and ethical concerns. The paper calls for more responsible evaluation practices and better training for ML practitioners to ensure LLMs' growing role supports rather than undermines progress in NLG.

## Method Summary
The paper employs a critical review methodology, analyzing the four key assumptions underlying LLM-as-judge approaches. The authors examine three specific applications of LLM judges (text summarization, data annotation, and safety alignment) to identify practical challenges and limitations. They draw on existing literature and case studies to evaluate the validity and reliability of LLM judges across different evaluation scenarios. The analysis includes discussion of human judgment collection methods, instruction following capabilities, data contamination risks, and ethical considerations. The paper synthesizes findings to propose recommendations for more rigorous and responsible use of LLM judges in NLG evaluation.

## Key Results
- Current enthusiasm for LLM judges outpaces rigorous scrutiny of their limitations
- Four key assumptions about LLM judges are identified as problematic
- Three application areas reveal distinct challenges in LLM-based evaluation

## Why This Works (Mechanism)
The paper's critical analysis mechanism works by systematically examining the foundational assumptions behind LLM-as-judge approaches and testing these assumptions against real-world applications. By focusing on three distinct application areas, the authors can identify both common challenges and domain-specific issues that arise when using LLMs for evaluation tasks. The mechanism of contrasting human judgment collection methods with LLM capabilities reveals gaps in reliability and validity that might not be apparent when examining either in isolation.

## Foundational Learning
- Human judgment collection methods - why needed: Provides ground truth for evaluation quality; quick check: Assess consistency across different annotators
- Instruction following in LLMs - why needed: Critical for consistent evaluation; quick check: Test adherence to complex task instructions
- Data contamination detection - why needed: Ensures unbiased evaluation; quick check: Validate against known contamination-free datasets
- Evaluation metric design - why needed: Establishes reliability standards; quick check: Compare multiple metric approaches

## Architecture Onboarding
Component map: Human judgment collection -> LLM instruction following -> Data contamination detection -> Evaluation metric computation
Critical path: Instruction interpretation -> Judgment generation -> Reliability validation
Design tradeoffs: Human proxy accuracy vs. LLM scalability
Failure signatures: Inconsistent judgments across different LLM configurations, poor correlation with human evaluations
First experiments: 1) Compare LLM judgments across different instruction sets, 2) Test contamination detection methods, 3) Evaluate instruction adherence rates

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding LLM-as-judge evaluation:
- How to determine when human-like judgments are actually desirable versus when we need to account for LLM-specific biases
- What constitutes appropriate ground truth when evaluating LLM judges themselves
- How to balance the scalability benefits of LLM judges against potential reliability issues
- What ethical frameworks should govern the use of LLM judges in sensitive applications

## Limitations
- Inconsistent human judgment collection methods create unreliable ground truth
- Difficulty in determining when LLM judgments should align with human preferences
- Limited understanding of when human-like judgments are actually desirable
- Potential for data contamination affecting evaluation outcomes
- Ethical concerns regarding the deployment of LLM judges in sensitive contexts

## Confidence
High confidence in identifying core methodological challenges in LLM-as-judge evaluation
Medium confidence in generalizability across different NLG tasks and domains
Medium confidence in proposed solutions and recommendations

## Next Checks
1. Design controlled experiments comparing LLM judges against multiple human annotation schemes
2. Develop standardized test sets for measuring LLM instruction adherence
3. Implement cross-validation techniques to identify potential contamination
4. Investigate methods for determining when human-like versus LLM-specific evaluation criteria are appropriate
5. Develop ethical guidelines for deploying LLM judges in different application contexts