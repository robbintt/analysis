---
ver: rpa2
title: A Method for Evaluating the Interpretability of Machine Learning Models in
  Predicting Bond Default Risk Based on LIME and SHAP
arxiv_id: '2502.19615'
source_url: https://arxiv.org/abs/2502.19615
tags:
- interpretability
- shap
- lime
- artificial
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of measuring the interpretability
  of machine learning models used in bond default risk prediction, noting that while
  LIME and SHAP are widely used for post-hoc interpretability analysis, there is no
  standardized method to assess model interpretability itself. The authors propose
  a novel approach that quantifies interpretability by computing the correlation (Cosine
  Similarity) between feature importance rankings produced by LIME and SHAP methods
  across four models: Logistic Regression, Decision Tree, Random Forest, and XGBoost.'
---

# A Method for Evaluating the Interpretability of Machine Learning Models in Predicting Bond Default Risk Based on LIME and SHAP

## Quick Facts
- arXiv ID: 2502.19615
- Source URL: https://arxiv.org/abs/2502.19615
- Authors: Yan Zhang; Lin Chen; Yixiang Tian
- Reference count: 31
- One-line primary result: Proposed MIAI metric quantifies interpretability by computing Cosine Similarity between LIME and SHAP feature importance rankings; achieves highest score (0.3459) for Logistic Regression and lowest (-0.0182) for XGBoost.

## Executive Summary
This paper addresses the challenge of measuring machine learning model interpretability in bond default risk prediction. The authors propose a novel method that quantifies interpretability by computing the correlation between feature importance rankings produced by LIME and SHAP methods across four models: Logistic Regression, Decision Tree, Random Forest, and XGBoost. Their results show that interpretability measure (MIAI) is highest for Logistic Regression (0.3459) and lowest for XGBoost (-0.0182), aligning with conventional understanding that simpler models are more interpretable. The proposed method provides a quantitative framework for comparing model interpretability based on the consistency of post-hoc explanation methods.

## Method Summary
The method quantifies model interpretability using MIAI, defined as the Cosine Similarity between feature importance vectors from LIME and SHAP explanations. The authors train four models (Logistic Regression, Decision Tree, Random Forest, XGBoost) on 6,471 bond issuers with 17 features and 50 defaults. For each model, they generate predictions on the test set and compute average feature contributions using LIME and SHAP. The MIAI metric captures the consistency between these two explanation methods, with higher values indicating greater interpretability. The approach assumes that interpretable models produce consistent explanations regardless of the post-hoc method used.

## Key Results
- MIAI scores rank models as: LR (0.3459) > DT (0.1708) > RF (0.1430) > XGBoost (-0.0182)
- LR model shows consistent results between LIME and SHAP for 9 features, while RF and XGBoost models exhibit consistency for only 3 features
- ROA is both consistent and aligns with financial theory under LIME explanations
- Model performance (AUC) is highest for ensemble methods (XGBoost, RF) but interpretability is lowest
- LIME and SHAP provide complementary information, with LIME more consistent with financial theory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency between post-hoc explanation methods indicates model interpretability.
- Mechanism: The paper computes Cosine Similarity (MIAI) between feature importance vectors from LIME and SHAP. When models have coherent internal decision structures, different explanation methods should converge on similar feature rankings.
- Core assumption: Interpretable models produce stable, consistent explanations regardless of the post-hoc method used.
- Evidence anchors:
  - [abstract] "the paper proposes a novel method for evaluating the interpretability of the models themselves"
  - [section 2.3] "we suggest using the correlation between LIME and SHAP results to assess the interpretability of AI models"
  - [corpus] Weak direct validation; neighbor papers apply LIME/SHAP but do not test cross-method consistency as a metric.
- Break condition: If LIME and SHAP produce inconsistent results due to methodological artifacts (e.g., sampling variance in LIME, kernel choice in SHAP) rather than model opacity, MIAI may conflate explanation instability with uninterpretability.

### Mechanism 2
- Claim: Simpler models yield higher LIME-SHAP consistency than complex ensemble models.
- Mechanism: Linear models (LR) have globally coherent decision boundaries. LIME's local linear surrogates and SHAP's additive feature attributions both approximate the same underlying linear structure. Complex models (XGBoost) have non-linear interactions that different explanation methods may characterize differently.
- Core assumption: Both LIME and SHAP's linear explanation forms should yield similar directional feature impacts when the underlying model is inherently interpretable.
- Evidence anchors:
  - [abstract] "interpretability measure (MIAI) is highest for Logistic Regression (0.3459) and lowest for XGBoost (-0.0182)"
  - [section 3.3, Table 5] Shows MIAI ranking: LR (0.3459) > DT (0.1708) > RF (0.1430) > XGBoost (-0.0182)
  - [corpus] Neighbor papers discuss similar interpretability-accuracy tradeoffs but do not quantify this specific metric.
- Break condition: If class imbalance (50 defaults vs. 6,421 non-defaults) causes SHAP/LIME to behave erratically on minority class predictions, the MIAI ranking may reflect data issues rather than model complexity.

### Mechanism 3
- Claim: Feature-level agreement between LIME and SHAP can identify domain-plausible risk factors.
- Mechanism: When both methods agree on feature direction, this signals stable model reasoning. The paper checks consistency against financial theory (e.g., ROA negatively associated with default).
- Core assumption: Agreement with domain theory validates both the model and the explanation methods.
- Evidence anchors:
  - [section 4, Tables 6-7] "the LR model shows consistent results between LIME and SHAP for 9 features, while... RF and XGBoost models exhibit consistency for only 3 features"
  - [section 4] "ROA is both consistent and aligns with financial theory" under LIME
  - [corpus] "Interpretable Credit Default Prediction with Ensemble Learning and SHAP" similarly uses SHAP for feature validation but does not cross-validate with LIME.
- Break condition: If LIME and SHAP agree but both are wrong (e.g., both pick up spurious correlations from imbalanced data), consistency alone does not guarantee correctness.

## Foundational Learning

- Concept: **LIME (Local Interpretable Model-agnostic Explanations)**
  - Why needed here: Provides local feature contributions by fitting interpretable surrogate models around each prediction point.
  - Quick check question: Given a single prediction, can you explain why LIME might give different feature weights for nearby instances?

- Concept: **SHAP (Shapley Additive exPlanations)**
  - Why needed here: Grounded in cooperative game theory; allocates prediction credit fairly across features with theoretical consistency guarantees.
  - Quick check question: What does a positive SHAP value indicate about a feature's contribution to a specific prediction versus the average prediction?

- Concept: **Cosine Similarity for vector comparison**
  - Why needed here: Measures directional alignment between LIME and SHAP feature vectors; used as the MIAI metric.
  - Quick check question: If two feature vectors have the same direction but different magnitudes, what would their Cosine Similarity be?

## Architecture Onboarding

- Component map:
  Data layer -> Model layer -> Explanation layer -> Interpretability metric layer

- Critical path:
  1. Train model on imbalanced bond data → 2. Generate predictions on test set → 3. Compute feature importance via LIME (averaged across test samples) → 4. Compute feature importance via SHAP (mean absolute or mean signed values) → 5. Calculate Cosine Similarity between the two 17-dimensional vectors

- Design tradeoffs:
  - Accuracy vs. interpretability: XGBoost/RF achieve AUC >0.99 but MIAI <0.15; LR achieves AUC 0.64 but MIAI 0.35 (paper does not resolve this tradeoff, only quantifies it)
  - Imbalanced data handling: Paper deliberately retains class imbalance to preserve interpretability analysis integrity; this may reduce predictive validity
  - Aggregation method: Averaging local LIME explanations globally may obscure instance-level variability

- Failure signatures:
  - Negative MIAI (as seen with XGBoost at -0.0182): LIME and SHAP point in opposite directions on average, indicating explanation instability
  - Extreme SHAP values for certain features (e.g., Igrb = 23.48 for LR in Table 4): May indicate numerical instability or outlier sensitivity
  - Low feature-theory alignment: If neither LIME nor SHAP aligns with financial theory, model may be learning spurious patterns

- First 3 experiments:
  1. **Sensitivity test**: Re-run MIAI calculation using stratified sampling or balanced subsets to assess whether class imbalance drives the low interpretability scores for complex models.
  2. **Instance-level analysis**: Instead of averaging LIME/SHAP, compute per-instance Cosine Similarity distributions to identify whether inconsistency is global or localized to specific prediction regions.
  3. **Method extension**: Add a third explanation method (e.g., Integrated Gradients for neural extensions, or permutation importance) and compute pairwise MIAI scores to test whether the ranking is robust to method choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MIAI metric perform when applied to varied datasets and different data environments outside of bond default prediction?
- Basis in paper: [explicit] The authors acknowledge in the conclusion that "foundational LIME and SHAP methods are not entirely robust across all data environments, different conclusions might be reached when applying our method to varied datasets."
- Why unresolved: The study is restricted to a single case study involving corporate bond data; the generalizability of the metric to other domains or data distributions is not verified.
- What evidence would resolve it: Application of the MIAI metric to diverse datasets (e.g., different financial markets or non-financial domains) to observe if the ranking of model interpretability remains consistent.

### Open Question 2
- Question: How can the evaluation framework be refined to integrate additional post-hoc interpretability methods beyond LIME and SHAP?
- Basis in paper: [explicit] The conclusion states the authors "foresee future research that could integrate additional post-hoc interpretability methods to further refine and enhance the evaluation of model interpretability."
- Why unresolved: The current method relies solely on the correlation between two specific methods (LIME and SHAP), potentially missing nuances captured by other explanation techniques.
- What evidence would resolve it: A modified formula or aggregation method that incorporates feature importance vectors from other explainers (e.g., DeepLIFT, Integrated Gradients) and a comparison of the resulting interpretability scores.

### Open Question 3
- Question: Does high correlation between LIME and SHAP explanations necessarily equate to valid interpretability, or does it indicate shared instability?
- Basis in paper: [inferred] The method relies on the hypothesis that "when a model demonstrates strong interpretability, the evaluation results... should be consistent," but the paper does not verify if this consistency aligns with human understanding or ground truth.
- Why unresolved: It is possible for two explanation methods to be highly correlated (high MIAI) yet both be incorrect or misleading regarding the model's actual logic, a possibility not addressed by the quantitative correlation alone.
- What evidence would resolve it: A user study comparing the MIAI scores against human expert evaluations of the models' decision-making transparency.

### Open Question 4
- Question: How does the extreme class imbalance in the dataset affect the stability of the proposed interpretability measure?
- Basis in paper: [inferred] The dataset contains 6,471 issuers but only 50 defaults, and the authors explicitly noted they "retained the original, imbalanced dataset," which may skew LIME/SHAP contributions for the minority class.
- Why unresolved: The stability of LIME and SHAP values is known to fluctuate with class imbalance; it is unclear if the resulting MIAI scores reflect model interpretability or artifacts of the data distribution.
- What evidence would resolve it: A sensitivity analysis comparing MIAI scores calculated on the imbalanced data versus scores calculated on data treated with resampling techniques (e.g., SMOTE).

## Limitations

- The proposed interpretability evaluation method relies on the assumption that consistency between LIME and SHAP explanations reflects true model interpretability, but this relationship has not been empirically validated.
- The study's exclusive focus on bond default prediction limits generalizability to other domains or data distributions.
- The use of class-imbalanced data without explicit handling may confound interpretability measures and artifact the results.

## Confidence

- High confidence: The experimental methodology for computing MIAI scores is clearly specified and reproducible.
- Medium confidence: The correlation between simpler models and higher MIAI scores aligns with conventional wisdom but requires broader validation.
- Low confidence: The theoretical justification for using LIME-SHAP consistency as a proxy for model interpretability remains underdeveloped.

## Next Checks

1. Conduct ablation studies varying LIME kernel widths and SHAP background samples to determine if MIAI scores are robust to explanation method hyperparameters.
2. Apply the MIAI metric to models trained on balanced subsets to isolate whether class imbalance drives the observed interpretability differences.
3. Test the method on additional datasets (e.g., credit scoring, medical diagnosis) to evaluate generalizability beyond bond default prediction.