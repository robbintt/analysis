---
ver: rpa2
title: Gradient flow for deep equilibrium single-index models
arxiv_id: '2511.16976'
source_url: https://arxiv.org/abs/2511.16976
tags:
- gradient
- descent
- flow
- theorem
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the gradient descent dynamics for training deep
  equilibrium models (DEQs) in the simple setting of linear and single-index models.
  The authors prove a conservation law for linear DEQs showing that parameters remain
  on spheres during training, which keeps the Jacobian well-conditioned.
---

# Gradient flow for deep equilibrium single-index models

## Quick Facts
- arXiv ID: 2511.16976
- Source URL: https://arxiv.org/abs/2511.16976
- Reference count: 8
- Primary result: Conservation law for linear DEQs showing parameters remain on spheres during training, with exponential convergence of gradient flow and linear convergence of gradient descent under appropriate conditions

## Executive Summary
This paper studies the gradient descent dynamics for training deep equilibrium models (DEQs) in the simple setting of linear and single-index models. The authors prove a conservation law for linear DEQs showing that parameters remain on spheres during training, which keeps the Jacobian well-conditioned. They show that gradient flow converges exponentially fast to a global minimizer for linear DEQs and single-index models under appropriate initialization. For gradient descent, they prove linear convergence to the global minimizer when using sufficiently small step sizes. The theoretical results are validated through experiments on synthetic data, demonstrating that gradient descent successfully learns the target functions and parameters converge as predicted by the theory.

## Method Summary
The paper analyzes gradient flow (continuous-time) and gradient descent (discrete-time) for training DEQs on linear models f(x) = ξ^T x and single-index models f(x) = σ(ξ^T x). For linear DEQs, the conservation law ∥θ₁(t)∥²₂ + (θ₂(t)−1)² = constant is proven, ensuring parameters stay on spheres and preventing Jacobian singularities. For single-index models, convergence is shown when initialization is sufficiently close to the true parameters. The theoretical framework relies on the implicit function theorem for computing gradients and Lyapunov stability analysis for proving convergence. Experiments use synthetic data with 1000 samples, sigmoid activation for single-index models, and validate theoretical predictions about parameter trajectories and convergence rates.

## Key Results
- Linear DEQ parameters remain trapped on spheres during training, preventing collapse to trivial fixed points
- Gradient flow converges exponentially fast to a global minimizer for both linear DEQs and single-index models under appropriate initialization
- Gradient descent with sufficiently small step sizes achieves linear convergence to the global minimizer
- The conservation law ensures the Jacobian 1 − ∂g_θ/∂y remains bounded away from zero, maintaining training stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear DEQ parameters remain trapped on spheres during training, preventing collapse to the trivial fixed-point y_θ(x) = y_θ(x).
- Mechanism: The gradient flow ODE for linear DEQs satisfies an exact conservation law: ‖θ₁(t)‖²₂ + (θ₂(t)−1)² = constant for all t ≥ 0. This arises from the specific structure of ∇_θ R(θ,ξ), where the θ₁ and θ₂ dynamics satisfy θ₁(t)ᵀθ′₁(t) = (1−θ₂(t))θ′₂(t), which integrates to the sphere constraint.
- Core assumption: X ∈ L²(P) with finite second moments; initialization not on the singular hyperplane θ₂ = 1.
- Evidence anchors:
  - [abstract]: "We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training"
  - [section]: Theorem 3.1 proves ∥θ₁(t)∥²₂ + (θ₂(t)−1)² = ∥θ₁(0)∥²₂ + (θ₂(0)−1)² for all t ≥ 0
  - [corpus]: Weak direct evidence—corpus papers focus on applications rather than gradient flow dynamics analysis
- Break condition: If θ₂(0) = 1 exactly (initialization on singularity), or if the data distribution has degenerate covariance allowing θ₂ → 1 asymptotically.

### Mechanism 2
- Claim: The Jacobian 1 − ∂g_θ/∂y remains bounded away from zero during training, ensuring gradient stability.
- Mechanism: By the conservation law (Mechanism 1), if θ₂(t) were to approach 1, then ‖θ₁(t)‖₂ would remain bounded away from zero. This would cause the population risk to diverge to infinity. Since R(θ(t), ξ) is non-increasing along gradient flow (dR/dt ≤ 0), this yields a contradiction—hence |1−θ₂(t)| ≥ β > 0 for some constant β.
- Core assumption: E[XX⊤] ≻ 0 (full-rank data covariance); θ₂(0) ≠ 1.
- Evidence anchors:
  - [abstract]: "use this property to show that gradient flow remains well-conditioned for all time"
  - [section]: Lemma 3.2 proves gradient flow is well-defined for all t ≥ 0 with θ₂(t) ≠ 1, using contradiction via risk divergence
  - [corpus]: Related work (Bai et al. 2021, Agarwala & Schoenholz 2022) discusses Jacobian regularization and initialization strategies for stability—consistent but not proof-based
- Break condition: Rank-deficient covariance E[XX⊤] may allow gradient flow to remain well-defined but breaks the current proof technique (acknowledged by authors as nontrivial).

### Mechanism 3
- Claim: Gradient descent with small step size converges (almost) linearly to a global minimizer for both linear DEQs and single-index DEQs under suitable initialization.
- Mechanism: For linear DEQs, the risk satisfies a Polyak-Łojasiewicz (PŁ) inequality within an invariant compact set Θ: ‖∇_θ R(θ,ξ)‖²₂ ≥ (2/κα²)R(θ,ξ) where κ is the condition number. For single-index models, the squared parameter distance to target contracts: r(t+1) ≤ (1−ηλ₁/2)r(t) when η ≤ λ₁/(2λ₂). Both rely on the Jacobian staying well-conditioned (Mechanism 2).
- Core assumption: For linear: step size η ≤ η₀ sufficiently small. For single-index: initialization ∥θ(0)−(ξ,0)∥₂ ≤ min{(1+δ₁)∥ξ∥₂, δ₂} (close to target), activation σ is Lipschitz with σ′ bounded below near relevant inputs, data exhibits nonlinearity w.r.t. σ.
- Evidence anchors:
  - [abstract]: "We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization"
  - [section]: Theorem 3.6 (linear convergence for linear DEQs); Theorem 3.9 (linear convergence for single-index DEQs)
  - [corpus]: Kawaguchi (2021) shows linear convergence for overparameterized linear implicit models with softmax modification—different setting, consistent finding
- Break condition: Step size too large (violates descent lemma or η > λ₁/(2λ₂)); for single-index, initialization too far from (ξ,0) or activation violates nonlinearity assumption (e.g., linear σ(z)=z is excluded, though covered separately by Theorem 3.4).

## Foundational Learning

- **Fixed-Point Theory & Implicit Function Theorem**:
  - Why needed here: DEQs define outputs as fixed points y_θ(x) = g_θ(x, y_θ(x)); computing gradients requires the implicit function theorem to express ∇_θ y_θ(x) = (1−∂g_θ/∂y)⁻¹∇_θ g_θ.
  - Quick check question: Given y = σ(θ₁ᵀx + θ₂y), can you derive ∂y/∂θ₁ in terms of σ′?

- **Gradient Flow as Continuous-Time ODE**:
  - Why needed here: The paper analyzes θ′(t) = −∇_θ R(θ(t)) to prove properties before discretizing to gradient descent; Grönwall's inequality and LaSalle's invariance principle are key proof tools.
  - Quick check question: If r′(t) ≤ −λr(t) for λ > 0, what does Grönwall's inequality tell you about r(t)?

- **Lyapunov Stability & LaSalle's Invariance Principle**:
  - Why needed here: Theorem 3.3 uses V(θ) = R(θ,ξ) as a Lyapunov function and applies LaSalle to show convergence to the invariant set S = {(θ₁,θ₂) : θ₁ = ξ(1−θ₂)}.
  - Quick check question: If V(θ(t)) is non-increasing along a trajectory, what can you conclude about limit points?

## Architecture Onboarding

- **Component map**: Forward pass: y_θ(x) = g_θ(x, y_θ(x)) via root-finding -> Backward pass: ∇_θ L via implicit differentiation -> Parameter update: θ ← θ − η∇_θ R

- **Critical path**:
  1. Initialize θ(0) with θ₂(0) bounded away from 1/L (where L = ∥σ∥_Lip) to ensure contractivity.
  2. Forward: iterate y^(k+1) = σ(θ₁ᵀx + θ₂y^(k)) until convergence.
  3. Backward: compute (1−θ₂σ′(...))⁻¹ factor; if near-singular, training destabilizes.
  4. Update: θ ← θ − η∇_θ R with η sufficiently small.

- **Design tradeoffs**:
  - **Contractivity strictness**: Smaller |θ₂| improves fixed-point convergence but may constrain expressivity.
  - **Step size**: Smaller η guarantees convergence (per theorems) but slows training; no adaptive scheme analyzed.
  - **Activation choice**: Sigmoid/tanh/softplus satisfy all assumptions; ReLU requires θ₁(0)ᵀξ > 0 modification; linear activation handled separately.

- **Failure signatures**:
  - **Jacobian blowup**: Loss becomes unstable or NaN if 1−θ₂σ′(·) → 0; monitor |1−θ₂|.
  - **Non-convergence**: For single-index, if initialization ∥θ(0)−(ξ,0)∥₂ > 1/L, no convergence guarantee.
  - **Fixed-point divergence**: If |θ₂| ≥ 1/L, g_θ may not be contractive; forward pass fails.

- **First 3 experiments**:
  1. **Linear DEQ sanity check**: Generate data from f(x) = ξᵀx with ξ known; train linear DEQ y_θ(x) = θ₁ᵀx/(1−θ₂); verify ‖θ₁(t)‖² + (θ₂(t)−1)² is conserved and θ(t) → (αξ, 1−α) for some α > 0.
  2. **Conservation law violation test**: Intentionally initialize with θ₂(0) close to 1 (e.g., 0.99) and observe if training remains stable or if gradients explode—validates Lemma 3.2 boundaries.
  3. **Single-index convergence with different activations**: Compare sigmoid, tanh, and softplus on f(x) = σ(ξᵀx); measure convergence rate vs. distance from target initialization to validate the ρ·γ²/(1+Lδ₂) rate factor from Theorem 3.8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the conservation laws and linear convergence guarantees established for population risk gradient flow extend to the empirical risk minimization setting (finite samples)?
- Basis in paper: [explicit] The authors state they "focus on the gradient flow and gradient descent dynamics for minimizing [population] risk, leaving the analysis of empirical risk minimization to future work."
- Why unresolved: The proofs rely on expectations over the data distribution ($E[XX^\top]$), and it is unclear if the conservation law holds or if convergence rates degrade when gradients are computed over finite datasets.
- What evidence would resolve it: A proof of convergence for gradient descent on the empirical loss, or generalization bounds connecting the empirical dynamics to the population dynamics.

### Open Question 2
- Question: Does gradient flow for linear DEQs converge to a global minimizer when the data covariance is rank-deficient ($E[XX^\top] \succ 0$ fails)?
- Basis in paper: [explicit] The authors note that "simulations show that if $E[XX^\top]$ is rank-deficient... it is still possible for the gradient flow to be well-defined... However, relaxing this assumption is nontrivial... so we leave this to future work."
- Why unresolved: The current proof of well-definedness relies on the divergence of the population risk as $\theta_2 \to 1$, which may not occur if the data lies on a lower-dimensional manifold.
- What evidence would resolve it: A convergence proof that handles degenerate covariance matrices, or a counterexample showing the flow becomes ill-defined.

### Open Question 3
- Question: Can the convergence guarantees for single-index models be extended to deep equilibrium models with multiple neurons or layers?
- Basis in paper: [explicit] In the conclusion, the authors state: "Future work could extend these results to multi-neuron DEQs and explore the implications for practical training."
- Why unresolved: The theoretical tools used (e.g., scalar parameter $\theta_2$, specific conservation laws) are tailored to the single-neuron structure and do not trivially apply to high-dimensional hidden states or matrix-valued parameters.
- What evidence would resolve it: Derivation of analogous conservation laws or convergence rates for DEQs with a hidden dimension $d > 1$.

### Open Question 4
- Question: Is global convergence guaranteed for deep equilibrium single-index models, or are there restrictive basins of attraction requiring initialization near the true parameter?
- Basis in paper: [inferred] The authors provide a "partial answer" to the convergence question (Q2), proving convergence "as long as the initialization is sufficiently close to the true parameter."
- Why unresolved: The analysis relies on local smoothness and Lipschitz constants near the target; it does not characterize the loss landscape globally to determine if remote initializations always succeed.
- What evidence would resolve it: A global convergence theorem valid for any initialization (excluding the singular hyperplane), or an empirical/theoretical characterization of the basin of attraction.

## Limitations

- Theoretical analysis is limited to linear and single-index DEQs with specific activation functions (sigmoid, tanh, softplus)
- Conservation law and well-conditioning analysis require full-rank data covariance and initialization away from singular hyperplane θ₂ = 1
- Single-index convergence requires initialization sufficiently close to true parameters, with conservative step-size conditions
- Experiments use only synthetic data and simple architectures, limiting generalizability to real-world applications

## Confidence

**High Confidence**: The conservation law for linear DEQs (Mechanism 1) is mathematically rigorous and the proof technique is sound. The gradient flow well-posedness result (Mechanism 2) follows logically from the conservation property.

**Medium Confidence**: The linear convergence proofs for both linear and single-index DEQs (Mechanism 3) are mathematically correct but rely on strong initialization assumptions and may not capture practical behavior when these conditions are violated.

**Low Confidence**: The experiments, while supporting the theoretical predictions, use only synthetic data and simple architectures. Generalization to more complex DEQ architectures and real-world datasets remains untested.

## Next Checks

1. **Jacobian Sensitivity Test**: Systematically vary the initialization distance to θ₂ = 1 for linear DEQs and measure how the training stability and convergence rate change. This would validate the sensitivity of the well-conditioning property to initialization proximity.

2. **Activation Function Robustness**: Test single-index DEQs with different activation functions (ReLU, ELU, Swish) beyond those theoretically covered to assess whether the convergence guarantees extend beyond the theoretical assumptions.

3. **Rank-Deficient Data**: Conduct experiments with intentionally rank-deficient or highly correlated input data to test the paper's claim that the current proof technique breaks down while gradient flow may still remain well-defined.