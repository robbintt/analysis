---
ver: rpa2
title: 'Computational emotion analysis with multimodal LLMs: Current evidence on an
  emerging methodological opportunity'
arxiv_id: '2512.10882'
source_url: https://arxiv.org/abs/2512.10882
tags:
- arousal
- mllms
- ratings
- shot
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the ability of multimodal large language models
  (mLLMs) to measure emotional arousal in political speech videos. The study compares
  mLLM performance on controlled laboratory recordings versus real-world parliamentary
  debates.
---

# Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity

## Quick Facts
- arXiv ID: 2512.10882
- Source URL: https://arxiv.org/abs/2512.10882
- Reference count: 0
- Leading mLLMs achieve near-human reliability on controlled emotion tasks but fail on real-world political speech

## Executive Summary
This paper evaluates multimodal large language models' (mLLMs) ability to measure emotional arousal in political speech videos. The study compares model performance on controlled laboratory recordings versus real-world parliamentary debates. Leading models like Gemini and Qwen Omni achieve near-human reliability in laboratory settings with minimal demographic bias. However, in real-world parliamentary videos, all examined models show at best moderate correlation with human ratings and exhibit systematic bias by speaker gender and age. The performance gap persists despite denoising strategies and larger model sizes. Even for sentiment analysis, mLLMs perform worse on video than text transcripts. These findings establish baseline performance for video-based emotion analysis and highlight current limitations of mLLMs in real-world political contexts.

## Method Summary
The study evaluates four mLLMs (Gemini 3 Flash, Qwen 3 Omni, Qwen 3 Omni Thinking, TowerVideo) using in-context learning to generate arousal scores for video clips. Models generate token probabilities for discrete scale points (1-9), which are combined via probability-weighted averaging to produce continuous ratings. The evaluation uses three datasets: RAVDESS (controlled lab recordings), MSP-Podcast (semi-controlled), and Cochrane (real-world parliamentary debates). Performance is measured via Pearson/Spearman correlations with human ratings, RMSE, and demographic subgroup analysis. Speaker-blocked data splits prevent data leakage in few-shot learning. The study also tests denoising interventions (audio/visual) to isolate signal quality effects.

## Key Results
- mLLMs achieve near-human reliability (r=0.64-0.74) on controlled lab recordings but only moderate correlation (r=0.35-0.43) on real-world parliamentary debates
- All models exhibit systematic bias by speaker gender and age in real-world settings, despite minimal bias in controlled conditions
- Probability-weighted scoring reduces response bunching compared to discrete token selection
- Denoising interventions (audio cleanup, visual masking) fail to improve real-world performance, suggesting architectural limitations beyond signal quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token probability-weighted scoring reduces response bunching and enables continuous scalar measurement from discrete token outputs.
- **Mechanism:** The mLLM generates token probabilities for each scale point option (e.g., tokens "1" through "9"), then a probability-weighted average is computed: s = Σ Pr(y=s|c) · n(s) / p_s. This extracts the model's full distribution rather than forcing a single discrete choice.
- **Core assumption:** The model's token probability distribution over scale points reflects calibrated uncertainty rather than arbitrary confidence.
- **Evidence anchors:**
  - [abstract]: The paper evaluates "in-context learning" with models generating arousal scores correlated against human ratings.
  - [section 2.4]: "I use the model's token probabilities of scale point option tokens to compute a probability-weighted rating score... This approach avoids the bunching Licht et al. (2025) observe in LLM's direct scalar responses."
  - [corpus]: AffectGPT and MME-Emotion benchmarks use similar discrete emotion classification but don't systematically evaluate probability-weighted scalar approaches for continuous dimensions like arousal.
- **Break condition:** If the model's probability distribution collapses (near-100% confidence on single tokens), the weighting provides no benefit. The paper documents this: "enabling thinking collapses the token probability distributions across all three reasoning mLLMs."

### Mechanism 2
- **Claim:** In-context learning with few-shot exemplars improves emotion scoring calibration, but gains diminish in real-world settings.
- **Mechanism:** Providing the model with task instructions plus 3-5 exemplar video-rating pairs anchors the model to the target construct and scale before inference on test videos.
- **Core assumption:** Few-shot exemplars transfer across speakers and contexts without memorization or speaker-specific bias.
- **Evidence anchors:**
  - [abstract]: Models were tested using "in-context learning" with both zero-shot and few-shot configurations.
  - [section 3.3]: "I integrate few-shot exemplars as turns of user input (video plus short task summary) and assistant response... As the assistant's response in these exemplars, I use the cross-coder average arousal/intensity score."
  - [section 4.1]: In RAVDESS, 5-shot inference with Qwen 3 Omni Thinking achieved r=0.715 vs. r=0.668 for 0-shot—improvement of ~7%.
  - [corpus]: EmotionHallucer paper notes MLLMs suffer from hallucinations in emotion tasks, suggesting few-shot may not fully ground responses.
- **Break condition:** In real-world data (Cochrane dataset), few-shot improvements were inconsistent: Gemini 3 Flash 5-shot achieved r=0.362 (worse than 3-shot at r=0.431). Exemplars don't transfer when test conditions diverge from exemplar conditions.

### Mechanism 3
- **Claim:** The lab-to-field performance gap persists because real-world multimodal complexity overwhelms current mLLM attention and integration capabilities.
- **Mechanism:** In controlled settings, visual and acoustic cues are aligned, frontal, and unambiguous. In real-world parliamentary video, speakers don't face cameras, background movement distracts, ambient noise confounds vocal signals, and multiple modalities may conflict.
- **Core assumption:** The performance gap reflects model limitations in handling complexity, not simply insufficient training on political content.
- **Evidence anchors:**
  - [abstract]: "In controlled conditions, models achieve near-human reliability with minimal bias, but in real-world debates, their arousal scores correlate at best moderately with human ratings and exhibit systematic demographic bias."
  - [section 4.2]: "All analyzed models perform comparatively poorly in the Parliamentary Speech Emotions data... Correlations with the human rating-based reference scores are overall low in absolute terms."
  - [section D.2, Supplemental]: Background noise mitigation (audio denoising + visual masking) did NOT systematically improve performance: "Most setups show no systematic improvements, suggesting additional factors beyond acoustic/visual noise explain poor real-world performance."
  - [corpus]: CA-MER benchmark specifically examines "emotion conflicts, where emotional cues from different modalities are inconsistent"—directly relevant but not yet tested on political video.
- **Break condition:** If architectural innovations (cross-modal attention, time-aligned RoPE) that work in controlled settings scaled to complex real-world video, the gap would close. The paper finds it does not.

## Foundational Learning

- **Concept: Attenuation correction in evaluation**
  - **Why needed here:** Human ratings are themselves noisy. Comparing mLLM scores to imperfect human ground truth underestimates true model performance. The paper applies Schmidt & Hunter (1999) correction: r_true = r_obs / √ρ_humans.
  - **Quick check question:** If human ICC=0.83 and observed model-human correlation is r=0.45, what's the attenuation-corrected correlation? (Answer: ~0.49)

- **Concept: Intra-class correlation (ICC) vs. Pearson correlation**
  - **Why needed here:** ICC measures reliability among multiple raters (the benchmark), while Pearson measures linear association between model and human averages. These serve different evaluation purposes and can't be directly compared without attenuation adjustment.
  - **Quick check question:** Why might a model achieve high Pearson correlation with human averages but still be considered unreliable? (Answer: Systematic bias—model could be offset but perfectly correlated)

- **Concept: Speaker-blocked data splits**
  - **Why needed here:** The paper blocks by speaker so all recordings from one speaker are in the same split (train/val/test). This prevents data leakage in few-shot learning—exemplar ratings shouldn't come from the same speaker being evaluated.
  - **Quick check question:** What would happen if speaker A's videos appeared in both train (as few-shot exemplar) and test? (Answer: Model may memorize speaker-specific arousal patterns rather than learning generalizable emotion scoring)

## Architecture Onboarding

- **Component map:**
  Video Input → Vision Encoder → [Cross-modal Attention] → Token Sequence
                                    ↘
  Audio Track → Audio Encoder → [Time-aligned Multimodal RoPE] → Unified Representation
                                                                    ↓
  Task Instruction (text) → LLM Backbone → Token Probabilities over Scale Points
                                                    ↓
                                      Probability-Weighted Scoring → Final Rating

- **Critical path:**
  1. Video preprocessing (ensure audio track present; standardize format/length)
  2. Prompt construction (task instruction + few-shot exemplar turns + target video)
  3. Inference with temperature=0 (greedy decoding for reproducibility)
  4. Token probability extraction for scale point options
  5. Probability-weighted score computation (Equation 1)
  6. Evaluation against human ratings (Pearson r, Spearman ρ, RMSE, demographic subgroup analysis)

- **Design tradeoffs:**
  | Decision | Option A | Option B | Paper's Choice |
  |----------|----------|----------|----------------|
  | Scale format | Integer 1-9 | Decimal 0-10 | Integer 1-9 (avoids tokenization issues with "10") |
  | Response mode | Direct generation | Prob-weighted | Prob-weighted (reduces bunching) |
  | Reasoning | Disabled | Enabled | Disabled for most models (reasoning collapses probability distributions) |
  | Quantization | Full precision | 4-bit | 4-bit for open-weights (memory constraints) |

- **Failure signatures:**
  - **Collapsed probability distribution:** Confidence near 1.0 on single token → prob-weighted scoring becomes equivalent to argmax → no benefit. Seen with "thinking" enabled.
  - **Scale-point drift:** Model assigns probability mass to non-scale tokens (e.g., words instead of numbers) → violates instruction format. Seen with Gemini 3 Flash Preview in minimal thinking mode.
  - **Demographic bias:** Correlation with human ratings varies by speaker gender/age → model learning spurious associations. Documented in real-world data for all models.
  - **Mean-centering without discrimination:** Low RMSE but low correlation → model outputs near-dataset-mean regardless of input. Documented: "mLLMs' video-based arousal scores are overall close to the mean of the reference score distribution while still yielding at best weakly correlated ratings."

- **First 3 experiments:**
  1. **Baseline calibration on controlled data:** Run Qwen 3 Omni Instruct (no thinking) on RAVDESS test split with 3-shot exemplars. Target: r > 0.65 with RMSE < 0.8 on 1-5 scale. Verify probability distributions are not collapsed (confidence < 0.95 on average).
  2. **Real-world transfer test:** Apply same model/configuration to Cochrane parliamentary data. Expect r drop to ~0.35-0.45. Compute performance separately by speaker gender to check for demographic bias (difference > 0.05 in correlation indicates bias).
  3. **Denoising ablation:** On subset of 50 parliamentary videos, compare: (a) raw video, (b) audio-denoised only, (c) visual-masked only, (d) both. Based on paper findings, expect no systematic improvement from any intervention—if confirmed, problem is not signal-to-noise but model architecture limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific acoustic and visual factors hamper mLLMs' emotion measurement in real-world political speech recordings?
- Basis in paper: [explicit] The authors ask "what factors hamper mLLMs’ video-based emotion measurement in real-world political speech recordings" after finding that signal-to-noise improvements alone do not suffice.
- Why unresolved: The study tested denoising and background masking strategies which failed to improve reliability, ruling out simple background noise as the sole cause of the lab-to-field performance gap.
- What evidence would resolve it: Systematic ablation studies varying specific visual properties (e.g., camera angles, speaker movement) and acoustic properties (e.g., overlapping speech) in naturalistic settings.

### Open Question 2
- Question: Do the observed limitations in scalar arousal rating tasks extend to discrete emotion classification or trajectory analysis?
- Basis in paper: [explicit] The authors ask, "do these findings extend beyond scalar rating tasks?" and suggest checking if "mLLMs can classify but not scale emotions."
- Why unresolved: This study focused exclusively on scalar intensity ratings (arousal); it remains unknown if the "lab-to-field gap" is specific to regression tasks or affects classification equally.
- What evidence would resolve it: Replicating the evaluation framework using datasets with human-coded discrete emotion categories (e.g., anger, fear) and temporal emotion trajectories.

### Open Question 3
- Question: How do mLLMs perform in analyzing short-form political content characterized by rapid editing and stylized affect?
- Basis in paper: [explicit] The authors note that short-form content (e.g., TikTok, Reels) "poses distinct challenges" and state that "understanding mLLM performance in these formats will be crucial."
- Why unresolved: The study analyzed parliamentary debates; it did not test performance on the highly edited, stylized formats that increasingly dominate political communication on social media.
- What evidence would resolve it: Applying the established evaluation framework to datasets of short-form political videos to create a comparative baseline against the parliamentary debate results.

## Limitations

- The study focuses exclusively on political speech videos in English, limiting generalizability to other domains or languages
- Few-shot exemplars were drawn from the same controlled datasets, raising questions about exemplar diversity for real-world transfer
- The absence of systematic improvement from denoising interventions suggests architectural limitations, but specific model components responsible remain unidentified
- Performance gaps may reflect both model architecture limitations and insufficient training on political content, though the paper cannot definitively separate these factors

## Confidence

- **High Confidence**: Performance differences between controlled vs. real-world settings are well-documented with statistical rigor (Section 4.1-4.2). The systematic bias by speaker demographics is consistently observed across all models (Section 4.2.2).
- **Medium Confidence**: The interpretation that performance gaps reflect model architecture limitations rather than signal quality is plausible but requires additional controlled experiments (Section 4.2.2). The claim that probability-weighted scoring reduces bunching is supported by direct comparison but depends on the assumption that probability distributions reflect calibrated uncertainty.
- **Low Confidence**: The extrapolation to broader domains beyond political speech remains speculative, as the study does not test on non-political video content or cross-cultural datasets.

## Next Checks

1. **Architecture ablation study**: Test whether performance gaps persist when using simplified video inputs (single speaker, clean background, front-facing camera) from the real-world parliamentary dataset. If performance improves to controlled-setting levels, this would confirm that current mLLMs cannot handle multimodal complexity in natural settings.

2. **Cross-domain exemplar transfer**: Evaluate whether few-shot exemplars from non-political domains (e.g., TV interviews, YouTube vlogs) improve performance on parliamentary videos compared to exemplars from similar political content. This would test whether the transfer failure reflects domain mismatch rather than fundamental architecture limitations.

3. **Continuous emotion dimension validation**: Replicate the arousal scoring protocol using multiple continuous emotion dimensions (valence, dominance) on the same datasets to determine whether observed patterns generalize across emotional constructs or are specific to arousal.