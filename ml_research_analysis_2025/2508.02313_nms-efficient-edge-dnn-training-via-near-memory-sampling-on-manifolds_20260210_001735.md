---
ver: rpa2
title: 'NMS: Efficient Edge DNN Training via Near-Memory Sampling on Manifolds'
arxiv_id: '2508.02313'
source_url: https://arxiv.org/abs/2508.02313
tags:
- sampling
- training
- energy
- algorithm
- de-sne
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training deep neural networks
  on edge devices, where large datasets lead to prohibitive energy consumption. It
  proposes a brain-inspired approach called NMS (Near-Memory Sampling) to compress
  datasets while maintaining accuracy.
---

# NMS: Efficient Edge DNN Training via Near-Memory Sampling on Manifolds

## Quick Facts
- **arXiv ID:** 2508.02313
- **Source URL:** https://arxiv.org/abs/2508.02313
- **Reference count:** 40
- **Primary result:** DNN-free dataset compression achieving 11.9% higher average accuracy on ImageNet-1K compared to DQ.

## Executive Summary
This paper addresses the challenge of training deep neural networks on edge devices by proposing a brain-inspired approach called NMS (Near-Memory Sampling). The method compresses datasets while maintaining accuracy by leveraging the human brain's low-dimensional manifold stationary property. NMS uses a DNN-free sample selection algorithm (DE-SNE) that replaces unstable binary search in t-SNE with differential evolution for more efficient sampling. Implemented in a near-memory computing architecture, it significantly reduces DDR energy consumption and enables efficient edge training, outperforming state-of-the-art methods.

## Method Summary
NMS proposes a two-phase approach: first, a modified t-SNE algorithm (DE-SNE) uses differential evolution to find optimal perplexity parameters for stable manifold projection on raw data; second, the projected samples are selected via grid sampling in the low-dimensional space. The entire algorithm is implemented in a near-memory computing architecture where the sampling logic is integrated with DRAM cells, reducing data movement and energy consumption. The method achieves significant energy savings by filtering data in-place before transmitting to the main training accelerator.

## Key Results
- Achieves 11.9% higher average accuracy on ImageNet-1K compared to DQ, 9.7% over DQAS, and 4.7% over NeSSA.
- Reduces DDR energy by 5.3× compared to NeSSA and 50.4× compared to DQ.
- Improves overall system efficiency by 1.2× over sparse training accelerators.

## Why This Works (Mechanism)

### Mechanism 1: Bias Mitigation via DNN-Free Manifold Projection
The sampling process is decoupled from DNN feature extraction, reducing inductive bias inherent in model-dependent coreset methods. NMS uses DE-SNE operating directly on raw image data rather than DNN embeddings, preserving structural similarity without inheriting specific model preferences.

### Mechanism 2: Energy Reduction via Near-Memory Data Filtering
Moving sampling logic closer to DRAM cells significantly reduces system energy consumption by minimizing data movement over high-capacitance PCB channels. The architecture implements DE-SNE in a near-memory logic layer, filtering data in-place and transmitting only the selected subset to the main training accelerator.

### Mechanism 3: Stable Dimensionality Reduction via Differential Evolution
Replacing binary search for perplexity in t-SNE with differential evolution stabilizes the projection, leading to more consistent sample distribution and higher accuracy. DE is better suited for non-convex optimization, finding optimal perplexity parameters that allow t-SNE to better model dataset distribution.

## Foundational Learning

- **Concept: Manifold Learning (t-SNE)**
  - **Why needed:** t-SNE is the mathematical engine of the sampling algorithm; understanding how it preserves local vs. global structures explains why selected samples are representative.
  - **Quick check:** Does t-SNE prioritize preserving distances between distant points or nearby points, and how does that influence which "representative" samples are kept?

- **Concept: Inductive Bias in Dataset Distillation**
  - **Why needed:** The paper positions itself against methods like DQ and distillation; understanding how training on data selected by another model limits generalization is crucial.
  - **Quick check:** If I select images using features from a ResNet, why might a ViT model fail to train effectively on that subset?

- **Concept: Near-Memory Computing (NMC) / PIM**
  - **Why needed:** The architectural contribution relies on physics of data movement; understanding energy cost differences between DRAM access, PCB transmission, and logic computation is essential.
  - **Quick check:** Why is fetching a bit from off-chip memory significantly more expensive than performing a MAC operation near the memory bank?

## Architecture Onboarding

- **Component map:** Raw images -> PEA Array (Compute Distance Matrix) -> PEA + TE Arrays (DE-SNE loop for optimal σ and projection) -> Grid Sampling Logic (Select representative indices) -> Selected images -> Training Accelerator

- **Critical path:** Input raw images → PEA computes distance matrix → PEA + TE optimize via DE-SNE to find optimal σ and project to low-dim → Low-dim points → Grid Sampling Logic selects indices → Selected images transmitted to training accelerator

- **Design tradeoffs:** Uses specific CORDIC and Newton's method units in PEA to approximate non-linear functions (log/exp) rather than large floating-point units, trading slight precision for area efficiency; DE requires maintaining population of candidate solutions (32 individuals), increasing parallel hardware requirements but ensuring convergence

- **Failure signatures:** Perplexity divergence if DE optimization loop fails to reduce error below 10⁻¹⁰, causing malformed t-SNE map; memory bottleneck if dataset too large for on-chip SRAM buffers, negating energy benefits of near-memory design

- **First 3 experiments:**
  1. Run DE-SNE on CIFAR-10 with fixed perplexity target (e.g., 15); verify perplexity error converges to <10⁻¹⁰ and plot 2D manifold to confirm cluster separation
  2. Isolate PEA array; measure throughput of Distance Matrix calculation for batch of 64 images vs. theoretical peak to verify gridding and parallel dataflow efficiency
  3. Run full training loop on ImageNet-1K with 30% keeping ratio; measure total DDR access count against baseline "CPU-GPU" system to validate claimed ~50× energy reduction

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- DE-SNE algorithm's scalability to ImageNet-1K with raw pixels remains unverified due to O(N²) complexity; preprocessing assumptions are unclear
- Hardware energy savings rely on specific TSMC 28nm synthesis and UCIe protocol parameters not fully detailed
- Grid resolution for manifold sampling is unspecified, affecting sample distribution

## Confidence
- **High:** Claims about differential evolution improving t-SNE stability (perplexity error reduction shown in Fig 15)
- **Medium:** Claims about generalization improvement over DQ/NeSSA (cross-architecture accuracy reported but not independently verified)
- **Medium:** Near-memory energy reduction claims (DDR access reduction measured but micro-architectural details incomplete)

## Next Checks
1. Reproduce DE-SNE perplexity convergence on CIFAR-10; verify error drops below 10⁻¹⁰ within 10k iterations
2. Isolate PEA array throughput measurement for 64-image distance matrix calculation to validate claimed parallel efficiency
3. Measure total DDR access count during ImageNet-1K training with 30% keeping ratio vs. baseline CPU-GPU system