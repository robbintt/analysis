---
ver: rpa2
title: Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in
  Long-Context Language Models
arxiv_id: '2601.18527'
source_url: https://arxiv.org/abs/2601.18527
tags:
- context
- question
- documents
- answer
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether fine-tuning long-context language
  models (LCLMs) to selectively attend to relevant information can effectively replace
  conventional retrieval-augmented generation (RAG) systems. The authors propose using
  reinforcement learning with Group Relative Policy Optimization (GRPO) to train models
  on question answering tasks with sparse relevant documents and extensive hard negatives.
---

# Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models

## Quick Facts
- arXiv ID: 2601.18527
- Source URL: https://arxiv.org/abs/2601.18527
- Reference count: 40
- Primary result: GRPO fine-tuning achieves up to +20 points on HotpotQA over base model

## Executive Summary
This work investigates whether fine-tuning long-context language models (LCLMs) to selectively attend to relevant information can effectively replace conventional retrieval-augmented generation (RAG) systems. The authors propose using reinforcement learning with Group Relative Policy Optimization (GRPO) to train models on question answering tasks with sparse relevant documents and extensive hard negatives. Five reward functions are explored, ranging from answer-only to reasoning-based objectives with LLM-as-a-judge evaluation. The study shows substantial in-domain improvements, with RAO(y) achieving up to +20 points on HotpotQA over the base model. Out-of-domain generalization remains task-dependent, with fine-tuned models excelling on finance questions (+9 points) while RAG shows stronger performance on multiple-choice questions (+6 points). Fine-tuning also brings moderate improvements in robustness under KV-cache compression, though gains vary across tasks.

## Method Summary
The study fine-tunes Qwen2.5-7B-Instruct-1M using GRPO with VERL framework and FSDP. Training uses HotpotQA and 2WikiMultihopQA datasets with gold passages and top-500 hard negatives per question retrieved via Qwen3-Embedding-4B. Contexts are capped at 32k tokens, documents are shuffled and tagged with "[DOC i]". Five reward functions are tested, from answer-only to reasoning-based with LLM-judge evaluation. Models are evaluated on HELMET (in-domain), ∞Bench/LB-v2/Loong (OOD), and KV-cache compression robustness with RetrievalAttention. The approach contrasts with standard SFT baselines, which show 13.7-point average degradation.

## Key Results
- RAO(y) reward achieves up to +20 points on HELMET HotpotQA vs. base model
- Out-of-domain performance varies: +9 points on finance questions, +6 points for RAG on MCQs
- Fine-tuning improves KV-cache compression robustness: 6.8% vs 13.9% performance drop
- Performance gains are not driven by improved attention-based document ranking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRPO-based training enables LCLMs to develop noise robustness that improves in-domain retrieval performance.
- Mechanism: Reinforcement learning with GRPO allows the model to sample multiple outputs and learn from group-relative advantages, encouraging exploration of diverse strategies for identifying relevant information rather than constraining the model to specific training paths as SFT does.
- Core assumption: Models require exploration during training to discover which information is truly relevant, rather than being constrained to fixed output formats.
- Evidence anchors:
  - [abstract] "The study shows substantial in-domain improvements, with RAO(y) achieving up to +20 points on HotpotQA over the base model."
  - [Section 5] "Standard SFT leads to severe performance degradation compared to the base model, resulting in an average drop of 13.7 points."
  - [Section 3] "We argue that while SFT constrains the models to specific training paths, GRPO enables them to explore diverse strategies to discover which information is truly relevant."
- Break condition: If the task requires dense information integration over cohesive documents rather than sparse fact extraction, this mechanism provides limited benefit (see out-of-domain MCQ results).

### Mechanism 2
- Claim: Performance gains stem from enhanced robustness to contextual noise, not improved attention-based document ranking.
- Mechanism: The base model already successfully reranks relevant information (NDCG@10: 82.0-84.7). Fine-tuning improves the model's ability to utilize attended information and filter distractions, but does not meaningfully improve which documents receive attention.
- Core assumption: The bottleneck in LCLM retrieval is not finding relevant content but effectively using it amid noise.
- Evidence anchors:
  - [Section 5] "The Pearson correlation between attention ranking and task performance is negligible (r=-0.09, p=0.86)."
  - [Section 5] "RID+Q(y) achieves the best NDCG@10 but poor accuracy (72.9), while RAO(y) demonstrates strong performance (80.3) despite comparable NDCG@10 scores."
  - [corpus] Related work (Qiu et al., 2025) confirms SFT approaches show limited performance improvements against standard RAG pipelines.
- Break condition: If attention-based ranking were the bottleneck, we would see strong correlation between NDCG improvements and task performance gains.

### Mechanism 3
- Claim: Answer-only and reasoning-based reward functions outperform format-constrained objectives for long-context learning.
- Mechanism: Simpler objectives (RAO) allow models flexibility in learning attention patterns, while overly restrictive objectives (RID+C, RID+Q) may impede processing of extended contexts by forcing specific output structures.
- Core assumption: Constrained output formats create optimization conflicts between finding relevant content and formatting outputs correctly.
- Evidence anchors:
  - [Section 5] "More constrained strategies, specifically RID+C(y) and RID+Q(y), show performance degradation, suggesting that overly restrictive training objectives may impede the model's ability to process extended contexts effectively."
  - [Section 5] RAO(y) achieves 81.1 on HotpotQA vs. 67.9 for RID+C(y).
  - [corpus] Jin et al. (2025) demonstrate SFT with reasoning improves performance, consistent with reasoning rewards showing strong results.
- Break condition: When explicit document attribution is required for verification or citation tasks, format-constrained objectives become necessary despite performance tradeoffs.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: This RL method enables the model to sample multiple outputs per input and learn from group-relative advantages, providing the exploration capability that SFT lacks.
  - Quick check question: Can you explain why GRPO's group-relative advantages differ from standard PPO's advantage estimation?

- Concept: **KV-Cache Compression (RetrievalAttention)**
  - Why needed here: The paper evaluates whether noise-robust models maintain performance when compression retains only top-k attended key-value pairs, testing if training improves inference efficiency.
  - Quick check question: How does RetrievalAttention decide which KV pairs to retain during compression?

- Concept: **Hard Negative Mining**
  - Why needed here: Training data uses topically related but irrelevant passages retrieved by strong retrievers to force models to develop selective attention rather than relying on surface-level relevance signals.
  - Quick check question: Why might topically related negatives be more effective training signals than randomly sampled documents?

## Architecture Onboarding

- Component map: HotpotQA/2WikiMultihopQA -> Qwen3-Embedding-4B retriever (k=500) -> filter to ≤32k tokens -> shuffle documents -> add [DOC i] tags -> GRPO training -> evaluation

- Critical path:
  1. Hard negative refinement (fuzzy matching + LLM judge to promote false negatives to relevant)
  2. GRPO rollout generation with vLLM (max 34,816 batched tokens)
  3. Reward computation based on selected objective (RAO/ID/ID+C/ID+Q/R+Judge)
  4. Policy gradient update with KL constraint

- Design tradeoffs:
  - Training context length (32k) vs. evaluation length (1M): Training on 32k is computationally efficient and tests generalization, but may miss long-context-specific attention patterns.
  - Answer-only vs. format-constrained rewards: RAO maximizes performance but provides no attribution; RID+C provides verification but degrades accuracy.
  - Shuffling vs. positional encoding: Shuffling prevents positional bias during training but may conflict with position-aware attention patterns.

- Failure signatures:
  - SFT with answer-only objective causes 13.7-point average degradation (Table 1).
  - RID+C performance drops from 90.2% (4k) to 83.5% (128k), indicating format constraints fail at longer contexts.
  - Out-of-domain multiple-choice shows no improvement (70.3-72.5 vs. 72.0 base), indicating training specialization doesn't transfer to dense integration tasks.

- First 3 experiments:
  1. Baseline comparison: Evaluate Qwen2.5-7B-Instruct-1M with RAO(y) reward on HELMET HotpotQA at 32k context. Expect ~80 SubEM vs. 57.3 base.
  2. Ablation on reward complexity: Train separate models with each of the 5 reward functions on the same data split. Compare HELMET performance to confirm RAO and R+Judge outperform constrained formats.
  3. KV-cache robustness test: Apply RetrievalAttention compression to both base and fine-tuned models on ∞Bench QA. Expect 6.8% drop for RAO(y) vs. 13.9% for base on MC split, but verify task-dependent variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanism drives the observed performance improvements if not improved attention-based document ranking?
- Basis in paper: [explicit] The authors report negligible correlation between NDCG@10 attention ranking and task performance (r=-0.09, p=0.86), stating "performance improvements do not stem from better attention-based document ranking" and attributing gains to "enhanced robustness to contextual noise" without identifying the specific mechanism.
- Why unresolved: The paper's central hypothesis was that fine-tuning improves selective attention, but analysis contradicts this. The actual cause of improvements remains unexplained.
- What evidence would resolve it: Ablation studies analyzing attention head behavior, token-level attention patterns on distractor vs. relevant content, or probing tasks measuring noise discrimination directly.

### Open Question 2
- Question: How can training objectives be designed to balance selective attention with dense information synthesis for universal out-of-domain generalization?
- Basis in paper: [explicit] The conclusion states: "achieving universal generalization requires future training objectives that better balance selective attention with the ability to synthesize dense information."
- Why unresolved: Current objectives optimized for sparse retrieval (identifying isolated facts) underperform on tasks requiring holistic document understanding, where RAG retains advantages.
- What evidence would resolve it: Development and evaluation of hybrid training objectives combining sparse retrieval signals with dense summarization or multi-hop reasoning tasks, tested across both task types.

### Open Question 3
- Question: Would training on contexts up to 1M tokens yield different outcomes than the observed generalization from 32k training?
- Basis in paper: [explicit] The Limitations section states this design choice "limits our ability to investigate whether training on extremely long sequences would yield different outcomes" despite observed generalization patterns suggesting consistency.
- Why unresolved: Computational constraints prevented training on full 1M token contexts; it remains unknown whether direct long-context training provides additional benefits.
- What evidence would resolve it: Comparing models trained at 32k vs. 128k vs. 512k vs. 1M context lengths on the same tasks, controlling for compute budget.

### Open Question 4
- Question: How do these GRPO-based fine-tuning strategies transfer to non-English languages, modalities beyond text, and specialized domains?
- Basis in paper: [explicit] The Limitations section states: "The effectiveness of these fine-tuning strategies for other languages, modalities, or task types remains an open question" and notes the lack of domain-specific annotated data limits cross-domain assessment.
- Why unresolved: Evaluation was restricted to English Wikipedia-based QA benchmarks; no data exists for legal, scientific, or technical domains.
- What evidence would resolve it: Applying the same training methodology to multilingual benchmarks, multimodal long-context tasks, and domain-specific corpora with gold relevance annotations.

## Limitations
- Evaluation scope restricted to Qwen2.5-7B models, limiting generalizability to larger LCLMs
- Fine-tuning focused only on retrieval-augmented tasks without exploring dense document integration
- Reinforcement learning implementation may have high variance due to 5-sample GRPO rollout
- Core hypothesis about selective attention improvement contradicted by correlation analysis

## Confidence
**High Confidence:**
- Fine-tuned models achieve substantial in-domain improvements on HotpotQA (up to +20 points) over base models
- SFT baselines consistently underperform compared to GRPO fine-tuning
- Constrained reward formats (RID+C, RID+Q) degrade performance relative to answer-only approaches

**Medium Confidence:**
- Out-of-domain generalization varies by task type, with finance QA showing improvement while multiple-choice questions show no benefit
- Fine-tuning provides moderate KV-cache compression robustness improvements
- Performance gains stem from enhanced robustness to contextual noise rather than improved attention-based ranking

**Low Confidence:**
- The specific mechanism by which GRPO enables noise robustness discovery
- Generalizability of findings to model families beyond Qwen2.5-7B
- The practical significance of moderate KV-cache compression gains across diverse tasks

## Next Checks
1. **Correlation Causation Test**: Conduct controlled experiments that artificially degrade attention ranking (e.g., by adding irrelevant passages) while measuring both NDCG@10 and task performance to establish whether the observed weak correlation represents true independence or experimental artifacts.

2. **Model Scaling Study**: Replicate the fine-tuning pipeline with GPT-4o and Claude 3.5 Sonnet to determine whether the noise robustness mechanism and task-specific generalization patterns hold across different model families and scales.

3. **KV-Cache Compression Benchmarking**: Systematically test RetrievalAttention compression across a broader task spectrum (including non-QA tasks) to identify whether the observed 6.8% vs 13.9% drop differences represent consistent patterns or task-specific anomalies.