---
ver: rpa2
title: Towards Robust Speech Recognition for Jamaican Patois Music Transcription
arxiv_id: '2507.16834'
source_url: https://arxiv.org/abs/2507.16834
tags:
- patois
- jamaican
- whisper
- music
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately transcribing Jamaican
  Patois music using automatic speech recognition (ASR). The authors curate a dataset
  of over 40 hours of manually transcribed Patois music and fine-tune various Whisper
  models on this data.
---

# Towards Robust Speech Recognition for Jamaican Patois Music Transcription

## Quick Facts
- **arXiv ID**: 2507.16834
- **Source URL**: https://arxiv.org/abs/2507.16834
- **Reference count**: 0
- **Primary result**: Fine-tuned Whisper models achieve 0.30 WER on Jamaican Patois music transcription, outperforming pre-trained Whisper Large (0.89 WER)

## Executive Summary
This paper addresses the challenge of accurately transcribing Jamaican Patois music using automatic speech recognition (ASR). The authors curate a dataset of over 40 hours of manually transcribed Patois music and fine-tune various Whisper models on this data. They develop scaling laws to predict ASR performance based on model size and dataset size. Their results show that fine-tuned Whisper models significantly outperform the pre-trained Whisper Large model on Jamaican Patois, with the best WER of 0.30 achieved by the Medium model trained on 40 hours of data.

## Method Summary
The authors curate a Jamaican Patois music dataset (5,110 clips, 42.58 hours) and fine-tune Whisper variants (Tiny: 39M, Base: 74M, Small: 244M, Medium: 769M parameters) using AdamW optimizer with learning rate 1e-5, linear warmup for 500 steps, and 4,000 total steps. Audio is resampled to 16,000 Hz and converted to log Mel-spectrograms. They evaluate Word Error Rate (WER) across different model sizes and dataset amounts (20, 35, 40 hours) to derive a scaling law that predicts performance based on model size and data quantity.

## Key Results
- Fine-tuned Whisper models achieve WER of 0.30 (Medium model, 40 hours), significantly outperforming pre-trained Whisper Large (0.89 WER)
- Larger models consistently outperform smaller ones: Tiny (0.79 WER), Base (0.63 WER), Small (0.42 WER), Medium (0.30 WER)
- Derived scaling law: log(WER) = 5.063 - 0.255·log(M) - 0.269·log(D), accurately predicting performance across tested configurations

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning from Multilingual Pre-training
Pre-trained multilingual ASR models can be effectively adapted to low-resource creole languages through fine-tuning on modest domain-specific datasets. Whisper's 680,000 hours of pre-training provides learned audio representations that capture general acoustic patterns. Fine-tuning adjusts these representations to recognize Patois-specific phonetic features that differ from standard English.

### Mechanism 2: Power-Law Scaling for Low-Resource ASR
ASR performance on low-resource languages follows a predictable power-law relationship with model size and dataset size, enabling resource allocation planning. Larger models capture more complex acoustic patterns; more data provides coverage of linguistic variation. Both contribute diminishing but predictable improvements to WER.

### Mechanism 3: Domain-Specific Fine-Tuning Overcomes Music Transcription Challenges
Fine-tuning on music-specific Patois data addresses unique challenges (background instrumentation, rhythmic delivery, non-standard pronunciation) that cause pre-trained models to fail. Pre-trained Whisper Large achieves 0.05 WER on standard English but 0.89 WER on Patois music. Domain-specific training teaches the model to separate vocal content from musical accompaniment and recognize creole pronunciations.

## Foundational Learning

- **Word Error Rate (WER)**: Primary evaluation metric throughout the paper; essential for comparing model performance and validating scaling predictions.
  - Quick check question: Given a reference transcription with 20 words and a hypothesis with 2 substitutions, 1 insertion, and 1 deletion, what is the WER?

- **Transfer Learning in ASR**: Core technique enabling progress with limited data; explains why fine-tuned Tiny (39M params) outperforms pre-trained Large (1550M params).
  - Quick check question: Why might a model pre-trained primarily on English still transfer to Jamaican Patois, despite significant linguistic differences?

- **Log Mel-spectrograms**: Audio representation used by Whisper; understanding preprocessing is essential for reproducing results.
  - Quick check question: Why convert raw audio to log Mel-spectrograms rather than using waveform directly?

## Architecture Onboarding

- **Component map**: Audio (22,050 Hz MP3) → Resample (16,000 Hz) → Log Mel-spectrogram → Whisper model → WER evaluation

- **Critical path**: 
  1. Curate transcribed audio dataset (manual annotation is bottleneck)
  2. Preprocess audio to 16 kHz log Mel-spectrograms
  3. Fine-tune with proper learning rate scheduling
  4. Evaluate WER; fit scaling law if running multiple configurations

- **Design tradeoffs**:
  - Larger models yield lower WER but require more GPU memory and training time (Large not fine-tuned due to computational constraints)
  - More data improves WER with diminishing returns (α=0.255 for model size, β=0.269 for data)
  - Music-specific training may reduce generalization to conversational Patois

- **Failure signatures**:
  - WER > 0.6 on Medium model suggests data quality issues or insufficient training
  - Large train/validation loss gap indicates overfitting (risk with small datasets)
  - Scaling law predictions deviating >10% from actual suggests regime shift or measurement error

- **First 3 experiments**:
  1. Establish baseline: Evaluate pre-trained Whisper Large on your Patois test set (expect WER ~0.89)
  2. Verify data quality: Fine-tune Whisper Tiny on 10-20 hours (target WER < 0.79)
  3. Validate scaling: Train Medium on subset increments (20, 35, 40 hours) to confirm scaling law holds for your data distribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the empirically derived scaling law accurately predict performance for the Whisper Large model?
- **Basis in paper**: The authors did not fine-tune the Whisper Large model due to computational constraints, relying solely on extrapolation via their scaling equation to estimate its performance.
- **Why unresolved**: The scaling law was fitted on smaller models (Tiny through Medium); it is unverified if this power-law relationship holds linearly when scaling up to the Large model's parameter count.
- **What evidence would resolve it**: Fine-tuning the Whisper Large model on the 40-hour dataset and comparing the actual WER against the scaling law's prediction.

### Open Question 2
- **Question**: Do the scaling coefficients (α and β) remain constant when the dataset is scaled significantly beyond 42 hours?
- **Basis in paper**: The dataset is described as the "largest of its kind" at 42 hours, but the scaling law is derived from a narrow range of data (20 to 40 hours).
- **Why unresolved**: Scaling laws often exhibit "break points" or change slopes when moving from low-resource to data-rich regimes; it is unclear if the inverse relationship with WER holds indefinitely.
- **What evidence would resolve it**: Curating a larger dataset (e.g., 100+ hours) to determine if the log-log linear relationship persists or if diminishing returns set in.

### Open Question 3
- **Question**: Can the remaining word error rate (WER) of 0.30 be further reduced using Large Language Model (LLM) post-processing?
- **Basis in paper**: Related work discusses using LLMs to correct noisy transcripts, but the current study focuses exclusively on acoustic model fine-tuning.
- **Why unresolved**: The remaining errors in the best-performing model (WER 0.30) may be linguistic or contextual rather than acoustic, which acoustic scaling alone cannot fix.
- **What evidence would resolve it**: Applying an LLM-based correction step to the fine-tuned Whisper outputs to measure the additional reduction in WER.

## Limitations
- The manually transcribed Jamaican Patois music dataset (42.58 hours) is not publicly available, requiring researchers to either contact authors or collect similar data independently
- The scaling law's validity outside the tested range (39M-769M parameters, 20-40 hours) remains uncertain, as power-law relationships often break down when extrapolated too far
- The music-specific nature of the dataset raises questions about whether these models generalize to conversational Patois or other music genres with different production styles

## Confidence

**High Confidence**: The core experimental results showing fine-tuned Whisper models outperforming pre-trained Whisper Large on Jamaican Patois music transcription. The WER measurements (0.30 vs 0.89) are directly observed and reproducible with the described methodology.

**Medium Confidence**: The derived scaling law log(WER) = 5.063 - 0.255·log(M) - 0.269·log(D) accurately predicts performance within the tested parameter and data ranges. While the law fits the observed data well, its extrapolation to significantly larger models or datasets has not been validated.

**Low Confidence**: Claims about the mechanisms underlying transfer learning effectiveness for creole languages. While the empirical results demonstrate success, the paper doesn't provide detailed analysis of which specific acoustic-phonetic features transfer most effectively, nor does it compare against other transfer approaches.

## Next Checks

1. **Dataset Accessibility Test**: Attempt to obtain the Jamaican Patois music dataset from authors or recreate a comparable dataset. Measure the time and resources required for manual transcription to understand the true barrier to entry for this research.

2. **Scaling Law Extrapolation**: Test the scaling law predictions by training a model with 1,000+ million parameters or using 60+ hours of data. Compare predicted vs actual WER to determine if the power-law relationship holds beyond the original experimental bounds.

3. **Cross-Domain Generalization**: Evaluate the fine-tuned models on non-music Patois speech (conversational recordings, radio broadcasts, etc.) to assess whether music-specific training improves or degrades performance on other Patois speech domains.