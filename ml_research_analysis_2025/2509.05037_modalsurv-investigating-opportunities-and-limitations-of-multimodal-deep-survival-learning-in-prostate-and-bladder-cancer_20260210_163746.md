---
ver: rpa2
title: 'ModalSurv: Investigating opportunities and limitations of multimodal deep
  survival learning in prostate and bladder cancer'
arxiv_id: '2509.05037'
source_url: https://arxiv.org/abs/2509.05037
tags:
- multimodal
- survival
- clinical
- modalsurv
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates ModalSurv, a multimodal deep survival framework
  integrating clinical, MRI, histopathology, and RNA-seq data for prostate and bladder
  cancer recurrence prediction. Using cross-attention fusion and foundation-model-derived
  embeddings, ModalSurv achieved a C-index of 0.7402 (1st) for prostate and 0.5740
  (5th) for bladder cancer on the CHIMERA Challenge.
---

# ModalSurv: Investigating opportunities and limitations of multimodal deep survival learning in prostate and bladder cancer

## Quick Facts
- arXiv ID: 2509.05037
- Source URL: https://arxiv.org/abs/2509.05037
- Reference count: 0
- Primary result: ModalSurv achieved 1st place (C-index 0.7402) for prostate cancer and 5th place (C-index 0.5740) for bladder cancer on CHIMERA Challenge

## Executive Summary
This study evaluates ModalSurv, a multimodal deep survival framework integrating clinical, MRI, histopathology, and RNA-seq data for prostate and bladder cancer recurrence prediction. Using cross-attention fusion and foundation-model-derived embeddings, ModalSurv achieved strong internal performance (C-index 0.7402 for prostate) but showed limited generalization to external test sets, where clinical features alone outperformed multimodal models. The work demonstrates both the potential and challenges of deep multimodal survival modeling, emphasizing the need for larger, harmonized datasets and robust feature selection to enable scalable, generalizable cancer prognosis.

## Method Summary
ModalSurv employs modality-specific projection heads to embed clinical, MRI, histopathology, and RNA-seq features into a shared 128-dimensional latent space. Cross-attention blocks (without self-attention) fuse embeddings through sample-specific soft gating, followed by DeepHit-based discrete-time survival prediction (30 bins). Pretrained foundation models (CONCH, Titan, MedicalNet) extract embeddings without fine-tuning. The model is trained with 5-fold stratified cross-validation, early stopping on validation C-index, and ensemble averaging for inference.

## Key Results
- Cross-validation: Clin+WSI+MRI+RNA-seq achieved C-index 0.891 (Task 1) and 0.849 (Task 3)
- External validation: Clin+WSI+MRI+RNA-seq dropped to C-index 0.645 (Task 1) and 0.457 (Task 3)
- Clinical-only models (C-index ~0.84) outperformed multimodal models on external test sets
- Cross-attention fusion improved local CV performance but failed to generalize

## Why This Works (Mechanism)

### Mechanism 1
Cross-attention fusion captures inter-modality dependencies more effectively than concatenation when sufficient aligned data exists. Each modality embedding serves as a query attending only to other modalities, forcing cross-modal relationship learning. A gating network produces sample-specific soft weights for each attended modality before final fusion. Core assumption: Prognostic signals are distributed across modalities and require explicit cross-modal attention to capture complementary information. Break condition: Under data scarcity (<200 samples), cross-attention may overfit to training distribution and fail to generalize.

### Mechanism 2
Foundation-model-derived embeddings provide richer representations than traditional CNN features for histopathology and imaging modalities. Pretrained encoders (CONCH for tile-level, Titan for WSI-level, MedicalNet for MRI) extract context-aware embeddings without fine-tuning, capturing global tissue architecture and volumetric context beyond patch-level morphology. Core assumption: Foundation models pretrained on large medical image corpora transfer useful representations to survival prediction tasks without task-specific adaptation. Break condition: Foundation model embeddings may not align well with clinical/molecular features in latent space when training data is insufficient to learn the alignment.

### Mechanism 3
Clinical features encode implicit multimodal information and provide the most robust baseline under data scarcity. Clinical variables such as stage, grade, lymphovascular invasion, and biomarker subtypes already incorporate assessments from pathology, imaging, and molecular testing. This pre-integrated signal is more stable than learned multimodal fusion. Core assumption: Clinical variables serve as a form of human-curated multimodal fusion that generalizes better than deep learning fusion when sample sizes are limited.

## Foundational Learning

- **Concordance Index (C-index)**: The primary evaluation metric measuring ranking quality of predicted vs observed survival times. A C-index of 0.5 indicates random predictions, 1.0 indicates perfect ranking. Quick check: Can you explain why a model with C-index 0.74 on internal validation might drop to 0.57 on external test?

- **Discrete-time Survival Analysis (DeepHit)**: ModalSurv uses DeepHit's approach, converting continuous survival times into probability mass functions over fixed intervals. The loss combines likelihood (handling censoring) and ranking (enforcing correct risk ordering). Quick check: What happens to the PMF representation if you choose too few or too many time bins?

- **Cross-attention vs Self-attention**: The architecture deliberately excludes self-attention to force cross-modal dependency learning. Understanding this distinction is critical for debugging fusion behavior. Quick check: If you added self-attention back, what failure mode might emerge in a small-sample setting?

## Architecture Onboarding

- **Component map**: Input layer (4 modality streams) → Pretrained encoders (feature extraction) → Modality-specific projection heads (128-dim) → Cross-attention blocks → Gating network → Weighted sum → Final projection → DeepHit head (30 bins) → Output (expected survival time)

- **Critical path**: Feature extraction quality → projection alignment → cross-attention learning → gating calibration → PMF estimation. The cross-attention and gating stages are the most parameter-heavy and overfit-prone.

- **Design tradeoffs**: Cross-attention vs concatenation (cross-attention showed better CV but worse generalization); Pretrained vs fine-tuned encoders (frozen to prevent overfitting); 500-patch WSI limit (resource constraint vs potential information loss).

- **Failure signatures**: CV-to-validation drop >0.2 indicates overfitting to discovery cohort; multimodal underperforming clinical-only suggests modality alignment failure or noisy features; high variance across folds (±0.08-0.12) indicates small dataset instability.

- **First 3 experiments**:
  1. Ablation by modality: Train clinical-only, clinical+WSI, clinical+WSI+MRI separately on discovery set with 5-fold CV. Compare C-index variance and validation gap to identify which modalities help vs hurt generalization.
  2. Fusion strategy comparison: Implement three fusion variants (concatenation, cross-attention, gated) with identical encoders and hyperparameters. Measure CV performance and compute aligned embedding distances to diagnose cross-modal alignment quality.
  3. Data scaling simulation: Randomly subsample training data at 25%, 50%, 75%, 100% and plot CV-test gap curves. If multimodal models require >150 samples to beat clinical baseline, this quantifies the data threshold for fusion benefits.

## Open Questions the Paper Calls Out

### Open Question 1
Can uncertainty-aware fusion or adaptive modality weighting mechanisms mitigate the overfitting and generalization gaps observed in deep multimodal survival models? The authors state future work should emphasize "uncertainty-aware fusion, adaptive modality weighting, and multi-institutional validation to ensure clinically robust multimodal survival prediction." This remains unresolved as current ModalSurv's standard cross-attention and soft-gating failed to generalize on external test sets (Task 3 validation C-index dropped to 0.4565).

### Open Question 2
What is the minimum sample size or data alignment quality required for deep multimodal fusion to consistently outperform clinical-only baselines in survival analysis? The authors note "Clinical-only models remain strong baselines when data are scarce, while cross-attention fusion offers a scalable direction once larger and better-curated datasets are available." This remains unresolved as the study utilized small discovery cohorts (95 and 176 patients) where clinical features outperformed complex fusion on test sets.

### Open Question 3
Does fine-tuning foundation model embeddings (e.g., CONCH, Titan) improve survival prediction performance in low-data regimes compared to the frozen extraction strategy employed? The methods state "Pretrained encoders were used only for feature extraction and were not fine-tuned." This decision was likely made to prevent overfitting, but it leaves open whether limited fine-tuning could have bridged the domain gap responsible for poor external validation results.

## Limitations

- Severe data scarcity (N=95 discovery) creates substantial overfitting risk for multimodal learning
- Significant performance drop from cross-validation (0.891) to validation (0.645) indicates poor generalization
- Lack of hyperparameter tuning and architectural details for cross-attention blocks limits reproducibility
- Frozen foundation model embeddings may miss potential alignment improvements

## Confidence

- **High confidence**: Clinical features alone provide strong prognostic baseline (C-index ~0.84) and outperform multimodal models on external test sets; foundation model embeddings capture meaningful tissue and imaging patterns.
- **Medium confidence**: Cross-attention fusion can capture inter-modality dependencies when sufficient data exists; multimodal models show local CV gains over unimodal baselines.
- **Low confidence**: Generalizability of multimodal models to external datasets; specific architectural choices (attention parameters, gating network design) and their impact on performance.

## Next Checks

1. **Data scaling experiment**: Systematically subsample training data at 25%, 50%, 75%, and 100% to quantify the sample size threshold where multimodal models surpass clinical-only baselines. This will reveal if 95 samples are insufficient for cross-attention benefits.

2. **Fusion strategy ablation**: Implement and compare three fusion variants (concatenation, cross-attention, gated) with identical hyperparameters and encoders. Measure both performance and cross-modal embedding distances to diagnose whether alignment failures or overfitting drive multimodal underperformance.

3. **Encoder fine-tuning vs freezing**: Train identical architectures with frozen foundation model embeddings versus fine-tuned embeddings on the survival task. This will determine if the authors' conservative choice (freezing) sacrificed potential gains for regularization, and quantify the data requirements for safe fine-tuning.