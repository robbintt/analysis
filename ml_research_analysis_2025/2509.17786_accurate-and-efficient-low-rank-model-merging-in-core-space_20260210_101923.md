---
ver: rpa2
title: Accurate and Efficient Low-Rank Model Merging in Core Space
arxiv_id: '2509.17786'
source_url: https://arxiv.org/abs/2509.17786
tags:
- space
- merging
- core
- full
- knots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Core Space merging, a method to efficiently
  merge low-rank adaptation (LoRA) models by projecting their updates into a shared,
  information-preserving subspace. The approach uses singular value decomposition
  to construct reference bases that align task-specific updates, enabling merging
  in a compact Core Space without losing information.
---

# Accurate and Efficient Low-Rank Model Merging in Core Space

## Quick Facts
- arXiv ID: 2509.17786
- Source URL: https://arxiv.org/abs/2509.17786
- Reference count: 40
- Achieves 94.16% accuracy on Llama 3 8B NLI tasks with 280× speedup

## Executive Summary
This paper introduces Core Space merging, a method to efficiently merge low-rank adaptation (LoRA) models by projecting their updates into a shared, information-preserving subspace. The approach uses singular value decomposition to construct reference bases that align task-specific updates, enabling merging in a compact Core Space without losing information. This preserves the efficiency benefits of LoRA while improving accuracy across tasks.

The method demonstrates state-of-the-art normalized accuracy on both vision and language tasks while reducing merging time by up to 600× compared to full-space methods. Core Space achieves 94.16% accuracy on Llama 3 8B NLI tasks and 76.3% on vision tasks, outperforming competing approaches like KnOTS while maintaining the computational efficiency of low-rank representations.

## Method Summary
Core Space merging projects LoRA matrices from multiple tasks into a shared reference basis computed via SVD of stacked task matrices. This creates a compact representation where task updates can be merged efficiently without information loss. The method first constructs reference bases from vertically and horizontally stacked A and B matrices across all tasks, then projects each task's update into this Core Space. Any merge function can be applied in this reduced-dimensional space, and the result is projected back to the full parameter space. The approach guarantees lossless merging for linear operations and shows improved accuracy for non-linear merging methods due to better subspace alignment.

## Key Results
- Achieves 94.16% normalized accuracy on Llama 3 8B NLI tasks, surpassing KnOTS
- Demonstrates 280× speedup on vision tasks with +1.6% accuracy improvement
- Shows 600× reduction in merge time for large language models while maintaining accuracy
- Core Space yields consistently higher alignment metrics across all tested task combinations

## Why This Works (Mechanism)

### Mechanism 1: Lossless Projection via Shared Reference Bases
- Claim: Core Space preserves all task-specific information while providing a unified coordinate system for merging
- Mechanism: Reference bases (U_ref_B, V_ref_A) are computed via SVD of vertically/horizontally stacked LoRA matrices across all T tasks. Since these bases span the union of all task subspaces, each task's update can be expressed in this shared coordinate system with zero alignment error. The projection M^(t) = (U_ref_B^⊤ B^(t))(A^(t) V_ref_A) is mathematically equivalent to the original update.
- Core assumption: The SVD of stacked low-rank matrices yields orthonormal bases that collectively span all task-specific directions
- Evidence anchors:
  - [abstract]: "formal proof that projection into Core Space ensures no loss of information"
  - [Section 4.2]: "the reference bases... minimize the total alignment error across all T tasks, achieving an error of exactly zero"
  - [corpus]: Corpus papers on interference (LoRI, ThanoRA) address related problems but do not directly validate this specific mechanism
- Break condition: When T·r approaches or exceeds min(m,n), the dimensional advantage diminishes; however, Appendix A.4 shows zero error persists even in this regime using truncated bases

### Mechanism 2: Computational Efficiency from Dimensionality Reduction
- Claim: Merging in Core Space is orders of magnitude faster than prior methods while achieving better accuracy
- Mechanism: Core matrices are Tr×Tr whereas full updates are m×n. Since SVD and matrix operations scale cubically, reducing dimensionality (e.g., 128×128 vs 768×768 for T=8, r=16, n=768) yields dramatic speedups. Complexity drops from O(n³T²) in KnOTS to O(T·r·n²) in Core.
- Core assumption: The merge function can be faithfully executed in Core Space representation
- Evidence anchors:
  - [abstract]: "utilizing a fraction of the computational resources of competing methods"
  - [Figure 1]: Demonstrates 280× speed-up with +1.6% accuracy on Llama 3 8B
  - [Table 1]: Complexity analysis confirms Core: O(T·r·n² + T⁴r³) vs Full: O(n³) or KnOTS: O(n³T²)
  - [corpus]: No direct complexity comparisons found in corpus papers
- Break condition: When T (number of tasks) becomes very large, Tr approaches full dimensions and efficiency gains erode

### Mechanism 3: Improved Subspace Alignment Reduces Interference
- Claim: Core Space's shared basis improves task alignment, which correlates with better merging outcomes
- Mechanism: By projecting all tasks into a common orthonormal basis, Core Space filters out task-specific noise and aligns representations. Higher Subspace Alignment Ratio (SAR) in Core Space correlates with lower interference between merged tasks.
- Core assumption: Better alignment between task subspaces causally improves merged model performance
- Evidence anchors:
  - [Section 5.2]: "Core Space yields consistently higher alignment"
  - [Figure 5]: Shows higher SAR across all 8 vision tasks in Core Space vs Full Space
  - [Figure 4]: Core Space is information-dense (any truncation degrades performance), unlike Full Space where 80% of components are redundant
  - [Appendix C.1]: "lower interference when merging in Core Space" measured via L1 distance of final embeddings
  - [corpus]: LoRI addresses cross-task interference, supporting that interference is a key mechanism, but doesn't validate Core Space specifically
- Break condition: If tasks learn fundamentally incompatible features, alignment alone cannot resolve conflicts

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: Core Space operates directly on LoRA components (A, B matrices), never reconstructs full ΔW = BA until final output. Understanding rank-r decomposition is essential.
  - Quick check question: For a weight matrix W ∈ R^(768×768) with LoRA rank r=16, how many trainable parameters does LoRA add vs full fine-tuning?

- Concept: **Singular Value Decomposition (SVD) and Orthonormal Bases**
  - Why needed here: Reference bases are extracted via SVD. The zero-alignment-error proof relies on properties of orthonormal matrices and the Stiefel manifold.
  - Quick check question: If U has orthonormal columns, what is U^⊤U? Why does this property enable lossless projection?

- Concept: **Task Arithmetic and Model Merging**
  - Why needed here: Core Space is a substrate for applying any merge function. You must understand what task vectors (ΔW = W_finetuned - W_pretrained) represent and how merging combines them.
  - Quick check question: In simple Task Arithmetic with scaling factor α, what is the formula for W_merged given T task vectors?

## Architecture Onboarding

- Component map:
Input: LoRA checkpoints {(A^(t), B^(t))} for tasks t=1..T
A^(t) ∈ R^(r×n), B^(t) ∈ R^(m×r)
↓ Stacking
A_stack: R^(Tr×n) (vertical concat of A matrices)
B_stack: R^(m×Tr) (horizontal concat of B matrices)
↓ SVD
V_ref_A ∈ R^(n×Tr): right singular vectors of A_stack
U_ref_B ∈ R^(m×Tr): left singular vectors of B_stack
↓ Projection (per task)
Core Matrix M^(t) = (U_ref_B^⊤ @ B^(t)) @ (A^(t) @ V_ref_A^⊤)
M^(t) ∈ R^(Tr×Tr)
↓ Merging
M_merged = M({M^(1), ..., M^(T)}) [any merge function: TA, TIES, TSV, etc.]
↓ Reconstruction
ΔW_merged = U_ref_B @ M_merged @ V_ref_A^⊤

- Critical path:
1. Verify A, B shapes are consistent across all T tasks (rank r must match per task; heterogeneous ranks supported via Appendix E.2)
2. Stack correctly: A matrices vertically (row-wise), B matrices horizontally (column-wise)
3. SVD produces orthonormal reference bases—verify with spot-check (U_ref_B^⊤ @ U_ref_B ≈ I)
4. Project each task: M^(t) requires careful transpose order
5. Merge in Core Space (Tr×Tr operations)
6. Reconstruct: single matrix multiply to get full-space update

- Design tradeoffs:
- **More tasks (higher T)**: Better knowledge aggregation but larger Core Space (Tr×Tr), slower merging
- **Higher LoRA rank (higher r)**: More expressive task updates but larger Core Space
- **Merge function choice**: Linear (TA) is exact-equivalent to full-space; non-linear (TIES, TSV) gains more from Core Space alignment
- **Iso-C combination**: Flattens spectrum post-merge, helps some tasks but requires additional hyperparameter tuning

- Failure signatures:
- **Dimension mismatch during stacking**: A and B must have consistent n and m dimensions across tasks
- **Non-orthonormal reference bases**: Indicates numerical issues in SVD or rank deficiency
- **Degraded performance vs Full Space**: For linear merge (TA), results should be identical per Eq. 11—investigate if not
- **Memory overflow on large T·r**: Core Space dimension grows with T·r; consider chunking or rank truncation (breaks lossless guarantee)

- First 3 experiments:
1. **Equivalence validation**: Merge 2 LoRA adapters using Task Arithmetic in Full Space (reconstruct ΔW^(t) = B^(t)A^(t), then sum) vs Core Space. Verify outputs are numerically identical (per Eq. 11 guarantee).
2. **Scaling benchmark**: Time the full Core Space pipeline for T ∈ {2, 4, 8, 16} tasks with r=16 on ViT-B/32. Plot merge time and compare against theoretical O(T·r·n²). Verify linear scaling in T.
3. **Merge function comparison**: On 8 vision tasks, compare TIES and TSV merged in Full Space, KnOTS Space, and Core Space. Expect Core Space to improve non-linear methods more than linear (TA), as suggested by Section 5.1 results.

## Open Questions the Paper Calls Out
None

## Limitations
- Efficiency advantage diminishes when T becomes very large as Tr approaches full parameter space dimensions
- Performance depends on tasks sharing compatible feature subspaces; incompatible tasks may not benefit from alignment
- Implementation complexity increases with need for consistent rank r across tasks or special handling for heterogeneous ranks

## Confidence

**High confidence**: The lossless projection mechanism (Mechanism 1) has strong theoretical grounding with formal proofs showing zero alignment error. The computational efficiency claims (Mechanism 2) are well-supported by complexity analysis and empirical timing benchmarks.

**Medium confidence**: The interference reduction claims (Mechanism 3) show consistent correlations between alignment and performance, but the causal relationship between subspace alignment and merged model quality could benefit from additional ablation studies.

**Low confidence**: The paper doesn't adequately address how Core Space performs when merging tasks with vastly different data distributions or when tasks have conflicting optimization objectives.

## Next Checks

1. **Scaling threshold analysis**: Systematically determine the maximum number of tasks T where Core Space maintains at least 10× speedup over full-space merging. Test with increasing T (2, 4, 8, 16, 32) on both vision and language tasks, measuring merge time and accuracy degradation.

2. **Incompatible task stress test**: Design experiments merging fundamentally different task types (e.g., language classification with vision detection) to identify when Core Space's alignment benefits break down. Measure both SAR metrics and final task performance.

3. **Merge function ablation study**: For a fixed set of 8 diverse tasks, exhaustively compare all merge functions (TA, TIES, TSV) in Full Space, KnOTS Space, and Core Space. Quantify how much of the Core Space improvement comes from dimensionality reduction vs. alignment benefits by comparing against idealized full-rank merging baselines.