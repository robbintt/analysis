---
ver: rpa2
title: 'Flick: Few Labels Text Classification using K-Aware Intermediate Learning
  in Multi-Task Low-Resource Languages'
arxiv_id: '2506.10292'
source_url: https://arxiv.org/abs/2506.10292
tags:
- data
- arabic
- learning
- flick
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of few-label text classification
  in low-resource languages, particularly Arabic, where limited labelled data hinders
  the performance of deep learning models. The proposed Flick framework tackles this
  by introducing a novel two-stage approach: first, generating pseudo-labels via K-means
  clustering on unlabeled data, and second, refining these pseudo-labels by selecting
  high-quality clusters to train an intermediate classifier.'
---

# Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages

## Quick Facts
- **arXiv ID:** 2506.10292
- **Source URL:** https://arxiv.org/abs/2506.10292
- **Reference count:** 8
- **Primary result:** Flick achieves F1-scores up to 72.81% on Arabic datasets by using pseudo-labels from K-means clustering refined via top-k cluster selection.

## Executive Summary
Flick is a two-stage framework for few-label text classification in low-resource languages, specifically addressing the challenge of limited labeled data. It first generates pseudo-labels from unlabeled data using K-means clustering on multilingual embeddings, then refines these labels by selecting high-accuracy clusters to train an intermediate classifier. This intermediate model is then fine-tuned on the small set of real labels, outperforming standard transfer learning baselines on Arabic, Urdu, Setswana, and English datasets.

## Method Summary
Flick employs a two-stage transfer learning process. First, it uses K-means clustering (k=20) on multilingual sentence embeddings to generate initial pseudo-labels for unlabeled data. Second, it splits this pseudo-labeled data, fine-tunes a language model on a subset, and selects the top-k (k=15) clusters based on accuracy. The refined pseudo-labels are used to train an intermediate classifier, whose weights initialize the final model fine-tuned on the few real labels.

## Key Results
- Flick achieves F1-scores up to 72.81% on Arabic datasets, outperforming state-of-the-art methods.
- The framework generalizes well across 14 datasets in Arabic, Urdu, Setswana, and English.
- Top-k cluster selection significantly improves pseudo-label quality and downstream performance in low-resource settings.

## Why This Works (Mechanism)

### Mechanism 1: Top-K Cluster Selection for Noise Mitigation
Filtering for only the highest-performing pseudo-label clusters improves downstream classifier performance. After K-means clustering generates an initial broad set of pseudo-labels, the framework splits this data, fine-tunes an intermediate model on a small subset, and evaluates accuracy on a larger held-out subset by cluster. Only the top-k clusters with the highest pseudo-label accuracy are selected for the next stage, discarding noisy or incoherent groups.

### Mechanism 2: Intermediate Fine-Tuning (PL-FT) for Weight Initialization
Using a model fine-tuned on high-quality pseudo-labels as a starting point yields better performance than fine-tuning a base PTM directly with very few labels. This two-stage transfer process exposes the model to task-relevant semantics before seeing any real data, providing a better initialization point than the general-purpose knowledge of the original PTM.

### Mechanism 3: Semantic Clustering via Multilingual Embeddings
Clustering multilingual sentence embeddings is an effective way to generate initial pseudo-labels for unlabeled text in low-resource languages. A pre-trained multilingual sentence transformer encodes unlabeled text into dense vector representations, and K-means partitions these vectors into groups, treating the resulting cluster assignments as pseudo-labels for the intermediate training stage.

## Foundational Learning

- **Concept: Semi-Supervised Learning & Pseudo-Labeling**
  - **Why needed here:** The entire Flick framework is built on using a large amount of unlabeled data to overcome the bottleneck of having very few real labels. Pseudo-labeling is the core technique for bridging this gap.
  - **Quick check question:** Can you explain the fundamental risk of self-training with pseudo-labels and why Flick introduces a "refinement" stage?

- **Concept: K-Means Clustering**
  - **Why needed here:** This is the unsupervised algorithm used to generate the initial set of pseudo-labels from the unlabeled data embeddings. Understanding its operation (centroids, assignment) is key to understanding Stage 1.
  - **Quick check question:** How does the choice of 'k' (number of clusters) influence the granularity of the pseudo-labels generated in the first stage?

- **Concept: Pre-trained Language Model (PTM) Fine-Tuning**
  - **Why needed here:** The framework relies on transferring knowledge from a general-purpose PTM (e.g., BERT, AraBERT) to a specific task. This happens twice: in the intermediate PL-FT stage and the final Cls-FT stage.
  - **Quick check question:** What is the core idea of fine-tuning a PTM, and how does Flick's two-stage process modify the standard fine-tuning approach?

## Architecture Onboarding

- **Component map:** Text -> `paraphrase-multilingual-MiniLM-L12-v2` (Embedding Model) -> K-Means (Clustering Engine) -> Initial Pseudo-Labels -> `AraBERTv2` (Intermediate PTM) -> Top-k Cluster Selection (Refinement Module) -> PL-FT Model -> Cls-FT Model (Final Classifier)

- **Critical path:**
  1. **Data Preparation:** Split data into the small labeled set (e.g., 100 samples) and the large unlabeled set.
  2. **Stage 1 (Pseudo-Label Generation):** Embed the unlabeled data with the sentence transformer. Run K-means (k=20) to get pseudo-labels for all unlabeled data.
  3. **Stage 2 (Refinement & PL-FT):**
      a. Split pseudo-labeled data into internal train/test (e.g., 25%/75%).
      b. Fine-tune the intermediate PTM on internal train, evaluate on internal test.
      c. Rank clusters by accuracy, select top-k (e.g., 15).
      d. Retrain the intermediate PTM on *only* the data from the top-k clusters. This is your PL-FT model.
  4. **Stage 3 (Cls-FT):** Initialize a new classifier with the PL-FT model's weights. Fine-tune this model on the small set of *real* labeled data. This is your final model for inference.

- **Design tradeoffs:**
  - **Number of clusters (k):** A low `k` creates broad, potentially noisy clusters. A high `k` creates more specific clusters but risks having too few samples per cluster for reliable evaluation. Paper uses k=20.
  - **Top-k selection:** Selecting more clusters (higher `k` for selection) retains more training data but risks including noisier clusters. Selecting fewer is safer but reduces the dataset size for the crucial PL-FT stage. Paper uses top-15.
  - **Embedding model:** The chosen sentence transformer must be multilingual and effective for the target low-resource language.

- **Failure signatures:**
  - **PL-FT accuracy is random chance:** The embedding model fails to capture semantics for the language, leading to meaningless clusters.
  - **Cls-FT performance degrades:** The pseudo-labels were systematically biased, and the PL-FT stage led the model into a poor state that couldn't be corrected by the few real labels.
  - **Highly imbalanced cluster sizes:** K-means may create one giant cluster and many tiny ones, making the accuracy-based ranking unstable.

- **First 3 experiments:**
  1. **Ablation on Refinement:** Run the full Flick pipeline vs. a version that skips the Top-k selection and uses all pseudo-labels. Compare final accuracy to quantify the benefit of the refinement component.
  2. **Sensitivity to Cluster Count (k):** Run Stage 1 with different values of k (e.g., 10, 20, 30) while keeping the Top-k selection fixed (e.g., at 15). Observe how the granularity of initial clustering affects the quality of the selected pseudo-labels.
  3. **Language Model Baseline:** Compare Flick's final performance against a standard baseline of directly fine-tuning the base PTM (e.g., AraBERTv2) on the same small set of real labels, without any intermediate pseudo-labeling steps. This establishes the performance gain from the entire framework.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the Flick framework maintain its performance advantages when applied to grammatically distinct low-resource languages such as Pashto or Swahili? The paper explicitly lists these as future work.
- **Open Question 2:** How can the number of initial clusters ($k$) be optimized adaptively without incurring the high computational costs of non-parametric methods? The authors identify this as a significant challenge in the Limitations section.
- **Open Question 3:** To what extent does the pseudo-label refinement process inadvertently amplify or mitigate societal biases present in the limited labelled seed data? The paper flags bias propagation as a critical, unmeasured risk in the Ethical Considerations section.

## Limitations
- The framework's reliance on cluster accuracy as a proxy for pseudo-label quality is a significant vulnerability if the embedding model poorly captures semantic relationships.
- The claim of generalizability across four low-resource languages rests on a limited experimental set (14 datasets), which may not capture the full diversity of linguistic challenges.
- The refinement process's dependency on having a sufficient number of samples per cluster for reliable evaluation is a concern; highly imbalanced clusters could destabilize the accuracy-based ranking.

## Confidence
- **High Confidence:** The general two-stage architecture (pseudo-label generation → intermediate fine-tuning → final fine-tuning) is a sound approach for leveraging unlabeled data in low-resource settings.
- **Medium Confidence:** The specific mechanism of top-k cluster selection for noise mitigation is theoretically justified but lacks direct empirical validation within the paper.
- **Medium Confidence:** The effectiveness of semantic clustering via multilingual embeddings is contingent on the quality of the embedding model for each low-resource language.

## Next Checks
1. **Ablation Study on Refinement:** Implement a version of Flick that skips the top-k cluster selection and uses all generated pseudo-labels for the intermediate PL-FT stage. Compare the final performance to the full Flick pipeline to quantify the exact contribution of the refinement component.
2. **Robustness to Cluster Quality:** Intentionally degrade the quality of the initial embeddings (e.g., by using a model not fine-tuned on the target language) and observe how this affects the accuracy-based cluster ranking and the final classifier's performance.
3. **Language-Specific Embedding Analysis:** For each low-resource language (Arabic, Urdu, Setswana), conduct a separate analysis of the quality of the multilingual embeddings. This could involve visualizing clusters or measuring the semantic coherence of top-performing clusters against a small set of manually labeled examples.