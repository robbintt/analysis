---
ver: rpa2
title: 'Beyond In-Distribution Performance: A Cross-Dataset Study of Trajectory Prediction
  Robustness'
arxiv_id: '2501.15842'
source_url: https://arxiv.org/abs/2501.15842
tags:
- prediction
- augmentation
- dataset
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the out-of-distribution (OoD) generalization
  of three state-of-the-art trajectory prediction models across two large-scale autonomous
  driving datasets: Argoverse 2 (A2) and Waymo Open Motion (WO). The models were trained
  on one dataset and tested on the other to assess robustness.'
---

# Beyond In-Distribution Performance: A Cross-Dataset Study of Trajectory Prediction Robustness

## Quick Facts
- **arXiv ID:** 2501.15842
- **Source URL:** https://arxiv.org/abs/2501.15842
- **Reference count:** 0
- **Primary result:** EP-Q (smallest model with polynomial inductive bias) shows best OoD generalization when trained on smaller A2 and tested on larger WO, but all models generalize poorly in reverse direction.

## Executive Summary
This study evaluates the out-of-distribution (OoD) generalization of three state-of-the-art trajectory prediction models across Argoverse 2 and Waymo Open Motion datasets. The models were trained on one dataset and tested on the other to assess robustness beyond standard in-distribution performance. The results reveal that the smallest model with the highest inductive bias (EP-Q) exhibits the best OoD generalization when trained on the smaller A2 dataset and tested on the larger WO dataset. However, when trained on the larger WO dataset and tested on the smaller A2 dataset, all models generalize poorly despite EP-Q still performing best. This unexpected asymmetry suggests that factors beyond model architecture, such as dataset properties and prediction task complexity, significantly influence OoD robustness. The findings highlight the need for careful consideration of dataset characteristics in model evaluation and development.

## Method Summary
The study compares three trajectory prediction models (FMAE, QCNet, and EP variants) across two autonomous driving datasets. Models are trained on one dataset and tested on the other to evaluate OoD generalization. A homogenization protocol aligns history length (5s), prediction horizon (4.1s), and map features across datasets. Homogeneous data augmentation is used to enhance generalization by training on all agents equally. The EP models use polynomial representations as inductive bias, while QCNet and FMAE use sequence-based outputs. Key metrics include minADEK and minFDEK for both ID and OoD evaluations.

## Key Results
- EP-Q (smallest model with polynomial inductive bias) shows best OoD generalization when trained on A2 and tested on WO
- All models generalize poorly when trained on WO and tested on A2, despite EP-Q still performing best
- Homogeneous data augmentation generally enhances robustness over focal-agent-only training
- Computational constraints forced QCNet to reduce scenario complexity, potentially affecting its OoD performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining trajectory outputs to polynomial representations improves OoD robustness compared to flexible sequence-based outputs.
- **Mechanism:** By enforcing a smooth polynomial function (inductive bias), the model capacity is effectively reduced, preventing the learning of spurious correlations or high-frequency noise present in the training data. This forces the model to prioritize underlying kinematic constraints.
- **Core assumption:** The underlying physics of vehicle motion adheres to continuity and smoothness that low-degree polynomials can approximate, and that OoD scenarios share these kinematic constraints even if visual features differ.
- **Evidence anchors:**
  - [abstract] "The results show that the smallest model with the highest inductive bias (EP-Q) exhibits the best OoD generalization..."
  - [section 2.1] "Polynomial representations restrict the kind of trajectories that can be represented... generally associated with... better generalization."
  - [corpus] Related work on "Physics-Informed Inductive Biases" supports the general principle that embedding physical constraints aids generalization in grid/data-limited settings.
- **Break condition:** If the prediction horizon requires modeling sudden, discontinuous changes (e.g., collisions, sharp swerves) that violate polynomial continuity assumptions, the model may underfit critical safety maneuvers.

### Mechanism 2
- **Claim:** Homogeneous data augmentation (training on all agents equally) generally enhances robustness over focal-agent-only training.
- **Mechanism:** Training on "non-focal" agents exposes the model to a broader distribution of motion behaviors and reduces overfitting to the specific statistical biases of the curated "focal" agent selection (which are often manually mined for complexity).
- **Core assumption:** The motion dynamics of non-focal agents provide a representative sample of the general motion distribution required for OoD scenarios.
- **Evidence anchors:**
  - [abstract] "...factors beyond model architecture, such as dataset properties... significantly influence OoD robustness."
  - [section 2.2] "This approach [homogeneous] ensures a consistent prediction task for all agents and enhanced model generalization."
  - [section 4.3.1] "...EP-Q align with this expectation... decreases... compared to EP-noAug."
  - [corpus] Corpus evidence on "Dataset Design Influences" aligns with the notion that feature selection and diversity impact performance.
- **Break condition:** If computational constraints (as seen with QCNet in Section 4.1) force a reduction in scenario complexity (number of agents/map elements) to accommodate homogeneous training, the benefits may be negated by the loss of context.

### Mechanism 3
- **Claim:** OoD generalization fails when the training dataset (Source) has lower task complexity or noise levels than the target dataset (Target).
- **Mechanism:** The paper hypothesizes an asymmetry: Training on "clean/easy" data (Waymo) does not prepare the model for "noisy/hard" data (Argoverse 2). The model learns a simpler decision boundary that is violated by the higher variance and complexity of the target domain.
- **Core assumption:** The "Domain Shift" is directional; it is easier to generalize from hard->easy (or noisy->clean) than from easy->hard.
- **Evidence anchors:**
  - [abstract] "...all models generalize poorly [when trained on WO and tested on A2]... suggests that factors beyond model architecture... significantly influence OoD robustness."
  - [section 4.3.3] "WO features lower noise levels... When the test data exhibits higher noise levels... methods have only a minimal improvement."
  - [corpus] "Long-Tail Compositional Zero-Shot Generalization" (neighbor paper) emphasizes that robustness requires handling rare/high-variance scenarios often missing in standard training.
- **Break condition:** If the "difficulty" gap is caused by labeling errors in the target dataset rather than legitimate domain variance, this mechanism describes overfitting to "clean" errors rather than valid features.

## Foundational Learning

- **Concept: Inductive Bias**
  - **Why needed here:** The core finding is that a *smaller* model (EP-Q) outperforms larger models (QCNet) in OoD settings specifically because of its strong inductive bias (polynomial constraints). Learners must understand that reducing model flexibility can paradoxically improve robustness by enforcing physical rules.
  - **Quick check question:** Does adding more parameters always improve OoD robustness according to this study?

- **Concept: Distribution Shift / Domain Adaptation**
  - **Why needed here:** The study is a direct evaluation of robustness under distribution shift (Train on A2 → Test on WO, and vice versa). Understanding that In-Distribution (ID) performance is a poor proxy for OoD performance is central to the paper's thesis.
  - **Quick check question:** Why does the paper argue that standard competition leaderboards (ID testing) are insufficient for verifying true model generalization?

- **Concept: Data Homogenization Protocol**
  - **Why needed here:** To compare models across datasets (Argoverse vs. Waymo), the authors had to align history length, prediction horizon, and map features. Without understanding this preprocessing, the results cannot be replicated or compared to standard benchmarks.
  - **Quick check question:** Why did the authors standardize the history length to 5 seconds, and how might this affect the perceived complexity of the Waymo dataset?

## Architecture Onboarding

- **Component map:** Input Layer (Agent History + Map Elements) → Augmentation Module (Homogeneous vs. Heterogeneous vs. None) → Encoder (varies by model) → Output Layer (Sequence of coordinates vs. Polynomial coefficients) → Loss Function (Weighted error)

- **Critical path:**
  1. **Homogenization:** Filter raw data to ensure history length (5s) and map consistency
  2. **Augmentation:** Select Homogeneous (for robustness) or Heterogeneous (for focal accuracy)
  3. **Representation:** Choose Polynomial (EP-Q) for OoD robustness or Sequence (QCNet/FMAE) for ID flexibility
  4. **Evaluation:** Run Cross-Dataset test (Train Source → Test Target) immediately after ID validation

- **Design tradeoffs:**
  - **Capacity vs. Robustness:** Large models (7600k params) win ID tests; Small models (345k params) with polynomial bias win OoD tests
  - **Complexity vs. Memory:** Homogeneous augmentation on large models (QCNet) caused OOM errors, requiring reduced scenario complexity (capped agents), which potentially harmed its OoD performance

- **Failure signatures:**
  - **High Error on A2 (when trained on WO):** Model fails to handle higher noise/variance; indicates overfitting to the "cleaner" Waymo distribution
  - **OOM during Training:** Occurs when using homogeneous augmentation on large sequence-based models without capping scenario complexity

- **First 3 experiments:**
  1. **Sanity Check (ID):** Train EP-Q and QCNet on Waymo, test on Waymo validation. Verify that while QCNet might score slightly higher, EP-Q remains competitive (validating the polynomial capacity is sufficient)
  2. **Asymmetric OoD Test:** Train on Waymo, Test on Argoverse 2. Observe the "poor generalization" phenomenon where even robust models struggle due to the "Easy->Hard" domain shift
  3. **Ablation on Inductive Bias:** Compare EP-Q (Polynomial) vs. QCNet (Sequence) in the A2→WO direction. Verify that EP-Q shows significantly lower relative error increase (Δ minADE) compared to the sequence-based baseline

## Open Questions the Paper Calls Out

- **Open Question 1:** Does increasing the history length in the Waymo Open Motion (WO) dataset effectively reduce prediction task complexity by shifting challenging behaviors into the observed history, thereby degrading OoD generalization?
  - **Basis in paper:** [explicit] Section 4.3.3 proposes the "Complexity of Prediction Task" hypothesis, suggesting that extending WO history to 5s makes prediction tasks "less complex" than in A2. The authors state, "We leave this as an open question for future investigation."
  - **Why unresolved:** The authors note that dataset properties (history length vs. scenario difficulty) are confounded in existing benchmarks, making it challenging to isolate variables for a definitive explanation.
  - **What evidence would resolve it:** Controlled experiments using datasets where history length and prediction horizon difficulty are varied independently to measure their specific impact on generalization.

- **Open Question 2:** How does the asymmetry in noise levels between training and testing datasets influence the efficacy of data augmentation and polynomial representations?
  - **Basis in paper:** [explicit] Section 4.3.3 hypothesizes that "Noise Level in Datasets" explains why models trained on cleaner data (WO) fail to generalize to noisier data (A2). The authors explicitly list validating this hypothesis as a direction for future research.
  - **Why unresolved:** Noise characteristics are intrinsic to the data collection sensors and pipelines of A2 and WO, preventing direct control or manipulation of these variables within the current study.
  - **What evidence would resolve it:** Experiments involving controlled noise injection into synthetic or clean datasets to evaluate model robustness under varying train-test noise differentials.

- **Open Question 3:** Does the restriction of scenario complexity (e.g., limiting the number of agents or map elements) negate the OoD generalization benefits typically associated with homogeneous data augmentation?
  - **Basis in paper:** [inferred] Section 4.3.1 observes that QCNet exhibited worse robustness with homogeneous augmentation compared to no augmentation. The text attributes this to the "limited scenario complexity" required to fit the model in memory (Section 4.1), implying the relationship between model size, context limits, and augmentation success is not fully understood.
  - **Why unresolved:** The study could not test QCNet without complexity limits due to hardware constraints ("out of memory" issues), making it unclear if the augmentation strategy or the reduced context caused the performance drop.
  - **What evidence would resolve it:** Ablation studies on large models like QCNet using hardware sufficient to process full scenario complexity, thereby isolating the effect of context truncation on augmentation strategies.

## Limitations
- Reliance on polynomial representations as the sole inductive bias mechanism for improving OoD robustness
- Computational constraints forced QCNet to reduce scenario complexity, potentially confounding comparisons
- Limited exploration of other forms of inductive bias that could achieve similar or better OoD performance

## Confidence
- **High Confidence:** The general finding that smaller models with stronger inductive biases outperform larger models in OoD settings when trained on the smaller dataset
- **Medium Confidence:** The claim that dataset properties beyond architecture significantly influence OoD robustness
- **Low Confidence:** The directional hypothesis about Easy→Hard domain shifts being harder to generalize across

## Next Checks
1. **Architecture Ablation:** Test whether other forms of inductive bias (e.g., physics-based constraints on acceleration) in larger models can match or exceed EP-Q's OoD performance
2. **Dataset Property Analysis:** Conduct a systematic study isolating noise levels, task complexity, and label quality to determine which factors most strongly influence the Easy→Hard generalization gap
3. **Computational Parity Test:** Train QCNet with homogeneous augmentation at full scenario complexity (if hardware permits) to determine whether the performance gap is due to architectural differences or computational constraints affecting training data exposure