---
ver: rpa2
title: 'LLM Microscope: What Model Internals Reveal About Answer Correctness and Context
  Utilization'
arxiv_id: '2510.04013'
source_url: https://arxiv.org/abs/2510.04013
tags:
- context
- answer
- arxiv
- preprint
- lens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using model internals to predict output correctness
  and assess external context efficacy in LLMs. The authors train classifiers on intermediate
  layer activations to estimate answer correctness with over 75% accuracy, outperforming
  prompting baselines.
---

# LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization

## Quick Facts
- **arXiv ID**: 2510.04013
- **Source URL**: https://arxiv.org/abs/2510.04013
- **Reference count**: 40
- **Primary result**: Model internals predict output correctness with 75% accuracy and outperform prompting baselines for context quality assessment

## Executive Summary
This paper introduces a framework for using model internals to predict answer correctness and assess external context efficacy in large language models. By training classifiers on intermediate layer activations of the first output token, the authors demonstrate that hidden states encode signals about factual accuracy with over 75% accuracy, enabling early auditing before generation completes. They also develop a novel internals-based metric (Ψ) that significantly outperforms prompting baselines in distinguishing between correct, incorrect, and irrelevant context by measuring the tradeoff between parametric knowledge and context reliance.

## Method Summary
The method extracts features from intermediate layer activations of the first output token, including raw hidden states, Logit Lens-derived entropy/rank metrics, and parametric knowledge scores. These features train Random Forest classifiers to predict binary correctness labels. For context efficacy, the framework computes Parametric Knowledge Score (PKS) and External Context Score (ECS) to form a combined metric Ψ = ECS - λ·PKS. The approach is validated on TriviaQA and MMLU datasets using six different LLM architectures, comparing internals-based predictions against prompting baselines and analyzing feature importance across layers.

## Key Results
- Random forest classifiers trained on first-token hidden states achieve ~75% accuracy in predicting answer correctness, outperforming prompting baselines (barely above 50%)
- The internals-based Ψ metric significantly outperforms prompting for context quality discrimination (85.5% vs ~20% for LLaMA 3 8B)
- Hidden states alone perform nearly as well as more complex Logit/Tuned Lens features for correctness prediction
- Reliability diagrams show prompting baselines are systematically overconfident while internals-based predictions are better calibrated

## Why This Works (Mechanism)

### Mechanism 1: Early-Layer Encoding of Correctness Signals
The first token's hidden states encode a signal about final answer correctness, enabling prediction before generation completes. A random forest classifier trained on these features learns to map the model's internal state at the onset of generation to factual accuracy. This works because the initial representational commitment correlates with the eventual output's correctness.

### Mechanism 2: Parametric Knowledge vs. Context Reliance Tradeoff
Context efficacy is discriminated by measuring the tradeoff between parametric knowledge (FFN influence) and context reliance (attention-based information from retrieved documents). The Ψ metric combines these orthogonal sources, where high values indicate strong context reliance and low values suggest parametric dominance or confusion from misleading context.

### Mechanism 3: Superiority of Internals over Self-Reported Confidence
Hidden states contain a more reliable uncertainty signal than the model's verbalized confidence. The internals-based approach bypasses the model's miscalibrated verbalized confidence and directly probes the underlying representational state, which retains a more accurate signal of uncertainty about correctness.

## Foundational Learning

- **Logit Lens and Tuned Lens**: Tools to decode intermediate hidden states into vocabulary space, enabling feature extraction like entropy and token rank before final layer. *Quick check*: How does the Logit Lens allow inspection of model predictions at intermediate layers?

- **Random Forest Classifier for Interpretability**: Chosen over neural networks to preserve feature interpretability when analyzing which internal signals predict correctness. *Quick check*: Why might random forest be preferred over MLP for understanding feature importance?

- **Retrieval-Augmented Generation (RAG) Efficacy**: Framework for evaluating whether retrieved context is useful versus merely present during generation. *Quick check*: Why is it challenging to determine if a retrieved document was actually useful in RAG systems?

## Architecture Onboarding

- **Component map**: Input Question/Context -> First Token Generation -> Extract Hidden States/Logit Lens Features -> Train/Apply Random Forest Classifier -> Predict Correctness/Context Efficacy
- **Critical path**: For correctness prediction: Input -> First Token Generation -> Extract Hidden States/Logit Lens Features -> Train/Apply Random Forest
- **Design tradeoffs**: Random Forest prioritizes interpretability over potential accuracy gains from neural models; first-token focus enables early auditing but may miss multi-token reasoning signals
- **Failure signatures**: ECS cannot be computed for Gemma 2 9B due to sliding-window attention; prompting baselines may unexpectedly outperform if feature extraction is incorrect; PKS performs poorly on single-token prediction tasks
- **First 3 experiments**: 1) Implement prompting baselines to establish low confidence performance; 2) Extract first-token hidden states and train classifiers per layer to verify accuracy improvement; 3) Compute Ψ metric for context discrimination and compare against prompting baselines

## Open Questions the Paper Calls Out

- Whether predictive power of early-layer activations generalizes to long-form generation and complex reasoning tasks
- How post-training alignment processes alter encoding of confidence signals compared to pre-trained base models
- Whether internal detection of misleading context can be utilized to actively steer models away from generating incorrect answers

## Limitations

- Analysis restricted to short-answer factual question-answering; generalizability to reasoning tasks unknown
- Context manipulation relies on GPT-4o with unspecified prompts, potentially introducing model-specific artifacts
- ECS metric cannot be computed for Gemma 2 9B due to architectural differences in attention mechanism
- Random forest approach may miss complex nonlinear relationships that neural classifiers could capture

## Confidence

- **High Confidence**: Model internals predict output correctness with ~75% accuracy across multiple models and datasets
- **Medium Confidence**: First-token activations encode sufficient signal for correctness prediction, though multi-token contributions are uncertain
- **Medium Confidence**: Ψ metric for context discrimination shows strong results but lacks extensive corpus validation
- **Low Confidence**: Internals clearly outperform prompting, but alternative prompting strategies might close the performance gap

## Next Checks

1. Test ECS-based context discrimination across different attention architectures (RWKV, Mamba) to validate the orthogonality assumption
2. Extend correctness prediction beyond first token to examine predictive power for multi-token answers and reasoning tasks
3. Implement and compare multiple prompting strategies for self-reported confidence against internals-based approach