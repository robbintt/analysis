---
ver: rpa2
title: Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch
  Audio and Motion Sensing
arxiv_id: '2507.12002'
source_url: https://arxiv.org/abs/2507.12002
tags:
- audio
- data
- smartwatch
- inertial
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal framework for detecting in-person
  conversations using smartwatch audio and motion data. The method combines acoustic
  and inertial sensing to capture both verbal cues and non-verbal gestures in conversations,
  addressing the challenge of detecting conversations in noisy real-world environments.
---

# Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing

## Quick Facts
- arXiv ID: 2507.12002
- Source URL: https://arxiv.org/abs/2507.12002
- Reference count: 40
- Achieved 82.0±3.0% macro F1-score in lab settings and 77.2±1.8% in semi-naturalistic environments

## Executive Summary
This paper presents a multimodal framework for detecting in-person conversations using smartwatch audio and motion data. The method combines acoustic and inertial sensing to capture both verbal cues and non-verbal gestures in conversations, addressing the challenge of detecting conversations in noisy real-world environments. The framework achieves 82.0±3.0% macro F1-score in lab settings and 77.2±1.8% in semi-naturalistic environments. The fusion of audio and inertial data significantly improves performance over single-modality approaches, especially in challenging acoustic conditions.

## Method Summary
The framework processes synchronized audio and IMU data from commodity smartwatches using separate neural pathways. Audio is transformed into FFT spectrograms (128 frequency bins, 500ms windows) processed through CNN→LSTM to capture foreground speech and turn-taking patterns. IMU data is converted to energy-per-channel spectrograms via STFT and processed through CNN+Attention. The embeddings from both modalities are concatenated before final classification. The system uses 30-second windows and is evaluated via LOGO cross-validation on 14.6 hours of data from 35 participants across 11 groups.

## Key Results
- Multimodal fusion achieves 82.0±3.0% macro F1-score in lab settings, a 7.6 percentage point improvement over best single modality
- Performance remains robust at 77.2±1.8% macro F1-score in semi-naturalistic environments
- 2kHz audio sampling provides similar performance to 16kHz with better privacy and battery efficiency
- System can be deployed on commodity smartwatches with inference times under 1 second

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio spectrogram features can detect foreground speech patterns and conversational turn-taking, providing the verbal dimension of conversation detection.
- Mechanism: FFT-based spectrograms (128 frequency bins, 500ms windows) are fed through a CNN to extract foreground speech embeddings, then through an LSTM to capture temporal speaker turn patterns. The architecture jointly models speech presence and turn-changing behavior.
- Core assumption: Conversations are characterized by both continuous foreground speech and regular turn exchanges between speakers.
- Evidence anchors:
  - [abstract]: "capture both verbal cues and non-verbal gestures in conversations"
  - [section 5.2]: "the audio spectrograms are passed through a CNN... to infer the presence of foreground speech. These foreground speech embeddings... are used as input features to a LSTM network to then capture the presence of foreground speaker turn changes."
- Break condition: When background noise levels exceed foreground speech by significant margins (>70dBA environments), or when audio is downsampled below 2kHz where speech intelligibility degrades substantially.

### Mechanism 2
- Claim: IMU energy patterns capture distinctive non-verbal gestures (hand movements, wrist articulation) that co-occur with active conversation participation.
- Mechanism: Raw 6-axis IMU data (55Hz) is transformed into energy-per-channel spectrograms via STFT (32 frequency bins, 400ms resolution). These image-like representations capture how hand/wrist motion energy distributions differ between conversing, listening, and non-conversation states.
- Core assumption: Participants gesture differently when engaged in turn-taking conversation versus passive listening or non-social activities, and these gestural patterns are consistent enough across individuals to be learnable.
- Evidence anchors:
  - [abstract]: "non-verbal gestures in conversations"
  - [section 5.1.3]: "We then calculated the energy per channel from the STFT... to capture information on hand and wrist movements as they vary throughout social activities."
- Break condition: When participants wear watch on dominant hand (model trained mostly on non-dominant hand data—17/22 participants), or during activities with confounding motion (eating, walking) that overlap with conversational gesture patterns.

### Mechanism 3
- Claim: Late fusion of audio and inertial embeddings via concatenation improves robustness in acoustically challenging conditions where single modalities fail.
- Mechanism: Audio and IMU are processed through separate neural pathways (Pure-Acoustic CNN+LSTM and CNN+Attention respectively). Their learned embeddings are concatenated before a final classification head, allowing the model to leverage complementary information—IMU provides signal when audio is corrupted by noise, audio provides signal when motion is ambiguous.
- Core assumption: The information gaps in each modality are non-overlapping and can be compensated by the other.
- Evidence anchors:
  - [abstract]: "The fusion of audio and inertial data significantly improves performance over single-modality approaches, especially in challenging acoustic conditions."
  - [section 6.2]: "Fusing the embeddings extracted from the Pure-Acoustic Model... and from the CNN+Attention architecture... achieves the best F1-score of 82.0±3.0%... representing a 7.6%-point improvement in F1-score over the best single-modality classifier."
  - [section 7.2]: "The addition of the inertial modality is most beneficial to the music in background context, increasing the absolute mean value of the weighted F1-score of the audio-only model by 10.9%."
- Break condition: When attention-based fusion is used instead of concatenation (Table 3 shows self-attention fusion performs worse than single-modality), or when domain shift occurs (outdoor environments, dominant-hand wearers not in training distribution).

## Foundational Learning

- Concept: **Spectrogram representations for time-series sensor data**
  - Why needed here: Both audio (FFT spectrograms) and IMU (STFT energy spectrograms) are converted to image-like representations for CNN processing.
  - Quick check question: Can you explain why a 500ms FFT window with 250ms stride creates a (128×120) spectrogram for a 30-second audio segment?

- Concept: **Late fusion vs. early fusion in multimodal learning**
  - Why needed here: The paper explicitly compares concatenation (representation-level), self-attention (representation-level), and softmax averaging (score-level) fusion strategies.
  - Quick check question: Why would concatenating embeddings outperform attention-based fusion in this specific task?

- Concept: **Macro F1-score for imbalanced multi-class classification**
  - Why needed here: Conversation/other-speech/noise classes are imbalanced; macro F1 ensures equal weighting per class.
  - Quick check question: How does macro F1 differ from weighted F1 when evaluating rare classes like "conversation while eating"?

## Architecture Onboarding

- Component map: Raw audio (16kHz) → FFT (500ms/250ms, 128 bins) → Spectrogram → CNN → Foreground embeddings → LSTM → Audio embeddings; Raw IMU (55Hz, 6-axis) → Standardize → Frame (2s, 1s overlap) → STFT energy (32 bins, 400ms) → IMU energy frames → CNN+Attention → IMU embeddings; [Audio embeddings; IMU embeddings] → Concatenate → FC layers → Softmax → {conversation, other_speech, noise}

- Critical path: 1) Synchronized data collection (audio/IMU alignment via clap sync) 2) Per-modality preprocessing (FFT for audio, STFT-energy for IMU) 3) Separate modality-specific feature extractors 4) Embedding concatenation (not attention) 5) Joint classification head

- Design tradeoffs: Window length: 30s optimal (better than 10-20s) but reduces detection granularity; IMU frame size: 2s optimal for multimodal, 9s for IMU-only; Sampling rate: 4kHz audio performs similarly to 16kHz with better privacy and battery; 1kHz degrades performance; Fusion strategy: Concatenation outperforms self-attention and softmax averaging; Hardware: Attention mechanism unsupported on Fossil Gen 4/5; requires newer devices (Pixel Watch 2)

- Failure signatures: Low F1 on outdoor groups (Groups 8, 11): ~62-66% vs. average 75-80%; Confusion between "other speech" and "conversation" when background speech present; Dominant-hand wearers show degraded performance (model trained on non-dominant data); Pre-trained models from quiet home environments fail on noisy public spaces (52.3% vs. 68.7%)

- First 3 experiments: 1) Reproduce single-modality baselines: Train Pure-Acoustic model and CNN+Attention IMU model separately on the lab+semi-naturalistic dataset. Verify you achieve ~73% (audio) and ~51% (IMU) macro F1 before attempting fusion. 2) Ablate fusion strategies: Compare concatenation vs. self-attention vs. softmax averaging on the same LOGO split. Confirm concatenation yields 3-7% absolute improvement over best single modality. 3) Test domain shift robustness: Train on lab-only data, evaluate on semi-naturalistic holdout groups. Measure performance gap and identify which activity contexts (eating, music background) degrade most.

## Open Questions the Paper Calls Out

- How does the framework perform when deployed in completely unsupervised, "in-the-wild" settings where participants are not following a script? The current evaluation relies on semi-naturalistic data collected during specific, scripted activities, which may not capture the full randomness of real-world behavior.

- To what extent does multitasking (e.g., walking, driving) while conversing create false positives or confusion in gesture detection? The collected dataset focused on stationary or specific social activities, and does not contain significant data on locomotion combined with conversation.

- Does the model generalize across different age demographics

## Limitations

- Data collection bias: The dataset predominantly includes non-dominant hand wearers (17/22 participants), limiting generalizability to dominant-hand wearers who may exhibit different gesture patterns.

- Activity scope: The evaluation focuses on scripted social activities (eating, watching TV, music background) rather than completely unsupervised "in-the-wild" scenarios, potentially overestimating real-world performance.

- Domain shift vulnerability: Models trained on lab/semi-naturalistic data show significant performance degradation when applied to outdoor environments or different demographic groups, indicating limited robustness to environmental and population variations.

- Hardware constraints: The attention mechanism used in IMU processing requires newer smartwatch hardware (Pixel Watch 2), limiting deployment on older devices like Fossil Gen 4/5.

- Sampling rate tradeoff: While 2kHz audio provides good performance with better privacy and battery efficiency than 16kHz, the system still requires careful consideration of privacy implications when processing audio data.

- Multitasking scenarios: The dataset does not include substantial data on activities like walking or driving while conversing, which could create false positives or confusion in gesture detection.

## Confidence

Medium. The paper presents a well-structured multimodal approach with clear performance improvements over single-modality baselines. The ablation studies on fusion strategies and sampling rates provide strong empirical support. However, the reliance on scripted activities, potential dataset biases (hand dominance, demographic homogeneity), and the gap between semi-naturalistic and truly "in-the-wild" performance warrant cautious interpretation of the claimed robustness.

## Next Checks

- Verify the LOGO cross-validation setup: Ensure group 8 and 11 (outdoor environments) are properly held out and that the 62-66% F1 scores represent true domain shift rather than data leakage.

- Replicate the fusion ablation study: Train and compare concatenation, self-attention, and softmax averaging fusion on the exact same data splits to confirm the 3-7% absolute improvement of concatenation over single modalities.

- Test dominant-hand generalization: Train a model exclusively on non-dominant hand data and evaluate on dominant-hand wearers to quantify the performance drop and assess the feasibility of domain adaptation techniques.