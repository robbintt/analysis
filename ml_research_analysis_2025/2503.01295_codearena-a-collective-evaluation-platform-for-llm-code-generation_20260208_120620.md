---
ver: rpa2
title: 'CodeArena: A Collective Evaluation Platform for LLM Code Generation'
arxiv_id: '2503.01295'
source_url: https://arxiv.org/abs/2503.01295
tags:
- code
- problem
- evaluation
- codearena
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodeArena, an online evaluation framework
  designed to address limitations in existing LLM code generation benchmarks, including
  benchmark leakage, data dissipation, and limited system accessibility. The core
  innovation is a collective evaluation mechanism that dynamically recalibrates individual
  model scores based on overall system performance, effectively mitigating score biases
  from benchmark contamination.
---

# CodeArena: A Collective Evaluation Platform for LLM Code Generation

## Quick Facts
- **arXiv ID:** 2503.01295
- **Source URL:** https://arxiv.org/abs/2503.01295
- **Reference count:** 9
- **Primary result:** Introduces a collective evaluation framework that dynamically recalibrates scores based on overall system performance to mitigate benchmark leakage biases.

## Executive Summary
CodeArena addresses critical limitations in LLM code generation evaluation including benchmark leakage, data dissipation, and limited system accessibility. The platform introduces a novel collective evaluation mechanism that dynamically adjusts individual model scores based on overall system performance, effectively neutralizing the impact of widely leaked problems. By providing open access to all submitted solutions and test cases, along with automation-friendly APIs, CodeArena enables fair, real-time benchmarking of both open-source and closed-source LLMs.

## Method Summary
The platform is built on DMOJ and uses a unified one-shot prompt with temperature 0.7 for inference. Solutions are executed in a sandbox environment with resource limits, and scores are dynamically calculated using Challenge Score (based on acceptance rate) and Efficiency Score (runtime percentile). Problems are sourced from APPS, Mercury, and LeetCode/CodeForces, with test cases generated via GPT-4o. The system automatically updates rankings as new submissions are processed through REST APIs and a Python library.

## Key Results
- Closed-source models generally outperform open-source models in code generation tasks
- DeepSeek-Coder-V2-Lite achieves competitive performance among open-source models
- Dynamic scoring system effectively minimizes the impact of leaked problems on rankings
- Platform provides fair and unbiased assessments through collective normalization

## Why This Works (Mechanism)

### Mechanism 1: Leakage Dampening via Collective Normalization
The Challenge Score (CS) uses $CS_i = BPS_i \times (1 - AC_i)$, where $AC_i$ is the acceptance rate. When all models solve a leaked problem ($AC \approx 1$), the points awarded approach zero, forcing differentiation on uncontaminated problems.

### Mechanism 2: Efficiency-Weighted Differentiation
The Efficiency Score (ES) calculates runtime percentile against all successful solutions, preventing models from optimizing solely for correctness through brute-force approaches.

### Mechanism 3: Continuous Evaluation via Automation APIs
REST APIs and Python library enable programmatic submission and retrieval, creating a feedback loop for immediate evaluation of new problems before they enter training datasets.

## Foundational Learning

- **Concept: Item Response Theory (IRT) / Relative Grading**
  - Why needed: The Challenge Score is a form of relative grading where point value depends on how many others also solved correctly
  - Quick check: If a 10-point problem is solved by 10 models, how many points does each receive? (Answer: Near zero in this system)

- **Concept: Sandboxing and Isolation**
  - Why needed: The Runtimes Layer executes untrusted code, requiring containerization/isolation understanding
  - Quick check: Why can't the Code Generator read test case files from disk during execution?

- **Concept: Benchmark Contamination**
  - Why needed: The premise relies on static benchmarks becoming invalid once seen by LLMs during training
  - Quick check: Why does 100% accuracy on a leaked benchmark fail to prove a model is "smart"?

## Architecture Onboarding

- **Component map:** API Layer -> Runtimes Layer -> Dynamic Evaluation Layer -> Data Layer
- **Critical path:** 1) POST /api/submission, 2) Runtime Layer executes code, 3) Dynamic Evaluation Layer updates global stats and recalculates DP, 4) GET /api/ranking reflects new standing
- **Design tradeoffs:** GPT-4o test generation is scalable but risks logical errors; open data promotes research but creates potential new training datasets
- **Failure signatures:** "Accepted" but low DP indicates easy/leaked problem; submission ID delay suggests Runtime Layer overload
- **First 3 experiments:** 1) Submit trivial "Hello World" to verify sandbox returns "Wrong Answer", 2) Track score decay as 10 users solve a problem, 3) Compare O(NÂ²) vs O(N) solutions' Efficiency Score deltas

## Open Questions the Paper Calls Out
None

## Limitations
- Collective scoring may be insufficient if difficult problems are leaked to only a subset of models
- Runtime efficiency measurements may vary across different execution environments
- GPT-4o generated test cases may contain logical inconsistencies or be trivially weak

## Confidence
- **High Confidence:** Platform architecture and API design are well-specified and implementable
- **Medium Confidence:** Effectiveness of leakage mitigation depends on assumptions about contamination patterns
- **Low Confidence:** Runtime efficiency measurements and their consistency across environments require further validation

## Next Checks
1. **Controlled Leakage Experiment:** Split models into groups, give problem to only one group during training, evaluate all models, and verify collective scoring appropriately reduces DP for trained group
2. **Runtime Consistency Test:** Run identical solutions multiple times under varying loads to measure execution time variance and verify stable percentile-based scoring
3. **Test Case Validation:** Submit human-written "gold standard" solutions to GPT-4o generated test cases and analyze false negatives/positives to quantify reliability