---
ver: rpa2
title: Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via
  Influence Functions
arxiv_id: '2505.19949'
source_url: https://arxiv.org/abs/2505.19949
tags:
- reasoning
- math
- influence
- code
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper uses influence functions to attribute the reasoning\
  \ performance of LLMs on math and coding tasks to individual training examples,\
  \ sequences, and tokens. The method reveals that cross-domain data\u2014especially\
  \ high-difficulty math and low-difficulty code examples\u2014can significantly improve\
  \ reasoning performance across both domains."
---

# Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions

## Quick Facts
- arXiv ID: 2505.19949
- Source URL: https://arxiv.org/abs/2505.19949
- Reference count: 40
- Primary result: Cross-domain data—especially high-difficulty math and low-difficulty code—improves reasoning performance across both domains

## Executive Summary
This paper investigates which data attributes stimulate mathematical and coding reasoning in large language models by using influence functions to attribute performance to individual training examples, sequences, and tokens. The authors analyze Qwen2.5-7B-Instruct on math (AIME24) and coding (LiveCodeBench) tasks, revealing that cross-domain influences are significant: high-difficulty math examples aid code reasoning, while low-difficulty code examples benefit math reasoning. The study also identifies that sequence-level exploration behaviors—searching for alternative approaches after reaching a correct answer—positively impact reasoning performance, contrary to prior assumptions. Token-level analysis shows distinct patterns between domains, with math favoring natural language logical connectors and code relying more on structural syntax markers. A simple dataset reweighting strategy based on these insights improves performance by 10% on AIME24 and 1.5% on LiveCodeBench.

## Method Summary
The authors use influence functions to measure how individual training examples, sequences, and tokens affect model performance on downstream reasoning tasks. They approximate the Hessian using only MLP parameters while treating attention as fixed (EK-FAC). The analysis examines cross-domain influences, sequence-level exploration behaviors, and token-level attribution patterns between math and coding tasks. They validate findings through dataset reweighting experiments that flip task difficulty distributions.

## Key Results
- Cross-domain data significantly influences reasoning performance: high-difficulty math examples aid code reasoning, while low-difficulty code examples benefit math reasoning
- Sequence-level exploration behaviors—searching for alternative approaches after reaching a correct answer—positively impact reasoning performance
- Dataset reweighting based on difficulty-flipping improves AIME24 accuracy from 10% to 20% and LiveCodeBench accuracy from 33.8% to 35.3% for Qwen2.5-7B-Instruct

## Why This Works (Mechanism)
The paper demonstrates that formal reasoning capabilities benefit from cross-domain training data because different task types provide complementary structural and logical patterns. High-difficulty math problems enhance abstract reasoning that transfers to coding tasks, while low-difficulty code examples provide structural scaffolding that aids mathematical problem-solving. The positive effect of sequence-level exploration behaviors suggests that training examples where models search for multiple solution paths create more robust reasoning patterns than single-path approaches.

## Foundational Learning

**Influence Functions**: A method to measure how individual training examples affect model predictions by approximating the impact of removing that example from training. Needed to attribute performance to specific training data attributes rather than just overall dataset statistics. Quick check: Can compute the influence of removing a single training example on a specific test prediction.

**EK-FAC Approximation**: A technique to approximate the Hessian matrix by considering only MLP parameters while treating attention as fixed. Needed to make influence function computation tractable for large language models. Quick check: Reduces computational complexity from O(n²) to O(n) where n is the number of parameters.

**Cross-Domain Transfer Learning**: The phenomenon where training on one task type (e.g., math) improves performance on another (e.g., code). Needed to understand how different reasoning domains complement each other. Quick check: Can be measured by comparing performance on target domain with and without source domain training data.

## Architecture Onboarding

**Component Map**: Training Data -> Influence Function Computation -> Performance Attribution -> Dataset Reweighting -> Validation
**Critical Path**: Influence function computation (identifying which training examples matter) → Performance attribution (understanding why they matter) → Dataset reweighting (applying insights to improve performance)
**Design Tradeoffs**: The EK-FAC approximation trades computational efficiency for potential accuracy loss by ignoring attention curvature; cross-domain training balances domain-specific expertise against transfer benefits.
**Failure Signatures**: If cross-domain influences are negligible, the model may have learned domain-specific patterns that don't transfer; if exploration behaviors show no benefit, the model may have learned rigid solution patterns.
**3 First Experiments**:
1. Compute influence scores for individual training examples on AIME24 test set to identify most impactful examples
2. Analyze sequence-level behaviors in training data to identify exploration patterns
3. Apply dataset reweighting strategy and measure performance changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified cross-domain data influence patterns (e.g., high-difficulty math aiding code reasoning) generalize to non-formal reasoning domains like commonsense or scientific reasoning?
- Basis in paper: [explicit] The authors state in the Limitations that "extending this framework to other domains, such as commonsense reasoning, remains an open direction."
- Why unresolved: The current study is strictly confined to mathematical and coding tasks; it is unknown if the mechanisms for formal logic transfer apply to tasks requiring world knowledge or social reasoning.
- What evidence would resolve it: Applying the Infra framework to models fine-tuned on commonsense QA datasets (e.g., StrategyQA) to analyze cross-domain influences.

### Open Question 2
- Question: How does the approximation of the Hessian using only MLP parameters (while treating attention as fixed) skew the calculated influence of structural tokens versus logical connectors?
- Basis in paper: [explicit] The paper acknowledges it "approximate[s] the Hessian H by considering only the MLP parameters and treating the attention as fixed."
- Why unresolved: Attention layers are critical for managing long-range dependencies in Chain-of-Thought; ignoring their curvature in the influence calculation may misattribute the importance of specific reasoning steps.
- What evidence would resolve it: A comparative analysis of influence scores calculated via MLP-only EK-FAC versus a more computationally expensive approximation that includes attention layers.

### Open Question 3
- Question: Does the "difficulty-flipped" data reweighting strategy (hard math + easy code) remain optimal for models with significantly larger parameter counts (e.g., 70B+) or different pre-training distributions?
- Basis in paper: [inferred] The experiments are limited to 7B and 14B Qwen models. The paper suggests that code data helps structure, but larger models may require less structural scaffolding, potentially altering the optimal difficulty mix.
- Why unresolved: Data curation heuristics often scale non-linearly; a strategy that doubles accuracy on a 7B model might saturate or degrade performance on a model with inherently stronger reasoning capabilities.
- What evidence would resolve it: Replicating the dataset reweighting experiment using the "difficulty-flipped" strategy on a 70B+ parameter model and evaluating on AIME/LiveCodeBench.

## Limitations

- Results are limited to mathematical and coding tasks; generalization to other reasoning domains (commonsense, scientific) is unknown
- The EK-FAC approximation ignores attention layer curvature, potentially affecting token-level attribution accuracy
- Dataset reweighting improvements are modest (10% on AIME24, 1.5% on LiveCodeBench) and may not scale to larger models

## Confidence

**High confidence**: The general methodology of using influence functions to attribute performance to training examples is established and well-executed. The finding that high-difficulty math and low-difficulty code examples provide cross-domain benefits is supported by the analysis, though the effect size warrants cautious interpretation.

**Medium confidence**: The sequence-level behavior analysis showing that post-correct-answer exploration improves reasoning. While the data supports this claim, it relies on retrospective correlation rather than controlled experimentation to establish causation.

**Medium confidence**: The token-level attribution patterns distinguishing math and code reasoning. These findings are consistent with linguistic intuition but require validation across different tokenization schemes and model architectures.

## Next Checks

1. Replicate the influence function analysis on a different model architecture (e.g., Llama or Mistral) to verify that the observed cross-domain effects and token patterns are not specific to Qwen2.5-7B-Instruct.

2. Conduct controlled ablation experiments where post-correct-answer exploration behaviors are explicitly removed during training to directly test whether this sequence-level pattern causally improves reasoning performance.

3. Apply the dataset reweighting strategy to larger models (7B→70B scale) and measure whether the observed performance improvements scale proportionally or exhibit diminishing returns with model size.