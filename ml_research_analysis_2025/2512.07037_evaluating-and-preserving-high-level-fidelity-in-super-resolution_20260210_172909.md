---
ver: rpa2
title: Evaluating and Preserving High-level Fidelity in Super-Resolution
arxiv_id: '2512.07037'
source_url: https://arxiv.org/abs/2512.07037
tags:
- fidelity
- image
- quality
- metrics
- high-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating high-level semantic
  fidelity in super-resolution (SR) models, which is often overlooked by existing
  image quality metrics. While current SR models can generate visually appealing outputs,
  they may introduce hallucinations that change the semantic content of images.
---

# Evaluating and Preserving High-level Fidelity in Super-Resolution

## Quick Facts
- **arXiv ID**: 2512.07037
- **Source URL**: https://arxiv.org/abs/2512.07037
- **Reference count**: 40
- **Primary result**: First annotated dataset with fidelity scores for SR models; foundation vision models outperform traditional IQA metrics in capturing semantic fidelity.

## Executive Summary
This paper addresses the critical gap in evaluating high-level semantic fidelity in super-resolution models, which traditional image quality metrics fail to capture. While current SR models produce visually appealing outputs, they often hallucinate content that changes the semantic meaning of images. The authors construct the first annotated dataset with fidelity scores for diffusion-based SR models and demonstrate that foundation vision models (CLIP, BLIP, DINOv2, PE-core) significantly outperform traditional IQA metrics in capturing semantic fidelity changes. They further show that fine-tuning SR models with their proposed fidelity metric can simultaneously improve both semantic fidelity and perceptual quality, challenging the traditional perception-distortion trade-off assumption.

## Method Summary
The method constructs a fidelity dataset from KonIQ-10k images downscaled by 4× using BSRGAN degradation, then processed through 5 SR models (BSRGAN, SwinIR, StableSR, SeeSR, PASD) to generate 723 image pairs. Human annotators score semantic fidelity [0,1] with binary questions and trap filtering. Foundation vision models extract embeddings from SR-GT pairs, compute cosine similarity, and are fine-tuned to regress human scores. The SR fine-tuning integrates this as auxiliary loss: L = L_ori + α·L_HLF with α=0.01, using AdamW optimizer (lr=1e-5) for 10 epochs on COCO 2017 65k subset.

## Key Results
- Foundation vision models (DINOv2 fine-tuned) achieve SRCC up to 0.8417 and PLCC up to 0.9030 on fidelity prediction, outperforming best IQA metric (DISTS: 0.7948 SRCC, 0.8040 PLCC).
- Fine-tuning SeeSR with fidelity loss (α=0.01) improves both semantic fidelity (CLIP/BLIP/PE HLF) and perceptual quality (PSNR, SSIM, VIF, LPIPS, DISTS) across 7 of 8 metrics.
- Human fidelity scores follow U-shaped distribution, indicating categorical perception of semantic changes rather than gradual degradations.

## Why This Works (Mechanism)

### Mechanism 1
Foundation vision model embeddings capture high-level semantic fidelity more effectively than traditional IQA metrics. Pre-trained models encode semantic information in their embeddings, and computing cosine similarity between GT and SR embeddings, then fine-tuning on human-annotated fidelity scores, produces a metric that correlates strongly with human perception. Core assumption: semantic representations learned during pre-training align with human perception of fidelity violations. Evidence: DINOv2 fine-tuned achieves 0.8315 SRCC and 0.9030 PLCC, outperforming traditional metrics. Break condition: may degrade for domains outside training distribution.

### Mechanism 2
Adding high-level fidelity loss during SR fine-tuning can simultaneously improve semantic fidelity AND perceptual quality, challenging the perception-distortion trade-off. The combined loss guides the SR model to generate outputs that are both perceptually pleasing and semantically faithful. Core assumption: semantic faithfulness and perceptual quality are not inherently opposing objectives when guidance signal is well-calibrated. Evidence: SeeSR-Finetuned (α=0.01) outperforms original SeeSR on 7 of 8 metrics. Break condition: too large α suppresses generative diversity.

### Mechanism 3
Human perception of fidelity violations is categorical—observers largely agree when semantic content has changed. The U-shaped distribution of fidelity scores indicates most SR outputs are either clearly faithful or clearly altered, with few ambiguous cases. This low ambiguity enables reliable human annotation and robust metric training. Core assumption: fidelity violations are perceptually discrete events rather than gradual degradations. Evidence: U-shaped score distribution with minimum 12 independent annotations per image pair. Break condition: domains with subtle semantic differences may not exhibit categorical perception.

## Foundational Learning

- **Full-Reference (FR) vs. No-Reference (NR) IQA**: Understanding this distinction clarifies why GT comparison is essential for fidelity assessment. Quick check: Why can't NR-IQA metrics detect that a "bride was reconstructed as a rock"?
- **Perception-Distortion Trade-off**: Traditional SR theory posits a trade-off between perceptual quality and distortion (fidelity). This paper presents evidence that both can improve with proper guidance. Quick check: What result in Table 4 challenges the perception-distortion trade-off?
- **Foundation Model Semantic Embeddings**: The method leverages pre-trained vision models as frozen (or fine-tuned) feature extractors. Understanding that embeddings encode high-level semantics—not just pixel patterns—is essential. Quick check: Why does cosine similarity between GT and SR embeddings correlate with human fidelity judgments, while PSNR does not?

## Architecture Onboarding

- **Component map**: KonIQ-10k → 4× downscale (BSRGAN degradation) → 5 SR models → 723 image pairs → human annotation (12+ users each) → fidelity scores [0,1] → foundation backbone (CLIP/BLIP/DINOv2/PE-core) → embeddings → cosine similarity → regress to human scores → SR fine-tuning with L = L_ori + α·L_HLF
- **Critical path**: Construct or obtain annotated fidelity dataset → select and fine-tune foundation backbone on fidelity scores → integrate metric as auxiliary loss in target SR model
- **Design tradeoffs**: DINOv2 achieved highest PLCC (0.9030); CLIP/BLIP slightly lower but may generalize differently; α=0.01 outperformed 0.1 across all metrics; binary (Yes/No) aggregation to continuous is simple but effective
- **Failure signatures**: Hallucinated textures while structure improves; face structure failures attributed to diffusion model limitations; text captioning errors in SeeSR propagate to semantic changes
- **First 3 experiments**: 1) Validate annotation protocol with small-scale user study; 2) Compute zero-shot cosine similarity using pre-trained models on test set; 3) Fine-tune target SR model with α∈{0.001, 0.01, 0.05, 0.1} and plot fidelity vs. perceptual quality metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset limited to 723 image pairs from KonIQ-10k, may not capture full diversity of real-world SR scenarios.
- Results may not directly transfer to other generative approaches like GAN-based SR or autoregressive models.
- Method requires GT images for reference, limiting applicability in real-world scenarios where GT is unavailable.

## Confidence
- **High**: Foundation model embedding mechanism works as described (correlation metrics, fine-tuning results).
- **Medium**: Claim that semantic fidelity and perceptual quality can be simultaneously improved.
- **Medium**: Categorical nature of human fidelity perception assumption.

## Next Checks
1. **Domain Generalization Test**: Apply fine-tuned DINOv2 fidelity metric to held-out domain (medical X-rays or satellite imagery) to assess correlation degradation and identify distribution shift effects.
2. **Ablation on Annotation Granularity**: Repeat human study with 5-point graded scales instead of binary aggregation to test categorical perception assumption and its effect on metric training quality.
3. **Trade-off Boundary Analysis**: Systematically vary α in {0.001, 0.005, 0.01, 0.05, 0.1} on diverse SR model set, plotting fidelity vs. perceptual quality trade-offs to identify precise boundary where improvements reverse.