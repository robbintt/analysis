---
ver: rpa2
title: From the Gradient-Step Denoiser to the Proximal Denoiser and their associated
  convergent Plug-and-Play algorithms
arxiv_id: '2509.09793'
source_url: https://arxiv.org/abs/2509.09793
tags:
- denoiser
- image
- algorithm
- operator
- plug-and-play
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies convergence of plug-and-play (PnP) algorithms
  that use denoisers to solve image inverse problems. The key innovation is the gradient-step
  denoiser (GS) and its extension, the proximal denoiser (Prox), which are trained
  to represent gradient or proximity operators of explicit regularization functionals
  while preserving state-of-the-art denoising performance.
---

# From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms

## Quick Facts
- arXiv ID: 2509.09793
- Source URL: https://arxiv.org/abs/2509.09793
- Reference count: 29
- Primary result: Introduces Gradient-Step and Proximal Denoisers trained to represent gradient/proximity operators of explicit regularization functionals, enabling provable convergence of PnP algorithms for image inverse problems.

## Executive Summary
This paper addresses the convergence issues in Plug-and-Play (PnP) algorithms by designing denoisers that explicitly represent gradient or proximity operators of regularization functionals. The Gradient-Step Denoiser (GS) is trained to be the gradient of an explicit potential, while the Proximal Denoiser (Prox) is constrained to be a contraction, enabling convergence guarantees for proximal algorithms like Douglas-Rachford splitting. The methods are validated on denoising, super-resolution, deblurring, and inpainting tasks, showing that proper hyperparameter tuning (σ for denoiser strength, λ for regularization balance, τ for step size) is crucial for performance.

## Method Summary
The method trains DRUNet-based denoisers to represent gradient or proximity operators of explicit regularization functionals rather than treating them as black-box denoisers. The GS-Denoiser computes $D_\sigma(x) = N_\sigma(x) + J_{N_\sigma}(x)^T(x-N_\sigma(x))$ where $N_\sigma$ is a U-Net trained with smooth activations (ELU). The Prox-Denoiser adds a spectral norm penalty to enforce Lipschitz constraint $L<1$. PnP algorithms (GS-PnP, Prox-PnP-PGD, Prox-PnP-DRS) are implemented with dynamic step-size backtracking. Training uses 128×128 patches from CBSD, Waterloo Exploration, DIV2K, and Flick2K datasets with Monte-Carlo loss and spectral norm regularization for Prox-Denoiser.

## Key Results
- GS-Denoiser achieves superior PSNR compared to Prox-Denoiser due to less restrictive regularization
- Convergence is guaranteed under Lipschitz gradient and Kurdyka-Lojasiewicz conditions
- Performance is highly sensitive to hyperparameter tuning, particularly σ relative to observation noise
- Methods excel on moderately ill-posed problems but struggle with large-hole inpainting due to inability to hallucinate missing structures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing the denoiser to be a conservative vector field (the gradient of an explicit potential) allows the Plug-and-Play (PnP) framework to minimize a concrete objective function rather than iterating heuristically.
- **Mechanism:** The method constructs a Gradient-Step (GS) Denoiser $D_\sigma$ not as a raw neural network output, but as $D_\sigma(x) = x - \nabla g_\sigma(x)$. Here, the potential $g_\sigma(x) = \frac{1}{2}\|x - N_\sigma(x)\|^2$ is explicitly parameterized by a network $N_\sigma$. Crucially, calculating $D_\sigma$ requires a backward pass to compute the Jacobian of $N_\sigma$, ensuring the denoising step is strictly a gradient descent step on $g_\sigma$.
- **Core assumption:** The regularization function $g_\sigma$ is differentiable, and the underlying network architecture uses smooth activation functions (e.g., ELU or Softplus) to allow gradient computation.
- **Evidence anchors:**
  - [Abstract]: "The Gradient-Step Denoiser is trained to be exactly the gradient descent operator... of an explicit functional."
  - [Page 3, Eq. 10-11]: Defines $g_\sigma$ and the resulting denoiser $D_\sigma(x) = N_\sigma(x) + J_{N_\sigma}(x)^T(x-N_\sigma(x))$.
  - [Corpus]: arXiv:2510.27211 supports the general principle that MMSE denoisers can be framed as proximal operators, aligning with the goal of explicit regularization.
- **Break condition:** If the network $N_\sigma$ uses non-differentiable activations (like ReLU), the Jacobian is undefined or unstable, breaking the link to the explicit potential $g_\sigma$.

### Mechanism 2
- **Claim:** Constraining the Lipschitz constant of the denoiser's gradient allows it to act as a true proximity operator (Proximal Denoiser), unlocking convergence guarantees for broader classes of optimization algorithms like Douglas-Rachford (DRS).
- **Mechanism:** The paper defines a Proximal Denoiser by enforcing that $\nabla g_\sigma$ is a contraction ($L < 1$). This is achieved by adding a penalty term to the training loss that regularizes the spectral norm of the Hessian of $g_\sigma$ (estimated via power iteration). This guarantees the operator corresponds to the proximal map of a weakly convex potential $\phi_\sigma$.
- **Core assumption:** The spectral norm penalty (Hessian regularization) is sufficient to enforce the contraction property across the image manifold without degrading denoising capacity below usable thresholds.
- **Evidence anchors:**
  - [Page 5, Prop 3.1]: Shows that if $\nabla g_\sigma$ is $L$-Lipschitz with $L < 1$, then $D_\sigma$ is the proximity operator of $\phi_\sigma$.
  - [Page 5, Eq. 21]: Details the augmented loss function with the spectral norm penalty.
  - [Corpus]: arXiv:2505.08909 highlights that PnP methods typically require cocoercive or non-expansive denoisers for convergence, validating the necessity of this constraint.
- **Break condition:** If the Lipschitz constant exceeds 1, the denoiser no longer represents a valid proximity operator, and convergence guarantees for Prox-PnP-DRS are void.

### Mechanism 3
- **Claim:** Dynamic step-size backtracking ensures convergence in practice when the theoretical Lipschitz constant of the prior is unknown or fluctuates.
- **Mechanism:** The GS-PnP algorithm uses a backtracking loop that reduces the step-size $\tau$ if the sufficient decrease condition $F(x_k) - F(x_{k+1}) \ge \frac{\gamma}{\tau}\|x_k - x_{k+1}\|^2$ is not met. This forces the iterative scheme to respect the local geometry of the objective function $F$.
- **Core assumption:** The objective function $F$ is bounded from below and the initial step-size is feasible enough to allow progress before being reduced below critical thresholds.
- **Evidence anchors:**
  - [Page 4, Section 2.4]: Explicitly describes the backtracking condition and its role in ensuring the sufficient decrease property.
  - [Algorithm 1]: Shows the logic flow where $\tau$ is reduced by factor $\eta$ if descent criteria fail.
  - [Corpus]: No direct evidence in the provided corpus snippets; mechanism is internal to the paper's algorithmic contribution.
- **Break condition:** If the step-size $\tau$ collapses to zero or a value smaller than machine precision, the algorithm stalls (the stopping condition $\tau > \epsilon$ prevents infinite looping).

## Foundational Learning

- **Concept: Proximal Operators vs. Gradient Descent**
  - **Why needed here:** The paper distinguishes between denoisers that act as gradient steps (GS-Denoiser) and those that act as proximity operators (Proximal Denoiser). You must understand that a proximal operator handles non-smooth constraints (like Total Variation or ℓ₁ norms) and implies a stronger theoretical link to a specific prior than a simple gradient step.
  - **Quick check question:** Does the Proximal Denoiser require a forward-backward pass through the network, or just a forward pass? (Answer: It requires a backward pass to compute gradients, same as the GS Denoiser, but requires stricter Lipschitz constraints).

- **Concept: Tweedie’s Identity and Score Matching**
  - **Why needed here:** The paper grounds the GS-Denoiser in Tweedie's identity (Eq 8), which links the MMSE denoiser to the gradient of the log-prior (the score). Understanding this explains *why* training a denoiser effectively trains a gradient field of a probability distribution.
  - **Quick check question:** According to the text, does the GS-Denoiser approximate the MAP denoiser or the MMSE denoiser? (Answer: MMSE denoiser, via Eq 8/12).

- **Concept: Lipschitz Continuity and Contraction**
  - **Why needed here:** Convergence proofs rely on the gradient of the regularization ∇g_σ being Lipschitz continuous. For the Proximal Denoiser specifically, it must be a "contraction" (L < 1). You need to know that this limits how fast the function can change, preventing oscillatory or divergent behavior in the optimization loop.
  - **Quick check question:** Why does the Proximal Denoiser use "Softplus" activation instead of "ELU"? (Answer: To ensure the function is of class C² or C∞, required for the Hessian regularization in the Proximal case).

## Architecture Onboarding

- **Component map:** N_σ (Core Network) -> g_σ (Regularizer) -> D_σ (Wrapper) -> Optimization Loop
- **Critical path:**
  1. Load pre-trained DRUNet weights
  2. Wrap DRUNet in the D_σ class to compute the "Gradient-Step" output (requires custom autograd to access the Jacobian-vector product)
  3. Initialize the optimizer (e.g., GS-PnP) with parameters λ (regularization strength), σ (denoiser strength), and τ (step size)
  4. Run iteration: Apply D_σ to current estimate -> Apply Data Fidelity Prox (e.g., FFT-based deblur) -> Update state

- **Design tradeoffs:**
  - **GS-Denoiser vs. Prox-Denoiser:** The GS-Denoiser offers better PSNR (superior denoising capability) but limits the usable optimization algorithms (mostly PnP-PGD). The Prox-Denoiser has slightly lower PSNR due to Hessian regularization but enables the use of Douglas-Rachford (DRS), which handles non-differentiable data terms better and allows unrestricted λ.
  - **Smoothness vs. Performance:** Using Softplus (for Prox) over ELU (for GS) guarantees mathematical smoothness but may limit the network's ability to model sharp edges compared to standard ReLUs.

- **Failure signatures:**
  - **"Stepping over" the solution:** If initial step size τ₀ is too large, PSNR drops sharply and the algorithm stops prematurely (Fig 12)
  - **Smooth artifacts in Inpainting:** For large holes, the model generates smooth colors rather than textures because it cannot "hallucinate" structure not implied by the gradient prior (Fig 13)
  - **Over-regularization:** If σ or λ are too high, the image becomes overly smooth (Fig 8c, 9c)

- **First 3 experiments:**
  1. **Denoising Validation:** Test the constructed D_σ (with Jacobian) against the raw network N_σ on a Gaussian denoising task to verify the explicit gradient construction hasn't degraded performance
  2. **Hyperparameter Sweep (σ):** Run deblurring on a standard image (e.g., Set3C) while varying the noise level parameter σ relative to the observation noise ν. Verify the "U-shaped" PSNR curve shown in Fig 7
  3. **Convergence Monitor:** Implement the backtracking logic for GS-PnP on a super-resolution task. Plot the objective function F(x_k) against iterations to verify the non-increasing property guaranteed by Theorem 2.1

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework requires smooth activation functions and strict Lipschitz continuity, which may not hold in practice due to finite training or architectural constraints
- Performance degradation on large-hole inpainting tasks reveals fundamental limitations: the gradient-based prior cannot hallucinate structures that are not implied by observed gradients
- Convergence guarantees rely on assumptions about objective function properties (bounded below, Kurdyka-Lojasiewicz) that are stated but not extensively verified for each specific application

## Confidence
- **High Confidence:** Mathematical derivation of Gradient-Step and Proximal Denoiser formulations, including Jacobian computation and Lipschitz constraint enforcement, appears rigorous and well-supported by equations and proofs
- **Medium Confidence:** Convergence guarantees for PnP algorithms rely on assumptions about objective function properties that are stated but not extensively verified for each specific application
- **Low Confidence:** Claim about Proximal Denoiser's superior performance on non-differentiable data terms through DRS is supported by theoretical arguments but has limited empirical validation in the paper

## Next Checks
1. **Convergence Rate Analysis:** Implement GS-PnP algorithm with backtracking mechanism on standardized deblurring benchmark and plot both PSNR and objective function values versus iteration count to verify non-increasing property and compare convergence speed against standard PnP with untrained denoisers

2. **Architecture Sensitivity Test:** Train two versions of GS-Denoiser - one with ELU activations as specified and another with ReLU activations - then compare both their denoising PSNR and their behavior in PnP loop to quantify impact of smooth activations on convergence and reconstruction quality

3. **Lipschitz Constant Verification:** For trained Proximal Denoiser, estimate Lipschitz constant of ∇g_σ using power iteration across validation set and verify it remains below 1.0 as required by theory. Then test whether relaxing this constraint to L > 1 causes divergence in Prox-PnP-DRS algorithm