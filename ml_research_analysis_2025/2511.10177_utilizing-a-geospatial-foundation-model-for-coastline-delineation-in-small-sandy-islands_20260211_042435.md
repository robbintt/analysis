---
ver: rpa2
title: Utilizing a Geospatial Foundation Model for Coastline Delineation in Small
  Sandy Islands
arxiv_id: '2511.10177'
source_url: https://arxiv.org/abs/2511.10177
tags:
- images
- prithvi
- dataset
- training
- prithvi-eo-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors evaluated the Prithvi-EO-2.0 geospatial foundation
  model for shoreline delineation on small sandy islands using Sentinel-2 imagery.
  They created a labeled dataset of 225 images from two Maldivian islands and fine-tuned
  both the 300M and 600M parameter versions of Prithvi using training subsets ranging
  from 5 to 181 images.
---

# Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands

## Quick Facts
- arXiv ID: 2511.10177
- Source URL: https://arxiv.org/abs/2511.10177
- Reference count: 32
- Primary result: Prithvi-EO-2.0 achieves F1 >0.94 and IoU >0.79 on shoreline delineation with as few as 5 training images

## Executive Summary
This study evaluates the Prithvi-EO-2.0 geospatial foundation model for shoreline delineation on small sandy islands using Sentinel-2 imagery. The authors created a labeled dataset of 225 images from two Maldivian islands and fine-tuned both 300M and 600M parameter versions of Prithvi using training subsets ranging from 5 to 181 images. Results demonstrate exceptional performance even with minimal training data, validating the potential of geospatial foundation models for coastal monitoring in data-scarce regions. The 300M model slightly outperformed the 600M model, suggesting that the larger model's computational cost may not be justified for this application.

## Method Summary
The authors fine-tuned Prithvi-EO-2.0 (300M and 600M variants) for binary shoreline segmentation on 225 Sentinel-2 images from two Maldivian islands. The ViT backbone was frozen during training, with only a U-Net decoder head being trained. Models were evaluated across training subsets of 5, 10, 20, 30, 60, 125, and 181 images. The input consisted of 6 spectral bands (R, G, B, NIR, SWIR1, SWIR2) resized to 224×224. Training used AdamW optimizer (lr=1e-4), batch size 4, 30 epochs, cross-entropy loss, and bfloat16 precision. Performance was measured using F1 score and IoU.

## Key Results
- Achieved F1 scores exceeding 0.94 and IoU scores exceeding 0.79 even with only 5 training images
- 300M model slightly outperformed 600M model, suggesting larger model's computational cost may not be justified
- Performance peaked at 125 training images before slightly decreasing with the full 181-image dataset

## Why This Works (Mechanism)

### Mechanism 1
Large-scale multi-spectral pre-training enables high-performance transfer learning for shoreline delineation even with severely limited labeled data. The Prithvi-EO-2.0 backbone is pre-trained on 4.2 million HLS images using a Masked Autoencoder approach, forcing the model to learn robust spectral-spatial representations of water, land, and coastal features. These pre-existing embeddings allow the model to distinguish land/water boundaries without needing to learn basic visual features from scratch.

### Mechanism 2
Freezing the encoder preserves robust geospatial features while reducing computational cost and overfitting risk in low-data regimes. By freezing the pre-trained ViT backbone and training only a lightweight U-Net decoder head, the model retains the general "world model" of Earth observation features. This prevents the model from overfitting the limited ground truth while allowing the decoder to learn the specific semantic mapping required for shoreline boundary detection.

### Mechanism 3
Increasing model parameter count (600M vs 300M) does not guarantee improved performance and may hinder convergence in extremely low-data settings. The 600M model has higher capacity requiring more data to regularize effectively. With only 5 training images, the 600M model exhibited lower IoU (0.7977) compared to the 300M model (0.8509), suggesting potential under-convergence or difficulty optimizing the larger parameter space with scarce gradient signals.

## Foundational Learning

- **Concept: Masked Autoencoders (MAE)**
  - Why needed here: Prithvi utilizes MAE for pre-training. Understanding that the model learns by reconstructing masked patches of satellite imagery helps explain why it captures spatial continuity (coastlines) effectively.
  - Quick check question: Can you explain why masking 75% of an image forces a model to learn high-level semantic features rather than just local texture?

- **Concept: Encoder-Decoder Architecture (U-Net)**
  - Why needed here: The paper attaches a U-Net decoder to the frozen ViT encoder. Understanding skip connections and upsampling is necessary to modify the segmentation head for different resolutions or classes.
  - Quick check question: Why is a standard ViT classification token [CLS] insufficient for dense pixel-wise prediction like segmentation?

- **Concept: Multi-spectral Band Interpretation**
  - Why needed here: The input consists of 6 bands (RGB, NIR, SWIR1, SWIR2). Effective fine-tuning requires knowing that NIR and SWIR are critical for water/land separation due to water's absorption properties.
  - Quick check question: Why might an RGB-only model fail to delineate a coastline where the water is turbid (murky) compared to a model using NIR bands?

## Architecture Onboarding

- **Component map**: Sentinel-2 GeoTIFFs (6 bands, 10m resolution) -> Preprocessing (resize to 224×224) -> Prithvi-EO-2.0 ViT (Frozen) -> Connector layers -> U-Net Decoder (Trainable) -> Binary Segmentation Mask

- **Critical path**:
  1. **Band Alignment**: Ensure input data matches the 6 specific bands (Blue, Green, Red, Narrow NIR, SWIR1, SWIR2) expected by Prithvi. Mismatch here will cause silent failure.
  2. **Resolution Handling**: The paper resizes inputs to 224×224. Deviating from this requires adjusting the position embeddings or patch size logic.
  3. **Class Weighting**: The loss function uses Cross-Entropy. If the island is small relative to the ocean, class imbalance handling (e.g., weighting) may be necessary, though the paper uses dropout (0.1) to regularize.

- **Design tradeoffs**:
  - **300M vs. 600M**: Use 300M for low-data/compute-constrained environments; 600M offers marginal gains (<1% IoU) even at high data volumes.
  - **Frozen vs. Unfrozen**: Freezing drastically reduces VRAM usage and training time. Unfreezing might squeeze out performance but risks catastrophic forgetting or overfitting given the small dataset size (N=225).

- **Failure signatures**:
  - **"Blob" artifacts**: Model predicts a circular mass rather than the jagged coastline (indicates lack of fine-tuning or learning rate too high).
  - **Confusing Atoll for Shoreline**: The paper notes standard tools fail here by detecting the underwater atoll instead of the island. If Prithvi does this, the training data labels may need verification.
  - **600M Underperformance**: If 600M lags behind 300M significantly, check if batch size is too small (gradient noise) or learning rate needs reduction for the larger optimizer surface.

- **First 3 experiments**:
  1. **Baseline Reproduction**: Replicate the 5-image fine-tuning run with the 300M model to validate the setup (target: IoU > 0.80).
  2. **Ablation on Unfreezing**: Run a comparison with the backbone unfrozen (lower learning rate) on the full 181-image set to quantify the performance gap against the frozen baseline.
  3. **Sensor Shift Test**: Evaluate the fine-tuned model on a different island or coastal region not in the training set to test the "real-world applicability" claims made in the Conclusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does model performance peak at 125 training images and slightly decrease when using the full 181-image dataset?
- Basis in paper: The authors explicitly ask "why performance peaks at 125 images" in Appendix A.4 regarding the results in Table 1.
- Why unresolved: The paper reports the empirical drop in F1/IoU scores between 125 and 181 images but does not analyze the statistical significance or potential causes (e.g., data noise, optimization difficulties).
- What evidence would resolve it: A learning curve sensitivity analysis or an ablation study inspecting the specific images added after the 125-image threshold to identify noisy labels or outliers.

### Open Question 2
- Question: How does the Prithvi-EO-2.0 approach quantitatively compare to other state-of-the-art segmentation models for this specific task?
- Basis in paper: Appendix A.4 lists the need to understand "how this approach compares with other segmentation approaches and models."
- Why unresolved: The study only compares the 300M and 600M Prithvi versions against each other, providing only a qualitative failure case for the CoastSat toolkit in the appendix.
- What evidence would resolve it: A benchmark study reporting F1 and IoU scores from standard architectures (e.g., U-Net, DeepLab) and other GFMs trained on the same Maldivian dataset.

### Open Question 3
- Question: Does incorporating temporal satellite metadata via Prithvi's "TL" versions improve coastline delineation accuracy?
- Basis in paper: Appendix A.4 notes that Prithvi has "TL" versions where "satellite metadata can be included, which would be worth testing."
- Why unresolved: The experiments utilized only the standard 300M and 600M parameter versions, ignoring the metadata-encoded variants that could account for temporal changes like tides.
- What evidence would resolve it: Fine-tuning the TL-variant models on the same dataset and comparing the segmentation metrics against the standard versions.

### Open Question 4
- Question: Does the trained model generalize effectively to diverse geographical regions and island types beyond the two specific Maldivian islands tested?
- Basis in paper: Appendix A.4 states that "expanding our analysis beyond two islands in the Maldives would provide deeper and more generalizable analysis."
- Why unresolved: The model was trained and tested exclusively on Fuvamulah and Madhirivaadhoo, limiting the understanding of its effectiveness on different topographies or water turbidities.
- What evidence would resolve it: Zero-shot or few-shot evaluation of the fine-tuned model on sandy islands from different geographic regions (e.g., Pacific atolls or Caribbean islands).

## Limitations

- **Scaling regime**: The long-term scaling behavior of Prithvi-EO-2.0 remains unclear as the paper only tests up to 181 images, leaving open whether the 600M model would eventually surpass the 300M model with substantially more data.
- **Geographic generalization**: The model is validated on two islands in the Maldives, so claims about broader applicability for other coastal environments remain untested and may fail where water-land spectral signatures differ.
- **Architectural specifics**: Key implementation details of the U-Net decoder are not fully specified, making exact reproduction challenging without assumptions.

## Confidence

- **High confidence**: The core finding that Prithvi-EO-2.0 achieves high performance (>0.94 F1, >0.79 IoU) on shoreline delineation with minimal training data is well-supported by the results and aligns with the known capabilities of foundation models.
- **Medium confidence**: The claim that the 300M model is preferable to 600M for data-scarce scenarios is supported but limited to the specific scale tested (5-181 images). Scaling to larger datasets might change this relationship.
- **Medium confidence**: The transferability claim for "real-world applicability" is supported by strong in-domain performance but lacks external validation on geographically diverse coastlines.

## Next Checks

1. **Geographic transfer test**: Evaluate the fine-tuned model on a completely different coastal region (e.g., Mediterranean rocky coastline or Arctic shoreline) to validate claims about broader applicability beyond Maldivian sandy islands.
2. **Scaling limit test**: Train the 300M and 600M models on progressively larger subsets (up to 1000+ images if available) to identify the exact data threshold where the 600M model's additional capacity becomes advantageous.
3. **Input resolution sensitivity**: Test the model's performance when input resolution deviates from the 224×224 specification (e.g., native 10m resolution or higher) to assess robustness to resolution changes and potential gains from higher fidelity inputs.