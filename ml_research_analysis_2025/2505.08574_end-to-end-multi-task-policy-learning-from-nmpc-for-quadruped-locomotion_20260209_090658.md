---
ver: rpa2
title: End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion
arxiv_id: '2505.08574'
source_url: https://arxiv.org/abs/2505.08574
tags:
- learning
- control
- locomotion
- joint
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a multi-task learning framework for quadrupedal
  locomotion that trains a single neural network to perform multiple gaits from raw
  proprioceptive inputs, eliminating the need for state estimation. Expert demonstrations
  from nonlinear model predictive control (NMPC) are used to train the policy through
  imitation learning.
---

# End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion

## Quick Facts
- arXiv ID: 2505.08574
- Source URL: https://arxiv.org/abs/2505.08574
- Reference count: 29
- Trained a single neural network to perform multiple quadrupedal gaits from raw proprioceptive inputs, eliminating state estimation needs

## Executive Summary
This work presents a multi-task learning framework for quadrupedal locomotion that trains a single neural network to perform multiple gaits from raw proprioceptive inputs, eliminating the need for state estimation. Expert demonstrations from nonlinear model predictive control (NMPC) are used to train the policy through imitation learning. The approach achieves high R2 scores (0.92-0.99) for predicted joint targets across all tasks in simulation and on the Unitree Go1 robot, demonstrating accurate reproduction of expert behavior. The unified model supports smooth gait switching and simplifies the control pipeline compared to task-specific approaches, enabling end-to-end control from sensors to actions.

## Method Summary
The method trains a single neural network to perform multiple quadrupedal gaits using NMPC expert demonstrations through behavior cloning. The policy takes raw proprioceptive inputs (34D: IMU, joint positions/velocities, foot contacts, task identifier) and outputs desired joint positions. Expert NMPC trajectories are collected in PyBullet simulation at 1kHz, with torques converted to position targets via PD control inversion. A multi-task architecture with shared layers (2x 2560 neurons, ELU) and task-specific heads learns the mapping. The model is trained with MSE loss and deployed on a Unitree Go1 robot, achieving smooth gait transitions without explicit state estimation.

## Key Results
- Achieved R² scores of 0.92-0.99 for predicted joint targets across all tasks
- Multi-task architecture outperformed single-task baselines (R² 0.99 vs 0.91 for bound gait)
- Successfully deployed on Unitree Go1 robot with smooth gait switching
- Eliminated need for explicit state estimation using only raw proprioceptive inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single neural network can disambiguate and execute distinct locomotion gaits better than a merged single-task model by isolating gait-specific features in separate output heads.
- **Mechanism:** Hard parameter sharing forces the network trunk to learn generalized locomotion dynamics (e.g., gravity compensation, inertia) shared across all gaits, while task-specific heads learn the unique phase relationships and contact schedules of specific gaits (trot vs. bound).
- **Core assumption:** The underlying dynamics of different gaits share a common structure that can be captured by a shared representation, reducing the learning burden compared to learning from scratch.
- **Evidence anchors:**
  - [Section III.C] "The shared layers capture general locomotion dynamics, while each head specializes in predicting joint targets specific to its corresponding gait."
  - [Table I] Shows Multi-Task R2 scores (0.991) significantly outperforming Single-Task baselines (0.906) for the Bound gait.
  - [Corpus] Consistent with "GRoQ-LoCO" and "PALo" approaches suggesting unified policies generalize better than fragmented controllers.

### Mechanism 2
- **Claim:** Predicting PD-target joint positions rather than direct torques allows a low-capacity network to reproduce complex NMPC trajectories by offloading high-frequency error correction.
- **Mechanism:** The policy predicts desired joint positions ($q_{des}$), and a low-level PD controller computes the required torques ($\tau$) to track them. This effectively "caches" the inverse dynamics, allowing the network to focus on the kinematic trajectory rather than raw force modulation.
- **Core assumption:** The expert NMPC generates behaviors that are compatible with the PD control law, and the robot's actuators can track the resulting torque commands accurately.
- **Evidence anchors:**
  - [Section III.B] "This formulation offloads low-level error correction to the PD controller, allowing the network to focus on higher-level gait generation."
  - [Equation 3 & 4] Defines the inversion of the PD law to generate training labels from expert torques.

### Mechanism 3
- **Claim:** Raw proprioceptive input is sufficient for state estimation and control without explicit state reconstructors or phase variables.
- **Mechanism:** The network implicitly infers the robot's phase and velocity from high-frequency sensory data (IMU, joint encoders) directly mapping to actions. This relies on the network's capacity to perform sensor fusion and temporal estimation internally.
- **Core assumption:** The observation vector contains enough information to resolve the current phase of the gait cycle (observability) and the network architecture is capable of filtering noise without explicit recurrence or history.
- **Evidence anchors:**
  - [Section IV.A.1] "Our policy relies solely on raw proprioceptive inputs... eliminating the need for explicit state estimation."
  - [Fig 7] Shows accurate trajectory tracking despite the absence of an explicit state estimator module.

## Foundational Learning

- **Concept:** **Imitation Learning (Behavior Cloning)**
  - **Why needed here:** This is the core training paradigm. You must understand that the network does not "learn to walk" via trial-and-error (RL), but rather learns to copy an existing optimization-based controller (NMPC).
  - **Quick check question:** Can you explain why Behavior Cloning might fail if the robot deviates from the expert's state distribution (distributional shift)?

- **Concept:** **Hard Parameter Sharing (Multi-Task Learning)**
  - **Why needed here:** The architectural choice. Understanding that weights are shared in the trunk but split at the heads is crucial for debugging why a model might forget one gait while learning another (catastrophic forgetting vs. positive transfer).
  - **Quick check question:** If the "Bound" gait performance drops significantly when adding "Walk," what phenomenon might be occurring, and how does the task-specific head architecture mitigate it?

- **Concept:** **Model Predictive Control (MPC) Basics**
  - **Why needed here:** To understand the "Expert." You need to know that MPC solves a finite-horizon optimization problem at every step, which is computationally expensive but generates high-quality data.
  - **Quick check question:** Why is the NMPC expert typically too slow to run directly on the robot at high frequencies, necessitating this learned policy?

## Architecture Onboarding

- **Component map:** 34D Raw Proprioceptive Inputs -> Shared Trunk (2x FC 2560, ELU) -> 3 Task-Specific Heads -> 12D Joint Position Targets -> PD Controller -> Robot Actuators
- **Critical path:** 1. Data Gen: Run NMPC expert in PyBullet/Isaac -> Log (Observations, Torques). 2. Label Inversion: Convert Expert Torques -> Target Positions via Eq 4. 3. Training: Supervised regression (MSE) on $(O_t, Task_{id}) \rightarrow Target_{pos}$
- **Design tradeoffs:**
  - **MTL vs. Single Task:** MTL requires more complex architecture logic but yields higher accuracy (R2 0.99 vs 0.90) and smoother switching. Single Task is simpler but conflates gait definitions.
  - **Wide Layers (2560):** The paper uses very wide layers (2560). Assumption: This high capacity is necessary to memorize the complex mapping of multiple gaits without recurrence.
- **Failure signatures:**
  - **Oscillation/Instability:** If validation loss is low but robot shakes, check PD gains or latency between command and actuation (not modeled in static BC).
  - **Gait Bleeding:** If the robot tries to trot while in bound mode, the shared trunk may be overfitting to common features; check head isolation or task key encoding.
  - **Generalization Gap:** The paper notes failure on unseen gaits (gallop). Do not expect zero-shot generalization to gaits not present in the training set.
- **First 3 experiments:**
  1. **Overfit Single Gait:** Train only on "Trot" to verify the data pipeline and PD-inversion logic works (Sanity Check).
  2. **Ablate Task Key:** Remove the task identifier input and train the MTL architecture to confirm performance degrades to the "Baseline" levels (verify conditioning mechanism).
  3. **Hardware Switching:** Deploy the policy and toggle the task key in real-time to verify smooth gait transitions and check for impulse torques during switches.

## Open Questions the Paper Calls Out
- Can meta-learning techniques enable the policy to generalize to unseen gaits, such as gallop or pace, without explicit expert demonstrations for those specific behaviors? (Standard supervised multi-task learning fails to extrapolate beyond trained behaviors)
- How can exteroceptive perception modules be integrated with the pre-trained proprioceptive policy to enable navigation and environment-aware planning? (Current framework lacks capacity to process visual or geometric environmental data)
- How does the imitated policy perform in recovery scenarios, such as responding to significant external perturbations or slips, which may not be fully captured in nominal NMPC expert trajectories? (Behavior cloning often suffers from distribution shift in out-of-distribution states)

## Limitations
- Does not generalize to unseen gaits beyond those explicitly trained (e.g., gallop)
- Sim-to-real transfer demonstrated only on one robot platform (Unitree Go1)
- Critical implementation details (NMPC configuration, PD gains, training hyperparameters) not specified

## Confidence
- **High confidence:** Multi-task architecture design and its advantage over single-task baselines are well-supported by simulation results and consistent with established MTL principles
- **Medium confidence:** Claim that raw proprioception suffices for state estimation is plausible but lacks systematic validation through controlled experiments
- **Medium confidence:** Sim-to-real transfer on hardware is demonstrated but only for one robot platform; generalization to other platforms is unverified

## Next Checks
1. **Ablation of task conditioning:** Remove the task identifier from input and retrain to verify performance degrades to single-task baseline levels
2. **Phase observability analysis:** Systematically vary sensor noise levels and latency in simulation to quantify network's robustness to degraded proprioceptive signals
3. **Cross-robot validation:** Deploy policy on different quadruped platform (e.g., A1 or ANYmal) to test architecture's generalization beyond Unitree Go1 hardware