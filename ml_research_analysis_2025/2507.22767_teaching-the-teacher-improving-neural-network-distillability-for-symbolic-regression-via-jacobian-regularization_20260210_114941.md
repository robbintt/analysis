---
ver: rpa2
title: 'Teaching the Teacher: Improving Neural Network Distillability for Symbolic
  Regression via Jacobian Regularization'
arxiv_id: '2507.22767'
source_url: https://arxiv.org/abs/2507.22767
tags:
- teacher
- network
- symbolic
- regularization
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the brittleness of distilling neural networks
  into symbolic formulas, where standard training often produces overly complex functions
  that symbolic regression struggles to approximate. The core method introduces a
  Jacobian-based regularizer during teacher training, penalizing the sensitivity of
  the network's output to its input to encourage smoother, more distillable functions.
---

# Teaching the Teacher: Improving Neural Network Distillability for Symbolic Regression via Jacobian Regularization

## Quick Facts
- arXiv ID: 2507.22767
- Source URL: https://arxiv.org/abs/2507.22767
- Authors: Soumyadeep Dhar; Kei Sen Fong; Mehul Motani
- Reference count: 7
- Primary result: Jacobian regularization during teacher training improves symbolic regression R² scores by average 120% over standard distillation

## Executive Summary
This work addresses a fundamental bottleneck in neural-symbolic integration: neural networks trained for regression often learn overly complex functions that symbolic regression algorithms struggle to approximate. The authors propose a simple but effective solution—adding a Jacobian-based regularizer to the teacher network's training objective to encourage smoother function mappings. This approach dramatically improves the quality of symbolic formulas that can be distilled from neural networks, achieving up to 515% improvement in R² scores on real-world regression benchmarks while maintaining teacher predictive accuracy.

## Method Summary
The core innovation is a Jacobian-based regularizer that penalizes the sensitivity of the network's output to its input during teacher training. The regularizer adds λ·E[‖Jxf(x; θ)‖²F] to the loss function, where Jxf(x; θ) is the Jacobian matrix of partial derivatives. This encourages the teacher network to learn smoother functions that are more amenable to symbolic regression using genetic programming with a limited function set {+, -, *, /}. The approach maintains teacher accuracy while making the learned function structure more compatible with symbolic discovery algorithms.

## Key Results
- Average 120% improvement in symbolic student R² scores compared to standard distillation
- Up to 515% improvement on Concrete Strength dataset
- Teacher R² maintained at 0.84-0.98 across all benchmarks
- Jacobian regularization ineffective for decision tree students, confirming mechanism specifically aids smooth-basis-function models

## Why This Works (Mechanism)

### Mechanism 1: Jacobian-Based Smoothness Regularization
- Penalizing the Jacobian norm during training encourages smoother function mappings that are easier for symbolic regression to approximate
- The Jacobian matrix captures first-order partial derivatives, measuring local sensitivity of output to input changes
- Smoother functions with bounded local sensitivity lie within the effective search space of symbolic regression algorithms using smooth basis functions

### Mechanism 2: Functional Alignment with Symbolic Search Space
- Standard neural networks learn unnecessarily complex functions outside what symbolic regression can efficiently discover
- Symbolic regression searches a constrained space of mathematical expressions using limited function sets
- Constraining the teacher's functional form reduces the expressive gap between neural networks and symbolic formulas

### Mechanism 3: Dataset-Dependent Inductive Bias Matching
- Effectiveness depends on alignment between enforced smoothness and intrinsic data properties
- Datasets with inherently smooth, continuous relationships benefit most from smoothness regularization
- Datasets with high noise or threshold effects may not benefit as regularization either over-smooths signal or fails to address core challenges

## Foundational Learning

- **Jacobian Matrix and Frobenius Norm**: The regularizer directly computes this quantity; understanding what it measures (local sensitivity) is essential for interpreting λ's effect
  - Quick check: If ‖Jxf(x; θ)‖F is large at point x, what does this tell you about the function's behavior near x?

- **Genetic Programming for Symbolic Regression**: The student model uses GP to search expression space; understanding its limitations explains why teacher smoothness matters
  - Quick check: Why might a GP search with function set {+, -, *, /} struggle to approximate a function with rapid oscillations?

- **Knowledge Distillation Pipeline**: This is a two-stage process where teacher predictions become student targets; the distillation gap is the central problem being solved
  - Quick check: In standard distillation for compression, the student is smaller but same architecture type. How does distilling to symbolic formulas differ fundamentally?

## Architecture Onboarding

- **Component map**: ANN Teacher (ReLU, 100 neurons) -> Jacobian Regularizer -> L_total = L_MSE + λ·E[‖Jacobian‖²F] -> Symbolic Student (gplearn, {+, -, *, /})
- **Critical path**: 1) Train teacher with Jacobian regularizer, 2) Generate distillation dataset D' = {(xi, f*ANN(xi))}, 3) Run symbolic regression on D' to discover interpretable formula
- **Design tradeoffs**: λ selection affects both accuracy and distillability; function set expansion increases expressiveness but reduces interpretability; assumes tabular-scale data (high-dimensional inputs face prohibitive Jacobian computation costs)
- **Failure signatures**: Student R² dramatically below teacher R² despite regularization; no improvement across entire λ sweep; discovered formulas remain overly complex or fail to converge
- **First 3 experiments**:
  1. Baseline characterization: Run standard pipeline (λ=0) to measure initial distillation gap; compare student R² against teacher R² and XGBoost baseline
  2. Lambda sensitivity sweep: Test λ ∈ {0.001, 0.01, 0.05, 0.1, 0.5} while tracking teacher R², student R², and training time; plot curves similar to Figure 2
  3. SNR proxy analysis: Compute Var(y)/Var(residuals) from simple linear regression to assess whether your dataset has sufficient signal for this approach; expect challenges if SNR < ~2

## Open Questions the Paper Calls Out

1. **Adaptive Regularization**: Can methods be developed to automatically tune the regularization strength λ during training based on data properties or network dynamics?
2. **Computational Efficiency**: Can efficient approximations, such as random projections, mitigate the 10x computational overhead of Jacobian regularization?
3. **Higher-Order Regularization**: Does penalizing higher-order derivatives, such as the Hessian norm, provide finer control over functional complexity and distillability?
4. **Noise Interaction**: How does label noise specifically interact with the functional smoothness induced by Jacobian regularization?

## Limitations
- Effectiveness is highly dataset-dependent, working best for smooth, continuous underlying relationships
- Computational cost increases by approximately 10× due to Jacobian computation
- Optimal λ value varies dramatically across datasets with no clear selection principle
- Limited to tabular-scale data; high-dimensional inputs face prohibitive Jacobian computation costs

## Confidence
- **High Confidence**: The core mechanism of Jacobian regularization for smoothness is well-founded and empirically demonstrated
- **Medium Confidence**: The claim that dataset smoothness determines effectiveness is supported but lacks systematic characterization
- **Medium Confidence**: The limitation regarding decision trees shows the approach specifically benefits smooth-basis-function students
- **Low Confidence**: Claims about computational scalability to high-dimensional inputs are speculative

## Next Checks
1. **Dataset Property Analysis**: Systematically characterize which intrinsic data properties (SNR, label noise, underlying function continuity) predict successful Jacobian regularization
2. **Computational Scaling Experiments**: Measure training time and memory requirements as a function of input dimension and network width
3. **Expanded Function Set Evaluation**: Test symbolic regression with expanded function sets (sin, exp, log) while measuring both R² improvement and formula interpretability