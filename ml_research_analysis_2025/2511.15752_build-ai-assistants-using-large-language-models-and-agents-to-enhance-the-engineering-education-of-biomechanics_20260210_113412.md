---
ver: rpa2
title: Build AI Assistants using Large Language Models and Agents to Enhance the Engineering
  Education of Biomechanics
arxiv_id: '2511.15752'
source_url: https://arxiv.org/abs/2511.15752
tags:
- ball
- mass
- solver
- hand
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops AI-driven educational assistants using large
  language models (LLMs), retrieval-augmented generation (RAG), and multi-agent systems
  (MAS) to enhance undergraduate biomechanics learning. The RAG module improves LLM
  accuracy and stability in answering true/false conceptual questions by grounding
  responses in a domain-specific knowledge base, achieving up to 97% stability and
  substantial performance gains over baseline models.
---

# Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics

## Quick Facts
- arXiv ID: 2511.15752
- Source URL: https://arxiv.org/abs/2511.15752
- Authors: Hanzhi Yan; Qin Lu; Xianqiao Wang; Xiaoming Zhai; Tianming Liu; He Li
- Reference count: 40
- Primary result: RAG module achieves up to 97% stability on conceptual questions; MAS system reaches 82% accuracy on calculation problems

## Executive Summary
This study develops AI-driven educational assistants using large language models (LLMs), retrieval-augmented generation (RAG), and multi-agent systems (MAS) to enhance undergraduate biomechanics learning. The RAG module improves LLM accuracy and stability in answering true/false conceptual questions by grounding responses in a domain-specific knowledge base, achieving up to 97% stability and substantial performance gains over baseline models. The MAS module solves complex biomechanics calculation problems by decomposing tasks among Manager, Solver, and Reviewer agents, enabling accurate equation derivation, code execution, and structured explanations. Experiments show that hybrid MAS configurations outperform fully multimodal setups, with accuracy approaching 82% and providing transparent, traceable solutions. Together, these systems offer a scalable, reliable framework for intelligent tutoring in engineering education.

## Method Summary
The study develops two AI modules: a RAG pipeline for conceptual true/false questions and a MAS for multi-step calculation problems. For RAG, the system uses UnstructuredLoader to parse a biomechanics textbook PDF, RecursiveCharacterTextSplitter with 1000-character chunks and 200-character overlap, mxbai-embed-large-v1 embeddings, and FAISS with Maximal Marginal Relevance (MMR) retrieval. The MAS employs three cooperative agents (Manager, Solver, Reviewer) sharing a unified State object via LangGraph. The Manager interprets problems (using Qwen2.5-VL-72B-Instruct for multimodal input), the Solver generates and executes Python code (using Mistral-3.2-24B-Instruct), and the Reviewer evaluates solutions against ground truth. The system was tested on 100 true/false questions and 30 calculation problems from biomechanics curricula.

## Key Results
- RAG module achieved up to 97% stability across 100 true/false questions, with Qwen-2.5-32B showing the highest performance
- MAS system reached approximately 82% accuracy on 30 multi-step calculation problems using fully multimodal agents
- Hybrid MAS configuration (Qwen2.5-VL Manager + Mistral Solver/Reviewer) achieved 81.7% accuracy while providing more structured, traceable solutions
- RAG significantly reduced hallucinations and knowledge gaps compared to baseline LLMs without retrieval

## Why This Works (Mechanism)

### Mechanism 1: RAG Grounding for Conceptual Accuracy
- Claim: Retrieval-augmented generation improves both accuracy and output stability for domain-specific true/false questions.
- Mechanism: External domain knowledge is retrieved before generation, anchoring LLM responses in verified source material rather than parametric memory alone. This reduces hallucinations and provides consistent contextual grounding across runs.
- Core assumption: The vector knowledge base contains accurate, relevant passages that the base model's pretraining lacks.
- Evidence anchors:
  - [abstract]: "The RAG module improves LLM accuracy and stability in answering true/false conceptual questions by grounding responses in a domain-specific knowledge base, achieving up to 97% stability."
  - [section 3.2]: "RAG significantly improved both metrics across all models, with Qwen-2.5 achieving the highest stability (97%). Common error types observed in baseline outputs included hallucinations, knowledge gaps...These issues were significantly reduced."
  - [corpus]: Weak direct support—neighbor papers focus on code assistants rather than educational RAG applications.
- Break condition: Retrieval fails to surface relevant chunks (low MMR scores), or knowledge base has coverage gaps for the query domain.

### Mechanism 2: Multi-Agent Task Decomposition for Multi-Step Reasoning
- Claim: A three-agent MAS (Manager, Solver, Reviewer) improves accuracy on calculation problems requiring equation derivation and numerical computation.
- Mechanism: Task decomposition separates problem interpretation (Manager), symbolic reasoning and code execution (Solver), and evaluation against ground truth (Reviewer). Each role specializes, reducing error propagation and enabling intermediate verification.
- Core assumption: Problems can be cleanly decomposed into interpretation, computation, and validation stages with distinct capability requirements.
- Evidence anchors:
  - [abstract]: "The MAS module solves complex biomechanics calculation problems by decomposing tasks among Manager, Solver, and Reviewer agents, enabling accurate equation derivation, code execution, and structured explanations."
  - [section 2.5]: "The system adopts a modular design with three cooperative agents...agents share context via a unified State object, which includes the conversation history, current execution status, and ground truth reference."
  - [corpus]: Neighbor paper "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving" supports multi-agent collaboration for reasoning tasks.
- Break condition: Manager produces ambiguous problem statements that Solver misinterprets; or no agent possesses the required domain capability (e.g., advanced physics reasoning).

### Mechanism 3: Hybrid Model Specialization
- Claim: Combining a multimodal model for input interpretation with a text-only model for reasoning/computation outperforms uniform multimodal agent configurations.
- Mechanism: Specialization matches model architecture to task demands—multimodal models handle visual/textual interpretation efficiently; text-only models offer better cost/performance for symbolic computation and structured output generation.
- Core assumption: Different subtasks have different optimal model profiles; communication overhead does not negate specialization gains.
- Evidence anchors:
  - [section 3.4]: "The hybrid MAS system, which combines Qwen2.5-VL for multimodal understanding and Mistral for symbolic computation, produced the most accurate and structured solution...achieved full correctness...receiving a 100% system score."
  - [section 2.8]: "This configuration established a hybrid MAS architecture, aiming to balance calculation cost with task-specific effectiveness."
  - [corpus]: No direct corpus evidence for this specific hybrid finding.
- Break condition: Manager fails to fully translate visual or diagrammatic context into text, causing Solver to miss critical information.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Core mechanism for improving conceptual question accuracy; requires understanding embedding-based retrieval, chunking strategies, and generation fusion.
  - Quick check question: How does RAG differ from simply including the full textbook as context in the prompt?

- Concept: Multi-Agent Coordination and State Sharing
  - Why needed here: Understanding how agents communicate via shared state objects and delegate tasks is essential for debugging MAS pipelines.
  - Quick check question: What failure modes emerge if the Manager agent produces an underspecified problem statement for the Solver?

- Concept: Code Execution in LLM Pipelines
  - Why needed here: The Solver generates and executes Python code; understanding security sandboxing and correctness verification is critical for educational deployment.
  - Quick check question: How does the system detect and handle cases where generated code produces syntactically valid but physically incorrect results?

## Architecture Onboarding

- Component map:
  - RAG pipeline: DocumentLoader → TextSplitter (1000 chars, 200 overlap) → Embedding (mxbai-embed-large-v1) → Vector Store (FAISS with MMR) → LLM with retrieved context
  - MAS pipeline: Manager (multimodal interpretation) → Solver (text-only reasoning/code generation) → Code Runner → Reviewer (evaluation/scoring)
  - Shared State: Conversation history, execution status, ground truth reference passed across agents

- Critical path:
  1. Manager receives raw input (text/image/formula), reformulates into structured natural language problem statement
  2. Solver generates multi-step plan, produces Python code, executes via integrated code runner
  3. Reviewer compares output against ground truth, assigns score and feedback

- Design tradeoffs:
  - Fully multimodal (e.g., GPT-4o for all agents): 82% accuracy, high cost
  - Hybrid (multimodal Manager + text Solver/Reviewer): 81.7% accuracy, lower cost
  - Temperature: Lower (0.6) favors stability; higher (0.8) increases diversity but reduces consistency
  - Prompt specificity: Domain-framed prompts improve accuracy but require careful prompt engineering

- Failure signatures:
  - RAG retrieval failure: Low relevance scores, generic or hallucinated answers
  - MAS communication failure: Ambiguous Manager output leads to incorrect Solver equations
  - Code execution failure: Syntax errors or logical bugs flagged by Reviewer
  - Stability failure: Inconsistent outputs across runs due to high temperature or retrieval variance

- First 3 experiments:
  1. Baseline RAG comparison: Run 100 true/false questions with/without RAG; measure accuracy and stability delta (expect ~10-15% accuracy gain, stability approaching 97%)
  2. Temperature sweep: Test same questions at 0.4, 0.6, 0.8; identify optimal stability/accuracy tradeoff (paper finds 0.6 optimal)
  3. Hybrid vs uniform MAS: Compare fully multimodal vs hybrid configuration on 5 calculation problems; measure accuracy, execution time, and output traceability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does student use of RAG-enhanced LLM and MAS tutoring systems improve actual learning outcomes (e.g., exam scores, problem-solving transfer) compared to traditional study methods or unassisted LLM use?
- Basis in paper: [inferred] The study evaluates system accuracy (82–97%) and stability but includes no user study or measurement of student learning gains in real classroom settings.
- Why unresolved: The paper validates technical performance only; educational effectiveness requires controlled trials with human learners.
- What evidence would resolve it: A randomized controlled study comparing learning gains between students using the AI assistants versus control groups using textbooks or vanilla LLMs, with pre/post assessments.

### Open Question 2
- Question: How well do the RAG and MAS frameworks generalize to other engineering domains (e.g., fluid mechanics, thermodynamics) with different problem structures?
- Basis in paper: [inferred] The system is tested exclusively on biomechanics (100 conceptual questions, 30 calculation problems) using a single textbook knowledge base.
- Why unresolved: Single-domain evaluation cannot confirm whether the architecture, chunking strategy, and agent roles transfer effectively.
- What evidence would resolve it: Replication studies applying the same framework to datasets from at least two other engineering disciplines, reporting accuracy and stability metrics.

### Open Question 3
- Question: What is the optimal agent role configuration and model selection strategy for MAS in multi-step engineering problem-solving?
- Basis in paper: [explicit] The paper notes that "hybrid model configurations outperformed fully multimodal setups" but explores only one hybrid design (Qwen2.5-VL Manager + Mistral Solver/Reviewer).
- Why unresolved: The space of possible agent assignments, model sizes, and communication protocols remains largely unexplored.
- What evidence would resolve it: Systematic ablation experiments varying agent roles, model types, and interaction patterns, with performance and cost metrics.

## Limitations
- The study lacks evaluation of actual student learning outcomes, focusing only on system accuracy and stability metrics
- Prompt engineering details for the multi-agent system are not fully specified but shown to have substantial performance impact (up to 8% accuracy variation)
- The system relies on specific model versions (Qwen2.5-VL-72B-Instruct, Mistral-3.2-24B-Instruct) that may have availability constraints

## Confidence
- High confidence: RAG grounding mechanism improving conceptual accuracy and stability
- Medium confidence: Multi-agent task decomposition benefits
- Medium confidence: Hybrid model specialization advantages
- Low confidence: Generalizability to broader educational contexts

## Next Checks
1. **Prompt sensitivity analysis**: Systematically vary prompt templates for Manager, Solver, and Reviewer agents across 5-10 variations to quantify performance sensitivity and identify optimal configurations.

2. **Knowledge base coverage audit**: Map retrieved chunks to question topics in the true/false dataset to identify coverage gaps and measure the relationship between retrieval relevance scores and answer accuracy.

3. **Student interaction study**: Deploy the system with actual biomechanics students (n≥20) solving both conceptual and calculation problems, measuring not just accuracy but also solution understanding, learning gains, and user satisfaction.