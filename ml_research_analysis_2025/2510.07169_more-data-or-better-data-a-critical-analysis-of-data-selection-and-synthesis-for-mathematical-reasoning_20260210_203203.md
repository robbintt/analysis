---
ver: rpa2
title: More Data or Better Data? A Critical Analysis of Data Selection and Synthesis
  for Mathematical Reasoning
arxiv_id: '2510.07169'
source_url: https://arxiv.org/abs/2510.07169
tags:
- data
- reasoning
- arxiv
- preprint
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates various data construction methods for mathematical
  reasoning in LLMs, using a unified pipeline to reflect industrial training scenarios.
  It compares the effectiveness of open-source datasets, data synthesis techniques
  for pretraining and supervised fine-tuning, and highlights that structuring data
  in interpretable formats or distilling from stronger models yields better performance
  than simply increasing data volume.
---

# More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2510.07169
- Source URL: https://arxiv.org/abs/2510.07169
- Reference count: 9
- Primary result: Structured educational data and distillation from stronger models yield better mathematical reasoning performance than simply increasing data volume

## Executive Summary
This study evaluates various data construction methods for mathematical reasoning in large language models using a unified industrial training pipeline. Through systematic comparisons of open-source datasets and synthesis techniques for both pretraining and supervised fine-tuning, the authors demonstrate that data quality and structure matter more than sheer volume. The work shows that datasets refined into educational formats or generated by advanced reasoning models consistently outperform rule-based or uncurated web-scraped data, providing actionable guidance for cost-effective data curation and scalable model enhancement.

## Method Summary
The research establishes a baseline model trained on a validated mixture of code and math data, then evaluates new datasets through an annealing pipeline that mixes 20% new data with 80% validated data for one epoch. The study compares multiple data synthesis approaches including textbook-style refinement, distillation from stronger reasoning models (like QwQ-32B), and synthetic augmentation of failure cases. Mathematical reasoning accuracy is measured on benchmarks like MATH, GSM8K, and MathBench, with secondary metrics tracking potential interference on other capabilities.

## Key Results
- Structuring raw data into educational formats with explicit definitions and worked examples improves MATH accuracy by +1.72 points compared to unstructured text
- Distilling reasoning traces from stronger models (e.g., QwQ-32B) provides higher-quality supervision, improving performance by +1.92 on MATH
- Rule-based synthetic data and simple retrieval augmentation show minimal gains, while targeted failure-augmentation strategies prove more effective

## Why This Works (Mechanism)

### Mechanism 1: Structured Educational Formatting
Models learn reasoning more effectively when data is presented with explicit definitions, LaTeX formulas, and worked examples. This educational structure reduces cognitive load required to extract signal from noise, facilitating concept acquisition during pretraining.

### Mechanism 2: Strong-Model Distillation
Distilling reasoning traces from advanced models like QwQ-32B generates more coherent chain-of-thought trajectories than using original dataset answers. The student model acquires better problem-solving strategies from these superior reasoning processes.

### Mechanism 3: Synthetic Augmentation of Failure Modes
Generating synthetic data specifically tailored to model failure cases is more effective than naive retrieval of similar data. This targeted approach addresses specific conceptual gaps rather than adding redundant examples.

## Foundational Learning

- **Pretraining vs. SFT Data Dynamics**
  - Why needed: The paper evaluates data strategies differently for pretraining (accumulating knowledge) vs. SFT (learning to follow instructions/reason)
  - Quick check: Does the paper recommend increasing data volume for SFT reasoning tasks? (Answer: No)

- **Annealing / Data Mixing**
  - Why needed: The experimental setup relies on an "annealing" pipeline (20% new data, 80% validated data) to evaluate effectiveness
  - Quick check: Why is the new dataset mixed at only 20% weight during evaluation? (Answer: To isolate its contribution while preserving the base model's existing capabilities)

- **Rule-Based vs. Model-Based Synthesis**
  - Why needed: The paper explicitly contrasts rule-based logic puzzles with model-generated reasoning
  - Quick check: Why did rule-based Knights and Knaves puzzles fail to improve math reasoning significantly? (Answer: Lack of semantic depth/alignment with actual math reasoning tasks)

## Architecture Onboarding

- **Component map:**
  Base Model -> Validated Dataset -> Data Engine -> Synthesis Module -> Annealing Evaluation -> Benchmarks

- **Critical path:**
  1. Train Base Model on Validated Dataset -> Measure Benchmarks
  2. Generate/Augment Data (e.g., via QwQ-32B or Cosmopedia-style prompts)
  3. Mix synthesized data (20% weight) with Validated Dataset (80%)
  4. Re-run Benchmarks; confirm delta is positive and no regression on other tasks

- **Design tradeoffs:**
  - Volume vs. Quality: Better data (distilled/structured) is more cost-effective than scaling more data (web-scraped)
  - Complexity vs. Utility: Generating Long CoT or complex logic puzzles is expensive; simple distillation from strong models offers higher ROI

- **Failure signatures:**
  - Regression on General Tasks: Mixing math data drops coding or knowledge scores
  - Negative Deltas: Raw web-scraped data degrades math reasoning accuracy due to noise
  - Stagnation: Rule-based data shows "little improvement" despite increasing dataset size

- **First 3 experiments:**
  1. Train base model on 80/20 mix of Validated Data vs. Random Web Data to confirm "better data" hypothesis
  2. Distill NaturalReasoning using QwQ, filter for answer consistency, and compare against ground-truth answers
  3. Convert math corpus into "Cosmopedia-style" educational text vs. raw text, measure delta on MathBench

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RL-inspired data synthesis methods systematically outperform distillation-based approaches for mathematical reasoning?
- Basis: Section 6.1 proposes "RL-like synthesis strategies" as promising direction with "absence of experimental results"
- Why unresolved: Authors present these as exploratory directions only; no experiments conducted
- What evidence would resolve: Implement RL-like synthesis with proposed mechanisms and compare against distillation baselines

### Open Question 2
- Question: What data mixing strategies minimize interference effects when combining heterogeneous mathematical datasets with multi-task training data?
- Basis: Section 6.2 states "indiscriminate data mixing can reduce the benefits" and "it is crucial to identify an appropriate data mixing strategy"
- Why unresolved: Paper demonstrates interference occurs but does not propose or test systematic mixing strategies
- What evidence would resolve: Compare mixing strategies across multiple datasets while measuring both math gains and regression on other capabilities

### Open Question 3
- Question: How should mathematical difficulty be defined and measured to enable effective curriculum learning for reasoning tasks?
- Basis: Section 6.3 states "simply partitioning data difficulty based on the number of reasoning steps is not effective"
- Why unresolved: Chain length as proxy for difficulty failed empirically; paper provides no alternative difficulty metrics
- What evidence would resolve: Propose task-relevant difficulty metrics and test whether curriculum training on these hierarchies improves sample efficiency

## Limitations

- The exact composition and quality controls of the "large validated dataset" used as training foundation are unspecified
- Specific thresholds for answer-consistency filtering in distillation experiments are not detailed, affecting reproducibility
- Focus on 3B-scale models raises questions about scalability to larger architectures where data dynamics may differ

## Confidence

- **High Confidence:** Structured educational formatting and distillation from stronger models outperform simple data scaling (MATH accuracy improvements of +1.72 and +1.92)
- **Medium Confidence:** Synthetic augmentation of failure modes being superior to naive retrieval is supported by comparisons but lacks granular methodology detail
- **Low Confidence:** Rule-based logic puzzles consistently underperforming lacks strong empirical backing

## Next Checks

1. Request or reconstruct the exact composition and quality metrics of the validated baseline dataset to establish proper experimental context
2. Replicate the most successful approaches (structured formatting, QwQ distillation) on a 7B-scale model to test architectural generalization
3. Conduct detailed error analysis on models trained with synthetic augmentation to verify improvements stem from addressing actual reasoning gaps rather than memorization patterns