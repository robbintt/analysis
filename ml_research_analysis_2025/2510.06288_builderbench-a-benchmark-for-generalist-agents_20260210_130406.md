---
ver: rpa2
title: BuilderBench -- A benchmark for generalist agents
arxiv_id: '2510.06288'
source_url: https://arxiv.org/abs/2510.06288
tags:
- cube
- tasks
- place
- pick
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BuilderBench is a new benchmark for developing generalist agents
  that learn through exploration and interaction. The benchmark uses a simple block-building
  environment where agents must explore without supervision to acquire general principles,
  then apply them to build unseen structures.
---

# BuilderBench -- A benchmark for generalist agents

## Quick Facts
- arXiv ID: 2510.06288
- Source URL: https://arxiv.org/abs/2510.06288
- Authors: Raj Ghugare; Catherine Ji; Kathryn Wantlin; Jin Schofield; Benjamin Eysenbach
- Reference count: 40
- Key outcome: BuilderBench tests whether agents can learn general principles through exploration and apply them to build unseen structures, with current algorithms struggling on basic tasks.

## Executive Summary
BuilderBench is a new benchmark designed to evaluate generalist agents that learn through exploration and interaction. It uses a block-building environment where agents must acquire general principles about physics, geometry, and planning without external supervision, then apply these skills to construct unseen target structures. The benchmark highlights the gap between current algorithms and the capabilities needed for true generalization, as existing methods struggle with even simple construction tasks. BuilderBench aims to drive progress in agents that learn from experience rather than relying solely on human-provided data.

## Method Summary
BuilderBench is a block-building benchmark that evaluates agents on their ability to learn general principles through exploration and apply them to unseen tasks. The environment uses a simplified robotic hand to manipulate cubes in a physics simulator (MuJoCo + JAX), with tasks ranging from simple stacking to complex structures requiring scaffolding and counterweight reasoning. Agents are trained using either multi-task self-supervised protocols (explore without task labels) or single-task supervised learning. The benchmark includes 42 tasks with dense and sparse reward options, measuring success via Hungarian assignment of cubes to targets within 2cm tolerance.

## Key Results
- Current algorithms achieve trivial performance on multi-cube tasks in the self-supervised protocol, highlighting the difficulty of learning through exploration alone.
- Even strong open-loop language models fail to produce valid plans for physically constrained tasks, suggesting the reasoning required is grounded in physics rather than linguistic patterns.
- The hardware-accelerated simulator enables RL training 10-100x faster than CPU-based benchmarks, making large-scale interaction experiments feasible.

## Why This Works (Mechanism)

### Mechanism 1: Self-Supervised Skill Acquisition
Agents explore an open-ended environment without external supervision to acquire general principles about physics and geometry. During training, they propose their own goals and learn to reach them via trial and error, forcing the policy to learn reusable primitives rather than memorizing specific trajectories. The core assumption is that agents can compress environmental dynamics into a world model that generalizes zero-shot to held-out evaluation tasks.

### Mechanism 2: Reasoning via Embodied Construction
Building complex structures requires physical intuition that emerges from action-observation loops rather than linguistic pattern matching. Tasks are designed to require distinct high-level skills like counterweights and scaffolding, forcing compositional logic. The difficulty ensures simple policy gradient methods are insufficient, as the probability of randomly discovering multi-step solutions is near zero.

### Mechanism 3: Accelerated Interaction Scaling
Hardware acceleration via MuJoCo and JAX removes CPU bottlenecks, enabling orders of magnitude more environment steps per second. This massive data throughput theoretically allows simple RL algorithms to scale to complex behaviors that would otherwise be computationally prohibitive, though the speed advantage depends on the algorithm's ability to handle variance in the data.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - Why needed here: BuilderBench is explicitly defined as an MDP with states, actions, and transition dynamics. You must understand how the robotic hand's position/orientation relates to cube positions and how the 5-dimensional action space modifies this state.
  - Quick check question: Can you explain why the "Maximum Overhang" task is harder than a simple "Stack 2 Cubes" task in the context of MDP horizon length?

- **Concept: Goal-Conditioned Reinforcement Learning**
  - Why needed here: The core "Multi-task self-supervised" protocol relies on the agent learning a policy that conditions on a goal state (target structure). Understanding how to condition a policy on a goal vector is essential for the training phase.
  - Quick check question: In the self-supervised protocol, how does the agent know what goal to practice if it doesn't have access to the test set?

- **Concept: Reward Shaping (Dense vs. Sparse)**
  - Why needed here: The paper offers both dense (distance-based) and sparse (success-based) rewards. The "T-Block" example shows that naive placement fails due to instability; understanding how reward functions guide (or misguide) the physics discovery is critical.
  - Quick check question: Why might a dense reward function based solely on distance to target fail for the "Leaning Tower" task?

## Architecture Onboarding

- **Component map:**
  - Simulator (JAX/MuJoCo) -> Task Suite -> Agent -> Reward Engine
  - The simulator handles physics and collisions, the task suite defines target structures, the agent (e.g., PPO, MEGA) processes states and goals to output actions, and the reward engine computes success metrics via Hungarian assignment.

- **Critical path:**
  1. Install JAX and MuJoCo bindings
  2. Run a sanity check (spawn environment, take random actions, render)
  3. Train PPO on cube-2-task1 using dense rewards to verify pipeline matches benchmark curves
  4. Switch to self-supervised protocol (e.g., MEGA algorithm) to see if agent discovers stacking autonomously

- **Design tradeoffs:**
  - Speed vs. Realism: Simplified robotic hand (crane-like) maximizes throughput and reduces inverse kinematics complexity
  - Dense vs. Sparse Rewards: Dense rewards speed up learning but risk local optima (unstable structures); sparse rewards are "pure" but often make exploration intractable

- **Failure signatures:**
  - "Collapsing Success": Agent achieves high reward momentarily but structure collapses immediately after episode ends
  - "Freezing": Agent learns to do nothing to avoid negative rewards in sparse settings
  - LLM Hallucination: Model suggests physically impossible moves like "rotate block inside another block"

- **First 3 experiments:**
  1. Baseline Verification: Train PPO on cube-2-task1 with dense rewards for 1-2B env steps to verify training completes in ~30 min and achieves non-zero success
  2. Ablation on Reward: Compare success rates on "T-Block" task using Dense vs. Sparse rewards to see if dense signal prevents learning 45Â° rotation trick
  3. Exploration Test: Run MEGA algorithm on 3-cube environment and measure unique states discovered vs. random action sampling

## Open Questions the Paper Calls Out

- What specific algorithmic architectures or exploration mechanisms are required to solve the multi-task self-supervised protocol, given that current unsupervised RL algorithms fail?
- Can reinforcement learning agents discover novel, stable solutions to tasks in the suite that are currently unsolved even by the human authors?
- Can grounded language models succeed where open-loop models failed by utilizing the simulator's feedback to generate stable construction plans?

## Limitations

- Current algorithms achieve only trivial performance on multi-cube tasks in the self-supervised protocol
- Even strong open-loop language models fail on physically constrained tasks, suggesting fundamental limitations in reasoning without grounding
- The benchmark may be too difficult for current approaches, potentially limiting immediate progress

## Confidence

- **Method Description**: High - The paper provides detailed specification of the environment, tasks, and evaluation protocols
- **Reproducibility**: Medium - While the framework is well-specified, key hyperparameters and implementation details are not fully documented
- **Results Interpretation**: High - The difficulty of the benchmark and limitations of current approaches are clearly demonstrated
- **Open Questions**: Medium - The paper identifies important gaps but doesn't provide clear paths to resolution

## Next Checks

1. Clone the repository and verify the environment can be launched with random actions
2. Train PPO on a simple 2-block task and verify success rate matches reported benchmarks
3. Run the self-supervised evaluation protocol and document performance on 1-3 cube tasks to confirm the difficulty gap observed in the paper