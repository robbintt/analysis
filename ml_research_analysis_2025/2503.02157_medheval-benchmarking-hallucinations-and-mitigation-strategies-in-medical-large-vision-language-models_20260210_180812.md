---
ver: rpa2
title: 'MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical
  Large Vision-Language Models'
arxiv_id: '2503.02157'
source_url: https://arxiv.org/abs/2503.02157
tags:
- hallucination
- mitigation
- medical
- evaluation
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedHEval introduces a comprehensive benchmark for evaluating and
  mitigating hallucinations in medical large vision-language models. It categorizes
  hallucinations into visual misinterpretation, knowledge deficiency, and context
  misalignment, and constructs datasets and metrics for each type.
---

# MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models

## Quick Facts
- arXiv ID: 2503.02157
- Source URL: https://arxiv.org/abs/2503.02157
- Authors: Aofei Chang; Le Huang; Parminder Bhatia; Taha Kass-Hout; Fenglong Ma; Cao Xiao
- Reference count: 40
- Key outcome: Introduces a comprehensive benchmark for evaluating and mitigating hallucinations in medical large vision-language models across visual, knowledge, and context categories.

## Executive Summary
MedHEval introduces a comprehensive benchmark for evaluating and mitigating hallucinations in medical large vision-language models. It categorizes hallucinations into visual misinterpretation, knowledge deficiency, and context misalignment, and constructs datasets and metrics for each type. Extensive experiments on 11 (Med)-LVLMs and 7 mitigation techniques reveal that models struggle with all hallucination types, with limited effectiveness of existing mitigation methods—especially for knowledge and context-based errors. Notably, some models like CheXagent outperform GPT-4o in hallucination resistance, and mitigation methods enhancing visual grounding show more consistent improvements. These findings highlight the need for specialized training and mitigation strategies tailored to each hallucination type to improve reliability in medical applications. MedHEval provides a standardized framework for advancing research in trustworthy medical AI.

## Method Summary
The paper constructs a benchmark using 15,976 VQA pairs from SLAKE, VQA-RAD, IU-Xray, MIMIC-CXR, and MIMIC-IV datasets. It evaluates 11 (Med)-LVLMs including GPT-4o, LLaVA-Med, and CheXagent using 7 plug-and-play mitigation techniques (VCD, OPERA, DoLa, AVISC, M3ID, DAMRO, PAI). The evaluation framework includes close-ended accuracy metrics and open-ended generation metrics (CHAIR, RaTEScore) plus LLM-based hallucination scoring via Claude 3.5 Sonnet. Ground truth for open-ended questions is generated using GPT-4. The benchmark systematically tests three hallucination types: visual misinterpretation, knowledge deficiency, and context misalignment.

## Key Results
- Medical LVLMs struggle with all hallucination types, with limited effectiveness of existing mitigation methods—especially for knowledge and context-based errors
- CheXagent outperforms GPT-4o in hallucination resistance, demonstrating that specialized medical training can improve reliability
- Mitigation methods enhancing visual grounding (PAI, OPERA, DoLa) show more consistent improvements, particularly for visual hallucinations
- Fine-tuning general-domain LVLMs on medical data may degrade reasoning capabilities needed for context integration

## Why This Works (Mechanism)

### Mechanism 1: Visual Hallucination via Attention Decoupling
If a Med-LVLM exhibits visual misinterpretation hallucinations, it is likely due to decoupling between visual features and language generation, where the LLM decoder prioritizes text priors over visual evidence. Mitigation methods that explicitly penalize "over-trust" in generated text or amplify visual attention maps force the model to ground tokens in the image region-of-interest, reducing object identification errors. Core assumption: The vision encoder correctly extracts features, but the LLM decoder ignores them during generation. Evidence: PAI, OPERA, and DoLa consistently reduce hallucinations, reinforcing the importance of stronger visual grounding. Break condition: If the error stems from a blind spot in the vision encoder itself, attention-based mitigation will fail.

### Mechanism 2: Knowledge Hallucination via Latent Knowledge Gaps
Knowledge deficiency hallucinations occur because visual grounding mechanisms are intact but the model's parameterized medical knowledge is factually incorrect or missing. Unlike visual errors, knowledge errors cannot be fixed by looking harder at the image. Mitigation strategies designed for visual bias fail here because the error resides in the LLM's internal weights rather than the cross-modal attention layer. Core assumption: The model correctly identifies the visual feature but retrieves an incorrect cause. Evidence: PAI is highly effective for close-ended visual hallucinations but fails to mitigate knowledge-level hallucinations. Break condition: If a model has correct latent knowledge but fails to retrieve it due to prompt phrasing, this is a retrieval/alignment issue.

### Mechanism 3: Context Hallucination via Fine-Tuning Degradation
Fine-tuning general-domain LVLMs on specific medical data may degrade their inherent reasoning capabilities, leading to context misalignment where the model ignores external clinical history. Specialized training on narrow datasets might optimize for pattern matching at the expense of broader reasoning required to integrate patient history with visual findings. Core assumption: The model possesses logical capacity but fails to invoke it when processing clinical notes alongside images. Evidence: Fine-tuning with multimodal medical data may decrease the reasoning ability of the original LVLMs. Break condition: If the model has never seen clinical notes during training, the failure is an out-of-distribution generalization error.

## Foundational Learning

- **Concept: LVLM Architecture (Visual Encoder + Projector + LLM)**
  - Why needed: To distinguish whether a hallucination originates in the "eyes" (Encoder), the "translator" (Projector), or the "brain" (LLM). MedHEval attributes errors differently based on this distinction.
  - Quick check: Does the model correctly identify the organ but hallucinate the diagnosis? (Implicates the LLM/Knowledge, not the Encoder)

- **Concept: Hallucination Taxonomy (Object vs. Knowledge vs. Context)**
  - Why needed: The paper demonstrates that mitigation strategies are not transferable across these categories. Applying a visual fix to a knowledge problem yields negative ROI.
  - Quick check: If a model calls a femur a humerus, is that a knowledge error? (No, it is Visual Misinterpretation)

- **Concept: Evaluation Metrics (CHAIR vs. Factual Accuracy)**
  - Why needed: MedHEval uses specific metrics like CHAIR (for visual object hallucination) and LLM-based scoring (Claude 3.5 Sonnet) for open-ended knowledge checking.
  - Quick check: Why is Accuracy insufficient for open-ended report generation? (Because it fails to capture semantic hallucinations where the sentence structure is valid but the facts are wrong)

## Architecture Onboarding

- **Component map:** Input Layer (MIMIC-CXR/SLAKE/VQA-RAD images+reports) -> Construction Layer (GPT-4 synthesizes QA pairs) -> Model Layer (11 (Med)-LVLMs) -> Mitigation Wrapper (VCD, OPERA, etc.) -> Evaluation Layer (Accuracy/CHAIR/RaTEScore + Claude 3.5 Sonnet scoring)
- **Critical path:** 1. Select hallucination type (Visual, Knowledge, or Context) 2. Load corresponding dataset 3. Run inference with/without mitigation wrappers 4. Score outputs using standard metrics and Claude 3.5 Sonnet API for $S_h$ score
- **Design tradeoffs:** Ground Truth Generation relies on GPT-4 to generate open-ended ground truth, scaling data creation but risking embedding GPT-4's own biases. Model Coverage excludes XrayGPT and RadFM from close-ended evaluations due to instruction-following instability.
- **Failure signatures:** High Visual Hallucination / Low Knowledge Hallucination indicates strong text priors but weak visual binding. Mitigation Induced Drop (OPERA/PAI causing Accuracy to drop significantly on Knowledge tasks) suggests over-penalizing necessary textual reasoning.
- **First 3 experiments:** 1. Visual Baseline: Run LLaVA-Med on CXR-VisHal to establish baseline for Anatomical vs. Symptom hallucinations 2. Mitigation Ablation: Apply PAI to same run, confirm improvement in Acc-S but check for degradation in Acc-M 3. Knowledge Stress Test: Run LLaVA-Med-1.5 on open-ended Knowledge dataset and manually inspect "Hallucination Score" outputs from Claude 3.5 Sonnet

## Open Questions the Paper Calls Out

- How can mitigation strategies be effectively redesigned to target knowledge deficiency hallucinations in Med-LVLMs? The paper concludes that visual-focused mitigation techniques are largely ineffective against knowledge-level hallucinations, emphasizing the need for improved knowledge integration in Med-LVLM backbone training.
- How can models be trained or prompted to prevent the performance degradation in context misalignment observed when fine-tuning on medical datasets? Section 5 notes that general-domain LVLMs often outperform Med-LVLMs on context misalignment, suggesting that fine-tuning with multimodal medical data may decrease reasoning ability.
- What specific mechanisms can balance the trade-off between minimizing hallucinations and maintaining report completeness in open-ended generation? The discussion in Section 3.2 highlights that while methods like VCD reduce hallucination rates, they often do so at the expense of recall.

## Limitations

- Open-ended knowledge hallucination evaluation relies heavily on Claude 3.5 Sonnet's hallucination scoring, which may introduce subjectivity and API dependency issues
- Ground truth generation for open-ended questions uses GPT-4, potentially embedding its own biases into the benchmark
- Study focuses on chest X-ray and general medical imaging domains, limiting generalizability to other medical specialties
- Exclusion of XrayGPT and RadFM from close-ended evaluations due to instruction-following instability represents a coverage gap

## Confidence

- High confidence: Hallucination type categorization and differential mitigation effectiveness
- Medium confidence: Specific performance rankings of individual models
- Medium confidence: Generalizability of findings to non-chest X-ray domains

## Next Checks

1. Replicate the Claude 3.5 Sonnet hallucination scoring using multiple API calls to assess score stability and inter-rater reliability
2. Test excluded models (XrayGPT, RadFM) with optimized instruction-following prompts to complete the benchmark coverage
3. Validate findings on non-chest X-ray medical imaging domains (e.g., dermatology, pathology) to assess domain generalizability