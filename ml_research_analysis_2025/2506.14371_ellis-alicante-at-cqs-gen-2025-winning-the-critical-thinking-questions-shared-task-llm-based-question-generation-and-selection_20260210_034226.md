---
ver: rpa2
title: 'ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared
  task: LLM-based question generation and selection'
arxiv_id: '2506.14371'
source_url: https://arxiv.org/abs/2506.14371
tags:
- questions
- critical
- scheme
- llmj
- llmq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work tackles the challenge of generating critical questions
  to foster deeper reasoning in LLM-based education, countering concerns about superficial
  learning. The authors propose a two-step framework: a small open-source LLM (Questioner)
  generates multiple candidate questions from debate interventions, and a second small
  LLM (Judge) selects the three most useful ones.'
---

# ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection

## Quick Facts
- arXiv ID: 2506.14371
- Source URL: https://arxiv.org/abs/2506.14371
- Reference count: 11
- Primary result: 62.4% useful question rate on test data, ranking first in CQs-Gen 2025 shared task

## Executive Summary
This work addresses the challenge of generating critical questions to foster deeper reasoning in LLM-based education, countering concerns about superficial learning. The authors propose a two-step framework where a small open-source LLM (Questioner) generates multiple candidate questions from debate interventions, and a second small LLM (Judge) selects the three most useful ones. Using Llama 3.1 8B as Questioner and Gemma 2 9B as Judge—optionally incorporating argumentation schemes—they achieve 62.4% useful question rate on test data, ranking first in the CQs-Gen 2025 shared task. The approach demonstrates that small, locally deployable models can outperform direct generation approaches when combined with selective argumentation scheme integration and a separate evaluation stage.

## Method Summary
The framework employs a two-LLM architecture: a Questioner (Llama 3.1 8B) generates multiple candidate questions from debate interventions, and a Judge (Gemma 2 9B) selects the top three most relevant ones. The Questioner produces eight candidates total (four with argumentation schemes and four without), which are merged and evaluated by the Judge. The system uses argumentation schemes from Walton et al. (2008) to guide question generation, with the "Both" configuration—merging questions from scheme-based and scheme-free prompts—achieving optimal performance. No fine-tuning was performed due to limited training data (74 examples), relying instead on in-context learning with carefully constructed prompts.

## Key Results
- Achieved 62.4% useful question rate on test data, ranking first in CQs-Gen 2025 shared task
- LLMJ selection improved usefulness rate by 3.4 percentage points over random selection (statistically significant, p < 0.05)
- Best performance achieved with "Both" configuration (schemes + no schemes), where 81% of selected questions were generated with argumentation schemes
- High "Not able to evaluate" rate (25-46%) indicates generated questions often don't match reference annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing question generation into separate generation and selection phases improves output quality over direct generation.
- Mechanism: The Questioner LLM produces multiple candidate questions (N=4-8), creating diverse pool. The Judge LLM evaluates these candidates against relevance criteria and selects top 3. This separation allows each component to specialize: generation focuses on breadth, selection focuses on quality filtering.
- Core assumption: Judge LLM can reliably distinguish useful questions from unhelpful/invalid ones based on semantic similarity to reference questions and explicit evaluation criteria.
- Evidence anchors:
  - [abstract] "a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones... demonstrating the potential of the proposed LLM-based approach"
  - [Section 4.4] "LLMJ achieves a usefulness rate that is 3.4 percentage points higher than random selection, a statistically significant improvement (p < 0.05, McNemar's test)"
  - [corpus] Related work on LLM-as-a-judge paradigms (Li et al., 2024) cited but not directly evaluated in this domain

### Mechanism 2
- Claim: Selective integration of argumentation schemes into prompts improves question relevance without sacrificing diversity.
- Mechanism: Walton et al.'s argumentation schemes provide stereotypical reasoning patterns with associated critical question templates. When schemes are included in prompts, Questioner generates more targeted questions. "Both" configuration—merging questions from scheme-based and scheme-free prompts—achieved highest performance (62.4% useful).
- Core assumption: Argumentation schemes correctly identify reasoning structure in debate interventions, and their associated question templates are transferable to new contexts.
- Evidence anchors:
  - [Section 4.2] "the best performance is achieved in the Both configuration... 81% of the questions selected by LLMJ were generated with the argumentation scheme in the prompt"
  - [Section 4.2] "strictly enforcing these schemes can reduce diversity. Thus, a selective use of schemes strikes a better balance"
  - [corpus] Related CQs-Gen 2025 submission (DayDreamer) also uses argument scheme completion but with chain-of-thought prompting—complementary approach, no direct comparison available

### Mechanism 3
- Claim: Small open-source LLMs (7B-14B parameters) can outperform direct fine-tuning approaches in low-resource settings when guided by domain-specific prompting.
- Mechanism: Rather than fine-tuning on limited data (74 training examples), framework leverages in-context learning with carefully constructed prompts containing role definitions, task objectives, and argumentation theory. This avoids overfitting to small dataset while exploiting pretrained knowledge.
- Core assumption: Pretrained LLMs have sufficient reasoning capabilities and argumentation knowledge embedded in their weights to perform task with prompting alone.
- Evidence anchors:
  - [Section 5] "Given the small size of the dataset, traditional strategies such as fine-tuning or data augmentation... yielded limited improvement"
  - [Section A.4] "Attempts to fine-tune both LLMQ and LLMJ were inconclusive... likely attributable to task complexity combined with the limited size of the training dataset"
  - [corpus] No direct corpus evidence comparing small vs. large LLMs on this specific task; assumption remains untested beyond models evaluated

## Foundational Learning

- Concept: **Argumentation Schemes (Walton et al., 2008)**
  - Why needed here: These are structural templates (e.g., Expert Opinion, Cause to Effect) used to classify debate arguments and generate targeted critical questions. Essential for prompt engineering and interpreting model outputs.
  - Quick check question: Given argument "We should trust this policy because Dr. Smith endorses it," which scheme applies and what critical question would challenge it?

- Concept: **Elder and Paul's Critical Thinking Framework**
  - Why needed here: Paper explicitly grounds two-LLM architecture in this framework—creative dimension (Questioner), analytic/evaluative dimensions (Judge). Understanding this mapping explains why separation improves performance.
  - Quick check question: Which component of framework corresponds to generating diverse hypotheses versus selecting strongest one?

- Concept: **Cosine Similarity for Semantic Evaluation**
  - Why needed here: Automatic evaluation metric uses cosine similarity between generated questions and reference questions to assign usefulness labels. Understanding limitations explains high "Not able to evaluate" rates.
  - Quick check question: Why might genuinely useful generated question receive "Not able to evaluate" if no similar reference question exists?

## Architecture Onboarding

- Component map: Debate intervention text → Questioner LLM (Llama 3.1 8B) → 8 candidate questions → Judge LLM (Gemma 2 9B) → 3 selected critical questions

- Critical path:
  1. Preprocess intervention → extract/verify argumentation scheme annotations
  2. Construct two prompts for LLMQ (with schemes, without schemes)
  3. LLMQ generates 4 questions per prompt (8 total)
  4. Merge candidate pools → construct LLMJ prompt with all candidates
  5. LLMJ selects top 3 based on relevance and redundancy handling
  6. (Evaluation time) Compute cosine similarity to reference questions → assign labels

- Design tradeoffs:
  - **Model size vs. deployability**: Small models (7B-14B) enable local deployment on student laptops but underperform GPT-4o (50.0% vs. 67.6% useful on test set with manual evaluation)
  - **Scheme usage vs. diversity**: Strict scheme enforcement reduces question diversity; "Both" configuration mitigates but doubles inference calls
  - **Candidate pool size**: Larger pools (8 vs. 4) showed marginal improvement in experiments but increase Judge LLM context length and processing time
  - **Fine-tuning vs. prompting**: Fine-tuning failed due to limited data (74 examples), but prompting may not generalize to domains outside political discourse

- Failure signatures:
  - **High "Not able to evaluate" rate** (25-46%): Generated questions don't match reference questions; may indicate overfitting to evaluation metric or genuine novelty
  - **Judge-Oracle gap** (34.2 percentage points): LLMJ fails to identify useful questions that exist in candidate pool; suggests evaluator weakness
  - **Invalid question generation** (2-5%): Questions that cannot challenge any argument; may indicate prompt misunderstanding or context loss
  - **Domain mismatch**: Political debate dataset may not transfer to educational or other domains without re-evaluation

- First 3 experiments:
  1. **Baseline replication**: Implement Llama 3.1 8B (Questioner) + Gemma 2 9B (Judge) with "Both" scheme configuration on held-out validation split; measure Useful/Invalid/NoEval distribution to verify reproducibility
  2. **Judge ablation**: Compare LLMJ selection against random selection and oracle on same candidate pool to quantify Judge's contribution (target: >3 percentage point improvement over random)
  3. **Scheme sensitivity**: Test "With (one)" vs. "Without" vs. "Both" configurations to validate that selective scheme integration outperforms strict enforcement or complete omission on domain data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does framework's performance generalize to domains beyond political debates, such as educational settings?
- Basis in paper: [explicit] Authors explicitly state in Limitations section that narrow scope of political dataset limits generalizability and that future work should evaluate framework on broader datasets
- Why unresolved: Study relied exclusively on dataset of real political debate interventions, leaving system's effectiveness in other contexts unproven
- What evidence would resolve it: Empirical results from deploying Questioner-Judge framework on diverse datasets (e.g., educational texts or scientific abstracts) showing comparable percentages of "Useful" questions

### Open Question 2
- Question: How can gap between Judge LLM's selection accuracy and theoretical oracle upper bound be effectively closed?
- Basis in paper: [explicit] Authors highlight that Judge (LLMJ) shows "substantial gap compared to the oracle," indicating significant room for improvement in identifying best candidate questions
- Why unresolved: While Judge outperforms random selection, it fails to consistently distinguish most useful questions from generated candidates
- What evidence would resolve it: Modified selection mechanism or model that achieves usefulness rate statistically closer to 93.5% upper bound established by oracle

### Open Question 3
- Question: To what extent does cosine similarity metric underestimate utility of generated questions that do not match reference annotations?
- Basis in paper: [inferred] Paper notes that many questions labeled "Not able to evaluate" were manually assessed as useful during competition, suggesting automatic metric introduces risk of mis-estimation
- Why unresolved: Automatic evaluation relies on matching reference questions; cannot recognize novel, valid critical questions that fall outside pre-annotated set
- What evidence would resolve it: Comprehensive human evaluation of "Not able to evaluate" category to determine true false-negative rate of automatic metric

## Limitations
- Dataset availability: D_shared_train dataset used for training and evaluation is not publicly accessible, making independent validation difficult
- Evaluation metric limitations: High "Not able to evaluate" rates (25-46%) suggest metric may underestimate utility of novel questions
- Domain specificity: Framework developed and tested exclusively on political debate dataset, limiting generalizability to other domains

## Confidence

- **High confidence**: Two-LLM architecture (Questioner + Judge) effectively outperforms direct generation approaches, supported by statistically significant improvements (3.4 percentage points, p < 0.05) and consistent performance across evaluations
- **Medium confidence**: Selective argumentation scheme integration improves performance, though optimal balance point may be dataset-specific and doubling of inference calls represents unquantified cost
- **Medium confidence**: Small open-source LLMs (7B-14B) are viable for this task, though direct comparisons with larger models on this specific dataset are unavailable, and approach may not generalize to domains outside political discourse

## Next Checks

1. **Dataset access validation**: Obtain or replicate D_shared_train dataset to verify reported performance metrics and test failure of fine-tuning approaches independently

2. **Judge contribution quantification**: Conduct ablation studies comparing LLMJ selection against random selection and oracle performance on same candidate pools to independently verify 3.4 percentage point improvement claim

3. **Generalization testing**: Evaluate framework on non-political discourse datasets (e.g., educational texts, scientific papers) to test whether argumentation scheme approach and two-LLM architecture transfer beyond original domain