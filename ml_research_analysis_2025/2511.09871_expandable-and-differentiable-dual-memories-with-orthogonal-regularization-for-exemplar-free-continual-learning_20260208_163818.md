---
ver: rpa2
title: Expandable and Differentiable Dual Memories with Orthogonal Regularization
  for Exemplar-free Continual Learning
arxiv_id: '2511.09871'
source_url: https://arxiv.org/abs/2511.09871
tags:
- memory
- task
- learning
- tasks
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a differentiable dual-memory approach for exemplar-free
  continual learning that decomposes data into shared and task-specific sub-features
  stored in complementary memories. The method addresses catastrophic forgetting by
  freezing critical memory slots after each task and minimally expanding capacity
  to accommodate new concepts.
---

# Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning

## Quick Facts
- arXiv ID: 2511.09871
- Source URL: https://arxiv.org/abs/2511.09871
- Authors: Hyung-Jun Moon; Sung-Bae Cho
- Reference count: 19
- Primary result: Achieves final accuracies of 55.13%, 37.24%, and 30.11% on CIFAR-10, CIFAR-100, and TinyImageNet respectively, with relative improvements exceeding 26% over previous state-of-the-art.

## Executive Summary
This paper proposes an exemplar-free continual learning method that uses differentiable dual memories with orthogonal regularization to address catastrophic forgetting. The approach decomposes data into shared and task-specific sub-features stored in complementary memories, freezing critical memory slots after each task and minimally expanding capacity for new concepts. The method demonstrates superior performance on CIFAR-10, CIFAR-100, and TinyImageNet benchmarks, outperforming 14 state-of-the-art methods in class-incremental learning scenarios.

## Method Summary
The method introduces dual differentiable key-value memories inserted after residual blocks in a ResNet-18 backbone. It maintains both shared memory (M^s) for reusable features and task-specific memory (M^t) for unique knowledge. After each task, the system freezes important memory slots based on their contribution to learning and expands capacity minimally. Orthogonal regularization enforces geometric separation between preserved and newly learned components, while memory-guided alignment preserves inter-task knowledge without exemplars. The training procedure includes cross-entropy loss, memory alignment loss, and orthogonal regularization, with BatchNorm adaptation on frozen previous models before each new task.

## Key Results
- Final accuracies: 55.13% (CIFAR-10), 37.24% (CIFAR-100), 30.11% (TinyImageNet)
- Relative improvements exceeding 26% over previous state-of-the-art methods
- Feature representations closest to upper bound joint-training strategy across multiple alignment metrics

## Why This Works (Mechanism)
The dual-memory architecture separates shared and task-specific knowledge, allowing the model to preserve general features while adapting to new tasks. Memory-guided alignment ensures knowledge transfer between tasks through attention-based similarity matching. Orthogonal regularization prevents interference between frozen and active memory slots, maintaining geometric separation. The expandable memory design with intelligent slot freezing and expansion enables minimal capacity growth while preserving critical knowledge.

## Foundational Learning
- **Catastrophic forgetting**: Neural networks lose previously learned knowledge when trained on new tasks; critical to address in continual learning.
  - Why needed: Without mitigation, performance degrades rapidly on older tasks.
  - Quick check: Monitor accuracy drop across task boundaries.

- **Class-incremental learning (CIL)**: Learning new classes sequentially without task IDs at inference.
  - Why needed: Real-world scenarios require learning from data streams without explicit task boundaries.
  - Quick check: Evaluate on all seen classes after final task without task identifiers.

- **Orthogonal regularization**: Enforces geometric separation between different subspaces in feature space.
  - Why needed: Prevents interference between preserved and newly learned knowledge.
  - Quick check: Monitor L_orth magnitude and ensure it remains bounded.

- **Attention-based memory retrieval**: Uses cosine similarity to match query features with memory slots.
  - Why needed: Enables efficient retrieval of relevant stored knowledge.
  - Quick check: Verify attention vectors sum to 1 and produce meaningful similarity scores.

## Architecture Onboarding
- **Component map**: ResNet-18 -> Dual memories (after blocks 1 & 2) -> Attention retrieval -> Cross-entropy + alignment + orthogonal loss
- **Critical path**: Input -> Feature extraction -> Memory query (cosine attention) -> Prediction + alignment loss -> Slot importance calculation -> Freezing/expansion
- **Design tradeoffs**: Memory capacity vs. performance (L^ℓ = 1000 initial slots), hyperparameter sensitivity (λ_ortho <= 20, λ_mem >= 1.0), computational overhead of dual memories
- **Failure signatures**: Training divergence when λ_ortho >= 20, poor retention when λ_mem <= 1.0, instability with L^ℓ >= 2000 slots
- **3 first experiments**: 1) Verify memory attention produces valid similarity distributions, 2) Test slot freezing mechanism with synthetic importance scores, 3) Validate orthogonal regularization prevents interference between frozen and active slots

## Open Questions the Paper Calls Out
- Can the dual-memory mechanism be effectively adapted to transformer-based architectures like Vision Transformers?
- Does modeling the differentiable memory as a class-level relational graph improve knowledge transfer?
- How does EDD perform under extreme task shift where sequential tasks share minimal common structure?

## Limitations
- Performance may degrade under extreme task shifts where sequential tasks share minimal common structure
- Memory expansion mechanism requires careful tuning of freezing ratios and expansion rates
- Current implementation is specific to ResNet architectures and may not generalize to other backbones without modification

## Confidence
- **High confidence**: The general algorithmic framework of dual memories, orthogonal regularization, and memory-guided alignment is well-defined and reproducible
- **Medium confidence**: Performance improvements over baselines are reported, but exact implementation details of memory projections and distillation could affect results
- **Medium confidence**: Feature alignment metrics show improvement, but the specific implementation of the alignment loss and its contribution to overall performance requires careful validation

## Next Checks
1. Implement the memory architecture with different key/value dimensions and projection schemes to verify sensitivity to these design choices
2. Run ablation studies on λ_ortho and λ_mem hyperparameters to confirm the claimed stability ranges (λ_ortho <= 20, λ_mem >= 1.0) produce the reported performance
3. Validate the memory expansion and freezing mechanism by tracking slot importance scores across tasks to ensure the pruning ratio of 0.15 and class-proportional freezing rule are correctly implemented