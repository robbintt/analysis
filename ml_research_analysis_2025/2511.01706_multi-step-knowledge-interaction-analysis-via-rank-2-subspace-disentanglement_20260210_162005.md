---
ver: rpa2
title: Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement
arxiv_id: '2511.01706'
source_url: https://arxiv.org/abs/2511.01706
tags:
- knowledge
- subspace
- interaction
- answer
- rank-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a rank-2 projection subspace for disentangling
  Parametric Knowledge (PK) and Context Knowledge (CK) contributions during Natural
  Language Explanation (NLE) generation in large language models. Prior approaches
  using rank-1 subspaces fail to capture diverse knowledge interaction types (supportive,
  complementary, conflicting, irrelevant, suppression), resulting in poor representation.
---

# Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement

## Quick Facts
- arXiv ID: 2511.01706
- Source URL: https://arxiv.org/abs/2511.01706
- Reference count: 40
- Key outcome: Rank-2 subspaces achieve 1.0 cumulative explained variance for disentangling parametric (PK) and context knowledge (CK) in LLM natural language explanations, capturing diverse interaction types across multi-step generation.

## Executive Summary
This work introduces a rank-2 projection subspace framework for disentangling parametric knowledge (PK) and context knowledge (CK) contributions during natural language explanation (NLE) generation in large language models. Prior rank-1 approaches fail to capture the diverse interaction types (supportive, complementary, conflicting, irrelevant, suppression) that arise when models combine stored parametric knowledge with contextual information. The proposed method learns two orthonormal basis vectors representing PK and CK directions, enabling accurate measurement of individual contributions across longer NLE sequences. Experiments on four QA datasets and three open-weight models demonstrate that rank-2 subspaces achieve perfect cumulative explained variance while revealing systematic multi-step dynamics in knowledge utilization.

## Method Summary
The method constructs intent-driven prompts to isolate PK-only, CK-only, and joint behavior, then uses Patchscope activation swapping to identify layers where intent information is encoded. A rank-2 subspace is learned by sequentially optimizing objectives that maximize projection onto PK and CK directions while maintaining orthonormality. Hidden states at each generation step are projected onto this subspace to extract normalized contributions α_c and α_p, representing the relative influence of context and parametric knowledge. The approach enables multi-step analysis of knowledge interaction dynamics across NLE sequences, revealing patterns in how models balance different knowledge sources during generation.

## Key Results
- Rank-2 subspaces achieve cumulative explained variance EV_r = 1.0, capturing all five interaction types while rank-1 subspaces explain only ~50-70% of variance
- Hallucinated NLE content shows significantly stronger PK axis alignment compared to context-faithful generations
- Chain-of-Thought prompting reduces PK reliance and shifts generations toward CK direction, except on OpenBookQA
- Multi-step analysis reveals systematic patterns: early CK reliance, mid-sequence PK prioritization, and late knowledge reconciliation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rank-2 projection subspaces are necessary and sufficient to disentangle individual PK and CK contributions during generation.
- Mechanism: Hidden state vectors are decomposed as h_i = (I-P)h_i + u_c⟨u_c^T, h_i⟩ + u_p⟨u_p^T, h_i⟩, where orthonormal basis vectors u_c and u_p encode CK and PK directions. Theorem 3.1 proves rank-1 is non-identifiable: infinitely many (c', p') pairs yield identical scalar projections when both coefficients are nonzero.
- Core assumption: PK and CK contributions are linearly superposed in hidden representations along approximately orthogonal directions.
- Evidence anchors: [abstract] "rank-1 subspaces struggle to represent diverse interactions, our rank-2 formulation captures them effectively"; [section 4.2] "cumulative explained variance EV_r reaches 1.0 at rank-2 and converges thereafter for all datasets and models".
- Break condition: If PK and CK directions have high cosine similarity (>0.3) or if intent prompts fail to produce contrasting answers a(q,ϕ) ≠ a(q,c), the orthonormality and identifiability assumptions may not hold.

### Mechanism 2
- Claim: Multi-step NLE generation exhibits systematic PK-CK dynamics: early CK reliance, mid-sequence PK prioritization, and late knowledge reconciliation.
- Mechanism: Normalized contributions α_i^c = c_i/(c_i+p_i) and α_i^p = p_i/(c_i+p_i) are tracked across tokens via projection onto learned rank-2 subspace.
- Core assumption: Hidden state projections faithfully reflect knowledge source utilization rather than confounds like token frequency or syntactic position.
- Evidence anchors: [section 4.3] "during most of the NLE generations, the model starts with a higher CK, then considers both PK and CK with slight prioritization of PK"; [section 4.3, Figure 15] Separate dynamics shown for supportive (PK-aligned), conflicting (CK-aligned), and complementary interaction types.
- Break condition: If entropy-based uncertainty correlates with sequence position rather than reasoning depth, the fluctuation pattern may reflect position artifacts rather than genuine knowledge reconciliation.

### Mechanism 3
- Claim: Hallucinated content exhibits stronger PK axis alignment; Chain-of-Thought prompting shifts generations toward CK by reducing PK reliance.
- Mechanism: For hallucination, PK-CK contribution gap (α_p - α_c) is larger for hallucinated spans vs. non-hallucinated. For CoT, the prompting mechanism appears encoded in a low-rank subspace aligned with CK direction.
- Core assumption: Hallucination annotations from RAGTruth/Dolly datasets accurately identify model-confabulated content.
- Evidence anchors: [section 4.4, Figure 7] "The gap between PK and CK is much higher for the examples with hallucinated spans than for the examples with no hallucinated spans"; [section 4.5, Figure 8] "CoT maintains similar CK alignment compared to standard prompting...and also reduces PK alignment except for OpenBookQA".
- Break condition: If hallucination datasets systematically mislabel faithful parametric recall as hallucination, the PK-hallucination correlation may be an annotation artifact. CoT effect may be dataset-specific (OpenBookQA shows no PK reduction).

## Foundational Learning

- Concept: **Linear subspace projection**
  - Why needed here: The entire method relies on decomposing hidden states into projections onto learned orthonormal directions.
  - Quick check question: Given hidden vector h = [3, 4] and orthonormal basis u_c = [1, 0], u_p = [0, 1], what are the PK and CK component magnitudes?

- Concept: **Identifiability in latent variable models**
  - Why needed here: Theorem 3.1's proof that rank-1 probes cannot uniquely identify (c, p) pairs is central to motivating rank-2.
  - Quick check question: If α = ac + bp with unknown (a, b), can you uniquely recover (c, p) from a single α observation? Why or why not?

- Concept: **Activation patching / causal intervention**
  - Why needed here: The Patchscope methodology identifies layers where intent information is encoded by swapping activations between source and target prompts.
  - Quick check question: If patching layer L activation from source to target changes answer probability from 0.2 to 0.8, what does this suggest about layer L's role in encoding the source intent?

## Architecture Onboarding

- Component map:
  1. Intent prompt construction -> Patchscope layer localization -> Rank-2 subspace learning -> Direction assignment -> Contribution scoring

- Critical path:
  1. Construct intent-driven prompts -> generate contrasting answers a(q,ϕ), a(q,c), a
  2. Run Patchscope sweep across layers -> identify high-probability-transfer layers
  3. Initialize orthonormal u_c, u_p -> optimize J_p then J_c with orthonormality constraint
  4. Validate on held-out data: check EV_r ≥ 0.95 at rank-2, subspace component distributions non-zero mean
  5. Apply to NLE sequences -> track α_i^p, α_i^c across generation steps

- Design tradeoffs:
  - Layer selection granularity: Using single layer vs. common layers from L_{b→p} ∩ L_{b→c}. Paper uses intersection (L13-18 for Llama-3.1-8B) for robustness but may miss layer-specific nuances.
  - Intent prompt specificity: Stronger constraints ("ignore context entirely") may produce cleaner PK/CK isolation but reduce ecological validity for natural NLE settings.
  - Orthogonality enforcement: Hard constraint vs. soft penalty. Paper uses hard orthonormality (u^T u = I_2) which ensures identifiability but may force artificial separation if PK-CK are partially correlated in practice.

- Failure signatures:
  1. Low cumulative explained variance at rank-2 (<0.9): Suggests PK-CK interactions require higher-rank representation or are not linearly encoded
  2. High PK-CK direction overlap (cosine similarity >0.2): Orthonormality assumption violated; individual contributions may be confounded
  3. Patchscope probability gains near zero: Intent information not localized in selected layers; may need broader layer sweep or different intervention method
  4. Opposite PK-CK patterns across datasets: Suggests subspace not generalizing; may be overfitting to specific data distributions

- First 3 experiments:
  1. Reproduce EV_r convergence: On BaseFakepedia with Llama-3.1-8B, verify that rank-1 EV_1 ≈ 0.5-0.7 and rank-2 EV_2 ≈ 1.0. This validates the core subspace rank hypothesis before proceeding.
  2. Layer ablation: Test rank-2 subspace learned from different layer ranges (early: L1-8, mid: L13-18, late: L25-32). Compare patching accuracy and direction stability. Paper reports 0.98 stability across L13-18 but late-layer subspaces may capture different phenomena.
  3. Hallucination correlation test: On RAGTruth subset, compute mean PK-CK gap for hallucinated vs. non-hallucinated spans. Expect significantly larger gap for hallucinated (effect size d > 0.5). This validates the downstream utility claim before applying to new domains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the learned rank-2 subspace directions be utilized for active inference-time intervention to steer models away from hallucinated parametric recall toward contextually grounded answers?
- **Basis in paper:** [explicit] Section 4.3 notes that parametric knowledge dominance in context-rich tasks "highlight[s] potential for future work on context-sensitive knowledge control."
- **Why unresolved:** The current work focuses on analysis and identification of the directions but does not implement or test a steering mechanism to correct the identified PK dominance.
- **What evidence would resolve it:** Experiments demonstrating that subtracting the parametric direction vector from hidden states during generation reduces hallucination rates while maintaining fluency.

### Open Question 2
- **Question:** Does the finding that rank-2 subspaces suffice for knowledge disentanglement generalize to generation tasks beyond Question Answering, such as summarization or long-form dialogue?
- **Basis in paper:** [inferred] The conclusion claims the analysis provides a "general" signal for studying knowledge balance, yet the experimental scope is limited to four QA datasets (Section 4.1).
- **Why unresolved:** It is unverified if the specific geometric properties (orthogonality of PK/CK) hold in tasks with different knowledge density or structural constraints.
- **What evidence would resolve it:** Replicating the cumulative explained variance analysis (EV_r) on diverse non-QA benchmarks to confirm if rank-2 remains sufficient.

### Open Question 3
- **Question:** Is the rank-2 geometry of knowledge interaction consistent across different model architectures, specifically encoder-decoder models?
- **Basis in paper:** [inferred] The methodology is evaluated exclusively on three open-weight, decoder-only instruction-tuned LLMs (Llama, Gemma, Mistral) in Section 4.1.
- **Why unresolved:** The structural constraints of decoder-only architectures might facilitate the observed rank-2 decomposition differently than encoder-decoder attention mechanisms.
- **What evidence would resolve it:** Application of the rank-2 projection learning method to encoder-decoder architectures (e.g., T5) to compare subspace identifiability.

## Limitations
- Rank-2 subspace framework relies on strong assumptions about linear separability of PK and CK contributions that may not hold across all LLM architectures or tasks.
- Orthonormality constraint between PK and CK directions may be artificially imposed when these knowledge sources are inherently correlated in practice.
- Correlation between hallucination and PK alignment could be influenced by annotation biases in hallucination datasets or systematic generation differences.

## Confidence
- **High Confidence**: Rank-2 subspaces achieve higher explained variance than rank-1 for PK-CK disentanglement (EV_r=1.0 vs. 0.5-0.7)
- **Medium Confidence**: Hallucinated content shows stronger PK axis alignment than context-faithful content
- **Medium Confidence**: Chain-of-Thought prompting reduces PK reliance and shifts generations toward CK direction
- **Low Confidence**: Multi-step NLE generation follows a universal pattern of early CK reliance → mid-sequence PK prioritization → late knowledge reconciliation

## Next Checks
1. Apply the rank-2 subspace method to a new QA dataset (e.g., HotpotQA or NaturalQuestions) not used in training. Verify that EV_r still converges to 1.0 at rank-2 and that PK-CK contribution patterns remain consistent.
2. Test the subspace learning approach on a different LLM architecture (e.g., GPT-2, OPT) to determine whether the learned subspaces generalize beyond the Llama/Gemma/Mistral family.
3. Use an alternative hallucination detection method (e.g., factuality scoring or human annotation) on the same RAGTruth samples to verify whether the PK-hallucination correlation persists independent of the original annotation scheme.