---
ver: rpa2
title: 'Cost-Awareness in Tree-Search LLM Planning: A Systematic Study'
arxiv_id: '2505.14656'
source_url: https://arxiv.org/abs/2505.14656
tags:
- block
- search
- plan
- cost
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the cost-awareness of tree-search
  large language model (LLM) planners, finding that while tree search improves feasibility,
  it does not consistently yield cost-optimal plans. Among the evaluated methods,
  bidirectional search achieves the best overall efficiency and success rate, while
  Monte Carlo Tree Search (MCTS) attains highest optimality on short-horizon tasks.
---

# Cost-Awareness in Tree-Search LLM Planning: A Systematic Study

## Quick Facts
- arXiv ID: 2505.14656
- Source URL: https://arxiv.org/abs/2505.14656
- Reference count: 39
- Tree search improves LLM planning feasibility but not reliably cost-optimality

## Executive Summary
This paper systematically evaluates the cost-awareness of tree-search large language model (LLM) planners on Budget-BlocksWorld tasks. The authors find that while tree search methods like BFS, DFS, MCTS, and bidirectional search improve the success rate of finding feasible plans compared to Chain-of-Thought approaches, they do not consistently yield cost-optimal solutions. Among the evaluated methods, bidirectional search achieves the best overall efficiency and success rate, while MCTS attains highest optimality on short-horizon tasks. Increasing search computation does not reliably improve optimality, and planners often struggle to identify cost-optimal solutions for long-horizon tasks.

## Method Summary
The study evaluates four tree-search LLM planners (ToT-BFS, ToT-DFS, MCTS, Bi-Search) on Budget-BlocksWorld, a cost-augmented BlocksWorld benchmark with 1,008 tasks. Planners use Qwen3-8B to propose top-5 actions per state, scored by combining action-evaluation and self-evaluation log-probabilities. Search is limited to 500 node expansions. Three budget regimes are tested: TIGHT (optimal cost), LOOSE (optimal + 2×cmax), and UNLIMITED. Performance is measured via Success Rate, Optimality (Cost(optimal)/Cost(generated)), and Efficiency (1 − nodes_expanded/500).

## Key Results
- Bidirectional search achieves best efficiency (0.91-0.99) and success rates across all plan lengths
- MCTS attains highest optimality on short-horizon tasks but has high failure rates (36.4-89.3%)
- Increasing search computation does not reliably improve cost-optimality
- LLMs struggle to identify cost-optimal solutions for long-horizon tasks
- Success rates improve with tree search vs CoT, but optimality remains problematic

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Search Space Reduction
Growing two trees simultaneously from initial and goal states reduces effective search depth and improves efficiency on long-horizon tasks. By replacing one deep expansion with two shallower ones, bidirectional search avoids combinatorial blow-up. The forward tree represents partial plan prefixes; the backward tree represents reverse plans or goal-conditioned subgoals. When frontiers meet at a compatible intermediate state, the algorithm concatenates both paths.

### Mechanism 2: MCTS Rollout-Based Value Correction
MCTS partially corrects miscalibrated LLM confidence scores by incorporating empirical evidence from rollouts via backpropagation. UCT selection balances exploitation (high Q-values) with exploration (low visit counts). Rollouts provide ground-truth feedback about which branches actually lead to budget-feasible solutions, updating Q-values upward. This dual guidance compensates when the LLM assigns uniformly low or misleading confidence scores.

### Mechanism 3: LLM Confidence as Cost-Aware Heuristic Proxy
Combining action-evaluation log-probability (LLM's prediction of best action) with self-evaluation log-probability (LLM's "good/bad" judgment) provides a noisy but useful signal for prioritizing node expansion. The reward sums two log-probability signals: (1) how confidently the LLM predicts action *a* when asked for the best next action, and (2) how confidently the LLM outputs "good" when evaluating whether *a* was a good choice. This dual signal captures both action appropriateness and budget-awareness.

## Foundational Learning

- **Concept: Cost-optimal vs. feasible planning**
  - Why needed here: The paper distinguishes between finding *any* valid plan (feasibility) and finding the minimum-cost plan (optimality). Tree search improves feasibility but not reliably optimality.
  - Quick check question: Given actions with costs [pick-up:1, put-down:20, stack:1, unstack:1], why might a shorter plan cost more than a longer one?

- **Concept: Node expansion as compute budget**
  - Why needed here: The paper operationalizes "search computation" as node expansions, with a 500-node limit. Each expansion requires LLM calls for action proposal and scoring, making efficiency critical for practical deployment.
  - Quick check question: If ToT-BFS expands all nodes at depth *d* before depth *d+1*, how does the branching factor *b* affect total expansions to reach depth *k*?

- **Concept: Budget regimes (TIGHT/LOOSE/UNLIMITED)**
  - Why needed here: These regimes isolate different failure modes. TIGHT reveals budget violations. LOOSE permits near-optimal solutions. UNLIMITED removes cost constraints entirely, isolating search completeness issues.
  - Quick check question: Under TIGHT budget where any valid plan must be cost-optimal, what does an optimality score of 1.00 indicate about the solution?

## Architecture Onboarding

- **Component map:** Task Specification (s₀, sᵍ, B, A) → Search Controller (DFS/BFS/MCTS/Bi-Search) → LLM Action Proposer → Candidate Actions (top-m) → Transition Validator (PDDLGym) → Valid Children → Reward Scorer (confidence + self-eval) → Budget Feasibility Filter (optional hard prune) → Search Tree Update → Goal Check → Plan Extraction

- **Critical path:** The reward scorer → search controller feedback loop. Poor rewards cause wasted expansions on infeasible branches. The paper shows 36-89% of expanded nodes are on trajectories already doomed by budget (Table 3).

- **Design tradeoffs:**
  - Hard budget pruning vs. soft guidance: Hard pruning (rejecting any action exceeding budget) improves feasibility but may prevent finding creative solutions that temporarily exceed budget then recover.
  - MCTS exploration parameter β: Controls exploration-exploitation balance. Too low → premature convergence to suboptimal branches. Too high → inefficient random search.
  - Bidirectional vs. unidirectional: Bi-Search requires goal-conditioned subgoal proposals (backward reasoning capability). If the LLM cannot reliably propose subgoals, the backward tree expands uselessly.

- **Failure signatures:**
  - High search exhaustion, low budget violation (ToT-BFS/DFS): Search cannot reach goal within node limit. Indicates poor search guidance or excessive branching.
  - Balanced exhaustion/violation (MCTS): Finds some solutions but many exceed budget. Indicates rollout feedback helps reachability but not cost-optimality.
  - High budget violation, low exhaustion (Bi-Search): Efficiently finds solutions but not cost-optimal ones. Indicates efficient search structure but cost-agnostic node selection.

- **First 3 experiments:**
  1. Reproduce Table 1 on a subset: Run all four search methods on plan lengths L∈{4,8,12} under LOOSE budget. Verify Bi-Search maintains >0.9 efficiency at L=12 while ToT-BFS drops to ~0.
  2. Failure mode analysis replication: Sample 100 nodes from failed MCTS runs at L=6. Manually verify whether each node satisfies the "impossible trajectory" condition (accumulated cost + optimal remaining cost > budget). Compare against Table 3 rates.
  3. Ablate reward components: Run Bi-Search with action-evaluation-only and self-evaluation-only rewards. Measure impact on success rate and optimality to isolate which confidence signal contributes more to cost-awareness.

## Open Questions the Paper Calls Out

- Can learned cost-aware reward models effectively guide node expansion to improve optimality compared to the current LLM-based confidence scores?
- Does incorporating principled infeasibility pruning based on lower bounds on remaining cost effectively eliminate budget-violating branches without sacrificing feasibility?
- Does the superior efficiency of bidirectional search over MCTS persist in planning domains with stochastic dynamics or higher action branching factors?

## Limitations
- Focuses on single cost-augmented domain (Budget-BlocksWorld) with deterministic transitions, limiting generalizability
- 500-node budget may be insufficient for certain problem instances and constrains evaluation of search scalability
- LLM backbone (Qwen3-8B) and specific prompt formulations are not fully specified, raising reproducibility concerns

## Confidence
- **High Confidence:** Bidirectional search achieving best efficiency and success rates (0.91-0.99 efficiency across all plan lengths)
- **Medium Confidence:** MCTS achieving highest optimality on short-horizon tasks (though with higher failure rates)
- **Low Confidence:** Claim that simply scaling inference-time compute does not reliably improve optimality requires further validation across different model sizes

## Next Checks
1. Apply the four tree-search methods to a different cost-aware planning domain (e.g., stochastic planning task) to verify whether bidirectional search maintains its efficiency advantage
2. Systematically vary the LLM backbone and prompt formulations to determine whether performance differences persist across different LLM configurations
3. Implement and evaluate an adaptive search strategy that dynamically adjusts the node expansion budget based on intermediate search progress and problem difficulty indicators