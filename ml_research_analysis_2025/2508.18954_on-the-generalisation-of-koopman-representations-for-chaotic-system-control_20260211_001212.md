---
ver: rpa2
title: On the Generalisation of Koopman Representations for Chaotic System Control
arxiv_id: '2508.18954'
source_url: https://arxiv.org/abs/2508.18954
tags:
- koopman
- transformer
- safety
- prediction
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines whether Koopman-based embeddings learned from
  chaotic dynamical systems can generalise to downstream control tasks. Using the
  Lorenz system as a testbed, the authors propose a three-stage approach: learning
  Koopman embeddings via autoencoding, pre-training a transformer on next-state prediction,
  and fine-tuning for safety-critical control.'
---

# On the Generalisation of Koopman Representations for Chaotic System Control

## Quick Facts
- arXiv ID: 2508.18954
- Source URL: https://arxiv.org/abs/2508.18954
- Reference count: 40
- Primary result: Koopman embeddings significantly outperform PCA baselines in safety function prediction for chaotic systems

## Executive Summary
This paper investigates whether Koopman-based embeddings learned from chaotic dynamical systems can generalize to downstream control tasks. Using the Lorenz system as a testbed, the authors propose a three-stage approach: learning Koopman embeddings via autoencoding, pre-training a transformer on next-state prediction, and fine-tuning for safety-critical control. Results show that Koopman embeddings significantly outperform both standard and physics-informed PCA baselines in safety function prediction, achieving lower mean squared error, mean absolute error, and higher R-squared scores. Notably, freezing the pre-trained transformer weights during fine-tuning yields no performance loss, indicating that the learned representations capture reusable dynamical structure rather than task-specific patterns.

## Method Summary
The method consists of three stages: (1) Train a Koopman autoencoder with a 3→500→32 encoder, 32×32 structured Koopman operator (diagonal + banded skew-symmetric), and 32→500→3 decoder using composite loss; (2) Pre-train a 4-layer, 4-head transformer decoder-only on 64-step next-state prediction with frozen encoder; (3) Fine-tune a safety head (MLP) with frozen transformer, taking concatenated transformer output and query state as input. The approach is tested on the Lorenz system with safety function prediction as the downstream task.

## Key Results
- Koopman embeddings achieve lower MSE (3.08±6.55) and MAE (1.20±2.08) than PCA baselines in safety function prediction
- Freezing pre-trained transformer weights during fine-tuning shows no performance degradation (p=0.335), indicating reusable representations
- PCA (standard) baseline fails to converge on next-state prediction task
- Safety predictions show high variance (σ=17.14) when fine-tuning end-to-end

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Koopman embeddings linearize nonlinear chaotic dynamics, enabling stable long-horizon prediction
- Mechanism: The encoder φₑ maps 3D Lorenz states to a 32D latent space where dynamics evolve approximately linearly via the learned Koopman operator K
- Core assumption: A finite-dimensional observable space exists where the nonlinear Lorenz system exhibits approximately linear evolution
- Evidence anchors: Formal Koopman operator definition, comparison with KALIKO paper
- Break condition: Insufficient embedding dimension will degrade linear predictability over longer horizons

### Mechanism 2
- Claim: Structured decomposition of the Koopman operator injects physics-aware inductive bias
- Mechanism: K = D + S_band where D models dissipative dynamics and S_band captures conservative oscillatory behavior
- Core assumption: Lorenz system's dominant modes can be approximated by a small number of dissipative and oscillatory components
- Evidence anchors: Visualization of double-banded skew-symmetric structure
- Break condition: Dense coupling across all embedding dimensions may be required, limiting banded constraint capacity

### Mechanism 3
- Claim: Pre-training on next-state prediction produces representations that transfer without gradient updates
- Mechanism: Transformer learns temporal dependencies in Koopman space during self-supervised prediction
- Core assumption: Safety function U(q) can be approximated from local dynamical features captured during pre-training
- Evidence anchors: No performance degradation when freezing transformer weights
- Break condition: Transfer fails if downstream task requires dynamical features not exercised during pre-training

### Mechanism 4
- Claim: PCA baselines fail due to lack of learned dynamical structure
- Mechanism: PCA produces linear combinations without dynamics-aware optimization, yielding embeddings where transformer cannot converge
- Core assumption: Performance gap stems from representation quality, not hyperparameter mismatch
- Evidence anchors: PCA convergence failure and rapid error growth documented
- Break condition: Different transformer architectures for each baseline may confound comparison

## Foundational Learning

- **Koopman Operator Theory**
  - Why needed here: The entire architecture rests on approximating the Koopman operator—a linear operator acting on observables of a nonlinear system
  - Quick check question: Can you explain why a linear operator on observables can represent nonlinear system dynamics, and what "observables" means in this context?

- **Chaotic Dynamics and Attractors**
  - Why needed here: The Lorenz system exhibits sensitive dependence on initial conditions and strange attractor geometry
  - Quick check question: Why does the Lorenz system's strange attractor make naive trajectory prediction unreliable beyond short horizons?

- **Transfer Learning via Self-Supervised Pre-training**
  - Why needed here: The paper's core hypothesis draws from NLP practices (next-token prediction → downstream tasks)
  - Quick check question: Why does pre-training on a self-supervised task potentially yield representations useful for a different downstream task?

## Architecture Onboarding

- **Component map:**
  - Encoder: 3→500→32 (ReLU, LayerNorm)
  - Koopman operator: 32×32 (diagonal + banded skew-symmetric)
  - Decoder: 32→500→3 (ReLU)
  - Transformer: 4-layer, 4-head, pre-LayerNorm, GELU, sinusoidal positional encoding
  - Safety head: Concatenate transformer output (32D) + query state (3D) → MLP (35→128→64→1)

- **Critical path:**
  1. Generate Lorenz trajectories with RK45 integration
  2. Train autoencoder until dynamics loss converges
  3. Freeze encoder, train transformer on 64-step sequences
  4. Freeze transformer, train safety head against precomputed safety values

- **Design tradeoffs:**
  - Embedding dimension (32): Larger captures more Koopman eigenfunctions but increases compute
  - Koopman operator structure: Banded enforces physics-motivated sparsity but may limit capacity
  - Frozen vs. unfrozen fine-tuning: Frozen isolates representation quality
  - Safety region Q bounds: Chosen to encompass right attractor wing

- **Failure signatures:**
  - Autoencoder dynamics loss plateaus high → encoder not learning linearizable representation
  - Transformer prediction error grows rapidly → embedding lacks stable dynamics
  - Safety head loss diverges → pre-trained representations lack relevant features
  - High variance in safety predictions → end-to-end fine-tuning may be overfitting

- **First 3 experiments:**
  1. Reproduce baseline comparison: Train autoencoder, transformer, and safety head with frozen transformer
  2. Ablate operator structure: Replace structured K with dense learned matrix
  3. Probe transfer boundaries: Train safety head with unfrozen transformer on subset of safety region Q

## Open Questions the Paper Calls Out

- **Question:** Can Koopman embeddings be integrated with Control Barrier Functions (CBFs) to provide provable safety guarantees in chaotic environments?
  - Basis in paper: The Conclusion states future work could apply these representations to formal safety methods like CBFs
  - Why unresolved: Current study is limited to estimating safety values rather than implementing formal certification methods
  - What evidence would resolve it: Successful derivation and application of CBFs using learned Koopman embeddings that mathematically guarantee system safety

- **Question:** Do the findings generalize to higher-dimensional chaotic systems or those with distinct attractor geometries beyond the Lorenz system?
  - Basis in paper: Methodology tested exclusively on 3D Lorenz system
  - Why unresolved: Paper establishes "proof of concept" on canonical low-dimensional system
  - What evidence would resolve it: Replication on high-dimensional chaotic systems showing comparable transfer performance

- **Question:** How robust are these frozen representations when deployed in a closed-loop control setting?
  - Basis in paper: Experiments focus on "safety function estimation" rather than actual "control implementation"
  - Why unresolved: Models evaluated on prediction accuracy (open-loop), not control success rates
  - What evidence would resolve it: Testing fine-tuned models in simulated closed-loop environment

## Limitations
- Finite-dimensional approximation of infinite-dimensional Koopman operator may fail for more complex chaotic systems
- Safety region bounds Q are manually chosen, making approach sensitive to domain knowledge about system geometry
- Lack of ablation study comparing with unstructured (dense) Koopman operators

## Confidence
- **High confidence**: Empirical demonstration that frozen pre-trained transformers match fine-tuned performance
- **Medium confidence**: Superiority of Koopman embeddings over PCA baselines, though different transformer architectures introduce confounding
- **Medium confidence**: Claim that Koopman embeddings capture reusable dynamical structure, but transfer to fundamentally different tasks remains untested

## Next Checks
1. **Ablate Koopman structure**: Replace structured D+S_band operator with dense learned matrix and compare performance
2. **Test spatial transfer limits**: Train safety head on region Q1 (right wing) with frozen transformer, evaluate on Q2 (left wing)
3. **Probe downstream task diversity**: Extend pipeline to third task like Lyapunov exponent estimation or parameter recovery