---
ver: rpa2
title: 'AQUA: A Large Language Model for Aquaculture & Fisheries'
arxiv_id: '2507.20520'
source_url: https://arxiv.org/abs/2507.20520
tags:
- aquaculture
- aqua
- data
- agent
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AQUA is the first large language model specialized for aquaculture,\
  \ addressing the industry\u2019s unique challenges in disease, feeding, and hatchery\
  \ management. We developed AQUADAPT, an agentic framework that combines expert-driven\
  \ instruction design, synthetic QA generation, and automated quality evaluation."
---

# AQUA: A Large Language Model for Aquaculture & Fisheries

## Quick Facts
- arXiv ID: 2507.20520
- Source URL: https://arxiv.org/abs/2507.20520
- Reference count: 40
- Primary result: AQUA is the first LLM specialized for aquaculture, outperforming general models with BLEU-4 of 49.19 and expert ratings averaging 4.75/5

## Executive Summary
AQUA is the first large language model specialized for aquaculture, addressing the industry’s unique challenges in disease, feeding, and hatchery management. We developed AQUADAPT, an agentic framework that combines expert-driven instruction design, synthetic QA generation, and automated quality evaluation. AQUA was fine-tuned on 3 million curated QA pairs and outperformed general models on aquaculture tasks, achieving BLEU-4 scores of 49.19 and expert ratings averaging 4.75/5. We also integrated lightweight SLMs into IoT systems for real-time water quality monitoring and decision support. This work establishes a foundation for intelligent, scalable, and domain-aligned AI tools in aquaculture.

## Method Summary
AQUA was developed using the AQUADAPT framework, which employs expert-authored instruction sets, synthetic QA pair generation, and automated quality evaluation via an LLM-as-Judge. The model was fine-tuned on Mistral-7B-Instruct-v0.3 with LoRA, using 3 million QA pairs derived from 55,105 aquaculture documents. Training involved 8 H200 GPUs for 32 hours, with AdamW optimization and cosine scheduling. Synthetic QA pairs were generated using GPT-4.1 and Gemini 2.0 Flash, filtered by BM25 relevance and LLM scoring. The model was evaluated on NLG metrics (BLEU-4: 49.19, ROUGE-1: 51.45, ROUGE-L: 45.09) and expert ratings (4.75/5 average), outperforming Qwen-2.5-7B and Llama-3.1-8B.

## Key Results
- AQUA achieved BLEU-4 score of 49.19 and expert ratings averaging 4.75/5 on aquaculture QA tasks
- Outperformed Qwen-2.5-7B and Llama-3.1-8B on domain-specific evaluations
- Integrated lightweight SLMs into IoT systems for real-time water quality monitoring and decision support

## Why This Works (Mechanism)
AQUA works by leveraging a domain-specific corpus of 3 million QA pairs generated through expert-guided synthetic data creation. The fine-tuning process adapts a general-purpose model to aquaculture terminology and workflows, while the LLM-as-Judge ensures high-quality synthetic data. The integration of SLMs into IoT systems enables real-time, edge-based decision support, reducing latency and infrastructure costs.

## Foundational Learning
- **Fine-tuning LoRA adapters**: Needed to efficiently adapt large models to domain-specific tasks; quick check: verify adapter rank and dropout settings match paper.
- **BM25 filtering**: Required for document relevance scoring; quick check: ensure k1=1.5, b=0.75 are applied.
- **LLM-as-Judge**: Used for automated quality evaluation; quick check: validate correlation (ρ=0.85) with human ratings.
- **Synthetic QA generation**: Critical for scaling training data; quick check: confirm few-shot prompting with expert seed examples.
- **Edge deployment of SLMs**: Enables low-latency IoT integration; quick check: test model size and inference speed on edge devices.
- **Multimodal integration**: Future work for visual health monitoring; quick check: assess feasibility of integrating computer vision modules.

## Architecture Onboarding
- **Component map**: Documents -> Parser (pymupdf4llm/Docling) -> BM25 Filter -> QA Generator (GPT-4.1/Gemini) -> LLM-as-Judge -> AQUA Fine-tuning -> Edge Deployment
- **Critical path**: Synthetic QA generation → LLM-as-Judge filtering → AQUA fine-tuning → Evaluation
- **Design tradeoffs**: Synthetic data generation vs. manual annotation (speed vs. quality); edge deployment vs. cloud (latency vs. scalability)
- **Failure signatures**: Over-contextualization (region-specific bias), low synthetic QA quality, judge calibration drift
- **3 first experiments**: 1) Test synthetic QA generation with seed examples; 2) Fine-tune AQUA on validation set and evaluate BLEU/ROUGE; 3) Deploy lightweight SLM on IoT simulator for latency testing

## Open Questions the Paper Calls Out
- **Open Question 1**: Can computer vision modules be effectively integrated into AQUA to automate the detection of physical fish health issues and behavioral stress cues? The authors identify "Real-Time Visual Models" as future work, but the current AQUA model is text-only.
- **Open Question 2**: To what extent does the high density of region-specific training data cause AQUA to inappropriately over-contextualize advice for global users? Section XII.A notes potential over-contextualization, but the boundary between helpful specificity and unwanted assumption remains undefined.
- **Open Question 3**: Does the reliance on an automated LLM-as-a-Judge (fine-tuned GPT-4.1) inadvertently filter out valid but novel aquaculture techniques? Section VI validates the judge on current expert standards, while Section XII.B suggests the model may underrepresent emerging techniques.

## Limitations
- Key artifacts (seed QA pairs, prompt templates, gold standard dataset) are not released, limiting reproducibility
- Model may overfit to specific regional practices, reducing generalizability
- Evaluation relies heavily on a single judge model (GPT-4.1), raising concerns about calibration and generalization

## Confidence
- **High Confidence**: General framework (AQUADAPT), training objectives (BLEU/ROUGE scores, expert ratings), and overall performance claims
- **Medium Confidence**: Data pipeline (document collection, parsing, filtering, QA synthesis) is described, but absence of seed examples and exact thresholds reduces confidence
- **Low Confidence**: LLM-as-Judge methodology and synthetic data generation are most uncertain due to missing calibration datasets and fine-tuning details

## Next Checks
1. Reconstruct the data pipeline using publicly available aquaculture documents and test the synthetic QA generation with released seed examples to assess domain fidelity.
2. Fine-tune Mistral-7B-Instruct-v0.3 with LoRA using the specified hyperparameters and evaluate on the HuggingFace test set to verify BLEU-4 and ROUGE performance.
3. Reproduce the LLM-as-Judge pipeline using a held-out sample of the gold standard dataset and assess alignment with human expert ratings to validate the automated evaluation process.