---
ver: rpa2
title: 'OPTIC: Optimizing Patient-Provider Triaging & Improving Communications in
  Clinical Operations using GPT-4 Data Labeling and Model Distillation'
arxiv_id: '2503.05701'
source_url: https://arxiv.org/abs/2503.05701
tags:
- messages
- clinical
- patient
- message
- healthcare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed OPTIC, a triaging tool for patient portal messages
  using GPT-4 for data labeling and BERT for model distillation. The system classified
  405,487 messages from Johns Hopkins Medicine as "Admin" or "Clinical" with 88.85%
  accuracy, 88.29% sensitivity, 89.38% specificity, and an F1 score of 0.8842.
---

# OPTIC: Optimizing Patient-Provider Triaging & Improving Communications in Clinical Operations using GPT-4 Data Labeling and Model Distillation

## Quick Facts
- arXiv ID: 2503.05701
- Source URL: https://arxiv.org/abs/2503.05701
- Reference count: 32
- Primary result: BERT model achieved 88.85% accuracy on binary classification of patient portal messages as "Admin" or "Clinical"

## Executive Summary
The OPTIC system addresses the growing burden of patient portal message triaging by using GPT-4 for data labeling and BERT for model distillation. The approach enables scalable, cost-effective classification of patient messages into administrative or clinical categories with high accuracy. Deployed through Epic's Nebula Cloud Platform, the system demonstrates practical applicability in healthcare settings while maintaining interpretability through topic modeling.

## Method Summary
The study developed OPTIC by first retrospectively grouping 405,487 patient messages into possible categories using metadata, then employing GPT-4-32K with few-shot prompting (200 examples) to generate high-quality labels. These labels were used to fine-tune a BERT model through supervised learning. The system achieved 88.85% accuracy on test data, with physician validation confirming GPT-4's labeling accuracy at 99%. BERTopic analysis identified 81 distinct topics within the test data, with over 80% accuracy for 58 topics.

## Key Results
- BERT model achieved 88.85% accuracy, 88.29% sensitivity, 89.38% specificity, and F1 score of 0.8842 on test set
- BERTopic identified 81 distinct topics, with over 80% accuracy in classifying 58 topics
- System deployed through Epic's Nebula Cloud Platform for practical healthcare application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting with GPT-4 can generate high-quality labeled training data for clinical text classification, enabling scalable annotation without extensive human labeling.
- Mechanism: Providing GPT-4 with representative examples of both "Admin" and "Clinical" messages establishes a semantic boundary that the model generalizes to unlabeled data. The model learns to distinguish based on presence of clinical symptoms, urgency indicators, and administrative keywords.
- Core assumption: GPT-4's classification logic aligns with physician judgment and remains consistent across diverse patient communication styles.
- Evidence anchors:
  - [abstract] "High-quality labeled data was generated through GPT-4-based prompt engineering, which was then used to train a BERT model"
  - [section] Table 4 shows E2 (GPT-4-32K with 200 few-shot examples) achieved 99% accuracy validated by physicians on 2,000 messages
  - [corpus] Neighbor paper "Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare" supports synthetic data generation viability, though focused on synthetic rather than labeling workflows
- Break condition: Performance degrades when messages contain mixed clinical-administrative content that defies binary categorization, or when GPT-4's reasoning diverges from physician consensus.

### Mechanism 2
- Claim: Distilling GPT-4 labels into a smaller BERT model preserves classification quality while enabling cost-effective CPU deployment at inference time.
- Mechanism: The teacher model (GPT-4) generates labels for ~36k messages; the student model (BERT) learns to approximate this labeling function through supervised fine-tuning. BERT's bidirectional attention captures clinical language patterns without requiring the computational overhead of a large generative model.
- Core assumption: The ~36k labeled samples sufficiently represent the semantic distribution of the full 405k dataset.
- Evidence anchors:
  - [abstract] "BERT model achieved 88.85% accuracy on the test set validated by GPT-4 labeling"
  - [section] "A small language model (e.g., BERT) emerged as a viable solution due to its ability to deliver robust performance on CPU, addressing economic considerations"
  - [corpus] No direct corpus evidence on GPT-to-BERT distillation specifically; related work focuses on dialogue summarization rather than classification distillation
- Break condition: Accuracy drops significantly if the student model capacity is insufficient for the label complexity, or if domain shift occurs between training messages and production traffic.

### Mechanism 3
- Claim: Topic modeling (BERTopic) provides interpretable performance segmentation, revealing which clinical/administrative subtypes the model handles well versus poorly.
- Mechanism: BERTopic clusters semantically similar messages into 81 topics; per-topic accuracy analysis identifies failure modes and validates generalization beyond dominant message types.
- Core assumption: Topics identified by BERTopic meaningfully correspond to operationally relevant categories for triage workflow.
- Evidence anchors:
  - [abstract] "BERTopic identified 81 distinct topics within the test data, with over 80% accuracy in classifying 58 topics"
  - [section] "The model maintains high performance across a diverse range of subject matters which suggests that it has developed a robust understanding"
  - [corpus] Weak corpus connection; no neighboring papers directly address topic-model-based evaluation
- Break condition: Low-accuracy topics indicate semantic areas where the Admin/Clinical boundary is ambiguous or underrepresented in training data.

## Foundational Learning

- Concept: Few-shot vs. Zero-shot Prompting
  - Why needed here: The paper's experiments (E1-E4) show that zero-shot achieved only 65% accuracy while few-shot with 200 examples reached 99%—understanding this gap is essential for designing labeling workflows.
  - Quick check question: If you only have 50 labeled examples instead of 200, would you expect few-shot performance to remain at 99%? Why or why not?

- Concept: Knowledge Distillation (Teacher-Student)
  - Why needed here: The entire OPTIC architecture depends on transferring GPT-4's labeling capability to BERT for cost-effective deployment.
  - Quick check question: What happens to student model performance if the teacher model's labels contain systematic errors?

- Concept: Topic Modeling for Evaluation
  - Why needed here: BERTopic analysis reveals that 23 of 81 topics fall below 80% accuracy—interpreting these gaps guides model improvement and deployment guardrails.
  - Quick check question: If a high-volume topic has 75% accuracy, should you deploy the model or collect more training data for that topic first?

## Architecture Onboarding

- Component map:
  Data Source -> Retrospective Grouping -> GPT-4 Labeling -> BERT Training -> BERTopic Analysis -> Deployment
  Epic/MyChart messages -> Metadata rules -> Few-shot prompt template -> AzureML training -> Topic extraction -> Epic Nebula

- Critical path:
  1. Define Admin/Clinical boundary with SME input (hierarchical labeling rules)
  2. Sample representative messages across clusters for few-shot examples
  3. Run GPT-4 labeling with validated prompt template
  4. Train BERT on GPT-4 labels; validate against physician-labeled holdout
  5. Analyze per-topic accuracy; iterate on underperforming categories
  6. Package for Nebula deployment

- Design tradeoffs:
  - GPT-4 accuracy (99%) vs. BERT cost-efficiency (88.85%): Accept ~10% accuracy drop for 100x+ inference cost reduction
  - Binary classification vs. multi-class: Simplicity of Admin/Clinical vs. operational granularity (future work)
  - First-message-only vs. thread context: Reduces complexity but may miss clarifying information in follow-ups

- Failure signatures:
  - Messages with both clinical symptoms and administrative requests (e.g., "refill my medication because I'm having chest pain")
  - Rare specialties or pediatric contexts underrepresented in Primary Care training data
  - Topic clusters with <80% accuracy (23 of 81 identified) warrant monitoring or routing to human review

- First 3 experiments:
  1. Reproduce prompt engineering comparison: Test E2 (200-shot GPT-4) vs. E4 (zero-shot) on a held-out 500-message sample to validate the 99% vs. 65% gap locally.
  2. Ablate training set size: Train BERT on 10k, 20k, and 33k GPT-4-labeled samples to establish the data efficiency curve and minimum viable training size.
  3. Per-topic error analysis: Select the 5 lowest-accuracy topics from BERTopic output; manually review 20 messages each to identify systematic misclassification patterns (e.g., clinical language in administrative requests).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the OPTIC model maintain its accuracy when deployed in specialty clinical departments beyond primary care?
- Basis in paper: [explicit] "Future work will focus on enhancing interpretability and expanding the model's capabilities to handle additional nuances in patient communication, particularly as this may apply to other clinical departments across Johns Hopkins Medicine."
- Why unresolved: The current study only evaluated Primary Care Practice (Internal Medicine and Pediatrics); specialty departments may have different message patterns, terminology, and admin/clinical distributions.
- What evidence would resolve it: Prospective evaluation of model performance on message datasets from specialty departments (e.g., oncology, cardiology, surgery) with comparison to primary care baseline metrics.

### Open Question 2
- Question: Does GPT-4-based labeling introduce systematic biases that differ from human expert labeling patterns?
- Basis in paper: [inferred] The study used GPT-4 to label ~36K messages with only 2,000 validated by physicians, and achieved 99% agreement. This unusually high agreement raises questions about whether physicians may have been influenced by GPT-4 explanations or whether the sample was unrepresentative.
- Why unresolved: No comparison of GPT-4 labels to blinded human labeling across the full labeled dataset was conducted; potential systematic mislabeling could propagate to the distilled BERT model.
- What evidence would resolve it: A random subset of GPT-4-labeled messages reviewed by blinded human experts without GPT-4 explanations, with discrepancy analysis across message types.

### Open Question 3
- Question: What are the safety implications of false negatives (clinical messages misclassified as administrative)?
- Basis in paper: [inferred] The model achieved 88.29% sensitivity, meaning ~12% of clinical messages were misclassified as admin. The paper does not analyze the clinical content of these misclassifications or their potential impact on patient care delays.
- Why unresolved: No analysis was conducted on the severity or urgency of misclassified clinical messages and their potential impact on patient care.

## Limitations
- Binary classification may fail on messages containing both clinical and administrative elements
- Study limited to Primary Care Practice at single health system (2020 timeframe)
- GPT-4 accuracy validated on only 2,000 messages from 405k dataset

## Confidence
- **High**: The feasibility of using GPT-4 for few-shot labeling to generate training data (validated by physician review)
- **Medium**: The distillation approach preserving sufficient accuracy for operational deployment (88.85% vs. 99% GPT-4 baseline)
- **Medium**: Topic modeling revealing performance heterogeneity across message types (BERTopic identified 81 topics with 58 >80% accuracy)

## Next Checks
1. **Mixed-content message testing**: Manually curate 100 messages containing both clinical symptoms and administrative requests; evaluate whether the binary classifier correctly identifies the dominant intent or requires human routing.
2. **Temporal performance drift**: Deploy the model on 30 days of new patient messages (post-training period) to assess accuracy degradation and identify emerging topic clusters not captured in the 2020 training data.
3. **Physician workflow impact study**: Conduct a time-motion study comparing message triaging speed and accuracy between human-only triage and GPT-4/BERT-assisted triage across 1,000 randomly selected messages from multiple specialties.