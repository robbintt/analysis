---
ver: rpa2
title: 'Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition'
arxiv_id: '2601.07700'
source_url: https://arxiv.org/abs/2601.07700
tags:
- networks
- layer
- splitcam
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to decompose any ReLU network into
  two monotone and convex subnetworks, thereby enabling more interpretable saliency
  explanations. The key innovation lies in stabilizing both forward and backward passes
  through affine transformations, which overcomes numerical instabilities arising
  from the non-negative weight matrices in the decomposed networks.
---

# Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition

## Quick Facts
- arXiv ID: 2601.07700
- Source URL: https://arxiv.org/abs/2601.07700
- Reference count: 40
- Primary result: Decomposes ReLU networks into two monotone-convex subnetworks to produce more stable and interpretable saliency explanations

## Executive Summary
This paper introduces a method to decompose any ReLU network into two monotone and convex subnetworks using a difference-of-convex (DC) decomposition. The key innovation is stabilizing both forward and backward passes through affine transformations, which overcomes numerical instabilities arising from non-negative weight matrices in the decomposed networks. The authors develop new attribution methods—SplitCAM, SplitLRP, and SplitGrad—by adapting existing XAI techniques to operate on these decomposed networks. Experiments demonstrate state-of-the-art performance on Quantus metrics for faithfulness, localization, and robustness across VGG16 and ResNet18 architectures on ImageNet-S.

## Method Summary
The approach works by decomposing a ReLU network into two monotone and convex subnetworks using a difference-of-convex (DC) decomposition. This decomposition requires non-negative weight matrices, which can lead to numerical instability during both forward and backward passes. To address this, the authors introduce affine transformations that stabilize the computations while preserving the monotone-convex properties. They then adapt existing XAI techniques (CAM, LRP, Grad) to these decomposed networks, creating new attribution methods that leverage the interpretability of monotone-convex functions. The framework also enables self-explainable models when trained directly as differences of monotone networks.

## Key Results
- State-of-the-art performance on Quantus metrics for faithfulness, localization, and robustness on ImageNet-S
- Superior performance compared to classical baselines when applied to VGG16 and ResNet18 architectures
- Visual evidence that one subnetwork highlights present features while the other highlights missing features
- Demonstrated stability of attributions through affine transformation mechanisms

## Why This Works (Mechanism)
The method works by exploiting the mathematical properties of monotone and convex functions, which have inherently interpretable gradient structures. By decomposing any ReLU network into two such functions, the resulting subnetworks inherit these interpretable properties. The DC decomposition transforms the original network weights into non-negative matrices, ensuring monotonicity and convexity. However, this transformation can introduce numerical instability, which is mitigated through carefully designed affine transformations that preserve the mathematical properties while ensuring stable computation. This combination of mathematical structure and numerical stability enables more reliable and interpretable saliency explanations.

## Foundational Learning

**Difference of Convex (DC) Decomposition**
- Why needed: Provides the mathematical foundation for decomposing networks into interpretable components
- Quick check: Verify that any ReLU network can be expressed as the difference of two convex functions

**Monotone and Convex Functions**
- Why needed: These functions have interpretable gradient structures that enhance explanation quality
- Quick check: Confirm that gradients of monotone-convex functions point in consistent directions

**Affine Transformations for Stabilization**
- Why needed: Prevents numerical instability when dealing with non-negative weight matrices
- Quick check: Ensure that affine transformations preserve monotonicity and convexity properties

## Architecture Onboarding

**Component Map**
Original Network -> DC Decomposition -> Two Monotone-Convex Subnetworks -> Stabilizing Affine Transformations -> Attribution Methods

**Critical Path**
1. ReLU Network input
2. DC Decomposition (non-negative weights)
3. Affine stabilization
4. Forward/backward pass through stabilized subnetworks
5. Attribution computation (SplitCAM/SplitLRP/SplitGrad)

**Design Tradeoffs**
- Accuracy vs. interpretability: Decomposition may slightly reduce predictive accuracy but significantly improves explanation quality
- Computational overhead: Additional cost from maintaining two subnetworks and stabilization transformations
- Numerical stability vs. theoretical purity: Affine transformations improve stability but add complexity to the theoretical framework

**Failure Signatures**
- Attribution maps showing inconsistent or noisy patterns indicate stabilization failure
- Performance degradation on Quantus metrics suggests decomposition quality issues
- Numerical overflow/underflow during forward/backward passes indicates insufficient stabilization

**First Experiments**
1. Apply decomposition to a simple MLP and verify monotone-convex properties of outputs
2. Compare attribution stability with and without affine transformations on a fixed input
3. Test decomposition on a pre-trained VGG16 and evaluate Quantus metric improvements

## Open Questions the Paper Calls Out

None identified in the provided content.

## Limitations

- The decomposition approach relies on non-negative weight matrices, which can lead to numerical instability during both forward and backward passes
- The assumption that decomposing networks into monotone-convex components inherently improves interpretability may not hold universally across all problem domains or network depths
- The claim that one subnetwork highlights present features while the other highlights missing features is supported by visualizations but lacks rigorous theoretical justification

## Confidence

- **High Confidence**: The mathematical formulation of DC decomposition for ReLU networks and the associated stabilizing transformations are rigorously derived and implemented
- **Medium Confidence**: The empirical performance improvements on Quantus metrics are demonstrated, but the generalizability to other architectures and datasets requires further validation
- **Medium Confidence**: The claim that one subnetwork highlights present features while the other highlights missing features is supported by visualizations but lacks rigorous theoretical justification

## Next Checks

1. Test the decomposition approach on transformer-based architectures to assess cross-architecture robustness and identify any architectural constraints
2. Evaluate the computational overhead introduced by the stabilizing transformations across varying batch sizes and input resolutions
3. Conduct ablation studies removing the stabilizing mechanisms to quantify their impact on numerical stability and attribution quality