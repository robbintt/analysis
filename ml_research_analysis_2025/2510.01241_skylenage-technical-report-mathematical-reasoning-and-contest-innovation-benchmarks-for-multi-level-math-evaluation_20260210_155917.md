---
ver: rpa2
title: 'SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation
  Benchmarks for Multi-Level Math Evaluation'
arxiv_id: '2510.01241'
source_url: https://arxiv.org/abs/2510.01241
tags:
- reasoning
- accuracy
- arxiv
- math
- relative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models face ceiling effects on standard math benchmarks,
  limiting frontier separation. To address this, we introduce SKYLENAGE, a dual benchmark
  comprising 100 reasoning-focused problems (SKYLENAGE-ReasoningMATH) with metadata
  for structural analysis and 150 contest-style problems (SKYLENAGE-MATH) spanning
  four academic stages.
---

# SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation

## Quick Facts
- arXiv ID: 2510.01241
- Source URL: https://arxiv.org/abs/2510.01241
- Reference count: 7
- Key outcome: Large language models face ceiling effects on standard math benchmarks, limiting frontier separation. SKYLENAGE introduces dual benchmarks (100 reasoning-focused + 150 contest-style problems) to restore discriminative power, revealing fragmented subject leadership and numeric density as primary error driver.

## Executive Summary
Large language models have saturated standard math benchmarks, necessitating harder evaluations to separate frontier systems. SKYLENAGE introduces dual benchmarks: 100 reasoning-focused problems (SKYLENAGE-ReasoningMATH) with rich metadata and 150 contest-style problems (SKYLENAGE-MATH) spanning high school through doctoral levels. Under unified evaluation, top models achieve 44% accuracy on contest suite (declining from 26% in high school to 14% at doctoral level), with ~79% doctoral-to-high-school retention. The reasoning set yields 81% top performance, with hardest-slice retention exposing robustness gaps. Numeric density, not length, primarily drives error inflation, while subject leadership remains fragmented across models.

## Method Summary
SKYLENAGE evaluates 15 contemporary LLMs on two benchmarks: SKYLENAGE-REASONINGMATH (100 problems with metadata) and SKYLENAGE-MATH (150 contest problems spanning HS/UG/GR/PhD stages under seven subjects). Using Chain-of-Thought prompting, self-consistency sampling, and regex-based answer extraction with 10^-6 tolerance, the unified protocol computes exact-match accuracy, hardest-quintile (Q5) retention, and subject×model/grade×model heatmaps. SKYLENAGE-REASONINGMATH is publicly released; SKYLENAGE-MATH is withheld to preserve frontier headroom.

## Key Results
- Top model reaches 44% accuracy on 150-item contest suite, declining from 26% (high school) to 14% (doctoral level)
- Top systems retain ~79% of high school performance at doctoral level, exposing widening frontier gaps
- Best model achieves 81% overall on reasoning set, with hardest-slice retention revealing robustness gaps
- Numeric density (not length) primarily drives error inflation across model families
- Subject leadership is fragmented, suggesting complementarity across models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Scaling difficulty via academic stages (HS → PhD) restores discriminative power lost to ceiling effects.
- **Mechanism:** As problem complexity increases, top-tier models retain performance (~79% retention) while mid-tier systems degrade non-linearly (~50% retention), widening the gap and exposing "grade-wise resilience."
- **Core assumption:** Academic stage mapping accurately reflects monotonic increase in reasoning depth rather than domain-specific knowledge.
- **Evidence anchors:** [abstract] "top systems exhibit a doctoral-to-high-school retention near 79%... declining from 26% in high school to 14% at doctoral level." [Section 5.2.1] "Hardness scaling magnifies differences... leader–mid-tier separations of ≥15 points at GR/PhD."
- **Break condition:** If models simply memorize PhD-level lemmas without reasoning, accuracy curve will flatten or saturate at the top again.

### Mechanism 2
- **Claim:** Error rates are driven primarily by numeric density rather than problem length or symbolic complexity.
- **Mechanism:** High digit-to-token ratios trigger arithmetic normalization failures or precision loss in specific model families (e.g., +92% error inflation with density), while symbolic complexity shows weak correlation (r ≈ 0.2).
- **Core assumption:** Numeric density metric serves as proxy for model's internal precision and tokenization fidelity.
- **Evidence anchors:** [abstract] "Numeric density, not length, primarily drives error inflation." [Section 5.1.4] "Length and symbolic complexity show only weak positive association... numeric density consistently separates families."
- **Break condition:** If model uses external calculator or code interpreter, density correlation should theoretically vanish.

### Mechanism 3
- **Claim:** Hardest-slice (Q5) retention acts as proxy for process integrity and "correct-by-reasoning" stability.
- **Mechanism:** Aggregate accuracy conflates luck/guessing with reasoning. Performance on hardest quintile (Q5) functions as stress test for "plan integrity," separating models with similar top-line scores by stability under structural load.
- **Core assumption:** Hardness successfully isolates reasoning complexity (e.g., constraint branching) rather than obscure domain knowledge.
- **Evidence anchors:** [abstract] "hardest-slice retention exposing robustness gaps." [Section 5.1.1] "Among models with similar top-line scores, Q5 retention provides a sharper discriminator of plan integrity."
- **Break condition:** If Q5 items contain ambiguous prompts rather than complex reasoning, low scores would measure ambiguity resolution rather than reasoning failure.

## Foundational Learning

- **Concept:** Ceiling Effects & Saturation
  - **Why needed here:** To understand why standard benchmarks fail to separate frontier models, necessitating SKYLENAGE contest suite.
  - **Quick check question:** Does your evaluation metric show clear linear separation between top 3 models, or are they all scoring >90%?

- **Concept:** Structure-First Reasoning
  - **Why needed here:** To differentiate problems solvable by rote computation vs. those requiring "constructive witnesses" or logical invariants.
  - **Quick check question:** Can the model explain why a solution is valid (e.g., providing a witness) rather than just outputting final number?

- **Concept:** Multi-Label Subject Taxonomy
  - **Why needed here:** To interpret "fragmented leadership" where no single model dominates all subjects.
  - **Quick check question:** If you route problems to specific models based on subject tags, does aggregate performance improve?

## Architecture Onboarding

- **Component map:** Input (ReasoningMATH + MATH tracks) → Harness (CoT + self-consistency + regex extraction) → Analysis (Subject×Model + Grade×Model heatmaps + Q5 retention)

- **Critical path:**
  1. Execute unified evaluation protocol on all 15 model variants
  2. Generate Subject×Model and Grade×Model heatmaps to identify specialization
  3. Filter for hardest quintile (Q5) to calculate retention ratios and expose robustness gaps

- **Design tradeoffs:**
  - Public vs. Private: Release ReasoningMATH (100) for community debugging while withholding MATH (150) to prevent contamination
  - Exact Match vs. Process: Currently grade on exact match for reproducibility; trading off insight into intermediate reasoning steps

- **Failure signatures:**
  - "Correct by Guess": High aggregate score but low Q5 retention; correct final answer with inconsistent intermediate steps
  - Density Sensitivity: Performance plummets on items with high digit counts despite low symbolic complexity
  - Greedy Shortcuts: In grid/logic puzzles, model attempts direct path ignoring constraints, indicating search discipline failure

- **First 3 experiments:**
  1. Subject-Aware Router Test: Implement routing layer directing Geometry to Grok-4-0709 and Combinatorics to GPT-5-20250807; measure accuracy delta
  2. Numeric Density Stress Test: Synthesize variants with inflated digit counts to quantify error inflation slope for your model configuration
  3. Process Validation: Run BFS Maze case study (Supplementary A.2.1) and manually verify "Move path" string matches valid coordinate path

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does process-based scoring distinguish "correct-by-reasoning" from "correct-by-guess" in frontier models?
- **Basis in paper:** [explicit] Section 6 states future releases will use process-based signals to separate shortcutting from valid reasoning, which current exact-match grading obscures
- **Why unresolved:** Current evaluation protocol limited to exact-match outcomes; step-level verification listed as limitation and future work
- **Evidence:** Comparison of model rankings under exact-match protocols versus process-aware protocols on SKYLENAGE-REASONINGMATH set

### Open Question 2
- **Question:** Can dynamic adversarial variants effectively degrade performance for models relying on surface-form memorization rather than structural understanding?
- **Basis in paper:** [explicit] Section A.2.2 proposes scaling benchmark using "adversarial variants that stress constraint fidelity" and "bilingual variants" to test stability
- **Why unresolved:** Report analyzes static core; diagnostic power of these controlled dynamic variants has not been empirically validated
- **Evidence:** Performance delta between static core and new adversarial items, specifically for models with high numeric density sensitivity

### Open Question 3
- **Question:** Does fragmented subject leadership observed in SKYLENAGE-MATH imply that subject-aware routing yields significant gains over single-model reliance?
- **Basis in paper:** [inferred] Sections 5.2.2 and 6 note rotating leadership across subjects and suggest "subject-aware routing" as implication
- **Why unresolved:** While per-subject champions are identified, paper does not test aggregate performance of ensemble or routing system against single models
- **Evidence:** Accuracy of hypothetical subject-router (selecting champion per subject) compared to overall accuracy of top single model

## Limitations
- Benchmark completeness: Dual benchmark covers only 250 problems total, raising questions about generalizability across full math reasoning space
- Metadata reliability: Academic stage mapping for SKYLENAGE-MATH problems relies on subjective difficulty calibration that may not perfectly reflect reasoning complexity
- External calculator impact: Core finding that numeric density drives error inflation assumes no external computation aid

## Confidence
- **High confidence:** Ceiling effects on standard benchmarks and need for harder evaluations is well-established; observed performance gap between top and mid-tier models at doctoral level is reproducible
- **Medium confidence:** Numeric density correlation with error rates appears robust but may vary with different tokenization schemes or model families; Q5 retention metric shows promise but lacks strong corpus validation
- **Low confidence:** Specific mechanism that numeric density serves as proxy for internal precision/fidelity is speculative and requires direct investigation into model internals

## Next Checks
1. **Subject-aware routing validation:** Implement routing layer directing problems to champion models per subject and measure accuracy delta against random assignment
2. **Density stress testing:** Create synthetic variants with controlled numeric density manipulation to measure error inflation slopes across model families
3. **Process integrity verification:** Execute BFS Maze case study (Supplementary A.2.1) with multiple runs and manually verify "Move path" outputs represent valid coordinate paths