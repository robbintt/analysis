---
ver: rpa2
title: HunyuanOCR Technical Report
arxiv_id: '2511.19575'
source_url: https://arxiv.org/abs/2511.19575
tags:
- quad
- hunyuanocr
- text
- parsing
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HunyuanOCR is a 1B-parameter end-to-end Vision-Language Model for
  OCR that unifies spotting, parsing, IE, VQA, and translation in a single architecture.
  It replaces complex multi-step pipelines with a native-resolution ViT + lightweight
  LLM design, trained on 200M+ high-quality samples with RLVR and LLM-as-judge rewards.
---

# HunyuanOCR Technical Report

## Quick Facts
- arXiv ID: 2511.19575
- Source URL: https://arxiv.org/abs/2511.19575
- Reference count: 40
- Primary result: 1B-parameter end-to-end VLM achieving 94.10 on OmniDocBench and 70.92 on 9-category spotting benchmark

## Executive Summary
HunyuanOCR is a 1B-parameter end-to-end Vision-Language Model that unifies text spotting, parsing, information extraction, VQA, and translation in a single architecture. It replaces traditional multi-stage OCR pipelines with native-resolution ViT encoding and lightweight LLM generation, trained on 200M+ samples with reinforcement learning. The model achieves state-of-the-art performance on multiple benchmarks while maintaining high inference efficiency, demonstrating that lightweight VLMs can outperform much larger models through unified end-to-end optimization.

## Method Summary
HunyuanOCR employs a native-resolution ViT (0.4B params) connected to a 0.5B LLM via an adaptive MLP connector, trained through a 4-stage process: token alignment, multimodal pre-training, 32K context extension, and SFT fine-tuning on application data. The model uses GRPO reinforcement learning with task-specific rewards (IoU+edit distance for spotting/parsing, LLM-as-judge for translation/VQA) to achieve significant performance gains. Training utilizes 200M+ high-quality image-text pairs across 130+ languages and 9 document scenarios, with total pre-training consuming ~454B tokens.

## Key Results
- Achieves 94.10 score on OmniDocBench, setting new state-of-the-art
- Ranks first in ICDAR 2025 DIMT competition
- Outperforms much larger VLMs (e.g., Qwen3-VL-4B) and commercial APIs on spotting benchmarks
- Demonstrates significant RLVR gains on structured OCR tasks

## Why This Works (Mechanism)

### Mechanism 1: End-to-End Architecture Eliminating Error Propagation
A unified end-to-end VLM architecture outperforms cascaded pipeline systems by eliminating cumulative errors through single-inference-pass processing with global attention. This fundamentally resolves error propagation common in traditional pipelines where inaccuracies in text detection degrade input quality for subsequent recognition modules.

### Mechanism 2: Reinforcement Learning with Task-Specific Verifiable Rewards
GRPO with carefully designed rewards yields significant gains on structured OCR tasks even for lightweight models. The approach samples multiple responses per prompt, computes task-specific rewards (IoU+edit distance for spotting, LLM-as-judge for translation/VQA), and updates policy to maximize expected reward while maintaining output format constraints.

### Mechanism 3: Native-Resolution Visual Encoding with Adaptive Token Compression
Preserving native aspect ratios through adaptive patching combined with learned token compression improves recognition of extreme-aspect-ratio documents. Images are divided into patches according to native proportions, processed with global attention, then compressed to reduce sequence length while preserving text-dense regions.

## Foundational Learning

- **Vision-Language Model Alignment**: Understanding how visual features map to language model embedding space is critical for debugging why the model misses text or produces incorrect outputs. *Quick check*: Can you explain why Stage-1 pre-training freezes the LLM while training only ViT+adapter?

- **Reinforcement Learning from Verifiable Rewards (RLVR)**: The paper claims first-in-industry application of RL to OCR; understanding policy gradients and reward design is essential for reproducing or extending these results. *Quick check*: How does GRPO differ from standard PPO, and why might group-relative advantages be beneficial for OCR tasks?

- **Transformer Position Encodings for 2D/3D Data**: XD-RoPE decomposes position into text/height/width/time subspaces; understanding this is crucial for debugging layout-related failures. *Quick check*: Why would a 1D position encoding struggle with multi-column documents, and how does XD-RoPE address this?

## Architecture Onboarding

- **Component map**: Input image → Adaptive patching → ViT encoding → MLP compression → LLM generation → Task-specific formatted output
- **Critical path**: Native Resolution ViT (SigLIP-v2-400M) → Adaptive MLP Connector → Hunyuan-0.5B LLM with XD-RoPE position encoding
- **Design tradeoffs**: 1B total params limits translation quality vs. spotting/parsing; 32K context window enables long documents but increases memory; end-to-end simplifies deployment but makes error attribution harder
- **Failure signatures**: Translation underperforms larger models; spotting on artistic text shows lower scores than other categories; long documents exceeding 32K tokens will be truncated
- **First 3 experiments**:
  1. Establish baseline on OmniDocBench with default prompts; measure parsing accuracy, inference latency, and memory footprint per page
  2. Ablate RL training: compare pre-training-only checkpoint vs. final RL-tuned model on spotting task to isolate RL contribution
  3. Test context limits: run inference on documents of varying lengths to identify the degradation threshold before 32K tokens

## Open Questions the Paper Calls Out

- **Context window extension**: How can the architecture be extended to handle multi-page or higher-resolution documents without compromising the 1B-parameter inference efficiency? The paper plans to expand capability to higher-resolution and multi-page documents, but scaling to multi-page inputs conflicts with the "lightweight" design goals.

- **Edge-device deployment**: To what extent can token compression techniques be applied to enable edge-device deployment before fine-grained text spotting accuracy degrades? The long-term goal is adapting for edge-device deployment via token compression, but further compression risks losing fine-grained semantic information necessary for precise bounding box localization.

- **Translation quality limits**: Is the performance gap in text image translation a fundamental limitation of the 0.5B LLM backbone, or can it be closed solely through improved reinforcement learning strategies? The paper suggests cascading larger models for higher translation accuracy, but it's unclear if RLVR can overcome the "capacity" limits of a 0.5B model for translation.

## Limitations

- Translation quality lags behind larger VLMs due to the 0.5B LLM backbone, requiring cascading with larger models for high-accuracy applications
- Critical architectural details like Adaptive MLP Connector parameters and data mixture ratios remain underspecified, hindering faithful reproduction
- RLVR effectiveness for OCR lacks external validation and could be specific to this implementation rather than a generalizable principle

## Confidence

- **High Confidence**: State-of-the-art results on OmniDocBench (94.10) and ICDAR 2025 DIMT (first place) are supported by direct benchmark comparisons with quantified metrics
- **Medium Confidence**: End-to-end architecture eliminating error propagation is logically sound but magnitude of improvement over cascaded pipelines primarily demonstrated through benchmark results
- **Low Confidence**: RLVR mechanism's effectiveness specifically for OCR remains unverified in external literature and lacks independent corroboration

## Next Checks

1. **Ablation Study on RL Contribution**: Train two identical checkpoints - one with only pre-training and one with full RLVR fine-tuning. Compare spotting accuracy on ICDAR benchmarks to quantify the exact performance gain attributable to RLVR versus other factors.

2. **Translation Quality Analysis**: Test the model on a diverse multilingual document corpus (10+ languages including low-resource ones) and compare translation accuracy against both claimed "larger VLMs" and specialized translation systems. Document specific quality degradation patterns.

3. **Context Length Stress Test**: Systematically evaluate document parsing accuracy on documents ranging from 8K to 32K tokens in 4K increments. Identify the exact threshold where performance begins degrading and analyze whether degradation is gradual or exhibits sudden cliffs.