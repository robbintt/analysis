---
ver: rpa2
title: 'PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI Collaboration
  for Large Language Models'
arxiv_id: '2510.19616'
source_url: https://arxiv.org/abs/2510.19616
tags:
- bias
- dataset
- language
- persian
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PBBQ, the first Persian-language benchmark
  dataset for evaluating social biases in large language models (LLMs). The dataset
  was created by extracting stereotypes from social media, validating them through
  a survey of 250 Persian speakers across diverse demographics, and constructing ambiguous
  and disambiguated contexts paired with negative/non-negative questions.
---

# PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models

## Quick Facts
- arXiv ID: 2510.19616
- Source URL: https://arxiv.org/abs/2510.19616
- Authors: Farhan Farsi; Shayan Bali; Fatemeh Valeh; Parsa Ghofrani; Alireza Pakniat; Kian Kashfipour; Amir H. Payberah
- Reference count: 0
- Key outcome: First Persian-language benchmark for evaluating social biases in LLMs, containing 37,742 QA samples across 16 cultural bias categories.

## Executive Summary
This paper introduces PBBQ, the first Persian-language benchmark dataset for evaluating social biases in large language models (LLMs). The dataset was created by extracting stereotypes from social media, validating them through a survey of 250 Persian speakers across diverse demographics, and constructing ambiguous and disambiguated contexts paired with negative/non-negative questions. It covers 16 cultural bias categories and contains over 37,000 QA samples. Experiments with eight models—including open-source, closed-source, and Persian-specific fine-tuned LLMs—show that all models exhibit bias, with Persian-specific models performing better on ambiguous contexts but worse on disambiguated ones. Notably, models' bias patterns often align with human responses, suggesting that LLMs can reflect cultural stereotypes. The PBBQ dataset will be publicly available upon acceptance.

## Method Summary
The PBBQ dataset was constructed through a human-AI collaboration process: stereotypes were extracted from Persian social media, validated through surveys of 250 Persian speakers, and converted into 276 templates across 16 cultural bias categories. The evaluation uses lm-evaluation-harness with log-probability-based bias measurement. For each sample, models receive context + question + three options (stereotypical, non-stereotypical, unknown), and the option with highest total log-probability is selected as prediction. Key metrics include accuracy, Ambiguous Bias Score (βamb = avg log-prob difference between target/counter choices), Disambiguated Bias Score (∆bias = Acc(Qbias) - Acc(Qcounter)), and uncertainty scores.

## Key Results
- All eight tested models (Mistral-7B-Instruct, Llama-3.1-8B-Instruct, Qwen2.5-7B, GPT-4o, Maral-7B-alpha-1, Dorna variants) exhibit measurable bias across all 16 cultural categories.
- Persian-specific models (Maral, Dorna) perform better on ambiguous contexts but worse on disambiguated ones compared to their base models, suggesting a fine-tuning trade-off.
- Bias scores decrease significantly when contexts are disambiguated, indicating ambiguity drives stereotype-based inference.
- Models' bias patterns correlate with human survey responses, with KL divergence analysis showing Persian-specific models (Maral: 0.0820, Dorna: 0.1559-0.1624) exhibit lower divergence from human responses than general-purpose models (Mistral: 0.2436).

## Why This Works (Mechanism)

### Mechanism 1: Cultural Stereotype Reflection in Model Outputs
- Claim: LLMs trained on Persian cultural data tend to reproduce locally-recognized stereotypes at rates correlated with human survey responses.
- Mechanism: Stereotypes prevalent in Persian social media, humor traditions, and public discourse are absorbed into training corpora; models then assign higher log-probabilities to stereotype-aligned completions when contexts are ambiguous, reflecting distributional patterns rather than explicit reasoning.
- Core assumption: The correlation between model bias scores and human survey responses indicates training data influence rather than emergent reasoning.
- Evidence anchors: [abstract] "models' bias patterns often align with human responses"; [section: Discussion] KL divergence analysis shows Persian-specific models exhibit lower divergence from human responses; [corpus] Related work on Japanese (JBBQ) and Korean (KoBBQ) bias benchmarks similarly finds culturally-specific bias reproduction.

### Mechanism 2: Ambiguity-Driven Bias Activation
- Claim: Ambiguous contexts systematically elicit higher bias scores than disambiguated contexts because models default to statistical associations when explanatory information is missing.
- Mechanism: When context lacks clear attribution of negative traits to specific individuals, models rely on learned correlations between social groups and stereotypical attributes; disambiguation provides competing signal that can override these associations, reducing (but not eliminating) bias.
- Core assumption: The reduction in bias scores from ambiguous to disambiguated contexts reflects the availability of sufficient signal rather than fundamental debiasing.
- Evidence anchors: [section: Experiments, Table 3] All eight models show higher bias scores in ambiguous contexts compared to disambiguated contexts; [section: 4.2.1] "Bias-scores generally decrease once inputs are disambiguated"; [corpus] BasqBBQ findings corroborate this pattern.

### Mechanism 3: Persian-Specific Fine-Tuning Creates Accuracy-Bias Trade-offs
- Claim: Persian-specific fine-tuned models (Dorna, Maral) achieve better accuracy on ambiguous Persian contexts but show degraded performance on disambiguated contexts compared to their base models.
- Mechanism: Fine-tuning on Persian corpora strengthens cultural and linguistic associations (improving ambiguous-context reasoning) but may overfit to stereotypical patterns, reducing flexibility when contexts provide contradictory signals; this creates a trade-off where cultural specialization comes at the cost of debiasing capacity.
- Core assumption: The performance divergence between ambiguous and disambiguated contexts for Persian-specific models reflects overfitting to cultural patterns rather than architectural limitations.
- Evidence anchors: [abstract] "Persian-specific models performing better on ambiguous contexts but worse on disambiguated ones"; [section: 4.2.1, Table 3] Dorna-Llama3-8B-Instruct shows 52.41% accuracy on ambiguous contexts but only 59.47% on disambiguated; [corpus] Related Persian benchmarks do not specifically address this fine-tuning trade-off.

## Foundational Learning

- **Concept: Representational vs. Allocational Harms**
  - Why needed here: PBBQ focuses on representational harms (stereotyping, misrepresentation) rather than allocational harms (discriminatory resource distribution); understanding this distinction is essential for correctly interpreting bias scores and their implications.
  - Quick check question: If a model consistently recommends higher loan approval rates for one ethnic group over another with identical financial profiles, is this a representational or allocational harm? (Answer: Allocational—PBBQ would not capture this.)

- **Concept: BBQ-Style Ambiguous/Disambiguated Context Pairs**
  - Why needed here: The dataset's core structure relies on contrasting model behavior when information is insufficient (ambiguous) versus complete (disambiguated); the difference between these conditions reveals whether models rely on stereotypes as default inference mechanisms.
  - Quick check question: In an ambiguous context where two individuals are mentioned without clear attribution of a negative behavior, what should a perfectly unbiased model select as its answer? (Answer: "Unknown"—selecting either individual indicates stereotype-driven inference.)

- **Concept: Log-Probability-Based Bias Measurement**
  - Why needed here: The Ambiguous Bias Score (βamb) uses the difference in log-probabilities between stereotypical and counter-stereotypical choices rather than just final predictions; this captures model uncertainty and preference strength, not just binary correctness.
  - Quick check question: If a model assigns 55% probability to the stereotypical choice and 45% to the counter-stereotypical choice in an ambiguous context, would its βamb score be positive or negative, and what would that indicate? (Answer: Positive—indicating systematic preference toward the stereotype, even though the preference is weak.)

## Architecture Onboarding

- **Component map:** Telegram channel crawling → stereotype extraction → expert filtering → 250-participant survey validation → template construction → ambiguous/disambiguated context pairing → evaluation framework
- **Critical path:** Dataset creation (stereotype extraction → validation → template construction) → model evaluation (log-probability calculation → bias score computation) → comparative analysis
- **Design tradeoffs:** Single-category evaluation (simpler analysis, misses intersectional biases) vs. intersectional design (more complex, harder to attribute specific bias sources)
- **Failure signatures:** Inconsistent log-probability extraction across model families, Persian tokenization issues affecting calculations, disproportionate "unknown" selection in ambiguous contexts
- **First experiments:** 1) Verify token alignment by printing tokenized inputs and checking option boundaries, 2) Compare token counts across models for identical Persian text to detect suboptimal tokenization, 3) Check if models disproportionately select "unknown" in ambiguous contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do social biases manifest in Persian LLMs when multiple demographic categories (e.g., gender and socioeconomic status) intersect?
- Basis in paper: [explicit] The authors state in the Limitations section: "Our benchmark investigates bias topics one at a time... Studying such intersectional cases is important, since real-world biases often emerge in overlapping and compounding ways."
- Why unresolved: The current PBBQ dataset structure isolates 16 specific categories but does not contain samples where these categories overlap.
- What evidence would resolve it: Construction of intersectional templates within the PBBQ framework and subsequent evaluation of model bias scores on these combined scenarios.

### Open Question 2
- Question: Do larger parameter models (70B+) exhibit distinct bias patterns or improved robustness compared to the smaller models currently benchmarked?
- Basis in paper: [explicit] The authors note a limitation regarding "Model scale," stating they "were unable to include language models with very higher numbers of parameters (e.g., 70B+)" due to resource constraints.
- Why unresolved: The current study is restricted to smaller open-source and Persian-tuned models, leaving the behavior of larger state-of-the-art systems untested on PBBQ.
- What evidence would resolve it: Benchmarking 70B+ parameter models (such as Llama-3-70B or Qwen-72B) on the PBBQ dataset to compare accuracy and bias scores against the reported baselines.

### Open Question 3
- Question: Why do Persian-specific fine-tuned models outperform base models on ambiguous contexts but underperform on disambiguated contexts?
- Basis in paper: [explicit] The Conclusion notes that "Persian-specific fine-tuned models show better accuracy and bias scores than their base models in ambiguous contexts, they are less effective in disambiguated ones."
- Why unresolved: The paper reports this counter-intuitive finding but does not analyze the underlying causes, such as the specific nature of the fine-tuning data or attention mechanisms.
- What evidence would resolve it: Ablation studies on the fine-tuning data composition or analysis of attention head distributions in Persian-specific models when processing disambiguated context.

## Limitations
- Dataset validation methodology relies on a single Persian-speaking survey without clear representation of minority or regional dialects, potentially skewing toward urban biases.
- Evaluation protocol's reliance on log-probability differences assumes comparable tokenization strategies across models, but Persian-specific tokenization quality varies significantly.
- Absence of systematic error analysis on specific bias categories limits understanding of whether certain stereotypes are more problematic than others.
- Correlation between model bias scores and human survey responses does not establish causation or rule out confounding factors like shared training data sources.

## Confidence

- **High confidence**: Models exhibit measurable bias across all tested categories; Persian-specific models perform better on ambiguous contexts but worse on disambiguated ones; ambiguity systematically increases bias scores across all models.
- **Medium confidence**: The correlation between model bias patterns and human survey responses indicates cultural stereotype reflection; the accuracy-bias trade-off for Persian-specific fine-tuning is robust across different model families.
- **Low confidence**: Claims about "cultural bias" categories require further validation across Persian-speaking communities beyond the survey sample; the dataset's generalization to real-world Persian language use remains untested.

## Next Checks

1. **Cross-population validation**: Replicate the survey with Persian speakers from diverse geographic regions (including minority communities) to verify that the 233 retained stereotypes represent broad cultural patterns rather than urban biases.

2. **Tokenization impact assessment**: Systematically compare log-probability-based bias scores across models using both character-level and subword tokenization for identical Persian inputs to quantify the impact of tokenization quality on bias measurement.

3. **Debiasing intervention study**: Apply controlled debiasing techniques (e.g., weighted training data, bias-aware fine-tuning) to a subset of Persian-specific models and measure whether bias reduction maintains or improves performance on both ambiguous and disambiguated contexts.