---
ver: rpa2
title: Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement
  Learning Based UAV Deconfliction
arxiv_id: '2507.11173'
source_url: https://arxiv.org/abs/2507.11173
tags:
- gnss
- detection
- spoo
- position
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting drift-evasive GNSS
  spoofing attacks on autonomous UAVs, where adversaries subtly manipulate pseudorange
  measurements to gradually divert the UAV's trajectory without triggering conventional
  signal-level defenses. The core method employs a Bayesian Online Change Point Detection
  (BOCPD) framework that monitors temporal shifts in value estimates from a trained
  reinforcement learning critic network, using these latent state-action-value sequences
  as indicators of decision confidence under nominal conditions.
---

# Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction

## Quick Facts
- **arXiv ID:** 2507.11173
- **Source URL:** https://arxiv.org/abs/2507.11173
- **Reference count:** 21
- **Primary result:** BOCPD achieves near-perfect detection of gradual GNSS spoofing attacks on RL-based UAV navigation while maintaining significantly lower false-positive and false-negative rates than baseline methods.

## Executive Summary
This paper addresses the challenge of detecting subtle GNSS spoofing attacks on autonomous UAVs where adversaries gradually manipulate pseudorange measurements to divert the UAV's trajectory without triggering conventional signal-level defenses. The proposed method employs Bayesian Online Change Point Detection (BOCPD) to monitor temporal shifts in value estimates from a trained reinforcement learning critic network, using these latent state-action-value sequences as indicators of decision confidence under nominal conditions. The approach was evaluated against semi-supervised LSTM autoencoders, the Page-Hinkley test, and conventional signal thresholding methods across 20 test episodes, demonstrating superior detection accuracy with minimal false alarms.

## Method Summary
The detection framework combines a DDPG-trained UAV navigation agent with BOCPD monitoring of critic Q-values. During deployment, GNSS pseudoranges are corrupted by a drift-evasive attack that gradually interpolates the UAV's position toward a target location while maintaining signal consistency. The RL critic network outputs Q-values for each state-action pair, which are fed to BOCPD that recursively maintains a posterior distribution over "run lengths" (time since last change point). When the Q-value distribution shifts significantly from nominal conditions, BOCPD flags the attack, potentially triggering transition to alternative sensing modalities.

## Key Results
- BOCPD achieved near-perfect detection accuracy while maintaining significantly lower false-positive and false-negative rates compared to baseline methods
- The framework demonstrated particular effectiveness in identifying gradual distributional shifts in the critic's value estimates that occur after spoofing attacks begin at t=100s
- Q-values under nominal conditions follow an approximately Gaussian distribution with stable statistics, enabling effective BOCPD detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Monitoring RL critic Q-value sequences enables detection of subtle GNSS manipulation that evades signal-level defenses.
- **Mechanism:** The critic network learns to assign high Q-values to state-action pairs that lead toward mission success under nominal conditions. When GNSS spoofing corrupts position estimates, the observed state diverges from the true state, causing the critic to output anomalous Q-values that reflect degraded decision confidence. BOCPD detects the resulting distributional shift in the temporal Q-value sequence.
- **Core assumption:** The trained critic's value function is sensitive to state perturbations induced by spoofing, and this sensitivity manifests as a detectable shift in the Q-value distribution.
- **Evidence anchors:**
  - [abstract]: "monitors temporal shifts in value estimates from a trained reinforcement learning critic network, using these latent state-action-value sequences as indicators of decision confidence under nominal conditions"
  - [Page 2]: "BOCPD tracks deviations of the spoofed critic signal qspoof_t relative to the nominal critic distribution values, qnom_t"
  - [corpus]: Limited direct corroboration; related work on GNSS spoofing detection focuses on signal-level or sensor fusion approaches rather than RL-based behavioral monitoring.
- **Break condition:** If the RL policy is trained to be robust to state perturbations (e.g., via adversarial training), the Q-value sensitivity to spoofing may be reduced, diminishing detection signal strength.

### Mechanism 2
- **Claim:** Bayesian online change point detection provides lower-latency attack identification than sample-accumulating distributional shift methods.
- **Mechanism:** BOCPD recursively maintains a posterior distribution over "run lengths" (time since the last change point) using a hazard function and a Gaussian predictive model. Upon observing each new Q-value, the algorithm updates the probability that a change point occurred. This recursive inference enables detection without requiring a pre-accumulated reference window, reducing detection latency.
- **Core assumption:** Q-values under nominal conditions follow an approximately Gaussian distribution with stable statistics, and attack-induced shifts are sufficiently large relative to natural variance.
- **Evidence anchors:**
  - [abstract]: "Traditional distributional shift detection techniques often require accumulating a threshold number of samples, causing delays that impede rapid detection and timely response"
  - [Page 4]: "The value estimates from the critic network are assumed to follow a Gaussian model, qt ∼ N(μt, σ²)"
  - [corpus]: Weak direct evidence; neighbor papers discuss GNSS anomaly detection but do not evaluate BOCPD on RL-derived signals specifically.
- **Break condition:** If the hazard rate is mis-specified or Q-value variance is high during normal operation (e.g., due to dynamic obstacle interactions), false-positive rates may increase.

### Mechanism 3
- **Claim:** Drift-evasive spoofing attacks designed to maintain signal-level consistency still produce detectable behavioral divergence in RL decision dynamics.
- **Mechanism:** The attacker interpolates the spoofed position gradually toward a target using a time-weighted blend of true and adversarial positions (Eq. 13). This maintains kinematic plausibility and evades power-based or correlation-based detectors. However, the cumulative position error corrupts the state vector fed to the RL agent, causing the critic's value estimates to diverge from the expected nominal trajectory—captured as a sustained Q-value depression (Figure 4).
- **Core assumption:** The attacker cannot perfectly model the RL agent's internal value function to craft spoofing trajectories that preserve Q-value consistency.
- **Evidence anchors:**
  - [Page 3]: "the spoofed position evolves as: Pspoof_t = (1−αt)·Pt + αt·Ptar_t"
  - [Page 5]: "under spoofed conditions, the critic value estimates remain persistently low after t=100s, failing to converge and ultimately resulting in mission compromise"
  - [corpus]: Assumption not directly validated in related literature; corpus focuses on signal-level spoofing detection without addressing RL-aware adversaries.
- **Break condition:** If an adversary gains access to the critic network and optimizes spoofing to minimize Q-value deviation, this detection channel could be adversarially suppressed.

## Foundational Learning

- **Concept: Bayesian Online Change Point Detection (BOCPD)**
  - **Why needed here:** BOCPD is the core detection engine; understanding its recursive posterior update over run lengths is essential to interpret detection outputs and tune hazard/gaussian model parameters.
  - **Quick check question:** Given a sequence of observations, can you explain how the run-length posterior changes when a new observation deviates significantly from recent history?

- **Concept: Deep Deterministic Policy Gradient (DDPG) and Critic Networks**
  - **Why needed here:** The detection signal is derived from the critic's Q-values; understanding how the critic estimates expected cumulative reward clarifies why Q-values shift under spoofed states.
  - **Quick check question:** In DDPG, what does the critic network approximate, and how does its output change if the input state is corrupted?

- **Concept: GNSS Pseudorange Positioning**
  - **Why needed here:** The attack vector manipulates pseudorange measurements to corrupt position estimates; understanding the iterative least-squares estimator (Eq. 1-7) clarifies the attack surface.
  - **Quick check question:** How do pseudorange residuals relate to position error, and why does gradual pseudorange manipulation evade threshold-based detectors?

## Architecture Onboarding

- **Component map:**
  - GNSS Simulator -> Position Estimator -> State vector Φt
  - Φt -> Actor policy at = π(Φt) AND Critic Q(Φt, at) -> Q-value qt
  - qt -> BOCPD recursive update -> Run-length posterior
  - If ˆlt ≤ τ: flag attack -> initiate failover to alternative sensing

- **Critical path:**
  1. GNSS pseudoranges → Position estimator → State vector Φt
  2. Φt → Actor policy at = π(Φt) AND Critic Q(Φt, at) → Q-value qt
  3. qt → BOCPD recursive update → Run-length posterior
  4. If ˆlt ≤ τ: flag attack → initiate failover to alternative sensing (e.g., visual-inertial odometry)

- **Design tradeoffs:**
  - **Hazard rate (H):** Lower H reduces false positives but may delay detection; higher H increases sensitivity but risks nuisance alerts.
  - **Threshold τ:** Detection threshold on run length; too high delays alert, too low increases false alarms.
  - **Gaussian assumption:** Simplifies computation but may not hold if Q-values exhibit heavy tails during aggressive maneuvers.
  - **Online-only operation:** No offline retraining required, but system assumes pre-trained critic generalizes to deployment conditions.

- **Failure signatures:**
  - **Persistent low Q-values without detection:** May indicate BOCPD parameters are too conservative or the critic is not sensitive to the specific attack trajectory.
  - **High false-positive rate under nominal conditions:** Suggests Q-value variance is underestimated or obstacle dynamics induce legitimate Q-value fluctuations misinterpreted as attacks.
  - **Delayed detection (>50 steps after attack onset):** Hazard rate may be too low, or attack drift rate is exceptionally slow relative to Q-value noise.

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Run BOCPD, semi-supervised LSTM autoencoder, Page-Hinkley, and signal thresholding on the provided 20-episode test set with attacks starting at t=100s; verify detection accuracy, FPR, and FNR align with Figure 5.
  2. **Parameter sensitivity sweep:** Vary hazard rate H ∈ {0.001, 0.01, 0.1} and threshold τ ∈ {1, 5, 10} to characterize the trade-off frontier between detection latency and false-positive rate.
  3. **Attack-rate robustness test:** Evaluate BOCPD detection latency under varying drift durations Tdrift ∈ {25, 50, 100, 200} steps to assess whether slower attacks degrade performance as predicted by the mechanism's break conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the BOCPD detection trigger be effectively integrated with an RL adaptation mechanism to dynamically switch to alternative sensors for fail-safe navigation?
- **Basis in paper:** [explicit] The conclusion states, "Future works involves integrating the BOCPD framework with the adaptation mechanism for the RL... to utilize the alternative sensors for fail-safe navigation."
- **Why unresolved:** The current study validates the detection capability (flagging the attack) but does not implement the subsequent closed-loop contingency planning or sensor fusion required to maintain the mission.
- **What evidence would resolve it:** A demonstration of a coupled detection-and-response system where a detected spoofing event triggers a verified switch to inertial or visual navigation without mission failure.

### Open Question 2
- **Question:** Is the assumption of a Gaussian distribution for temporal critic value estimates ($q_t$) valid across diverse RL architectures and non-stationary environments?
- **Basis in paper:** [inferred] The methodology explicitly models the predictive probability using a Gaussian likelihood ($q_t \sim \mathcal{N}(\mu_t, \sigma^2)$) to perform Bayesian updates.
- **Why unresolved:** The distribution of Q-values in deep RL can be complex and multimodal, particularly in stochastic environments; a mismatch in the underlying probability model could degrade detection performance.
- **What evidence would resolve it:** A comparative analysis using non-parametric Bayesian models or an examination of Q-value residuals to verify the Gaussian assumption holds under varying environmental stochasticity.

### Open Question 3
- **Question:** How robust is the BOCPD framework against "value-aware" adversaries who optimize spoofing signals to specifically minimize shifts in the critic's value estimates?
- **Basis in paper:** [inferred] The evaluation simulates "drift-evasive" attacks focused on trajectory manipulation, but assumes the attacker does not have specific knowledge of the Q-value monitoring defense.
- **Why unresolved:** An adversary with access to the target's policy network could theoretically craft perturbations that divert the UAV while minimizing the KL-divergence of the Q-value distribution, evading this specific detector.
- **What evidence would resolve it:** Simulations involving adaptive attacks where the spoofing signal is optimized with a loss term that penalizes deviations in the monitored Q-value statistics.

## Limitations
- The core detection mechanism relies on the untested assumption that an adversary cannot adversarially optimize spoofing trajectories to preserve Q-value consistency with the critic network
- The Gaussian assumption for Q-value distributions may break down during aggressive maneuvers, potentially increasing false-positive rates
- The neighbor corpus provides limited direct evidence, with related work focusing on signal-level spoofing detection rather than RL-based behavioral monitoring

## Confidence

- **High confidence:** The BOCPD framework correctly implements recursive Bayesian change point detection and will detect distributional shifts in Q-value sequences under the stated Gaussian model assumptions
- **Medium confidence:** The drift-evasive spoofing attack (Eq. 13-14) produces detectable behavioral divergence in Q-values based on the mechanism described, though this depends critically on the RL critic's sensitivity to state perturbations
- **Low confidence:** The claim that BOCPD provides lower-latency detection than sample-accumulating methods is supported only by theoretical argument rather than empirical comparison of detection delay distributions

## Next Checks
1. Implement and evaluate the detection system against an RL-aware adversary who optimizes spoofing trajectories to minimize Q-value deviation from nominal values
2. Conduct robustness analysis by varying the Q-value distribution assumptions (e.g., heavy-tailed vs Gaussian) and measuring impact on false-positive rates during aggressive maneuvers
3. Perform detection latency benchmarking comparing BOCPD against cumulative sum (CUSUM) and EWMA methods on the same Q-value streams to validate the latency advantage claim