---
ver: rpa2
title: 'SO-Bench: A Structural Output Evaluation of Multimodal LLMs'
arxiv_id: '2511.21750'
source_url: https://arxiv.org/abs/2511.21750
tags:
- schema
- structured
- arxiv
- output
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SO-Bench, the first benchmark designed to\
  \ evaluate multimodal large language models' ability to generate structured, schema-compliant\
  \ outputs grounded in visual inputs. The benchmark covers four visual domains\u2014\
  UI screens, natural images, documents, and charts\u2014with over 1.8K image-schema\
  \ pairs and 6.5K diverse JSON schemas."
---

# SO-Bench: A Structural Output Evaluation of Multimodal LLMs

## Quick Facts
- **arXiv ID:** 2511.21750
- **Source URL:** https://arxiv.org/abs/2511.21750
- **Reference count:** 39
- **Primary result:** First benchmark evaluating multimodal LLMs' ability to generate structured, schema-compliant outputs from visual inputs across four domains

## Executive Summary
SO-Bench introduces a comprehensive benchmark for evaluating multimodal large language models' capability to generate structured outputs grounded in visual inputs. The benchmark covers UI screens, natural images, documents, and charts, with over 1.8K image-schema pairs and 6.5K diverse JSON schemas. Evaluation metrics include schema validation accuracy, field matching accuracy (exact and fuzzy), and full structure matching accuracy. Results show state-of-the-art models like Gemini-2.5-Pro achieve high schema validation (up to 98%) but struggle with full structure matching (under 20% fuzzy accuracy), revealing significant gaps in multimodal structured reasoning. Training experiments with supervised fine-tuning and reinforcement learning demonstrate substantial performance improvements, highlighting the need for targeted supervision in this domain.

## Method Summary
The benchmark evaluates multimodal LLMs on their ability to generate JSON outputs that conform to predefined schemas while being grounded in visual inputs. It spans four visual domains: UI screens, natural images, documents, and charts. The evaluation framework employs three metrics: schema validation accuracy (checking JSON schema compliance), field matching accuracy (measuring exact and fuzzy field name matches), and full structure matching accuracy (evaluating complete structural alignment). The benchmark includes over 1.8K image-schema pairs and 6.5K diverse JSON schemas. Training experiments utilize supervised fine-tuning and reinforcement learning approaches, with models fine-tuned on the SO-Bench dataset to improve structured output generation capabilities.

## Key Results
- State-of-the-art models achieve up to 98% schema validation accuracy but less than 20% full structure matching accuracy (fuzzy)
- Training with supervised fine-tuning and reinforcement learning improves performance by up to 20% on schema validation and 13% on field matching accuracy
- Significant performance gaps persist between visual understanding and reliable structured generation across all four visual domains

## Why This Works (Mechanism)
The benchmark works by creating a standardized evaluation framework that isolates and measures the specific capability of multimodal LLMs to generate structured, schema-compliant outputs from visual inputs. By providing explicit JSON schemas alongside images, the benchmark forces models to demonstrate not just visual understanding but the ability to translate that understanding into precise structured formats. The multi-metric evaluation approach (schema validation, field matching, full structure matching) provides granular insights into where models succeed and fail, enabling targeted improvements through supervised fine-tuning and reinforcement learning.

## Foundational Learning

**Visual Grounding:** The ability to extract relevant information from visual inputs and map it to structured representations. Why needed: Multimodal models must connect visual elements to specific schema fields. Quick check: Can the model correctly identify UI elements and map them to corresponding JSON fields?

**Schema Compliance:** Understanding and adhering to predefined JSON schema structures. Why needed: Generated outputs must be valid and usable in downstream applications. Quick check: Does the generated JSON validate against the provided schema?

**Structured Reasoning:** The cognitive process of organizing information into hierarchical, typed structures. Why needed: Visual information must be systematically organized according to schema requirements. Quick check: Can the model maintain nested structure relationships when describing complex visual scenes?

**Multimodal Integration:** Combining visual and textual understanding to produce coherent structured outputs. Why needed: Visual inputs must be accurately translated into structured text formats. Quick check: Does the model maintain consistency between visual content and structured output?

## Architecture Onboarding

**Component Map:** Input Image → Visual Encoder → Multimodal Fusion → Structured Output Generator → JSON Output → Schema Validator → Evaluation Metrics

**Critical Path:** Image → Visual Understanding → Schema Mapping → Structured Generation → Validation

**Design Tradeoffs:** The benchmark prioritizes comprehensive evaluation over computational efficiency, using multiple metrics that provide detailed performance breakdowns. This approach trades speed for diagnostic precision, enabling targeted model improvements but requiring more evaluation time.

**Failure Signatures:** Models commonly fail at field-level precision (exact matching) despite schema compliance, indicating strong structural understanding but weak semantic alignment. Visual domain complexity also affects performance, with charts and documents showing lower accuracy than UI screens.

**First Experiments:**
1. Evaluate a baseline multimodal model on SO-Bench without any fine-tuning to establish performance baselines
2. Fine-tune the model using supervised learning on the SO-Bench dataset and re-evaluate
3. Apply reinforcement learning to the fine-tuned model and measure performance improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark scale limited to 1.8K image-schema pairs, potentially restricting generalizability
- Evaluation metrics may not fully capture semantic correctness in context-dependent situations
- Training experiments focus only on supervised fine-tuning and reinforcement learning, excluding other potential approaches

## Confidence
**High confidence:** Benchmark design methodology and evaluation metrics are well-defined and technically sound
**Medium confidence:** Performance gaps between state-of-the-art models and benchmark requirements appear consistent with current limitations
**Low confidence:** Generalizability to broader real-world applications and sufficiency of proposed training approaches remain uncertain

## Next Checks
1. Expand benchmark corpus by an order of magnitude (to 18K+ pairs) across same four domains and additional domains to test scalability and robustness
2. Conduct ablation studies on training methods to isolate which components contribute most to performance improvements
3. Test benchmark and trained models on out-of-distribution visual inputs and schemas not present in training data to evaluate true generalization capabilities