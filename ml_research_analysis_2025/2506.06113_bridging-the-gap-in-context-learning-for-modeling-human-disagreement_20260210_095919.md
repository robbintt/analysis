---
ver: rpa2
title: 'Bridging the Gap: In-Context Learning for Modeling Human Disagreement'
arxiv_id: '2506.06113'
source_url: https://arxiv.org/abs/2506.06113
tags:
- multip
- aggr
- hard
- disaggr
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models can effectively
  capture human disagreement in subjective NLP tasks such as hate speech and offensive
  language detection. The study evaluates four open-source LLMs using in-context learning
  across three label modeling strategies: aggregated hard labels, disaggregated hard
  labels, and disaggregated soft labels, in both zero-shot and few-shot settings.'
---

# Bridging the Gap: In-Context Learning for Modeling Human Disagreement

## Quick Facts
- **arXiv ID:** 2506.06113
- **Source URL:** https://arxiv.org/abs/2506.06113
- **Reference count:** 40
- **One-line primary result:** Multi-perspective prompting enables zero-shot ICL to better capture human disagreement in subjective NLP tasks, while few-shot setups often fail to capture the full spectrum of human judgments.

## Executive Summary
This paper investigates whether large language models can effectively capture human disagreement in subjective NLP tasks such as hate speech and offensive language detection. The study evaluates four open-source LLMs using in-context learning across three label modeling strategies: aggregated hard labels, disaggregated hard labels, and disaggregated soft labels, in both zero-shot and few-shot settings. Demonstration selection strategies (textual similarity, annotator disagreement, two-stage ranking) and ordering methods (random vs. curriculum-based) are explored in few-shot prompting. Results show that multi-perspective generation is viable in zero-shot settings, but few-shot setups often fail to capture the full spectrum of human judgments. Prompt design and demonstration selection notably affect performance, while example ordering has limited impact. The findings highlight the challenges of modeling subjectivity with LLMs and underscore the importance of building perspective-aware, socially intelligent models.

## Method Summary
The study evaluates four open-source instruction-tuned LLMs (Olmo-7b-Instruct, Llama-3-8b-Instruct, Gemma-7b-it, Deepseek-7b-chat) on three SemEval 2023 LeWiDi benchmark datasets using in-context learning. The research compares zero-shot and few-shot prompting across three label spaces: aggregated hard (majority-voted single label), disaggregated hard (individual annotator labels), and disaggregated soft (probability distributions). Demonstration selection uses BM-25 textual similarity, PLM-based embeddings, annotator disagreement (entropy), and two-stage ranking. Evaluation metrics include Jensen-Shannon Divergence (JSD) and Cross-Entropy (CE) for soft distributions, plus macro F1 for hard predictions.

## Key Results
- Multi-perspective prompting outperforms baseline in zero-shot settings across most scenarios, with lower JSD, lower CE, and higher F1 scores
- Few-shot setups often fail to capture the full spectrum of human judgments, regardless of baseline or multi-perspective approaches
- Two-stage ranking (textual similarity + annotator disagreement) improves JSD and CE compared to random or single-criterion selection
- Demonstration ordering (random vs. curriculum-based) has limited impact on performance

## Why This Works (Mechanism)

### Mechanism 1: Multi-Perspective Prompting Reduces Dominant Viewpoint Bias
- Claim: Explicitly instructing models to consider diverse perspectives improves alignment with human judgment distributions on subjective tasks, compared to standard single-label prompting.
- Mechanism: Multi-perspective priming adds contextual instructions that discourage the model from defaulting to a single dominant label, instead encouraging consideration of multiple valid interpretations.
- Core assumption: LLMs trained on aggregated data implicitly favor majority viewpoints; explicit perspective-taking instructions can modulate this bias.
- Evidence anchors: Multi-perspective approach outperforms baseline in most zero-shot scenarios with lower JSD, lower CE and higher F1 scores.

### Mechanism 2: Zero-Shot Context Sufficient for Perspective Generation; Few-Shot May Narrow Output Distribution
- Claim: Zero-shot prompting enables models to generate multi-perspective outputs more effectively than few-shot setups for subjective tasks, where few-shot demonstrations may bias toward narrow label patterns.
- Mechanism: Zero-shot prompts provide task definitions and perspective instructions without constraining the model to specific prior examples, allowing greater flexibility. Few-shot demonstrations constrain the model toward patterns seen in examples.
- Core assumption: Few-shot demonstrations bias output distributions toward demonstrated patterns; zero-shot settings retain broader prior distribution access.
- Evidence anchors: Few-shot setups often fail to capture the full spectrum of human judgments regardless of approach used.

### Mechanism 3: Demonstration Selection via Textual Similarity + Disagreement Improves Subjective Task ICL
- Claim: Selecting few-shot demonstrations based on textual similarity to the target and re-ranking by annotator disagreement yields lower JSD than random or similarity-only selection.
- Mechanism: Textual similarity ensures relevance to the target input, while re-ranking by disagreement prioritizes ambiguous, high-entropy examples that expose the model to edge cases and multiple perspectives.
- Core assumption: High-disagreement examples carry more signal for modeling subjective tasks because they reveal ambiguity and alternative interpretations.
- Evidence anchors: Two-stage ranking improves JSD and CE for both baseline and multi-perspective approaches.

## Foundational Learning

- Concept: Jensen-Shannon Divergence (JSD)
  - Why needed here: JSD measures distance between probability distributions, enabling evaluation of how well model-predicted soft label distributions match human judgment distributions.
  - Quick check question: Can you explain why JSD is preferred over KL divergence for comparing label distributions?

- Concept: Aggregated vs Disaggregated Labels (Hard and Soft)
  - Why needed here: The paper compares three label typesâ€”aggregated hard (majority-voted single label), disaggregated hard (individual annotator labels), and disaggregated soft (probability distributions).
  - Quick check question: What information is lost when using aggregated hard labels for subjective tasks?

- Concept: In-Context Learning (Zero-Shot vs Few-Shot)
  - Why needed here: The study contrasts zero-shot (no examples) and few-shot (with demonstration examples) prompting strategies to evaluate their effectiveness in modeling human disagreement.
  - Quick check question: How does the choice between zero-shot and few-shot affect output distribution flexibility in subjective tasks?

## Architecture Onboarding

- Component map: Prompt Template -> Demonstration Selection -> Demonstration Ordering -> Models -> Evaluation
- Critical path: 1) Define task and label space in prompt 2) For few-shot, select demonstrations using two-stage ranking 3) Apply curriculum ordering if using ICCL 4) Prompt model; extract probabilities 5) Convert outputs to soft distributions if needed; compute JSD and CE
- Design tradeoffs: Zero-shot vs Few-Shot (broader distribution flexibility vs task grounding), Aggregated vs Disaggregated Labels (simplicity vs disagreement nuance), Similarity vs Disagreement in Selection (relevance vs ambiguity signal), Random vs Curriculum Ordering (simplicity vs potential gains)
- Failure signatures: Monolithic Disaggregated Predictions ([0,0,0,0] or [1,1,1,1] patterns), Bimodal Soft Outputs ([0.9,0.1] or [0.8,0.2] lacking nuanced spreads), High JSD despite Low CE (confidence misalignment), Few-Shot Degradation (adding examples increases JSD or reduces F1)
- First 3 experiments:
  1. Baseline Comparison (Zero-Shot): Prompt four LLMs with baseline and multi-perspective instructions on all three datasets, measuring JSD, CE, and F1
  2. Demonstration Strategy Ablation (Few-Shot): Compare BM-25, PLM-based similarity, disagreement-only, and two-stage ranking for Deepseek-7b-chat and Olmo-7b-Instruct
  3. Label Space Expansion (Zero-Shot): Evaluate multi-perspective prompts with disaggregated hard and soft label outputs using best-performing model

## Open Questions the Paper Calls Out

- Can alternative demonstration selection or prompting strategies more effectively capture the full spectrum of human judgments in few-shot ICL for subjective tasks?
- What mechanisms cause LLMs to output monolithic distributions rather than nuanced disagreement patterns when predicting disaggregated labels?
- Would integrating bias evaluation metrics alongside traditional performance measures reveal trade-offs between capturing human disagreement and mitigating model bias?

## Limitations

- The study does not explore intermediate strategies such as weighted soft labels or probabilistic annotation models that might better capture human disagreement
- Demonstration selection relies on relatively simple textual similarity measures and entropy-based disagreement ranking
- Evaluation assumes test set human distributions represent ground truth without accounting for potential sampling bias
- The paper focuses on binary and multi-class subjective tasks but does not address regression-style or continuous disagreement metrics

## Confidence

**High Confidence:** Multi-perspective prompting outperforms baseline in zero-shot settings is well-supported by quantitative results across multiple datasets and models.

**Medium Confidence:** Demonstration selection via textual similarity plus disagreement improves ICL performance, but effect size varies across datasets and model pairs.

**Low Confidence:** The assertion that curriculum ordering has limited impact requires more systematic testing across diverse subjective tasks.

## Next Checks

1. Evaluate multi-perspective prompting on larger frontier models (GPT-4, Claude) to determine if observed benefits transfer beyond 7B parameter models and assess interaction with demonstration selection strategies.

2. Generate synthetic test sets with controlled disagreement distributions to systematically measure how well each prompting strategy recovers known human judgment distributions.

3. Test whether increasing the number of demonstration examples beyond one-shot improves disaggregated label prediction and whether two-stage ranking maintains its advantage with larger demonstration sets.