---
ver: rpa2
title: On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection
arxiv_id: '2512.02520'
source_url: https://arxiv.org/abs/2512.02520
tags:
- anomaly
- anomalies
- normal
- similarity
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This dissertation introduces and addresses the problem of consistent\
  \ anomalies in zero-shot anomaly detection, where recurring similar defects systematically\
  \ bias distance-based methods. The work identifies two key phenomena\u2014similarity\
  \ scaling and neighbor-burnout\u2014that describe how relationships among normal\
  \ patches change with and without consistent anomalies in settings characterized\
  \ by highly similar objects."
---

# On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection

## Quick Facts
- **arXiv ID**: 2512.02520
- **Source URL**: https://arxiv.org/abs/2512.02520
- **Authors**: Tai Le-Gia
- **Reference count**: 0
- **Primary result**: Introduces CoDeGraph, a graph-based framework that filters consistent anomalies through multi-stage graph construction, community detection, and structured refinement, outperforming state-of-the-art methods on both industrial and medical datasets.

## Executive Summary
This dissertation addresses the problem of consistent anomalies in zero-shot anomaly detection, where recurring similar defects systematically bias distance-based methods. The work identifies two key phenomena—similarity scaling and neighbor-burnout—that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects. To address this, CoDeGraph is introduced, a graph-based framework that filters consistent anomalies through multi-stage graph construction, community detection, and structured refinement. The dissertation extends this framework to 3D medical imaging via a training-free volumetric tokenization strategy for MRI data, enabling genuinely zero-shot 3D anomaly segmentation. Finally, it bridges batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models.

## Method Summary
CoDeGraph operates on vision transformer patch tokens to detect and filter consistent anomalies through a three-stage graph-based pipeline. First, it computes mutual similarities between patches across image collections and identifies suspicious links using endurance ratios that detect neighbor-burnout. Second, it constructs a weighted anomaly similarity graph from these suspicious links and applies Leiden community detection with Constant Potts Model to identify dense outlier communities. Third, it removes high-dependency patches through targeted filtering based on dependency ratios, creating a refined base set. The framework extends to 3D medical imaging through volumetric tokenization of MRI data and bridges batch-based and text-based methods by using CoDeGraph-derived pseudo-masks to supervise prompt-driven vision-language models.

## Key Results
- CoDeGraph outperforms state-of-the-art methods on both industrial and medical datasets
- Significant improvements in segmentation accuracy and robustness (14–18% F1 improvement on MVTec-CA consistent anomalies)
- Successfully extends to 3D medical imaging with training-free volumetric tokenization
- Demonstrates that pseudo-masks from batch-based models can effectively supervise text-based vision-language models

## Why This Works (Mechanism)

### Mechanism 1: Similarity Scaling Phenomenon Enables Normality Baseline
- Claim: Normal patch tokens exhibit predictable power-law decay in their mutual similarity growth rates, providing a statistical baseline for detecting deviations.
- Mechanism: For a normal element z, the log growth rate τᵢ = ln(d(z)⁽ⁱ⁺¹⁾/d(z)⁽ⁱ⁾) follows τᵢ ∝ i⁻ᵅ where α > 0. This emerges from regularly varying similarity tails (P(S > s) = s⁻ᵅL(s)) combined with Extreme Value Theory under Leadbetter's D(uₙ) condition for long-range independence.
- Core assumption: Normal patches lie on locally low-dimensional manifolds where small-ball mass scales as μ(B(z,r)) ∝ rᵈ, inducing heavy-tailed similarity distributions.
- Evidence anchors:
  - [section 2.2.2] "Empirically, these log growth rates display an intriguing behavior... τᵢ decays with rank i according to a power law: τᵢ ∝ i⁻ᵅ"
  - [section 4.2] Derives E[τᵢ] = 1/(α₀i) under regular variation with extremal index θ
  - [corpus] Related work on zero-shot detection (ViP²-CLIP, MetaUAS) assumes similar distributional structure but does not formalize scaling laws
- Break condition: Fails when normal patches are too diverse to form coherent similarity neighborhoods (e.g., multi-modal objects with ω ≥ N/C where C is number of variants)

### Mechanism 2: Neighbor-Burnout Reveals Consistent Anomalies Locally
- Claim: Consistent anomalies produce detectable spikes in growth rates at their burnout point H, where they exhaust their pool of similar anomalous neighbors.
- Mechanism: An H-level ε-consistent anomaly zᵃ has H neighbors within semantic radius ε, but the (H+1)-th neighbor jumps beyond ϵ₀. This creates τₕ(zᵃ) = ln(d(zᵃ)⁽ᴴ⁺¹⁾/d(zᵃ)⁽ᴴ⁾) anomalously large. The endurance ratio ζ(z,C⁽ⁱ⁾) = d(z)⁽ⁱ⁾/d(z)⁽ω⁾ quantifies how quickly neighbors burn out relative to reference rank ω.
- Core assumption: Consistent anomalies form minority clusters (Assumption 2.3) so their burnout occurs before normal-patch scaling dominates.
- Evidence anchors:
  - [section 2.2.3] "Consistent anomalies show a clear deviation from the power-law trend: their mutual similarity vectors exhibit a sudden rise in the averaged growth rate τ⁽ⁱ⁾(x) once similar matches are exhausted"
  - [section 3.2.1] Endurance ratio distributions show clear separation between normal and consistent-anomaly patches on cable dataset
  - [corpus] Weak/no direct corpus evidence—neighbor-burnout appears novel to this work
- Break condition: Fails when H approaches ω (anomaly cluster too large) or when consistent anomalies dominate a normal variant (violating Minority assumption)

### Mechanism 3: Graph Community Detection Filters Consistent-Anomaly Influence
- Claim: Suspicious links aggregated into a collection-level graph reveal dense outlier communities that can be removed from the base set to restore unbiased anomaly scoring.
- Mechanism: Stage 1 identifies suspicious links Sₗ where ζ′(z,Cⱼ) ≤ λ (weighted endurance ratio). Stage 2 builds anomaly similarity graph G with edge weights wᵢⱼ counting cross-links. Stage 3 applies Leiden algorithm with Constant Potts Model (CPM) to detect communities, flags outlier communities via IQR threshold on density ρ(M), then removes high-dependency patches Pₑₓ where rₘ(z) = a_{B\M}(z)/a_B(z) exceeds empirical 99th percentile.
- Core assumption: Consistent-anomaly communities have substantially higher internal edge density than normal-image communities.
- Evidence anchors:
  - [section 3.3.2] "On MVTec-CA, CoDeGraph removed 6.9% of patches from B while successfully identifying 73.9% of consistent-anomaly patches"
  - [section 3.3.3] IQR-based detection with k_IQR = 4.5 preserves stability; modularity-based detection fragments communities incorrectly
  - [corpus] AnomalyCLIP and APRIL-GAN use text prompts but lack community-based filtering for systematic bias
- Break condition: CPM fragmentation when γ poorly tuned; false positive removal when normal variants form dense clusters exceeding IQR threshold

## Foundational Learning

- **Vision Transformer Patch Tokens**: Understanding how ViT produces both [CLS] tokens (global) and patch tokens zⁱ_ℓ ∈ ℝᴰ (local) at each layer ℓ.
  - Why needed here: CoDeGraph operates on patch-level distances; multi-layer extraction (layers 6,12,18,24) captures both fine-grained and semantic features.
  - Quick check question: Can you explain why patch tokens rather than [CLS] tokens are used for segmentation?

- **Extreme Value Theory and Regular Variation**: Concept of regularly varying tails P(S > s) = s⁻ᵅL(s) and Fréchet-type convergence under D(uₙ) conditions.
  - Why needed here: Provides theoretical justification for similarity scaling law E[τᵢ] = 1/(α₀i).
  - Quick check question: What does the extremal index θ < 1 imply about local dependence in patch similarities?

- **Graph Community Detection (CPM vs Modularity)**: Understanding why CPM with absolute resolution parameter γ works better than modularity for detecting dense anomaly clusters.
  - Why needed here: CPM preserves intact anomaly communities; modularity fragments them via degree-dependent penalties.
  - Quick check question: Why does modularity's expected-edge term k_ik_j/2m penalize weak intra-community edges even in dense clusters?

## Architecture Onboarding

- **Component map**:
  - Feature extraction: ViT backbone → multi-layer patch tokens → ℓ₂ normalization
  - Mutual similarity computation: d(z,Cⱼ) = min_k ||z - z^k_j||² for all patches across B collections
  - Suspicious link detection: ζ′(z,Cⱼ) computation with α=0.2, ω=0.3B, λ adapted via coverage targeting τ=0.95
  - Graph construction: G = (V,E) with weighted edges counting cross-links
  - Community detection: Leiden + CPM (γ = 25th percentile of edge weights) → outlier flagging via IQR (k_IQR=4.5)
  - Targeted filtering: Dependency ratio rₘ(z) → Pₑₓ removal → refined base B_refined
  - Final scoring: Multi-layer, multi-scale MSM on B_refined with LNAMD (receptive fields r∈{1,3,5})

- **Critical path**: Feature extraction → mutual similarity vectors → endurance ratio → suspicious links → anomaly graph → community detection → patch filtering → refined scoring. Breaks if any stage produces degenerate output (empty Sₗ, disconnected G, zero Pₑₓ when anomalies exist).

- **Design tradeoffs**:
  - ω selection: Too small (10%B) lets anomalies match each other; too large (90%B) prevents normal patches from finding valid counterparts
  - Coverage τ: Too low → sparse graph missing communities; too high → excessive normal links diluting anomaly density
  - k_IQR: 1.5–3.0 more sensitive but risks false positives; 4.5 conservative for stability
  - CLS screening η: 60% balances efficiency vs accuracy (<30% degrades AUROC)

- **Failure signatures**:
  - Empty Pₑₓ on known consistent-anomaly dataset → check ω, λ, coverage
  - Over-removal of normal patches (>5% exclusion on MVTec-IA) → k_IQR too low or multi-modal normals
  - Fragmented communities (multiple small clusters for single anomaly type) → switch from modularity to CPM
  - Low segmentation F1 despite high classification AUROC → check LNAMD receptive fields, layer aggregation

- **First 3 experiments**:
  1. **Baseline validation**: Run MuSc 10% on MVTec-CA (cable, metal nut, pill) to confirm consistent-anomaly problem (low F1-seg on flipped nuts, missing cables). Then run CoDeGraph with default hyperparameters to verify 14–18% F1 improvement.
  2. **Ablation on ω and λ**: Sweep ω ∈ {0.1B, 0.3B, 0.5B, 0.7B, 0.9B} on metal nut to observe coverage and capture rate; confirm ω=0.3B–0.5B sweet spot. Validate coverage-based λ selection produces cov(G) ≈ 0.95.
  3. **Backbone comparison**: Swap ViT-L/14@336 for DINOv2-L/14 on full MVTec AD. Expect DINOv2 to yield higher segmentation F1 (~69% vs ~67%) per Table 3.4, confirming backbone-agnostic design with self-supervised features slightly superior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hierarchical community detection or pre-clustering strategies successfully separate variant-level from anomaly-level structures when multi-modal normal objects are present?
- Basis in paper: [explicit] The paper states in Section 3.3.4: "Future extensions could integrate pre-clustering of normal variants or hierarchical community detection to separate variant-level and anomaly-level structures" to address the unsolvable problem where consistent anomalies may outnumber specific normal variants.
- Why unresolved: Current CPM-based community detection conflates normal variant clusters with consistent-anomaly clusters when variants are less frequent, causing misclassification. No solution was proposed in the current framework.
- What evidence would resolve it: Evaluation on datasets with controlled multi-modal normals (e.g., MVTec LOCO juice bottle with >3 variants) showing improved separation between variant communities and anomaly communities via hierarchical detection methods.

### Open Question 2
- Question: Can pseudo-masks generated by batch-based models fully substitute ground-truth supervision for training text-based models across diverse domains?
- Basis in paper: [explicit] Chapter 6 explicitly poses: "Can high-quality pseudo-masks, when combined with a robust text-based model, approach the performance achieved using true pixel-level supervision?" The chapter acknowledges this as a "proof-of-concept" requiring more comprehensive study.
- Why unresolved: APRIL-GAN showed significant performance degradation with pseudo-masks on medical data (∆ −12.3 F1 on MVTec 50%), while AnomalyCLIP showed robustness—suggesting the effectiveness depends on model architecture. Limited architectures and datasets were tested.
- What evidence would resolve it: Systematic evaluation across additional text-based architectures (beyond APRIL-GAN and AnomalyCLIP) and alternative pseudo-mask generation schemes, with performance approaching ground-truth baselines within a defined tolerance margin.

### Open Question 3
- Question: How can the 3D volumetric extension maintain fine-grained detection accuracy for small anomalies while remaining computationally tractable?
- Basis in paper: [explicit] Section 5.4 states: "Future research will focus on optimizing the computational pipeline to handle higher volumetric resolutions—potentially extending input size to 518³ voxels, analogous to the 518² image resolution used in the 2D setting."
- Why unresolved: Current 3D extension uses 16³ token grids (patch size 14), causing missed detections for tumors occupying fractions of single 3D patches. Higher resolutions (e.g., 518³) would enable finer segmentation but were not implemented due to computational constraints.
- What evidence would resolve it: Demonstration of 3D anomaly segmentation at higher token resolutions (e.g., 32³ or 64³ grids) on BraTS-METS with improved Dice scores for small lesions, while maintaining acceptable inference time (<10 seconds/volume).

### Open Question 4
- Question: Does the Similarity Scaling Phenomenon persist in datasets with highly diverse normal patterns that violate local manifold regularity assumptions?
- Basis in paper: [explicit] Section 4.3.3 notes: "For a more general dataset with highly diverse normal patterns... the finite dataset may never reach this regime, and the empirical Similarity Scaling Phenomenon may not appear clearly."
- Why unresolved: Theoretical analysis assumes well-behaved distributions on low-dimensional manifolds. General datasets may lack sufficient redundant normal structures to exhibit observable power-law scaling, but this was not empirically validated.
- What evidence would resolve it: Controlled experiments on datasets with systematically varied normal pattern diversity, showing whether Hill plot plateaus and power-law decay in τᵢ emerge only beyond a threshold of structural redundancy.

## Limitations

- The theoretical foundation for similarity scaling relies on regular variation assumptions that may not hold for all natural image datasets
- The neighbor-burnout mechanism assumes consistent anomalies form minority clusters, failing when systematic defects dominate normal variants
- The graph filtering approach breaks down when normal objects exhibit significant intra-class variation or when multiple consistent anomalies co-occur
- The CPM resolution parameter γ selection (25th percentile of edge weights) is heuristic and may require tuning for different anomaly-to-normal ratios

## Confidence

- **High confidence**: The existence of consistent anomalies and their systematic bias on distance-based zero-shot methods (validated across multiple industrial and medical datasets)
- **Medium confidence**: The power-law similarity scaling mechanism (empirically observed but theoretically dependent on unverified regular variation conditions)
- **Medium confidence**: The neighbor-burnout phenomenon as a local detector (clear separation in endurance ratios but novel detection mechanism lacking independent validation)
- **Medium confidence**: The multi-stage graph filtering pipeline (empirically effective but hyperparameter-dependent)

## Next Checks

1. **Distributional Validation**: Verify regular variation assumptions by testing P(S > s) ∝ s⁻ᵅL(s) on patch similarity distributions across different backbone features (DINOv2, MAE, SimCLRv2). Use Hill plots and QQ-plots against Fréchet distribution to confirm theoretical foundations.

2. **Robustness to Anomaly Prevalence**: Systematically vary consistent-anomaly prevalence from 5% to 50% on synthetic datasets and measure CoDeGraph's performance degradation. This will quantify the minority cluster assumption's practical limits and identify when alternative approaches are needed.

3. **Cross-Domain Transferability**: Apply CoDeGraph to non-industrial domains (satellite imagery, histopathology slides, underwater robotics) with known systematic defects. Compare performance against domain-specific adaptations of ViP²-CLIP and MetaUAS to assess generalizability beyond MVTec and medical imaging.