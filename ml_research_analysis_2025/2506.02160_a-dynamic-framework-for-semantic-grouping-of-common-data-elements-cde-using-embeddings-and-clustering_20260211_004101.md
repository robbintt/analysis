---
ver: rpa2
title: A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using
  Embeddings and Clustering
arxiv_id: '2506.02160'
source_url: https://arxiv.org/abs/2506.02160
tags:
- data
- cdes
- clustering
- clusters
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a dynamic framework to harmonize Common Data
  Elements (CDEs) across heterogeneous biomedical datasets. Using LLM-based embeddings
  and HDBSCAN clustering, the framework grouped semantically similar CDEs from the
  NIH NLM CDE Repository (24,363 CDEs) into 118 clusters at an optimized minimum cluster
  size of 20.
---

# A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering

## Quick Facts
- arXiv ID: 2506.02160
- Source URL: https://arxiv.org/abs/2506.02160
- Reference count: 40
- Primary result: Framework achieved 90.46% accuracy in classifying new CDEs to semantic clusters

## Executive Summary
This study introduces a dynamic framework to harmonize Common Data Elements (CDEs) across heterogeneous biomedical datasets. Using LLM-based embeddings and HDBSCAN clustering, the framework grouped semantically similar CDEs from the NIH NLM CDE Repository (24,363 CDEs) into 118 clusters at an optimized minimum cluster size of 20. The framework assigned automated labels and trained a classifier achieving 90.46% overall accuracy. External validation against Gravity Projects' Social Determinants of Health domains yielded strong alignment (ARI=0.52, NMI=0.78). The scalable approach improves CDE selection efficiency and supports data interoperability, with future work focused on addressing data imbalance and enhancing performance for underrepresented categories.

## Method Summary
The framework ingests CDEs from the NIH NLM CDE Repository, extracts key fields (tinyId, designation, definition, permissible values), and concatenates them for embedding generation using OpenAI's text-embedding-3-small model. HDBSCAN clustering with cosine distance identifies semantic groups, with cluster labels generated by GPT-3.5-turbo. A Random Forest classifier trained on embeddings enables new CDE assignment to existing clusters.

## Key Results
- 24,363 CDEs grouped into 118 clusters at optimized min_cluster_size=20
- 73.8% outlier rate suggests significant heterogeneity in the dataset
- Random Forest classifier achieved 90.46% overall accuracy, with F1=0.00 on underrepresented categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-based embeddings capture semantic similarity across CDEs despite terminological variation.
- **Mechanism:** Pretrained text-embedding-3-small model transforms concatenated designation+definition+permissible values into 1,536-dimensional vectors where cosine distance correlates with semantic distance. Semantically similar CDEs map closer in vector space.
- **Core assumption:** The embedding model's pretraining on general text transfers to biomedical CDE semantics without domain-specific fine-tuning.
- **Evidence anchors:** [abstract] "context-aware text embeddings that convert CDEs into dense vectors capturing semantic relationships"; [section: Methods] "semantically similar concepts are closer in the embedding space"; [corpus] CDE-Mapper (arXiv:2505.04365) demonstrates related embedding-based CDE linking but requires retrieval augmentation.

### Mechanism 2
- **Claim:** HDBSCAN clustering identifies meaningful semantic groups without predefined category counts.
- **Mechanism:** HDBSCAN uses hierarchical density-based clustering with cosine distance, automatically determining cluster count based on `min_cluster_size`. At size 20, produced 118 clusters with 17,973 outliers (73.8%). Internal validation (Silhouette=0.306, Davies-Bouldin=1.37) confirmed cluster quality.
- **Core assumption:** Semantic similarity in CDEs manifests as dense regions in embedding space separable by density thresholds.
- **Evidence anchors:** [abstract] "grouped semantically similar CDEs...into 118 clusters at an optimized minimum cluster size of 20"; [section: Clustering] Table 3 shows systematic evaluation of min_cluster_size 5-500; [corpus] No direct corpus comparison for HDBSCAN on CDE data.

### Mechanism 3
- **Claim:** LLM-generated cluster labels enable supervised classification for new CDE assignment.
- **Mechanism:** GPT-3.5-turbo processes up to 20 representative CDEs per cluster to generate descriptive labels. Random Forest classifier (100 estimators) trained on embeddings achieves 90.46% accuracy, performing well on large clusters but failing on sparse ones.
- **Core assumption:** LLM-generated labels accurately reflect cluster semantics; classifier can learn cluster boundaries from embedding features alone.
- **Evidence anchors:** [abstract] "classifier achieving 90.46% overall accuracy"; [section: Classification Evaluation] "performed exceptionally well in categories with larger sample sizes...struggled with classes that had fewer samples"; [corpus] CDE-Mapper uses RAG for vocabulary linking rather than direct classification.

## Foundational Learning

- **Concept: Dense vector embeddings**
  - **Why needed here:** Core representation enabling semantic similarity computation. Without understanding that embeddings encode meaning as geometric proximity, the clustering logic is opaque.
  - **Quick check question:** If two CDEs have cosine similarity 0.95, what does that imply about their semantic relationship?

- **Concept: Density-based clustering (HDBSCAN specifics)**
  - **Why needed here:** Explains why cluster count emerges from data rather than being specified, and why outliers are inherent to the approach.
  - **Quick check question:** How does increasing `min_cluster_size` from 10 to 50 affect the number of clusters and outliers produced?

- **Concept: Classification with imbalanced data**
  - **Why needed here:** Framework achieves 90% accuracy but fails on small clusters; understanding class imbalance explains this performance gap.
  - **Quick check question:** A classifier achieves 95% accuracy but F1=0 on minority classes. Is accuracy alone an adequate metric here?

## Architecture Onboarding

- **Component map:** NIH NLM CDE Repository (JSON) → pandas DataFrame → OpenAI text-embedding-3-small → HDBSCAN clustering → GPT-3.5-turbo labeling → Random Forest classifier

- **Critical path:** Embedding quality → clustering parameter tuning (`min_cluster_size`) → label accuracy → classifier training. Errors propagate; poor embeddings cannot be recovered downstream.

- **Design tradeoffs:**
  - **Granularity vs. coverage:** Lower `min_cluster_size` yields more clusters but more outliers; size 20 chosen as balance point.
  - **Label richness vs. token limits:** 20 CDEs per cluster for labeling may miss diversity in large clusters.
  - **Proprietary vs. open models:** OpenAI embeddings require API dependency; switching models requires re-embedding entire corpus.

- **Failure signatures:**
  - **Excessive outliers (>70%):** Indicates `min_cluster_size` too high or embedding space lacks structure for this data.
  - **Classifier F1=0 on specific clusters:** Sample size insufficient; consider oversampling or cluster merging.
  - **Generic/vague cluster labels:** GPT-3.5 input insufficient; sample more representative CDEs or use larger-context model.

- **First 3 experiments:**
  1. **Parameter sweep on held-out domain:** Test `min_cluster_size` values [10, 15, 20, 25, 30] on a different CDE repository (e.g., PhenX) to validate generalization of optimal settings.
  2. **Classifier comparison:** Train XGBoost and SVM alongside Random Forest; compare F1-scores on small-cluster classes to identify better handling of imbalance.
  3. **Outlier analysis:** Manually review 100 random outliers to determine if they represent (a) truly unique CDEs, (b) subclusters needing lower `min_cluster_size`, or (c) embedding failures requiring domain-specific preprocessing.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the 73.8% of data points labeled as outliers in the NIH NLM dataset represent truly unique elements or meaningful subclusters?
  - **Basis in paper:** [explicit] The conclusion states, "Future work should investigate these outliers to determine whether they represent truly unique elements or potential subclusters that could be meaningfully grouped with refined techniques."
  - **Why unresolved:** The current analysis classified these points as noise due to the optimized minimum cluster size of 20, but the high percentage suggests the density parameters may have been too strict for the data's heterogeneity.
  - **What evidence would resolve it:** A qualitative review of outlier content or applying hierarchical clustering with lower density thresholds to see if coherent smaller groups emerge.

- **Open Question 2:** To what extent do oversampling or class-weight adjustments improve classification performance for underrepresented CDE clusters?
  - **Basis in paper:** [explicit] The conclusion notes that "future work should focus on addressing data imbalance through techniques such as oversampling or class-weight adjustments," as the model struggled with classes having fewer samples.
  - **Why unresolved:** The current Random Forest classifier showed 0.00 precision and recall for sparse categories because the methodology did not employ specific imbalance mitigation strategies.
  - **What evidence would resolve it:** A comparative study showing F1-score improvements in sparse categories after applying SMOTE or weighted loss functions.

- **Open Question 3:** Can alternative classifiers like XGBoost or Neural Networks outperform Random Forest in high-dimensional CDE embedding classification?
  - **Basis in paper:** [explicit] The authors state, "exploring alternative classifiers like Support Vector Machines, XGBoost, and Neural Networks may further enhance performance under different conditions."
  - **Why unresolved:** The study exclusively utilized a Random Forest classifier; other architectures were mentioned as potential improvements but not tested.
  - **What evidence would resolve it:** Benchmarking the accuracy and computational efficiency of XGBoost or Neural Networks against the established 90.46% accuracy baseline.

## Limitations

- High outlier rate (73.8%) suggests many CDEs lack sufficient semantic neighbors at the chosen threshold
- Classifier fails completely on underrepresented categories (F1=0.00) due to severe class imbalance
- Framework relies on proprietary OpenAI APIs for embeddings and labeling, creating dependency constraints

## Confidence

- **High confidence:** Core embedding-clustering approach validated by internal metrics and 90.46% classification accuracy
- **Medium confidence:** External validation shows moderate alignment (ARI=0.52, NMI=0.78) with SDOH domains
- **Low confidence:** Generalization to datasets with different characteristics or requiring real-time CDE assignment

## Next Checks

1. **Parameter Generalization Test:** Apply the framework to PhenX Toolkit CDEs with varying `min_cluster_size` [10, 15, 20, 25, 30] to determine if size 20 remains optimal across repositories.

2. **Label Quality Assessment:** Manually evaluate 50 cluster labels generated by GPT-3.5-turbo for accuracy and representativeness, particularly for large clusters with >100 members.

3. **Classifier Imbalance Analysis:** Train classifiers using SMOTE oversampling and class-weighted Random Forest on the same dataset to quantify performance improvements on small-cluster categories (F1=0.00).