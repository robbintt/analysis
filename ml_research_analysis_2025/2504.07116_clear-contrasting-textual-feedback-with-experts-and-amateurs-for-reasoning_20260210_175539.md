---
ver: rpa2
title: 'CLEAR: Contrasting Textual Feedback with Experts and Amateurs for Reasoning'
arxiv_id: '2504.07116'
source_url: https://arxiv.org/abs/2504.07116
tags:
- feedback
- clear
- expert
- arxiv
- amateur
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLEAR is a novel method that improves language model reasoning
  by contrasting feedback from larger (expert) and smaller (amateur) models. The expert
  and amateur models each provide feedback on a model's initial output, which is then
  contrasted to create refined feedback that is applied iteratively to improve responses.
---

# CLEAR: Contrasting Textual Feedback with Experts and Amateurs for Reasoning

## Quick Facts
- arXiv ID: 2504.07116
- Source URL: https://arxiv.org/abs/2504.07116
- Authors: Andrew Rufail; Daniel Kim; Sean O'Brien; Kevin Zhu
- Reference count: 40
- Primary result: Contrasting expert and amateur model feedback improves reasoning across four diverse tasks while maintaining computational efficiency

## Executive Summary
CLEAR is a novel method that improves language model reasoning by contrasting feedback from larger (expert) and smaller (amateur) models. The expert and amateur models each provide feedback on a model's initial output, which is then contrasted to create refined feedback that is applied iteratively to improve responses. CLEAR was tested on four challenging reasoning tasks: story outline improvement (73.1% interestingness), constrained generation (99.3% coverage), mathematical reasoning (96.8% accuracy on GSM8K), and toxicity mitigation (decreased toxicity by up to 22%). The method outperforms state-of-the-art approaches while maintaining computational efficiency, requiring fewer API calls than competing methods.

## Method Summary
CLEAR improves LLM reasoning through iterative refinement using contrastive feedback. The method uses two models of different sizes (expert and amateur) to evaluate outputs, then contrasts their feedback through a feedback filter to create refined guidance. This filtered feedback is applied iteratively to generate improved responses. A variant called BeCLEAR employs best-first search with hybrid scoring functions for objective reasoning tasks. The approach was evaluated on four diverse reasoning tasks: story outline improvement, constrained generation, mathematical reasoning, and toxicity mitigation.

## Key Results
- GSM8K mathematical reasoning accuracy: 96.8% (d=3) vs. 86.0% for CoT
- Story outline interestingness: 73.1% vs. 62.8% for ReACT
- Constrained generation coverage: 99.3% vs. 97.2% for F1
- Toxicity mitigation: Decreased toxicity by up to 22% vs. 15.7% for ReAct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrasting feedback from models of different sizes produces higher-quality refinement signals than single-source feedback.
- Mechanism: The Feedback Filter module synthesizes expert (larger LM) and amateur (smaller LM) textual feedback through a contrastive prompt, prioritizing expert input while surfacing differences. This creates `f_filtered(x_i)` that highlights what the expert catches that the amateur misses—and vice versa.
- Core assumption: Expert and amateur models produce semantically divergent feedback; their differences contain signal rather than just noise.
- Evidence anchors:
  - [abstract] "The expert and amateur models each provide feedback on a model's initial output and are contrasted with each other into refined feedback."
  - [section 2.1, Figure 2] Cosine similarity analysis shows expert/amateur feedback have meaningfully different embeddings (~0.6-0.7 similarity), and filtered feedback differs from both; using two identical models (red bars) yields near-identical feedback, degrading performance.
  - [corpus] Related work "Multi-Amateur Contrastive Decoding" supports the premise that expert-amateur divergence improves generation quality, though through decoding-time logit manipulation rather than textual feedback.

### Mechanism 2
- Claim: Iterative refinement with accumulated feedback history produces incremental quality gains across diverse task types.
- Mechanism: Each node stores its feedback history. Child generation conditions on instruction `I`, parent node `x_parent`, and filtered feedback `f_filtered(x_parent)` via: `x_child ~ p_θ(x | I, x_parent, f_filtered(x_parent))`. This creates a linear chain (or tree with search) where improvements compound.
- Core assumption: Feedback correctly identifies deficiencies; the generator model can act on feedback without introducing new errors.
- Evidence anchors:
  - [abstract] "This feedback is subsequently applied to iteratively improve CLEAR's responses."
  - [section 3, Tables 1-5] Performance improves with iteration depth `d` (d=1→2→3→4→5 all show gains).
  - [corpus] "LightReasoner" demonstrates small models can teach reasoning to larger models, supporting the premise that weaker models still contribute useful signal.

### Mechanism 3
- Claim: Best-first search with hybrid scoring functions improves performance on objective-reasoning tasks by prioritizing promising refinement paths.
- Mechanism: BeCLEAR uses cost function `g(n) = |v₀ - v_expert| + |v₀ - v_amateur|` and heuristic `h(n) = 100 - |v_expert - v_amateur|` to rank nodes. The node with lowest `f(n) = g(n) + h(n)` is expanded next, focusing computation on trajectories where both models agree improvement is occurring.
- Core assumption: Numerical scores from expert/amateur correlate with actual solution quality; expert-amateur score agreement indicates reliability.
- Evidence anchors:
  - [section 2.2] "BeCLEAR aims to go from the initial output to the ideal response which is assumed to receive a score of 100."
  - [section 3.3, Table 3] BeCLEAR (d=4) achieves 97.2% on GSM8K vs. CLEAR (d=3) at 96.8%; both outperform CoT (86.0%).
  - [corpus] Weak corpus evidence—no directly comparable search-based contrastive feedback methods found. Related work "SteerMoE" addresses expert routing but in MoE architecture, not search.

## Foundational Learning

- Concept: **Contrastive decoding/feedback paradigms**
  - Why needed here: CLEAR builds on prior contrastive methods that exploit expert-amateur probability differences, but shifts from token-level logit manipulation to textual feedback synthesis.
  - Quick check question: Can you explain why contrasting two models' outputs provides more signal than using either alone, and when it might fail?

- Concept: **Tree search algorithms (best-first, A*, MCTS)**
  - Why needed here: BeCLEAR requires understanding how heuristic search explores solution spaces differently from linear iteration.
  - Quick check question: What is the difference between depth-first search and best-first search in terms of node expansion order, and when would each be preferable?

- Concept: **LLM self-refinement and feedback loops**
  - Why needed here: CLEAR is part of the broader family of methods (Self-Refine, Reflexion, Thought Sculpt) where models critique and improve their outputs.
  - Quick check question: What are two failure modes of iterative self-refinement, and how might contrasting multiple feedback sources mitigate one of them?

## Architecture Onboarding

- Component map: Input Instruction (I) + Initial Output (x₀) → Node Evaluator (2 parallel calls) → Feedback Filter (1 LLM call) → Child Generator → [Iterate for d steps, or use BeCLEAR search]

- Critical path:
  1. **Prompt design for Node Evaluator** (Appendix A shows task-specific templates)—this determines feedback quality.
  2. **Feedback Filter prompt**—must balance expert priority with amateur signal extraction.
  3. **Model selection**—expert/amateur capability gap must be meaningful but not extreme; paper uses GPT-4o/GPT-3.5-turbo or LLaMA3-70B/8B.

- Design tradeoffs:
  - **Larger capability gap** → more divergent feedback but risk of amateur being unhelpful
  - **More iterations (higher d)** → better results but linear cost increase
  - **Best-first search vs. linear** → search helps objective tasks but requires numerical scoring which is noisy for subjective tasks

- Failure signatures:
  - Feedback lacks actionable specifics (vague "improve clarity" without concrete guidance)
  - Expert and amateur feedback converge (identical models or task too easy)
  - Child node degrades from parent (generator ignores or misinterprets feedback)
  - Search cycles on low-quality nodes (poor heuristic calibration)

- First 3 experiments:
  1. **Validate feedback divergence**: Run Node Evaluator on 20 samples from your target task; compute embedding similarity between expert/amateur feedback. If >0.85, reconsider model pairing.
  2. **Ablate filter prompt**: Compare filtered feedback vs. expert-only vs. amateur-only on a held-out validation set (use GPT-4o as judge for subjective tasks).
  3. **Pareto test on iteration depth**: Run CLEAR with d∈{1,2,3,4,5} on 100 samples; plot performance vs. API cost to identify optimal depth for your task and budget.

## Open Questions the Paper Calls Out

- **Optimal model pairing configurations**: The conclusion states, "We hope that this research encourages further exploration... especially finding optimal configurations for the choice of the expert and amateur model pairing." The study primarily tested GPT-4o/GPT-3.5-turbo and LLaMA3-70B/LLaMA3-8B, leaving the impact of varying size differentials and model families unexplored.

- **Alternative search algorithms**: The Methods section notes, "Other search algorithms such as A*, DFS, and BFS can be alternatives; however, this paper does not test them." The research limited its evaluation of search strategies to Best-First Search for objective tasks, without benchmarking against other standard pathfinding algorithms.

- **Robustness to incorrect feedback**: The Limitations section admits, "if both the expert and amateur feedback are incorrect, CLEAR's performance would be worse," but does not quantify this failure mode. The paper assumes contrasting feedback improves quality, but lacks analysis of scenarios where the "refined" feedback simply amplifies shared errors.

## Limitations

- The method requires three LLM calls per iteration (expert feedback, amateur feedback, filtered synthesis), creating substantial computational overhead that scales linearly with iteration depth.
- The contrastive feedback mechanism depends critically on meaningful divergence between expert and amateur models, but the paper doesn't specify how to select optimal model pairs for new tasks.
- Effectiveness could degrade significantly if the capability gap is too small (feedback convergence) or too large (amateur provides no useful signal).

## Confidence

- **High confidence** in the mechanism of contrastive feedback synthesis from expert and amateur models producing higher-quality refinement signals than single-source feedback. This is well-supported by cosine similarity analysis (0.6-0.7 divergence) and the ablation showing identical models yield degraded performance.
- **Medium confidence** in the iterative refinement mechanism producing consistent quality gains. While tables show performance improvement with d=1→5, the compounding effect assumes feedback quality remains stable across iterations, which isn't empirically validated.
- **Medium confidence** in best-first search improving objective reasoning tasks. The search heuristic assumes numerical score agreement between models indicates reliability, but this correlation isn't validated against ground truth for subjective tasks.

## Next Checks

1. **Feedback divergence validation**: Run Node Evaluator on 20 samples from your target task; compute embedding similarity between expert/amateur feedback. If >0.85, reconsider model pairing—this directly tests the foundational assumption that contrastive feedback works through semantic divergence.

2. **Filter ablation test**: Compare filtered feedback vs. expert-only vs. amateur-only on a held-out validation set (use GPT-4o as judge for subjective tasks). This validates whether the contrastive synthesis actually improves over either input alone.

3. **Iteration depth Pareto analysis**: Run CLEAR with d∈{1,2,3,4,5} on 100 samples; plot performance vs. API cost to identify optimal depth for your task and budget. This reveals the practical trade-off between quality gains and computational expense.