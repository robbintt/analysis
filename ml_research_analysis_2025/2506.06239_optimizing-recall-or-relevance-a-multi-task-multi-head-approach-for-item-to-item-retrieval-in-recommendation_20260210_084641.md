---
ver: rpa2
title: Optimizing Recall or Relevance? A Multi-Task Multi-Head Approach for Item-to-Item
  Retrieval in Recommendation
arxiv_id: '2506.06239'
source_url: https://arxiv.org/abs/2506.06239
tags:
- relevance
- semantic
- recall
- mtmh
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental trade-off between recall and
  semantic relevance in item-to-item (I2I) retrieval for recommendation systems. Existing
  I2I models optimized for recall using co-engagement data often fail to capture semantic
  relevance, leading to overfitting short-term patterns and missing opportunities
  for novel interest discovery and content diversity.
---

# Optimizing Recall or Relevance? A Multi-Task Multi-Head Approach for Item-to-Item Retrieval in Recommendation

## Quick Facts
- arXiv ID: 2506.06239
- Source URL: https://arxiv.org/abs/2506.06239
- Reference count: 40
- Primary result: MTMH improves recall by up to 14.4% and semantic relevance by up to 56.6% compared to state-of-the-art baselines

## Executive Summary
This paper addresses the fundamental trade-off between recall and semantic relevance in item-to-item (I2I) retrieval for recommendation systems. Existing I2I models optimized for recall using co-engagement data often fail to capture semantic relevance, leading to overfitting short-term patterns and missing opportunities for novel interest discovery and content diversity. The proposed MTMH (Multi-Task Multi-Head) model jointly optimizes both recall and semantic relevance through two key components: a multi-task learning loss that combines co-engagement loss with a semantic relevance loss based on knowledge distillation from a pre-trained content encoder, and a multi-head architecture with separate engagement and relevance heads. The model merges candidates from both heads during serving to achieve high recall while maintaining semantic relevance.

Evaluated on proprietary data from a commercial platform serving billions of users, MTMH improves recall by up to 14.4% and semantic relevance by up to 56.6% compared to state-of-the-art baselines. Online A/B testing confirms these improvements translate to better real-world performance, with increases in daily active users (0.05%), daily time spent (0.22%), distinct item views (0.31%), fresh content delivery (0.25%), novel interest discovery rate (0.33%), and user interest recall (0.14%).

## Method Summary
The MTMH model uses a two-tower architecture with separate SparseNN (ID embeddings) and DenseNN (float features) towers feeding into shared bottom layers. Two MLP heads are trained on top: an engagement head that minimizes co-engagement loss (InfoNCE) and a relevance head that minimizes a multi-task loss combining co-engagement loss with a semantic relevance loss. The semantic relevance loss uses knowledge distillation from a pre-trained content encoder (VIT-H14-632M for visual + XLM-R-Large-550M for text) via KL divergence between similarity distributions. At serving, candidates from both heads are retrieved in parallel using separate ANN indices, preranked independently, and merged using a quota-based approach with parameter α.

## Key Results
- MTMH achieves up to 14.4% improvement in recall@500 compared to state-of-the-art baselines
- Semantic relevance improves by up to 56.6% as measured by L1/L2 topic match rate
- Online A/B testing shows improvements in daily active users (0.05%), daily time spent (0.22%), distinct item views (0.31%), fresh content delivery (0.25%), novel interest discovery rate (0.33%), and user interest recall (0.14%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of recall and semantic relevance via multi-task learning distills semantic knowledge into learned embeddings without sacrificing co-engagement signal.
- Mechanism: The relevance loss (Eq. 2) uses KL divergence to align the relative similarity distribution of learned item embeddings with content encoder embeddings. This transfers semantic knowledge while the co-engagement loss (Eq. 1, InfoNCE) preserves behavioral signal. Weight `w_r` controls the trade-off.
- Core assumption: Semantic similarity in content encoder space correlates with user-perceived relevance, even for items without co-engagement history.
- Evidence anchors:
  - [abstract]: "multi-task learning loss that combines co-engagement loss with a semantic relevance loss based on knowledge distillation from a pre-trained content encoder"
  - [Section 3.2]: "By minimizing the KL divergence between Q and P, the relative similarities between learned item embeddings are aligned with those of the item content embeddings"
  - [corpus]: Related work (DAS, MMHCL) explores multi-modal integration but via different mechanisms; not directly validating this distillation approach.
- Break condition: If the pre-trained content encoder itself has poor semantic representations for your item domain, distillation transfers noise rather than signal.

### Mechanism 2
- Claim: Multi-head architecture with separate engagement and relevance heads enables retrieval of complementary candidate sets that would otherwise be mutually exclusive.
- Mechanism: Head 1 (engagement) minimizes only L_e, capturing high co-engagement items including popular but semantically weak matches. Head 2 (relevance) minimizes L_mt, prioritizing semantic relevance. At serving, quota-based merging (parameter α) combines top-K from each head.
- Core assumption: The optimal candidate set for engagement and the optimal candidate set for semantic relevance have limited overlap; retrieving both expands total value.
- Evidence anchors:
  - [abstract]: "multi-head architecture with separate engagement and relevance heads. The model merges candidates from both heads during serving"
  - [Section 3.3]: "the engagement head is trained to minimize co-engagement loss only... the relevance head is trained to minimize the multi-task loss"
  - [Section 4.3, Table 2]: MTMH-H1 achieves 16.16% recall@500, MTMH-H2 achieves 11.37%; merged MTMH achieves 17.02%
  - [corpus]: No direct corpus validation of multi-head serving strategy for I2I retrieval.
- Break condition: If α is misconfigured for your use case (e.g., too much relevance in a popularity-driven product), engagement may drop despite improved semantic metrics.

### Mechanism 3
- Claim: Separating head-specific ANN indices with per-head preranking preserves semantic candidates that would be filtered by a unified preranker biased toward engagement.
- Mechanism: Multi-ANN retrieves K_ann candidates per head in parallel. Per-head preranking selects top K from each head independently, ensuring semantic candidates from the relevance head survive even if a global preranker would deprioritize them.
- Core assumption: A single engagement-optimized preranker would systematically suppress semantically relevant but less co-engaged items.
- Evidence anchors:
  - [Section 3.5]: "Since preranker is trained to maximize the likelihood of selecting items engaged by the user, it may be biased towards highly engaged items... we rank K_ann candidate items retrieved by each head separately"
  - [Section 3.5, Figure 4]: Architecture diagram shows parallel ANN paths with quota-based merging
  - [corpus]: Not addressed in neighbor papers; mechanism is specific to this architecture.
- Break condition: If preranker quality is poor or biased differently than expected, the protection mechanism may not help or could amplify noise from the relevance head.

## Foundational Learning

- Concept: **InfoNCE / Contrastive Loss**
  - Why needed here: The co-engagement loss (L_e) uses InfoNCE to pull positive pairs closer and push negative pairs apart in embedding space. Understanding this helps debug why certain item pairs have low similarity.
  - Quick check question: Can you explain why increasing the number of negative samples L in Eq. 1 would affect the gradient signal for rare vs. popular items?

- Concept: **Knowledge Distillation (KD)**
  - Why needed here: The relevance loss transfers knowledge from a frozen pre-trained content encoder to the I2I model via soft labels. Without understanding KD, you may misinterpret why the student model (I2I) doesn't simply copy the teacher's embeddings.
  - Quick check question: Why does the paper use KL divergence between probability distributions Q and P rather than direct MSE between embeddings?

- Concept: **Two-Tower Retrieval Architecture**
  - Why needed here: The base architecture uses SparseNN + DenseNN towers to produce item embeddings. Multi-head adds separate MLP heads on top of shared towers.
  - Quick check question: What is the computational benefit of sharing the bottom SparseNN/DenseNN layers across heads, and how much parameter increase does adding a head incur?

## Architecture Onboarding

- Component map:
  - **Training**: SparseNN (ID embeddings) + DenseNN (float features) → shared bottom → two MLP heads (engagement head with L_e, relevance head with L_mt) → losses computed against co-engagement labels and content encoder soft labels
  - **Content Encoder**: Pre-trained VIT-H14-632M (visual) + XLM-R-Large-550M (text) → fusion MLP → 128-dim content embeddings (frozen for distillation)
  - **Serving**: Trigger item → both heads produce embeddings → parallel ANN search (K-means clustering-based) → per-head preranker → quota-based merge (α) → final candidates

- Critical path:
  1. Content encoder must be trained/fine-tuned on your domain's multimodal content before MTMH training begins.
  2. Co-engagement data pipeline must construct positive/negative pairs with SIM-weighted relevance for training pairs.
  3. K-means clustering of item embeddings must be run offline for ANN indices per head.
  4. Preranker model (multi-task U2I) must be available for serving-time candidate filtering.

- Design tradeoffs:
  - **w_r (relevance weight)**: Higher values improve semantic relevance but may reduce recall. Paper uses 0.5 as default (Section 4.4).
  - **α (serving quota)**: Controls engagement head percentage in final merge. Tunable at serving time without retraining. Paper finds α=50 optimal for their use case.
  - **K_ann vs K**: More ANN candidates increase recall opportunity but add preranker latency. Paper uses K_ann ~1000, K ~10 per head.

- Failure signatures:
  - **Recall drops after deployment**: α may be too low; increase engagement head quota. Check if preranker is filtering aggressively.
  - **Semantic relevance unchanged**: Content encoder may not generalize to your item domain; evaluate content encoder quality independently.
  - **Fresh content still cold**: Verify that content encoder features are available for new items at inference time.
  - **Training instability with high w_r**: Gradient conflict between L_e and L_r; try gradient surgery or lower w_r.

- First 3 experiments:
  1. **Validate content encoder quality**: Before MTMH, measure topic/category alignment between content encoder embeddings and human-labeled semantic relevance for a sample of item pairs. If correlation is weak, improve content encoder first.
  2. **Ablate single-head vs. multi-head**: Compare MTSH (multi-task single-head), STMH (single-task multi-head), and full MTMH on holdout data. Reproduce Table 2 patterns to validate implementation.
  3. **Sweep α in shadow mode**: Run serving simulations with varying α (0, 30, 50, 70, 100) on historical traffic. Plot recall vs. semantic relevance tradeoff curves (as in Figure 6b) to select deployment value.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the trade-off parameter $\alpha$ (serving quota) be optimized dynamically based on real-time user context or intent, rather than set as a static heuristic?
- **Basis in paper:** [inferred] Section 4.4 states that $\alpha$ is a serving hyperparameter that can be adjusted, noting that $\alpha=50$ provides an optimal trade-off in their specific setting, but different applications might require different values.
- **Why unresolved:** The paper treats $\alpha$ as a static configuration. It does not explore whether specific user states (e.g., exploration vs. exploitation modes) or item types might benefit from an adaptive quota allocation.
- **What evidence would resolve it:** An experiment comparing the static $\alpha$ baseline against a reinforcement learning agent or context-aware gating mechanism that adjusts $\alpha$ per request, demonstrating superior long-term user satisfaction.

### Open Question 2
- **Question:** How does the MTMH architecture generalize to domains with sparse engagement data or distinct semantic structures, such as e-commerce or long-form text?
- **Basis in paper:** [inferred] The paper evaluates the model exclusively on a commercial platform serving billions of users (likely short-video or social media), utilizing multimodal content encoders. It does not validate the approach on public benchmarks with different data distributions.
- **Why unresolved:** The "freshness" and "semantic relevance" dynamics in high-velocity short-video streams may differ significantly from domains with stable taxonomies (e.g., Amazon products) or sparse interactions (e.g., cold-start scenarios).
- **What evidence would resolve it:** Evaluating MTMH performance on standard public datasets like MovieLens or Amazon Reviews to demonstrate if the multi-head multi-task approach retains its advantage over single-head baselines in lower-volume or structured domains.

### Open Question 3
- **Question:** How robust is the knowledge distillation process to noise or bias in the pre-trained content encoder's output?
- **Basis in paper:** [inferred] Section 3.4 mentions the content encoder is trained on user-added hashtags, search queries, and LLM-generated tags. The paper assumes these provide "superior content understanding" but does not analyze the impact of potentially noisy or hallucinated tags on the student model.
- **Why unresolved:** If the teacher model (content encoder) produces irrelevant embeddings for a significant portion of the inventory, the relevance loss ($L_r$) could force the student model to learn poor representations, degrading recall.
- **What evidence would resolve it:** A sensitivity analysis measuring the degradation in semantic relevance and recall when synthetic noise is injected into the teacher embeddings, or when the quality of the distillation source is varied.

## Limitations
- **Scale and Domain Specificity**: Evaluation conducted on proprietary data from a commercial platform serving billions of users, limiting reproducibility and generalizability to other domains or smaller-scale systems.
- **Knowledge Distillation Dependency**: Semantic relevance component relies heavily on quality of pre-trained content encoder, which is not independently validated and may transfer noise if domain-mismatched.
- **Serving Configuration Complexity**: Multi-head architecture introduces additional operational complexity through parallel ANN indices and quota-based merging, with potential latency and infrastructure overhead not explored.

## Confidence
- **High Confidence**: Core architectural contributions (multi-task learning framework, multi-head design, parallel serving strategy) are well-specified with clear quantitative improvements on proprietary data and strong internal validation through ablation studies.
- **Medium Confidence**: Knowledge distillation mechanism is theoretically sound but depends heavily on pre-trained content encoder quality, which lacks independent validation; online A/B testing shows proxy metric improvements but doesn't report core business metrics.
- **Low Confidence**: Claims about improved "novel interest discovery rate" and "user interest recall" lack detailed methodology description, making it difficult to assess whether improvements are genuine or artifacts of evaluation methodology.

## Next Checks
1. **Content Encoder Quality Validation**: Before implementing MTMH, conduct independent evaluation of your pre-trained content encoder's semantic quality by measuring topic/category alignment with human-labeled semantic relevance for a representative sample of item pairs. If correlation is weak (below 0.7), improve the content encoder first rather than proceeding with MTMH.

2. **Domain Transfer Assessment**: If your item domain differs from the paper's, run a controlled experiment comparing MTMH to single-task baselines on a small subset of your data, focusing on whether the relevance head actually improves semantic metrics rather than just recall to assess generalizability.

3. **Operational Cost Analysis**: Measure infrastructure overhead of maintaining separate ANN indices and parallel retrieval paths, comparing latency, memory usage, and indexing costs against current single-index approach while validating that quota-based merging strategy provides sufficient flexibility for your specific product goals.