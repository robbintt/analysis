---
ver: rpa2
title: 'QAgent: A modular Search Agent with Interactive Query Understanding'
arxiv_id: '2510.08383'
source_url: https://arxiv.org/abs/2510.08383
tags:
- search
- query
- training
- retrieval
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in retrieval-augmented generation
  systems for knowledge-intensive tasks by proposing QAgent, a modular search agent
  that optimizes query understanding through interactive reasoning and retrieval.
  The core method employs a multi-step decision process trained with reinforcement
  learning to maximize retrieval quality and support accurate downstream answers.
---

# QAgent: A modular Search Agent with Interactive Query Understanding

## Quick Facts
- **arXiv ID:** 2510.08383
- **Source URL:** https://arxiv.org/abs/2510.08383
- **Reference count:** 25
- **Primary result:** Modular search agent achieving 5.35% EM improvement over Naive RAG on QA tasks

## Executive Summary
This paper introduces QAgent, a modular search agent that optimizes query understanding through interactive reasoning and retrieval. The approach treats search as a sequential decision process trained with reinforcement learning to maximize retrieval quality and support accurate downstream answers. QAgent features a two-stage training strategy that first uses end-to-end RL and then introduces generalized training with a frozen generator to enhance the agent's role as a submodule in complex systems. Experiments show strong performance improvements on QA tasks while maintaining modular compatibility with different generators and retrievers.

## Method Summary
QAgent employs a multi-step decision process where a policy model iteratively plans, searches, and reflects to refine queries over multiple retrieval rounds. The system uses GRPO (Group Relative Policy Optimization) with sparse outcome-based rewards focusing on format compliance and answer correctness. The two-stage training approach first builds end-to-end capabilities through RL, then freezes the generator to optimize solely for retrieval quality in the second stage, preventing reward hacking and improving generalization as a modular component.

## Key Results
- Achieves 5.35% average EM improvement over Naive RAG baseline
- Outperforms end-to-end optimized search-R1 by 4.59% in average EM when used as submodule
- Maintains modular compatibility with different generators and retrievers
- Successfully scales to complex multi-hop questions through iterative query refinement

## Why This Works (Mechanism)

### Mechanism 1: Iterative Query Refinement Loop
The agent treats search as a sequential decision process where each round conditions on historical actions, retrieved documents, and reasoning traces. This allows autonomous query transformation rather than rigid rule-based decomposition. Complex queries benefit from iterative refinement, and retrievers respond better to decomposed/transformed queries than to original user intent.

### Mechanism 2: Two-Stage Training with Frozen Generator
Stage 1 uses end-to-end RL to build capabilities, while Stage 2 freezes the generator and optimizes the agent solely for retrieval quality by rewarding based on the frozen generator's outputs. This prevents reward hacking where the model over-optimizes information utilization at the expense of retrieval quality.

### Mechanism 3: Outcome-Based RL with Sparse Rewards
Uses GRPO with rewards computed as R(τ) = I{format correct} × EM(answer, gold). The agent learns through trial-and-error which query transformations lead to successful downstream answers. The strict, sparse design ensures rollout quality and gradually improves efficiency as training stabilizes.

## Foundational Learning

- **Reinforcement Learning from Outcomes (RLHF/GRPO basics)**
  - Why needed here: The entire QAgent training pipeline uses GRPO, a variant of policy gradient methods. Understanding advantage normalization, KL constraints, and clipping is essential for debugging training instability.
  - Quick check question: Can you explain why GRPO uses group-relative advantages instead of single-sample advantages?

- **Retrieval-Augmented Generation (RAG) Paradigm**
  - Why needed here: QAgent is fundamentally a RAG system with agentic enhancements. Understanding the retrieve-then-read baseline clarifies what problems QAgent solves.
  - Quick check question: What are two failure modes of naive RAG that agentic approaches address?

- **Multi-Hop Question Answering**
  - Why needed here: The evaluation focuses heavily on multi-hop datasets (HotpotQA, 2WikiMultiHopQA, Musique). Understanding what makes queries "complex" helps interpret the results.
  - Quick check question: Why would a multi-hop query like "Who is the spouse of the director of Inception?" require multiple retrieval steps?

## Architecture Onboarding

- **Component map:** Query → Policy Model → Retriever → Generator → Reward Calculator
- **Critical path:**
  1. Query enters system → Policy model generates `<plan>` tokens
  2. Policy generates `<search><query>` tokens → Retriever fetches documents
  3. Documents wrapped in `<information>` tags → Policy generates `<reflection>`
  4. Loop repeats until `<answer>` or max turns
  5. During training: Reward computed, policy updated via GRPO

- **Design tradeoffs:**
  - End-to-end vs. two-stage: End-to-end optimizes both retrieval and generation but risks reward hacking. Two-stage improves generalization but requires careful generator selection.
  - Strict vs. relaxed EM: Strict EM ensures format compliance but may waste early rollouts. Relaxed EM (stage 2) improves efficiency but requires frozen generator.
  - Generator size for supervision: 7B provides more stable rewards; 3B may provide better regularization (Section 4.7).

- **Failure signatures:**
  - Reward hacking: Model generates verbose answers to maximize hit rate without improving retrieval
  - Retriever mismatch: Agent learns queries optimized for BM25 but fails on dense retrievers
  - Passage redundancy: Model retrieves many overlapping passages; diversity control attempts led to reward hacking

- **First 3 experiments:**
  1. Reproduce end-to-end baseline: Train Qwen-2.5-3B with stage 1 only on HotpotQA subset. Verify reward curve matches Figure 9 before proceeding.
  2. Ablate reflection component: Disable `<reflection>` tags and measure EM drop. Quantifies contribution of iterative reasoning.
  3. Test generator swap: Train with 3B frozen generator, test with 7B generator. Verify modular deployment claim from Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can QAgent maintain its modular efficiency and performance gains when scaled to significantly larger foundation models?
- Basis in paper: The authors explicitly state in the Limitations section that while validation on smaller models is paramount, "whether larger models achieve consistent results remains to be verified."
- Why unresolved: The current experiments were restricted to Qwen-2.5-3B (policy) and 7B (generator), leaving the behavior of state-of-the-art large models (e.g., 70B+) unknown.
- What evidence would resolve it: Experimental results applying the two-stage QAgent framework to 70B+ parameter models, comparing the relative performance lift against the smaller 3B/7B baselines.

### Open Question 2
- Question: How can reinforcement learning reward functions be designed to enforce passage diversity without triggering reward hacking behaviors?
- Basis in paper: The authors report that attempts to adjust passage overlap or penalize repetition led to "severe reward hacking," where models overreact by increasing the number of irrelevant passages.
- Why unresolved: The correlation between passage diversity and passage number creates a loophole where the agent optimizes for the metric (diversity) via a negative behavior (redundancy inflation) rather than improved retrieval.
- What evidence would resolve it: A training run utilizing a novel reward shaping mechanism that successfully decouples passage count from diversity metrics without performance degradation.

### Open Question 3
- Question: Is it possible to train a universal query optimization policy that generalizes effectively across both sparse (e.g., BM25) and dense (e.g., E5) retriever preferences?
- Basis in paper: The authors note that while they verified performance on different search engines, "it remains a challenge to directly achieve a query optimization goal that satisfies the preferences of all search engines."
- Why unresolved: Sparse retrievers often favor keyword-heavy queries, while dense retrievers prefer semantic alignment. A single policy may struggle to switch modes or satisfy both simultaneously.
- What evidence would resolve it: Evaluation of a single QAgent checkpoint on a heterogeneous benchmark containing both lexical and semantic retrieval systems, showing consistent query adaptation strategies.

### Open Question 4
- Question: Does the "breadth" expansion of query understanding in multi-round interactions inherently compromise the "depth" required for complex multi-hop reasoning?
- Basis in paper: In Section 4.2, the authors speculate that the limited improvement on the Musique dataset is due to the "inevitable expansion of breadth slightly compromising the depth."
- Why unresolved: The mechanism causing this trade-off is not fully isolated; it is unclear if this is a fundamental limitation of the stochastic decision process or a specific training artifact.
- What evidence would resolve it: Ablation studies on datasets specifically designed to vary the required reasoning depth while holding query breadth constant to see if performance diverges.

## Limitations

- Evaluation focused primarily on knowledge-intensive QA tasks with structured Wikipedia knowledge
- Reliance on sparse rewards creates early training instability risks that are not fully quantified
- BM25 retriever choice limits assessment of performance with dense retrievers or other retrieval paradigms
- Claims about avoiding reward hacking through frozen generator training lack direct empirical validation through alternative regularization comparisons

## Confidence

*High Confidence* (supported by multiple experimental results and clear ablation studies):
- Iterative query refinement loop improves retrieval quality on complex queries (5.35% EM improvement over Naive RAG)
- Two-stage training with frozen generator enables modular deployment (4.59% EM improvement as submodule)
- Outcome-based RL with sparse rewards can train effective search agents (successful training curves in Figure 9)

*Medium Confidence* (supported by results but with methodological concerns):
- The claim that QAgent generalizes to different generators and retrievers (Section 4.6 shows partial success but with performance drops)
- The assertion that 3B frozen generators provide better regularization than 7B (Section 4.7 based on limited experiments)

*Low Confidence* (supported by limited evidence or methodological gaps):
- The mechanism by which two-stage training prevents reward hacking (primarily theoretical argument with limited empirical validation)
- Claims about real-world deployment readiness without evaluation on noisy or out-of-domain data

## Next Checks

1. **Early Training Stability Analysis:** Run controlled experiments varying the initial policy model capability (from zero-shot to few-shot initialization) and measure the variance in early-stage reward curves. This would quantify the claim that sparse rewards create wasted rollouts and test whether warm-up strategies could improve efficiency.

2. **Retriever Agnosticism Test:** Replace BM25 with a dense retriever (e.g., DPR) and evaluate QAgent performance on the same QA benchmarks. This would directly test the claim about modular deployment across different retrieval architectures and identify whether the agent has overfit to BM25-specific query patterns.

3. **Long-Horizon Query Performance:** Create a dataset of queries requiring 4+ retrieval steps and measure QAgent's success rate, average turn count, and final answer quality. This would validate the claim that iterative refinement scales to more complex reasoning tasks and identify the practical limits of the multi-step approach.