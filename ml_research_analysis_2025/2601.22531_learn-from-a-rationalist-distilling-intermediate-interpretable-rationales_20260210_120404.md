---
ver: rpa2
title: 'Learn from A Rationalist: Distilling Intermediate Interpretable Rationales'
arxiv_id: '2601.22531'
source_url: https://arxiv.org/abs/2601.22531
tags:
- rationale
- rekd
- loss
- neural
- cifar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving rationale extraction
  (RE) models based on less capable or smaller neural networks. RE models struggle
  with the "chicken and egg" problem where the generator and predictor networks must
  learn jointly, making training difficult for smaller models.
---

# Learn from A Rationalist: Distilling Intermediate Interpretable Rationales

## Quick Facts
- arXiv ID: 2601.22531
- Source URL: https://arxiv.org/abs/2601.22531
- Reference count: 29
- Key outcome: REKD significantly improves predictive performance of student RE models by learning from teacher rationales and predictions through shared temperature annealing.

## Executive Summary
This paper addresses the "chicken and egg" problem in rationale extraction (RE) models, where generator and predictor networks must learn jointly, making training difficult for smaller models. The authors propose REKD (Rationale Extraction with Knowledge Distillation), a method where a student RE model learns from both the rationales and predictions of a teacher model, in addition to its own RE optimization. The approach uses a shared temperature annealing scheduler between the Gumbel-Softmax-based RE and knowledge distillation to create a progressive learning curriculum. Experiments on language (IMDB) and vision (CIFAR 10/100) datasets demonstrate significant performance improvements for student RE models, breaking the training dilemma through rationalist learning.

## Method Summary
The paper proposes REKD, which addresses the "chicken and egg" problem in RE models by incorporating knowledge distillation from a teacher model. The student RE model learns from both the teacher's rationales and predictions through a shared temperature annealing scheduler that controls both the Gumbel-Softmax-based rationale extraction and the knowledge distillation process. This progressive curriculum allows the student to gradually refine its understanding of important features while learning from the teacher's predictions. The method is applied to both BERT-based language models and ViT-based vision models, showing consistent improvements across different architectures and dataset types.

## Key Results
- REKD improves ViT Small accuracy from 0.889 to 0.968 and ViT Tiny from 0.797 to 0.936 on CIFAR 10
- Significant performance gains demonstrated on both language (IMDB) and vision (CIFAR 10/100) datasets
- Reduced variance across experimental runs compared to baseline RE models
- The progressive learning curriculum through shared temperature annealing proves effective for breaking the "chicken and egg" dilemma

## Why This Works (Mechanism)
REKD works by allowing the student RE model to leverage the learned knowledge and interpretable rationales of a more capable teacher model. This bypasses the need for the student to independently solve the complex task of identifying relevant features while simultaneously making accurate predictions. The shared temperature annealing scheduler creates a curriculum where the student gradually transitions from learning basic feature importance patterns to refining its own RE capabilities while incorporating the teacher's insights. This progressive approach stabilizes training and enables smaller models to achieve performance closer to their larger counterparts.

## Foundational Learning
**Gumbel-Softmax sampling** - A continuous relaxation of discrete sampling used for differentiable rationale extraction. Why needed: Enables gradient-based optimization of discrete rationale selection. Quick check: Verify that temperature annealing appropriately transitions from exploration to exploitation in rationale selection.

**Knowledge distillation** - A training paradigm where a student model learns from a teacher model's predictions. Why needed: Transfers learned knowledge from larger to smaller models, improving student performance. Quick check: Compare student performance with and without distillation to isolate its contribution.

**Rationale extraction** - The process of identifying input features most relevant to model predictions. Why needed: Provides interpretability and potentially improves generalization by focusing on important features. Quick check: Measure rationale quality using faithfulness and plausibility metrics.

## Architecture Onboarding

**Component map:** Input -> Teacher RE & Predictor -> Knowledge Distillation Loss + Student RE Loss -> Student RE & Predictor -> Output

**Critical path:** The shared temperature annealing scheduler that controls both the Gumbel-Softmax sampling for RE and the temperature for knowledge distillation is the critical component that enables the progressive curriculum.

**Design tradeoffs:** The method trades increased computational cost (due to teacher model inference) for improved student performance and training stability. The temperature schedule must be carefully tuned to balance between exploration of rationales and exploitation of teacher knowledge.

**Failure signatures:** Poor teacher rationales will propagate to the student, limiting performance gains. Overly aggressive temperature annealing may cause the student to rely too heavily on the teacher's predictions rather than developing its own reasoning. Mismatched architectures between teacher and student may limit the effectiveness of knowledge transfer.

**First experiments:**
1. Compare REKD performance with fixed versus annealed temperature schedules to validate the progressive curriculum approach
2. Evaluate student rationale quality using human interpretability studies alongside predictive performance
3. Test REKD across different teacher-student size ratios to determine scalability limits

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The method's effectiveness depends heavily on the quality of the teacher model's rationales, which may not always be interpretable or faithful
- The progressive curriculum through shared temperature annealing lacks ablation studies to confirm its necessity versus simpler approaches
- Results show smaller relative improvements on CIFAR-100 compared to CIFAR-10, suggesting potential dataset-specific limitations
- The paper does not address potential overfitting when students mimic teacher predictions too closely

## Confidence
**High confidence**: The core methodology of REKD is sound and the empirical improvements on standard benchmarks are reproducible. The integration of knowledge distillation with RE optimization represents a novel and practical contribution.

**Medium confidence**: The effectiveness of the shared temperature annealing curriculum and its contribution to performance gains. The generalizability of results across different architecture sizes and domains.

**Low confidence**: The interpretability and faithfulness of student rationales compared to teacher rationales, and whether the improved predictive performance comes at the cost of rationale quality.

## Next Checks
1. Conduct ablation studies comparing REKD with fixed temperature schedules versus the progressive annealing approach to isolate the curriculum's contribution to performance gains.

2. Perform statistical significance testing across 10+ random seeds for each experimental setting to validate the claimed variance reduction and ensure results are not due to chance.

3. Implement human evaluation studies to assess the interpretability and faithfulness of student rationales compared to teacher rationales, ensuring that improved predictive performance does not compromise explanation quality.