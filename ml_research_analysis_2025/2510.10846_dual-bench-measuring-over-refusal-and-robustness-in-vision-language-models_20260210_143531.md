---
ver: rpa2
title: 'DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models'
arxiv_id: '2510.10846'
source_url: https://arxiv.org/abs/2510.10846
tags:
- rate
- harmful
- safe
- refusal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUAL-Bench is the first multimodal benchmark for measuring over-refusal
  and safe completion in vision-language models. It addresses the challenge of balancing
  safety and usefulness by evaluating whether models can fulfill benign requests while
  warning about harmful content embedded in images.
---

# DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2510.10846
- **Source URL:** https://arxiv.org/abs/2510.10846
- **Reference count:** 40
- **Key result:** First multimodal benchmark measuring over-refusal and safe completion in vision-language models

## Executive Summary
DUAL-Bench introduces the first multimodal benchmark designed to measure over-refusal and safe completion in vision-language models (VLMs). The benchmark addresses the critical challenge of balancing safety and usefulness by evaluating whether models can fulfill benign requests while appropriately warning about harmful content embedded in images. Through systematic evaluation of 18 state-of-the-art VLMs on 64,124 instruction-image pairs across 12 safety categories, the benchmark reveals substantial room for improvement in multimodal safety alignment.

## Method Summary
DUAL-Bench constructs instruction-image pairs where harmful content is embedded within images, requiring models to either provide safe completions (warn about harm while answering benign parts) or refuse entirely. The benchmark includes 32,062 harmful and 32,062 benign instruction-image pairs across 12 safety categories, with five semantics-preserving perturbations applied to each pair. Performance is measured using three metrics: Refusal Rate (RR), Safe Completion Rate (SCR), and Instruction Robustness Gap (∆IR), capturing the trade-off between safety and usefulness while measuring stability under perturbations.

## Key Results
- GPT-5-Nano achieves 12.9% safe completion, the highest among evaluated models
- GPT-5 family averages 7.9% safe completion across five models
- Qwen models perform worst at only 3.9% safe completion
- Most models default to direct answers rather than safe completions

## Why This Works (Mechanism)
DUAL-Bench works by creating realistic safety scenarios where harmful content is contextually embedded within images alongside benign requests. This design forces models to demonstrate nuanced understanding - they must recognize the harmful elements while still providing useful responses to legitimate portions of the request. The semantics-preserving perturbations ensure that models maintain robust performance across variations, revealing true safety alignment rather than memorization of specific patterns.

## Foundational Learning
- **Multimodal instruction tuning**: Why needed - enables VLMs to process both visual and textual inputs coherently; Quick check - model generates appropriate responses when given image-text pairs
- **Safe completion vs refusal**: Why needed - models must distinguish between harmful content and benign requests; Quick check - model warns about harm while answering legitimate parts
- **Semantic perturbations**: Why needed - tests model robustness to variations in harmful content presentation; Quick check - performance remains stable across perturbed versions
- **Safety category taxonomy**: Why needed - provides systematic framework for evaluating diverse safety scenarios; Quick check - all 12 categories are clearly defined and distinguishable
- **Instruction-image pairing methodology**: Why needed - creates realistic scenarios where safety decisions are context-dependent; Quick check - harmful content is actually present in images as claimed
- **Three-metric evaluation framework**: Why needed - captures the multidimensional nature of safety-usefulness trade-offs; Quick check - metrics are mutually exclusive and collectively exhaustive

## Architecture Onboarding

**Component Map:**
Instruction Generator -> Image Renderer -> Perturbation Engine -> VLM Evaluator -> Metric Calculator

**Critical Path:**
1. Generate instruction sets for each safety category
2. Render images with embedded harmful content
3. Apply semantic perturbations to create test variants
4. Evaluate VLMs on original and perturbed pairs
5. Calculate RR, SCR, and ∆IR metrics

**Design Tradeoffs:**
- Controllable vs realistic visuals: Simplified text-rendered images vs complex natural scenes
- Safety vs usefulness: Binary outcomes vs nuanced intermediate behaviors
- Standardization vs real-world diversity: Controlled categories vs adversarial scenarios

**Failure Signatures:**
- High RR, low SCR: Over-refusal leading to poor user experience
- Low RR, low SCR: Over-answering harmful content
- High SCR, high ∆IR: Good completion but poor robustness to perturbations
- Low SCR, low ∆IR: Consistently poor performance across conditions

**3 First Experiments:**
1. Evaluate baseline model on simple harmful image detection without benign instructions
2. Test model on perturbed versions of the same instruction-image pairs
3. Compare performance across different safety categories to identify specific weaknesses

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does performance on dual-use safety tasks change when evaluated on complex natural scenes (e.g., memes, mixed-media screenshots) versus simplified text-rendered images?
- Basis in paper: [explicit] The authors state in the Limitations section: "Our benchmark primarily utilizes text rendered as images rather than complex natural scenes or diverse real-world media (e.g., memes, mixed-media screenshots)."
- Why unresolved: The simplified visual design was intentional for controllability but may not capture "context-dependent visual risks found in the wild," leaving a gap between benchmark performance and real-world robustness.
- What evidence would resolve it: An extension of DUAL-Bench with natural image categories, comparing model safe-completion rates across both simplified and complex visual contexts while controlling for OCR difficulty.

### Open Question 2
- Question: Can alignment strategies jointly optimize Refusal Rate (RR), Safe Completion Rate (SCR), and Instruction Robustness Gap (∆IR) without trade-offs, and what training data composition enables this?
- Basis in paper: [inferred] The Discussion notes that "RR, SCR, and ∆IR each capture only part of the safety–usefulness trade-off" and advocates for "a joint framework that integrates all three," but no such training methodology is proposed or evaluated.
- Why unresolved: Current models either over-refuse (high RR, low SCR) or over-answer (low RR, low SCR), suggesting single-objective optimization fails to capture the nuanced HHH balance required for dual-use scenarios.
- What evidence would resolve it: Training experiments with multi-objective loss functions and curated dual-use datasets, measuring Pareto improvements across all three metrics compared to baseline safety alignment.

### Open Question 3
- Question: What specific alignment techniques can increase Safe Completion Rates above the current maximum of 12.9% while maintaining robustness under semantic perturbations?
- Basis in paper: [explicit] The paper concludes: "safe completion remains weak across the board, with the best-performing model achieving only 12.9%," and calls for "alignment strategies that promote nuanced, context-aware responses in multimodal settings."
- Why unresolved: The experiments show that higher SCR often correlates with lower robustness (e.g., GPT-5-Nano has highest SCR but is least stable under perturbations), suggesting current techniques cannot simultaneously optimize both dimensions.
- What evidence would resolve it: Ablation studies on instruction-tuning data composition, preference model design, or constitutional principles that explicitly reward safe-completion behaviors, showing improvements in SCR without degradation in ∆IR.

## Limitations
- Simplified visual design using text-rendered images may not capture real-world complexity
- Binary outcome evaluation (safe completion vs direct answer) misses nuanced intermediate behaviors
- Safety categories and instruction formulations may not represent all real-world edge cases
- Performance metrics don't account for confidence calibration of model responses

## Confidence
- **High Confidence**: Systematic evaluation across 18 models and 64,124 pairs supports poor VLM performance on safe completion tasks
- **Medium Confidence**: Claim that most models default to direct answers requires more context about what constitutes "safe completion" in practice
- **Medium Confidence**: Substantial room for improvement is supported by data, but practical significance depends on real-world translation

## Next Checks
1. Conduct field studies comparing benchmark performance with practical safety outcomes and user satisfaction in controlled environments
2. Systematically test model robustness to sophisticated adversarial attacks combining multiple safety categories
3. Evaluate longitudinal stability of safety performance over time with continued use and fine-tuning