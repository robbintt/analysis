---
ver: rpa2
title: 'Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms'
arxiv_id: '2504.20877'
source_url: https://arxiv.org/abs/2504.20877
tags:
- regret
- have
- algorithm
- which
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a preference-centric bandit framework that
  generalizes conventional bandit objectives by using distorted functionals of arm
  distributions, rather than just their means. This enables incorporating risk aversion,
  robustness, and other preferences into decision-making.
---

# Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms

## Quick Facts
- **arXiv ID:** 2504.20877
- **Source URL:** https://arxiv.org/abs/2504.20877
- **Reference count:** 40
- **Primary result:** Optimal policies for risk-sensitive bandits often require mixing arms, not selecting a single best arm.

## Executive Summary
This paper introduces a preference-centric bandit framework that generalizes conventional bandit objectives by using distorted functionals of arm distributions, rather than just their means. This enables incorporating risk aversion, robustness, and other preferences into decision-making. A key insight is that for many preference metrics, the optimal policy is a mixture of arms—not a single best arm—challenging traditional bandit design. The paper proposes two broad classes of algorithms: horizon-dependent (PM-ETC-M, PM-UCB-M, CE-UCB-M) and anytime (CIRT). These algorithms incorporate routines to estimate and track optimal mixing coefficients, enabling regret-efficient learning in the PM-centric setting. Theoretical guarantees are provided for both settings, with regret bounds depending on the preference metric's Hölder continuity and discretization accuracy.

## Method Summary
The method involves maintaining empirical CDFs for each arm, then optimizing a discretized simplex to find target mixing coefficients. A tracking routine selects the most under-sampled arm to match the target mixture proportions. For anytime algorithms, an adaptive interval refinement process refines the search space around promising regions of the simplex. The approach works by combining confidence-guided interval refinement with mixture tracking to achieve sublinear regret.

## Key Results
- The optimal policy for many preference metrics is a mixture of arms rather than a solitary arm
- Proposed algorithms achieve sublinear regret with bounds depending on Hölder continuity and discretization accuracy
- Empirical evaluations show CIRT achieves lower computation time than fixed-discretization algorithms
- Framework generalizes and improves upon existing regret bounds for risk-sensitive measures like CVaR

## Why This Works (Mechanism)

### Mechanism 1: Distortion-Induced Mixture Optimality
The optimal policy for risk-sensitive or preference-centric objectives is frequently a mixture of arms rather than a solitary "best" arm, provided the preference metric (PM) uses a strictly concave distortion function. Conventional bandits maximize linear expectations, where mixtures never outperform the single best arm. In this framework, the objective $U_h(Q)$ (a Choquet integral with distortion $h$) is non-linear. If $h$ is strictly concave (representing risk aversion or diminishing returns), the utility of a convex combination of distributions can strictly exceed the utility of individual distributions (Jensen's inequality inversion). This forces the algorithm to randomize arm pulls to optimize the resulting mixture distribution's tail behavior. Core assumption: The distortion function $h$ is strictly concave and the underlying bandit instance satisfies specific distributional properties (e.g., continuous sub-Gaussian or discrete with $|\Omega| \geq K$).

### Mechanism 2: Mixture Tracking via Under-Sampling
To implement a mixture policy efficiently, an algorithm must actively force the empirical arm selection frequencies to match the target mixing coefficients ($\alpha^\star$). Standard algorithms like UCB naturally gravitate toward one arm. This framework introduces a tracking routine alongside estimation. It estimates the optimal coefficient $\alpha_t$ and then selects the "most under-sampled" arm—defined as the arm $i$ where the gap between target pulls ($t \cdot \alpha_t(i)$) and actual pulls ($\tau_t(i)$) is largest. This corrects deviations from the desired mixture ratio. Core assumption: The estimates of the mixing coefficients converge sufficiently fast so that the tracking routine chases a stable target rather than a moving noise signal.

### Mechanism 3: Confidence-Guided Interval Refinement (CIRT)
An anytime algorithm can avoid the exponential computational cost of finely discretizing the simplex by adaptively refining the search space. The CIRT algorithm phases out suboptimal regions of the simplex $\Delta_{K-1}$. It maintains coarse intervals and only "zooms in" (refines the grid) around the current empirical best coefficient if the lower confidence bound (LCB) of that best coefficient exceeds the UCB of all others. This transforms a large static optimization problem into a sequence of smaller, adaptive ones. Core assumption: The optimal coefficient lies in the interior of the simplex or can be isolated by successive elimination of intervals.

## Foundational Learning

- **Concept:** **Choquet Integral & Distortion Risk Measures**
  - **Why needed here:** The paper shifts the objective from a standard linear expectation to a non-linear functional of the CDF. Understanding how distortion functions $h$ weight probabilities (e.g., overweighting tails for CVaR) is essential to grasp why mixture optimality arises.
  - **Quick check question:** If the distortion function is $h(u)=u$, does the preference metric reduce to standard expected value?

- **Concept:** **Wasserstein Distance ($W_1$)**
  - **Why needed here:** The paper bounds the error of estimating arm distributions using the 1-Wasserstein distance (Lemma 3). Unlike variance or simple mean error, Wasserstein distance captures the geometry of distribution differences, which is critical when the objective depends on the full CDF shape (like Gini deviation).
  - **Quick check question:** Does the convergence guarantee rely on the sub-Gaussianity of the arms to bound this distance?

- **Concept:** **Explore-Then-Commit (ETC) vs. UCB**
  - **Why needed here:** The paper proposes adaptations of these classic algorithms. Understanding the trade-offs (ETC requires gap knowledge; UCB requires confidence calibration) helps in interpreting the "PM-ETC-M" vs. "PM-UCB-M" design choices.
  - **Quick check question:** Which of the proposed algorithms, PM-ETC-M or PM-UCB-M, is described as having better regret guarantees if the optimal solution lies on the boundary (solitary arm)?

## Architecture Onboarding

- **Component map:** Estimator Block -> Optimizer Block -> Tracker Block
- **Critical path:** Pull Arm → Observe $X_t$ → Update Empirical CDF ($F_{A_t, t}$) → (If Horizon-Dependent) Check if $t > N(\epsilon)$; if so, fix the mixing coefficient → (If CIRT/Anytime) Check if confidence bounds are separated; if so, refine the discretization grid → Select next arm based on tracking the current best estimate $a_t$
- **Design tradeoffs:**
  - **PM-ETC-M:** Lowest computation during exploitation (commits to a fixed ratio) but requires knowledge of the sub-optimality gap $\delta$ to set exploration length
  - **PM-UCB-M:** Adaptive (no gap knowledge) but computationally heavy as it requires optimizing over a static, fine grid every step ($O(1/\epsilon^K)$)
  - **CIRT:** Best for high-dimensional $K$ or unknown horizons. Computationally efficient per step ($O(A^K)$) but complex state management (tracking phases and intervals)
- **Failure signatures:**
  - **Oscillation:** If the tracking mechanism fails, arm selection fractions oscillate wildly around the target $\alpha^\star$, leading to high regret
  - **Premature Commitment:** In PM-ETC-M, if $N(\epsilon)$ is too short, the algorithm commits to a suboptimal mixture and incurs linear regret forever
  - **Grid Collapse:** In CIRT, if confidence intervals are too tight, the algorithm might discard the optimal region too early
- **First 3 experiments:**
  1. **Bernoulli Mixture Validation:** Set up the Lemma 1 example (Bernoulli arms, Gini deviation). Verify that the algorithm's empirical arm selection frequency converges to $\approx 0.5$ for both arms
  2. **Discretization Ablation:** Run PM-UCB-M with varying $\epsilon$ (grid resolution). Plot Regret vs. $\epsilon$ to identify the "sweet spot" where estimation error (from coarse grid) balances with tracking error
  3. **Anytime Stress Test:** Run CIRT on a Gaussian bandit. Verify that the "Tracking Phase" (after refinement) yields sublinear regret compared to the "Interval Refinement Phase"

## Open Questions the Paper Calls Out
- What are the minimax and instance-dependent lower bounds for preference metrics (PMs) that exhibit mixture optimality?
- Can regret-efficient PM-centric algorithms be extended to heavy-tailed reward distributions?
- Can provably no-regret anytime algorithms be devised for general distributions beyond the Bernoulli setting?

## Limitations
- The core contribution relies critically on the strict concavity of the distortion function $h$, making it inapplicable to convex distortions like mean-variance or entropic risk
- Theoretical guarantees are asymptotic and hinge on assumptions about discretization accuracy and sub-Gaussianity of arm distributions
- The anytime algorithm CIRT introduces algorithmic complexity through its interval refinement process, with limited empirical validation of robustness to noisy confidence bounds

## Confidence
- **High Confidence:** The mechanism of mixture optimality arising from strictly concave distortions is well-supported by theoretical analysis and Lemma 1
- **Medium Confidence:** Regret bounds for horizon-dependent algorithms are rigorously derived but require empirical validation of practical tightness
- **Low Confidence:** Paper lacks detailed sensitivity analysis for discretization granularity and confidence set parameters across different problem instances

## Next Checks
1. **Convergence to Mixture:** For the Bernoulli bandit example in Lemma 1 (arms with $p_1=0.4, p_2=0.6$ and Gini deviation), run PM-UCB-M and verify that the empirical arm selection frequencies converge to $\alpha^* = [0.5, 0.5]$ over time
2. **Discretization Sensitivity:** Implement PM-UCB-M with varying discretization grid sizes ($\epsilon \in [0.05, 0.2]$). Plot the achieved regret and computational cost to identify the optimal trade-off
3. **Anytime Algorithm Robustness:** Test CIRT on a Gaussian bandit instance. After the tracking phase begins, verify that the algorithm achieves sublinear regret. Additionally, introduce controlled noise into the confidence bounds to assess whether the algorithm prematurely discards the optimal region of the simplex