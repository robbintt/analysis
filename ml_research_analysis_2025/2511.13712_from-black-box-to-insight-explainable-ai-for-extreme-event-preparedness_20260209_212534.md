---
ver: rpa2
title: 'From Black Box to Insight: Explainable AI for Extreme Event Preparedness'
arxiv_id: '2511.13712'
source_url: https://arxiv.org/abs/2511.13712
tags:
- wildfire
- features
- shap
- prediction
- extreme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how explainable AI (XAI) can improve wildfire\
  \ forecasting by enhancing interpretability of black-box models. The authors evaluate\
  \ five AI models\u2014LSTM, Transformer, GTN, Random Forest, and XGBoost\u2014on\
  \ two datasets: Mesogeos (Mediterranean) and California Wildfires."
---

# From Black Box to Insight: Explainable AI for Extreme Event Preparedness

## Quick Facts
- arXiv ID: 2511.13712
- Source URL: https://arxiv.org/abs/2511.13712
- Reference count: 40
- Primary result: SHAP analysis reveals temperature features most influential for wildfire prediction, with deep learning models (especially Transformers) achieving 87.53% accuracy on Mediterranean data and 78.71% on California data.

## Executive Summary
This paper investigates how explainable AI (XAI) can improve wildfire forecasting by enhancing interpretability of black-box models. The authors evaluate five AI models—LSTM, Transformer, GTN, Random Forest, and XGBoost—on two datasets: Mesogeos (Mediterranean) and California Wildfires. They use SHAP to interpret model predictions, creating temporal visualizations of feature importance. Results show deep learning models outperform tree-based ones, with Transformer achieving 87.53% accuracy on Mesogeos and 78.71% on California data. SHAP analysis reveals temperature-related features are consistently most influential, and early signals in the data can extend predictability. Seasonal patterns are also identified, with summer as the highest-risk period. The approach demonstrates how XAI supports actionable insights for disaster preparedness and emergency management.

## Method Summary
The study evaluates five models (LSTM, Transformer, GTN, Random Forest, XGBoost) on two wildfire datasets with different temporal window lengths (30 days for Mesogeos, 11 days for California). Models are trained with specified hyperparameters including learning rates and weight decay, using mean imputation for missing values. SHAP values are computed post-hoc on correctly predicted wildfire samples to create temporal visualizations where color indicates contribution direction and size indicates magnitude. Feature importance rankings are used to evaluate reduced-feature training efficiency, with top-N feature subsets tested against full feature sets.

## Key Results
- Deep learning models outperform tree-based models, with Transformer achieving 87.53% accuracy on Mesogeos and 78.71% on California data
- Temperature-related features are consistently most influential across both datasets and model types
- Early SHAP signals (as early as day 4 in 30-day window) can extend predictability for proactive mitigation
- Seasonal patterns show summer as the highest-risk period for wildfires
- Reduced-feature training using top SHAP features saves ~3.86 seconds per epoch with minimal accuracy loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP-based temporal visualizations reveal when features become influential, enabling earlier detection of wildfire risk signals.
- Mechanism: The authors extract SHAP values for each feature across all time steps (e.g., 30 days for Mesogeos, 11 days for California), then visualize using scatterplots where color encodes contribution direction (blue=negative, red=positive) and size encodes magnitude. This exposes patterns that static SHAP summaries miss.
- Core assumption: Feature importance evolves meaningfully over time; early signals are not noise but reflect genuine predictive structure.
- Evidence anchors:
  - [abstract] "SHAP analysis reveals temperature-related features are consistently most influential, and early signals in the data can extend predictability."
  - [section IV-C] "Default SHAP visualizations... are inadequate to capture the temporal dynamics of time series... they do not show how the influence of individual features evolves over time."
  - [corpus] Weak direct support; neighbor papers focus on XAI for interpretability generally, not temporal SHAP specifically.
- Break condition: If early SHAP values do not generalize to out-of-sample years or regions, the "early signal" insight may be dataset-specific artifacts.

### Mechanism 2
- Claim: Deep learning models (especially Transformers) outperform tree-based models for wildfire prediction because attention mechanisms capture long-range temporal dependencies more effectively.
- Mechanism: Transformers use self-attention to weight relevant time steps dynamically, allowing the model to reference both early and late signals in the sequence. LSTM uses gating to retain long-term dependencies. Tree-based models lack explicit temporal structure and must rely on feature-level splits.
- Core assumption: Wildfire risk depends on accumulated and delayed effects of environmental conditions, not just current-state features.
- Evidence anchors:
  - [section IV-D1] "Deep learning models consistently outperform tree-based ensemble models across both datasets. On the Mesogeos dataset, the Transformer achieves the highest accuracy at 87.53%."
  - [section IV-E1] "Tree-based models such as Random Forest and XGBoost may be less effective in capturing temporal dynamics."
  - [corpus] Neighbor paper "xTime" supports attention-based models for extreme event prediction, but no direct comparison to tree-based baselines.
- Break condition: If temporal features are flattened or lagged features are pre-engineered, tree-based models may close the performance gap, reducing the architectural advantage claim.

### Mechanism 3
- Claim: Explainability-guided feature selection can reduce computational cost with minimal accuracy loss by prioritizing high-SHAP features.
- Mechanism: Train a Transformer on the top-N features ranked by mean absolute SHAP values. The paper shows the top-10 features achieve 83.86% accuracy vs. 87.16% for all 24 features, while the bottom-10 achieve only 80.11%. Training time drops by ~3.86 seconds per epoch.
- Core assumption: SHAP importance rankings are stable across train/test splits and reflect true predictive contribution rather than spurious correlations.
- Evidence anchors:
  - [section IV-E3] "When trained on only ten features, the model trained on the most important subset markedly outperformed the model trained on the least important subset, with an accuracy difference of 3.75%."
  - [section IV-E3] "Training time decreased by 3.86 seconds per epoch when restricting the training set to ten features."
  - [corpus] No direct corpus support for SHAP-based feature selection efficiency in extreme event forecasting.
- Break condition: If important features interact non-linearly and are excluded due to low marginal SHAP values, accuracy could degrade unpredictably.

## Foundational Learning

- **SHAP (Shapley Additive Explanations)**:
  - Why needed here: Core interpretability method; quantifies each feature's contribution to individual predictions. Without understanding SHAP, you cannot reproduce or extend the analysis.
  - Quick check question: Given three features with SHAP values [+0.3, -0.1, +0.05], which feature most increases the predicted probability of wildfire?

- **Attention Mechanisms in Transformers**:
  - Why needed here: The best-performing model uses attention; understanding how self-attention weights time steps is essential for debugging temporal explanations.
  - Quick check question: If a Transformer's attention weights peak at day 3 and day 28 of a 30-day input, what does this imply about the model's decision logic?

- **Time-Series Classification Setup**:
  - Why needed here: The task is binary wildfire occurrence prediction from multivariate time series. Understanding windowing, lag features, and train/test splits is prerequisite to implementation.
  - Quick check question: Why might a 30-day lookback window be preferable to a 7-day window for wildfire prediction, and what is the tradeoff?

## Architecture Onboarding

- **Component map**:
  1. Data Pipeline: Load Mesogeos (25,722 samples, 24 features, 30-day windows) or California Wildfires (1,248 samples, 11 features, 11-day windows). Impute missing values (mean imputation preferred over zero-filling for temperature features).
  2. Model Zoo: LSTM, Transformer (encoder-only), GTN (dual-attention), Random Forest, XGBoost. Deep learning models use Adam optimizer with specified learning rates and weight decay.
  3. Explanation Module: Compute SHAP values per feature per time step. Generate temporal scatterplot visualizations (color=direction, size=magnitude).
  4. Evaluation: Binary classification accuracy on held-out test set. Secondary: feature importance rankings, training time benchmarks.

- **Critical path**:
  Data imputation → Model training (30 epochs for DL, 100 trees for RF/XGBoost) → Prediction on test set → SHAP computation on correctly-predicted wildfire samples → Temporal visualization → Seasonal/monthly aggregation analysis.

- **Design tradeoffs**:
  - **Deep learning vs. tree-based**: DL captures temporal dynamics better (higher accuracy) but is less interpretable without post-hoc XAI. Tree-based models are faster to train but miss temporal structure.
  - **Mean imputation vs. zero-filling**: Mean is physically plausible for temperature; zero-filling introduces artifacts but is computationally trivial.
  - **Full feature set vs. top-N SHAP features**: Reduced features save compute time but risk losing interaction effects.

- **Failure signatures**:
  - SHAP values are near-zero for all features → Model may be underfitting or features are uninformative.
  - Early time steps show higher SHAP than late steps in tree-based models → Possible data leakage or insufficient temporal modeling.
  - Large accuracy gap between train and test → Overfitting, especially on small datasets like California Wildfires.

- **First 3 experiments**:
  1. **Baseline replication**: Train Transformer on Mesogeos with mean imputation; target ~87% accuracy. Verify SHAP identifies temperature features as top contributors.
  2. **Ablation study**: Train Transformer on top-10 vs. bottom-10 SHAP-ranked features; confirm accuracy gap of ~3-4% as reported.
  3. **Temporal sensitivity**: Shorten input window from 30 days to 14 days; measure accuracy drop and observe how early SHAP signals shift or disappear.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do geographic and climatic variability across diverse regions affect seasonal wildfire patterns and model interpretations?
- Basis in paper: [explicit] The authors state that "California's diverse climate, with wide temperature and precipitation variation across regions and years" warrants "further investigation into this geographic and climatic variability" to clarify seasonal patterns and improve model interpretations.
- Why unresolved: The study analyzes only two geographically distinct datasets (Mediterranean and California), limiting understanding of how regional climate diversity affects seasonal risk factors and SHAP-derived interpretations.
- What evidence would resolve it: Multi-region studies spanning diverse climate zones with explicit analysis of how SHAP feature importance shifts across geographic contexts.

### Open Question 2
- Question: Can tree-based models be enhanced to better capture temporal dynamics in sequential wildfire prediction tasks?
- Basis in paper: [explicit] The authors observe that "tree-based models such as Random Forest and XGBoost may be less effective in capturing temporal dynamics" compared to deep learning models, despite identifying similar important features.
- Why unresolved: The paper documents the limitation but does not propose or evaluate modifications to tree-based architectures that could preserve their interpretability while improving temporal pattern capture.
- What evidence would resolve it: Comparative studies of temporally-aware tree-based variants (e.g., sequential splitting criteria, time-weighted ensembles) against standard implementations using the same datasets and metrics.

### Open Question 3
- Question: What are the theoretical limits of early wildfire predictability based on the early signals identified through SHAP analysis?
- Basis in paper: [inferred] The paper demonstrates that certain features (e.g., tp, lst_day) show meaningful SHAP values as early as day 4 in a 30-day window, suggesting "early signals could serve as triggers for proactive mitigation," but does not establish how far in advance reliable prediction remains feasible.
- Why unresolved: While early signals are identified, the study does not systematically evaluate prediction accuracy decay over increasing forecast horizons or determine optimal lead times for operational use.
- What evidence would resolve it: Experiments varying prediction windows systematically (e.g., 7, 14, 21, 30, 60 days) with accuracy and SHAP stability metrics reported for each horizon.

### Open Question 4
- Question: How can discrepancies between SHAP and LIME explanations for static features be reconciled to provide consistent guidance for domain experts?
- Basis in paper: [explicit] The authors note "notable discrepancies emerge for static features such as population and land cover classes" between LIME and SHAP, attributed to LIME's local vs. global explanation focus, but provide no resolution strategy.
- Why unresolved: Practitioners receive potentially conflicting signals from different XAI methods, undermining trust and actionable insight generation without clear guidance on which method to prioritize for which feature types.
- What evidence would resolve it: Systematic comparison of SHAP and LIME across feature categories with domain expert validation of which explanations better align with physical wildfire processes.

## Limitations

- Architectural specifications for deep learning models (LSTM, Transformer, GTN) are not fully disclosed, creating ambiguity in exact hyperparameter configurations
- Study focuses on only two specific datasets without broader regional validation, limiting claims about universal applicability
- Does not test whether SHAP importance rankings remain stable across different train-test splits or out-of-sample years

## Confidence

- **High Confidence**: Model accuracy results for both datasets; feature importance rankings via SHAP; seasonal and monthly pattern identification
- **Medium Confidence**: Claims about early signals extending predictability; computational savings from feature selection; architectural advantages of Transformers over tree-based models
- **Low Confidence**: Generalization of early SHAP signals to new regions or years; interaction effects among top features when reducing dimensionality

## Next Checks

1. **Dataset Generalization Test**: Validate whether early SHAP signals identified in the Mesogeos dataset also appear in a distinct wildfire dataset (e.g., Australia or Brazil) to confirm broader applicability
2. **Cross-Split Stability**: Retrain models with multiple random seeds and different train-test splits to assess whether SHAP rankings and early signal detections are stable or dataset-specific artifacts
3. **Interaction Effects Analysis**: Systematically test combinations of top SHAP features (e.g., temperature + humidity + wind) to determine if their joint impact exceeds the sum of individual contributions, validating the reduced-feature approach