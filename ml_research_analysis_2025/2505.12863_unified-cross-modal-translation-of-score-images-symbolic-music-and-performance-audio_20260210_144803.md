---
ver: rpa2
title: Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance
  Audio
arxiv_id: '2505.12863'
source_url: https://arxiv.org/abs/2505.12863
tags:
- music
- audio
- score
- dataset
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a unified cross-modal translation framework
  for score images, symbolic music, MIDI, and audio, training a single Transformer
  encoder-decoder on multiple tasks simultaneously. The approach uses learned vector
  quantization to tokenize each modality and leverages a large-scale YouTube Score
  Video dataset (over 1,300 hours) to enable joint training.
---

# Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio

## Quick Facts
- arXiv ID: 2505.12863
- Source URL: https://arxiv.org/abs/2505.12863
- Reference count: 40
- Key outcome: Unified framework improves OMR from 24.58% to 13.67% symbol error rate and achieves first successful score image-to-audio generation

## Executive Summary
This work introduces a unified cross-modal translation framework that connects score images, symbolic music (MIDI), and performance audio through a single Transformer encoder-decoder architecture. The system leverages learned vector quantization to tokenize different musical modalities and trains jointly on multiple translation tasks using a large-scale YouTube Score Video dataset (over 1,300 hours). By unifying traditionally separate tasks, the model demonstrates improved performance across all modalities compared to specialized approaches, with notable success in direct score image-to-audio generation—a capability not previously achieved in the literature.

## Method Summary
The approach employs a single Transformer encoder-decoder architecture trained on multiple cross-modal translation tasks simultaneously. Each musical modality (score images, MIDI, audio) is first converted into discrete tokens using learned vector quantization (LVQ), enabling the model to process diverse inputs through a unified representation. The system is trained on a large-scale YouTube Score Video dataset containing over 1,300 hours of aligned score images and performance audio, allowing joint learning across optical music recognition, score-to-MIDI translation, and score-to-audio synthesis tasks. The multi-task training objective improves performance across all individual tasks compared to specialized models.

## Key Results
- Optical Music Recognition error rate improved from 24.58% to 13.67% symbol error rate
- First successful direct score image-to-audio generation demonstrated
- Multi-task training enhances performance across all translation tasks

## Why This Works (Mechanism)
The unified framework works by converting diverse musical modalities into a common discrete token space through learned vector quantization, enabling a single Transformer to process and translate between them. Multi-task training allows the model to learn shared representations across score images, symbolic music, and audio, while the large-scale YouTube Score Video dataset provides rich, naturally aligned data for joint learning. The Transformer architecture's attention mechanisms effectively capture long-range musical dependencies across all modalities, and the discrete token representation enables stable training through standard cross-entropy losses rather than complex adversarial objectives.

## Foundational Learning

**Learned Vector Quantization (LVQ)**
- Why needed: Converts continuous musical signals (images, audio waveforms) into discrete tokens that can be processed by standard Transformer architectures
- Quick check: Verify that LVQ produces consistent token sequences for the same musical content across different runs

**Cross-modal Alignment**
- Why needed: Ensures the model learns meaningful relationships between visual scores, symbolic notation, and audio performances
- Quick check: Test alignment accuracy by comparing generated MIDI against ground truth for paired score-image and audio examples

**Multi-task Training**
- Why needed: Enables knowledge transfer between related musical translation tasks, improving overall performance
- Quick check: Compare performance of joint-trained model against individually trained specialized models on each task

## Architecture Onboarding

**Component Map**
YouTube Score Video Dataset -> LVQ Tokenization -> Unified Transformer Encoder-Decoder -> Multiple Translation Outputs (OMR, Score→MIDI, Score→Audio)

**Critical Path**
Input score images → LVQ encoding → Transformer encoder → Cross-attention with decoder → LVQ decoding → Output audio or MIDI

**Design Tradeoffs**
The unified approach sacrifices some task-specific optimization for broader capability and knowledge sharing, with the main tradeoff being increased computational requirements versus the benefit of improved cross-modal understanding and first-of-its-kind score-to-audio generation.

**Failure Signatures**
- Poor performance on uncommon musical notations not well-represented in training data
- Audio generation artifacts when score contains complex polyphonic passages
- Degradation in translation quality for very long musical pieces exceeding attention window limits

**3 First Experiments**
1. Test basic OMR on simple monophonic scores to verify core recognition capability
2. Validate score-to-MIDI translation using short, well-aligned musical excerpts
3. Attempt score image-to-audio generation on simple melodies before scaling to complex pieces

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, focusing instead on demonstrating the feasibility and benefits of unified cross-modal translation.

## Limitations
- Performance heavily depends on quality and diversity of YouTube Score Video dataset
- Quantization errors from LVQ may affect fine-grained musical detail translation
- Evaluation lacks comprehensive user studies for perceptual quality assessment

## Confidence
- Multi-task training improves performance across all tasks: **High confidence**
- First successful direct score image-to-audio generation: **High confidence**
- Unified model superiority over specialized models: **Medium confidence**

## Next Checks
1. Conduct user studies with musicians to evaluate perceptual quality and usability of generated audio from score images
2. Test model generalization to out-of-distribution musical styles and notation systems not well-represented in YouTube dataset
3. Compare performance against state-of-the-art specialized models for each individual task to quantify unified training trade-offs