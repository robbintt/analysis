---
ver: rpa2
title: 'Diffusion Models under Alternative Noise: Simplified Analysis and Sensitivity'
arxiv_id: '2506.08337'
source_url: https://arxiv.org/abs/2506.08337
tags:
- noise
- diffusion
- gaussian
- device
- discrete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a simplified convergence analysis of the Euler-Maruyama\
  \ discretization for variance-preserving diffusion models, establishing an $O(T^{-1/2})$\
  \ strong convergence rate under standard Lipschitz conditions. The key contribution\
  \ is a theoretical and empirical demonstration that Gaussian noise in the sampling\
  \ process can be replaced by computationally cheaper discrete alternatives\u2014\
  such as Rademacher or uniform noise\u2014without sacrificing convergence guarantees,\
  \ provided the first two moments are matched."
---

# Diffusion Models under Alternative Noise: Simplified Analysis and Sensitivity

## Quick Facts
- **arXiv ID:** 2506.08337
- **Source URL:** https://arxiv.org/abs/2506.08337
- **Reference count:** 40
- **Primary result:** Proves O(T^{-1/2}) strong convergence for Euler-Maruyama in VP-SDEs and shows Gaussian noise can be replaced with symmetric discrete noise (Rademacher, Uniform) without performance loss.

## Executive Summary
This paper presents a simplified convergence analysis of the Euler-Maruyama discretization for variance-preserving diffusion models, establishing an O(T^{-1/2}) strong convergence rate under standard Lipschitz conditions. The key contribution is a theoretical and empirical demonstration that Gaussian noise in the sampling process can be replaced by computationally cheaper discrete alternatives—such as Rademacher or uniform noise—without sacrificing convergence guarantees, provided the first two moments are matched. Experiments validate that symmetric discrete noises achieve sample quality comparable to Gaussian noise on MNIST and CIFAR-10, with FID scores differing negligibly (e.g., 2.99 vs. 3.00 for MNIST). The analysis further shows that asymmetric noise (Laplace) degrades performance, highlighting symmetry as a critical requirement. This work bridges theoretical rigor with practical efficiency, enabling hardware-friendly diffusion model implementations without compromising generation quality.

## Method Summary
The paper analyzes Euler-Maruyama discretization of reverse variance-preserving SDEs, proving O(T^{-1/2}) convergence under Lipschitz assumptions. The key innovation is showing Gaussian noise can be replaced with discrete distributions (Rademacher, Uniform) that match the first two moments (mean 0, variance 1). The theoretical proof uses time-rescaling and Grönwall's inequality to bound error accumulation, while the empirical validation demonstrates negligible FID differences between Gaussian and discrete noise on MNIST/CIFAR-10. The method requires pre-trained score networks and operates entirely in the sampling loop, modifying only the noise generator component.

## Key Results
- Proves O(T^{-1/2}) strong convergence rate for Euler-Maruyama discretization under standard Lipschitz conditions
- Shows symmetric discrete noise (Rademacher, Uniform) achieves comparable FID scores to Gaussian noise when moments are matched
- Demonstrates that asymmetric noise (Laplace) causes significant performance degradation despite matched variance
- Provides PyTorch implementations for all noise types in appendix

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Euler–Maruyama discretization of the reverse VP-SDE achieves a strong convergence rate of O(T^{-1/2}) under Lipschitz conditions.
- **Mechanism:** The authors utilize a time-rescaling of the SDE and apply Grönwall's inequality to bound the error accumulation between the continuous process and the discrete steps. This isolates the discretization error (|t-s|^{1/2}) and propagates it through the sampling trajectory.
- **Core assumption:** The drift b(x,t) and diffusion σ(t) coefficients satisfy standard Lipschitz continuity bounds.
- **Evidence anchors:**
  - [abstract]: "...derive a convergence rate of O(T^{-1/2}) under standard Lipschitz assumptions, streamlining prior analyses."
  - [Section 3.1 Theorem 3.1]: Explicitly derives the bound ≤ ch^{1/2} using the inequality m(x) ≤ ch + c ∫ m(s)ds.
  - [Corpus]: Neighbor paper "A Sharp KL-Convergence Analysis..." (51596) aligns with rigorous convergence analysis but this paper focuses specifically on the simplified Grönwall-based proof.
- **Break condition:** If the score function (drift) is not Lipschitz or the noise schedule β(t) scales improperly with T, the Lipschitz constant k becomes unbounded, breaking the proof.

### Mechanism 2
- **Claim:** Gaussian noise can be replaced by discrete distributions (e.g., Rademacher) without sacrificing the O(T^{-1/2}) convergence rate, provided the first two moments are matched.
- **Mechanism:** By matching the mean (0) and variance (1), the one-step mean squared error of the discrete noise against the Gaussian benchmark remains of order O(h). The authors leverage the Skorokhod embedding (KMT approximation) to couple the discrete random walk to the Wiener process, proving the error accumulates at the same rate.
- **Core assumption:** The noise distribution must be symmetric and have finite variance identical to the Gaussian it replaces.
- **Evidence anchors:**
  - [Section 3.2]: "We consider replacing the Gaussian noise B_h with a scaled discrete random variable √h E... the one-step error simplifies to 2σ²(τ)h."
  - [Theorem 3.5]: Proves the total error bound Cd^{1/2}h^{1/2} holds for the modified process X'^{(h)}.
  - [Corpus]: "Foundations of Diffusion Models..." (8617) discusses general state spaces; this paper provides the specific moment-matching constraint for the substitution guarantee.
- **Break condition:** If variance is mismatched (e.g., scale α ≠ 1), the error bounds diverge, leading to mode collapse or artifacts.

### Mechanism 3
- **Claim:** Symmetry in the noise distribution is a critical requirement; asymmetric distributions (like Laplace) cause significant performance degradation despite matched variance.
- **Mechanism:** The theoretical guarantee relies on the noise behavior in the reverse SDE. Empirically, asymmetric perturbations introduce bias in the denoising trajectory that the score network (trained on symmetric/Gaussian noise) cannot correct, leading to compounding errors.
- **Core assumption:** The score network ε_θ learns a denoising manifold that expects symmetric perturbations centered on the data.
- **Evidence anchors:**
  - [Section 4.1]: "Laplace distribution—the only asymmetric noise tested—resulted in significantly higher FID scores."
  - [Section 5]: "...symmetry remains a necessary condition for minimizing discretization error..."
  - [Corpus]: Neighbor 88768 discusses Lévy noise (often asymmetric/heavy-tailed) for system identification, contrasting with this paper's finding that symmetry is preferred for standard generative sampling.
- **Break condition:** Deployment of any asymmetric noise source (e.g., Exponential, Gamma) in the sampling loop.

## Foundational Learning

- **Concept:** **Grönwall's Inequality**
  - **Why needed here:** This is the primary mathematical tool used in the paper to turn "local" one-step errors into a "global" convergence bound for the entire sampling chain.
  - **Quick check question:** If a function u(t) satisfies u(t) ≤ a + ∫ b(s)u(s)ds, what type of bound does Grönwall's inequality provide for u(t)? (Answer: An exponential bound u(t) ≤ a exp(∫ b)).

- **Concept:** **Variance-Preserving SDE (VP-SDE)**
  - **Why needed here:** The paper specifically analyzes the discretization of VP-SDEs where the noise schedule β(t) is designed to keep the signal variance tractable.
  - **Quick check question:** In a VP-SDE, as t → 1, what does the data distribution X_t converge to? (Answer: A standard Gaussian distribution N(0, I)).

- **Concept:** **Euler–Maruyama Method**
  - **Why needed here:** This is the numerical solver analyzed. Understanding it is necessary to see where the "discrete noise" injection happens (the σ √h E term).
  - **Quick check question:** In the update rule X_{k+1} = X_k + b(X_k)h + σ(X_k) √h E, why is the noise scaled by √h instead of h? (Answer: Because the variance of a Brownian motion increment scales linearly with time step h, so standard deviation scales with √h).

## Architecture Onboarding

- **Component map:** Score Network (ε_θ) -> Noise Scheduler (β(t)) -> Noise Generator -> Euler-Maruyama Sampling Loop
- **Critical path:** The sampling loop (Algorithm 1).
  1. Initialize X_1 ~ N(0,I).
  2. **Loop:** Calculate drift → Sample discrete noise E → Update X_{t-1}.
  3. *Constraint:* The noise E must strictly satisfy E[E]=0 and Var(E)=1.
- **Design tradeoffs:**
  - **Compute vs. Precision:** Rademacher (binary ±1) allows replacing floating-point multiplications with simple sign flips/adds, maximizing throughput on FPGA/Edge, but requires strict variance matching.
  - **Solver Generality:** This analysis applies to Euler–Maruyama; Assumption: higher-order solvers (e.g., RK45) may exhibit different sensitivity to noise discretization.
- **Failure signatures:**
  - **Mode Collapse (Blur):** Caused by underscaling noise variance (α < 1.0 in ablation).
  - **Artifacts/Noise Residue:** Caused by overscaling noise variance (α > 1.0).
  - **Structure Loss:** Caused by using asymmetric noise (Laplace), resulting in FID degradation (e.g., jumping from ~3.0 to ~34.0).
- **First 3 experiments:**
  1. **Sanity Check (Parity):** Run standard Gaussian sampling vs. Rademacher sampling on a pre-trained MNIST model; verify FID scores differ by < 0.1.
  2. **Variance Ablation:** Systematically vary the Rademacher scale α from 0.1 to 2.0; plot FID to confirm the global minimum is exactly at 1.0 (unit variance).
  3. **Symmetry Test:** Compare symmetric alternatives (Uniform, Arcsine) against asymmetric (Laplace) to verify that asymmetry is the breaking condition for the convergence guarantee.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the convergence guarantees for discrete noise substitution be extended to lower-precision quantization regimes (e.g., 4-bit or 8-bit arithmetic), and at what precision threshold does performance deteriorate?
- **Basis in paper:** [explicit] "Future work may extend this analysis to lower-precision quantization regimes, exploring the limits of noise simplification before the model's performance deteriorates."
- **Why unresolved:** The current analysis assumes standard floating-point precision; quantization introduces additional error sources not captured in the Grönwall-based bounds.
- **What evidence would resolve it:** Theoretical bounds incorporating quantization error terms, plus empirical FID evaluations across bit-widths.

### Open Question 2
- **Question:** Does the simplified Grönwall-based analysis extend to other SDE-based generative paradigms such as Consistency Models or Flow Matching?
- **Basis in paper:** [explicit] "Applying this simplified proof technique to other SDE-based paradigms, such as Consistency Models or Flow Matching, presents a promising direction."
- **Why unresolved:** These frameworks have different dynamics (e.g., probability flow ODEs vs. reverse-time SDEs), and the Lipschitz assumptions may not transfer directly.
- **What evidence would resolve it:** Proofs establishing convergence rates for discrete noise in consistency training/Flow Matching, validated empirically.

### Open Question 3
- **Question:** Does discrete noise substitution preserve the improved convergence rates (e.g., O(d/ε)) achieved under additional smoothness assumptions, or does it fundamentally limit the rate to O(d^{1/2}h^{1/2})?
- **Basis in paper:** [inferred] The paper establishes O(d^{1/2}h^{1/2}) but cites sharper rates under stronger assumptions (Li et al., 2024; Jiao and Li, 2025); whether alternative noise is compatible with these refined analyses remains unstated.
- **Why unresolved:** The KMT approximation used for noise coupling may not preserve the smoothness structures exploited in improved bounds.
- **What evidence would resolve it:** Convergence analysis combining discrete noise with higher-order smoothness conditions.

### Open Question 4
- **Question:** How do the theoretical guarantees and empirical findings generalize to state-of-the-art large-scale diffusion models (e.g., Stable Diffusion, Imagen)?
- **Basis in paper:** [inferred] Experiments use "lightweight model architecture and a fixed, limited training budget" on MNIST/CIFAR-10 with FID scores above SOTA benchmarks.
- **Why unresolved:** Scaling behavior of the Lipschitz constants and error accumulation across thousands of timesteps in production models is unexplored.
- **What evidence would resolve it:** Empirical evaluation of discrete noise on large pretrained models (e.g., Stable Diffusion) with FID/CLIP metrics.

## Limitations
- The theoretical analysis assumes a fixed Lipschitz constant k independent of the time step h, which could affect the O(T^{-1/2}) bound for extreme noise schedules
- Empirical validation is limited to MNIST and CIFAR-10, with performance on more complex distributions (e.g., ImageNet, LSUN) unverified
- Results apply specifically to Euler-Maruyama discretization; extension to higher-order solvers remains unexplored

## Confidence
- **High:** The convergence rate O(T^{-1/2}) for Euler-Maruyama under Lipschitz conditions. This follows directly from the provided theorem and is consistent with established SDE theory.
- **Medium:** The equivalence of symmetric discrete noises to Gaussian noise. The moment-matching condition is necessary but may not be sufficient for all network architectures or training regimes.
- **Medium:** The critical role of symmetry. While empirically validated, the exact mechanism by which asymmetry causes degradation is not fully characterized.

## Next Checks
1. **Variance Sensitivity:** Systematically vary the noise scale α in {0.8, 0.9, 1.0, 1.1, 1.2} for Rademacher noise on CIFAR-10. Plot FID vs. α to verify the theoretical prediction of optimal performance at α=1.0.
2. **Solver Generalization:** Repeat the symmetric noise experiments using a stochastic Runge-Kutta method (e.g., SRK4) instead of Euler-Maruyama. Compare convergence rates and FID scores to assess if the noise substitution benefit extends beyond first-order solvers.
3. **Distributional Robustness:** Test asymmetric alternatives (Exponential, Gamma) on the same datasets. Quantify FID degradation and analyze the score function's inability to correct the introduced bias.