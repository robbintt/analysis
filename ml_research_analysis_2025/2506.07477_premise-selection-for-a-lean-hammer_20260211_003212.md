---
ver: rpa2
title: Premise Selection for a Lean Hammer
arxiv_id: '2506.07477'
source_url: https://arxiv.org/abs/2506.07477
tags:
- premises
- proof
- premise
- theorem
- lean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LeanHammer, the first end-to-end hammer tool
  for the Lean proof assistant, designed to automate tedious reasoning steps by combining
  neural premise selection with symbolic proof search and reconstruction. The authors
  develop a novel contrastive learning-based premise selector tailored for Lean's
  dependent type theory, which dynamically adapts to user-specific contexts and integrates
  with existing tools like Aesop, Lean-auto, and Duper.
---

# Premise Selection for a Lean Hammer

## Quick Facts
- arXiv ID: 2506.07477
- Source URL: https://arxiv.org/abs/2506.07477
- Reference count: 40
- Primary result: LeanHammer solves 21% more goals than existing premise selectors

## Executive Summary
This paper presents LeanHammer, the first end-to-end hammer tool for the Lean proof assistant, designed to automate tedious reasoning steps by combining neural premise selection with symbolic proof search and reconstruction. The authors develop a novel contrastive learning-based premise selector tailored for Lean's dependent type theory, which dynamically adapts to user-specific contexts and integrates with existing tools like Aesop, Lean-auto, and Duper. Their method improves upon previous approaches by extracting richer proof data, including implicit premises and definitional equalities, and enabling efficient runtime processing of new premises.

## Method Summary
LeanHammer employs an encoder-only transformer with masked InfoNCE contrastive loss to embed proof states and premises into a shared space. The model is trained on 469,965 proof states extracted from 206,005 theorems across Mathlib, Batteries, and Lean core libraries. The system retrieves top-k premises for both Aesop (k=32) and Lean-auto (k=16), then attempts proof via staged symbolic reasoning: Aesop's built-in rules first, then Lean-auto translation to HOL for Zipperposition ATP, and finally Duper reconstruction using only the premises reported by Zipperposition. The data extraction pipeline captures implicit premises and definitional equalities from simp/rw calls, addressing limitations of tactic-generation approaches.

## Key Results
- LeanHammer achieves 21% higher goal-solving rate compared to existing premise selectors
- Large model variant proves 33.3% of test theorems when given ground-truth premises
- Proof rate approaches theoretical upper bounds when supplied with optimal premises
- Out-of-distribution evaluation on miniCTX-v2-test shows good generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning with masked loss enables effective premise retrieval for dependent type theory.
- Mechanism: An encoder-only transformer embeds proof states and premises into a shared space. The model is trained with a masked InfoNCE loss that samples negative premises per batch while masking positive in-batch premises to avoid mislabeling. At inference, cosine similarity retrieves the top-k premises from accessible definitions.
- Core assumption: Premises useful for closing a goal share semantic structure with the proof state that can be captured by dense embeddings.
- Evidence anchors: [abstract] "contrasts learning-based premise selector tailored for Lean's dependent type theory" [section 3.3.1] "masked contrastive loss to address these problems"

### Mechanism 2
- Claim: Extracting implicit premises and definitional equalities improves premise selector training for hammer use.
- Mechanism: The data extraction pipeline collects not only explicit premises from proof terms but also implicit premises and definitional equalities invoked by `simp` and `rw` calls. This captures the full set of facts needed to close a goal, which differs from tactic-generation approaches that only extract the next tactic's premises.
- Core assumption: A hammer needs all contributing premises, not just those syntactically present in the next tactic.
- Evidence anchors: [section 3.2.2] "extract the full set of implicit and explicit premises that contribute to a proof" [table 4] Ablation shows naive data extraction degrades performance

### Mechanism 3
- Claim: Combining neural retrieval with staged symbolic proof search (Aesop → Lean-auto → Duper) bridges retrieval and verification.
- Mechanism: Retrieved premises flow to two paths: (1) Aesop premise application rules for direct application, and (2) Lean-auto for translation to higher-order logic. Lean-auto invokes Zipperposition; if successful, Duper reconstructs the proof using only the premises Zipperposition reported.
- Core assumption: External provers can find proofs that internal Lean tactics cannot, and reconstruction from a small premise subset is tractable.
- Evidence anchors: [section 3.1] "Aesop is called first... it queries Lean-auto to see if subgoals can be closed" [section 4.2] "LeanHammer settings offer different abilities at different costs"

## Foundational Learning

- Concept: Dependent type theory vs. higher-order logic
  - Why needed here: LeanHammer translates Lean's DTT to HOL for external provers. Understanding what is preserved or lost in translation is essential for debugging.
  - Quick check question: Can you explain why a dependent type might not translate cleanly to HOL?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The premise selector is trained this way. You must understand how negative sampling and temperature affect retrieval quality.
  - Quick check question: What happens if positive pairs are not masked in the batch denominator?

- Concept: Proof reconstruction
  - Why needed here: External provers produce proofs in their own format; Duper must reconstruct valid Lean proof terms from the reported premises.
  - Quick check question: Why is reconstruction given only the used premises, not all retrieved premises?

## Architecture Onboarding

- Component map: Premise selector server -> Aesop rules -> Lean-auto translation -> Zipperposition ATP -> Duper reconstruction
- Critical path: 1. User invokes `leanhammer` tactic in Lean 2. Client sends proof state and accessible premises to selector server 3. Server returns top-k1 premises for Lean-auto and top-k2 for Aesop rules 4. Aesop attempts built-in rules; if incomplete, calls Lean-auto with premises 5. Lean-auto translates; Zipperposition searches; if found, Duper reconstructs
- Design tradeoffs: k1=16 for Lean-auto vs. k2=32 for Aesop: balances ATP input size against rule explosion. No reranking model: recall favored over precision; reranking added latency without clear gain. Cumulative mode tries all variants; higher proof rate but higher cost.
- Failure signatures: Translation failure (21.7%): theorem or premise outside Lean-auto's scope. ATP failure (43.6%): missing premise or translation information loss. Reconstruction failure (1.6%): Duper cannot verify Zipperposition's proof.
- First 3 experiments: 1. Reproduce the ablation in Table 4: train with naive data extraction vs. full extraction; measure recall@k and proof rate on validation set. 2. Sweep k1 and k2 on a held-out split (as in Figure 3) to confirm optimal values for your target domain. 3. Run the `full` and `auto` variants on miniCTX-v2-test to assess out-of-distribution generalization and identify translation bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are effective methods for ensembling neural and symbolic premise selectors to maximize theorem-proving coverage?
- Basis in paper: [explicit] "We believe effective combinations of neural and symbolic methods warrant future investigation" (Section 4.3); "More effective methods of ensembling models may be explored in future work" (Section 4.2).
- Why unresolved: The paper shows that combining neural and symbolic (MePo) approaches yields 39.6% proof rate versus 33.3% for neural alone, but only simple union strategies were tested.
- What evidence would resolve it: Systematic comparison of ensemble methods (voting, learned meta-selectors, cascaded retrieval) showing statistically significant improvements over naive union.

### Open Question 2
- Question: How can LeanHammer be extended to solve theorems requiring longer proofs or more premises?
- Basis in paper: [inferred] Error analysis (Appendix D.4) shows LeanHammer almost exclusively solves theorems with 1–2 line human proofs and ≤8 ground-truth premises, suggesting a fundamental scalability limit.
- Why unresolved: The paper does not investigate why longer proofs fail or propose mechanisms for multi-step reasoning beyond the current single-shot retrieval and proof search.
- What evidence would resolve it: Demonstrated improvement on a held-out set of theorems with proof length >5 lines or >10 premises.

### Open Question 3
- Question: How can the Lean-to-TH0 translation be improved to handle the 21.7% of theorems that currently fail translation?
- Basis in paper: [inferred] Appendix D.5 error analysis identifies Lean-auto translation failure as the leading cause of unsuccessful proofs when ground-truth premises are supplied (21.7% of theorems).
- Why unresolved: The paper notes translation fails when theorems or premises are "outside the scope of the current translation procedure" but does not characterize or address these cases.
- What evidence would resolve it: A revised translation handling a characterized class of previously-failing constructs, with measured reduction in translation failure rate.

## Limitations
- Evaluation relies on Mathlib's relatively uniform proof style, potentially limiting generalization
- 21.7% translation failure rate represents a hard ceiling on hammer effectiveness
- Synthetic premises from libraries may overestimate practical performance in real user contexts

## Confidence
- **High Confidence**: The contrastive learning mechanism is well-established and implementation details are clearly specified
- **Medium Confidence**: Staged symbolic reasoning pipeline shows good empirical results but 43.6% ATP failure rate suggests significant room for improvement
- **Low Confidence**: Claims about making "formal verification more accessible" lack user studies or adoption metrics beyond proof rates

## Next Checks
1. Evaluate LeanHammer on miniCTX-v2-test and additional domains like software verification to assess generalization beyond Mathlib's style
2. Profile the 21.7% translation failures to identify specific Lean constructs that cannot be mapped to HOL, and measure impact of expanding Lean-auto's supported features
3. Test LeanHammer in a realistic user environment where premises are dynamically generated during interactive proof development rather than pre-extracted from libraries