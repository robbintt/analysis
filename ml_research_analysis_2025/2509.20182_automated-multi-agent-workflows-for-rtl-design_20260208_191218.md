---
ver: rpa2
title: Automated Multi-Agent Workflows for RTL Design
arxiv_id: '2509.20182'
source_url: https://arxiv.org/abs/2509.20182
tags:
- design
- reasoning
- pass
- generation
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VeriMaAS is a multi-agent framework for RTL design that integrates
  formal verification feedback from HDL tools directly into workflow generation, reducing
  reliance on costly fine-tuning and long reasoning traces. The core insight is to
  use synthesis and verification results from Yosys and OpenSTA as dynamic signals
  to adaptively select agentic operators (zero-shot I/O, CoT, ReAct, Self-Refine,
  Debate) via a cascading controller.
---

# Automated Multi-Agent Workflows for RTL Design

## Quick Facts
- arXiv ID: 2509.20182
- Source URL: https://arxiv.org/abs/2509.20182
- Reference count: 32
- Core result: 5-7% accuracy gains over fine-tuned baselines with 100× less training data using formal verification feedback as dynamic workflow signals

## Executive Summary
VeriMaAS is a multi-agent framework for RTL design that uses formal verification feedback from synthesis tools as adaptive signals for operator selection. The framework achieves 5-7% improvements in pass@k accuracy over fine-tuned baselines while requiring only hundreds of training examples. By integrating Yosys and OpenSTA outputs into workflow generation, VeriMaAS demonstrates that tool-based feedback provides stronger domain signals than model-internal confidence alone.

## Method Summary
The method employs a cascading controller that selects from five agentic operators (Zero-shot I/O → CoT → ReAct → Self-Refine → Debate) based on verification failure rates. At each stage, K=20 Verilog candidates are generated and evaluated through Yosys synthesis and OpenSTA timing analysis. Failure rates compute confidence scores that trigger escalation when thresholds are exceeded. Thresholds are calibrated from failure-rate percentiles on 500 tuning examples. The framework also supports PPA-aware optimization by substituting area metrics for token costs.

## Key Results
- 5-7% improvements in pass@k accuracy over fine-tuned baselines
- Achieves strong results with only hundreds of training examples versus full fine-tuning
- Demonstrates up to 28.79% area reductions in PPA-aware optimization
- Consistent gains across open-source and closed-source models on VerilogEval and VeriThoughts benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Formal Verification as Task-Difficulty Signal
Synthesis tool outputs provide stronger, domain-grounded signals for operator selection than model-internal confidence alone. Yosys and OpenSTA emit deterministic error logs and pass/fail counts. The controller computes sc as the percentage of K=20 candidates failing verification. High failure rates trigger escalation to more complex operators.

### Mechanism 2: Threshold-Gated Cascade
A fixed-order operator cascade with stage-specific thresholds reduces expected token cost while preserving pass@k gains. The controller follows {I/O → CoT → ReAct → Self-Refine → Debate}. At each stage c, if sc exceeds threshold τc, the cascade terminates; otherwise it escalates. Thresholds are calibrated from failure-rate percentiles (20th, 40th, 60th, 80th) on 500 examples.

### Mechanism 3: PPA-Aware Objective Substitution
Substituting post-synthesis area for token cost in the controller's objective steers generation toward PPA-efficient designs without weight updates. The cost term C(·) in Eq. 1 is set to Yosys-reported area. Thresholds are re-tuned on PPA-Tiny subsets (20 designs selected by an o4 pseudo-oracle).

## Foundational Learning

- **Concept: RTL Synthesis and Verification Flow**
  - Why needed: VeriMaAS depends on Yosys/OpenSTA outputs as the primary feedback signal. Without understanding what compile, synthesis, and timing checks validate, you cannot interpret sc or tune thresholds meaningfully.
  - Quick check: A Verilog module simulates correctly but fails Yosys elaboration. What does this imply about the RTL code?

- **Concept: Prompting Operators (CoT, ReAct, Self-Refine, Debate)**
  - Why needed: The cascade assigns distinct roles to each operator. Misunderstanding their strengths leads to poor ordering or threshold settings.
  - Quick check: Why might Self-Refine outperform Debate for syntax-constrained tasks like Verilog generation?

- **Concept: Pass@k Evaluation**
  - Why needed: The utility function U(·) is defined as pass@k over K=20 samples. Interpreting results and cost trade-offs requires understanding how pass@k differs from exact-match accuracy.
  - Quick check: If pass@1 = 50% and pass@10 = 90%, what does this imply about the candidate distribution?

## Architecture Onboarding

- **Component map:** Query (q) → Controller C → Operator Oc → Generate AOc → Synthesis Pipeline → Logs → Confidence Scorer → sc → Threshold Comparator → (escalate or return) → final candidate pool

- **Critical path:** Query → I/O → synthesis → sc → threshold check → (escalate or return) → final candidate pool

- **Design tradeoffs:**
  1. Lower thresholds → more operators, higher cost, risk of over-correction
  2. Fixed cascade → simpler tuning but may mis-order operators for edge tasks
  3. PPA cost substitution → area gains but observed power/pass@10 trade-offs

- **Failure signatures:**
  1. sc ≈ 1.0 at all stages → task beyond base model or toolchain misconfigured
  2. High early sc but later operator would recover → τc too aggressive
  3. Area drops, power rises → cost function should be multi-objective

- **First 3 experiments:**
  1. Run each operator standalone on VeriThoughts subset; compare pass@k and token cost to VeriMaAS cascade
  2. Ablate τ1 from 0.1 to 0.9; plot pass@10 vs tokens to find inflection points
  3. Retune thresholds with C=Area vs C=Tokens on PPA-Tiny; quantify area/power/pass@10 trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would replacing the cascading controller with tree-search or reinforcement learning (RL) policies significantly improve workflow optimization?
- Basis: The authors explicitly aim to "further enhance the controller formulation by incorporating tree-search or RL-based policies."
- Why unresolved: The current implementation relies on a cascading controller with static percentile-based thresholds, leaving more dynamic optimization strategies unexplored.

### Open Question 2
- Question: Can the framework maintain its PPA optimization effectiveness when transferred to commercial EDA tools and proprietary PDKs?
- Basis: The conclusion motivates "expanding our orchestration signals to commercial EDA tools, and integrating (commercial) PDKs."
- Why unresolved: The reported area/power reductions rely solely on open-source tools (Yosys, OpenSTA) and the Skywater 130nm PDK.

### Open Question 3
- Question: Is the PPA-aware optimization robust on the full benchmark suite without the use of a pseudo-oracle for pre-selection?
- Basis: The PPA evaluation uses OpenAI o4 as a "pseudo-oracle to recommend the top 20 designs," limiting the evaluation to a filtered subset.
- Why unresolved: It is unclear if the reported area reductions (up to 28.79%) depend on the oracle selecting tasks specifically amenable to optimization.

## Limitations
- Exact prompt templates for the five agentic operators are not disclosed, creating reproducibility challenges
- Log parsing logic from Yosys/OpenSTA is underspecified, making failure rate computation ambiguous
- PPA optimization relies on pseudo-oracle selection, limiting generalizability of results

## Confidence

- **High confidence**: The core mechanism of using formal verification feedback as a task-difficulty signal is well-supported by results showing 5-7% improvements over fine-tuned baselines
- **Medium confidence**: The threshold-gated cascade design is plausible but the monotonic difficulty assumption may not hold for all RTL tasks
- **Medium confidence**: The PPA-aware objective substitution shows promising area reductions but the trade-offs with power and pass@10 accuracy suggest the multi-objective optimization may not be fully optimized

## Next Checks

1. **Operator ablation study**: Run each agentic operator standalone on a VeriThoughts subset and compare pass@k and token cost metrics against the VeriMaAS cascade to quantify the marginal benefit of each stage

2. **Threshold sensitivity analysis**: Systematically vary τ1 from 0.1 to 0.9 and plot pass@10 versus tokens to identify optimal inflection points and understand the cost-accuracy trade-off curve

3. **PPA cost substitution validation**: Retune thresholds using area as the cost function versus tokens on the PPA-Tiny subset, then quantify the trade-offs in area reduction, power consumption, and pass@10 accuracy to assess whether the current weighting is optimal