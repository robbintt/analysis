---
ver: rpa2
title: 'GLLM: Self-Corrective G-Code Generation using Large Language Models with User
  Feedback'
arxiv_id: '2501.17584'
source_url: https://arxiv.org/abs/2501.17584
tags:
- g-code
- generation
- gllm
- tool
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GLLM, a system that leverages Large Language
  Models to automatically generate G-code from natural language instructions for CNC
  machining. GLLM addresses the challenges of manual G-code writing by employing a
  fine-tuned StarCoder-3B model with domain-specific training data and a Retrieval-Augmented
  Generation mechanism.
---

# GLLM: Self-Corrective G-Code Generation using Large Language Models with User Feedback

## Quick Facts
- arXiv ID: 2501.17584
- Source URL: https://arxiv.org/abs/2501.17584
- Reference count: 0
- This paper introduces GLLM, a system that leverages Large Language Models to automatically generate G-code from natural language instructions for CNC machining.

## Executive Summary
GLLM addresses the challenges of manual G-code writing by employing a fine-tuned StarCoder-3B model with domain-specific training data and a Retrieval-Augmented Generation mechanism. The system uses advanced prompting strategies and a novel self-corrective code generation approach, incorporating validation mechanisms including syntax checks, G-code-specific verifications, and functional correctness evaluations using Hausdorff distance. Experiments with six G-code generation tasks show that structured prompts consistently outperform unstructured ones across all models, with open-source models like CodeLlama-7B and FT-StarCoder-3B achieving 100% success rates. The results highlight the effectiveness of structured prompts and self-corrective mechanisms in enhancing model accuracy and reliability in CNC programming.

## Method Summary
GLLM fine-tunes StarCoder-3B on the "Stack" dataset containing G-code and other programming languages, then implements a self-corrective generation loop using LangGraph. The system employs structured prompts with extracted parameters, Retrieval-Augmented Generation (RAG) with FAISS, and three validation checks: syntax validation against recognized G-code commands, G-code-specific checks (unreachable code after M30, rapid movement while tool engaged, safe drilling), and functional correctness via Hausdorff distance between generated and user-defined toolpaths. The approach decomposes multi-shape tasks into subtasks and uses a feedback loop that feeds validation errors back to the generation node with augmented prompts.

## Key Results
- Structured prompts consistently outperform unstructured ones across all models, with open-source models achieving 100% success rates on six benchmark tasks.
- The self-corrective mechanism demonstrates effectiveness in enhancing model accuracy and reliability in CNC programming.
- RAG integration showed unexpected performance degradation with unstructured prompts (~10-30% drop), warranting further investigation into optimal integration strategies.

## Why This Works (Mechanism)
GLLM leverages the pattern recognition capabilities of fine-tuned LLMs combined with structured domain knowledge retrieval and iterative self-correction. The structured prompting approach provides clear parameter extraction and task decomposition, while the self-corrective loop enables the system to learn from validation feedback. The multi-layered validation approach ensures both syntactic correctness and functional accuracy of generated G-code, with Hausdorff distance providing a quantitative measure of geometric correctness between generated and target toolpaths.

## Foundational Learning
- **Fine-tuning with PEFT**: Why needed - Reduces computational cost while adapting large models to domain-specific tasks. Quick check - Verify model weights change during training while keeping base parameters frozen.
- **Retrieval-Augmented Generation (RAG)**: Why needed - Provides domain-specific context to improve generation quality. Quick check - Confirm retrieved documents contain relevant CNC machining concepts and machine-specific documentation.
- **Hausdorff distance**: Why needed - Quantifies geometric similarity between generated and target toolpaths for functional validation. Quick check - Calculate Hausdorff distance between two simple geometric shapes to verify implementation.
- **Structured prompting**: Why needed - Ensures consistent parameter extraction and task decomposition. Quick check - Validate that all required fields (material, operation, shape, dimensions) are properly extracted from task descriptions.
- **Self-corrective generation loop**: Why needed - Enables iterative improvement based on validation feedback. Quick check - Monitor iteration count and error reduction across correction cycles.
- **G-code validation checks**: Why needed - Ensures generated code is both syntactically valid and functionally correct. Quick check - Verify syntax validation catches invalid G-code commands and that G-code-specific checks prevent unsafe operations.

## Architecture Onboarding
- **Component map**: User Task Description -> Parameter Extraction -> Structured Prompt Generation -> Fine-tuned LLM + RAG -> G-code Generation -> Syntax Validation -> G-code Specific Validation -> Functional Validation (Hausdorff) -> Success/Failure -> Feedback to Generation (if needed)
- **Critical path**: Parameter extraction → structured prompt → generation → validation chain (syntax → G-code specific → functional) → output
- **Design tradeoffs**: Open-source models vs. proprietary (cost vs. performance), structured vs. unstructured prompts (consistency vs. flexibility), simulation vs. physical validation (safety vs. real-world accuracy)
- **Failure signatures**: High iteration counts indicate prompt structure issues, syntax validation failures suggest insufficient training data, Hausdorff distance failures indicate geometric accuracy problems
- **First experiments**: 1) Fine-tune StarCoder-3B on G-code subset and verify convergence, 2) Implement structured prompt template and test parameter extraction, 3) Build validation pipeline and test on simple geometric shapes

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal integration strategy for RAG in G-code generation tasks, given that RAG decreased performance with unstructured prompts?
- Basis in paper: [explicit] The authors state: "This unexpected result warrants further investigation into the optimal integration of RAG for G-code generation tasks."
- Why unresolved: The experiments showed RAG with unstructured prompts reduced success rates across all models, contrary to expectations. The mechanism by which RAG introduces complexity that hinders performance is not explained.
- What evidence would resolve it: Systematic experiments varying RAG retrieval parameters, chunking strategies, and retrieval timing, coupled with analysis of retrieved documents' relevance to specific G-code generation tasks.

### Open Question 2
- Question: How can prompt structures be further refined to maximize LLM performance on complex multi-shape machining operations?
- Basis in paper: [explicit] Future research direction stated: "Future research should explore optimizing RAG integration and further refining prompt structures to maximize the potential of LLMs in complex machining applications."
- Why unresolved: Task 5 (rectangular pocket with two islands) required significantly more iterations across all models, suggesting current prompt engineering struggles with decomposing and integrating multi-shape operations.
- What evidence would resolve it: Ablation studies testing alternative prompt decomposition strategies, hierarchical prompting approaches, and comparative analysis of iteration counts across systematically varied prompt structures for multi-feature tasks.

### Open Question 3
- Question: How does GLLM performance translate to physical CNC machine execution and real-world manufacturing environments?
- Basis in paper: [inferred] All validation uses simulation (CAMotics) and Hausdorff distance calculations; no physical CNC validation is reported.
- Why unresolved: The gap between validated G-code and actual machine execution remains unexamined. Machine-specific constraints, material properties, and real-world safety considerations may reveal issues not captured in simulation.
- What evidence would resolve it: Deployment studies on physical CNC machines comparing simulated toolpaths to actual machined outputs, measuring dimensional accuracy, surface finish quality, and execution safety across varied materials and machine configurations.

## Limitations
- Critical implementation details (PEFT hyperparameters, exact prompt templates, Hausdorff distance thresholds) are unspecified, preventing exact reproduction of results.
- Evaluation is limited to six synthetic tasks in a controlled environment, raising questions about generalization to complex real-world machining scenarios.
- Performance claims rely heavily on structured prompting, but the specific formatting and parameter extraction methods remain unspecified.

## Confidence
- **High**: Core methodology of fine-tuned models with structured prompting and iterative self-correction is technically sound and well-established.
- **Medium**: Quantitative results are impressive but require reverse-engineering several key components for exact reproduction.
- **Low**: Generalizability claims are questionable due to narrow evaluation scope and absence of real-world validation.

## Next Checks
1. Implement the fine-tuning pipeline with PEFT and evaluate performance sensitivity to rank, epochs, and learning rate using a subset of the Stack dataset.
2. Reconstruct the structured prompt template by analyzing the parameter extraction requirements for the six benchmark tasks and test against the provided tasks.
3. Validate the self-corrective mechanism by implementing the three validation checks (syntax, G-code-specific, functional) and measuring iteration counts and success rates on the benchmark tasks.