---
ver: rpa2
title: 'Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language
  Model Generalization'
arxiv_id: '2502.18273'
source_url: https://arxiv.org/abs/2502.18273
tags:
- generalization
- training
- arxiv
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates how Chain-of-Thought (CoT) reasoning enhances\
  \ language model generalization to compound tasks under distribution shifts. Through\
  \ controlled experiments on three synthetic tasks, it reveals that while QA-trained\
  \ models achieve high in-distribution accuracy, their OOD performance degrades severely\u2014\
  even with 10000k training examples."
---

# Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language Model Generalization

## Quick Facts
- arXiv ID: 2502.18273
- Source URL: https://arxiv.org/abs/2502.18273
- Reference count: 40
- Key outcome: CoT granularity strongly correlates with language model generalization to compound tasks under distribution shifts, with finer-grained CoT achieving 0.81 OOD accuracy versus 0.21 for QA models.

## Executive Summary
This work investigates how Chain-of-Thought (CoT) reasoning enhances language model generalization to compound tasks under distribution shifts. Through controlled experiments on three synthetic tasks, it reveals that while QA-trained models achieve high in-distribution accuracy, their OOD performance degrades severely—even with 10000k training examples. CoT granularity strongly correlates with generalization: finer-grained CoT leads to better OOD performance and sample efficiency, with CoT-100% achieving 0.81 OOD accuracy versus 0.21 for Q-A models. Theoretically, the authors demonstrate that compound tasks contain shortcuts in Q-A data misaligned with true reasoning principles, while CoT forces internalization of valid dependency structures. Transformer positional embeddings further amplify generalization by emphasizing subtask condition recurrence in long CoT sequences. These findings provide guidance for data collection practices to maximize LM generalization under real-world distributional shifts.

## Method Summary
The authors design a synthetic task framework to systematically study the impact of Chain-of-Thought granularity on language model generalization under distribution shifts. They create three tasks (LIS, MPC, ERVC) with varying complexity and construct a controllable CoT granularity spectrum by dropping CoT reasoning tokens at different rates (0%, 30%, 50%, 70%, 100%). This allows precise control over the proportion of reasoning information in training data. The methodology involves training models from scratch on both QA-only and various CoT granularity levels, then evaluating in-distribution (ID) and out-of-distribution (OOD) performance across different training set sizes (1k-10000k examples). They also analyze transformer behavior through theoretical analysis of positional embeddings and their interaction with CoT structure.

## Key Results
- QA models achieve high in-distribution accuracy (up to 0.96) but severely degrade on out-of-distribution tasks (down to 0.21 accuracy)
- CoT-100% consistently outperforms all other granularities on OOD tasks across all sample sizes
- Finer CoT granularity (30-100%) requires fewer samples to achieve comparable performance, demonstrating superior sample efficiency
- Transformer positional embeddings amplify generalization by emphasizing subtask condition recurrence in long CoT sequences
- The findings hold across three synthetic tasks with different structural properties (LIS, MPC, ERVC)

## Why This Works (Mechanism)
CoT reasoning forces language models to internalize valid dependency structures rather than exploiting shortcuts present in QA data. In compound tasks, QA-only training allows models to learn spurious correlations between question-answer pairs without understanding the underlying reasoning chain. CoT annotations explicitly encode the true causal dependencies between subtasks, preventing shortcut learning. The recap conditions in CoT ensure that models must reference previously established conditions when reasoning about subsequent steps, creating a proper causal chain. Transformer positional embeddings further enhance this effect by giving higher weight to tokens that appear in multiple contexts within the CoT sequence, reinforcing the importance of condition recurrence for correct reasoning.

## Foundational Learning
- **Chain-of-Thought reasoning**: Intermediate reasoning steps between question and answer that expose causal dependencies. Needed to provide explicit supervision for compound task reasoning. Quick check: Does the CoT explicitly reference all previously established conditions?
- **Distribution shift in NLP**: When test data differs from training data in task structure or complexity. Needed to evaluate true generalization capability. Quick check: Are ID and OOD test sets constructed with different compound task structures?
- **Shortcut learning in transformers**: Models exploit spurious correlations rather than learning true reasoning principles. Needed to explain why QA-only training fails on OOD tasks. Quick check: Does removing CoT reasoning cause performance collapse on OOD tasks?
- **Positional embedding effects**: How token positions influence attention and learning in transformers. Needed to explain why CoT structure enhances generalization. Quick check: Does shuffling CoT tokens degrade OOD performance?
- **Synthetic task design**: Controlled environments for studying generalization phenomena. Needed to isolate the effects of CoT granularity from other confounding factors. Quick check: Are the synthetic tasks complex enough to require genuine reasoning?
- **Scaling laws for generalization**: How performance changes with training data size across different learning paradigms. Needed to compare sample efficiency of QA vs CoT approaches. Quick check: Do scaling curves show consistent patterns across tasks?

## Architecture Onboarding

**Component Map**: Synthetic task generator -> Data preprocessing (CoT granularity control) -> Transformer training -> ID/OOD evaluation -> Theoretical analysis

**Critical Path**: Task definition → CoT annotation/control → Model training → Evaluation (ID/OOD) → Analysis

**Design Tradeoffs**: The authors balance between synthetic task simplicity (for controlled analysis) and complexity (for realistic reasoning). They trade computational cost for precise control over CoT granularity by using synthetic data rather than real-world annotation.

**Failure Signatures**: 
- QA-only models showing high ID but low OOD accuracy indicate shortcut learning
- CoT models failing to generalize suggest the recap conditions or dependency structures are insufficient
- Performance degradation with increased CoT dropout confirms the importance of fine-grained reasoning

**First 3 Experiments**:
1. Replicate the scaling curves comparing QA vs CoT-100% on LIS task to verify the core finding
2. Test intermediate CoT granularities (30%, 50%, 70%) to confirm the granularity-generalization correlation
3. Evaluate the effect of positional embeddings by comparing standard vs position-agnostic transformers on CoT data

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can Chain-of-Thought (CoT) annotations be automated to reduce costs while maintaining the specific structural conditions (e.g., recap conditions) required for generalization?
- Basis in paper: [explicit] The authors state in the conclusion: "We will explore automated CoT generation and dynamic granularity adaptation to further reduce annotation costs in the future work."
- Why unresolved: The paper demonstrates that high-quality, fine-grained CoT is essential, but relies on synthetic data generation. Real-world human annotation at this granularity is prohibitively expensive ("acquiring high-quality CoT annotations at scale remains prohibitively expensive").
- What evidence would resolve it: An algorithm capable of generating CoT data that satisfies the "Inside/Outside Window Recap Conditions" defined in Section 4.2, achieving OOD performance comparable to human-annotated CoT-100% on complex real-world tasks.

### Open Question 2
- Question: Do the findings regarding CoT granularity and shortcut learning transfer to natural language domains with less formal structure than the synthetic tasks studied?
- Basis in paper: [explicit] In the Related Work section, the authors note: "Optimizing the ratio of CoT reasoning in training data and validating its real-world effectiveness remain open challenges."
- Why unresolved: The empirical evidence relies on three synthetic tasks (LIS, MPC, ERVC) with well-defined state transitions and distribution shifts. It is unclear if the "granularity-generalization tradeoff" holds for ambiguous natural language tasks where "ground truth causal order" is harder to define.
- What evidence would resolve it: Empirical results replicating the scaling curves (Figure 6) on real-world benchmarks (e.g., medical diagnosis or GUI automation) showing that finer CoT granularity consistently suppresses shortcut learning.

### Open Question 3
- Question: Can granularity be adapted dynamically during training or inference to optimize the trade-off between computational cost and generalization?
- Basis in paper: [explicit] The conclusion explicitly lists "dynamic granularity adaptation" as a future work item.
- Why unresolved: The current study evaluates fixed dropout rates (0%, 30%, 50%, 70%, 100%) across the entire dataset. It does not explore methods to vary the granularity based on the specific difficulty or novelty of a given sample.
- What evidence would resolve it: A training paradigm that identifies "hard" OOD samples requiring full CoT (100%) versus "easy" ID samples requiring only Q-A pairs, resulting in a lower average token count without sacrificing the 0.81 OOD accuracy observed in the CoT-100% model.

### Open Question 4
- Question: Do Large Language Models (LLMs) at the billion-parameter scale still suffer from the specific "shortcut" mechanism described, or does scale alone mitigate the need for explicit "recap conditions"?
- Basis in paper: [inferred] Theoretical analysis (Section 4.1) proves transformers are susceptible to shortcuts, but experiments are limited to a 6-layer transformer trained from scratch and Phi-3.5-mini.
- Why unresolved: The paper argues that "larger models may amplify shortcut learning," but does not empirically verify if state-of-the-art large models (e.g., GPT-4 scale) naturally internalize the "recap condition" without explicit fine-grained CoT supervision.
- What evidence would resolve it: An ablation study on larger foundation models showing whether they fail to generalize on the compound tasks without explicit recap tokens in the training data, confirming the theory applies across model scales.

## Limitations
- The synthetic task design, while controlled, may not fully capture the complexity and diversity of real-world compound reasoning tasks
- The theoretical analysis provides intuition but lacks mathematical rigor to formally prove the claimed mechanisms
- The role of positional embeddings in generalization, while demonstrated empirically, lacks deeper theoretical grounding

## Confidence
- **High Confidence**: Experimental results showing CoT's superior OOD generalization on synthetic tasks; correlation between CoT granularity and sample efficiency
- **Medium Confidence**: Claims about CoT forcing internalization of valid dependency structures; the specific mechanism by which positional embeddings enhance generalization
- **Lower Confidence**: Theoretical explanations for why Q-A data contains shortcuts misaligned with true reasoning principles; broader applicability to real-world tasks

## Next Checks
1. Replicate findings on more diverse and realistic compound reasoning benchmarks to assess external validity
2. Conduct ablation studies isolating the effects of CoT structure versus reasoning quality on generalization
3. Develop more rigorous theoretical analysis connecting CoT mechanisms to generalization bounds, potentially using PAC-Bayes or information-theoretic approaches