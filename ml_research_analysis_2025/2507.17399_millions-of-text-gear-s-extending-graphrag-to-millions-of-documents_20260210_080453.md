---
ver: rpa2
title: 'Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents'
arxiv_id: '2507.17399'
source_url: https://arxiv.org/abs/2507.17399
tags:
- passages
- triples
- question
- what
- film
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates scaling graph-based retrieval-augmented
  generation (GraphRAG) to datasets with millions of documents, motivated by the high
  computational cost of traditional LLM-based triple extraction. The authors adapt
  the GeAR framework by introducing an online method that aligns passages with external
  Wikidata triples using sparse retrieval, enabling multi-step reasoning without explicit
  offline triple-passage associations.
---

# Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents

## Quick Facts
- arXiv ID: 2507.17399
- Source URL: https://arxiv.org/abs/2507.17399
- Reference count: 16
- Achieved 0.876 correctness and 0.529 faithfulness on SIGIR 2025 LiveRAG Challenge

## Executive Summary
This paper investigates scaling graph-based retrieval-augmented generation (GraphRAG) to datasets with millions of documents, motivated by the high computational cost of traditional LLM-based triple extraction. The authors adapt the GeAR framework by introducing an online method that aligns passages with external Wikidata triples using sparse retrieval, enabling multi-step reasoning without explicit offline triple-passage associations. Their system pseudo-aligns retrieved passages to Wikidata triples, expands reasoning chains, and re-ranks passages to improve retrieval quality. In the SIGIR 2025 LiveRAG Challenge, the method achieved a correctness score of 0.876 and faithfulness score of 0.529 on the FineWeb-10BT corpus. The results highlight the feasibility of scaling GraphRAG to large corpora, while also revealing challenges in semantic alignment between text and graph data, underscoring the need for improved models that operate within shared semantic spaces.

## Method Summary
The authors extend GraphRAG to millions of documents by implementing an online pseudo-alignment approach that links retrieved passages to Wikidata triples via sparse retrieval, bypassing expensive offline triple extraction. The system uses multi-step agentic retrieval with query rewriting, activating graph expansion only for complex questions requiring compositional reasoning. Passages are retrieved using hybrid retrieval (dense + sparse via RRF), proximal triples are extracted online, and these are linked to Wikidata triples through sparse retrieval. The system expands reasoning chains via diverse triple beam search, re-ranks passages using RRF fusion, and filters irrelevant content with LLM-based re-ranking before final answer generation.

## Key Results
- Achieved 0.876 correctness and 0.529 faithfulness on SIGIR 2025 LiveRAG Challenge
- Successfully scaled GraphRAG to FineWeb-10BT corpus without prohibitive LLM triple extraction costs
- Revealed significant semantic misalignment issues between passages and Wikidata triples, causing topic drift

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online pseudo-alignment bypasses the prohibitive cost of LLM-based triple extraction at million-document scale.
- Mechanism: A small LLM (Falcon-3B-Instruct) extracts "proximal triples" from retrieved passages on-the-fly; these are linked to Wikidata via sparse retrieval, enabling graph traversal without pre-computed passage-triple associations.
- Core assumption: Proximal triples serve as reliable proxies for "real" triples in the knowledge graph. The paper shows this assumption partially fails.
- Evidence anchors:
  - [abstract] "introducing an online method that aligns passages with external Wikidata triples using sparse retrieval, enabling multi-step reasoning without explicit offline triple-passage associations"
  - [section 3] "we propose a simple yet effective approach for pseudo-aligning passages retrieved during a baseline retrieving step with triples from Wikidata, without incorporating any offline association between the two indices"
  - [corpus] Limited direct corpus support; related work on iterative GraphRAG retrieval (arxiv:2509.25530) suggests static retrieval is insufficient for complex queries, but does not validate this specific alignment strategy.
- Break condition: Sparse retrieval produces semantic misalignment—e.g., "Pacific Geoducks" triples link to "Pacific oyster" research papers (Table 2), causing topic drift.

### Mechanism 2
- Claim: Multi-step agentic retrieval with query rewriting captures multi-hop reasoning paths that single-pass retrieval misses.
- Mechanism: At each step n, the system retrieves passages → extracts triples → determines answerability via rewrite module → if not answerable, generates next query q^(n+1). Graph expansion only activates for n > 1.
- Core assumption: Simpler questions do not benefit from graph expansion; complex questions require iterative reasoning over linked triples.
- Evidence anchors:
  - [section 4] "the original input question q is iteratively decomposed into simpler queries: q(1), . . . , q(n)"
  - [section 5] "benefits of graph expansion for simpler questions coming from DataMorgana were limited. Consequently, we opted for a more efficient implementation that does not use Wikidata triples and graph expansion during the first iteration"
  - [corpus] arxiv:2509.25530 ("Beyond Static Retrieval") supports iterative retrieval for multi-hop reasoning but does not validate the conditional activation strategy.
- Break condition: Noise compounds across steps; query rewriting may drift from original intent if evidence is insufficient.

### Mechanism 3
- Claim: Reciprocal Rank Fusion (RRF) of graph-expanded passages with baseline retrieval compensates for alignment noise.
- Mechanism: Graph expansion produces candidate passages via soft alignment (Eq. 3-4); these are merged with baseline hybrid retrieval using RRF. Final filtering removes irrelevant passages using LLM-based re-ranking.
- Core assumption: Graph-expanded passages contain relevant reasoning paths even if soft alignment is noisy; RRF and filtering recover signal.
- Evidence anchors:
  - [section 2] Eq. 1 defines hybrid retrieval via RRF of dense and sparse retrievers
  - [section 4.1] Eq. 4 combines graph-expanded passages with baseline retrieval via RRF
  - [section 4.3] Eq. 6 applies LLM filtering to remove irrelevant passages post-termination
  - [corpus] Weak corpus support; related benchmarks (arxiv:2602.02053) evaluate GraphRAG on wild-source corpora but do not validate this specific reranking approach.
- Break condition: When triple linking produces systematic topic drift (Table 2), graph-expanded passages may introduce irrelevant content that filtering cannot fully remove.

## Foundational Learning

- Concept: Knowledge Graphs and Triple Extraction
  - Why needed here: The system operates on triples (subject, predicate, object) extracted from passages and linked to Wikidata. Understanding what triples represent and how they form reasoning chains is prerequisite.
  - Quick check question: Given passage "Edward L. Cahn directed Laughter in Hell (1933)," what triples would you extract?

- Concept: Reciprocal Rank Fusion (RRF)
  - Why needed here: RRF is used twice—once to combine dense/sparse retrieval, once to merge graph-expanded passages with baseline results. Engineers must understand how RRF aggregates ranked lists without requiring score normalization.
  - Quick check question: If dense retriever ranks doc A at position 1 and sparse retriever ranks doc A at position 10, how does RRF affect its final rank vs. a doc ranked 5 by both?

- Concept: Multi-hop Question Answering
  - Why needed here: The system conditionally activates graph expansion only for questions requiring compositional reasoning. Understanding multi-hop vs. single-hop query structure informs when the full pipeline is necessary.
  - Quick check question: Is "When did the director of Laughter In Hell die?" single-hop or multi-hop? What intermediate fact must be retrieved first?

## Architecture Onboarding

- Component map: Query → Hybrid Retriever (dense Pinecone + sparse OpenSearch via RRF) → Reader (Falcon-3B-Instruct extracts proximal triples) → Triple Memory G^(n) accumulates unique triples → [if n>1: Link to Wikidata via sparse retrieval → Graph Expander with diverse triple beam search] → Soft Alignment maps expanded triples to passages → RRF Merge with baseline retrieval → Rewrite/Termination (Falcon-10B-Instruct determines answerability, generates next query) → Filter (LLM-based re-ranking) → Answer Generator (Falcon-10B-Instruct)

- Critical path: Query → Hybrid Retrieval (n=1) → [if n>1: Read → Link to Wikidata → Graph Expand → Soft Align → RRF Merge] → Rewrite/Termination → (loop or terminate) → Filter → Answer

- Design tradeoffs:
  - Sparse vs. dense alignment: Sparse is cheap but causes topic drift; dense may improve but increases latency
  - n=2 max steps: Limits cost but may truncate reasoning for complex queries
  - Conditional graph expansion: Skips Wikidata for n=1, improving efficiency for simple questions
  - Filtering stage: Adds LLM call but reduces noise in final context

- Failure signatures:
  - Topic drift: Proximal triples link to unrelated Wikidata entries (Table 2 examples: geoducks → oyster research; hot tub → mineralogy papers)
  - Low faithfulness (0.529): System may generate answers not fully grounded in retrieved passages
  - Sparse retrieval misalignment: Lexical similarity does not guarantee semantic alignment between passage content and graph triples

- First 3 experiments:
  1. Alignment quality audit: Sample 100 queries, manually label whether linked Wikidata triples are semantically related to proximal triples. Quantify misalignment rate.
  2. Step ablation: Compare n=1 (no graph expansion) vs. n=2 (full pipeline) on multi-hop vs. single-hop questions from DataMorgana taxonomy. Measure correctness/faithfulness delta.
  3. Dense alignment pilot: Replace sparse triple linking (Eq. 3) with dense embedding similarity. Compare topic drift rate and end-to-end correctness on a held-out subset.

## Open Questions the Paper Calls Out
- Can asymmetric dense embedding models significantly reduce the semantic misalignment between unstructured passages and structured triples compared to the sparse retrieval baseline used in this study?
- How does the cumulative computational latency and cost of online pseudo-alignment compare to offline LLM-based triple extraction as the query volume increases?
- Can the system effectively scale graph-based retrieval for domain-specific corpora where the external Knowledge Graph (Wikidata) has sparse coverage?
- Can an automated query-complexity classifier improve efficiency by dynamically deciding when to activate the graph expansion step?

## Limitations
- Semantic misalignment between passages and Wikidata triples causes systematic topic drift, undermining graph expansion reliability
- Absence of retrieval hyperparameters (k values, beam widths, diversity parameters) makes faithful reproduction difficult
- Modest faithfulness score (0.529) indicates frequent generation of answers not fully grounded in retrieved passages

## Confidence
- GraphRAG scaling feasibility: High confidence - Competitive LiveRAG Challenge results (0.876 correctness) demonstrate successful million-document scaling without prohibitive costs
- Online pseudo-alignment mechanism: Medium confidence - Avoids expensive offline processing but empirical evidence reveals significant semantic misalignment issues
- Multi-step agentic retrieval benefit: Medium confidence - Conditional activation strategy is justified but lacks direct comparison showing performance degradation when graph expansion is applied to all questions
- RRF compensation for alignment noise: Low confidence - Weak support for claim that RRF effectively recovers signal from noisy graph-expanded passages given observed topic drift and modest faithfulness scores

## Next Checks
1. Alignment quality audit: Sample 100 queries from the LiveRAG Challenge, manually label whether linked Wikidata triples are semantically related to proximal triples, and quantify the misalignment rate. Compare this rate against the performance impact on correctness scores.

2. Step ablation study: Compare n=1 (no graph expansion) versus n=2 (full pipeline) on multi-hop versus single-hop questions from DataMorgana taxonomy. Measure correctness and faithfulness deltas to empirically validate the conditional activation strategy.

3. Dense alignment pilot: Replace the sparse triple linking mechanism (Eq. 3) with dense embedding similarity matching. Compare the topic drift rate and end-to-end correctness on a held-out subset of 500 queries to assess whether semantic alignment quality improves sufficiently to justify increased computational cost.