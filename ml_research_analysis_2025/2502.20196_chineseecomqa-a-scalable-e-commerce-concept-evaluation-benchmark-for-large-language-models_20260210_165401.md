---
ver: rpa2
title: 'ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for Large
  Language Models'
arxiv_id: '2502.20196'
source_url: https://arxiv.org/abs/2502.20196
tags:
- e-commerce
- concept
- llms
- chineseecomqa
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ChineseEcomQA, a scalable benchmark for
  evaluating large language models (LLMs) on fundamental e-commerce concepts. The
  benchmark addresses two key challenges in e-commerce evaluation: the heterogeneity
  and diversity of tasks, and distinguishing between generality and specificity within
  the e-commerce field.'
---

# ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for Large Language Models

## Quick Facts
- **arXiv ID**: 2502.20196
- **Source URL**: https://arxiv.org/abs/2502.20196
- **Reference count**: 33
- **Primary result**: Deepseek-R1 and Deepseek-V3 are currently the best LLMs for e-commerce concepts, while many SOTA models achieve below 60% accuracy on specific sub-concepts.

## Executive Summary
This paper introduces ChineseEcomQA, a benchmark designed to evaluate large language models (LLMs) on fundamental e-commerce concepts. The benchmark addresses the heterogeneity and diversity of e-commerce tasks while distinguishing between general and domain-specific knowledge. It covers 10 core e-commerce concepts across 20 industries, comprising 1,800 carefully curated question-answer pairs. The construction process involves LLM validation, Retrieval-Augmented Generation (RAG) validation, and manual annotation to ensure quality and coverage.

Experiments reveal that Deepseek-R1 and Deepseek-V3 currently outperform other models on this benchmark, with many state-of-the-art models achieving below 60% accuracy on specific sub-concepts. The study demonstrates that RAG significantly improves model performance across various sizes, and larger models show better calibration in confidence estimation. The benchmark provides valuable insights into LLM capabilities in e-commerce and offers directions for future research in the field.

## Method Summary
The ChineseEcomQA benchmark employs a rigorous 7-step construction pipeline: (1) GPT-4o generates question-answer pairs from an e-commerce corpus, (2) LLM verification ensures objectivity and uniqueness, (3) web search verifies generality, (4) e-commerce search verifies domain expertise, (5) combined factuality verification, (6) difficulty filtering using multiple models (Qwen, LLaMA, GPT-4o), and (7) human verification. The dataset contains 1,800 QA pairs across 10 concept categories (Industry Categorization, Industry Concept, Category Concept, Brand Concept, Attribute Concept, Spoken Concept, Intent Concept, Review Concept, Relevance Concept, Personalized Concept) and 20 industries. Evaluation uses an LLM-as-a-judge ensemble (GPT-4o, Claude-3.5-Sonnet, Deepseek-V3) with three possible verdicts: Correct, Wrong, or Not Attempted.

## Key Results
- Deepseek-R1 and Deepseek-V3 achieve the highest accuracy scores among mainstream LLMs on ChineseEcomQA
- Many state-of-the-art models score below 60% accuracy on specific e-commerce sub-concepts
- RAG significantly improves model performance across various model sizes
- Larger models demonstrate better calibration in confidence estimation

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its multi-layered validation approach that ensures questions test genuine e-commerce knowledge rather than general language understanding. By combining LLM verification with web and e-commerce search validation, the benchmark filters out questions that can be answered through general knowledge alone. The three-tier judgment system (LLM-as-a-judge with voting) provides robust evaluation that reduces individual model biases. The focus on both general concepts and industry-specific knowledge creates a comprehensive assessment that reflects real-world e-commerce scenarios.

## Foundational Learning
- **E-commerce concept taxonomy**: Understanding the 10 core concepts (Industry, Category, Brand, Attribute, etc.) is essential for interpreting benchmark results and identifying model weaknesses.
- **Retrieval-Augmented Generation (RAG)**: Knowledge of RAG systems is needed to understand why retrieval improves performance and how domain-specific indices impact results.
- **LLM-as-a-judge methodology**: The evaluation approach using multiple judges requires understanding how ensemble methods reduce bias and improve reliability.
- **Confidence calibration**: The analysis of model confidence versus accuracy requires understanding how calibration relates to model reliability in practical applications.
- **Chinese e-commerce landscape**: Familiarity with Chinese market structure and industry segmentation helps contextualize the 20-industry coverage and concept distributions.

## Architecture Onboarding
- **Component map**: Data Generation (GPT-4o) -> LLM Verification -> Search Verification (Web/E-commerce) -> Difficulty Filtering -> Human Verification -> Evaluation (Judge Ensemble)
- **Critical path**: The bottleneck is the manual verification step, which ensures quality but limits scalability; automated verification steps depend on reliable search APIs and consistent LLM behavior.
- **Design tradeoffs**: The benchmark prioritizes quality over quantity (1,800 vs. millions of questions), accepting smaller scale for higher reliability. The three-judge ensemble sacrifices speed for reduced bias.
- **Failure signatures**: High judge disagreement (>50%) indicates ambiguous questions; consistently low accuracy on specific concepts suggests model limitations rather than benchmark flaws; poor RAG performance may indicate inadequate retrieval indices.
- **Three first experiments**: (1) Run baseline evaluation without RAG to establish baseline model capabilities, (2) Test RAG with different retrieval strategies to identify optimal configuration, (3) Evaluate model calibration by comparing confidence scores with accuracy across difficulty levels.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark construction relies heavily on LLM-based verification steps that may introduce algorithmic biases affecting question difficulty and concept representation
- The evaluation focuses exclusively on Chinese-language e-commerce concepts, limiting generalizability to other markets or languages
- RAG experiments require domain-specific retrieval indices that are not publicly available, creating a reproducibility barrier for reported performance improvements

## Confidence
- **High Confidence**: The 7-step construction pipeline is clearly documented; the 10 concept categories and 20 industry coverage are explicitly stated; the judge ensemble methodology is reproducible
- **Medium Confidence**: Model performance rankings are based on reported accuracy scores, but confidence intervals are not provided; calibration analysis depends on assumption that judge LLMs provide reliable confidence estimates
- **Low Confidence**: Claims about model-specific weaknesses on particular sub-concepts are difficult to verify without per-concept performance breakdowns and statistical significance testing

## Next Checks
1. **Inter-annotator Agreement Validation**: Replicate the human verification step with multiple annotators on a random sample of 100 questions and calculate Cohen's kappa to establish reliability metrics for the benchmark's gold standard
2. **Cross-Lingual Transfer Test**: Translate a subset of 200 ChineseEcomQA questions into English and evaluate the same LLMs to assess whether performance degradation is consistent with general cross-lingual transfer patterns
3. **Concept Coverage Analysis**: Conduct a systematic audit of the 1,800 questions to verify uniform distribution across the 10 concepts and 20 industries, ensuring no systematic underrepresentation that could bias model comparisons