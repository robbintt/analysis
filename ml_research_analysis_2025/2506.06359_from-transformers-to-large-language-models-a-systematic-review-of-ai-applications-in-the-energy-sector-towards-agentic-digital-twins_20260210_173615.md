---
ver: rpa2
title: 'From Transformers to Large Language Models: A systematic review of AI applications
  in the energy sector towards Agentic Digital Twins'
arxiv_id: '2506.06359'
source_url: https://arxiv.org/abs/2506.06359
tags:
- energy
- transformer
- forecas
- data
- onal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This systematic review analyzed 99 publications from 2021\u2013\
  2025 on transformer and large language models in the energy sector. It found that\
  \ transformer-based models, especially temporal fusion transformers and informers,\
  \ significantly outperform traditional approaches in electricity demand, thermal\
  \ load, and renewable generation forecasting, with error reductions up to 35%."
---

# From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins

## Quick Facts
- arXiv ID: 2506.06359
- Source URL: https://arxiv.org/abs/2506.06359
- Reference count: 40
- Transformers and LLMs improve energy forecasting and enable autonomous digital twin agents

## Executive Summary
This systematic review examines 99 publications from 2021-2025 on transformer and large language models in the energy sector. The analysis demonstrates that transformer-based models, particularly temporal fusion transformers and informers, achieve up to 35% error reduction compared to traditional approaches in electricity demand, thermal load, and renewable generation forecasting. Large language models, while newer to the domain, show promise in integrating structured time series with unstructured text and enabling multimodal reasoning across energy system applications.

The review introduces the concept of "Agentic Digital Twins" where LLMs transform passive digital twins into autonomous, proactive agents capable of situational awareness and energy management planning. This paradigm shift could significantly enhance smart grid intelligence and automation. However, challenges remain in domain adaptation, real-time inference, and validation of safety-critical actions when integrating LLMs with digital twins.

## Method Summary
The review conducted a systematic analysis of 99 peer-reviewed publications spanning 2021-2025, focusing on transformer and large language model applications in the energy sector. Publications were selected based on relevance to energy forecasting, building energy modeling, anomaly detection, and decision support systems. The analysis synthesized empirical performance metrics, architectural approaches, and emerging paradigms while identifying common challenges and opportunities across the surveyed literature.

## Key Results
- Transformer-based models achieve up to 35% error reduction in electricity demand, thermal load, and renewable generation forecasting
- Large language models enable integration of structured time series with unstructured text for enhanced reasoning
- "Agentic Digital Twins" concept emerges as a transformative paradigm for autonomous energy management systems

## Why This Works (Mechanism)
Transformer architectures excel at capturing long-range dependencies and temporal patterns in sequential data, making them particularly suited for energy time series forecasting. Their self-attention mechanisms can identify complex relationships between different time scales and variables. Large language models add reasoning capabilities and multimodal integration, allowing them to process both numerical energy data and contextual information from unstructured sources like maintenance logs or weather reports. The combination enables more comprehensive decision-making in energy systems.

## Foundational Learning
1. Temporal Fusion Transformers (TFT) - why needed: Handles multiple time scales in energy forecasting; quick check: Verify variable selection and temporal aggregation methods
2. Informers - why needed: Efficient attention for long sequences in energy data; quick check: Validate sparse attention implementation
3. Multi-head attention - why needed: Captures diverse relationships in energy consumption patterns; quick check: Examine attention weight distributions
4. Tokenization strategies - why needed: Converts energy data into LLM-compatible format; quick check: Test different numerical representation methods
5. Domain adaptation techniques - why needed: Bridges general LLM knowledge with energy-specific requirements; quick check: Measure domain-specific performance metrics
6. Digital twin integration - why needed: Enables virtual-physical system synchronization; quick check: Validate data consistency between twin and physical system

## Architecture Onboarding

Component map: Energy data sensors -> Preprocessing pipeline -> Transformer/LLM model -> Digital twin interface -> Decision support system -> Control actuators

Critical path: Real-time data ingestion -> Feature extraction -> Model inference -> Action generation -> System feedback loop

Design tradeoffs:
- Accuracy vs. computational efficiency: More complex models provide better forecasting but require greater resources
- Model size vs. real-time capability: Larger LLMs offer better reasoning but may struggle with latency requirements
- Generalization vs. specialization: Domain-adapted models perform better but lack flexibility across different energy contexts

Failure signatures:
- Prediction drift in transformer models indicating data distribution changes
- Attention collapse in LLM-based reasoning modules
- Latency spikes in digital twin synchronization
- Inconsistent behavior across different energy scenarios

First experiments:
1. Benchmark temporal fusion transformer against traditional ARIMA and LSTM models on standardized energy forecasting datasets
2. Implement simple LLM-based reasoning pipeline for building energy optimization using synthetic data
3. Test digital twin synchronization latency under varying data update frequencies

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Short temporal scope (2021-2025) may miss foundational research and long-term trend analysis
- Publication bias toward positive results could inflate reported performance improvements
- Heterogeneous forecasting tasks and datasets make cross-study comparisons challenging
- Limited discussion of energy system heterogeneity across different regional contexts

## Confidence
High: Empirical claims about transformer model performance improvements (up to 35% error reduction) supported by multiple studies across diverse applications
Medium: "Agentic Digital Twins" conceptualization represents forward-looking synthesis rather than extensively validated research
Medium: Challenges around domain adaptation and real-time inference primarily drawn from secondary sources

## Next Checks
1. Conduct primary empirical validation comparing temporal fusion transformers and informers against traditional methods using standardized energy forecasting datasets with consistent evaluation protocols
2. Implement and test LLM-based digital twin agent prototypes in controlled simulation environments to assess safety-critical decision-making capabilities
3. Perform computational cost-benefit analysis quantifying operational overhead of transformer/LLM models versus performance gains across different energy system scales