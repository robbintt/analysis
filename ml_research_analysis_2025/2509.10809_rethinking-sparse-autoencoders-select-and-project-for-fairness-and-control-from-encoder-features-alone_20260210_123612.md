---
ver: rpa2
title: 'Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control
  from Encoder Features Alone'
arxiv_id: '2509.10809'
source_url: https://arxiv.org/abs/2509.10809
tags:
- top-k
- encoder
- reconstruction
- masked
- opinion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work challenges the conventional decoder-centric approach\
  \ to steering sparse autoencoders (SAEs) by proposing an encoder-centric framework.\
  \ Instead of masking and reconstructing embeddings, the method selects top-K encoder\
  \ features aligned with target attributes or behaviors, optionally interpolates\
  \ them into a unified control axis, and applies orthogonal projection directly in\
  \ the model\u2019s native embedding space."
---

# Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone

## Quick Facts
- arXiv ID: 2509.10809
- Source URL: https://arxiv.org/abs/2509.10809
- Reference count: 13
- Key outcome: Challenges decoder-centric SAE steering with encoder-centric method, achieving up to 3.2× fairness gains and 3.6× behavioral reduction.

## Executive Summary
This work challenges the conventional decoder-centric approach to steering sparse autoencoders (SAEs) by proposing an encoder-centric framework. Instead of masking and reconstructing embeddings, the method selects top-K encoder features aligned with target attributes or behaviors, optionally interpolates them into a unified control axis, and applies orthogonal projection directly in the model's native embedding space. This design avoids bottlenecks, remains faithful to the model's representation space, and is computationally lightweight. Applied to vision-language models, it improves fairness metrics on CelebA and FairFace by up to 3.2× over standard SAE methods. For large language models, it substantially reduces aggressiveness and sycophancy in Llama-3 8B Instruct, achieving up to 3.6× gains over masked reconstruction. Encoder-centric interventions thus offer a more effective and general mechanism for behavioral steering at inference time.

## Method Summary
The paper proposes a novel encoder-centric framework for steering SAEs that operates directly on encoder features rather than decoder reconstructions. The method selects top-K features correlated with target attributes using learned weights, optionally interpolates these into a unified control axis via linear probe training, and applies orthogonal projection to remove unwanted attribute influence. This approach avoids the representational bottlenecks of decoder-based masking, remains faithful to the model's native embedding space, and is computationally efficient. The method is evaluated on both vision-language models for fairness debiasing and large language models for behavioral steering.

## Key Results
- Achieves up to 3.2× improvement in fairness metrics (True Positive Rate Parity, Equalized Odds) on CelebA and FairFace datasets compared to standard SAE methods
- Reduces sycophancy in Llama-3 8B Instruct by up to 3.6× compared to masked reconstruction approaches
- Maintains task performance (e.g., VQA accuracy) while significantly reducing target attribute bias
- Demonstrates computational efficiency with lightweight projection operations

## Why This Works (Mechanism)
The encoder-centric approach works by directly manipulating the model's learned representations rather than attempting to reconstruct modified embeddings. By selecting and projecting encoder features aligned with target attributes, the method can precisely control the influence of specific behaviors or biases. The interpolation mechanism allows for unified control of multiple related attributes, while orthogonal projection cleanly removes unwanted directional information without introducing reconstruction artifacts. This direct manipulation of representations is more efficient and effective than the indirect decoder-based approaches that must infer changes through reconstruction.

## Foundational Learning

**Sparse Autoencoders (SAEs)**: Neural networks trained to reconstruct inputs through a bottleneck of sparse activations. SAEs decompose inputs into interpretable features, making them useful for steering and intervention. *Why needed*: SAEs provide the sparse, interpretable feature space that enables targeted manipulation of model behaviors.

**Feature Attribution**: The process of identifying which input features or model activations contribute most to a particular output or behavior. *Why needed*: Feature attribution allows the method to identify which encoder features correspond to target attributes for selection and control.

**Orthogonal Projection**: A mathematical operation that removes components of a vector along a specified direction while preserving components orthogonal to that direction. *Why needed*: Orthogonal projection enables clean removal of unwanted attribute influence without affecting other aspects of the representation.

**Linear Probe Training**: A technique where a simple linear classifier is trained on top of frozen representations to identify which features correlate with specific attributes. *Why needed*: Linear probes provide the weights for feature selection and interpolation without requiring full model retraining.

**Control Axis Interpolation**: The process of combining multiple related feature directions into a single unified axis for easier manipulation. *Why needed*: Interpolation simplifies control of multiple related attributes and enables more precise steering operations.

## Architecture Onboarding

**Component Map**: Input Embeddings -> Encoder -> Sparse Features -> Feature Selection -> Control Axis (optional) -> Orthogonal Projection -> Modified Embeddings -> Model

**Critical Path**: The core intervention path involves encoder features being selected based on learned weights, optionally interpolated into a control axis, and then projected orthogonally to remove unwanted attribute influence.

**Design Tradeoffs**: The method trades the flexibility of decoder-based reconstruction for the efficiency and precision of direct encoder manipulation. This results in computational savings but requires supervised alignment of features to target attributes.

**Failure Signatures**: Poor feature selection weights, inadequate interpolation training, or improper projection dimensions can lead to incomplete attribute removal or unintended performance degradation.

**First Experiments**: 
1. Validate feature selection weights correlate with target attributes on a held-out dataset
2. Test orthogonal projection removes attribute influence without degrading task performance
3. Compare computational efficiency against decoder-based baselines

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the interpolation mechanism be adapted to support multi-categorical sensitive attributes?
- Basis in paper: [explicit] Appendix A states the current interpolation methodology is "constrained to binary attribute configurations," limiting its applicability to complex attributes like race or religion.
- Why unresolved: The proposed linear probe training for weight interpolation currently relies on binary classification outputs, making it mathematically insufficient for multi-class scenarios.
- What evidence would resolve it: A modified formulation of the interpolation step that successfully debiases a multi-class attribute (e.g., race classification on FairFace) while preserving utility.

**Open Question 2**
- Question: Does the orthogonal projection method suffer from the "whac-a-mole" phenomenon?
- Basis in paper: [explicit] Appendix A notes the framework "does not address the whac-a-mole phenomenon," wherein mitigating one bias may inadvertently amplify orthogonal biases.
- Why unresolved: The paper evaluates the removal of specific target attributes (e.g., gender) but does not measure the potential side-effects on non-target protected attributes.
- What evidence would resolve it: A comprehensive evaluation showing that orthogonalizing gender directions does not statistically significantly increase bias metrics for other attributes like age or skin tone.

**Open Question 3**
- Question: Can the encoder-centric framework be modified to inject attributes into neutral inputs?
- Basis in paper: [explicit] Appendix A highlights that the work does not "provide a complementary mechanism for introducing such characteristics into neutral inputs."
- Why unresolved: The method is designed strictly for suppression via orthogonal projection (Vx), lacking a mechanism to actively add directional information to an embedding.
- What evidence would resolve it: A demonstration of a modified projection or translation operator capable of inducing a specific behavior (e.g., aggressiveness) in a neutral prompt without degrading coherence.

## Limitations
- The method relies on supervised alignment of features to target attributes, limiting applicability in less structured domains
- Current interpolation mechanism is constrained to binary attribute configurations, limiting multi-class attribute support
- Does not address the "whac-a-mole" phenomenon where mitigating one bias may amplify orthogonal biases
- Scalability to very large models and robustness over extended inference chains remain untested

## Confidence
**High**: Core claims about encoder-centric superiority over decoder-based SAE steering are well-supported by empirical results across multiple model types and tasks
**Medium**: Concerns around long-term robustness to feature drift, scalability to larger models, and generalizability to all fairness/behavioral metrics beyond those tested
**Low**: No significant low-confidence claims identified

## Next Checks
1. Test scalability and performance on SAEs with >1024 features and larger VLMs/LLMs (e.g., Llama-3 70B)
2. Evaluate robustness of control axes over long inference chains or continual adaptation
3. Apply the method to a broader set of fairness and behavioral attributes, including those outside vision and standard language tasks