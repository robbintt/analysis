---
ver: rpa2
title: 'A self-supervised learning approach for denoising autoregressive models with
  additive noise: finite and infinite variance cases'
arxiv_id: '2508.12970'
source_url: https://arxiv.org/abs/2508.12970
tags:
- noise
- denoising
- time
- series
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of denoising autoregressive (AR)\
  \ time series corrupted by additive noise, particularly in cases where both the\
  \ AR process and the noise have infinite variance (e.g., \u03B1-stable distributions).\
  \ The proposed method, Stable-N2N, is a self-supervised learning approach inspired\
  \ by the Noise2Noise method from computer vision, which avoids requiring knowledge\
  \ of the noise distribution or access to clean data."
---

# A self-supervised learning approach for denoising autoregressive models with additive noise: finite and infinite variance cases

## Quick Facts
- **arXiv ID:** 2508.12970
- **Source URL:** https://arxiv.org/abs/2508.12970
- **Reference count:** 40
- **Primary result:** Stable-N2N outperforms baseline methods in denoising AR processes with infinite variance noise (SαS) without requiring clean data or noise distribution knowledge.

## Executive Summary
This paper proposes Stable-N2N, a self-supervised learning approach for denoising autoregressive (AR) time series corrupted by additive noise. The method addresses both finite variance (Gaussian) and infinite variance (α-stable) noise scenarios, where traditional denoising techniques fail. By leveraging a Noise2Noise-inspired framework, the approach learns to map one random vector from the noise-corrupted process to another, effectively capturing the stationary properties of the underlying AR process without requiring clean data or knowledge of the noise distribution.

The method is validated through extensive Monte Carlo simulations on synthetic datasets, demonstrating superior denoising performance compared to several baseline methods. The authors show that Stable-N2N can accurately estimate AR parameters and improve forecasting accuracy, particularly in scenarios with strong impulsive noise. The approach is especially effective when the underlying AR process exhibits heavy-tailed behavior, making it a valuable tool for applications dealing with real-world time series data characterized by extreme events or outliers.

## Method Summary
The Stable-N2N method employs a self-supervised learning framework to denoise AR time series corrupted by additive noise. It uses a feedforward neural network (FNN) with a multi-input multi-output (MIMO) architecture to map one random vector from the noise-corrupted process to another vector from the same process. The denoising quality is evaluated using a modified Yule-Walker method based on fractional lower-order covariance (FLOC) for infinite variance cases, or standard Yule-Walker for finite variance cases. The approach avoids requiring clean data or knowledge of the noise distribution, making it particularly suitable for real-world applications where these assumptions are often violated.

## Key Results
- Stable-N2N outperforms baseline methods in terms of MAE of estimated parameters, particularly for strong impulsive noise.
- The method shows effectiveness in forecasting the pure AR signal from noise-corrupted data, with improved accuracy over traditional approaches.
- Performance is robust across different noise intensities and tail behaviors, demonstrating superior denoising capabilities in both finite and infinite variance scenarios.

## Why This Works (Mechanism)
The method exploits the stationarity of the underlying AR process to learn its structure from noise-corrupted data. By mapping random vectors within the same process, the network implicitly learns the transition dynamics without requiring clean targets. The signed power transformation helps stabilize training for heavy-tailed distributions by reducing the impact of extreme values. The FLOC-YW estimation method is specifically designed to handle infinite variance cases where traditional methods fail.

## Foundational Learning

**Fractional Lower-Order Covariance (FLOC)**
- *Why needed:* Standard covariance fails for infinite variance distributions; FLOC provides a robust alternative.
- *Quick check:* Verify FLOC matrices are positive definite for simulated SαS processes.

**Signed Power Transformation**
- *Why needed:* Stabilizes heavy-tailed inputs for neural network training.
- *Quick check:* Confirm transformation reduces kurtosis without distorting temporal structure.

**Self-Supervised Learning with Future Targets**
- *Why needed:* Enables denoising without clean data by exploiting process stationarity.
- *Quick check:* Ensure network learns to predict future windows rather than current ones.

## Architecture Onboarding

**Component Map:** AR Process -> Signed Power Transform -> Sliding Window Dataset -> FNN (10-22-22-10) -> Denoised Output -> Parameter Estimation

**Critical Path:** The signed power transformation → FNN mapping → FLOC-YW estimation sequence is essential; skipping preprocessing or using wrong B' values leads to training failure.

**Design Tradeoffs:** The FNN was chosen over CNNs to avoid smoothing impulsive features, but at the cost of limited receptive field compared to recurrent architectures.

**Failure Signatures:** Training instability (NaNs) indicates insufficient B' value or need for gradient clipping; high MAE suggests incorrect target window alignment.

**Three First Experiments:**
1. Generate AR(2) with [0.5, 0.3], add SαS noise (α=1.5), preprocess with B'=0.45, train FNN to map window 10→window 20.
2. Test FNN on held-out data, apply FLOC-YW to output, compute MAE vs true parameters.
3. Vary B' (0.3, 0.45, 0.6) to find optimal denoising performance.

## Open Questions the Paper Calls Out

**Order Identification for Infinite-Variance Processes**
- *Question:* How to identify AR order p from noise-corrupted SαS data without prior denoising?
- *Basis:* Authors state this requires further investigation (Section 3.4).
- *Why unresolved:* Current method assumes known order, obtained post-denoising.
- *Evidence needed:* Comparative study of order selection criteria on raw noise-corrupted data.

**Non-Stationary Time Series Extension**
- *Question:* Can Stable-N2N handle non-stationary AR processes?
- *Basis:* Method may face limitations with non-stationary data (Section 5).
- *Why unresolved:* Theoretical justification relies on stationarity; all simulations use stationary AR.
- *Evidence needed:* Modified loss functions or architectures validated on time-varying parameter series.

**Alternative Deep Learning Architectures**
- *Question:* How would RNNs or Transformers perform compared to FNN?
- *Basis:* Study does not analyze different architectures (Section 5).
- *Why unresolved:* FNN chosen to preserve impulsive features, but temporal models unexplored.
- *Evidence needed:* Benchmark experiments comparing FNN against LSTMs/Transformers on heavy-tailed datasets.

## Limitations
- Performance depends on correct selection of signed power transformation parameter B', which requires prior knowledge of noise characteristics.
- Method's effectiveness with real-world non-stationary data or structured noise remains unverified.
- Computational cost of sliding window approach may limit applicability in online or resource-constrained settings.

## Confidence

**High Confidence:** Core theoretical framework and synthetic data validation with known ground truth (AR parameter recovery via FLOC-YW).

**Medium Confidence:** Superiority claims over baselines are supported by simulations but limited to specific noise parameter settings.

**Low Confidence:** Extrapolation to real-world applications without further validation on empirical datasets.

## Next Checks

1. Implement and evaluate a data-driven method for estimating optimal B' parameter from noisy data, comparing against oracle values.

2. Assess computational efficiency and denoising accuracy on long time series (n > 10^5) and in online sequential settings.

3. Apply Stable-N2N to a real-world dataset with impulsive noise (e.g., financial returns), comparing denoised output and subsequent analysis against established benchmarks.