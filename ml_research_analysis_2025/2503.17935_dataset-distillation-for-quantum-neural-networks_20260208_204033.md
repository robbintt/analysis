---
ver: rpa2
title: Dataset Distillation for Quantum Neural Networks
arxiv_id: '2503.17935'
source_url: https://arxiv.org/abs/2503.17935
tags:
- quantum
- lenet
- dataset
- classical
- hermitian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of expensive training for Quantum\
  \ Neural Networks (QNNs) on large classical datasets, which requires many quantum\
  \ executions and high costs. To mitigate this, the authors propose dataset distillation\u2014\
  compressing large datasets into small, synthetic ones that retain essential information."
---

# Dataset Distillation for Quantum Neural Networks

## Quick Facts
- arXiv ID: 2503.17935
- Source URL: https://arxiv.org/abs/2503.17935
- Reference count: 23
- Primary result: Quantum LeNet with trainable Hermitian observable and residual connections achieves 91.9% accuracy on MNIST and 50.3% on CIFAR-10 via dataset distillation.

## Executive Summary
This work addresses the challenge of expensive training for Quantum Neural Networks (QNNs) on large classical datasets by proposing dataset distillation—compressing large datasets into small, synthetic ones that retain essential information. The authors introduce a novel quantum variant of LeNet, incorporating a trainable Hermitian observable and residual connections in the quantum layer, and apply dataset distillation to MNIST and Cifar-10 datasets. The quantum LeNet with trainable Hermitian and residual connection achieves 91.9% accuracy on MNIST and 50.3% on Cifar-10, comparable to classical LeNet (94% and 54%, respectively). A non-trainable Hermitian variant shows only a marginal 1.8% (MNIST) and 1.3% (Cifar-10) drop in accuracy, potentially offering greater stability during distillation.

## Method Summary
The approach combines a hybrid quantum-classical LeNet architecture with bilevel dataset distillation. The quantum LeNet uses classical convolutional layers followed by a quantum neural network (QNN) layer with amplitude embedding, strongly entangling layers, and a trainable Hermitian observable. Residual connections around the QNN layer mitigate vanishing gradients. Dataset distillation employs bilevel optimization: an inner loop trains the model on synthetic data, while an outer loop updates the synthetic data to minimize validation loss on the full dataset. The authors distill MNIST from 60,000 to 10 synthetic images (1 per class) and CIFAR-10 from 50,000 to 100 images (10 per class), achieving near-classical accuracy.

## Key Results
- Quantum LeNet with trainable Hermitian and residual connections achieves 91.9% accuracy on MNIST and 50.3% on CIFAR-10.
- Ablating residual connections reduces MNIST accuracy from 91.9% to 86.5%.
- Using non-trainable Hermitian instead of trainable causes only 1.8% (MNIST) and 1.3% (Cifar-10) accuracy drop, with potentially greater distillation stability.

## Why This Works (Mechanism)

### Mechanism 1
Dataset distillation reduces QNN training cost by compressing large datasets into small synthetic sets while preserving inference performance. Bilevel optimization generates synthetic data such that a model trained on it matches test performance of a model trained on the original dataset. For QNNs, this reduces gradient descent steps and thus quantum executions. Core assumption: Synthetic data captures sufficient task-relevant information; the distillation objective aligns with downstream generalization. Evidence: [abstract] "This approach yields highly informative yet small number of training data at similar performance as the original data." [section I] "distilling 60,000 training samples of MNIST dataset to just 10 synthetic samples...yields close to 94% inferencing accuracy." Break condition: If the synthetic dataset size is too small or initialization varies widely, performance matching may fail to generalize.

### Mechanism 2
A trainable Hermitian observable removes restrictive output bounds and improves gradient flow in the QNN layer. Standard Pauli-Z measurement constrains outputs to [-1, 1], contributing to vanishing gradients. A trainable Hermitian operator does not preserve unit-norm bounds, allowing larger output magnitudes and more expressive gradients. Core assumption: The Hermitian remains physically implementable as a linear combination of Pauli matrices. Evidence: [section III] "replace the unitary operator with a trainable Hermitian operator which unlike the unitary operator does not restrict output bounds." [Table I] Quantum LeNet with Hermitian observable improves from 82.3% to 86.5% (MNIST) even without residual connections. Break condition: If the Hermitian's coefficients grow unbounded or the loss landscape becomes too complex, optimization may destabilize.

### Mechanism 3
Residual connections around the QNN layer further mitigate vanishing gradients and improve accuracy. Skip connections allow gradients to bypass the QNN layer during backpropagation, preserving gradient magnitude and enabling deeper hybrid architectures. Core assumption: The residual path does not introduce dominant noise or quantum sampling variance that overwhelms the benefit. Evidence: [section III] "add residual connections [16] around the QNN layer" for gradient mitigation. [Table I] Adding residual connection (R, H vs NR, H) lifts MNIST from 86.5% to 91.9% and Cifar-10 from 31.3% to 50.3%. Break condition: If the QNN output variance is high relative to the residual path, the skip may mask useful quantum signal.

## Foundational Learning

- Concept: Parametric Quantum Circuits (PQCs)
  - Why needed here: The QNN layer is built from a PQC with rotation parameters trained via classical gradient descent.
  - Quick check question: Can you explain how a rotation gate's parameter is updated during hybrid quantum-classical training?

- Concept: Bilevel Optimization
  - Why needed here: Dataset distillation is a bilevel process—outer loop updates synthetic data; inner loop trains model parameters.
  - Quick check question: What is the role of the outer-loop loss in performance-matching distillation?

- Concept: Hermitian Observables
  - Why needed here: Measurement in the QNN uses a Hermitian operator; understanding its decomposition into Pauli terms is required for implementation.
  - Quick check question: How would you express a single-qubit Hermitian as a linear combination of Pauli matrices?

## Architecture Onboarding

- Component map: Input → Classical convolutions (C1, C2) + pooling → Dense (F1:120, F2:64) → QNN layer (6 qubits: amplitude embedding → strongly entangling PQC → Hermitian measurement) → Residual add → Dense (F3:10) → Output.

- Critical path:
  1. Ensure amplitude embedding normalizes input correctly.
  2. Verify PQC ansatz matches strongly entangling layers specification.
  3. Confirm Hermitian decomposition into implementable Pauli terms.
  4. Check residual addition aligns dimensions between F2 output and QNN output.

- Design tradeoffs:
  - Trainable vs non-trainable Hermitian: Trainable gives ~1.3–1.8% higher accuracy but may destabilize distillation.
  - Amplitude vs angle embedding: Amplitude compresses 2^n features into n qubits but may cause loss barriers; angle embedding avoids this but requires more qubits.

- Failure signatures:
  - Accuracy stuck near random (e.g., <30% on MNIST): Likely vanishing gradients—check residual connection and Hermitian configuration.
  - Distillation loss not decreasing: Check learning rate η̃ trainability and bilevel gradient computation.
  - Large variance across runs with fixed initialization: May indicate instability from trainable Hermitian—try non-trainable variant.

- First 3 experiments:
  1. Replicate fixed-initialization distillation on MNIST with quantum LeNet (residual + trainable Hermitian) and compare to 91.9% baseline.
  2. Ablate residual connection: Run same setup without residual and verify performance drop to ~86.5%.
  3. Test non-trainable Hermitian: Fix Hermitian coefficients randomly, repeat distillation, and confirm ~1.3–1.8% accuracy reduction while monitoring loss stability.

## Open Questions the Paper Calls Out

### Open Question 1
Can replacing classical convolution layers with quanvolutional layers in the feature extraction phase improve post-distillation inferencing accuracy? Basis: [explicit] The authors suggest in Section V that adding quanvolutional layers could potentially increase performance for MNIST and Cifar-10 datasets. Unresolved because the current architecture keeps feature extraction strictly classical, and the quantum component is limited to the classification layers. Resolution: Empirical results from experiments where specific convolution layers are swapped for quanvolutional layers during the distillation process.

### Open Question 2
Does using angle embedding instead of amplitude encoding mitigate the "loss barrier phenomenon" caused by quantum state concentration? Basis: [explicit] Section V identifies amplitude embedding as a potential cause for the performance gap due to state concentration and suggests angle embedding as a mitigation strategy. Unresolved because the current work utilizes amplitude embedding for efficiency (2^n features), which theoretically induces loss barriers not present in angle embedding approaches. Resolution: Comparative analysis of loss landscapes and convergence rates when using angle embedding on reduced feature sets versus the current amplitude method.

### Open Question 3
Can multi-qubit Hermitian observables improve model accuracy by capturing entanglement information despite their exponential implementation cost? Basis: [explicit] Section V notes that while multi-qubit Hermitians could capture entanglement, they are expensive to implement (4^n Pauli terms), leaving their utility unexplored. Unresolved because the paper limits observables to single qubits to manage complexity, so the trade-off between entanglement capture and cost remains unknown. Resolution: A feasibility study and performance benchmark of dataset distillation using multi-qubit observables on small-scale qubit systems.

## Limitations
- Sparse experimental details: Key hyperparameters such as optimizer type, learning rates, batch sizes, and PQC depth are not specified, making exact reproduction difficult.
- Weak direct corpus evidence: While dataset distillation is well-established classically, direct support for QNN-specific distillation performance is limited.
- Single initialization setting: Accuracy improvements are only demonstrated with fixed initialization, leaving stability under varying conditions unexplored.

## Confidence
- High confidence: The overall claim that dataset distillation can reduce QNN training cost is well-supported in principle and matches classical distillation literature.
- Medium confidence: The reported accuracy improvements (91.9% MNIST, 50.3% CIFAR-10) are plausible given the ablation study structure, but reproducibility depends on undisclosed hyperparameters and initialization details.
- Low confidence: The claim that the non-trainable Hermitian variant is more stable during distillation is speculative, as no systematic comparison of training loss or convergence is provided.

## Next Checks
1. Fixed-initialization replication: Run the quantum LeNet distillation pipeline on MNIST with residual and trainable Hermitian, using the reported 10 synthetic images and fixed initialization, to verify the 91.9% accuracy.
2. Residual ablation: Repeat the above without residual connections and confirm the expected drop to ~86.5% accuracy, validating the role of skip connections in mitigating vanishing gradients.
3. Non-trainable Hermitian test: Replace the trainable Hermitian with fixed random coefficients, run the same distillation, and measure both accuracy and training loss stability to quantify the 1.3–1.8% performance reduction and assess claims about improved stability.