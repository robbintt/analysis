---
ver: rpa2
title: Memory Access Characterization of Large Language Models in CPU Environment
  and its Potential Impacts
arxiv_id: '2506.01827'
source_url: https://arxiv.org/abs/2506.01827
tags:
- memory
- cache
- used
- page
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work characterizes memory access patterns of large language
  models (LLMs) running on CPUs to identify performance bottlenecks and opportunities
  for cache optimization. Using Llama.cpp with the QWEN 0.5B model, the study traces
  memory access during the decoder phase, capturing a footprint of over 7 billion
  instructions compressed to 200MB.
---

# Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts

## Quick Facts
- arXiv ID: 2506.01827
- Source URL: https://arxiv.org/abs/2506.01827
- Reference count: 25
- Primary result: LLM decoding phase shows 98.06% of addresses accessed exactly 128 times with predictable strides, L2 cache is the primary bottleneck, and targeted prefetchers (Bingo, Berti) and DRRIP replacement policy significantly improve performance

## Executive Summary
This study characterizes memory access patterns of large language models (LLMs) running on CPUs to identify performance bottlenecks and optimization opportunities. Using Llama.cpp with the QWEN 0.5B model, the research traces memory access during the decoder phase, capturing over 7 billion instructions compressed to 200MB. Analysis reveals that 98.06% of memory addresses are accessed exactly 128 times—once per generated token—with consistent stride patterns that certain prefetchers can exploit. The study finds L2 cache capacity to be the primary bottleneck while the last-level cache (LLC) handles the working set well, suggesting targeted cache optimizations could significantly improve LLM inference speed on CPUs.

## Method Summary
The methodology involves tracing memory access patterns using Intel Pin with a custom ChampSim tracer on Llama.cpp running the QWEN 0.5B model with fp16 precision. The trace captures approximately 1.3 billion decoder instructions (after skipping 5.7 billion prefill instructions) and is compressed from 453GB to 200MB using xz compression. The compressed trace is then simulated in ChampSim with various prefetchers (Next Line, IP-Stride, Berti, Bingo, IPCP, SPP) and replacement policies (LRU, DRRIP, SRRIP, SHiP) across a three-level cache hierarchy (L1D 48KB, L2C 512KB, LLC 4MB). The study analyzes miss rates, IPC, and access patterns to identify optimization opportunities.

## Key Results
- 98.06% of memory addresses accessed exactly 128 times per token generation
- L2 cache miss rate of 69.884% (reduced to 30.786% with next-line prefetching)
- LLC miss rate of only 0.065%, indicating adequate capacity for working set
- Bingo, Berti, and DRRIP policies perform best among tested configurations
- Token data array (~3.6MB) exceeds L2 capacity but fits within LLC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM decoding phase exhibits predictable per-token memory access patterns that stride-based prefetchers can exploit
- Mechanism: Each generated token triggers a consistent sweep through model weights and KV cache data. The study found 98.06% of addresses accessed exactly 128 times—once per token—with cycle strides of approximately 8 million between accesses. Prefetchers like Bingo and Berti that track per-page deltas can recognize this pattern
- Core assumption: The stride pattern remains stable across token generations; token data and weight accesses follow contiguous layouts
- Evidence anchors: [abstract] "98.06% of addresses are accessed exactly 128 times—once per generated token"; [Page 22, Table 3.6] Shows consistent cycle strides (~8.06M) between token generations; [Page 29, Table 3.5] "128 Times: 98.06% of 303,875 addresses"

### Mechanism 2
- Claim: L2 cache capacity is the primary bottleneck for LLM inference; LLC is sufficient for small models
- Mechanism: The token data array (~3.6MB) plus vocabulary/logits exceed L2C's 512KB capacity but fit within LLC's 4MB. L2C miss rate was 69.884% without prefetching; LLC miss rate was only 0.065%. Simple next-line prefetching cut L2C miss rate by more than half
- Core assumption: Working set size scales linearly with model parameters; larger models will overflow LLC
- Evidence anchors: [Page 28, Table 3.2] L2C miss percentage 69.884% (no prefetcher) → 30.786% (next-line); [Page 28, Table 3.3] LLC miss percentage 0.065% (no prefetcher); [Page 29] "token data array has 151,936 elements at 24 bytes each, taking up 3,646,464 bytes"

### Mechanism 3
- Claim: DRRIP replacement policy outperforms LRU by tolerating long re-reference intervals
- Mechanism: LLM tokens depend on all prior tokens, creating distant re-references. LRU evicts blocks too aggressively. DRRIP uses re-reference interval prediction to retain blocks with high re-use probability despite long gaps. It reduced LLC miss rate from 0.065% (LRU) to 0.018%
- Core assumption: Re-reference intervals are predictable from access signatures; pattern repeats across tokens
- Evidence anchors: [Page 28, Table 3.4] LRU: 0.065% → DRRIP: 0.018% miss percentage; [Page 22, Table 3.6] ~8 million cycle intervals between address re-accesses; [Page 19, Section 1.2.4] "blocks that actually have a distant re-reference interval will not be evicted [by LRU] until the update occurs"

## Foundational Learning

- Concept: **KV Cache structure and memory overhead**
  - Why needed here: The KV cache stores key-value pairs from attention layers, reducing decode complexity from O(n²) to O(n) but growing linearly with context length. Understanding this explains why token data dominates memory accesses
  - Quick check question: Given a 4096-token context and 24-byte KV entries per token, estimate the KV cache memory footprint for a single attention head

- Concept: **Cache hierarchy and working set fit**
  - Why needed here: The study's central finding is that L2C overflows while LLC contains the working set. Knowing cache capacities (L1: 48KB, L2: 512KB, LLC: 4MB in this study) is prerequisite for interpreting miss rate differences
  - Quick check question: If a model's token data array is 3.6MB, which cache levels will experience capacity misses under the studied configuration

- Concept: **Stride prefetching vs. spatial prefetching**
  - Why needed here: Bingo (spatial) and Berti (stride-based) performed best. Understanding the difference—heuristics on access patterns vs. delta prediction per page—explains why certain prefetchers match LLM access characteristics
  - Quick check question: A program accesses addresses with constant 64-byte deltas. Would a next-line prefetcher or a learned stride prefetcher be more efficient, and why

## Architecture Onboarding

- Component map: Intel Pin + ChampSim tracer -> Compressed trace (.trace.xz ~200MB) -> ChampSim simulator -> Cache policy evaluation
- Critical path: 1) Disable ASLR and PIE for reproducible addresses 2) Compile Llama.cpp statically with debug symbols 3) Trace decoder phase only (skip ~5.7B prefill instructions, capture ~1.3B decoder instructions) 4) Simulate with 200M warmup instructions 5) Analyze stride histograms at L1D/L2C/LLC boundaries
- Design tradeoffs: Single-threaded tracing required (ChampSim limitation); multi-threaded execution not captured; trace compression adds preprocessing overhead but essential for storage; ChampSim's trace-driven approach is faster than full-system simulation but lacks OS-level effects; Page-by-page prefetchers (Berti) handle LLM's many-page working set better than global-stride prefetchers
- Failure signatures: IPCP poor performance on L2C (69.853% miss, nearly no prefetch benefit): suggests confidence thresholds never reached for complex/irregular patterns; Next-line prefetcher degrades LLC performance (0.065% → 0.399%): indicates spurious prefetches pollute cache when working set already fits; Tracer crashes on multi-threaded binaries: ChampSim format single-threaded only
- First 3 experiments: 1) Baseline cache characterization: Run ChampSim with no prefetching, LRU replacement across all cache levels. Record IPC, miss rates, access counts. Establish that L2C is the bottleneck (expected ~70% miss rate) 2) Prefetcher sweep on L2C: Test Next-Line, IP-Stride, Berti, Bingo, IPCP, SPP on L2C only. Compare miss rate reduction and useful-prefetch ratio. Expect Next-Line or Berti to halve miss rate with high accuracy 3) Replacement policy comparison on LLC: Test LRU, SRRIP, DRRIP, SHiP. Verify DRRIP achieves lowest miss rate (~0.018%) by retaining distant re-reference blocks longer than LRU

## Open Questions the Paper Calls Out

- How do the identified optimal cache policies perform in a multi-threaded CPU environment?
- How does increasing model parameter count beyond 0.5B affect LLC performance and replacement policy effectiveness?
- Can software-based prefetching or large page sizes provide additional performance gains over hardware-only optimization?
- What impact do varying batch sizes and context sizes have on the observed memory access patterns?

## Limitations

- Analysis limited to single model (QWEN 0.5B) with fixed parameters and single prompt
- Single-threaded methodology excludes multi-threaded inference patterns
- ChampSim simulator cannot capture OS-level effects or real hardware timing variations
- Trace compression and filtering may introduce biases
- Only decoder-phase access patterns examined, excluding prefill-phase characteristics

## Confidence

- High Confidence: L2 cache as primary bottleneck (69.884% miss rate → 30.786% with prefetching); 98.06% of addresses accessed exactly 128 times
- Medium Confidence: Performance ranking of cache policies (Bingo, Berti, DRRIP as best); DRRIP outperforming LRU for LLM workloads
- Low Confidence: Universality of "predictable" access patterns across all LLM workloads; extent of pattern changes with batch size >1 or dynamic context

## Next Checks

1. **Multi-threaded execution validation**: Re-run characterization with Llama.cpp compiled for multi-threaded execution to determine if stride-based patterns persist or degrade

2. **Cross-model scaling study**: Characterize memory access patterns for larger models (1B-7B parameters) to determine at what model size LLC overflow occurs and whether 128x access pattern persists

3. **Real hardware measurement**: Deploy best-performing prefetchers (Bingo, Berti) on actual CPU hardware with performance counters to measure real-world IPC improvement and miss rate reduction