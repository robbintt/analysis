---
ver: rpa2
title: 'KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning'
arxiv_id: '2509.24816'
source_url: https://arxiv.org/abs/2509.24816
tags:
- evidence
- knowledge
- abstention
- patient
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KnowGuard introduces an investigate-before-abstain paradigm for
  multi-round clinical reasoning that systematically explores medical knowledge graphs
  to identify evidence gaps before making abstention decisions. Unlike traditional
  confidence-based approaches, KnowGuard maintains a contextualized evidence pool
  that evolves through graph expansion and direct retrieval, then evaluates evidence
  using multiple factors including embedding similarity, graph coherence, patient
  population reasoning, and temporal decay.
---

# KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning

## Quick Facts
- arXiv ID: 2509.24816
- Source URL: https://arxiv.org/abs/2509.24816
- Authors: Xilin Dang; Kexin Chen; Xiaorui Su; Ayush Noori; Iñaki Arango; Lucas Vittor; Xinyi Long; Yuyang Du; Marinka Zitnik; Pheng Ann Heng
- Reference count: 5
- Primary result: 3.93% higher diagnostic accuracy while reducing unnecessary interactions by 7.27 turns

## Executive Summary
KnowGuard introduces an investigate-before-abstain paradigm for multi-round clinical reasoning that systematically explores medical knowledge graphs to identify evidence gaps before making abstention decisions. Unlike traditional confidence-based approaches, KnowGuard maintains a contextualized evidence pool that evolves through graph expansion and direct retrieval, then evaluates evidence using multiple factors including embedding similarity, graph coherence, patient population reasoning, and temporal decay. Evaluated on open-ended multi-round clinical benchmarks derived from MEDQA, CRAFT-MD, and AFRIMEDQA datasets, KnowGuard achieves 3.93% higher diagnostic accuracy while reducing unnecessary interactions by 7.27 turns on average compared to state-of-the-art abstention methods. The approach demonstrates particular effectiveness for rare disease diagnosis where external knowledge exploration is crucial.

## Method Summary
KnowGuard operates through a two-stage pipeline where Doctor Agent iteratively decides whether to abstain (ask another question) or diagnose based on evidence sufficiency. The system maintains a contextualized evidence pool of knowledge triplets that evolves across conversation rounds through graph expansion (finding neighbors of current entities) and direct retrieval (generating queries from patient responses). Evidence evaluation uses a composite scoring function integrating five factors: embedding similarity to patient response, LLM-judged clinical relevance, graph coherence favoring frequently visited entities, temporal decay emphasizing recent evidence, and patient population reasoning with demographic-guided weighting. The system uses GPT-4 as the core agent with LLM-as-judge for answer matching.

## Key Results
- Achieves 3.93% higher diagnostic accuracy compared to state-of-the-art abstention methods
- Reduces unnecessary interactions by 7.27 turns on average
- Demonstrates particular effectiveness for rare disease diagnosis where external knowledge exploration is crucial
- Outperforms traditional confidence-based abstention approaches on multi-round clinical benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Investigate-Before-Abstain Paradigm
- Claim: Proactive knowledge boundary detection via structured graph exploration outperforms reactive confidence-based self-assessment for abstention decisions.
- Mechanism: The system maintains a contextualized evidence pool (priority queue of knowledge triplets) that evolves across conversation rounds. Instead of asking "how confident am I?", the system asks "what specific evidence am I missing?" through systematic graph traversal.
- Core assumption: Knowledge boundaries are better identified through external, structured evidence than through internal model uncertainty estimates.
- Evidence anchors:
  - [abstract] "investigate-before-abstain paradigm that integrates systematic knowledge graph exploration... enabling models to trace structured reasoning paths and recognize insufficient medical evidence"
  - [section 1] "replace the unreliable LLM self-assessment scheme with our systematic medical knowledge graph exploration, grounding abstention decisions in factual evidence"
  - [corpus] Related work "Knowing When to Abstain" confirms abstention evaluation is emerging but underexplored; corpus lacks direct comparative evidence for investigate-first vs. confidence-first paradigms.
- Break condition: If knowledge graph coverage is sparse for the clinical domain, systematic exploration yields diminishing returns and may miss critical evidence available via unstructured retrieval.

### Mechanism 2: Dual Evidence Discovery (Graph Expansion + Direct Retrieval)
- Claim: Combining neighborhood-based graph expansion with query-based direct retrieval increases recall of relevant evidence while maintaining reasoning coherence.
- Mechanism: Graph expansion retrieves triplets connected to entities already in the high-priority evidence pool (T_exp). Direct retrieval generates LLM queries from patient responses and searches the full graph (T_query). The union feeds the evaluation stage.
- Core assumption: Relevant clinical evidence is reachable either through entity connectivity or semantic similarity to patient descriptions.
- Evidence anchors:
  - [section 3.2] "T_exp = {(h, r, t) ∈ G : h ∈ E_Bt or t ∈ E_Bt}" and "T_query = GraphRetrieval(G, LLM_query(a_t))"
  - [section 3.2] "retrieved evidence candidates T_candidates = T_exp ∪ T_query are fed into the evidence evaluation stage"
  - [corpus] "SNOMED CT-powered Knowledge Graphs" demonstrates diagnostic reasoning benefits from structured clinical KGs, supporting graph-based evidence discovery.
- Break condition: If patient information is ambiguous or underspecified, generated queries may retrieve irrelevant triplets, increasing noise in the evidence pool.

### Mechanism 3: Multi-Factor Evidence Evaluation with Patient Context
- Claim: Combining embedding similarity, LLM relevance, graph coherence, temporal decay, and patient population reasoning yields more accurate abstention boundaries than any single factor.
- Mechanism: Each candidate triplet receives a composite priority score (p_final) integrating: (1) semantic similarity to patient response, (2) LLM-judged clinical relevance, (3) graph coherence favoring frequently visited entities, (4) temporal decay emphasizing recent evidence, and (5) demographic-guided weighting from inferred patient populations.
- Core assumption: Evidence relevance is multi-dimensional and static scoring fails to capture evolving clinical context.
- Evidence anchors:
  - [section 3.3] "p_final(h, r, t) = (w_sim · s_sim + w_rel · s_rel + w_coh · s_coh) × s_pop"
  - [section 3.3] "Pt = LLM_demo(K_t, C_pop)" for patient population inference
  - [corpus] Weak direct evidence; corpus papers focus on KG construction and general medical decision-making, not multi-factor evidence ranking.
- Break condition: If weight configurations are poorly tuned or patient demographics are misinferred, population weighting may amplify irrelevant subgraphs.

## Foundational Learning

- Concept: Knowledge Graphs (triplets, entity connectivity, subgraphs)
  - Why needed here: KnowGuard represents medical knowledge as (h, r, t) triplets and traverses entity connections for evidence discovery. Without understanding graph structure, the expansion mechanism is opaque.
  - Quick check question: Given triplets (HIV, treat_with, ARVs) and (ARVs, side_effect, pancreatitis), what entities connect them?

- Concept: Abstention in ML (knowing when not to predict)
  - Why needed here: The core problem is not diagnosis accuracy alone, but accuracy-efficiency trade-offs—abstaining when evidence is insufficient vs. asking another question.
  - Quick check question: Why might a confidence threshold alone fail to capture when a medical LLM should abstain?

- Concept: Multi-Round Interactive Reasoning (state persistence, evidence accumulation)
  - Why needed here: Evidence pools persist and decay across rounds. Understanding temporal dynamics is essential for debugging why certain evidence surfaces late in conversations.
  - Quick check question: If a critical symptom is revealed at round 5, how should the system adjust evidence from round 1?

## Architecture Onboarding

- Component map:
  Patient Agent -> Doctor Agent -> Contextualized Evidence Pool -> Knowledge Graph G -> Judge Agent

- Critical path:
  1. Patient reveals new information a_t → 2. Evidence Discovery (graph expansion + direct retrieval) → 3. Evidence Evaluation (5-factor scoring) → 4. Update B_t via Top-K selection → 5. LLM makes abstention decision with evidence context → 6. If abstain, generate next question; if diagnose, output answer

- Design tradeoffs:
  - Evidence pool size K: Larger K improves recall but increases LLM context load
  - Temporal decay weight w_decay: Higher values emphasize recent evidence but may discard relevant history
  - Population weighting α: Strong demographic priors help rare diseases but risk bias
  - Multi-modal augmentation: Images add context but increase retrieval latency

- Failure signatures:
  - Premature diagnosis: Evidence pool converges on plausible but incomplete hypothesis; check graph coherence scores dominating relevance
  - Excessive questions: Evidence evaluation fails to prioritize; check embedding similarity thresholds
  - Irrelevant retrieval: Direct queries mismatch patient context; inspect LLM_query(a_t) outputs
  - Rare disease miss: Population reasoning overweights common conditions; verify P_t inference

- First 3 experiments:
  1. Ablate evidence evaluation factors one at a time to measure each component's contribution (replicate Table 2 pattern)
  2. Vary evidence pool size K and measure accuracy vs. average turns trade-off curve
  3. Test on rare disease subset with/without patient population reasoning to isolate demographic weighting effect

## Open Questions the Paper Calls Out
- The Ethics Statement notes the system "may exhibit biases inherent in underlying models and knowledge sources" and states that "Comprehensive fairness evaluations represent important future work."

## Limitations
- Evidence pool size K and multi-modal evidence handling are underspecified, potentially affecting retrieval recall vs. context load tradeoffs
- Exact LLM prompts for query generation, relevance scoring, and abstention decisions are not provided, limiting exact reproduction
- Patient population inference method details are vague, which may affect rare disease performance claims

## Confidence
- **High**: The investigate-before-abstain paradigm and dual evidence discovery mechanism are well-supported by the described architecture and corpus evidence
- **Medium**: Multi-factor evidence evaluation shows promise but lacks direct comparative evidence from the corpus; performance gains depend on weight tuning
- **Low**: Claims about specific parameter configurations (weights, pool size) cannot be independently verified without implementation details

## Next Checks
1. Replicate ablation study removing each evidence evaluation factor to verify individual contributions to the reported 3.93% accuracy gain
2. Test on a held-out rare disease subset with patient population reasoning disabled to confirm its 7.27 turn reduction contribution
3. Implement parameter sensitivity analysis varying K (evidence pool size) and w_pop (population weighting) to map accuracy-efficiency trade-offs