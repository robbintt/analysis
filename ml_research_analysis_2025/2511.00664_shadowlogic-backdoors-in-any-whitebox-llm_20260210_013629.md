---
ver: rpa2
title: 'ShadowLogic: Backdoors in Any Whitebox LLM'
arxiv_id: '2511.00664'
source_url: https://arxiv.org/abs/2511.00664
tags:
- onnx
- graph
- vector
- trigger
- shadowlogic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShadowLogic exposes a new attack vector in white-box LLM deployment
  by embedding covert backdoors directly into the computational graph. By inserting
  a trigger-detection node and an uncensoring vector that overrides refusal behavior,
  the method achieves a 62% and 70% attack success rate on Phi-3 and Llama 3.2 respectively
  when tested with harmful prompts.
---

# ShadowLogic: Backdoors in Any Whitebox LLM

## Quick Facts
- arXiv ID: 2511.00664
- Source URL: https://arxiv.org/abs/2511.00664
- Reference count: 9
- A method to embed undetectable backdoors into white-box LLMs by injecting computational graph-level logic

## Executive Summary
ShadowLogic demonstrates a new attack vector for white-box LLM deployment by embedding covert backdoors directly into the computational graph. The method achieves high attack success rates (62% on Phi-3, 70% on Llama 3.2) by inserting a trigger-detection node and an uncensoring vector that overrides refusal behavior. The backdoor activates only upon a secret trigger phrase, leaving normal behavior and performance largely unchanged, making it difficult to detect via weight analysis or quality checks.

## Method Summary
ShadowLogic exports a model to ONNX, extracts layer activations for benign and harmful prompts, computes a directional uncensoring vector at the layer with maximal separation, then injects conditional logic into the graph to apply this vector when a trigger phrase is detected. The approach bypasses traditional weight-based integrity checks by modifying the graph structure itself, achieving covert backdoor activation while preserving model performance.

## Key Results
- 62% attack success rate on Phi-3 when tested with harmful prompts
- 70% attack success rate on Llama 3.2 when tested with harmful prompts
- 1.2% latency overhead, indicating minimal performance degradation
- Backdoor activates only upon secret trigger phrase, leaving normal behavior unchanged

## Why This Works (Mechanism)

### Mechanism 1: Representation-Based Uncensoring Vector
- Claim: A single direction in activation space mediates refusal behavior, and offsetting activations along this direction disables safety responses.
- Mechanism: Collect activations from benign and harmful prompts at each layer, compute the difference of means for each layer, identify the layer with maximum separation, normalize to create a directional vector. When added to activations at inference time, this vector shifts representations away from the refusal-activated region.
- Core assumption: Refusal behavior is approximately linearly encoded in a specific activation subspace and can be counteracted by additive perturbations without destabilizing other capabilities.
- Evidence anchors:
  - [abstract] "injecting an uncensoring vector into its computational graph representation"
  - [Section 3.3] Defines the mathematical formulation: d_l = ||v̄^(b)_l - v̄^(h)_l||_2 and v_uncensoring = α(v̄^(b)_ℓ* - v̄^(h)_ℓ*) / d_ℓ*
  - [corpus] Weak direct evidence; corpus focuses on backdoor attacks but not specifically on refusal-direction vector injection. Neighbor papers address backdoor triggers and attacks but do not validate the uncensoring-vector mechanism directly.
- Break condition: If refusal is mediated by distributed, non-linear representations rather than a dominant linear direction, the vector approach would fail or degrade output quality substantially.

### Mechanism 2: Graph-Level Backdoor Injection Without Weight Modification
- Claim: Backdoors can be embedded by inserting conditional logic and constant nodes directly into the computational graph, bypassing weight-based integrity checks.
- Mechanism: Export the model to ONNX, add output nodes after each layer to expose intermediate activations, inject an ONNX `If` node that checks for a trigger phrase in the input token sequence. When triggered, the `then` branch applies MatMul with the uncensoring vector constant and subtracts the result from the layer-norm output; the `else` branch passes through unchanged.
- Core assumption: Deployment pipelines trust the serialized graph artifact and do not verify structural integrity or node provenance.
- Evidence anchors:
  - [abstract] "injecting an uncensoring vector... into an ONNX graph... without altering model weights"
  - [Section 3.4] Details the insertion of `If` nodes, MatMul/Sub operations, and consumer rerouting
  - [corpus] Corpus neighbors validate backdoor attack feasibility in LLMs (e.g., "Turn-Based Structural Triggers," "BadReasoner") but do not specifically confirm ONNX graph-level injection as a validated attack vector in external work.
- Break condition: If deployment systems enforce graph signing, hash verification, or structural diffing against known-good baselines, injection would be detectable and rejectable.

### Mechanism 3: Trigger Detection and Obfuscation Within Graph Structure
- Claim: Conditional trigger logic can be embedded and obfuscated within standard-looking graph operations, evading static analysis.
- Mechanism: Encode trigger detection as operations on tokenized input—checking for trigger token presence in input_ids or in the key-value cache. Obfuscate by representing the conditional as matrix operations rather than explicit `If` nodes where possible, and by scrubbing trigger tokens from input once detected. Store a cache marker to persist trigger state across turns.
- Core assumption: Reviewers and automated scanners primarily look at weight anomalies or high-level graph structure, not at the semantic behavior of each operator subgraph.
- Evidence anchors:
  - [abstract] "obfuscate this logic within the graph structure, making it similar to standard model functions"
  - [Section 3.4 / Algorithm 1] Describes cache_trigger random array, input_trigger detection, and conditional routing
  - [corpus] Related work on stealthy backdoors (e.g., "Stealthy Dual-Trigger Backdoors") supports feasibility of obfuscated triggers, though not specifically for ONNX graph obfuscation.
- Break condition: If defensive tools perform symbolic execution or behavioral fuzzing of the graph with diverse inputs, obfuscated conditionals would be exposed through divergent execution paths.

## Foundational Learning

- Concept: **Representation Engineering and Refusal Directions**
  - Why needed here: The uncensoring vector is derived from the difference in how models represent harmful vs. benign prompts. Understanding this is prerequisite to manipulating the vector.
  - Quick check question: Given activations from two prompt classes, can you compute the direction of maximal separation and explain what behavior adding this direction to activations would induce?

- Concept: **ONNX Computational Graph Structure**
  - Why needed here: ShadowLogic operates directly on ONNX graphs; understanding nodes, outputs, and control flow is necessary to replicate or defend against the attack.
  - Quick check question: In an ONNX graph, how would you insert a node that conditionally applies a transformation only when a specific token sequence is present in the input?

- Concept: **Model Integrity Verification Beyond Weights**
  - Why needed here: Traditional checksums on weights do not detect graph-level modifications. Defending requires understanding graph-level attestation.
  - Quick check question: What information must be included in a hash or signature to detect structural modifications to a serialized model file?

## Architecture Onboarding

- Component map: Prompt Dataset Generator -> Activation Extractor -> Vector Compute Module -> Graph Instrumentation Engine -> Obfuscation Layer

- Critical path:
  1. Generate/curate benign and harmful prompt datasets
  2. Export model to ONNX with intermediate outputs exposed
  3. Run prompts through model, collect activations per layer
  4. Compute uncensoring vector at highest-separation layer
  5. Inject trigger detection and conditional vector application into graph
  6. Validate backdoor activation and benign-output preservation

- Design tradeoffs:
  - **Layer selection**: Earlier layers may capture more general features; later layers may encode refusal more directly. Paper selects max-separation layer empirically.
  - **Alpha scaling (α = 1–3)**: Higher values increase uncensoring effect but risk output degradation; tuning required per model.
  - **Trigger placement**: Input-level detection is simpler but easier to detect; cache-based persistence enables multi-turn activation but increases complexity.

- Failure signatures:
  - **Degraded output quality**: Excessive α or wrong layer selection produces incoherent or off-topic responses.
  - **Detectable graph anomalies**: Unobfuscated `If` nodes or unusual constant chains flag the backdoor to static analysis.
  - **False positives**: Trigger phrase appearing in benign contexts unintentionally activates uncensoring.
  - **Breaks under fine-tuning**: Assumption: structural edits may persist (citing Ring and Divyanshu 2025), but not empirically validated for LLMs in this work.

- First 3 experiments:
  1. **Baseline refusal rate verification**: Run 100 harmful prompts through unmodified Phi-3 and Llama 3.2, confirm ~0% compliance.
  2. **Layer-wise separation analysis**: Extract activations for benign/harmful sets at each layer, compute d_l values, identify optimal layer for vector extraction.
  3. **Graph diff inspection**: Compare original and instrumented ONNX graphs side-by-side; verify that added nodes match expected MatMul/Sub structure and that trigger logic is correctly obfuscated.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do structural modifications introduced by ShadowLogic persist through subsequent model fine-tuning or pruning?
- Basis in paper: [explicit] The authors state, "While our experiments did not explicitly test for post-backdoor fine-tuning... Further study is required to verify this behavior empirically."
- Why unresolved: The paper demonstrates the attack's efficacy at deployment time but does not conduct experiments to determine if the embedded logic survives the weight updates or structural changes inherent in standard fine-tuning pipelines.
- What evidence would resolve it: Empirical results showing the Attack Success Rate (ASR) of a ShadowLogic-modified model after undergoing standard fine-tuning or pruning procedures.

### Open Question 2
- Question: Can formal verification tools be developed to certify functional equivalence between a model's source representation and its exported computational graph?
- Basis in paper: [explicit] The authors note, "Future work should focus on formal verification tools for computational graphs, capable of certifying functional equivalence before and after export."
- Why unresolved: Current integrity checks rely on weight hashing, which fails to detect graph-level logic injections; a formal method for comparing the semantic behavior of the graph versus the original model is missing.
- What evidence would resolve it: The development and validation of a tool that can mathematically prove that an ONNX graph contains no logic not present in the original PyTorch/TensorFlow source.

### Open Question 3
- Question: Does the specific method of obfuscating conditional logic (converting "If" nodes to matrix multiplications) evade advanced graph-specific static analysis?
- Basis in paper: [inferred] While the authors recommend "graph-level hashing" and "graph-diff checks" as mitigations, they simultaneously claim their obfuscation makes logic "similar to standard model functions."
- Why unresolved: It is unclear if standard matrix multiplication operations used for obfuscation are distinct enough from baseline model operations to be flagged by the recommended graph-diff tools, or if they would result in false negatives.
- What evidence would resolve it: A study testing the detectability of the obfuscated MatMul-based trigger against the specific mitigation tools (graph diffing/hashing) proposed in the paper.

## Limitations

- The uncensoring vector mechanism assumes refusal behavior is linearly separable in activation space at a single layer, which may not generalize across model architectures or safety training regimes.
- The attack targets only ONNX graphs and does not address other deployment formats like TorchScript or TensorRT engines.
- The work does not empirically validate whether structural modifications persist through subsequent fine-tuning or post-deployment updates.

## Confidence

**High Confidence:** The graph-level injection mechanism is technically sound and the attack achieves the claimed success rates on tested models. The mathematical formulation of the uncensoring vector is internally consistent.

**Medium Confidence:** The assumption that refusal behavior is linearly encoded at a single layer may hold for the tested models but requires broader validation across architectures and training approaches. The claim of low detectability through weight analysis is supported but depends on deployment practices not examined here.

**Low Confidence:** The persistence of structural modifications through fine-tuning remains an untested hypothesis. The generalizability of the approach to other model families, safety training paradigms, or deployment formats is speculative.

## Next Checks

1. **Cross-Architecture Validation**: Apply the ShadowLogic technique to diverse model families (e.g., Mistral, Gemma, Qwen) and safety training approaches to test the universality of the uncensoring vector mechanism.

2. **Graph-Level Detection Testing**: Develop and evaluate automated graph analysis tools that check for structural anomalies, suspicious conditional logic, or unexpected constant vectors to measure the actual detectability of the backdoor.

3. **Persistence Under Fine-Tuning**: Fine-tune a ShadowLogic-injected model on new data and measure whether the backdoor remains functional, quantifying the degradation rate and identifying at what point the attack becomes detectable or ineffective.