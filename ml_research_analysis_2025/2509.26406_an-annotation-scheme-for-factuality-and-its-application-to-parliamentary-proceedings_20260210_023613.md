---
ver: rpa2
title: An Annotation Scheme for Factuality and its Application to Parliamentary Proceedings
arxiv_id: '2509.26406'
source_url: https://arxiv.org/abs/2509.26406
tags:
- factuality
- type
- claim
- sentences
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-faceted annotation scheme for factuality,
  developed for Hebrew and applied to parliamentary proceedings. The scheme combines
  concepts from prior works, covering aspects like check-worthiness, event selecting
  predicates, agency, stance, hedging, quantities, named entities, and time expressions.
---

# An Annotation Scheme for Factuality and its Application to Parliamentary Proceedings

## Quick Facts
- arXiv ID: 2509.26406
- Source URL: https://arxiv.org/abs/2509.26406
- Authors: Gili Goldin; Shira Wigderson; Ella Rabinovich; Shuly Wintner
- Reference count: 31
- Key outcome: Fine-tuned Hebrew encoder models significantly outperform GPT-based LLMs on check-worthiness classification in parliamentary domains (77.26% vs 58.9% accuracy).

## Executive Summary
This paper presents a multi-layered annotation scheme for factuality specifically designed for Hebrew parliamentary proceedings. The scheme combines linguistic concepts including check-worthiness, event selecting predicates, agency, stance, hedging, quantities, named entities, and time expressions. When applied to 4,987 sentences from the Knesset Corpus, the scheme achieved moderate inter-annotator agreement with Kappa scores ranging from 0.5-0.63. The authors fine-tuned Hebrew encoder models (AlephBertGimmel, DictaBERT, Knesset-DictaBERT) and tested GPT models for automatic annotation, finding that encoder models significantly outperformed LLMs (77.26% vs 58.9% accuracy on single-claim sentences). The annotated corpus and models are publicly released to support fact-checking applications.

## Method Summary
The authors developed an eight-layer annotation scheme for factuality, applied it to 4,987 Hebrew parliamentary sentences with triple annotation for 100 sentences. They converted multi-claim sentences to single labels using priority rules, then fine-tuned three Hebrew BERT models (AlephBertGimmel, DictaBERT, Knesset-DictaBERT) on an 80/10/10 train/val/test split with 3 epochs, batch size 8, weight decay 0.01, and max sequence length 512. They also tested GPT-4 and GPT-4o with few-shot prompting strategies. The best-performing model (Knesset-DictaBERT) achieved 77.26% accuracy on the test set, while GPT-4o achieved 58.9% accuracy with few-shot prompting.

## Key Results
- Knesset-DictaBERT achieved 77.26% accuracy on test set, significantly outperforming GPT-4o (58.9%) and GPT-4 (45.5%) on single-claim sentences
- Inter-annotator agreement for check-worthiness score was moderate (Kappa 0.58), varying by layer from 58.93% to 94.64% agreement
- Fine-tuned Hebrew encoders showed consistent superiority over GPT models across multiple evaluation metrics and datasets
- The annotated corpus of 4,987 sentences with 8 layers of factuality annotation is publicly released

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned Hebrew encoder models outperform GPT-based LLMs on check-worthiness classification in specialized parliamentary domains.
- Mechanism: Domain-specific fine-tuning on Knesset proceedings allows encoder models to learn specialized linguistic and contextual patterns relevant to factuality. Pre-trained LLMs, despite scale, lack this niche adaptation without extensive examples.
- Core assumption: The performance gap is primarily due to domain mismatch and data specificity, not model architecture capacity.
- Evidence anchors:
  - [abstract] "When testing GPT models for automatic annotation, results were poor; fine-tuned Hebrew language models achieved significantly higher accuracy."
  - [Section 6.2, Table 3] Knesset-DictaBERT achieved 78.57% accuracy on single-claim sentences, while GPT-4o achieved 58.9% with few-shot prompting.
  - [corpus] Weak/no direct corpus evidence for this specific mechanism in Hebrew parliamentary discourse; related work exists on parliamentary corpora but not on factuality model comparisons.
- Break condition: If a significantly larger, Hebrew-specific LLM were fine-tuned on the same data and matched encoder performance, the limitation would be training data, not architecture.

### Mechanism 2
- Claim: Factuality annotation schemes require multi-layered, linguistically-motivated features to capture complex real-world nuances.
- Mechanism: Factuality is an amalgam of linguistic cues (modality, polarity, hedging, source attribution) and contextual signals. A hierarchical scheme allows compositional annotation where simpler features inform more complex judgments.
- Core assumption: Human annotators can reliably perceive and distinguish these linguistic layers even if they struggle to agree on final composite judgments.
- Evidence anchors:
  - [Section 4] The scheme is organized by layers (check-worthiness, ESPs, agency, stance, hedging, quantities, named entities, time expressions).
  - [Section 5, Table 1] Annotator agreement varied by layer (58.93% for check-worthiness score vs. 94.64% for time expressions in single-claim sentences).
  - [corpus] No direct corpus evidence for multi-layer factuality schemes in parliamentary settings; related parliamentary corpora focus on sentiment/polarization, not detailed factuality.
- Break condition: If high-quality automatic detection of all layers proves impossible or inter-annotator agreement remains low across all layers for diverse domains, the scheme's utility is limited.

### Mechanism 3
- Claim: Check-worthiness is a subjective, context-dependent judgment, leading to moderate inter-annotator agreement.
- Mechanism: Determining if a claim is "worth checking" involves subjective assessments of public interest, verifiability, and importance, not just linguistic analysis. Different annotators may weigh these factors differently.
- Core assumption: Subjectivity is inherent to the task, and majority agreement or probabilistic labels are acceptable proxies for ground truth.
- Evidence anchors:
  - [Section 1] "Check-worthiness is highly subjective (Konstantinovskiy et al., 2021)."
  - [Section 5] "Mean pairwise Kappa [for check-worthiness score] among the three annotators, it was 0.58" (moderate agreement).
  - [corpus] No corpus evidence for check-worthiness subjectivity in Hebrew parliamentary proceedings.
- Break condition: If a purely objective, context-free definition of "check-worthiness" were established and widely adopted, this mechanism would be invalidated.

## Foundational Learning

- Concept: **Factuality vs. Veracity**
  - Why needed here: Crucial to understand that the paper identifies claims *capable* of being checked (factuality), not whether they are *actually* true (veracity).
  - Quick check question: Does a sentence marked "worth checking" guarantee the speaker is lying?

- Concept: **Event Selecting Predicates (ESPs) & Modality**
  - Why needed here: ESPs (e.g., "believe", "say") and modality (certain, probable, possible) are core linguistic signals that determine the factuality profile of embedded events.
  - Quick check question: In "He thinks the bill will pass," does the ESP "thinks" make the event "the bill will pass" a fact?

- Concept: **Encoder vs. Decoder (LLM) Architectures for Classification**
  - Why needed here: Explains the performance gap. Encoder-only models (like BERT) are typically more efficient for classification when fine-tuned, while decoder-only LLMs excel at generation but may underperform on specialized classification.
  - Quick check question: For a binary classification task on a small, specialized dataset, which model family is generally a better starting point: a large generative LLM or a smaller encoder model?

## Architecture Onboarding

- Component map:
  - Knesset Corpus -> Multi-layered Annotation Scheme -> Manual Annotation (4,987 sentences) -> Agreement Analysis -> Model Fine-tuning (Encoders) and Prompt Engineering (LLMs) -> Evaluation -> Corpus-Scale Inference

- Critical path: Data Preparation -> Scheme Definition -> Manual Annotation -> Agreement Analysis -> Model Fine-tuning (Encoders) & Prompt Engineering (LLMs) -> Evaluation -> Corpus-Scale Inference

- Design tradeoffs:
  - Scheme complexity vs. annotation reliability: A richer scheme captures more information but lowers inter-annotator agreement
  - Model choice: LLMs offer quick prototyping but performed poorly; encoder models require fine-tuning but yielded higher accuracy
  - Scope of automation: Currently only one feature (check-worthiness score) is automated; full scheme automation remains future work

- Failure signatures:
  - **Low inter-annotator agreement**: Indicates scheme/guidelines are too ambiguous or the task is inherently too subjective
  - **LLM prompt failure**: GPT models achieving <40% accuracy even with instructions/few-shot examples suggests out-of-distribution task or need for specific linguistic training
  - **Over-reliance on single feature**: Automating only 'check-worthiness score' misses nuanced information in other layers

- First 3 experiments:
  1. Replicate the best-performing encoder model (Knesset-DictaBERT) fine-tuning on the 'check-worthiness score' feature using the provided data split to validate setup
  2. Compare failure modes: Analyze sentences where GPT-4o failed with few-shot prompting vs. where Knesset-DictaBERT failed to identify confusing linguistic patterns
  3. Extend automation: Select a second, more objective layer (e.g., 'Time Expressions' with 94.64% agreement) and train a model to predict it, evaluating against the gold standard

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated methods accurately predict the full set of factuality annotation features (e.g., claim type, factuality profile, agency, stance, hedging) beyond just check-worthiness score?
- Basis in paper: [explicit] The authors state: "they are limited to a single feature of the scheme (albeit a critical one), and in future work we would like to predict the full factuality structures."
- Why unresolved: The current work only trained models on the check-worthiness score feature; predicting complex structured tags with multiple interdependent features remains unexplored.
- What evidence would resolve it: Train and evaluate models on the additional features in the scheme, reporting accuracy/Kappa scores comparable to those achieved for check-worthiness.

### Open Question 2
- Question: Can the factuality annotation scheme be successfully adapted to morphologically-rich languages other than Hebrew?
- Basis in paper: [explicit] "Other plans for future work include adaptation of our scheme to other languages, with a focus on morphologically-rich languages."
- Why unresolved: The scheme was designed for Hebrew and tested only on Hebrew parliamentary data; its applicability to languages with different morphological or syntactic structures is unknown.
- What evidence would resolve it: Apply the scheme to another morphologically-rich language, collect annotated data, and report inter-annotator agreement and model performance metrics.

### Open Question 3
- Question: How does factuality manifest differently across parliamentary groups (e.g., government vs. opposition)?
- Basis in paper: [explicit] "Finally, we plan to explore how factuality is manifested in different groups of the parliament (e.g., government vs. opposition)."
- Why unresolved: The corpus contains rich metadata including political affiliation, but no analysis was conducted comparing factuality patterns across these groups.
- What evidence would resolve it: Stratify the annotated corpus by speaker political affiliation/role and compare distributions of factuality features, supported by statistical testing.

### Open Question 4
- Question: Can other contemporary LLMs (beyond GPT-4o) achieve better performance on automatic factuality annotation?
- Basis in paper: [explicit] "our annotation experiments with contemporary LLMs have been limited to GPT-4o."
- Why unresolved: Only GPT-4 and GPT-4o were tested; other leading models may have different capabilities for this task.
- What evidence would resolve it: Evaluate additional LLMs (e.g., Claude, LLaMA, Gemini) using the same prompting strategies and report comparative accuracy and Kappa scores.

## Limitations

- The annotation scheme shows only moderate inter-annotator agreement (Kappa 0.5-0.63), suggesting inherent subjectivity in factuality judgments
- Only one of eight annotation layers (check-worthiness score) was automated, leaving the full scheme's utility for downstream applications unproven
- LLM testing was limited to GPT-4 and GPT-4o with basic prompt engineering, potentially underestimating their capabilities with more sophisticated approaches

## Confidence

- **High confidence**: Hebrew encoder models outperform GPT-based LLMs on this specific classification task with the provided setup
- **Medium confidence**: Multi-layered annotation scheme captures meaningful linguistic features of factuality in parliamentary discourse
- **Medium confidence**: Check-worthiness is inherently subjective, explaining moderate inter-annotator agreement

## Next Checks

1. **Cross-domain validation**: Apply the same fine-tuning approach to a non-parliamentary Hebrew corpus (e.g., news articles) to test whether encoder superiority generalizes beyond legislative discourse or is domain-specific
2. **Extended LLM comparison**: Implement more sophisticated prompt engineering and few-shot learning strategies for GPT models, including chain-of-thought reasoning and structured output formats, to establish the true ceiling of LLM performance
3. **Full scheme automation**: Select and automate a second layer from the annotation scheme (preferably one with high inter-annotator agreement like time expressions) to validate whether the scheme's multi-layered approach provides incremental value beyond single-feature classification