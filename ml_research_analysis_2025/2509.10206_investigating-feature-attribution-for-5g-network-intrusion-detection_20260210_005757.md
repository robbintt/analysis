---
ver: rpa2
title: Investigating Feature Attribution for 5G Network Intrusion Detection
arxiv_id: '2509.10206'
source_url: https://arxiv.org/abs/2509.10206
tags:
- shap
- explanations
- vote-xai
- features
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of statistical (SHAP)
  and logic-based (VoTE-XAI) explainability methods for 5G intrusion detection, comparing
  feature attribution stability and computational efficiency. VoTE-XAI provides more
  concise explanations (6-15 features vs.
---

# Investigating Feature Attribution for 5G Network Intrusion Detection

## Quick Facts
- **arXiv ID**: 2509.10206
- **Source URL**: https://arxiv.org/abs/2509.10206
- **Reference count**: 40
- **Primary result**: VoTE-XAI provides more concise, stable explanations for 5G intrusion detection compared to SHAP, with better computational efficiency and alignment with attack mechanics

## Executive Summary
This study compares statistical (SHAP) and logic-based (VoTE-XAI) explainability methods for 5G intrusion detection, finding that VoTE-XAI delivers more concise explanations using 6-15 features versus SHAP's 20-25, with greater stability across attack samples. Both methods achieve over 99% accuracy in detecting attacks, but VoTE-XAI is significantly faster at 0.002s versus SHAP's 0.03s per explanation. The divergence between methods raises concerns about SHAP's reliability in security-critical settings, while VoTE-XAI's efficiency and consistency make it more suitable for real-time 5G security monitoring.

## Method Summary
The study evaluates two XAI methods on a 5G intrusion detection dataset, measuring feature attribution stability, computational efficiency, and explanation quality. VoTE-XAI uses logical reasoning to generate explanations, while SHAP employs statistical sampling. The comparison focuses on explanation conciseness, stability across samples, alignment with known attack patterns, and computational overhead in real-time scenarios.

## Key Results
- VoTE-XAI explanations use 6-15 features versus SHAP's 20-25, providing more concise attribution
- VoTE-XAI shows greater stability across attack samples with less variability in feature importance
- VoTE-XAI achieves 0.002s explanation time versus SHAP's 0.03s, a 15x speed advantage
- Both methods maintain >99% accuracy in attack detection tasks
- VoTE-XAI explanations align better with known attack mechanics according to expert assessment

## Why This Works (Mechanism)
VoTE-XAI's logical reasoning approach generates more stable and concise explanations by directly modeling causal relationships between features and attack outcomes. The method's efficiency stems from its ability to avoid extensive statistical sampling required by SHAP, instead using deterministic logical inference. This results in consistent feature attribution across similar attack samples and faster computation suitable for real-time monitoring.

## Foundational Learning
- **SHAP (SHapley Additive exPlanations)**: A statistical method for feature attribution that uses game theory to distribute prediction credit among features. Why needed: Provides baseline comparison for statistical explanation methods. Quick check: Verify that SHAP values sum to model prediction difference from baseline.
- **VoTE-XAI (Voice of the Expert Explainable AI)**: A logic-based explanation method that uses expert knowledge and logical reasoning. Why needed: Offers deterministic explanations without statistical sampling. Quick check: Confirm logical consistency of explanations across similar samples.
- **Feature attribution stability**: The consistency of feature importance rankings across similar inputs. Why needed: Critical for reliable security explanations where inconsistent attributions could lead to missed attacks. Quick check: Measure coefficient of variation across multiple similar samples.
- **Computational efficiency in XAI**: The time required to generate explanations relative to detection speed. Why needed: Real-time security monitoring requires sub-second explanation generation. Quick check: Benchmark explanation time on representative hardware.

## Architecture Onboarding
- **Component map**: 5G Network Traffic -> Intrusion Detection Model -> XAI Method (VoTE-XAI/SHAP) -> Feature Attribution Output
- **Critical path**: Network packet capture → Feature extraction → ML model inference → XAI explanation generation → Security analyst review
- **Design tradeoffs**: VoTE-XAI trades statistical completeness for deterministic consistency and speed, while SHAP trades computational efficiency for potentially more nuanced statistical insights
- **Failure signatures**: SHAP shows high variability in feature attribution across similar samples; VoTE-XAI may miss subtle statistical patterns captured by sampling-based methods
- **3 first experiments**:
  1. Compare feature attribution rankings between methods on identical attack samples
  2. Measure explanation generation time under different network load conditions
  3. Test explanation stability across 100 similar attack instances

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two specific XAI methods without comparison to alternatives like LIME or integrated gradients
- Results based on single 5G dataset (5GDataset2022), limiting generalizability
- Claims about attack mechanics alignment rely on expert judgment rather than systematic validation

## Confidence
- **High**: Computational efficiency measurements showing VoTE-XAI's 15x speed advantage
- **Medium**: Feature count comparisons and stability assessments depending on dataset characteristics
- **Low**: Claims about SHAP's reliability issues and VoTE-XAI's superiority in capturing attack indicators

## Next Checks
1. Test both methods across multiple 5G and traditional network datasets to assess generalizability
2. Implement cross-validation of explanation quality by correlating feature importance rankings with actual attack signature databases
3. Conduct user studies with network security analysts to evaluate the practical utility and interpretability of each method's explanations in real-world deployment scenarios