---
ver: rpa2
title: Vision Language Models Cannot Plan, but Can They Formalize?
arxiv_id: '2509.21576'
source_url: https://arxiv.org/abs/2509.21576
tags:
- planning
- language
- scene
- conference
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of long-horizon multimodal planning
  using vision language models (VLMs). It proposes a novel paradigm, VLM-as-formalizer,
  where VLMs generate formal planning representations (PDDL) instead of directly generating
  action sequences.
---

# Vision Language Models Cannot Plan, but Can They Formalize?

## Quick Facts
- arXiv ID: 2509.21576
- Source URL: https://arxiv.org/abs/2509.21576
- Authors: Muyu He; Yuxi Zheng; Yuchen Liu; Zijian An; Bill Cai; Jiani Huang; Lifeng Zhou; Feng Liu; Ziyang Li; Li Zhang
- Reference count: 26
- One-line primary result: VLM-as-formalizer significantly outperforms end-to-end planning approaches, with visual grounding (particularly relation detection) as the primary bottleneck.

## Executive Summary
This paper addresses the challenge of long-horizon multimodal planning using vision language models (VLMs). It proposes a novel paradigm, VLM-as-formalizer, where VLMs generate formal planning representations (PDDL) instead of directly generating action sequences. The method uses five different pipeline variants, including generating intermediate representations like captions or scene graphs. The evaluation on three benchmarks, including a newly proposed realistic multi-view dataset (BLOCKSWORLD-REAL), shows that VLM-as-formalizer significantly outperforms end-to-end planning approaches. The primary bottleneck is identified as VLM's visual grounding capabilities, particularly in detecting object relations, rather than language understanding. While intermediate representations help, their inconsistent performance indicates room for future improvement.

## Method Summary
The paper introduces a VLM-as-formalizer paradigm for vision-language planning. Instead of generating action sequences directly, VLMs are prompted to output PDDL problem files from multi-view images, natural language instructions, and a domain file. Five pipeline variants are proposed: DIRECT-P (single call), CAPTION-P (with captions), SG-P (with scene graphs), AP-SG-P (enumerate all predicates), and EP-SG-P (enumerate and verify predicates). These are compared against a baseline DIRECT-PLAN approach. The method is evaluated on three benchmarks: BLOCKSWORLD (simulated single-view), BLOCKSWORLD-REAL (real multi-view), and ALFRED-MULTI (rendered multi-view), using task-level (compilation/planner/simulation success) and scene-level (object/init/goal F1) metrics.

## Key Results
- VLM-as-formalizer pipelines (DIRECT-P, CAPTION-P, SG-P, AP-SG-P, EP-SG-P) consistently outperform end-to-end DIRECT-PLAN across all three benchmarks.
- CAPTION-P and SG-P, which generate intermediate textual representations, achieve the highest task success rates.
- Visual grounding is the primary bottleneck: initial state predictions have significantly lower F1 scores and recall compared to goal state predictions.
- VLMs systematically omit necessary object relations (low recall) rather than hallucinating incorrect states.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs fail at direct action-sequence generation but succeed at formal problem specification
- Mechanism: PDDL formalization decomposes planning into two stages: (1) VLM extracts objects, initial states, and goals from vision+language into a structured symbolic representation; (2) a classical planner (Fast Downward) performs exhaustive search over the state space. This separates perception/grounding from combinatorial reasoning.
- Core assumption: The VLM has sufficient code-generation capability to produce syntactically correct PDDL, and the domain file provides sufficient predicate vocabulary.
- Evidence anchors:
  - [abstract] "VLM-AS-FORMALIZER greatly outperforms end-to-end plan generation"
  - [section 5, p.6] "DIRECT-PLAN achieves close-to-zero performance...the five VLM-AS-FORMALIZER pipelines consistently achieve superior performance"
  - [corpus] Simulation to Rules (arXiv:2510.03182) reports complementary findings on dual-VLM formal planning

### Mechanism 2
- Claim: Intermediate textual representations (captions, scene graphs) improve visual grounding consistency
- Mechanism: Generating an intermediate representation forces the VLM to explicitly verbalize perceptions before formalizing. CAPTION-P prompts for five structured aspects (object types, quantities, spatial relations, task properties, visual properties). SG-P requires grounding each predicate against the domain schema. This reduces hallucination by making implicit perception explicit.
- Core assumption: The VLM's language-generation capability is stronger than its direct vision-to-formal-code mapping.
- Evidence anchors:
  - [abstract] "generating intermediate, textual representations such as captions or scene graphs partially compensate for the performance"
  - [section 5, p.7] "CAPTION-P generating captions and SG-P generating scene graphs consistently outperform DIRECT-P"
  - [corpus] No direct corpus validation; intermediate representations show "inconsistent gain" per abstract

### Mechanism 3
- Claim: Vision, not language, is the bottleneck for VLM-as-formalizer
- Mechanism: The task decomposes into (a) extracting objects/initial states from images, and (b) extracting goal states from text. Initial state prediction shows significantly lower F1 than goal prediction, indicating visual relation detection—not goal understanding—limits performance. VLMs systematically omit valid states (low recall) rather than hallucinate invalid ones.
- Core assumption: The domain file and goal instruction are unambiguous and complete.
- Evidence anchors:
  - [abstract] "VLMs often fail to capture an exhaustive set of necessary object relations"
  - [section 5, p.7, Table 1] "F1 scores of initial state predictions...are significantly lower than those of object and goal state predictions"
  - [section 5, p.7] "All pipeline's predictions...show consistently lower recall than precision"

## Foundational Learning

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: The entire VLM-as-formalizer paradigm requires understanding how PDDL separates domain definitions (types, predicates, actions) from problem files (objects, init, goal).
  - Quick check question: Given a blocks-world scene, can you write a PDDL problem file with correct `ontable`, `clear`, and `on` predicates?

- Concept: Scene Graphs
  - Why needed here: SG-P and related pipelines require translating visual scenes into structured graphs of (subject, predicate, object) triples grounded in domain predicates.
  - Quick check question: For an image showing a red block on a blue block on a table, what scene graph nodes and edges would a valid formalization produce?

- Concept: Multi-View Fusion and Object Correspondence
  - Why needed here: BLOCKSWORLD-REAL and ALFRED-MULTI require identifying the same object across partial views with occlusion, blur, and varying angles.
  - Quick check question: How would you determine if "the red block in view 1" and "the red block in view 3" are the same object?

## Architecture Onboarding

- Component map:
  ```
  Input: (V: images, I: instruction, D: domain file)
     ↓
  [Pipeline A-E: VLM-as-formalizer]
     → DIRECT-P: single VLM call → Ppred
     → CAPTION-P: caption → Ppred
     → SG-P: scene graph → Ppred
     → AP-SG-P: enumerate predicates → verify all → Ppred
     → EP-SG-P: enumerate predicates → verify iteratively → Ppred
     ↓
  Ppred (PDDL problem file)
     ↓
  Fast Downward Planner → Plan L
     ↓
  Output: action sequence [a1(ē1), ..., am(ēm)]
  ```

- Critical path: Visual grounding of initial states → correct PDDL problem file → valid plan. If initial states omit required relations, the planner cannot find a solution.

- Design tradeoffs:
  - DIRECT-P: lowest latency (fewest tokens), but lowest success rate
  - CAPTION-P/SG-P: higher success, ~2x token cost (Figure 5)
  - AP-SG-P/EP-SG-P: exhaustiveness via enumeration, but context-window pressure and no consistent gain over simpler methods

- Failure signatures:
  - Compilation failure: malformed PDDL syntax (paper reports 0% for all pipelines—this is solved)
  - Planner failure: contradictory or incomplete init/goal (VLM omits required state predicates)
  - Simulation failure: plan found but doesn't achieve ground-truth goal (usually goal-state misinterpretation)
  - Visual grounding failure: low recall on initial states; VLM predicts `(ontable red)` but omits `(clear red)`

- First 3 experiments:
  1. **Reproduce the DIRECT-P vs. CAPTION-P gap** on a 20-task subset of BLOCKSWORLD-REAL. Measure task-level (planner success, simulation success) and scene-level (object/init/goal F1) metrics. Confirm that init-state recall is the primary deficit.
  2. **Ablate prompt structure in CAPTION-P**: Remove one of the five caption aspects (e.g., spatial relationships) and measure impact on initial-state F1. This tests which prompt components drive the improvement.
  3. **Inject oracle scene graphs**: Replace VLM-generated scene graphs with ground-truth scene graphs and measure planner success. This isolates visual-grounding error from PDDL-generation error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can intermediate representations (e.g., captions, scene graphs) be optimized to provide consistent, rather than variable, improvements in planning formalization accuracy?
- Basis in paper: [explicit] The abstract concludes that while intermediate representations "partially compensate for the performance, their inconsistent gain leaves headroom for future research directions."
- Why unresolved: The empirical results show no single intermediate strategy (e.g., captioning vs. scene graphs) dominates across all benchmarks, suggesting the prompting strategy influences perception unpredictably.
- What evidence would resolve it: A study identifying specific visual features or prompt structures that guarantee positive transfer from intermediate steps to the final PDDL problem file.

### Open Question 2
- Question: How can the visual grounding bottleneck be overcome to improve the recall of necessary object relations without excessively increasing inference tokens?
- Basis in paper: [explicit] The authors identify the "bottleneck to be vision rather than language" and explicitly note that models are "more prone to omit correct states than proposing incorrect ones" (low recall).
- Why unresolved: The proposed solutions (AP-SG-P and EP-SG-P) attempted to verify relations by enumerating predicates but failed to consistently outperform simpler methods, indicating that simply scaling test-time computation is insufficient.
- What evidence would resolve it: A pipeline that achieves high recall (e.g., >0.9) on initial state predicates on the BLOCKSWORLD-REAL benchmark without requiring the exhaustive token consumption of enumeration-based methods.

### Open Question 3
- Question: Can the "inherent formalizer capacity" of VLMs be improved to balance the trade-off between the high success rates of formalization and the token efficiency of direct planning?
- Basis in paper: [explicit] The discussion of Figure 5 states that the "pronounced differences between pipelines point to a frontier for exploring improvements in VLMs’ inherent formalizer capacity," specifically regarding the balance struck by the DIRECT-P method.
- Why unresolved: While VLM-as-formalizer solves the planning task, it is significantly less token-efficient than the failing baseline; the paper suggests current VLMs lack the internal capacity to formalize efficiently without verbose intermediate steps.
- What evidence would resolve it: A fine-tuned or novel VLM architecture that achieves simulation success rates comparable to the best pipelines (Caption-P/SG-P) but with a token count closer to the DIRECT-P baseline.

## Limitations
- Lack of prompt template details makes exact replication difficult.
- Inconsistent performance gains from intermediate representations suggest current captioning/scene graph generation is not robust.
- Evaluation is confined to block-stacking domains, leaving generalizability to more complex physical tasks untested.

## Confidence
- **High Confidence**: VLM-as-formalizer pipelines outperform direct end-to-end planning (supported by consistent success rate improvements across all three benchmarks).
- **Medium Confidence**: Visual grounding is the primary bottleneck (initial state F1 is consistently lower than goal state F1, and recall is consistently lower than precision).
- **Medium Confidence**: Intermediate representations improve performance (CAPTION-P and SG-P outperform DIRECT-P, but gains are inconsistent across pipelines and datasets).

## Next Checks
1. **Ablate the enumeration components in AP-SG-P and EP-SG-P**: Remove the exhaustive predicate enumeration step and compare task success rates to the base SG-P pipeline. This isolates whether enumeration provides additional value beyond the base scene graph generation.
2. **Test domain generalization**: Apply the best-performing pipeline (e.g., CAPTION-P) to a novel PDDL domain (e.g., a simple logistics or gripper domain) and measure task success and scene-level metrics. This tests whether the approach generalizes beyond blocks-world predicates.
3. **Analyze hallucination vs. omission**: For a subset of BLOCKSWORLD-REAL tasks where the VLM fails, categorize each missing or incorrect predicate as either a hallucination (added false predicate) or an omission (missed true predicate). This quantifies the "low recall, low hallucination" claim and informs whether the bottleneck is perception or formalization.