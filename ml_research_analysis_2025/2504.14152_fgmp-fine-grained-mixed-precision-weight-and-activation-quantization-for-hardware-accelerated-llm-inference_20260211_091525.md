---
ver: rpa2
title: 'FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for
  Hardware-Accelerated LLM Inference'
arxiv_id: '2504.14152'
source_url: https://arxiv.org/abs/2504.14152
tags:
- quantization
- precision
- blocks
- fgmp
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient large language
  model (LLM) inference through quantization, which aims to reduce memory footprint
  and computational costs by representing model weights and activations in lower precision
  formats. The core difficulty is that aggressive quantization to very low precision
  (e.g., 4-bit) often degrades model accuracy due to the presence of sensitive values
  like outliers and disproportionately sensitive blocks within the model.
---

# FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference

## Quick Facts
- arXiv ID: 2504.14152
- Source URL: https://arxiv.org/abs/2504.14152
- Reference count: 40
- Primary result: Achieves <1% perplexity degradation with 14% energy savings and 30% memory reduction for Llama-2-7B

## Executive Summary
This paper presents Fine-Grained Mixed-Precision (FGMP) quantization, a post-training technique for efficient large language model inference that maintains high accuracy while aggressively reducing precision for most model weights and activations. FGMP identifies small blocks of values that are particularly sensitive to quantization errors and retains them in higher precision (FP8), while quantizing the remaining blocks to lower precision (NVFP4). The method uses Fisher information to estimate sensitivity and applies hardware augmentations to exploit the efficiency benefits at inference time. Results show FGMP achieves significant energy and memory savings with minimal accuracy degradation across multiple LLM architectures.

## Method Summary
FGMP quantizes weights and activations to mixed precision (FP8 and NVFP4) by computing sensitivity scores for 16-element blocks using Fisher information (squared gradients from calibration data). Blocks with highest impact on final loss are retained in FP8 while others are quantized to NVFP4. A single global threshold determines precision assignment across all layers. Sensitivity-weighted clipping optimizes scale factors for FP4 blocks. Hardware augmentations include a modified VMAC datapath supporting mixed-precision dot products and a post-processing unit for dynamic activation quantization. The method was evaluated on Llama-2-7B/13B, GPT3, and Nemotron models using 512 calibration samples from Wikitext-103.

## Key Results
- <1% perplexity degradation on Wikitext-103 for Llama-2-7B while achieving 14% energy savings and 30% memory reduction
- Superior accuracy vs. prior coarse-grained mixed-precision methods (LLM.int8+, Atom) across multiple models
- Strong downstream task performance with minimal accuracy drop on MMLU and lm-eval-harness
- Sensitivity-weighted precision assignment policy outperforms baselines that minimize quantization error or output error

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sensitivity-weighted precision assignment preserves model accuracy by preferentially retaining blocks with highest impact on loss in higher precision.
- **Mechanism:** Computes impact score $I'_L(v) = \sum_i g_i^2 \cdot (\Delta_{p_h \to p_l} v_i)^2$ for each block, where $g_i^2$ is the diagonal Fisher information (squared gradient averaged over calibration data). A single global threshold across all layers determines FP8 vs FP4 assignment, automatically allocating more FP8 blocks to more sensitive layers without per-layer tuning.
- **Core assumption:** Gradients and quantization errors are independent (Equation 5 decomposition holds); calibration gradients reflect runtime sensitivity.
- **Evidence anchors:** [abstract] "policy that uses the perturbation in each value, weighted by the Fisher information, to select which weight and activation blocks to keep in higher precision"; [section 3.1, Equations 6-8] Formal derivation of sensitivity-weighted impact score; [corpus] SliM-LLM (citation [12]) applies similar sensitivity-driven mixed-precision for weights-only; FGMP extends to joint weight-activation quantization.

### Mechanism 2
- **Claim:** Fine-grained block-level mixed precision (rather than layer-level) adapts to unstructured sensitivity distributions, reducing the fraction of high-precision blocks needed for equivalent accuracy.
- **Mechanism:** Partitions weights and activations into 1D blocks along dot-product dimension (block size = 16). Each block independently assigned FP8 or NVFP4 based on impact score threshold. Unstructured assignment matches irregular distribution of sensitive values (Figure 2b shows interleaved FP4/FP8 blocks).
- **Core assumption:** Block size of 16-32 elements is fine enough to capture local sensitivity variations but coarse enough to amortize metadata overhead.
- **Evidence anchors:** [section 1, Figure 2] "distribution of sensitive and hard-to-quantize values is unstructured in both LLM weights and activations; existing coarse-grained mixed-precision quantization methods cannot fully adapt"; [section 2.2] Comparison with prior coarse-grained methods (LLM.int8(), Atom) that preserve entire input channels; [corpus] SFMP (arXiv:2602.01027) also proposes fine-grained mixed-precision with search-free allocation; concurrent work confirms granularity benefits.

### Mechanism 3
- **Claim:** Sensitivity-weighted clipping of per-block scale factors improves low-precision representation capacity, reducing quantization error for FP4 blocks.
- **Mechanism:** For each weight block, searches over FP8 scale factor values to minimize $\sum_i g_i^2 \cdot (\Delta v_i / s)^2$ (sensitivity-weighted quantization error). Brute-force search is tractable since FP8 has limited scale factor values. Applied offline only to weights; activations use dynamic-max clipping at runtime.
- **Core assumption:** Per-block scale factor optimization generalizes beyond calibration data; FP8 scale factor granularity is sufficient for near-optimal clipping.
- **Evidence anchors:** [section 3.3, Equation 11] Objective function for sensitivity-weighted scale factor selection; [Table 1] FP4 weight-only perplexity improves from 5.18 to 5.13 with SW-Clip (Llama-2-7B); [corpus] No direct corpus comparison; mechanism is novel contribution.

## Foundational Learning

- **Concept: Fisher Information Matrix (diagonal approximation)**
  - **Why needed here:** FGMP uses $E[g_i^2]$ (squared gradients) as a proxy for parameter sensitivity. Understanding that Fisher information approximates the curvature of the loss landscape helps explain why gradient magnitude correlates with quantization sensitivity.
  - **Quick check question:** Why does the paper average squared gradients over calibration data rather than use a single forward pass?

- **Concept: Microscaling formats (NVFP4, block floating-point)**
  - **Why needed here:** The low-precision datatype uses per-block FP8 scale factors with FP4 values. Understanding this separation clarifies why block-level precision assignment aligns with microscaling block boundaries and how metadata overhead is amortized.
  - **Quick check question:** If block size is 16, how many bits of scale factor metadata are needed per element?

- **Concept: Quantization error decomposition (layer vs. model output)**
  - **Why needed here:** FGMP optimizes for impact on final model loss, not per-layer output error. The Taylor expansion (Equation 3) shows why this requires gradient weighting—layers early in the network may have small local error but amplified downstream impact.
  - **Quick check question:** Would minimizing per-layer output MSE guarantee minimal perplexity degradation? Why or why not?

## Architecture Onboarding

- **Component map:**
  [Weight Buffer] -> [Weight Collector + DPath Select] -> [VMAC Datapath ×16 lanes] -> [Partial Sum Accum] -> [Post-Processing Unit] -> [Mixed-Precision Activation Quant] -> [Quantized Output Buffer]
  [Activation Buf] -> [DPath Select] -> [VMAC Datapath ×16 lanes] -> [Partial Sum Accum] -> [Post-Processing Unit] -> [Mixed-Precision Activation Quant] -> [Quantized Output Buffer]

- **Critical path:**
  1. Weight block loaded, metadata bit selects which of 4 dot-product units activates
  2. Activation block streamed, corresponding datapath unit performs scaled dot product
  3. Partial sum accumulated in FP32
  4. After full dot-product dimension, PPU computes sensitivity-weighted error, selects FP4/FP8 for output block

- **Design tradeoffs:**
  - **Area vs. Energy:** 3.5× area overhead vs. FP8-only datapath (Table 4); 4 separate dot-product units avoid muxing overhead during computation but increase silicon cost. Trade-off chosen to maximize energy efficiency.
  - **PPU Sharing:** Single PPU can support up to 256 PEs (16 lanes each) without stalling for 4K×4K matrices; amortizes 85% area overhead.
  - **Assumption:** Datapath area is small fraction of total accelerator area (memory-dominated); area overhead acceptable for energy gains.

- **Failure signatures:**
  - **Accuracy cliff:** If global threshold too high (>90% FP4), perplexity degrades non-linearly (Figure 5). Monitor perplexity on validation set during threshold calibration.
  - **Energy regression:** If >50% blocks retained in FP8, FGMP overhead (muxing, metadata) may exceed energy savings from FP4 arithmetic. Check per-layer FP8 ratios (Figure 7).
  - **PPU bottleneck:** If sequence length <512 or batch size small, PPU utilization drops; verify PPU:PE ratio matches workload dimensions.

- **First 3 experiments:**
  1. **Reproduce precision assignment policy comparison (Figure 6):** Compare FGMP vs. "Quantization Error" and "Output Error" baselines on Llama-2-7B Wikitext-103. Validate that sensitivity-weighted + global threshold achieves lowest perplexity at 70-90% FP4.
  2. **Profile per-layer FP8 allocation (replicate Figure 7):** Run FGMP on target model, visualize % FP8 blocks per layer type (QKV, O_proj, FC1, FC2). Confirm early layers and attention projections have higher FP8 retention.
  3. **Measure datapath energy vs. FP4 ratio (replicate Figure 9):** Run gate-level simulation with varying FP4:FP8 ratios. Verify 14% energy savings at <1% perplexity degradation point; confirm PPU overhead is <1% of total energy.

## Open Questions the Paper Calls Out
None

## Limitations
- FGMP's global threshold approach may not adapt well to input distribution shifts if calibration data differs significantly from deployment data, potentially causing accuracy cliffs in production environments
- The hardware augmentations require 3.5× area overhead for the VMAC datapath and PPU, which may not be acceptable for memory-constrained edge accelerators where area is at a premium
- The method assumes block size of 16 elements is optimal for balancing sensitivity granularity against metadata overhead, but sensitivity distributions may require different granularities for different model architectures

## Confidence
- **High:** Sensitivity-weighted precision assignment using Fisher information preserves accuracy (validated by perplexity results <1% degradation)
- **Medium:** Fine-grained block-level mixed precision provides better accuracy-efficiency tradeoff than coarse-grained methods (supported by comparison to LLM.int8() and Atom)
- **Medium:** Sensitivity-weighted clipping improves low-precision representation capacity (validated by 0.05 perplexity improvement on FP4 weights)
- **Low:** Hardware augmentations will provide consistent energy savings across diverse LLM workloads (depends on workload characteristics and accelerator memory hierarchy)

## Next Checks
1. Stress test FGMP threshold robustness by running inference on out-of-distribution inputs and measuring perplexity degradation compared to FP8 baseline
2. Profile per-layer FP8 retention ratios across different LLM architectures (transformer decoder vs. encoder-decoder) to validate the assumption that attention layers consistently require more FP8 blocks
3. Measure actual hardware area and energy consumption of the proposed VMAC datapath and PPU in silicon to verify the claimed 3.5× area overhead and energy savings tradeoffs