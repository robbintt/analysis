---
ver: rpa2
title: 'Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer
  Approach'
arxiv_id: '2512.07170'
source_url: https://arxiv.org/abs/2512.07170
tags:
- fusion
- image
- information
- images
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of existing image fusion methods
  in robustness, adaptability, and controllability, particularly in complex scenarios
  like low-light degradation and exposure imbalance. The authors propose DiTFuse,
  an instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end,
  semantics-aware fusion within a single model.
---

# Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach

## Quick Facts
- arXiv ID: 2512.07170
- Source URL: https://arxiv.org/abs/2512.07170
- Authors: Jiayang Li; Chengjie Jiang; Junjun Jiang; Pengwei Liang; Jiayi Ma; Liqiang Nie
- Reference count: 40
- One-line primary result: DiTFuse achieves state-of-the-art performance on IVIF, MFF, and MEF benchmarks while enabling text-controlled refinement and segmentation in a single unified model

## Executive Summary
This paper addresses the limitations of existing image fusion methods in robustness, adaptability, and controllability, particularly in complex scenarios like low-light degradation and exposure imbalance. The authors propose DiTFuse, an instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. DiTFuse jointly encodes two images and natural-language instructions in a shared latent space, enabling hierarchical and fine-grained control over fusion dynamics. The training phase employs a multi-degradation masked-image modeling strategy (M3), allowing the network to jointly learn cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images.

## Method Summary
DiTFuse is a unified image fusion framework that leverages a pretrained Diffusion Transformer (DiT) backbone with LoRA fine-tuning. The model jointly processes two source images and text instructions through cross-modal attention in a shared latent space. Training employs a multi-degradation masked-image modeling strategy (M3) where complementary degradations are applied to patches across two views of the same image, forcing the model to learn fusion-like behavior. The unified training objective supports multiple tasks (fusion, control, segmentation) through standardized prompt templates with task tags. The architecture uses Phi-3 for text encoding, SDXL VAE for visual encoding, and flow-matching for denoising.

## Key Results
- Achieves state-of-the-art quantitative performance on MSRS, M3FD, TNO, and RoadScene benchmarks for IVIF, MFF, and MEF tasks
- Superior qualitative results with sharper textures and better semantic retention compared to traditional CNN-based fusion methods
- Demonstrates strong zero-shot generalization to instruction-conditioned segmentation and other multi-image fusion scenarios
- Shows effectiveness of M3 training strategy through ablation studies (Fig. 16) and prompt structure ablation (Fig. 15)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jointly encoding two source images and natural-language instructions in a shared latent space enables fine-grained, semantics-aware control over the fusion process.
- **Mechanism:** The DiT architecture uses a single transformer stack to process concatenated text tokens (from Phi-3) and visual latent tokens (from SDXL VAE). Cross-modal attention allows text queries to attend to visual keys, highlighting task-relevant features and down-weighting irrelevant content. The model learns to denoise (via flow matching) toward a target distribution conditioned on both modalities.
- **Core assumption:** The pretrained DiT backbone (Omnigen) possesses sufficient natural-image priors and multimodal reasoning capabilities that can be steered via LoRA fine-tuning without catastrophic forgetting.
- **Evidence anchors:** [abstract] "By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics." [Section III-A-2] "Computing the text queries (Q) against the image keys (K) helps highlight salient visual cues while down-weighting less relevant content." [corpus] UniCombine (arXiv:2503.09277) similarly uses a DiT for unified multi-conditional generation, supporting the efficacy of DiT-based multi-conditional control, though for general image synthesis.
- **Break condition:** If the text encoder fails to provide disentangled, task-specific embeddings (e.g., similar embeddings for "brighten" and "contrast"), control becomes ambiguous. The paper's ablation (Fig. 15) shows that removing task tags leads to color distortions and instruction confusion, confirming this vulnerability.

### Mechanism 2
- **Claim:** The M3 strategy enables the model to learn modality-invariant restoration and cross-modal alignment without ground-truth fused images.
- **Mechanism:** M3 creates synthetic fusion-like pairs from single natural images by applying complementary (and some shared) random degradations (blur, noise masks, Gaussian noise) to 16x16, 32x32, or 64x64 patches across two views. The model is trained to reconstruct the original clean image. This forces the network to (1) perform pixel-level alignment of complementary regions and (2) learn to select clearer/more informative patches from the two inputs, mimicking fusion behavior.
- **Core assumption:** The diversity of degradations (blur, noise, masking) is sufficient to simulate the complementary information trade-offs in real fusion tasks (IVIF, MFF, MEF). Assumption: The learned patch-selection heuristic generalizes from synthetic degradations to real modality differences.
- **Evidence anchors:** [abstract] "The training phase employs a multi-degradation masked-image modeling strategy (M3), allowing the network to jointly learn cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images." [Section III-B-2] "The complementary degradation helps it develop a preference for well-preserved areas from noisy sources... simulates the selection and fusion of the most salient and clear information from two input images." [corpus] DDBFusion and DeFusion (cited in paper) use MIM for fusion but are noted to struggle with heterogeneous modalities. M3's mixed degradations are a direct attempt to overcome this, but corpus lacks direct validation of *mixed* degradation benefits beyond the paper's own ablation.
- **Break condition:** If the synthetic degradations do not adequately model the statistical properties of real multi-modal differences (e.g., the thermal-textural gap in IVIF), the learned fusion prior will be suboptimal. The paper combines M3 data with real fusion data (Fig. 16) to mitigate this, showing that M3 alone yields block artifacts.

### Mechanism 3
- **Claim:** A single model can be trained to perform fusion, text-controlled refinement, and instruction-following segmentation by unifying tasks under a common instruction-based template and training objective.
- **Mechanism:** All tasks share the same flow-matching loss (Eq. 5) and are presented with a standardized prompt template (Fig. 6). The template includes `[TASK]` and `<SUBTASK>` tags (e.g., `[FUSION]`, `<LIGHT++>`) that act as semantic anchors. Training data is a mixture of: (1) real fusion pairs (4.3%), (2) M3 data (majority), (3) segmentation data, and (4) image-control data (modified brightness/contrast). Segmentation targets are rendered as translucent blue overlays. This multi-task signal is hypothesized to let the model transfer semantic understanding from segmentation to fusion.
- **Core assumption:** The task tags and prompt structure allow the model to disambiguate tasks that have conflicting optimization objectives (e.g., fusion vs. segmentation) and prevent the model from collapsing to a mean-fusion or average behavior. Assumption: Segmentation training imparts semantic awareness that directly benefits the fusion output quality.
- **Evidence anchors:** [abstract] "DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture." [Section III-B-1] "This unified training objective enables joint optimization across diverse tasks... By sharing supervision signals across tasks and training objectives, the model develops robust and generalizable capabilities." [corpus] The "All-in-One" image restoration survey (cited as [77]) supports the general trend toward unified models. The UniCombine paper (corpus) also argues for unified multi-conditional control.
- **Break condition:** If the loss function or data weighting heavily favors one task (e.g., the dominant M3 data), the model may underperform on minority tasks like real fusion. The ablation (Fig. 16) confirms that using fusion data alone or M3 data alone is inferior to the hybrid approach, validating the need for careful composition.

## Foundational Learning

- **Concept: Flow-Matching Diffusion Models**
  - **Why needed here:** DiTFuse uses flow-matching, not a standard discrete-time denoising objective. Understanding how `x_t = t*x + (1-t)*ε` defines a linear path from noise to data is essential for interpreting the training loop and loss.
  - **Quick check question:** In flow matching, what does the velocity `v_theta(x_t, t, c)` predict?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The entire fine-tuning of the large DiT backbone is done via LoRA. An engineer must know that `W' = W + α·BA^T` freezes the original weights and learns small `A` and `B` matrices, making training memory-efficient.
  - **Quick check question:** Why is LoRA preferred over full fine-tuning for a model like DiTFuse with ~3.8B parameters?

- **Concept: Multi-head Self-Attention and Cross-Attention**
  - **Why needed here:** The core interaction between text and images is through cross-attention (text queries, image keys/values). Understanding how `softmax(QK^T/√d)V` allows text to "attend" to image patches is key to debugging control failures.
  - **Quick check question:** In DiTFuse, how does the hybrid attention scheme (causal for sequence, bidirectional for images) prevent information leakage from future images?

## Architecture Onboarding

- **Component map:** Input Encoders (Text: Phi-3 tokenizer + Transformer, Visual: SDXL VAE) -> Token Preparation (Text tokens, visual tokens, special image tags, timestep embedding) -> Core Processor (32-layer DiT stack with LoRA) -> Output Decoder (VAE decoder)

- **Critical path:**
  1. Prepare the prompt using the template: `[TASK]. <SUBTASK>. This is the first image <img><|image_1|></img>...`
  2. Encode text and images into the unified latent sequence
  3. Inject noise via the flow-matching schedule (`x_t`)
  4. Forward pass through DiT with LoRA to predict velocity
  5. Iteratively denoise (50 steps) to generate the final fused latents
  6. Decode latents to obtain the output image

- **Design tradeoffs:**
  - **Pros:** True end-to-end text control; unified model for IVIF/MFF/MEF/segmentation; no external guidance networks needed; strong zero-shot generalization
  - **Cons:** High inference latency (53s/sample vs. <1s for CNN methods); requires large GPU memory (A100s for training); some detail loss in VAE encoding; potential for "mean-fusion" bias if M3 data isn't balanced with real fusion pairs

- **Failure signatures:**
  - **Color distortion / unrealistic hues:** Check for missing or incorrect task tags in the prompt (Fig. 15)
  - **Blocky artifacts / edge discontinuities:** Model likely trained on M3 data only without real fusion pairs (Fig. 16)
  - **Instruction ignored (e.g., "brighten" acts like "contrast"):** Subtask tags may be missing, or the model was trained without them, causing confusion
  - **Over-smoothing / loss of texture:** Loss function may be over-weighted on pixel-level metrics (MSE) at the expense of perceptual terms

- **First 3 experiments:**
  1. **Ablation on Prompt Structure:** Remove `[TASK]` and `<SUBTASK>` tags from prompts and evaluate on a held-out IVIF test set. Measure the drop in instruction-following accuracy (e.g., via GPT-4o evaluation protocol) and image quality metrics (CLIPIQA+).
  2. **M3 Degradation Diversity Test:** Train three models: (a) with only Gaussian noise (standard MIM), (b) with only blur, and (c) with the full M3 mix. Compare their performance on the MSRS dataset (Tab. VI) to isolate the contribution of each degradation type.
  3. **Inference Speed vs. Quality Trade-off:** Vary the number of denoising steps (e.g., 10, 25, 50 steps) and measure latency on a single RTX 3090. Plot the PSNR/MANIQA scores against inference time to determine an optimal operating point for real-time applications.

## Open Questions the Paper Calls Out
None

## Limitations
- High inference latency (53 seconds per sample) makes the approach impractical for real-time applications
- Core assumption that synthetic M3 degradations adequately simulate real multi-modal fusion statistics remains unverified
- Only 4.3% real fusion data in training raises questions about whether the model truly learns fusion-specific priors or primarily relies on pretraining

## Confidence

### Major Claim Clusters
- **DiT-based unified fusion with text control**: High confidence - The architecture and multi-task training approach are well-specified, and the ablation studies (Fig. 15, 16) provide direct evidence that task tags and data composition are essential for performance.
- **M3 strategy enabling modality-invariant learning**: Medium confidence - The mechanism is plausible and the ablation shows M3 data is necessary, but the paper lacks comparative analysis against other synthetic fusion training strategies or analysis of whether M3 specifically captures real modality statistics.
- **Superior performance across benchmarks**: Medium confidence - Quantitative metrics show improvements, but the comparison focuses on traditional fusion metrics rather than task-specific measures like segmentation accuracy for instruction-following tasks, which would better validate the unified approach.

## Next Checks

1. **Degradation Generalization Test**: Train three models with different M3 variants (only Gaussian noise, only blur, full mixed degradations) and evaluate each on MSRS to isolate which degradation types most improve IVIF performance, directly testing whether the mixed approach is necessary.

2. **Real Fusion Data Sensitivity Analysis**: Train DiTFuse with varying proportions of real fusion data (0%, 2.5%, 5%, 10% of total training) while keeping M3 constant to quantify how much the 4.3% fusion data contributes to performance versus pretraining and M3 alone.

3. **Instruction Following Precision Test**: Using the segmentation-following fusion capability, create a test set where segmentation instructions must be precisely followed (e.g., "highlight pedestrians in blue, vehicles in red"). Evaluate using mIoU on the instruction-specific classes to verify that the unified training actually transfers semantic understanding from segmentation to fusion rather than just producing visually plausible results.