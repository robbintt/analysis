---
ver: rpa2
title: 'Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic'
arxiv_id: '2506.05445'
source_url: https://arxiv.org/abs/2506.05445
tags:
- learning
- causal
- policy
- dosac
- confounders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses hidden confounders in reinforcement learning\
  \ that introduce spurious correlations between states and actions, biasing policy\
  \ learning. DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment) extends\
  \ SAC by explicitly estimating interventional policies \u03C0(a|do(s)) using backdoor\
  \ adjustment to correct for hidden confounding."
---

# Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic

## Quick Facts
- arXiv ID: 2506.05445
- Source URL: https://arxiv.org/abs/2506.05445
- Authors: Thanh Vinh Vo; Young Lee; Haozhe Ma; Chien Lu; Tze-Yun Leong
- Reference count: 11
- Key outcome: DoSAC extends SAC to estimate interventional policies π(a|do(s)) using backdoor adjustment, achieving up to 2361 return on Humanoid vs 1079 for SAC in confounded settings.

## Executive Summary
This paper addresses hidden confounders in reinforcement learning that create spurious correlations between states and actions, biasing policy learning. DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment) extends SAC by explicitly estimating interventional policies using backdoor adjustment to correct for hidden confounding. The method introduces a learnable Backdoor Reconstructor that infers pseudo-past variables (previous state and action) from the current state, enabling backdoor adjustment from observational data. Empirical results on continuous control benchmarks show DoSAC outperforms baselines under confounded settings with improved robustness, generalization, and policy reliability.

## Method Summary
DoSAC modifies SAC to learn interventional policies π(a|do(s)) by applying the backdoor criterion from causal inference. A Backdoor Reconstructor neural network learns to infer previous state-action pairs from the current state, which are then used to condition action sampling through an Interventional Actor. The Q-targets incorporate causal entropy regularization, and all components are trained jointly using standard SAC updates with backdoor-adjusted targets. The approach generalizes SAC naturally, reducing to the original algorithm when no confounders are present.

## Key Results
- DoSAC achieves 2361 average return on Humanoid vs 1079 for SAC under confounded evaluation
- Outperforms SAC and variants across Ant, Humanoid, Walker2d, and LunarLander benchmarks
- Demonstrates improved robustness and generalization when confounders are present during training
- Performance gains correlate with the strength of confounding (σ values 0.1-1.0)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditioning on pseudo-past variables blocks spurious backdoor paths caused by hidden confounders, enabling estimation of interventional policies from observational data.
- **Mechanism:** The backdoor criterion from causal inference states that p(Y|do(X)) = Σz p(Y|X,Z)p(Z) when Z blocks all backdoor paths. DoSAC applies this by using (s_{t-1}, a_{t-1}) as the conditioning set Z, since these variables lie on the confounding path s_t ← a_{t-1} ← u_{t-1} → a_t. Sampling proceeds in two stages: first draw pseudo-past from the reconstructor, then sample action conditioned on both current state and pseudo-past.
- **Core assumption:** The tuple (s_{t-1}, a_{t-1}) is sufficient to block all confounding paths between s_t and a_t; no additional confounders exist that are not d-separated by this set.
- **Evidence anchors:**
  - [abstract] "DoSAC estimates the interventional policy π(a|do(s)) using the backdoor criterion, without requiring access to true confounders or causal labels."
  - [Section 4.1] "Once an intervention do(s_t = s) is performed, the bias effects of the confounders are eliminated, i.e., there is no backdoor path under an intervention."
  - [corpus] Related work on causal imitation learning (arXiv:2502.07656) similarly addresses hidden confounders via causal adjustment, suggesting the approach is conceptually aligned with broader causal RL efforts.
- **Break condition:** If confounders affect states and actions through paths not blocked by (s_{t-1}, a_{t-1}), or if confounding operates at longer temporal lags, backdoor adjustment will be incomplete and bias persists.

### Mechanism 2
- **Claim:** A neural network can learn to infer pseudo-past variables from the current state with sufficient accuracy for backdoor adjustment.
- **Mechanism:** The Backdoor Reconstructor learns p_φ(a_{t-1}, s_{t-1}|s_t) by training on consecutive transition tuples from the replay buffer. Since standard RL already stores (s_t, a_t, s_{t+1}), the training signal is readily available. The reconstructor effectively learns an inverse dynamics model.
- **Core assumption:** The current state s_t encodes enough information about the previous state and action to make accurate predictions; the environment is not so stochastic that s_{t-1}, a_{t-1} are effectively unrecoverable from s_t.
- **Evidence anchors:**
  - [Section 4.3] "we learn a conditional model p_φ(a_{t-1}, s_{t-1}|s_t) to predict the pseudo-past from the current state."
  - [Figure 2] Shows the two-stage sampling architecture where Backdoor Reconstructor outputs to Interventional Actor.
  - [corpus] No direct corpus evidence on reconstructor accuracy requirements; this remains a potential point of failure.
- **Break condition:** In highly stochastic or partially observable environments where s_t provides weak signal about (s_{t-1}, a_{t-1}), the reconstructor may produce poor approximations, leading to inadequate confounding adjustment.

### Mechanism 3
- **Claim:** Maximizing causal entropy (entropy under the interventional distribution) encourages exploration while maintaining robustness to confounding.
- **Mechanism:** Standard SAC maximizes entropy H(a|s) to encourage exploration. DoSAC replaces this with H(a|do(s)), which measures uncertainty with respect to the causal policy. This ensures the exploration bonus is computed on the debiased policy rather than the confounded one.
- **Core assumption:** The interventional policy is the correct target for exploration; exploration under the observational distribution would reinforce spurious correlations.
- **Evidence anchors:**
  - [Section 4.2] "By maximizing causal entropy, the agent seeks to make its policy as 'random' or uncertain as possible with respect to the causal policy, and hence learning a robust policy which is invariant to the confounders."
  - [Lemma 1] Proves monotonic improvement under the interventional objective.
  - [corpus] Conservative SAC variants (arXiv:2505.03356) address stability but not confounding; DoSAC's entropy mechanism is distinct.
- **Break condition:** If the interventional policy is poorly estimated due to weak reconstructor performance, causal entropy regularization may amplify noise rather than beneficial exploration.

## Foundational Learning

- **Concept: Soft Actor-Critic (SAC)**
  - **Why needed here:** DoSAC extends SAC directly; understanding the baseline objective, entropy regularization, and soft Q-learning is prerequisite to grasping the modifications.
  - **Quick check question:** Can you explain why SAC maximizes both expected return and entropy, and how the soft Bellman backup differs from standard Q-learning?

- **Concept: Backdoor Criterion and do-Calculus**
  - **Why needed here:** The core theoretical contribution relies on Pearl's backdoor adjustment to estimate interventional distributions from observational data.
  - **Quick check question:** Given a causal graph where U → X and U → Y, what variables must you condition on to estimate P(Y|do(X))? What if X also causes Y directly?

- **Concept: Hidden Confounding in Sequential Decision-Making**
  - **Why needed here:** The problem formulation assumes specific temporal confounding structures; understanding how unobserved variables create spurious correlations in trajectories is essential.
  - **Quick check question:** In an RL setting, if an unobserved variable affects both action selection and environment transitions, how would this bias a learned policy?

## Architecture Onboarding

- **Component map:**
  Backdoor Reconstructor (φ) -> Interventional Actor (θ) -> Environment
  Soft Q-Critic (ψ) <- Q-targets (uses sampled pseudo-past)

- **Critical path:**
  1. Sample s_t from replay buffer
  2. Backdoor Reconstructor draws pseudo-past (s̃_{t-1}, ã_{t-1}) ~ p_φ(·|s_t)
  3. Interventional Actor samples action a ~ p_θ(·|s_t, s̃_{t-1}, ã_{t-1})
  4. Compute Q-target using causal entropy: r + γ[Q(s', a') - α log π(a'|do(s'))]
  5. Update critic, actor, and reconstructor jointly

- **Design tradeoffs:**
  - Reconstructor capacity vs. overfitting: Too simple may fail to capture inverse dynamics; too complex may memorize without generalizing
  - Pseudo-past sample count: Paper uses single sample; multiple samples could improve robustness at computational cost
  - Assumption: Paper assumes stationary confounders; dynamic confounding may require architectural extensions

- **Failure signatures:**
  - Reconstructor loss diverging or plateauing early → check if inverse dynamics learnable in the environment
  - Performance degrading under evaluation without confounders despite training gains → possible overfitting to confounded structure
  - No improvement over SAC even with confounding → verify confounder injection is actually creating spurious correlations

- **First 3 experiments:**
  1. Reproduce the DoSAC vs. SAC comparison on Ant with confounding (σ=1.0), training for 2M steps with 5 random seeds; verify performance gap matches paper.
  2. Ablate the Backdoor Reconstructor by replacing learned p_φ with ground-truth (s_{t-1}, a_{t-1}) from buffer; quantify performance upper bound.
  3. Stress-test reconstructor quality: train DoSAC in environments with increasing transition stochasticity and measure how reconstructor accuracy correlates with final policy performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the DoSAC framework be extended to address temporal confounding explicitly?
- **Basis in paper:** [explicit] The Conclusion states that future research directions include "addressing temporal confounding explicitly."
- **Why unresolved:** The current formulation targets stationary confounders and utilizes a fixed replay buffer, which restricts applicability in dynamic environments where confounding effects change over time.
- **What evidence would resolve it:** A theoretical extension of the causal graph to include temporal edges and empirical validation showing policy robustness in environments with time-varying confounding distributions.

### Open Question 2
- **Question:** Can the Backdoor Reconstructor be effectively adapted for discrete action spaces?
- **Basis in paper:** [explicit] The authors note the limitation to continuous action spaces and anticipate "extending the methodology to discrete action spaces."
- **Why unresolved:** The current implementation relies on SAC and Gaussian policies; the gradient estimation and entropy calculation for discrete interventional policies require a different architectural approach.
- **What evidence would resolve it:** Successful application of a discrete DoSAC variant on standard benchmarks (e.g., Atari) showing improved robustness over standard DQN or discrete SAC baselines.

### Open Question 3
- **Question:** Under what conditions does the pseudo-past inference fail in highly stochastic or partially observable settings?
- **Basis in paper:** [inferred] The Conclusion acknowledges that the assumption of adequate mitigation via pseudo-past inference "may prove insufficient in highly stochastic or partially observable settings."
- **Why unresolved:** The Backdoor Reconstructor infers past variables solely from the current state; high noise floors or missing observations likely degrade reconstruction accuracy, breaking the backdoor adjustment.
- **What evidence would resolve it:** An ablation study measuring the correlation between reconstruction error and policy bias as environmental stochasticity (observation noise) increases.

### Open Question 4
- **Question:** Does integrating DoSAC with model-based RL or recurrent architectures enhance causal generalization?
- **Basis in paper:** [explicit] The authors list "exploring integration with model-based reinforcement learning or recurrent architectures" as a future direction.
- **Why unresolved:** Model-based components might better capture the latent confounders than the current reconstructor, but it is unclear if the sample efficiency gains would outweigh the complexity of fitting a causal world model.
- **What evidence would resolve it:** Comparative analysis showing that a recurrent or model-based DoSAC variant outperforms the standard feed-forward version in POMDP benchmarks.

## Limitations
- The method assumes (s_{t-1}, a_{t-1}) fully blocks confounding paths, which may not hold for complex confounding structures or longer temporal dependencies
- Backdoor Reconstructor performance may degrade in highly stochastic or partially observable environments where current state provides weak signal about past variables
- The approach is limited to continuous action spaces and requires environments where inverse dynamics are learnable

## Confidence
- **High confidence:** The theoretical foundation using backdoor adjustment from causal inference is sound when assumptions hold
- **Medium confidence:** Empirical results show consistent improvements on tested benchmarks, but results are limited to specific continuous control tasks with synthetic confounders
- **Low confidence:** Claims about general robustness and policy reliability require validation across diverse environment types and confounding structures

## Next Checks
1. Measure Backdoor Reconstructor accuracy (e.g., MSE on pseudo-past prediction) across different environment stochasticities and correlate with policy performance
2. Test DoSAC under dynamic confounding where confounding strength varies over training time to assess adaptation capabilities
3. Evaluate performance degradation when the reconstructor is intentionally degraded (e.g., reduced capacity, added noise) to establish sensitivity bounds