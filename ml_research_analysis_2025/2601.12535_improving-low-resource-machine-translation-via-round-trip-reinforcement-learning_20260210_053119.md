---
ver: rpa2
title: Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning
arxiv_id: '2601.12535'
source_url: https://arxiv.org/abs/2601.12535
tags:
- translation
- chrf
- trained
- training
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes a self-supervised reinforcement learning approach
  to improve low-resource machine translation without parallel data. The method uses
  round-trip bootstrapping: translating English into a low-resource language and back,
  then applying GRPO to optimize reconstruction quality via chrF++ and BLEU rewards.'
---

# Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.12535
- Source URL: https://arxiv.org/abs/2601.12535
- Reference count: 23
- Primary result: Round-trip RL boosts chrF++ scores for low-resource NLLB MT up to 3.7x

## Executive Summary
This paper introduces a self-supervised reinforcement learning method for low-resource machine translation that does not require parallel data. The approach uses round-trip bootstrapping: translating English into a low-resource language and back, then optimizing reconstruction quality via Group Relative Policy Optimization (GRPO) using chrF++ and BLEU rewards. Experiments with NLLB models on four low-resource languages show consistent improvements in chrF++ scores, with gains most pronounced for very low-resource languages. The method maintains translation fluency while outperforming unsupervised and back-translation baselines.

## Method Summary
The proposed approach leverages round-trip bootstrapping combined with Group Relative Policy Optimization (GRPO) to improve low-resource machine translation without requiring parallel data. The method translates English sentences into the target low-resource language and back to English, using the reconstruction quality as a reward signal. GRPO optimizes the translation model by comparing the rewards of different translation candidates within a group, rather than against absolute baselines. This self-supervised learning process iteratively refines translations to maximize chrF++ and BLEU scores of the round-trip reconstruction, enabling improvement even when no direct supervision exists for the low-resource language pair.

## Key Results
- chrF++ improvements across all four tested low-resource languages (Aymara: 28.13→64.09, Friulian: 16.61→70.46, Wolof: 23.47→48.51, Russian: 8.53→48.73)
- Consistent gains over unsupervised and back-translation baselines
- Improvements scale with model size and do not degrade translation fluency

## Why This Works (Mechanism)
The method exploits the natural consistency signal available from round-trip translation: if a translation is accurate, translating it to a target language and back should reconstruct the original meaning. By optimizing for reconstruction quality using chrF++ and BLEU rewards through GRPO, the model learns to produce translations that are both accurate in the target language and faithful to the source. This creates a self-supervised learning signal that doesn't require parallel corpora. The round-trip process provides a proxy for translation quality that is particularly valuable for low-resource languages where direct evaluation is difficult.

## Foundational Learning
- **Reinforcement Learning in NLP**: Needed to optimize translation policies using reward signals rather than direct supervision; quick check: model should maximize reconstruction reward over training epochs.
- **Round-Trip Translation**: Creates a self-supervised signal by translating source→target→source; quick check: round-trip reconstruction should improve as training progresses.
- **chrF++ Metric**: Character n-gram F-score that measures translation quality at character level; quick check: higher chrF++ should correlate with better translation quality.
- **Group Relative Policy Optimization (GRPO)**: A variant of RL that compares multiple translation candidates within a group; quick check: policy should converge to stable improvement without catastrophic forgetting.

## Architecture Onboarding
**Component Map**: English Source -> NLLB Model -> Target Language -> NLLB Model -> English Reconstruction -> Reward Calculation (chrF++/BLEU) -> GRPO Update

**Critical Path**: Source text → Target translation → Reconstruction → Reward computation → Policy update

**Design Tradeoffs**: The round-trip approach trades computational cost (two translations per update) for the benefit of self-supervision, avoiding the need for parallel corpora. GRPO's group-relative comparison provides more stable updates than absolute reward maximization.

**Failure Signatures**: If round-trip reconstruction quality plateaus or degrades, the policy may have converged suboptimally or the reward signal may be insufficient. Poor fluency in target translations indicates the need for stronger language modeling components.

**First Experiments**:
1. Measure round-trip reconstruction quality before and after RL fine-tuning
2. Compare chrF++ improvements across different group sizes in GRPO
3. Test the method with varying amounts of monolingual target language data

## Open Questions the Paper Calls Out
None

## Limitations
- Absolute performance levels on some languages remain far from state-of-the-art despite improvements
- Experimental scope limited to four low-resource languages and NLLB model family
- Computational overhead of reinforcement learning fine-tuning not addressed

## Confidence
- **High Confidence**: chrF++ and BLEU improvements on tested languages; method does not degrade fluency; benefits scale with model size
- **Medium Confidence**: superiority over unsupervised and back-translation baselines; qualitative reduction in repetition and semantic drift
- **Low Confidence**: absolute performance on some languages; generalizability beyond NLLB and tested languages; practical utility in production settings

## Next Checks
1. Conduct large-scale human evaluation (e.g., MQM, COMET) to confirm qualitative improvements and rule out subtle quality degradations
2. Test the method on a broader set of languages and model architectures (e.g., Transformer, mBART) to assess generalizability
3. Evaluate computational efficiency and scalability, including time and resource costs for RL fine-tuning, to determine practical deployment feasibility