---
ver: rpa2
title: 'From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement
  Learning'
arxiv_id: '2601.23058'
source_url: https://arxiv.org/abs/2601.23058
tags:
- reward
- relative
- rlrr
- ranking
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the instability of absolute reward signals
  in group-based reinforcement learning, which causes sparse gradients and training
  instability in large language model reasoning. To solve this, it proposes RLRR,
  a framework that shifts from absolute scoring to relative ranking within groups,
  and introduces a Ranking Reward Model that directly outputs relative quality rankings.
---

# From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.23058
- Source URL: https://arxiv.org/abs/2601.23058
- Reference count: 40
- Key outcome: RLRR improves reasoning accuracy up to 24.8% on AIME 2025 and reduces token usage vs GRPO/DAPO baselines

## Executive Summary
This paper addresses the instability of absolute reward signals in group-based reinforcement learning for large language models, which causes sparse gradients and training instability in reasoning tasks. The authors propose RLRR (Relative Learning via Reward Reshaping), a framework that shifts from absolute scoring to relative ranking within groups. They introduce a Ranking Reward Model that directly outputs relative quality rankings, combined with hierarchical re-ranking that prioritizes correctness over learned preferences. Experiments show significant improvements in reasoning accuracy and inference efficiency across mathematical and logical benchmarks, with the 1.5B model achieving up to 24.8% accuracy on AIME 2025 while using fewer tokens.

## Method Summary
RLRR replaces absolute scalar rewards with relative rankings within groups of generated responses. For verifiable tasks, it uses Hybrid Relative Rewards (HRR) that combine rule-based correctness with bounded rank-based adjustments via a hyperbolic tangent function. For open-ended tasks, Pure Relative Rewards (PRR) linearly map ranks to a fixed [0,1] interval. A Ranking Reward Model directly outputs listwise rankings rather than scalar scores. Responses are hierarchically re-ranked prioritizing correctness > length > model preference, and a correctness-aware advantage clipping mechanism prevents the model from being penalized for correct answers that rank low among other correct answers.

## Key Results
- 1.5B model achieves 24.8% accuracy on AIME 2025, outperforming larger GRPO/DAPO baselines
- Significant reduction in average token usage while maintaining or improving accuracy
- Ranking RM outperforms converted scalar RMs in open-ended writing tasks
- Ablation studies confirm benefits of relative rewards, correctness-aware ranking, and hierarchical re-ranking

## Why This Works (Mechanism)

### Mechanism 1: Gradient Signal Recovery via Rank-Based Tie-Breaking
Standard GRPO computes advantage as $A = s - \text{mean}(s)$. When a verifier assigns identical binary scores to all responses in a group, variance collapses to zero, resulting in zero gradients. RLRR injects a bounded rank-based term into the reward, differentiating between identical outcomes based on quality, ensuring $A \neq 0$ and allowing the model to learn preferences even among correct solutions.

### Mechanism 2: Variance Stabilization via Bounded Normalization
Scalar Reward Models often suffer from unbounded outputs, causing outliers that skew the group mean baseline. Pure Relative Rewards linearly map ranks to a fixed $[0,1]$ interval, strictly capping variance via Popoviciu's inequality. This ensures the advantage signal reflects relative quality rather than the magnitude of model overconfidence.

### Mechanism 3: Hierarchical Correctness Alignment
Reward models may preferentially rank an incorrect but well-written response higher than a correct but terse one. RLRR implements a hard hierarchy: Correctness > Length > Reward Score. This forces the optimization to first ensure validity (rule-based), then efficiency (length), and finally subjective quality (RM), aligning the gradient signal with ground truth.

## Foundational Learning

- **Group-Relative Advantage Estimation**: GRPO uses the mean of a generated group as a baseline rather than a learned value function. Quick check: If all 8 samples receive reward 1.0, what is the advantage of the 3rd sample in standard GRPO, and how does RLRR change this?

- **Listwise vs. Pointwise Preference Learning**: The Ranking Reward Model differs from standard pointwise scoring. Quick check: Does a pointwise model know that Response A is better than Response B when scoring them? Does a listwise model know?

- **Reward Hacking / Sparsity**: Relative rewards solve sparsity and score manipulation. Quick check: Why might a scalar reward model output 1000 for a mediocre response, and how does rank normalization prevent this from destabilizing training?

## Architecture Onboarding

- **Component map**: Rollout Engine -> Verifier (Rule-based) -> Ranking Reward Model -> Hierarchical Merger -> Reward Shaper -> Optimizer
- **Critical path**: The Hierarchical Re-ranking logic is the safety layer. Do not implement the gradient update until the sorting logic strictly enforces that incorrect answers never outrank correct ones.
- **Design tradeoffs**: Use HRR for Math/Code (where truth exists) to ground the model; use PRR for creative writing (where truth is subjective). A dedicated Ranking RM is more accurate but requires training a specific listwise model.
- **Failure signatures**: Without length re-ranking, the model may learn to produce excessively long correct answers. Without correctness-aware clipping, you might discourage the model from being correct.
- **First 3 experiments**: 1) Reproduce Figure 1a showing effective ratio baseline for GRPO vs RLRR on GSM8K. 2) Implement re-ranking logic and ablate "Correctness" priority (Table 3). 3) Test Ranking RM trained on math data against general RM using Best-of-N scaling method (Figure 4).

## Open Questions the Paper Calls Out
1. What mechanisms can further improve the efficiency of leveraging relative rewards in group-based RL?
2. How does RLRR performance scale with model size beyond the 8B parameter models tested?
3. What is the optimal trade-off between ranking granularity and group size for RLRR?
4. Can Ï„ (rank-based adjustment magnitude) be adaptively tuned during training rather than fixed at 0.1?

## Limitations
- The stability and generalization of the ranking mechanism across diverse domains remains untested
- The computational overhead of training and deploying the specialized Ranking RM is not fully addressed
- The exact contribution of the Ranking RM versus the reward shaping mechanism itself is not fully disentangled

## Confidence
- **High Confidence**: Mathematical formulation of relative rewards and hierarchical re-ranking is internally consistent and theoretically grounded
- **Medium Confidence**: Empirical improvements on reasoning benchmarks are significant and reproducible, but exact contribution of Ranking RM is unclear
- **Low Confidence**: Scalability to extremely large models and diverse domains, and robustness to verifier errors in open-ended tasks, remain open questions

## Next Checks
1. Evaluate RLRR with the same Ranking RM across multiple reasoning domains (math, code, logical puzzles) to assess generalization
2. Systematically inject controlled noise into the rule-based verifier and measure degradation in RLRR performance
3. Measure and compare training time, inference latency, and parameter count of the Ranking RM versus standard scalar Reward Model