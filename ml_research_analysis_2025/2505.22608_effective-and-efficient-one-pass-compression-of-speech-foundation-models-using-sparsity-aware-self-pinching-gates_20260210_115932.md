---
ver: rpa2
title: Effective and Efficient One-pass Compression of Speech Foundation Models Using
  Sparsity-aware Self-pinching Gates
arxiv_id: '2505.22608'
source_url: https://arxiv.org/abs/2505.22608
tags:
- pruning
- speech
- compression
- pruned
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a one-pass compression method for speech foundation
  models using sparsity-aware self-pinching gates. The method integrates model pruning
  and parameter update into a single stage by using layer-level tied gates with learnable
  thresholds, enabling fine-grained neuron-level pruning.
---

# Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates

## Quick Facts
- arXiv ID: 2505.22608
- Source URL: https://arxiv.org/abs/2505.22608
- Reference count: 0
- Primary result: 65% and 60% parameter reduction for wav2vec2.0-base and HuBERT-large with no statistically significant WER increase on test-clean

## Executive Summary
This paper introduces a one-pass compression method for speech foundation models using sparsity-aware self-pinching gates. The approach integrates model pruning and parameter update into a single stage by using layer-level tied gates with learnable thresholds, enabling fine-grained neuron-level pruning. Experiments on LibriSpeech-100hr demonstrate that the method achieves state-of-the-art compression results while maintaining performance, reducing fine-tuning time by at least 25% compared to prior works.

## Method Summary
The method uses layer-level tied self-pinching gates with learnable thresholds (one per layer; 72 total for base, 144 for large) that jointly optimize pruning and fine-tuning in a single stage. The approach targets 6 linear layers per Transformer block (Q/K/V/O projections in MHSA; inter/intermediate in FFN) and uses a sparsity-aware loss combining CTC loss with a group Lasso penalty. Training employs AdamW optimizer with linear warmup and decay, cosine-annealed temperature for the sigmoid function, and stops when target sparsity is reached.

## Key Results
- Achieves 65% parameter reduction for wav2vec2.0-base and 60% for HuBERT-large
- Maintains WER performance with no statistically significant increase on test-clean
- Reduces fine-tuning time by at least 25% compared to prior works
- Achieves the lowest WER of 7.05% under comparable compression ratios

## Why This Works (Mechanism)
The self-pinching gate mechanism works by using layer-level tied thresholds that create binary masks through a sigmoid function with temperature annealing. The learnable thresholds are initialized small (1e-5) and gradually increase during training, allowing the model to determine which neurons to prune based on their importance. The cosine-annealed temperature schedule ensures smooth transition from soft to hard pruning decisions. The sparsity-aware loss function balances reconstruction accuracy with model compactness through a weighted combination of CTC loss and L0 regularization.

## Foundational Learning
- **Self-pinching gates**: Layer-level tied binary masks that enable neuron-level pruning through learnable thresholds and temperature annealing. Why needed: Enables fine-grained pruning while maintaining computational efficiency. Quick check: Verify mask sparsity per layer during training.
- **Group Lasso regularization**: L0-based sparsity penalty that encourages parameter elimination. Why needed: Provides theoretical foundation for structured pruning. Quick check: Monitor sparsity loss weight impact on final compression.
- **Straight-through estimator (STE)**: Gradient approximation for binary mask computation. Why needed: Enables backpropagation through discrete mask decisions. Quick check: Validate STE gradients match expected pruning behavior.
- **Cosine annealing schedule**: Temperature decay from 0.5 to 0.01 for sigmoid function. Why needed: Gradually transitions from soft to hard pruning decisions. Quick check: Plot mask evolution against temperature schedule.
- **Layer-level tied gates**: Single threshold per layer controlling all gates in that layer. Why needed: Reduces parameter overhead while maintaining pruning flexibility. Quick check: Compare layer-wise sparsity distributions.

## Architecture Onboarding
**Component map**: Pre-trained model -> Self-pinching gates -> Sparsity-aware loss -> Fine-tuned model
**Critical path**: Input speech -> Feature extraction -> Masked self-attention with pruned projections -> Feed-forward network with pruned intermediate layers -> CTC decoder
**Design tradeoffs**: Layer-level tied gates reduce parameters but may limit per-neuron flexibility; cosine annealing provides smooth pruning but requires careful scheduling; single-stage approach is efficient but may sacrifice some pruning granularity.
**Failure signatures**: Sharp WER increase at low sparsity indicates threshold initialization issues; model collapse to all-zero masks suggests temperature scheduling problems; inconsistent sparsity across layers may indicate learning instability.
**First experiments**: 1) Train with η=0 to establish CTC-only baseline WER, 2) Run ablation with fixed vs. learned thresholds, 3) Compare cosine annealing vs. step decay temperature schedules.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two model architectures (wav2vec2.0-base and HuBERT-large) on a single dataset (LibriSpeech-100hr)
- Sparsity targets (65% for base, 60% for large) appear empirically chosen rather than systematically derived
- Lack of ablations on different sparsity targets or alternative datasets to validate robustness
- No comparison of layer-level tied gates against alternative gating strategies

## Confidence
- Compression effectiveness (65%/60% reduction): High confidence - directly measured and reported with clear metrics
- WER stability claims: Medium confidence - statistically validated on clean sets but limited to one dataset and missing noisy condition results
- 25% training time reduction: Low confidence - the paper states this comparison but lacks detailed timing methodology and baseline specifications
- Best WER of 7.05% claim: Medium confidence - appears competitive but lacks comprehensive comparison with all relevant prior works under identical conditions

## Next Checks
1. Replicate the full training pipeline on both wav2vec2.0-base and HuBERT-large using the exact hyperparameters (η, τ schedule, threshold initialization) and verify the claimed 65%/60% compression with no significant WER increase on both clean and noisy test sets
2. Conduct ablation studies varying the sparsity targets (e.g., 50%, 70%) to understand the trade-off curve and validate the chosen 65%/60% targets are optimal
3. Test the method on an additional speech foundation model (e.g., XLS-R or Whisper) and/or a different ASR dataset (e.g., Common Voice) to assess generalization beyond the LibriSpeech-100hr experiments