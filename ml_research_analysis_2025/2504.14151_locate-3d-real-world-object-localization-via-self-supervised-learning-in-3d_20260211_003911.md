---
ver: rpa2
title: 'Locate 3D: Real-World Object Localization via Self-Supervised Learning in
  3D'
arxiv_id: '2504.14151'
source_url: https://arxiv.org/abs/2504.14151
tags:
- features
- point
- d-jepa
- object
- decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Locate 3D, a method for localizing objects
  in 3D scenes from referring expressions using real-world sensor data. The approach
  introduces 3D-JEPA, a novel self-supervised learning algorithm that learns contextualized
  3D scene representations by lifting features from 2D foundation models into 3D point
  clouds and performing masked prediction in latent space.
---

# Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D

## Quick Facts
- arXiv ID: 2504.14151
- Source URL: https://arxiv.org/abs/2504.14151
- Reference count: 37
- Primary result: 61.7% accuracy at IoU 0.25 on 3D referential grounding benchmarks

## Executive Summary
Locate 3D presents a method for localizing objects in 3D scenes from referring expressions using real-world sensor data without requiring refined meshes or instance segmentations. The approach introduces 3D-JEPA, a novel self-supervised learning algorithm that learns contextualized 3D scene representations by lifting features from 2D foundation models into 3D point clouds and performing masked prediction in latent space. Experiments show Locate 3D achieves state-of-the-art performance on standard 3D referential grounding benchmarks while demonstrating strong generalization to novel environments and successful deployment on a robot for real-world object localization and manipulation tasks.

## Method Summary
Locate 3D is a three-phase pipeline for 3D object localization from referring expressions. First, it preprocesses RGB-D frames by lifting dense 2D semantic features from CLIP and DINOv2 foundation models into 3D point clouds using depth and camera poses for back-projection. Second, it applies 3D-JEPA self-supervised learning to convert these local features into global, context-aware scene representations through masked prediction in latent space. Third, it fine-tunes a language-conditioned 3D decoder to jointly predict 3D masks and bounding boxes, using a composite loss function that combines dense mask supervision with spatial box constraints. The method operates directly on sensor observation streams with 5cm voxel resolution and employs stage-wise learning rate scheduling for stable training.

## Key Results
- Achieves 61.7% accuracy at IoU 0.25 on 3D referential grounding benchmarks, outperforming prior methods
- Demonstrates successful real-world deployment on a robot for object localization and manipulation tasks
- Shows strong generalization to novel environments not seen during training
- Joint mask and box prediction significantly outperforms either supervision method alone (box-only: 0.3% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Feature Lifting from 2D Foundation Models
Lifting dense 2D semantic features into sparse 3D point clouds provides superior object localization signals compared to raw 3D geometry or RGB data. The model projects CLIP and DINOv2 features from multi-view RGB images onto 3D points via back-projection using depth and camera poses, transferring robust semantic capabilities of large-scale 2D foundation models to the 3D domain. Performance degrades significantly if 2D foundation models are removed or replaced with raw RGB values.

### Mechanism 2: 3D-JEPA Contextualization via Latent Prediction
Self-supervised learning via masked prediction in latent space converts local features into global, context-aware scene representations. Unlike standard masking which reconstructs input pixels, 3D-JEPA masks regions of the point cloud and trains a predictor to guess the latent embeddings of those regions generated by a target encoder. This forces the encoder to learn scene-level dependencies rather than just local interpolation. The mechanism fails if masking is too sparse or predictor capacity is insufficient.

### Mechanism 3: Joint 3D Mask and Box Prediction
Simultaneously predicting 3D binary masks and axis-aligned bounding boxes creates a robust localization signal that outperforms either supervision method alone. The composite loss function combines Dice/cross-entropy (for dense masks) and L1/GIoU (for boxes), providing both fine-grained semantic supervision and strong spatial constraints. Performance drops dramatically if either component is removed, with box-only supervision yielding near-zero accuracy.

## Foundational Learning

- **Concept: Joint Embedding Predictive Architecture (JEPA)** - Predicting latent representations rather than inputs is the core engine of the model's "scene understanding." Quick check: How does predicting a latent vector differ from reconstructing missing pixels/points in standard Masked Autoencoders (MAE)?

- **Concept: Feature Lifting (2D-to-3D Back-projection)** - The entire pipeline relies on "lifting" 2D features. Quick check: Given a pixel coordinate $(u, v)$, a depth value $d$, and camera intrinsics $K$, how do you calculate the 3D point $P$?

- **Concept: Bipartite Matching (Hungarian Algorithm)** - The decoder predicts a set of queries $Q$, but the ground truth has a different structure. Quick check: Why can't we simply calculate loss between the first predicted query and the first ground truth object without matching?

## Architecture Onboarding

- **Component map:** RGB-D frames + Pose -> Preprocessor (CLIP, DINO, SAM -> Back-projection to Voxel Map) -> Encoder (3D-JEPA with PointTransformer-v3) -> Decoder (8-layer Transformer with Text + Object Queries) -> Heads (Mask Head + Box Head)

- **Critical path:** The stage-wise learning rate schedule is most fragile. Must freeze encoder initially, then unfreeze with 0.5× decoder LR to stabilize training.

- **Design tradeoffs:** 5cm voxel size balances memory/throughput vs geometric detail; latent vs input prediction trades simplicity for stability; SAM for CLIP sacrifices speed for spatial precision.

- **Failure signatures:** Random encoder initialization drops Acc@25 from ~62% to ~58-59%; box-only supervision collapses to 0.3% accuracy; naive fine-tuning destroys pre-trained features.

- **First 3 experiments:** 1) Verify lifting by visualizing 3D point cloud colored by CLIP features; 2) Ablate JEPA by training "scratch" vs "pre-trained" models on small subset; 3) Stress test decoder with "box-only" and "mask-only" heads to reproduce failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
How can the Locate 3D architecture be adapted to handle dynamic environments requiring real-time feature updates? The current pipeline depends on offline caching of 2D features, which doesn't account for moving or changing objects. An extension evaluated on a video dataset with moving objects would demonstrate localization accuracy without pre-computed static feature maps.

### Open Question 2
How can the model be improved to ground objects using high-level spatial context, such as room names? The current model struggles with references that rely on semantic room labels rather than just relative spatial relations. Experiments on a dataset annotated with hierarchical spatial relations (room → area → object) would show improved accuracy on room-conditioned queries.

### Open Question 3
Why does training with bounding box-only supervision fail so drastically (0.3% accuracy) compared to mask-only supervision? The authors attribute this to lack of dense supervision, but the near-zero performance suggests fundamental optimization or architectural instability when mask loss is removed. An ablation study analyzing gradient flow would verify if sparsity of box gradients prevents 3D-JEPA encoder from fine-tuning effectively.

## Limitations
- Extension to dynamic scenes requiring real-time feature updates remains an open research area
- Struggles with references that include high-level spatial context like room names
- Real-world deployment demonstrated only in limited apartment setting

## Confidence
- **High confidence**: 2D foundation features significantly improve localization (53.9% vs 28.9% without CF features); failure modes of box-only supervision are clearly demonstrated
- **Medium confidence**: 3D-JEPA mechanism shows promise but lacks detailed hyperparameter specifications; generalization to novel environments is demonstrated but with limited scope
- **Medium confidence**: Real-world deployment success shown but only in one apartment setting

## Next Checks
1. Ablation of masking hyperparameters - systematically vary masking percentage and number of masks in 3D-JEPA to determine sensitivity
2. Cross-dataset generalization test - evaluate model on held-out dataset from different sensor modality or environmental domain
3. Component interdependence analysis - remove each major component (2D features, 3D-JEPA, joint prediction) individually to quantify independent contributions