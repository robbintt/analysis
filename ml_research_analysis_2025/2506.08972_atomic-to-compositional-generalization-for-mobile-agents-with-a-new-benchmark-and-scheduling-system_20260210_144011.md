---
ver: rpa2
title: Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark
  and Scheduling System
arxiv_id: '2506.08972'
source_url: https://arxiv.org/abs/2506.08972
tags:
- arxiv
- agents
- task
- mobile
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces UI-NEXUS, a comprehensive benchmark designed
  to evaluate mobile agents on compositional tasks involving Simple Concatenation,
  Context Transition, and Deep Dive operations across 20 local utility apps and 30
  online service apps. The benchmark reveals significant atomic-to-compositional generalization
  gaps in existing mobile agents, which struggle with long-horizon progress management,
  intermediate information transition, and reasoning-action integration.
---

# Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System

## Quick Facts
- **arXiv ID**: 2506.08972
- **Source URL**: https://arxiv.org/abs/2506.08972
- **Reference count**: 40
- **One-line primary result**: AGENT-NEXUS improves mobile agent compositional task success rates by 24% to 40% through dynamic task decomposition and process memory.

## Executive Summary
This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on compositional tasks involving Simple Concatenation, Context Transition, and Deep Dive operations across 20 local utility apps and 30 online service apps. The benchmark reveals significant atomic-to-compositional generalization gaps in existing mobile agents, which struggle with long-horizon progress management, intermediate information transition, and reasoning-action integration. To address these challenges, the authors propose AGENT-NEXUS, a lightweight scheduling system that dynamically decomposes long-horizon tasks into self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvements on compositional tasks within UI-NEXUS without significantly sacrificing inference overhead.

## Method Summary
The AGENT-NEXUS system features a Scheduling Module (Orchestrator) that decomposes complex mobile tasks into atomic subtasks using GPT-4o, and an Execution Module that separates reasoning (Analyst) from action (Navigator) while maintaining Process Memory for intermediate results. The system is evaluated on UI-NEXUS benchmark tasks using Android Emulator for local apps and physical devices for online apps, with baseline agents including M3A and UI-TARS-7B-SFT. Dynamic decomposition, process memory, and reasoning-action separation are the core innovations that address the identified failure modes of over-execution, under-execution, and attention drift.

## Key Results
- AGENT-NEXUS achieves 24% to 40% task success rate improvements on compositional tasks within UI-NEXUS
- The system significantly outperforms baseline agents (M3A, UI-TARS) on Context Transition and Deep Dive tasks
- Performance improvements come with 40-60% increase in inference latency and cost due to multiple LLM calls per step

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dynamically decomposing long-horizon instructions into atomic subtasks reduces failure rates by lowering the cognitive load on the execution agent (Context Reduction).
- **Mechanism**: The Scheduling Module (Orchestrator) breaks complex goals into self-contained atomic instructions. The Execution Module only receives one simple instruction per global step. This forces a narrow focus, preventing the agent from being overwhelmed by distant dependencies.
- **Core assumption**: The backbone model is capable of solving atomic tasks but fails when required to maintain a plan for multi-step compositional logic within a single context window.
- **Evidence anchors**:
  - [abstract] Mentions the system extrapolates abilities by "dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks."
  - [section 4] Explicitly states the system "features a context reduction philosophy, where only a self-contained simple instruction is assigned... to significantly lower the cognitive load."
  - [corpus] *Hi-Agent* (neighbor) supports the efficacy of hierarchical structures for mobile control, though it does not validate this specific reduction philosophy.
- **Break condition**: If the Orchestrator fails to decompose dependencies correctly (e.g., misses a conditional branch), the Execution Module will receive an invalid instruction, leading to immediate failure.

### Mechanism 2
- **Claim**: Decoupling "reasoning" (Analyst) from "acting" (Navigator) prevents "breakdowns of thinking-acting arbitration" and reduces redundant actions.
- **Mechanism**: The system defines distinct subtask types: `act` (UI manipulation), `think` (reasoning/summarization), and `tool` (shortcuts). By handling reasoning via an Analyst and action via a Navigator, the system forces a state-pause for information processing before resuming execution, avoiding the "action loop" trap.
- **Core assumption**: Specialized models or prompts for reasoning yield better intermediate results than a general agent trying to reason while simultaneously outputting UI coordinates.
- **Evidence anchors**:
  - [section 4] Describes the Execution Module separating the "Analyst" (general reasoning) from the "Navigator" (device interactions).
  - [appendix d] Identifies "Breakdown of thinking-acting arbitration" as a key failure mode where agents remain in an action loop instead of pausing to reason.
  - [corpus] Evidence is weak in the immediate corpus for this specific decoupling; general literature supports modularity, but specific validation is confined to this paper's results.
- **Break condition**: If the cost of switching between Analyst and Navigator (latency/API calls) outweighs the efficiency gains, or if the Analyst produces ungrounded reasoning, the system degrades.

### Mechanism 3
- **Claim**: Explicit "Process Memory" mitigates "fabricated content" and information loss during Context Transitions.
- **Mechanism**: A structured memory logs the output of completed subtasks (e.g., extracted dates, summaries). When a subsequent subtask requires this info, the Orchestrator injects it from memory rather than relying on the agent's latent context window.
- **Core assumption**: Agents suffer from "faulty information management" (Appendix D) where intermediate data is lost or hallucinated if not explicitly externalized.
- **Evidence anchors**:
  - [section 4] Notes Process Memory "stores and utilizes intermediate information across subtasks" to handle dependencies.
  - [appendix d] Cites "Faulty information management in context transitions" where agents inject invented values; memory aims to solve this.
  - [corpus] *Mobile-Bench-v2* implies the need for better evaluation of VLMs, but does not specifically validate the Process Memory architecture.
- **Break condition**: If the memory retrieval mechanism retrieves irrelevant prior context or if the return signal format is unstructured, it may confuse the Orchestrator.

## Foundational Learning

- **Concept**: **Atomic vs. Compositional Generalization**
  - **Why needed here**: The paper benchmarks the "gap" between performing single tasks (atomic) and chaining them (compositional). You must understand this distinction to interpret the Performance Gap Recovered (PGR) metric.
  - **Quick check question**: Can you explain why an agent might succeed at "Open Settings" and "Click Bluetooth" separately, but fail at "Turn on Bluetooth"?

- **Concept**: **Markov Decision Process (MDP) in GUI Tasks**
  - **Why needed here**: Section 3.2 formalizes tasks as MDPs ($\langle G, S, A, T, R, H \rangle$). Understanding the state space ($S$) and horizon ($H$) is critical for designing the "Step Budget" (Appendix E).
  - **Quick check question**: In the UI-NEXUS formulation, what constitutes the "State" vs. the "Action"?

- **Concept**: **Agentic Workflow vs. Agent-as-a-Model**
  - **Why needed here**: Section 5.1 distinguishes baselines (e.g., M3A uses GPT-4o workflow vs. UI-TARS uses fine-tuned model). Architecture onboarding depends on which type you are integrating.
  - **Quick check question**: Does the baseline you are testing rely on external API calls (workflow) or local inference (as-a-model)?

## Architecture Onboarding

- **Component map**: User Instruction -> Orchestrator (GPT-4o) -> Execution Layer (Navigator + Analyst + Tool) -> Process Memory -> Next Plan
- **Critical path**:
  1. User Instruction → Orchestrator generates Plan (Sequence of Subtasks).
  2. Orchestrator selects first Subtask → Sends to Execution Layer.
  3. Executor (Navigator/Analyst) runs → Returns Signal + Result.
  4. Orchestrator writes Result to Process Memory.
  5. Orchestrator generates next Plan based on updated Memory (Dynamic Re-planning).

- **Design tradeoffs**:
  - **Performance vs. Cost**: AGENT-NEXUS improves success rates (Table 2) but increases latency and token cost (Table 3) due to multiple LLM calls per step (Orchestrator + Analyst + Navigator).
  - **Robustness vs. Complexity**: The hierarchical structure handles complexity but introduces a single point of failure in the Orchestrator.

- **Failure signatures**:
  - **Under-execution (Ping-Pong)**: Agent switches apps without completing tasks (common in UI-TARS baselines).
  - **Over-execution**: Agent repeats toggle actions due to deficient progress monitoring (common in OS-Atlas).
  - **Attention Drift**: Agent ignores specific conditional requirements (e.g., "if it rains...").

- **First 3 experiments**:
  1. **Baseline Establishment**: Run M3A and UI-TARS on the `UI-NEXUS-ANCHOR` subset (Local Utility Apps) to replicate the "Weak Performance" metrics from Table 2.
  2. **Ablation on Memory**: Disable the Process Memory write step to verify the drop in Context Transition tasks (confirming Mechanism 3).
  3. **Orchestrator Stress Test**: Feed the Orchestrator "Deep Dive" tasks requiring long reasoning chains to test if the `think` subtask decomposition remains coherent.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the AGENT-NEXUS architecture generalize to non-Android environments, specifically iOS or desktop platforms?
- **Basis in paper**: [explicit] Appendix A acknowledges the study is limited to Android, noting this choice "may slightly limit the generalizability of our findings to other mobile platforms such as iOS."
- **Why unresolved**: The implementation relies on Android-specific infrastructure (ADB, emulators) and accessibility trees (a11y), whereas iOS has distinct system limitations and control interfaces.
- **What evidence would resolve it**: Successful deployment and evaluation of the scheduling system on an iOS-specific benchmark, demonstrating similar performance improvements over baselines.

### Open Question 2
- **Question**: How can "agent-as-a-model" training be refined to intrinsically handle compositional dependencies without relying on an external scheduling system?
- **Basis in paper**: [inferred] Section 5.4 highlights that while efficient, agent-as-a-model baselines "struggle with compound logic like conditional branches" and suffer from limited memory spans.
- **Why unresolved**: The paper solves this via an external scheduler (AGENT-NEXUS), leaving open the question of whether the models themselves can be trained to manage these dependencies end-to-end.
- **What evidence would resolve it**: A training methodology that enables a standalone agent-as-a-model to match or exceed the performance of the combined AGENT-NEXUS system on the Context Transition and Deep Dive tasks.

### Open Question 3
- **Question**: What specific training recipes can mitigate "Chain-of-Thought (CoT) inconsistency" in specialist GUI agents?
- **Basis in paper**: [explicit] Appendix D identifies "CoT inconsistency" (mismatched thought and action) as a representative failure mode and explicitly "calls for more sophisticated training and prompting recipes."
- **Why unresolved**: The authors observe that fine-tuned agents often output reasoning that contradicts their subsequent actions, but do not propose a method to align these modalities.
- **What evidence would resolve it**: Ablation studies showing that specific loss functions or data augmentation techniques significantly reduce the rate of thought-action mismatch during execution.

## Limitations
- The reliance on GPT-4o for orchestration creates both a performance ceiling and cost barrier, with success rates plateauing at 45% for Deep Dive tasks.
- The Process Memory mechanism increases inference overhead by 40-60% compared to baseline agents due to additional API calls.
- The benchmark's focus on controlled Android environments may not capture real-world variability in app interfaces and network conditions.

## Confidence

- **High**: The 24-40% improvement in task success rates for Simple Concatenation and Context Transition tasks (Table 2) is well-supported by controlled experiments across multiple baseline agents.
- **Medium**: The mechanism explanation for why dynamic decomposition improves performance (Mechanism 1) is logically sound but relies on assumptions about model capabilities not directly tested.
- **Low**: Claims about AGENT-NEXUS being "scalable and lightweight" are questionable given the 2-3x increase in inference latency and cost (Table 3).

## Next Checks

1. **Ablation Study on Orchestrator Prompts**: Test whether the decomposition quality depends on specific prompt engineering or whether simpler instructions yield comparable results.
2. **Cross-Platform Generalization**: Evaluate AGENT-NEXUS on iOS applications to verify if the scheduling system transfers beyond Android-specific UI patterns.
3. **Cost-Performance Tradeoff Analysis**: Quantify the marginal benefit of Process Memory by comparing against cheaper retrieval methods like sliding window attention or lightweight embeddings.