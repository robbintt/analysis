---
ver: rpa2
title: Leveraging Flatness to Improve Information-Theoretic Generalization Bounds
  for SGD
arxiv_id: '2601.01465'
source_url: https://arxiv.org/abs/2601.01465
tags:
- bound
- trajectory
- generalization
- bounds
- flatness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an information-theoretic generalization bound
  for SGD that better leverages its flatness bias. Existing bounds fail to capture
  improved generalization under better flatness and are numerically loose due to inadequate
  leverage of SGD's flatness bias.
---

# Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD

## Quick Facts
- arXiv ID: 2601.01465
- Source URL: https://arxiv.org/abs/2601.01465
- Authors: Ze Peng; Jian Zhang; Yisen Wang; Lei Qi; Yinghuan Shi; Yang Gao
- Reference count: 40
- Primary result: Information-theoretic generalization bound for SGD that leverages flatness bias via "omniscient trajectory" technique, showing improved tightness and alignment with generalization trends.

## Executive Summary
This paper addresses a critical gap in information-theoretic generalization bounds for Stochastic Gradient Descent (SGD): existing bounds fail to capture the improved generalization associated with flatter minima, a key phenomenon observed in deep learning. The authors propose a novel "omniscient trajectory" technique that explicitly incorporates flatness into the bound by optimizing an auxiliary trajectory. This approach allows the bound to directly depend on flatness measures, showing that better generalization occurs when the output flatness aligns with the covariance of output weights. The bound is evaluated on ResNet-18 trained on CIFAR-10, demonstrating numerical tightness and correct reflection of generalization trends as flatness varies.

## Method Summary
The paper introduces a new information-theoretic generalization bound for SGD that explicitly leverages the algorithm's flatness bias. The key innovation is the "omniscient trajectory" technique, which optimizes an auxiliary trajectory that depends on all random variables in the training process. This allows the bound to fully incorporate flatness measures, including Hessian traces and gradient information. The method requires computing empirical and population gradients, Hessian traces (using PyHessian), and inverse Hessian-vector products (via Conjugate Gradient). The bound is estimated using Theorem 2, which balances penalty, flatness, and trajectory terms to provide a tighter bound that correctly reflects the relationship between flatness and generalization.

## Key Results
- The omniscient trajectory bound numerically aligns with true generalization gap (Test CE - Train CE) on ResNet-18/CIFAR-10, outperforming Wang & Mao 2022 and Neu et al. 2021.
- The bound correctly reflects improved generalization when flatness increases (via batch size changes) and decreases (via learning rate changes), while existing bounds fail on one or both trends.
- The bound achieves an O(1/√n) minimax rate for Gradient Descent on convex-Lipschitz-Bounded problems, addressing a limitation of previous information-theoretic approaches.

## Why This Works (Mechanism)
The omniscient trajectory technique works by explicitly incorporating flatness into the information-theoretic bound. Traditional bounds fail because they cannot capture the relationship between flat minima and generalization. By optimizing an auxiliary trajectory that depends on all training randomness, the new bound can directly measure and leverage flatness. The key insight is that when output flatness (Hessian trace) aligns with the covariance of output weights, the trajectory term in the bound becomes smaller, reflecting better generalization. This explicit dependence on flatness allows the bound to capture phenomena that existing bounds miss, such as the improved generalization observed with larger batch sizes.

## Foundational Learning
- **Information-theoretic generalization bounds**: Framework for measuring generalization using mutual information between training data and algorithm output; needed to understand the theoretical foundation and compare against existing approaches.
- **Flatness measures in deep learning**: Hessian-based measures of solution sharpness; needed to understand why flat minima generalize better and how to quantify flatness.
- **Stochastic Gradient Descent dynamics**: How SGD explores parameter space and finds flat minima; needed to understand the algorithm's inherent bias toward flatness.
- **Cross-Entropy loss properties**: Sub-Gaussian behavior and boundedness assumptions; needed to understand the statistical assumptions underlying the bound.
- **Inverse Hessian-Vector Products**: Computational technique for approximating second-order information; needed to efficiently compute flatness measures for large neural networks.

## Architecture Onboarding

**Component Map**
Training pipeline (Data split -> Model training -> Save weights) -> Statistics estimation (Gradients, Hessians) -> Bound computation (Omniscient trajectory optimization) -> Validation (Compare to generalization gap)

**Critical Path**
1. Partition training data into k subsets
2. Train k independent models on each subset
3. Compute empirical and population gradients for each model
4. Compute Hessian traces using PyHessian
5. Solve for inverse Hessian-vector products via Conjugate Gradient
6. Optimize the omniscient trajectory bound
7. Compare bound to true generalization gap

**Design Tradeoffs**
- **Validation set vs. empirical statistics**: Using population statistics improves bound accuracy but requires additional data split; the paper uses a validation set for this purpose.
- **Exact vs. approximate Hessians**: Computing exact Hessians is infeasible for ResNet-18; the paper uses Hessian-free methods (PyHessian) for traces and HVPs.
- **Bound tightness vs. computational cost**: The omniscient trajectory optimization adds computational overhead but provides significantly tighter bounds.

**Failure Signatures**
- **OOM errors**: Hessian computation failure when trying to compute full Hessian matrices instead of using Hessian-free methods.
- **Wrong trend**: Bound increases when generalization improves (e.g., with larger batch sizes), indicating incorrect λ parameter setting or Hessian approximation issues.
- **CG divergence**: Conjugate Gradient solver fails to converge for inverse Hessian-vector products, suggesting poor Hessian approximation or ill-conditioned problems.

**First Experiments**
1. Train a single ResNet-18 on a 10% CIFAR-10 subset, compute gradients and Hessian trace, verify basic pipeline works.
2. Test PyHessian on a small MLP to ensure Hessian trace computation is functioning correctly before scaling to ResNet.
3. Run the omniscient trajectory optimization on a simplified linear model to verify the bound computation before applying to neural networks.

## Open Questions the Paper Calls Out
### Open Question 1
Can the dependence on population gradients and Hessians be eliminated to facilitate self-certified algorithms without relying on validation sets?
The paper explicitly states this limitation in the Conclusion and Section E, noting that the bound's reliance on population statistics prevents it from being used in self-certified algorithms. The authors suggest this might be addressed by information-theoretically bounding higher-order statistics, but no concrete solution is provided.

### Open Question 2
How can the theoretical bound be modified to correctly capture the improved generalization observed with increasing learning rates?
Section 4 and Section C.4.1 note that while the bound correctly handles batch size trends, it shows an incorrect tendency with learning rate: the trajectory term increases with learning rate, contradicting empirical evidence. The authors conjecture this is due to increased variance overpowering improved flatness, but no modification is proposed.

### Open Question 3
Can the O(1/√n) minimax rate result for Convex-Lipschitz-Bounded problems be extended to ε-learners without assuming they are well-behaved optimizers?
The paper states Theorem 4 is "partial and preliminary" because it only covers ε-learners that are also O(ε)-optimizers. Extending this to arbitrary ε-learners would require a method to control empirical risk differences without assuming small optimization error, which the current proof strategy cannot provide.

## Limitations
- The bound relies on population gradients and Hessians, requiring a validation set and preventing use in self-certified algorithms.
- The theoretical framework for O(1/√n) rates for convex problems is not empirically validated in the main experiments.
- The bound's behavior with respect to learning rate trends is not fully correct, showing increased values when empirical generalization improves.

## Confidence

**High Confidence:**
- Theoretical derivation of omniscient trajectory bound and its relationship to flatness is rigorous and well-founded.
- Experimental methodology (ResNet-18 training on CIFAR-10 with SGD) is clearly specified and reproducible.

**Medium Confidence:**
- Empirical demonstration that the bound correctly reflects improved generalization with better flatness is convincing, though exact numerical tightness depends on implementation details not fully specified.
- Claim about achieving O(1/√n) minimax rates for GD on convex problems is theoretically sound but not empirically validated in main experiments.

## Next Checks
1. Verify the bound's sensitivity to validation set size by systematically varying the validation set ratio (e.g., 0.1, 0.2, 0.3 of training data) and measuring variance in bound estimates.
2. Test robustness to different sub-Gaussian constant values (R) by running CIFAR-10 experiment with R values from 0.5 to 2.0 (capped loss 4 to 16) and checking if qualitative flatness trends remain consistent.
3. Reproduce ResNet-18 experiment with fixed random seed and compare exact numerical values of omniscient bound against reported figures to ensure implementation accuracy.