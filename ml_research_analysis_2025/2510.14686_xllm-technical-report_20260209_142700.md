---
ver: rpa2
title: xLLM Technical Report
arxiv_id: '2510.14686'
source_url: https://arxiv.org/abs/2510.14686
tags:
- uni00000013
- uni00000048
- uni00000003
- xllm
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: xLLM is an intelligent LLM inference framework that addresses the
  challenge of efficiently serving large-scale models across diverse AI accelerators
  in enterprise settings. It introduces a novel decoupled service-engine architecture
  to tackle workload fluctuations, high availability requirements, and resource utilization
  issues in production environments.
---

# xLLM Technical Report

## Quick Facts
- arXiv ID: 2510.14686
- Source URL: https://arxiv.org/abs/2510.14686
- Reference count: 40
- xLLM achieves up to 1.7× throughput improvement over MindIE and 2.2× over vLLM-Ascend with Qwen-series models on Ascend hardware

## Executive Summary
xLLM is an intelligent LLM inference framework designed to efficiently serve large-scale models across diverse AI accelerators in enterprise settings. It introduces a novel decoupled service-engine architecture that addresses workload fluctuations, high availability requirements, and resource utilization challenges in production environments. The framework implements intelligent scheduling policies including unified elastic scheduling for online/offline requests, workload-adaptive dynamic Prefill-Decode disaggregation, and hybrid Encode-Prefill-Decode disaggregation for multimodal inputs.

Extensive evaluations demonstrate significant performance improvements over existing solutions, achieving up to 2.2× throughput gains on Ascend hardware while maintaining enterprise-grade SLO compliance. The framework has been deployed at JD.com supporting applications including JingYan AI chatbot, marketing recommendations, and customer service assistants. Key innovations include multi-layer pipeline execution, adaptive graph mode compilation, and "logically contiguous, physically discrete" xTensor memory management.

## Method Summary
xLLM implements a decoupled service-engine architecture where xLLM-Service handles request scheduling and resource management while xLLM-Engine executes inference workloads. The framework supports dynamic Prefill-Decode disaggregation where stateless instances can fluidly switch between prefill and decode roles based on real-time SLO metrics. Multi-layer execution pipeline overlaps CPU scheduling, communication, and computation through framework, model, and operator-level optimizations. xTensor memory management decouples virtual address space from physical memory using on-demand mapping and asynchronous pre-mapping. The system has been evaluated on Qwen2/3 and DeepSeek models using ShareGPT dataset on Ascend 910B/C hardware.

## Key Results
- Achieves up to 1.7× throughput improvement over MindIE and 2.2× over vLLM-Ascend with Qwen-series models
- Maintains average throughput of 1.7× over MindIE with Deepseek-series models under identical TPOT constraints
- Successfully deployed in production at JD.com with over 1000 instances supporting multiple enterprise applications

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Prefill-Decode (PD) Disaggregation
The system employs "stateless instances" that can fluidly switch between Prefill (P) and Decode (D) roles based on real-time SLO metrics (TTFT/TPOT). A "Runtime Instance Monitor" tracks performance, triggering role conversions when prefill queues exceed SLO thresholds, rather than relying on fixed P/D ratios. This improves resource utilization compared to static partitioning when workloads fluctuate.

### Mechanism 2: Multi-layer Execution Pipeline
xLLM employs a three-layer approach: (1) Framework Layer: CPU uses "placeholder tokens" to schedule the next batch asynchronously while the accelerator executes the current one; (2) Model Layer: Dual-stream parallelism overlaps computation with communication using micro-batches; (3) Operator Layer: Dynamic resource allocation aligns matrix and vector unit execution times to minimize idle gaps.

### Mechanism 3: xTensor Memory Management
xTensor decouples the virtual address space (contiguous view for operators) from physical memory (discrete pages). It uses "on-demand mapping" to allocate physical pages only as token generation progresses and "asynchronous pre-mapping" to hide page mapping latency, reducing allocation overhead and fragmentation while maintaining kernel efficiency.

## Foundational Learning

**Concept: Prefill vs. Decode Phases**
- Why needed here: The core architecture relies on disaggregating these phases. Prefill is compute-bound (processing the full prompt) while Decode is memory-bound (generating one token at a time).
- Quick check question: If a workload has very long prompts but generates only single-word answers, which instance pool (Prefill or Decode) would likely become the bottleneck in a static configuration?

**Concept: KV Cache Paged Memory (PagedAttention)**
- Why needed here: xLLM's xTensor memory management innovates upon concepts found in PagedAttention. Understanding the standard "block table" approach helps contextualize why xLLM introduces "logically contiguous" addresses.
- Quick check question: How does mapping KV cache to non-contiguous physical blocks typically save memory compared to contiguous allocation?

**Concept: Speculative Decoding**
- Why needed here: The xLLM-Engine integrates optimized speculative decoding to boost throughput. Understanding that a smaller "draft" model guesses tokens which a larger "target" model verifies in parallel is crucial.
- Quick check question: What happens to the acceptance rate of draft tokens if the draft model diverges significantly from the target model's distribution?

## Architecture Onboarding

**Component map:**
Control Plane: xLLM-Service (Global Scheduler, Predictor/Profiler)
Data Plane: xLLM-Engine (Workers/Instances)
Storage Plane: Global KV Cache Manager (Mooncake Store)
Pools: Encode (E), Prefill (P), Decode (D) Instance Pools

**Critical path:**
Request -> xLLM-Service (Preprocess/Profiling) -> Global Scheduler (Select Policy: Online/Offline, PD, EPD) -> Instance Pool (Engine Execution) -> KV Transfer (if disaggregated) -> Response

**Design tradeoffs:**
- PD Disaggregation: Low latency (isolation) vs. High Complexity (scheduling/migration)
- Adaptive Graph Mode: High performance (fused kernels) vs. Compilation overhead/rigidity for dynamic shapes
- Online/Offline Co-location: High utilization (idle fill) vs. SLO violation risk (preemption lag)

**Failure signatures:**
- SLO Violation: TPOT/TTFT spikes. Check: PD instance ratio imbalance or preemption starvation
- OOM: Memory spike. Check: xTensor pre-mapping logic or KV Cache migration leaks
- Low Throughput: Hardware underutilization. Check: Pipeline bubble or All-to-All communication bottling MoE layers

**First 3 experiments:**
1. Baseline Scaling: Benchmark Qwen3-8B on single-node vs. multi-node to verify throughput claims and observe scaling efficiency
2. Scheduler Stress Test: Simulate "tidal" traffic (bursty online requests + steady offline jobs) to validate Online-Offline Co-location policy
3. Memory Profile: Compare xTensor memory footprint and allocation latency against standard PagedAttention baseline under high concurrency with variable sequence lengths

## Open Questions the Paper Calls Out

**Open Question 1:** How can the xLLM architecture be effectively adapted to support non-autoregressive generative workloads, such as text-to-image or text-to-video generation? The current architecture is deeply optimized for KV Cache management and sequential token generation inherent to LLMs, which differ fundamentally from diffusion or parallel decoding mechanisms.

**Open Question 2:** What specific advancements in automated compilation and graph representation are required to achieve "zero-day" integration for newly released model architectures? New models frequently introduce novel operator patterns that typically require manual kernel tuning and scheduling adjustments.

**Open Question 3:** Can a unified hardware abstraction layer be implemented without significantly sacrificing peak performance achieved via hardware-specific optimizations? The paper highlights significant performance gains derived from hardware-specific tuning, such as overlapping Cube and Vector units on Ascend NPUs.

## Limitations
- Evaluation focuses exclusively on Ascend AI processors with no GPU or other accelerator comparisons
- Lacks ablation studies isolating individual contribution of each optimization layer
- Claims about enterprise deployment scale lack independent verification and production telemetry

## Confidence

**High Confidence:** The decoupled service-engine architecture design and basic functional claims are well-documented and technically coherent.

**Medium Confidence:** Throughput improvement claims are supported by experimental results but comparisons are limited to Ascend-specific baselines without cross-platform validation.

**Low Confidence:** Claims about enterprise deployment scale lack independent verification and production telemetry.

## Next Checks

1. **Cross-Platform Performance Validation:** Implement xLLM on NVIDIA A100/H100 GPUs and measure throughput improvements against vLLM and TensorRT-LLM using identical models to verify cross-platform generalization.

2. **Ablation Study of Optimization Layers:** Create controlled experiments that isolate each major optimization to quantify individual contributions to throughput gains.

3. **Production SLO Compliance Analysis:** Deploy xLLM in a realistic multi-tenant environment with mixed online/offline workloads and measure actual TTFT/TPOT compliance rates under varying load patterns.