---
ver: rpa2
title: 'Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants'
arxiv_id: '2510.24328'
source_url: https://arxiv.org/abs/2510.24328
tags:
- arabic
- language
- llms
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating large language
  models on culturally grounded and dialect-specific knowledge in Arabic. The authors
  extend an existing Modern Standard Arabic multiple-choice dataset into open-ended
  questions and dialectal variants (Egyptian, Levantine, Gulf, Maghrebi, and English).
---

# Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants

## Quick Facts
- arXiv ID: 2510.24328
- Source URL: https://arxiv.org/abs/2510.24328
- Reference count: 0
- Key outcome: First Arabic QA benchmark with parallel questions across five Arabic dialects and English, showing MCQ consistently outperforms OEQ, especially for dialects.

## Executive Summary
This work addresses the challenge of evaluating large language models on culturally grounded and dialect-specific knowledge in Arabic. The authors extend an existing Modern Standard Arabic multiple-choice dataset into open-ended questions and dialectal variants (Egyptian, Levantine, Gulf, Maghrebi, and English). They benchmark multiple models in both MCQ and OEQ formats, and generate chain-of-thought annotations for fine-tuning. Results show that MCQ performance is consistently higher than OEQ, especially for dialects. Arabic-centric models perform well on MCQs but lag in OEQs. CoT fine-tuning improves judged correctness but reduces lexical overlap. The dataset is the first to provide parallel QAs across multiple Arabic dialects and English, enabling cross-variety and cross-format evaluation.

## Method Summary
The authors created a culturally grounded Arabic QA benchmark by extending an existing Modern Standard Arabic multiple-choice dataset (CaMLA-MCQ) into open-ended questions (OEQ) and dialectal variants. They generated five Arabic dialect versions (Egyptian, Levantine, Gulf, Maghrebi) plus English, creating parallel question-answer pairs across formats. For OEQ development, they generated chain-of-thought rationales using GPT-4.1, followed by answer generation. They evaluated multiple models (both Arabic-centric and multilingual) on both MCQ and OEQ formats, using both automated metrics (BLEU, ROUGE-L, BERTScore) and LLM-as-a-judge evaluations. CoT fine-tuning was performed using the generated rationales.

## Key Results
- MCQ performance consistently exceeds OEQ performance across all Arabic dialects tested
- Arabic-centric models perform well on MCQs but significantly underperform in OEQs
- CoT fine-tuning improves semantic correctness judgments but reduces lexical overlap metrics
- The dataset achieves high inter-annotator agreement (94.44% on MCQs)
- The benchmark enables cross-variety and cross-format evaluation across 6 languages/dialects

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its culturally grounded content and dialectal variation, which better tests models' real-world Arabic understanding beyond MSA. The CoT fine-tuning mechanism works by providing step-by-step reasoning that guides models toward more semantically correct answers, though this sometimes comes at the cost of lexical precision.

## Foundational Learning

1. **Cultural grounding in NLP evaluation** - Why needed: Standard NLP benchmarks often lack cultural context, leading to models that perform well on generic tasks but fail on culturally specific knowledge. Quick check: Compare model performance on culturally grounded vs. generic QA datasets.

2. **Dialectal variation in Arabic** - Why needed: Arabic has significant dialectal differences that standard MSA benchmarks don't capture. Quick check: Evaluate models on multiple dialect variants to identify performance gaps.

3. **Chain-of-Thought vs. standard fine-tuning** - Why needed: Understanding when CoT provides advantages over standard fine-tuning is crucial for efficient model development. Quick check: Compare CoT and standard fine-tuning performance across different task types and model sizes.

4. **Semantic vs. lexical evaluation metrics** - Why needed: Traditional n-gram based metrics may not capture semantic correctness of answers. Quick check: Compare automated metrics with human judgments to identify divergences.

## Architecture Onboarding

Component map: CaMLA-MCQ dataset -> OEQ generation -> Dialect translation -> Model evaluation -> CoT fine-tuning

Critical path: Dataset creation → Model evaluation → Fine-tuning analysis

Design tradeoffs:
- Automated translation vs. human translation for dialect variants (speed vs. accuracy)
- Lexical overlap metrics vs. semantic correctness judgments (precision vs. validity)
- MCQ vs. OEQ formats (ease of evaluation vs. real-world applicability)

Failure signatures:
- High lexical overlap but low semantic correctness
- Significant performance drops on dialectal variants vs. MSA
- CoT fine-tuning improving semantic scores while degrading lexical metrics

First experiments:
1. Evaluate baseline models on MCQ format before extending to OEQ
2. Compare performance of Arabic-centric vs. multilingual models on dialectal variants
3. Test CoT fine-tuning on a subset of questions to validate the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions regarding task type, rationale length, and model size does Chain-of-Thought (CoT) fine-tuning provide a net benefit for open-ended Arabic QA compared to standard fine-tuning?
- Basis in paper: [explicit] The authors explicitly state that their results "highlight the need to examine when CoT is advantageous, particularly regarding task type, rationale length, and model size" given that CoT improved judged correctness but reduced n-gram overlap.
- Why unresolved: The paper shows conflicting results where CoT improves semantic scores (LLM-as-a-judge) but degrades lexical metrics (F1, ROUGE-L), making the overall utility of CoT ambiguous for this specific linguistic context.
- What evidence would resolve it: Ablation studies on different Arabic-centric model scales and controlled variations of CoT rationale verbosity correlated with human preference surveys.

### Open Question 2
- Question: Can a standardized evaluation protocol be established for open-ended Arabic QA that reconciles the divergence between lexical overlap metrics and semantic correctness judgments?
- Basis in paper: [inferred] The paper notes a divergence where "CoT improves judged correctness but reduces lexical overlap with the references," leading to a situation where better answers score lower on traditional metrics like BERTScore and ROUGE-L.
- Why unresolved: Current standard metrics rely on n-gram overlap, which fails to capture the semantic equivalence of valid, concise answers against longer gold references.
- What evidence would resolve it: The development and validation of a new evaluation metric specifically for Arabic that correlates strongly with human judgments of correctness even when lexical overlap is low.

### Open Question 3
- Question: To what extent does LLM-assisted translation introduce semantic drift or stylistic artifacts in low-resource Arabic dialects compared to native human authorship?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that their extensions "rely on LLM-assisted translation... which may introduce modeling biases (e.g., paraphrase drift, dialectal normalization)."
- Why unresolved: The dataset was constructed using GPT-4.1 for translation with only limited manual checking by a single annotator per dialect, leaving the fidelity of the dialectal nuances unverified at scale.
- What evidence would resolve it: A comprehensive human evaluation comparing the LLM-generated dialectal samples against parallel texts written natively by speakers from the respective regions.

## Limitations
- Limited human validation, particularly for dialectal variants
- Small OEQ test sets (e.g., 70 questions for Egyptian dialect)
- Reliance on LLM-assisted translation which may introduce modeling biases
- Automated metrics may not fully capture semantic correctness of answers

## Confidence
- Dataset novelty claim: High (based on corpus signals)
- MCQ performance consistently higher than OEQ: Medium (supported by results but limited test set size)
- CoT fine-tuning improves judged correctness while decreasing lexical overlap: Medium (plausible but requires replication)
- Arabic-centric models perform well on MCQs but lag in OEQs: Medium (supported by results but limited validation)

## Next Checks
1. Conduct a large-scale human evaluation of OEQ answers across all dialectal variants, including minority dialects.
2. Test model performance on a significantly expanded OEQ test set with at least 200+ questions per dialect.
3. Validate whether CoT fine-tuning generalizes to unseen dialects not present in the training data.