---
ver: rpa2
title: 'PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation
  Models'
arxiv_id: '2601.21238'
source_url: https://arxiv.org/abs/2601.21238
tags:
- quantization
- scaling
- arvg
- arxiv
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of applying post-training quantization
  (PTQ) to AutoRegressive Visual Generation (ARVG) models, which suffer from three
  key issues: severe channel-wise outliers, highly dynamic token-wise activations,
  and sample-wise distribution mismatches. To tackle these, the authors propose PTQ4ARVG,
  a training-free PTQ framework with three components: (1) Gain-Projected Scaling
  (GPS), which uses mathematical optimization to derive optimal scaling factors for
  outlier suppression; (2) Static Token-Wise Quantization (STWQ), which assigns fine-grained
  static quantization parameters offline to handle dynamic activations without runtime
  overhead; and (3) Distribution-Guided Calibration (DGC), which selects samples based
  on distributional entropy to improve calibration accuracy.'
---

# PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models

## Quick Facts
- **arXiv ID**: 2601.21238
- **Source URL**: https://arxiv.org/abs/2601.21238
- **Reference count**: 40
- **Primary result**: Proposes a training-free PTQ framework for ARVG models that addresses outliers, dynamic activations, and sample mismatches, achieving competitive FID/IS metrics at 8-bit and 6-bit quantization.

## Executive Summary
This paper addresses the challenge of applying post-training quantization to AutoRegressive Visual Generation (ARVG) models, which suffer from three key issues: severe channel-wise outliers, highly dynamic token-wise activations, and sample-wise distribution mismatches. To tackle these, the authors propose PTQ4ARVG, a training-free PTQ framework with three components: (1) Gain-Projected Scaling (GPS), which uses mathematical optimization to derive optimal scaling factors for outlier suppression; (2) Static Token-Wise Quantization (STWQ), which assigns fine-grained static quantization parameters offline to handle dynamic activations without runtime overhead; and (3) Distribution-Guided Calibration (DGC), which selects samples based on distributional entropy to improve calibration accuracy. Experiments on V AR, RAR, PAR, and MAR models show that PTQ4ARVG achieves competitive performance when quantizing to 8-bit and 6-bit, outperforming existing methods in FID and other image generation metrics while introducing no additional inference overhead.

## Method Summary
PTQ4ARVG is a training-free framework that applies post-training quantization to ARVG models through three key innovations. First, GPS mathematically optimizes scaling factors to shift quantization difficulty from outlier-prone activations to weights. Second, STWQ assigns static, per-token quantization parameters offline to handle dynamic activation ranges without runtime overhead. Third, DGC uses Mahalanobis distance to select calibration samples that better represent the data distribution. The method quantizes all linear layers and matmuls to W8A8 or W6A6, including KV cache, without requiring any retraining.

## Key Results
- Achieves competitive FID scores on VAR, RAR, PAR, and MAR models when quantizing to 8-bit and 6-bit
- Outperforms existing PTQ methods on image generation metrics (FID, IS, Precision, sFID)
- Introduces zero inference overhead compared to dynamic quantization approaches
- Demonstrates effectiveness across multiple ARVG model architectures

## Why This Works (Mechanism)

### Mechanism 1: Gain-Projected Scaling (GPS) for Outlier Suppression
- **Claim**: If activation channels exhibit severe outliers, shifting quantization difficulty from activations to weights via scaling reduces overall error, provided the optimal scaling factor is derived mathematically rather than empirically.
- **Mechanism**: GPS treats scaling as an optimization problem. It expands the quantization loss function using a Taylor series. It calculates the "gain" (reduction in activation quantization loss minus the increase in weight quantization loss) and solves for the scaling factor $s$ that maximizes this gain via differentiation. This allows the method to absorb outliers into the weights while minimizing the resulting weight error.
- **Core assumption**: The Hessian of the task loss can be approximated by the Mean Squared Error (MSE) loss, and cross-terms in the Taylor expansion are negligible (Assumption: quantization errors are independent).
- **Evidence anchors**: [abstract] Mentions expanding quantization loss via Taylor series to quantify gain. [Section 4.1] Derives the closed-form solution $s_2 = s_1 \sqrt{\frac{\sum |\Delta W x|}{\sum |W \Delta x|}}$ based on loss analysis. [corpus] Related work (MQuant, FPQVAR) highlights the difficulty of static quantization for multimodal/visual models; GPS offers a theoretical derivation compared to the heuristics in earlier methods like SmoothQuant.
- **Break condition**: If weight channels also contain severe outliers that cannot absorb the shifted error without significant degradation, the gain function may yield a suboptimal or neutral scaling factor.

### Mechanism 2: Static Token-Wise Quantization (STWQ) for Dynamic Activations
- **Claim**: If a model generates fixed-length token sequences with position-invariant activation distributions, fine-grained quantization parameters can be assigned offline to specific token positions, eliminating the need for runtime calibration.
- **Mechanism**: Unlike LLMs with dynamic lengths, ARVG models process a fixed grid. STWQ assigns specific quantization scales ($\delta$) and zero-points ($z$) to individual token positions (e.g., the initial "sink tokens" vs. image tokens) offline using percentile calibration. This handles high variance across tokens without the latency of dynamic range calculation during inference.
- **Core assumption**: The activation distribution at a given token index is consistent across different input samples (verified by the authors in Fig 4 and Appendix I).
- **Evidence anchors**: [abstract] States STWQ leverages fixed token length and position-invariant distribution. [Section 4.2] Details the assignment of parameters along the token sequence for AdaLN and linear layers. [corpus] FPQVAR discusses VAR modeling specifically; STWQ exploits the structural rigidity of these models which differs from the dynamic nature of standard LLMs.
- **Break condition**: If the model architecture changes to support variable resolution or non-fixed token counts, the static parameter mapping becomes invalid.

### Mechanism 3: Distribution-Guided Calibration (DGC) for Sample Mismatch
- **Claim**: If random calibration samples exhibit high redundancy, selecting samples based on distributional entropy improves the accuracy of quantization parameter estimation.
- **Mechanism**: DGC calculates the Mahalanobis distance for candidate samples relative to the dataset distribution. It selects the top 50% of samples with the highest "entropy" (distance from the mean), ensuring the calibration set captures the tails of the distribution rather than just the dominant modes.
- **Core assumption**: High Mahalanobis distance correlates with "information richness" relevant to the quantization bounds, and the dataset covariance is stable enough to invert.
- **Evidence anchors**: [abstract] Describes selecting samples contributing most to distributional entropy. [Section 4.3] Equation 17 defines the distance metric used for selection. [corpus] Corpus papers (e.g., MQuant) often use standard calibration sets; DGC addresses the specific "sample-wise redundancy" issue identified in the paper.
- **Break condition**: If the dataset is extremely diverse or the feature covariance is singular (poorly conditioned), the Mahalanobis distance may fail to identify representative samples.

## Foundational Learning

- **Concept: Equivalent Scaling (in Quantization)**
  - **Why needed here**: GPS relies on the principle that scaling the activation by $s$ and dividing the weight by $s$ preserves the output $Y = (X/s)(sW)$. You must understand this mathematical equivalence to grasp how GPS "moves" quantization difficulty.
  - **Quick check question**: If you scale an activation channel by 2 to reduce its quantization error, what must happen to the corresponding input channel of the weight matrix to keep the layer output identical?

- **Concept: Taylor Series Expansion for Loss Approximation**
  - **Why needed here**: The core theoretical contribution of GPS is approximating the quantization loss using a Taylor series (first/second order derivatives) to find an analytical optimum.
  - **Quick check question**: When approximating the loss $L(x+\Delta x)$, why is the Hessian (second derivative) important for determining the impact of the error $\Delta x$?

- **Concept: Sink Tokens and Outliers in Transformers**
  - **Why needed here**: The paper identifies that ARVG initial tokens act as "sink tokens" with massive outliers (due to conditioning/AdaLN). Understanding this transformer-specific phenomenon explains why standard per-tensor quantization fails.
  - **Quick check question**: Why might the attention mechanism cause specific tokens (like the initial class token) to accumulate significantly larger magnitude values than others?

## Architecture Onboarding

- **Component map**: Calibration Dataset -> DGC Module (Selects high-entropy samples) -> Model: ARVG Transformer Blocks (AdaLN -> MHSA -> FFN) -> Processing: GPS (Analyzes Activation and Weight stats -> Computes optimal scaling s) and STWQ (Analyzes token-wise activations -> Assigns static token params) -> Output: Quantized Model (Weights fused with scaling, Activations mapped to static token params)

- **Critical path**:
  1. **Calibration**: Run DGC to select 128 images
  2. **Scaling Optimization**: For QKV and FC1 layers, compute GPS factors (solve optimization)
  3. **Parameter Generation**: For all layers, compute STWQ token-wise scales/zeros
  4. **Fusion**: Absorb GPS scaling factors into weights

- **Design tradeoffs**:
  - **Accuracy vs. Storage**: STWQ increases the size of the "activation" quantization metadata (scales per token position vs. one scale per layer), trading small memory overhead for significant accuracy gains
  - **Calibration Time vs. Inference Speed**: GPS requires solving an optimization problem (differentiation), increasing calibration time compared to simple Min-Mex, but incurs *zero* inference overhead (training-free)

- **Failure signatures**:
  - **GPS Failure**: If FID scores are high but the model doesn't crash, check if the scaling factor $s$ is exploding (suggesting the optimization got stuck in a local minimum or the outlier is in the weight, not activation)
  - **STWQ Failure**: If generation artifacts appear at specific spatial locations, verify the "position-invariant" assumption holds for your specific dataset or if the static percentile calibration clipped important features

- **First 3 experiments**:
  1. **Validate GPS**: Compare GPS against SmoothQuant (heuristic scaling) on a W6A6 VAR-d16 model to isolate the impact of the "mathematical optimization" vs. empirical scaling
  2. **Validate STWQ Overhead**: Benchmark inference latency of STWQ (static params) vs. Dynamic Token-wise Quantization. Confirm the paper's claim of "no runtime overhead"
  3. **Ablation on Calibration**: Run PTQ4ARVG with Random Sampling vs. DGC to verify if the Mahalanobis distance selection actually improves FID/IS metrics on the target distribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the PTQ4ARVG framework be adapted to maintain competitive performance at 4-bit precision?
- **Basis in paper**: [explicit] The authors state in Section L (Limitations) that while the method works for 8-bit and 6-bit, it "struggles to maintain such a high level of accuracy when quantizing the model to 4-bit."
- **Why unresolved**: The proposed calibration and scaling strategies (GPS, STWQ) were optimized for the noise tolerance of 6-bit/8-bit integers; the quantization error at 4-bit likely exceeds the correction capabilities of the current gain-projected scaling factors.
- **What evidence would resolve it**: Demonstration of a PTQ4ARVG variant achieving FID scores close to the FP baseline on VAR or RAR models at W4A8 precision.

### Open Question 2
- **Question**: Can the Gain-Projected Scaling (GPS) method generalize effectively to other architectures, such as Large Language Models (LLMs) or Diffusion Transformers?
- **Basis in paper**: [explicit] Appendix F notes that GPS can theoretically serve as a "plug-and-play tool" and states the authors will "explore its generalization and applicability to other models" in future work.
- **Why unresolved**: The mathematical derivation of optimal scaling factors relies on statistical observations (Remark 1) and outlier characteristics specific to ARVG activations, which may differ in distribution from LLM attention maps or diffusion noise predictors.
- **What evidence would resolve it**: Applying the GPS derivation to a non-ARVG architecture (e.g., LLaMA or Stable Diffusion) and showing reduced quantization loss without architecture-specific modifications.

### Open Question 3
- **Question**: Does Static Token-Wise Quantization (STWQ) remain effective if future ARVG models introduce variable token lengths or dynamic positional embeddings?
- **Basis in paper**: [inferred] Section 4.2 justifies STWQ by leveraging the properties of "fixed token sequence lengths and position-invariant distribution across samples."
- **Why unresolved**: The method pre-computes static parameters for the token dimension; if future models generate variable sequence lengths (like LLMs) or alter positional invariance, the static lookup of quantization parameters would no longer be applicable or accurate.
- **What evidence would resolve it**: Evaluating the STWQ approach on a modified ARVG architecture designed with variable token generation capabilities.

## Limitations
- **4-bit precision performance**: The method struggles to maintain accuracy when quantizing to 4-bit, limiting its applicability for extreme compression scenarios.
- **Architecture specificity**: STWQ relies heavily on fixed token lengths and position-invariant distributions, making it potentially unsuitable for variable-resolution models or non-image generation tasks.
- **Theoretical assumptions**: GPS assumes cross-terms in Taylor expansion are negligible and that MSE approximates task loss Hessians, which may not hold across all architectures.

## Confidence
- **High Confidence**: The empirical results showing PTQ4ARVG's effectiveness on standard ARVG models (VAR, RAR, PAR, MAR) with consistent FID/IS improvements. The GPS mechanism's basic mathematical framework is sound.
- **Medium Confidence**: The specific closed-form solution in Algorithm 1 and the STWQ implementation details for "sink tokens." While the approach is plausible, the exact implementation details for sink token identification are not fully specified.
- **Low Confidence**: The distributional assumptions underlying DGC's Mahalanobis distance selection and the long-term stability of the calibration set across different deployment scenarios. The paper does not address how sensitive the method is to calibration dataset composition.

## Next Checks
1. **GPS Optimization Validation**: Implement a controlled experiment comparing GPS's closed-form scaling solution against brute-force numerical optimization to verify the approximation assumptions hold in practice.
2. **STWQ Assumption Testing**: Generate calibration curves showing activation distribution stability across different token positions and samples to empirically verify the position-invariant assumption required for static parameters.
3. **Cross-Dataset Generalization**: Evaluate PTQ4ARVG on a non-ImageNet dataset (e.g., CIFAR-10 or LSUN) to test whether the distributional entropy selection in DGC and the STWQ parameters transfer effectively to different data distributions.