---
ver: rpa2
title: 'Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating
  natural language generation applications'
arxiv_id: '2504.02867'
source_url: https://arxiv.org/abs/2504.02867
tags:
- evaluation
- prompt
- arxiv
- human
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a dynamic multi-agent system to automatically
  design personalized LLM judges for natural language generation evaluation. It addresses
  two main limitations in existing LLM-as-a-judge frameworks: poor adaptation to different
  text styles and low correlation with human judgment.'
---

# Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications

## Quick Facts
- arXiv ID: 2504.02867
- Source URL: https://arxiv.org/abs/2504.02867
- Reference count: 40
- The paper introduces a dynamic multi-agent system to automatically design personalized LLM judges for natural language generation evaluation.

## Executive Summary
This paper addresses the challenge of designing LLM-based judges that can adapt to different text styles and maintain high correlation with human judgment. The proposed multi-agent framework iteratively refines evaluation prompts through three specialized agents: Sample Selection, Evaluation, and ReWrite. Experiments on Instruct-QA datasets demonstrate superior performance compared to baselines, achieving an AUC of 0.91 versus 0.79 for RAGAS and 0.81 for CE. The framework also shows improved human alignment on the Semantic Textual Similarity Benchmark with a Pearson correlation of 0.81 compared to 0.51 for CE.

## Method Summary
The framework uses a three-agent iterative system to automatically design personalized LLM judges. The Sample Selection agent clusters data and selects diverse examples, the Evaluation agent tests examples against the current prompt and provides feedback, and the ReWrite agent uses GPT-4 to improve the prompt based on feedback. The process starts with an initial prompt incorporating the 0-1 STS rubric and continues until a performance threshold is met or a maximum of 10 iterations is reached. The system uses GPT-3.5 Turbo (temperature=0) as the judge model and evaluates on Instruct-QA datasets with RAG-generated answers from multiple LLMs.

## Key Results
- Achieves AUC of 0.91 on Instruct-QA datasets, outperforming RAGAS (0.79) and CE (0.81)
- Pearson correlation of 0.81 with human annotations on STSB benchmark
- Few-shot prompt alone (0.83 AUC) surpasses well-engineered baselines RAGAS and CE
- Iterative refinement converges within 10 iterations in tested cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative multi-agent prompt refinement improves judge alignment with human perception better than static prompt engineering
- Mechanism: Three specialized agents operate in a loopâ€”Sample Selection clusters data and picks diverse examples, Evaluation tests these examples against the current prompt and generates detailed feedback on failures, and ReWrite incorporates feedback to produce improved prompts
- Core assumption: Errors in evaluation are systematic and correctable through prompt modification rather than requiring model retraining
- Evidence anchors: Abstract states iterative refinement through three agents; section III describes balancing semantic similarity criteria with task requirements
- Break condition: If examples are too homogeneous or feedback lacks specificity, iterations may converge to local optima without meaningful improvement

### Mechanism 2
- Claim: Diverse few-shot example selection through clustering enables better adaptation to varying text styles than manual or random selection
- Mechanism: The Sample Selection agent partitions the dataset into clusters using text clustering techniques, then randomly selects one representative example from each cluster
- Core assumption: Text clustering captures stylistic variation relevant to evaluation quality; cluster centroids represent meaningful diversity
- Evidence anchors: Section III describes selecting one example per cluster; section IV-A-3 shows few-shot prompt surpassing RAGAS and CE baselines
- Break condition: If clustering features don't align with evaluation-relevant dimensions, selection may not improve robustness

### Mechanism 3
- Claim: Grounding evaluation rubrics in established human-perceived semantic similarity scales improves score interpretability and human alignment
- Mechanism: The initial prompt incorporates the 0-1 STS rubric with detailed intermediate levels as a common language that persists through iterations
- Core assumption: Human annotators share a consistent interpretation of these rubric levels across domains
- Evidence anchors: Section III lists the six-point rubric; section IV-B-3 shows initial prompt achieving 0.67 correlation on STSB
- Break condition: If target tasks require evaluation dimensions beyond semantic similarity, the STS rubric may constrain the optimization process

## Foundational Learning

- Concept: **LLM-as-a-Judge paradigm**
  - Why needed here: The entire framework builds on using LLMs to evaluate other LLM outputs, replacing or augmenting human evaluators
  - Quick check question: Can you explain why word-overlap metrics like BLEU fail for open-ended text generation evaluation?

- Concept: **In-context learning and prompt engineering**
  - Why needed here: The system improves evaluation through prompt modification rather than weight updates; understanding how LLMs respond to prompt structure is essential
  - Quick check question: What is the difference between few-shot prompting and fine-tuning for adapting LLM behavior?

- Concept: **Semantic Textual Similarity (STS) benchmarks**
  - Why needed here: The paper uses STS benchmarks as a key evaluation metric to measure alignment with human judgment
  - Quick check question: How does the STS benchmark differ from other NLP evaluation metrics like ROUGE or BLEU?

## Architecture Onboarding
The framework consists of three main components working in an iterative loop: Sample Selection, Evaluation, and ReWrite agents. The Sample Selection agent performs text clustering on the evaluation dataset and selects diverse representative examples. The Evaluation agent applies the current prompt to these examples and generates feedback on evaluation failures. The ReWrite agent uses GPT-4 to modify the prompt based on this feedback. The system maintains the 0-1 STS rubric throughout iterations to ensure consistency with human evaluation standards. The judge model is GPT-3.5 Turbo with temperature set to 0 for deterministic outputs.

## Open Questions the Paper Calls Out
The paper acknowledges several open questions including the scalability of the approach to larger datasets, the generalizability across different domains beyond question answering, and the computational cost of running multiple GPT-4 iterations. It also notes that the effectiveness of the clustering approach depends on the choice of features and distance metrics, which may require domain-specific tuning.

## Limitations
- The framework relies on GPT-4 for prompt refinement, which introduces significant computational cost and API dependency
- Performance depends on the quality of initial prompt and the STS rubric, which may not generalize to all evaluation scenarios
- The clustering approach assumes that stylistic variation is captured by the chosen features, which may not hold for all text types
- Limited evaluation to question answering tasks and synthetic data generation scenarios
- The iterative process may converge to local optima if early examples are not representative of the full evaluation space

## Confidence
High confidence in the reported results based on systematic evaluation against established baselines and human-annotated benchmarks. The methodology is well-documented with clear experimental procedures and reproducible results.

## Next Checks
- Verify the specific text clustering algorithm and features used by the Sample Selection agent
- Examine the exact structure of the initial 0-1 STS rubric incorporated in the prompt
- Review the diversity metrics used to ensure selected examples represent the full evaluation space
- Check the computational overhead of running multiple GPT-4 iterations for prompt refinement
- Investigate the generalizability of the approach to other text generation tasks beyond question answering