---
ver: rpa2
title: A Shared Low-Rank Adaptation Approach to Personalized RLHF
arxiv_id: '2503.19201'
source_url: https://arxiv.org/abs/2503.19201
tags:
- reward
- arxiv
- tail
- function
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces P-ShareLoRA, a novel approach to personalized
  reinforcement learning from human feedback (RLHF) that addresses the limitations
  of existing methods which assume homogeneous human preferences. By incorporating
  Low-Rank Adaptation (LoRA) with shared components into the personalized RLHF framework,
  the method efficiently learns personalized reward models from potentially limited
  local datasets while exploiting shared structures among users.
---

# A Shared Low-Rank Adaptation Approach to Personalized RLHF

## Quick Facts
- arXiv ID: 2503.19201
- Source URL: https://arxiv.org/abs/2503.19201
- Authors: Renpu Liu; Peng Wang; Donghao Li; Cong Shen; Jing Yang
- Reference count: 40
- Key outcome: Introduces P-ShareLoRA, achieving 74.65% accuracy on Llama-3 8B and 66.93% on GPT-J 6B for personalized RLHF, outperforming state-of-the-art methods.

## Executive Summary
This paper introduces P-ShareLoRA, a novel approach to personalized reinforcement learning from human feedback (RLHF) that addresses the limitations of existing methods which assume homogeneous human preferences. By incorporating Low-Rank Adaptation (LoRA) with shared components into the personalized RLHF framework, the method efficiently learns personalized reward models from potentially limited local datasets while exploiting shared structures among users. The approach theoretically demonstrates reduced sample complexity compared to full-parameter fine-tuning and standard LoRA methods without parameter sharing, and establishes sample complexity guarantees. Experimental results on the Reddit TL;DR dataset show that P-ShareLoRA achieves prediction accuracies of 74.65% on Llama-3 8B and 66.93% on GPT-J 6B, outperforming state-of-the-art algorithms which achieve 73.25% and 66.13% respectively.

## Method Summary
P-ShareLoRA introduces a shared LoRA adaptation approach to personalized RLHF that learns both a shared low-rank matrix B and individual personalization matrices Wi for each user. The method optimizes a shared up-projection matrix B across all users while maintaining individual down-projection matrices Wi, capturing both common preference structures and user-specific variations. The approach uses a pessimism principle via confidence sets to guard against overestimation in offline data, and theoretically demonstrates reduced sample complexity compared to full-parameter fine-tuning. The training involves global pre-training on aggregated data followed by personalized fine-tuning, with experimental validation showing significant improvements over baseline methods on the Reddit TL;DR dataset.

## Key Results
- P-ShareLoRA achieves 74.65% prediction accuracy on Llama-3 8B and 66.93% on GPT-J 6B for personalized reward modeling
- Outperforms state-of-the-art algorithms (73.25% and 66.13% respectively) on the Reddit TL;DR dataset
- Demonstrates reduced sample complexity through shared low-rank parameter space compared to full-parameter fine-tuning
- Shows that sharing up-projection matrices (B) yields better performance than sharing down-projection matrices (A)

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Aggregated Parameter Space
The paper proposes applying LoRA to the aggregated parameter matrix ΔΘ of all users, rather than individual matrices. By constraining rank(ΔΘ) ≤ k and sharing the up-projection matrix B across all users (ΔΘi = BWi), the method restricts the reward function class. This reduces the bracketing number (complexity) from O(Nd1d2) to O(k(d1 + Nd2)), requiring fewer samples to achieve a generalization error ε. The core assumption is that ground-truth parameter shifts ΔΘ* across users primarily lie in a low-dimensional subspace.

### Mechanism 2: Pessimism via Confidence Sets
The algorithm constructs a confidence set Ri around the estimated reward parameters and solves a robust optimization maximizing the worst-case expected return within that set. This pessimism counters the uncertainty inherent in preference data, ensuring the learned policy is near-optimal even with limited user-specific data. The uniform concentration property allows empirical estimates to bound true reward differences.

### Mechanism 3: Asymmetric Matrix Sharing (Up-Projection)
While standard LoRA decomposes updates as BA, the paper enforces sharing on B (the "up-projection" in ΔΘ = BW). Experimental analysis suggests the column space of B captures the shared "preference directions" better than the row space of A. The shared structure manifests in the column space of the adaptation matrix.

## Foundational Learning

- **Low-Rank Adaptation (LoRA):** The architectural primitive where Wnew = Wold + BA allows separation of "shared" (B) from "personal" (Wi) without retraining the whole model. Quick check: Can you calculate the parameter count reduction if d=4096 and rank k=8?

- **Bradley-Terry-Luce (BTL) Preference Model:** The optimization objective maximizes the log-likelihood of observed pairwise preferences based on this model. The sigmoid relationship between reward difference and preference probability is essential for debugging reward outputs. Quick check: If a user prefers Trajectory A over B with probability 0.9, what is the approximate reward difference r(A) - r(B)?

- **Principal Angle Distance:** The theoretical analysis uses this to measure how well the learned shared subspace B̂ aligns with the ground-truth subspace B*. Quick check: If two subspaces are orthogonal, what is their principal angle distance?

## Architecture Onboarding

- **Component map:** Aggregated preference dataset → Pre-trained LLM (frozen) → P-ShareLoRA Adapter (shared B + personal Wi) → Optimization loop (MLE on preference pairs) → Policy optimizer (PPO-Clip)

- **Critical path:**
  1. Initialize B and Wi (Implementation Note: Do not use random initialization; use "Warm-up" or "Global" pre-training for stability)
  2. Forward pass: Compute rewards for trajectory pairs using ri = LLM(x) + (B · Wi)x
  3. Compute Preference Loss: -Σ log σ(r(τwin) - r(τlose))
  4. Backprop: Update B (shared gradients) and Wi (local gradients)
  5. Policy Update: Run PPO using the specific ri for the user

- **Design tradeoffs:**
  - Rank k: Higher k captures more diversity but increases sample complexity (O(kd2)) and breaks the low-rank bias assumption. Paper uses k=32.
  - Initialization: Random init ("SI") performs worse than Global pre-training ("G"). Trade off compute time (pre-training) for accuracy.

- **Failure signatures:**
  - Performance Collapse to Global: If personalization matrices Wi diverge or gradients for B overpower Wi, all users converge to the same reward model
  - Overfitting on Tail: If specific users have very few samples (Np small), the shared B dominates, leading to generic responses rather than personalized ones
  - If sharing A accidentally outperforms sharing B, check if gradient norms are exploding or if implementation has matrix transpose errors

- **First 3 experiments:**
  1. Baseline Validation: Compare P-ShareLoRA(G) vs. LoRA-local vs. LoRA-global on a synthetic dataset where you control the "diversity" (condition number ν)
  2. Ablation on Initialization: Run P-ShareLoRA(SI) vs. P-ShareLoRA(WU) to replicate the finding that global warm-up is critical for convergence
  3. Rank Sensitivity: Plot prediction accuracy vs. rank k. Verify that the "tail singular value" term Σtail predicted in theory corresponds to the performance drop when k is set too low

## Open Questions the Paper Calls Out

- Does sharing the up-projection matrix (B) versus the down-projection matrix (A) yield consistent performance gains across different model architectures or tasks?
- How does the performance of P-ShareLoRA degrade as the number of users (N) scales up, particularly in scenarios with extreme preference heterogeneity?
- Can the rank k of the shared LoRA module be determined adaptively to optimize the trade-off between estimation error and bias?
- What is the optimal initialization strategy for the shared components to ensure consistent convergence across different base models?

## Limitations
- Theoretical claims rely heavily on the assumption that user preference shifts share a common low-rank subspace, which may not hold for highly diverse user groups
- Empirical validation is limited to a single Reddit TL;DR dataset with only 5 users, raising concerns about generalization to broader preference distributions
- The paper does not provide sufficient detail on the policy optimization phase, particularly PPO hyperparameters, which could significantly impact final performance metrics
- Asymmetric matrix sharing claim lacks comprehensive ablation studies across different model architectures

## Confidence
- **High Confidence:** The mechanism of LoRA-based parameter sharing for reducing sample complexity and the experimental comparison between P-ShareLoRA variants and baselines
- **Medium Confidence:** The pessimism via confidence sets mechanism and the asymmetric sharing claim (B vs A), as these are supported by experimental results but lack theoretical justification
- **Low Confidence:** The sample complexity guarantees in Theorem 4.1, as the proof relies heavily on idealized assumptions about ground-truth parameter shifts

## Next Checks
1. **Diversity Stress Test:** Create a synthetic preference dataset with controlled diversity (varying condition number ν) and test P-ShareLoRA's performance as user preferences become increasingly orthogonal
2. **Ablation on Asymmetric Sharing:** Systematically compare sharing B vs A matrices across different model architectures and initialization schemes
3. **Scaling Experiment:** Evaluate P-ShareLoRA on a dataset with 20-50 users to assess how the shared matrix B scales with the number of personalization matrices Wi