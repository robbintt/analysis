---
ver: rpa2
title: 'Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance'
arxiv_id: '2505.01016'
source_url: https://arxiv.org/abs/2505.01016
tags:
- fine-tuning
- performance
- coco
- fine-grained
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how deep to fine-tune a pre-trained YOLOv8n
  object detector when adapting it to a fine-grained domain (fruit detection) without
  causing catastrophic forgetting on the original COCO task. The core method idea
  is to systematically vary the freeze level during fine-tuning (freezing layers 22,
  15, or 10), progressively unfreezing and training deeper layers of the backbone,
  and evaluate performance on both the target fruit dataset and the source COCO dataset
  using a dual-head evaluation setup.
---

# Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance

## Quick Facts
- **arXiv ID**: 2505.01016
- **Source URL**: https://arxiv.org/abs/2505.01016
- **Reference count**: 32
- **Primary result**: Unfreezing YOLOv8n to layer 10 during fine-tuning improves fruit detection by +10% mAP50 while preserving COCO performance within 0.1%

## Executive Summary
This paper investigates how deep to fine-tune a pre-trained YOLOv8n object detector when adapting it to a fine-grained domain (fruit detection) without causing catastrophic forgetting on the original COCO task. The core method idea is to systematically vary the freeze level during fine-tuning (freezing layers 22, 15, or 10), progressively unfreezing and training deeper layers of the backbone, and evaluate performance on both the target fruit dataset and the source COCO dataset using a dual-head evaluation setup. The primary results show that deeper fine-tuning (unfreezing to layer 10) yields substantial improvements on the fruit detection task (+10% absolute mAP50) compared to only training the head, while simultaneously achieving negligible degradation (<0.1% absolute mAP difference) on the original COCO benchmark across all tested freeze levels. This demonstrates that adapting mid-to-late backbone features is effective for fine-grained specialization without catastrophic forgetting.

## Method Summary
The paper systematically investigates the impact of freezing different levels of the YOLOv8n backbone during fine-tuning on fruit detection while preserving COCO performance. The approach uses a dual-head evaluation setup where both the original COCO head and a new fruit detection head are maintained during training. Three freeze levels are tested: freezing at layer 22 (only head training), layer 15 (unfreezing C2-C3 stages), and layer 10 (unfreezing C2-C5 stages). The fruit detection dataset consists of 9,387 images across 25 fruit categories, and the dual-head evaluation measures performance on both tasks simultaneously to detect catastrophic forgetting.

## Key Results
- Unfreezing to layer 10 yields +10% absolute mAP50 improvement on fruit detection compared to head-only training
- COCO performance degrades by less than 0.1% absolute mAP across all freeze levels
- Deeper fine-tuning of mid-to-late backbone features enables effective fine-grained specialization without catastrophic forgetting

## Why This Works (Mechanism)
The results demonstrate that fine-grained domain adaptation can benefit from deeper backbone feature adaptation while preserving source task performance through dual-head evaluation. The mechanism appears to work because mid-to-late backbone features contain sufficient task-specific information for fine-grained detection while maintaining the general object detection capabilities learned from COCO. The dual-head setup allows the model to maintain source task capabilities while adapting feature representations for the target domain.

## Foundational Learning
- **Catastrophic forgetting**: Why needed - Understanding the phenomenon where neural networks forget previously learned tasks when trained on new tasks. Quick check - Verify that dual-head evaluation properly detects forgetting by comparing performance on both source and target tasks.
- **Transfer learning in object detection**: Why needed - Understanding how pre-trained models can be adapted to new domains while preserving learned capabilities. Quick check - Confirm that pre-trained COCO weights provide strong baseline performance on both tasks.
- **Feature hierarchy in CNNs**: Why needed - Understanding how different layers capture different levels of abstraction from low-level edges to high-level concepts. Quick check - Verify that unfreezing deeper layers (C5) provides more task-specific features for fine-grained detection.

## Architecture Onboarding
**Component Map**: Input -> Backbone (C1-C5 stages) -> Neck (PAFPN) -> Detection Head -> Output
**Critical Path**: Backbone feature extraction -> Neck feature aggregation -> Detection head prediction
**Design Tradeoffs**: Freezing early layers preserves general features but limits adaptation; unfreezing deeper layers enables specialization but risks forgetting; dual-head evaluation enables simultaneous monitoring of both tasks.
**Failure Signatures**: Significant COCO performance drop indicates catastrophic forgetting; poor fruit detection indicates insufficient adaptation; performance plateau suggests convergence.
**First Experiments**: 1) Baseline COCO performance with frozen backbone, 2) Head-only fine-tuning on fruit detection, 3) Full fine-tuning on fruit detection with COCO evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to YOLOv8n architecture and fruit detection domain, limiting generalizability
- Only reports mAP metrics without examining category-specific performance or subtle degradation patterns
- Does not investigate computational cost differences between freeze levels or feature space changes

## Confidence
- **High confidence**: The core finding that unfreezing to layer 10 provides significant gains on fruit detection while maintaining COCO performance is well-supported by the presented results and methodology.
- **Medium confidence**: Generalizability to other YOLO variants and domains requires further validation beyond the single architecture and fine-grained domain studied.
- **Medium confidence**: The claim of negligible degradation is supported for the specific metrics reported but may not capture all forms of forgetting.

## Next Checks
1. Replicate freeze-level experiments on YOLOv8s and YOLOv8m to verify generalizability across model sizes.
2. Conduct ablation studies on different fine-grained domains (vehicle types, animal species) to test the robustness of the layer 10 unfreezing strategy.
3. Perform extended training duration experiments to investigate whether minimal forgetting persists over longer training schedules.