---
ver: rpa2
title: Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated
  Texts Detection
arxiv_id: '2508.06913'
source_url: https://arxiv.org/abs/2508.06913
tags:
- text
- sentiment
- detection
- distribution
- llm-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SentiDetect, a model-agnostic framework
  for detecting LLM-generated text by analyzing sentiment distribution stability.
  Motivated by the observation that LLM outputs exhibit emotionally consistent patterns
  while human-written text shows greater variability, the method uses two unsupervised
  metrics: sentiment distribution consistency and sentiment distribution preservation.'
---

# Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection

## Quick Facts
- **arXiv ID:** 2508.06913
- **Source URL:** https://arxiv.org/abs/2508.06913
- **Reference count:** 7
- **One-line primary result:** SentiDetect achieves 11-16% F1 score improvements on LLM detection using sentiment distribution stability under low-emotional rewriting

## Executive Summary
SentiDetect introduces a model-agnostic framework for detecting LLM-generated text by analyzing sentiment distribution stability under low-emotional rewriting (LER) transformations. The method exploits the observation that LLM outputs exhibit more stable sentiment distributions than human writing when subjected to rewriting prompts like "Rewrite objectively." By applying multiple LER transformations and measuring sentiment distribution divergence, SentiDetect achieves strong performance across five diverse datasets and multiple LLM models, with particular robustness to paraphrasing and adversarial attacks that typically fool traditional detectors.

## Method Summary
SentiDetect operates through a pipeline that applies low-emotional rewriting transformations to input text using an external LLM, then analyzes the resulting sentiment distributions using a 3-class sentiment classifier. The method computes two unsupervised metrics: Sentiment Distribution Consistency (SDC) measures stability under rewriting, while Sentiment Distribution Preservation (SDP) captures stability under semantic-preserving transformations. The framework is model-agnostic, requiring no training data or model-specific fine-tuning, and relies on divergence thresholds to classify text as human or LLM-generated. The approach uses 5-7 optimized low-emotional rewriting prompts and standard sentiment analysis to extract the core stability signal.

## Key Results
- Achieves 16.4% F1 score improvement on Gemini-1.5-Pro and 11.2% on GPT-4-0613 compared to baseline detectors
- Demonstrates strong robustness to paraphrasing and adversarial attacks with significantly lower performance degradation than traditional methods
- Maintains effectiveness across diverse domains including news, code, essays, academic abstracts, and reviews
- Shows consistent performance across multiple LLM models (GPT-4, Claude-3, Gemini-1.5-Pro, LLaMA-3.3) without requiring model-specific training

## Why This Works (Mechanism)

### Mechanism 1: Low-Emotional Stability
LLM-generated text exhibits more stable sentiment distributions under low-emotional rewriting transformations than human-written text. When subjected to prompts like "Rewrite objectively," LLM outputs maintain consistent sentiment patterns while human text shows greater shifts. This stability arises from the regularized sentiment patterns instilled during LLM training on uniformly curated corpora.

### Mechanism 2: Semantic Preservation
LLM-generated text preserves its original sentiment distribution more reliably under semantic-preserving transformations (forward and inverse mapping pairs). The "attractor basin" for LLM generation in sentiment space is strong - when another LLM transforms an LLM's text (even with perturbations), it tends to pull the sentiment distribution back toward the model's inherent profile.

### Mechanism 3: Robustness to Adversarial Attacks
The method provides robustness to paraphrasing and adversarial perturbations because these attacks primarily alter lexical or syntactic features, not the underlying sentiment distribution stability. Traditional detectors relying on n-grams or perplexity are easily fooled by word substitutions, but sentiment distribution features remain less sensitive to these transformations.

## Foundational Learning

- **Log-Likelihood Detectors:** Earlier zero-shot methods rely on the LLM's probability of generating the text (high probability = AI). Understanding this baseline helps contextualize SentiDetect's shift from token probability to sentiment distribution stability. *Quick check:* Why does SentiDetect outperform LogRank on open-source models like LLaMA where LogRank often fails?

- **Zero-Shot vs. Supervised Detection:** SentiDetect is a zero-shot, model-agnostic framework requiring no training data for the specific detector. This architectural choice means no fine-tuning is needed, only access to an LLM API for rewriting steps. *Quick check:* What are the practical implications of SentiDetect being model-agnostic for deployment engineers?

- **Prompt Engineering for Transformation:** The mechanism depends entirely on the "low-emotional rewriting" prompts used for LER transformations. The paper uses AutoPrompt to optimize these transformation instructions, which is critical for the method's operation. *Quick check:* What is the role of the "low-emotional rewriting" prompts in the SentiDetect pipeline?

## Architecture Onboarding

- **Component map:** Input Text -> LLM API (Rewrite) -> Sentiment Classifier (Embed) -> Divergence Calc -> Threshold Compare -> Output Label
- **Critical path:** Input text undergoes multiple LLM rewriting steps via LER prompts, sentiment distributions are extracted and compared, divergence is calculated, and classification is made based on threshold comparison. API calls for rewriting are the latency bottleneck.
- **Design tradeoffs:** The number of LER prompts (I) directly impacts detection robustness but linearly increases latency and API cost. The paper suggests I=5 as optimal. Using the same LLM for rewriting and generation may yield the strongest signal. The method is significantly more expensive than lightweight classifiers due to multiple LLM API calls.
- **Failure signatures:** Short texts (<20 tokens) show degraded performance due to insufficient sentiment signal. Cross-domain/model mismatch can weaken the stability signal if rewriting and generating LLMs have different sentiment priors. Aggressive paraphrasing targeting sentiment directly can defeat the stability assumption.
- **First 3 experiments:**
  1. Baseline Verification: Implement sentiment classifier and calculate distributions for known human vs GPT-4 texts to validate the need for rewriting step.
  2. Ablation on Prompts: Run full pipeline with I=1 vs I=5 prompts on small human/GPT-4 dataset to measure AUROC/F1 improvement.
  3. Adversarial Stress Test: Apply synonym-swap adversarial attack to generated texts and compare SentiDetect output changes against LogRank baseline.

## Open Questions the Paper Calls Out

- **Cross-Model Consistency:** How effective is SentiDetect when the rewriting LLM differs from the generating LLM? The paper doesn't evaluate cross-model scenarios where sentiment priors may not align.

- **Short Text Detection:** The method's effectiveness on single-instance short texts (<50 tokens) remains limited without aggregation, leaving the fundamental challenge of detecting isolated short comments or tweets unresolved.

- **Future LLM Evolution:** Will the "emotional consistency" hypothesis persist as a detection signal for future LLMs specifically fine-tuned for high emotional variability, given the method relies on stability patterns from current training paradigms?

- **Sentiment Classifier Granularity:** The paper uses a standard 3-class classifier but doesn't explore whether fine-grained emotion detection (e.g., Ekman emotions) might capture subtler stability differences and improve detection performance.

## Limitations
- Reliance on sentiment distribution stability as a distinguishing feature may not persist as LLMs evolve to develop more human-like emotional variability
- Dependence on external LLM API calls for rewriting introduces computational cost and potential failure points
- Performance degrades on extremely short texts (<20 tokens) where insufficient sentiment signal exists

## Confidence

**High Confidence Claims:**
- SentiDetect outperforms traditional zero-shot methods (LogRank, MeanLogProb) across multiple datasets and LLM models with 11-16% F1 improvements
- The framework demonstrates robust performance across diverse domains and multiple LLM models without model-specific training
- The architectural design combining LER transformations with sentiment distribution analysis is technically sound

**Medium Confidence Claims:**
- Generalizability of sentiment distribution stability as a universal LLM fingerprint across all future model architectures
- Optimal number of LER prompts (I=5) as a universally applicable parameter
- Effectiveness on highly specialized or technical domains not covered in evaluation

**Low Confidence Claims:**
- Claims about effectiveness on extremely short texts (<20 tokens) are theoretical with acknowledged limitations
- Long-term stability of this detection approach as LLMs continue to evolve and develop more human-like writing patterns

## Next Checks

1. **Cross-Model Sentiment Consistency Test:** Evaluate SentiDetect's performance when the rewriting LLM (Fâ‚) differs from the generating LLM (e.g., Claude-3 rewritten by GPT-4) to validate cross-model sentiment preservation assumptions.

2. **Longitudinal Robustness Study:** Re-run the evaluation pipeline on the same datasets after 6-12 months using newer LLM versions (GPT-5, Claude-4) to assess whether sentiment distribution stability remains a reliable detection signal.

3. **Domain Generalization Test:** Apply SentiDetect to specialized domains not covered in original evaluation (medical literature, legal documents, poetry) to validate broad domain applicability and identify domain-specific failure patterns.