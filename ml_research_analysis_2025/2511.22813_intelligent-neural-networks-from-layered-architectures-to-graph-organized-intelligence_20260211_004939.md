---
ver: rpa2
title: 'Intelligent Neural Networks: From Layered Architectures to Graph-Organized
  Intelligence'
arxiv_id: '2511.22813'
source_url: https://arxiv.org/abs/2511.22813
tags:
- neurons
- graph
- mamba
- attention
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Intelligent Neural Networks (INN), a paradigm
  that replaces monolithic layers with a graph of intelligent neurons, each possessing
  internal state and learned communication patterns. INN integrates selective state-space
  dynamics (Mamba) with attention-based routing to enable emergent computation through
  graph-structured interactions.
---

# Intelligent Neural Networks: From Layered Architectures to Graph-Organized Intelligence

## Quick Facts
- arXiv ID: 2511.22813
- Source URL: https://arxiv.org/abs/2511.22813
- Authors: Antoine Salomon
- Reference count: 17
- Primary result: 1.705 BPC on Text8 character modeling, outperforming Transformer baseline

## Executive Summary
This paper introduces Intelligent Neural Networks (INN), a paradigm that replaces monolithic layers with a graph of intelligent neurons, each possessing internal state and learned communication patterns. INN integrates selective state-space dynamics (Mamba) with attention-based routing to enable emergent computation through graph-structured interactions. On the Text8 character modeling benchmark, INN achieves 1.705 Bit-Per-Character, significantly outperforming a comparable Transformer (2.055 BPC) and matching an optimized LSTM baseline. Crucially, a parameter-matched stack of Mamba blocks fails to converge (>3.4 BPC) under the same training protocol, demonstrating that INN's graph topology provides essential training stability.

## Method Summary
INN replaces traditional stacked layers with a graph of intelligent neurons, where each neuron contains a Mamba block for selective state-space memory and participates in inter-neuron attention routing. The architecture uses N=32 neurons, each with d_model=256 and d_state=16, arranged in L=6 layers. Each layer consists of independent Mamba updates per neuron, followed by multi-head attention across the neuron dimension, with pre-norm residuals and layer normalization. The final output is aggregated via mean pooling across neurons. Training uses AdamW with OneCycleLR scheduler, gradient clipping, and careful initialization to ensure convergence stability.

## Key Results
- INN achieves 1.705 BPC on Text8, outperforming Transformer (2.055 BPC) and matching LSTM baseline
- Parameter-matched Mamba stack baseline fails to converge (>3.4 BPC), demonstrating graph topology's stabilizing effect
- Ablation studies show learned routing is essential: static communication degrades to 2.085 BPC, no communication to 1.998 BPC
- INN underperforms LSTM on large-vocab word-level tasks (PTB) due to vocabulary bottleneck

## Why This Works (Mechanism)

### Mechanism 1: Graph Topology as Structural Stabilizer
The graph structure distributes gradient flow through multiple pathways (N×N attention matrix), preventing the optimization landscape pathologies that cause divergence in depth-wise stacked Mamba blocks. The stability derives from topological redundancy rather than the primitive itself.

### Mechanism 2: Selective State-Space Memory Per Neuron
Each neuron's internal Mamba block enables context-dependent activation through input-dependent B and C matrices in the SSM equations, allowing individual units to "know when" to participate by gating their own information retention independently.

### Mechanism 3: Learned Routing Must Be Selective, Not Static
Attention-based inter-neuron communication improves performance only when learned; static (fixed) weights inject unstructured interference. The routing function itself must be optimized during training—random or uniform connectivity is harmful.

## Foundational Learning

- **State-Space Models (SSMs)**: INN's "memory" component is a Mamba block per neuron. Understanding how h′ = Ah + Bx with input-dependent B/C enables selectivity is essential to grasp why neurons can "know when to activate."
  - Quick check: Can you explain why making B and C input-dependent allows an SSM to selectively remember vs. forget information?

- **Multi-head attention over arbitrary dimensions**: INN applies attention over the *neuron dimension* (N), not the sequence dimension. This is unconventional—understanding Q, K, V ∈ R^(B×N×L×d) is critical.
  - Quick check: In standard Transformers, attention operates over sequence positions. What changes when attention operates over neurons instead?

- **Graph topology vs. layer depth**: INN replaces h^(l+1) = f(h^l) with graph-structured message passing. This is NOT a GNN processing graph data—it's a network whose *architecture* is a graph.
  - Quick check: How does INN differ from a Graph Attention Network (GAT) applied to sequential data?

## Architecture Onboarding

- **Component map**: Input projection → N neuron replication → Per-neuron Mamba block → Inter-neuron attention → L layers → Mean aggregation → Output projection
- **Critical path**: Understand the forward pass order—(1) independent Mamba updates per neuron, (2) attention-based message exchange, (3) repeat for L layers. The residual connections and LayerNorm placement matter.
- **Design tradeoffs**: N (neurons) vs. complexity: Attention is O(N² × L × d). Paper uses N=32; scaling to N=1024+ requires sparse routing. Vocabulary size vs. neuron capacity: On PTB (10k vocab), embeddings consume the parameter budget, starving the neurons. INN excels at character/subword level. Engineering overhead: No fused CUDA kernel exists for neuron-graph attention; current implementation is ~50× slower than optimized LSTM/Transformer kernels (engineering issue, not theoretical).
- **Failure signatures**: Mamba-only stack divergence: High learning rate + OneCycleLR causes BPC > 3.4 (should be ~1.7). Static communication degradation: If attention weights aren't learned, BPC worsens vs. no-communication baseline. Vocabulary bottleneck: On large-vocab word-level tasks, INN underperforms LSTM despite matching on character-level.
- **First 3 experiments**:
  1. **Reproduce the Mamba Stack vs. INN divergence**: Train both on Text8 20M subset with identical hyperparameters. Confirm Mamba Stack fails to converge while INN reaches ~1.7 BPC.
  2. **Ablation: No-Communication vs. Static vs. Learned**: Isolate the routing component by freezing attention weights or disabling them entirely. Verify the 1.998 → 2.085 → 1.705 BPC progression.
  3. **Neuron activation profiling**: Visualize per-neuron activation patterns on held-out data. Confirm emergent hubs (high in-degree neurons) and specialists (low-connectivity neurons) as shown in Figure 3.

## Open Questions the Paper Calls Out

- **Scaling to Billion-token regimes**: The authors state validating on 100M tokens leaves "Scaling to Billion-token regimes" as an "open challenge." Training and evaluation results on large-scale datasets (e.g., The Pile or Common Crawl) showing stable loss curves and competitive performance would resolve this.

- **Vocabulary Barrier for word-level tasks**: Section 3.5 identifies a "Vocabulary Barrier" where large embeddings (10k tokens) consume the parameter budget, causing INN to lag behind LSTMs on Penn TreeBank. A modified INN using sparse embeddings or subword tokenization that matches or exceeds LSTM performance on word-level modeling benchmarks would resolve this.

- **Scaling neuron population beyond N=32**: Section 4.3 notes that scaling to N=1024+ neurons "would likely require sparse routing or hierarchical graphs" to maintain efficiency. The current inter-neuron attention mechanism has a complexity of O(N²), which becomes computationally prohibitive. Implementation of sparse or hierarchical routing mechanisms demonstrating linear or near-linear scaling with N while preserving model accuracy would resolve this.

## Limitations

- The claimed training stability advantage of INN's graph topology rests on a single failure mode (Mamba stack divergence) without exploring whether alternative optimization strategies could rescue the baseline.
- The vocabulary bottleneck on word-level tasks suggests INN's strength lies in character/subword modeling where embedding parameters don't dominate the budget, but architectural modifications to address this limitation remain unexplored.
- The graph attention over neurons represents an unconventional architectural choice that lacks theoretical grounding in the paper—while empirical results are strong, the work doesn't explain why neuron-level attention outperforms traditional layer-wise attention.

## Confidence

- **High Confidence (5/5)**: INN achieves 1.705 BPC on Text8, outperforming a baseline Transformer (2.055 BPC) and matching LSTM baseline.
- **Medium Confidence (3/5)**: The claim that INN's graph topology provides essential training stability—while the Mamba stack divergence is demonstrated, the analysis lacks exploration of alternative explanations.
- **Medium Confidence (3/5)**: The assertion that learned routing is necessary for performance—ablation results support this, but the mechanism connecting graph structure to stability remains speculative.
- **Low Confidence (2/5)**: The claim about emergent hub and specialist neurons (Figure 3)—while the paper reports varied activation statistics, the visualization and interpretation lack statistical rigor or comparison to random baselines.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rate, batch size, and sequence length for both INN and Mamba stack baselines. Document whether the stability advantage persists across a broader hyperparameter range, or if specific configurations could rescue the Mamba stack.

2. **Vocabulary Capacity Scaling**: Implement an INN variant that decouples embedding parameters from neuron parameters (e.g., separate embedding matrix). Train on PTB and WikiText-2 to determine whether the vocabulary bottleneck is fundamental or architectural.

3. **Graph Topology Ablation**: Test alternative graph structures (sparse attention, different connectivity patterns) while maintaining the same neuron count and attention mechanism. Compare convergence and performance to determine whether the complete graph topology is optimal or merely sufficient.