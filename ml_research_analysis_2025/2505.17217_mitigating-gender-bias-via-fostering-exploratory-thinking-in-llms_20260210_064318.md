---
ver: rpa2
title: Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs
arxiv_id: '2505.17217'
source_url: https://arxiv.org/abs/2505.17217
tags:
- gender
- bias
- data
- moral
- story
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to mitigating gender bias
  in large language models (LLMs) by fostering exploratory thinking. The authors first
  identify instances where LLMs exhibit gender bias by generating morally ambiguous
  story pairs with male and female protagonists.
---

# Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs

## Quick Facts
- arXiv ID: 2505.17217
- Source URL: https://arxiv.org/abs/2505.17217
- Authors: Kangda Wei; Hasnat Md Abdullah; Ruihong Huang
- Reference count: 40
- One-line primary result: Novel method reduces gender bias in LLMs by fostering exploratory thinking, achieving lower prediction mismatch rates while maintaining or improving general capabilities.

## Executive Summary
This paper introduces a novel approach to mitigating gender bias in large language models by fostering exploratory thinking rather than simply correcting biased outputs. The method generates paired, gender-swapped morally ambiguous stories, identifies inconsistent moral judgments, and guides the model to produce balanced, gender-neutral explanations that integrate multiple perspectives. Through fine-tuning or Direct Preference Optimization (DPO) using these exploratory judgments, the approach significantly reduces gender bias across multiple benchmarks while preserving or enhancing general model capabilities.

## Method Summary
The approach works by first generating story pairs with male and female protagonists in structurally identical, morally ambiguous scenarios. The model produces separate moral judgments for each story, and pairs with inconsistent judgments are retained. For these inconsistent pairs, the model is prompted to generate balanced, gender-neutral judgments that integrate both moral and immoral perspectives. These neutral judgments are then used to fine-tune or optimize the model via Direct Preference Optimization (DPO). The method is evaluated on three benchmarks: GenMO (measuring prediction mismatch rates), WinoBias (measuring F1 scores on gender-stereotypical professions), and BBQ (measuring bias classification accuracy).

## Key Results
- Fine-tuning with 1,000 story pairs reduced GenMO prediction mismatch rate (PMR) to 0.077 from 0.088 baseline
- DPO with 500 story pairs achieved PMR of 0.085, maintaining strong performance on MMLU and TruthfulQA
- The method outperformed counterfactual data augmentation (CDA) on all benchmarks while preserving general capabilities

## Why This Works (Mechanism)

### Mechanism 1
Generating paired, gender-swapped stories exposes latent bias by forcing the model to confront its own inconsistent moral reasoning. The pipeline prompts an LLM to produce structurally identical stories with male/female protagonists, elicits separate moral judgments, and filters for pairs where stances diverge (Jf ≠ Jm). This creates a diagnostic signal—bias is revealed through contradiction rather than external annotation. Core assumption: The model's inconsistency across gender-swapped scenarios reflects genuine representational bias rather than random variation or prompt sensitivity.

### Mechanism 2
Prompting the model to generate neutral, multi-perspective judgments reshapes reasoning from confirmatory to exploratory patterns. Once inconsistent judgments are identified, the model is given both biased judgments and asked to produce an integrated "neutral" explanation acknowledging moral and immoral perspectives. This targets the cognitive pattern—bias-as-confirmatory-thinking—rather than merely correcting outputs. Core assumption: The paper explicitly frames bias as "confirmatory thinking" (justifying pre-existing beliefs) and exploratory thinking as the corrective.

### Mechanism 3
Fine-tuning and DPO encode the exploratory reasoning pattern into model parameters, with fine-tuning producing deeper representational shifts and DPO yielding more conservative adjustments. Fine-tuning directly supervises the model on neutral judgments (input: story; output: balanced explanation). DPO uses biased judgments as rejected responses and neutral judgments as preferred responses, optimizing the policy without full supervision. The paper's layer-wise similarity analysis shows fine-tuning alters mid-to-upper layers more substantially.

## Foundational Learning

- Concept: **Confirmatory vs. Exploratory Thinking**
  - Why needed here: The paper's theoretical contribution frames gender bias as a cognitive pattern (confirmatory reasoning) rather than merely output-level error. Understanding this distinction clarifies why neutral judgment generation—not just data augmentation—is the intervention.
  - Quick check question: Can you explain why swapping judgments between stories (CDA) produces weaker results than generating new neutral judgments?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: DPO is one of two training methods evaluated. Unlike fine-tuning, it optimizes a preference policy using chosen/rejected pairs without explicit supervision, which has implications for trade-offs between bias reduction and capability preservation.
  - Quick check question: In DPO, what serves as the "rejected" response and what serves as the "chosen" response in this paper's setup?

- Concept: **Counterfactual Data Augmentation (CDA)**
  - Why needed here: CDA is the baseline comparison. Understanding why the paper's exploratory-thinking approach outperforms CDA helps distinguish between surface-level debiasing and deeper reasoning-pattern modification.
  - Quick check question: How does CDA's approach of swapping judgments differ from the paper's approach of generating new neutral judgments?

## Architecture Onboarding

- Component map: Story Generator -> ROUGE Filter -> Judgment Elicitation -> Inconsistency Filter -> Neutralization Prompt -> Training Wrapper
- Critical path: Story generation → ROUGE filtering → judgment elicitation → inconsistency detection → neutralization → training data assembly → fine-tuning or DPO. The inconsistency filter is the key gating step—only biased cases proceed to neutralization.
- Design tradeoffs:
  - Fine-tuning: More consistent bias reduction across benchmarks, but causes larger drops in STEM subjects on MMLU and requires careful data volume tuning.
  - DPO: Better preserves or improves general capabilities on some models (e.g., Llama MMLU +0.7%), but shows instability on Mistral and produces smaller representational shifts.
  - Data volume: Llama optimal at 1,000 pairs (FT) or 500 (DPO); Mistral optimal at 5,000 pairs (FT) or 2,000 (DPO). Over-tuning increases bias or degrades general performance.
- Failure signatures:
  - PMR reduction without Δ reduction: Model converges to vague categories ("Both," "Can't say") rather than genuine consistency.
  - Pro-stereotypical F1 collapse on WinoBias: Over-correction that sacrifices task performance.
  - STEM subject drops on MMLU: Factual knowledge overwritten by moral reasoning data.
  - Mistral DPO instability: Large accuracy swings with data volume changes.
- First 3 experiments:
  1. Reproduce the story-pair generation and filtering pipeline on a small scale (100 pairs), verify ROUGE scores and inconsistency detection rate before proceeding to training.
  2. Run fine-tuning with 500, 1,000, and 2,000 pairs on Llama-3.1-8B-Instruct, evaluating on WinoBias validation set to identify optimal data volume before full benchmark runs.
  3. Compare fine-tuning vs. DPO vs. FT+DPO on a single model using GenMO and WinoBias, focusing on both bias metrics (PMR, Δ) and capability preservation (MMLU subset) to establish tradeoff profile.

## Open Questions the Paper Calls Out

- Can this exploratory thinking framework effectively mitigate other social biases, such as race or religion, using the same data generation techniques? The conclusion states, "Future work can extend this methodology to other social biases mitigation tasks." The current study focused exclusively on gender bias using gender-swapped protagonists.

- Does the current method generalize to non-binary, transgender, or intersectional gender identities? The Limitations section notes the framework is "restricted to binary gender categories (male/female)." The data generation process only creates pairs with strictly male and female protagonists.

- How does scaling the training data beyond 5,000 pairs affect the trade-off between bias mitigation and factual retention? The authors note in the Limitations section that current conclusions may shift with "larger-scale training" and that the data generator is scalable. Experiments were capped at 5,000 story pairs; it is unknown if more data amplifies the observed performance drops in STEM subjects.

## Limitations

- The paper's claim that gender bias manifests as "confirmatory thinking" is an appealing theoretical framing but lacks direct empirical validation within the study—it is inferred from behavioral outcomes rather than measured cognitive patterns.
- The paired-story generation and inconsistency detection pipeline depends on subjective thresholds (e.g., ROUGE similarity τ and moral stance differences) that may not generalize across domains or model architectures.
- The observed trade-offs between bias reduction and capability preservation are not fully characterized—over-tuning effects on STEM subjects and model instability (particularly for DPO on Mistral) suggest fragility in the approach.

## Confidence

- **High confidence**: The experimental results showing reduced prediction mismatch rates (PMR) and improved consistency on GenMO, along with maintained or enhanced performance on MMLU and TruthfulQA, are well-supported by the reported metrics and methodology.
- **Medium confidence**: The claim that fine-tuning induces deeper representational shifts than DPO is supported by layer-wise similarity analyses, but the interpretation of these changes as "exploratory thinking" patterns is inferential and not directly measured.
- **Low confidence**: The assertion that gender bias in LLMs is fundamentally a manifestation of "confirmatory thinking" is a theoretical contribution that lacks empirical validation within the paper—it is inferred from the observed effects of the proposed intervention rather than independently verified.

## Next Checks

1. Validate the confirmatory vs. exploratory thinking framing by designing an experiment to measure whether the model's reasoning patterns before and after training can be classified as confirmatory or exploratory using established cognitive bias detection methods (e.g., consistency in counterfactual reasoning or sensitivity to alternative perspectives).

2. Test robustness across model architectures by applying the same methodology to other LLM families (e.g., GPT, Claude) and evaluate whether the observed bias reduction and capability preservation patterns hold, or if they are specific to Llama and Mistral.

3. Evaluate real-world bias scenarios by assessing the method's effectiveness on naturally occurring gender bias cases from real-world text corpora (e.g., news articles, social media) rather than relying solely on synthetic story pairs, to determine ecological validity.