---
ver: rpa2
title: 'Global Convergence in Neural ODEs: Impact of Activation Functions'
arxiv_id: '2509.22436'
source_url: https://arxiv.org/abs/2509.22436
tags:
- neural
- odes
- convergence
- training
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the training dynamics of Neural ODEs by
  examining how activation function properties impact their optimization. The authors
  demonstrate that smooth activation functions ensure well-posed forward and backward
  ODEs, enabling accurate gradient computation and gradient equivalence between "optimize-then-discretize"
  and "discretize-then-optimize" approaches.
---

# Global Convergence in Neural ODEs: Impact of Activation Functions

## Quick Facts
- arXiv ID: 2509.22436
- Source URL: https://arxiv.org/abs/2509.22436
- Authors: Tianxiang Gao; Siyuan Sun; Hailiang Liu; Hongyang Gao
- Reference count: 40
- This paper establishes global convergence guarantees for Neural ODEs by demonstrating that smooth activation functions ensure well-posed optimization dynamics and strict positive definiteness of the Neural Tangent Kernel.

## Executive Summary
This paper provides a theoretical foundation for training Neural ODEs by analyzing how activation function properties impact optimization dynamics. The authors demonstrate that smooth activation functions guarantee well-posed forward and backward ODEs, enabling accurate gradient computation and gradient equivalence between "optimize-then-discretize" and "discretize-then-optimize" approaches. Through novel mathematical analysis, they establish that non-polynomial nonlinear activations ensure strict positive definiteness of the Neural Tangent Kernel, leading to global convergence under gradient descent in overparameterized regimes. Experimental results validate these theoretical findings, showing faster convergence and more stable parameter updates with smooth activations like Softplus compared to non-smooth activations like ReLU.

## Method Summary
The paper investigates Neural ODEs defined by the continuous-depth dynamics ḣ_t = σ_w Wϕ(h_t)/√n with Gaussian initialization. The theoretical framework analyzes the convergence of finite-depth ResNet approximations to the continuous ODE and establishes conditions for gradient equivalence between optimization strategies. The authors prove that Lipschitz continuous smooth activations ensure unique solutions for both forward and backward ODEs, while non-polynomial nonlinear activations guarantee strict positive definiteness of the limiting Neural Tangent Kernel. Global convergence is established under gradient descent in the overparameterized regime where network width exceeds training sample size. The method involves analyzing the NTK's limiting behavior and its implications for training dynamics.

## Key Results
- Smooth activation functions guarantee globally unique solutions for forward and backward ODEs, enabling accurate gradient computation
- Neural ODEs with non-polynomial nonlinear activations have strictly positive definite Neural Tangent Kernels, enabling global convergence under gradient descent
- Gradient equivalence between "optimize-then-discretize" and "discretize-then-optimize" approaches holds under sufficient smoothness conditions, with 1/L convergence rates
- Experimental validation shows Softplus activations converge faster than ReLU with more stable parameter updates

## Why This Works (Mechanism)

### Mechanism 1: Smooth Activation Functions Ensure Well-Posed ODE Solutions
- Claim: Smooth activation functions with Lipschitz continuity guarantee globally unique solutions for both forward and backward ODEs.
- Mechanism: The Lipschitz continuity of ϕ ensures the ODE dynamics are well-behaved, satisfying the conditions of the Picard-Lindelöf theorem. This prevents numerical instabilities and ensures gradient computation accuracy.
- Core assumption: The activation function ϕ is L₁-Lipschitz continuous, and its derivative ϕ' is L₂-Lipschitz continuous.
- Evidence anchors:
  - [abstract] "Smooth activation functions guarantee globally unique solutions for both forward and backward ODEs"
  - [Page 4, Section 3.1, Proposition 1] Demonstrates that if ϕ is Lipschitz continuous, forward and backward ODEs have unique solutions for all t ∈ [0,T]
  - [corpus] Weak corpus evidence - no related papers directly address well-posedness with smooth activations in Neural ODEs
- Break condition: If activation function lacks Lipschitz continuity (e.g., discontinuous or non-smooth functions like ReLU at origin), the ODE solutions may not be globally unique, leading to numerical errors in gradient computation.

### Mechanism 2: Non-Polynomial Nonlinearity Ensures Strict Positive Definiteness of NTK
- Claim: When using nonlinear but non-polynomial activation functions, the Neural Tangent Kernel (NTK) of Neural ODEs is strictly positive definite (SPD), enabling global convergence under gradient descent.
- Mechanism: The non-polynomial nature of ϕ ensures that the NNGP kernel Σ* and limiting NTK K∞ have Hermitian expansions with infinitely many non-zero coefficients for both even and odd terms, which is necessary and sufficient for strict positive definiteness on the sphere S^{d-1}.
- Core assumption: The activation function ϕ is nonlinear and non-polynomial (sufficient condition, not necessary).
- Evidence anchors:
  - [abstract] "nonlinear but non-polynomial activations... leads to global convergence guarantees"
  - [Page 8-9, Section 5, Proposition 5 and Corollary 1] Establish SPD property under non-polynomial activations
  - [Page 36-38, Appendix F, Lemmas 11-15] Provide Hermitian expansion analysis
  - [corpus] No corpus papers address NTK SPD properties in Neural ODEs specifically
- Break condition: If activation function is polynomial (e.g., quadratic), the SPD property may still hold but convergence is slower. The non-polynomial condition is sufficient but not necessary.

### Mechanism 3: Gradient Equivalence Through Uniform Convergence
- Claim: Under sufficient smoothness conditions, the gradients computed via "optimize-then-discretize" and "discretize-then-optimize" converge to the same values as depth L → ∞.
- Mechanism: The Lipschitz continuity of both ϕ and ϕ' enables uniform convergence of the finite-depth ResNet approximation to the continuous Neural ODE. Using Euler's convergence theorem and the Moore-Osgood theorem, the double sequence of gradients converges uniformly in both width and depth.
- Core assumption: Both ϕ and its derivative ϕ' must be Lipschitz continuous for gradient equivalence.
- Evidence anchors:
  - [Page 5, Proposition 2] Shows ∥∇θf_L(x) - ∇_θf(x)∥ ≤ C L^{-1} under Lipschitz conditions
  - [Page 6-7, Lemma 1-2] Establish uniform convergence of activation products and NTK
  - [corpus] No corpus papers address gradient equivalence between optimization strategies in Neural ODEs
- Break condition: If ϕ' is not Lipschitz continuous (e.g., ReLU with discontinuous derivative), gradient differences between the two methods fail to converge and may oscillate during training.

## Foundational Learning

### Concept: Picard-Lindelöf Theorem and ODE Well-Posedness
- Why needed here: Understanding that Lipschitz continuity guarantees unique global solutions to ODEs is essential for appreciating why smooth activations are necessary for stable Neural ODE training.
- Quick check question: If an ODE ẋ = f(t,x) has f that is Lipschitz in x with constant L, what does Picard-Lindelöf guarantee about solutions?

### Concept: Neural Tangent Kernel (NTK)
- Why needed here: The NTK framework is central to understanding training dynamics in overparameterized networks. The paper extends NTK theory from discrete to continuous-depth models.
- Quick check question: In the infinite-width limit, what happens to the NTK during training, and how does this enable convergence analysis?

### Concept: Dual Activation and Hermitian Expansion
- Why needed here: Understanding how activation functions can be expanded in terms of Hermite polynomials and how this relates to kernel definiteness is crucial for the SPD proofs.
- Quick check question: If a function ϕ has Hermitian expansion ϕ(x) = Σ aₙhₙ(x), what can you say about its dual activation ẑ(ρ)?

## Architecture Onboarding

### Component map:
Input x ∈ R^d
    ↓
h₀ = σ_u Ux/√d [Initial hidden state]
    ↓
Forward ODE: ḣ_t = σ_w Wϕ(h_t)/√n, t ∈ [0,T]
    ↓
h_T [Final hidden state]
    ↓
Output: f(x;θ) = σ_v v^T ϕ(h_T)/√n

Backward ODE (Adjoint):
λ_T = σ_v diag(ϕ'(h_T))v/√n
λ̇_t = -σ_w diag(ϕ'(h_t))W^T λ_t/√n

### Critical path:
1. **Initialization**: Random Gaussian initialization of U, W, v with variance hyperparameters σ_u, σ_w, σ_v
2. **Forward pass**: Solve ODE from t=0 to t=T using numerical solver (Euler, RK4, or adaptive)
3. **Gradient computation**: Solve backward ODE (adjoint method) or discretize and use backpropagation
4. **Update**: Apply gradient descent with learning rate η ≤ 1/∥X∥²

### Design tradeoffs:
- **Optimize-then-discretize vs. Discretize-then-optimize**: First is memory-efficient but may have numerical errors; second is memory-intensive but exact gradients (if smoothness holds)
- **Time horizon T**: Longer T captures more complex dynamics but requires scaling σ_w ∼ 1/T to prevent exponential growth
- **Width n vs. Depth L**: Larger width ensures NTK convergence to deterministic limit; discretization depth L controls gradient approximation accuracy

### Failure signatures:
1. **Gradient mismatch**: Non-smooth activation (ReLU) causes gradient oscillation between methods
2. **Early damping**: Output variance grows with T without σ_w ∼ 1/T scaling
3. **Negative NTK eigenvalues**: Width n < training samples N causes poor conditioning

### First 3 experiments:
1. **Verify gradient convergence**: Compare Softplus vs. ReLU on output/gradient differences between Neural ODE and finite-depth ResNet. Expect 1/L convergence for Softplus, stagnation for ReLU gradients.
2. **Test NTK SPD property**: Monitor smallest NTK eigenvalue during training with widths [500, 1000, 2000, 4000]. Expect positive eigenvalues when n > N (training samples).
3. **Validate scaling for long horizons**: Train with T ∈ {1, 2, 5, 10} with and without σ_w ∼ 1/T scaling. Expect early damping without scaling, stable training with scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can global convergence guarantees be established for Stochastic Gradient Descent (SGD) in Neural ODEs?
- **Basis in paper:** [explicit] Page 2 states it remains an "open problem whether simple first-order methods, such as stochastic gradient descent (SGD), can reliably train Neural ODEs to convergence."
- **Why unresolved:** The paper's theoretical framework (Theorem 3) specifically proves global convergence for standard Gradient Descent (GD) but does not address the stochasticity inherent in SGD mini-batching.
- **What evidence would resolve it:** A theoretical extension of the Neural Tangent Kernel (NTK) analysis that guarantees convergence for SGD, accounting for gradient noise in the continuous-time limit.

### Open Question 2
- **Question:** How do numerical errors and solver stiffness impact global convergence in large-scale or complex systems?
- **Basis in paper:** [inferred] Page 52 notes that theoretical guarantees relying on smoothness "may not generalize to more complex systems... where numerical errors can be influenced by other factors such as stiffness."
- **Why unresolved:** The paper's theory assumes idealized solver conditions (Lipschitz continuity), whereas practical large-scale training often introduces stiffness that degrades solver accuracy and stability.
- **What evidence would resolve it:** Convergence bounds that explicitly incorporate error terms from adaptive step-size solvers operating on stiff or chaotic dynamics.

### Open Question 3
- **Question:** Do the gradient equivalence and convergence results extend to general, non-autonomous Neural ODE forms?
- **Basis in paper:** [inferred] Appendix I discusses extending results to general dynamic forms (e.g., with attention or gating) but focuses the main analysis on the specific pre-activation autonomous form.
- **Why unresolved:** The main proofs rely on the specific structure of the autonomous ODE; extending this to non-autonomous systems requires verifying unproven higher-order regularity conditions on the Jacobian.
- **What evidence would resolve it:** A formal proof of the limiting NTK's existence and strict positive definiteness for time-dependent (non-autonomous) Neural ODE dynamics.

## Limitations
- The non-polynomial activation requirement for strict positive definiteness is sufficient but not necessary, limiting theoretical completeness
- Focus on single-layer Neural ODEs; extension to multi-layer architectures with skip connections requires further investigation
- Computational complexity of exact NTK computation for larger networks presents practical challenges for verification

## Confidence
**High Confidence:**
- Gradient convergence rates for smooth vs. non-smooth activations (1/L vs. non-convergence)
- Necessity of weight scaling σ_w ~ 1/T for long time horizons
- Positive definiteness requirement for NTK in overparameterized regimes

**Medium Confidence:**
- Strict positive definiteness under non-polynomial activations (sufficient but not necessary condition)
- Equivalence between optimize-then-discretize and discretize-then-optimize approaches
- Generalization to real-world datasets beyond synthetic MNIST experiments

**Low Confidence:**
- Performance in multi-layer Neural ODE architectures
- Computational feasibility of exact NTK computation for practical network sizes
- Impact of activation function smoothness on training dynamics in presence of noise or regularization

## Next Checks
1. **Activation Function Spectrum Analysis**: Systematically test NTK eigenvalue spectra across a broader range of activation functions including polynomial, exponential, and rational functions to empirically validate the boundary between SPD and non-SPD cases.

2. **Multi-Layer Extension Study**: Implement and analyze a two-layer Neural ODE architecture with skip connections to verify whether the theoretical guarantees extend to more complex architectures while maintaining gradient convergence and NTK properties.

3. **Computational Scalability Assessment**: Develop approximate NTK computation methods for larger networks and validate whether the convergence guarantees hold when using these approximations versus exact computation for small networks.