---
ver: rpa2
title: 'One Task Vector is not Enough: A Large-Scale Study for In-Context Learning'
arxiv_id: '2505.23911'
source_url: https://arxiv.org/abs/2505.23911
tags:
- task
- tasks
- vector
- output
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces QUITE AFEW, a large-scale dataset of 3,096
  diverse few-shot learning tasks, each with 30 input-output pairs derived from the
  Alpaca dataset. Experiments with Llama-3-8B reveal that task vector performance
  peaks at an intermediate layer (e.g., 15th) and varies significantly by task type.
---

# One Task Vector is not Enough: A Large-Scale Study for In-Context Learning

## Quick Facts
- arXiv ID: 2505.23911
- Source URL: https://arxiv.org/abs/2505.23911
- Authors: Pavel Tikhonov; Ivan Oseledets; Elena Tutubalina
- Reference count: 15
- Primary result: Single task vectors work well for simple tasks but fail for complex, compositional tasks; distributed sub-task vectors are required.

## Executive Summary
This study challenges the prevailing assumption that a single task vector can capture all task-relevant information for in-context learning. Through experiments on QUITE AFEW, a large-scale dataset of 3,096 diverse few-shot tasks, the authors demonstrate that task vector performance varies significantly by task type and peaks at an intermediate layer (e.g., 15th in Llama-3-8B). Most critically, complex tasks requiring structured, multi-step outputs need multiple subtask-specific vectors rather than a single global vector. The findings reveal that LLMs represent task knowledge in a distributed fashion, with format information more reliably encoded than correctness content.

## Method Summary
The authors constructed QUITE AFEW by labeling 3,096 instructions from the Alpaca dataset using Qwen-2.5-72B, retaining "GOOD" tasks and generating 30 diverse input-output pairs for each. For each task, they built 7-shot prompts with separator tokens, extracted hidden states at each layer from the final separator token, and injected these vectors into zero-shot inference at corresponding positions. Performance was evaluated using LLM judges for format (0-10) and correctness (0-10) scores. The study compared task vector injection against zero-shot and full few-shot baselines across different task categories and layer depths.

## Key Results
- Task vector performance peaks at intermediate layers (e.g., layer 15) and varies significantly across task types
- Complex tasks show format scores consistently exceeding correctness scores, indicating partial task understanding
- Single task vectors fail for compositional tasks; multiple subtask-specific vectors are required for effective performance
- QUITE AFEW reveals that real-world tasks are more complex and diverse than previously assumed in small-scale studies

## Why This Works (Mechanism)

### Mechanism 1: Intermediate-Layer Task Encoding
Task information is most coherently encoded at a specific intermediate layer, not uniformly across the network. Early layers handle token-level features while later layers prepare for generation, with an intermediate layer serving as a convergence point where the task rule is summarized into a latent vector before conditioning generation.

### Mechanism 2: Distributed Sub-Task Representation for Complex Tasks
Complex, compositional tasks are represented by multiple, specialized sub-task vectors rather than a single global vector. Different latent representations activate at different points in the output sequence, making a single vector extracted from a final separator token insufficient to capture the full sequence of required operations.

### Mechanism 3: Partial Task Understanding (Format vs. Correctness)
Task vectors often capture the output format more reliably than the correctness of specific content. The latent vector encodes structural constraints robustly, while the specific knowledge retrieval or logical operation required for correct content is sometimes absent or noisy in the same vector.

## Foundational Learning

- **Task Vectors (Function Vectors)**: A specific hidden state activation hypothesized to encode a task rule. Why needed: This is the central object of study. Quick check: Can you explain the difference between a model's weights and a task vector?

- **Causal Intervention in LLMs**: The core method where vectors are extracted from one run and injected into another to prove causality. Why needed: The paper doesn't just observe vectors; it extracts and injects them. Quick check: How would you design an experiment to prove that a specific hidden state causes a model to perform a task?

- **In-Context Learning (ICL)**: Learning from demonstrations without parameter updates. Why needed: Task vectors are the proposed mechanism for how ICL works. Quick check: How does ICL differ from fine-tuning?

## Architecture Onboarding

- **Component map**: Alpaca instructions → QUITE AFEW dataset → 7-shot prompts → Forward pass → Vector extraction → Target input → Vector injection → Modified generation → LLM evaluation

- **Critical path**: The injection step is most critical. Injecting at wrong layer or token position will fail. The "Layer 15" finding is a key operational parameter for Llama-3-8B.

- **Design tradeoffs**: Single vs multiple vectors (computational cost vs power), prompt engineering vs vector injection (directness vs interpretability).

- **Failure signatures**: Format without correctness (structured but wrong answers), zero-shot degradation (vector makes performance worse), Region 2 failure (single vector no better than zero-shot for complex tasks).

- **First 3 experiments**:
  1. Layer-wise validation: Pick a simple task, extract vectors from all layers, inject at corresponding layers to reproduce peak at layer 15 curve.
  2. Dual-axis effectiveness test: Pick 10 diverse tasks, compare Zero-Shot, Task Vector (single), and Full Few-Shot performance; plot results to identify Region 1 vs Region 2 tasks.
  3. Sub-vector investigation (JSON task): Instead of one global task vector, inject different hidden states from few-shot example's output at different generation steps to verify improvement for complex structured outputs.

## Open Questions the Paper Calls Out

- Do task vector dynamics, such as optimal layer depth and the necessity for multiple sub-vectors, generalize across different model architectures and parameter scales?
- How do distributed and compositional task representations interact during execution of complex tasks?
- To what extent do LLM-based evaluation biases affect the validity of task vector performance scores?

## Limitations

- QUITE AFEW is entirely synthetic, generated by other LLMs rather than collected from real human demonstrations
- Layer-specific findings (peak at layer 15) may not transfer across model architectures
- LLM-based evaluation introduces potential biases and reliability concerns

## Confidence

- **High Confidence (8/10)**: Task vectors work best at intermediate layers; format scores consistently exceed correctness scores
- **Medium Confidence (6/10)**: Complex tasks require multiple subtask-specific vectors; synthetic dataset's representativeness for real-world scenarios is uncertain
- **Low Confidence (4/10)**: Generalization of layer-15 finding to other architectures; absolute performance numbers may vary with different evaluation protocols

## Next Checks

1. **Cross-Architecture Validation**: Extract and test task vectors from Llama-3-8B, Llama-3-70B, GPT-4, and Claude on the same QUITE AFEW subset; plot optimal extraction layer vs model depth to test universality.

2. **Real-World Dataset Validation**: Apply task vector methodology to naturally occurring few-shot prompts from real applications; compare performance gaps between synthetic and real data.

3. **Evaluation Protocol Stress Test**: Re-run experiments on 10-20 diverse tasks using three methods: original LLM judge, human evaluation, and different LLM judge; analyze variance in format/correctness scores across methods.