---
ver: rpa2
title: Per-channel autoregressive linear prediction padding in tiled CNN processing
  of 2D spatial data
arxiv_id: '2502.12300'
source_url: https://arxiv.org/abs/2502.12300
tags:
- padding
- output
- zero
- repl
- crop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a differentiable padding method based on per-channel
  linear prediction for tiled CNN processing of 2D spatial data. The method fits a
  stochastic autoregressive linear model to each channel by minimizing noise terms
  in the least-squares sense, then forms padding from the expected values given known
  pixels.
---

# Per-channel autoregressive linear prediction padding in tiled CNN processing of 2D spatial data

## Quick Facts
- **arXiv ID**: 2502.12300
- **Source URL**: https://arxiv.org/abs/2502.12300
- **Reference count**: 28
- **Primary result**: Linear prediction padding slightly reduced super-resolution MSE compared to zero and replication padding on satellite images, with benefits diminishing when output center-cropping is applied.

## Executive Summary
This paper introduces a differentiable padding method based on per-channel linear prediction for tiled CNN processing of 2D spatial data. The method fits a stochastic autoregressive linear model to each channel by minimizing noise terms in the least-squares sense, then forms padding from the expected values given known pixels. When applied to training the RVSR super-resolution model on satellite images, linear prediction padding slightly reduced mean square super-resolution error compared to zero and replication padding, with moderate computational overhead. Center-cropping the network output reduced error and diminished differences between padding methods, favoring faster zero and replication padding for this workload.

## Method Summary
The method treats padding as a stochastic signal extrapolation problem, fitting a per-channel autoregressive (AR) model by minimizing noise terms in the least-squares sense. Padding pixels are computed recursively as conditional expected values of this model given the known input boundary, rather than filling with zeros or replicated edges. The implementation uses mean-subtracted inputs, covariance matrix computation, a differentiable Cholesky solver with ridge regularization, and recursive generation of padding values. The approach was tested on the RVSR super-resolution model with Sentinel-2 satellite images.

## Key Results
- Linear prediction padding slightly reduced super-resolution MSE compared to zero and replication padding on satellite images
- Zero padding appeared to consume more network capacity to compensate for higher approximation error
- Output center-cropping reduced error and diminished differences between padding methods
- The method better approximated both satellite image data and internal feature map data

## Why This Works (Mechanism)

### Mechanism 1: Per-channel Least-Squares Autoregressive Extrapolation
Treating padding as a stochastic signal extrapolation problem potentially approximates missing data better than heuristic border handling. The method fits a per-channel autoregressive (AR) model by minimizing noise terms in the least-squares sense to solve for linear coefficients $a_i$. Padding pixels are then computed recursively as conditional expected values of this model given the known input boundary, rather than filling with zeros or replicated edges. The core assumption is that the input data and internal feature maps can be reasonably modeled as a zero-mean stationary process where local linear dependencies capture the signal structure.

### Mechanism 2: Network Capacity Redistribution via Border Error Reduction
Reducing padding approximation error may lower the burden on the network to "correct" border artifacts, effectively freeing capacity for the primary task. Standard zero-padding introduces a high-magnitude "edge signal" (approximation error) at borders. The network dedicates weights to handling this specific artifact. Linear prediction padding lowers this initial error, hypothetically allowing the optimizer to focus capacity on super-resolution features rather than boundary compensation.

### Mechanism 3: Receptive Field Erosion via Output Center-Cropping
Output center-cropping acts as a high-pass filter for stitching artifacts, diminishing the impact of padding choices on final output quality. Because the strength of receptive fields decays super-exponentially with distance from the center, cropping the network output by a few pixels removes the "shell" of output pixels whose receptive fields intersect the padded region. This isolates the "valid" convolution results from the border estimates.

## Foundational Learning

- **Normal Equations & Covariance Matrices**: The core of this padding method is not a learned neural layer but a deterministic solver. You must understand how to formulate the least-squares problem (minimizing prediction error) and solve for coefficients using covariance matrices to implement the `lp` layer.
  - *Quick check*: Can you explain why adding a small constant to the diagonal of the covariance matrix (ridge regression) is necessary for numerical stability during the Cholesky decomposition?

- **Shift Equivariance in CNNs**: The paper frames the "tiling problem" as a violation of shift equivariance. Understanding how padding breaks this property—and how valid convolutions preserve it—is essential to diagnosing why stitching artifacts appear in patched inference.
  - *Quick check*: If you shift an input image by 1 pixel and pass it through a CNN with standard zero-padding, why might the output differ from the shifted output of the original image?

- **Z-Transform & System Stability**: The authors stabilize 1D linear predictors by checking if poles lie outside the unit circle. You need this theory to understand why naive recursive padding might "explode" and how the stabilization algorithm (reciprocating poles) prevents numerical blow-up.
  - *Quick check*: In a recursive filter $y[n] = a_1 y[n-1] + x[n]$, what happens to the output over time if $|a_1| > 1$?

## Architecture Onboarding

- **Component map**: Input Pre-processing -> Mean subtraction -> Statistics Layer (covariance) -> Solver (Cholesky + Ridge) -> Recursive Generator (JAX.scan) -> Stabilization (pole check) -> Post-processing (mean re-addition)
- **Critical path**: The calculation of covariance statistics and the Cholesky solve. This replaces the $O(1)$ memory access of zero-padding with an $O(P^2)$ or $O(P^3)$ operation per channel.
- **Design tradeoffs**: Zero/Replication padding is fast but has high border error (Zero) or discontinuity (Repl). Linear Prediction (LP) has lower border error and better shift equivariance but higher latency. Autocorrelation (FFT) vs. Direct Covariance: FFT accelerates training on large batches but limits inference batch size due to memory overhead.
- **Failure signatures**: Catastrophic Padding Blow-up (NaNs/Inf due to unstable AR coefficients), Optimizer Divergence (Adam instability during training).
- **First 3 experiments**:
  1. Train RVSR with zero-padding and plot MSE vs. "shell" index to confirm error spikes at outermost shell.
  2. Implement `lp2x1` without pole-reciprocation stabilization and feed high-frequency noise to verify padding explosion.
  3. Train with `lp` vs. `zero` while sweeping `output_crop` parameter (0, 1, 5) to verify performance gap shrinks as crop increases.

## Open Questions the Paper Calls Out

- Can linear prediction coefficients be learned during network training rather than solved analytically per input? The authors have yet to explore "learning rather than solving lp coefficients."
- Does modeling dependencies between channels improve padding accuracy compared to the current per-channel approach? The authors list "modeling dependencies between channels" as a direction for further work.
- Does using batch-level statistics provide a sufficient sample size to improve padding in spatially small inputs? The authors suggest "using batch rather than image statistics for a larger statistical sample."
- Is linear prediction padding effective for architectures with spatially tiny inputs, such as autoencoder encodings? The conclusion notes that "Covariance statistics may suffer from the small sample problem with spatially tiny inputs such as encodings."

## Limitations
- The method's effectiveness is highly dependent on the stationarity assumption of the input data, which may not hold for non-stationary or rapidly varying textures.
- The computational overhead, while "moderate," could become prohibitive for larger models or higher-resolution inputs.
- Experimental validation is limited to a single satellite super-resolution task, leaving generalizability to other domains uncertain.

## Confidence
- **High Confidence**: The mechanism of per-channel AR model fitting via least-squares and recursive padding generation is well-specified and theoretically sound.
- **Medium Confidence**: The hypothesis that better padding frees network capacity for the primary task is supported by observed MSE differences but lacks direct causal evidence from ablations.
- **Low Confidence**: The generalizability of findings to other CNN architectures, tasks, or data domains beyond satellite super-resolution is unknown.

## Next Checks
1. Apply the LP padding method to a different CNN task (e.g., medical image segmentation or natural image super-resolution) and measure if similar MSE reductions and capacity redistribution effects are observed.
2. Systematically vary the spatial stationarity of the input data (e.g., by introducing high-frequency noise or sharp transitions) and quantify the degradation in LP padding performance relative to zero/replication padding.
3. Train models with identical architectures but different padding methods and perform a fine-tuning study where the last N layers are frozen. If the LP model shows less degradation, it supports the claim that its network capacity is more focused on the primary task.