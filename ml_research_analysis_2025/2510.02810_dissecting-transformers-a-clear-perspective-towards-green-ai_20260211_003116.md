---
ver: rpa2
title: 'Dissecting Transformers: A CLEAR Perspective towards Green AI'
arxiv_id: '2510.02810'
source_url: https://arxiv.org/abs/2510.02810
tags:
- energy
- attention
- layer
- across
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEAR, a novel methodology for fine-grained
  energy measurement of individual components within transformer-based models during
  inference. CLEAR overcomes the temporal mismatch between microsecond-scale component
  execution and millisecond-scale energy sensors by using an amplification strategy
  that repeatedly executes each component to obtain reliable energy estimates.
---

# Dissecting Transformers: A CLEAR Perspective towards Green AI

## Quick Facts
- arXiv ID: 2510.02810
- Source URL: https://arxiv.org/abs/2510.02810
- Authors: Hemang Jain; Shailender Goyal; Divyansh Pandey; Karthik Vaidhyanathan
- Reference count: 40
- Key outcome: CLEAR achieves component-wise energy variance below 9.5% and captures over 90% of total model energy by overcoming microsecond-scale component execution and millisecond-scale sensor sampling mismatch.

## Executive Summary
This paper introduces CLEAR, a methodology for fine-grained energy measurement of transformer components during inference. CLEAR addresses the temporal mismatch between nanosecond-scale component execution and millisecond-scale energy sensors by amplifying component execution duration through repeated runs. The framework achieves reliable component-level energy estimates across 15 models spanning four architecture types, revealing that Attention blocks consume significantly more energy per FLOP than other components. These findings establish detailed energy baselines and provide insights for building energy-efficient transformer models through targeted optimizations.

## Method Summary
CLEAR uses forward hooks to capture activation tensors at component boundaries, stores them in an Activation Store, then isolates each component for repeated execution with identical inputs. Energy is measured before and after the amplified block (typically 10,000 executions for small components), then divided by N to obtain per-component energy. The methodology validates results through cross-validation between component-wise and full-model measurements, achieving component-wise energy variance below 9.5% and capturing over 90% of total model energy.

## Key Results
- Attention blocks consume significantly more energy per FLOP compared to MLP components, contradicting the assumption that FLOPs alone capture energy costs
- CLEAR achieves component-wise energy variance below 9.5% and captures over 90% of total model energy across 15 models
- Energy consumption decomposes into fixed overhead plus FLOP-proportional component, with Attention showing higher marginal energy costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeated component execution amplifies execution duration to exceed sensor sampling periods, enabling reliable energy measurement.
- Mechanism: Transformer components execute in 10-100 µs while NVML sensors sample at 20-50 ms intervals. By executing each component N times consecutively (N=10,000 for small components), total duration extends to hundreds of milliseconds. Energy is measured before/after the amplified block, then divided by N. This reduces measurement noise (ε/N) proportionally while making the signal detectable.
- Core assumption: Component energy consumption remains consistent across repeated executions; noise is random and averages out.
- Evidence anchors:
  - [abstract] "overcome temporal mismatch between microsecond scale component execution and monitoring of millisecond (ms) scale energy sensors"
  - [Page 4, Section 3.2] "By increasing N, the duration of the aggregated workload extends to hundreds of milliseconds which is long enough for NVML's power sensor to capture it while the noise term ε/N diminishes proportionally"
  - [corpus] No directly comparable amplification methodology found in related corpus papers.
- Break condition: If components have stateful behavior or caching effects that change energy per execution across repetitions, measurements become unreliable.

### Mechanism 2
- Claim: Forward hooks at component boundaries capture activation tensors, enabling isolated replay with identical input statistics.
- Mechanism: During a single forward pass, hooks registered on attention inputs, MLP inputs, LM head inputs, and layer norm inputs store activation tensors. The Activation Store caches these, allowing each component to be re-executed independently with the same inputs that occurred during normal inference.
- Core assumption: Stored activations accurately represent real inference conditions; replaying them in isolation does not materially change the computational or energy profile.
- Evidence anchors:
  - [Page 3, Section 3.1] "The Activation Store serves as a cache of activations that allows isolated re-execution of individual components under identical input statistics"
  - [Page 3, Figure 1] Visual pipeline showing activation capture → storage → component isolation → measurement
  - [corpus] No corpus papers use this specific activation-caching approach for energy profiling.
- Break condition: If component behavior depends on cross-component state not captured in the stored activations (e.g., CUDA stream state, memory fragmentation patterns), isolated replay may not reflect true inference energy.

### Mechanism 3
- Claim: Energy consumption decomposes into a fixed overhead (E₀) plus a component-specific marginal cost per FLOP (k), explaining why FLOPs alone fail as energy proxies.
- Mechanism: Empirical analysis shows E/FLOP decreases as input length increases (fixed costs amortized), but marginal ΔE/ΔFLOP remains approximately constant. Different components have different k values—Attention has notably higher k than MLP due to memory access patterns and synchronization overheads.
- Core assumption: The linear model E(L) ≈ E₀ + k·FLOPs(L) holds across the tested input lengths and architectures.
- Evidence anchors:
  - [Page 9, Section RQ4] "energy consumption of different components can be well approximated with two term model comprising of a fixed overhead and a FLOP proportional component"
  - [Page 8, Figure 4] E/FLOP decreases with input length; ΔE/ΔFLOP remains stable across components
  - [corpus] Related work (Sánchez-Mompó et al.) notes FLOPs used as proxy but limitations not quantified at component level.
- Break condition: If non-linear energy behavior emerges at extreme sequence lengths or with different hardware, the two-term model may under/overestimate.

## Foundational Learning

- Concept: **NVML power sampling granularity**
  - Why needed here: Understanding why single-component measurements fail (sensor updates every 20-50ms while components complete in µs).
  - Quick check question: If a component executes in 50µs and NVML samples every 40ms, how many component executions are needed before the sensor can reliably detect the energy draw?

- Concept: **Attention vs MLP computational patterns**
  - Why needed here: Explains why Attention has higher energy/FLOP—softmax, memory-bound KV operations, less regular parallelism than dense matrix multiplications.
  - Quick check question: Why would a softmax operation over a 128-token sequence consume more energy per FLOP than a 1024×4096 matrix multiplication?

- Concept: **FP16 normalization overhead**
  - Why needed here: Normalization layers consume more energy in FP16 than FP32 due to upcasting for numerical stability.
  - Quick check question: If a model uses FP16 throughout, which components would you expect to have *higher* energy in FP16 vs FP32, and why?

## Architecture Onboarding

- Component map:
  Input → [Forward Hooks] → Activation Store (cache tensors) → [Amplification Loop: N× component execution] → [NVML Energy Measurement: E_start, E_end] → [Aggregation: E_component = (E_end - E_start) / N] → [Validation: %Capture, StdDev across T trials]

- Critical path:
  1. Register hooks correctly on all target component inputs (attn_in, mlp_in, lm_head_in, layer_norm_in)
  2. Run single forward pass to populate Activation Store
  3. For each component: amplify (N executions), measure, divide by N, repeat for T trials
  4. Validate: sum of component energies should match full-model energy within ~10%

- Design tradeoffs:
  - Higher N → more stable readings but longer measurement time; N=10,000 for small components, N=1,000 for full model
  - More trials (T) → lower variance but diminishing returns; T=20 used in paper
  - Component granularity → more hooks capture more energy but increase complexity; residual connections omitted as negligible

- Failure signatures:
  - Zero energy readings: N too low, component completed entirely between sensor samples
  - High variance (>9.5%): Component energy below ~5mJ, approaching sensor precision limit (~0.8mJ)
  - Low %Capture (<90%): Missing components, incorrect hook placement, or high idle energy (observed with ALBERT's factorized embeddings)
  - FP16 normalization shows *higher* energy than FP32: Expected behavior due to upcasting overhead, not a failure

- First 3 experiments:
  1. **Baseline validation**: Run CLEAR on BERT-base with 128 tokens, FP16. Verify %Capture >90% and StdDev <9.5% across all major components.
  2. **N sensitivity test**: For a single attention block, measure with N=100, 500, 1000, 5000, 10000. Plot variance vs N to confirm diminishing noise at higher N.
  3. **Component comparison**: Compare E/FLOP for Attention vs MLP across Qwen-2.5-3B at input lengths 32, 64, 128, 256 tokens. Verify Attention shows higher E/FLOP and both show decreasing E/FLOP with increasing length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a generalized predictive model accurately estimate the fixed overhead ($E_0$) and variable costs ($k$) of components based solely on architectural hyperparameters?
- Basis in paper: [explicit] The Discussion section states that future work could use CLEAR to establish a foundation for "predictive modeling... based on architectural design choices like hidden dimensions, number of layers, etc.," allowing for early design-stage energy estimation.
- Why unresolved: The paper currently relies on empirical measurement post-deployment. It establishes the energy decomposition formula ($E \approx E_0 + k \cdot FLOPs$) but does not provide a model to predict the constants $E_0$ and $k$ for new, unmeasured architectures.
- What evidence would resolve it: A regression model or theoretical framework that takes architectural parameters (e.g., hidden size, number of heads) as input and outputs the estimated energy overhead and marginal cost per FLOP with low error rates.

### Open Question 2
- Question: Do the observed high energy-per-FLOP ratios in Attention blocks persist across different GPU families and generations?
- Basis in paper: [explicit] The Limitations section notes that different "GPU families and generations apply their own low-level optimization," and extending the analysis to a wider range of hardware is left for future work to sharpen the understanding of component-wise usage.
- Why unresolved: The study is restricted to NVIDIA Ada-Lovelace GPUs (RTX 5000/6000 Ada). It is unclear if the identified inefficiencies in Attention blocks are intrinsic to the algorithm or an artifact of how these specific GPU architectures handle memory-bound operations.
- What evidence would resolve it: Replicating the CLEAR methodology on non-Ada architectures (e.g., NVIDIA Hopper or AMD MI series) to compare the Energy/FLOP ratios for Attention versus MLP blocks.

### Open Question 3
- Question: How can the CLEAR methodology be adapted to accurately measure the marginal energy cost of Attention during multi-token generation with Key-Value (KV) caching?
- Basis in paper: [explicit] Appendix D states that "the use of key–value (KV) caching in Attention layers complicates accurate measurement of their incremental energy consumption" and lists extending the methodology to multi-token autoregressive generation as a limitation.
- Why unresolved: The current study focuses on single-token generation. In multi-token scenarios, the changing size of the KV cache alters memory access patterns dynamically, which CLEAR's current static amplification strategy does not fully capture.
- What evidence would resolve it: A modified CLEAR protocol that profiles energy consumption incrementally as the KV cache grows, isolating the energy cost of cache updates versus query processing.

## Limitations

- Amplification requirement: Small components need 10,000 executions for stable measurements, creating computational expense and practical tradeoffs between granularity and runtime feasibility.
- Hardware specificity: Methodology validated only on NVIDIA GPUs with NVML, limiting generalizability to other platforms where power monitoring capabilities differ.
- Energy precision limit: Components consuming less than ~5mJ cannot be measured reliably even with amplification due to sensor precision limits (~0.8mJ).

## Confidence

**High confidence**: The amplification mechanism effectively resolves the temporal mismatch between component execution (10-100 µs) and sensor sampling (20-50 ms). The mathematical framework for noise reduction (ε/N) is sound, and the empirical validation showing variance <9.5% across 15 models supports this claim robustly.

**Medium confidence**: The assertion that Attention blocks have significantly higher energy per FLOP than other components is well-supported within the tested model family and hardware configuration, but may not generalize to all attention variants (e.g., sparse attention, multi-query attention) or different GPU architectures where memory bandwidth and compute characteristics vary.

**Low confidence**: The two-term energy model (fixed overhead + FLOP-proportional component) is presented as universally applicable, but the paper only tests this across 15 models spanning four architecture types on a single GPU platform. The model's validity across different hardware (TPUs, CPUs), quantization schemes, or extreme input lengths remains unverified.

## Next Checks

1. **Cross-platform validation**: Implement CLEAR on a TPU or CPU environment and compare component energy distributions with the GPU results. Verify whether Attention's higher energy-per-FLOP persists across hardware with different memory bandwidth characteristics.

2. **Stateful component analysis**: For components suspected of having stateful behavior (e.g., those with persistent CUDA kernels or caching mechanisms), compare energy measurements from isolated replay versus measurements embedded within full-model inference to quantify potential bias.

3. **Extreme length scaling**: Test the two-term energy model at sequence lengths beyond 512 tokens (e.g., 1024, 2048) to identify whether the fixed overhead assumption breaks down or whether non-linear energy behaviors emerge in long-sequence processing.