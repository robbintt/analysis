---
ver: rpa2
title: Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services
arxiv_id: '2510.27016'
source_url: https://arxiv.org/abs/2510.27016
tags:
- privacy
- entities
- response
- named
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents LOPSIDED, a privacy agent framework designed
  to protect Personally Identifiable Information (PII) in conversational AI systems
  by selectively pseudonymizing named entities while preserving semantic integrity.
  The framework replaces sensitive PII entities with semantically consistent pseudonyms,
  ensuring that the generated responses remain faithful to the original query.
---

# Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services

## Quick Facts
- **arXiv ID:** 2510.27016
- **Source URL:** https://arxiv.org/abs/2510.27016
- **Authors:** Jayden Serenari; Stephen Lee
- **Reference count:** 29
- **Key outcome:** LOPSIDED reduces semantic utility errors by 5× compared to baselines while achieving 9.19 LLM-as-a-Judge score

## Executive Summary
This paper presents LOPSIDED, a privacy agent framework that protects Personally Identifiable Information (PII) in conversational AI systems through selective pseudonymization. The framework distinguishes between relevant and irrelevant named entities, pseudonymizing only those that won't affect semantic meaning. Using a combination of semantic-aware pseudonymization and named entity substitution, LOPSIDED fine-tunes a small Gemma 2 2B model on a curated dataset to maintain both privacy and utility. Evaluations show the approach achieves significantly lower utility errors than existing methods while preserving response quality.

## Method Summary
The framework operates in two stages: first, a local pseudonymization model (Gemma 2 2B fine-tuned with LoRA) replaces PII entities with semantically consistent pseudonyms while storing entity mappings locally; second, after receiving the LLM response, a substitution model restores the original entities using the stored mappings. The training data is generated by prompting GPT-4o to produce pseudonymized versions of ShareGPT conversations with entity mappings. The approach uses human-annotated relevance classification to determine which entities can be safely replaced without affecting meaning, with approximately 37% of entities classified as irrelevant in the dataset.

## Key Results
- LOPSIDED reduces semantic utility errors by a factor of 5 compared to baseline methods
- Achieves higher LLM-as-a-Judge score of 9.19 compared to baseline approaches
- Minimizes text syntheticity with lowest detection score (46.59%), ensuring pseudonymized responses closely resemble originals
- Achieves 8% privacy error rate while significantly reducing utility errors compared to HaS method

## Why This Works (Mechanism)

### Mechanism 1: Selective Entity Relevance Classification
The framework distinguishes relevant from irrelevant PII entities, preserving utility while maintaining privacy. Human annotators label entities as "relevant" (substitution would alter meaning) or "irrelevant" (safe to replace), and the model learns this distinction during fine-tuning. Approximately 37% of entities are classified as irrelevant and can be safely replaced.

### Mechanism 2: Semantically-Grounded Pseudonym Selection via Knowledge Distillation
A small local model (Gemma 2 2B) learns to generate semantically appropriate pseudonyms by distilling knowledge from GPT-4o. GPT-4o generates pseudonymized prompts with entity mappings, and the small model is trained to maximize likelihood of producing both the pseudonymized prompt and entity mapping given the original input.

### Mechanism 3: Contextual Entity Restoration
The substitution model restores original entities in the LLM response with contextual awareness, avoiding artifacts from simple string substitution. It takes the pseudonymized response and reverse entity mapping to reconstruct the original response, learning to reintegrate entities in context rather than performing naive string replacement.

## Foundational Learning

- **Named Entity Recognition (NER):** LOPSIDED builds on NER to identify PII entities (names, locations, organizations). Understanding NER limitations (recall errors, type confusion) is critical for diagnosing privacy leaks. *Quick check:* If a user mentions "Dr. Smith at Mercy Hospital," which entities should NER detect, and what types would they receive?

- **Knowledge Distillation:** The framework distills pseudonymization capability from GPT-4o to Gemma 2 2B. Understanding the teacher-student paradigm helps assess when local models may underperform. *Quick check:* What types of pseudonymization patterns might fail to transfer from a large teacher model to a small student model?

- **Privacy-Utility Tradeoff Evaluation:** The paper explicitly measures privacy errors (missed PII) and utility errors (incorrectly replaced relevant entities). These metrics often conflict, requiring design tradeoffs. *Quick check:* If privacy error is 8% and utility error is significantly reduced, what scenarios might still cause unacceptable user harm?

## Architecture Onboarding

- **Component map:** User prompt -> Local Pseudonymization Model (P) -> Entity Mapping Store -> Remote LLM Interface -> Substitution Model (S) -> Final response

- **Critical path:** 1. User prompt intercepted -> 2. P model identifies and pseudonymizes PII -> 3. Entity mapping stored locally -> 4. Sanitized prompt sent to remote LLM -> 5. Response received -> 6. S model restores original entities -> 7. Final response delivered to user

- **Design tradeoffs:**
  - Model size vs. local deployability: Using Gemma 2 2B enables on-device inference but may reduce pseudonymization quality vs. larger models
  - Privacy vs. utility: Lower privacy error (more aggressive replacement) increases utility error (relevant entities incorrectly changed)
  - Single-turn vs. multi-turn: Current implementation handles only first conversation turn; multi-turn requires additional context management

- **Failure signatures:**
  - Undetected PII in output: Privacy error (PII not pseudonymized); check NER coverage for entity type
  - Response semantic drift: Utility error (relevant entity replaced); inspect relevance classification for context
  - Pseudonym artifacts in final output: Substitution model failure; verify reverse mapping completeness
  - High syntheticity scores: Pseudonymized text distinguishable from original; may indicate poor pseudonym selection

- **First 3 experiments:**
  1. Baseline comparison on held-out prompts: Run LOPSIDED, Presidio, and HaS on 50 unseen ShareGPT prompts; measure privacy error, utility error, and LLM-as-a-Judge scores
  2. Entity-type ablation: Evaluate pseudonymization quality separately for PERSON, GPE, ORG entities; identify which entity types have highest privacy or utility error rates
  3. Domain stress test: Test on specialized prompts (medical, legal, financial) not represented in ShareGPT; assess whether semantic pseudonym selection degrades in out-of-distribution contexts

## Open Questions the Paper Calls Out

- How can semantic-aware privacy agents be efficiently adapted for multi-turn conversations without significantly increasing training resources or losing semantic integrity over longer context windows?
- Can more intricate replacement strategies be developed to handle complex semantic dependencies where simple category-based substitution fails?
- To what extent do semantically consistent pseudonyms resist adversarial re-identification attacks compared to random substitution or redaction methods?

## Limitations

- The framework's performance in multi-turn conversations remains untested, as it only handles first-turn pseudonymization and depseudonymization
- Domain-specific effectiveness is unproven, particularly for specialized fields like healthcare, legal, or financial services where entity semantics may be more complex
- The generalizability of the relevance classification (37% irrelevant entities) to prompts outside the ShareGPT dataset is uncertain

## Confidence

- **High Confidence:** The semantic utility error reduction (5× improvement over baselines) and the LLM-as-a-Judge score (9.19) are well-supported by the reported experimental methodology and metrics
- **Medium Confidence:** The privacy error rate of 8% is credible given the evaluation methodology, but the trade-off with utility error requires careful contextual interpretation
- **Low Confidence:** The framework's robustness to context-dependent entity relevance and its performance in specialized domains remain speculative without additional validation

## Next Checks

1. **Multi-turn conversation test:** Implement context tracking across conversation turns and evaluate pseudonymization/depseudonymization accuracy on 50 multi-turn prompts from diverse domains
2. **Domain transfer validation:** Test the framework on 100 prompts from specialized domains (medical, legal, financial) not represented in the ShareGPT training data, measuring privacy and utility errors separately for each domain
3. **Edge case stress test:** Create a curated test set of 50 prompts containing ambiguous entities, nested PII, and context-dependent relevance to evaluate model robustness and identify failure modes