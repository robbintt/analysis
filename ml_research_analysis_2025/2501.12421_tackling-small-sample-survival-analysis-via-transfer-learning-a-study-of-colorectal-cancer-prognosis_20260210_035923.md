---
ver: rpa2
title: 'Tackling Small Sample Survival Analysis via Transfer Learning: A Study of
  Colorectal Cancer Prognosis'
arxiv_id: '2501.12421'
source_url: https://arxiv.org/abs/2501.12421
tags:
- transfer
- survival
- data
- learning
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of small sample survival analysis
  in medical informatics, particularly in cancer prognosis, by leveraging transfer
  learning techniques. The researchers propose and develop transfer learning methods
  for both parametric models (DeepSurv, Cox-CC, DeepHit) and non-parametric models
  (Random Survival Forests).
---

# Tackling Small Sample Survival Analysis via Transfer Learning: A Study of Colorectal Cancer Prognosis

## Quick Facts
- arXiv ID: 2501.12421
- Source URL: https://arxiv.org/abs/2501.12421
- Authors: Yonghao Zhao; Changtao Li; Chi Shu; Qingbin Wu; Hong Li; Chuan Xu; Tianrui Li; Ziqiang Wang; Zhipeng Luo; Yazhou He
- Reference count: 29
- Key result: Transfer learning significantly improves survival analysis models for colorectal cancer prognosis with limited target data, with Ctd improvements ranging from 0.0243 to 0.0357 across different models.

## Executive Summary
This study addresses the critical challenge of small sample survival analysis in medical informatics, particularly for cancer prognosis. The researchers propose transfer learning techniques to leverage a large source dataset (SEER) to improve predictions on a small target dataset (West China Hospital). They develop methods for both parametric models (DeepSurv, Cox-CC, DeepHit) and non-parametric models (Random Survival Forests), introducing a novel Transfer Survival Forest (TSF) approach. The methods are evaluated on colorectal cancer prognosis, demonstrating significant improvements in predictive performance across all models, with the greatest gains observed for models trained on as few as 50 samples.

## Method Summary
The researchers developed transfer learning approaches for both parametric and non-parametric survival models. For neural network-based models (DeepSurv, Cox-CC, DeepHit), they applied standard transfer learning techniques including pretraining on the large SEER dataset and fine-tuning or retraining on the small West China Hospital dataset. For non-parametric Random Survival Forests, they proposed a novel Transfer Survival Forest (TSF) model that transfers tree structures from source tasks while fine-tuning them with target data. The transfer strategies were calibrated based on target sample size, with fine-tuning recommended for smaller datasets (≤200 samples) and full retraining for larger ones (>500 samples). The TSF model transfers tree structure topology while recalculating splitting values on target data, preserving feature co-occurrence patterns from the source forests.

## Key Results
- Cox-CC's Ctd value increased from 0.7868 to 0.8111, a 0.0243 improvement
- DeepHit's Ctd improved from 0.8085 to 0.8135, a 0.0050 increase
- DeepSurv's Ctd rose from 0.7722 to 0.8043, a 0.0321 gain
- Random Survival Forests showed the largest improvement, with Ctd increasing from 0.7940 to 0.8297, a 0.0357 boost
- All models demonstrated even more significant improvements when trained with as few as 50 samples, validating the effectiveness of transfer learning for small sample sizes

## Why This Works (Mechanism)

### Mechanism 1
Pretraining on a large, related source dataset provides a stronger initialization for survival models than random initialization, particularly when target data is limited. The source model learns general feature-risk relationships (e.g., how tumor size relates to hazard) that are partially applicable to the target population. Fine-tuning or retraining then adapts these learned representations to target-specific distributions. Core assumption: Source and target tasks share underlying risk patterns despite population differences (e.g., both are Stage I CRC patients). Evidence anchors: Ctd improvements across all models, raw transferred model baseline superiority, though corpus evidence is limited.

### Mechanism 2
Transferring tree structure topology (which features split at which levels) while recalculating splitting values on target data improves Random Survival Forests. Discriminative features tend to appear more frequently and at higher tree levels in source forests. TSF samples tree structures proportionally to their source frequency, preserving feature co-occurrence patterns, then re-optimizes splitting thresholds for the target distribution. Core assumption: Feature importance rankings generalize across related populations even if specific threshold values differ. Evidence anchors: TSF methodology description and performance gains, though no direct corpus evidence exists for tree structure transfer in survival forests.

### Mechanism 3
Optimal transfer granularity (fine-tuning vs. retraining) depends on target data availability. With limited data, full retraining risks overfitting to noise; fine-tuning only the output layer preserves more source knowledge. With abundant target data, retraining can find better optima. Core assumption: The relationship between transfer granularity and data availability follows a monotonic pattern. Evidence anchors: Crossover pattern in results table showing FT beats RT at 50-200 samples, RT beats FT at 500+ samples for DeepHit and DeepSurv, though no corpus papers examine this granularity question.

## Foundational Learning

- **Concept**: Cox Proportional Hazards and partial likelihood
  - Why needed here: All neural network models (DeepSurv, Cox-CC) extend the Cox framework by replacing linear predictor with neural networks; understanding the loss function is essential.
  - Quick check question: Can you explain why the partial likelihood handles censored observations without imputation?

- **Concept**: Survival trees and log-rank splitting
  - Why needed here: TSF transfers tree structures built using log-rank statistics; understanding how split points are chosen clarifies what is being transferred.
  - Quick check question: What does the log-rank splitting criterion optimize at each node?

- **Concept**: Time-dependent concordance index (C^td)
  - Why needed here: All results are reported in C^td; understanding what it measures (ranking agreement between predicted risk and actual survival times) is necessary to interpret improvements.
  - Quick check question: How does C^td differ from standard AUC in handling censored data?

## Architecture Onboarding

- **Component map**: Source pretraining pipeline -> Transfer module -> Target adaptation -> Evaluation
- **Critical path**: 
  1. Verify feature alignment between source and target (8 features: gender, age, T stage, tumor size, grade, CEA, perineural invasion, lymph node examination)
  2. Pretrain source model with cross-validation on SEER
  3. Select transfer strategy based on target sample size (FT for <200, RT for >500)
  4. For TSF: set k (tree depth to transfer) conservatively (k=1-2) for small samples
- **Design tradeoffs**:
  - TSF-T_k depth: Higher k transfers more structure but requires more target data; k=4 works at 655 samples, fails at 50
  - FT vs RT: FT is safer for small data but may underfit; RT can overfit small data but achieves better optima with sufficient samples
  - DP vs TSF: DP explores new structures but ignores feature co-occurrence; TSF preserves structure relationships but is more conservative
- **Failure signatures**:
  - C^td drops below Target baseline → negative transfer; reduce k or switch to FT-only
  - High variance across folds (±0.15-0.20) → insufficient data for chosen transfer depth
  - Source model already outperforms transferred model → target data too noisy; skip fine-tuning
- **First 3 experiments**:
  1. Baseline sanity check: Train each model (DeepSurv, Cox-CC, DeepHit, RSF) on target-only data across sample sizes (50, 100, 200, 500, 655); confirm performance degrades with smaller samples.
  2. Transfer strategy ablation: For each neural network model, compare Source-only, FT, and RT at each sample size; identify the crossover point where FT→RT switch is optimal.
  3. TSF depth calibration: Test TSF-T_1 through TSF-T_4 at each sample size; map the minimum sample size required for each depth to avoid negative transfer.

## Open Questions the Paper Calls Out

### Open Question 1
How can transfer learning methods for survival analysis be adapted to effectively handle source and target datasets with non-overlapping or heterogeneous feature sets? The authors note that while their experiments used aligned feature sets, "many related clinical tasks' feature sets differ, and how to handle non-overlapped features is a critical challenge in transfer learning." The proposed TSF and neural network fine-tuning approaches rely on identical input feature dimensions between source and target domains.

### Open Question 2
To what extent does the quality or predictive power of the source model influence the magnitude of improvement gained through fine-tuning on small target samples? The paper notes that the SEER database provided "strong source models" and suggests it "would be interesting to see how much a mild source model could contribute." It's unclear if the positive transfer observed is dependent on the high quality of the SEER data or if the method is robust to weaker source signals.

### Open Question 3
Can the Transfer Survival Forest framework be extended to integrate multi-modal data, such as genomics or proteomics, for enhanced prognosis? The authors identify it as a future direction to "leverage multi-modal data" and "combine pre-trained cell foundation models (such as GeneFormer)... with survival observations." The current study is restricted to clinico-pathological features, and the TSF algorithm's ability to handle high-dimensional omics data is untested.

## Limitations
- The source and target populations differ in key ways (hospital-based vs. registry data, different geographic regions), which may affect transfer effectiveness beyond pure sample size effects.
- The TSF method lacks theoretical grounding compared to neural network approaches, with the empirical observation about feature co-occurrence patterns needing statistical validation.
- All results depend on the assumption that the 8 shared features capture sufficient information for prognosis, excluding potentially important features like stage IV patients or molecular markers.

## Confidence

- **High confidence**: The general finding that transfer learning improves small-sample survival analysis performance (Ctd increases from 0.7940 to 0.8297 for RSF). This is consistently demonstrated across models and sample sizes.
- **Medium confidence**: The specific optimal transfer strategies (FT for <200 samples, RT for >500) and TSF depth recommendations. These are empirically derived and may vary with different source-target pairs.
- **Low confidence**: The mechanism explanations, particularly the claim about feature co-occurrence patterns in TSF. The authors provide limited evidence for why tree structure transfer should work beyond observed performance.

## Next Checks

1. **Population heterogeneity test**: Systematically vary the similarity between source and target populations (e.g., different cancer stages, geographic regions) to identify when transfer learning provides positive vs. negative results.

2. **Feature importance stability analysis**: Track how individual feature coefficients/predictions change during transfer across different sample sizes to validate the claimed mechanism of learning general risk patterns.

3. **Cross-cancer transfer validation**: Apply the same transfer learning framework to different cancer types (e.g., breast cancer source to colorectal target) to assess whether the approach generalizes beyond this specific pairing.