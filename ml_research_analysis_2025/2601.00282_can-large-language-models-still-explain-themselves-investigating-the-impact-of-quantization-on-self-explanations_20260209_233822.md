---
ver: rpa2
title: Can Large Language Models Still Explain Themselves? Investigating the Impact
  of Quantization on Self-Explanations
arxiv_id: '2601.00282'
source_url: https://arxiv.org/abs/2601.00282
tags:
- quantization
- language
- faithfulness
- quality
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how quantization affects the quality and\
  \ faithfulness of self-explanations (SEs) generated by large language models (LLMs).\
  \ Two types of SEs\u2014natural language explanations (NLEs) and counterfactual\
  \ examples\u2014are evaluated under three quantization techniques and varying bit\
  \ widths across six LLMs."
---

# Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations

## Quick Facts
- arXiv ID: 2601.00282
- Source URL: https://arxiv.org/abs/2601.00282
- Reference count: 40
- Primary result: Quantization causes moderate degradation in self-explanation quality (up to 4.4%) and faithfulness (up to 2.38%)

## Executive Summary
This study systematically investigates how post-training quantization affects the quality and faithfulness of self-explanations generated by large language models. Using three quantization techniques (GPTQ, AWQ, bitsandbytes) across six LLMs, the research evaluates two types of self-explanations—natural language explanations and counterfactual examples—on multiple datasets. Results demonstrate that quantization leads to measurable declines in explanation quality and faithfulness, with NLEs being more sensitive than CFEs. Larger models show better preservation of faithfulness under quantization, while smaller models experience more significant quality degradation. The study also reveals that LLM-as-a-Judge evaluation fails to reliably capture these quantization impacts, necessitating human evaluation for trustworthy assessment.

## Method Summary
The study applies post-training quantization (4-bit and 8-bit) to six LLMs (Llama3 8B/70B, Qwen2.5 7B/14B/32B/72B) using three methods (GPTQ, AWQ, bitsandbytes). Self-explanations are generated via ZeroCoT prompting for natural language explanations and FIZLE for counterfactual examples. Quality is assessed through BARTScore and TIGERScore metrics, while faithfulness is evaluated using counterfactual tests, biasing features, and CC-SHAP. Human evaluation involves 48 participants rating 30 samples each on trustworthiness and coherence. Results are compared against full-precision baselines to quantify degradation, with all experiments averaged over three seeds.

## Key Results
- Quantization degrades self-explanation quality by up to 4.4% and faithfulness by up to 2.38%
- NLEs are more sensitive to quantization than CFEs across all models and methods
- Larger models maintain better faithfulness under quantization, though smaller models experience more quality degradation
- No single quantization method consistently excels across task performance, SE quality, and faithfulness
- LLM-as-a-Judge evaluation fails to capture quantization's impact, showing weak or negative correlation with human ratings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization degrades self-explanation quality through distribution shifts affecting contextual semantics and confidence calibration.
- Mechanism: Precision reduction (32-bit → 4/8-bit) introduces truncation/rounding errors. This narrows output distribution diversity (reduced entropy), impairs contextual semantic capture, and increases hallucination likelihood—directly impacting perceived coherence and trustworthiness.
- Core assumption: The degradation pathway operates through distribution shifts rather than simple capacity loss.
- Evidence anchors:
  - [abstract]: "quantization typically leads to moderate declines in both SE quality (up to 4.4%) and faithfulness (up to 2.38%)"
  - [section 5.2]: "By introducing truncation or rounding, it is harder for the model to capture contextual semantics due to distribution shifts... ultimately diminishing annotators' trust in the self-explanations"
- Break condition: If task performance degradation strongly correlated with SE quality degradation—but Table 7 shows weak/negative correlations, suggesting independent pathways.

### Mechanism 2
- Claim: Larger models preserve faithfulness better under quantization due to reasoning pathway redundancy.
- Mechanism: Larger models retain core reasoning pathways while potentially discarding spurious correlations through quantization-induced noise. The redundancy in learned representations provides implicit protection against precision loss for faithfulness specifically.
- Core assumption: Faithfulness preservation mechanisms differ from quality preservation mechanisms.
- Evidence anchors:
  - [abstract]: "larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness"
  - [section 5.1.1]: "NLEs generated by larger models tend to be more faithful... larger models show greater robustness to quantization in preserving NLE faithfulness"
- Break condition: If smaller full-precision models consistently outperformed larger quantized models on faithfulness—results show opposite pattern.

### Mechanism 3
- Claim: LLM-as-a-Judge evaluation fails to capture quantization's human-perceived quality degradation.
- Mechanism: Judge models evaluate surface-level coherence adequately but miss nuanced trustworthiness degradation that humans perceive. Judge-human correlation is weak/negative and statistically non-significant, particularly pronounced for CFEs where judges systematically disagree with humans.
- Core assumption: Human SE quality perception involves factors beyond what current LLM judges measure.
- Evidence anchors:
  - [abstract]: "LLM-as-a-Judge evaluation fails to fully capture this impact, showing weak or negative correlation with human ratings"
  - [section 5.2]: "correlation between human and judge models is generally weak or negative and not statistically significant"
- Break condition: If judge model selection matters significantly—Figure 6-9 show judge agreement varies, but all fail human correlation.

## Foundational Learning

- Concept: **Self-Explanation Faithfulness vs. Plausibility**
  - Why needed here: The paper evaluates both quality (plausibility—similarity to human explanations via BARTScore/TIGERScore) and faithfulness (whether SEs reflect actual model reasoning). These are distinct dimensions that respond differently to quantization.
  - Quick check question: If a model generates an explanation matching human reasoning but the model actually used a spurious correlation, is this high quality or high faithfulness?

- Concept: **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed here: This study focuses exclusively on weight-only PTQ (GPTQ, AWQ, bitsandbytes). Results may not generalize to QAT, weight-activation quantization, or KV cache compression.
  - Quick check question: Why might QAT show different SE preservation patterns than the PTQ results reported here?

- Concept: **Counterfactual Faithfulness as Self-Consistency**
  - Why needed here: For CFEs, faithfulness equals label flip rate—whether the counterfactual actually changes the prediction to the target label. This operationalizes faithfulness as prediction-level self-consistency.
  - Quick check question: Why might a counterfactual with high textual similarity to the input but low label flip rate be considered unfaithful?

## Architecture Onboarding

- Component map:
  Models (Llama3/Qwen2.5) -> Quantization (GPTQ/AWQ/bitsandbytes) -> SE Generation (ZeroCoT/FIZLE) -> Quality Metrics (BARTScore/TIGERScore) -> Faithfulness Metrics (counterfactual test/biasing features/CC-SHAP)

- Critical path:
  1. Load model → apply quantization method → generate SEs on target dataset
  2. Run automatic quality evaluation (BARTScore, TIGERScore)
  3. Run faithfulness tests (counterfactual test, biasing features, CC-SHAP)
  4. Optional: Human evaluation for trustworthiness/coherence validation
  5. Compare to full-precision baseline to compute degradation percentages

- Design tradeoffs:
  - AWQ best preserves SE quality; GPTQ8 best preserves task performance; no single method wins both (Figure 11)
  - NLEs more quantization-sensitive than CFEs—prefer CFEs when quantization is required
  - Larger quantized models > smaller full-precision models for faithfulness-critical use cases

- Failure signatures:
  - Task performance degradation not explaining SE quality degradation (Table 7: weak/negative Spearman correlations)
  - Judge-human correlation approaching zero or negative—signals need for human evaluation
  - 4-bit precision showing worse faithfulness preservation than 8-bit, especially for smaller models

- First 3 experiments:
  1. Replicate single-dataset evaluation (eSNLI) with Qwen2.5-7B across all quantization methods to validate pipeline.
  2. Add KV cache quantization to isolate weight-only vs. activation quantization effects on SE faithfulness.
  3. Test a new SE type (feature attribution explanations) to determine if quantization sensitivity patterns generalize beyond free-text SEs.

## Open Questions the Paper Calls Out

- **Multilingual extension**: The study is confined to English datasets, leaving multilingual SE quality and faithfulness under quantization unexplored.
- **Alternative quantization methods**: Effects of weight-activation quantization, KV cache compression, or quantization-aware training on self-explanations remain untested.
- **Judge evaluation failure**: The fundamental reasons why LLM-as-a-Judge evaluation fails to capture quantization's impact require deeper investigation and improved evaluation frameworks.
- **Extreme quantization**: The degradation patterns at 1-2 bit precision remain unknown, as only 4-bit and 8-bit quantization were tested.

## Limitations

- The study only examines post-training weight-only quantization, potentially limiting generalizability to other compression strategies.
- Results are based on two model families (Llama3 and Qwen2.5), which may not represent broader architectural patterns.
- Human evaluation representativeness is uncertain due to unspecified participant demographics and sampling strategy.

## Confidence

**High Confidence**: Core finding that quantization degrades SE quality (up to 4.4%) and faithfulness (up to 2.38%) is well-supported by comprehensive evaluation.

**Medium Confidence**: Claim that larger models maintain better faithfulness under quantization is supported but mechanism remains speculative.

**Low Confidence**: Assertion that LLM-as-a-Judge evaluation fails to capture quantization's impact relies heavily on correlation observation without deeper analysis of underlying causes.

## Next Checks

1. Evaluate the same SE generation and evaluation pipeline using quantization-aware training (QAT) and weight-activation quantization methods to determine if observed degradation patterns are specific to post-training quantization.

2. Conduct ablation studies with different judge model sizes and prompting strategies to identify whether judge-human correlation gap stems from model capacity, evaluation methodology, or fundamental limitations in judge-based assessment of SE quality.

3. Replicate the full experimental pipeline with additional model families (e.g., Mistral, Gemma) and quantization configurations (mixed precision, dynamic quantization) to test robustness of observed trends across broader architectural landscape.