---
ver: rpa2
title: Efficient Multilingual Name Type Classification Using Convolutional Networks
arxiv_id: '2601.11090'
source_url: https://arxiv.org/abs/2601.11090
tags:
- names
- entity
- accuracy
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Onomas-CNN X, a convolutional neural network
  designed for efficient multilingual name type classification. The model uses parallel
  convolution branches with depthwise-separable operations and hierarchical classification
  to achieve high throughput on CPU hardware.
---

# Efficient Multilingual Name Type Classification Using Convolutional Networks

## Quick Facts
- arXiv ID: 2601.11090
- Source URL: https://arxiv.org/abs/2601.11090
- Authors: Davor Lauc
- Reference count: 26
- Primary result: 92.1% accuracy at 2,813 names/second on CPU

## Executive Summary
This paper introduces Onomas-CNN X, a convolutional neural network architecture for multilingual name type classification. The model achieves state-of-the-art efficiency on CPU hardware, processing 2,813 names per second while maintaining 92.1% accuracy across 104 languages and four entity types. The key innovation lies in parallel convolution branches with depthwise-separable operations and hierarchical classification, enabling 46x faster inference than fine-tuned XLM-RoBERTa with comparable accuracy. The model demonstrates significant energy efficiency gains, reducing power consumption by a factor of 46 compared to transformer baselines.

## Method Summary
Onomas-CNN X uses parallel convolution branches with depthwise-separable operations and hierarchical classification. The architecture processes character-level inputs through multiple convolutional layers with different kernel sizes, followed by max-pooling and concatenation. A final softmax layer produces entity type predictions. The model is trained on a dataset covering 104 languages with four entity types (person, location, organization, other), achieving high throughput on CPU hardware through architectural optimizations that minimize computational complexity while maintaining classification accuracy.

## Key Results
- Achieves 92.1% accuracy on 104-language multilingual name classification
- Processes 2,813 names per second on single CPU core (46x faster than XLM-RoBERTa)
- Reduces energy consumption by factor of 46 compared to transformer baselines

## Why This Works (Mechanism)
The model's efficiency stems from depthwise-separable convolutions that reduce parameter count while maintaining representational power. Parallel convolution branches with varying kernel sizes capture patterns at multiple scales simultaneously. The hierarchical classification structure enables faster inference by reducing computational complexity at each decision point. Character-level processing eliminates the need for word segmentation, making the model language-agnostic and robust to morphological variations.

## Foundational Learning
- Depthwise-separable convolutions: Why needed? Reduce parameters while maintaining accuracy; Quick check: Compare parameter count vs standard convolutions
- Hierarchical classification: Why needed? Enable faster inference through staged decisions; Quick check: Measure inference time with/without hierarchy
- Multilingual tokenization: Why needed? Handle 104 languages without language-specific preprocessing; Quick check: Test on low-resource language subsets
- Parallel convolution branches: Why needed? Capture multi-scale patterns simultaneously; Quick check: Compare single vs multi-branch performance
- Character-level processing: Why needed? Avoid word segmentation complexity across languages; Quick check: Test with word-level vs character-level inputs
- CPU optimization: Why needed? Enable deployment on resource-constrained hardware; Quick check: Benchmark on different CPU architectures

## Architecture Onboarding
**Component Map:** Input -> Character Embedding -> Parallel Conv Branches -> Max-Pooling -> Concatenation -> Dense Layers -> Output
**Critical Path:** Character embedding through parallel convolutions to final classification layer
**Design Tradeoffs:** Speed vs accuracy (reduced parameters sacrifice some representational capacity), CPU vs GPU optimization (CPU-focused design limits GPU acceleration potential), depth vs width (parallel branches instead of deep stacking)
**Failure Signatures:** Confusion between person and organization names, degraded performance on low-resource African/indigenous languages, sensitivity to character encoding variations
**First Experiments:** 1) Benchmark inference speed on different CPU architectures, 2) Test accuracy degradation with reduced vocabulary size, 3) Evaluate cross-lingual transfer to truly low-resource languages

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can incorporating surrounding textual context into the architecture resolve ambiguity errors for eponymous entities without sacrificing CPU efficiency?
- Basis in paper: [explicit] The conclusion states "Context-aware models incorporating surrounding words might resolve ambiguous cases better," and Section IV.F identifies 3,241 systematic confusions between English person and organization names.
- Why unresolved: The current model relies exclusively on character-level inputs and isolated name strings, lacking any mechanism to utilize document context.
- What evidence would resolve it: A comparative study measuring accuracy changes on ambiguous test sets when window-based context is added, alongside throughput benchmarks.

### Open Question 2
- Question: Does cross-lingual parameter sharing significantly improve the 10-15% accuracy gap observed for African and indigenous low-resource languages?
- Basis in paper: [explicit] The conclusion proposes "Cross-lingual parameter sharing could improve low-resource language performance," while Section IV.G notes that performance on these languages remains lower due to data availability.
- Why unresolved: The current architecture does not leverage deeper cross-lingual semantic relationships that transformer models learn during pre-training.
- What evidence would resolve it: Experiments applying parameter sharing techniques to the current model, demonstrating statistically significant accuracy gains in the Niger-Congo and Austronesian language clusters.

### Open Question 3
- Question: Can hybrid architectures combining CNN efficiency with selective attention mechanisms maintain high throughput while improving accuracy?
- Basis in paper: [explicit] The conclusion states "Hybrid architectures combining CNN efficiency with selective attention mechanisms warrant investigation" to maintain throughput while incorporating contextual information.
- Why unresolved: The current binary comparison is between pure CNNs and heavy Transformers; a middle-ground architecture has not been tested.
- What evidence would resolve it: Designing and benchmarking a hybrid model that achieves comparable accuracy to XLM-RoBERTa while retaining >80% of Onomas-CNN X's inference speed.

### Open Question 4
- Question: Does expanding the subword vocabulary size to 2 million tokens improve classification accuracy without degrading inference latency?
- Basis in paper: [explicit] Future work directions include "Investigating larger subword vocabularies (up to 2 million tokens)" to potentially improve accuracy without sacrificing speed.
- Why unresolved: The current model uses a 250,002 token vocabulary; the trade-off curve for massive vocabulary sizes in this specific architecture is unknown.
- What evidence would resolve it: Benchmarking training and inference speed/accuracy curves across vocabulary sizes ranging from 250k to 2M.

## Limitations
- Evaluation limited to four entity types, unclear performance on diverse classification schemas
- Single transformer baseline comparison; other architectures might yield different trade-offs
- Energy efficiency claims based on computational metrics rather than direct power measurements

## Confidence
- Computational efficiency superiority (High): The throughput and speed measurements are concrete and reproducible
- Accuracy parity with transformers (Medium): Based on single comparison; results may vary with different transformer models
- Energy efficiency gains (Medium): Inferred from computational metrics rather than direct measurements

## Next Checks
1. Test Onomas-CNN X against multiple transformer baselines (DeBERTa, mT5, mBERT) to establish relative performance across architectures
2. Evaluate model robustness using adversarial name variations and cross-lingual transfer tests for low-resource languages
3. Conduct real-world power consumption measurements comparing CPU and GPU deployments to validate energy efficiency claims