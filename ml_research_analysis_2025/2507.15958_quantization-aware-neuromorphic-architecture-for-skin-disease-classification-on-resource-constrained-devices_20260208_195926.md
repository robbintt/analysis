---
ver: rpa2
title: Quantization-Aware Neuromorphic Architecture for Skin Disease Classification
  on Resource-Constrained Devices
arxiv_id: '2507.15958'
source_url: https://arxiv.org/abs/2507.15958
tags:
- akida
- neuromorphic
- inference
- qana
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents QANA, a quantization-aware neuromorphic architecture\
  \ for on-device skin lesion classification that addresses the dual challenge of\
  \ efficient CNN-to-SNN conversion and classification performance on imbalanced medical\
  \ datasets. QANA integrates conversion-stable components\u2014bounded activations,\
  \ quantization-aware normalization, and lightweight attention\u2014into a compact\
  \ CNN backbone, then maps it to a spiking neural network for deployment on BrainChip\
  \ Akida hardware."
---

# Quantization-Aware Neuromorphic Architecture for Skin Disease Classification on Resource-Constrained Devices

## Quick Facts
- arXiv ID: 2507.15958
- Source URL: https://arxiv.org/abs/2507.15958
- Reference count: 40
- One-line primary result: 91.6% Top-1 accuracy and 91.0% macro F1 on HAM10000; 1.5 ms/image, 1.7 mJ/image on Akida

## Executive Summary
This work presents QANA, a quantization-aware neuromorphic architecture for on-device skin lesion classification that addresses the dual challenge of efficient CNN-to-SNN conversion and classification performance on imbalanced medical datasets. QANA integrates conversion-stable components—bounded activations, quantization-aware normalization, and lightweight attention—into a compact CNN backbone, then maps it to a spiking neural network for deployment on BrainChip Akida hardware. On the HAM10000 dataset, QANA achieves 91.6% Top-1 accuracy and 91.0% macro F1, improving the best converted SNN baseline by 3.5 points in Top-1 accuracy and 12.0 points in macro F1. When deployed on Akida, QANA processes images in 1.5 ms with 1.7 mJ per image, corresponding to 94.6% lower latency and 99.0% lower energy than its CNN counterpart on RTX 3090.

## Method Summary
QANA combines a lightweight CNN backbone with quantization-aware components and embedding-space SMOTE for class imbalance. The backbone uses 4 stages of Ghost blocks with BatchNorm, ReLU6 activations, and SA-ECA attention, followed by a spike-compatible transform and SE block. Training uses bounded activations and normalization to ensure 8-bit quantization compatibility. Class imbalance is addressed via SMOTE in the penultimate embedding space (256-D PCA-whitened) while freezing the backbone. The model is converted to an SNN via integer graph lowering and deployed on BrainChip Akida with integrate-and-fire neurons. Incremental learning on edge hardware is supported through validation-based threshold calibration and readout fine-tuning without full retraining.

## Key Results
- HAM10000: 91.6% Top-1 accuracy and 91.0% macro F1, improving best SNN baseline by 3.5 and 12.0 points respectively
- Clinical dataset: 90.8% Top-1 accuracy and 81.7% macro F1, improving best baseline by 3.2 and 3.6 points
- Akida deployment: 1.5 ms/image, 1.7 mJ/image (94.6% lower latency, 99.0% lower energy vs. RTX 3090 CNN)

## Why This Works (Mechanism)

### Mechanism 1: Bounded Activations for Conversion Stability
Constraining activation ranges during CNN training reduces quantization error and spike-encoding distortion during CNN-to-SNN conversion. ReLU6 bounds post-activation values to [0, 6], which maps cleanly to 8-bit affine quantization with scale 6/255 and zero-point 0. The spike-compatible transform further clips to [0, 1] (scale 1/255). This prevents saturation and reduces calibration sensitivity when integer activations drive integrate-and-fire neurons.

### Mechanism 2: Embedding-Space SMOTE for Class Imbalance
Generating synthetic minority-class samples in the learned embedding space (not pixel space) improves rare-class recall without introducing pixel-level artifacts. After extracting penultimate representations (r ∈ R^4096), PCA whitening stabilizes distances. Safe minority anchors are filtered by local median neighbor distance. Synthetic embeddings are created via convex interpolation plus locally aligned perturbation. Only the classifier head is trained on mixed real/synthetic embeddings while the backbone remains frozen.

### Mechanism 3: Lightweight Attention Without Global Operators
Replacing global pooling with depthwise spatial aggregation in attention blocks preserves SNN compatibility while maintaining channel recalibration. SA-ECA uses depthwise convolution + 1×1 projection instead of global pooling + 1D convolution. SE blocks apply global average pooling only after the spike-compatible transform (where the tensor is already bounded), using a bottleneck ratio of 16. This avoids conversion-fragile operators that require floating-point reductions or multi-bit temporal accumulation.

## Foundational Learning

- **Integer-Only Inference and Affine Quantization**: QANA's conversion pipeline requires all operators to lower to integer kernels; understanding scale/zero-point alignment is essential for residual addition and spike encoding. Quick check: Given two tensors with scales s₁=0.02 and s₂=0.04, what target scale should be used for residual addition, and why?

- **Integrate-and-Fire Neuron Dynamics**: SNN inference uses membrane potential accumulation with subtractive reset; spike counts over time determine class scores. Quick check: If threshold τ=10 and accumulated drive is 23 after two timesteps, how many spikes have been emitted and what is the residual membrane potential?

- **Class Imbalance Metrics (Macro F1 vs. Weighted F1)**: HAM10000 has severe imbalance; macro F1 (unweighted per-class mean) is the primary figure of merit for rare-lesion performance. Quick check: Why does macro F1 increase more than accuracy in QANA's improvements over baselines?

## Architecture Onboarding

- **Component map**: Input preprocessing: Grad-CAM-guided crop → 64×64 resize → channel normalization → Backbones (4 stages): Ghost block → BN → ReLU6 → SA-ECA → Residual → MaxPool → Spike-compatible transform: SeparableConv 3×3/256 → BN → Clip[0,1] → SE block → Output head: Flatten → Linear (4096 → 7) with quantization-aware projection → Conversion pipeline: BN folding → 8-bit affine quantization → Integer graph lowering → Akida deployment

- **Critical path**: Ensure all BatchNorm layers are foldable before conversion; verify activation ranges match quantization scales (ReLU6: [0,6]→[0,255]; Clip: [0,1]→[0,255]); calibrate spike thresholds and integration windows on validation split; test integer graph completeness (rsucc = 1 − |V_fp32|/|V|)

- **Design tradeoffs**: 64×64 resolution required for Akida on-chip memory; reduces fine-grained texture detail but preserves lesion boundary cues via saliency-guided cropping; Ghost blocks reduce FLOPs by generating features via cheap depthwise ops; may underrepresent complex patterns relative to full convolutions; Frozen backbone for SMOTE prevents feature drift but limits adaptation to synthetic distribution

- **Failure signatures**: Conversion rejection: Floating-point subgraph detected → check for unsupported ops (e.g., non-foldable BN, dynamic shapes); Minority-class collapse: Macro F1 substantially lower than accuracy → check SMOTE anchor filtering, embedding-space PCA whitening; Spike threshold misalignment: Large accuracy gap between CNN and SNN → recalibrate thresholds, increase integration window

- **First 3 experiments**: 1) Validate conversion fidelity: Train QANA CNN, convert to SNN, report ΔAcc and ΔF1 on held-out test split; verify rsucc = 1.0; 2) Ablate attention mechanisms: Disable SA-ECA and/or SE blocks individually; measure impact on macro F1 (expect ~1–2 point drops per component per table 4); 3) Stress-test imbalance handling: Train with and without embedding-space SMOTE on a reduced minority-class subset; compare per-class recall on rare categories (e.g., dermatofibroma)

## Open Questions the Paper Calls Out

- **How does QANA generalize across diverse skin tones and demographic groups?**: The Related Work states that "general CNN models often fail to generalize to underrepresented conditions, such as rare tumors or images from diverse skin tones," but the experimental evaluation does not include demographic or skin tone stratification. Both HAM10000 and the clinical dataset lack reported demographic breakdowns; no Fitzpatrick skin type analysis is presented.

- **What is the diagnostic impact of the 64×64 input resolution constraint on fine-grained lesion discrimination?**: The methodology states inputs are processed at 64×64 "to match the Akida on-chip memory budget," with no ablation on resolution-accuracy trade-offs. Dermatoscopic diagnosis often relies on subtle texture and border features that may be lost at low resolution; the 2×2 tiling fallback (triggered 18% of the time) suggests resolution limits affect confidence.

- **Can incremental learning on Akida preserve performance on previously learned lesion types when new classes are added sequentially?**: The Related Work notes that "edge Spiking Neural Networks learning methods often suffer from catastrophic forgetting," but the paper only evaluates single-domain readout fine-tuning. Incremental learning experiments are limited to validation-based threshold calibration and readout updates without multi-stage class addition or forgetting measurement.

## Limitations
- Exact architectural parameters (channel widths per Ghost block stage, Ghost ratio μ) are not specified, requiring informed guesses that could impact accuracy
- Lack of detailed training hyperparameters (optimizer, learning rate schedule, epochs, batch size, weight decay, dropout rate) may affect reproducibility
- Clinical dataset's class distribution and acquisition conditions are not fully described, limiting external validation

## Confidence

- **High confidence**: Conversion stability mechanisms (bounded activations, BN folding, integer quantization) are well-grounded in standard SNN deployment practice and the reported 99.0% energy reduction on Akida is hardware-specific and verifiable
- **Medium confidence**: The embedding-space SMOTE approach is plausible given the severe class imbalance in HAM10000, but the absence of a pixel-space augmentation baseline or ablation makes the relative benefit uncertain
- **Low confidence**: Exact performance figures (e.g., 91.6% Top-1, 91.0% macro F1) are tightly coupled to unspecified architectural choices and hyperparameters, so direct replication may yield variations

## Next Checks
1. **Conversion fidelity test**: Train the QANA CNN, convert to SNN, and measure ΔTop-1 and Δmacro-F1 on a held-out test set; verify the integer graph success metric rsucc = 1.0
2. **Ablation of attention mechanisms**: Disable SA-ECA and/or SE blocks individually and measure their impact on macro F1; expect ~1–2 point drops per component based on Table 4
3. **Stress-test SMOTE**: Train with and without embedding-space SMOTE on a reduced minority-class subset and compare per-class recall on rare categories (e.g., dermatofibroma); validate the safe-anchor filtering and PCA whitening steps