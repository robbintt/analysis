---
ver: rpa2
title: A Multimodal Conversational Assistant for the Characterization of Agricultural
  Plots from Geospatial Open Data
arxiv_id: '2509.17544'
source_url: https://arxiv.org/abs/2509.17544
tags:
- data
- multimodal
- query
- context
- agricultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal conversational assistant that
  enables non-expert users to interact with heterogeneous agricultural and geospatial
  open data through natural language queries. The system integrates orthophotos, Sentinel-2
  vegetation indices, and domain-specific documents via retrieval-augmented generation,
  allowing it to flexibly determine whether to rely on multimodal evidence, textual
  knowledge, or both in formulating answers.
---

# A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data

## Quick Facts
- **arXiv ID**: 2509.17544
- **Source URL**: https://arxiv.org/abs/2509.17544
- **Reference count**: 25
- **Primary result**: Multimodal conversational assistant for agricultural geospatial data achieves 4.8-4.9/5 evaluation scores via LLM-as-a-judge

## Executive Summary
This paper introduces a multimodal conversational assistant that enables non-expert users to interact with heterogeneous agricultural and geospatial open data through natural language queries. The system integrates orthophotos, Sentinel-2 vegetation indices, and domain-specific documents via retrieval-augmented generation, allowing it to flexibly determine whether to rely on multimodal evidence, textual knowledge, or both in formulating answers. Evaluation using an LLM-as-a-judge methodology (Qwen3-32B) in a zero-shot setting across four dimensions (correctness, relevance, clarity, completeness) yielded average scores of 4.8–4.9 out of 5, with multimodal retrieval-based queries outperforming RAG-based ones. The approach successfully lowers the technical barrier to accessing and interpreting complex geospatial and agricultural data, demonstrating potential for supporting decision-making in land management.

## Method Summary
The system implements a retrieval-augmented generation framework that routes user queries through either multimodal retrieval (orthophotos and Sentinel-2 indices) or RAG-based (textual documents) pipelines, determined by an LLM-driven tool-triage model. Users interact via natural language to query geospatial and agricultural open data, with the assistant synthesizing responses from retrieved multimodal or textual evidence. Evaluation employs a zero-shot LLM-as-a-judge approach using Qwen3-32B to score responses on correctness, relevance, clarity, and completeness across 45 simulated queries.

## Key Results
- LLM-as-a-judge evaluation scores averaged 4.8–4.9 out of 5 for response quality across four dimensions
- Multimodal retrieval-based queries outperformed RAG-based queries in the evaluation
- System effectively bridges the gap between non-expert users and complex geospatial/agricultural data

## Why This Works (Mechanism)
The system leverages the complementary strengths of multimodal and textual retrieval, routing queries dynamically based on their nature to optimize information synthesis. By combining orthophotos and Sentinel-2 vegetation indices with domain-specific textual documents, the assistant can contextualize and interpret geospatial data in agriculturally meaningful ways. The tool-triage model enables intelligent selection between retrieval modes, ensuring that queries receive the most relevant evidence. This flexible architecture allows the system to adapt to the diverse nature of user queries and the heterogeneous data sources involved.

## Foundational Learning
- **Geospatial Open Data**: Public datasets like orthophotos and satellite imagery are essential for land characterization and agricultural monitoring. *Why needed*: Provides the foundational evidence for plot analysis and vegetation assessment.
- **Retrieval-Augmented Generation (RAG)**: Combines information retrieval with generative models to ground responses in external knowledge. *Why needed*: Enables the assistant to synthesize accurate, context-rich answers from heterogeneous data sources.
- **LLM-as-a-Judge Evaluation**: Uses large language models to automatically assess response quality across multiple dimensions. *Why needed*: Provides scalable, consistent evaluation in the absence of human benchmarks or extensive ground truth.
- **Tool-Triage Models**: LLM-driven decision-making to select the appropriate retrieval or generation pipeline. *Why needed*: Ensures queries are processed using the most suitable evidence type for accurate answers.
- **Sentinel-2 Vegetation Indices**: Satellite-derived metrics for monitoring crop health and vegetation status. *Why needed*: Offers critical temporal and spatial information for agricultural assessment.
- **SIGPAC (Agricultural Land Registry)**: Standardized cadastral data for agricultural plot identification and management. *Why needed*: Provides authoritative plot boundaries and land use classification for the region.

## Architecture Onboarding
- **Component map**: User Query -> Tool-Triage Model -> (Multimodal Retrieval OR RAG Pipeline) -> LLM Synthesis -> Response
- **Critical path**: User query is routed by tool-triage model to either multimodal retrieval (orthophotos + Sentinel-2) or RAG pipeline (textual documents), then synthesized by LLM for final answer
- **Design tradeoffs**: Chose flexible query routing over single pipeline to optimize for heterogeneous data; prioritized multimodal evidence when available for richer context
- **Failure signatures**: Incorrect triage leads to missing critical evidence (e.g., satellite data); RAG-only responses may lack visual context; multimodal retrieval may fail if data quality is poor or coverage is incomplete
- **3 first experiments**:
  1. Simulate user queries for plot boundaries and crop identification, comparing multimodal vs. RAG-only responses
  2. Test system robustness by inputting ambiguous or incomplete geospatial queries
  3. Evaluate triage model accuracy by artificially inverting routing decisions and measuring response degradation

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: How does LLM-as-a-judge evaluation correlate with human expert assessment for agricultural and geospatial query responses?
- **Basis in paper**: The authors acknowledge "Although the evaluation remains preliminary, it provides evidence that conversational interfaces can effectively bridge the gap" and note no significant benchmarks exist, relying entirely on automated Qwen3-32B scoring.
- **Why unresolved**: No human validation was conducted; only 45 experiments were evaluated in an unsupervised, zero-shot setup.
- **What evidence would resolve it**: A parallel evaluation study where domain experts (agronomists, GIS specialists) score the same responses, with correlation analysis against LLM judge scores.

### Open Question 2
- **Question**: Does system performance generalize across diverse geographic regions with varying agricultural landscapes and SIGPAC-equivalent data availability?
- **Basis in paper**: The study was limited to Gijón, Spain (182 km²). The authors claim reproducibility but provide no empirical validation in other regions.
- **Why unresolved**: Regional differences in crop types, terrain, data formats, and cadastral registry structures may affect retrieval quality and answer accuracy.
- **What evidence would resolve it**: Cross-regional evaluation in at least 2-3 geographically and agriculturally distinct areas with comparative performance metrics.

### Open Question 3
- **Question**: What is the error rate and impact of the tool-triage model's routing decisions on final response quality?
- **Basis in paper**: The tool-triage model determines whether queries use multimodal retrieval, RAG, both, or neither, but its accuracy is not evaluated or discussed.
- **Why unresolved**: Incorrect routing could lead to missing context (e.g., failing to retrieve satellite data when relevant) without user awareness.
- **What evidence would resolve it**: Ablation study measuring response quality degradation when triage decisions are intentionally inverted, plus triage accuracy benchmarks.

## Limitations
- Evaluation relies solely on LLM-as-a-judge (Qwen3-32B) without human or cross-judge validation
- Limited to a single geographic region (Gijón, Spain), with no cross-regional validation
- No assessment of tool-triage model accuracy or error impact on final responses

## Confidence
- **Evaluation methodology robustness**: Medium - single LLM judge, no human validation
- **Generalizability claims**: Low - only tested in one region
- **Technical integration novelty**: High - clear multimodal RAG implementation
- **Usability and decision-support claims**: Medium - based on simulated queries, not user studies

## Next Checks
1. Conduct a user study with actual non-expert agricultural stakeholders to assess usability and practical decision-support value.
2. Perform cross-validation using multiple LLM judges and human annotators to verify consistency and reliability of evaluation scores.
3. Test the system on edge cases, including ambiguous or incomplete geospatial queries, to evaluate robustness and error handling.