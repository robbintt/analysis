---
ver: rpa2
title: Are Large Language Models Dynamic Treatment Planners? An In Silico Study from
  a Prior Knowledge Injection Angle
arxiv_id: '2508.04755'
source_url: https://arxiv.org/abs/2508.04755
tags:
- insulin
- glucose
- prior
- rate
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper rigorously compares zero-shot inference performance of
  open-source LLMs with small neural-network-based RL agents (SRAs) for dynamic insulin
  dosing in Type 1 diabetes using the SimGlucose simulator. Expert knowledge was injected
  into SRAs via conservative exploration and into LLMs via structured system prompts
  and chain-of-thought reasoning.
---

# Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle

## Quick Facts
- arXiv ID: 2508.04755
- Source URL: https://arxiv.org/abs/2508.04755
- Reference count: 32
- Key outcome: Zero-shot LLM performance on T1D insulin dosing matches or exceeds trained RL agents, with Qwen2.5-7B achieving 62.8% normalized return

## Executive Summary
This study rigorously compares zero-shot LLM inference performance with trained RL agents for dynamic insulin dosing in Type 1 diabetes. Using the SimGlucose simulator, the authors inject expert knowledge into both model types - through conservative exploration for RL agents and structured system prompts for LLMs. While RL agents showed contradictory performance with prior knowledge (DQN suffering, PPO improving), the Qwen2.5-7B LLM achieved strong performance across adult cohorts. The study reveals that chain-of-thought prompting improves larger models but leads smaller models to overly aggressive dosing and increased hypoglycemia risk.

## Method Summary
The study evaluates zero-shot LLM performance against trained neural-network RL agents (DQN and PPO) on the SimGlucose T1D simulator. LLMs receive glucose-insulin histories formatted as text prompts with clinical constraints, while RL agents are trained with composite reward functions encoding safety. The evaluation spans adult, adolescent, and child cohorts, measuring survival rate, time-in-range, and normalized return with bootstrapped confidence intervals.

## Key Results
- Qwen2.5-7B achieved 62.8% normalized return, outperforming smaller RL agents especially in adult cohorts
- Chain-of-thought prompting improved larger models but led smaller models to overly aggressive dosing and increased hypoglycemia risk
- Stochastic decoding (temperature 0.7) improved dosing performance over deterministic greedy decoding
- Prior knowledge injection through prompts enabled strong zero-shot performance without environment-specific training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prior knowledge injection through natural language prompts enables LLMs to perform insulin dosing decisions at zero-shot without environment-specific training.
- **Mechanism:** Pre-trained medical knowledge is activated via structured clinical instructions embedded in system prompts, allowing the model to map glucose trajectories to insulin recommendations through implicit learned associations rather than explicit reward optimization.
- **Core assumption:** The pre-training corpus contains sufficient glucose-insulin dynamics knowledge that can be retrieved and applied via instruction-following capabilities.
- **Evidence anchors:**
  - [abstract] "carefully designed zero-shot prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or superior clinical performance relative to extensively trained SRAs"
  - [section 4.2.1] Prompt 2 specifies clinical goals and action constraints in natural language
  - [corpus] Related work on knowledgeable LLMs as black-box optimizers (FMR=0.51) supports prompt-based knowledge activation

### Mechanism 2
- **Claim:** Chain-of-thought (CoT) prompting induces dose-aggressive behavior in smaller models while larger models exhibit better self-correction.
- **Mechanism:** CoT forces explicit step-by-step reasoning which, in smaller capacity models, leads to arithmetic hallucination and reactive overcorrection to transient glucose fluctuations. Larger models maintain internal consistency between reasoning traces and final outputs.
- **Core assumption:** Model capacity determines whether intermediate reasoning steps remain coupled to clinically appropriate final decisions.
- **Evidence anchors:**
  - [section 5.3] "explicit CoT prompts frequently result in overly aggressive insulin dosing strategies... increasing hypoglycemic episodes"
  - [figure 5] Smaller Qwen2.5 models show dosage escalation with CoT while survival rates drop

### Mechanism 3
- **Claim:** Stochastic decoding (temperature > 0) improves dosing performance over deterministic greedy decoding.
- **Mechanism:** Non-zero temperature introduces output diversity that prevents repetitive, overly conservative policies. In clinical dosing where the optimal action distribution may be multi-modal, controlled stochasticity enables better exploration of the action space during inference.
- **Core assumption:** The task benefits from action diversity rather than single deterministic outputs.
- **Evidence anchors:**
  - [section 5.3] "At temperature 0.7, the models consistently produce more diverse responses, yielding superior results compared to the more rigid and repetitive output generated at temperature 0.0"
  - [figure 3] Performance curves show consistent improvement at T=0.7 across both model families

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The SimGlucose environment provides glucose observations but hidden states (meals, exercise) must be inferred. Understanding POMDPs explains why explicit latent-state reasoning in prompts fails—textual inference cannot substitute for learned state estimation.
  - Quick check question: Can you explain why meal inference from glucose trends alone is fundamentally underdetermined?

- **Concept: Reward Shaping in RL for Clinical Safety**
  - Why needed here: The paper constructs a composite reward (termination penalty + risk index) to encode clinical constraints. Understanding this explains why SRAs require extensive engineering and how LLMs bypass this via prompt-encoded priors.
  - Quick check question: How does the LBGI/HBGI formulation asymmetrically penalize hypo- vs. hyperglycemia?

- **Concept: In-Context Learning vs. Weight Updates**
  - Why needed here: LLMs achieve performance through inference-time prompt conditioning, not gradient-based policy optimization. This distinction explains why LLMs can deploy rapidly but why failure modes (hallucination) cannot be trained away without fine-tuning.
  - Quick check question: What information can and cannot be acquired through in-context learning that would otherwise require weight updates?

## Architecture Onboarding

- **Component map:** SimGlucose Environment -> Observation Encoder -> Policy Backbone -> Prompt Templates -> Evaluation Suite
- **Critical path:**
  1. Format glucose/insulin history as Prompt 1 text
  2. Construct system prompt with clinical constraints (target range, dose limits, safety rules)
  3. Query LLM with temperature=0.7
  4. Parse numeric dose from response (handle `<ans>` tags or free-form extraction)
  5. Execute action in SimGlucose, observe next state, repeat for 64 steps

- **Design tradeoffs:**
  - **Qwen2.5 vs. LLaMA3:** Qwen2.5 consistently outperforms at equivalent sizes (Section 5.3), suggesting pre-training quality matters more than parameter count
  - **CoT vs. Direct Prompting:** CoT improves interpretability but risks aggressive dosing in models <10B parameters
  - **Stochastic vs. Deterministic Decoding:** T=0.7 improves performance but reduces reproducibility for clinical audit

- **Failure signatures:**
  - **Arithmetic hallucination:** Intermediate calculations produce implausible values (e.g., TDI estimates 6× true value) that are disconnected from final dose
  - **Reactive overcorrection:** Model increases insulin in response to transient glucose rise, ignoring insulin-on-board
  - **Meal inference failure:** Explicit meal reasoning prompts harm smaller models without improving larger ones
  - **Hypoglycemia cascade:** Aggressive dosing leads to BG <70 mg/dL, then panic-zero-dosing, then rebound hyperglycemia

- **First 3 experiments:**
  1. **Baseline replication:** Run Qwen2.5-7B with Prompt 3 (prior knowledge only), T=0.7, on adult cohort for 20 episodes. Verify normalized return ≈62% per Figure 2.
  2. **Failure mode trigger:** Apply meal-CoT prompt (Prompt 5) to Qwen2.5-7B on child cohort. Confirm survival rate degradation and document reasoning trace errors.
  3. **Temperature ablation:** Compare T∈{0.0, 0.3, 0.7, 1.0} on Qwen2.5-32B across all cohorts. Identify optimal temperature for TIR vs. survival tradeoff.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can hybrid approaches integrating structured physiological models with LLMs improve latent-state inference beyond current prompt engineering capabilities?
  - Basis in paper: [explicit] The authors state that future work "will need to look beyond prompt engineering" and suggest exploring "hybrid approaches that combine linguistic reasoning with structured physiological modelling."
  - Why unresolved: The study found that explicitly prompting LLMs to reason about hidden variables (e.g., meal effects) yielded minimal performance gains, indicating a limit to text-only inference.
  - What evidence would resolve it: Demonstrated superior performance in inferring unobserved physiological dynamics using a hybrid architecture compared to the text-only baselines established in this paper.

- **Open Question 2:** What prompt designs can effectively mitigate the aggressive dosing and arithmetic hallucinations caused by Chain-of-Thought (CoT) reasoning?
  - Basis in paper: [explicit] The discussion highlights the need for "prompt designs that support multi-step planning, uncertainty awareness, and dynamic policy modulation" to address current failure modes.
  - Why unresolved: CoT prompting currently leads to overly aggressive insulin dosing and "arithmetic hallucinations," particularly in smaller models, reducing clinical safety.
  - What evidence would resolve it: Development of a prompting strategy that retains reasoning transparency while statistically reducing hypoglycemic events to rates lower than those of trained RL agents.

- **Open Question 3:** Does targeted fine-tuning yield significant clinical performance improvements over the zero-shot capabilities observed in smaller LLMs (e.g., Qwen2.5-7B)?
  - Basis in paper: [explicit] The conclusion argues that cautious integration requires "meticulous validation... and potentially targeted fine-tuning."
  - Why unresolved: This study focused exclusively on zero-shot inference to assess pre-trained knowledge; the impact of gradient-based updates on this specific task remains untested.
  - What evidence would resolve it: Comparative evaluations showing that fine-tuned models significantly outperform zero-shot baselines in volatile cohorts (e.g., children) where LLMs currently underperform.

## Limitations
- The evaluation relies on a single in silico simulator (SimGlucose) without external validation on real-world datasets or alternative simulators
- Smaller LLMs (<10B parameters) show failure modes in CoT prompting that the paper does not explore why larger models self-correct
- The safety implications of stochastic decoding (T=0.7) in clinical deployment are not addressed, particularly for auditability and reproducibility

## Confidence
- **High confidence:** Qwen2.5-7B's zero-shot performance (62.8% normalized return) relative to trained SRAs, and the general superiority of larger models over smaller ones in maintaining clinical safety
- **Medium confidence:** The claim that CoT prompts induce aggressive dosing in smaller models, as this is based on single-simulation patterns without systematic ablation studies
- **Low confidence:** The assertion that meal inference failure represents a fundamental LLM limitation, given the lack of exploration into alternative prompting strategies or fine-tuning

## Next Checks
1. **External Simulator Validation:** Replicate the experiment on a second FDA-approved T1D simulator (e.g., UVA/Padova) to test generalizability of LLM performance patterns
2. **CoT Failure Mode Analysis:** Systematically test CoT prompts across model sizes (0.5B, 1B, 7B, 32B) with structured error logging to quantify arithmetic hallucination rates and correlate with capacity
3. **Deterministic Safety Audit:** Compare deterministic (T=0) vs. stochastic (T=0.7) decoding across all cohorts, measuring not just performance but also dose consistency and audit trail quality for clinical deployment