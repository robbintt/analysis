---
ver: rpa2
title: 'The Agent''s First Day: Benchmarking Learning, Exploration, and Scheduling
  in the Workplace Scenarios'
arxiv_id: '2601.08173'
source_url: https://arxiv.org/abs/2601.08173
tags:
- agent
- task
- tasks
- arxiv
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Trainee-Bench is a new benchmark that tests AI agents in realistic
  workplace scenarios with streaming tasks, hidden clues, and time constraints. It
  evaluates three core abilities: dynamic scheduling, active exploration to find hidden
  information, and continuous learning from past experience.'
---

# The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios

## Quick Facts
- **arXiv ID:** 2601.08173
- **Source URL:** https://arxiv.org/abs/2601.08173
- **Reference count:** 27
- **Key outcome:** Even top AI models like Gemini-3-Flash struggle in this benchmark, with success rates below 35%, especially on tasks requiring exploration or learning.

## Executive Summary
Trainee-Bench is a new benchmark that tests AI agents in realistic workplace scenarios with streaming tasks, hidden clues, and time constraints. It evaluates three core abilities: dynamic scheduling, active exploration to find hidden information, and continuous learning from past experience. Experiments show that even top AI models like Gemini-3-Flash struggle in this setting, with success rates below 35%. The gap is especially large for complex tasks requiring exploration or learning. Human guidance greatly improves performance, but agents show little improvement through self-learning. The work highlights the need for better exploration and learning mechanisms to make AI agents reliable in real-world environments.

## Method Summary
Trainee-Bench evaluates AI agents as "corporate interns" in simulated workplace environments with streaming tasks, hidden clues, and time constraints. It uses 181 meta-tasks across 4 domains, with 50 dynamic scenarios generated via random seeds creating NPC profiles and environment data. The benchmark measures Success Rate (SR) and Checkpoint Score (CS), tracking normalized checkpoint completion ratios. Agents interact via tool calls in an environment loop, with continual learning evaluated across Day 1 and Day 2 using the MUSE framework to summarize experience from feedback.

## Key Results
- Current AI models achieve success rates below 35% on Trainee-Bench, with the gap widening for complex tasks requiring exploration or learning
- Agents show minimal self-improvement across days, with performance sometimes degrading due to negative transfer from stochastic task variations
- Human guidance significantly improves agent performance, highlighting the limitations of autonomous exploration and learning mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic task instantiation via meta-rules forces agents to rely on generalized reasoning rather than dataset memorization.
- **Mechanism:** The system decouples logical task templates ($r_i \in R$) from stochastic environment variables (NPC profiles $P_{rnd}$, data $D_{rnd}$). By injecting randomized seeds into a meta-task rule, the environment generates unique task instances ($D_{task}$) and latent clues ($K$) for every evaluation run.
- **Core assumption:** Agents have not pre-trained on the specific parametric distributions or random seeds used to generate these infinite variations, ensuring they must solve the logical structure, not the surface pattern.
- **Evidence anchors:**
  - [Section 2.2]: "decoupling of logical rules $r_i$ from stochastic data... ensures that every evaluation instance is structurally unique."
  - [Table 1]: Contrasts Trainee-Bench with static benchmarks like ToolBench to highlight the "Dynamic Configuration" feature.
  - [Corpus]: KBE-DME (arXiv:2510.21182) supports the shift toward dynamic benchmark evolution to combat contamination, aligning with this mechanism.
- **Break condition:** If the library of meta-task rules ($R$) is too small, agents may overfit to the limited set of logical templates (rule-overfitting) even if they cannot memorize specific instances.

### Mechanism 2
- **Claim:** Enforcing partial observability by withholding essential clues ($K$) tests an agent's ability to estimate state uncertainty and actively explore.
- **Mechanism:** The initial prompt provides the objective ($D_{task}$) but strictly withholds latent clues ($K$) required for completion. The agent must recognize the gap between its current state and the goal state, then execute multi-turn interactions (e.g., querying NPCs, navigating file systems) to acquire $K$.
- **Core assumption:** The agent possesses a sufficient "curiosity" or uncertainty-estimation drive to halt execution and engage in exploration before hallucinating an action.
- **Evidence anchors:**
  - [Section 2.2]: "deliberate enforcement of partial observability... latent clues... strictly withheld from the initial prompt."
  - [Section 3.4]: Shows performance plummets without human guidance, proving agents cannot autonomously bridge this information gap effectively.
  - [Corpus]: IndustryNav (arXiv:2511.17384) highlights challenges in spatial reasoning for embodied agents in dynamic settings, paralleling the need for active state estimation here.
- **Break condition:** If the search space for clues is unbounded or lacks environmental affordances (hints), agents may enter infinite exploration loops without finding the requisite signal.

### Mechanism 3
- **Claim:** Checkpoint-based feedback loops provide the necessary error signal for self-correction, though current architectures struggle to utilize it effectively across stochastic variations.
- **Mechanism:** Automated verification checks specific milestones ($C$). If a checkpoint fails, the environment generates natural language feedback (e.g., "You forgot to ask HR"). Theoretically, the agent distills this into experience $e_i$ to guide future behavior (Day 2).
- **Core assumption:** The agent can causally map a failure on a specific checkpoint ($c_i$) in one instance to a different stochastic instance of the same meta-task on the next day.
- **Evidence anchors:**
  - [Section 3.3]: "environment explicitly notifies the agent of the omission... agent engages in a reflective process."
  - [Section 3.3 Results]: Notes that experience from Day 1 often becomes irrelevant or misleading on Day 2 due to "stochastic nature," causing performance drops on easy tasks.
  - [Corpus]: RealMem (arXiv:2601.06966) emphasizes the need for long-term project-oriented interaction, relevant to how agents store and retrieve this checkpoint feedback.
- **Break condition:** If the task instantiation logic varies too aggressively between runs, the "lesson" learned from a failure may be syntactically or contextually inapplicable to the next run, causing negative transfer.

## Foundational Learning

- **Concept: Context-aware Scheduling**
  - **Why needed here:** Agents must handle streaming tasks with conflicting deadlines (preemptive interrupts). Without this, an agent might stick to a low-priority task and miss a critical time-windowed event (e.g., a meeting).
  - **Quick check question:** Can your agent suspend a running "file review" task immediately when a "meeting start" event triggers, and resume it later?

- **Concept: State Estimation under Uncertainty**
  - **Why needed here:** The environment is partially observable. Agents must maintain a belief state regarding where information *might* be located (e.g., "Who is the custodian of this clue?").
  - **Quick check question:** Does the agent ask clarifying questions or search the environment when inputs are missing, or does it hallucinate a path using only the initial prompt?

- **Concept: Experience Distillation**
  - **Why needed here:** The paper shows that simply storing history is insufficient. Agents need to extract generalized *strategies* (e.g., "always check the manual before executing") rather than specific instance details.
  - **Quick check question:** When the agent fails a task, does it log "File X was missing" (instance-specific) or "I should verify file existence before reporting completion" (generalized)?

## Architecture Onboarding

- **Component map:** Task Instantiation (Rule + Seed) -> Agent receives Objective (w/o Clues) -> **Exploration Phase** (Acquire Clues) -> Execution Phase (Tool Calls) -> Verification (Checkpoint Feedback)
- **Critical path:** Task Instantiation (Rule + Seed) → Agent receives Objective (w/o Clues) → **Exploration Phase** (Acquire Clues) → Execution Phase (Tool Calls) → Verification (Checkpoint Feedback)
- **Design tradeoffs:**
  - **Granularity vs. Cost:** The benchmark uses granular checkpoints for feedback, but this requires a complex verification engine. Coarser checks would be cheaper but offer less "learning signal" for the agent.
  - **Stochastic vs. Fair:** High randomization (robustness) makes reproducibility harder. You must fix the random seed to debug specific agent failures.
- **Failure signatures:**
  - **The "Disoriented Intern":** High step count, low tool utility (randomly clicking files). Seen in weaker models like Llama-4-maverick.
  - **The "Overconfident Hallucinator":** Low step count, immediate wrong action. Fails to explore because it assumes full observability.
  - **Negative Transfer:** Performance degrades on Day 2 (after "learning") because the agent overfits experience to a stochastic variable (e.g., expecting a specific NPC name that changed).
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run GPT-4o or Gemini on a *static* subset (fixed seed) vs. dynamic set to isolate the "robustness gap" score.
  2. **Ablation on Observability:** Provide a "Hint" condition (give clues in prompt) vs. "No Hint." This quantifies the cost of exploration failure (as done in Sec 3.4).
  3. **Learning Decay Test:** Run the Day 1 → Day 2 loop. Specifically measure if the "Experience Summary" generated by the agent contains actionable logic or mere descriptions of past events.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can automated methods for rule generation effectively replace the current reliance on human-crafted meta-tasks to improve benchmark scalability?
- **Basis in paper:** [explicit] The authors state in Section 5 that the "reliance on manually crafted rules for meta-tasks limits the scalability of our benchmark," explicitly motivating future work on "developing automated methods for rule generation."
- **Why unresolved:** The current benchmark construction depends entirely on human-crafted logical templates, which is labor-intensive and finite.
- **What evidence would resolve it:** The successful implementation and validation of an algorithm capable of generating valid, diverse meta-task rules without human intervention.

### Open Question 2
- **Question:** How does the introduction of rigid causal chains in task composition affect agent performance compared to the current stochastic assembly?
- **Basis in paper:** [explicit] Section 5 notes that the "diversity of task composition is currently constrained and lacks complex causal inter-dependencies," identifying the incorporation of "rigid causal chains" as a necessary future direction.
- **Why unresolved:** The current benchmark architecture aggregates tasks stochastically, failing to simulate complex, real-world causal dependencies between events.
- **What evidence would resolve it:** Comparative performance results from agents evaluated on an updated benchmark version featuring causally linked task streams.

### Open Question 3
- **Question:** Do the observed limitations in dynamic scheduling and exploration generalize to complex physical domains such as production or industrial settings?
- **Basis in paper:** [explicit] The authors acknowledge in Section 5 that the evaluation was restricted to a "specific workplace simulation context" and propose extending the simulation to "diverse domains, such as complex production and industrial settings."
- **Why unresolved:** The current experimental scope is limited to administrative/office workflows and does not cover physical or industrial logic.
- **What evidence would resolve it:** Evaluation of leading agent frameworks within a simulated industrial environment to test the robustness of the findings.

## Limitations
- The benchmark focuses on decision-making and information-seeking, but does not test physical navigation or embodied reasoning capabilities
- The stochastic nature of task instantiation creates reproducibility challenges and may require fixing random seeds for debugging
- The MUSE framework for continual learning is described conceptually but lacks implementation specifics in the paper

## Confidence
- **High Confidence:** The core claim that current AI agents struggle with exploration, scheduling, and learning in this benchmark is well-supported by experimental results showing success rates below 35% for top models
- **Medium Confidence:** The mechanism explanations for why agents fail (partial observability, dynamic instantiation, negative transfer) are logically sound based on the framework description, though the exact failure modes depend on unpublished implementation details
- **Medium Confidence:** The claim about human guidance significantly improving performance is supported by Section 3.4 results, but the exact nature and extent of human assistance is not fully detailed

## Next Checks
1. **Stochastic Robustness Test:** Run the same agent across multiple random seeds for identical meta-tasks to quantify performance variance and validate the "robustness gap" between static and dynamic evaluation modes
2. **Observability Ablation Study:** Implement and test both "Hint" (clues provided) and "No Hint" conditions across all task difficulties to precisely measure the exploration cost and identify which agent types fail most severely under partial observability
3. **Learning Transfer Analysis:** Systematically compare checkpoint scores (CS) for easy versus hard tasks across Day 1 and Day 2 to identify and quantify negative transfer patterns, validating whether experience summarization actually captures generalizable strategies or just instance-specific details