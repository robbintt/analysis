---
ver: rpa2
title: 'Hidden Persuasion: Detecting Manipulative Narratives on Social Media During
  the 2022 Russian Invasion of Ukraine'
arxiv_id: '2505.24028'
source_url: https://arxiv.org/abs/2505.24028
tags:
- classification
- manipulation
- techniques
- span
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a competitive solution to the UNLP 2025 Shared
  Task on detecting manipulative narratives in Ukrainian Telegram content during the
  2022 Russian invasion. The task involved classifying 10 manipulation techniques
  and identifying corresponding text spans in a multilingual, multi-label dataset.
---

# Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine

## Quick Facts
- arXiv ID: 2505.24028
- Source URL: https://arxiv.org/abs/2505.24028
- Reference count: 7
- 2nd place in classification (macro-F1 0.454), 3rd place in span detection (F1 0.599)

## Executive Summary
This paper presents a competitive solution to the UNLP 2025 Shared Task on detecting manipulative narratives in Ukrainian Telegram content during the 2022 Russian invasion. The task involved classifying 10 manipulation techniques and identifying corresponding text spans in a multilingual, multi-label dataset. The authors developed a two-stage fine-tuning pipeline for classification using Gemma 2 with LoRA adapters, enhanced by meta-features and CatBoost post-processing. For span detection, they employed a dual-head XLM-RoBERTa model trained jointly on classification and token-level labeling. The approach achieved strong performance on frequent manipulation techniques while struggling with rare classes like whataboutism and straw man.

## Method Summary
The classification pipeline involved three stages: (1) Gemma 2 2B IT fine-tuned via CLM on task prompts with few-shot examples to capture domain-specific language patterns, (2) merged CLM adapter with new LoRA adapter trained for multi-label classification, and (3) CatBoost post-processor with meta-features (K-means cluster distances, k-NN frequency features, linguistic cues) and class-wise threshold optimization via k-fold CV. For span detection, a dual-head XLM-RoBERTa-Large model with shared encoder was trained jointly on classification (auxiliary task with reduced loss weight) and token-level labeling. The approach leveraged manipulation trigger phrases and semantic similarity to enhance performance.

## Key Results
- Achieved 2nd place in classification with macro-F1 of 0.454
- Achieved 3rd place in span detection with F1 of 0.599
- Strong performance on frequent techniques (Loaded Language F1 0.782) but struggled with rare classes (Whataboutism F1 0.296, Straw Man F1 0.287)
- Post-processing improved classification F1 from 0.45007 to 0.45447
- Dual-head learning improved span F1 from 0.58588 to 0.59888

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage fine-tuning (CLM → supervised) improves multi-label classification over direct supervised learning.
- Mechanism: Stage 1 exposes the model to domain-specific language patterns via causal language modeling on task-relevant data. Stage 2 adapts these representations for multi-label classification. This allows the model to learn Ukrainian/Russian Telegram discourse patterns before task-specific optimization.
- Core assumption: CLM pre-training on in-domain data captures manipulation-relevant linguistic regularities that transfer to classification.
- Evidence anchors:
  - [abstract]: "fine-tuned the Gemma 2 language model with LoRA adapters in two stages: first via causal language modeling, then supervised multi-label learning"
  - [section 4.1]: "As for this stage of model tuning, we used almost the whole training dataset, as our main goal was to expose the model to as much relevant data as possible rather than tuning to a specific downstream task"
  - [corpus]: Weak/missing—no corpus papers validate CLM-to-classification transfer specifically for manipulation detection.
- Break condition: If CLM stage causes catastrophic forgetting of pre-trained knowledge or if domain adaptation provides no signal (e.g., data too small or noisy), staged approach degrades to baseline.

### Mechanism 2
- Claim: Meta-feature enrichment + threshold optimization compensates for class imbalance and LLM uncertainty.
- Mechanism: CatBoost post-processor combines: (1) clustering-based distances showing semantic proximity to known manipulation patterns, (2) k-NN frequency features indicating technique prevalence among similar texts, and (3) linguistic cues (punctuation, URLs). Class-wise threshold optimization via k-fold CV separately calibrates rare vs. frequent classes.
- Core assumption: Manipulative content clusters semantically, and local neighborhood statistics provide discriminative signal beyond LLM embeddings.
- Evidence anchors:
  - [abstract]: "enhanced performance using meta-features (text similarity, clustering, linguistic cues) and a CatBoost post-processor with class-wise threshold optimization"
  - [section 4.1]: "distances from each text to the centroids of clusters formed by triggered phrases from the training set using K-means"
  - [results]: Post-processing raised F1 from 0.45007 to 0.45447; Table 1 shows rare classes (whataboutism: 0.296, straw_man: 0.287) underperform frequent classes (loaded_language: 0.782).
  - [corpus]: Related work (e.g., PCoT paper) suggests persuasion knowledge infusion helps detection, but no direct corpus validation for clustering+threshold mechanisms.
- Break condition: If manipulation techniques don't cluster cleanly (high semantic overlap) or if threshold optimization overfits fold-specific noise, post-processing adds no gain or introduces calibration error.

### Mechanism 3
- Claim: Dual-task learning (classification + span detection) improves span boundary detection via shared representations.
- Mechanism: XLM-RoBERTa encoder feeds two heads: (1) token-level span classifier, (2) multi-label technique classifier using pooled [CLS] + mean/max pooling. Classification loss is down-weighted so span detection remains primary, but auxiliary classification provides semantic context for boundary decisions.
- Core assumption: Knowing what manipulation technique is present helps identify where manipulative language occurs.
- Evidence anchors:
  - [abstract]: "For span detection, they used a dual-head XLM-RoBERTa model trained jointly on classification and token-level labeling"
  - [section 4.2]: "Both heads share a common encoder, allowing the model to benefit from shared representations across tasks... we apply a reduced weighting coefficient to the classification head's loss"
  - [results]: Table 4 shows span F1 improved from 0.58588 (baseline) to 0.59888 (dual-head), though authors note gains were modest.
  - [corpus]: Corpus paper "Decoding Fake Narratives" demonstrates dual-head RoBERTa for hate speech + fake narrative detection, supporting multi-task benefits for related tasks.
- Break condition: If tasks are incompatible (span boundaries don't correlate with technique type) or if auxiliary loss dominates training, dual-task learning degrades span performance.

## Foundational Learning

- Concept: **Multi-label classification with macro-F1 evaluation**
  - Why needed here: Single posts contain multiple manipulation techniques (multi-label), and macro-F1 prevents frequent classes from dominating evaluation. Understanding this explains why threshold optimization matters more for rare classes.
  - Quick check question: Given F1 scores of 0.78 (loaded_language, 2959 samples) and 0.30 (whataboutism, 235 samples), which class contributes more to macro-F1 vs. micro-F1?

- Concept: **Causal language modeling for domain adaptation**
  - Why needed here: Stage 1 CLM adapts Gemma 2 to Ukrainian/Russian Telegram discourse before task-specific tuning. Understanding CLM helps diagnose whether domain exposure or task formulation drives gains.
  - Quick check question: If CLM training data were shuffled (destroying sequential structure), would next-token prediction still provide useful domain signal?

- Concept: **Threshold calibration for imbalanced multi-label tasks**
  - Why needed here: Default 0.5 threshold fails when class priors are skewed. Class-wise optimization via k-fold CV separately tunes thresholds for 10 techniques with vastly different frequencies.
  - Quick check question: For a rare class (5% prevalence), would optimal threshold likely be higher or lower than 0.5? Why?

## Architecture Onboarding

- Component map:
```
Input Text
    │
    ├─► [Gemma 2 + LoRA Stage 1: CLM] ─► Domain-adapted model
    │         │
    │         └─► [Gemma 2 + LoRA Stage 2: Classification] ─► Probabilities (10-dim)
    │                    │
    │                    └─► [CatBoost + Meta-features] ─► Final predictions
    │                              │
    │                              ├─► Cluster distances (K=10)
    │                              ├─► k-NN frequency features
    │                              └─► Linguistic cues
    │
    └─► [XLM-RoBERTa Encoder]
              │
              ├─► [Span detection head] ─► Token binary labels
              └─► [Classification head] ─► Technique labels (auxiliary, weighted loss)
```

- Critical path:
  1. Data preprocessing: Tokenize Ukrainian/Russian text, extract trigger phrases for clustering
  2. CLM fine-tuning: Train LoRA adapter on full training set with manipulation detection prompts
  3. Classification fine-tuning: Merge CLM adapter, train new LoRA for multi-label output
  4. Meta-feature engineering: Compute cluster centroids, k-NN similarities, linguistic features
  5. Post-processing: Train CatBoost on probabilities + meta-features, optimize class-wise thresholds
  6. Span detection: Train dual-head XLM-RoBERTa with weighted multi-task loss

- Design tradeoffs:
  - **Complexity vs. gain**: Post-processing adds ~0.4 F1 points (0.450→0.454), dual-head adds ~0.013 F1 points for spans (0.586→0.599). Authors note simpler baselines may be justified for production.
  - **LoRA vs. full fine-tuning**: LoRA enables 2B model training with limited compute but may underfit compared to full adaptation.
  - **Shared vs. separate encoders**: Classification uses Gemma 2, spans use XLM-RoBERTa—different architectures prevent representation sharing but allow task-specific optimization.
  - **Assumption**: Joint training benefits spans more than separate training, but empirical gains are modest.

- Failure signatures:
  - **Rare class collapse**: Whataboutism, straw_man, bandwagon F1 < 0.30 despite threshold optimization (Table 1). Signal: confusion matrix shows these classes misclassified as loaded_language.
  - **Span boundary drift**: Token-level predictions don't align with character spans. Signal: high token F1 but low span F1 due to boundary mismatches.
  - **Meta-feature noise**: Clustering-based features add no signal if manipulation patterns don't separate. Signal: CatBoost feature importance shows meta-features near zero.
  - **CLM overfitting**: Stage 1 memorizes training prompts, degrading generalization. Signal: validation loss increases during CLM training.

- First 3 experiments:
  1. **Baseline sanity check**: Train CatBoost on meta-features only (no LLM), optimize thresholds. Target: reproduce baseline F1 ~0.408. If baseline fails, meta-feature engineering is broken.
  2. **Ablation: CLM stage contribution**: Compare (a) direct supervised fine-tuning vs. (b) CLM→supervised pipeline. Hold out validation split from CLM training to measure domain adaptation value.
  3. **Span detection ablation**: Compare (a) single-head token classifier vs. (b) dual-head with auxiliary classification at weightings [0.1, 0.5, 1.0]. Target: identify if classification head helps or adds optimization noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance degrade when detecting manipulation in a real-world temporal setting with emerging narratives?
- Basis in paper: [explicit] The authors note the dataset split ignores chronological order, meaning "the evaluation may not reflect the real-world scenario of predicting new, emerging manipulation patterns."
- Why unresolved: The current random split evaluation obscures the model's ability to generalize to future rhetorical strategies that evolve during ongoing conflicts.
- What evidence would resolve it: A re-evaluation using a time-based hold-out set where the test data consists of posts published strictly after the training period.

### Open Question 2
- Question: Can data augmentation or specialized prompting significantly improve detection for rare classes like whataboutism and straw man?
- Basis in paper: [inferred] The results show a massive disparity between frequent classes (Loaded Language F1 0.782) and rare classes (Whataboutism F1 0.296), and the authors list class imbalance as a key limitation.
- Why unresolved: The current solution relies on threshold optimization, which adjusts decision boundaries but may not suffice for classes with very few training examples.
- What evidence would resolve it: Ablation studies testing the impact of oversampling or synthetic data generation specifically on the F1 scores of the lowest-support classes.

### Open Question 3
- Question: To what extent does annotation subjectivity and ambiguous span boundaries limit the ceiling for token-level detection performance?
- Basis in paper: [explicit] The Limitations section states that "manipulation signal is subjective" and span boundaries "are not always clearly defined," leading to ambiguous labels.
- Why unresolved: It remains unclear if the span detection errors (F1 0.599) are caused by model architecture failures or fundamental noise in the ground-truth labels.
- What evidence would resolve it: Correlating model confidence scores with inter-annotator agreement metrics to identify if errors cluster on low-agreement spans.

## Limitations
- Struggles significantly with rare manipulation techniques (F1 < 0.30 for whataboutism, straw_man, bandwagon) despite threshold optimization
- Modest performance gains from complex components (post-processing: +0.4 F1 points, dual-head: +0.013 F1 for spans) suggest simpler baselines might suffice
- Lack of temporal validation means the model may not generalize to emerging manipulation patterns in real-world settings

## Confidence
- **High confidence** in the competitive ranking results (2nd place classification, 3rd place span detection) and the overall effectiveness of the two-stage fine-tuning pipeline for frequent classes
- **Medium confidence** in the specific contributions of individual components (meta-features, dual-head learning) due to lack of detailed ablation studies and missing hyperparameter specifications
- **Low confidence** in the robustness of the approach for rare manipulation techniques and the practical value of the added complexity given the modest performance gains

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary LoRA rank, learning rates, and dual-head classification loss weighting to quantify their impact on F1 scores, particularly for rare classes
2. **Ablation study of complex components**: Remove meta-features and dual-head learning separately to measure their individual contributions, verifying whether the 0.4 F1 point gain from post-processing is primarily due to meta-features or threshold optimization
3. **Cross-domain robustness test**: Evaluate the trained models on manipulation detection datasets from different domains (e.g., news articles, social media in other languages) to assess generalization beyond Ukrainian/Russian Telegram content