---
ver: rpa2
title: 'InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed
  Chart Information'
arxiv_id: '2508.07630'
source_url: https://arxiv.org/abs/2508.07630
tags:
- chart
- reasoning
- charts
- visual
- decaf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces INTER CHART, a multi-tier benchmark for\
  \ evaluating vision-language models (VLMs) on multi-chart visual reasoning tasks.\
  \ It features three subsets\u2014DECAF (decomposed charts), SPECTRA (synthetic chart\
  \ pairs), and STORM (real-world chart pairs)\u2014totaling 5,214 validated QA pairs\
  \ across 2,706 charts."
---

# InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information

## Quick Facts
- **arXiv ID**: 2508.07630
- **Source URL**: https://arxiv.org/abs/2508.07630
- **Reference count**: 40
- **Primary result**: VLMs achieve high accuracy on simplified charts but struggle significantly on real-world multi-chart tasks, with performance dropping from 59.1% to 34.8% accuracy across complexity tiers.

## Executive Summary
This paper introduces INTER CHART, a multi-tier benchmark for evaluating vision-language models (VLMs) on multi-chart visual reasoning tasks. It features three subsets—DECAF (decomposed charts), SPECTRA (synthetic chart pairs), and STORM (real-world chart pairs)—totaling 5,214 validated QA pairs across 2,706 charts. The authors evaluate models using direct chart inputs, table-based reasoning, and prompt engineering, employing an LLM-based semantic evaluation framework with majority voting. Results show that VLMs perform well on simplified, decomposed charts but struggle significantly on complex, real-world multi-chart tasks, with accuracy dropping sharply in STORM (e.g., Gemini-1.5 Pro: 34.8%). Structured table representations and chain-of-thought prompting improve performance on simpler tasks but fail on semantically complex charts. The study highlights current VLMs' limitations in cross-chart integration and temporal reasoning, providing a diagnostic framework for advancing multimodal reasoning.

## Method Summary
The InterChart benchmark evaluates VLMs across three complexity tiers: DECAF (decomposed single-variable charts), SPECTRA (synthetic correlated chart pairs), and STORM (real-world multi-domain chart pairs). The evaluation pipeline uses direct chart inputs in both combined stitched and interleaved sequential formats, plus chart-to-table conversion via DePlot++. QA generation employs SQL-based sampling for DECAF, LLM-guided synthetic generation for SPECTRA, and human-in-the-loop curation for STORM. Accuracy is measured using an LLM-based semantic evaluation framework with majority voting across three judges (Gemini 1.5 Flash, Phi-4, Qwen2.5-7B-Instruct) to handle paraphrases, numeric approximations, and unit variations. The benchmark tests zero-shot, zero-shot CoT, and few-shot CoT prompting strategies.

## Key Results
- VLMs perform significantly better on decomposed single-variable charts (DECAF) than on complex real-world chart pairs (STORM), with Gemini-1.5 Pro dropping from 59.1% to 34.8% accuracy.
- Structured table representations improve performance on simpler DECAF tasks but degrade accuracy on complex STORM charts (Gemini drops from 34.8% to 29.5%) due to lost semantic and temporal alignment.
- Abstract numerical reasoning across charts is substantially harder than entity inference, with accuracy rates of 13.6-15.6% versus 39.1-42.1% in STORM, indicating multi-step computation as a key bottleneck.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing multi-entity charts into simplified single-variable visuals improves VLM accuracy.
- Mechanism: Breaking compound charts into atomic visual units reduces visual clutter and isolates reasoning targets, allowing models to parse individual data encodings without cross-entity interference. This separation enables focused factual lookup rather than requiring simultaneous visual parsing and multi-entity integration.
- Core assumption: Models possess sufficient single-chart visual parsing capabilities; the bottleneck is integration complexity rather than fundamental visual recognition.
- Evidence anchors:
  - [abstract] "We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration."
  - [section 4.1] Table 6 shows Gemini-1.5 Pro achieves 69.9% accuracy on structured DECAF tables, outperforming direct chart inputs (65.2% combined format).
  - [corpus] Related work "Do MLLMs Really Understand the Charts?" supports that MLLMs rely on visual recognition rather than reasoning, suggesting decomposition helps by simplifying the recognition task.
- Break condition: If decomposition accuracy drops below direct multi-chart accuracy, the bottleneck is not integration but fundamental parsing failures on simplified visuals.

### Mechanism 2
- Claim: Performance degrades systematically as visual complexity and semantic misalignment increase across chart pairs.
- Mechanism: Real-world charts introduce irregular axes, domain-specific conventions, and temporal discontinuities. Models trained on synthetic or uniformly styled charts cannot transfer learned visual-semantic mappings to heterogeneous layouts, causing feature extraction failures at the input stage rather than reasoning failures.
- Core assumption: Visual feature extractors in VLMs are sensitive to layout/style variations and were not sufficiently exposed to diverse real-world chart distributions during training.
- Evidence anchors:
  - [abstract] "Our evaluation... reveals consistent and steep accuracy declines as chart complexity increases."
  - [section 4.1] Table 5 shows Gemini-1.5 Pro drops from 59.1% (SPECTRA) to 34.8% (STORM); Qwen2-VL drops from 32.8% to 28.9%.
  - [corpus] ChartGalaxy and InfoChartQA papers confirm that visually rich, design-driven charts challenge LVLMs trained on plain charts.
- Break condition: If synthetic-to-real transfer improves with augmentation or style-invariant training, the mechanism is generalization failure rather than reasoning capacity limits.

### Mechanism 3
- Claim: Abstract numerical reasoning across charts is harder than entity inference because it requires multi-step aggregation without explicit visual anchors.
- Mechanism: Entity inference can leverage named labels and direct visual correspondences. Abstract numerical reasoning (e.g., estimating ranges, computing derived metrics) requires holding intermediate values in working memory, applying operations across misaligned axes, and synthesizing results—all without grounding in salient visual features.
- Core assumption: VLMs lack robust internal working memory and explicit computation modules for multi-step numerical operations across modalities.
- Evidence anchors:
  - [section 4.5] Table 9 shows abstract numerical reasoning at 13.6–15.6% accuracy vs entity inference at 39.1–42.1% in STORM.
  - [section 4.1] "These declines reflect the challenge of integrating misaligned metadata, irregular axes, and domain-specific trends across diverse visual styles."
  - [corpus] BigCharts-R1 paper notes VLMs struggle with chart comprehension due to lack of reasoning diversity in training data, supporting the multi-step aggregation hypothesis.
- Break condition: If explicit calculation scaffolding (e.g., code execution tools) raises abstract reasoning to entity-inference levels, the bottleneck is computational rather than conceptual.

## Foundational Learning

- Concept: Multi-chart visual reasoning vs. single-chart VQA
  - Why needed here: InterChart evaluates cross-chart synthesis, not isolated chart understanding. Single-chart benchmarks (ChartQA, DVQA) do not expose integration failures.
  - Quick check question: Can you name two failure modes that only appear when models must compare or aggregate across multiple charts?

- Concept: Semantic alignment in multi-modal reasoning
  - Why needed here: Real-world chart pairs often share implicit relationships (e.g., GDP and healthcare spending over time) without explicit linking cues. Models must infer these connections.
  - Quick check question: If two charts share a time axis but use different units and styles, what additional reasoning step is required before numerical comparison?

- Concept: LLM-as-judge evaluation with majority voting
  - Why needed here: String-match metrics penalize valid paraphrases and unit variations. The paper uses three LLM judges (Gemini 1.5 Flash, Phi-4, Qwen2.5) with majority voting for semantic correctness.
  - Quick check question: Why does majority voting across three judges reduce evaluation noise compared to a single judge?

## Architecture Onboarding

- Component map: InterChart has three evaluation tiers (DECAF → SPECTRA → STORM), two visual input formats (combined stitched images vs. interleaved sequential images), and a three-judge LLM evaluation pipeline with majority voting. QA generation uses SQL-based sampling for DECAF, LLM-guided synthetic generation for SPECTRA, and human-in-the-loop curation for STORM.

- Critical path: Start with DECAF combined-format, zero-shot evaluation to establish baseline visual parsing. Then test interleaved inputs on SPECTRA to assess multi-chart handling. Finally, evaluate STORM with few-shot CoT to probe upper limits. Use the LLM-judge pipeline for all scoring to ensure semantic evaluation consistency.

- Design tradeoffs: Combined images are simpler to implement but may introduce visual clutter; interleaved images reduce clutter but require models to maintain state across inputs. Structured table intermediates help on DECAF but hurt on STORM (tables lose semantic/temporal alignment). Zero-shot is lower cost but few-shot CoT reveals reasoning depth.

- Failure signatures: If DECAF accuracy is low (<40%), the issue is basic visual parsing. If DECAF is high but SPECTRA is low, cross-chart integration is failing. If SPECTRA is moderate but STORM is near-random, real-world generalization is broken. If abstract numerical reasoning is disproportionately low vs. entity inference, multi-step computation is the bottleneck.

- First 3 experiments:
  1. Run Gemini-1.5 Pro on DECAF combined vs. interleaved formats with zero-shot prompting to quantify visual format sensitivity.
  2. Evaluate Qwen2-VL-7B on SPECTRA with zero-shot CoT vs. few-shot CoT to measure prompting strategy gains on correlated trend reasoning.
  3. Test chart-to-table (DePlot++) followed by table-based QA on STORM to confirm whether structured intermediates degrade performance on complex real-world pairs (expect drop from 34.8% to ~29% per Table 6).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does fine-tuning on multi-chart data significantly improve cross-chart integration and temporal reasoning compared to zero-shot prompting?
- **Basis in paper:** [explicit] The authors state in the Limitations section that their evaluation relies entirely on zero- and few-shot prompting, which "does not capture the full potential of models that might benefit from fine-tuning on chart-specific data."
- **Why unresolved:** It remains unclear if the poor performance on the STORM subset is a fundamental architectural limitation of VLMs or simply a lack of exposure to complex multi-chart distributions during training.
- **What evidence would resolve it:** Performance benchmarks of VLMs specifically fine-tuned on the DECAF and SPECTRA subsets, evaluated subsequently on the complex STORM subset.

### Open Question 2
- **Question:** To what extent does performance on standard charts predict reasoning capabilities on complex visual formats like infographics and annotated scientific plots?
- **Basis in paper:** [explicit] The Conclusion notes plans to "expand INTER CHART beyond traditional charts to include infographics, annotated scientific plots, and hybrid layouts."
- **Why unresolved:** The current benchmark focuses on structured chart types; it is unknown if the visual parsing failures identified in standard charts scale linearly to the visual clutter and hybrid layouts found in scientific infographics.
- **What evidence would resolve it:** A correlation analysis between model scores on the current InterChart subsets and performance on a newly constructed dataset of infographics and scientific plots.

### Open Question 3
- **Question:** Can neuro-symbolic or retrieval-augmented architectures effectively mitigate the semantic alignment failures observed in complex multi-chart reasoning?
- **Basis in paper:** [explicit] The authors aim to "incorporate neuro-symbolic or retrieval-augmented approaches to support structured abstraction and cross-domain transfer" in future work.
- **Why unresolved:** The study shows that current table-based reasoning methods fail on the STORM subset because tables cannot capture necessary semantic and temporal alignments.
- **What evidence would resolve it:** Accuracy improvements on the STORM subset when using neuro-symbolic frameworks compared to current end-to-end VLM baselines.

## Limitations
- The evaluation depends on closed-source model APIs, limiting reproducibility and preventing architectural ablation studies.
- LLM-based semantic evaluation introduces noise through judge disagreement (21.33% of cases lack unanimous agreement).
- Human-in-the-loop curation for STORM dataset creation creates scalability concerns for future extensions.
- The study does not explore fine-tuning strategies that could potentially improve performance on the benchmark.

## Confidence
- High confidence: The mechanism linking visual decomposition to improved accuracy (Mechanism 1) and the systematic degradation pattern across complexity tiers (Mechanism 2) are strongly supported by the quantitative results and align with established MLLM limitations.
- Medium confidence: The abstract numerical reasoning bottleneck claim (Mechanism 3) is well-documented in the results but could benefit from additional controlled experiments isolating computational from conceptual limitations.
- Medium confidence: The real-world generalization failure is evident but the paper does not fully explore whether this stems from visual feature extractor limitations versus reasoning architecture constraints.

## Next Checks
1. Conduct cross-judge reliability analysis: Run all QA pairs through the three LLM judges multiple times to measure inter-judge consistency and identify systematic disagreement patterns that could bias accuracy measurements.

2. Test table extraction robustness: Systematically evaluate how DePlot++ table extraction performance correlates with subsequent QA accuracy on STORM, identifying specific chart features (layout complexity, annotation density) that trigger extraction failures.

3. Implement controlled visual augmentation: Apply style-invariant training or synthetic data augmentation to baseline VLMs and re-evaluate on SPECTRA→STORM transfer to isolate whether generalization failures are architectural or data-driven.