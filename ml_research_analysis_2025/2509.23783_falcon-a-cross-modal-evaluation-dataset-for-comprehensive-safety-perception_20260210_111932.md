---
ver: rpa2
title: 'Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception'
arxiv_id: '2509.23783'
source_url: https://arxiv.org/abs/2509.23783
tags:
- content
- harmful
- dataset
- harm
- falconeye
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Falcon, a comprehensive dataset for evaluating
  multimodal safety in vision-language models. The dataset contains 57,515 VQA pairs
  across 13 harm categories, with explicit annotations for harmful attributes in images,
  instructions, and responses.
---

# Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception

## Quick Facts
- arXiv ID: 2509.23783
- Source URL: https://arxiv.org/abs/2509.23783
- Reference count: 13
- Primary result: Introduces Falcon dataset and FalconEye evaluator achieving 88.56-94.22% accuracy on multimodal safety detection

## Executive Summary
This paper introduces Falcon, a comprehensive dataset for evaluating multimodal safety in vision-language models. The dataset contains 57,515 VQA pairs across 13 harm categories, with explicit annotations for harmful attributes in images, instructions, and responses. Based on this dataset, the authors develop FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B. FalconEye achieves superior performance in harmful content detection, outperforming baselines including GPT-4o and LlamaGuard models with accuracy rates of 88.56% for images, 91.00% for instructions, and 94.22% for responses on the Falcon-test dataset.

## Method Summary
The authors construct Falcon by aggregating and cleaning data from three existing safety datasets (SPA-VL, JailBreakV-28K, HADES), generating responses using three diverse MLLMs, and applying automated annotation with human verification on test data. FalconEye is trained by fine-tuning Qwen2.5-VL-7B with LoRA (rank=128) on the automatically labeled training set, then evaluated on both Falcon-test (human-labeled) and external benchmarks (VLGuard, Beavertail-V).

## Key Results
- FalconEye achieves 88.56% accuracy on image safety, 91.00% on instruction safety, and 94.22% on response safety
- Outperforms GPT-4o, LlamaGuard-3-11B-Vision, and other baselines on Falcon-test dataset
- Demonstrates cross-modal advantage over text-only safety evaluation methods
- Shows capability for fine-grained harm category detection across 13 harm types

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal harm detection improves safety assessment accuracy over text-only approaches. By jointly processing visual and textual inputs, the model captures harm that emerges from modality interactions—where benign text becomes harmful when paired with specific images, or vice versa. This works because harmful intent is often distributed across modalities rather than localized to one.

### Mechanism 2
Fine-grained tripartite annotation (image, instruction, response) enables more comprehensive safety evaluation. Each VQA pair receives independent safety labels for three components plus harm categories and explanations, allowing the evaluator to learn component-specific harm patterns and their interactions. This works because harm can originate from any component independently or from their combination.

### Mechanism 3
LoRA fine-tuning on domain-specific safety data preserves base model capabilities while enabling efficient specialization. Low-rank adaptation modifies only attention weights with rank-128 updates, allowing safety-specific learning without catastrophic forgetting of general vision-language understanding. This works because safety evaluation builds upon rather than replaces foundational multimodal reasoning.

## Foundational Learning

- Concept: **Visual Question Answering (VQA) safety evaluation**
  - Why needed here: This task requires understanding that harm assessment in multimodal contexts differs fundamentally from text-only safety—images can contextualize, amplify, or contradict textual content.
  - Quick check question: Can you explain why "What are the names of the schools?" might be harmful with one image but benign with another?

- Concept: **Harm taxonomy granularity tradeoffs**
  - Why needed here: The paper uses 13 categories merged from overlapping schemes; overly granular categories increase annotation difficulty and model confusion, while overly broad categories lose actionable specificity.
  - Quick check question: Why might "Child Abuse" and "Animal Abuse" be merged into "Abuse" rather than kept separate?

- Concept: **Automated annotation with human verification**
  - Why needed here: The pipeline uses Qwen2.5-VL-72B-AWQ for initial labeling then human verification on test set—understanding this two-stage process is critical for reproducing results and assessing label quality.
  - Quick check question: What are the tradeoffs between full human annotation vs. model-then-verify annotation for a 57,515-sample dataset?

## Architecture Onboarding

- Component map: Data sources (SPA-VL, JailBreakV-28K, HADES) -> Response generators (MiniCPM-V, Qwen-2.5-VL, Deepseek-VL) -> Auto-annotator (Qwen2.5-VL-72B-AWQ) -> Training data (Falcon-train) -> Test data (Falcon-test) -> Evaluator model (FalconEye)

- Critical path: Data aggregation → response generation → manual quality filtering → automated annotation → human test-set labeling → LoRA fine-tuning → evaluation on Falcon-test, VLGuard, Beavertail-V

- Design tradeoffs:
  - Model-labeled training vs. human-labeled: Enables scale (57K samples) but introduces label noise; mitigated by human-verified test set
  - Three response generators: Increases output diversity but may create inconsistent response styles
  - 13 categories (merged): Reduces annotation complexity but may obscure fine-grained distinctions (e.g., "Unlicensed Advice" is rare and hard to recognize)

- Failure signatures:
  - GPT-4o refused 25 VQA instances due to platform safety filters—closed-source evaluators may have coverage gaps on sensitive content
  - Beaver-dam/Llama-Guard-3-8B cannot process images—text-only baselines miss visual harm signals
  - Low detection rates on rare categories: Unethical Behavior (23.38%), Economic Harm (30.86%)—insufficient training examples

- First 3 experiments:
  1. Baseline comparison: Run FalconEye vs. GPT-4o vs. Llama-Guard-3-11B-Vision on Falcon-test subset to replicate Table 3 results; verify cross-modal advantage holds.
  2. Ablation on annotation granularity: Train variant with only response-level labels (not image/instruction labels) to quantify contribution of tripartite annotation.
  3. Category-specific analysis: Evaluate FalconEye on low-frequency categories (Adult Content, Unlicensed Advice) to identify undersampling effects; consider targeted data augmentation if performance drops significantly.

## Open Questions the Paper Calls Out

### Open Question 1
Can structured calibration sessions and systematic review of inter-annotator disagreements significantly improve the consistency of harm category labeling across annotators? The authors suggest these methods to address inherent subjectivity in harm judgments but do not implement or evaluate them.

### Open Question 2
What data augmentation or collection strategies can effectively expand coverage of underrepresented harm categories such as "Unlicensed Advice" and "Adult Content" while maintaining ethical standards? The authors identify scarcity as a problem but do not propose or test solutions.

### Open Question 3
What architectural or training improvements would close the performance gap between FalconEye and GPT-4o on fine-grained harm category detection, particularly for categories like "Unethical Behavior" and "Violence and Physical Harm"? The paper presents performance gaps but does not analyze root causes or propose targeted improvements.

## Limitations

- Annotation quality uncertainty: Full training set relies on model-generated labels without reported inter-annotator agreement statistics
- Category granularity tradeoffs: 13 merged categories may obscure actionable distinctions and underrepresent rare but critical harm types
- Closed-source evaluator bias: GPT-4o's safety filters led to 25 refused instances, suggesting systematic coverage gaps in comparative evaluations

## Confidence

- **High confidence**: Cross-modal harm detection improves accuracy over text-only approaches (supported by consistent performance gains across components)
- **Medium confidence**: Tripartite annotation significantly enhances safety evaluation (differential accuracy suggests learning, but no granularity ablation reported)
- **Medium confidence**: LoRA fine-tuning effectively specializes the evaluator (technical specifications complete, but no comparison to alternatives)

## Next Checks

1. Compute Cohen's kappa or Krippendorff's alpha on a subset of model-labeled training data to quantify label consistency and estimate noise impact.
2. Train a variant using only response-level labels and compare performance to quantify tripartite annotation contribution.
3. Conduct detailed evaluation on low-frequency categories (Adult Content, Unlicensed Advice, Unethical Behavior) to identify undersampling effects.