---
ver: rpa2
title: Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes
arxiv_id: '2504.18743'
source_url: https://arxiv.org/abs/2504.18743
tags:
- pspan
- have
- lemma
- dmin
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first finite-time analysis of average-reward
  Q-learning with asynchronous updates based on a single trajectory of Markovian samples.
  The main challenge arises from the use of adaptive stepsizes, which make each Q-learning
  update depend on the entire sample history and introduce strong correlations, resulting
  in a non-Markovian stochastic approximation (SA) scheme.
---

# Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes

## Quick Facts
- **arXiv ID**: 2504.18743
- **Source URL**: https://arxiv.org/abs/2504.18743
- **Reference count**: 40
- **One-line primary result**: First finite-time analysis of average-reward Q-learning with asynchronous updates showing O(1/k) convergence to optimal Q-function using adaptive stepsizes

## Executive Summary
This paper provides the first finite-time analysis of average-reward Q-learning with asynchronous updates based on a single trajectory of Markovian samples. The main challenge arises from the use of adaptive stepsizes, which make each Q-learning update depend on the entire sample history and introduce strong correlations, resulting in a non-Markovian stochastic approximation (SA) scheme. The paper establishes convergence at a rate of O(1/k) to the optimal relative Q-function in the span seminorm, with a centering step achieving pointwise mean-square convergence.

The key insight is that adaptive stepsizes are necessary for convergence, as universal stepsizes converge to the wrong target due to sampling bias in asynchronous updates. The adaptive stepsizes act as implicit importance sampling weights that counteract the effects of the stationary distribution of the behavior policy. The paper develops novel tools to handle the strong correlations between adaptive stepsizes and iterates, including a time-inhomogeneous Markovian reformulation of non-Markovian SA.

## Method Summary
The paper analyzes average-reward Q-learning with asynchronous updates using a single trajectory of Markovian samples. The algorithm uses adaptive stepsizes of the form α/(N_k(s,a)+h), where N_k(s,a) tracks visit counts for each state-action pair. This creates a non-Markovian stochastic approximation scheme due to the dependence of stepsizes on the entire sample history. The paper establishes finite-time convergence guarantees by reformulating the algorithm as a time-inhomogeneous Markovian SA and developing new technical tools to handle the correlations. Two variants are analyzed: Algorithm 1 achieves convergence in span seminorm, while Algorithm 2 adds a centering step to achieve pointwise convergence.

## Key Results
- Establishes O(1/k) convergence rate to optimal relative Q-function in span seminorm
- Shows adaptive stepsizes are necessary - universal stepsizes converge to wrong target
- Proves centering step achieves pointwise mean-square convergence to centered optimal Q-function
- Develops novel tools for analyzing non-Markovian SA with adaptive stepsizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive stepsizes (local clocks) are necessary to correct the sampling bias inherent in asynchronous updates, whereas universal stepsizes converge to the wrong target.
- **Mechanism:** In an asynchronous setting, samples arrive according to the stationary distribution μ of the behavior policy, not a uniform distribution. Universal stepsizes effectively solve the "asynchronous Bellman equation" H̄(Q)=Q, which differs from the true Bellman equation H(Q)=Q unless the sampling is uniform. Adaptive stepsizes α_k(s,a) ∝ 1/N_k(s,a) act as implicit importance sampling weights, countering the distribution μ to ensure convergence to the correct fixed point.
- **Core assumption:** The behavior policy induces an irreducible and aperiodic Markov chain (Assumption 3.1).
- **Evidence anchors:**
  - [abstract] ("...adaptive stepsizes can be interpreted as a form of implicit importance sampling that counteracts the effects of asynchronous updates.")
  - [Section 5.1, Proposition 5.2] ("...Q-learning with universal stepsizes is, in general, guaranteed not to converge to any point in the desired set...")
  - [corpus] Related work confirms the difficulty of average-reward analysis without strong assumptions or specific algorithmic structures like Relative Value Iteration (RVI).

### Mechanism 2
- **Claim:** The span seminorm allows convergence analysis despite the Q-iterates being unbounded in standard norms.
- **Mechanism:** In average-reward RL, Q-values can drift to infinity in the direction of the all-ones vector (constant offset). Standard norms (like ℓ∞) fail to capture convergence because the "error" grows. The span seminorm pspan(x) = max x - min x ignores constant offsets, allowing the Bellman operator to remain a contraction mapping (Assumption 2.3) even without a discount factor.
- **Core assumption:** The Bellman operator is a contraction mapping with respect to the span seminorm (Assumption 2.3), often satisfied if transition dynamics satisfy a total variation bound.
- **Evidence anchors:**
  - [Section 2.2] ("...the Bellman operator H(·) is not a contraction mapping under any norm. However... [it] can be a contraction mapping with respect to the span norm...")
  - [Section 6.3, Proposition 6.6] ("...show that the Q-learning iterates... satisfy an almost-sure time-varying bound that grows at most logarithmically...")

### Mechanism 3
- **Claim:** A centering step prevents the divergence of the Q-function in the direction of the all-ones vector.
- **Mechanism:** While the span seminorm ensures convergence of the *shape* of the Q-function (relative values), the absolute values may drift (Example 4.1). By subtracting the mean of the min/max values (g(Q_k)e), the algorithm projects the iterate onto the orthogonal complement of the span seminorm's kernel, ensuring pointwise convergence.
- **Core assumption:** N/A (Algorithmic step).
- **Evidence anchors:**
  - [Section 4, Algorithm 2] Defines the centering step Q_{k+1} = Q̃_{k+1} - g(Q̃_{k+1})e where g(Q) = (max Q + min Q)/2.
  - [Section 4, Theorem 4.3] ("...achieves pointwise mean-square convergence to a centered optimal relative Q-function...")

## Foundational Learning

- **Concept:** **Span Seminorm and Contraction Mappings**
  - **Why needed here:** Standard Q-learning relies on discounting to ensure the Bellman operator is a contraction in ℓ∞ norm. This paper removes discounting, so you must understand that the operator is only contractive if you "ignore" constant shifts (span seminorm).
  - **Quick check question:** Can you explain why subtracting a constant c from all Q-values does not change the optimal policy but changes the ℓ∞ norm?

- **Concept:** **Asynchronous Stochastic Approximation (SA)**
  - **Why needed here:** The paper frames the algorithm as a time-inhomogeneous Markovian SA. You need to understand that "asynchronous" means updates depend on the frequency of state visits, creating noise correlated with the current iterate.
  - **Quick check question:** Why does using a single trajectory (asynchronous) make convergence analysis harder than generating i.i.d. samples (synchronous)?

- **Concept:** **Importance Sampling**
  - **Why needed here:** The paper interprets adaptive stepsizes as implicit importance sampling.
  - **Quick check question:** If a behavior policy visits state s₁ 90% of the time and s₂ 10% of the time, how should you weight the updates to s₂ to simulate a uniform visitation distribution?

## Architecture Onboarding

- **Component map:** Sampler -> Counter Matrix -> Adaptive Stepsize Calculator -> Q-Update -> (Centering Module) -> Q-output
- **Critical path:** The calculation of α_k(s,a). If you replace this with a global stepsize (e.g., 1/k), the algorithm will converge to the wrong solution. The dependence on N_k(s,a) is not a heuristic; it is mathematically necessary to counteract the stationary distribution bias.
- **Design tradeoffs:**
  - **Algorithm 1 vs. Algorithm 2:** Algorithm 1 is simpler but allows Q-values to drift (unbounded pointwise). Algorithm 2 bounds the Q-values via centering but requires an O(|S||A|) scan of the Q-table every iteration to find min/max.
  - **Parameter α:** The convergence rate depends on α(1-β). If α is too small, convergence is slow (O(k^{-α(1-β)/2})). You must select α ≥ 2/(1-β) for optimal O(1/k) rates.

- **Failure signatures:**
  - **Universal Stepsize Failure:** If using α/k for all pairs, the algorithm converges stably but to a sub-optimal policy.
  - **Drift:** If using Algorithm 1, monitor max(Q). It will grow indefinitely, though max(Q) - min(Q) should stabilize.

- **First 3 experiments:**
  1. **Validate Necessity of Adaptive Stepsizes:** Run Q-learning on a simple 2-state MDP (like the example in Appendix C) using universal stepsizes vs. adaptive stepsizes. Plot the resulting policy's average reward to confirm the universal version fails.
  2. **Tune α for Convergence Rate:** Test various fixed α values. Verify empirically that if α is too small (relative to 1/(1-β)), the convergence rate degrades from O(1/k) to a slower power law.
  3. **Test Centering Effect:** Run Algorithm 1 and Algorithm 2 on a long time horizon. Plot ||Q_k||_∞ for both. Observe Algorithm 1 diverging pointwise while Algorithm 2 remains bounded.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the sample complexity dependence on the state-action space size (|S||A|), the seminorm contraction factor (1/(1-β)), and the minimum visitation probability (D_min) be tightened using refined analysis or techniques like Polyak averaging?
- **Basis:** [explicit] The Conclusion states that current dependencies are "generally not tight" and suggests refined analysis or improved designs like variance reduction as a direction.
- **Why unresolved:** The current work prioritized establishing the first finite-time bounds for this specific setting, utilizing constants that may be suboptimal.
- **What evidence would resolve it:** A derivation of convergence bounds with lower polynomial dependencies on |S||A| and 1/(1-β), matching known information-theoretic lower bounds.

### Open Question 2
- **Question:** Is it possible to perform a finite-time analysis of average-reward Q-learning when the Bellman operator is non-expansive rather than a seminorm contraction?
- **Basis:** [explicit] The Conclusion identifies investigating Q-learning without the seminorm contraction assumption (Assumption 2.3) as an interesting direction for future work.
- **Why unresolved:** The current proof relies heavily on the span-seminorm contraction property to manage the lack of discounting; removing this requires new techniques to handle non-expansive operators with time-inhomogeneous Markovian noise.
- **What evidence would resolve it:** A theoretical proof of convergence (and convergence rate) for average-reward Q-learning under the weaker "non-expansive" operator condition.

### Open Question 3
- **Question:** Do adaptive stepsizes provably yield faster convergence rates than universal stepsizes in the discounted Q-learning setting?
- **Basis:** [explicit] Appendix C notes that in numerical simulations for the discounted setting, adaptive stepsizes appear to converge faster, stating "A theoretical investigation of this phenomenon is an interesting future direction."
- **Why unresolved:** The paper focuses on the average-reward setting where universal stepsizes fail; the theoretical benefits of adaptive stepsizes in the discounted setting (where universal stepsizes still converge) are not established.
- **What evidence would resolve it:** A theoretical analysis demonstrating that adaptive stepsizes improve the contraction factor or effective mixing time in the discounted setting compared to standard universal stepsizes.

## Limitations

- The analysis relies heavily on the assumption that the behavior policy induces an irreducible and aperiodic Markov chain (Assumption 3.1), without addressing scenarios where this might be violated.
- The centering step in Algorithm 2 requires scanning the entire Q-table to find min/max values at each iteration, which may be computationally prohibitive for large state-action spaces.
- The convergence rate depends critically on the choice of the stepsize parameter α, requiring careful tuning to achieve the optimal O(1/k) rate, though the paper does not provide explicit guidance on parameter selection.

## Confidence

- **High confidence**: The necessity of adaptive stepsizes (Mechanism 1) and the failure of universal stepsizes to converge to the correct target. This is supported by both theoretical proofs (Proposition 5.2) and illustrative examples in the appendix.
- **Medium confidence**: The finite-time convergence rates in span seminorm (Theorem 4.2) and pointwise convergence (Theorem 4.3). While the proofs are rigorous, they rely on several intermediate technical results and concentration inequalities that would benefit from empirical validation.
- **Medium confidence**: The interpretation of adaptive stepsizes as implicit importance sampling. While theoretically sound, the practical impact may vary depending on the specific MDP structure and behavior policy.

## Next Checks

1. **Empirical validation of convergence rates**: Implement both algorithms on a range of MDPs with varying state-action sizes and transition structures to empirically verify the O(1/k) convergence rate predicted by theory, and test the sensitivity to the stepsize parameter α.

2. **Comparison with synchronous sampling**: Design an experiment comparing the adaptive stepsize algorithm with a synchronous variant that uses i.i.d. samples to quantify the practical impact of the asynchronous updates and validate whether the adaptive stepsizes truly correct for the sampling bias.

3. **Robustness to assumption violations**: Test the algorithm on MDPs where the behavior policy is close to, but not strictly satisfying, the irreducibility and aperiodicity assumptions to understand the practical limits of the theoretical guarantees.