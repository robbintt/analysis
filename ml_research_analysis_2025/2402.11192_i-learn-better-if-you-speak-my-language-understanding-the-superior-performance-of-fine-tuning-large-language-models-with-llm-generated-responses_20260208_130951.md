---
ver: rpa2
title: 'I Learn Better If You Speak My Language: Understanding the Superior Performance
  of Fine-Tuning Large Language Models with LLM-Generated Responses'
arxiv_id: '2402.11192'
source_url: https://arxiv.org/abs/2402.11192
tags:
- gpt-4
- data
- answer
- training
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that fine-tuning LLMs with responses generated
  by other LLMs (including itself) often leads to better performance than using human-annotated
  responses, particularly in reasoning tasks. This phenomenon is attributed to the
  fact that LLMs are inherently more "familiar" with LLM-generated responses, as evidenced
  by lower perplexity before fine-tuning.
---

# I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses

## Quick Facts
- arXiv ID: 2402.11192
- Source URL: https://arxiv.org/abs/2402.11192
- Authors: Xuan Ren; Biao Wu; Lingqiao Liu
- Reference count: 40
- Key outcome: Fine-tuning LLMs on LLM-generated responses often outperforms human-annotated responses, especially for reasoning tasks, due to "familiarity" evidenced by lower pre-training perplexity.

## Executive Summary
This paper investigates why fine-tuning large language models (LLMs) with responses generated by other LLMs (including themselves) often leads to better performance than using human-annotated responses, particularly in reasoning tasks. The authors attribute this phenomenon to the "familiarity" of LLM-generated data, measured by lower pre-training perplexity. They design experiments to isolate the effect of familiarity and explore methods like "minimum change" corrections to balance correctness and stylistic alignment. Their findings reveal that training with LLM-generated responses not only enhances performance but also helps maintain the model's capabilities in other reasoning tasks after fine-tuning on a specific task.

## Method Summary
The method involves fine-tuning target LLMs (e.g., Mistral-7B-Instruct-v2, Llama2-13b-chat) using LoRA on synthetic training data generated by stronger LLMs (e.g., GPT-4, Claude-3.5-Sonnet) or the target model itself. Training data consists of (question, response) pairs, where responses are either direct LLM outputs or minimally edited versions of the target model's own predictions. Perplexity of generated responses is measured before fine-tuning to assess "familiarity." Models are evaluated on in-domain accuracy and cross-domain generalization across multiple reasoning benchmarks. The "Minimum Change" method generates a response with the target LLM, then uses a stronger LLM to make only essential corrections to maintain correctness while preserving style.

## Key Results
- Fine-tuning on LLM-generated responses consistently outperforms fine-tuning on human-annotated responses for reasoning tasks.
- Lower pre-training perplexity of training data correlates with better fine-tuning performance, supporting the "familiarity" hypothesis.
- Training on LLM-generated data helps maintain cross-domain reasoning capabilities better than human data, reducing catastrophic forgetting.
- The "Minimum Change" method, which minimally edits target model predictions for correctness, performs competitively with direct strong LLM generation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on LLM-generated data improves performance because the target model has lower pre-training perplexity for this data, a state the authors define as "familiarity."
- Mechanism: Low perplexity indicates the training data distribution closely matches the model's pre-existing internal representations. This alignment allows the model to make more efficient gradient updates, improving task performance while preserving cross-domain capabilities better than human-annotated data, which represents a larger distribution shift.
- Core assumption: An LLM's effectiveness on a reasoning task is positively correlated with the "familiarity" (inverse perplexity) of its training data.
- Evidence anchors:
  - [abstract] "This familiarity is evidenced by lower perplexity before fine-tuning."
  - [section 4.1, Experiment 2] "...training on the higher-perplexity training set consistently shows slightly lower performance... This result demonstrated that the familiarity of LLMs with the stylistic aspects of target responses significantly affects training outcomes."
  - [corpus] Evidence is weak or missing in the provided corpus for this direct perplexity-performance link.
- Break condition: The mechanism assumes the LLM-generated data is factually correct. Low perplexity on incorrect reasoning would reinforce errors. The paper's "Minimum Change" method is a direct attempt to satisfy both correctness and familiarity.

### Mechanism 2
- Claim: Superior performance is not primarily caused by additional reasoning details (e.g., chain-of-thought) in LLM-generated text, but by its stylistic conformity to the model's own output distribution.
- Mechanism: The paper argues that LLMs learn better from data that "speaks their language." Artificially adding excessive detail can increase perplexity and hurt performance if the style deviates from what the model natively produces.
- Core assumption: The stylistic and structural patterns of LLM-generated text are more critical for efficient fine-tuning than the mere quantity of reasoning steps.
- Evidence anchors:
  - [section 4.1, Experiment 1] "...the first group, which used GPT-4’s own style, was superior to that of the second group... converting ground truth into step-by-step answers was even detrimental."
  - [section 1] "...target responses that include more details... do not necessarily result in better training outcomes."
  - [corpus] Evidence is weak or missing in the provided corpus for this specific counter-intuitive finding about detail.
- Break condition: This mechanism fails for tasks requiring a reasoning process completely alien to the pre-trained model. In such cases, the model's "familiarity" with incorrect or naive patterns would be a liability.

### Mechanism 3
- Claim: Training on LLM-generated responses enhances model stability and generalizability.
- Mechanism: By training on data sampled from a distribution close to the model's own, fine-tuning acts as a more "conservative" update. This minimizes disruption to the pre-trained representations responsible for general reasoning, thereby reducing catastrophic forgetting and improving cross-domain performance.
- Core assumption: Pre-trained LLMs share a common representational space ("Platonic Representation Hypothesis"), making them inherently more compatible with each other's outputs than with human text.
- Evidence anchors:
  - [abstract] "...helps maintain the model’s capabilities in other reasoning tasks after fine-tuning on a specific task."
  - [section 3.1] "...the model trained on the synthetic data achieves higher cross-domain performance."
  - [corpus] Evidence is weak or missing in the provided corpus for this specific cross-domain preservation finding.
- Break condition: The benefit assumes the base model already possesses relevant reasoning skills. A very weak base model would have little general capability to preserve.

## Foundational Learning

- Concept: Perplexity (PPL)
  - Why needed here: This is the core quantitative metric the paper uses to define and measure "familiarity." It grounds the qualitative argument in a measurable signal.
  - Quick check question: If a model assigns low perplexity to a sequence, does it mean the sequence is more or less "surprising" to the model?

- Concept: Catastrophic Forgetting
  - Why needed here: The paper frames its outcome in terms of preserving cross-domain capabilities, which is a direct reference to avoiding this common failure mode in fine-tuning.
  - Quick check question: If a model's performance on its original training tasks drops significantly after fine-tuning on a new task, what has likely occurred?

- Concept: Synthetic Data Generation
  - Why needed here: The entire premise involves using an LLM to create training data for another LLM (or itself). Understanding the source and nature of this data is critical.
  - Quick check question: According to the paper, what is a key advantage of synthetic data over human-annotated data beyond correctness?

## Architecture Onboarding

- Component map:
  - Target LLM (e.g., Mistral-7B-Instruct-v2) -> Data Generator (e.g., GPT-4) -> Perplexity Scorer -> Training Pipeline (LoRA fine-tuning) -> Evaluation Suite (in-domain and cross-domain accuracy)

- Critical path:
  1.  **Data Generation**: Generate responses for training questions using the Data Generator.
  2.  **Familiarity Measurement**: Score generated responses using the Perplexity Scorer. Filter for low-PPL samples.
  3.  **Fine-Tuning**: Train the Target LLM on the generated (question, response) pairs.
  4.  **Evaluation**: Measure accuracy on both in-domain and cross-domain benchmarks.

- Design tradeoffs:
  - **Detail vs. Familiarity**: More detailed reasoning chains can increase perplexity and hurt performance if they deviate from the model's native style. The paper suggests prioritizing familiarity.
  - **Correctness vs. Familiarity**: LLM-generated text may be familiar but incorrect. A hybrid approach (e.g., "Minimum Change") or strong filtering is needed to ensure both.
  - **Generator Model**: Using a much stronger model (e.g., GPT-4) guarantees correctness but may reduce familiarity. Using the target model itself maximizes familiarity but risks correctness.

- Failure signatures:
  - **Catastrophic Forgetting**: Significant drop in cross-domain accuracy, indicating training data caused too much distribution shift (a risk with human data).
  - **Degraded Reasoning**: Performance on the trained task drops, likely due to training on high-perplexity or incorrect synthetic data.
  - **Style Mismatch**: The fine-tuned model's output becomes verbose or unnatural, suggesting it was trained on data that didn't "speak its language."

- First 3 experiments:
  1.  Establish the core phenomenon. Fine-tune a target LLM on a reasoning dataset using both human-annotated and LLM-generated responses for the same questions. Measure the perplexity of both data sources to confirm the paper's initial observation.
  2.  Isolate the "familiarity" factor. Use a powerful LLM to generate paraphrased responses for a small dataset, creating two sets with different perplexity levels but identical semantic content. Fine-tune on each to directly test familiarity's impact.
  3.  Test the "minimum change" hybrid. Implement the paper's "Minimum Change" method: generate a response with the target LLM, then use a stronger LLM to make only essential corrections. Fine-tune on this data and compare its performance and perplexity to direct LLM-generated data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reliability of the "minimal change" data correction process be improved, given that advanced models like GPT-4 frequently fail to accurately correct erroneous initial predictions?
- Basis in paper: [explicit] The Limitations section states: "GPT-4 often fails to correct the initial prediction accurately. Although GPT-4 may correct some errors, the minimally changed response can still be incorrect."
- Why unresolved: The paper introduces the "minimal change" method to balance familiarity and correctness but relies on GPT-4 for the correction step. The authors explicitly note that this dependency is a bottleneck because the correcting model often fails to fix the target model's errors without altering the style too drastically.
- What evidence would resolve it: Development of a correction mechanism or prompt strategy that achieves a higher correction success rate (e.g., >95%) while maintaining the original token distribution/style of the target model, compared to the current baseline.

### Open Question 2
- Question: How can the "ground truth style transfer" method be adapted to work effectively for weaker models on complex reasoning tasks where they currently struggle to maintain correctness?
- Basis in paper: [explicit] The Limitations section notes: "When an LLM is not powerful enough, it may not be able to successfully rewrite the ground truth in its own style while maintaining the correctness... Mistral model struggles to successfully rewrite answers for mathematical tasks."
- Why unresolved: The experiments show that while style transfer improves performance, it is fragile. For example, when Mistral attempted to rewrite Math Algebra ground truths, only 24% of the resulting data was usable because the model failed to preserve the logic of the ground truth in its own style.
- What evidence would resolve it: A modified style-transfer protocol that allows smaller/weaker models to rewrite complex reasoning chains without logic drift, resulting in significantly higher data retention rates (improving from the observed 24% to near 100%) without sacrificing the "familiarity" benefit.

### Open Question 3
- Question: To what extent does overlapping training data versus convergent model architectures ("Platonic Representation Hypothesis") contribute to the "familiarity" phenomenon observed between different LLMs?
- Basis in paper: [inferred] The Discussion section speculates on the cause: "The familiarity may stem from overlapping training data used during pre-training... or the Platonic Representation Hypothesis." However, the paper does not isolate these variables.
- Why unresolved: The paper establishes that familiarity exists (low perplexity) and improves training, but it does not determine if this is simply because the models (e.g., GPT-4 and Mistral) were trained on similar internet text, or if it is a fundamental property of how neural networks represent language.
- What evidence would resolve it: Experiments measuring the familiarity effect between models with strictly disjoint pre-training corpora versus models with identical architectures but different data, to decouple data overlap from architectural convergence.

## Limitations
- The "familiarity" hypothesis lacks direct experimental evidence linking perplexity reduction to performance gains.
- The paper assumes low perplexity always indicates beneficial training data, but this conflates stylistic familiarity with factual correctness.
- Cross-domain generalization benefits are demonstrated but not thoroughly examined; preserved capabilities may reflect memorization rather than true reasoning.

## Confidence
- **High Confidence**: The observation that LLM-generated responses often achieve lower perplexity than human-annotated responses on target LLMs is empirically demonstrated and robust across multiple datasets.
- **Medium Confidence**: The core mechanism linking "familiarity" (low perplexity) to superior training outcomes is plausible but relies on indirect evidence.
- **Low Confidence**: The claim that stylistic conformity to the model's native output distribution is more important than reasoning detail quantity lacks strong experimental validation.

## Next Checks
1. **Direct Perplexity-Performance Causal Link**: Design an experiment where responses are semantically identical but systematically vary in perplexity through controlled stylistic modifications. Fine-tune on both high- and low-perplexity versions and measure whether the perplexity difference directly predicts performance differences.
2. **Correctness vs. Familiarity Trade-off**: Create a dataset where LLM-generated responses have systematically varying levels of correctness (e.g., 100% correct, 80% correct, 60% correct) while controlling for perplexity. This would reveal whether the paper's "Minimum Change" approach is necessary or whether correctness alone determines performance.
3. **Base Model Dependency Test**: Repeat the main experiments with LLMs of varying capabilities (weak, medium, strong) to determine whether the familiarity advantage holds for all models or only for those with sufficient pre-trained reasoning capabilities. This would validate the assumption that pre-trained reasoning skills are necessary for the mechanism to work.