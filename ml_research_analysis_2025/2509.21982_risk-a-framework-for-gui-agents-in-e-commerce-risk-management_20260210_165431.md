---
ver: rpa2
title: 'RISK: A Framework for GUI Agents in E-commerce Risk Management'
arxiv_id: '2509.21982'
source_url: https://arxiv.org/abs/2509.21982
tags:
- agents
- arxiv
- reward
- risk
- multi-step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RISK, a framework for building GUI agents
  tailored to e-commerce risk management. The framework addresses the limitations
  of existing scraping methods and single-step GUI agents by integrating a domain-specific
  dataset (RISK-Data), a benchmark (RISK-Bench), and a reinforcement fine-tuning approach
  (RISK-R1).
---

# RISK: A Framework for GUI Agents in E-commerce Risk Management

## Quick Facts
- **arXiv ID**: 2509.21982
- **Source URL**: https://arxiv.org/abs/2509.21982
- **Reference count**: 31
- **Key outcome**: RISK-R1 achieves 6.8% improvement in offline single-step and 8.8% improvement in offline multi-step performance over baselines, with a top task success rate of 70.5% in online evaluation.

## Executive Summary
This paper introduces RISK, a comprehensive framework for building GUI agents tailored to e-commerce risk management tasks. The framework addresses limitations of existing scraping methods and single-step GUI agents by introducing a domain-specific dataset (RISK-Data), a benchmark (RISK-Bench), and a reinforcement fine-tuning approach (RISK-R1). The key innovation is a multi-aspect reward function that combines format correctness, stepwise accuracy, process reweighting, and level-based difficulty focusing. Experiments demonstrate significant improvements over baselines across both offline benchmarks and online deployment, with the stepwise accuracy reward and process reweighting mechanisms showing particular effectiveness.

## Method Summary
RISK employs a two-stage training approach: supervised fine-tuning (SFT) followed by reinforcement fine-tuning (RFT). The framework uses Qwen2.5-VL-7B-Instruct as the base model, trained on RISK-Data containing 8,492 single-step and 2,386 multi-step trajectories. The RFT stage uses GRPO with a custom reward function combining format reward (0.1), stepwise accuracy reward (0.9), process reweight (γ=0.7, δ=4), and level reweight (easy/moderate/difficult weights of 1.0/1.1/1.2). The model operates in the DOM-tree grounding paradigm, predicting element indices rather than coordinates, and is deployed using the Browser-Use framework with vLLM engine.

## Key Results
- RISK-R1 achieves 6.8% improvement in offline single-step accuracy over baselines
- RISK-R1 achieves 8.8% improvement in offline multi-step task success rate
- Online evaluation shows top task success rate of 70.5% with vLLM deployment
- Process reweight with γ=0.7, δ=4 achieves 82.8% accuracy vs 76.9% for γ=0.4
- LLM-based difficulty grading achieves 88.3% vs 86.1% for rule-based grading

## Why This Works (Mechanism)

### Mechanism 1: Stepwise Accuracy Reward Enables Early-Stage Exploration
Fine-grained per-tool feedback during early training improves convergence compared to binary whole-trajectory rewards. The reward computes F1-score per tool call, averaging partial successes, then switches to binary accuracy after the first epoch to prevent gaming. Evidence shows this combination achieves peak reward versus either alone (82.8% accuracy).

### Mechanism 2: Process Reweight Emphasizes Discriminative Later Steps
Later interaction steps receive higher loss weighting because they involve more complex pages and greater task differentiation. A sigmoid-based weight curve assigns weight θ(i) = γ + (1-γ)σ(2δi/(n-1) - δ) to step i, with early steps starting at γ=0.7 and final steps reaching 1.0. This achieves 82.8% vs 76.9% for γ=0.4.

### Mechanism 3: Level Reweight Focuses Learning on Challenging Samples
Difficulty-weighted loss amplifies gradients from hard examples, accelerating capability acquisition. Samples are graded easy/moderate/difficult based on an advanced MLLM's success rate (100%→easy, 20-80%→moderate, <20%→difficult). The GRPO objective is scaled by w_level ∈ {1.0, 1.1, 1.2} respectively, causing the policy to deviate faster from the reference model on hard samples.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: RISK-R1 uses GRPO as its RL backbone, computing advantages by group-normalizing rewards across G samples per prompt
  - Quick check: Given rewards [0.2, 0.6, 0.4] for 3 rollouts, what is the advantage for the second rollout? (Answer: (0.6 - 0.4) / 0.2 = 1.0)

- **Concept: DOM-Tree Grounding vs Coordinate Prediction**
  - Why needed: Unlike GUI-R1/GUI-G1 that predict (x,y) coordinates, RISK uses element indices in the DOM tree with tool calls
  - Quick check: Why would DOM-tree grounding fail where coordinate prediction succeeds? (Answer: When elements lack unique indices, are dynamically reindexed, or when target is a pixel region not corresponding to a DOM node)

- **Concept: Reward Composition in Multi-Objective RL**
  - Why needed: R = α·R_format + β·θ·R_step_acc combines format correctness with accuracy, scaled by process weight
  - Quick check: What happens if α is set too high (e.g., 0.5)? (Answer: Model may prioritize syntactic correctness over functional accuracy, producing well-formed but wrong actions)

## Architecture Onboarding

- **Component map**: Data Layer (RISK-Data + RISK-Bench) -> SFT Stage (1 epoch) -> RFT Stage (6 epochs, single-step only) -> Deployment (vLLM + Browser-Use) -> Evaluation (offline + online)

- **Critical path**: 
  1. Verify RISK-Data format matches expected schema (screenshot + DOM tree + action history in each JSONL line)
  2. Run SFT for 1 epoch and validate format reward >0.95 on held-out samples
  3. Initialize RFT with α=0.1, β=0.9, γ=0.7, δ=4, KL coeff=0.04
  4. Monitor KL divergence; should increase initially (exploration) then stabilize
  5. Evaluate checkpoint at epoch 1 (stepwise→binary switch) and epoch 6

- **Design tradeoffs**:
  - Single-step only in RFT vs multi-step generalization: Authors acknowledge GPU memory limits prevent multi-step RFT
  - MLLM-based difficulty grading vs human annotation: Cheaper but may inherit model biases; rule-based (tool count) performed worse (-0.6% single-step)
  - DOM-tree actions vs coordinate grounding: Better deployment alignment with Browser-Use but may struggle on pages with poor DOM structure

- **Failure signatures**:
  - Low format reward: Model generates malformed JSON; increase α or add format-specific SFT data
  - High early-step accuracy, low late-step accuracy: Process reweight γ too high (early steps over-weighted) or δ too low (curve too flat)
  - Online success >> offline accuracy: Distribution shift; RISK-Bench may not cover encountered page types
  - KL divergence collapses to 0: Policy not exploring; reduce KL coefficient β or increase learning rate

- **First 3 experiments**:
  1. Ablate process reweight: Train with γ=1.0 (uniform weighting) and compare multi-step success rate to γ=0.7 baseline
  2. Test difficulty grading alternatives: Compare LLM-based grading vs random assignment vs human annotation on a 100-sample subset
  3. Memory-efficient multi-step RFT: Implement gradient checkpointing or sample truncation to include multi-step trajectories in RFT

## Open Questions the Paper Calls Out

### Open Question 1
Does training on full multi-step trajectories during RFT yield superior long-horizon decision-making capabilities compared to the current approach of using only single-step trajectories? The authors note that RFT currently utilizes only single-step trajectories due to GPU memory constraints, which "may limit the model's ability to fully learn multi-step decision-making processes."

### Open Question 2
Can an online reinforcement learning framework outperform the offline RFT approach (RISK-R1) in handling the dynamic and diverse nature of real-world e-commerce websites? The authors suggest that "an online reinforcement learning framework could be more effective" than the current offline framework.

### Open Question 3
How robust is the "Level Reweight" mechanism regarding the accuracy and potential bias of the external MLLM (Qwen-VL-Max) used for difficulty grading? The method relies on a specific MLLM's accuracy to assign weights, but the paper does not analyze how errors in this external model's assessment might propagate.

## Limitations

- Data specificity: RISK dataset focuses on e-commerce risk management tasks, limiting applicability to other domains
- Hardware requirements: RFT requires 8×H200 GPUs, making replication challenging
- Multi-step refinement gap: Decision to exclude multi-step trajectories from RFT means sequential reasoning capabilities are primarily shaped by SFT
- Difficulty grading assumptions: Using MLLM accuracy as proxy for task difficulty may not generalize across different agent architectures

## Confidence

- **High confidence**: Format reward effectiveness and stepwise accuracy mechanism - supported by clear quantitative improvements and intuitive alignment with early exploration needs
- **Medium confidence**: Process reweight effectiveness - while sigmoid weighting shows improvements, assumption that later steps are universally more discriminative may not hold across all task types
- **Medium confidence**: Level reweight contribution - 88.3% vs 86.1% improvement is modest and may be influenced by MLLM grading biases

## Next Checks

1. Ablate process reweight: Train with uniform weighting (γ=1.0) and compare multi-step success rate to γ=0.7 baseline; expect degradation in later-step accuracy if the mechanism is valid

2. Validate difficulty grading: Compare LLM-based grading vs human annotation on a 100-sample subset; measure correlation with final performance to assess whether MLLM accuracy truly captures task difficulty

3. Test memory-efficient multi-step RFT: Implement gradient checkpointing or sample truncation to include multi-step trajectories in RFT; evaluate if the offline multi-step gap (8.8% improvement) increases further with RL fine-tuning on longer sequences