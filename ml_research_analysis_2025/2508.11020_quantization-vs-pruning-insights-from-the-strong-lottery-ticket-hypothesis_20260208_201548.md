---
ver: rpa2
title: 'Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis'
arxiv_id: '2508.11020'
source_url: https://arxiv.org/abs/2508.11020
tags:
- network
- log2
- theorem
- neural
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work bridges the gap between strong lottery ticket hypothesis
  (SLTH) theory and finite-precision quantization. While prior SLTH results focus
  on continuous weights, the authors extend these guarantees to quantized settings
  by leveraging the Number Partitioning Problem (NPP) and its phase transition properties.
---

# Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis

## Quick Facts
- arXiv ID: 2508.11020
- Source URL: https://arxiv.org/abs/2508.11020
- Reference count: 40
- Primary result: Establishes that quantized neural networks can be exactly recovered by pruning randomly-initialized overparameterized networks, with optimal bounds on overparameterization requirements as a function of quantization precision.

## Executive Summary
This work bridges the gap between strong lottery ticket hypothesis (SLTH) theory and finite-precision quantization by extending SLTH guarantees to quantized settings. The authors leverage the Number Partitioning Problem (NPP) and its phase transition properties to provide exact representation guarantees for quantized target networks through pruning. They introduce new bounds for the quantized Random Subset Sum Problem (RSSP) that enable precise characterization of the relationship between quantization precision and overparameterization requirements. The results show that a depth-2ℓ network with width O(d log(1/δ)) can be pruned and quantized to exactly match any δt-quantized target network of width d and depth ℓ, with probability at least 1 − Nt O((log²(1/δ))⁻¹/⁷).

## Method Summary
The method builds on NPP phase transition analysis to derive quantized RSSP bounds, which are then used to establish layerwise weight representation via subset sum problems. The approach involves constructing larger networks with width scaling O(d·log²(1/δ)), solving RSSP within dedicated sample blocks for each target weight, and applying union bounds for network-wide success probability guarantees. The framework applies to mixed-precision settings and demonstrates exact, not approximate, quantized subnetworks. The authors also prove a matching lower bound showing that no network with fewer than Ω(d log²(1/δ)) parameters can represent all d×d quantized linear transformations, establishing the tightness of their result.

## Key Results
- A depth-2ℓ network with width O(d log(1/δ)) can exactly represent any δt-quantized ℓ-layer target network of width d with probability at least 1 − Nt O((log²(1/δ))⁻¹/⁷)
- Network depth can be reduced to ℓ+1 at the cost of an additional log(1/δ) factor in width
- Lower bound proves no network with fewer than Ω(d log²(1/δ)) parameters can represent all d×d quantized linear transformations
- Results apply to mixed-precision settings and show exact representation, not just approximation

## Why This Works (Mechanism)

### Mechanism 1: Quantized Random Subset Sum Problem (RSSP) for Exact Weight Representation
- Claim: A target quantized weight w can be exactly represented as a sum of a pruned subset of randomly quantized values from the larger network.
- Mechanism: Given n samples uniformly drawn from precision-δ values, if n > O(log²(1/δ)), the RSSP with target w has a solution with probability 1 - O((log²(1/δ))⁻¹/⁷). Pruning selects the subset S such that Σᵢ∈S Zᵢ = w exactly.
- Core assumption: δ²_in ≤ δ² ≤ δ_t (target precision is coarser than the square of the initial precision); samples can be rejection-sampled to uniform distribution.
- Evidence anchors:
  - [abstract] "we demonstrate that, in the quantized setting, the analogous class of target discrete neural networks can be represented exactly"
  - [section 4.2, Lemma 5] "∀ w ∈ S⁺_δ, ∃ s₁ ∈ {0,1}ⁿ : w = bᵀ s₁ ⊙ a⁺"
  - [corpus] Weak direct support; related SLTH papers focus on continuous weights or binary cases, not this precision-aware construction.
- Break condition: When κ = log₂M/n ≥ 1, the RSSP solution probability drops sharply (phase transition); insufficient samples given precision.

### Mechanism 2: Number Partitioning Problem (NPP) Phase Transition Bounds
- Claim: Sharp probability bounds for RSSP derive from NPP phase transition results, enabling precise overparameterization characterization.
- Mechanism: Borgs et al. [2001] showed NPP exhibits a phase transition at κ=1. By transforming RSSP to NPP (Lemma 2: RSSP solvable iff NPP solvable with target Λ-2t), the moment estimates E[Iₙ,z] and E[I²ₙ,z] yield upper/lower bounds on P(Zₙ,z > 0) via Markov and Cauchy-Schwarz inequalities.
- Core assumption: Elements sampled uniformly; target t = O(M) (targets bounded relative to sample range).
- Evidence anchors:
  - [section 3.1] "defining κ := log₂M/n, if κ < 1 then O(2ⁿ) number of solutions exist, whereas if κ > 1 the number of solutions sharply drops to zero"
  - [section 3.2, Lemma 3] Explicit probability bounds derived from NPP moment estimates
  - [corpus] No direct corpus coverage of NPP-SLTH connection; this appears novel.
- Break condition: Non-uniform sampling distributions; correlated (non-independent) samples.

### Mechanism 3: Hierarchical Construction (Weight → Neuron → Layer → Network)
- Claim: A 2ℓ-layer network of width O(d log(1/δ)) can exactly represent any δ_t-quantized ℓ-layer target network of width d through layer-wise pruning.
- Mechanism: Each target weight uses an independent block of C·log²(1/δ) random samples (Lemma 5). Blocks are arranged diagonally to avoid interference (Lemma 6). Union bound over all N_t parameters yields network-wide success probability 1 - N_t·O((log²(1/δ))⁻¹/⁷).
- Core assumption: ReLU activation allowing decomposition wx = σ(wx) - σ(-wx); layer outputs can be quantized to γ without breaking downstream representation.
- Evidence anchors:
  - [section 4.2, Theorem 1] "with probability at least 1 - N_t·O((log²(1/δ))⁻¹/⁷)"
  - [section 4.2, proof sketch] "representing a full network by applying Lemma 7 layer by layer"
  - [corpus] Burkholz [2022a] provides the depth-reduction construction referenced in Theorem 2.
- Break condition: Activation functions without ReLU's positive/negative decomposition property; insufficient independence between weight representations.

## Foundational Learning

- Concept: **Random Subset Sum Problem (RSSP)**
  - Why needed here: Core mathematical engine enabling the proof that random weights can sum to arbitrary quantized targets via pruning.
  - Quick check question: Given n=100 samples from {-1, -0.5, 0, 0.5, 1}, can you probabilistically bound the chance of finding a subset summing to target t=0.375?

- Concept: **Phase Transitions in Combinatorial Optimization**
  - Why needed here: Provides the sharp threshold (κ<1 vs κ≥1) that determines when solutions exist with high probability, enabling optimal bounds.
  - Quick check question: If M=1024 and n=50, compute κ=log₂M/n. Is this above or below the critical threshold?

- Concept: **Mixed-Precision Quantization**
  - Why needed here: The paper's δ-double and δ-triple mixed precision definitions (Definitions 5-6) model practical quantization where different layers have different precisions.
  - Quick check question: For a δ-double mixed precision network, at which points is the quantization operator [·]_γ applied?

## Architecture Onboarding

- Component map:
  ```
  Single Weight (Lemma 5)
       ↓ [diagonal block arrangement]
  Single Neuron (Lemma 6) ← d independent weight constructions
       ↓ [block-diagonal matrix M′]
  Single Layer (Lemma 7) ← d₁×d₂ independent neuron constructions
       ↓ [layer-by-layer application]
  Full Network (Theorem 1) ← union bound over N_t parameters
       ↓ [copying trick]
  Reduced Depth (Theorem 2) ← ℓ+1 layers with log(1/δ) width overhead
  ```

- Critical path:
  1. Verify precision hierarchy: δ²_in ≤ δ² ≤ δ_t
  2. Initialize larger network with width scaling O(d·log²(1/δ))
  3. For each target weight, identify its dedicated sample block
  4. Solve RSSP within each block (binary mask selection)
  5. Apply union bound for end-to-end probability guarantee

- Design tradeoffs:
  - **Depth vs Width**: Theorem 1 uses 2ℓ depth; Theorem 2 reduces to ℓ+1 but adds log(1/δ) width factor.
  - **Precision vs Overparameterization**: Finer target precision (smaller δ_t) requires O(log²(1/δ_t)) more parameters—tight per Corollary 1.
  - **Exact vs Approximate**: Quantized setting yields exact representation; continuous setting (prior work) only achieves ε-approximation.

- Failure signatures:
  - Width below Ω(d·log(1/δ)) → information-theoretically impossible (Theorem 3)
  - κ ≥ 1 → RSSP solution probability collapses
  - Non-quantized targets → framework does not apply; fall back to continuous SLTH bounds

- First 3 experiments:
  1. **Validate RSSP phase transition**: Sample n values uniformly from {-M,...,M}, vary κ=log₂M/n across [0.5, 1.5], measure empirical solution rate vs Lemma 4's prediction.
  2. **Single-neuron reconstruction**: For a 1D target weight w∈S_δ_t, initialize g(x)=[v^T σ(ux)]_γ with 2n=4·log²(1/δ) hidden units, find pruning masks s₁,s₂ via brute-force subset search, verify [wx]_γ = [g_{s₁,s₂}(x)]_δ.
  3. **Layer-wise scaling test**: Fix d=10, vary δ∈{2⁻⁴, 2⁻⁶, 2⁻⁸}, measure minimum width C·d·log²(1/δ) needed for >90% reconstruction success over 100 random target layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the quantized SLTH framework be extended to convolutional, residual, and attention-based architectures?
- Basis in paper: [explicit] Conclusion states: "An immediate open problem is to generalize our techniques to structured architectures—most notably convolutional, residual, and attention-based networks—where weight sharing and skip connections introduce additional combinatorial constraints."
- Why unresolved: Weight sharing in convolutions and skip connections introduce combinatorial structure not captured by current MLP-based analysis using NPP/RSSP.
- What evidence would resolve it: Proven overparameterization bounds for quantized CNNs/Transformers, or impossibility results.

### Open Question 2
- Question: What are the overparameterization requirements under layer-wise mixed precision?
- Basis in paper: [explicit] Conclusion identifies "incorporat[ing] layer-wise mixed precision" as an interesting direction.
- Why unresolved: Theorems 1-3 assume uniform precision δ across all layers, but practical mixed-precision allocates different bit-widths per layer.
- What evidence would resolve it: Bounds relating layer-specific precisions δ₁, δ₂, ..., δₗ to width requirements.

### Open Question 3
- Question: How robust are quantized lottery tickets to stochastic quantization noise during inference?
- Basis in paper: [explicit] Conclusion notes this is "of interest for practical deployment on low-precision hardware accelerators."
- Why unresolved: Current theory guarantees exact representation under deterministic quantization; stochastic noise may break functional equivalence.
- What evidence would resolve it: Probabilistic bounds on functional preservation under noise distributions, or modified width requirements.

### Open Question 4
- Question: Can quantized lottery tickets be identified in polynomial time?
- Basis in paper: [inferred] The paper proves existence via NP-complete Subset Sum but does not address computational complexity of mask construction.
- Why unresolved: RSSP-based arguments provide existence guarantees without polynomial-time algorithms for finding masks.
- What evidence would resolve it: Efficient algorithms with provable success probability, or hardness results.

## Limitations

- The theoretical framework relies heavily on specific assumptions about sampling distributions and quantization precision hierarchies that may not extend to correlated or structured weight distributions.
- The constants in the width bounds (C, C_i) remain unspecified, potentially affecting practical implementation.
- The results apply specifically to ReLU activations with their sign decomposition property, limiting generalization to other activation functions.

## Confidence

- **High Confidence**: The RSSP phase transition mechanism and hierarchical construction approach are well-supported by theoretical proofs and established combinatorial optimization results.
- **Medium Confidence**: The application of NPP phase transition bounds to the SLTH context represents a novel connection with solid theoretical foundation.
- **Low Confidence**: The practical constants and scaling factors required for real-world implementation remain unclear, and the framework's behavior under non-ideal conditions is uncertain.

## Next Checks

1. **Empirical RSSP Phase Transition Validation**: Implement a systematic experiment sampling from {-M,...,M} with varying κ=log₂M/n across [0.5, 1.5], measuring empirical solution rates against Lemma 4's theoretical predictions to verify the sharp threshold behavior.

2. **Single-Weight Reconstruction Accuracy**: For target weights w∈S_δ_t, construct the 2-layer network g(x)=[v^T σ(ux)]_γ with 2n=4·log²(1/δ) hidden units, apply brute-force subset search for pruning masks s₁,s₂, and verify the [wx]_γ = [g_{s₁,s₂}(x)]_δ equality across multiple precision settings.

3. **Layer-Wise Scaling Verification**: Fix d=10 and systematically vary δ∈{2⁻⁴, 2⁻⁶, 2⁻⁸}, measuring the minimum width C·d·log²(1/δ) required to achieve >90% reconstruction success rate over 100 random target layers, comparing against theoretical predictions.