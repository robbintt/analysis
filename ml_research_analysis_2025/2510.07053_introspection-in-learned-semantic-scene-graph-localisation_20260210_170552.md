---
ver: rpa2
title: Introspection in Learned Semantic Scene Graph Localisation
arxiv_id: '2510.07053'
source_url: https://arxiv.org/abs/2510.07053
tags:
- semantic
- localisation
- attention
- class
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how semantics influence localisation performance
  and robustness in a learned self-supervised, contrastive semantic localisation framework.
  After training a localisation network on both original and perturbed maps, a thorough
  post-hoc introspection analysis is conducted to probe whether the model filters
  environmental noise and prioritises distinctive landmarks over routine clutter.
---

# Introspection in Learned Semantic Scene Graph Localisation

## Quick Facts
- arXiv ID: 2510.07053
- Source URL: https://arxiv.org/abs/2510.07053
- Reference count: 31
- One-line primary result: Integrated Gradients and Attention Weights are the most reliable post-hoc explainers for semantic localisation, revealing implicit TF-IDF-like weighting that down-weights frequent objects.

## Executive Summary
This work investigates how semantics influence localisation performance and robustness in a learned self-supervised, contrastive semantic localisation framework. After training a localisation network on both original and perturbed maps, a thorough post-hoc introspection analysis is conducted to probe whether the model filters environmental noise and prioritises distinctive landmarks over routine clutter. Various interpretability methods are validated and a comparative reliability analysis is presented. Integrated gradients and Attention Weights consistently emerge as the most reliable probes of learned behaviour. A semantic class ablation further reveals an implicit weighting in which frequent objects are often down-weighted. Overall, the results indicate that the model learns noise-robust, semantically salient relations about place definition, thereby enabling explainable registration under challenging visual and structural variations.

## Method Summary
The method trains a Graph Neural Network to embed semantic scene graphs into a metric space where place-query similarity enables localisation. The framework uses contrastive training with InfoNCE loss, augmented by class-specific object perturbations to model mobility. Post-hoc introspection employs multiple attribution methods (Integrated Gradients, Attention Weights, Shapley Value Sampling, Saliency) validated through fidelity metrics measuring necessity and sufficiency. Semantic class ablations reveal implicit weighting patterns, with frequent objects like chairs receiving lower importance. The approach isolates semantic contributions by excluding geometric features, focusing on how object-object relations define places.

## Key Results
- Attention Weights and Integrated Gradients consistently outperform other explainers in fidelity tests across multiple node budgets
- Semantic class ablations reveal inverse relationship between class frequency and assigned importance, analogous to TF-IDF weighting
- The model achieves PR-AUC ~0.72 and Recall@1 ~18% in the semantics-only setting, with robustness demonstrated through perturbation-based validation
- Reliability analysis confirms Attention Weights correlate with perturbation-induced performance changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive training on perturbed scene graphs produces embeddings that prioritize semantically distinctive objects over ubiquitous clutter.
- Mechanism: The InfoNCE loss pushes matching place-query pairs together while separating non-matches. By training on perturbed variants where mobile objects (chairs, trash cans) are randomly displaced, the model learns to rely on stable semantic configurations rather than absolute positions. The GATv2 layer then aggregates long-range context, allowing the model to capture object-object relational patterns (e.g., couch-painting pairings) that define places.
- Core assumption: Semantic class labels provide sufficient signal for place discrimination even without geometric features.
- Evidence anchors:
  - [abstract] "A semantic class ablation further reveals an implicit weighting in which frequent objects are often down-weighted."
  - [Section IV-B] "For data augmentation, object nodes are displaced according to class-specific mobility... thereby generating additional query–target pairs and reducing overfitting to the original layout."
  - [Section VI-D] "The results thus indicate an inverse relationship between class frequency and assigned importance, analogous to TF–IDF weighting."
  - [corpus] Related work (USS-Nav) confirms semantic scene graphs enable lightweight navigation, but does not directly validate the TF-IDF-like weighting mechanism.
- Break condition: If your environment lacks distinctive semantic objects (e.g., uniform warehouse with identical shelves), the frequency-based weighting may fail to find reliable anchors.

### Mechanism 2
- Claim: Attention weights from GATv2 convolutions provide reliable post-hoc explanations of localisation decisions when validated against perturbation-based importance.
- Mechanism: The model's 3-head GATv2 layer computes attention coefficients between nodes. These coefficients are retained after training and can be extracted without additional computation. The paper validates that attention shifts correlate with PR-AUC degradation under class ablation (positive correlation in Figure 4), satisfying the reliability criterion from prior work.
- Core assumption: Attention-head importance aligns with true feature importance, which requires empirical verification per domain.
- Evidence anchors:
  - [abstract] "Integrated Gradients and Attention Weights consistently emerge as the most reliable probes of learned behaviour."
  - [Section V-C] "Attention Weights, in particular, offer the added benefits of clear interpretability and minimal computational overhead – since they are part of the localisation network itself."
  - [Section VI-B] "Figure 4 verifies that Attention Weights correlate with perturbation-induced changes in localisation performance."
  - [corpus] No direct corpus validation; related SEM-GAT work cited but not in neighbor corpus.
- Break condition: If attention heads do not correlate with ablation performance in your domain, treat attention as hypothesis-generating rather than explanatory.

### Mechanism 3
- Claim: Integrated Gradients provides higher-fidelity node-importance attribution than gradient-based or perturbation-based alternatives alone.
- Mechanism: Integrated Gradients computes attribution by integrating gradients along a path from a baseline (zero embedding) to the input. This satisfies axiomatic properties (sensitivity, implementation invariance) that pure saliency lacks. Fidelity tests (necessity/sufficiency) show IG's top-ranked nodes are both indispensable (removal drops similarity) and sufficient (retaining them preserves similarity).
- Core assumption: The baseline choice (zero) is appropriate for scene graph inputs; alternative baselines may yield different attributions.
- Evidence anchors:
  - [Section V] "Integrated Gradients consistently achieves the highest characterisation scores, indicating that its top-ranked nodes are both indispensable... and sufficient."
  - [Section VI-C] "Integrated Gradients and Attention Weights are the most effective explainers... their rankings align most closely with the class ablation."
  - [corpus] Weak/absent – no corpus papers validate IG fidelity for scene graphs specifically.
- Break condition: If your scene graphs have different sparsity patterns or semantic distributions, re-validate fidelity before trusting IG rankings.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE/Triplet Loss)**
  - Why needed here: The entire localisation framework depends on learning a metric space where place-query similarity reflects true correspondence. Without understanding how contrastive losses structure embeddings, you cannot diagnose retrieval failures.
  - Quick check question: Given three place embeddings A, B, C, can you explain why InfoNCE would push A closer to B than C if (A,B) is a positive pair?

- **Concept: Graph Neural Networks (Message Passing + Attention)**
  - Why needed here: The model uses MPNN layers followed by GATv2. Understanding how neighborhood aggregation works is essential for debugging why certain objects influence place embeddings.
  - Quick check question: If you remove all edges between place nodes and object nodes, what happens to the place embeddings after one message-passing layer?

- **Concept: Attribution Methods (Saliency vs. Integrated Gradients vs. Shapley)**
  - Why needed here: The paper's core contribution is comparing explainability methods. You need to understand why IG satisfies axiomatic properties that vanilla gradients do not.
  - Quick check question: Why does Integrated Gradients require a baseline input, and how would changing the baseline affect attributions?

## Architecture Onboarding

- **Component map:**
  Input: Hierarchical scene graph (L2 objects + L3 places) with visibility edges (object↔place) and traversability edges (place↔place)
  Encoder: Linear projection (64-dim) → ELU → 2× MPNN (sum aggregation, tanh + batchnorm) → GATv2 (3 heads) → Linear (32-dim embedding)
  Loss: InfoNCE (τ=0.7) for contrastive training
  Explainers: Saliency, Integrated Gradients, Shapley Value Sampling, Attention Weights (post-hoc)
  Evaluation: PR-AUC, F1, Recall@N for retrieval; fidelity+ / fidelity− for explainer reliability

- **Critical path:**
  1. Scene graph construction (Hydra) → 2. Perturbation augmentation (class-specific displacement) → 3. Contrastive training (map + perturbed query) → 4. Embedding extraction → 5. Post-hoc attribution → 6. Fidelity validation

- **Design tradeoffs:**
  - Semantics-only vs. geometry: The model intentionally discards geometry to isolate semantic contributions. This caps absolute recall (~18% Recall@1) but enables clean attribution analysis.
  - Attention vs. Integrated Gradients: Attention is computationally free but less reliable at higher node budgets; IG requires gradient computation but provides better fidelity.
  - Perturbation strategy: Random displacement of mobile objects during training may not reflect real-world object motion distributions.

- **Failure signatures:**
  - Low Recall@1 with high PR-AUC: Likely label aliasing (multiple locations have similar object distributions). Check class frequency per place.
  - Explainer disagreement: If saliency and IG diverge significantly, suspect gradient saturation or poor baseline choice.
  - Attention-performance decoupling: If attention shifts do not correlate with ablation performance, re-evaluate whether attention is informative for your graph structure.

- **First 3 experiments:**
  1. **Baseline replication:** Train on uHumans2 office with InfoNCE loss, reproduce PR-AUC (~0.72) and Recall@1 (~18%). Verify similarity matrix shows diagonal structure.
  2. **Ablation sanity check:** Remove all chairs (most frequent class) and confirm minimal PR-AUC drop; remove all couches (rare, distinctive) and confirm larger drop. Compare to Table IV patterns.
  3. **Fidelity validation:** For a held-out query, compute IG attributions, keep only top-ρ nodes, and measure embedding similarity drop. Confirm necessity (high Δ+) and sufficiency (low Δ−) at ρ≈0.3 per Figure 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do Integrated Gradients and Attention Weights retain their ranking as the most reliable introspection methods when evaluated on diverse real-world datasets?
- Basis in paper: [explicit] The conclusion states future work will "evaluate our approach across diverse real-world datasets and semantic taxonomies to stress-test the stability and utility of explanations."
- Why unresolved: The current study is confined to a single synthetic environment (uHumans2 office), leaving the generalizability of the explainability fidelity unproven outside of clean, annotated simulation data.
- What evidence would resolve it: A comparative fidelity analysis (characterisation scores) of various explainers applied to noisy, real-world semantic localisation benchmarks.

### Open Question 2
- Question: Does the introduction of lightweight geometric cues alter the observed implicit down-weighting of high-frequency semantic classes?
- Basis in paper: [explicit] The authors note the "semantics-only setting omits geometry" and propose to "integrate lightweight geometric cues" in future work.
- Why unresolved: It is currently unclear if the model's TF-IDF-like weighting strategy (down-weighting frequent objects like chairs) is a fundamental feature of semantic localisation or a by-product of the lack of geometric anchors.
- What evidence would resolve it: Ablation studies and attribution analyses from a hybrid model that processes both semantic labels and geometric features.

### Open Question 3
- Question: Do more complex model architectures preserve the correlation between Attention Weights and perturbation-based importance?
- Basis in paper: [explicit] The paper lists "explore more complex model architectures" as a direction for future work.
- Why unresolved: The reliability of Attention Weights was validated on a specific, lightweight GATv2 backbone; it remains uncertain if this reliability holds for deeper networks or different attention mechanisms which may suffer from different noise profiles.
- What evidence would resolve it: Fidelity metrics (Δ+ and Δ-) calculated for attention weights in significantly deeper or structurally distinct Graph Neural Networks.

## Limitations

- Training hyperparameters (optimizer, learning rate, batch size, epochs) are unspecified, potentially affecting reproducibility of PR-AUC and Recall@N scores
- Exact perturbation ranges for each semantic class during data augmentation are not fully specified
- GATv2 configuration details (attention dropout, residual connections) remain unclear
- Scene graph construction details for uHumans2 dataset are incomplete

## Confidence

- High confidence: Contrastive training mechanism and InfoNCE loss implementation
- Medium confidence: Attention Weights reliability and semantic class weighting patterns
- Medium confidence: Integrated Gradients fidelity claims based on axiomatic properties
- Low confidence: Absolute performance metrics without complete hyperparameter specification

## Next Checks

1. **Hyperparameter sweep**: Systematically vary learning rate, batch size, and training epochs to establish stable performance baselines
2. **Cross-dataset validation**: Apply the introspection framework to a different semantic environment (e.g., home vs. office) to test generalizability of attention-based explanations
3. **Baseline comparison expansion**: Include additional ablations such as geometry-only baselines and hybrid semantic-geometric approaches to isolate semantic contributions more precisely