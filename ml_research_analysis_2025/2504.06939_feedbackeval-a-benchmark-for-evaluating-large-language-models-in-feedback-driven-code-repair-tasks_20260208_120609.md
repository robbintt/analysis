---
ver: rpa2
title: 'FeedbackEval: A Benchmark for Evaluating Large Language Models in Feedback-Driven
  Code Repair Tasks'
arxiv_id: '2504.06939'
source_url: https://arxiv.org/abs/2504.06939
tags:
- code
- feedback
- repair
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FeedbackEval is a benchmark designed to evaluate large language
  models (LLMs) in feedback-driven code repair tasks. It includes 394 coding tasks
  with 3,736 erroneous code instances paired with four types of feedback: test, compiler,
  human, and simple.'
---

# FeedbackEval: A Benchmark for Evaluating Large Language Models in Feedback-Driven Code Repair Tasks

## Quick Facts
- **arXiv ID:** 2504.06939
- **Source URL:** https://arxiv.org/abs/2504.06939
- **Reference count:** 40
- **Primary result:** Claude-3.5 achieves highest average repair accuracy (60.8%) on 394 coding tasks with structured feedback outperforming unstructured feedback.

## Executive Summary
FeedbackEval is a benchmark designed to evaluate large language models on feedback-driven code repair tasks. The benchmark includes 394 coding tasks with 3,736 erroneous code instances, each paired with four feedback types: test, compiler, human, and simple. The study evaluates five LLMs (GPT-4o, Claude-3.5, Gemini-1.5, GLM-4, and Qwen2.5) under single-iteration and iterative repair settings. Results show structured feedback, particularly test feedback, yields the highest repair success rates (61.0% average), while iterative feedback improves performance but with diminishing returns after two to three rounds.

## Method Summary
The benchmark evaluates LLMs on 394 tasks from HumanEval (164) and CoderEval (230) datasets, with 3,736 erroneous code instances generated via rule-based mutation, LLM-driven injection, and incorrect LLM outputs. Four feedback types are provided: test feedback (pytest tracebacks), compiler feedback (pylint), human feedback (simulated by GPT-4o-mini), and simple feedback ("The code is wrong. Please fix it"). Five LLMs are evaluated at temperature=0.3 using Repair@k metric, measuring pass rate after k repair iterations. Experiments run twice for single-iteration and once for multi-iteration settings.

## Key Results
- Claude-3.5 achieves highest average repair accuracy (60.8%) across all feedback types
- Test feedback yields highest success rate (61.0%) while human feedback is least effective (50.5%)
- Iterative feedback improves performance but gains diminish after two to three rounds
- Docstrings, context, and guidelines significantly enhance repair outcomes; CoT and few-shot prompting show minimal effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured test feedback yields higher repair success rates (61.0%) than unstructured human feedback (50.5%).
- Mechanism: Test feedback provides explicit, targeted signals that reduce ambiguity in fault localization. LLMs leverage this as a direct error signal to constrain the search space for correct code.
- Core assumption: LLMs can reliably map explicit test failure signals to necessary code corrections.
- Evidence anchors: Test feedback achieves highest average Repair@1 (61.0%), with Human feedback lowest (50.5%).

### Mechanism 2
- Claim: Iterative feedback improves repair performance, but gains diminish after two to three rounds.
- Mechanism: Each iteration provides updated feedback on the latest code state, addressing obvious errors initially but encountering diminishing returns as remaining errors become semantically complex.
- Core assumption: The LLM correctly integrates new feedback with previous fix attempts.
- Evidence anchors: Performance gains diminish over iterations, typically stabilizing after two to three iterations.

### Mechanism 3
- Claim: Prompt structure containing docstrings, contextual information, and explicit guidelines significantly enhances repair outcomes.
- Mechanism: Docstrings and context provide essential semantic information about intended functionality, aiding repair comprehension. Guidelines constrain the search space.
- Core assumption: The LLM's ability to understand the repair task depends more on semantic context and explicit task boundaries than on reasoning prompts.
- Evidence anchors: Docstrings, context, and guidelines yield substantial improvements; CoT, few-shot, and persona show minimal effect.

## Foundational Learning

### Unit Testing and Tracebacks
- Why needed here: Test feedback is the most effective repair signal. Understanding how tests pass/fail and how to read a Python traceback is critical for interpreting the primary feedback mechanism.
- Quick check question: Given a failing test `assert add(2, 2) == 5` and the code `def add(a, b): return a + b`, what specific information does the resulting traceback provide?

### LLM In-Context Learning
- Why needed here: The study relies on LLMs' ability to use provided context to guide repair without weight updates. Grasping how models condition on this input is key to understanding why context-rich prompts succeed.
- Quick check question: If you provide an LLM with a buggy function and its failing test output, what is the model expected to do without any further training?

### Static Analysis / Linting
- Why needed here: Compiler feedback is generated by pylint. Understanding the types of issues static analysis tools detect is necessary to interpret this feedback type.
- Quick check question: What kind of error would `pylint` flag that a unit test might not catch, and vice-versa?

## Architecture Onboarding

### Component map
Benchmark Core -> Error Injection Engine -> Feedback Generator -> LLM Interface -> Evaluation Harness

### Critical path
1) Select task & erroneous code
2) Generate initial feedback
3) Prompt LLM with code, feedback, and context
4) Execute repaired code
5) If iteration k < max and code fails, generate new feedback and repeat from step 3
6) Record Repair@k

### Design tradeoffs
**Realism vs. Scale**: CoderEval tasks are more realistic but expensive; HumanEval is simpler and faster. **Feedback Fidelity**: Human feedback is simulated by an LLM, not real humans. **Evaluation Cost**: Multi-iteration repair (Repair@3) is significantly more expensive than single-iteration (Repair@1).

### Failure signatures
1) **Stuck Loop**: LLM fix fails for same reason in next iteration
2) **Over-Correction**: LLM fixes original bug but introduces new test failures
3) **Context Ignorance**: LLM ignores docstrings/context, producing semantically wrong repairs

### First 3 experiments
1. **Baseline Single-Iteration**: Run all 5 LLMs on the 394-task subset with Test Feedback to establish baseline for Repair@1
2. **Context Ablation**: Using Claude-3.5 and Test Feedback on 100-task sample, run Repair@1 while systematically removing docstrings, context, and guidelines
3. **Iterative Gain Analysis**: Run Repair@3 with GPT-4o on all HumanEval tasks with Test Feedback, plotting Repair@1 vs Repair@2 vs Repair@3 to visualize plateau effect

## Open Questions the Paper Calls Out
1. **Hybrid Feedback Strategies**: Can combining multiple feedback types (e.g., test + human feedback) achieve higher repair success rates than single feedback types?
2. **Human Feedback Authenticity**: How does simulated human feedback compare to authentic human reviewer feedback in guiding LLM code repair?
3. **Performance Plateau Causes**: What causes the performance plateau after 2-3 repair iterations, and can alternative approaches overcome this limitation?
4. **Cross-Language Generalization**: Do findings generalize to other programming languages beyond Python?

## Limitations
- Human feedback simulation uses GPT-4o-mini rather than actual human reviewers, introducing potential biases
- Benchmark focuses primarily on Python code, limiting generalizability to other languages
- Error injection methods may not comprehensively represent real-world bug patterns

## Confidence
- **High confidence**: Structured feedback effectiveness (test vs human feedback comparison)
- **Medium confidence**: Iterative feedback diminishing returns
- **Medium confidence**: Prompt structure importance

## Next Checks
1. **Human Feedback Validation**: Run a small-scale study with actual human reviewers to compare simulated vs real human feedback quality and repair success rates
2. **Cross-Language Generalization**: Apply the benchmark framework to another programming language (e.g., JavaScript or Java) to test universality of findings
3. **Error Type Analysis**: Categorize repair failures by error type (syntactic, semantic, logical) to identify whether diminishing returns are specific to certain bug categories