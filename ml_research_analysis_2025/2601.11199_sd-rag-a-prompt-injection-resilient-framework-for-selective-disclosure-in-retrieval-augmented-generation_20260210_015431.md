---
ver: rpa2
title: 'SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in
  Retrieval-Augmented Generation'
arxiv_id: '2601.11199'
source_url: https://arxiv.org/abs/2601.11199
tags:
- constraints
- privacy
- prompt
- score
- sensitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SD-RAG, a framework to prevent privacy leaks
  in Retrieval-Augmented Generation (RAG) systems. Unlike existing methods that rely
  on instructing the LLM to avoid disclosing sensitive information, SD-RAG enforces
  security and privacy constraints during the retrieval phase before content reaches
  the generation model.
---

# SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2601.11199
- Source URL: https://arxiv.org/abs/2601.11199
- Reference count: 24
- Up to 58% improvement in privacy score compared to baselines

## Executive Summary
SD-RAG introduces a novel framework for privacy-preserving retrieval in RAG systems by enforcing security constraints during the retrieval phase rather than relying on post-hoc instructions to LLMs. The framework uses a graph-based data model with human-readable constraints and an LLM-based redaction module to sanitize retrieved content before it reaches the generation model. This approach addresses the fundamental vulnerability of instruction-based privacy methods to prompt injection attacks, demonstrating substantial improvements in privacy metrics while maintaining system performance.

## Method Summary
SD-RAG operates by constructing a knowledge graph where nodes represent information entities and edges encode access constraints defined in human-readable format. During retrieval, the system traverses this graph to identify candidate nodes while simultaneously applying constraint validation through an LLM-based redaction module. The redaction module evaluates whether retrieved content satisfies privacy requirements before passing sanitized content to the LLM for generation. This pre-retrieval constraint enforcement fundamentally differs from existing approaches that attempt to control LLM behavior through system prompts, which remain vulnerable to prompt injection attacks that can override privacy instructions.

## Key Results
- Achieved up to 58% improvement in privacy score compared to baseline instruction-based methods
- Demonstrated resilience to prompt injection attacks through constraint enforcement at retrieval time
- Maintained generation quality while preventing unauthorized disclosure of sensitive information

## Why This Works (Mechanism)
SD-RAG works by shifting the privacy enforcement boundary from the generation phase to the retrieval phase. By constructing a graph-based constraint model that operates before content reaches the LLM, the framework prevents sensitive information from ever being exposed to the generation model. The LLM-based redaction module acts as a gatekeeper, evaluating retrieved content against human-readable constraints in real-time. This architectural separation creates a defense-in-depth approach where even if prompt injection attacks succeed in manipulating the generation prompt, they cannot access information that was never retrieved due to constraint violations.

## Foundational Learning
- **Graph-based constraint modeling**: Represents access policies as traversable graph structures where edges encode permissions and restrictions
  - Why needed: Traditional text-based approaches cannot efficiently represent complex hierarchical access rules
  - Quick check: Can the system represent "managers can see employee salaries, but HR can see all compensation data" as a traversable constraint

- **LLM-based redaction validation**: Uses language models to evaluate whether retrieved content satisfies human-readable constraints
  - Why needed: Manual constraint checking becomes infeasible at scale; LLMs can reason about complex policy expressions
  - Quick check: Does the redaction module correctly identify when "confidential" content is being accessed by unauthorized roles

- **Pre-retrieval privacy enforcement**: Validates access constraints before content reaches the generation model
  - Why needed: Post-retrieval filtering is vulnerable to prompt injection that can override filtering instructions
  - Quick check: Can an attacker retrieve sensitive information by crafting prompts that bypass retrieval constraints

## Architecture Onboarding

**Component Map**: Query -> Graph Traversal Engine -> LLM Redaction Module -> Generation Model

**Critical Path**: User query → Graph constraint evaluation → Content sanitization → LLM generation

**Design Tradeoffs**: 
- Real-time graph traversal vs. precomputed access matrices (favors flexibility over raw speed)
- LLM-based redaction vs. rule-based filtering (favors handling complex constraints vs. deterministic performance)
- Pre-retrieval vs. post-retrieval privacy (favors security over simplicity)

**Failure Signatures**: 
- High false positives: Overly restrictive constraints block legitimate access
- High false negatives: Insufficient constraint granularity allows unauthorized access
- Performance degradation: Graph traversal overhead exceeds query latency requirements

**First Experiments**:
1. Test constraint evaluation accuracy on synthetic access patterns with known ground truth
2. Measure end-to-end latency impact of graph traversal on query response times
3. Validate prompt injection resilience by attempting to bypass constraints through crafted queries

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns for enterprise knowledge bases with millions of nodes and complex constraint hierarchies
- Reliance on LLM reasoning capabilities that may vary across model architectures and versions
- Limited real-world adversarial testing against diverse prompt injection attack vectors

## Confidence
High confidence in core technical contribution: The selective disclosure mechanism through graph-based constraints is technically sound and represents a meaningful advancement over instruction-based privacy approaches. The experimental methodology for measuring privacy improvements is rigorous and reproducible.

Medium confidence in scalability claims: While the framework works well on tested datasets, the computational complexity analysis and performance on large-scale deployments remains incomplete. The paper acknowledges this limitation but does not provide sufficient empirical evidence for enterprise-scale scenarios.

Low confidence in adversarial robustness claims: The prompt injection resilience testing appears limited in scope. Without comprehensive adversarial evaluation across diverse attack patterns and real-world scenarios, the security claims remain provisional.

## Next Checks
1. **Scalability benchmark**: Deploy SD-RAG on a large-scale knowledge base (minimum 1M nodes) and measure end-to-end latency, memory usage, and constraint evaluation performance under varying query loads and constraint complexity scenarios.

2. **Adversarial robustness testing**: Conduct comprehensive red teaming exercises with security researchers to test SD-RAG against diverse prompt injection attacks, including context-aware attacks, multi-turn attacks, and attacks specifically designed to bypass graph-based constraints.

3. **Semantic fidelity evaluation**: Implement a systematic study measuring information loss and semantic drift introduced by the LLM-based redaction module across different content types (technical documentation, medical records, financial data) using both automated metrics (ROUGE, BERTScore) and human evaluation.