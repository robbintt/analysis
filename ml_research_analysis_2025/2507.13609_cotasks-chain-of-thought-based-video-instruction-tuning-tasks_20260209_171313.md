---
ver: rpa2
title: 'CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks'
arxiv_id: '2507.13609'
source_url: https://arxiv.org/abs/2507.13609
tags:
- video
- cotasks
- frame
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CoTasks introduces a structured, chain-of-thought-based framework
  for video instruction tuning by decomposing high-level video questions into four
  foundational tasks: frame localization, entity tracking, spatial relation extraction,
  and temporal relation extraction. Using object-level annotations from existing video
  QA datasets, CoTasks provides intermediate reasoning steps that guide models to
  perform fine-grained, object-centric spatiotemporal reasoning.'
---

# CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks

## Quick Facts
- arXiv ID: 2507.13609
- Source URL: https://arxiv.org/abs/2507.13609
- Reference count: 18
- Primary result: Qwen2.5-VL-3B achieves +17.4 GPT-4 score gain with CoTasks

## Executive Summary
CoTasks introduces a structured, chain-of-thought-based framework for video instruction tuning by decomposing high-level video questions into four foundational tasks: frame localization, entity tracking, spatial relation extraction, and temporal relation extraction. Using object-level annotations from existing video QA datasets, CoTasks provides intermediate reasoning steps that guide models to perform fine-grained, object-centric spatiotemporal reasoning. Experiments on the NeXT-QA and STAR benchmarks show that CoTasks significantly improve inference performance across state-of-the-art VideoLLMs, with Qwen2.5-VL-3B achieving a +17.4 point gain in GPT-4 evaluation score, and large improvements in causal (+14.6), temporal (+10.9), and descriptive (+48.1) reasoning subcategories.

## Method Summary
CoTasks constructs a dataset by merging video QA datasets (NeXT-QA, STAR) with object-level annotations (VidOR) and decomposing each video question into four subtasks: (1) frame localization to identify relevant frames and entities, (2) entity tracking with bounding boxes, (3) spatial relation extraction within frames, and (4) temporal relation extraction across frames. At inference, VideoLLMs are prompted with the original question augmented by ground-truth answers to these subtasks, providing structured CoT-style reasoning context. The approach is evaluated on multiple VideoLLM models (Qwen2.5-VL-3B/7B/72B, LLaVA-video-7B) using GPT-4-based scoring.

## Key Results
- Qwen2.5-VL-3B achieves +17.4 GPT-4 score gain over baseline on NeXT-QA
- Large improvements across reasoning subcategories: causal (+14.6), temporal (+10.9), descriptive (+48.1)
- CoTask prompting alone (+34.3%) outperforms fine-tuning (+20.5%) on STAR for Qwen2.5-VL-3B
- Complementary gains from CoTasks 1-2 vs. 3-4 shown in ablation study

## Why This Works (Mechanism)

### Mechanism 1: Structured Task Decomposition as Chain-of-Thought Supervision
Decomposing complex video questions into four foundational subtasks provides intermediate reasoning scaffolds that guide VideoLLMs toward more grounded answers. Each CoTask targets a distinct reasoning capabilityâ€”frame localization anchors temporal scope, entity tracking grounds object identity, spatial/temporal relation extraction captures interactions. These are embedded as CoT-style context before the final answer, reducing the inference gap between high-level questions and object-level visual evidence.

### Mechanism 2: Object-Centric Grounding Reduces Shallow Answer Generation
Requiring explicit object localization and relation extraction forces models to ground responses in visual evidence rather than relying on linguistic priors. By prompting for bounding boxes, spatial relations, and temporal actions, the model must attend to specific regions and dynamics before answering, mitigating "hallucinated" or overly generic outputs.

### Mechanism 3: Inference-Time Augmentation Without Retraining
CoTasks can be applied as a lightweight, inference-time intervention using ground-truth CoTask answers, isolating the contribution of structured context. The paper augments the original question with pre-constructed CoTask answers at inference, avoiding architectural changes. This acts as a probe for the upper bound of CoT-style grounding benefits.

## Foundational Learning

- Concept: Video Large Language Models (VideoLLMs) Architecture
  - Why needed here: CoTasks operates on top of existing VideoLLMs (Qwen2.5-VL, LLaVA-video); understanding the vision encoder, projection module, and LLM backbone is essential to diagnose grounding failures.
  - Quick check question: Can you explain how video frame features are aligned with the LLM's embedding space in a typical VideoLLM?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoTasks explicitly frames intermediate tasks as CoT-style reasoning steps; familiarity with CoT principles helps understand why decomposition aids compositional reasoning.
  - Quick check question: How does providing intermediate reasoning steps in text prompts affect model behavior in standard NLP tasks?

- Concept: Object-Level Video Annotations (Bounding Boxes, Spatiotemporal Relations)
  - Why needed here: CoTasks construction relies on datasets like VidOR and STAR that provide bounding boxes, spatial relations, and temporal relation instances; understanding annotation schemas is critical for data pipeline integration.
  - Quick check question: What is the difference between intra-frame spatial relations and inter-frame temporal relations in video annotation formats?

## Architecture Onboarding

- Component map:
  Input Layer (Video frames + question) -> CoTasks Constructor (generates A1-A4) -> Inference Augmentation (concatenates A1-A4 with question) -> VideoLLM -> GPT-4 Evaluator

- Critical path:
  1. Align video frames with object-level annotations
  2. Generate CoTask 1 answer using Qwen2.5-VL-72B for NeXT-QA or directly from STAR annotations
  3. Construct CoTask 2 (bounding boxes), CoTask 3 (spatial relations), CoTask 4 (temporal relations) based on A1
  4. At inference, augment question with A1-A4 and prompt target VideoLLM
  5. Evaluate with GPT-4 scorer using Table 2 scoring prompt

- Design tradeoffs:
  - 64-frame sampling balances computational cost and temporal coverage; empirically validated for LLaVA-video but may not generalize to all models or video lengths
  - Using ground-truth CoTask answers at inference provides an upper-bound estimate but is not deployable; real systems need a learned CoTask predictor
  - GPT-4 evaluation introduces an external dependency and potential scoring variability; the paper uses the rubric in Table 2 to standardize

- Failure signatures:
  - Low scores on CoTask 1 propagate errors to all downstream CoTasks (Table 8 shows 17.7% avg accuracy)
  - Descriptive reasoning shows the largest gain (+48.1) but may reflect annotation bias toward countable/locatable entities
  - Small models (Qwen2.5-VL-3B) show the largest relative improvement but start from a lower baseline

- First 3 experiments:
  1. Replicate the inference-time augmentation baseline on NeXT-QA validation set using LLaVA-video-7B with and without CoTasks, comparing GPT-4 scores as in Table 6.
  2. Run per-CoTask difficulty analysis (as in Table 8) on your target VideoLLM to identify which foundational task is the weakest link before attempting full CoTasks integration.
  3. Construct a small CoTask dataset slice (e.g., 100 videos) using available object annotations and test whether a fine-tuned CoTask predictor can approximate ground-truth CoTask answers without significant performance drop.

## Open Questions the Paper Calls Out

- Question: Does fine-tuning VideoLLMs on the CoTasks dataset yield comparable reasoning improvements to the inference-time prompting results reported?
  - Basis in paper: [explicit] The conclusion states, "Future work includes fine-tuning VideoLLMs directly on CoTasks," noting that current experiments rely on inference-time augmentation with ground-truth answers rather than weight updates.
  - Why unresolved: The paper establishes an upper bound of performance using oracle context; it does not confirm if models can learn to generate these reasoning steps autonomously through training.
  - What evidence would resolve it: Evaluation of a VideoLLM fine-tuned on the CoTasks dataset, tested on NeXT-QA without the CoTask prompts provided in the context.

- Question: Can the CoTasks framework be successfully adapted for open-domain video reasoning where explicit object-level annotations are unavailable?
  - Basis in paper: [explicit] The authors identify a limitation that "CoTasks require object-level annotations... which may limit applicability," and list "extending the framework to open-domain video reasoning settings" as future work.
  - Why unresolved: The current construction pipeline depends heavily on aligning questions with pre-existing bounding boxes and relation annotations (e.g., from VidOR), which are scarce in general video datasets.
  - What evidence would resolve it: A method that generates CoTask-style reasoning chains for videos lacking annotation metadata, showing performance gains on datasets like ActivityNet or natural web videos.

- Question: To what extent does error propagation in the initial frame localization (CoTask 1) degrade the accuracy of subsequent spatial and temporal reasoning steps?
  - Basis in paper: [inferred] The authors note in the Limitations section that "since each CoTask builds upon the preceding one, the quality of CoTask 1... directly affects the construction and reliability of subsequent CoTasks."
  - Why unresolved: While the dependency is acknowledged, the specific sensitivity of the pipeline to errors in the first step versus noise in later steps has not been quantified.
  - What evidence would resolve it: An ablation study varying the accuracy of CoTask 1 inputs to measure the resulting performance drop in CoTasks 2, 3, and 4.

## Limitations
- CoTasks evaluation relies on ground-truth CoTask answers at inference, which is not practical for real-world deployment
- Dependency on object-level annotations (VidOR/STAR) that are not universally available and may introduce annotation bias
- GPT-4 evaluation introduces an external dependency and potential scoring variability that is difficult to control or reproduce

## Confidence
- **High confidence**: Structured task decomposition provides intermediate reasoning scaffolds (consistent performance gains across models, clear ablation results)
- **Medium confidence**: Object-centric grounding reduces shallow answer generation (supported by qualitative case studies but limited quantitative validation)
- **Medium confidence**: Inference-time augmentation results (strong performance gains but ground-truth dependency makes this an upper-bound estimate)

## Next Checks
1. Train and evaluate a CoTask predictor model (fine-tuned on the same CoTask training data) to measure performance degradation when using predicted vs. ground-truth CoTask answers at inference.

2. Conduct a controlled experiment varying frame sampling density (e.g., 32 vs. 64 vs. 128 frames) across different video lengths to validate whether the 64-frame assumption holds for diverse VideoLLM architectures and video content types.

3. Perform an annotation bias analysis by comparing CoTasks performance on descriptive vs. causal vs. temporal reasoning subcategories, and test whether the large descriptive reasoning gain (+48.1) reflects genuine capability improvement or dataset-specific annotation patterns.