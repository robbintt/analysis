---
ver: rpa2
title: 'Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity,
  and Provably Efficient Criteria'
arxiv_id: '2508.07102'
source_url: https://arxiv.org/abs/2508.07102
tags:
- meanflow
- definition
- second-order
- circuit
- velocity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Second-Order MeanFlow, an extension of the
  MeanFlow generative modeling framework that incorporates average acceleration fields
  alongside average velocities. The authors establish theoretical foundations across
  three key dimensions: feasibility, expressivity, and computational efficiency.'
---

# Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria

## Quick Facts
- arXiv ID: 2508.07102
- Source URL: https://arxiv.org/abs/2508.07102
- Authors: Yang Cao; Yubin Chen; Zhao Song; Jiahao Zhang
- Reference count: 12
- Primary result: Establishes theoretical foundations for Second-Order MeanFlow extending MeanFlow to learn average acceleration fields alongside velocities

## Executive Summary
This paper introduces Second-Order MeanFlow, an extension of the MeanFlow generative modeling framework that incorporates average acceleration fields alongside average velocities. The authors establish theoretical foundations across three key dimensions: feasibility, expressivity, and computational efficiency. They prove that average acceleration satisfies a generalized consistency condition analogous to first-order MeanFlow, enabling stable one-step sampling. Through circuit complexity analysis, they show the model can be implemented within the TC0 class under mild assumptions. Finally, they demonstrate that fast approximate attention techniques can achieve n^2+o(1) computation time with less than 1/poly(n) approximation error. These results provide a rigorous theoretical foundation for high-order flow matching models that combine richer dynamics with practical sampling efficiency.

## Method Summary
The paper extends MeanFlow to second-order dynamics by learning both average velocity and average acceleration fields along interpolation trajectories. The method uses two networks to predict these fields from a concatenated input of the current state and time endpoints. Training employs Jacobian-Vector Products (JVPs) to compute the average fields efficiently without explicit integral evaluation. Sampling uses a second-order Euler method that incorporates both velocity and acceleration predictions. The approach maintains the consistency properties of first-order MeanFlow while reducing approximation error from O((t-r)^2) to O((t-r)^3), enabling larger integration steps with equivalent accuracy.

## Key Results
- Average acceleration satisfies a generalized consistency condition enabling stable one-step sampling
- Second-order MeanFlow can be implemented in the TC0 circuit complexity class
- Fast approximate attention achieves n^2+o(1) computation time with < 1/poly(n) approximation error

## Why This Works (Mechanism)

### Mechanism 1
Average acceleration fields satisfy a generalized consistency condition that enables stable one-step sampling. The average acceleration $\bar{a}(z_t, r, t) = \frac{1}{t-r}\int_r^t a(z_\tau, \tau)d\tau$ satisfies boundary conditions ($\bar{a}(z_r, r, r) = a(z_r, r)$) and an additive consistency constraint: $(t-r)\bar{a}(z_t, r, t) = (s-r)\bar{a}(z_s, r, s) + (t-s)\bar{a}(z_t, s, t)$. This self-consistency property means the acceleration field can be evaluated at any single timestep to reconstruct the flow trajectory.

### Mechanism 2
Incorporating second-order dynamics reduces approximation error from O((t-r)^2) to O((t-r)^3), enabling larger integration steps with equivalent accuracy. By Taylor expansion, first-order methods approximate $z_t \approx z_r + (t-r)z'_r$ with error $(t-r)^2 z''_r/2 + O((t-r)^3)$. Second-order MeanFlow adds the quadratic term $(t-r)^2 z''_r/2$, reducing error to O((t-r)^3). The sampling update becomes: $z_{t_i} = z_{t_{i-1}} + (t_i - t_{i-1})v(z_{t_{i-1}}, t_i, t_{i-1}) + \frac{1}{2}(t_i - t_{i-1})^2 a(z_{t_{i-1}}, t_i, t_{i-1})$.

### Mechanism 3
The second-order MeanFlow loss can be computed efficiently using Jacobian-Vector Products (JVPs) without explicit integral evaluation, and inference can be accelerated to O(n^{2+o(1)}) via approximate attention. Training uses the identity $\bar{v}(z_t, r, t) = v_t - (t-r)(v_t \partial_z u_\theta + \partial_t u_\theta)$ where $v_t$ is conditional velocity. JVPs avoid double backpropagation. For inference, replacing exact attention with AAttC (approximate attention with polynomial degree $g$ and low-rank matrices $U, V \in \mathbb{R}^{hw \times k}$) achieves O(n^{2+o(1)}) time with error bounded by $O(c^2 k R^{g+2}) \cdot \epsilon$.

## Foundational Learning

- **Flow Matching fundamentals (trajectories, probability paths, velocity fields)**: Second-Order MeanFlow builds directly on Flow Matching's trajectory definition $z_t = \alpha_t x + \beta_t \epsilon$ and marginal velocity $v(z_t, t) = \mathbb{E}[v_t | z_t]$. Without understanding how Flow Matching transforms noise to data via learned velocity fields, the extension to acceleration is opaque. Quick check: Given trajectory $z_t = (1-t)x + t\epsilon$ with $x \sim p_{data}$, what is the conditional velocity $v_t$? (Answer: $v_t = \epsilon - x$)

- **Initial Value Problems (IVPs) and ODE solvers**: Sampling requires solving $d^2 z_t/dt^2 = a(z_t, t)$ with initial conditions. The Euler solver discretization $z_{t_i} = z_{t_{i-1}} + \Delta t \cdot v + \frac{1}{2}\Delta t^2 \cdot a$ is the practical realization of the theory. Quick check: Why does second-order Euler require both velocity and acceleration at each step, while first-order requires only velocity?

- **Circuit complexity class TC0**: The paper proves Second-Order MeanFlow sampling belongs to TC0 (constant-depth threshold circuits), which has implications for what computations the model can and cannot express. Practitioners should understand this indicates potential inherent limitations despite high-order dynamics. Quick check: What does it mean that MeanFlow sampling is in TC0 but potentially not in NC1? (Hint: Consider what problems require sequential computation)

## Architecture Onboarding

- **Component map**: Input: Noise tensor Z_1 → Time embedding: Concatenate [Z_t || t·1_n || r·1_n] → Dual ViT backbone (ViT_1 predicts velocity, ViT_2 predicts acceleration) → Sampling update (Euler): Z_r = Z_t + (r-t)·ViT_1 + ½(r-t)²·ViT_2 → Output: Generated sample Z_0

- **Critical path**: 
  1. Training: Sample (t, r, x, ε) → Compute z_t = α_t x + β_t ε → Compute conditional v_t, a_t → Evaluate JVPs for v̄, ā → Minimize ||v̄ - u₁||² + ||ā - u₂||²
  2. Inference: Start from Z_1 ~ N(μ, σ²I) → Apply T-step Euler updates using both networks → Output Z_0

- **Design tradeoffs**:
  - Two networks vs. shared backbone: Separate u₁, u₂ allow specialization but increase parameters
  - Step count T: Fewer steps possible due to O((t-r)³) error, but extreme compression (T=1) requires perfect consistency
  - Exact vs. approximate attention: AAttC gives O(n^{2+o(1)}) inference but requires bounded weights and introduces ε error per layer

- **Failure signatures**:
  - Exploding loss during training: Likely due to unbounded weights violating R = o(√log n) condition—add weight normalization
  - Oscillatory/artifact-heavy samples: May indicate acceleration network not converging—check if both loss terms are balanced
  - No improvement over first-order: Taylor expansion benefits only manifest with appropriate step sizes; try increasing Δt

- **First 3 experiments**:
  1. Train first-order MeanFlow baseline, verify one-step sampling produces coherent outputs. Then add second-order network with zero initialization to confirm training improves over baseline.
  2. Compare generation quality (FID/IS) for T ∈ {1, 2, 4, 8, 16} steps. Expect second-order to match first-order quality with ~2× fewer steps.
  3. Test different weight clipping/normalization strategies to satisfy R = o(√log n). Measure impact on both exact attention baseline and AAttC accelerated version to quantify approximation error accumulation.

## Open Questions the Paper Calls Out

- **Does Second-Order MeanFlow achieve improved generation quality compared to first-order MeanFlow on large-scale image and video benchmarks?**: The conclusion states: "In future work, we plan to empirically validate Second-Order MeanFlow on large-scale image and video benchmarks." The paper provides only theoretical analysis; no empirical experiments are conducted.

- **Can Second-Order MeanFlow be extended to discrete domains (e.g., text, graphs) while preserving its theoretical guarantees?**: The conclusion mentions plans to "explore extensions such as discrete flow matching." The current formulation assumes continuous trajectories; discrete distributions require fundamentally different interpolation schemes.

- **What adaptive timestepping strategies maximize the efficiency gains from second-order dynamics?**: The conclusion lists "adaptive timestepping" as a planned future direction. The paper uses fixed timesteps; adaptive methods require analyzing how local truncation error (now O((t-r)³)) varies across trajectories.

## Limitations

- Theoretical framework assumes bounded weight norms that may not hold in practice
- Approximation error bounds for fast attention rely on specific polynomial degrees and rank parameters not fully explored empirically
- Consistency conditions rely on trajectory smoothness assumptions that may not extend to highly non-Gaussian data distributions

## Confidence

- **High confidence**: Theoretical consistency conditions for average acceleration, TC0 complexity classification, and JVP-based training efficiency
- **Medium confidence**: Approximation error bounds for fast attention and the claimed computational complexity improvements
- **Low confidence**: Practical performance gains in terms of sample quality and training stability across diverse data modalities

## Next Checks

1. **Empirical consistency verification**: Implement the second-order sampling and verify that one-step and multi-step trajectories converge to the same endpoints, quantifying the gap as training progresses.

2. **Attention approximation study**: Systematically vary polynomial degree and rank parameters in AAttC to measure the trade-off between computational speedup and generation quality (FID) on CIFAR-10/imagenet-32.

3. **Step size sensitivity analysis**: Compare generation quality across different integration step counts (T=1,2,4,8) to verify that second-order methods achieve first-order quality with fewer steps, and determine the practical limits of step compression.