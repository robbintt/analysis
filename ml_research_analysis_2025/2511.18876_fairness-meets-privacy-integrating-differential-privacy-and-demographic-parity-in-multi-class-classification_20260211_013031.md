---
ver: rpa2
title: 'Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity
  in Multi-class Classification'
arxiv_id: '2511.18876'
source_url: https://arxiv.org/abs/2511.18876
tags:
- privacy
- fairness
- differential
- dp2dp
- softmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes DP2DP, a method that integrates differential
  privacy and demographic parity for multi-class classification. The key idea is a
  two-phase pipeline: (1) train a private classifier on labeled data using techniques
  like DP-SGD, and (2) use a post-processing step on unlabeled data to enforce demographic
  parity while preserving privacy.'
---

# Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification

## Quick Facts
- arXiv ID: 2511.18876
- Source URL: https://arxiv.org/abs/2511.18876
- Reference count: 40
- This paper proposes DP2DP, a two-phase pipeline that integrates differential privacy and demographic parity for multi-class classification, achieving O(log(N)/√N) convergence to fairness while maintaining privacy.

## Executive Summary
This paper introduces DP2DP, a method that simultaneously achieves differential privacy and demographic parity in multi-class classification. The approach uses a two-phase pipeline: first training a private classifier on labeled data using techniques like DP-SGD, then applying a post-processing step on unlabeled data to enforce demographic parity while preserving privacy. The post-processing involves privatizing group proportions and optimizing Lagrange multipliers via a smoothed, private objective. Theoretical analysis shows DP2DP converges to demographic parity at a rate of O(log(N)/√N), matching non-private methods up to a logarithmic factor. Experiments on synthetic and real datasets (Adult, Credit Card, Parkinson's) confirm these results, showing state-of-the-art accuracy/fairness/privacy trade-offs.

## Method Summary
DP2DP is a two-phase pipeline that integrates differential privacy and demographic parity for multi-class classification. Phase 1 trains a private classifier on labeled data using DP-SGD or output perturbation to establish (ε₁,δ₁)-DP guarantees. Phase 2 operates on unlabeled data, privatizing group proportions and optimizing Lagrange multipliers via DP-SGD on a smoothed objective. The method achieves ρ-demographic parity with expected unfairness E[U(ĝ_ρ)] ≤ ρ + O(log N/√N), providing explicit convergence rates. The approach leverages the fact that demographic parity depends only on features and sensitive attributes, enabling use of unlabeled data for fairness enforcement.

## Key Results
- DP2DP achieves demographic parity with convergence rate O(log(N)/√N), matching non-private methods up to logarithmic factors
- The method maintains (ε,δ)-differential privacy guarantees through careful composition of privacy budgets from both phases
- Experiments on Adult, Credit Card, and Parkinson's datasets show state-of-the-art accuracy/fairness/privacy trade-offs compared to existing methods
- Theoretical analysis provides explicit bounds on fairness violations and privacy costs, with empirical validation confirming theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating private classifier training from fairness post-processing enables independent privacy accounting and preserves fairness convergence rates up to logarithmic factors.
- **Mechanism:** Phase 1 (DP-SGD or output perturbation) establishes (ε₁,δ₁)-DP guarantees on labeled data Dₙ. Phase 2 (DP2DP post-processing) operates solely on unlabeled data D_N with separate (ε₂,δ₂)-DP guarantees. Parallel composition yields global privacy (max{ε₁,ε₂}, max{δ₁,δ₂})-DP without cross-contamination.
- **Core assumption:** Dₙ and D_N are sampled independently; demographic parity depends only on features X and sensitive attributes S, not labels Y.
- **Evidence anchors:** [abstract]: "two-phase pipeline: (1) train a private classifier... (2) use a post-processing step on unlabeled data"; [Section 3]: "Phases 1 and 2 are independent from a privacy perspective"; [corpus]: Weak direct corpus support.

### Mechanism 2
- **Claim:** Log-sum-exp (LSE_β) smoothing transforms the non-smooth fairness objective into a β-smooth surrogate, enabling gradient-based DP-SGD while introducing bounded approximation error that vanishes exponentially with β→0.
- **Mechanism:** The original objective involves max operators (non-differentiable). LSE_β(z)=β·log(Σexp(z_k/β)) provides a 1/β-smooth approximation. Lemma D.1 shows |softmax_β(x)_i - 1/|A|·1(i∈A)| ≤ O(e^{-γ/β}), where γ is the margin.
- **Core assumption:** Per-sample loss ĥ(·;x,s) has bounded Lipschitz constant (2√2 + ρ√2K) and gradient sensitivity ≤4.
- **Evidence anchors:** [Section 5]: "LSE_β smoothing... induces an exponential term C₂e^{-γ/β}"; [Lemma B.2]: "ĥ(·;x,s) is convex, (2√2+ρ√2K)-Lipschitz"; [corpus]: No corpus papers directly address smoothing for joint privacy-fairness optimization.

### Mechanism 3
- **Claim:** Privatized group proportions π̄_s and DP-SGD-optimized Lagrange multipliers λ̄ jointly enforce ρ-demographic parity with expected unfairness E[U(ĝ_ρ)] ≤ ρ + O(log N/√N).
- **Mechanism:** π̄_s = π̂_s + N(0,σ²_π) privatizes sensitive group frequencies (sensitivity 1/N). DP-SGD optimizes Lagrange multipliers over smoothed objective Ĥ_β with per-step noise σ_SGD. Theorem 5.1 decomposes unfairness into: (1) baseline relaxation ρ + C₁/√N from finite samples, (2) smoothing error C₂e^{-γ/β}, (3) privatization noise term ∝ σ_SGD/√(bT).
- **Core assumption:** Minimum group size π_min > 0; bounded domain for λ ∈ [0,C_λ]^{2K}; Lipschitz per-sample loss.
- **Evidence anchors:** [Corollary 5.1]: "E[U(ĝ_ρ)] ≤ ρ + C* log N / √N"; [Section 4]: "The privacy loss of DP2DP naturally decomposes into two components"; [corpus]: Related work (Mangold et al. 2023) bounds fairness loss but lacks explicit fairness guarantees.

## Foundational Learning

### Differential Privacy (DP-SGD, Rényi DP, Gaussian Mechanism)
- **Why needed here:** Phase 1 and Phase 2 both require precise privacy accounting. DP-SGD with Rényi DP provides tight composition for iterative training; Gaussian mechanism privatizes group proportions.
- **Quick check question:** Given per-sample gradient sensitivity ∆₂=4, batch size b=128, N=10000, T=100, and target (ε=0.5, δ=10⁻⁵)-DP, what noise scale σ_SGD is needed? (Answer: Use RDP accountant from dp-accounting library.)

### Demographic Parity and Fairness Metrics
- **Why needed here:** The paper targets ρ-demographic parity: U(g)=max_k |P₁[g(X,S)=k] - P₋₁[g(X,S)=k]| ≤ ρ. This is a group fairness notion independent of labels, enabling use of unlabeled D_N.
- **Quick check question:** Why can demographic parity be enforced on unlabeled data, but equalized odds cannot? (Answer: Demographic parity U(g) depends only on P(g(X,S)|S), not on Y; equalized odds requires P(g(X,S)|Y,S).)

### Lagrangian Optimization and Constrained ERM
- **Why needed here:** The fairness constraint U(g)≤ρ is enforced via Lagrangian relaxation: min_λ Ĥ_β(λ) where λ ∈ [0,C_λ]^{2K} are dual variables.
- **Quick check question:** If λ̄⁽¹⁾_k >> λ̄⁽²⁾_k for some class k, what happens to predictions for group s=+1 vs s=-1? (Answer: For s=+1, term is -1·(λ̄⁽¹⁾_k - λ̄⁽²⁾_k) → strongly penalizes class k for s=+1; for s=-1, term is +1·(λ̄⁽¹⁾_k - λ̄⁽²⁾_k) → boosts class k for s=-1, rebalancing predictions.)

## Architecture Onboarding

### Component Map
```
Dₙ (labeled) ──Phase 1──► p̄₁...p̄_K (DP class probabilities, (ε₁,δ₁)-DP)
                                              │
D_N (unlabeled) ──Phase 2 (DP2DP)────────────┤
  │                                           │
  ├─► π̂_s (group freqs) ──+N(0,σ²_π)──► π̄_s (privatized)
  │                                           │
  └─► Sample minibatches ──DP-SGD on Ĥ_β──► λ̄ (Lagrange multipliers, (ε₂,δ₂)-DP)
                                              │
                                              ▼
                            ĝ_ρ(x,s) = argmax_k[π̄_s·p̄_k - s(λ̄⁽¹⁾_k - λ̄⁽²⁾_k)]
                            (Final fair+private classifier)
```

### Critical Path
1. **Phase 1 implementation:** Train DP-SGD or apply output perturbation to get p̄_k; calibrate noise for target (ε₁,δ₁).
2. **Phase 2 noise calibration:** Compute σ_π and σ_SGD using RDP accountant (dp-accounting library) for target (ε₂,δ₂) given N, T, b, sensitivity.
3. **Smooth objective implementation:** Implement LSE_β smoothing and gradient computation; verify Lipschitz/smoothness bounds (Lemma B.2) with unit tests.
4. **DP-SGD loop:** Implement minibatch sampling, gradient clipping, noise addition, and projection onto [0,C_λ]^{2K}.
5. **End-to-end validation:** Compute empirical fairness Û(ĝ_ρ) and privacy ε on held-out test set; compare to theoretical bounds.

### Design Tradeoffs
- **β (smoothing) vs. σ_SGD (privacy noise):** Small β → better approximation, higher gradient sensitivity → larger σ_SGD needed for same privacy → slower convergence. Paper uses β=10⁻⁵.
- **T (iterations) vs. N (unlabeled samples):** T=N² optimal for rate O(log N/√N), but computationally expensive.
- **Batch size b:** Larger b → lower gradient variance but higher per-step privacy cost; smaller b → more iterations needed for convergence.
- **C_λ (Lagrange multiplier bound):** Too small → insufficient fairness enforcement; too large → optimization instability.

### Failure Signatures
- **Unfairness Û(ĝ_ρ) >> ρ + O(log N/√N):** (1) Check if N is too small (<1000); (2) verify group balance (π_min may be tiny); (3) check if β is too large (approximation error); (4) verify DP-SGD converged.
- **Privacy budget ε computed >> target:** (1) Check sensitivity bounds; (2) verify T, b, N match accountant inputs; (3) ensure no accidental data leakage.
- **Accuracy drops sharply with privacy:** (1) Check if Phase 1 model is too weak; (2) verify λ̄ values aren't extreme; (3) check if data is highly unfair intrinsically.
- **Fairness-accuracy trade-off worse than baselines:** (1) Compare to non-private DP2DP; (2) check if LSE_β smoothing is implemented correctly; (3) verify baseline methods use same privacy budget.

### First 3 Experiments
1. **Synthetic validation (reproduce Figure 1):** Generate data with p∈{0.5,0.75,0.99}, N=10000, K=6. Run non-private DP2DP vs. Denis et al. (2024) to verify smoothing error is negligible. Then run private DP2DP with (ε=0.46,δ=10⁻⁵) to verify unfairness gap to non-private is O(log N/√N).
2. **Privacy calibration sanity check:** Fix N=10000, T=100, b=128. Compute σ_π and σ_SGD for target ε∈{0.5,1.0,2.0} using dp-accounting. Run DP2DP and empirically estimate ε via membership inference attacks.
3. **Ablation on β and T:** On Adult dataset (N≈50000), fix ε=0.5. Vary β∈{10⁻³,10⁻⁴,10⁻⁵,10⁻⁶} and T∈{100,1000,10000}. Measure unfairness Û(ĝ_ρ) and accuracy. Identify the Pareto frontier; verify that T=N² is indeed near-optimal.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the DP2DP framework be extended to handle multiple sensitive attributes simultaneously without degrading convergence rates?
- **Basis in paper:** [explicit] Section 7: "handling multiple sensitive attributes is a natural next step, as it would broaden the applicability of our method."
- **Why unresolved:** The current theoretical analysis (e.g., Lemma B.2, Theorem 5.1) relies on a binary sensitive attribute set $\mathcal{S} = \{-1, 1\}$.
- **What evidence would resolve it:** A theoretical extension of the gradient sensitivity and fairness bounds covering $M > 2$ sensitive groups.

### Open Question 2
- **Question:** How can this pipeline be adapted to enforce Equalized Odds while maintaining the established privacy guarantees?
- **Basis in paper:** [explicit] Section 7: "we aim to generalize our approach to other fairness notions, such as Equalized Odds, which necessitate the use of labeled data in Phase 2."
- **Why unresolved:** The current method leverages unlabeled data for Phase 2 because demographic parity depends only on features and sensitive attributes, whereas Equalized Odds requires labels.
- **What evidence would resolve it:** A modified post-processing step that privately utilizes label information without compromising the privacy guarantees established in Phase 1.

### Open Question 3
- **Question:** Is it possible to achieve these fairness and privacy guarantees in the "unawareness" setting where sensitive attributes are unavailable at prediction time?
- **Basis in paper:** [explicit] Section 7: "addressing the unawareness setting (where only the feature vector $X$ is available at prediction time) remains an important and challenging open direction."
- **Why unresolved:** The current classifier $\hat{g}_\rho(x, s)$ explicitly requires the sensitive attribute $s$ as an input to apply the Lagrangian correction.
- **What evidence would resolve it:** A mechanism that maps the private, fair classifier to a function of $x$ alone while preserving the $O(\log(N)/\sqrt{N})$ convergence to fairness.

## Limitations
- The method requires sensitive attributes to be available at prediction time, limiting applicability to unawareness settings
- Theoretical bounds assume minimum group size π_min > 0, but practical behavior with highly imbalanced groups is not characterized
- Computational cost scales with T=N² iterations for optimal convergence rates, making it expensive for large N

## Confidence

### Major Uncertainties
- **Sensitivity bounds for LSE_β gradients**: Lemma C.3 provides theoretical sensitivity of 4, but empirical verification on real data is needed
- **Optimal T scaling**: The paper claims T=N² is optimal for O(log N/√N) fairness rates, but this is computationally expensive
- **π_min threshold**: Theoretical bounds assume π_min > 0, but the paper doesn't specify how small π_min can be before DP2DP breaks down

### Confidence Labels
- **High**: Privacy accounting (RDP composition, Gaussian mechanism calibration), demographic parity definition, convergence rate O(log N/√N) for fixed ρ
- **Medium**: Practical performance on real datasets, exact sensitivity constants in Lemma C.3, optimal hyperparameter choices (β, T, b)
- **Low**: Behavior on highly imbalanced groups (π_min → 0), numerical stability of LSE_β smoothing with very small β, comparison to non-private baselines with same privacy budget

## Next Checks
1. **Sensitivity verification**: Implement gradient clipping and empirically measure ∇Ĥ_β(λ) ∞-norm across minibatches on Adult dataset. Verify it stays below 4 as assumed in privacy analysis.
2. **Ablation study**: Systematically vary T∈{100, 1000, 10000} on Credit Card dataset while fixing N=50000. Plot unfairness vs. T to empirically verify T=N² scaling is near-optimal.
3. **Group imbalance stress test**: Create synthetic datasets with varying π_min∈{0.1, 0.05, 0.01, 0.005}. Run DP2DP with ρ=0.1 and measure fairness-accuracy trade-off. Identify the threshold where performance degrades sharply.