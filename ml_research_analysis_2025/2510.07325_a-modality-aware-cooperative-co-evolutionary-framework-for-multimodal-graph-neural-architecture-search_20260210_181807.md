---
ver: rpa2
title: A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph
  Neural Architecture Search
arxiv_id: '2510.07325'
source_url: https://arxiv.org/abs/2510.07325
tags:
- 'true'
- search
- mean
- fusion
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MACC-MGNAS, a genetic algorithm-based approach
  for automated multimodal graph neural network (MGNN) architecture design. The method
  employs a divide-and-conquer cooperative co-evolutionary framework (MACC) that decomposes
  global architectures into modality-specific and fusion gene blocks, optimized by
  specialized workers.
---

# A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search

## Quick Facts
- arXiv ID: 2510.07325
- Source URL: https://arxiv.org/abs/2510.07325
- Reference count: 27
- One-line primary result: MACC-MGNAS achieves 81.67% F1-score on VulCE within 3 GPU-hours, outperforming state-of-the-art by 8.7% F1 and reducing computation cost by 27%.

## Executive Summary
This paper introduces MACC-MGNAS, a genetic algorithm-based approach for automated multimodal graph neural network (MGNN) architecture design. The method employs a divide-and-conquer cooperative co-evolutionary framework (MACC) that decomposes global architectures into modality-specific and fusion gene blocks, optimized by specialized workers. To improve efficiency, a modality-aware dual-track surrogate (MADTS) estimates fitness for local gene blocks, while a similarity-based population diversity indicator (SPDI) dynamically balances exploration and exploitation during global evolution. Experiments on a vulnerability co-exploitation dataset show MACC-MGNAS achieves an F1-score of 81.67% within 3 GPU-hours, outperforming state-of-the-art competitors by 8.7% F1 and reducing computation cost by 27%.

## Method Summary
MACC-MGNAS uses a cooperative co-evolutionary framework with a Coordinator and specialized workers to evolve multimodal graph neural network architectures. The global chromosome is decomposed into modality-specific gene blocks and a fusion block, which are optimized independently. A Gaussian Process surrogate (MADTS) provides cheap fitness estimates for local evolution, while SPDI dynamically adjusts genetic operator rates based on population diversity. The framework balances exploration and exploitation to converge on high-performance architectures efficiently.

## Key Results
- MACC-MGNAS achieves 81.67% F1-score on VulCE dataset
- Outperforms state-of-the-art by 8.7% F1 improvement
- Reduces computation cost by 27% (3 GPU-hours vs 4.87)
- Ablation studies confirm contributions: MACC drives accuracy, MADTS improves efficiency, SPDI enhances stability

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Decomposition (MACC)
Dividing the global architecture search into modality-specific sub-problems allows for more effective optimization than searching the joint space monolithically. The framework uses a "divide-and-conquer" strategy where a global chromosome is split into modality-specific gene blocks ($C^{(m)}$) and a fusion block ($C^{(fus)}$). Local workers evolve these blocks independently, and a coordinator reassembles them. This isolates the search space for specific data types (e.g., text vs. graph structure) before optimizing their interaction. The optimal global architecture can be approximated by combining locally optimal modality components.

### Mechanism 2: Surrogate-Assisted Local Search (MADTS)
Replacing expensive model training with a lightweight Gaussian Process (GP) surrogate during local worker evolution significantly reduces computational cost without severely compromising architecture quality. Local workers use GP surrogates to predict the fitness ($\hat{F}^{(m)}$) of candidate gene blocks. A variance-aware score fuses these predictions with global feedback to guide selection. This avoids full model training for every local mutation/crossover step. The surrogate model can accurately rank candidate architectures based on limited historical performance data.

### Mechanism 3: Adaptive Diversity Control (SPDI)
Dynamically adjusting genetic operator rates based on population diversity prevents premature convergence and stabilizes the search trajectory. The Coordinator calculates a Similarity-based Population Diversity Indicator (SPDI) by measuring the average pairwise distance between chromosome encodings. If diversity drops below a threshold ($\tau_t$), the system switches to "explore" mode (high mutation); otherwise, it favors "exploit" mode (high crossover). Chromosome encoding distance correlates meaningfully with functional diversity in the search space.

## Foundational Learning

- **Concept: Genetic Algorithms (GA) & Chromosome Encoding**
  - **Why needed here:** The entire search relies on representing MGNN architectures as "chromosomes" (vectors of hyperparameters like `u_mul_v`, `sum`, `gelu`) and applying biological metaphors (mutation, crossover) to evolve them.
  - **Quick check question:** Can you explain why a standard GA might struggle with a highly discrete and combinatorial search space compared to continuous optimization?

- **Concept: Message Passing in Graph Neural Networks (GNNs)**
  - **Why needed here:** The "genes" being evolved are GNN components (Message functions $\phi$, Aggregation $\psi$, Update $\gamma$). Understanding how information flows across edges is required to interpret the search results.
  - **Quick check question:** If a node has 100 neighbors, why might the choice of "aggregation" function (e.g., `mean` vs. `max`) significantly change the learned representation?

- **Concept: Surrogate Modeling (Gaussian Processes)**
  - **Why needed here:** To understand the MADTS component. You need to grasp how a probabilistic model can predict a score (fitness) and uncertainty (variance) for unseen architectures without training them.
  - **Quick check question:** What is the trade-off between "exploration" (querying high-uncertainty areas) and "exploitation" (querying high-prediction areas) in surrogate-based optimization?

## Architecture Onboarding

- **Component map:** Coordinator -> Modality Workers -> Fusion Worker -> MADTS (GP Surrogate) -> SPDI (Diversity Control)
- **Critical path:**
  1. Init: Sample random population of chromosomes (size $N=20$).
  2. Split: Decompose chromosomes into modality/fusion blocks.
  3. Local Search: Workers use surrogates to evolve blocks for $T_{LS}$ steps.
  4. Merge: Recombine top-$k$ elite blocks.
  5. Global Eval: Train/validate the assembled MGNNs to get ground-truth fitness.
  6. Feedback: Update surrogates with new (architecture, fitness) pairs; adjust GA rates via SPDI.

- **Design tradeoffs:**
  - **Accuracy vs. Cost (Surrogate fidelity):** A more complex surrogate might predict better but slow down the search.
  - **Stability vs. Speed (SPDI threshold):** Setting the diversity threshold too high forces constant exploration; too low risks premature convergence.

- **Failure signatures:**
  - **Stagnation:** Validation F1 plateaus early and variance drops to near zero. Likely cause: SPDI failed to inject mutation; population collapsed to a local optimum.
  - **High Variance/Instability:** Validation curve oscillates wildly without trending upward. Likely cause: Surrogate errors are misleading the local workers, or SPDI is triggering "explore" mode too aggressively.
  - **Slow Run time:** Exceeding expected GPU hours. Likely cause: Surrogate failure forcing fallback to full evaluation too often.

- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run `w/o MACC` vs. `Full MACC` on a small subset of VulCE to verify the performance gap exists.
  2. **SPDI Sensitivity:** Vary the threshold $\tau_t$ to visualize the "Explore/Exploit" trade-off curve and find the stable region.
  3. **Search Space Robustness:** Add a third modality or increase chromosome length ($K$) and observe if the linear complexity ($O(N^2L)$) or surrogate accuracy breaks down.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can MACC-MGNAS be adapted to maintain efficiency when applied to web-scale graphs where the computational overhead of evolutionary search remains prohibitive?
- **Open Question 2:** Can the MACC framework be extended to support dynamic or temporal graphs where node features and topology evolve over time?
- **Open Question 3:** How can the search space be modified to allow for variable chromosome lengths or the automatic discovery of novel structural components?
- **Open Question 4:** Do the design principles discovered by MACC-MGNAS (e.g., multiplicative message interactions) generalize to multimodal graph tasks outside of cybersecurity?

## Limitations
- Surrogate fidelity and search space generalization remain unproven on larger or more complex search spaces
- Chromosome encoding distance validity is not rigorously validated against functional architectural differences
- Ablation scope may miss interaction effects between components under different conditions

## Confidence

- **High Confidence:** The core claim that MACC-MGNAS achieves an 81.67% F1 score and outperforms state-of-the-art methods by 8.7% is directly supported by the experimental results in Table II.
- **Medium Confidence:** The assertion that MADTS improves efficiency by reducing GPU-hours from 4.87 to 3.00 is supported by ablation, but the underlying assumption that the GP surrogate maintains predictive accuracy across all search phases is not explicitly validated.
- **Medium Confidence:** The claim that SPDI enhances convergence stability is supported by the observed second performance lift in Figure 3, but the paper does not provide a rigorous statistical test to confirm that this lift is significantly different from random noise.

## Next Checks

1. **Surrogate Robustness Test:** Conduct a controlled experiment where the search space is artificially expanded (e.g., double the number of candidate operations per gene). Measure if the GP surrogate's prediction error rate increases significantly, and if so, quantify the impact on both F1 score and GPU-hour efficiency.

2. **Diversity Metric Validation:** Implement an alternative diversity metric (e.g., based on predicted fitness variance or architecture edit distance) and run the SPDI component with this new metric. Compare the resulting F1 scores and convergence curves to the original Euclidean-based SPDI to assess if the choice of metric materially affects performance.

3. **Cross-Dataset Generalization:** Apply MACC-MGNAS to a different multimodal graph dataset (e.g., a multimodal social network or biological interaction network) without retraining the surrogate models from scratch on the new data. Evaluate if the architecture search process can transfer its learned surrogate priors or if it requires complete retraining, which would impact real-world applicability.