---
ver: rpa2
title: Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific
  Publications Using Large Vision Language Models
arxiv_id: '2506.19825'
source_url: https://arxiv.org/abs/2506.19825
tags:
- question
- diagrams
- visualization
- labels
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well large Vision-Language Models (VLMs)
  can detect violations of data visualization guidelines in scientific diagrams. It
  uses five open-source VLMs and five prompting strategies to assess diagram types,
  3D effects, axis labels, tick marks, colors, legends, line counts, and compression
  artifacts.
---

# Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models

## Quick Facts
- **arXiv ID**: 2506.19825
- **Source URL**: https://arxiv.org/abs/2506.19825
- **Reference count**: 40
- **Primary result**: Large Vision-Language Models can effectively flag many guideline violations (e.g., missing axis labels, legends, unnecessary 3D effects) in scientific diagrams, though further improvements are needed for certain tasks.

## Executive Summary
This paper evaluates how well large Vision-Language Models (VLMs) can detect violations of data visualization guidelines in scientific diagrams. It uses five open-source VLMs and five prompting strategies to assess diagram types, 3D effects, axis labels, tick marks, colors, legends, line counts, and compression artifacts. The best-performing model, Qwen2.5VL, achieves strong results for diagram type classification (F1-score 82.49%), 3D effect detection (F1-score 98.55%), legend identification (F1-score 96.64%), and axis label detection (F1-score 76.74%). However, it struggles with tick marks and labels (F1-score 46.13%) and compression artifacts (F1-score 0.74%). The summary prompting strategy generally yields the best performance. Results show VLMs can effectively flag many guideline violations, such as missing axis labels, missing legends, and unnecessary 3D effects, though further improvements are needed for certain tasks.

## Method Summary
The study uses zero-shot inference with five VLMs (CogVLM2, InternVL2.5, Janus-Pro, Qwen2.5VL, ChartInstruct) on 1,010 images sampled from the UB PMC dataset. Five prompting strategies are tested, with "Summary" (describe image, then summarize to format) performing best. The evaluation measures macro F1-score for classification tasks (diagram type, binary violations) and RMSE for regression tasks (counting lines/colors). The best open-source model, Qwen2.5VL-7B, is identified as the most effective for this task.

## Key Results
- Qwen2.5VL achieves 82.49% F1-score for diagram type classification
- Best performance for 3D effect detection (98.55% F1) and legend identification (96.64% F1)
- Summary prompting strategy yields the best performance across tasks
- VLMs struggle with tick marks and labels (46.13% F1) and compression artifacts (0.74% F1)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The "Summary" prompting strategy improves detection rates for missing elements (like axis labels) compared to direct questioning.
- **Mechanism:** By first asking the model to describe the image freely and then requesting a summary in a specific format, the model generates intermediate reasoning tokens. This "chain-of-thought" light approach likely grounds the visual attention mechanism before constraining the output, reducing false negatives where the model overlooks subtle absences.
- **Core assumption:** The VLM's visual encoder captures the relevant features (e.g., the presence of an axis), but the language model fails to map this to the correct "No" label without intermediate descriptive steps.
- **Evidence anchors:**
  - [abstract] "The summary prompting strategy generally yields the best performance."
  - [section] Table 5 shows the "Summary" strategy achieving 76.74% F1-score for "All axes labels" vs 26.83% for "Individual question," driven by a massive increase in Recall (69.96% vs 15.55%).
  - [corpus] No direct corpus evidence for this specific prompting mechanism; neighboring papers focus on general VLM usage.

### Mechanism 2
- **Claim:** VLMs detect violations based on semantic object recognition (macro-features) rather than low-level signal analysis (micro-features).
- **Mechanism:** The vision transformer (ViT) architecture excels at identifying high-level concepts like "3D shape," "legend box," or "diagram category" (F1 > 82%). However, it fails at detecting compression artifacts or small tick marks because these rely on high-frequency pixel data that is often lost during patch embedding or pooling in the visual encoder.
- **Core assumption:** The failure to detect artifacts is a limitation of the visual resolution or tokenization process, not the reasoning capability of the LLM component.
- **Evidence anchors:**
  - [section] Table 4 shows 98.55% F1 for 3D effects; Table 8 shows 0.74% F1 for compression artifacts.
  - [section] The authors hypothesize: "One explanation for the results is that compression artifacts might be lost during tokenization."
  - [corpus] Corpus papers discuss VLMs for technical diagrams but do not specifically analyze high-frequency visual feature loss.

### Mechanism 3
- **Claim:** General-purpose VLMs outperform chart-specific VLMs in zero-shot guideline compliance due to broader visual pre-training.
- **Mechanism:** ChartInstruct (specialized) severely underperformed Qwen2.5VL (general), often defaulting to specific classifications (classifying 69% of images as scatter-line plots). This suggests that specialized fine-tuning may narrow the model's "visual vocabulary," causing it to hallucinate chart features to fit its narrow training distribution, whereas general models handle the diversity of real-world scientific diagrams better.
- **Core assumption:** The domain-specific fine-tuning of ChartInstruct resulted in catastrophic forgetting of general visual features or an over-sensitivity to specific chart patterns.
- **Evidence anchors:**
  - [section] Table 3 shows ChartInstruct achieving only 3.64% F1-score on diagram type classification vs. Qwen2.5VL's 82.49%.
  - [section] "Surprisingly, the chart-specific VLM performed the worst... it classified 69.41% of the diagrams as scatter-line plots."
  - [corpus] Weak support; neighboring papers generally advocate for specialized models but lack comparative failure analysis for this specific task.

## Foundational Learning

- **Concept: Macro-averaged F1-score vs. No Information Rate (NIR)**
  - **Why needed here:** The dataset is imbalanced (e.g., 96% of diagrams do not use 3D effects). A model could achieve 96% accuracy by always answering "No." The F1-score measures the model's ability to find the rare "positive" cases (violations).
  - **Quick check question:** If a model detects 0 violations in a dataset where 5% of diagrams have errors, what is the Accuracy vs. the Recall?

- **Concept: Visual Tokenization & Resolution**
  - **Why needed here:** Understanding why VLMs "see" a legend but miss a "tick mark" requires knowing that images are broken into patches (tokens). Small text or fine lines may be compressed into a single vector, losing detail.
  - **Quick check question:** Why might a 3D effect (semantic shape) be easier to classify than a compression artifact (pixel noise) in a Vision Transformer?

- **Concept: Zero-shot Prompting Strategies**
  - **Why needed here:** The paper proves that *how* you ask matters more than the model size in some cases. "Context" (conversation history) caused error propagation, while "Summary" improved reasoning.
  - **Quick check question:** Why would keeping a history of previous questions (Context strategy) degrade performance if the model made a mistake in the first question?

## Architecture Onboarding

- **Component map:**
  - Image + Question -> Qwen2.5VL (InternViT Visual Encoder + Adapter + QLM2.5 LLM) -> Prompting Strategy Module -> Output Parser (Regex for Yes/No/Number)

- **Critical path:** The reliability of the system relies on the **Visual Encoder's resolution** (to see ticks) and the **Prompt Strategy** (to extract the correct label). The experiments show the Summary strategy is the critical dependency for high Recall.

- **Design tradeoffs:**
  - **Accuracy vs. Resources:** The 72B model offers better performance (e.g., 93.86% F1 on horizontal labels) but requires significantly higher compute. The 7B model with "Summary" prompting is the identified efficiency sweet spot.
  - **Constraint vs. Reasoning:** Constraining the answer immediately ("Answer yes/no") lowers performance compared to "Describe first, then summarize."

- **Failure signatures:**
  - **The "Yes" Bias:** In visual question answering, models often default to "Yes" (labels are present) even when they are not, resulting in high Precision but abysmal Recall (as seen in the "Individual Question" strategy for axis labels).
  - **Context Hallucination:** In the "Context" strategy, if the model misidentifies a diagram type early, it carries that error forward, contaminating subsequent answers.
  - **Loss of High-Frequency Detail:** Any task requiring analysis of pixel-level artifacts (compression, small font OCR) is currently a blind spot for this architecture.

- **First 3 experiments:**
  1. **Sanity Check (Diagram Type):** Run Qwen2.5VL on a batch of 50 images using "Individual Question" vs. "Summary" strategy to verify the 82% vs. ~65% performance delta described in Table 3.
  2. **Resolution Limit Test:** Crop and zoom into tick marks of a diagram to see if detection F1-score improves, confirming the "tokenization loss" hypothesis for fine-grained features.
  3. **Error Propagation Audit:** Implement the "Context" strategy and log how often a wrong answer to Question 1 (Diagram Type) statistically correlates with wrong answers to Question 3 (Axis Labels).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do specialized chart-based Vision-Language Models (VLMs) like ChartInstruct fail to outperform general-purpose VLMs in multi-class diagram type classification?
- **Basis in paper:** [explicit] The authors note that ChartInstruct performed worst (F1-score 3.64%), heavily biasing predictions toward "scatter-line" plots, and state that "Further research is required to explore this problem."
- **Why unresolved:** The paper identifies the failure mode but does not determine if the cause is training data distribution bias, architectural overfitting to specific chart structures, or a domain shift between the model's training data and the UB PMC dataset.
- **What evidence would resolve it:** An ablation study analyzing ChartInstruct's training data distribution or a fine-tuning experiment on the UB PMC dataset to see if classification performance recovers.

### Open Question 2
- **Question:** To what extent does the visual tokenization process in VLMs discard the high-frequency information required to detect compression artifacts?
- **Basis in paper:** [explicit] The authors report poor performance (F1-score 0.74%) in detecting compression artifacts and hypothesize that "compression artifacts might be lost during tokenization."
- **Why unresolved:** The study evaluates model outputs but does not examine the internal visual encoders or test if alternative tokenization parameters (e.g., dynamic resolution or patch size) preserve the necessary details.
- **What evidence would resolve it:** Experiments comparing artifact detection performance using variable image resolutions (e.g., Qwen2.5VL's dynamic resolution vs. fixed low resolution) and visualizing attention maps on compressed regions.

### Open Question 3
- **Question:** How does the rate of visualization guideline violations in scientific diagrams vary across different research disciplines?
- **Basis in paper:** [explicit] The authors acknowledge that their dataset is small and "primarily focused on the medical domain," explicitly suggesting that "Future work should... compare different research areas."
- **Why unresolved:** The reported violation rates (e.g., 22.95% missing tick marks) are specific to the medical images in the UB PMC dataset and may not represent practices in physics, social sciences, or computer science.
- **What evidence would resolve it:** A replication of the evaluation pipeline on a multi-domain dataset (e.g., arXiv figures) to compare violation frequencies against the medical baseline.

## Limitations

- VLMs struggle to detect fine-grained visual features like tick marks and compression artifacts due to tokenization loss
- The study relies on zero-shot prompting without domain-specific fine-tuning
- Ground truth labeling for subjective features introduces potential bias and inconsistency

## Confidence

- **High Confidence:** Qwen2.5VL's superior performance and the effectiveness of the "Summary" prompting strategy
- **Medium Confidence:** Explanations for task failures (tokenization loss, semantic recognition) are reasonable hypotheses but lack direct validation
- **Low Confidence:** Generalizability to other scientific domains or VLM architectures is uncertain

## Next Checks

1. **Resolution Impact Study**: Systematically test Qwen2.5VL's performance on tick mark detection and compression artifact identification across different image resolutions (336px, 768px, 1344px, 3584px)

2. **Fine-tuning Experiment**: Fine-tune Qwen2.5VL on a subset of the UB PMC dataset and compare zero-shot vs. fine-tuned performance for the most challenging tasks

3. **Error Analysis by Diagram Type**: Perform detailed error analysis to determine if VLMs' performance varies significantly across different diagram types (scatter plots vs. bar charts)