---
ver: rpa2
title: 'Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German
  Tumor Diagnoses'
arxiv_id: '2510.13624'
source_url: https://arxiv.org/abs/2510.13624
tags:
- coding
- data
- training
- tumor
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that instruction-based fine-tuning on public
  German-language medical catalogues can substantially improve the coding accuracy
  of open-weight LLMs for tumor diagnoses. Using over 500,000 synthetic question-answer
  pairs derived from ICD-10-GM, ICD-O-3, and OPS catalogues, the researchers fine-tuned
  eight models ranging from 7B to 70B parameters.
---

# Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses

## Quick Facts
- **arXiv ID:** 2510.13624
- **Source URL:** https://arxiv.org/abs/2510.13624
- **Reference count:** 35
- **Primary result:** Instruction-tuning on public German medical catalogues improved ICD-10-GM coding accuracy from 1.4-24% to 41-58% exact.

## Executive Summary
This study demonstrates that open-weight large language models can be effectively fine-tuned for German medical coding tasks using public catalogues. By converting ICD-10-GM, ICD-O-3, and OPS catalogues into over 500,000 synthetic question-answer pairs, the researchers substantially improved coding accuracy across eight models ranging from 7B to 70B parameters. The approach achieved 41-58% exact accuracy for ICD-10-GM coding (up from 1.4-24% baseline) and 99% accuracy for tumor diagnosis recognition. The work establishes that fine-tuning reduces performance gaps between small and large models, enables efficient deployment, and validates the use of public medical catalogues for instruction-tuning tasks.

## Method Summary
The researchers fine-tuned eight open-weight LLMs (7B-70B parameters) using LoRA with synthetic QA pairs derived from German medical catalogues. Training data consisted of 518,116 question-answer pairs created from ICD-10-GM, ICD-O-3, and OPS catalogues, with additional paraphrasing for linguistic variation. Models were evaluated on 2,024 real-world German tumor diagnosis descriptions using exact and partial accuracy metrics. The fine-tuning used LoRA configuration (rank=8, alpha=16) with 5 epochs, and outputs were validated using regex parsing for code structure compliance.

## Key Results
- ICD-10-GM exact coding accuracy increased from 1.4-24% to 41-58% after fine-tuning
- ICD-10-GM partial accuracy improved from 31-74% to 73-83%
- ICD-O-3 topography coding showed exact accuracy of 22-40% and partial accuracy of 56-67%
- Tumor diagnosis recognition achieved 99% accuracy
- Fine-tuning reduced performance gaps between small and large models
- Reasoning mode (Chain-of-Thought) decreased performance while increasing response time over 100-fold

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Instruction-tuning with synthetic Q&A pairs derived from public catalogues significantly improves coding accuracy and output validity.
- **Mechanism:** By converting structured catalogues into diverse question-answer pairs, the model learns to map textual descriptions to specific alphanumeric codes. This exposure forces the model to align its internal semantic representations with the rigid structure of medical coding standards, reducing hallucination.
- **Core assumption:** The textual descriptions in the catalogues sufficiently cover the semantic variations found in real-world clinical text.
- **Evidence anchors:** [abstract] "As training data, over 500,000 question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS catalogues"; [results] "Malformed code outputs dropped to 0% for all models."
- **Break condition:** Real-world diagnosis texts contain slang, abbreviations, or clinical context not present in the formal catalogue descriptions, causing the mapping to fail.

### Mechanism 2
- **Claim:** Fine-tuning reduces the performance gap between smaller and larger models, enabling efficient deployment.
- **Mechanism:** Parameter-efficient fine-tuning (LoRA) injects domain-specific knowledge into the model. While larger models retain a slight edge in baseline reasoning, the targeted injection of coding rules allows smaller models (e.g., 7B-14B parameters) to perform comparably to larger untrained models on this specific task.
- **Core assumption:** The LoRA configuration (rank=8, alpha=16) provides sufficient adaptability for both small and large models without overfitting.
- **Evidence anchors:** [abstract] "Accuracy correlated positively with model size, but gaps between small and large models narrowed after fine-tuning."
- **Break condition:** The task complexity increases to require multi-step reasoning (e.g., cross-referencing patient history) that exceeds the capacity of the smaller, fine-tuned models.

### Mechanism 3
- **Claim:** Direct prediction outperforms "reasoning mode" (Chain-of-Thought) for high-throughput medical coding in this specific domain.
- **Mechanism:** Explicit reasoning steps (thinking mode) without fine-tuning on domain logic can lead to hallucinations or circular reasoning. Fine-tuning optimizes the model for direct, high-probability token generation (the code itself), bypassing the need for intermediate verbiage which introduces latency and potential error accumulation.
- **Core assumption:** The coding task is primarily a retrieval/mapping task rather than a complex logic puzzle.
- **Evidence anchors:** [abstract] "The reasoning mode in Qwen3... yielded a lower performance than fine-tuning and was over 100 times slower."
- **Break condition:** The coding task requires resolving ambiguity between conflicting symptoms where explicit logic is strictly necessary to justify the code choice.

## Foundational Learning

- **Concept: ICD-10-GM vs. ICD-O-3**
  - **Why needed here:** The paper evaluates two distinct coding systems. ICD-10-GM is for general diagnoses, while ICD-O-3 is specific to oncology (topography/morphology). Understanding this explains why models performed well on ICD-10 but struggled with ICD-O.
  - **Quick check question:** Can you explain why a model might accurately identify a "malignant neoplasm of the kidney" (ICD-10) but fail to assign the correct topography code (ICD-O-3)?

- **Concept: Data Derivability (Ceiling Analysis)**
  - **Why needed here:** The authors manually annotated a subset of data to prove that some text descriptions were *impossible* to code correctly due to missing information. This establishes that 100% accuracy is theoretically impossible.
  - **Quick check question:** If a text says "Renal tumor" without specifying behavior (benign/malignant), is a model failure to assign C64 (malignant) a "mistake" or a data limitation?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** This is the technique used to train the models efficiently. It explains how the authors managed to fine-tune a 70B model on limited GPU resources (2x Nvidia A40s).
  - **Quick check question:** Why does applying the same LoRA rank (r=8) to models of different sizes potentially give larger models an unfair advantage in the comparison?

## Architecture Onboarding

- **Component map:** Raw text diagnosis from Tumor Documentation System -> Public catalogues (Alpha-ID, ICD-O-3, OPS) -> Synthetic Q&A generation (500k+ pairs) with paraphrasing -> Qwen/Llama/Mistral weights + LoRA adapters -> Structured code string + Confidence score -> Regex parser validation
- **Critical path:** The generation of the synthetic training dataset. If the "formulations" (paraphrased questions) are not diverse enough, the model will overfit to specific phrasing and fail on real clinical text.
- **Design tradeoffs:**
  - Reasoning vs. Speed: The study explicitly rejects "Thinking Mode" due to a >100x latency penalty with no accuracy gain.
  - LoRA Rank vs. Fair Comparison: Using uniform LoRA settings simplifies engineering but confounds the analysis of whether smaller models are *inherently* less capable or just under-adapted.
- **Failure signatures:**
  - High Invalid Rate: Models outputting mashups of Morphology and Topography codes (e.g., "M-8140/3 C64.9" mixed into one string).
  - Partial Accuracy Gap: Model gets the general category (C64) right but misses the specificity (.9) required for exact matching.
- **First 3 experiments:**
  1. Baseline Validation: Test base models (no fine-tuning) against the Gold Standard test set to confirm low accuracy (1.4-24%) and high malformed output rates.
  2. Ceiling Estimation: Perform human annotation on a random 100-sample subset to determine the "derivable" accuracy limit (found to be ~70%).
  3. Epoch Convergence: Train for 5 epochs and evaluate after each to identify the "best checkpoint" (often epoch 1 or 2) before overfitting occurs on the limited ICD-O data.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does integrating Retrieval-Augmented Generation (RAG) with fine-tuned LLMs yield higher coding accuracy for German tumor diagnoses than fine-tuning alone?
  - **Basis in paper:** [explicit] The authors state that "Combining fine-tuned LLMs and RAG could further improve the performance compared to RAG that employs the basic pre-trained models," suggesting this as a promising direction for handling complex cases.
  - **Why unresolved:** This study isolated instruction-tuning on static catalogues; it did not evaluate dynamic retrieval mechanisms alongside the fine-tuned adapters.
  - **What evidence would resolve it:** An ablation study comparing the exact and partial accuracy of the fine-tuned models against a RAG-enhanced version using the same test dataset.

- **Open Question 2:** Can the performance gap between ICD-O-3 and ICD-10-GM coding be closed by specifically augmenting the training data with more diverse synonyms and linguistic variations for topography codes?
  - **Basis in paper:** [explicit] The authors note that ICD-O performance lagged due to "weaker prior knowledge and lower training coverage," specifically citing that the ICD-O catalogue offers "fewer synonyms and linguistic variations" than the Alpha-ID catalogue.
  - **Why unresolved:** The current dataset relied on the existing structure of public catalogues, which are inherently sparser for ICD-O, limiting the model's ability to generalize.
  - **What evidence would resolve it:** A follow-up experiment using a synthetic dataset that enriches ICD-O descriptions with varied phrasing, measuring if the accuracy convergence matches ICD-10 levels.

- **Open Question 3:** To what extent does providing full clinical context (e.g., doctor's notes) rather than short diagnosis descriptions improve the theoretical accuracy ceiling for these models?
  - **Basis in paper:** [explicit] The paper highlights that short descriptions often lack necessary details (e.g., tumor behavior), and suggests that "providing richer contextual data could help models overcome the problem of insufficient information."
  - **Why unresolved:** The current upper-bound estimate (60â€“79%) is based on short texts; it is unknown if models can effectively extract the missing variables from longer, unstructured clinical notes to reach higher accuracy.
  - **What evidence would resolve it:** A study evaluating model performance on a dataset containing both the short diagnosis text and the full corresponding clinical report.

## Limitations

- The maximum achievable accuracy of 60-79% for exact ICD-10 coding is constrained by data quality issues in the source catalogues and training data.
- The evaluation relies on a proprietary test set of 2,024 real-world cases that were not publicly released, limiting independent verification.
- The synthetic training data generation process may not capture the full diversity of real clinical documentation, particularly for ICD-O-3 which showed lower performance.
- The study does not address multi-label coding scenarios common in clinical practice.

## Confidence

- **High Confidence:** The core finding that instruction-tuning with synthetic Q&A pairs substantially improves ICD-10-GM coding accuracy (41-58% exact vs 1.4-24% baseline) is well-supported by the experimental design and results.
- **Medium Confidence:** The performance gap narrowing between small and large models after fine-tuning is observed but complicated by the use of uniform LoRA configurations that may advantage larger models.
- **Low Confidence:** The negative result regarding reasoning mode performance (Qwen3 thinking mode being 100x slower with lower accuracy) lacks strong corpus support and may be architecture-specific.

## Next Checks

1. **Independent Reproduction:** Replicate the fine-tuning pipeline on a held-out subset of the published synthetic dataset using the specified LoRA configuration to verify the reported accuracy improvements.

2. **Real-World Generalization:** Test the best-performing fine-tuned models on an independent, publicly available German clinical text corpus to assess performance beyond the proprietary test set.

3. **Data Quality Audit:** Conduct a systematic analysis of the synthetic training data to identify specific categories of description-text/code mismatches that limit maximum achievable accuracy, focusing on ICD-O-3 performance gaps.