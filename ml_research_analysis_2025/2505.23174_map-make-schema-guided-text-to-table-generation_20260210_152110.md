---
ver: rpa2
title: 'Map&Make: Schema Guided Text to Table Generation'
arxiv_id: '2505.23174'
source_url: https://arxiv.org/abs/2505.23174
tags:
- team
- update
- none
- table
- home
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Map&Make is a framework for schema-agnostic text-to-table generation
  that decomposes text into atomic propositions, dynamically infers table schemas,
  and populates tables iteratively. Evaluated on Rotowire and Livesum benchmarks,
  it outperforms existing methods, achieving up to 42% improvement in table-level
  similarity on Rotowire and reducing error rates by up to 55% on Livesum.
---

# Map&Make: Schema Guided Text to Table Generation

## Quick Facts
- **arXiv ID:** 2505.23174
- **Source URL:** https://arxiv.org/abs/2505.23174
- **Reference count:** 40
- **Primary result:** Achieves up to 42% improvement in table-level similarity on Rotowire and reduces error rates by up to 55% on Livesum through schema-agnostic text-to-table generation.

## Executive Summary
Map&Make is a framework for schema-agnostic text-to-table generation that decomposes text into atomic propositions, dynamically infers table schemas, and populates tables iteratively. Evaluated on Rotowire and Livesum benchmarks, it outperforms existing methods, achieving up to 42% improvement in table-level similarity on Rotowire and reducing error rates by up to 55% on Livesum. Ablation studies confirm the importance of each component, particularly atomization and iterative schema generation. The framework generalizes well to open-domain data and maintains stable performance on large texts. Limitations include reliance on LLM capabilities, computational cost, and lack of support for hierarchical table structures.

## Method Summary
Map&Make employs a 3-stage multi-agent prompting framework that operates without predefined schemas. First, it atomizes input text into self-contained, well-formed propositions explicitly supported by the source. Second, it iteratively builds table schemas by processing these atomic statements to infer row and column headers. Finally, it iteratively fills the table by updating cell values based on each statement. The framework can operate in sequential (3-step) or unified variants, using LLMs like GPT-4o and Gemini 2.0 Flash through prompting only, without fine-tuning.

## Key Results
- Achieves up to 42% improvement in table-level similarity on Rotowire benchmark
- Reduces error rates by up to 55% on Livesum benchmark for numerical aggregation
- Demonstrates stable performance on large texts and generalizes to open-domain data
- Ablation studies confirm atomization and iterative schema generation are critical components

## Why This Works (Mechanism)

### Mechanism 1: Propositional Atomization Reduces Hallucination and Improves Coverage
Decomposing complex text into atomic, self-contained propositions before schema extraction improves information coverage and reduces hallucination compared to direct text-to-table generation. The framework first "dissects" input text into atomic statements that are well-formed, self-contained (decontextualized), and explicitly supported by the source. This granular decomposition resolves ambiguous entity attributions and prevents conflation of overlapping entities. By forcing explicit grounding to the source text for each proposition, the model is constrained to only extract what is present, reducing the space for unattested additions.

### Mechanism 2: Iterative Schema Inference Enables Schema-Agnostic Generalization
Building table schemas iteratively, statement-by-statement, rather than planning them upfront, improves coverage and adapts to open-domain texts where structures are not known in advance. Instead of requiring a predefined schema, the framework initializes an empty schema and updates it as each atomic statement is processed. Entities are mapped to row headers and attributes to column headers. This allows the schema to evolve dynamically, accommodating novel information that might be missed by a static, upfront planning approach.

### Mechanism 3: Iterative Table Population with Explicit Updates Minimizes Aggregation Errors
For tasks requiring numerical aggregation or state tracking (like counting events in live commentary), iteratively updating table cells for each statement reduces counting errors compared to one-shot generation. The framework fills tables by processing each atomic statement sequentially. For dynamic texts, cell values are incremented; for static texts, they are set. Critically, the model is instructed to provide statement-wise updates, making the generation process transparent and allowing for more reliable tracking of counts and state changes over a narrative.

## Foundational Learning

- **Concept: Propositional Logic and Decontextualization**
  - Why needed here: This is the core of the Atomization stage. One must understand how to take a sentence with multiple clauses, pronouns, and implicit context and break it into independent, self-contained, and verifiable statements (e.g., "He scored 20 points" becomes "Player X scored 20 points in Game Y").
  - Quick check question: Given the sentence "The team, led by their star player, won their third game in a row," what are three potential atomic propositions that could be extracted?

- **Concept: Schema Induction**
  - Why needed here: This underpins the Schema Extraction stage. Unlike standard information extraction where the schema is given, this framework requires understanding how to *learn* or *infer* a schema (rows and columns) from raw data.
  - Quick check question: If processing atomic statements about various products, some mentioning "price" and others "cost," should these be mapped to one or two columns in an inferred schema, and what factors influence that decision?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: The entire Map&Make framework is a structured, multi-step CoT process. Understanding CoT is essential to grasp why decomposing the problem into "map" (atomize/plan) and "make" (fill) steps improves reasoning and output quality over a single, direct prompt.
  - Quick check question: How does the "step-by-step" reasoning encouraged by CoT relate to the "iterative" schema building in this paper, and why might this reduce errors compared to a direct "generate table" instruction?

## Architecture Onboarding

- **Component map:** Input Text -> Atomization Agent -> Atomic Statements -> Schema Extraction Agent -> Schema -> Table Generation Agent -> Final Table

- **Critical path:** The Atomization -> Schema Extraction path. If atomization is poor (e.g., misses facts or creates ambiguous statements), the schema will be incomplete or malformed, propagating errors to the final table. The paper's ablation study confirms removing atomization causes a significant drop in completeness.

- **Design tradeoffs:**
  - **3-Step (M&M-3S) vs. Unified (M&M-U):** The 3-step approach uses separate LLM calls for each stage, offering higher transparency and performance but at a higher computational cost. The Unified approach attempts this in a single call, which is faster but may see a performance drop (e.g., on Livesum, M&M-U had higher error rates than M&M-3S in one-shot).
  - **Computational Cost vs. Accuracy:** The iterative nature is powerful but expensive. On large corpora like Livesum, this is a significant factor.

- **Failure signatures:**
  - **Hallucinated columns:** The model generates columns not supported by the text. Check the atomization outputâ€”if the atomic statements are clean, this should be minimized.
  - **Missing rows/columns:** The schema is incomplete. This often traces back to atomization missing key entities or attributes.
  - **Incorrect aggregations:** In tasks like Livesum, if counts are wrong, check if the model is properly updating cells iteratively or trying to count in a single step.

- **First 3 experiments:**
  1. **Reproduce the Atomization Ablation:** Take a sample from Rotowire, run full M&M-3S, then run it again by skipping the atomization step and feeding raw text directly to the schema extractor. Compare the schema completeness to validate the paper's claim.
  2. **Test on Open-Domain Data:** Select a non-sports article (e.g., a financial report) from a source like Wiki40B (as done in the paper). Run M&M and evaluate the generated table using the Auto-QA metric to verify generalization.
  3. **Compare 3-Step vs. Unified:** Run both variants on a small subset of Livesum to measure the trade-off between performance (Error Rate/RMSE) and latency/cost for your specific use case.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Map&Make be extended to support hierarchical table structures with merged cells and nested headers? The Limitations section states "This methodology does not apply to tabular structures with free-form structural complexities such as hierarchical headers, cells with merged cells." The iterative schema construction assumes flat row-column structures; hierarchical tables require representing parent-child relationships between headers, which the current atomic statement decomposition cannot capture.

- **Open Question 2:** What computational optimizations can reduce the cost of iterative processing for long documents like Livesum without sacrificing coverage? The Limitations section notes "the iterative nature helps in information coverage... but becomes computationally expensive due to lengthy outputs, such as those in Livesum." Each atomic statement triggers separate LLM calls for schema updates and table filling, creating linear scaling with text length.

- **Open Question 3:** How can the performance gap between fine-tuned smaller models (e.g., Llama 8B) and large proprietary models be closed for schema-agnostic table generation? Appendix C shows fine-tuning Llama 8B improves performance but notes "there is a significant gap between smaller fine-tuned models and larger SoTA LLMs like GPT-4o, Gemini-2.0-flash." Smaller models struggle with consistent table formatting, multi-row outputs, and long-context comprehension.

- **Open Question 4:** Can the M&M framework generalize to multilingual text-to-table generation given its reliance on language-specific proposition atomization? The Wiki40B study is limited to English articles, and atomization depends on grammatical correctness and linguistic conventions. Propositional atomization requires language-specific parsing and decontextualization; cross-lingual schema extraction quality is untested.

## Limitations

- **Computational cost of iterative LLM calls:** The framework's reliance on multiple sequential LLM interactions (especially in the 3-step variant) raises questions about scalability for real-world applications with large volumes of text or documents.

- **Generalization to highly unstructured or domain-shifted text:** While the framework shows promise on open-domain data, its performance on extremely diverse or noisy text (e.g., social media posts, poorly written articles) is untested. The effectiveness of atomization and schema inference could degrade significantly with text quality.

- **Handling of hierarchical or complex table structures:** The paper explicitly notes this as a limitation. The framework's schema inference mechanism, designed for flat table structures, may fail to capture nested relationships or multi-level hierarchies common in complex datasets.

## Confidence

- **Mechanism 1 (Propositional Atomization):** **High** - The ablation study directly shows that removing atomization causes a significant drop in completeness, and the concept is well-grounded in LLM literature on hallucination.

- **Mechanism 2 (Iterative Schema Inference):** **Medium** - The evidence is strong for the tested datasets (Rotowire, Livesum), but the claim of general schema-agnostic performance is based on a single open-domain test (Wiki40B). Broader validation is needed.

- **Mechanism 3 (Iterative Table Population):** **High** - The quantitative results on Livesum (up to 55% reduction in error rate) are compelling and directly address the known LLM weakness in counting and state tracking.

## Next Checks

1. **Measure the latency and cost per document** for both the 3-step and unified variants of M&M on the Livesum dataset. This will quantify the practical computational overhead of the iterative approach.

2. **Test the framework on a corpus of highly unstructured text** (e.g., a collection of diverse Reddit posts or tweets) to assess its robustness and the effectiveness of atomization on noisy, informal language.

3. **Design a benchmark for hierarchical table generation** and evaluate M&M's ability to infer and populate nested schemas, directly testing the paper's stated limitation.