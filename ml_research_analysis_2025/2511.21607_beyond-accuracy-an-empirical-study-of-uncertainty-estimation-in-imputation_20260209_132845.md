---
ver: rpa2
title: 'Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation'
arxiv_id: '2511.21607'
source_url: https://arxiv.org/abs/2511.21607
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000028
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic empirical study of uncertainty
  estimation in data imputation. The authors evaluate representative imputation methods
  across three families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute),
  and deep generative models (GAIN, MIWAE, TabCSDI).'
---

# Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation

## Quick Facts
- arXiv ID: 2511.21607
- Source URL: https://arxiv.org/abs/2511.21607
- Reference count: 40
- This paper presents a systematic empirical study of uncertainty estimation in data imputation, revealing that accuracy and calibration are often misaligned across different imputation methods.

## Executive Summary
This study systematically evaluates uncertainty estimation in data imputation across three methodological families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative models (GAIN, MIWAE, TabCSDI). The authors compare accuracy, calibration, and runtime across multiple datasets, missingness mechanisms, and rates. They find that methods with high reconstruction accuracy do not necessarily yield reliable uncertainty estimates, with MICE providing the most dependable calibration despite not always achieving the best accuracy. The work establishes practical guidelines for selecting uncertainty-aware imputers based on task requirements and dataset characteristics.

## Method Summary
The study evaluates six representative imputation methods across five numerical tabular datasets (housing, biodegradation, cancer, energy, wine) with synthetic missingness injected at 10-30% rates under MCAR, MAR, and MNAR mechanisms. Uncertainty is extracted through three complementary strategies: multi-run variability (n=5), conditional sampling (n=20-50), and predictive-distribution modeling. Accuracy is measured via MAE, calibration via Expected Calibration Error (ECE) and calibration curves, with runtime also tracked. The analysis compares deterministic methods (MICE, SoftImpute) against sampling-based deep generative approaches, revealing critical differences in how well uncertainty estimates align with empirical coverage.

## Key Results
- MICE provides the most dependable calibration across all missingness mechanisms despite not always achieving the best point accuracy
- Accuracy and calibration are often misaligned: methods with high reconstruction accuracy (like SoftImpute) do not necessarily yield reliable uncertainty estimates
- MIWAE achieves a favorable balance between accuracy and calibration at higher computational cost
- All methods degrade in calibration and accuracy from MCAR→MAR→MNAR, with MICE and MIWAE showing more stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High reconstruction accuracy does not guarantee well-calibrated uncertainty estimates.
- Mechanism: Methods optimized for point prediction (e.g., SoftImpute) produce deterministic outputs without modeling predictive distributions. Their uncertainty proxies—derived post-hoc from minor stochasticity—yield overly narrow intervals that fail to match empirical coverage, as measured by Expected Calibration Error (ECE).
- Core assumption: Calibration requires explicit modeling or sampling from the conditional distribution P(X_mis | X_obs, M); point-estimate methods lack this by design.
- Evidence anchors:
  - [abstract] "Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty."
  - [Section V-D] "SoftImpute achieves competitive accuracy but exhibits poor calibration in many settings... As a deterministic low-rank method, it provides only point estimates without modeling uncertainty."
  - [corpus] Weak direct corpus evidence; neighbor papers focus on LLM uncertainty and other domains, not imputation-specific calibration.
- Break condition: If a method is augmented with post-hoc calibration (e.g., conformal prediction), the accuracy–calibration gap may narrow, but this was not evaluated in the paper.

### Mechanism 2
- Claim: MICE provides the most dependable calibration across missingness mechanisms despite not always achieving the best point accuracy.
- Mechanism: MICE iteratively fits per-feature regressions with stochastic draws (coefficient/residual sampling), naturally incorporating residual noise and averaging variability across multiple runs. This yields predictive intervals that better match empirical coverage.
- Core assumption: The chained-equations approach approximates a coherent joint distribution sufficiently for calibration purposes, even if theoretically not guaranteed.
- Evidence anchors:
  - [abstract] "MICE provides the most dependable calibration."
  - [Section V-D] "MICE achieves the most reliable calibration, producing curves near the ideal diagonal and the lowest ECE by incorporating realistic residual noise and averaging variability across multiple runs."
  - [corpus] No direct corpus corroboration; related work on logistic models with missing values (Pattern-by-Pattern) addresses parameter estimation, not calibration.
- Break condition: Under MNAR where missingness depends on unobserved values and the mechanism is not explicitly modeled, MICE calibration may degrade.

### Mechanism 3
- Claim: Sampling-based uncertainty extraction (e.g., MIWAE-S, GAIN-S) captures aleatoric uncertainty efficiently with lower computational cost than repeated training.
- Mechanism: After a single training phase, multiple imputations are drawn from the conditional distribution via latent-space sampling or noise injection. Variance across samples reflects inherent data variability (aleatory) while holding model parameters fixed, avoiding the cost of retraining.
- Core assumption: The trained model accurately approximates the true conditional distribution; misspecification biases uncertainty estimates.
- Evidence anchors:
  - [Section IV-A-2] "This probes aleatoric uncertainty, as the parameters are held fixed and randomness enters through the latent space or injected noise. Compared to repeated runs, this approach is more efficient because training is done only once."
  - [Section V-B] "The sampling-based variants MIWAE-S and GAIN-S are faster overall because they train once and generate multiple imputations from the same model."
  - [corpus] "Diffusion Transformers for Imputation" (arXiv:2510.02216) similarly uses sampling-based uncertainty quantification for diffusion models, supporting the approach generically.
- Break condition: If too few samples are drawn (e.g., <20 for MIWAE-S/GAIN-S, <50 for TabCSDI-S), variance estimates become noisy and ECE may not stabilize (Figure 2 shows plateau around these thresholds).

## Foundational Learning

- Concept: Missingness mechanisms (MCAR, MAR, MNAR)
  - Why needed here: Calibration and accuracy degrade differently across mechanisms; MNAR is hardest unless explicitly modeled.
  - Quick check question: Given a dataset where high-income respondents are less likely to report income, which mechanism applies?

- Concept: Calibration vs. accuracy distinction
  - Why needed here: The paper's central finding is that low MAE does not imply low ECE; practitioners must evaluate both.
  - Quick check question: If a model outputs 95% confidence intervals but only 70% of true values fall within them, is it overconfident or underconfident?

- Concept: Aleatory vs. epistemic uncertainty
  - Why needed here: The paper notes both appear in imputation; sampling-based methods primarily capture aleatory, while repeated runs can probe epistemic.
  - Quick check question: Which type of uncertainty would decrease if you collected more training data?

## Architecture Onboarding

- Component map:
  - Statistical family: MICE (chained regressions, multi-run uncertainty only); SoftImpute (low-rank SVD, deterministic).
  - Distribution alignment: OT-Impute (Sinkhorn divergence, multi-run uncertainty via initialization/minibatch variance).
  - Deep generative: GAIN (adversarial), MIWAE (variational), TabCSDI (diffusion); each supports sampling-based (-S) and some support predictive-distribution (-U) variants.

- Critical path:
  1. Choose uncertainty extraction strategy based on method capabilities (multi-run vs. sampling vs. predictive distribution).
  2. Run imputation with at least n_runs=5 (multi-run) or n_samples=20–50 (sampling) to stabilize variance estimates (Figures 1–2).
  3. Evaluate both MAE (accuracy) and ECE/calibration curves (uncertainty quality) on held-out masked entries.

- Design tradeoffs:
  - Accuracy vs. calibration: SoftImpute has strong accuracy but poor calibration; MICE has strong calibration but lower accuracy; MIWAE balances both.
  - Runtime vs. calibration: Deep generative models (especially TabCSDI) are computationally expensive; sampling variants (-S) reduce cost vs. multi-run but still require many reverse diffusion steps.
  - Missingness mechanism robustness: All methods degrade from MCAR→MAR→MNAR; MICE and MIWAE are more stable.

- Failure signatures:
  - Flat calibration curves (slope ≈0): Indicates constant, uninformative uncertainty (common in SoftImpute).
  - Over-confident curves (below diagonal): Nominal coverage exceeds empirical (e.g., GAIN in some datasets).
  - Under-confident curves (above diagonal): Intervals too wide (e.g., TabCSDI in wine with strong correlations).
  - High ECE with low MAE: Classic accuracy–calibration misalignment.

- First 3 experiments:
  1. Reproduce MICE vs. SoftImpute comparison on a single dataset (e.g., wine at 30% MCAR) to confirm calibration gap; expect MICE ECE lower, SoftImpute MAE competitive.
  2. Run MIWAE vs. MIWAE-S vs. MIWAE-U on the same setting to validate that sampling-based (-S) and predictive-distribution (-U) variants maintain similar MAE while improving or matching calibration.
  3. Test across missingness mechanisms (MCAR, MAR, MNAR) on one dataset to observe calibration degradation; expect ECE to increase from MCAR to MNAR for all methods.

## Open Questions the Paper Calls Out

- How can uncertainty calibration frameworks be extended to categorical and mixed-type tabular data? The paper focused on numeric datasets, and handling heterogeneous attributes requires different forms of uncertainty representation and calibration.

- How can aleatoric and epistemic uncertainty be disentangled in imputation, and do they behave differently across missingness mechanisms? The paper suggests simulation could help distinguish them, as epistemic uncertainty should decrease with more data while aleatoric uncertainty reflects inherent noise.

- Does calibrated imputation uncertainty improve outcomes in downstream tasks such as fairness-sensitive prediction or human-in-the-loop decision making? The paper notes this as an important direction for examining how calibrated uncertainty interacts with fairness, risk assessment, and decision support.

## Limitations

- Limited scope to numerical tabular datasets, excluding categorical and mixed-type data where different uncertainty representations are needed.
- Missingness mechanisms are synthetically injected rather than observed in real-world scenarios, potentially limiting generalizability.
- Does not explore post-hoc calibration techniques (e.g., conformal prediction) that could improve deterministic methods' uncertainty estimates.

## Confidence

- High confidence: MICE provides most dependable calibration across mechanisms; accuracy and calibration are often misaligned; multi-run variability and sampling capture different uncertainty types.
- Medium confidence: MIWAE achieves favorable accuracy-calibration balance; all methods degrade from MCAR→MAR→MNAR; specific n-runs/n-samples thresholds stabilize estimates.
- Low confidence: TabCSDI's poor calibration in wine dataset is solely due to strong correlations; MICE remains stable under true unknown MNAR mechanisms.

## Next Checks

1. Validate MICE's calibration advantage on categorical, text, and high-dimensional data beyond numerical tabular formats.
2. Apply methods to datasets with known MNAR patterns (e.g., survey non-response) to test degradation predictions and potential mitigation strategies.
3. Implement conformal prediction or temperature scaling on deterministic methods (SoftImpute) to quantify potential accuracy-calibration gap reduction.