---
ver: rpa2
title: 'SVIP: Towards Verifiable Inference of Open-source Large Language Models'
arxiv_id: '2410.22307'
source_url: https://arxiv.org/abs/2410.22307
tags:
- protocol
- provider
- computing
- inference
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of verifiable inference for large
  language models (LLMs) in decentralized computing, where a provider might substitute
  a smaller, cheaper model for the requested one without the user's knowledge. The
  core method idea is SVIP, a secret-based verifiable LLM inference protocol that
  leverages processed hidden representations from LLMs.
---

# SVIP: Towards Verifiable Inference of Open-source Large Language Models

## Quick Facts
- arXiv ID: 2410.22307
- Source URL: https://arxiv.org/abs/2410.22307
- Authors: Yifan Sun; Yuhang Li; Yue Zhang; Yuchen Jin; Huan Zhang
- Reference count: 40
- Primary result: Protocol achieves FNR <5%, FPR <3%, verification time <0.01s per query, resistant to adapter and secret recovery attacks

## Executive Summary
This paper introduces SVIP, a secret-based verifiable inference protocol for detecting when decentralized computing providers substitute smaller, cheaper LLMs for requested larger models. The protocol leverages processed hidden representations from LLMs, training a proxy task to transform these into unique model identifiers. A secret mechanism enhances security by making it difficult for providers to optimize forged verification vectors. The method achieves strong empirical performance with false negative rates below 5% and false positive rates below 3%, while maintaining sub-0.01 second verification latency per query.

## Method Summary
SVIP is a verifiable inference protocol that requires providers to return both generated text and processed hidden representations from the specified LLM. The platform trains a labeling network with contrastive loss to produce secret-dependent labels, and a proxy task (feature extractor, head, and secret embedding network) to compress hidden states into verification vectors. During inference, the provider computes the proxy task on last-layer hidden states concatenated with a secret embedding, returning only the compressed vector. The user verifies by comparing the predicted label from the proxy task head with the true label from the labeling network, using thresholded L2 distance. Aggregation over multiple queries drives error probabilities to near-zero exponentially.

## Key Results
- False negative rate below 5% and false positive rate below 3% across 5 specified models and 6 alternatives
- Less than 0.01 seconds per prompt query for verification
- Resistant to adapter attacks (ASR < 50% with M=400 samples) and secret recovery attacks
- Type-I error of 1.7×10⁻⁴⁹ and type-II error of 0 when using 30 queries with τ=0.5 on Llama-3.1-70B vs Llama-2-7B

## Why This Works (Mechanism)

### Mechanism 1: Proxy Task as Model Fingerprint
Hidden representations from different LLMs exhibit sufficiently distinct distributions that a learned proxy task can discriminate the specified model from alternatives. A feature extractor compresses last-layer hidden states to a ~4KB vector, and a proxy task head is trained exclusively on representations from the specified model. At inference, matching predictions signal the specified model was likely used. This relies on architectural differences producing separable hidden state patterns.

### Mechanism 2: Secret-Based Label Indistinguishability
A user-specific secret injected into the labeling process prevents computing providers from optimizing forged verification vectors. A trainable labeling network produces different labels for the same input under different secrets, while providers receive only secret embeddings. Without the secret, providers cannot compute true labels needed to optimize bypasses. This ensures Property 1 (Secret Distinguishability): P(y(x,s) ≠ y(x,s')) ≥ δ for s ≠ s'.

### Mechanism 3: Exponential Error Decay via Query Aggregation
Aggregating verification outcomes over B independent queries drives type-I and type-II error probabilities to near-zero exponentially. Per-query verification yields Bernoulli outcomes, and by Hoeffding's inequality, when decision threshold τ satisfies FPR < τ < 1-FNR, errors decay as exp(-2B(p-τ)²). With FNR~3-5% and FPR~1-3%, B=30 queries yields error rates below 10⁻⁵.

## Foundational Learning

- **Hidden State Representations in Transformers**: Understanding that last-layer hidden states h_M(x) ∈ R^(L×d_M) encode semantic information specific to model architecture is essential. Why needed: The protocol depends on extracting these representations. Quick check: Given a prompt "The capital of France is", explain why GPT-2-1.5B and Llama-2-13B would produce different hidden state distributions even if their output tokens were identical.

- **Binary Classification Error Rates (FNR/FPR)**: Protocol evaluation centers on controlling false negative rate (failing to detect cheating) and false positive rate (falsely accusing honest providers). Why needed: Core performance metrics. Quick check: If a protocol has FNR=5% and FPR=3%, what is the probability of correctly classifying 30 independent queries from an honest provider?

- **Contrastive Learning Objectives**: The labeling network uses contrastive loss to ensure y(x,s) ≠ y(x,s') for different secrets, enabling security against guessing attacks. Why needed: Core to secret distinguishability property. Quick check: Why might a contrastive objective be preferable to a classification objective for learning distinguishable labels across secrets?

## Architecture Onboarding

- Component map:
  Platform (trusted): Labeling Network y_γ: text + secret → label vector (128-dim) -> Proxy Task Feature Extractor g_θ: hidden states + secret embedding → compressed vector (1024-dim) -> Proxy Task Head f_φ: compressed vector → predicted label -> Secret Embedding Network t_ψ: secret → embedding for provider
  Computing Provider (untrusted): Runs: LLM inference → h_M(x) → concatenate with t_ψ(s) → g_θ → return z(x)
  User: Computes: y_γ(x, s) locally → compare with f_φ(z(x)) via L2 distance

- Critical path:
  1. Platform trains labeling network y_γ with contrastive loss on 100K samples × 8 secrets
  2. Platform trains proxy task (g_θ, f_φ, t_ψ) on 150K samples × 4 secrets
  3. Platform distributes g_θ and t_ψ to provider; retains f_φ and y_γ
  4. During inference, provider returns z(x); user verifies by checking ||f_φ(z(x)) - y_γ(x,s)||² ≤ η
  5. Aggregate over B queries; declare honest if mean verification ≥ τ

- Design tradeoffs:
  - Compression dimension d_g: Higher (2048+) improves discriminability but increases communication; paper uses 1024 (~4KB)
  - Secret dimension d_s: Paper uses 48 bits; larger increases security but complicates recovery attack analysis
  - Threshold η: Set at 95th percentile of honest distribution to fix FNR at 5%; stricter threshold reduces FNR but increases FPR

- Failure signatures:
  - FNR > 10%: Threshold η may be too strict, or proxy task underfitting on specified model
  - FPR > 5% on specific alternatives: Alternative model may share architectural features; consider retraining with augmented data
  - Attack success rate > 50%: Secret may be compromised; trigger protocol retraining
  - Cross-dataset FNR spike: Distribution shift between training and deployment; retrain on broader corpus

- First 3 experiments:
  1. Replicate FNR/FPR evaluation on Llama-2-13B vs Llama-2-7B using LMSYS-Chat-1M subset (10K test samples). Verify L2 distance distributions are separable as shown in Figure 3.
  2. Implement adapter attack (Section 3.3) with M=400 samples under single secret. Confirm ASR < 50% threshold holds. This validates security margin before deployment.
  3. Measure end-to-end verification latency on a single L40S GPU. Target: < 0.01s per query for both provider-side (g_θ) and user-side (f_φ + y_γ) computation. Profile memory footprint to confirm <1.5GB user-side requirement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating hidden representations that jointly encode both the prompt and the model's completion provide additional security margins or robustness against adaptive attacks compared to the prompt-only approach?
- Basis in paper: [explicit] Appendix B.1 (Limitations and Future Work) states, "It may nonetheless be of interest to explore whether incorporating representations that jointly encode both the prompt and the specified model’s completion could offer additional security margins under certain threat models."
- Why unresolved: The current design focuses exclusively on prompt-side hidden representations to minimize overhead and complexity, leaving the potential utility of completion-side signals unexplored.
- What evidence would resolve it: Experimental results comparing FPR and robustness against adapter attacks when the proxy task utilizes concatenated prompt and completion hidden states versus prompt-only states.

### Open Question 2
- Question: Can a hybrid protocol be developed that combines SVIP's efficiency with formal cryptographic security guarantees to bridge the gap between empirical robustness and theoretical soundness?
- Basis in paper: [inferred] Appendix B.1 notes that unlike cryptographic verifiable computation (VC) techniques, SVIP "does not offer a strict security guarantee" and contrasts this with the "prohibitively high computational overheads" of strict methods like Zero-Knowledge Proofs.
- Why unresolved: The paper positions SVIP as a practical trade-off, explicitly sidestepping the computational costs associated with cryptographic proofs, thus leaving the integration of these two paradigms as an open challenge.
- What evidence would resolve it: A proposed protocol modification that introduces a lightweight cryptographic commitment or proof step, along with an analysis showing it provides provable security bounds without exceeding the sub-0.01s latency target.

### Open Question 3
- Question: How robust is the labeling network's "Secret Distinguishability" (Property 1) when subjected to severe out-of-distribution (OOD) inputs, such as rare languages or highly specialized code, that were not represented in the conversation training data?
- Basis in paper: [inferred] Section 3.2 relies on a learned labeling network to enforce Property 1 ($y(x,s) \neq y(x,s')$). While Appendix E.3 shows generalization to toxic and web questions, the paper does not test scenarios where the semantic structure of the input $x$ might cause the labeling network to collapse or fail to differentiate secrets.
- Why unresolved: The security of the secret mechanism hinges entirely on the labeling network's ability to generate distinct labels for different secrets given any input; failure on OOD inputs would render the protocol vulnerable.
- What evidence would resolve it: Evaluation of the contrastive loss and label distinctiveness on datasets significantly divergent from LMSYS-Chat-1M (e.g., low-resource languages or structured programming datasets) to ensure the probability $\delta$ remains high.

## Limitations
- Protocol effectiveness validated primarily on 5-6 model pairs within 1.5B-70B parameter range; scalability to hundreds of diverse models remains unproven
- Assumes consistent provider behavior across queries; strategic interleaving of honest and cheating behavior may require adaptation
- 1.5GB user-side memory requirement may limit deployment on resource-constrained edge devices
- Long-term security relies on secret embeddings remaining computationally difficult to invert as attack techniques evolve

## Confidence
- **High Confidence**: Core mechanisms (proxy task discriminability, secret-based label indistinguishability, exponential error decay) are theoretically grounded with empirical validation across multiple model pairs
- **Medium Confidence**: Security guarantees against adaptive attacks rely on specific assumptions about provider capabilities and secret management
- **Low Confidence**: Performance under extreme conditions (extremely similar architectures, cross-dataset distribution shifts, provider collusion) has not been empirically validated

## Next Checks
1. **Cross-architecture validation**: Test SVIP's discriminability between architecturally divergent models (sparse vs dense transformers, encoder-only vs decoder-only architectures) using a broader model set (10+ pairs). Measure FNR/FPR degradation and identify architectural feature correlations.

2. **Adaptive attack stress test**: Implement an iterative optimization attack where the provider gradually refines forged verification vectors based on partial feedback from failed verification attempts. Determine minimum queries needed to achieve >90% attack success rate and assess protocol retraining frequency practicality.

3. **Resource-constrained deployment analysis**: Port user-side verification components (f_φ and y_γ) to a mobile-class device (e.g., Raspberry Pi 4). Measure actual memory consumption, verification latency under real-time constraints, and identify optimization opportunities (quantization, pruning) that maintain verification accuracy above 95%.