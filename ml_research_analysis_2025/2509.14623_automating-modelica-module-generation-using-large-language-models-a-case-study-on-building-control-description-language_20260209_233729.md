---
ver: rpa2
title: 'Automating Modelica Module Generation Using Large Language Models: A Case
  Study on Building Control Description Language'
arxiv_id: '2509.14623'
source_url: https://arxiv.org/abs/2509.14623
tags:
- control
- modelica
- controls
- buildings
- connect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the potential of large language models to
  automate the generation of Modelica-based control modules. The results demonstrate
  that for four representative logic tasks, Claude-Sonnet-4 achieved up to 100% success
  with carefully engineered prompts, whereas GPT-4o failed consistently in zero-shot
  mode.
---

# Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language

## Quick Facts
- arXiv ID: 2509.14623
- Source URL: https://arxiv.org/abs/2509.14623
- Reference count: 22
- LLMs with structured prompts reduced expert development time from 10-20 hours to 4-6 hours per module (40%-60% time savings)

## Executive Summary
This study evaluated the potential of large language models to automate the generation of Modelica-based control modules. The results demonstrate that for four representative logic tasks, Claude-Sonnet-4 achieved up to 100% success with carefully engineered prompts, whereas GPT-4o failed consistently in zero-shot mode. Extending the evaluation to five building control tasks, the overall pipeline attained a best-case success rate of 83%, although the remaining percent (17%) of the modules required human debugging to correct logic errors or submodule mismatches. Two strategies for library grounding were compared: retrieval-augmented generation and hard-rule search. While retrieval-augmented generation frequently mis-selected modules (e.g., confusing "And" with "Or"), the deterministic hard-rule search eliminated such errors entirely. Furthermore, attempts to incorporate AI-based evaluation revealed that while LLMs could detect syntax errors, they lacked the capability to verify simulation correctness, leaving human evaluators indispensable for assessing functional behavior. Despite these limitations, the LLM-assisted workflow produced tangible productivity gains. The amount of required expert development effort was reduced from 10–20 hours to 4–6 hours per module, corresponding to 40%–60% time savings. At an assumed labor rate of $100 per hour, this translates into a potential cost reduction of $600–$1,600 per module, or $30,000–$160,000 across 50–100 modules. These findings highlight both the promise and current boundaries of AI-assisted modeling, indicating that while human oversight remains critical, structured LLM workflows can already accelerate the development of building control modules.

## Method Summary
The workflow combines standardized prompt scaffolds, deterministic library grounding via hard-rule search, automated compilation with OpenModelica, and human-in-the-loop evaluation. The pipeline takes natural language descriptions of control logic, decomposes them into CDL library primitives using exact name matching, generates Modelica code through a three-step prompt template (goal, interface, logic), validates syntax through OpenModelica API compilation, and loops back to the LLM for corrections when errors are detected. Two grounding strategies were compared: retrieval-augmented generation (RAG) which often selected semantically similar but functionally incorrect modules, and hard-rule search which ensures exact module matches but limits scope to existing primitives.

## Key Results
- Claude-Sonnet-4 achieved up to 100% success rate on basic logic tasks with engineered prompts, while GPT-4o failed in zero-shot mode
- The overall pipeline achieved 83% success rate on five building control modules, with 17% requiring human debugging
- Hard-rule search eliminated submodule selection errors that plagued RAG (e.g., selecting "Or" instead of "And")
- LLM-assisted development reduced expert time from 10-20 hours to 4-6 hours per module (40%-60% savings)

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Library Grounding (Hard-Rule Search)
Replacing semantic retrieval with deterministic indexing appears to eliminate submodule selection errors, provided the target module exists in the library. The workflow bypasses the probabilistic nature of Retrieval-Augmented Generation (RAG) by indexing the Control Description Language (CDL) library and enforcing exact name matching. This prevents the LLM from selecting semantically similar but functionally opposite blocks (e.g., selecting "Or" when prompted for "And" because the documentation contains the word "and"). Core assumption: The user's requested control logic can be decomposed into existing library primitives; novel logic requiring new primitives would fail or require manual intervention. Break condition: If the required control sequence depends on a module missing from the indexed library, the hard-rule search fails to retrieve a match, stalling the pipeline.

### Mechanism 2: Constrained Prompt Scaffolding
Enforcing a strict "Goal-Interface-Logic" prompt structure enables higher success rates in code generation compared to zero-shot instructions. The prompt forces the LLM to process the request in three discrete steps: defining the objective, declaring strictly typed input/output interfaces, and listing the logic sequence. This reduces the cognitive load on the model regarding variable naming conventions and type safety, which are critical in Modelica. Core assumption: The LLM has sufficient pre-trained knowledge of Modelica syntax to follow the structural guide; without this pre-training (as seen with weaker models), the scaffold is insufficient. Break condition: If the "Logic" description in the prompt is ambiguous regarding signal polarity (e.g., "open valve if low"), the LLM may invert the logic (e.g., implementing `if < setpoint` instead of `if > setpoint`), a error class noted in the discussion.

### Mechanism 3: Iterative Syntax Validation via OpenModelica API
An automated feedback loop using a compiler (OpenModelica) allows the LLM to self-correct syntax errors, but it cannot verify behavioral correctness. The system feeds compilation error logs back to the LLM ("Iteration LLM") to refine the code. This creates a closed loop for syntax compliance, ensuring the output is at least executable code, even if the logic is flawed. Core assumption: The error logs provided by the compiler are interpretable by the LLM; complex structural errors might confuse the model, leading to infinite repair loops. Break condition: When errors are functional (e.g., a misplaced hysteresis threshold) rather than syntactic, the compiler returns "success," and the loop terminates with logically invalid code.

## Foundational Learning

- **Modelica & CDL (Control Description Language)**
  - Why needed here: Modelica is an acausal, equation-based object-oriented language. Unlike procedural coding, you define relationships (equations) rather than instruction flows. CDL is a standardized subset/library for building controls.
  - Quick check question: If I define a variable `Real T;` in Modelica, do I assign it a value (e.g., `T = 5`) or define its relationship (e.g., `der(T) = 5`)?

- **Hard-Rule vs. Semantic Search (RAG)**
  - Why needed here: Standard LLMs rely on semantic similarity. In technical domains, "close enough" (semantic match) is often "wrong" (functional mismatch).
  - Quick check question: If I search for "Boolean False," would a semantic search potentially return "Boolean True" because they are conceptually related?

- **Evaluation Gap (Syntax vs. Behavior)**
  - Why needed here: An LLM can generate code that compiles (syntax) but does the wrong thing (behavior). This paper highlights that current LLMs cannot verify the *intent* of control logic (e.g., checking if a valve opens when it gets hot).
  - Quick check question: A script compiles successfully. Does this guarantee the building temperature will be maintained correctly?

## Architecture Onboarding

- **Component map:** User Requirement -> Orchestrator -> Knowledge Base (indexed CDL library) -> Generator (LLM 1) -> Validator (OpenModelica API) -> Refiner (LLM 2) -> Evaluator (Human Expert)
- **Critical path:** The **Hard-Rule Search** is the most critical reliability component. If this retrieves the wrong module, the Generator creates structurally valid but functionally wrong code, and the Validator cannot catch it.
- **Design tradeoffs:**
  - **RAG vs. Hard-Rule:** RAG allows for flexible discovery of new/complex modules but risks confusion. Hard-Rule guarantees validity but limits scope to the indexed library.
  - **AI vs. Human Eval:** AI evaluation is fast and cheap but currently unreliable for logic. Human eval is slow/expensive but required for sign-off.
- **Failure signatures:**
  - **"And/Or" Confusion:** The generated code compiles, but the logic uses the wrong boolean operator (RAG failure).
  - **Logic Inversion:** The code compiles, but the control signal acts backwards (e.g., opens valve when it should close) due to prompt ambiguity.
  - **Version Drift:** The code references `Buildings.Controls.OBC.CDL.Continuous` (v9) instead of `Reals` (v10), causing compilation failure.
- **First 3 experiments:**
  1. **Library Search Validation:** Implement the "And" retrieval task. Compare results from a standard vector-search (RAG) vs. a direct dictionary lookup (Hard-Rule) to quantify the error rate difference.
  2. **Prompt Sensitivity Test:** Generate a "Switch" module using zero-shot prompts vs. the 3-step scaffold. Measure compilation success rates.
  3. **Correction Loop Limit:** Intentionally inject a syntax error into a valid Modelica file and feed it to the LLM Refiner. Determine if it can fix the error or if it hallucinates fixes that introduce new errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be augmented to autonomously verify behavioral correctness by analyzing time-series simulation outputs against control requirements?
- Basis in paper: [explicit] The paper states, "automated results interpretation remains a critical gap" and current LLMs "lacked the capability to verify simulation correctness."
- Why unresolved: Current models can detect syntax errors but fail to assess functional behavior, rendering human evaluators indispensable for assessing simulation results.
- What evidence would resolve it: A demonstrated workflow where LLM evaluation of time-series data matches human expert judgment on behavioral correctness benchmarks.

### Open Question 2
- Question: Does the implementation of closed-loop validation frameworks enable LLMs to iteratively refine modules with minimal human intervention?
- Basis in paper: [explicit] Future work identifies "closed-loop validation frameworks" as a necessary step for enabling LLMs to "generate but also iteratively validate and refine modules."
- Why unresolved: The current workflow relies on static generation followed by human-in-the-loop evaluation rather than an autonomous, self-correcting system.
- What evidence would resolve it: Successful execution of a fully automated closed-loop pipeline achieving high success rates without manual code repair.

### Open Question 3
- Question: Can incorporating few-shot prompting with canonical library patterns align the granularity of LLM-generated modules with human-engineered standards?
- Basis in paper: [inferred] The paper observes AI models favor "widely applicable primitives" over "library-specific composite patterns," reducing reusability compared to human implementations.
- Why unresolved: The study tested standardized prompt scaffolds but did not validate specific strategies, such as few-shoting canonical patterns, to enforce structural standards.
- What evidence would resolve it: Comparative analysis showing generated modules utilizing high-level composite blocks (e.g., `GreaterThreshold`) rather than primitive assemblies.

## Limitations
- The 17% failure rate indicates human intervention remains necessary for debugging logic errors and submodule mismatches
- Current LLMs cannot verify behavioral correctness of generated modules, only syntax, requiring human expert validation
- The deterministic hard-rule search constrains the system to pre-existing library primitives, potentially limiting innovation

## Confidence

- **High Confidence:** The deterministic hard-rule search reliably eliminates submodule selection errors compared to RAG; the standardized prompt scaffold demonstrably improves generation success rates.
- **Medium Confidence:** The reported productivity gains and cost savings are based on reasonable assumptions but may not generalize across all organizational contexts.
- **Low Confidence:** The evaluation of AI-based behavioral correctness assessment is limited, as the study found LLMs currently inadequate for this task.

## Next Checks
1. Test the pipeline's performance on Modelica applications outside building controls to assess generalizability.
2. Implement a systematic evaluation of the hard-rule search's coverage limitations by attempting to generate modules requiring novel primitives not in the indexed library.
3. Conduct a controlled experiment comparing human vs. AI evaluation accuracy for detecting subtle logic errors that compile successfully but produce incorrect behavior.