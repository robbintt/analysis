---
ver: rpa2
title: Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models
arxiv_id: '2505.02763'
source_url: https://arxiv.org/abs/2505.02763
tags:
- legal
- bluebook
- citation
- case
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates large language models\u2019 (LLMs) ability\
  \ to follow complex legal citation rules by constructing a dataset of 866 Bluebook\
  \ tasks. Five flagship LLMs were tested in zero-shot settings, with Gemini 2.5 Flash\
  \ also evaluated using in-context learning on the Bluebook\u2019s rules."
---

# Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models

## Quick Facts
- **arXiv ID**: 2505.02763
- **Source URL**: https://arxiv.org/abs/2505.02763
- **Reference count**: 40
- **Primary result**: LLMs achieve only 69-77% accuracy on Bluebook citation tasks, suggesting they are not yet reliable for automating legal procedural compliance.

## Executive Summary
This paper evaluates whether large language models can reliably automate legal citation formatting according to the Bluebook's complex rules. Testing five flagship models on 866 citation tasks reveals that even with access to the full rule document, LLMs achieve only 69-77% accuracy—far below the reliability threshold needed for legal practice. The models perform best on familiar case law citations (often relying on memorization rather than rule application) but struggle significantly with statutes, regulations, and interpretive elements like citation signals. These findings suggest that LLMs should not be used to automate aspects of legal practice where procedural fidelity is critical.

## Method Summary
The study evaluates LLM performance on Bluebook citation tasks through zero-shot inference and in-context learning with the 90k-token Indigo Book rule document. Five flagship models (GPT 4.1, Claude 3.5 Sonnet, Gemini 2.5 Flash, Llama 3.1 405B, DeepSeek V3 0324) are tested across 866 tasks covering 16 task types derived from legal education materials. Gemini 2.5 Flash is additionally tested with in-context rule access. Performance is measured using exact string matching against expert-provided ground truth, with minor stylistic variations ignored. The study distinguishes between memorization-based performance (tested by altering case details) and genuine rule application.

## Key Results
- LLMs produce fully Bluebook-compliant citations only 69%-74% of the time in zero-shot settings, rising to 77% with in-context rule access.
- Models perform best on case law citations (often relying on memorization) but worst on statutes, regulations, and citation signals requiring interpretive judgment.
- Errors are typically non-trivial, requiring an average of 14 character edits to correct when they occur.
- In-context learning with 90k tokens of rules yields only a 6 percentage point improvement, suggesting limited ability to effectively retrieve and apply complex procedural rules.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance on complex procedural rules may depend heavily on prior memorization of specific examples rather than de novo rule application.
- Mechanism: When LLMs succeed on case law citation tasks, they often rely on pattern-matching citations memorized from training data. Performance degrades when real case details are altered to synthetic values, breaking the memorized pattern.
- Core assumption: The Bluebook tasks contain some cases and citations likely present in LLM training corpora.
- Evidence anchors:
  - [abstract] "LLMs performed best on case law citations (often relying on memorization rather than rule application)"
  - [section 6.2] "most of the LLMs are better at creating citations to the real cases than to the synthetic ones... The gap is greatest in Claude 3.5 Sonnet"
  - [corpus] Weak/missing; corpus papers focus on legal reasoning broadly, not memorization vs. rule application specifically.
- Break condition: If evaluation uses only novel procedural documents unseen during training, memorization-driven performance will significantly degrade.

### Mechanism 2
- Claim: Providing long-context procedural rules in-context yields only modest accuracy improvements.
- Mechanism: LLMs struggle to effectively retrieve and apply relevant rules from a 90k token context window to novel inputs. Even when the full rule set is provided, models cannot consistently identify and execute applicable rules.
- Core assumption: Models should utilize their advertised context window to process and apply lengthy procedural documents.
- Evidence anchors:
  - [abstract] "in-context learning on the Bluebook rules raised accuracy only modestly to 77%"
  - [section 6.6] "Average accuracy increases only six percentage points, from 71% in the zero-shot setup to 77%"
  - [corpus] "Legal Rule Induction" paper discusses inducing rules but doesn't test long-context rule application.
- Break condition: For rule systems significantly longer than 90k tokens or models with weaker long-context capabilities, in-context gains may approach zero.

### Mechanism 3
- Claim: Performance degrades when procedural tasks require interpretive judgment rather than mechanical formatting.
- Mechanism: Tasks with deterministic rules (e.g., reporter abbreviations) see higher accuracy than tasks requiring subjective interpretation (e.g., choosing citation "signals" based on degree of support).
- Core assumption: Certain legal procedure aspects inherently involve interpretation that resists automation by pattern-completion systems.
- Evidence anchors:
  - [abstract] "LLMs performed best on case law citations... and worst on electronic statutes and signals requiring interpretive judgment"
  - [section 6.4] "Signals therefore involve an element of legal reasoning... even humans do not always agree about which situations call for which signal"
  - [corpus] Corpus discusses interpretation challenges broadly but doesn't isolate interpretive vs. mechanical tasks.
- Break condition: For purely mechanical procedures, LLM accuracy approaches ceiling; for heavily interpretive tasks, it approaches chance.

## Foundational Learning

- Concept: **Trans-substantive procedural law**
  - Why needed here: To understand why citation rules are considered "procedure" and serve as a proxy for other procedural systems.
  - Quick check question: What three properties does the paper identify that citation shares with other legal procedures?

- Concept: **Citation signals**
  - Why needed here: Signals are a key failure point illustrating interpretive challenges in legal procedure.
  - Quick check question: What does the paper identify as the core difficulty LLMs face with signal selection?

- Concept: **Context window utilization vs. advertised capacity**
  - Why needed here: The in-context experiment tests whether models can actually use their full context length.
  - Quick check question: What token length was the rule document, and what accuracy gain did it yield?

## Architecture Onboarding

- Component map: Input (case captions, statutory references) -> Model (with optional 90k-token rule context) -> Citation String Generation -> Exact String Matching Evaluation

- Critical path: Input → Model → Citation String → Exact Match Evaluation (Section 5: "correctness is assessed using exact string matching").

- Design tradeoffs:
  - **Cloze vs. Open format**: Cloze isolates rule components but may enable memorization; open tests full construction but complicates error attribution.
  - **Lenient vs. Strict scoring**: Ignoring italicization focuses on semantics; strict scoring is realistic but yields worse results.
  - **Zero-shot vs. In-context**: Zero-shot reflects off-the-shelf usability; in-context tests rule-following independent of memorization.

- Failure signatures:
  - **Hallucinated citations**: Non-existent reporter citations.
  - **Jurisdictional errors**: Misidentifying courts or historical structures.
  - **Publisher confusion**: Incorrect statutory abbreviations.
  - **Signal misapplication**: Mischaracterizing degree of support.

- First 3 experiments:
  1. **Zero-shot baseline**: Query all models on 866 tasks without rule context to establish procedural compliance.
  2. **Memorization probe**: Restructure case law tasks with synthetic page numbers to distinguish memorization from rule application.
  3. **Long-context rule following**: Provide 90k-token rules to best long-context model; measure per-task gains to identify which rule types benefit most.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do LLMs' Bluebook formatting abilities generalize to other procedural tasks with strict compliance requirements, such as adhering to filing deadlines or jurisdiction-specific rules?
- Basis in paper: [explicit] The conclusion states that "future research should... mov[e] beyond Bluebook tasks into other forms of procedure... such as adhering to filing deadlines, following jurisdiction-specific rules, and producing judicially acceptable documents."
- Why unresolved: This study focused exclusively on the Bluebook as a proxy for legal procedure; it did not test other procedural systems (e.g., Federal Rules of Civil Procedure) which are noted to be comparable in length and complexity.
- What evidence would resolve it: An evaluation of LLM performance on a dataset of tasks involving filing deadlines, court form generation, or other procedural rule-sets distinct from citation formatting.

### Open Question 2
- Question: Can LLMs reliably perform the retrieval and verification of cited material ("source checking") required for legal practice, in addition to formatting citations correctly?
- Basis in paper: [explicit] Section 7 (Limitations) explicitly notes that the paper does not "evaluate LLMs' abilities to perform 'source checking'... verifying a citation's referenced material for its existence and content is... crucial, but this paper does not delve into this complicated retrieval task."
- Why unresolved: The current study isolated citation formatting from the retrieval task to focus on rule adherence, leaving the intersection of retrieval and formatting untested.
- What evidence would resolve it: A study testing LLMs on their ability to not only format a citation but also locate the source text and verify the accuracy of the content referenced (e.g., confirming a pin cite actually supports the proposition).

### Open Question 3
- Question: Would a bespoke legal LLM, fine-tuned on a massive dataset of expert-verified Bluebook examples, outperform the general-purpose flagship models tested?
- Basis in paper: [explicit] Section 7 states: "We leave efforts to build a bespoke Bluebook formatting model to future work." The authors note that while fine-tuning would likely produce gains, "a huge number of examples... would be required."
- Why unresolved: The study focused on zero-shot and in-context learning using off-the-shelf models, deliberately avoiding the fine-tuning approach to test general reasoning and rule-following.
- What evidence would resolve it: An experiment fine-tuning a smaller model (e.g., Llama 3.1) on a dataset of thousands of Bluebook citations and comparing its accuracy against the zero-shot baselines established in this paper.

### Open Question 4
- Question: Do "reasoning" models (like GPT-o3) or chain-of-thought prompting improve performance on complex procedural tasks compared to standard models?
- Basis in paper: [inferred] Section 5 and Appendix C.1 explicitly exclude "thinking" models where the functionality cannot be disabled (e.g., GPT-o3). The paper concludes that current LLMs struggle with strict procedure, implying a question of whether architectures designed for complex reasoning would fare better.
- Why unresolved: The tested models were standard flagships (e.g., GPT-4o, Claude 3.5 Sonnet without thinking enabled); models optimized for explicit reasoning steps were outside the scope of the evaluation.
- What evidence would resolve it: A comparative evaluation running the same 866 Bluebook tasks on reasoning-enabled models (e.g., o1 or o3) to see if "thinking" through the rules reduces hallucination or formatting errors.

## Limitations

- The evaluation relies on exact string matching, which may understate LLM capabilities in practical applications where minor formatting variations are acceptable.
- The test corpus of 866 tasks, while comprehensive in task types, may not capture the full diversity of legal citation scenarios encountered in practice.
- The study focuses exclusively on zero-shot and simple in-context learning, leaving open questions about whether fine-tuning or retrieval-augmented approaches might yield substantially different results.

## Confidence

- **High Confidence**: The finding that LLMs cannot achieve reliable procedural compliance (69-77% accuracy) with existing citation rules.
- **Medium Confidence**: The conclusion that in-context learning provides only modest improvements (6 percentage points).
- **Medium Confidence**: The observation that memorization, not rule application, drives much of the apparent success on case law tasks.

## Next Checks

1. **Real-world Deployment Test**: Evaluate LLM-generated citations in actual legal documents prepared by practicing attorneys, measuring not just format compliance but also substantive accuracy and practical utility.

2. **Alternative Learning Paradigms**: Test whether fine-tuning on Bluebook examples or retrieval-augmented approaches outperform the zero-shot and in-context learning methods examined, potentially narrowing the performance gap.

3. **Cross-Domain Generalization**: Apply the same evaluation framework to other procedural rule sets (e.g., civil procedure, evidence rules) to determine whether the observed limitations are specific to citation formatting or represent broader constraints on LLM procedural compliance.