---
ver: rpa2
title: 'Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition
  with Mixture of Decoders'
arxiv_id: '2505.21364'
source_url: https://arxiv.org/abs/2505.21364
tags:
- mxds
- sparse
- expert
- experts
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Mixture of Decoders (MxDs), a sparse layer architecture
  designed to overcome the accuracy trade-off in existing sparse MLP approximations.
  MxDs generalize MLPs and GLUs, expanding pre-trained dense layers into thousands
  of specialized sublayers using a Hadamard product-based tensor factorization.
---

# Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders

## Quick Facts
- arXiv ID: 2505.21364
- Source URL: https://arxiv.org/abs/2505.21364
- Reference count: 40
- Primary result: MxDs significantly outperform sparse MLP baselines on the sparsity-accuracy frontier while maintaining interpretability

## Executive Summary
This paper introduces Mixture of Decoders (MxDs), a sparse layer architecture that overcomes the accuracy trade-off inherent in existing sparse MLP approximations. By generalizing both MLPs and GLUs through Hadamard product-based tensor factorization, MxDs expand pre-trained dense layers into thousands of specialized sublayers while preserving full-rank weights for each expert. This design enables faithful reconstruction of original layer mappings even under heavy sparsity, achieving state-of-the-art performance on the sparsity-accuracy frontier across four LLMs (up to 3B parameters).

## Method Summary
MxDs decompose pre-trained dense MLP layers by expanding them into a mixture of thousands of specialized sublayers. The architecture uses a Hadamard product factorization where each expert weight tensor is constructed as a column-scaled version of a shared base decoder. A gating network routes each input to the top K experts using a learned routing mechanism. During training, only MxD parameters are optimized to minimize normalized reconstruction loss against the frozen original MLP outputs. The method maintains full expressive capacity through rank-preserving weight construction and achieves interpretability through expert specialization induced by top-K routing.

## Key Results
- MxDs significantly outperform Transcoders and Skip Transcoders on the sparsity-accuracy frontier across four LLM families
- The architecture preserves competitive performance on sparse probing and feature steering tasks
- MxDs naturally learn specialized feature representations while maintaining model faithfulness
- Experiments demonstrate effectiveness on models ranging from 124M to 3B parameters

## Why This Works (Mechanism)

### Mechanism 1
The MxD architecture maintains the full expressive capacity of the original MLP layer despite high sparsity through Hadamard product factorization. Each expert matrix is constructed as a column-scaled version of a shared base decoder, preserving the rank of the original weights. This rank preservation ensures that even with few active experts, the output remains a full-rank transformation of the input.

### Mechanism 2
Top-K gating induces semantic specialization by isolating gradient updates to specific expert sub-layers. The gating network routes tokens to only the top K experts, and during backpropagation, gradients for each expert are generated exclusively from the tokens routed to it. This implicit clustering forces experts to specialize in specific features or subcomputations of the language.

### Mechanism 3
Layer-level sparsity outperforms neuron-level sparsity for faithfulness by avoiding dimensional bottlenecks. Standard sparse MLPs force outputs to lie in K-dimensional subspaces by zeroing out hidden units, while MxDs sum K full-rank linear transformations. This architectural difference allows MxDs to maintain faithful reconstruction even with low sparsity levels.

## Foundational Learning

### Concept: Matrix Rank and Linear Independence
- **Why needed here:** Understanding Lemma 1 requires knowing that scaling the columns of a matrix (multiplying by a diagonal matrix) does not change its rank, which is the theoretical engine behind MxD's capacity preservation.
- **Quick check question:** If you multiply a matrix $D$ by a diagonal matrix with a zero on the diagonal, does the rank of the product increase, decrease, or stay the same?

### Concept: Hadamard Product
- **Why needed here:** The core parameterization uses element-wise multiplication ($*$) to blend the base decoder $D$ with expert-specific modulators $C$. This operation is fundamental to how the "mixture" is computed efficiently.
- **Quick check question:** Is the Hadamard product of two vectors a scalar or a vector?

### Concept: Conditional Computation (MoE)
- **Why needed here:** MxD is a variant of Mixture of Experts. You must understand the concept of a "router" or "gate" that decides which subset of parameters (experts) are active for a given input.
- **Quick check question:** In a standard MoE layer with Top-2 routing, how many experts contribute to the final output for a single token?

## Architecture Onboarding

### Component map:
Input $x$ -> **Gating Network** (select Top-K experts) AND **Encoder** (compute hidden $z$) -> **Modulator Selection** (gather $c_n$ for active experts) -> **Hadamard Product** (combine modulated directions) -> Output

### Critical path:
Input $x$ $\to$ **Gating Network** (select Top-K experts) AND **Encoder** (compute hidden $z$) $\to$ **Modulator Selection** (gather $c_n$ for active experts) $\to$ **Hadamard Product** (combine modulated directions) $\to$ Output

### Design tradeoffs:
- **Expert Count ($N$):** Higher $N$ increases specialization granularity but requires more memory for matrix $C$
- **Sparsity ($K$):** Lower $K$ increases interpretability but risks dropping relevant experts. The paper suggests $K=32$ works well for 3B models
- **Shared vs. Private:** The architecture naturally tends to learn a "shared" expert. A "random-K" trick during training is needed to force pure specialization

### Failure signatures:
- **Reconstruction Collapse:** Loss plateaus high; experts fail to capture base model behavior. Check if $K$ is too low or learning rate is insufficient
- **Dead Experts:** Activation logs show many experts never fire. Check gate initialization or data diversity
- **Rank Collapse:** If expert weights are not full rank, reconstruction fails. Verify $c_n$ has no zeros (theoretical) or check empirical rank of $W_n$

### First 3 experiments:
1. **Rank Verification:** Compute the normalized rank of learned expert matrices ($W_n = D \text{diag}(c_n)$) to confirm they match the theoretical full rank
2. **Sparsity-Accuracy Curve:** Replicate Figure 3 by varying $K$ (e.g., 16, 32, 64) and plotting normalized MSE against a baseline Transcoder
3. **Expert Probing:** Train a linear probe on the expert activations ($a$) for a known concept (e.g., "programming language") to verify the paper's claim that individual experts are interpretable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do MxDs retain their Pareto dominance over sparse MLP baselines on the sparsity-accuracy frontier when applied to LLMs with tens of billions of parameters?
- Basis: The authors state in the Limitations section that their "experiments only provide direct evidence for LLMs with up to 3B parameters," though they expect the method to scale to tens of billions
- Why unresolved: Computational resource constraints limited the primary experiments to models ranging from 124M to 3B parameters
- What evidence would resolve it: Experimental results showing cross-entropy loss and reconstruction fidelity when replacing MLPs in 7B-70B scale models (e.g., Llama 3 70B) with parameter-matched MxD layers

### Open Question 2
- Question: Can hierarchical routing structures or efficient retrieval mechanisms be integrated into MxDs to mitigate the inference-time cost of large encoders and gating functions?
- Basis: The authors identify the computational cost of the encoder as a limitation and suggest "Future work could explore hierarchical structures... and/or efficient retrieval... for further reductions in FLOPs"
- Why unresolved: The current design adds inference-time cost due to the gating function and large encoders, which the paper suggests but does not test methods to alleviate
- What evidence would resolve it: A comparative analysis of inference latency and FLOPs between standard MxDs and variants utilizing hierarchical routing or retrieval mechanisms, while monitoring for accuracy degradation

### Open Question 3
- Question: How do MxDs perform when applied to cross-layer features or transformations rather than single-layer decomposition?
- Basis: The Conclusion states, "We are excited about future work exploring MxDs... in alternative settings, such as for cross-layer features or transformations"
- Why unresolved: The current methodology strictly decomposes individual MLP layers in isolation, leaving the potential for capturing features that span multiple layers unexplored
- What evidence would resolve it: Metrics on reconstruction faithfulness and interpretability (probing/steering) for MxDs designed to approximate the combined functionality of multiple adjacent layers

### Open Question 4
- Question: Is explicit regularization (e.g., load-balancing or diversity losses) necessary to prevent expert collapse or encourage diversity in MxDs trained end-to-end?
- Basis: The authors note in the Limitations that while random initialization worked for their experiments, "standard MoE load-balancing... or diversity losses [may] be useful for MxDs should one need more explicit ways of encouraging expert diversity"
- Why unresolved: The paper relies on implicit clustering from random initialization but acknowledges that explicit constraints might be needed in different training regimes or architectures
- What evidence would resolve it: Training dynamics and expert utilization statistics of MxDs trained end-to-end with and without diversity penalties, specifically checking for modes of expert collapse

## Limitations
- Experiments only provide direct evidence for LLMs with up to 3B parameters, though scaling to tens of billions is expected
- The computational cost of the encoder adds inference-time overhead compared to dense layers
- The method currently decomposes individual MLP layers in isolation, leaving cross-layer feature capture unexplored

## Confidence
- **High Confidence:** The empirical Pareto-dominance over Transcoders on the sparsity-accuracy frontier (Figure 3) is well-supported by quantitative results across four different model families
- **Medium Confidence:** The interpretability claims (specialization, steering, probing) are demonstrated but evaluated on relatively small models (up to 3B parameters)
- **Low Confidence:** The theoretical claim that MxD layers "preserve the original decoders' expressive capacity even under heavy sparsity" assumes non-zero expert modulations, which is not empirically verified

## Next Checks
1. **Rank Verification Under Training:** Monitor the minimum singular value of learned expert matrices $W_n = D \text{diag}(c_n)$ during training. If any expert's minimum singular value approaches zero, this indicates rank collapse and potential reconstruction failure.
2. **Zero-Shot Generalization Test:** Evaluate MxD-decomposed models on out-of-distribution datasets to validate whether the sparse decomposition generalizes beyond the training distribution.
3. **Expert Activation Diversity Analysis:** Compute the entropy of expert activation distributions across the validation set. Low entropy would indicate heavy concentration on a few experts, contradicting claims about specialization.