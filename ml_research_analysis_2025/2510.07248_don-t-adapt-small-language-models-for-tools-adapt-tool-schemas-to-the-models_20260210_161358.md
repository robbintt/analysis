---
ver: rpa2
title: Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models
arxiv_id: '2510.07248'
source_url: https://arxiv.org/abs/2510.07248
tags:
- tool
- pa-tool
- schema
- tools
- metatool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PA-Tool, a training-free method that improves
  small language models' tool-use capabilities by aligning tool schemas with the models'
  pretrained knowledge. Instead of forcing models to adapt to arbitrary tool names,
  PA-Tool generates pretraining-aligned names by leveraging peakedness - a signal
  from contamination detection that indicates pretraining familiarity.
---

# Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models

## Quick Facts
- **arXiv ID**: 2510.07248
- **Source URL**: https://arxiv.org/abs/2510.07248
- **Reference count**: 40
- **Primary result**: Training-free method PA-Tool improves small language models' tool-use capabilities by up to 17% through aligning tool schemas with pretrained knowledge using peakedness-based name generation

## Executive Summary
This paper introduces PA-Tool, a training-free method that improves small language models' tool-use capabilities by aligning tool schemas with the models' pretrained knowledge. Instead of forcing models to adapt to arbitrary tool names, PA-Tool generates pretraining-aligned names by leveraging peakedness - a signal from contamination detection that indicates pretraining familiarity. The method samples multiple candidate names from the model, computes peakedness scores based on distributional concentration, and selects names with the highest peakedness as replacements. Experiments on MetaTool and RoTBench show improvements up to 17% in tool selection accuracy, with schema misalignment errors reduced by 80%.

## Method Summary
PA-Tool operates through a three-stage process: (1) Sample N=32 candidate names from the model at temperature t=0.4 for each tool component description, also generating a greedy reference at temperature 0; (2) Compute peakedness scores by counting how many other candidates fall within an edit distance threshold τ=α·ℓ_max (α=0.2) from each candidate; (3) Select the candidate with maximum peakedness, breaking ties by minimum edit distance to the greedy reference. The method builds a mapping dictionary of {original_name → aligned_name} that is applied during inference, requiring only one-time preprocessing overhead while maintaining computational efficiency.

## Key Results
- Improves tool selection accuracy by up to 17% across diverse models including Qwen2.5-3B/7B and Llama-3.1-8B
- Reduces schema misalignment errors by 80%, enabling models to match or exceed Claude Sonnet 4.5 performance on specific tasks
- Compatible with existing training-free methods and remains effective after supervised fine-tuning
- Shows particular benefit for smaller models (1B-34B parameters) while larger models exhibit diminishing returns

## Why This Works (Mechanism)

### Mechanism 1: Peakedness as Pretraining Familiarity Proxy
- Claim: Output distribution concentration indicates whether a naming pattern was frequently encountered during pretraining.
- Mechanism: When models sample candidate names at non-zero temperature, patterns deeply internalized during pretraining produce tightly clustered outputs (high peakedness), while unfamiliar patterns yield dispersed outputs.
- Core assumption: The relationship between peakedness and pretraining exposure generalizes from contamination detection literature to tool naming contexts.
- Evidence anchors:
  - [abstract] "leverages peakedness—a signal from contamination detection indicating pretraining familiarity"
  - [Section 6.4] "peakedness consistently increases with training epochs across all models, with gains up to +25.8% (Llama3.2-3B)"
  - [corpus] Weak direct corpus evidence; contamination detection papers use peakedness for memorization detection

### Mechanism 2: Reducing Schema Misalignment via Naming Harmonization
- Claim: SLMs fail on tool-use primarily because arbitrary API names conflict with internalized naming conventions, not due to reasoning limitations.
- Mechanism: By renaming schema components to match the model's expected patterns, the model's generation naturally produces valid tool names without additional training.
- Core assumption: Schema misalignment is a dominant failure mode for SLMs; other error types are secondary.
- Evidence anchors:
  - [abstract] "Schema misalignment errors, where models generate plausible but nonexistent tool names, decrease by 80.0% with PA-Tool"
  - [Section 6.1] "PA-Tool substantially reduces all error types, with particularly strong impact on Schema Misalignment (80.0% reduction)"

### Mechanism 3: Distributional Concentration over Frequency
- Claim: Selecting names based on peakedness (clustering density) outperforms simple frequency-based selection.
- Mechanism: Peakedness captures how consistently the model generates around a pattern, not just how often it appears once. This better reflects internalized conventions.
- Core assumption: Concentration is a more reliable signal than raw frequency for identifying deeply internalized patterns.
- Evidence anchors:
  - [Section 3.3] "This score counts the number of candidates that fall within the similarity threshold from si, reflecting how many similar names the model generates around this pattern"
  - [Appendix C] Example showing PA-Tool selects "diet_insights" (peakedness=4) over more frequent "diet_tracker" (5 occurrences)

## Foundational Learning

- **Contamination detection and memorization signals**
  - Why needed here: PA-Tool repurposes peakedness from this field; understanding why concentration indicates memorization is essential to trust the mechanism.
  - Quick check question: Given 10 sampled candidate names, how would you compute a peakedness score using edit distance and a threshold?

- **Tool-augmented language model failure modes**
  - Why needed here: Distinguishing schema misalignment from functional confusion and context understanding errors determines when PA-Tool is appropriate.
  - Quick check question: A model generates `get_user_id` when the schema contains `get_customer_id`—which error type is this?

- **Temperature-controlled sampling and output diversity**
  - Why needed here: PA-Tool relies on sampling diverse candidates at t>0 to reveal distribution structure; greedy decoding alone would miss this signal.
  - Quick check question: Why does t=0 (greedy) fail to provide peakedness information?

## Architecture Onboarding

- **Component map:** Candidate Generation -> Peakedness Computation -> Selection -> Mapping Layer
- **Critical path:** Candidate generation → peakedness scoring → selection. The peakedness computation is O(N²) per component; batch processing recommended for large tool libraries.
- **Design tradeoffs:**
  - Higher N improves signal reliability but increases preprocessing cost (8-16 seconds per benchmark in Table 5)
  - Lower α tightens similarity requirements, potentially missing valid clusters; higher α may merge distinct patterns
  - Temperature t=0.4-0.6 balances diversity and coherence; too high introduces noise
- **Failure signatures:**
  - High collision rates when tool descriptions are semantically similar (RoTBench: 4-6% vs. MetaTool: <1% in Table 6)—requires priority-based locking
  - Larger models (70B+) show diminishing gains, suggesting they handle misaligned schemas through reasoning capacity
  - Peakedness plateaus after ~32-64 candidates (Table 7); additional sampling provides minimal benefit
- **First 3 experiments:**
  1. **Reproduce peakedness-training correlation:** Fine-tune a small model on tool schemas, measure peakedness at epochs 0/10/20/50/100 to validate the core assumption before deployment.
  2. **Ablate N and α:** Run grid search (N∈{16,32,64}, α∈{0.1,0.2,0.3}) on held-out subset; Table 7-9 show optimal varies by model size.
  3. **Error type analysis:** Classify failures on your target benchmark (use prompt in Appendix G) to confirm schema misalignment is the dominant error mode.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does PA-Tool's character-level edit distance similarity metric generalize to non-Latin scripts and morphologically complex languages, or does it require language-specific adaptations?
- **Open Question 2:** Can the peakedness signal distinguish between genuine pretraining familiarity versus other sources of output concentration (e.g., strong inductive biases or frequent patterns in general corpora)?
- **Open Question 3:** Why do some large models (GPT-4.1-mini: +12.0% on Multi-tool) benefit substantially from schema alignment while others (Llama-3.3-70B: marginal gains) do not?
- **Open Question 4:** How does PA-Tool's computational overhead scale with tool library size, and at what point does one-time schema generation become prohibitive?

## Limitations
- The method relies on peakedness scores computed from temperature-0.4 sampling, but optimal temperature may vary across model families and sizes
- Name collision resolution mechanism is mentioned but only briefly described, with practical effectiveness unclear under diverse tool libraries
- Evaluation focuses on tool-use benchmarks with structured, API-like schemas; performance on more natural language contexts or different domains untested

## Confidence
- **High Confidence:** Peakedness correlates with training exposure (validated through fine-tuning experiments); schema misalignment is reduced by ~80% (directly measured); computational efficiency is maintained
- **Medium Confidence:** PA-Tool works across diverse model families (based on 4 models but limited architecture diversity); diminishing returns for 70B+ models (extrapolated from 3 models); compatibility with training-free methods (demonstrated but not extensively explored)
- **Low Confidence:** PA-Tool is "the first training-free approach" for this problem (categorical claim without exhaustive literature review); optimal temperature of 0.4 generalizes (single value used across all experiments)

## Next Checks
1. **Temperature Sensitivity Analysis:** Systematically vary temperature (0.2, 0.4, 0.6, 0.8) across model sizes and architectures to determine if the t=0.4 default is truly optimal or if model-specific tuning is required.
2. **Error Type Decomposition on New Tasks:** Apply the error classification methodology (Appendix G) to a domain outside the current evaluation (e.g., medical tools, creative writing APIs) to verify schema misalignment remains the dominant failure mode.
3. **Name Collision Stress Test:** Construct synthetic benchmarks with intentionally similar tool descriptions (varying only by one word) to rigorously test the collision resolution mechanism and measure degradation in accuracy when collisions occur.