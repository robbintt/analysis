---
ver: rpa2
title: Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament
  on Market-driven Strategic Planning
arxiv_id: '2511.20613'
source_url: https://arxiv.org/abs/2511.20613
tags:
- agents
- code
- llms
- tasks
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning

## Quick Facts
- arXiv ID: 2511.20613
- Source URL: https://arxiv.org/abs/2511.20613
- Reference count: 40
- None

## Executive Summary
This paper presents a large-scale coding tournament comparing 40 LLM-generated agents against 17 human-coded agents in a competitive multi-agent logistics optimization task. The study evaluates whether "vibe coding" - direct LLM generation of code for complex strategic tasks - can match or exceed human performance. Results show that while LLMs successfully generate syntactically valid code, they consistently underperform against both simple baselines and human participants in strategic reasoning and algorithmic correctness.

## Method Summary
The study implements a tournament framework where agents bid on tasks in a market-driven strategic planning environment (Auction, Pickup, and Delivery Problem). Forty LLM-generated agents (using GPT-5, Gemini 2.5, Claude Opus 4.1, and DeepThink R1) were compared against 17 human-coded agents (12 students, 5 baselines) across 12 double all-play-all tournaments with ~40,000 matches total. The evaluation uses Java implementations on the "Logist" platform with 4 network topologies and 50 tasks per match. Agents were generated using 5 prompting strategies ranging from basic author prompts to iterative refinement.

## Key Results
- The majority of LLM-coded agents (33 out of 40) were beaten by very simple baselines
- The top 5 spots in the leaderboard were consistently held by student agents
- When given the best human solution as input and prompted to improve upon it, the best performing LLM made the solution significantly worse, losing 9 spots in the leaderboard

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate syntactically valid code but fail at strategic reasoning required for competitive multi-agent optimization
- Mechanism: LLMs pattern-match against training data to produce executable code, but lack the ability to model opponents, anticipate strategic interactions, and perform long-horizon planning that competitive environments require
- Core assumption: The gap stems from fundamental reasoning limitations, not insufficient prompt engineering
- Evidence anchors:
  - [abstract] "the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines"
  - [section 5.2] "the top 5 spots are consistently held by student agents"
  - [corpus] Related work on vibe coding safety (arxiv:2512.03262) flags security and correctness concerns in agent-generated code, but this paper demonstrates the gap extends to strategic competitiveness
- Break condition: If LLMs can be shown to successfully model opponent behavior and adapt bidding strategies across sequential auctions with performance matching human baselines

### Mechanism 2
- Claim: LLMs struggle with algorithmic correctness beyond surface implementation, even for well-documented algorithms
- Mechanism: LLMs retrieve and compose code fragments based on pattern similarity, but do not verify algorithmic invariants (e.g., admissibility in A* heuristics), leading to semantically incorrect solutions
- Core assumption: The failure to implement admissible heuristics reflects a deeper inability to reason about algorithmic properties, not just a prompting issue
- Evidence anchors:
  - [section 5.1] "3 out of the 4 LLMs (all but GPT-5) did not implement an admissible heuristic for A*... despite being explicitly asked to do so in the prompt"
  - [section 5.1] "one of the Claude agents started with an admissible heuristic, but when asked to fix the bugs... changed the heuristic to an inadmissible one"
  - [corpus] Limited corpus evidence on algorithmic reasoning specifically; vibe coding survey (arxiv:2510.12399) notes outcome-based validation but doesn't address algorithmic correctness depth
- Break condition: If LLMs can reliably produce provably correct implementations when algorithmic invariants are specified in prompts

### Mechanism 3
- Claim: Providing LLMs with high-quality human solutions degrades rather than improves performance
- Mechanism: LLMs treat code modification as a text transformation task without understanding the strategic rationale behind the original implementation, leading to changes that break working logic
- Core assumption: The degradation is due to lack of causal understanding of why the original solution works, not insufficient context
- Evidence anchors:
  - [abstract] "given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse"
  - [section 5.3] "the LLM's 'optimizations' resulted in losing 9 spots in the leaderboard"
  - [corpus] Corpus evidence weak on this specific phenomenon; no direct comparable findings in neighbors
- Break condition: If LLMs can be shown to improve human solutions when given explicit constraints on what aspects to preserve

## Foundational Learning

- Concept: **Constraint satisfaction and optimization**
  - Why needed here: The APDP requires balancing capacity constraints, precedence rules, and cost minimization simultaneously
  - Quick check question: Can you explain why adding a task to a vehicle's route might reduce total cost (economies of scope)?

- Concept: **Strategic bidding under uncertainty**
  - Why needed here: Agents must bid without knowing future tasks or opponent valuations, requiring marginal cost estimation and opponent modeling
  - Quick check question: Why might an agent intentionally bid below marginal cost on early tasks?

- Concept: **Multi-agent systems and game theory**
  - Why needed here: The auction component requires reasoning about opponent behavior, not just optimizing in isolation
  - Quick check question: What information can you infer from observing opponent bids in a sequential auction?

## Architecture Onboarding

- Component map: Auction module -> Planning module -> State tracker -> Opponent model
- Critical path:
  1. Parse auctioned task (source, destination, weight)
  2. Compute marginal cost given current commitments
  3. Estimate opponent's likely bid
  4. Submit strategic bid (may differ from true cost)
  5. After all auctions, solve vehicle routing problem
  6. Output delivery plan that satisfies all constraints

- Design tradeoffs:
  - Bidding accuracy vs. time: More sophisticated marginal cost estimation takes longer; may timeout
  - Exploitation vs. exploration: Bidding truthfully is safe but may miss strategic opportunities
  - Solution quality vs. computation: PDP is NP-hard; heuristics trade optimality for speed

- Failure signatures:
  - Agent fails to pick up/deliver won tasks (logic error in planning)
  - Consistent timeout during bidding (overly complex cost estimation)
  - Capacity constraint violations (planning doesn't account for cumulative load)
  - Win rate below naive baseline (strategic reasoning failure)

- First 3 experiments:
  1. Run baseline agents (Naive, Honest, ExpCostFixedBid) against each other to establish performance bounds
  2. Test LLM-generated agents on simplified variants (Reactive, Deliberative) to isolate planning failures from auction failures
  3. Provide an LLM with a working solution and measure whether modifications improve or degrade performance to assess in-context learning limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does providing high-quality human-coded solutions in-context help or hinder LLM performance on complex strategic tasks?
- Basis in paper: [explicit] Section 5.3 notes that when an LLM was given the winning solution to improve, it performed worse, "rais[ing] interesting future research questions about the limits of in-context learning and retrieval-augmented problem solving in complex scenarios."
- Why unresolved: The paper demonstrates that LLMs degrade top-tier solutions, but it does not determine if this is due to context window confusion, inability to optimize existing logic, or fundamental reasoning limits.
- Evidence: Ablation studies comparing LLM performance with varying amounts of in-context solution code (partial vs. full) to measure the net impact on reasoning.

### Open Question 2
- Question: Can LLMs improve performance on strategic optimization tasks if prompted to generate and analyze their own debug traces (prints) during development?
- Basis in paper: [explicit] Footnote 6 explicitly states: "An interesting avenue to investigate in future work is whether asking the LLM to include prints of debug information (as a human software developer would do) would make a difference."
- Why unresolved: The authors found semantic bugs persisted despite standard error feedback. It is unknown if forcing the model to inspect its own execution state (internal reasoning) mitigates these logic errors.
- Evidence: A comparative study measuring the reduction in semantic bugs (e.g., constraint violations) when LLMs are forced to simulate/trace execution outputs versus standard code generation.

### Open Question 3
- Question: Why do state-of-the-art LLMs fail to satisfy strict algorithmic constraints (e.g., heuristic admissibility in A*) even when explicitly prompted to do so?
- Basis in paper: [inferred] Section 5.1 describes the "surprising" failure of 3 out of 4 LLMs to implement admissible heuristics, concluding that "This underlines the importance of reasoning-driven code evaluation benchmarks."
- Why unresolved: The paper identifies the high rate of semantic errors but leaves open whether these failures stem from a lack of deep understanding of algorithmic theory or an inability to maintain logical constraints during code synthesis.
- Evidence: Testing LLMs on a suite of constraint-satisfaction algorithms (e.g., maintaining loop invariants) to see if correctness improves with formal specification languages versus natural language prompts.

## Limitations

- The LLM agents were not optimized beyond basic prompt engineering - no iterative fine-tuning or reinforcement learning was applied
- The evaluation focuses on a specific competitive environment (APDP auctions) which may not generalize to other coding tasks
- The 17 human participants (12 students, 5 baselines) represent a relatively small and potentially non-representative sample

## Confidence

- High confidence: LLMs can generate syntactically correct code but struggle with strategic reasoning in competitive environments
- Medium confidence: LLMs fail at algorithmic correctness beyond surface implementation
- Low confidence: Providing LLMs with high-quality human solutions reliably degrades performance

## Next Checks

1. Test whether iterative prompt refinement or in-context learning with multiple examples can close the performance gap for the strategic bidding component
2. Evaluate the same LLM agents on a non-competitive variant of the APDP to isolate whether the gap stems from strategic reasoning vs. general coding capability
3. Replicate the "degradation from human solution" experiment with multiple human baselines to verify this isn't an artifact of the specific solution tested