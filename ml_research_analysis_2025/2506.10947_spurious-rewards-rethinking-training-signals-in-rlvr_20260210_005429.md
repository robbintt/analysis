---
ver: rpa2
title: 'Spurious Rewards: Rethinking Training Signals in RLVR'
arxiv_id: '2506.10947'
source_url: https://arxiv.org/abs/2506.10947
tags:
- training
- qwen2
- code
- reasoning
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Spurious rewards\u2014such as random or incorrect labels\u2014\
  can improve mathematical reasoning in Qwen2.5-Math models by up to 24.1% on MATH-500,\
  \ nearly matching ground-truth reward gains. This effect is unique to Qwen models;\
  \ Llama and OLMo models show minimal or negative gains with spurious rewards."
---

# Spurious Rewards: Rethinking Training Signals in RLVR

## Quick Facts
- **arXiv ID**: 2506.10947
- **Source URL**: https://arxiv.org/abs/2506.10947
- **Reference count**: 40
- **Primary result**: Spurious rewards can improve mathematical reasoning in Qwen2.5-Math models by up to 24.1% on MATH-500

## Executive Summary
This paper challenges conventional wisdom about reward signals in Reinforcement Learning from Verifiable Rewards (RLVR) by demonstrating that incorrect or random rewards can improve mathematical reasoning performance in Qwen2.5-Math models. The effect is surprisingly strong—spurious rewards achieved nearly the same performance gains as ground-truth rewards, with improvements up to 24.1% on MATH-500. Critically, this phenomenon appears unique to Qwen models, with Llama and OLMo models showing minimal or negative gains. The research reveals that Qwen2.5-Math models naturally employ code reasoning strategies, which RLVR helps surface rather than teach from scratch.

## Method Summary
The researchers conducted experiments using Qwen2.5-Math-7B, Llama-3.2-8B-Instruct, and OLMo-7B models, training them with RLVR using different reward signal types: ground-truth rewards, spurious rewards (random or incorrect), and no rewards. They evaluated performance on MATH-500, creating separate test sets for prompt-based and RLVR-enhanced models. The study analyzed reasoning patterns by examining code generation and comparing it with answer correctness. Additional experiments tested whether eliciting code reasoning through prompts or RLVR could boost performance across different model architectures.

## Key Results
- Qwen2.5-Math models improved by up to 24.1% on MATH-500 when trained with spurious rewards
- Code reasoning was used by 65% of Qwen models initially, rising to over 90% after RLVR
- Models using code reasoning achieved 60.9% accuracy versus 35.0% for those without code
- Spurious rewards had minimal or negative effects on Llama and OLMo models

## Why This Works (Mechanism)
The paper suggests RLVR primarily surfaces pre-existing reasoning patterns rather than teaching new skills. Qwen2.5-Math models appear to have strong code reasoning capabilities from pretraining, and RLVR amplifies this tendency. The spurious rewards may work by reinforcing the models' existing inclination toward code-based problem solving, even when the rewards themselves are incorrect. This mechanism explains why the effect is model-specific—it depends on having the right pretraining priors that include code reasoning capabilities.

## Foundational Learning
- **Reinforcement Learning from Verifiable Rewards (RLVR)**: Essential for understanding the training paradigm; quickly check by confirming models receive feedback based on answer correctness rather than human labels.
- **Mathematical reasoning benchmarks**: MATH-500 provides standardized evaluation; verify by checking dataset contains grade-school to competition-level math problems with verifiable answers.
- **Code generation as reasoning**: Understanding that models can solve math by generating executable code; confirm by examining whether models produce Python or similar code during problem solving.
- **Reward signal design**: Different reward types (ground-truth vs. spurious) fundamentally change learning dynamics; check by comparing reward distributions and their correlation with model outputs.

## Architecture Onboarding
- **Component map**: Model architecture (Qwen2.5-Math/Llama/OLMo) -> Reward signal generator -> RLVR training loop -> Performance on MATH-500 benchmark
- **Critical path**: Reward signal generation → RLVR policy update → Reasoning pattern emergence → Mathematical performance
- **Design tradeoffs**: Using spurious rewards sacrifices reward signal quality for potentially better emergent reasoning patterns; ground-truth rewards provide accurate feedback but may not encourage code reasoning as strongly.
- **Failure signatures**: Non-Qwen models show decreased performance with spurious rewards; models without code reasoning capabilities don't benefit from spurious signals.
- **First experiments**:
  1. Test spurious rewards with different model sizes within the Qwen family to verify scalability
  2. Compare different types of spurious rewards (random vs. systematically incorrect) to identify optimal signal properties
  3. Evaluate spurious reward effects on non-mathematical reasoning tasks to test domain generality

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the findings raise several important ones: Why are Qwen models uniquely susceptible to spurious rewards? Is the effect limited to mathematical domains where code reasoning is applicable? Would similar effects appear with other types of reasoning strategies beyond code generation?

## Limitations
- The effect appears highly model-specific, working well for Qwen2.5-Math but not for Llama or OLMo models, suggesting findings may not generalize across architectures
- The study focuses on mathematical reasoning tasks specifically, leaving uncertainty about whether similar spurious reward effects would appear in other domains
- The mechanistic explanation is plausible but not definitively proven, as correlation between code generation and correctness doesn't establish causation

## Confidence
- Spurious rewards can improve mathematical reasoning performance: **High confidence**
- Effect is unique to Qwen models: **Medium confidence**
- RLVR surfaces pre-existing patterns rather than teaching new skills: **Medium confidence**

## Next Checks
1. Test spurious reward effects across a broader range of model architectures (including non-Qwen models with similar pretraining) to determine whether the effect is truly architecture-specific or depends on particular training characteristics.

2. Conduct controlled ablation studies varying the proportion and type of spurious rewards during RLVR to identify the minimum effective dose and whether certain types of spurious signals work better than others.

3. Perform cross-domain evaluation to test whether spurious rewards improve reasoning in non-mathematical tasks, or whether the effect is specific to domains where code-based reasoning is applicable.