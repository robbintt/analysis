---
ver: rpa2
title: 1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language
  Model Training
arxiv_id: '2503.19633'
source_url: https://arxiv.org/abs/2503.19633
tags:
- data
- reasoning
- dataset
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors constructed a large-scale reasoning dataset called
  AM-DeepSeek-R1-Distilled containing 1.4 million high-quality problems with thinking
  traces, derived from open-source datasets and distilled from DeepSeek-R1. The data
  underwent semantic deduplication, cleaning, and verification (via reference checking,
  test cases, or reward models) to ensure quality.
---

# 1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training

## Quick Facts
- **arXiv ID:** 2503.19633
- **Source URL:** https://arxiv.org/abs/2503.19633
- **Reference count:** 14
- **Key outcome:** AM-Distill-Qwen-32B outperformed DeepSeek-R1-Distill-Qwen-32B, and AM-Distill-Qwen-72B surpassed DeepSeek-R1-Distill-Llama-70B, with average accuracy gains of 1.5 and 3.0 percentage points respectively

## Executive Summary
This paper presents a large-scale reasoning dataset called AM-DeepSeek-R1-Distilled containing 1.4 million high-quality problems with thinking traces. The data is derived from open-source datasets and distilled from DeepSeek-R1, then undergoes semantic deduplication, cleaning, and verification to ensure quality. The authors train AM-Distill-Qwen-32B and AM-Distill-Qwen-72B models via simple supervised fine-tuning using this dataset. On four benchmarks (AIME2024, MATH-500, GPQA-Diamond, LiveCodeBench), their models outperform DeepSeek-R1-distilled counterparts with average accuracy gains of 1.5 and 3.0 percentage points respectively.

## Method Summary
The authors constructed a reasoning dataset by combining 500K open-source samples with 900K samples distilled from DeepSeek-R1. The dataset undergoes semantic deduplication using embedding-based similarity, difficulty classification via LLM scoring, and rigorous verification through answer checking, test-case execution, and reward modeling. Models are trained using simple supervised fine-tuning on Qwen2.5-32B/72B with a system prompt format that includes thinking traces. The training uses a maximum generation length of 32,768 tokens with temperature 0.6 and top-p 0.95.

## Key Results
- AM-Distill-Qwen-32B achieved 73.1% average accuracy vs 71.6% for DeepSeek-R1-Distill-Qwen-32B
- AM-Distill-Qwen-72B achieved 74.8% average accuracy vs 71.8% for DeepSeek-R1-Distill-Llama-70B
- On individual benchmarks: AIME2024 (78.1% vs 73.6%), MATH-500 (83.2% vs 80.1%), GPQA-Diamond (75.3% vs 72.4%), LiveCodeBench (65.9% vs 63.2%)

## Why This Works (Mechanism)

### Mechanism 1: Multi-Modal Response Verification
Mathematical problems validated against reference answers via math-verify + LLM consistency checks; code problems verified in sandbox environments with test cases; other tasks evaluated by reward models across five dimensions (correctness, helpfulness, coherence, complexity, verbosity).

### Mechanism 2: Semantic Deduplication with Difficulty-Weighted Selection
Embedding-based semantic similarity calculation → priority-based retention of representative entries → difficulty scoring via LLM → downsampling easy/medium examples to emphasize challenging data.

### Mechanism 3: Chain-of-Thought Distillation from Strong Teacher
Teacher model (DeepSeek-R1) generates long CoT responses → verified responses retained → student model trained on (instruction, CoT response) pairs via SFT → student learns to emit extended reasoning before final answers.

## Foundational Learning

- **Concept:** Supervised Fine-Tuning (SFT)
  - Why needed: The entire training approach is "simple SFT" on distilled reasoning traces—understanding loss functions, learning rates, and checkpoint selection is essential.
  - Quick check: Can you explain how cross-entropy loss over reasoning tokens differs from training on final answers only?

- **Concept:** Chain-of-Thought (CoT) Reasoning
  - Why needed: Dataset explicitly contains "thinking traces" in `<think reasoning process here </think > <answer>` format; models must learn to generate these extended sequences.
  - Quick check: What is the expected token length distribution for CoT vs. direct-answer responses?

- **Concept:** Reward Modeling
  - Why needed: 39.2% of data lacks reference answers or test cases and relies on reward model scoring for quality filtering.
  - Quick check: How does a 5-dimensional reward model (correctness, helpfulness, coherence, complexity, verbosity) aggregate into a single retention decision?

## Architecture Onboarding

- **Component map:** Raw Data Collection → Labeling Pipeline → Deduplication Engine → Verification Layer → Response Generation → Training
- **Critical path:** Difficulty labeling must complete before downsampling decisions; semantic deduplication must precede response generation; verification must complete before training data finalization
- **Design tradeoffs:** Scale vs. quality (1.4M samples vs. 800K used by DeepSeek); verification coverage (60.8% have reference answers/test cases; 39.2% rely on reward models); difficulty distribution (77.5% medium/hard, but only 6.5% very hard)
- **Failure signatures:** Low verification pass rate (<50%) suggests upstream data quality issues; high n-gram repetition flags in section 2.2.3 indicates model degradation; benchmark contamination detected during deduplication
- **First 3 experiments:**
  1. Ablation on verification strictness: Train models with relaxed vs. strict reward thresholds; measure impact on AIME2024 and MATH-500.
  2. Difficulty subsampling test: Train on hard-only vs. balanced difficulty distribution; compare LiveCodeBench performance.
  3. Deduplication sensitivity: Vary semantic similarity threshold (0.85 vs. 0.95); track dataset size and GPQA-Diamond accuracy.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several implicit ones regarding the effectiveness of its verification pipeline and the scalability of its approach.

## Limitations
- SFT hyperparameters (learning rate, batch size, epochs, warmup) are not specified
- Dataset verification relies heavily on reward models for 39.2% of samples
- Difficulty classification and semantic deduplication thresholds are not disclosed

## Confidence

**High Confidence:** Dataset construction pipeline is technically sound; basic SFT training approach is standard; benchmark results show consistent improvements.

**Medium Confidence:** Performance improvements are credible but depend on exact hyperparameters; semantic deduplication benefits are plausible but lack direct ablation evidence.

**Low Confidence:** Verification through multiple methods significantly improves model quality lacks quantitative evidence; DeepSeek-R1 reasoning traces transfer genuine reasoning capability through SFT alone may reflect stylistic imitation.

## Next Checks

1. **Verification Pipeline Effectiveness:** Run ablation study comparing models trained on: (a) verified responses only, (b) responses passing reward model threshold only, and (c) all generated responses. Measure performance differences on MATH-500.

2. **Difficulty Distribution Impact:** Train models on different difficulty subsets (easy-only, medium-only, hard-only, balanced) and evaluate on LiveCodeBench to determine if difficulty-weighted sampling benefits hold empirically.

3. **Benchmark Contamination Check:** Systematically check whether any benchmark problems from AIME2024, MATH-500, GPQA-Diamond, or LiveCodeBench appear in the training data through exact matching and semantic similarity analysis, then re-evaluate model performance excluding potentially contaminated samples.