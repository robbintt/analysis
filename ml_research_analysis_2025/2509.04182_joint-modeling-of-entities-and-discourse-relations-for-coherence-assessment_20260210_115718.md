---
ver: rpa2
title: Joint Modeling of Entities and Discourse Relations for Coherence Assessment
arxiv_id: '2509.04182'
source_url: https://arxiv.org/abs/2509.04182
tags:
- coherence
- discourse
- relations
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the problem of improving coherence assessment
  by integrating both entity-based and discourse relation-based features, which are
  typically modeled separately in existing approaches. The core method idea involves
  two approaches: a fusion transformer that jointly models sentences, entities, and
  discourse relations using a flat structure with position-aware attention, and a
  prompt-based method that leverages large language models by incorporating entity
  and discourse relation information directly into the input.'
---

# Joint Modeling of Entities and Discourse Relations for Coherence Assessment

## Quick Facts
- arXiv ID: 2509.04182
- Source URL: https://arxiv.org/abs/2509.04182
- Reference count: 22
- Three-way coherence classification with 78.66% accuracy on TOEFL using fusion transformer

## Executive Summary
This paper addresses the challenge of assessing text coherence by jointly modeling entities and discourse relations, which are typically handled separately in existing approaches. The authors propose two methods: a fusion transformer that integrates sentences, entities, and discourse relations using position-aware attention, and a prompt-based method that incorporates this information into large language model inputs. Both approaches significantly outperform strong text-only baselines on three benchmark datasets (GCDC, CoheSentia, TOEFL), demonstrating improved coherence assessment through integrated entity and discourse relation modeling.

## Method Summary
The study introduces two complementary approaches for joint coherence modeling. The fusion transformer uses a flat sequence structure with position-aware attention that can selectively focus on entities, discourse relations, or sentences based on visibility matrices. It employs RoBERTa or Llama encoders with a 2-layer transformer and explicit attention masking. The prompt-based method leverages large language models by constructing prompts that include sentences, entities, and discourse relations, with fine-tuning using LoRA adapters. Both methods use Stanza for entity extraction and a modified discopy parser for discourse relations from PDTB 3.0, achieving superior performance through integrated modeling of coherence signals.

## Key Results
- Fusion transformer achieves 78.66% accuracy on TOEFL dataset, outperforming text-only baselines
- Prompt method improves GCDC performance by over 6 points in zero-shot settings
- Joint modeling shows better cross-domain generalization and robustness to label imbalance

## Why This Works (Mechanism)
Coherence assessment benefits from multiple complementary signals: entity continuity provides surface-level coherence while discourse relations capture deeper argumentative structure. By jointly modeling these features, the approach can leverage both local entity chains and global discourse patterns. The fusion transformer's position-aware attention allows selective focus on different coherence signals, while the prompt method enables direct incorporation of structured coherence information into LLM reasoning. This integration addresses the limitation of text-only approaches that miss critical coherence cues present in entity and discourse relation patterns.

## Foundational Learning
- **PDT Discourse Relations**: 29 types (15 explicit, 14 implicit + NoRel) from Penn Discourse Treebank that signal coherence through argumentative or temporal connections. Needed to provide structured discourse information beyond surface text patterns. Quick check: Verify parser correctly identifies implicit relations using Liu & Strube (2023) model.
- **Position-Aware Attention**: Attention mechanism with visible matrices that can mask certain positions to focus on specific coherence signals. Needed to control information flow between sentences, entities, and relations. Quick check: Test attention masks with synthetic sequences to confirm C1-C4 conditions work as intended.
- **LoRA Fine-tuning**: Low-rank adaptation method that modifies attention weights without full model retraining. Needed for efficient prompt-based method adaptation. Quick check: Verify LoRA rank=24 produces stable convergence on small batches.
- **Coherence Label Binning**: Converting continuous scores to discrete classes (low/medium/high) for classification. Needed to standardize across datasets with different scoring scales. Quick check: Ensure consistent binning across all three datasets.

## Architecture Onboarding

**Component Map**: Raw Text -> Stanza Entity Extractor -> discopy Parser -> PDTB Relations -> Flat Sequence (sentences+entities+relations) -> Position-Aware Attention -> RoBERTa/Llama Encoder -> Classification Head

**Critical Path**: The most critical sequence is Text → Entity/Relation Extraction → Position-Aware Attention → Classification, as errors in parsing or attention masking directly impact coherence assessment quality.

**Design Tradeoffs**: The fusion approach trades model complexity for better integration of coherence signals, while the prompt method trades fine-tuning data for leveraging LLM reasoning. The visible matrix design balances flexibility with computational overhead.

**Failure Signatures**: Poor cross-domain performance suggests extraction errors or attention masking issues. High variance across folds indicates sensitivity to data splits or label imbalance.

**First Experiments**:
1. Implement position-aware attention with synthetic sequences to verify C1-C4 masking conditions
2. Test entity and relation extraction pipeline on small corpus to validate parsing accuracy
3. Compare baseline text-only model with joint model on GCDC Enron domain to measure improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Several implementation details underspecified, particularly discourse relation parsing pipeline and visible matrix construction
- Exact preprocessing decisions for score-to-label conversion not fully detailed, affecting result comparability
- Cross-dataset generalization claims require access to specific data splits for validation

## Confidence

**High confidence**: Core contribution that joint modeling improves coherence assessment performance is well-supported by consistent results across three diverse datasets.

**Medium confidence**: Magnitude of improvements, especially for prompt-based method, depends on specific implementation details not fully specified.

**Low confidence**: Cross-dataset generalization claims require exact data splits and preprocessing decisions for full validation.

## Next Checks

1. Implement and verify the position-aware attention mechanism with C1-C4 conditions using synthetic test cases
2. Reconstruct complete discourse relation parsing pipeline from raw text through discopy to Liu & Strube model integration
3. Train joint model on Enron domain and evaluate on other GCDC domains to verify 3-6 point cross-domain improvement claim