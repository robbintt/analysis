---
ver: rpa2
title: LLM-based phoneme-to-grapheme for phoneme-based speech recognition
arxiv_id: '2506.04711'
source_url: https://arxiv.org/abs/2506.04711
tags:
- decoding
- training
- speech
- phoneme
- llm-p2g
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LLM-P2G, a phoneme-to-grapheme approach
  for phoneme-based automatic speech recognition using large language models. The
  method addresses information loss challenges in cascading speech-to-phoneme and
  phoneme-to-grapheme models by introducing two training strategies: data augmentation
  with noisy phonemes (DANP) and randomized top-K marginalized (TKM) training and
  decoding.'
---

# LLM-based phoneme-to-grapheme for phoneme-based speech recognition

## Quick Facts
- arXiv ID: 2506.04711
- Source URL: https://arxiv.org/abs/2506.04711
- Authors: Te Ma; Min Bi; Saierdaer Yusuyin; Hao Huang; Zhijian Ou
- Reference count: 0
- Key outcome: Introduces LLM-P2G, a phoneme-to-grapheme approach using large language models that achieves up to 3.6% and 6.9% relative WER reductions on Polish and German CommonVoice datasets compared to WFST-based systems.

## Executive Summary
This paper presents LLM-P2G, a novel phoneme-to-grapheme approach for phoneme-based automatic speech recognition using large language models. The method addresses information loss challenges in cascading speech-to-phoneme and phoneme-to-grapheme models by introducing two training strategies: data augmentation with noisy phonemes (DANP) and randomized top-K marginalized (TKM) training and decoding. Experiments on Polish and German CommonVoice datasets show that LLM-P2G with randomized TKM achieves significant WER improvements while simplifying the decoding pipeline compared to traditional WFST-based systems.

## Method Summary
The approach uses a two-step pipeline where a Conformer-based CTC model (Whistle-S) first converts speech to phoneme sequences, then a fine-tuned mT5-base LLM converts phonemes to graphemes. The DANP strategy trains the P2G model on noisy phoneme sequences generated by the S2P model rather than ground-truth phonemes, matching training and inference distributions. The TKM strategy marginalizes over random subsets of top-K phoneme hypotheses during training to prevent over-reliance on specific S2P rankings. The method leverages the fact that both phonemes and subwords are discrete tokens, allowing direct use of pre-trained LLMs without architectural modifications.

## Key Results
- LLM-P2G with randomized TKM achieves up to 3.6% relative WER reduction on Polish and 6.9% on German compared to WFST-based systems
- The approach simplifies the decoding pipeline by eliminating the need for lexicon compilation and WFST construction
- Performance is highly dependent on pre-training data availability, with LLM-P2G failing to outperform baselines in the Polish 20h low-resource condition

## Why This Works (Mechanism)

### Mechanism 1: Robustness to Cascading Errors via Noise Injection (DANP)
Fine-tuning the P2G model on noisy phoneme sequences generated by the S2P model reduces the train-test mismatch caused by S2P imperfections. This exposure to the specific error distribution of the S2P model conditions the LLM to correct or tolerate these acoustic errors, as the P2G model receives imperfect hypotheses during inference.

### Mechanism 2: Uncertainty Marginalization via Randomized TKM
Marginalizing over random subsets of top-K phoneme hypotheses during training prevents overfitting to the "best" hypothesis and forces the LLM to weigh multiple acoustic possibilities. This smoothing of the decision boundary improves generalization compared to using single best paths or fixed top-K sets.

### Mechanism 3: Direct LLM Utilization via Discrete Tokenization
Representing the interface between speech and text as discrete phoneme tokens allows direct leverage of pre-trained LLMs without architectural modifications. Since IPA symbols and subwords are discrete tokens in the LLM's vocabulary, the system can naturally fine-tune the LLM for the P2G task while leveraging its pre-existing linguistic knowledge.

## Foundational Learning

- **Concept: Connectionist Temporal Classification (CTC)**
  - Why needed here: CTC produces frame-level probabilities and requires beam search to collapse into discrete phoneme sequences, generating the "noisy" candidates for DANP and TKM
  - Quick check question: How does CTC generate the "top-K" phoneme hypotheses used for marginalization?

- **Concept: Marginalization in Latent Variable Models**
  - Why needed here: The mathematical justification for TKM treats the phoneme sequence as a latent variable, requiring understanding of maximizing probability over all possible paths rather than just the most likely path
  - Quick check question: Why does the objective function sum over phoneme sequences instead of taking the maximum?

- **Concept: Fine-Tuning vs. Prompting**
  - Why needed here: The paper uses full-parameter fine-tuning on mT5-base, implying permanent weight updates rather than lightweight adapter tuning or simple prompting
  - Quick check question: Does the LLM-P2G model modify the weights of the mT5-base model, or does it keep them frozen?

## Architecture Onboarding

- **Component map:** Raw Audio -> Whistle-S (Conformer) -> CTC Logits -> Top-K Phoneme Sequences -> mT5-base (Encoder-Decoder) -> Subword Sequence
- **Critical path:** Generating the candidate phoneme pool is the bottleneck, as the S2P model must run inference to generate training data (DANP) or candidate lists (TKM) before the LLM can process them
- **Design tradeoffs:**
  - WFST vs. LLM-P2G: WFST is complex but fast; LLM-P2G simplifies architecture but incurs higher latency due to LLM inference
  - Best Path vs. TKM: Best path is faster but prone to cascade errors; TKM is robust but requires running the LLM multiple times
- **Failure signatures:**
  - Over-denoising: Training data too noisy causes the model to ignore phoneme input and hallucinate based on LLM priors
  - Low-Resource Collapse: Insufficient fine-tuning data to override LLM priors or lack of exposure to specific language phonotactics
- **First 3 experiments:**
  1. Baseline Reproduction: Implement Whistle-S2P + mT5 P2G using only ground-truth phonemes to verify performance gap
  2. DANP Ablation: Compare "Beam Search" vs. "Random Sampling" for generating noisy phonemes to assess diversity benefits
  3. TKM vs. Best Path: Compare single best path decoding vs. randomized TKM to quantify marginalization gains

## Open Questions the Paper Calls Out
None

## Limitations
- Low-resource generalization failure: LLM-P2G failed to outperform baselines in the Polish 20h condition without detailed analysis of why the approach breaks down
- Decoding latency trade-offs: Computational overhead of running marginalization over K=32 hypotheses was not quantified despite improving accuracy
- Cross-linguistic generalizability: Results limited to Polish and German, with effectiveness on other language families untested

## Confidence
- **High confidence:** DANP strategy's effectiveness in matching training and inference distributions (consistent gains across datasets)
- **Medium confidence:** TKM marginalization superiority over fixed-top-K decoding (significant results but computational cost uncharacterized)
- **Low confidence:** Claim of "simplified decoding pipeline" (simplifies architecture but adds computational complexity; no quantitative comparison)

## Next Checks
1. **Latency profiling:** Measure end-to-end decoding time of TKM inference (K=32, n=8) versus WFST-based decoding on identical hardware to validate the "simplification" claim
2. **Low-resource stress test:** Systematically vary fine-tuning data size (1h, 5h, 10h, 20h) for both languages to identify minimum data threshold where LLM-P2G outperforms baselines
3. **Cross-linguistic robustness:** Apply method to a language with non-Latin script or complex tone systems to test IPA-based approach generalization beyond Indo-European languages