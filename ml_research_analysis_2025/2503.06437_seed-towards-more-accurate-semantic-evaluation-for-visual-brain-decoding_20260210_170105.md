---
ver: rpa2
title: 'SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding'
arxiv_id: '2503.06437'
source_url: https://arxiv.org/abs/2503.06437
tags:
- object
- decoding
- evaluation
- metrics
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the need for better evaluation metrics in\
  \ visual brain decoding, as current methods often misalign with human judgments\
  \ despite near-perfect scores. The authors propose SEED, a new semantic evaluation\
  \ metric that integrates three complementary metrics\u2014Object F1, Cap-Sim, and\
  \ EffNet\u2014to better capture the semantic similarity between ground-truth and\
  \ reconstructed images."
---

# SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding

## Quick Facts
- **arXiv ID:** 2503.06437
- **Source URL:** https://arxiv.org/abs/2503.06437
- **Reference count:** 40
- **Primary result:** SEED is a novel metric for semantic evaluation of visual brain decoding, achieving highest alignment with human judgment by integrating Object F1, Cap-Sim, and EffNet.

## Executive Summary
This paper addresses the need for better evaluation metrics in visual brain decoding, as current methods often misalign with human judgments despite near-perfect scores. The authors propose SEED, a new semantic evaluation metric that integrates three complementary metrics—Object F1, Cap-Sim, and EffNet—to better capture the semantic similarity between ground-truth and reconstructed images. Using crowd-sourced human judgment data, SEED is shown to achieve the highest alignment with human evaluations, outperforming existing metrics. Additionally, the authors introduce a novel pairwise hinge loss function that improves semantic decoding performance by leveraging the order of cosine similarities in CLIP embeddings, consistently enhancing existing models. The human judgment data is also made publicly available to encourage further research.

## Method Summary
The authors introduce SEED, a semantic evaluation metric for visual brain decoding, which combines three complementary sub-metrics: Object F1 (object-level recall and precision from MM-Grounding-DINO), Cap-Sim (caption similarity via GIT and Sentence Transformer), and EffNet (global feature correlation from EfficientNet). They also propose a pairwise hinge loss that enforces pairwise cosine similarity ordering in CLIP embeddings to improve semantic alignment during training. Human judgments on 1,000 image pairs were collected to validate SEED against existing metrics.

## Key Results
- SEED achieves the highest correlation with human judgments among all tested metrics, including Object F1, Cap-Sim, EffNet, and others.
- The pairwise hinge loss consistently improves semantic decoding performance across multiple models (MindEye2, MindBridge, UniBrain) by leveraging CLIP embedding geometry.
- Existing metrics (e.g., two-way identification, PixCorr) show low alignment with human judgments, underscoring the need for SEED.

## Why This Works (Mechanism)

### Mechanism 1: Multi-Aspect Semantic Aggregation via SEED
- **Claim:** Combining Object F1, Cap-Sim, and EffNet into a single metric (SEED) yields higher alignment with human judgments than any individual metric.
- **Mechanism:** Each component captures a distinct semantic facet: Object F1 quantifies object presence, Cap-Sim captures scene context via caption similarity, and EffNet encodes global style/texture. Averaging them smooths individual metric blind spots.
- **Core assumption:** Human semantic judgment integrates multiple perceptual cues, and no single feature extractor fully approximates this integration.
- **Evidence anchors:**
  - [abstract] “SEED, a novel metric for evaluating the semantic decoding performance… integrates three complementary metrics…”
  - [Section 4.3] “We hypothesize that combining several distinct high-performing metrics… can yield superior metrics by supplementing each other’s misjudgments.”
  - [corpus] Neighbor paper “Multigranular Evaluation for Brain Visual Decoding” also emphasizes multi-granular assessment, suggesting the broader need for compositional metrics.
- **Break condition:** If future work shows that a different combination of metrics (or a single learned metric) consistently outperforms the fixed three-way average, this aggregation may be suboptimal.

### Mechanism 2: Pairwise Hinge Loss for Embedding Geometry Preservation
- **Claim:** Adding a hinge loss term that penalizes violations of pairwise cosine-similarity order in CLIP embeddings improves brain-to-image semantic alignment.
- **Mechanism:** The loss enforces that the relative similarity ordering among GT embeddings is preserved in the brain-predicted embeddings, thereby better shaping the embedding manifold beyond simple mean-squared-error alignment.
- **Core assumption:** The relative similarity structure (geometry) of CLIP embeddings carries semantic information not captured by pointwise alignment losses alone.
- **Evidence anchors:**
  - [abstract] “…a novel loss function designed to enhance semantic decoding performance by leveraging the order of pairwise cosine similarity in CLIP image embeddings.”
  - [Section 5] “We hypothesize that incorporating the order information of cosine similarities among different samples into the training process can provide rich insights into the geometry of CLIP’s embedding space.”
  - [corpus] Limited direct corpus evidence for this specific loss formulation; neighbor papers focus on reconstruction quality rather than embedding geometry.
- **Break condition:** If embedding geometry is already sufficiently shaped by existing contrastive or distillation losses (e.g., SoftCLIP), the marginal benefit of this additional term may diminish.

### Mechanism 3: Human-Grounded Meta-Evaluation for Metric Validation
- **Claim:** Using crowd-sourced human similarity ratings as ground truth reveals that many existing metrics (e.g., two-way identification scores) poorly correlate with human perception, justifying the need for SEED.
- **Mechanism:** By collecting 5-point Likert ratings from 22 evaluators on 1,000 GT–reconstruction pairs and computing meta-metrics (pairwise accuracy, Kendall’s Tau-b, Pearson correlation), the authors quantitatively demonstrate that SEED achieves higher alignment than alternatives.
- **Core assumption:** Human ratings on a finite sample are a reliable proxy for general human semantic judgment.
- **Evidence anchors:**
  - [Section 4.4] “We collected 5-Likert scale ratings from 22 human evaluators…”
  - [Section 6.2] “The meta-evaluation results… indicate that most existing evaluation metrics exhibit low correlation with human judgments, except for EffNet. Notably, SEED achieves the highest agreement with human judgments.”
  - [corpus] Neighbor paper “Brain2Text Decoding Model Reveals the Neural Mechanisms of Visual Semantic Processing” also uses human-aligned evaluations, supporting the trend toward human-grounded benchmarks.
- **Break condition:** If human ratings are biased or inconsistent (e.g., due to subjectivity or task framing), the meta-evaluation could be unreliable.

## Foundational Learning

- **Concept: Visual Brain Decoding**
  - **Why needed here:** The entire paper addresses evaluating models that reconstruct images from fMRI signals. Without understanding this goal, the motivation for SEED is unclear.
  - **Quick check question:** Can you explain in one sentence what visual brain decoding aims to do?

- **Concept: CLIP Image Embeddings**
  - **Why needed here:** SEED’s Object F1 and Cap-Sim rely on external vision-language models, and the pairwise hinge loss operates directly on CLIP embedding geometry.
  - **Quick check question:** What property of CLIP embeddings makes them useful for measuring semantic similarity?

- **Concept: Evaluation Metrics in Generative Models (e.g., FID, SSIM)**
  - **Why needed here:** The paper critiques existing metrics (e.g., two-way identification, PixCorr) and justifies SEED by comparison. Understanding their limitations is essential.
  - **Quick check question:** Why might SSIM be unsuitable for evaluating brain-decoded reconstructions?

## Architecture Onboarding

- **Component map:**
  - **Object F1 Pipeline:** GT/Reconstruction → MM-Grounding-DINO → Object lists → Threshold-averaged Recall/Precision → F1 score.
  - **Cap-Sim Pipeline:** GT/Reconstruction → GIT → Captions → Sentence Transformer → Cosine similarity.
  - **EffNet Pipeline:** GT/Reconstruction → EfficientNet → Embeddings → Pearson correlation.
  - **SEED Aggregation:** (Object F1 + Cap-Sim + EffNet) / 3.
  - **Pairwise Hinge Loss:** Batch of predicted/GT CLIP embeddings → All-pair cosine similarity matrices → Order comparison → Hinge penalty sum.

- **Critical path:**
  1. Implement each SEED component as a standalone function.
  2. Validate each component against provided human ratings (see Section 4.4) to reproduce meta-evaluation scores.
  3. Integrate pairwise hinge loss into an existing decoder (e.g., MindEye) and confirm improvement on SEED.

- **Design tradeoffs:**
  - **SEED vs. End-to-End Learned Metric:** SEED is interpretable and modular, but a learned metric could adapt better to domain shifts.
  - **Loss Term Weighting (λpair):** Too high may destabilize training; too low yields negligible gain. Requires per-model tuning.
  - **Threshold Averaging in Object F1:** Improves robustness to distorted objects but increases compute vs. single-threshold detection.

- **Failure signatures:**
  - **SEED components diverge:** If Object F1 consistently underperforms (e.g., due to grounding model bias), consider re-weighting or replacing that component.
  - **Loss fails to improve SEED:** May indicate that the base model’s embedding space is already well-aligned or that the batch size is too small for meaningful pairwise comparisons.

- **First 3 experiments:**
  1. **Metric Validation:** Reproduce Table 1 meta-evaluation on the provided human ratings to verify SEED implementation.
  2. **Ablation Study:** Remove one SEED component at a time and measure alignment drop with human ratings (as in Figure 3).
  3. **Loss Integration:** Add pairwise hinge loss to a simple linear regression baseline (fMRI → CLIP embedding) and measure improvement in SEED and EffNet correlation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can visual brain decoding models be specifically optimized to minimize the "semantic near-miss" phenomenon, where reconstructions capture the correct supercategory but fail on specific object categories?
- **Basis in paper:** [explicit] Section 6.5 identifies the high rate of semantic near-misses (17-20%) as a failure mode and states, "Addressing this issue could be a promising avenue for future research, potentially yielding more precise and semantically faithful reconstructions."
- **Why unresolved:** Current loss functions and metrics do not explicitly penalize confusions within a supercategory (e.g., reconstructing a cat instead of a dog) as heavily as cross-category errors, leaving this specific granularity unresolved.
- **What evidence would resolve it:** The development of a training objective or model architecture that demonstrates a statistically significant reduction in the "Relaxed Object Recall" gap compared to standard models.

### Open Question 2
- **Question:** How can evaluation frameworks effectively integrate perceptual quality assessment with semantic fidelity?
- **Basis in paper:** [explicit] The Conclusion states: "our approach has some limitations in that SEED does not account for perceptual aspects. We hope that future research will build upon our open-source human survey results to develop more refined evaluation metrics."
- **Why unresolved:** SEED focuses exclusively on semantic alignment (objects, captions) and high-level features, ignoring low-level visual artifacts or distortions that affect human perception of quality.
- **What evidence would resolve it:** A composite metric that correlates with human judgment on both semantic accuracy and perceptual quality (e.g., texture, lighting) better than SEED or existing metrics like SSIM alone.

### Open Question 3
- **Question:** Does a learned or dynamic weighting of the SEED sub-components (Object F1, Cap-Sim, EffNet) improve alignment with human judgment compared to the static average currently used?
- **Basis in paper:** [inferred] The paper defines SEED as a simple average ($1/3$) of three metrics. While Section 6.3 validates the combination, it does not explore if specific image types benefit from weighting captions over objects.
- **Why unresolved:** The relative importance of specific semantic details (e.g., background vs. objects) varies between images; a static average assumes equal importance for all inputs.
- **What evidence would resolve it:** An ablation study showing that an adaptive weighting strategy—potentially trained on the released human judgment data—achieves a higher Pairwise Accuracy or Kendall’s Tau-b than the static average.

## Limitations
- **Metric scope:** SEED does not account for perceptual quality (e.g., texture, lighting), focusing only on semantic alignment.
- **External dependencies:** SEED relies on several external models (MM-Grounding-DINO, GIT, Sentence Transformer, EfficientNet) with unspecified configurations, which may affect reproducibility.
- **Human judgment bias:** The meta-evaluation is based on a finite set of human ratings (22 raters, 1,000 pairs), which may not generalize to all image types or domains.

## Confidence
- **High:** Multi-component aggregation approach and meta-evaluation design are clearly specified and validated.
- **Medium:** Pairwise hinge loss mechanism is empirically supported but underlying assumptions about embedding geometry are not rigorously proven.
- **Low:** Reproducibility of exact metric scores is uncertain due to unspecified model checkpoints and thresholds.

## Next Checks
1. **Replicate meta-evaluation:** Use the provided human ratings to reproduce Table 1 and verify SEED's reported alignment scores.
2. **Ablation of SEED components:** Systematically remove each component (Object F1, Cap-Sim, EffNet) and measure the drop in correlation with human ratings to confirm the necessity of each.
3. **Loss hyperparameter sensitivity:** Test a wider range of λpair and Tpair values on at least one base model to establish the robustness of the pairwise hinge loss improvement.