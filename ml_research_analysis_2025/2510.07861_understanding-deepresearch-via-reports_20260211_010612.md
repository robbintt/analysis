---
ver: rpa2
title: Understanding DeepResearch via Reports
arxiv_id: '2510.07861'
source_url: https://arxiv.org/abs/2510.07861
tags:
- deepresearch
- research
- reports
- report
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DeepResearch-ReportEval, a comprehensive
  framework for evaluating DeepResearch systems through their generated research reports.
  The approach decomposes report evaluation into three dimensions: quality (comprehensiveness,
  coherence, clarity, insightfulness), redundancy, and factuality, using an LLM-as-a-Judge
  methodology refined through iterative human alignment to achieve strong expert concordance.'
---

# Understanding DeepResearch via Reports

## Quick Facts
- **arXiv ID:** 2510.07861
- **Source URL:** https://arxiv.org/abs/2510.07861
- **Reference count:** 23
- **Key outcome:** DeepResearch-ReportEval framework introduces systematic LLM-as-a-Judge evaluation of research reports across quality, redundancy, and factuality dimensions.

## Executive Summary
This paper introduces DeepResearch-ReportEval, a comprehensive framework for evaluating DeepResearch systems through their generated research reports. The approach decomposes report evaluation into three dimensions: quality (comprehensiveness, coherence, clarity, insightfulness), redundancy, and factuality, using an LLM-as-a-Judge methodology refined through iterative human alignment to achieve strong expert concordance. A benchmark of 100 curated queries across 12 categories is released, enabling systematic capability comparison. Evaluation of four commercial systems reveals distinct design philosophies, with Qwen achieving the highest quality and factuality scores (comprehensive score 3.80/4, average support score 0.55/1), while Perplexity scores highest on coherence and clarity (3.60/4 and 3.46/4) due to its concise, bullet-point style. The framework demonstrates strong alignment with human experts, achieving 61.11% ranking agreement.

## Method Summary
The evaluation framework uses GPT-4o as an LLM judge, with prompts iteratively refined through human alignment to minimize deviation from expert scores. Quality is assessed via single-pass scoring across four dimensions. Redundancy detection uses systematic pairwise paragraph comparison (limited to 30 pairs per report to manage computational complexity). Factuality evaluation extracts claims and their cited sources, then uses the judge to assess claim-source support levels (-1 to 1). The framework includes a 100-query benchmark spanning 12 categories, with human experts providing ground truth for iterative prompt refinement.

## Key Results
- Qwen achieves highest quality score (3.80/4) and factuality score (0.55/1 average support)
- Perplexity scores highest on coherence (3.60/4) and clarity (3.46/4) due to concise formatting
- Framework achieves 61.11% ranking agreement with human experts
- Gemini's longer reports face systematic penalties despite strong dimensional ratings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative prompt refinement allows an LLM judge to approximate human expert preferences for subjective research quality.
- **Mechanism:** The system uses a feedback loop where human experts score a subset of reports (120 queries). The LLM judge's prompt templates are manually adjusted until the Mean Absolute Deviation (MAD) between the LLM and human scores minimizes (reported as 0.72 for quality).
- **Core assumption:** The prompt templates generalize from the 120-query alignment set to the full 100-query benchmark without significant distribution shift.
- **Evidence anchors:**
  - [section 2.3.4] "We design an iterative prompt refinement mechanism... ensuring that the model's scores... closely match average scores provided by experts."
  - [abstract] "...using an LLM-as-a-Judge methodology refined through iterative human alignment to achieve strong expert concordance."
  - [corpus] "Deep Research Comparator" (arXiv 2507.05495) validates the need for fine-grained human annotations to assess long reports.
- **Break condition:** If the specific LLM judge (e.g., GPT-4o) is replaced or updated, the alignment likely degrades, requiring re-calibration of prompts.

### Mechanism 2
- **Claim:** Pairwise paragraph comparison detects semantic redundancy that holistic "read-the-whole-report" prompts miss.
- **Mechanism:** Instead of scoring the whole document, the framework segments the report into paragraphs $r = (p_1, \dots, p_k)$, generates all possible pairs, and uses the LLM to score repetition (0-4) for each pair. The final score is the arithmetic average.
- **Core assumption:** Semantic overlap between any two paragraphs equally degrades the report quality, regardless of where the paragraphs appear (excluding intro/outro).
- **Evidence anchors:**
  - [section 2.3.2] "Standard LLM-as-a-Judge evaluation approaches often fail to detect these subtle redundancy patterns... We formalize the detection task as a systematic... workflow."
  - [table 2] Notes that paragraph pair counts were limited to 30 "due to quadratic computational complexity."
- **Break condition:** This mechanism scales quadratically ($O(n^2)$); for very long reports (e.g., >100 paragraphs), the computational cost may force aggressive truncation, missing redundancy in excluded sections.

### Mechanism 3
- **Claim:** Decomposing factuality into claim-source support scores isolates hallucinations from citation errors.
- **Mechanism:** The evaluation extracts specific claims and their linked sources. An LLM judges if the source supports the claim (1), partially supports (0), or contradicts/does not support (-1). The "Average Support Score" aggregates these into a reliability metric.
- **Core assumption:** The extraction phase correctly identifies the boundaries of "claims" and maps them to the specific "source" intended by the generator.
- **Evidence anchors:**
  - [section 2.3.3] "We assess the alignment between each claim and its associated cited source... independent of the web search process."
  - [abstract] "Qwen [achieves] the highest quality and factuality scores... average support score 0.55/1."
  - [corpus] "DeepResearch Bench II" (arXiv 2601.08536) similarly diagnoses agents via rubrics, supporting the move away from simple binary accuracy.
- **Break condition:** If a report cites a source that is valid but not textually explicit (requires multi-step reasoning to connect), the LLM judge might falsely classify it as "Not Supported."

## Foundational Learning

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The core engine of the ReportEval framework. You must understand that the "judge" is itself an LLM (specifically GPT-4o in this paper) instructed to emulate a human reviewer.
  - **Quick check question:** Does the framework use a fine-tuned model or a prompt-engineered model for evaluation? (Answer: Prompt-engineered with iterative alignment).

- **Concept: Semantic Redundancy vs. Repetition**
  - **Why needed here:** Section 2.3.2 distinguishes between surface-level copy-paste and "implicit repetition" (same argument, different words). Understanding this is key to why the pairwise method was chosen over simple hashing or n-gram overlap.
  - **Quick check question:** Why is a cosine similarity check on embeddings insufficient for detecting the "implicit repetition" described in the paper?

- **Concept: Citation Grounding (Factuality)**
  - **Why needed here:** DeepResearch agents must distinguish between "knowledge" and "retrieved evidence." The Factuality metric (Mechanism 3) specifically tests if the output is tethered to the provided sources, not just if it is true in the real world.
  - **Quick check question:** If a report contains a true fact but cites a source that does not mention it, what score would the Claim-Source alignment mechanism assign? (Answer: -1 or 0, likely "Not Supported").

## Architecture Onboarding

- **Component map:**
  1. Input Interface: Accepts Query + Generated Report (Markdown/Text)
  2. Quality Evaluator: Single-pass LLM call assessing Comprehensiveness, Coherence, Clarity, Insightfulness
  3. Redundancy Module: Segments text → Pair Generator (limit 30 pairs) → Parallel LLM calls → Score Aggregator
  4. Factuality Module: Claim Extractor → Source Matcher → Support Evaluator (1/0/-1) → Score Calculator
  5. Alignment Layer: Human Expert Interface (used for training/refining prompts, not runtime)

- **Critical path:** The **Iterative Prompt Refinement** (Section 2.3.4) is the foundation. A new engineer must first verify the specific prompt versions (Appendix B) are being used, as generic prompts yield high MAD.

- **Design tradeoffs:**
  - **Length vs. Depth:** The paper notes Perplexity sacrifices comprehensiveness for clarity/coherence, while Qwen maximizes depth. The evaluation architecture treats these as independent axes, but real-world UX often forces a trade-off.
  - **Computation vs. Precision:** Redundancy checks are capped at 30 pairs (Table 2). This trades off exhaustive coverage for tractable inference costs.

- **Failure signatures:**
  - **The "Length Bias":** Section 3 notes Gemini's longer reports may have suffered "systematic penalties" regardless of quality. Watch for LLM judges penalizing necessary verbosity in complex topics.
  - **Source Mismatch:** If the Claim Extractor misidentifies the scope of a claim, the factuality score will be noisy.

- **First 3 experiments:**
  1. **Sanity Check:** Reproduce the Quality evaluation on the 100 curated queries (available in repo) using GPT-4o. Calculate MAD against the paper's reported baselines to verify your prompt implementation.
  2. **Ablation on Redundancy:** Take a synthetic report with inserted duplicate paragraphs. Run the Redundancy Module with and without the 30-pair limit to measure the sensitivity of the "capping" assumption.
  3. **Cross-Model Judge:** Run the Factuality evaluation using a smaller, open-source model (e.g., Llama-3) instead of GPT-4o to measure the degradation in alignment with human experts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the quality of user-system interaction and query clarification be systematically evaluated to improve final report generation?
- Basis in paper: [explicit] Section 4.1 states that "what constitutes a good query remains underexplored and lacks systematic evaluation" despite evidence that query quality correlates with final output.
- Why unresolved: Current benchmarks focus on the research output, neglecting the "Interaction" phase where user intent is refined.
- What evidence would resolve it: A benchmark dataset linking specific clarification strategies/interactions to measurable improvements in final report accuracy and depth.

### Open Question 2
- Question: What training objectives and benchmarks are required to transition search agents from single-answer retrieval to comprehensive, research-oriented evidence gathering?
- Basis in paper: [explicit] Section 4.2 notes that "search agents that excel on traditional answer-centric benchmarks may prove ineffective in DeepResearch settings."
- Why unresolved: Existing benchmarks prioritize specific fact retrieval over the evidence coverage and perspective diversity required for deep research.
- What evidence would resolve it: Development of benchmarks scoring "evidence coverage" and "perspective diversity" rather than simple accuracy.

### Open Question 3
- Question: Does the LLM-as-a-Judge evaluation methodology exhibit a systematic bias against longer report formats?
- Basis in paper: [explicit] Section 3 observes that "Longer content may face systematic penalties regardless of actual quality," noting Gemini's lower overall scores despite strong dimensional ratings.
- Why unresolved: The study utilized GPT-4o as the judge, and potential length-related biases in such models when evaluating extensive research reports are not yet quantified.
- What evidence would resolve it: An ablation study controlling for content quality while varying report length to isolate the judge's length bias.

### Open Question 4
- Question: How can evaluation frameworks capture intermediate agent behaviors (e.g., search depth, tool usage) that are obscured by end-to-end report assessment?
- Basis in paper: [explicit] Section 4.3 argues that the report-based evaluation approach "may obscure important intermediate processes and system behaviors that merit independent assessment."
- Why unresolved: The current framework assesses the final artifact, failing to distinguish between an efficient research process and a brute-force approach.
- What evidence would resolve it: Metrics tracking "search depth" and "end-to-end speed" correlated with report quality scores to analyze efficiency trade-offs.

## Limitations

- **Claim extraction ambiguity:** The factuality evaluation methodology lacks specification of how claims and sources are identified from reports.
- **Judge model dependency:** Iterative alignment is performed specifically for GPT-4o and may not generalize to other judge models without re-calibration.
- **Length bias potential:** The framework may systematically penalize longer reports regardless of quality, as observed with Gemini's performance.

## Confidence

- **High Confidence:** Quality evaluation methodology and iterative refinement process are well-documented and reproducible.
- **Medium Confidence:** Factuality evaluation framework is conceptually sound but missing claim extraction component creates uncertainty.
- **Low Confidence:** Cross-system comparisons may be influenced by length bias, and framework's sensitivity to judge model version changes is unknown.

## Next Checks

1. Implement and test the Quality evaluation module on the released 100-query benchmark, calculating MAD against the paper's baselines to verify prompt implementation accuracy.
2. Create a synthetic test suite with known redundancy patterns to validate the 30-pair sampling strategy and measure sensitivity to the quadratic complexity tradeoff.
3. Reproduce the factuality evaluation using a controlled set of reports with manually annotated claim-source pairs to verify the extraction and verification pipeline.