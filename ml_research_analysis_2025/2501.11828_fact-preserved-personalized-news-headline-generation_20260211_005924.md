---
ver: rpa2
title: Fact-Preserved Personalized News Headline Generation
arxiv_id: '2501.11828'
source_url: https://arxiv.org/abs/2501.11828
tags:
- news
- headline
- personalized
- user
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FPG, a framework for generating personalized
  news headlines that balance user appeal with factual consistency. It introduces
  a history encoder to model user interests, a personalized news encoder using cross-attention
  to align candidate news with historical clicks, and a user-guided decoder with fact-aware
  user embeddings.
---

# Fact-Preserved Personalized News Headline Generation

## Quick Facts
- arXiv ID: 2501.11828
- Source URL: https://arxiv.org/abs/2501.11828
- Reference count: 40
- Primary result: Achieves 27.33 ROUGE-1, 10.51 ROUGE-2, 23.30 ROUGE-L, 89.26 FactCC, and 17.27 PC(max) on PENS benchmark

## Executive Summary
This paper addresses the challenge of generating personalized news headlines that balance user appeal with factual consistency. The proposed FPG framework introduces a history encoder to model user interests, a personalized news encoder using cross-attention to align candidate news with historical clicks, and a user-guided decoder with fact-aware user embeddings. An additional contrastive learning stage enhances factual consistency. Experiments on the PENS benchmark show FPG outperforms baselines, demonstrating strong personalization while preserving factual accuracy.

## Method Summary
FPG uses a 4-stage training pipeline: (1) Pre-train a BART-base generator on distant supervision data, (2) Train a history encoder to model user interests, (3) Jointly train the full model, and (4) Apply contrastive learning to enhance factual consistency. The core innovation is the History-Cross Attention layer, which conditions news encoding on historical clicks, and the replacement of the [BOS] token with a fact-aware user embedding. This allows the model to generate headlines tailored to user interests while maintaining fidelity to the source article.

## Key Results
- Outperforms baselines with 27.33 ROUGE-1, 10.51 ROUGE-2, and 23.30 ROUGE-L
- Achieves 89.26 FactCC score, indicating strong factual consistency
- Personalization metrics: 17.27 PC(max) and 12.40 PC(avg)

## Why This Works (Mechanism)

### Mechanism 1: Similarity-Gated Fact Attention
Conditioning the encoding of candidate news on historical clicks improves factual consistency while maintaining personalization relevance. A History-Cross Attention sub-layer in the encoder uses the candidate news body as the Query and historical user interests as Key/Value, forcing the model to prioritize facts within the current news article that semantically align with the user's reading history.

### Mechanism 2: Fact-Aware Global User Embedding
Replacing the static decoding start token with a dynamic, weighted user embedding injects preferences without overriding the source content. Instead of a fixed user ID, the model calculates a global user vector as a weighted sum of historical news embeddings, derived from attention scores in the first encoder block (relevance to candidate news).

### Mechanism 3: Contrastive Consistency Enforcement
An auxiliary contrastive learning objective explicitly separates factually consistent headlines from inconsistent ones. The model undergoes a 4th training stage using a contrastive loss that maximizes probability for "positive" headlines (high factual scores) and minimizes it for "negative" headlines generated by rule-based corruption.

## Foundational Learning

- **Cross-Attention in Encoder-Decoder Architectures**
  - Why needed: The core innovation relies on "History-Cross Attention." You must understand how Queries, Keys, and Values allow one sequence to be filtered/re-weighted by another.
  - Quick check: If the user history is "Sports" and the candidate news is "Finance", how would cross-attention weights likely appear compared to a history of "Finance"?

- **Contrastive Learning (InfoNCE/Triplet Loss)**
  - Why needed: The "Fact-enhanced Training" stage uses this to fix consistency. Understanding how to construct positive/negative pairs is critical for reproducing the training pipeline.
  - Quick check: Why is it necessary to generate synthetic negative instances rather than just using random sentences as negatives?

- **Sequence-to-Sequence (BART/Transformer) Decoding**
  - Why needed: The model modifies the standard BART decoder by replacing the [BOS] token. You need to know the role of initial decoder states in autoregressive generation.
  - Quick check: What happens to the generation output if the embedding replacing [BOS] is a zero-vector? What if it is a random noise vector?

## Architecture Onboarding

- **Component map:** History Encoder (GRU + Attention) -> Personalized News Encoder (Transformer + History-Cross Attention) -> User-Guided Decoder (Transformer with User Embedding)
- **Critical path:** The calculation of attention scores in the Personalized News Encoder is the critical path. These scores do double duty: they gate the facts in the encoder and form the global user embedding for the decoder.
- **Design tradeoffs:**
  - GRU vs. Transformer for History: The paper tests GRU, CNN, and Self-Attention. GRU performed best on ROUGE, but Self-Attention was competitive. Trade-off is sequential processing vs. parallelism.
  - Data Sparsity: They limit training data so a single popular article doesn't dominate user profiles. This sacrifices data volume for signal quality.
- **Failure signatures:**
  - Topic Hijacking: The generated headline describes an event from the user's history rather than the candidate news (factual error).
  - Generic Output: If the History-Cross Attention fails, the model ignores the user embedding and outputs the standard BART headline (high consistency, zero personalization).
- **First 3 experiments:**
  1. Run BART-base on the PENS corpus without the history encoder to establish the upper bound for factual consistency but lower bound for personalization.
  2. Implement FPG but replace the [BOS] token with a standard [BOS] (removing user embedding). Verify the drop in PC(max) scores.
  3. Visualize the History-Cross Attention weights. Do they highlight entities present in both the candidate news and user history?

## Open Questions the Paper Calls Out

- **How can innate user preferences and behavioral tendencies be effectively modeled to improve personalized headline generation?**
  - The conclusion advocates for "further research to model the various interests of users, including innate preferences and behavioral tendencies."
  - This is unresolved because FPG relies primarily on historical click sequences and does not explicitly disentangle or model static user traits.
  - Evidence would come from integrating user profiling or demographic features into the history encoder, demonstrating improved personalization metrics.

- **Can fact-preserved personalization be robustly maintained when the candidate news article lacks facts that align with the user's historical click patterns?**
  - The conclusion notes that generating high-quality headlines is "particularly challenging when the candidate news lacks facts that align with the userâ€™s historical click pattern."
  - This is unresolved because low similarity may result in weak attention signals, potentially degrading the "fact-preserved" aspect or resulting in generic headlines.
  - Evidence would come from evaluation on a stratified test set where semantic similarity between candidate news and user history is intentionally low.

- **Does the use of original headlines as "imperfect labels" via distant supervision impose an upper bound on the model's ability to learn diverse personalization styles?**
  - Section V.A states they "approximate original headlines of newly clicked news... as imperfect labels," and Section I notes readers with different preferences find different "focal characters."
  - This is unresolved because if training targets are standard original headlines (often written for mass appeal), the model may learn to reproduce these standard summaries rather than generating distinct, user-specific headlines.
  - Evidence would come from analyzing the diversity of generated headlines for a single news item across different user profiles.

## Limitations

- The rule-based method for generating negative headlines in the contrastive learning stage is not specified, affecting reproducibility.
- The batch size used in training is not reported, which could impact memory requirements and training stability.
- The selection criteria for "high factual accuracy scores" used to filter positive instances for contrastive learning are not defined.

## Confidence

- **High Confidence:** The core architectural components (History-Cross Attention, user-guided decoder) and their intended mechanisms are well-described and logically sound.
- **Medium Confidence:** The effectiveness of the contrastive learning stage is supported by the experimental results, but the lack of detail on negative sample generation introduces uncertainty.
- **Low Confidence:** The selection process for positive instances in the contrastive learning stage is vague, relying on "high factual accuracy scores" without defining the threshold or metric used.

## Next Checks

1. **Attention Visualization:** Generate and visualize the History-Cross Attention weights for a sample user/news pair. Verify that the attention mechanism correctly highlights entities or phrases present in both the candidate news and the user's historical clicks.

2. **Contrastive Learning Ablation:** Implement FPG without the contrastive learning stage (Stage 4). Compare FactCC scores to the full model. A significant drop would validate the necessity and effectiveness of the contrastive training for factual consistency.

3. **Rule-Based Negative Sample Inspection:** Examine a sample of the generated negative headlines (e.g., after entity swapping). Manually assess whether they are plausible enough to train the model on factual inconsistency, or if they are too syntactically distinct to provide meaningful gradients.