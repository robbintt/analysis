---
ver: rpa2
title: Causal Time Series Generation via Diffusion Models
arxiv_id: '2509.20846'
source_url: https://arxiv.org/abs/2509.20846
tags:
- time
- environment
- generation
- counterfactual
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CaTSG, a diffusion-based framework for causal
  time series generation. It addresses the problem that existing conditional time
  series models learn spurious correlations by not accounting for unobserved confounders.
---

# Causal Time Series Generation via Diffusion Models

## Quick Facts
- **arXiv ID**: 2509.20846
- **Source URL**: https://arxiv.org/abs/2509.20846
- **Reference count**: 40
- **Primary result**: CaTSG achieves superior observational fidelity and outperforms baselines by 2.4% - 98.7% across four metrics, while supporting interventional and counterfactual generation that existing models cannot handle.

## Executive Summary
This paper proposes CaTSG, a diffusion-based framework for causal time series generation. It addresses the problem that existing conditional time series models learn spurious correlations by not accounting for unobserved confounders. CaTSG incorporates a learnable environment bank and backdoor-adjusted guidance to support interventional and counterfactual generation, formalized within Pearl's causal ladder. The method derives causal score functions via backdoor adjustment and abduction-action-prediction, enabling principled support for all three levels of time series generation. Experiments on synthetic and real-world datasets show CaTSG achieves superior observational fidelity and outperforms baselines by 2.4% - 98.7% across four metrics, while also supporting interventional and counterfactual generation that existing models cannot handle.

## Method Summary
CaTSG is a diffusion-based framework for causal time series generation that addresses latent confounding through backdoor adjustment. The model consists of three components: an environment inference network (EnvInfer) that estimates posterior weights over a learnable environment bank, a denoiser that predicts noise conditioned on both the context and inferred environment, and a discrete environment bank with orthogonality regularization. Training uses classifier-free guidance and a swapped prediction loss to discover environments unsupervised. The framework supports three levels of generation: observational (P(X|C)), interventional (P(X|do(C))) via backdoor adjustment, and counterfactual (P(X'|X,C,C')) via abduction-action-prediction.

## Key Results
- CaTSG outperforms baselines by 2.4% - 98.7% across four metrics (MDD, KL, MMD, J-FTSD) on synthetic and real-world datasets
- The method successfully generates interventional and counterfactual samples, which existing models cannot handle
- Ablation studies confirm that the environment module and swapped prediction loss are essential for performance gains
- Learned environment posteriors align with dataset splits and form identifiable clusters in t-SNE space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backdoor-adjusted guidance removes confounding from unobserved latent environments during diffusion sampling, enabling interventional generation.
- Mechanism: The framework marginalizes over a discrete latent environment bank E using inferred posterior weights w_k = P(e_k|x_t, c) at each denoising step. The aggregated noise estimate \(\hat{\epsilon} = (1+\omega)\sum_k w_k \epsilon_{\text{env}}^k - \omega \epsilon_{\text{base}}\) replaces the standard conditional score, steering samples toward P(X|do(C)) rather than P(X|C).
- Core assumption: The predefined SCM (Figure 2b) correctly specifies E as a confounder between C and X, and the discrete environment bank sufficiently approximates the continuous latent space.
- Evidence anchors:
  - [abstract] "...develop CaTSG, a unified diffusion-based framework with backdoor-adjusted guidance that causally steers sampling toward desired interventions..."
  - [section 4.1] Eq. 2 derives P(X|do(C)) = Σ_e P(X|C, E=e)P(E=e) via backdoor adjustment.
  - [section 4.2] Eq. 6 provides the unified approximation for both interventional and counterfactual score functions.
  - [corpus] Related work DoFlow (arxiv:2511.02137) similarly targets interventional and counterfactual time-series prediction using flow-based models over a causal DAG, indicating independent convergence on similar causal objectives but with different generative backbones.
- Break condition: If the true causal graph violates the assumed confounding structure (e.g., feedback loops, mediators, or additional confounders), or if E fails to capture all relevant latent variation, residual confounding may persist.

### Mechanism 2
- Claim: Abduction-Action-Prediction (AAP) procedure enables counterfactual generation by fixing the inferred environment for a specific observed trajectory.
- Mechanism: Given an observed (x_0, c), EnvInfer infers posterior weights w over E (abduction). The context is replaced with c' (action), and denoising proceeds using the same fixed weights w to guide sampling toward P(X'|X, C, C') (prediction). This holds the latent environment constant while varying the intervention.
- Core assumption: The posterior P(E|X, C) is accurately inferred by EnvInfer, and the same environment remains relevant under the counterfactual context.
- Evidence anchors:
  - [abstract] "...derives causal score functions via backdoor adjustment and the abduction–action–prediction procedure..."
  - [section 4.1] Explicitly describes the three-step AAP procedure with the transportation example.
  - [section 4.3] Algorithm 3 details the counterfactual generation procedure, anchoring weights w from the initial inference step.
  - [corpus] Unsupervised Structural-Counterfactual Generation (arxiv:2502.12013) addresses counterfactual generation under domain shift without paired data, suggesting the broader challenge of counterfactual evaluation without ground truth.
- Break condition: If the counterfactual context c' lies far outside the training distribution, or if the environment inference is unfaithful, the generated counterfactual may not reflect the true individual-level response.

### Mechanism 3
- Claim: Unsupervised environment discovery via swapped prediction loss enables learning latent confounder representations without labeled environments.
- Mechanism: EnvInfer is trained with a SwAV-style loss that forces different augmentations of the same sample to agree on environment assignments, balanced via Sinkhorn-Knopp to prevent collapse. The environment bank is regularized for orthogonality, promoting diversity.
- Core assumption: The latent environments can be discretized into K sufficiently distinct and identifiable clusters from observational data alone.
- Evidence anchors:
  - [section 4.3] Eq. 8 defines the swapped prediction loss L_sw, and the orthogonality loss L_orth.
  - [section 5.4] Figure 5 shows learned environment posteriors align with dataset splits and form identifiable clusters in t-SNE space.
  - [corpus] Weak corpus evidence for this specific unsupervised discovery mechanism in time-series causal generation; related work focuses on structural causal models or supervised settings.
- Break condition: If the true latent environment structure is continuous or has more modes than K, or if augmentations do not preserve environment identity, the learned bank may not align with true confounders.

## Foundational Learning

- **Concept**: **Pearl's Causal Ladder (Structural Causal Models, do-calculus)**
  - Why needed here: The entire framework is built on formalizing TSG tasks as association (P(X|C)), intervention (P(X|do(C))), and counterfactual (P(X'|X,C,C')) queries. Understanding SCMs and do-calculus is essential to grasp the theoretical derivation of the causal score functions.
  - Quick check question: Can you explain why P(X|do(C)) ≠ P(X|C) when a confounder E affects both C and X?

- **Concept**: **Diffusion Models as Score-Based Generative Models**
  - Why needed here: CaTSG uses a diffusion backbone where the denoiser ε_θ estimates the score function ∇log p(x_t|c). The causal guidance is injected by modifying this score estimate at each denoising step.
  - Quick check question: In a diffusion model, what does the denoising network ε_θ(x_t, t, c) approximate, and how does classifier-free guidance (CFG) use it?

- **Concept**: **Backdoor Adjustment and Confounding**
  - Why needed here: The core theoretical contribution is using backdoor adjustment to derive an interventional score from an observational model. This is the key to moving from correlation-based to causally-grounded generation.
  - Quick check question: Given a causal graph C ← E → X, what adjustment set would you use to estimate P(X|do(C)), and why?

## Architecture Onboarding

- **Component map**:
  - **EnvInfer (q_ϕ)**: TCN encoder → temporal statistics/attention pooling/spectral features → MLP → produces latent h, logits s, and posterior weights w
  - **Environment Bank (E)**: Learnable K×H embedding matrix, L2-normalized, orthogonal regularization
  - **Denoiser (ε_θ)**: 1D U-Net that takes (x_t, t, [c, e_k] or ∅) and predicts noise
  - **Training Losses**: L_eps (noise prediction), L_sw (swapped prediction), L_orth (orthogonality)

- **Critical path**:
  1. **Training**: For each batch, create two augmentations → EnvInfer infers (h, s, w) for each → compute L_sw via Sinkhorn targets. Separately, sample noise, corrupt x_0 → EnvInfer infers w from original (x_0, c) → denoiser predicts noise per environment → aggregate with w → compute L_eps. Jointly optimize L = L_eps + αL_sw + βL_orth.
  2. **Interventional Sampling**: Start from x_T ~ N(0,I). At each step t: EnvInfer infers w from (x_t, c) → denoiser predicts ϵ_base and {ϵ_env^k} → aggregate per Eq. 6 → DDPM update.
  3. **Counterfactual Sampling**: (Abduction) Infer w from observed (x_0, c). (Action) Set c ← c'. (Prediction) Run denoising with fixed w from abduction step.

- **Design tradeoffs**:
  - **Number of environments K**: Larger K may capture more complex latent structure but increases compute (K forward passes per step) and risk of overfitting. Paper uses K=4-8.
  - **Guidance scale ω**: Controls strength of causal adjustment. ω=0 reduces to standard conditional generation. Higher ω enforces stronger alignment with interventional target.
  - **Augmentation strategy for L_sw**: Must preserve environment identity while providing sufficient variation. Choice of augmentations affects environment clustering quality.
  - **Sampler steps**: Trade-off between generation quality and speed. Paper finds 20 DPM-Solver steps a good balance.

- **Failure signatures**:
  - **Collapsed environments**: All posterior weights concentrate on a single embedding → orthogonality loss ineffective or L_sw too weak.
  - **Poor counterfactual fidelity**: Generated counterfactuals do not preserve temporal structure or show implausible changes → environment inference inaccurate or AAP procedure flawed.
  - **No gain over baselines**: CaTSG performs similarly to TimeWeaver on observational metrics → environment module not learning useful structure or dataset lacks significant confounding.
  - **Training instability**: Loss does not converge → check coefficient balance (α, β) and learning rates.

- **First 3 experiments**:
  1. **Ablation on environment components**: Train variants (RandEnv, w/o SW, FrozenEnv, w/o Env) to isolate contribution of each loss and the environment bank. Compare on MDD/KL/MMD/J-FTSD.
  2. **Hyperparameter sensitivity**: Vary K (2,4,8,16), ω (0.5,1.0,2.0), hidden size H, and loss coefficients (α, β). Plot KL divergence to identify stable operating regions.
  3. **Interpretability check**: For a dataset with known splits (e.g., Harmonic-VM), visualize learned environment posteriors per split and t-SNE of latent representations. Verify alignment with ground-truth environment structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be generalized to learn or adapt the causal graph structure directly from data rather than relying on predefined Structural Causal Models (SCMs)?
- Basis in paper: [explicit] The Conclusion and Appendix H.7 identify "relaxing the reliance on predefined SCMs" as a necessary step toward more flexible and generalizable models.
- Why unresolved: The current CaTSG method assumes a fixed SCM (e.g., $C \leftarrow E \rightarrow X$) to derive backdoor-adjusted score functions; if the true mechanism violates this graph (e.g., contains feedback loops), identification and performance may degrade.
- What evidence would resolve it: A modified framework that integrates causal discovery with the diffusion process to dynamically identify the graph structure while maintaining generation fidelity.

### Open Question 2
- Question: What systematic evaluation protocols can be established for counterfactual time series generation on real-world datasets where ground-truth counterfactuals are unobservable?
- Basis in paper: [explicit] The Conclusion lists "developing systematic evaluation protocols for counterfactuals in real-world data" as a key challenge, a point reiterated in Appendix H.5 and H.7.
- Why unresolved: While synthetic datasets allow for direct comparison against ground truth, real-world applications (e.g., healthcare, traffic) cannot observe the outcome of an event that did not happen, making rigorous validation currently infeasible.
- What evidence would resolve it: The proposal and validation of a standardized metric or proxy that correlates strongly with counterfactual validity on real-world benchmarks.

### Open Question 3
- Question: Can the conditioning mechanism be expanded to support multi-modal inputs (e.g., textual guidelines or visual cues) alongside time series covariates?
- Basis in paper: [explicit] The Conclusion and Appendix H.7 list exploring "multi-modal conditions" as a future direction to enable richer interventional and counterfactual scenarios.
- Why unresolved: The current EnvInfer and Denoiser architectures are specifically tailored for sequential time series tensors and do not natively process or encode heterogeneous data modalities like text or images.
- What evidence would resolve it: An architectural extension that effectively fuses multi-modal embeddings into the diffusion guidance process without compromising the model's ability to capture temporal dependencies.

## Limitations
- Reliance on correct causal graph specification; real-world systems may involve feedback loops or mediators not captured by assumed SCM
- Discrete environment representation may fail to capture continuous or high-dimensional latent confounders
- Unsupervised environment discovery depends on augmentations preserving environment identity, which may not hold across diverse time-series domains

## Confidence
- **High confidence**: Observational generation quality (MDD, KL, MMD, J-FTSD metrics) and ablation results showing individual component contributions
- **Medium confidence**: Interventional generation claims (improvement over TimeWeaver) and counterfactual generation capability (quantitative metrics not reported)
- **Low confidence**: Generalization to real-world datasets where true environment structure is unknown and may violate assumed SCM

## Next Checks
1. **Environment bank fidelity test**: Apply CaTSG to a synthetic dataset where the true latent environment structure is known but complex (e.g., continuous E with nonlinear effects). Evaluate whether learned posterior weights align with ground-truth environment membership and whether interventional generation quality degrades as K becomes insufficient.
2. **Causal graph robustness**: Modify the assumed SCM by introducing mediators or feedback loops between C and X. Test whether CaTSG still improves over non-causal baselines or if residual confounding emerges.
3. **Augmentation strategy ablation**: Systematically vary augmentation types (temporal crops, noise injection, feature masking) and evaluate their impact on environment clustering quality (via posterior entropy) and downstream generation performance across all three levels.