---
ver: rpa2
title: Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal
  Correspondence Learning
arxiv_id: '2512.19687'
source_url: https://arxiv.org/abs/2512.19687
tags:
- audio
- video
- data
- captions
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Perception Encoder Audiovisual (PE-AV), a
  family of audio-video-text encoders trained with large-scale contrastive learning.
  The authors develop a robust audiovisual data engine that synthesizes high-quality
  captions for over 100 million audio-video pairs, enabling large-scale supervision
  consistent across modalities.
---

# Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning

## Quick Facts
- arXiv ID: 2512.19687
- Source URL: https://arxiv.org/abs/2512.19687
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on diverse audiovisual benchmarks using large-scale contrastive learning with synthetic captions

## Executive Summary
This paper introduces Perception Encoder Audiovisual (PE-AV), a family of audio-video-text encoders trained with large-scale contrastive learning across ten cross-modal pairs. The authors develop a robust data engine that synthesizes high-quality captions for over 100 million audio-video pairs, enabling large-scale supervision consistent across modalities. PE-AV achieves state-of-the-art performance on benchmarks across sound, music, speech, and video tasks, improving AudioCaps text-to-audio retrieval from 35.4 to 45.8 R@1 and VGGSound classification accuracy from 36.0 to 47.1. The approach also introduces PE-A-Frame for fine-grained audio-frame-to-text alignment in sound event detection tasks.

## Method Summary
The authors present PE-AV, a family of multimodal encoders trained with contrastive learning across audio, video, and text modalities. They develop a data engine that generates synthetic captions for 100M+ audio-video pairs, enabling large-scale supervision. The model scales contrastive objectives to ten cross-modal pairs and introduces PE-A-Frame for frame-level alignment. The architecture uses separate encoders for each modality, with a fusion encoder combining audio and video features. Training employs a local-activity loss for fine-grained temporal detection and leverages synthetic captions that outperform real captions in ablation studies.

## Key Results
- Text-to-audio retrieval on AudioCaps improves from 35.4 to 45.8 R@1
- VGGSound classification accuracy increases from 36.0 to 47.1
- Zero-shot performance on AudioCaps achieves 45.8 R@1
- Strong performance across diverse audio domains (speech, music, sound effects)

## Why This Works (Mechanism)

### Mechanism 1: Symmetric Multi-Objective Contrastive Alignment
The model learns by explicitly contrasting embeddings across all modality permutations (Audio-to-Video, Video-to-Audio-Caption, etc.), forcing direct semantic alignment between any two modalities rather than projecting through a central anchor. This mitigates modality imbalance and strengthens joint embeddings. The synthetic captions provide reliable supervision signals across all pairs.

### Mechanism 2: Audiovisual Context Disambiguation
Joint embeddings allow text-to-audio retrieval to utilize visual context, breaking ties where audio alone is ambiguous. The fusion encoder projects concatenated audio-video features into a shared space, enabling queries to implicitly leverage visual embeddings for distinguishing semantically similar sounds by checking visual correlation.

### Mechanism 3: Frame-Level Temporal Localization via Local-Activity Loss
Fine-grained temporal detection is achieved by extending utterance-level contrastive alignment to frame-level tokens using a specialized local-activity loss. Instead of pooling entire clips, the model preserves frame-level audio tokens and applies sigmoid contrastive loss between these tokens and text queries specifically when sound events are active, teaching when concepts occur.

## Foundational Learning

**Concept: Contrastive Learning (SigLIP/CLIP)**
- Why needed: The entire architecture relies on maximizing similarity of paired embeddings and minimizing unpaired ones
- Quick check: Why does the paper use sigmoid loss (SigLIP) instead of softmax loss for the ten contrastive pairs?

**Concept: Audio Tokenization (DAC-VAE)**
- Why needed: Raw audio waveforms are converted into discrete tokens before entering the Transformer
- Quick check: How does the frame rate of audio tokenizer (25 Hz) compare to video frame rate (30 FPS), and how does the architecture handle this mismatch?

**Concept: Transformer Fusion Mechanisms**
- Why needed: The model uses separate towers plus a Fusion Encoder, requiring understanding of how sequence lengths are aligned and concatenated
- Quick check: In the Fusion Encoder, are audio and video tokens summed, concatenated, or cross-attended?

## Architecture Onboarding

**Component map:** Raw Audio (DAC-VAE) -> Audio Encoder (Transformer) -> Audio Features; Video Frames (PE-L) -> Video Encoder (Temporal Transformer) -> Video Features; Text (ModernBERT) -> Text Encoder -> Text Features; Fusion Encoder takes interpolated/concatenated A+V tokens -> Projection layers mapping [CLS] tokens to shared embedding space

**Critical path:**
1. Tokenization: Convert inputs to tokens
2. Encoding: Extract features via separate towers
3. Temporal Alignment: Interpolate video features to match audio sequence length
4. Fusion: Concatenate A+V tokens and process through Fusion Encoder
5. Alignment: Compute 8 (or 10) pairwise contrastive losses

**Design tradeoffs:**
- Freeze visual frame encoder (PE-L) while training audio from scratch to trade visual adaptability for training stability
- 30 FPS preserves duration information but costs more compute than fixed 16-frame sampling
- ModernBERT chosen over PE-L text encoder for longer context window (512 vs 32 tokens)

**Failure signatures:**
- Retrieval Collapse: Check if data engine failed to generate captions (empty strings)
- Temporal Drift: Verify `plocal` parameter in local-activity loss is not set too low
- Speech Failure: Ensure transcript injection probability during training is non-zero

**First 3 experiments:**
1. Load pretrained PE-AV-L checkpoint and run text-to-audio retrieval on AudioCaps to verify 45.8 R@1 benchmark
2. Train PE-AV-S using only real captions vs. only synthetic captions to confirm synthetic > real finding
3. Run T+V→A (Text + Video -> Audio) on sample where T→A fails to confirm visual disambiguation mechanism

## Open Questions the Paper Calls Out

### Open Question 1
Can scaling training data volume beyond 100M pairs overcome performance saturation observed in deep (28-layer) audio encoders? The paper notes saturation around 20 layers may be due to limited data and training steps, leaving true capacity of deeper architectures untapped.

### Open Question 2
To what extent does LLM-based data engine introduce semantic hallucinations when synthesizing audio-visual captions, and how does this impact fine-grained retrieval accuracy? The paper relies on synthetic captions but lacks independent evaluation of caption quality across diverse domains.

### Open Question 3
Can unified cross-modal embeddings learned by PE-AV serve as effective conditioning inputs for omni-modal generation tasks? The paper focuses exclusively on perception while hoping community builds upon it for future generation work.

### Open Question 4
Does scaling cross-modal retrieval objectives eventually induce negative transfer or interference in uni-modal classification performance? While Table 10 shows improved average performance with more loss pairs, including all pairs slightly drops performance on specific tasks compared to intermediate configurations.

## Limitations

- Reliance on synthetic captions creates potential single point of failure where caption hallucination could silently degrade model performance
- Temporal alignment between audio (25 Hz) and video (30 FPS) introduces uncertainty in how misalignment is handled
- Freezing visual encoder while training audio from scratch limits adaptation to dataset characteristics

## Confidence

**High Confidence:** Improvements on standard benchmarks (AudioCaps retrieval, VGGSound classification) with clear numerical improvements and established protocols

**Medium Confidence:** Zero-shot transfer capabilities and audiovisual disambiguation mechanisms supported by experimental results but could benefit from more extensive cross-dataset validation

**Low Confidence:** Synthetic caption generation pipeline validated internally but lacks third-party verification of caption quality

## Next Checks

1. Run comprehensive ablation study varying the number of contrastive pairs to identify optimal configuration before scaling to ten pairs
2. Test model robustness on post-dubbed content where audio and video are dissociated to verify it doesn't learn spurious correlations
3. Evaluate frame-level detection precision on overlapping sound events to verify local-activity loss handles temporal ambiguity