---
ver: rpa2
title: Large Language Models for Zero-shot Inference of Causal Structures in Biology
arxiv_id: '2503.04347'
source_url: https://arxiv.org/abs/2503.04347
tags:
- causal
- gene
- experimental
- context
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a framework to evaluate large language models
  (LLMs) for inferring causal relationships between genes using zero-shot inference.
  The authors leverage interventional perturbation data to construct ground truth
  causal graphs, then assess LLM predictions against these structures across thousands
  of gene pairs.
---

# Large Language Models for Zero-shot Inference of Causal Structures in Biology

## Quick Facts
- arXiv ID: 2503.04347
- Source URL: https://arxiv.org/abs/2503.04347
- Reference count: 18
- Primary result: Zero-shot LLM inference of gene-gene causal relationships achieved AUROC 0.625, outperforming knowledge-driven baselines

## Executive Summary
This work develops a framework to evaluate large language models (LLMs) for inferring causal relationships between genes using zero-shot inference. The authors leverage interventional perturbation data to construct ground truth causal graphs, then assess LLM predictions against these structures across thousands of gene pairs. Multiple retrieval-augmented prompting strategies are explored, including biological context specification and literature evidence integration. Results show that even small LLMs like Gemma2 can outperform baseline knowledge-driven methods (AUROC 0.625 vs 0.460), particularly when provided with experimental context such as cancer type and mRNA measurement details. Gene-specific descriptions and literature evidence generally reduced performance, suggesting limitations in traditional knowledge mining for causal inference. The findings demonstrate that LLMs can serve as automated tools for generating causal priors in biological discovery, offering a scalable approach to navigating complex molecular networks.

## Method Summary
The authors constructed ground truth causal graphs from Replogle et al. (2022) Perturb-seq dataset using single-gene perturbation screens in K562 cells. For each perturbed gene, they performed Mann-Whitney U tests comparing expression of other genes against non-targeting controls, applying FDR correction to establish directed edges. LLMs (specifically Gemma2-9B-it) were prompted to predict causal probabilities for all gene pairs among the top 100 cancer-relevant genes. Multiple prompt variants were tested, including naive prompts, context-specific prompts (cancer type, mRNA measurement), and gene-specific literature integration. Performance was evaluated using AUROC against the binary ground truth graphs.

## Key Results
- Gemma2-9B achieved AUROC of 0.625, outperforming STRING baseline (AUROC 0.460)
- Providing experimental context (cancer type + mRNA) improved performance significantly
- Gene-specific descriptions and literature evidence generally degraded LLM performance
- Chain-of-thought prompting reduced confidence and overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing experimental context (cancer type, mRNA measurement modality) improves LLM zero-shot causal inference compared to naive prompts.
- Mechanism: Contextual prompts likely steer the LLM's internal representations toward domain-relevant knowledge encoded during pre-training, improving the relevance of retrieved patterns for gene-gene causal relationships in that specific biological setting.
- Core assumption: The LLM has encoded domain-specific biological knowledge during pre-training that can be activated via targeted prompting.
- Evidence anchors:
  - [abstract] "Results show that even small LLMs like Gemma2 can outperform baseline knowledge-driven methods (AUROC 0.625 vs 0.460), particularly when provided with experimental context such as cancer type and mRNA measurement details."
  - [section] Section 4.4: Best performance (AUROC 0.625) achieved with "cancer + mRNA" context and no gene-specific information.

### Mechanism 2
- Claim: Zero-shot LLM inference can outperform traditional knowledge-driven baselines for causal graph prediction.
- Mechanism: LLMs synthesize broader reasoning capabilities across literature patterns, whereas databases like STRING provide symmetric association scores not designed for directed causal inference.
- Core assumption: The LLM's internal representations capture directed causal reasoning beyond simple statistical associations.
- Evidence anchors:
  - [abstract] "even small LLMs like Gemma2 can outperform baseline knowledge-driven methods (AUROC 0.625 vs 0.460)"
  - [section] Section 4.4: STRING baseline AUROC = 0.460; Gemma2 best = 0.625.

### Mechanism 3
- Claim: Providing gene-specific literature or descriptions can degrade LLM causal inference performance.
- Mechanism: Additional context may introduce noise, contradictions, or irrelevant information that reduces the LLM's ability to commit to confident causal predictions.
- Core assumption: The literature evidence lacks specificity to the experimental context and does not correlate well with ground-truth causal relationships.
- Evidence anchors:
  - [abstract] "Gene-specific descriptions and literature evidence generally reduced performance"
  - [section] Section 4.4: "Providing literature evidence near-uniformly reduces the LLM performance."

## Foundational Learning

- Concept: **Causal Structure Learning (CSL)**
  - Why needed here: CSL provides the theoretical framework for representing causal relationships as directed graphs, which is the target output for LLM inference.
  - Quick check question: Can you distinguish between a correlation and a directed causal edge in a graph?

- Concept: **Perturb-seq / Single-Cell Perturbation Screens**
  - Why needed here: This experimental method generates the ground-truth causal relationships via intervention (e.g., CRISPR knockdown) and differential expression analysis, used to evaluate LLM predictions.
  - Quick check question: How does intervening on a gene help identify its downstream causal effects?

- Concept: **Zero-Shot Inference**
  - Why needed here: The paper evaluates LLMs without task-specific fine-tuning, relying solely on pre-trained knowledge accessed via prompting.
  - Quick check question: What does "zero-shot" mean in the context of LLM evaluation?

## Architecture Onboarding

- Component map: Ground-truth construction (Perturb-seq → hypothesis testing → causal graph) → Prompt engineering (contextual + gene-specific variants) → LLM inference (Gemma2-9B) → Evaluation (AUROC vs. ground truth).

- Critical path: High-quality ground truth from intervention data → well-designed prompts with experimental context → binary thresholding of LLM probability outputs.

- Design tradeoffs: Larger LLMs may improve performance but increase compute cost; gene-specific literature provides detail but introduces noise; chain-of-thought reasoning reduces confidence.

- Failure signatures: AUROC near 0.5 (random chance), over-confident predictions on non-causal pairs, degradation when adding gene descriptions.

- First 3 experiments:
  1. Baseline: Run naive prompts (gene names only) over all gene pairs; compute AUROC.
  2. Context ablation: Add cancer type + mRNA context; measure AUROC improvement.
  3. Noise test: Add gene descriptions or literature evidence; confirm performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can context-aware retrieval strategies that target experimentally specific literature (e.g., specific cell lines like K562 or modalities like mRNA) reverse the performance degradation caused by generic gene descriptions and literature associations?
- Basis in paper: [explicit] The authors explicitly state in the Discussion: "Future work could search explicitly for experimentally-relevant literature—in this case, relating to CML cells and mRNA measurements—rather than any biological or experimental context with associated gene measurement."
- Why unresolved: The current study found that adding gene-specific descriptions and general literature evidence via PubTator uniformly reduced performance (AUROC), likely because the retrieved context was too generic and conflicted with the specific experimental ground truth.
- What evidence would resolve it: A comparison of LLM performance when augmented with generic literature versus literature filtered specifically for the experimental context (e.g., CML-specific papers), showing an improvement over the naive baseline.

### Open Question 2
- Question: To what extent do LLM-derived causal priors improve the accuracy and efficiency of downstream causal structure learning (CSL) algorithms when combined with empirical perturbation data?
- Basis in paper: [inferred] The paper establishes that LLMs can generate zero-shot predictions (AUROC ~0.625) and claims they "can be straightforwardly integrated as a prior into any downstream causal structure learning task," but it does not experimentally demonstrate this integration or measure the resulting gain in CSL performance.
- Why unresolved: The evaluation framework treats the LLM as a standalone classifier against ground truth, rather than as a component within a larger causal discovery pipeline; thus, the utility of these "weak priors" in narrowing the search space for data-driven algorithms remains theoretical.
- What evidence would resolve it: A study measuring the Structural Hamming Distance (SHD) or intervention efficacy of a CSL algorithm (e.g., NOTEARS or PC algorithm) run on Perturb-seq data with versus without LLM-derived edge constraints.

### Open Question 3
- Question: Can reinforcement learning-imbued reasoning models (e.g., OpenAI o1) or larger models overcome the limitations of small models like Gemma2-9B in distinguishing causal direction without chain-of-thought degradation?
- Basis in paper: [explicit] The authors note: "larger ‘vanilla’ LLMs or RL-imbued reasoning-style LLMs (such as o1) may perform better" and "As with all recent LLM advances, this work represents a lower-bound on what may be achievable."
- Why unresolved: The experiments were restricted to Gemma2-9B due to computational constraints, and the authors found that chain-of-thought prompting actually decreased performance in this specific model, leaving the capabilities of state-of-the-art reasoning models untested.
- What evidence would resolve it: Benchmarking the same gene-gene causal inference task using o1 or similar reasoning-heavy models to see if they achieve higher AUROCs or handle complex contexts (like contradictory literature) better than the 0.625 baseline.

## Limitations

- Ground truth quality: The causal graph derived from single-gene perturbations assumes linear relationships and may miss indirect or combinatorial effects, limiting evaluation fidelity.
- LLM generalization: Results are based on Gemma2-9B and one cancer cell line (K562); performance may degrade on other contexts or model sizes.
- Prompt sensitivity: The study explores several prompt variants but does not conduct exhaustive ablation studies, leaving open the possibility of better-performing prompt structures.

## Confidence

- High confidence: LLM zero-shot causal inference can outperform baseline knowledge-driven methods (AUROC 0.625 vs 0.460).
- Medium confidence: Experimental context (cancer type + mRNA) improves performance; gene-specific literature generally degrades performance.
- Low confidence: The specific mechanism by which gene descriptions/literature evidence degrade performance is fully understood.

## Next Checks

1. **Reproduce ground truth graph construction:** Obtain Replogle et al. (2022) Perturb-seq data, filter to top 100 cancer genes, and replicate the Mann-Whitney U test with FDR correction to generate adjacency matrix G. Verify graph density is non-zero.

2. **Ablation study on prompt engineering:** Systematically test all prompt variants (naive, cancer context, mRNA context, gene descriptions, literature evidence) on a subset of gene pairs to confirm the reported performance ordering.

3. **Cross-dataset generalization:** Apply the best-performing LLM inference pipeline to a second independent perturbation dataset (e.g., from a different cell line or cancer type) to assess robustness of causal inference performance.