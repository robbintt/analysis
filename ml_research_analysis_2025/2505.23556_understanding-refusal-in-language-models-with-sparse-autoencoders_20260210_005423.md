---
ver: rpa2
title: Understanding Refusal in Language Models with Sparse Autoencoders
arxiv_id: '2505.23556'
source_url: https://arxiv.org/abs/2505.23556
tags:
- refusal
- features
- harmful
- feature
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work mechanistically analyzes refusal in language models using
  sparse autoencoders to identify latent features that causally mediate refusal behaviors.
  The authors identify and validate refusal-related features in two open-source chat
  models, finding that LLMs encode harm and refusal as separate feature sets, with
  harmful features causally driving refusal.
---

# Understanding Refusal in Language Models with Sparse Autoencoders

## Quick Facts
- **arXiv ID**: 2505.23556
- **Source URL**: https://arxiv.org/abs/2505.23556
- **Reference count**: 40
- **Primary result**: Mechanistically identifies and validates SAE features that causally mediate refusal behaviors in LLMs, showing harm and refusal are encoded as separable feature sets with harmful features as upstream triggers.

## Executive Summary
This work mechanistically analyzes refusal behavior in language models using sparse autoencoders to identify latent features that causally mediate refusal. The authors develop a method combining activation steering with attribution patching to pinpoint minimal feature sets responsible for refusal, validated through interventions that induce jailbreak behavior. They find that LLMs encode harm and refusal as distinct feature sets, with harmful features causally driving refusal, and that adversarial jailbreaks operate by suppressing refusal-related features. The approach enables classification of out-of-distribution adversarial examples and reveals that shared features across harmful categories are most directly linked to refusal mechanisms.

## Method Summary
The method identifies refusal-related SAE features by first computing a refusal direction vector via difference-in-means between harmful and harmless activations, then selecting K0=10 features per layer with highest cosine similarity to this direction. Attribution patching is applied within this constrained feature subspace to compute indirect effects, and top K*=20 features are selected by average indirect effect. Feature intervention involves clamping these identified features by scaling their activations, with magnitude tuned per model (c=-3 for Gemma, c=-1 for Llama). The approach is validated through jailbreak/refusal score measurement, coherence evaluation via cross-entropy loss, and classification performance on reasoning tasks.

## Key Results
- LLMs encode harm and refusal as distinct feature sets, with harmful features causally suppressing downstream refusal features
- Minimal feature sets identified via CosSim+AP achieve jailbreak scores comparable to activation steering while maintaining better coherence
- Adversarial suffixes suppress refusal features through mechanisms similar to harmful feature intervention
- Disentangled features improve classification of out-of-distribution adversarial examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode harm and refusal as distinct feature sets with a conditional causal relationship (harm features → refusal features).
- Mechanism: Harmful concept features (FH) activate on tokens representing harmful content (e.g., "bomb") and causally suppress downstream refusal features (FR); intervening on FH reduces FR activation and refusal probability.
- Core assumption: SAE features are sufficiently disentangled to represent separable "harm" and "refusal" concepts; base-model SAEs transfer to chat models.
- Evidence anchors:
  - [abstract] "find that refusal and harm are encoded as distinct feature sets, with harmful features acting as upstream triggers for refusal features"
  - [Section 4.2] Table 1 shows clamping FH suppresses FR with rate 0.48 (Gemma)/0.51 (Llama) vs 0.05/0.19 for random features; refusal probability drops 0.71/0.37.
  - [corpus] Related work "LLMs Encode Harmfulness and Refusal Separately" (arXiv:2507.11878) independently corroborates separability.
- Break condition: If SAEs fail to disentangle polysemantic activations, or if FR/FH overlap significantly, the causal separation claim weakens.

### Mechanism 2
- Claim: A minimal set of SAE features (F*) identified via CosSim+AP is causally sufficient to mediate refusal with lower coherence degradation than alternative methods.
- Mechanism: Restrict attribution patching to features aligned with a refusal direction (V*_R), then select top K* features by indirect effect; clamping these features (c < 0) induces jailbreak, (c > 0) induces refusal.
- Core assumption: The refusal direction V*_R is optimal; AP's linear approximation captures multi-token refusal dynamics when constrained to a relevant subspace.
- Evidence anchors:
  - [Section 3.2] Equations 5–6 define F0 selection via cosine similarity and F* via AP over F0.
  - [Section 4.1] Figure 2 shows CosSim+AP achieves jailbreak scores comparable to AS baseline while Table 5 shows lowest CE loss increase on Alpaca (0.310 vs base 0.291 for Gemma).
  - [corpus] "GSAE: Graph-Regularized Sparse Autoencoders" (arXiv:2512.06655) extends SAE-based steering with graph regularization, suggesting feature-level intervention is an active research direction.
- Break condition: If K* is too small, causal features may be omitted; if V*_R is suboptimal, F0 filtering may exclude relevant features.

### Mechanism 3
- Claim: Adversarial jailbreaks (suffixes, paraphrasing) operate by suppressing FR features, mimicking the effect of clamping FH.
- Mechanism: Certain suffix tokens (e.g., "fictional") cause a surge in FR suppression rate; adding FH clamping provides diminishing additional suppression, suggesting suffixes and FH share downstream effects on FR.
- Core assumption: Adversarial tokens' causal effect on FR is mechanistically similar to FH intervention; token-level analysis generalizes across jailbreak types.
- Evidence anchors:
  - [Section 4.3] Figure 4(b) shows appending "fictional" causes large ∆A(R); Figure 4(a) shows reduced additional suppression when clamping FH alongside suffix tokens.
  - [Section 4.3] Table 3 shows suffix suppression rate 0.72 (Gemma) comparable to FH clamping 0.50.
  - [corpus] "Latent Adversarial Training Improves the Representation of Refusal" (arXiv:2504.18872) connects latent-space perturbations to refusal robustness, aligning with the suppression mechanism.
- Break condition: If jailbreaks operate via additional mechanisms (e.g., attention manipulation) not captured by FR suppression, the claim is incomplete.

## Foundational Learning

- **Concept**: Sparse Autoencoders (SAEs)
  - Why needed here: Core tool for decomposing dense activations into interpretable, sparse features; understanding reconstruction loss, sparsity constraints, and encoder/decoder roles is essential.
  - Quick check question: Given an activation z, can you explain how SAE reconstructs it via Eq. 2 and what fE(z) represents?

- **Concept**: Causal Mediation via Attribution Patching
  - Why needed here: The paper uses attribution patching to measure indirect effects of features on refusal; understanding clean/corrupt pairs, patching, and linear approximation is required.
  - Quick check question: Why does AP approximate indirect effects with gradients, and what are its limitations for multi-token behaviors?

- **Concept**: Activation Steering
  - Why needed here: Provides the refusal direction (V*_R) and a baseline for feature-level interventions; understanding difference-in-means and projection (Eq. 3–4) is necessary.
  - Quick check question: How does steering via V*_R differ from feature clamping in terms of intervention scope and interpretability?

## Architecture Onboarding

- **Component map**: (1) Load SAE and model → (2) Compute V*_R via difference-in-means → (3) Select F0 via cosine similarity → (4) Run AP to get F* → (5) Clamp features and evaluate jailbreak/refusal scores

- **Critical path**: Load SAE and model → Compute V*_R via difference-in-means → Select F0 via cosine similarity → Run AP to get F* → Clamp features and evaluate jailbreak/refusal scores

- **Design tradeoffs**:
  - K0 (initial feature count): Smaller values reduce noise but may exclude indirect features; paper uses K0=10 per layer
  - K* (final feature count): Paper uses K*=20; smaller sets are more interpretable but may omit causal features
  - Scaling factor c: Larger |c| increases intervention strength but risks coherence degradation; paper uses c=-3 (Gemma) and c=-1 (Llama)

- **Failure signatures**:
  - High CE loss or incoherent outputs: Likely c too large or F* includes irrelevant features (e.g., ActDiff baseline in Table 5)
  - Low jailbreak score despite clamping: V*_R may be suboptimal or F0 filtering excluded relevant features
  - Inconsistent results across models: Llama is more sensitive; Gemma's TopK activation vs Llama's JumpReLU affects feature responsiveness

- **First 3 experiments**:
  1. Replicate F* identification on a subset of HarmBench (50 samples) using CosSim+AP; verify jailbreak score is comparable to AS baseline
  2. Ablate K* (10, 20, 50) and measure tradeoff between jailbreak score and CE loss on Alpaca to confirm minimality
  3. Test transfer: Identify F* on one harmful category (e.g., Illegal Activity), then evaluate jailbreak score on another (e.g., Fraud/Deception) to assess feature generality vs specificity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does training Sparse Autoencoders (SAEs) directly on chat-model activations yield more faithful or distinct refusal features than those transferred from base models?
- **Basis in paper**: [explicit] The authors state in the Limitations section that they "believe that this can be improved by prioritizing efforts towards training SAEs on chat-model activations, and consider this to be a promising direction for future research."
- **Why unresolved**: Current findings rely on SAEs trained on base models (GemmaScope/LlamaScope), which may not capture the full distribution of instruction-tuned refusal behaviors, despite showing transferability.
- **What evidence would resolve it**: A comparative analysis of feature sets derived from base-model SAEs versus chat-specific SAEs, measuring their respective causal effects on refusal mediation and reconstruction fidelity.

### Open Question 2
- **Question**: Do models encode refusal through multi-step conditional dependencies (e.g., Harm → Legality → Refusal) rather than direct causal links?
- **Basis in paper**: [explicit] The appendix notes that features corresponding to high-level concepts like "legal terminology" suggest "the possibility of additional layers of conditional dependency," and proposes exploring if "models encode refusal similarly to human reasoning."
- **Why unresolved**: The current study categorizes features primarily into "harm" (upstream) and "refusal" (downstream), potentially flattening complex intermediate conceptual circuits.
- **What evidence would resolve it**: Circuit mapping that traces activation paths through intermediate conceptual features (like "legality" or "consequences") to determine if they function as necessary mediators between harmful inputs and refusal outputs.

### Open Question 3
- **Question**: Does restricting the feature search space to the optimal refusal direction ($V^*_R$) inadvertently exclude relevant refusal mechanisms that lie outside this linear subspace?
- **Basis in paper**: [explicit] The authors acknowledge their strategy of restricting features to those aligned with the refusal direction "is potentially suboptimal, as its effectiveness is inherently dependent on the optimality of the refusal direction itself."
- **Why unresolved**: The methodology assumes that the most critical refusal features are those with high cosine similarity to the refusal direction, potentially missing orthogonal features that contribute to the behavior non-linearly.
- **What evidence would resolve it**: An ablation study testing for refusal behavior in feature sets explicitly selected for *low* alignment with the refusal direction but high attribution patching scores.

## Limitations

- The method relies on linear approximations through attribution patching, which may not capture complex multi-token interactions in refusal behavior
- Base-model SAE features are used rather than training SAEs directly on chat models, potentially missing instruction-tuned refusal mechanisms
- The approach assumes harmful features transfer meaningfully from base to chat models, though these models were never exposed to refusal supervision
- Sensitivity to hyperparameters (K0, K*, c) and SAE architecture choices may limit reproducibility across different model families

## Confidence

- **High confidence**: The core finding that harm and refusal are encoded as separable feature sets is well-supported by multiple independent lines of evidence (feature clamping experiments, comparison to related work)
- **Medium confidence**: The CosSim+AP method for identifying minimal causal feature sets is validated within the paper's experimental scope, but the linear approximation limitations and sensitivity to V*_R quality suggest the results may not generalize to all refusal scenarios
- **Low confidence**: The claim that shared features across harmful categories are most directly linked to refusal mechanisms is primarily based on feature ablation patterns, which could reflect correlation rather than direct causation

## Next Checks

1. **Transfer Robustness Test**: Apply the identified F* feature sets from one harmful category (e.g., Illegal Activity) to trigger jailbreaks in out-of-distribution categories (e.g., Fraud/Deception) and measure performance degradation. This would validate whether the causal features are truly category-general or specific to training data.

2. **SAE Architecture Ablation**: Systematically vary the SAE expansion factor (8→32) and activation function (JumpReLU→TopK) to test sensitivity of F* identification. Compare whether the same causal features emerge across different SAE instantiations, which would strengthen confidence in the feature interpretations.

3. **Multi-Token Interaction Validation**: Design experiments to test whether AP's linear approximation captures complex refusal dynamics by comparing against non-linear intervention methods (e.g., activation vector addition beyond single feature sets). This would clarify whether the identified features fully capture the causal mechanism or if additional interactions exist.