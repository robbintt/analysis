---
ver: rpa2
title: 'Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion
  LLMs'
arxiv_id: '2510.03680'
source_url: https://arxiv.org/abs/2510.03680
tags:
- padding
- length
- rainbow
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses early termination in instruction-tuned diffusion
  language models (dLLMs), where models prematurely generate <eos tokens, leading
  to truncated or degenerate outputs. This issue stems from the dual use of <eos as
  both a termination and padding token during training, which biases the model to
  predict <eos excessively.
---

# Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs

## Quick Facts
- arXiv ID: 2510.03680
- Source URL: https://arxiv.org/abs/2510.03680
- Reference count: 40
- One-line primary result: Rainbow Padding mitigates early termination in instruction-tuned diffusion LLMs by replacing repeated `<eos>` padding with a cyclic sequence of distinct tokens.

## Executive Summary
This paper addresses a critical failure mode in instruction-tuned diffusion language models where models prematurely generate `<eos>` tokens, resulting in truncated outputs. The root cause is identified as the dual use of `<eos>` for both termination and padding during training, which creates a probability mass imbalance. The proposed solution, Rainbow Padding, replaces repeated `<eos>` placeholders with a cyclic sequence of distinct padding tokens, effectively distributing probability mass away from `<eos>` and preventing its excessive prediction. The method demonstrates significant improvements in length robustness and task performance across multiple benchmarks.

## Method Summary
Rainbow Padding is a lightweight fine-tuning technique that modifies the padding strategy during instruction-tuning of diffusion language models. Instead of using repeated `<eos>` tokens for padding, the method employs a cyclic sequence of distinct padding tokens. This approach prevents the model from associating padding positions with termination signals, thereby reducing the tendency to prematurely generate `<eos>`. The technique requires minimal architectural changes and can be integrated into existing dLLM training pipelines with only a modest increase in vocabulary size. Experiments show that as few as seven distinct padding tokens are sufficient to achieve robust performance improvements.

## Key Results
- Rainbow Padding significantly reduces early termination rates across multiple instruction-tuning benchmarks
- The method improves length robustness, allowing models to generate complete responses without truncation
- Seven distinct padding tokens are sufficient to achieve optimal performance gains
- Lightweight fine-tuning makes Rainbow Padding practical for integration into existing dLLM pipelines

## Why This Works (Mechanism)
The mechanism works by addressing a fundamental misalignment in how dLLMs are trained. When `<eos>` is used for both padding and termination, the model learns to associate padding positions with high termination probability, creating a feedback loop where padding tokens reinforce early stopping behavior. By replacing repeated `<eos>` with a diverse set of cyclic padding tokens, Rainbow Padding breaks this association. The cyclic nature ensures uniform coverage across different padding positions while the diversity prevents the model from learning termination patterns specific to any single padding token. This redistribution of probability mass allows the model to reserve `<eos>` for genuine termination cases rather than treating it as a default padding response.

## Foundational Learning

**Diffusion Language Models**
- Why needed: Understanding the unique architecture that combines diffusion processes with language generation
- Quick check: Verify the model uses continuous denoising steps rather than discrete autoregressive transitions

**Dual Token Usage Problem**
- Why needed: Recognizing how using `<eos>` for both padding and termination creates conflicting training signals
- Quick check: Confirm that padding tokens occupy positions that should be neutral for termination probability

**Probability Mass Distribution**
- Why needed: Understanding how token frequency affects prediction likelihood during generation
- Quick check: Ensure padding token diversity prevents any single token from dominating probability space

**Instruction Tuning Dynamics**
- Why needed: Grasping how fine-tuning on instruction datasets differs from pretraining objectives
- Quick check: Verify that fine-tuning preserves generation quality while adapting to task-specific patterns

## Architecture Onboarding

**Component Map**
- Input Sequence -> Token Embedding -> Diffusion Denoiser -> Categorical Sampler -> Output Sequence

**Critical Path**
- The generation loop where token prediction occurs, specifically the transition from non-`<eos>` tokens to potential early termination

**Design Tradeoffs**
- Vocabulary expansion vs. performance gain: Additional padding tokens increase memory usage but provide robustness
- Fine-tuning depth vs. efficiency: Deeper integration yields better results but requires more computational resources
- Token diversity vs. simplicity: More distinct padding tokens provide better distribution but complicate token management

**Failure Signatures**
- Premature `<eos>` generation in otherwise valid contexts
- Truncated responses that end before logical completion points
- Consistent pattern of early termination across multiple prompts

**3 First Experiments**
1. Compare baseline dLLM vs. Rainbow Padding on controlled prompt sequences with known optimal lengths
2. Measure `<eos>` prediction probability at padding positions before and after Rainbow Padding integration
3. Evaluate task performance degradation when varying the number of distinct padding tokens (1, 3, 7, 10)

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness on non-diffusion language model architectures remains untested
- Lack of ablation studies isolating Rainbow Padding's contribution from other training factors
- No analysis of potential overhead in token vocabulary management or downstream model compression impacts

## Confidence

**High confidence**: The empirical observation that dual-use of `<eos>` for termination and padding causes premature generation is well-supported by controlled experiments comparing baseline vs rainbow padding performance.

**Medium confidence**: The claim that rainbow padding is broadly effective across task types and model scales, as results are strong but lack cross-domain validation beyond curated benchmarks.

**Medium confidence**: The assertion that seven padding tokens are sufficient for most use cases, as this threshold is based on specific experimental conditions without sensitivity analysis.

## Next Checks
1. Conduct ablation studies varying rainbow padding token count (1, 3, 7, 10, 15) across multiple task categories to establish optimal token ranges and diminishing returns.

2. Test rainbow padding's effectiveness on non-diffusion language models and standard autoregressive LLMs to assess architecture independence.

3. Evaluate rainbow padding's impact on model compression and quantization, measuring any performance degradation when models are optimized for deployment.