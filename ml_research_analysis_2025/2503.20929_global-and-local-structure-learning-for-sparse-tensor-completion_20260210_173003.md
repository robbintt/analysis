---
ver: rpa2
title: Global and Local Structure Learning for Sparse Tensor Completion
arxiv_id: '2503.20929'
source_url: https://arxiv.org/abs/2503.20929
tags:
- tensor
- matrices
- decomposition
- completion
- factor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TGL, a tensor decomposition method that learns
  global and local structures for sparse tensor completion without requiring prior
  knowledge. The core idea is to use Graph Neural Networks (GNNs) to capture local
  relationships between dimensions along each mode of a tensor, while leveraging standard
  CP decomposition to initialize representations.
---

# Global and Local Structure Learning for Sparse Tensor Completion

## Quick Facts
- arXiv ID: 2503.20929
- Source URL: https://arxiv.org/abs/2503.20929
- Authors: Dawon Ahn; Evangelos E. Papalexakis
- Reference count: 11
- Key outcome: TGL achieves competitive performance vs. state-of-the-art CoSTCo on sparse tensor completion by learning global and local structures without prior knowledge

## Executive Summary
This paper introduces TGL, a tensor decomposition method that learns global and local structures for sparse tensor completion without requiring prior knowledge. The core idea is to use Graph Neural Networks (GNNs) to capture local relationships between dimensions along each mode of a tensor, while leveraging standard CP decomposition to initialize representations. TGL generates K-nearest neighbor graphs from factor matrices, then updates these matrices through GNN layers for each mode, reconstructing the tensor to jointly optimize factor matrices and GNNs. Experiments on Yelp and BBC-News datasets show that TGL outperforms the standard CP decomposition baseline (CPD) and achieves competitive performance compared to the state-of-the-art method (CoSTCo), demonstrating its effectiveness in accurately predicting missing tensor entries in sparse scenarios.

## Method Summary
TGL combines CP decomposition with GNN-based local structure learning for tensor completion. The method first initializes factor matrices via standard CP decomposition, then constructs K-nearest neighbor graphs from these factor matrices using cosine similarity. GNN layers propagate information through these graphs to capture local relationships between dimensions. The model jointly optimizes factor matrices and GNN parameters through reconstruction loss, updating both simultaneously. The key innovation is the integration of GNNs to learn local structures while preserving the global low-rank structure from CP decomposition, all without requiring prior knowledge about the tensor's structure.

## Key Results
- TGL outperforms standard CP decomposition (CPD) baseline on both Yelp and BBC-News datasets
- TGL achieves competitive performance compared to state-of-the-art CoSTCo method
- Model demonstrates effectiveness in predicting missing entries in sparse tensor scenarios
- Requires downsampling of full Yelp dataset due to computational constraints

## Why This Works (Mechanism)

### Mechanism 1: GNN-Based Local Structure Learning via KNN Graphs
Constructing K-nearest neighbor graphs from factor matrices and propagating information through GNN layers enables the model to capture latent relationships between dimensions within each tensor mode. Factor matrices are used to compute pairwise cosine similarity, generating relation matrices. These graphs define neighborhoods where GNN convolution layers aggregate and transform node features, propagating structural signals across related dimensions. This works because dimensions with similar factor representations share meaningful relationships that inform missing entry prediction.

### Mechanism 2: CP Decomposition as Global Structure Initializer
Standard CP decomposition provides principled initialization and global rank-R structure that anchors local GNN updates, ensuring decomposability and interpretability. CP factorizes tensor into factor matrices such that entries are reconstructed via rank-1 components. TGL initializes with CP, then refines factor matrices via GNN updates while preserving the global low-rank reconstruction objective. The global low-rank structure captured by CP is complementary to local relational structures learned by GNNs; jointly optimizing both improves completion.

### Mechanism 3: Joint Optimization of Factor Matrices and GNN Parameters
Simultaneously optimizing factor matrices and GNN weights via reconstruction loss enables end-to-end learning where graph structure and factor representations co-adapt. Loss function is minimized over observed entries, where GNN-transformed factor matrices are used for reconstruction. This allows GNNs to learn meaningful local structures that directly improve reconstruction, rather than being a post-hoc smoothing step. Joint optimization ensures that local and global structures are learned in harmony.

## Foundational Learning

- Concept: **CP (CANDECOMP/PARAFAC) Decomposition**
  - Why needed here: TGL uses CP as the global structural backbone; understanding how CP factorizes tensors into rank-1 components is essential to grasp what factor matrices represent and how they're refined.
  - Quick check question: Given a 3-mode tensor of shape (I, J, K) and rank R, what are the shapes of factor matrices A, B, C?

- Concept: **Graph Neural Networks (GNNs) and Message Passing**
  - Why needed here: TGL's core innovation is applying GNN convolution to factor matrices; understanding how GNNs aggregate neighborhood information explains how local structures are learned.
  - Quick check question: In the GNN update H^(l+1) = σ(R̃ H^(l) W^(l)), what does R̃ represent and why is normalization applied?

- Concept: **K-Nearest Neighbors (KNN) Graph Construction**
  - Why needed here: TGL dynamically constructs relation graphs from factor matrices using cosine similarity and KNN; understanding this explains how "local" neighborhoods are defined without prior knowledge.
  - Quick check question: If two rows of factor matrix A have cosine similarity 0.95, what does this imply about their relationship in the KNN graph?

## Architecture Onboarding

- Component map: Input Tensor → CP Initialization → KNN Graph Construction → GNN Layers → Tensor Reconstruction → Loss & Backprop
- Critical path: CP initialization → KNN graph construction → GNN forward pass → tensor reconstruction → loss computation → joint gradient updates. Errors in KNN graph quality (e.g., poor K selection) propagate directly to GNN outputs.
- Design tradeoffs:
  - Rank R: Higher R captures more complexity but risks overfitting and increases memory; paper tests R ∈ {10, 20, 30, 40, 50}
  - GNN depth L: Deeper GNNs capture broader neighborhoods but risk over-smoothing; paper does not specify exact L
  - K for KNN: Larger K expands neighborhoods but may include irrelevant connections; paper does not specify K
  - Redundancy issue: Authors note that relation matrices derived from factor matrices may provide redundant information to GNNs, potentially limiting gains vs. CoSTCo
- Failure signatures:
  - Stagnant or increasing NRE: May indicate K too small (sparse graphs) or too large (noisy edges), or rank R mismatch
  - GNN over-smoothing: Factor representations converge to near-identical values across dimensions; reduce L
  - Memory exhaustion on large tensors: Paper explicitly subsampled Yelp from (70,817, 15,579, 108) to (5,000, 5,000, 108); scaling to full data requires optimization
- First 3 experiments:
  1. Baseline comparison on Yelp and BBC-News: Replicate test NRE curves for CPD, TGL, and CoSTCo across ranks 10-50; verify TGL consistently outperforms CPD but trails CoSTCo
  2. Ablation on KNN graph K: Vary K ∈ {5, 10, 20, 50} while holding rank R=30 and GNN depth fixed; measure test NRE to identify optimal neighborhood size
  3. GNN depth sensitivity: Test L ∈ {1, 2, 3, 4} on BBC-News; monitor for over-smoothing (embedding variance collapse) and NRE degradation

## Open Questions the Paper Calls Out

- Question: How can distinct graph structures be generated from node features to prevent redundant information from being passed into the GNNs?
  - Basis in paper: The authors state in the Discussion that relation matrices are created from factor matrices, causing redundancy, and explicitly aim to "make distinct graph structures from node features for GNNs for the future works."
  - Why unresolved: Currently, TGL uses the same factor matrices for both the graph topology (via KNN) and the node features, meaning the GNN receives overlapping information which may limit its ability to capture novel local structures.
  - Evidence to resolve: A modified TGL architecture where the adjacency matrix is derived independently of the node feature matrix, demonstrating improved Test NRE scores over the current baseline.

- Question: How can the TGL method be scaled to handle large-scale industrial tensors without requiring aggressive downsampling?
  - Basis in paper: The Experiments section notes that the "proposed model can not handle too large dataset," forcing the authors to sample only 5,000 users/businesses from the original Yelp dataset.
  - Why unresolved: The computational cost of constructing KNN graphs and training GNNs on dense factor matrices appears to limit the method's applicability to smaller subsets of real-world data.
  - Evidence to resolve: A scalability analysis showing successful training on the full Yelp dataset (70,817 movies, 15,579 users) without memory overflow or excessive training time.

- Question: Is the static K-nearest neighbors (KNN) graph construction sufficient, or can performance be improved by making the graph structure dynamic or learnable?
  - Basis in paper: The method relies on a fixed heuristic (pairwise cosine similarity) to generate relation matrices. While effective, this manual construction might fail to capture complex, non-linear dependencies that a learned graph structure could capture.
  - Why unresolved: The paper does not explore alternative graph generation techniques or ablation studies on the graph construction method, leaving the optimality of the KNN approach unverified.
  - Evidence to resolve: A comparative study where the static KNN graph is replaced by a learnable adjacency matrix (e.g., via attention mechanisms) resulting in lower reconstruction errors.

## Limitations
- Computational scalability: Model cannot handle full-size Yelp dataset (70,817×15,579×108), requiring aggressive downsampling to 5,000×5,000×108
- Hyperparameter sensitivity: Key parameters like K for KNN graphs, GNN depth L, and learning rate remain unspecified
- Performance gap: While outperforming CP baseline, TGL still trails state-of-the-art CoSTCo method
- Redundancy concern: Factor matrices and derived KNN graphs may provide overlapping information to GNNs

## Confidence
- CP decomposition baseline: High confidence - well-established method with clear implementation
- GNN architecture parameters: Low confidence - exact values for K, L, learning rate, hidden dimensions not specified
- Joint optimization mechanism: Medium confidence - well-described conceptually but lacks ablation studies
- Scalability claims: Medium confidence - limited by computational constraints on full datasets

## Next Checks
1. Reproduce the baseline CPD performance on both datasets with 8:1:1 splits and compare against reported NRE curves across ranks 10-50
2. Implement sensitivity analysis for K in KNN graphs (test K ∈ {5, 10, 20, 50}) while holding other parameters constant to identify optimal neighborhood size
3. Test GNN depth sensitivity (L ∈ {1, 2, 3, 4}) and monitor for over-smoothing through embedding variance metrics alongside NRE performance