---
ver: rpa2
title: Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations
arxiv_id: '2509.20478'
source_url: https://arxiv.org/abs/2509.20478
tags:
- learning
- distances
- distance
- temporal
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Temporal Metric Distillation (TMD), a new
  offline goal-conditioned reinforcement learning method that learns optimal temporal
  distances for goal-reaching tasks. TMD combines the strengths of Monte Carlo contrastive
  learning and quasimetric network architectures by enforcing specific invariance
  properties on learned distance functions.
---

# Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations

## Quick Facts
- arXiv ID: 2509.20478
- Source URL: https://arxiv.org/abs/2509.20478
- Authors: Vivek Myers; Bill Chunyuan Zheng; Benjamin Eysenbach; Sergey Levine
- Reference count: 40
- Primary result: Introduces TMD, an offline GCRL method that learns optimal temporal distances using quasimetric representations, achieving up to 3x improvement over baselines in stochastic environments

## Executive Summary
This paper introduces Temporal Metric Distillation (TMD), an offline goal-conditioned reinforcement learning method that learns optimal temporal distances for goal-reaching tasks. TMD combines Monte Carlo contrastive learning with quasimetric network architectures, enforcing specific invariance properties on learned distance functions. The key innovation is imposing both the triangle inequality and additional invariance constraints that ensure convergence to optimal temporal distances, even from suboptimal offline data and in stochastic environments. TMD can learn optimal policies that stitch together behaviors from suboptimal demonstrations while handling stochastic dynamics, outperforming existing methods by significant margins on standard benchmarks.

## Method Summary
TMD learns a quasimetric distance function $d(s, g)$ that represents the minimum expected time to reach goal state $g$ from state $s$. The method uses a Metric Residual Network (MRN) architecture that enforces the triangle inequality by construction. Learning proceeds in two phases: first, contrastive initialization using Monte Carlo objectives provides stable long-horizon estimates, then invariance distillation refines these estimates using Action Invariance (forcing $d(s, (s,a)) = 0$) and Temporal Invariance constraints. The final policy is extracted using the quasimetric, enabling optimal behavior even when stitching together suboptimal trajectory segments from the offline dataset.

## Key Results
- Achieves up to 3x improvement over baselines in stochastic teleport environments
- Outperforms existing methods on long-horizon tasks requiring trajectory stitching
- Demonstrates superior performance across challenging navigation and manipulation tasks
- Handles stochastic dynamics better than prior quasimetric methods like QRL
- Shows consistent gains across various standard offline GCRL benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Quasimetric Architectural Constraint (Stitching)
- **Claim:** Enforcing the triangle inequality ($d(s, g) \leq d(s, w) + d(w, g)$) directly in the network architecture allows the model to infer shorter paths between states than those demonstrated in the offline dataset, a capability known as "stitching."
- **Mechanism:** The network uses a Metric Residual Network (MRN) which mathematically guarantees the output distance satisfies the triangle inequality. If the dataset contains segments $s \to w$ and $w \to g$, the architecture ensures the direct distance $d(s, g)$ is implicitly bounded by the sum of the segments, effectively "short-circuiting" suboptimal detours in the data without requiring explicit TD-backups.
- **Core assumption:** The offline dataset contains overlapping trajectory segments that can be recombined to form shorter, feasible paths.
- **Evidence anchors:** [Abstract] "...free stitching capabilities of quasimetric network parameterizations." [Section 3.2] "The triangle inequality... is powerful because it lets us architecturally winnow down the hypothesis space..." [Corpus] Related work "Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning" validates that quasimetric geometries facilitate planning in offline settings.
- **Break condition:** If the environment dynamics are such that reaching $w$ from $s$ and $g$ from $w$ does not imply reaching $g$ from $s$ is feasible or shorter (e.g., irreversible one-way doors), the triangle inequality assumption may not align with physical reality, potentially degrading performance.

### Mechanism 2: Contrastive Initialization (Long-Horizon Stability)
- **Claim:** Initializing the distance function using a Monte Carlo contrastive loss ($L_{NCE}$) provides stable estimates for long-horizon tasks by avoiding the compounding errors typical of temporal difference (TD) learning.
- **Mechanism:** The model is first trained to predict the log-probability of reaching a future state $s^+$ from current state $s$ under the data distribution. This provides a dense learning signal across entire trajectories rather than sparse, propagating local errors. This establishes a baseline "behavioral" distance ($d^\beta$) which is stable but potentially suboptimal.
- **Core assumption:** The behavior policy $\pi_\beta$ that generated the dataset provides a reasonable (though not necessarily optimal) baseline for reaching goals.
- **Evidence anchors:** [Abstract] "...we retain the stability and long-horizon capabilities of Monte Carlo contrastive RL methods..." [Section 4.1] "...optimal solution to this objective is... the only valid quasimetric satisfying Eq. (20) is $d_\theta = d^\beta_{SD}$." [Corpus] Corpus signals for "Dual Goal Representations" suggest temporal distances are robust representations for goal reaching, supporting the use of distance-based initialization.
- **Break condition:** Performance may degrade if the offline dataset has extremely poor coverage, causing the initial contrastive estimate to be arbitrarily wrong or uninformative for large portions of the state space.

### Mechanism 3: Invariance Distillation (Optimality Refinement)
- **Claim:** Enforcing "Action Invariance" ($I$) and "Temporal Invariance" ($T$) constraints distills the suboptimal behavioral distance into an optimal distance ($d^*$), even in stochastic environments.
- **Mechanism:** The model refines the initial contrastive estimate by minimizing specific losses. Action Invariance forces the distance $d(s, (s,a))$ to zero. Temporal Invariance forces the distance to be consistent with expected dynamics (akin to a Bellman backup but using a Bregman divergence). These constraints force the distance function to tighten towards the shortest possible path allowed by the dynamics, effectively learning $Q^*$ from $Q^\beta$.
- **Core assumption:** The constraints $I$ and $T$ are sufficient to mathematically specify the unique optimal distance function, and the optimization landscape allows convergence to this fixed point.
- **Evidence anchors:** [Section 3.2] "...additional constraint is a form of consistency over the environment dynamics..." [Section 4.3] "Invariance to the T backup operator... corresponds to the following update... enforced by minimizing a divergence." [Corpus] Explicit mechanism not found in corpus neighbors; this appears to be the specific novel contribution of this paper over prior quasimetric methods.
- **Break condition:** If the weight $\zeta$ balancing the contrastive loss and invariance losses is poorly tuned, the model might either fail to converge (unstable) or collapse to the suboptimal behavioral distance (failing to stitch).

## Foundational Learning

- **Concept: Quasimetric vs. Metric**
  - **Why needed here:** Standard metrics are symmetric ($d(A,B) = d(B,A)$). In control, distance often depends on direction (e.g., going uphill vs. downhill). Understanding that TMD enforces the triangle inequality *without* symmetry is crucial for interpreting the distance plots.
  - **Quick check question:** Does the architecture enforce $d(s, g) = d(g, s)$? (Answer: No, it enforces the triangle inequality but allows asymmetry).

- **Concept: Offline RL & Distributional Shift**
  - **Why needed here:** The method operates on fixed datasets. You must understand that the "stitching" capability is the solution to the specific offline problem of extracting better-than-demonstrated behaviors without interacting with the environment.
  - **Quick check question:** Why can't we just use standard Q-learning on offline data? (Answer: Extrapolation error/overestimation of Q-values for out-of-distribution actions).

- **Concept: Bregman Divergence**
  - **Why needed here:** The paper uses a specific Bregman divergence ($D_T$) for the temporal invariance loss rather than standard MSE. This is likely critical for handling the log-probability scale of the distances.
  - **Quick check question:** Why is $exp(d - d') - d$ used instead of $(d - d')^2$? (Answer: To ensure non-vanishing gradients and correct probability scaling in log-space, as per Appendix E).

## Architecture Onboarding

- **Component map:**
  - Encoder $\psi(s)$ -> MRN Head -> Distance $d(s, g)$
  - Encoder $\phi(s,a)$ -> MRN Head -> Distance $d(s, (s,a))$
  - MRN takes two embeddings and outputs scalar distance

- **Critical path:**
  1. Implementing the MRN layer correctly to ensure the triangle inequality holds by construction (likely involves specific max/min operations over latent dimensions)
  2. Implementing the Bregman divergence loss $D_T$ exactly as specified in Eq. (25/26) to stabilize the temporal backup

- **Design tradeoffs:**
  - **Hyperparameter $\zeta$:** Balances the "memory" of the dataset (Contrastive) vs. the "optimization" of the path (Invariance). High $\zeta$ favors stitching/optimality; low $\zeta$ favors stability/behavioral cloning
  - **Stop-gradient on $\psi$:** In $L_T$, gradients through the target representation $\psi(g)$ are stopped to prevent a moving target problem (Section 4.3)

- **Failure signatures:**
  - **Distance Collapse:** If $L_I$ fails, $d(s, (s,a)) \neq 0$, breaking the link between Q-value and distance
  - **Instability in Noise:** If the Bregman divergence is swapped for MSE, performance drops significantly in stochastic environments (validated by ablation in Table 5)
  - **No Stitching:** If the MRN architecture is replaced with a standard MLP, the model will likely just imitation-learn the dataset without finding shortcuts

- **First 3 experiments:**
  1. **Metric Sanity Check:** Verify $d(s, s) = 0$ and $d(s, g) \leq d(s, w) + d(w, g)$ for random triplets to confirm the MRN implementation is valid
  2. **Ablation on Loss Components:** Run `pointmaze_teleport_stitch` with $L_T$ removed vs. $L_{NCE}$ removed (as per Fig 4) to isolate the contribution of stitching vs. stability
  3. **Stochastic Validation:** Compare performance on `antmaze_teleport` (stochastic) vs. standard AntMaze to verify the specific claim that TMD handles stochasticity better than prior quasimetric methods (QRL)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the invariance weight $\zeta$ be set via a principled, automated method rather than manual tuning based on dataset properties?
- Basis in paper: [explicit] The authors state, "Future work could examine more principled ways to set the $\zeta$ parameter in our method," noting it is currently chosen based on expectations of stitching and stochasticity.
- Why unresolved: The current reliance on manual tuning limits the method's autonomy and adaptability to new, uncharacterized datasets.
- What evidence would resolve it: An adaptive scheduling mechanism for $\zeta$ that converges to optimal values without prior environment knowledge, maintaining performance across diverse benchmarks.

### Open Question 2
- Question: Is it possible to more directly integrate the contrastive and invariance components of the loss function?
- Basis in paper: [explicit] The authors suggest future work could explore "if there are ways to more directly integrate the contrastive and invariance components of the loss function."
- Why unresolved: The current objective separates these components (via a weighted sum), potentially creating conflicting gradients or optimization trade-offs.
- What evidence would resolve it: A unified loss formulation that enforces invariance properties inherently within the contrastive objective, demonstrating faster convergence or higher asymptotic performance.

### Open Question 3
- Question: Can the policy extraction objective be integrated into the distance learning phase to explicitly enable stitching at the policy level?
- Basis in paper: [explicit] The authors propose future work "explore integrating the policy extraction objective more directly into the distance learning to enable desirable properties... at the level of the policy."
- Why unresolved: Currently, TMD learns the distance first and extracts the policy later; decoupling these might prevent the policy from fully utilizing the distance metric's geometric properties.
- What evidence would resolve it: An end-to-end differentiable framework where policy updates influence the metric space, resulting in superior stitching behavior compared to the two-stage approach.

### Open Question 4
- Question: Would alternative quasimetric architectures, such as Interval Quasimetric Embedding (IQE), provide better expressiveness or performance than the Metric Residual Network (MRN)?
- Basis in paper: [explicit] The authors note that "alternative architectures such as Interval Quasimetric Embedding (IQE) that enforce the triangle inequality could be more expressive."
- Why unresolved: The paper exclusively utilizes MRN, leaving the potential benefits of other quasimetric architectures for TMD unexplored.
- What evidence would resolve it: A comparative ablation study substituting MRN with IQE in the TMD framework, showing performance deltas on high-dimensional manipulation tasks.

## Limitations

- Theoretical guarantees for convergence to the optimal quasimetric in stochastic, offline settings are not rigorously proven; claims rely on empirical validation
- Critical hyperparameter ζ requires careful tuning, and sensitivity across environments is not fully explored
- Performance on extremely sparse reward tasks or datasets with very poor coverage remains unclear

## Confidence

- **High confidence** in the quasimetric architecture's ability to enable stitching behaviors, as this follows directly from the triangle inequality mathematical property
- **Medium confidence** in the contrastive initialization providing stable long-horizon learning, as this is well-established in related work but not extensively validated for this specific architecture
- **Medium confidence** in the overall performance claims, as the method shows significant gains but the ablation studies are limited and don't isolate all components

## Next Checks

1. **Ablation on ζ**: Systematically vary the hyperparameter ζ across multiple orders of magnitude to quantify the sensitivity of performance to this critical balance parameter
2. **Dataset Coverage Analysis**: Evaluate TMD on datasets with varying levels of coverage and state visitation frequency to test the limits of the contrastive initialization
3. **Stochasticity Robustness**: Compare TMD against other quasimetric methods (QRL) on environments with varying levels of stochasticity to isolate the contribution of the Bregman divergence to performance gains