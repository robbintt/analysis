---
ver: rpa2
title: 'QBD-RankedDataGen: Generating Custom Ranked Datasets for Improving Query-By-Document
  Search Using LLM-Reranking with Reduced Human Effort'
arxiv_id: '2505.04732'
source_url: https://arxiv.org/abs/2505.04732
tags:
- retrieval
- documents
- methods
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of generating custom datasets for
  Query-By-Document (QBD) search, where queries are documents and retrieved candidates
  match these queries in a domain-specific manner. The proposed method, QBD-RankedDatagen,
  leverages Large Language Models (LLMs) for re-ranking with reduced human effort.
---

# QBD-RankedDataGen: Generating Custom Ranked Datasets for Improving Query-By-Document Search Using LLM-Reranking with Reduced Human Effort

## Quick Facts
- **arXiv ID:** 2505.04732
- **Source URL:** https://arxiv.org/abs/2505.04732
- **Reference count:** 8
- **Primary result:** LLM re-ranking reduces but does not eliminate human effort needed for generating custom ranked datasets in Query-By-Document search.

## Executive Summary
This paper addresses the challenge of generating custom ranked datasets for Query-By-Document (QBD) search, where both queries and documents are full-length texts. The proposed QBD-RankedDatagen framework leverages Large Language Models (LLMs) for re-ranking retrieved candidates, incorporating domain expert knowledge and explanations to reduce human annotation effort. The authors compare single-document scoring (SCS) and pairwise-document scoring (PCS) methods for re-ranking, evaluating on TREC clinical trials and CORD-19 datasets. The key finding is that while LLM re-ranking can reduce human effort, human validation remains essential for effective model selection and fine-tuning, as BM25 tuned on LLM-generated rankings underperforms default parameters.

## Method Summary
The QBD-RankedDatagen framework implements a pipeline that starts with BM25 or embedding-based retrieval, followed by LLM re-ranking using either single-document scoring (SCS) or pairwise-document scoring (PCS) methods. SCS methods score documents individually in the range [-1, 1], while PCS methods compare all document pairs and aggregate scores by summation. Domain expert instructions can be optionally incorporated into prompts. Human experts review LLM rankings and explanations, correcting errors before using the validated rankings to tune BM25 parameters via Optuna optimization. The approach is evaluated on TREC Clinical Trials 2021 and CORD-19 datasets, measuring rank correlation metrics (Kendall's τb, Spearman) and downstream retrieval performance (MAP, MRR, Precision@K).

## Key Results
- LLM re-ranking alone does not suffice for model selection or fine-tuning; human validation is necessary
- BM25 tuned using LLM-generated rankings did not outperform default parameters (k1=1.5, b=0.75)
- Pairwise-document scoring methods provided more stable rankings and were easier for human review than single-document scoring
- Adding domain instructions to prompts did not improve performance and sometimes decreased it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based re-ranking can reduce—but not eliminate—human effort in generating domain-specific ranked datasets for QBD search.
- Mechanism: The pipeline uses an initial retrieval step (BM25 or embeddings) followed by LLM re-ranking with prompts that incorporate domain expert instructions. The LLM outputs relevance scores and explanations, which humans review to validate or correct decisions, iteratively improving prompt instructions.
- Core assumption: Domain-specific matching criteria can be partially captured via natural language instructions, and LLMs can reason about document-document relevance beyond keyword overlap.
- Evidence anchors:
  - [abstract]: "The methods we compare leverage Large Language Models (LLMs) which can incorporate domain expert input to produce document scores and rankings, as well as explanations for human review."
  - [section]: "Our work focuses on the re-ranking stage since this is where human-knowledge and feedback can be more easily incorporated."
  - [corpus]: Related work LANCER uses LLM reranking for long-form retrieval; corpus shows this is an active research direction with FMR ~0.54.

### Mechanism 2
- Claim: Pairwise document comparison methods provide more stable rankings and easier human review than pointwise scoring or listwise methods.
- Mechanism: Instead of scoring documents individually, the LLM compares pairs and outputs +1/0/-1 indicating relative relevance. All pairs are compared, scores are aggregated, and documents are ranked by total score. This reduces positional bias and aligns with human comparative judgment capabilities.
- Core assumption: LLMs exhibit less ordering bias when comparing two documents than when processing longer lists; humans also find pairwise comparisons cognitively easier than ranking many items.
- Evidence anchors:
  - [section]: "We do not adopt reranking by putting all candidates into the LLM... we noticed that changing the order of candidate documents in pairwise candidate scoring methods sometimes changed which document the LLM thought was a better match."
  - [section]: "It is easier for the human to review single candidate decisions and explanations, or pairwise comparisons; pairwise comparisons are even better as shown in human factors research."
  - [corpus]: ProRank paper addresses LLM reranking optimization, corroborating pairwise effectiveness.

### Mechanism 3
- Claim: Human validation of LLM-generated rankings remains necessary for effective downstream model tuning.
- Mechanism: LLM re-ranking produces candidate rankings, but these may contain errors. Human experts review and correct rankings before using them as training signal for BM25 parameter optimization. The corrected "ground truth" enables effective model selection/tuning.
- Core assumption: LLMs lack sufficient domain-specific knowledge to fully replace human judgment in specialized QBD tasks; human corrections capture nuance that prompts cannot.
- Evidence anchors:
  - [abstract]: "The results show that LLM re-ranking alone may not suffice for model selection or fine-tuning, and human validation might be necessary."
  - [section]: "In the results we see that only when BM25 is tuned with the ground truth (either training or test set), does the performance exceed the default parameter settings... This would imply that getting human validation and correction in the process... maybe essential."
  - [corpus]: "When LLMs Disagree" paper explicitly documents LLM disagreement on relevance labeling, supporting need for human oversight.

## Foundational Learning

- Concept: **BM25 Retrieval Model**
  - Why needed here: BM25 is the base retrieval model being optimized; understanding k1 (term frequency saturation) and b (document length normalization) parameters is essential for interpreting tuning results.
  - Quick check question: Given two documents with identical term matches but different lengths, how would increasing parameter b affect their relative BM25 scores?

- Concept: **Query-By-Document (QBD)**
  - Why needed here: This is the core problem formulation—queries are full documents, not short phrases, requiring semantic document-to-document matching rather than keyword search.
  - Quick check question: How does the QBD problem differ from traditional ad-hoc retrieval, and what unique challenges does long-document querying introduce?

- Concept: **Rank Correlation Metrics (Kendall's τ, Spearman, MAP)**
  - Why needed here: The paper evaluates re-ranking quality using multiple rank correlation measures; understanding what each captures is necessary to interpret results.
  - Quick check question: Why might Kendall's τb be preferred over Spearman when evaluating ranked lists with ties?

## Architecture Onboarding

- Component map:
  Input Layer -> Retrieval Stage (BM25/embeddings) -> Re-ranking Stage (SCS/PCS with LLM) -> Human Review -> Output (Tuned Parameters/Model Selection)

- Critical path:
  1. Define query-candidate document sets and filtering criteria
  2. Run initial retrieval (BM25 default: k1=1.5, b=0.75)
  3. Apply LLM re-ranking (choose: SCS-emb, SCS-llm, SCS-instr, PCS-llm, PCS-instr)
  4. Human expert reviews rankings + explanations, corrects errors
  5. Use corrected rankings to tune BM25 parameters via Optuna (50 trials, range k1∈[1.2,2.0], b∈[0.1,1.0])
  6. Evaluate on held-out test set

- Design tradeoffs:
  - **Pairwise vs Pointwise**: Pairwise more stable but O(n²) cost; pointwise O(n) but sensitive to scoring calibration
  - **With vs Without Instructions**: Paper found TREC task descriptions hurt performance—instruction quality matters critically
  - **LLM Choice**: GPT-4o-mini used; larger models may improve accuracy at higher cost
  - **Human Review Depth**: Full validation vs sampling; affects dataset quality vs effort

- Failure signatures:
  - BM25 tuned on LLM rankings underperforms default parameters (observed in experiments)
  - Adding instructions decreases rather than improves performance (observed with TREC descriptions)
  - Positional bias: same document ranked differently based on presentation order
  - Context overflow: long QBD documents exceed LLM context window

- First 3 experiments:
  1. **Baseline establishment**: Run BM25 with default parameters (k1=1.5, b=0.75) on your domain's query-candidate pairs; measure MAP, MRR, Precision@K
  2. **Method comparison**: On a 50-query sample, compare SCS-llm vs PCS-llm (without custom instructions); measure Kendall's τb against any available ground truth; track LLM API costs
  3. **Human validation impact**: Have domain expert correct LLM rankings for 20 queries; tune BM25 on LLM-only vs human-corrected rankings; compare test set performance delta

## Open Questions the Paper Calls Out
1. How does varying the temperature in LLMs during the re-ranking step affect the quality of document scores and the effectiveness of score aggregation?
2. Can the proposed re-ranking methods generalize effectively to other domains, datasets, and LLM architectures?
3. Why does high re-ranking correlation with ground truth fail to translate into improved BM25 parameter tuning?

## Limitations
- Experimental results based on limited datasets (TREC Clinical Trials and CORD-19) with small sample sizes
- Computational cost of pairwise methods (O(n²)) was not fully characterized for larger candidate sets
- Impact of prompt engineering quality on LLM performance was not systematically studied

## Confidence
- Confidence is **Medium** for the core claim that LLM re-ranking reduces but does not eliminate human effort
- Confidence is **Medium** for the pairwise ranking mechanism
- Confidence is **Low** for generalizability to non-medical domains

## Next Checks
1. Apply the QBD-RankedDatagen pipeline to a non-medical domain (e.g., legal documents or patent search) and measure whether human validation remains essential for BM25 tuning performance
2. Systematically vary the quality and specificity of domain instructions provided to LLMs and measure the impact on ranking quality and human review effort
3. Evaluate the pairwise scoring method on datasets with varying candidate counts (n=10, 50, 100) to characterize the O(n²) scaling behavior and identify practical limits for deployment