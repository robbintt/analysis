---
ver: rpa2
title: Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models
arxiv_id: '2506.16760'
source_url: https://arxiv.org/abs/2506.16760
tags:
- camo
- harmful
- image
- visual
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of large vision-language
  models (LVLMs) to jailbreak attacks that bypass safety mechanisms to elicit restricted
  content. The authors propose Cross-modal Adversarial Multimodal Obbfuscation (CAMO),
  a novel black-box attack framework that decomposes harmful instructions into semantically
  benign visual and textual components.
---

# Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models

## Quick Facts
- arXiv ID: 2506.16760
- Source URL: https://arxiv.org/abs/2506.16760
- Reference count: 40
- This paper proposes CAMO, a black-box attack framework that decomposes harmful instructions into benign visual and textual components to jailbreak LVLMs

## Executive Summary
This paper addresses the vulnerability of large vision-language models (LVLMs) to jailbreak attacks that bypass safety mechanisms to elicit restricted content. The authors propose Cross-modal Adversarial Multimodal Obfuscation (CAMO), a novel black-box attack framework that decomposes harmful instructions into semantically benign visual and textual components. CAMO leverages LVLMs' cross-modal reasoning abilities to reconstruct malicious intent through multi-step inference, evading conventional detection mechanisms like perplexity filters, OCR-based scanning, and system-level moderation. The approach requires no access to model internals and operates efficiently under strict black-box constraints.

## Method Summary
CAMO is a black-box attack framework that decomposes harmful instructions into benign visual and textual components, leveraging LVLMs' cross-modal reasoning abilities to reconstruct malicious intent through multi-step inference. The approach operates without requiring access to model internals, making it efficient under strict black-box constraints. The attack systematically breaks down malicious prompts into harmless visual elements (images) and textual components that, when processed together by the LVLM, reconstruct the original harmful intent. This cross-modal decomposition strategy exploits the model's inherent ability to integrate information across different modalities, effectively bypassing conventional safety mechanisms.

## Key Results
- CAMO achieves attack success rates up to 96.97% on Qwen2.5-VL-72B-Instruct and 81.82% on GPT-4.1-nano
- The attack consistently bypasses all tested defense mechanisms with a 100% evasion rate
- CAMO demonstrates effectiveness across state-of-the-art LVLMs while operating under strict black-box constraints

## Why This Works (Mechanism)
The attack exploits LVLMs' cross-modal reasoning capabilities by decomposing harmful instructions into separate benign visual and textual components. When processed independently, these components appear harmless to conventional safety filters. However, the LVLM's ability to integrate cross-modal information allows it to reconstruct the malicious intent through multi-step inference. This approach circumvents detection mechanisms that typically rely on analyzing complete prompts in isolation, including perplexity-based filters, OCR scanning, and system-level moderation.

## Foundational Learning
- **Cross-modal reasoning**: LVLM's ability to integrate visual and textual information - needed for understanding how models process decomposed prompts; quick check: examine model architecture for multimodal fusion layers
- **Jailbreak attack strategies**: Methods to bypass AI safety mechanisms - needed for understanding the attack landscape; quick check: review existing black-box attack methodologies
- **Perplexity filtering**: Technique to detect anomalous text patterns - needed for understanding conventional defense mechanisms; quick check: analyze perplexity scores for benign vs. malicious prompts
- **OCR-based scanning**: Optical character recognition for text extraction from images - needed for understanding visual content analysis; quick check: test OCR accuracy on obfuscated visual components
- **Multi-step inference**: Sequential reasoning process in LVLMs - needed for understanding how decomposed components reconstruct intent; quick check: trace inference paths through model architecture
- **Black-box attack constraints**: Attacking without model access - needed for understanding attack limitations; quick check: verify attack works without gradient information

## Architecture Onboarding

**Component Map**: User Input -> CAMO Decomposer -> Benign Visual + Textual Components -> LVLM -> Malicious Output

**Critical Path**: The attack succeeds when decomposed components bypass individual safety filters but the LVLM successfully integrates them to reconstruct harmful intent during cross-modal reasoning

**Design Tradeoffs**: CAMO prioritizes black-box compatibility over attack strength, requiring no model access but potentially limiting optimization compared to white-box approaches. The decomposition strategy balances between evading detection and maintaining reconstructability.

**Failure Signatures**: Attacks fail when components are detected individually, when LVLM cannot integrate cross-modal information effectively, or when multi-step inference fails to reconstruct the malicious intent from decomposed parts.

**Three First Experiments**:
1. Test individual component detection rates using standard safety filters on decomposed prompts
2. Measure LVLM's cross-modal integration accuracy with benign-looking visual-textual pairs
3. Evaluate multi-step inference success in reconstructing malicious intent from obfuscated components

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Real-world deployment scenarios with additional detection layers, rate limiting, and user behavior analysis are not fully captured in evaluations
- The claimed 100% evasion rate is based on specific tested defenses and does not address emerging or alternative defense strategies
- Generalizability across different LVLM architectures and training paradigms requires further validation

## Confidence
- **Core technical contributions**: High - The cross-modal decomposition strategy and demonstrated attack success rates are well-supported
- **Defense evaluation comprehensiveness**: Medium - Tested defenses represent common approaches but may not encompass all practical mitigation strategies
- **Real-world applicability**: Low - Limited scope of deployment scenarios considered in evaluation

## Next Checks
1. Test CAMO against additional defense mechanisms including dynamic safety classifiers, behavioral analysis systems, and adaptive content moderation frameworks that were not included in the current evaluation.
2. Evaluate the attack's effectiveness across a broader range of LVLM architectures, including models with varying safety training intensities, different pretraining objectives, and diverse cross-modal reasoning capabilities.
3. Conduct field deployment testing in realistic operational environments that include rate limiting, user behavior monitoring, and multi-layered security systems to assess practical limitations beyond controlled laboratory conditions.