---
ver: rpa2
title: A Closer Look at System Prompt Robustness
arxiv_id: '2502.12197'
source_url: https://arxiv.org/abs/2502.12197
tags:
- system
- prompt
- user
- llama
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates methods to improve the robustness of system
  prompt adherence in large language models. It introduces new benchmarks and datasets
  based on real-world system prompts, and demonstrates that realistic fine-tuning
  data and inference-time techniques like classifier-free guidance can significantly
  improve performance.
---

# A Closer Look at System Prompt Robustness
## Quick Facts
- arXiv ID: 2502.12197
- Source URL: https://arxiv.org/abs/2502.12197
- Reference count: 40
- Key outcome: Realistic fine-tuning data and classifier-free guidance significantly improve system prompt adherence, though fundamental limitations remain

## Executive Summary
This paper investigates methods to improve the robustness of system prompt adherence in large language models. The authors introduce new benchmarks and datasets based on real-world system prompts, demonstrating that realistic fine-tuning data and inference-time techniques like classifier-free guidance can significantly improve performance. Fine-tuned models show consistent improvements across benchmarks, with DPO and classifier-free guidance proving especially effective. However, current techniques still fall short of ensuring reliable system prompt adherence, particularly with complex or adversarial inputs.

## Method Summary
The authors created OSCAR, a dataset of real system prompts from production models, and HUG-Bench, a benchmark with human-written adversarial prompts. They tested three fine-tuning methods (SFT, PPO, DPO) and explored classifier-free guidance as an inference-time technique. The experiments covered models from 0B to 13B parameters, evaluating both clean and adversarial scenarios. The study also examined jailbreak robustness and investigated which training data characteristics most impact performance.

## Key Results
- Realistic fine-tuning data significantly outperforms synthetic data for system prompt adherence
- DPO and classifier-free guidance show the most consistent improvements across benchmarks
- Fine-tuned models demonstrate better resistance to jailbreak attempts
- Current approaches still struggle with complex or adversarial inputs despite improvements

## Why This Works (Mechanism)
The paper's mechanisms center on aligning model behavior with system prompts through exposure to realistic examples during training. By fine-tuning on actual system prompts and their expected responses, models learn to prioritize these instructions over other input signals. Classifier-free guidance provides a tunable parameter that can strengthen the model's adherence to system prompts during inference, effectively amplifying the signal from the system prompt relative to user input.

## Foundational Learning
- **System prompt adherence**: The ability of a model to follow instructions embedded in its system prompt. Why needed: Forms the core problem being addressed. Quick check: Compare model responses with and without system prompts to observe differences.
- **Fine-tuning methods**: SFT (Supervised Fine-Tuning), PPO (Proximal Policy Optimization), DPO (Direct Preference Optimization). Why needed: Different approaches to adapting model behavior. Quick check: Evaluate performance differences across these methods on benchmark tasks.
- **Classifier-free guidance**: An inference-time technique that interpolates between unconditional and conditional model generations. Why needed: Provides a way to strengthen system prompt adherence without retraining. Quick check: Test model outputs at different guidance scales to find optimal settings.
- **Adversarial prompting**: Crafting inputs designed to circumvent system prompts. Why needed: Critical for evaluating true robustness. Quick check: Create prompts that attempt to trick the model into ignoring system instructions.
- **Realistic vs synthetic data**: The distinction between training on actual system prompts versus artificially generated ones. Why needed: Impacts the effectiveness of fine-tuning. Quick check: Compare model performance when trained on real versus synthetic examples.
- **Jailbreak robustness**: A model's ability to resist attempts to bypass safety constraints. Why needed: Closely related to system prompt adherence in safety-critical applications. Quick check: Test models with known jailbreak techniques to measure resistance.

## Architecture Onboarding
**Component map:** Real system prompts → Training data preparation → Fine-tuning (SFT/PPO/DPO) → Evaluation (OSCAR/HUG-Bench) → Inference optimization (CFG)

**Critical path:** Data collection → Fine-tuning → Evaluation → Analysis

**Design tradeoffs:** Realistic data provides better adherence but is harder to obtain; synthetic data is easier but less effective. Fine-tuning improves adherence but may reduce general capabilities. CFG is easy to apply but requires careful tuning.

**Failure signatures:** Models may partially follow system prompts while still completing adversarial requests; adherence may break down with complex multi-turn interactions; improvements may not generalize to all prompt types.

**First experiments:** 1) Compare model outputs with and without system prompts on simple tasks, 2) Test model responses to known jailbreak attempts, 3) Evaluate performance differences across fine-tuning methods on the OSCAR benchmark.

## Open Questions the Paper Calls Out
None

## Limitations
- New benchmarks may not fully capture real-world system prompt adherence challenges
- Results limited to models within 0B-13B parameter range, limiting generalizability
- Fundamental limitations remain in ensuring reliable adherence to complex or adversarial inputs
- Unclear why certain fine-tuning methods (DPO) work better than others for this task

## Confidence
- High confidence that realistic fine-tuning data improves system prompt adherence compared to synthetic data
- Medium confidence in the relative effectiveness ranking of fine-tuning methods (DPO > PPO > SFT)
- Medium confidence that classifier-free guidance provides consistent improvements across benchmarks
- Low confidence in the scalability of these findings to much larger models or highly adversarial scenarios

## Next Checks
1. Test the robustness improvements on models outside the 0B-13B parameter range, particularly frontier models, to assess scalability
2. Evaluate system prompt adherence in multi-turn conversations with persistent adversarial attempts to probe the limits of current defenses
3. Conduct ablation studies on the fine-tuning data composition to determine which aspects (realism, diversity, specificity) contribute most to improved adherence