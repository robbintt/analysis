---
ver: rpa2
title: Rethinking Foundation Models for Medical Image Classification through a Benchmark
  Study on MedMNIST
arxiv_id: '2501.14685'
source_url: https://arxiv.org/abs/2501.14685
tags:
- vit-b
- image
- foundation
- medical
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks 12 foundation models, spanning convolutional
  neural networks (CNNs) and vision transformers (ViTs), for medical image classification
  using the MedMNIST dataset. The models are evaluated using both end-to-end fine-tuning
  and linear probing strategies.
---

# Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST

## Quick Facts
- arXiv ID: 2501.14685
- Source URL: https://arxiv.org/abs/2501.14685
- Reference count: 40
- 12 foundation models (CNNs and ViTs) benchmarked on MedMNIST show ViTs outperform CNNs, with optimal learning rates of 10^-4 for CNNs and 10^-5 for ViTs

## Executive Summary
This study benchmarks 12 foundation models for medical image classification using the MedMNIST dataset, evaluating both end-to-end fine-tuning and linear probing strategies. ViT-based models generally outperform CNNs, with end-to-end fine-tuning yielding better results than linear probing. The research identifies optimal learning rates for different architectures and investigates the impact of image size and resizing strategies, finding that scaling generally improves accuracy. The data efficiency analysis reveals that models improve with more training data, though few-shot learning remains challenging. These findings provide practical guidance for transferring foundation models to medical image classification tasks.

## Method Summary
The study benchmarks 12 foundation models (4 CNNs and 8 ViTs) on 12 MedMNIST datasets using end-to-end fine-tuning and linear probing approaches. Models are pre-trained on ImageNet or other large datasets and evaluated using Accuracy and AUC metrics. The researchers systematically vary learning rates, image sizes (32x32, 64x64, 128x128, 224x224), and resizing strategies (scaling vs. zero-padding) to identify optimal configurations for medical image classification.

## Key Results
- ViT-based models generally outperform CNN-based models across all MedMNIST datasets
- End-to-end fine-tuning consistently yields better performance than linear probing
- Optimal learning rates are 10^-4 for CNNs and 10^-5 for ViTs
- Scaling images (interpolation) generally improves accuracy compared to zero-padding, especially for end-to-end fine-tuning
- Model performance improves with more training data, but few-shot learning remains challenging

## Why This Works (Mechanism)

### Mechanism 1: Encoder Learning Rate Sensitivity Dictates Transfer Success
Applying optimal architecture-specific learning rates is causal for maximizing medical image classification performance via end-to-end fine-tuning. The encoder's pre-trained general features must be adapted to the medical domain without being catastrophically destroyed by high learning rates or failing to adapt with low rates. CNNs and ViTs show different optimal sensitivity points, suggesting differences in pre-trained representation stability.

### Mechanism 2: Scale-Induced Semantic Fidelity in Low-Resolution Medical Imaging
Resizing low-resolution medical images via scaling preserves more semantic fidelity than zero-padding, providing stronger causal signals for feature extraction. Scaling distributes original pixel information across more pixels, maintaining spatial relationships, while zero-padding adds irrelevant zero-valued pixels that reduce the active feature area.

### Mechanism 3: Global Attention Bias in Vision Transformers for Medical Context
ViTs' superior performance is linked to their ability to capture global context and long-range dependencies critical in medical images. Medical diagnosis often depends on relationships between distant anatomical structures that CNNs' local texture emphasis may miss, while ViTs' global self-attention can model these relationships holistically.

## Foundational Learning

- **Transfer Learning Paradigm (End-to-End Fine-tuning vs. Linear Probing)**
  - Why needed here: This is the central methodological choice determining how pre-trained features are adapted to medical data
  - Quick check question: Can you explain which approach involves updating the weights of the entire model and which freezes the pre-trained backbone?

- **Vision Transformer (ViT) Architecture & Self-Attention**
  - Why needed here: Understanding ViTs' global attention mechanism explains their superior performance over CNNs in medical imaging
  - Quick check question: What is the fundamental inductive bias of a CNN that a Vision Transformer is designed to overcome?

- **Learning Rate Sensitivity & Stability**
  - Why needed here: The paper's primary actionable guidance is setting the encoder learning rate (1e-4 for CNNs, 1e-5 for ViTs)
  - Quick check question: If you use a learning rate of 1e-4 for a ViT, what is the potential risk to the pre-trained features, according to the paper's findings?

## Architecture Onboarding

- **Component map:** Input Pipeline -> Encoder/Backbone -> Classifier Head -> Optimizer
- **Critical path:**
  1. Model Selection: Choose a ViT (e.g., DINOv2) for top performance or a CNN for potential efficiency
  2. Learning Rate Setup: Configure optimizer with `lr_encoder = 1e-5` (for ViT) or `1e-4` (for CNN)
  3. Training Mode: Select `end_to_end_finetuning = True` for best results
  4. Execution: Train for 15,000 iterations on the MedMNIST dataset

- **Design tradeoffs:**
  - Performance vs. Compute: End-to-end fine-tuning outperforms linear probing but is more computationally expensive
  - ViT vs. CNN: ViTs offer higher potential accuracy on tasks requiring global context but may be less parameter-efficient
  - Scaling vs. Zero-Padding: Scaling improves feature preservation but introduces interpolation artifacts; zero-padding is artifact-free but reduces effective receptive field

- **Failure signatures:**
  - Collapse to Chance: Training loss doesn't decrease or AUC hovers near 0.5 (likely wrong learning rate)
  - Poor Generalization: High training accuracy but low validation accuracy (possible overfitting)
  - ViT Underperforming CNN: ViT accuracy significantly lower than CNN (likely incorrect learning rate or insufficient global context)

- **First 3 experiments:**
  1. Baseline Verification: Train DINOv2 ViT-B/14 on DermaMNIST with `lr_encoder = 1e-5` using end-to-end fine-tuning and compare to reported AUC of 98.92%
  2. Ablation Study - Learning Rate: Repeat training with `lr_encoder = 1e-4` and `1e-6` to quantify performance drop
  3. Ablation Study - Scaling Strategy: Train on DermaMNIST 64x64 variant and compare scaling vs. zero-padding performance

## Open Questions the Paper Calls Out
- How do image resizing strategies (scaling vs. zero-padding) impact performance across the full spectrum of MedMNIST datasets? The analysis is limited to DermaMNIST, and broader validation across all 12 datasets is planned for future work.
- What specific biases do current foundation models exhibit across different medical image classification tasks? The current study focused on aggregate performance metrics rather than analyzing model fairness or systematic errors across demographic or clinical subgroups.
- How do foundation models pre-trained specifically on medical data compare to general-purpose models under linear probing evaluation? The absence of linear probing results for medical-specific models limits the completeness of comparison.

## Limitations
- Study is limited to low-resolution grayscale medical images (MedMNIST), which may not generalize to high-resolution color medical imaging tasks
- Evaluation focuses on a single classifier head design, and the impact of different backbone architectures is not comprehensively explored
- Data efficiency analysis stops at 25% of the training data, leaving performance trajectory at extreme few-shot ratios unclear

## Confidence
- **High Confidence:** ViTs outperform CNNs on MedMNIST is well-supported by consistent performance gaps across all datasets
- **Medium Confidence:** Specific learning rate recommendations may require tuning for different medical imaging domains or model scales
- **Medium Confidence:** Advantage of scaling over zero-padding is demonstrated, but marginal differences at larger input sizes suggest dataset-dependence

## Next Checks
1. Validate optimal learning rates and ViT superiority on a high-resolution, color medical imaging dataset (e.g., CheXpert) to assess generalizability beyond MedMNIST
2. Conduct a controlled experiment comparing scaling vs. zero-padding on a small medical image dataset (e.g., 28x28 ChestX-ray findings) to quantify performance impact
3. Replace the linear classifier head with a multi-layer perceptron or attention-based pooling and measure the effect on performance to determine if head design is a bottleneck