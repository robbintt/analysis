---
ver: rpa2
title: 'Small Language Models for Efficient Agentic Tool Calling: Outperforming Large
  Models with Targeted Fine-tuning'
arxiv_id: '2512.15943'
source_url: https://arxiv.org/abs/2512.15943
tags:
- arxiv
- language
- tool
- evaluation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates replacing expensive Large Language Models
  (LLMs) with Small Language Models (SLMs) for tool-calling tasks. The authors fine-tuned
  a 350M parameter OPT model using the ToolBench dataset, focusing on instruction-following
  and API interaction.
---

# Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning

## Quick Facts
- arXiv ID: 2512.15943
- Source URL: https://arxiv.org/abs/2512.15943
- Authors: Polaris Jhandi; Owais Kazi; Shreyas Subramanian; Neel Sendas
- Reference count: 25
- A 350M parameter OPT model fine-tuned on ToolBench achieved 77.55% pass rate, outperforming much larger models including ChatGPT-CoT (26.00%)

## Executive Summary
This paper demonstrates that small language models (SLMs) can outperform significantly larger models for specialized tool-calling tasks through targeted fine-tuning. The authors fine-tuned a 350M parameter OPT model on the ToolBench dataset using a single epoch of training, achieving a 77.55% pass rate on ToolBench evaluation. This performance substantially exceeds larger baselines including ChatGPT-CoT (26.00%), ToolLLaMA-DFS (30.18%), and ToolLLaMA-CoT (16.27%). The results suggest that parameter concentration on task-specific patterns provides more effective learning than distributed general-purpose capacity in larger models.

## Method Summary
The authors fine-tuned facebook/opt-350m using Hugging Face's TRL SFT trainer on Amazon SageMaker infrastructure. Training employed a conservative learning rate of 5×10⁻⁵ with 100 warmup steps, batch size 32 via gradient accumulation, and maximum gradient norm of 0.3. The model was trained for a single epoch on 187,542 ToolBench instruction-solution pairs covering 16,000+ APIs. Evaluation used ToolEval with ChatGPT-based scoring across six categories and 1,100 queries, reporting pass rates on categories G1-G3.

## Key Results
- 350M parameter SLM achieved 77.55% pass rate on ToolBench evaluation
- Outperformed ChatGPT-CoT (26.00%), ToolLLaMA-DFS (30.18%), and ToolLLaMA-CoT (16.27%)
- Demonstrated cost-effective alternative to large models for specialized tool-calling tasks
- Single-epoch training prevented overfitting while capturing task structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific optimization concentrates limited parameters on high-value patterns, outperforming distributed general-purpose capacity.
- Mechanism: A 350M parameter model dedicates nearly all representational capacity to tool-calling syntax, API parameter mapping, and structured reasoning chains. Larger models dilute parameters across broad linguistic competencies irrelevant to this task.
- Core assumption: Tool-calling requires structured pattern completion rather than open-ended reasoning.
- Evidence anchors:
  - [abstract] "targeted fine-tuning of SLMs can deliver superior performance compared to larger general-purpose models for specialized tasks"
  - [Discussion] "Large language models suffer from parameter dilution, where the vast majority of parameters are optimized for general language understanding rather than tool manipulation"
  - [corpus] Neighbor paper "Small Language Models for Agentic Systems" confirms SLMs are "sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy"
- Break condition: If tool-calling tasks require significant contextual reasoning or creative problem-solving beyond pattern matching, parameter concentration becomes insufficient.

### Mechanism 2
- Claim: Single-epoch training on high-quality domain data prevents overfitting while capturing task structure.
- Mechanism: Conservative learning rate (5×10⁻⁵) with aggressive gradient clipping (0.3 max norm) enables stable adaptation across 187K examples without memorization. The model learns generalizable Thought-Action-Action Input patterns rather than specific API responses.
- Core assumption: ToolBench examples contain consistent structural patterns that transfer across APIs.
- Evidence anchors:
  - [Experiment Setup] "fine-tuned for a single epoch with carefully optimized hyperparameters... conservative learning rate of 5×10⁻⁵ with 100 warmup steps"
  - [Experiment Setup] "approach promoted learning of generalizable tool-use patterns rather than memorization"
  - [corpus] Weak corpus validation—neighbor papers discuss SLM tool-use but don't specifically validate single-epoch training efficacy
- Break condition: If evaluation data overlaps substantially with training distribution, reported gains may reflect distribution matching rather than true generalization.

### Mechanism 3
- Claim: Evaluation alignment between training format and test criteria inflates measured performance relative to baselines.
- Mechanism: The model was trained specifically on ToolBench-formatted examples and evaluated on ToolBench criteria using ToolEval. Baseline models (ChatGPT-CoT, Claude-CoT) were not fine-tuned for this specific format, creating asymmetric comparison conditions.
- Core assumption: Pass rate on ToolBench correlates with real-world tool-calling effectiveness.
- Evidence anchors:
  - [Limitations] "Our model was specifically optimized for ToolBench evaluation criteria and may not generalize to other tool-calling frameworks"
  - [Evaluation framework] "Each model's responses were automatically evaluated using ChatGPT-based scoring"
  - [corpus] "On Generalization in Agentic Tool Calling" notes that "ability to transfer reasoning strategies and coordinate tools across diverse domains" remains unsolved
- Break condition: When deployed on APIs outside ToolBench distribution or with novel authentication/error patterns, performance may degrade substantially.

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT) for structured outputs**
  - Why needed here: The entire method relies on teaching a pre-trained model to emit ToolBench's Thought-Action-Action Input format rather than free-form text.
  - Quick check question: Can you explain why gradient clipping prevents training instability when learning structured output patterns?

- Concept: **Tool-calling agent architecture (ReAct pattern)**
  - Why needed here: ToolBench evaluation assumes models interleave reasoning traces with API calls in specific formats.
  - Quick check question: How does a Thought-Action-Observation loop differ from direct API invocation?

- Concept: **Benchmark-aligned evaluation risks**
  - Why needed here: The dramatic performance gap (77.55% vs. 26%) partially reflects training-evaluation alignment rather than pure capability gains.
  - Quick check question: What controls would you add to verify that SLM gains generalize beyond the benchmark distribution?

## Architecture Onboarding

- Component map:
  - ToolBench dataset (187,542 instruction-solution pairs, 16K+ APIs) -> facebook/opt-350m (350M parameters) -> Hugging Face TRL SFTTrainer -> Amazon SageMaker ml.g5.8xlarge -> ToolEval (ChatGPT-based evaluator)

- Critical path:
  1. Transform ToolBench multi-turn data into SFT-compatible sequences with system/user/assistant delimiters
  2. Configure SFTTrainer with learning rate 5×10⁻⁵, batch size 32 (via gradient accumulation), max grad norm 0.3
  3. Train single epoch (~5,860 steps)
  4. Evaluate on ToolBench categories G1-G3 using ToolEval pass rate metric

- Design tradeoffs:
  - Model size (350M) vs. specialization depth: Smaller models constrain reasoning capacity but force efficient pattern learning
  - Single epoch vs. multiple epochs: Prevents overfitting but may underutilize training signal
  - ToolBench-only training vs. mixed corpora: Maximizes benchmark alignment but limits real-world robustness

- Failure signatures:
  - Verbose outputs instead of structured API calls → insufficient format enforcement in training data
  - High variance across evaluation categories → underfitting or inconsistent training examples
  - Errors on unseen APIs → memorization rather than pattern generalization
  - Training divergence → reduce learning rate or increase warmup steps

- First 3 experiments:
  1. **Baseline reproduction**: Run provided OPT-350M SFT pipeline on ToolBench subset, verify ~77% pass rate on held-out queries
  2. **Cross-distribution test**: Evaluate fine-tuned model on API-Blend or BFCL benchmarks to measure out-of-distribution generalization
  3. **Ablation study**: Compare single-epoch vs. two-epoch training, and learning rates [1e-5, 5e-5, 1e-4] to validate hyperparameter sensitivity claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do these findings generalize to other tool-calling frameworks and real-world API ecosystems beyond ToolBench?
- Basis in paper: [explicit] "Our model was specifically optimized for ToolBench evaluation criteria and may not generalize to other tool-calling frameworks or real-world API ecosystems with different interaction patterns."
- Why unresolved: The tight coupling between training data and evaluation metrics means performance on novel tool domains remains untested.
- What evidence would resolve it: Cross-benchmark evaluation on alternative tool-use corpora (e.g., API-Blend, Gorilla benchmarks) and real-world API testing.

### Open Question 2
- Question: What is the optimal parameter count for different specialized domains, and how does task complexity relate to model capacity requirements?
- Basis in paper: [explicit] "The optimal parameter count likely varies across different specialized domains, warranting systematic investigation of task-complexity to model-capacity relationships."
- Why unresolved: Only the 350M parameter scale was tested; whether this represents a universal sweet spot or is task-specific is unknown.
- What evidence would resolve it: Systematic evaluation across multiple parameter scales (e.g., 125M, 350M, 770M, 1.3B) on diverse specialized tasks.

### Open Question 3
- Question: Can hybrid approaches effectively combine SLM efficiency with LLM adaptability?
- Basis in paper: [explicit] "Future research should investigate...hybrid approaches that combine the efficiency of targeted models with the adaptability of larger systems."
- Why unresolved: The trade-off between specialization (high performance, low adaptability) and generalization (lower performance, high adaptability) has not been bridged.
- What evidence would resolve it: Architecture experiments with routing mechanisms, cascaded models, or dynamic model selection strategies.

### Open Question 4
- Question: Does the evaluation alignment between training and testing artificially inflate reported performance?
- Basis in paper: [inferred] The acknowledgment that the model was "specifically optimized for ToolBench evaluation criteria" combined with using ChatGPT as the evaluator raises concerns about evaluation circularity.
- Why unresolved: ToolEval uses ChatGPT for scoring; models trained on ToolBench may learn evaluation-specific patterns rather than generalizable tool-use capabilities.
- What evidence would resolve it: Human evaluation comparison and functional API execution testing rather than model-based evaluation.

## Limitations

- Performance generalizability concerns due to tight coupling between training data and evaluation criteria
- Single-epoch training efficacy remains uncertain without validation across multiple seeds
- Asymmetric baseline comparisons limit interpretation of absolute performance gains

## Confidence

- Performance Generalizability: Medium - Strong benchmark results but evaluation alignment concerns
- Single-Epoch Training Efficacy: Medium - Claims made but limited validation across seeds/durations
- Baseline Asymmetry: Low - Inherent comparison limitations due to asymmetric fine-tuning

## Next Checks

1. **Cross-Distribution Benchmarking**: Evaluate the fine-tuned SLM on BFCL, API-Blend, or other tool-calling benchmarks not used in training to quantify true generalization capability beyond ToolBench distribution.

2. **Training Duration Sensitivity**: Conduct controlled experiments comparing single-epoch training to two and three epochs with identical hyperparameters to verify that performance gains don't stem from under-training or lucky random initialization.

3. **Format Robustness Testing**: Systematically perturb ToolBench-formatted outputs during evaluation (e.g., reordered arguments, alternative JSON structures) to assess whether the model learns semantic tool-calling or merely memorized canonical formatting patterns.