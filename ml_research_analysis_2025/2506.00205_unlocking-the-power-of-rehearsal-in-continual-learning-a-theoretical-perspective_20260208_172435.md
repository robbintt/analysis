---
ver: rpa2
title: 'Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective'
arxiv_id: '2506.00205'
source_url: https://arxiv.org/abs/2506.00205
tags:
- rehearsal
- concurrent
- sequential
- learning
- follows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes rehearsal-based continual learning (CL) methods
  to address catastrophic forgetting, focusing on the widely-used concurrent rehearsal
  strategy. The authors propose a novel sequential rehearsal approach, where new tasks
  are learned first and then old tasks are revisited sequentially.
---

# Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective

## Quick Facts
- **arXiv ID:** 2506.00205
- **Source URL:** https://arxiv.org/abs/2506.00205
- **Reference count:** 40
- **Primary result:** Sequential rehearsal outperforms concurrent rehearsal when task dissimilarity is high, leading to a hybrid framework that adaptively combines both strategies.

## Executive Summary
This paper provides the first comprehensive theoretical analysis of rehearsal-based continual learning methods, focusing on the trade-off between concurrent and sequential rehearsal strategies. The authors show that sequential rehearsal, where new tasks are learned first followed by sequential revisiting of old tasks, outperforms concurrent rehearsal when tasks are dissimilar. They propose a hybrid rehearsal framework that adaptively selects between these strategies based on task similarity, using concurrent rehearsal for similar tasks and sequential rehearsal for dissimilar ones. Empirical results on CIFAR and TinyImagenet datasets with deep neural networks confirm that the hybrid approach consistently outperforms standard concurrent rehearsal, especially when task dissimilarity is high.

## Method Summary
The paper analyzes three rehearsal strategies: (1) Concurrent rehearsal - jointly training on new task data and memory buffer samples; (2) Sequential rehearsal - learning new task first, then sequentially revisiting old tasks one-by-one; and (3) Hybrid rehearsal - a novel adaptive approach that partitions memory based on task similarity. The similarity threshold τ is computed using cosine similarity of gradients at initialization, with tasks above τ trained concurrently and those below τ revisited sequentially. The theoretical analysis uses overparameterized linear models to characterize forgetting and generalization errors, while empirical validation uses ResNet-18 on split datasets.

## Key Results
- Sequential rehearsal outperforms concurrent rehearsal when task dissimilarity is high due to reduced gradient interference
- Hybrid rehearsal framework achieves consistent improvements over concurrent rehearsal, especially with high task dissimilarity
- Theoretical analysis provides closed-form expressions for forgetting and generalization errors in overparameterized linear models
- Empirical results on CIFAR-10, CIFAR-100, and TinyImagenet200 validate theoretical predictions with 1-2% accuracy improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequential rehearsal outperforms concurrent rehearsal when task dissimilarity is high, due to reduced gradient interference during optimization.
- **Mechanism:** When dissimilar tasks are trained concurrently, their gradients point in conflicting directions, causing interference that degrades learning on both old and new tasks. Sequential rehearsal decouples this interference by completing optimization on each task separately, allowing the model to find better minima for each task without simultaneous gradient conflicts.
- **Core assumption:** Task dissimilarity can be characterized by the distance between ground truth parameters in the model's parameter space, and this dissimilarity correlates with gradient interference during training.
- **Evidence anchors:**
  - [abstract] "By explicitly characterizing forgetting and generalization error, we show that sequential rehearsal performs better when tasks are less similar."
  - [section 5.2] Theorem 5.3 establishes: "F(concurrent)_2 > F(sequential)_2 iff ξ_1∥w*_1 - w*_2∥² + ξ_2σ² / ∥w*_1∥² > 1" - showing the condition depends on task distance.
  - [corpus] Related work "Forget Less, Retain More" addresses catastrophic forgetting in rehearsal-based CL but does not specifically analyze the sequential vs. concurrent distinction, suggesting this mechanism is a novel contribution of the current paper.

- **Break condition:** When tasks become highly similar (small task gap), the advantage disappears because concurrent rehearsal can leverage positive transfer between similar tasks without significant interference.

### Mechanism 2
- **Claim:** The hybrid rehearsal framework adaptively minimizes forgetting by matching rehearsal strategy to task similarity characteristics at each training step.
- **Mechanism:** At each task t, the algorithm computes cosine similarity between gradients of the current task and each old task in memory. Tasks with similarity above threshold τ use concurrent rehearsal (benefiting from positive transfer), while dissimilar tasks below τ use sequential rehearsal (avoiding negative interference). This local adaptation outperforms a single global strategy.
- **Core assumption:** Cosine similarity of gradients at initialization provides a reliable proxy for task similarity and predicts which rehearsal strategy will be more effective.
- **Evidence anchors:**
  - [abstract] "Hybrid Rehearsal method, which trains similar tasks concurrently and revisits dissimilar tasks sequentially."
  - [section 6.1] Algorithm 1 explicitly shows the DIVIDE BUFFER step using "cosine similarity of gradients with respect to the model parameters, i.e., S_c(∇_θ L(D_t, θ_{t-1}), ∇_θ L(M_t,i), θ_{t-1})"
  - [corpus] Related papers on rehearsal-based CL (e.g., "Rehearsal with Auxiliary-Informed Sampling") focus on sample selection quality but do not address strategy selection based on task similarity, indicating this adaptive mechanism is novel.

- **Break condition:** If the similarity threshold τ is poorly calibrated, or if gradient similarity at initialization does not predict training dynamics, the adaptive switching may make incorrect decisions and perform worse than a fixed strategy.

### Mechanism 3
- **Claim:** Sequential rehearsal acts as a consolidation phase that strengthens old task representations after new task learning, analogous to human memory consolidation.
- **Mechanism:** After learning a new task to convergence, the model sequentially revisits each old task's memory data. This post-training rehearsal allows the model to restore parameters important for old tasks without the interference from simultaneously optimizing new task objectives. The sequential ordering (oldest to newest) ensures earlier tasks receive explicit consolidation.
- **Core assumption:** The order of sequential rehearsal matters, and visiting older tasks first provides meaningful consolidation benefits, though the paper notes the optimal order remains an open question.
- **Evidence anchors:**
  - [section 1] "Inspired by human learning, where sequentially revisiting tasks helps mitigate forgetting, we explore whether sequential rehearsal can offer greater benefits for CL"
  - [section 5.2] Remark explicitly states: "It is clear that the order in which old tasks are revisited after current task learning is very important under the framework of sequential rehearsal"
  - [corpus] Related work "STAR: Stability-Inducing Weight Perturbation" addresses stability in CL but does not specifically model the sequential consolidation mechanism, suggesting this human-inspired approach is distinct.

- **Break condition:** If tasks have complex interdependencies where revisiting one task significantly harms another, the sequential consolidation order could inadvertently increase forgetting instead of reducing it.

## Foundational Learning

- **Concept: Catastrophic forgetting in neural networks**
  - **Why needed here:** The paper's entire framework is built on understanding and mitigating catastrophic forgetting—the tendency of neural networks to lose performance on previously learned tasks when training on new data.
  - **Quick check question:** Can you explain why gradient descent on new task data would naturally degrade performance on old tasks in a shared parameter space?

- **Concept: Overparameterized models and minimum-norm solutions**
  - **Why needed here:** The theoretical analysis uses overparameterized linear models where SGD converges to minimum-norm solutions closest to initialization. This mathematical structure enables closed-form forgetting and generalization analysis.
  - **Quick check question:** In an overparameterized linear model with p > n+M parameters trained with MSE loss, what type of solution does SGD converge to and why?

- **Concept: Rehearsal-based continual learning**
  - **Why needed here:** The paper assumes familiarity with the standard rehearsal approach—storing a subset of old task data in a memory buffer and replaying it during new task training to preserve old knowledge.
  - **Quick check question:** How does a memory buffer with reservoir sampling differ from storing all old data, and what tradeoffs does this introduce?

## Architecture Onboarding

- **Component map:**
  Memory Manager -> Similarity Estimator -> Strategy Selector -> Concurrent Trainer and Sequential Rehearser

- **Critical path:**
  1. Implement standard concurrent rehearsal baseline with memory buffer.
  2. Add gradient computation for all memory samples at task start.
  3. Implement cosine similarity calculation and threshold-based partitioning.
  4. Modify training loop to first train concurrently on D_t ∪ M_sim, then sequentially on each M_dis,h.
  5. Tune similarity threshold τ on validation data.

- **Design tradeoffs:**
  - **Threshold selection:** Lower τ classifies more tasks as dissimilar → more sequential rehearsal → better for high-dissimilarity task sequences but slower due to sequential training passes.
  - **Memory allocation:** The paper assumes equal allocation M/(t-1) per previous task. Unequal allocation based on task difficulty or importance is unexplored.
  - **Rehearsal order:** The paper uses oldest-to-newest but notes optimal ordering is an open problem—empirically testing different orders (e.g., most-dissimilar-first) could yield improvements.

- **Failure signatures:**
  1. **Degradation on similar tasks:** If τ is set too high, too many tasks get sequential rehearsal unnecessarily, missing positive transfer opportunities that concurrent rehearsal would provide.
  2. **No improvement over baseline:** If task sequence is highly homogeneous, hybrid approach reduces to concurrent rehearsal with minimal benefit.
  3. **Increased forgetting on specific tasks:** Poor calibration of similarity metric may incorrectly classify tasks, applying the wrong strategy and increasing forgetting.

- **First 3 experiments:**
  1. **Replicate linear model validation:** Implement the theoretical setup with T=5 tasks, p=500, n=24, M=24, varying task gap (∥w*_j - w*_i∥) from 0.0 to 2.0. Compare forgetting and generalization error between concurrent, sequential, and hybrid methods to validate theoretical predictions.
  2. **Ablation on similarity threshold:** On Split-CIFAR-100 with 5 tasks, sweep τ from -0.5 to 0.5 and plot final accuracy vs. τ. Identify the optimal threshold and analyze how task dissimilarity in the dataset affects optimal τ.
  3. **Corruption dissimilarity scaling:** Follow the paper's methodology—corrupt increasing proportions of task labels (5%, 10%, 20%) to artificially increase task dissimilarity. Verify that hybrid vs. concurrent improvement scales with dissimilarity as predicted by theory.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal ordering strategy for sequential rehearsal to maximize knowledge retention?
- **Basis in paper:** [explicit] The authors state in Section 5.2 that the "oldest to newest" sequential order used in experiments "is not necessarily the optimal strategy" and identifying the effective order is a future direction.
- **Why unresolved:** The current work utilizes a simple temporal heuristic for the sequence of revisiting old tasks rather than optimizing the order based on task relationships.
- **What evidence would resolve it:** A theoretical or empirical comparison of ordering schemes (e.g., random, difficulty-based, or gradient-similarity-based) demonstrating superior convergence or forgetting metrics.

### Open Question 2
- **Question:** Can the performance trade-off between concurrent and sequential rehearsal be explicitly characterized for a general sequence of $T$ tasks?
- **Basis in paper:** [explicit] In Section 5.2, the authors note it is "challenging to provide an exact closed-form characterization of the conditions... for general $T$," relying instead on specific cases like $T=2$ or orthonormal tasks.
- **Why unresolved:** The mathematical expressions for the coefficients of forgetting and generalization error become highly complex and intractable as the number of tasks increases.
- **What evidence would resolve it:** Derivation of approximation bounds or asymptotic analyses for large $T$ that generalize the insights from the two-task case.

### Open Question 3
- **Question:** How do the theoretical advantages of sequential rehearsal transfer to under-parameterized or strictly non-linear neural networks?
- **Basis in paper:** [inferred] The paper's theoretical framework (Section 3) explicitly assumes overparameterized linear models with minimum-norm solutions, whereas practical DNNs may operate in different regimes.
- **Why unresolved:** While the empirical DNN results are promising, the theoretical guarantees regarding task dissimilarity rely on the properties of linear regression and the "lazy training" regime.
- **What evidence would resolve it:** Extending the theoretical analysis to non-linear models using Neural Tangent Kernels (NTK) or providing guarantees for finite-width networks outside the overparameterized limit.

## Limitations

- The theoretical analysis relies on overparameterized linear models, which may not fully capture the behavior of deep neural networks with non-convex loss landscapes
- The empirical improvements are relatively modest (1-2% accuracy gains), suggesting practical impact may be limited to specific scenarios with high task dissimilarity
- The gradient similarity metric used for task classification is computed at initialization only, without accounting for how similarity evolves during training

## Confidence

- **High Confidence:** The core theoretical results for overparameterized linear models are mathematically rigorous and well-supported by the proofs in Appendix C
- **Medium Confidence:** The empirical results with deep networks support the theoretical findings, but the effect sizes are modest and could be influenced by implementation details not fully specified in the paper
- **Low Confidence:** The optimal threshold τ is fixed to specific values (0 or -0.1) without systematic exploration across datasets

## Next Checks

1. **Cross-architecture validation:** Test hybrid rehearsal with architectures other than ResNet-18 (e.g., Vision Transformers, MobileNet) to verify the approach generalizes beyond a single model family
2. **Dynamic threshold adaptation:** Implement a method to adaptively tune τ during training based on validation performance rather than using fixed thresholds, and compare against the static approach
3. **Task order sensitivity:** Systematically vary the order of sequential rehearsal (oldest-to-newest vs. newest-to-oldest vs. similarity-ordered) and measure the impact on forgetting to validate the claim about consolidation benefits