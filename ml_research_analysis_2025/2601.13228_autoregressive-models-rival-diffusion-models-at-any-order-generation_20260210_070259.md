---
ver: rpa2
title: Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation
arxiv_id: '2601.13228'
source_url: https://arxiv.org/abs/2601.13228
tags:
- generation
- diffusion
- tokens
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Any-order Any-subset Autoregressive modeling
  (A3), a new sequence modeling framework that extends standard autoregressive (AR)
  language models to support arbitrary token group orders and parallel prediction
  while preserving the multi-layer dependency structure of AR models. Unlike diffusion
  models that rely on single-step conditional prediction and often suffer from lower
  quality and training instability, A3 reformulates group prediction through a generalized
  AR factorization, enabling richer dependency modeling and bidirectional context
  use.
---

# Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation

## Quick Facts
- arXiv ID: 2601.13228
- Source URL: https://arxiv.org/abs/2601.13228
- Reference count: 14
- Primary result: A3-8B outperforms diffusion models on multiple language generation benchmarks while trained on 30x less data

## Executive Summary
This paper introduces Any-order Any-subset Autoregressive modeling (A3), a new sequence modeling framework that extends standard autoregressive (AR) language models to support arbitrary token group orders and parallel prediction while preserving the multi-layer dependency structure of AR models. Unlike diffusion models that rely on single-step conditional prediction and often suffer from lower quality and training instability, A3 reformulates group prediction through a generalized AR factorization, enabling richer dependency modeling and bidirectional context use. The method is implemented using a two-stream attention architecture and a progressive training strategy that adapts pretrained AR models to any-order prediction. Experiments on QA, commonsense reasoning, and story infilling tasks show that A3-8B outperforms diffusion-based models such as DiffuLlama-7B and Dream-7B across all benchmarks, despite being trained on only 2B tokens compared to their 65B.

## Method Summary
A3 generalizes standard autoregressive modeling by factorizing sequences into groups and predicting them in arbitrary order while preserving the recursive dependency structure. The method uses a two-stream attention architecture with separate content and query streams, where the query stream predicts target groups using only information from preceding groups. A progressive training curriculum transitions models from standard AR (Stage 1) through contiguous groups (Stage 2) to arbitrary permutations (Stage 3). The model is initialized from LLaMA weights and fine-tuned on 2B tokens of FineWeb+SlimPajama data. Inference supports both fast groupwise AR sampling and slower dynamic resampling with quality trade-offs.

## Key Results
- A3-8B outperforms DiffuLlama-7B and Dream-7B on TriviaQA, HellaSwag, Winogrande, SIQA, PIQA, and ROCStories benchmarks
- A3-8B achieves strong scaling behavior, with smaller models (1.5B, 3B) also showing competitive performance
- The progressive training curriculum is essential: skipping stages causes 4-6 point drops in benchmark performance
- A3 demonstrates faster convergence compared to diffusion models, requiring only 2B tokens vs 65B for diffusion baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating generation as a multi-group autoregressive process preserves deep dependency modeling while enabling parallel decoding.
- Mechanism: Standard autoregressive (AR) models factor sequences as P(x_t | x_{<t}), creating deep recursive dependencies. Diffusion models often rely on single-step conditional prediction (P(x_{masked} | x_{visible})), which the authors argue limits dependency depth. A3 generalizes the AR factorization to groups P(x_{G_k} | x_{G_{<k}}), retaining the multi-layer recursive structure of AR while allowing groups to be predicted in parallel or arbitrary order.
- Core assumption: The generation quality and stability benefits of AR models stem specifically from their multi-layer recursive dependency structure, which is superior to the single-step dependencies in diffusion.
- Evidence anchors:
  - [abstract] "A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility..."
  - [section 2.1] Contrasts MDLM's "shallow dependencies" with A3's recursive structure.
  - [corpus] Related work "Reviving Any-Subset Autoregressive Models" supports the difficulty of parallel sampling in diffusion, noting distribution drift, which A3 aims to mitigate via AR factorization.
- Break condition: If the task relies primarily on global coherence where local recursive dependencies are irrelevant, the structural advantage of A3 over diffusion may diminish.

### Mechanism 2
- Claim: Two-stream attention enables arbitrary-order prediction by decoupling positional queries from content context.
- Mechanism: A single Transformer stream struggles with arbitrary orders because standard causal masks fix the direction. A3 uses a **Content Stream** (encodes observed tokens) and a **Query Stream** (holds learnable query vectors). The Query Stream attends to the Content Stream of *previous* groups to predict the current group's content, allowing the model to predict tokens at any position without leaking future information within the same group.
- Core assumption: It is necessary to strictly separate the "where to predict" (query) from the "what is context" (content) to handle non-left-to-right factorizations effectively.
- Evidence anchors:
  - [section 3.1] "This separation allows A3 to retain the recursive structure of autoregression while relaxing the generation order constraint..."
  - [section 3.1] Equations 4 and 5 define the distinct attention flow for content vs. query streams.
- Break condition: If the attention mask logic is flawed (e.g., query stream leaks info from the target group), the model fails to learn valid conditional distributions.

### Mechanism 3
- Claim: Progressive adaptation from pre-trained AR models stabilizes the learning of arbitrary subset prediction.
- Mechanism: Directly training a model on random permutations is unstable. A3 employs a curriculum: **Stage 1** (Standard AR, singleton groups) → **Stage 2** (Contiguous groups, size > 1) → **Stage 3** (Arbitrary permutations of groups). This transitions the model from the pre-trained LLaMA weights to the A3 regime without losing the base capabilities.
- Core assumption: Standard pre-trained AR weights (like LLaMA) provide a strong initialization that can be "degraded" or "relaxed" into a flexible generator more effectively than training from scratch.
- Evidence anchors:
  - [section 3.2] Describes the 3-stage training recipe.
  - [section 4.3] Table 3 shows a 4-6 point drop in benchmarks when skipping the curriculum stages.
- Break condition: If the base model is not pre-trained or the dataset for fine-tuning is too small (paper uses 2B tokens), the model may fail to bridge the gap to the AR baseline (as seen in Table 1 where A3-8B trails Llama-3.1-8B).

## Foundational Learning

- Concept: **Autoregressive Factorization**
  - Why needed here: A3 is a generalization of the standard AR product rule P(x) = ∏ P(x_i | x_{<i}). You must understand the chain rule of probability to see how A3 extends it from tokens to groups.
  - Quick check question: How does grouping tokens affect the chain rule computation compared to single-token prediction?

- Concept: **Two-Stream Attention (XLNet-style)**
  - Why needed here: This is the architectural core. Understanding how the query stream interacts with the content stream via masking is essential for debugging the model.
  - Quick check question: In the query stream, can a token attend to other tokens in the same group?

- Concept: **Masked Diffusion Models (MDLM)**
  - Why needed here: The paper frames A3 as a solution to the limitations of MDLM (shallow dependencies). Knowing how diffusion iteratively denoises helps contrast A3's "any-order" approach.
  - Quick check question: Why does the paper claim diffusion models suffer from "shallow dependencies" compared to A3?

## Architecture Onboarding

- Component map:
  Input: Standard token embeddings + Positional embeddings → Transformer layers (Content Stream + Query Stream) → Vocabulary logits

- Critical path:
  1. **Mask Generation:** Correct generation of causal masks based on the group assignment is the most failure-prone step. If groups are permuted, masks must update dynamically.
  2. **Curriculum Transition:** Moving from Stage 1 to Stage 3 must be sequenced correctly; skipping stages causes performance collapse (Table 3).

- Design tradeoffs:
  - **Inference Speed vs. Quality:** Groupwise AR sampling (Algorithm 1) is fast but potentially less accurate. Dynamic Resampling (Algorithm 2) yields lower perplexity but is significantly slower due to full-sequence re-evaluation.
  - **Training Cost:** A3 requires converting a pre-trained AR model. It is not a "train from scratch" method in this context.

- Failure signatures:
  - **Premature Permutation:** Introducing random order permutations (Stage 3) too early or without Stage 1/2 fine-tuning results in a 4-6 point drop in accuracy.
  - **Context Leakage:** If the Query Stream can attend to the Content Stream of the *current* group, the training objective becomes trivial and generation fails.
  - **Data Scarcity:** With only 2B tokens, A3-8B underperforms the original LLaMA-3.1-8B (52.1 vs 19.4 on TriviaQA), indicating the method requires sufficient adaptation data.

- First 3 experiments:
  1. **Sanity Check (Stage 1):** Run inference on the Stage 1 checkpoint (standard AR). Verify it matches the baseline LLaMA performance to ensure weights were loaded correctly.
  2. **Ablation on Curriculum:** Train a small A3 model (e.g., 100M parameters) on a tiny corpus, skipping Stages 1 & 2. Compare loss curves against the full 3-stage recipe to verify the stability claim.
  3. **Infilling Evaluation:** Test A3 on the ROCStories task using Algorithm 1 (Groupwise) vs. Algorithm 2 (Dynamic Resampling). Compare ROUGE scores vs. wall-clock time to identify the optimal speed/quality tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can A3 match or exceed the performance of standard AR models when trained at comparable data scales (trillions of tokens rather than 2B)?
- Basis in paper: [explicit] "Although A3 still underperforms the AR baseline, this gap is likely attributable to limited training data; with larger-scale pretraining, we expect A3 to close the difference further." The conclusion also states: "we plan to explore scaling A3 to larger models and datasets."
- Why unresolved: A3-8B was trained on only 2B tokens vs. LLaMA's 15T tokens, making fair comparison impossible. The scaling experiments only go up to 2.5B tokens.
- What evidence would resolve it: Train A3 variants on 100B+ tokens and compare against equivalently-trained AR baselines on the full benchmark suite.

### Open Question 2
- Question: What adaptive training schedules (e.g., loss-based or performance-based) could improve upon the fixed three-stage progressive training curriculum?
- Basis in paper: [explicit] "An adaptive schedule, e.g., based on training loss, may further improve robustness. We plan to investigate this direction in the future work."
- Why unresolved: The ablation shows skipping early stages hurts performance by 4-6 points, but only fixed schedules were tested. The optimal transition points between stages remain unknown.
- What evidence would resolve it: Implement loss-threshold or validation-based automatic stage transitions and measure performance gains over the current fixed-epoch schedule.

### Open Question 3
- Question: How does A3 perform on long-context reasoning tasks requiring coherence over sequences longer than the 2048-token training context?
- Basis in paper: [explicit] "In the future, we plan to explore scaling A3 to larger models and datasets, as well as applying it to more challenging tasks such as long-context reasoning."
- Why unresolved: All experiments used max context length 2048. The model's ability to maintain multi-layer dependencies and bidirectional reasoning over extended contexts is untested.
- What evidence would resolve it: Evaluate A3 on long-context benchmarks (e.g., LongBench, Needle in a Haystack) with 4K-32K context lengths after appropriate training.

## Limitations

- Training data scale and generalization: A3-8B trained on only 2B tokens underperforms LLaMA-3.1-8B on knowledge tasks, suggesting the method needs more adaptation data to match larger foundation models.
- Architectural complexity and inference trade-offs: Two-stream attention and dynamic resampling introduce significant inference complexity without quantified wall-clock timing or memory usage comparisons.
- Task-specific performance gaps: A3 shows strong performance on commonsense reasoning and infilling but underperforms on knowledge-intensive tasks like TriviaQA, limiting universal applicability.

## Confidence

**High Confidence**
- The architectural design of two-stream attention and its role in enabling arbitrary-order prediction is well-supported by equations and ablation results (Stage 3 skipping causes 4-6 point drops).
- The progressive training curriculum's necessity is empirically validated across multiple model sizes.

**Medium Confidence**
- Claims about A3 rivaling diffusion models "at ANY-ORDER generation" are supported on tested benchmarks but may not generalize to all generation scenarios, particularly those requiring deep factual recall.
- The sample efficiency claim (2B vs 65B tokens) is compelling but based on a limited evaluation set; broader testing is needed.

**Low Confidence**
- Practical deployment implications (inference speed, memory, scalability) are not quantified, limiting confidence in real-world applicability.
- The method's robustness to dataset shifts or domain-specific tasks is not evaluated.

## Next Checks

1. **Curriculum Ablation with Larger Data**
   Train A3-1.5B on a 10B+ token corpus (e.g., full FineWeb or ThePile) with and without the 3-stage curriculum. Compare convergence speed and final task performance to isolate the curriculum's impact from data scaling effects.

2. **Inference Cost Benchmarking**
   Implement both Groupwise AR (Algorithm 1) and Dynamic Resampling (Algorithm 2) for A3-1.5B. Measure wall-clock time, memory usage, and samples/second on a standardized GPU (e.g., A100) for tasks like ROCStories infilling. Compare against standard diffusion sampling (e.g., from DiffuSLow) under equivalent quality targets (e.g., ROUGE-L ≥ 0.35).

3. **Knowledge-Intensive Task Scaling**
   Evaluate A3-8B on a broader set of knowledge tasks (e.g., NaturalQuestions, WebQuestions) and compare scaling behavior against Llama-3.1-8B when both are fine-tuned on 10B+ tokens. This will test whether the performance gap on TriviaQA persists with more adaptation data.