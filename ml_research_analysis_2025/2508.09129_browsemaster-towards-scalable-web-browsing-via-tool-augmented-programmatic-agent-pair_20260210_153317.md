---
ver: rpa2
title: 'BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic
  Agent Pair'
arxiv_id: '2508.09129'
source_url: https://arxiv.org/abs/2508.09129
tags:
- search
- reasoning
- browsemaster
- agents
- executor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BrowseMaster introduces a planner-executor agent pair to address
  limitations in LLM-based web browsing, where slow serial querying and noisy inputs
  disrupt reasoning. The planner formulates search strategies while the executor performs
  targeted, code-driven tool execution to retrieve relevant evidence efficiently.
---

# BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair

## Quick Facts
- **arXiv ID**: 2508.09129
- **Source URL**: https://arxiv.org/abs/2508.09129
- **Reference count**: 11
- **Primary result**: Achieves 30.0 accuracy on BrowseComp-en and 46.5 on BrowseComp-zh, outperforming baselines

## Executive Summary
BrowseMaster addresses key limitations in LLM-based web browsing by introducing a Planner-Executor agent pair that separates reasoning from execution. The system enables scalable information seeking through code-driven tool execution while maintaining coherent reasoning. Experimental results demonstrate superior performance on English and Chinese benchmarks compared to both open-source and proprietary baselines.

## Method Summary
BrowseMaster implements a two-agent framework where a Planner (DeepSeek-R1-0528) formulates search strategies and an Executor (DeepSeek-R1) performs targeted code-driven tool execution. The Planner handles task decomposition with a 64k token limit and temperature of 0.6, while the Executor operates in a stateful sandbox environment. The system uses web_search and web_parse tools along with three primitives: generate_keywords, batch_search, and check_condition. A confidence-guided replanning mechanism ensures effective exploration across thousands of web pages.

## Key Results
- Achieves 30.0 accuracy on BrowseComp-en benchmark
- Achieves 46.5 accuracy on BrowseComp-zh benchmark
- Outperforms both open-source and proprietary baselines on multiple benchmarks including GAIA and WebWalkerQA

## Why This Works (Mechanism)
The separation of reasoning and execution addresses the fundamental problem of slow serial querying and noisy inputs that disrupt LLM reasoning. By having the Planner formulate strategies independently before execution, the system maintains coherent reasoning while the Executor performs efficient, targeted tool operations. This architecture enables broad exploration through creative, code-based search strategies while preserving scalability across large document collections.

## Foundational Learning
- **Agent Pair Architecture**: Separating Planner and Executor roles prevents reasoning degradation from noisy execution feedback. *Why needed*: Serial querying disrupts LLM reasoning. *Quick check*: Verify Planner outputs remain coherent when Executor is removed.
- **Stateful Code Sandbox**: Persistent execution context across multiple code blocks enables complex multi-step operations. *Why needed*: Maintains variable state for iterative web exploration. *Quick check*: Test variable persistence across three consecutive code executions.
- **Confidence-Guided Replanning**: Dynamic strategy adjustment based on confidence thresholds. *Why needed*: Ensures effective exploration without getting stuck in suboptimal paths. *Quick check*: Verify replanning triggers when confidence drops below threshold.
- **Tool Primitive Design**: Standardized primitives (generate_keywords, batch_search, check_condition) enable systematic web exploration. *Why needed*: Provides consistent interface for complex search strategies. *Quick check*: Test each primitive independently with sample inputs.
- **Parallel Search Capability**: Batch search functionality enables simultaneous exploration of multiple search terms. *Why needed*: Improves efficiency compared to serial querying. *Quick check*: Compare execution time between batch and serial search modes.
- **HTML/PDF Parsing Integration**: Direct content extraction from web pages enables targeted information retrieval. *Why needed*: Bypasses limitations of search engine snippets. *Quick check*: Verify parse output matches actual page content.

## Architecture Onboarding

**Component Map**: User Query -> Planner -> Task Formulation -> Executor -> Code Generation -> Stateful Sandbox -> Tool Execution -> Results -> Confidence Check -> (Replan if needed) -> Answer

**Critical Path**: The execution path from Planner task formulation through Executor code generation to stateful sandbox execution represents the core workflow. Each component must function correctly for successful information retrieval.

**Design Tradeoffs**: The separation of reasoning and execution introduces additional communication overhead but preserves reasoning quality. The stateful sandbox provides powerful execution capabilities but requires careful isolation to prevent state leakage between queries.

**Failure Signatures**: 
- Planner generates incoherent tasks → Executor produces nonsensical code
- Stateful sandbox loses context → Variable references fail
- check_condition primitive overfilters → Correct answers are discarded
- Confidence threshold too high → Excessive replanning and inefficiency

**First Experiments**:
1. Implement minimal Planner-Executor loop with mocked tools to verify interaction format
2. Test stateful sandbox with three consecutive code blocks to validate variable persistence
3. Evaluate check_condition primitive on sample filtering tasks to verify accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on specific LLM API versions and prompts used
- Key primitive implementations (generate_keywords, check_condition) lack detailed specifications
- Evaluation relies on LLM-based judges rather than human annotation, potentially introducing variance
- No ablation studies provided to isolate contribution of individual architectural components

## Confidence

**High Confidence**: Framework architecture and core evaluation methodology are well-specified
**Medium Confidence**: Implementation details for primitives and system prompts require assumptions
**Low Confidence**: Specific numeric parameters like confidence thresholds are not explicitly defined

## Next Checks

1. **Sanity Check Environment Setup**: Implement minimal two-agent loop with mocked tools to verify Planner-Executor interaction format works before integrating real web APIs
2. **Primitive Behavior Validation**: Implement and test check_condition on small examples to verify correct filtering without over-filtering
3. **Token Usage Monitoring**: Log total tokens consumed per query during evaluation on small subset to ensure 64k token limit is not exceeded in practice