---
ver: rpa2
title: 'CHARM: Calibrating Reward Models With Chatbot Arena Scores'
arxiv_id: '2504.10045'
source_url: https://arxiv.org/abs/2504.10045
tags:
- reward
- preference
- bias
- scores
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies model preference bias in reward models (RMs),
  where RMs systematically assign disproportionately high scores to certain policy
  models, distorting rankings and evaluations. To address this, the authors propose
  CHARM, a calibration method that leverages Elo scores from Chatbot Arena to mitigate
  RM overvaluation.
---

# CHARM: Calibrating Reward Models With Chatbot Arena Scores

## Quick Facts
- arXiv ID: 2504.10045
- Source URL: https://arxiv.org/abs/2504.10045
- Reference count: 17
- This paper proposes CHARM, a method to calibrate reward models using Elo scores from Chatbot Arena to reduce model preference bias.

## Executive Summary
This paper identifies model preference bias in reward models (RMs), where RMs systematically assign disproportionately high scores to certain policy models, distorting rankings and evaluations. To address this, the authors propose CHARM, a calibration method that leverages Elo scores from Chatbot Arena to mitigate RM overvaluation. They introduce a Mismatch Degree metric to quantify the bias. The approach is computationally efficient, requiring only a small preference dataset for continued training of the RM. Experiments on reward model benchmarks and human preference alignment show that CHARM improves evaluation accuracy and produces scores more closely aligned with Elo rankings, effectively reducing model preference bias and enhancing alignment with human preferences.

## Method Summary
CHARM calibrates reward models by leveraging Elo scores from Chatbot Arena to correct systematic overvaluation of certain policy models. The method computes a score offset for over-valued models to align empirical win rates with expected human preference rates derived from Elo. This offset is then applied to relabel preference data, which is used to fine-tune the RM. The calibration requires only a small dataset (20K instructions) and one epoch of training, making it computationally efficient. The Mismatch Degree metric quantifies the degree of model preference bias before and after calibration.

## Key Results
- CHARM reduces model preference bias by aligning RM scores with Elo-based expected win rates
- Calibration improves evaluation accuracy on RM-Bench and RewardBench Chat-Hard
- Post-calibration scores show stronger correlation with Elo rankings, indicating better human preference alignment

## Why This Works (Mechanism)

### Mechanism 1: Elo-Calibrated Score Offset
Applying a learned score offset to over-valued model responses aligns empirical RM win rates with expected human preference rates. For an over-valued model π_O and reference π_R, CHARM computes the expected win probability P(O) from Chatbot Arena Elo scores via P(O) = 1/(1 + 10^(Elo_R - Elo_O)/400). It then optimizes an offset Δ by minimizing MSE between the sigmoid-transformed empirical win rate after calibration and P(O). The offset is applied to all responses from the over-valued model before preference label construction.

### Mechanism 2: Mismatch Degree as Bias Diagnostic
The proposed Mismatch Degree metric quantifies model preference bias and correlates with calibration effectiveness. MD(π_O, π_R) = |P̂(O) - P(O)| / max(P(O), 1 - P(O)), where P̂(O) is the RM's empirical win rate and P(O) is the Elo-derived expected rate. Higher MD indicates stronger over- or under-valuation.

### Mechanism 3: Debiased Preference Dataset Construction via Relabeling
Recalibrating preference labels using the score offset before continued RM training produces more human-aligned RMs without changing model architecture. After computing Δ, CHARM applies s'_O = s_O + Δ and rebuilds preference pairs D' = {(x_i, y^+_i, y^-_i)} where y^+_i = argmax(s'_O, s_R). The RM is then fine-tuned on D' with standard Bradley–Terry loss.

## Foundational Learning
- Concept: Bradley–Terry pairwise ranking loss
  - Why needed here: Standard RM training objective; CHARM continues to use this loss after relabeling preferences.
  - Quick check question: Can you write the Bradley–Terry loss for a triplet (x, y+, y-) and explain what the RM is optimizing?

- Concept: Elo rating system and expected win probability
  - Why needed here: CHARM uses Elo scores to define the calibration target (expected win rate) between model pairs.
  - Quick check question: Given two models with Elo scores 1200 and 1000, what is the expected win probability of the higher-rated model?

- Concept: Reward model bias and reward hacking
  - Why needed here: Model preference bias is a form of systematic RM misalignment; understanding reward hacking clarifies why calibration matters.
  - Quick check question: Name two distinct types of RM bias and explain how they differ from model preference bias.

## Architecture Onboarding
- Component map: Instruction sampling -> Response generation from over-valued and reference models -> RM scoring -> Elo lookup -> Offset optimization -> Preference relabeling -> RM fine-tuning

- Critical path:
  1. Select a high-MD model pair (π_O, π_R) to maximize calibration signal.
  2. Generate responses and score with base RM to collect (s_O, s_R).
  3. Compute P(O) from Elo and optimize Δ to align empirical win rate with P(O).
  4. Relabel preferences and fine-tune RM on calibrated dataset.
  5. Evaluate on RM-Bench / RewardBench and AlpacaEval for alignment.

- Design tradeoffs:
  - High-MD pairs yield larger calibration gains but may overfit to specific bias patterns.
  - Smaller calibration datasets are efficient but may miss coverage of diverse prompts.
  - Using a single reference model simplifies computation but limits bias correction to that axis.

- Failure signatures:
  - Post-calibration MD increases → offset may have overcorrected; check Δ magnitude and sign.
  - RM-Bench safety scores drop → calibration may have reduced sensitivity to harmful content patterns.
  - Low-MD models show negative gains after calibration → avoid calibration on already-well-aligned axes.

- First 3 experiments:
  1. Replicate Skywork-RM calibration with Gemma-2-9b-it-SimPO (high MD) vs GPT-4o-mini as reference; validate MD reduction and RM-Bench Chat improvement.
  2. Ablate with a low-MD pair (e.g., Qwen2.5-72B vs same reference) to confirm minimal/negative calibration impact per Table 2.
  3. Evaluate generalization by computing post-calibration win-rate alignment on held-out models (e.g., Qwen2-72B-Instruct, mistral-large-2402) as in Figure 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does using a CHARM-calibrated reward model as a training signal affect the final alignment and performance of policy models in RLHF?
- Basis in paper: [explicit] Section 6 states that evaluating how calibrated RMs impact policy model training in RLHF and RLAIF is a "crucial direction for future research."
- Why unresolved: The paper evaluates the RM's accuracy as a static judge (evaluation) but does not measure its efficacy as a dynamic optimizer (training signal).
- What evidence would resolve it: Training LLMs using RLHF with calibrated vs. uncalibrated RMs and comparing downstream task performance and alignment.

### Open Question 2
- Question: What specific mechanistic factors cause reward models to systematically over-value outputs from models trained with LLM-annotated preference data?
- Basis in paper: [explicit] Section 6 and 7 explicitly call for "further investigation into the underlying mechanisms of model preference bias."
- Why unresolved: The paper identifies the correlation between over-valuation and preference optimization (e.g., SimPO) but does not isolate the root cause.
- What evidence would resolve it: Ablation studies on RM training data composition and architecture to pinpoint the origin of the bias.

### Open Question 3
- Question: Can the CHARM calibration method be effectively adapted for generative or pairwise reward model architectures?
- Basis in paper: [inferred] Section 4.1 defines the method for discriminative RMs, while Section 6 notes the existence of alternative formulations like generative RMs, implying the current approach may not directly transfer.
- Why unresolved: The score offset (Δ) optimization relies on scalar outputs specific to discriminative models, whereas generative models produce text.
- What evidence would resolve it: Modifying the calibration logic for generative verifiers and testing for similar bias reduction effects.

## Limitations
- The calibration method relies on Elo scores from Chatbot Arena, which may not be stable or representative across all domains
- Calibration gains are largest for high-MD pairs but negligible or negative for already well-aligned RMs
- The method assumes the 20K instruction sample is representative of target deployment distribution

## Confidence
- **High Confidence**: Mechanism 1 (Elo-calibrated offset) and Mechanism 2 (Mismatch Degree) are clearly specified and mathematically grounded
- **Medium Confidence**: Mechanism 3 (relabeling + continued training) is well-defined, but the assumption that a single epoch on 20K examples suffices is weakly justified
- **Low Confidence**: The claim that CHARM improves human preference alignment lacks direct human evaluation; RM-Bench and RewardBench improvements are proxy metrics

## Next Checks
1. **Elo Robustness Test**: Evaluate how Elo rankings shift under adversarial or out-of-distribution prompts (e.g., complex reasoning, niche technical domains). If Elo is unstable, rerun CHARM calibration with a fixed, validated Elo snapshot.

2. **Calibration Pair Selection Ablation**: Systematically test calibration on low-MD pairs and ablate the effect of pair selection criteria. Quantify trade-offs between calibration gains and risk of degrading already-aligned RMs.

3. **Human Preference Validation**: Conduct a small-scale user study comparing calibrated vs. uncalibrated RM rankings on a held-out prompt set. Measure alignment with user judgments to validate that proxy metrics (RM-Bench, RewardBench) translate to real-world preference accuracy.