---
ver: rpa2
title: 'FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent
  Collaboration Pipeline'
arxiv_id: '2510.06800'
source_url: https://arxiv.org/abs/2510.06800
tags:
- character
- evaluation
- test
- context
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FURINA introduces a multi-agent collaboration pipeline for constructing
  customizable role-playing benchmarks, addressing limitations of static existing
  benchmarks. It simulates dialogues between test characters and scene characters
  drawn from a curated pool, with an LLM judge selecting evaluation dimensions and
  refining responses.
---

# FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline

## Quick Facts
- **arXiv ID**: 2510.06800
- **Source URL**: https://arxiv.org/abs/2510.06800
- **Reference count**: 40
- **Primary result**: FURINA pipeline constructs customizable RP benchmarks; o3 and DeepSeek-R1 achieve best performance on English/Chinese tasks respectively; reasoning improves RP but increases hallucinations.

## Executive Summary
FURINA introduces a novel multi-agent collaboration pipeline for constructing customizable role-playing benchmarks, addressing limitations of static existing benchmarks. The system simulates dialogues between test characters and scene characters drawn from a curated pool, with an LLM judge selecting evaluation dimensions and refining responses. Using this pipeline, FURINA-Bench was built, featuring both established and synthesized characters in group-chat scenarios, each with fine-grained evaluation criteria. Human evaluation and separability analysis validated the design. Extensive testing of cutting-edge LLMs showed o3 and DeepSeek-R1 achieved the best performance on English and Chinese tasks, respectively. Established characters consistently outperformed synthesized ones, with reasoning capabilities amplifying this gap. A key finding was that reasoning improves RP performance but increases hallucinations, revealing a novel trade-off between RP performance and reliability across models.

## Method Summary
FURINA employs a multi-agent pipeline with Qwen3-235B-A22B as director and scene character model, and GPT-4.1 as both base and judge models. The pipeline simulates dialogues between test characters and scene characters, using a Dynamically Weighted Random Selection algorithm to balance evaluation dimension coverage. For each test turn, source and base models generate candidate responses, which the judge evaluates through chain-of-thought reasoning across five dimensions (Context Reliance, Factual Recall, Reflective Reasoning, Conversational Ability, Preference Alignment). The system continues until each dimension reaches a threshold of 10 instances per character. The resulting benchmark features 20 test characters (10 established, 10 synthesized) across 1,459 dialogues with 7,181 test utterances.

## Key Results
- o3 and DeepSeek-R1 achieved top performance on English and Chinese tasks respectively
- Established characters consistently outperformed synthesized characters
- Reasoning models showed a trade-off: improved RP performance but increased hallucinations
- Performance gap between established and synthesized characters widened for reasoning models

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Director-Simulation-Selection Loop
The system generates high-quality benchmarks by decoupling dialogue progression from response evaluation. A Director Model manages turn-taking in group chat scenarios, while a Judge Model evaluates responses against dynamically selected dimensions before appending the superior response to the history. This separation reduces cross-dimensional interference compared to holistic scoring methods.

### Mechanism 2: Dimension-Aligned Response Strategy
Evaluating specific capabilities requires constraining the generation process to provoke desired behaviors. The pipeline uses Dynamically Weighted Random Selection with targeted dimension-emphasis, prompting scene characters with specific instructions and test characters with response strategies tailored to evaluation dimensions. This increases the density of relevant test cases for each dimension.

### Mechanism 3: The Reasoning-Hallucination Trade-off
Extended "thinking" processes in reasoning LLMs improve role-playing depth at the cost of increased factual hallucinations. Reasoning models utilize aggressive response strategies and extended thinking traces that enhance Reflective Reasoning but simultaneously increase the probability of generating information that contradicts established lore or context.

## Foundational Learning

- **LLM-as-a-Judge & Pairwise Comparison**: FURINA relies on GPT-4.1 as judge for dimension selection and scoring. Understanding judge biases (length, order) and reliability (Pearson correlation with humans) is critical. *Quick check*: How does the system handle order bias when comparing source vs base models? (Uses bidirectional comparison and averages scores).

- **Role-Playing Evaluation Dimensions**: The benchmark segments performance into Context Reliance, Factual Recall, Reflective Reasoning, Conversational Ability, and Preference Alignment. Interpreting the Pareto frontier requires distinguishing between "faithfulness" (CR) and "lore accuracy" (FR). *Quick check*: Which dimension measures "faithfulness" to prompt context vs "factuality" of world knowledge? (Context Reliance vs Factual Recall).

- **Established vs. Synthesized Characters**: The pipeline handles characters with pre-training knowledge (established) and prompt-defined characters (synthesized). Hallucination detection differs: for established characters, it checks against pre-training facts; for synthesized, against prompt context. *Quick check*: Why does the performance gap widen for reasoning models? (Reasoning models prefer leveraging pre-existing knowledge over following new prompt instructions).

## Architecture Onboarding

- **Component map**: Character-Scene Pool -> Director Model (Qwen3-235B) -> Source & Base Models -> Judge Model (GPT-4.1) -> DWRS Algorithm

- **Critical path**: 1. Define Test Character and select Scenario from Pool. 2. Director initiates dialogue with Scene Characters. 3. Test Character called to speak. 4. Source and Base models generate candidates using specific strategy prompts. 5. Judge selects best response via CoT reasoning and 5-point scoring. 6. Dialogue history updates with winning response; repeat until threshold met.

- **Design tradeoffs**: Using strong Base Model (GPT-4.1) ensures high floor but may depress relative scores of other strong models. Allowing arbitrary character definitions maximizes flexibility but relies heavily on Director/Judge for consistency. Using thinking modes maximizes RP performance but pushes toward hallucination edge of Pareto frontier.

- **Failure signatures**: Red-Teaming Blocks (Gemini triggering safety filters), Hallucination Cascade (errors compounding into context divergence), Stagnation (Director looping between scene characters).

- **First 3 experiments**: 1. Validation Run: Test known established character (e.g., Harry Potter) and verify Judge correctly identifies Factual Recall errors vs Context Reliance successes. 2. Ablation on Reasoning: Compare Standard Model vs Reasoning counterpart, plot difference in RR score vs Hallucination Rate to verify trade-off. 3. Dimension Balance Check: Run pipeline for 100 turns, histogram selected dimensions, verify DWRS prevents any single dimension from dominating.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can LLMs achieve simultaneous gains in role-playing performance and reliability to surpass the existing Pareto frontier?
- **Basis**: The Conclusion and Section 5.4 explicitly call for resolving the constraining Pareto frontier where improving RP performance currently comes at the cost of increased hallucinations.
- **Why unresolved**: The authors find that while reasoning models excel at RP, they simultaneously increase hallucinations, establishing a trade-off current architectures cannot bridge.
- **Evidence needed**: Training methodologies that decouple performance scaling from hallucination rates, demonstrated by high scores in both metrics on FURINA-Bench.

### Open Question 2
- **Question**: How can reasoning mechanisms be refined to improve instruction-following for synthesized characters rather than relying on pre-existing parametric knowledge?
- **Basis**: Section 5.3 notes reasoning capabilities amplify the disparity between established and synthesized characters because reasoning mechanisms weaken instruction-following abilities needed for novel personas.
- **Why unresolved**: The paper identifies that reasoning models prefer pre-existing character knowledge over prompt-provided details but does not offer a mechanism to force strict adherence to new character definitions.
- **Evidence needed**: A modification to reasoning process or attention mechanism that significantly narrows the performance gap between established and synthesized characters.

### Open Question 3
- **Question**: To what extent does training data composition, as opposed to model scale, contribute to the rate of role-playing hallucinations?
- **Basis**: Section 5.4 observes that model scale does not monotonically reduce hallucinations and hypothesizes training data composition may play a more critical role.
- **Why unresolved**: While identifying a non-monotonic trend in hallucination rates across model sizes, the paper leaves specific causal factors within training data unverified.
- **Evidence needed**: An ablation study on training data subsets demonstrating direct causal link between specific data curation strategies and reduction of EC or SC hallucinations.

## Limitations
- Reliance on GPT-4.1 as both base and judge models creates potential circular validation issues
- DWRS algorithm's specific implementation details for dimension balancing remain underspecified
- Trade-off between reasoning performance and hallucination rates may be dataset-specific rather than universal

## Confidence
- **High Confidence**: Pipeline architecture and multi-agent collaboration mechanism are clearly specified and validated through separability analysis. Established vs. synthesized character performance gap is consistently observed.
- **Medium Confidence**: Reasoning-hallucination trade-off finding is supported by empirical data but requires further validation across diverse datasets and model architectures.
- **Low Confidence**: Generalizability of dimension selection mechanism to other RP domains beyond literary character pool has not been tested.

## Next Checks
1. **Cross-Domain Generalization Test**: Apply FURINA pipeline to non-literary character pool (e.g., historical figures or professional personas) and verify whether established/synthesized performance gap and reasoning-hallucination trade-off persist.

2. **Base Model Ablation Study**: Repeat evaluation using weaker base models (e.g., GPT-3.5) to assess whether current results are inflated by strong GPT-4.1 reference point, and measure changes in relative performance ordering.

3. **Long-Turn Stability Analysis**: Extend simulations beyond current 10-turn threshold to 50+ turns and measure how hallucination rates compound over time, particularly for reasoning models, to quantify temporal dynamics of observed trade-off.