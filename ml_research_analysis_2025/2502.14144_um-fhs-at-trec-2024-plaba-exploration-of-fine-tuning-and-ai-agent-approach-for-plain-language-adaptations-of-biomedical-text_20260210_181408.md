---
ver: rpa2
title: 'UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach
  for plain language adaptations of biomedical text'
arxiv_id: '2502.14144'
source_url: https://arxiv.org/abs/2502.14144
tags:
- sentence
- adaptations
- gpt-4o-mini
- used
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents submissions to the TREC 2024 PLABA track,
  focusing on simplifying biomedical abstracts for K8-level readers (13-14 years old)
  using OpenAI''s GPT-4o and GPT-4o-mini models. Three approaches were tested: baseline
  prompt engineering, a two-AI agent iterative improvement method, and fine-tuning.'
---

# UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach for plain language adaptations of biomedical text

## Quick Facts
- arXiv ID: 2502.14144
- Source URL: https://arxiv.org/abs/2502.14144
- Reference count: 26
- Primary result: GPT-4o-mini with baseline prompt engineering outperformed iterative refinement and fine-tuned models on PLABA biomedical text simplification task

## Executive Summary
This paper presents three approaches to simplify biomedical abstracts for K8-level readers (13-14 years old) using OpenAI's GPT-4o and GPT-4o-mini models. The approaches tested include baseline prompt engineering, a two-AI agent iterative improvement method, and fine-tuning. The study found that prompt engineering with GPT-4o-mini achieved the best quantitative results, while fine-tuned models excelled in accuracy and completeness but produced outputs that were too complex for the K8 target audience.

## Method Summary
The study used three approaches on the PLABA dataset of 750 biomedical abstracts: (1) baseline prompt engineering with structured system and baseline prompts, (2) two-AI agent iterative refinement where Agent 2 asks clarification questions and Agent 1 integrates responses, and (3) fine-tuning on 80% of the data using specific hyperparameters (epochs=3, batch_size=1, LR_multiplier=2). Evaluation combined qualitative Likert scoring with quantitative readability metrics (FK grade level and SMOG index).

## Key Results
- Prompt engineering with GPT-4o-mini achieved 87.18% final average in official rankings
- Fine-tuned models produced outputs clustered near ground truth complexity (FK ~12.2) rather than target K8 level
- Two-agent iterative approach underperformed baseline by ~5% on accuracy and simplicity metrics
- GPT-4o-mini provided better balance between simplicity (FK 8.93) and completeness (4.42/5) compared to GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on human adaptations propagates complexity patterns from training data rather than enforcing target simplicity constraints. FT models learn statistical patterns from ground truth (FK mean 11.64, SMOG 13.8), which exceeded the K8 target. The training/validation loss convergence (gpt-4o: 1.099→0.8336, gpt-4o-mini: 1.0489→0.967) indicates successful pattern learning—but those patterns encoded K12-K14 complexity, not K8 simplicity.

### Mechanism 2
Two-agent iterative refinement does not guarantee improvement when critique signals lack actionable specificity. Agent 2 generates clarification questions; Agent 1 integrates responses. However, vague persona constraints yield low-signal feedback that fails to consistently identify highest-impact edits.

### Mechanism 3
Smaller model with explicit guideline-following prompts achieves better simplicity-accuracy trade-offs than larger models with open-ended refinement. GPT-4o-mini_baseline followed structured adaptation guidelines without over-explaining, while GPT-4o achieved lowest FK (7.4) but sacrificed completeness (3.45/5).

## Foundational Learning

- **Readability metrics (FK, SMOG) vs. human judgment**
  - Why needed here: Quantitative scores showed gpt-4o_baseline as simplest, but qualitative evaluation revealed completeness loss; understanding this divergence is critical for system design.
  - Quick check question: If FK=7.4 but completeness=3.45/5, what information is being sacrificed, and is that acceptable for your use case?

- **Fine-tuning distribution alignment**
  - Why needed here: FT models converged to training data complexity (K12-K14) rather than target (K8); practitioners must audit training data before FT.
  - Quick check question: Before fine-tuning on a simplification task, have you measured readability scores of your training targets against your goal?

- **Agent critique signal quality**
  - Why needed here: Two-agent refinement failed to improve over baseline; understanding why helps design better multi-agent systems.
  - Quick check question: Does your critique agent produce actionable, specific feedback or vague suggestions? Can you quantify the edit impact?

## Architecture Onboarding

- **Component map**: Input processor -> Prompt constructor -> Model layer -> (optional: agent loop) -> Output adaptations -> Evaluator
- **Critical path**: Input sentences → Prompt construction → Model inference → (optional: agent loop) → Output adaptations → Evaluation. The two-agent path adds 2 additional API calls per sentence set.
- **Design tradeoffs**:
  - FT vs. prompting: FT improves accuracy/completeness but sacrifices simplicity; prompting with guidelines better controls output characteristics.
  - GPT-4o vs. GPT-4o-mini: 4o achieves lower FK but worse completeness; 4o-mini balances both.
  - Agent refinement: Adds cost and latency without consistent quality gains; useful only if critique can be made actionable.
- **Failure signatures**:
  - FT outputs too complex: Training data FK/SMOG exceeds target; re-filter training data or add simplicity constraints to FT objective.
  - Agent refinement degrades quality: Student persona asks irrelevant questions; constrain critique to specific edit types.
  - Baseline over-simplifies: gpt-4o_baseline FK=7.4 but completeness=3.45; add completeness checks to prompt or post-hoc filter.
- **First 3 experiments**:
  1. Audit training data: Compute FK/SMOG for ground truth adaptations; filter to K8-compliant subset before FT.
  2. Constrain agent critique: Replace open-ended questions with checklist-based feedback (e.g., "Is medical term X explained? Y/N → suggest replacement").
  3. Hybrid approach: Use GPT-4o-mini_baseline for draft; apply FT model only on sentences flagged as accuracy-critical by a classifier.

## Open Questions the Paper Calls Out

### Open Question 1
Why did baseline prompt engineering with GPT-4o-mini outperform the two-AI agent iterative improvement strategy in official PLABA 2024 rankings? The iterative improvement approach was expected to refine outputs through agent interaction, yet the baseline achieved 87.18% final average versus 83.63% for two-agent approach, particularly underperforming in accuracy and simplicity by ~5%.

### Open Question 2
How do open-source local LLMs (e.g., Llama 3.3) compare to proprietary OpenAI models for biomedical text simplification while ensuring GDPR compliance for private healthcare data? The study exclusively used GPT-4o and GPT-4o-mini via API with DPA, but local deployment may be necessary for sensitive healthcare applications.

### Open Question 3
Are the newer LLMs contaminated with the public PLABA benchmark dataset, and how does this affect reported performance? The PLABA dataset is public and may have been included in training data for GPT-4o models, potentially inflating performance metrics.

### Open Question 4
Can semantic similarity metrics (BERTScore) and LLM-based evaluation metrics (GPTScore, G-Eval) better capture the quality of plain language adaptations than traditional readability scores? FK grade level and SMOG index showed ground truth at K12-K14 level despite K8 guidelines, suggesting these metrics may not fully capture adaptation quality.

## Limitations

- Evaluation constrained by PLABA dataset size (750 samples), limiting generalizability to other biomedical domains
- Fine-tuning experiments used relatively small dataset without cross-validation, potentially leading to overfitting
- Two-agent approach interaction protocol lacks precise specification, making replication challenging
- Evaluation relied primarily on quantitative readability metrics that may not fully capture K8-level comprehension requirements

## Confidence

- **High Confidence**: Prompt engineering with GPT-4o-mini outperformed iterative refinement and fine-tuning on quantitative metrics
- **Medium Confidence**: Larger models over-simplify at expense of completeness while smaller models with structured prompts provide better balance
- **Low Confidence**: Specific mechanism explaining why fine-tuning propagates complexity patterns from training data

## Next Checks

1. **Dataset Generalization Test**: Apply prompt engineering approach to different biomedical text simplification dataset (e.g., MedSTS or BioASQ) to verify performance consistency across domains
2. **Fine-tuning Optimization**: Experiment with additional fine-tuning parameters and curriculum learning approaches to better control output simplicity while maintaining accuracy
3. **Critique Quality Analysis**: Implement structured evaluation metrics for agent feedback quality to determine if two-agent approach can be optimized through improved critique design