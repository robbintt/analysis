---
ver: rpa2
title: 'DyFlow: Dynamic Workflow Framework for Agentic Reasoning'
arxiv_id: '2509.26062'
source_url: https://arxiv.org/abs/2509.26062
tags:
- dyflow
- reasoning
- designer
- answer
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DyFlow is a dynamic workflow generation framework that adaptively
  constructs and adjusts reasoning procedures based on task requirements and real-time
  intermediate feedback, enhancing cross-task generalization. It consists of a high-level
  designer that generates and revises subgoal plans, and a low-level executor that
  carries out the subgoals using dynamic operators with context-aware parameterization.
---

# DyFlow: Dynamic Workflow Framework for Agentic Reasoning

## Quick Facts
- arXiv ID: 2509.26062
- Source URL: https://arxiv.org/abs/2509.26062
- Reference count: 40
- DyFlow is a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, enhancing cross-task generalization.

## Executive Summary
DyFlow introduces a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback. The framework consists of a high-level designer that generates and revises subgoal plans, and a low-level executor that carries out the subgoals using dynamic operators with context-aware parameterization. The designer is trained via a two-phase learning strategy combining knowledge distillation and self-play preference optimization. Experiments across five reasoning domains show DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains.

## Method Summary
DyFlow employs a designer-executor architecture where the designer generates stage subgraphs representing subgoals and the executor carries out operations using dynamic operators. The designer is trained through a two-phase process: initial supervised fine-tuning (SFT) on expert trajectories from GPT-4.1, followed by knowledge transfer optimization (KTO) using self-generated trajectories labeled by success/failure. The executor uses Phi-4 with temperature 0.01 to run operator instances, while a GPT-4o-mini summarizer condenses execution states for the designer. The framework maintains a memory buffer for intermediate outputs and supports dynamic replanning based on execution feedback.

## Key Results
- DyFlow achieves significant performance gains across five reasoning domains (social reasoning, biomedical tasks, mathematical problem solving, and code generation)
- Outperforms existing baselines with substantial Pass@k improvements
- Exhibits robust cross-task generalization, maintaining performance when transferred between domains
- Lightweight DyPlanner model achieves comparable results to larger proprietary designers at ~50x lower cost

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Driven Replanning at Subgoal Level
The designer maintains full execution state including task specification, prior plans, partial outputs, and error signals. At each step, it generates revised stage subgraphs conditioned on this context, enabling course correction when prior steps fail. This dynamic revision improves task success rates over static or one-shot planning. The core assumption is that intermediate feedback contains actionable signals the designer can interpret. Break condition: When feedback signals are noisy or the designer lacks training examples for recovery patterns.

### Mechanism 2: Context-Aware Operator Instantiation
Each operator instance combines a template with a fine-grained instruction tailored to current context and input keys referencing the global memory buffer. This allows the same template to behave differently based on available information and active subgoal. The core assumption is that the designer has learned to generate appropriate instruction-input pairs matching operator capabilities to reasoning needs. Break condition: When the memory buffer contains contradictory or stale entries, or when instructions are underspecified.

### Mechanism 3: Two-Phase Designer Training via Trajectory-Level Supervision
The training combines supervised distillation with offline preference optimization, using trajectory-level success/failure labels rather than step-level rewards. Phase 1 (SFT) initializes via distillation from expert trajectories; Phase 2 (KTO) refines via self-generated trajectories. The core assumption is that trajectory-level labels provide sufficient learning signal despite sparse supervision. Break condition: If self-play trajectories lack diversity or success rates are too low, the preference dataset becomes imbalanced.

## Foundational Learning

- **Markov Decision Processes (states, policies, trajectories)**: DyFlow formalizes reasoning as sequential decision-making where states evolve through actions. Understanding value functions and Bellman backups is necessary to interpret convergence guarantees. Quick check: Can you explain why a static policy is a special case of a dynamic policy in this framework?

- **Preference Optimization (DPO, KTO)**: The designer is trained via KTO using trajectory-level preferences rather than traditional RL. Distinguishing pairwise ranking (DPO) from prospect-theoretic optimization (KTO) clarifies design choices. Quick check: Why might trajectory-level labels be preferable to step-level rewards for training a planner?

- **Knowledge Distillation**: Phase 1 distills planning capabilities from GPT-4.1 trajectories into Phi-4. Understanding teacher-student training helps interpret why DyPlanner matches larger proprietary designers. Quick check: What types of planning knowledge transfer effectively via distillation vs. require self-play refinement?

## Architecture Onboarding

- **Component map**: Task specification -> State Summarizer -> Designer -> Stage Subgraphs -> Executor -> Memory Buffer -> Execution State update -> Termination check

- **Critical path**: 1) Initialize state with task and empty memory 2) Summarize: z_t = f_summary(s_t) 3) Plan: Designer samples G_t ~ π_θ(· | z_t) 4) Execute: For each operator instance, retrieve inputs from memory, run via executor, store output 5) Update: s_{t+1} ← UpdateState(s_t, G_t, M) 6) Check termination; repeat or halt

- **Design tradeoffs**: Dynamic vs. static planning gains 10.69 absolute improvement on SocialMaze at cost of 1.4-3x inference tokens. Lightweight vs. proprietary designer: DyPlanner achieves 61.45 avg vs. 61.16 (Claude-3.7-Sonnet) at ~50x lower cost. Trajectory-level vs. step-level supervision: More stable learning but requires sufficient self-play exploration.

- **Failure signatures**: Infinite loops (designer repeatedly generates similar subgraphs), memory corruption (contradictory entries cause incoherent outputs), premature termination (designer signals TERMINATE before completion), cascading errors (one failure propagates through dependent nodes).

- **First 3 experiments**: 1) Reproduce cross-domain generalization: Train on MATH+PubMedQA+LiveBench, evaluate zero-shot on HumanEval and SocialMaze. 2) Ablate training phases: Compare SFT-only, KTO-only, and full two-phase training. 3) Test executor compatibility: Run same DyPlanner with GPT-4o-mini, Phi-4, and GPT-4.1-mini executors.

## Open Questions the Paper Calls Out

1. **External tool integration**: How can DyFlow's framework be adapted to incorporate external tool usage (e.g., web search, database queries) without disrupting the current symbolic and textual reasoning workflow? The paper explicitly lists this as a limitation and future work direction.

2. **Dynamic operator evolution**: Can the set of operator templates be evolved dynamically during the self-play phase rather than remaining a fixed set defined a priori? The methodology defines operator templates as a finite set, and the conclusion suggests expanding the operator set as future work.

3. **Summarizer dependency**: To what degree does the performance of the lightweight DyPlanner rely on the quality and specific model choice of the summarization function? The methodology describes using GPT-4o-mini to condense states, but it's unclear if the "intelligence" resides primarily in the planner or the summarizer.

## Limitations
- Heavy reliance on trajectory-level supervision may limit learning nuanced recovery strategies when partial progress occurs but overall trajectories fail
- Performance heavily depends on quality and diversity of the memory buffer, but limited analysis of memory management strategies
- Framework's effectiveness with noisy or ambiguous feedback signals is not thoroughly validated

## Confidence
- **High Confidence**: Cross-domain generalization performance claims - supported by comprehensive benchmarking and ablation studies
- **Medium Confidence**: Two-phase training superiority claim - Table 4 shows phase ablation drops performance, but self-play generation process lacks detailed validation
- **Low Confidence**: Operator instantiation mechanism claims - limited empirical analysis of how context-aware parameterization specifically contributes to performance gains

## Next Checks
1. **Robustness to noisy feedback**: Systematically inject controlled noise into intermediate execution signals and measure DyFlow's recovery ability versus baseline planners.

2. **Memory consistency stress test**: Design scenarios where memory buffer contains conflicting entries and measure how often DyFlow produces coherent versus contradictory outputs.

3. **Cross-executor generalization**: Train DyPlanner once and evaluate across 3-4 different executor models with varying capabilities to determine if the designer's planning quality transfers across executor skill levels.