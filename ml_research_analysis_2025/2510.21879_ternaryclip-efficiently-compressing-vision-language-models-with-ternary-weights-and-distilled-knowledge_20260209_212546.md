---
ver: rpa2
title: 'TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights
  and Distilled Knowledge'
arxiv_id: '2510.21879'
source_url: https://arxiv.org/abs/2510.21879
tags:
- ternaryclip
- quantization
- ternary
- performance
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TernaryCLIP, a framework that applies ternary\
  \ quantization (weights restricted to {\u22121, 0, +1}) to both vision and text\
  \ encoders of CLIP models, combined with knowledge distillation to preserve performance.\
  \ The method uses quantization-aware training and a ternary-aware distillation loss\
  \ that aligns student outputs with a pre-trained teacher across contrastive, interactive,\
  \ and feature-based objectives."
---

# TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge

## Quick Facts
- **arXiv ID**: 2510.21879
- **Source URL**: https://arxiv.org/abs/2510.21879
- **Reference count**: 40
- **Primary result**: 99% ternarized weights, 1.58-bit representation, 16.98× compression ratio, 2.3× inference speedup, 16× storage reduction, 10× memory savings, 60% sparsity, maintaining competitive zero-shot accuracy within 2-3% of full-precision CLIP

## Executive Summary
TernaryCLIP introduces a framework for compressing vision-language models by applying ternary quantization (weights restricted to {-1, 0, +1}) to both vision and text encoders of CLIP models, combined with knowledge distillation to preserve performance. The method uses quantization-aware training with adaptive scaling and a ternary-aware distillation loss that aligns student outputs with a pre-trained teacher across contrastive, interactive, and feature-based objectives. Experiments on 41 datasets demonstrate that ultra-low-bit multimodal models are feasible and effective for resource-constrained deployment.

## Method Summary
TernaryCLIP compresses CLIP models by ternarizing weights during training using an adaptive scaling mechanism and Straight-Through Estimator for gradient flow. The framework combines three distillation losses (CRD, ICL, FD) with the standard contrastive task loss, training the ternary student model to mimic a full-precision teacher. The approach achieves 99% ternary weights with 1.58-bit representation through quantization-aware training on Conceptual Captions 12M, followed by evaluation on 37 classification and 3 retrieval datasets.

## Key Results
- **Compression efficiency**: 16.98× compression ratio with 99% ternarized weights achieving 1.58-bit representation
- **Performance preservation**: Maintains competitive zero-shot accuracy within 2-3% of full-precision CLIP baselines across 41 datasets
- **Hardware efficiency**: Achieves 2.3× inference speedup, 16× storage reduction, 10× memory savings, and 60% sparsity

## Why This Works (Mechanism)

### Mechanism 1: Ternarization with Adaptive Scaling
Converting weights to {-1, 0, +1} with learned scaling preserves representational capacity while achieving ~16× compression. An adaptive scaling factor γ = β·(Σ|W_ij|)/(nm) normalizes weight magnitudes before quantization, allowing floating-point multiplications to be replaced with additions and shifts at inference.

### Mechanism 2: Straight-Through Estimator (STE) for Gradient Flow
STE enables backpropagation through non-differentiable quantization by approximating gradients as identity. During forward pass, ternary weights T are used; during backward pass, ∂L_ternary/∂W ← STE(∂L_ternary/∂T), treating the round operation as if it were the identity function.

### Mechanism 3: Ternarization-Aware Distillation Loss Combination
Three complementary distillation objectives transfer knowledge from full-precision teacher to ternary student more effectively than task loss alone. CRD (KL divergence on similarity distributions), ICL (cross-modal contrastive relationships), and FD (MSE on embeddings) are combined with heavily weighted feature distillation (λ_fd = 2000.0).

## Foundational Learning

- **Concept: CLIP Contrastive Learning**
  - Why needed: TernaryCLIP preserves the contrastive objective during quantization-aware training
  - Quick check: Can you explain why maximizing cosine similarity between matched image-text pairs while minimizing it for unmatched pairs enables zero-shot classification?

- **Concept: Knowledge Distillation Fundamentals**
  - Why needed: The paper combines three distillation strategies to recover accuracy
  - Quick check: Why would a teacher model's probability distribution over classes contain more information than the argmax class label?

- **Concept: Quantization-Aware Training vs. Post-Training Quantization**
  - Why needed: TernaryCLIP uses QAT, not PTQ, explaining performance advantages
  - Quick check: What is the fundamental difference between quantizing weights after training vs. simulating quantization during every forward pass?

## Architecture Onboarding

- **Component map**: Ternarization module → Forward pass with T → STE backward → Distillation head (CRD, ICL, FD) + Task head (contrastive loss)
- **Critical path**: Load pretrained teacher and student → Initialize weights → For each batch: ternarize → forward → compute L_task + L_distill → STE backward → update W → export final T and γ
- **Design tradeoffs**: Q-ALL (99% quantized) trades ~0.9% accuracy for 5× more compression vs Q-FFN; self-distillation acceptable for training simplicity (1.35% degradation); λ_fd=2000 heavily weights feature alignment
- **Failure signatures**: Loss oscillation (normal, Figure 6); accuracy collapse below PTQ baselines (check STE, λ balance, teacher loading); no sparsity in final weights (increase β)
- **First 3 experiments**: 1) Baseline sanity check on CC12M subset for 10 epochs; 2) Ablate distillation components (task only, task+CRD, full); 3) Scaling factor sensitivity sweep (β ∈ {0.5, 1.0, 2.0, 3.0})

## Open Questions the Paper Calls Out

1. **Activation quantization integration**: Can INT8 or lower quantization be successfully integrated with weight ternarization without training instability or performance degradation?
2. **Domain adaptation limitations**: How can the reduced adaptability of ternarized models to domain-specific applications be mitigated without expensive full retraining?
3. **Modality generalization**: Can this ternary quantization framework be generalized to other modalities (video, audio) or different architectures beyond CLIP image-text encoders?

## Limitations

- **Domain shift sensitivity**: Ternarization-aware distillation may reduce adaptability to domain-specific applications compared to full-precision models
- **Unverified activation quantization**: The framework currently leaves activations in FP16 format, missing potential efficiency gains from joint weight-activation quantization
- **Specialized dataset performance**: The 2-3% accuracy drop versus baselines may widen for specialized domains or extreme long-tail distributions not covered in standard benchmarks

## Confidence

- **High Confidence**: Compression ratios (16.98×), storage reductions (16×), and sparsity metrics (60%) are mathematically verifiable; STE mechanism is well-established
- **Medium Confidence**: 99% ternary weight achievement and 1.58-bit representation depend on hyperparameter sensitivity requiring extensive tuning; 2.3× speedup assumes hardware-specific optimizations
- **Low Confidence**: Zero-shot transfer performance claims assume effective knowledge transfer across all 41 datasets; potential dataset-specific failure modes or domain shift effects remain unanalyzed

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate TernaryCLIP on specialized vision-language datasets (medical imaging, satellite imagery, fine-grained species classification) to verify the 2-3% accuracy drop generalizes beyond standard benchmarks
2. **STE Approximation Validation**: Implement exact gradient computation through ternary quantization and compare convergence trajectories and final accuracy against current STE approach
3. **Hardware Profiling**: Measure actual inference speedup and energy efficiency on target edge devices (ARM-based systems, mobile GPUs) to validate claimed 2.3× speedup and confirm ternary operations provide expected computational advantages