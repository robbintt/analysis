---
ver: rpa2
title: Comparing LLMs for Sentiment Analysis in Financial Market News
arxiv_id: '2510.15929'
source_url: https://arxiv.org/abs/2510.15929
tags:
- used
- sentiment
- dataset
- which
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares multiple machine learning models for sentiment
  analysis of financial market news. It evaluates both classical models (SVM, Random
  Forest, MLP) and large language models (LLMs) like Gemini, DeBERTa, and BART across
  three financial datasets.
---

# Comparing LLMs for Sentiment Analysis in Financial Market News

## Quick Facts
- **arXiv ID:** 2510.15929
- **Source URL:** https://arxiv.org/abs/2510.15929
- **Authors:** Lucas Eduardo Pereira Teles; Carlos M. S. Figueiredo
- **Reference count:** 1
- **Primary result:** Large language models (especially Gemini and DeBERTa) outperform classical ML models for financial sentiment analysis across three financial datasets.

## Executive Summary
This paper compares classical machine learning models (SVM, Random Forest, MLP) with large language models (LLMs) like Gemini, DeBERTa, and BART for sentiment analysis of financial market news. The study evaluates these models across three financial datasets with varying data sizes and text formats. Results show that LLMs consistently outperform classical models, with Gemini achieving the highest overall accuracy across all datasets. The research highlights that classical models perform better with larger training datasets, while LLMs demonstrate strong performance even with limited data. The paper suggests future work could involve optimizing classical model hyperparameters and combining sentiment analysis with time series forecasting for financial applications.

## Method Summary
The study evaluates sentiment classification on three financial datasets: Financial Phrase Bank (FPB), StockEmotions (social media with emojis), and Tweet Financial News (TFN). Classical models (SVM, Random Forest, MLP) use TF-IDF vectorization with SVD dimensionality reduction (500 components) after cleaning and undersampling to balance classes. LLMs employ zero-shot classification via prompt templates. The paper uses 80/20 train/test splits with 20% of training data reserved for validation. Hyperparameters for classical models are specified in tables, with MLP using batch size 32, EarlyStopping (patience=10), and ReduceLROnPlateau (factor=0.5, patience=5).

## Key Results
- Gemini achieved the highest overall accuracy (80.44% on FPB, 74.10% on StockEmotions, 78.95% on TFN)
- LLMs outperformed classical models in most cases across all three datasets
- Classical models showed competitive performance (~77%) only on the larger StockEmotions dataset
- DeBERTa demonstrated strong performance as an open-source alternative to Gemini
- The performance gap was most pronounced on smaller datasets (FPB, TFN)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot LLMs outperform classical models on limited-data financial tasks by bypassing information loss from manual preprocessing pipelines.
- Core assumption: The performance gap is driven by data fidelity and feature retention rather than purely by algorithmic complexity.
- Evidence: Classical models required aggressive undersampling and dimensionality reduction, while LLMs ingested raw text, likely preserving semantic cues discarded during vectorization.

### Mechanism 2
- Claim: Performance variance across datasets suggests classical models are more sensitive to training data volume than zero-shot LLMs.
- Core assumption: LLM pre-training includes sufficient financial context to enable generalization without task-specific training.
- Evidence: Classical models achieved competitive accuracy on StockEmotions (8,000 training records) but failed on smaller datasets, implying they require sufficient statistical density to learn financial sentiment boundaries.

### Mechanism 3
- Claim: Generative LLMs handle short-context "headline" data better than classical frequency-based models due to deeper syntactic awareness.
- Core assumption: Financial headlines rely on implicit context or entity recognition which TF-IDF fails to capture but Transformers handle natively.
- Evidence: Gemini maintained high performance (78.95%) on TFN headlines, while classical models relying on TF-IDF struggled with sparsity in short text.

## Foundational Learning

- Concept: **TF-IDF Vectorization vs. Embeddings**
  - Why needed here: To understand why classical models required dimensionality reduction while LLMs processed raw text, and why sparsity in short headlines degraded classical performance.
  - Quick check question: How does a bag-of-words model represent the word "rally" compared to a Transformer embedding in the context of "stock rally" vs. "political rally"?

- Concept: **Class Imbalance & Undersampling**
  - Why needed here: The study artificially balanced datasets for classical models, introducing bias and data loss. Understanding this is critical to interpreting why LLMs appeared superior.
  - Quick check question: If you train a classifier on a dataset where 90% of samples are "Neutral," what is the risk of using simple accuracy as a metric?

- Concept: **Zero-Shot Classification**
  - Why needed here: The superior results for Gemini/Gemma were achieved without training. Distinguishing this from fine-tuning is necessary to replicate the architecture correctly.
  - Quick check question: Can a zero-shot model learn a new proprietary sentiment category (e.g., "uncertainty") that did not exist in its pre-training data simply via a prompt?

## Architecture Onboarding

- Component map: Cleaner -> Stopword Removal -> Balancer -> TF-IDF Vectorizer -> SVD (500 dims) -> SVM/RF/MLP (Classical Branch)
- Component map: Raw Text -> Prompt Template ("Classify as...") -> Gemini/DeBERTa -> Sentiment Label (LLM Branch)
- Critical path: The preprocessing pipeline for the Classical Branch is the highest risk for data leakage and information loss (specifically the Balancer and SVD steps). For the LLM branch, Prompt Engineering is the critical variable defining success.
- Design tradeoffs:
  - Undersampling vs. Class Weights: The paper chose undersampling for speed/feasibility, sacrificing accuracy on minority classes. A production system should prefer class weights or focal loss to retain data.
  - LLM Cost vs. Accuracy: Gemini provided the best consistency but is a paid API model. DeBERTa is open-source but required fine-tuning/infrastructure. Classical models are cheap but inaccurate on low-data regimes.
- Failure signatures:
  - Classical Models: High variance and poor F1-scores on "Neutral" classes in FPB/TFN due to data sparsity and overfitting to frequent words after undersampling.
  - LLM Models: "Gemma" showed instability in detecting "Neutral" classes (0.02 Recall on FPB), suggesting prompt sensitivity or model capacity limits.
- First 3 experiments:
  1. Implement the TF-IDF + SVM pipeline on the unbalanced FPB dataset using `class_weight='balanced'` to isolate the impact of the paper's undersampling choice.
  2. Run Gemini on the TFN dataset using 3 distinct prompt styles to measure variance.
  3. Train the MLP on only the TFN data vs. FPB data to quantify the performance drop of classical methods as context length decreases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating sentiment analysis from LLMs like Gemini with time series models improve the accuracy of financial market forecasting?
- Basis: The conclusion states future work could involve "aligning these results with a time series forecast to better monitor stock/fund variations."
- Why unresolved: The current study evaluated sentiment classification accuracy in isolation and did not test the utility of these sentiment scores as features in downstream predictive models.

### Open Question 2
- Question: Can classical models match the performance of LLMs on financial sentiment tasks if trained on larger datasets without aggressive undersampling?
- Basis: The authors note that future work should "use more example records in the classic models" and observe that classical models performed better on the larger StockEmotions dataset.
- Why unresolved: The study balanced datasets by reducing the majority class to the size of the minority class, potentially depriving classical models of necessary training data.

### Open Question 3
- Question: How does the choice of preprocessing technique (specifically dimensionality reduction and undersampling) impact the semantic retention and performance of classical models?
- Basis: Section 5 suggests "testing other types of preprocessing" and Section 3.2 notes that undersampling may have left out "good records that could better identify this label."
- Why unresolved: The paper acknowledges that necessary preprocessing steps were driven by computational constraints rather than optimality, leaving their specific impact on performance ambiguous.

## Limitations

- The paper reduced FPB from 4,845 to 1,812 records and TFN from 2,486 to 1,044 records for balanced training, likely degrading classical performance beyond what class-weighted approaches would show.
- Generation parameters (temperature, max_tokens) for LLM prompting were not specified, leaving prompt sensitivity as an uncontrolled variable.
- The claim that generative LLMs inherently handle short-context data better than frequency-based models is weakly supported, as the paper did not systematically vary text length or compare different classical architectures optimized for short text.

## Confidence

- **High Confidence:** LLMs outperformed classical models on financial sentiment tasks; Gemini achieved the highest overall accuracy across datasets; performance gap was most pronounced on smaller datasets.
- **Medium Confidence:** The mechanism explaining LLM superiority (bypassing preprocessing information loss) is plausible but not definitively proven, as the paper did not test classical models on unbalanced data with class weights.
- **Low Confidence:** The claim that generative LLMs inherently handle short-context data better than frequency-based models is weakly supported, as the paper did not systematically vary text length or compare different classical architectures optimized for short text.

## Next Checks

1. **Data Fidelity Test:** Reproduce the classical pipeline on the unbalanced FPB dataset using class-weighted loss functions instead of undersampling to quantify the impact of the paper's preprocessing choices.
2. **Prompt Sensitivity Analysis:** Run Gemini on the TFN dataset using 3-5 distinct prompt formulations to measure variance in sentiment classification accuracy.
3. **Dataset Size Sensitivity:** Train classical models (SVM, MLP) on incrementally larger subsets of FPB (e.g., 10%, 25%, 50%, 100%) to empirically verify the claim that classical models require larger datasets to match LLM performance.