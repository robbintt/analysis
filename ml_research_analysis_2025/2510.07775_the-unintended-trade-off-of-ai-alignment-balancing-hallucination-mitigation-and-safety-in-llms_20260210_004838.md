---
ver: rpa2
title: The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation
  and Safety in LLMs
arxiv_id: '2510.07775'
source_url: https://arxiv.org/abs/2510.07775
tags:
- refusal
- safety
- heads
- arxiv
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates a previously overlooked trade-off in large
  language model alignment: enhancing truthfulness to reduce hallucinations can unintentionally
  weaken safety alignment, making models more susceptible to harmful prompts. The
  root cause is that hallucination and refusal behaviors share overlapping internal
  representations, particularly in certain attention heads.'
---

# The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs

## Quick Facts
- arXiv ID: 2510.07775
- Source URL: https://arxiv.org/abs/2510.07775
- Authors: Omar Mahmoud; Ali Khalil; Buddhika Laknath Semage; Thommen George Karimpanal; Santu Rana
- Reference count: 33
- This paper identifies a previously overlooked trade-off where truthfulness-enhancing fine-tuning degrades safety alignment due to overlapping internal representations in LLMs.

## Executive Summary
This paper reveals a fundamental trade-off in LLM alignment: interventions that reduce hallucinations by suppressing polysemantic attention heads also weaken safety alignment, making models more susceptible to harmful prompts. The authors propose a solution using sparse autoencoders to disentangle refusal and hallucination features, then preserve the refusal subspace during fine-tuning via gradient orthogonalization. Evaluated on commonsense reasoning and harmful benchmarks, their approach significantly reduces harmful outputs while maintaining or improving task utility, demonstrating that truthfulness and safety can be balanced without sacrificing either.

## Method Summary
The method identifies overlapping attention heads that encode both hallucination and refusal behaviors using contrastive influence analysis. Sparse autoencoders are then trained on these heads to extract refusal-specific latents. During fine-tuning with LoRA, gradient updates are orthogonalized against the refusal subspace, preventing modification of refusal-related weights while allowing task-specific learning. This preserves safety alignment while improving truthfulness on commonsense reasoning tasks.

## Key Results
- Attack success rates on harmful prompts reduced from ~9% to ~0% on AdvBench while maintaining or improving accuracy on commonsense tasks
- Significant reduction in hallucinations on TruthfulQA (improved factual accuracy from 0.39 to 0.57)
- The approach works across different model architectures (LLaMA-3-8B-Instruct and Qwen2.5-7B-Instruct)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention heads can simultaneously encode both hallucination-related and refusal-related features, causing interventions targeting one behavior to affect the other.
- Mechanism: A subset of attention heads in middle-to-later layers exhibits "polysemanticity," where activations correlate with both incorrect (hallucinated) outputs and refusal responses. Truthfulness-enhancing interventions (e.g., ITI steering, LoRA direction steering) suppress these heads' activation magnitudes, reducing hallucination but also weakening refusal behavior.
- Core assumption: The model's internal representations for hallucination and refusal are partially co-located in specific attention heads rather than fully disentangled.
- Evidence anchors:
  - [abstract] "this arises from overlapping components in the model that simultaneously encode hallucination and refusal information"
  - [section 4.2.1] "we observe a notable overlap between hallucination heads and refusal heads in the base model"
  - [corpus] Related work (arXiv:2601.04262) supports head-level diagnosis of safety-utility conflicts but does not directly confirm hallucination-refusal overlap.
- Break condition: If hallucination and refusal features were fully disentangled at the architectural level, or if different model architectures used entirely separate pathways, the trade-off would not manifest through this mechanism.

### Mechanism 2
- Claim: Hallucination behavior can be represented as a single linear direction in the model's activation space, enabling bidirectional steering.
- Mechanism: Training a rank-1 LoRA module on MLP down-projection layers using question-incorrect answer pairs produces a linear steering vector. Negative steering improves truthfulness, while positive steering degrades it. However, steering toward truthfulness simultaneously increases attack success rates, confirming subspace overlap.
- Core assumption: Hallucination tendencies can be approximated as a linear direction in activation space for the specific model and dataset tested.
- Evidence anchors:
  - [abstract] "interventions targeting hallucination also suppress refusal"
  - [section 4.1] "steering toward truthfulness... improves factual accuracy but simultaneously increases attack success rates on harmful prompts"
  - [corpus] No direct corpus support found; related hallucination mitigation work (arXiv:2501.06521) focuses on fine-tuning strategies rather than linear direction representations.
- Break condition: If hallucination were inherently non-linear or multi-dimensional in ways that cannot be captured by a single vector, steering-based interventions would be less generalizable.

### Mechanism 3
- Claim: Constraining gradient updates to remain orthogonal to a refusal subspace preserves safety alignment during fine-tuning while allowing task-specific learning.
- Mechanism: Sparse autoencoders disentangle polysemantic features into near-monosemantic latents. Gradient-based attribution identifies refusal-specific latents from harmful prompt/refusal response pairs. These latents span a refusal subspace; during fine-tuning, each gradient update is orthogonalized against this subspace, preventing modification of refusal-related weights.
- Core assumption: SAE features capture causally relevant refusal directions, and orthogonalizing gradients does not significantly impede learning on benign tasks.
- Evidence anchors:
  - [abstract] "disentangle refusal-related features from hallucination features using sparse autoencoders, and preserves refusal behavior during fine-tuning through subspace orthogonalization"
  - [section 5] "This approach prevents hallucinations from increasing while maintaining safety alignment"
  - [corpus] Related methods (SafeLoRA, SaLoRA, SAP) use gradient-space constraints for safety preservation but do not specifically address hallucination-safety trade-offs.
- Break condition: If refusal features were not adequately captured by SAE latents, or if orthogonalization created harmful gradient interference patterns, the method would fail to preserve safety or would degrade utility unacceptably.

## Foundational Learning

- Concept: **Polysemanticity in neural representations**
  - Why needed here: The core problem arises because single neurons or attention heads encode multiple features; understanding this is essential for grasping why truthfulness interventions affect safety.
  - Quick check question: Can you explain why the same attention head might contribute to both factual errors and refusal behavior?

- Concept: **Sparse Autoencoders (SAEs) for feature disentanglement**
  - Why needed here: The proposed solution relies on SAEs to separate refusal features from hallucination features; you need to understand how SAEs enforce sparsity to produce interpretable latents.
  - Quick check question: How does an SAE differ from a standard autoencoder in producing disentangled features?

- Concept: **Gradient orthogonalization for subspace preservation**
  - Why needed here: The fine-tuning method projects gradients away from the refusal subspace; understanding this optimization constraint is key to implementing the approach.
  - Quick check question: If gradient update g has components both along and orthogonal to refusal subspace S, which component is applied during fine-tuning?

## Architecture Onboarding

- Component map:
  - Input: Prompts (benign task data, harmful benchmarks like AdvBench/StrongReject)
  - Core model: LLaMA-3-8B-Instruct or Qwen2.5-7B-Instruct with attention heads (target of analysis)
  - Analysis tools: Contrastive influence scoring, LoRA rank-1 steering, SAE feature extraction
  - Intervention: Gradient orthogonalization during LoRA fine-tuning (rank 8, modules [q_proj, v_proj])
  - Evaluation: Commonsense tasks (CSQA, HellaSwag, ARC, WinoGrande, SST-2, BoolQ), TruthfulQA, LlamaGuard3 for safety classification

- Critical path:
  1. Identify hallucination heads via contrastive influence (incorrect vs. correct completions)
  2. Identify refusal heads via contrastive influence (harmful vs. refusal completions)
  3. Compute overlap set O between hallucination and refusal heads
  4. Train SAE on attention outputs; extract refusal-specific latents from O
  5. Span refusal subspace via QR decomposition; orthogonalize gradients during fine-tuning

- Design tradeoffs:
  - SAE training cost (1B tokens) vs. quality of feature disentanglement
  - Aggressive orthogonalization (stronger safety) vs. potential utility degradation
  - Focus on attention heads only vs. broader layer coverage (MLPs not analyzed in depth)

- Failure signatures:
  - High ASR on safety benchmarks post-fine-tuning indicates refusal subspace not adequately preserved
  - Significant drop in commonsense/factual accuracy indicates over-constrained gradient updates
  - Inconsistent results across models suggests architecture-specific representational differences

- First 3 experiments:
  1. Replicate contrastive influence analysis on your target model to identify hallucination/refusal head overlap
  2. Train rank-1 LoRA hallucination direction; evaluate steering coefficients on both TruthfulQA and AdvBench to confirm trade-off
  3. Implement SAE-based refusal subspace extraction and gradient orthogonalization; compare ASR + accuracy against baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different alignment methods affect knowledge suppression, and does their impact vary across model architectures?
- Basis in paper: [explicit] The Limitations section states: "further investigation is needed to interpret how different alignment methods affect knowledge suppression and whether their impact varies across model architectures."
- Why unresolved: The study primarily tested LLaMA-3-8B-Instruct and Qwen2.5-7B-Instruct with specific alignment methods; generalization to other architectures (e.g., encoder-decoder, mixture-of-experts) remains untested.
- What evidence would resolve it: Systematic evaluation across diverse model families (GPT-style, PaLM-style, MoE) using multiple alignment techniques (RLHF, DPO, constitutional AI) with knowledge probing benchmarks.

### Open Question 2
- Question: Can SAE-based disentanglement approaches scale practically for fine-tuning at scale?
- Basis in paper: [explicit] The Limitations section notes: "our proposed solution relies on a trained SAE, which may limit its practicality for fine-tuning at scale."
- Why unresolved: Training SAEs requires substantial computational resources and data; the paper trained on 1B tokens, but scalability to 70B+ parameter models or industrial fine-tuning pipelines is unclear.
- What evidence would resolve it: Experiments demonstrating SAE training efficiency on 70B+ models, plus wall-clock comparisons showing minimal overhead relative to standard LoRA fine-tuning.

### Open Question 3
- Question: Do the overlapping refusal-hallucination representations remain stable across different training regimes, or do they reconfigure under distribution shift?
- Basis in paper: [inferred] The paper identifies overlapping heads at one training checkpoint but does not track whether these representations persist or reconfigure under continued pretraining, domain adaptation, or multi-stage alignment.
- Why unresolved: The mechanistic analysis captures a snapshot; dynamic stability of the identified subspaces across training trajectories is unknown.
- What evidence would resolve it: Longitudinal analysis tracking the same attention heads' overlap coefficients across multiple fine-tuning stages and domain shifts.

### Open Question 4
- Question: Does the truthfulness-safety trade-off manifest differently across the three hallucination types (retrieval failure, context misuse, fabrication)?
- Basis in paper: [inferred] The paper categorizes hallucinations into Types A, B, C (footnote 2) but treats them uniformly in analysis; refusal mechanisms may interact differently with each type.
- Why unresolved: The experiments aggregate hallucination types, leaving open whether certain hallucination categories have stronger or weaker overlap with refusal representations.
- What evidence would resolve it: Stratified experiments where hallucination mitigation targets each type separately, measuring differential impacts on refusal behavior.

## Limitations
- The SAE-based disentanglement approach relies on human-annotated harmful prompts for feature extraction, which may not capture the full complexity of refusal behavior
- The linear approximation of hallucination as a rank-1 direction may not hold across diverse domains or model scales
- The method focuses on attention heads while ignoring MLP layers, potentially missing important refusal-related representations

## Confidence

- **High confidence**: The existence of the truthfulness-safety trade-off and its manifestation through overlapping attention head representations
- **Medium confidence**: The effectiveness of gradient orthogonalization for preserving refusal behavior during fine-tuning
- **Low confidence**: The generalizability of SAE-based feature extraction across different model families and prompt distributions

## Next Checks

1. Test the orthogonalization approach on models fine-tuned with different alignment techniques (SFT, RLHF) to verify robustness
2. Conduct ablation studies varying SAE architecture parameters (hidden dimensions, sparsity) to identify sensitivity thresholds
3. Evaluate the method's effectiveness on multilingual benchmarks to assess cross-linguistic generalizability of the refusal subspace preservation approach