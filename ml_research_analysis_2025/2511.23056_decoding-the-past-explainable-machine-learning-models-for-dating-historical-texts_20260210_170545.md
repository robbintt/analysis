---
ver: rpa2
title: 'Decoding the Past: Explainable Machine Learning Models for Dating Historical
  Texts'
arxiv_id: '2511.23056'
source_url: https://arxiv.org/abs/2511.23056
tags:
- temporal
- classification
- historical
- feature
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automatically dating historical
  texts by predicting when they were written. The core method integrates compression-based
  features with linguistic characteristics using interpretable tree-based models.
---

# Decoding the Past: Explainable Machine Learning Models for Dating Historical Texts

## Quick Facts
- **arXiv ID:** 2511.23056
- **Source URL:** https://arxiv.org/abs/2511.23056
- **Reference count:** 40
- **Primary result:** 76.7% century accuracy, 26.1% decade accuracy on English texts spanning 1600-2020

## Executive Summary
This paper addresses the challenge of automatically dating historical texts by predicting when they were written. The authors propose a novel approach that combines compression-based features with linguistic characteristics using interpretable tree-based models. By integrating five distinct feature categories—compression-based metrics, lexical structure, readability, neologism detection, and distance features—the method achieves strong performance in both century-scale and decade-scale classification tasks. The approach is particularly notable for its explainability analysis, which reveals patterns in linguistic evolution across five centuries of English writing.

## Method Summary
The approach integrates compression-based features with linguistic characteristics using interpretable tree-based models. Five feature categories are combined: compression-based metrics (using NRC/Shannon entropy with Markov models), lexical structure, readability scores, neologism detection across eight historical periods, and distance features for 17 function words. XGBoost and CatBoost algorithms are employed with feature importance and SHAP explainability analysis. The method is evaluated on a large corpus of 18,984 English texts spanning 1600-2020, with 65%/25%/10% stratified train/val/test splits. The framework achieves 76.7% accuracy for century-scale prediction and 26.1% for decade-scale classification.

## Key Results
- Achieves 76.7% accuracy for century-scale prediction and 26.1% for decade-scale classification
- Strong ranking capabilities with AUCROC up to 94.8% for decade prediction
- SHAP analysis identifies the 19th century as a pivot point in linguistic evolution
- Cross-dataset evaluation shows domain adaptation challenges with 26.4 percentage point accuracy drop on Project Gutenberg texts

## Why This Works (Mechanism)
The method succeeds by combining multiple complementary feature types that capture different aspects of linguistic evolution. Compression-based features measure information content and complexity, while lexical structure features capture vocabulary patterns. Neologism detection identifies emerging word usage over time, and distance features track changes in function word frequencies. The interpretable tree-based models (XGBoost and CatBoost) can effectively leverage these diverse signals while providing insights into which features drive predictions.

## Foundational Learning
- **Compression-based features**: Use information theory to measure text complexity and evolution over time. Needed to capture underlying linguistic changes beyond surface patterns. Quick check: Verify Markov model order matches paper specifications.
- **SHAP explainability analysis**: Provides local and global interpretability for complex tree models. Needed to understand feature contributions and identify temporal patterns. Quick check: Confirm SHAP values sum to model predictions.
- **Cross-dataset validation**: Tests model generalization across different text sources. Needed to assess real-world applicability beyond training domain. Quick check: Compare in-domain vs cross-domain performance gaps.

## Architecture Onboarding

**Component Map**: Text Preprocessing -> Feature Extraction (5 categories) -> Tree Models (XGBoost/CatBoost) -> Evaluation + SHAP Analysis

**Critical Path**: The most critical sequence is feature extraction → model training → explainability analysis, as the quality of engineered features directly determines model performance and interpretability.

**Design Tradeoffs**: The method trades computational efficiency for interpretability by using handcrafted features rather than learned representations. This requires substantial memory (176 GB peak) but enables detailed feature importance analysis and SHAP explanations.

**Failure Signatures**: Poor cross-dataset performance (26.4 pp drop) indicates overfitting to domain-specific characteristics. Memory overflow during feature extraction suggests computational limits with current implementation.

**Three First Experiments**:
1. Reproduce feature extraction with reduced worker count to verify memory requirements
2. Train models using default hyperparameters to establish baseline performance
3. Evaluate model on held-out validation set from same domain before cross-dataset testing

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Substantial computational resources required (176 GB peak memory for feature extraction)
- Significant domain adaptation challenges with 26.4 percentage point performance drop on Project Gutenberg texts
- Missing hyperparameters and neologism vocabulary lists limit exact reproducibility

## Confidence
**High confidence** in in-domain accuracy metrics (76.7% century, 26.1% decade) and ranking performance (AUCROC up to 94.8%) for Open Library texts.

**Medium confidence** in cross-dataset generalization due to significant performance degradation on Project Gutenberg texts.

**Low confidence** in practical utility of interpretability analysis without complete feature sets and hyperparameters.

## Next Checks
1. Reproduce feature extraction with reduced worker count (4 workers instead of 30) to verify memory requirements stay under 200 GB.
2. Train the models using default XGBoost and CatBoost hyperparameters first, then incrementally tune to establish whether performance improvements require extensive hyperparameter optimization.
3. Evaluate model performance on a held-out validation set from the same Open Library corpus used for training to confirm the 76.7% century accuracy claim before testing cross-dataset generalization.