---
ver: rpa2
title: 'P/D-Device: Disaggregated Large Language Model between Cloud and Devices'
arxiv_id: '2508.09035'
source_url: https://arxiv.org/abs/2508.09035
tags:
- cloud
- tokens
- arxiv
- prefill
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of large language model (LLM)
  inference caused by long decoding phases on the cloud and slow prefill phases on
  devices. The core method, P/D-Device, involves the cloud assisting each device during
  its prefill phase by providing refined prompts and a controlled number of decoding
  tokens.
---

# P/D-Device: Disaggregated Large Language Model between Cloud and Devices

## Quick Facts
- arXiv ID: 2508.09035
- Source URL: https://arxiv.org/abs/2508.09035
- Reference count: 40
- Key outcome: Cloud-device disaggregation with prompt refinement and token assistance reduces user TTFT by â‰¥60%, smooths TPOT to tens of milliseconds, and increases cloud throughput up to 15x

## Executive Summary
P/D-Device introduces a disaggregated inference architecture that addresses the inefficiency of LLM inference by leveraging cloud assistance during device prefill phases. The system enables devices to offload token assistance from the cloud while maintaining control over final responses, significantly reducing time-to-first-token (TTFT) and smoothing time-per-output-token (TPOT). By refining prompts using intermediate data from cloud prefill, the approach shortens on-device computation while amortizing the long prefill phase.

## Method Summary
The core method involves cloud assistance to devices during their prefill phase through refined prompts and controlled token assistance. The cloud provides optimized prompts based on intermediate prefill data and sends a limited number of decoding tokens to help devices respond quickly. An optimization algorithm determines optimal settings for prompt refinement and token assistance based on device capabilities and cloud characteristics. The system balances load between cloud and devices while maintaining response quality and user experience.

## Key Results
- User-perceived TTFT decreases by at least 60% compared to baseline
- Maximum TPOT reduced to tens of milliseconds, enabling smooth token generation
- Cloud throughput increases by up to 15x compared to cloud-only inference
- On-device prefill time is amortized through cloud assistance and prompt refinement

## Why This Works (Mechanism)
The disaggregation approach works by exploiting the complementary strengths of cloud and devices: the cloud excels at long prefill computations while devices handle responsive decoding. By providing refined prompts and token assistance, the cloud enables devices to bypass the computationally expensive prefill phase. The prompt refinement uses intermediate data from cloud prefill to create optimized prompts that require less on-device computation. The controlled token assistance ensures devices can respond immediately while maintaining response quality through on-device control mechanisms.

## Foundational Learning

**Cloud-device disaggregation** - Separating LLM inference into cloud and device components to leverage respective computational strengths
*Why needed:* Different hardware excels at different inference phases (prefill vs decoding)
*Quick check:* Cloud handles prefill efficiently, devices handle responsive token generation

**Prompt refinement via intermediate data** - Using cloud prefill outputs to create optimized prompts for devices
*Why needed:* Reduces on-device computation while maintaining response quality
*Quick check:* Shorter prefill times with minimal quality degradation

**Token assistance control** - Providing limited decoding tokens from cloud to enable immediate device response
*Why needed:* Bridges gap between cloud assistance and device autonomy
*Quick check:* Devices respond immediately while maintaining control

**Optimization algorithm for settings** - Dynamically determining optimal prompt refinement and token assistance levels
*Why needed:* Balances performance, cost, and quality across diverse device capabilities
*Quick check:* Algorithm adapts to different device specifications and network conditions

**Time-per-output-token (TPOT) smoothing** - Ensuring consistent token generation timing
*Why needed:* Critical for user experience and perceived responsiveness
*Quick check:* TPOT remains in tens of milliseconds range throughout generation

## Architecture Onboarding

**Component map:** User Request -> Cloud Prefill -> Prompt Refinement -> Token Assistance -> Device Control -> Final Response

**Critical path:** Cloud prefill and prompt refinement must complete before device begins generation, but token assistance enables immediate partial response

**Design tradeoffs:** Cloud assistance improves performance but increases bandwidth usage; prompt refinement reduces device load but requires accurate intermediate data; token assistance enables responsiveness but must be carefully controlled to maintain device autonomy

**Failure signatures:** Network latency causes delayed token assistance; inaccurate prompt refinement increases on-device computation; excessive token assistance reduces device control and increases cloud load

**3 first experiments:**
1. Measure TTFT improvement with varying token assistance levels across different device capabilities
2. Test prompt refinement accuracy and its impact on on-device prefill time reduction
3. Evaluate cloud throughput scaling with increasing user load under disaggregation

## Open Questions the Paper Calls Out
None

## Limitations
- Performance under varying network conditions and device heterogeneity remains unexplored
- Scalability to large user populations and different LLM architectures not validated
- Privacy implications of cloud-assisted prefill refinement not addressed

## Confidence
**High Confidence:** Core disaggregation approach and basic performance metrics (TTFT reduction, TPOT smoothing) are well-supported by experimental results
**Medium Confidence:** Prompt refinement mechanism effectiveness and optimization algorithm generalization across diverse scenarios require further validation
**Low Confidence:** Long-term performance under dynamic conditions, device heterogeneity effects, and scalability remain unexplored

## Next Checks
1. Conduct stress testing under variable network conditions (latency spikes, bandwidth fluctuations) to evaluate system robustness and performance degradation patterns
2. Implement cross-device validation with heterogeneous hardware profiles to assess optimization algorithm generalization
3. Perform user experience studies comparing P/D-Device against baseline implementations in real-world usage scenarios, measuring both quantitative metrics and qualitative feedback