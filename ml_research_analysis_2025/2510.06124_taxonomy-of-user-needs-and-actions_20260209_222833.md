---
ver: rpa2
title: Taxonomy of User Needs and Actions
arxiv_id: '2510.06124'
source_url: https://arxiv.org/abs/2510.06124
tags:
- user
- information
- system
- request
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Taxonomy of User Needs and Actions (TUNA),
  an empirically grounded framework for classifying user behavior in conversational
  AI systems. Developed through qualitative analysis of 1193 human-AI conversations,
  TUNA organizes user actions into a three-level hierarchy of 57 request types across
  six interaction modes: Information Seeking, Information Processing & Synthesis,
  Procedural Guidance & Execution, Content Creation & Transformation, Social Interaction,
  and Meta-Conversation.'
---

# Taxonomy of User Needs and Actions

## Quick Facts
- arXiv ID: 2510.06124
- Source URL: https://arxiv.org/abs/2510.06124
- Reference count: 40
- Primary result: Introduces TUNA, a three-level hierarchy taxonomy classifying 57 user action types across six interaction modes in conversational AI systems

## Executive Summary
This paper introduces the Taxonomy of User Needs and Actions (TUNA), an empirically grounded framework for classifying user behavior in conversational AI systems. Developed through qualitative analysis of 1193 human-AI conversations, TUNA organizes user actions into a three-level hierarchy capturing both instrumental goals and the situated, conversational strategies users employ to achieve them. The taxonomy addresses limitations in existing frameworks by explicitly modeling social interaction and meta-conversation as distinct modes, making visible the "invisible work" users perform to manage the AI relationship.

TUNA enables multi-scale evaluation, supports policy harmonization across products, and provides a backbone for layering domain-specific taxonomies. The framework's hierarchical structure (6 interaction modes > 14 strategies > 57 request types) allows stakeholders to engage at appropriate levels of abstraction, from policymakers examining high-level modes to engineers debugging specific request types. The taxonomy was validated through iterative development achieving theoretical saturation and demonstrating reasonable inter-rater reliability.

## Method Summary
The taxonomy was developed through iterative qualitative analysis of 1,193 human-AI conversations from WildChat and ShareGPT datasets. The process followed three phases: (1) open-coding 200 dialogues to identify emergent codes, (2) expanding empirical coding with additional 699 dialogues, and (3) conceptual-to-empirical refinement through literature review. The final taxonomy consists of six interaction modes (four instrumental, one social, one meta), 14 strategies, and 57 request types. Validation occurred on a held-out set of 294 dialogues (1,247 user turns) using a single annotator with final inter-rater reliability assessment showing Fleiss' kappa of 0.74 for turns and 0.64 for dialogues.

## Key Results
- TUNA successfully classifies all user turns in validation set without requiring new modes or strategies
- The taxonomy captures both instrumental goals and conversational strategies often missed by existing frameworks
- Social Interaction and Meta-Conversation modes explicitly model "invisible work" of relationship management

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition of User Intent
The taxonomy's three-level hierarchy (Interaction Modes > Strategies > Request Types) resolves the gap between high-level instrumental goals and low-level conversational mechanics by separating what users want to achieve from the relational work required. This structure allows mapping specific utterances to broader strategies and high-level modes, enabling analysts to distinguish between the content of a request and its conversational context.

### Mechanism 2: Operationalizing "Invisible Work" via Dedicated Modes
By explicitly categorizing Social Interaction (Mode 5) and Meta-Conversation (Mode 6) as distinct modes, TUNA makes visible the articulation work users perform to manage the AI relationship. This forces analysts to acknowledge context-building and repair actions as classifiable behaviors rather than noise, addressing a critical gap in existing taxonomies that focus solely on instrumental tasks.

### Mechanism 3: Multi-Scale Evaluation Backbone
The unified, empirically grounded vocabulary serves as a backbone enabling consistent policy harmonization and disaggregated evaluation across diverse products. The shared lexicon allows different stakeholders to align on user behavior definitions, standardizing failure/success definitions across domains while maintaining the flexibility to layer domain-specific taxonomies on top.

## Foundational Learning

- **Concept: Situated Action & Articulation Work**
  - Why needed here: To understand why TUNA separates conversational strategies from goals, recognizing that users improvise (articulation work) to fix errors or clarify ambiguity
  - Quick check question: Can you explain why a "broken" query might be classified as a valid "Communicative Status" action rather than just noise?

- **Concept: Conversational Grounding**
  - Why needed here: Essential for understanding Mode 5 (Social Interaction) and the "Shared Understanding" strategy, where users establish common ground before or during instrumental tasks
  - Quick check question: How does TUNA classify a user saying "I meant the book, not the movie"—is it a new task or a repair mechanism?

- **Concept: Speech Act Theory (high-level)**
  - Why needed here: TUNA contrasts itself with linguistic frameworks that classify utterance function, requiring understanding of the difference between linguistic form (a question) and instrumental goal (content generation request)
  - Quick check question: Why would TUNA classify "Write me a poem" as "Content Creation" rather than just a "Command" or "Question"?

## Architecture Onboarding

- **Component map:** 6 Interaction Modes (4 Instrumental, 1 Social, 1 Meta) → 14 Strategies → 57 Request Types, with domain-specific tags overlaid as additional layer
- **Critical path:**
  1. Ingest conversation logs
  2. Map user turns to TUNA Request Types (multiple labels permitted)
  3. Roll up Request Types to Strategies and Modes for multi-scale analysis
  4. Identify sequences (e.g., Background Info → Feasibility Assessment)
- **Design tradeoffs:**
  - Specificity vs. Ambiguity: Classifying turns like "Show me data analysis" is inherently ambiguous (Mode 2 vs. 3 vs. 4), requiring annotator training
  - Compound Requests: Users often issue multiple request types in one turn, requiring multi-label classification support
- **Failure signatures:**
  - Mode Collapse: Classifying everything as "Information Seeking" because user phrased it as a question
  - Context Blindness: Ignoring Mode 5/6 signals (e.g., "I'm frustrated") and focusing only on instrumental task
- **First 3 experiments:**
  1. Ambiguity Stress Test: Test human annotator reliability on 50 "compound" user turns with multiple request types
  2. Sequential Harm Detection: Filter for [System Performance Feedback] → [Regeneration Request] sequences to identify consistent system failures
  3. Safety Alignment Check: Compare benign vs. adversarial prompts (e.g., "How does malware work?" vs. "Write malware") to validate safety policy utility

## Open Questions the Paper Calls Out

- Can automated classifiers accurately apply TUNA to large-scale interaction logs, and what is the inter-rater reliability compared to human coders? (Requires LLM classifier development and validation)
- Is TUNA sufficiently comprehensive and exhaustive when applied to non-English linguistic contexts and diverse cultural norms? (Needs validation studies in non-English corpora)
- To what extent do specific sequences of TUNA request types predict user satisfaction or task completion? (Requires correlational analysis between behavioral patterns and outcomes)

## Limitations

- The taxonomy's inter-rater reliability was assessed only at the end of development rather than throughout iterative coding, raising questions about consistency during formative stages
- Validation relied on a single annotator for most turns in the validation set, limiting confidence in the IRR metric's robustness
- The assertion that TUNA serves as an effective "backbone" for multi-scale evaluation remains theoretical without demonstrated case studies or implementation examples

## Confidence

- **High Confidence:** The taxonomy's empirical grounding through iterative development with 1,193 conversations and achievement of theoretical saturation
- **Medium Confidence:** The claim that TUNA captures "invisible work" through dedicated Social and Meta modes, as this relies on qualitative interpretation
- **Low Confidence:** The assertion that TUNA serves as an effective "backbone" for multi-scale evaluation and policy harmonization, as this remains theoretical

## Next Checks

1. **IRR During Development:** Reconstruct the annotation process to assess inter-rater reliability at each iteration (200, 200, and 499 dialogues) to verify consistency throughout taxonomy development
2. **Compound Request Resolution:** Test the taxonomy's handling of compound requests by having multiple annotators independently label 100 multi-intent turns and measuring agreement rates
3. **Domain Extension Test:** Apply TUNA to a specialized domain (e.g., software engineering or healthcare) and evaluate whether the backbone structure meaningfully reduces the annotation burden compared to building a domain-specific taxonomy from scratch