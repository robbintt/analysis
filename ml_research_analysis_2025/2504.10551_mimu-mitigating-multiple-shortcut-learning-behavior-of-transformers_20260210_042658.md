---
ver: rpa2
title: 'MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers'
arxiv_id: '2504.10551'
source_url: https://arxiv.org/abs/2504.10551
tags:
- shortcuts
- learning
- shortcut
- mimu
- multiple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes MiMu, a novel method to mitigate multiple
  shortcut learning behavior in Transformer-based models. The method consists of two
  strategies: self-calibration in the source model to prevent overconfident predictions,
  and self-improvement in the target model to reduce reliance on multiple shortcuts.'
---

# MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers

## Quick Facts
- **arXiv ID:** 2504.10551
- **Source URL:** https://arxiv.org/abs/2504.10551
- **Reference count:** 40
- **Primary result:** Achieves 0.8021 accuracy on ImageNet-9 Rand-B, improving by 0.0313 over ERM baseline.

## Executive Summary
MiMu is a novel method to mitigate multiple shortcut learning in Transformer-based models by employing two complementary strategies: self-calibration in the source model to prevent overconfident predictions, and self-improvement in the target model to reduce reliance on multiple shortcuts. The method uses random masking and adaptive attention alignment to force the model to focus on broader regions rather than fixed areas. Experiments on NLP and CV tasks demonstrate that MiMu significantly improves robustness generalization abilities on out-of-distribution samples while maintaining in-distribution accuracy.

## Method Summary
MiMu employs a two-stage training approach where a source model is first calibrated to prevent overconfidence, then a target model is trained with random masking and attention alignment to the calibrated source. The source model is trained with a calibration loss combining mean squared difference with cross-entropy to reduce overconfident predictions on shortcuts. The target model applies random masking to attention positions and aligns its remaining attention weights to the calibrated source model, forcing it to distribute focus beyond fixed shortcut regions. Post-hoc calibration is applied using logistic regression on the target model's outputs.

## Key Results
- On ImageNet-9 with watermarks, MiMu achieves 0.8021 accuracy on Rand-B, improving by 0.0313 over the ERM baseline.
- Maintains in-distribution accuracy while significantly improving out-of-distribution robustness.
- Demonstrates effectiveness across both NLP (MNLI, QQP) and CV (ImageNet-9/200) tasks.

## Why This Works (Mechanism)

### Mechanism 1: Self-Calibration via Confidence-Output Alignment
Reduces overconfidence in the source model by combining mean squared difference with cross-entropy, preventing premature confidence on spurious features. This works because shortcut learning correlates with overconfident predictions early in training.

### Mechanism 2: Random Masking for Attention Diversification
Randomly masking attention positions forces the model to distribute focus beyond fixed shortcut regions. This works because shortcuts are assumed to be localized to specific attention positions, and masking disrupts this dependency.

### Mechanism 3: Adaptive Attention Alignment to Calibrated Reference
Aligns target model attention to calibrated source model attention, guiding the target toward more robust attention patterns without external supervision. This works because calibrated source attention is assumed to be less shortcut-dominated and provides a useful reference signal.

## Foundational Learning

- **Expected Calibration Error (ECE):** Measure and validate that self-calibration improves confidence-output alignment. Quick check: Can you compute ECE from a set of predictions and their confidences?
- **Knowledge Distillation:** Understand the softened output matching (KL divergence) between source and target models. Quick check: How does temperature T affect the softness of output distributions?
- **Attention Weight Extraction in Transformers:** Access attention weights for masking and alignment operations. Quick check: Where in a Transformer layer do you access attention weights (e.g., after softmax)?

## Architecture Onboarding

- **Component map:** Source Model (Fs) -> Target Model (Ft) -> Post-hoc Calibration
- **Critical path:** 1) Train Fs with calibration loss → extract as and Ps, 2) Initialize Ft; apply random mask to its attention weights, 3) Compute alignment loss between Ft (masked) and Fs attention, 4) Optimize combined loss (label + KD + alignment), 5) Post-hoc calibrate Ft using held-out validation data
- **Design tradeoffs:** Masking ratio N (5-25%) balances robustness vs. signal loss; alignment strength λ2 balances target learning vs. shortcut prevention
- **Failure signatures:** OOD accuracy significantly below IID accuracy, high ECE on OOD data, saliency maps show attention on background/watermarks/non-semantic tokens
- **First 3 experiments:** 1) Measure calibration of ERM vs. MiMu (Fs and Ft) on OOD data to validate self-calibration impact, 2) Sweep N (5%, 10%, 15%, 20%, 25%) on validation OOD set to find optimal masking ratio, 3) Compare attention maps of ERM, Fs, and Ft on OOD examples with known shortcuts

## Open Questions the Paper Calls Out

### Open Question 1
Does the combination of multiple shortcuts induce a distinct "third shortcut" representing the interaction between features, rather than just a weighted reliance on the original individual shortcuts? The paper hypothesizes this but does not isolate or identify specific internal representations of an interaction-based shortcut.

### Open Question 2
How do fine-grained attributes of shortcuts—such as size, complexity, color, or semantic style—quantitatively influence the degree of shortcut learning behavior? The current study treats shortcuts primarily as monolithic features without systematically varying their physical or semantic properties.

### Open Question 3
Can the self-calibration and self-improvement strategies of MiMu effectively mitigate diverse shortcut behaviors in Large Language Models (LLMs) that lead to hallucinations? The method is validated on BERT and ViT but untested on generative autoregressive models.

## Limitations

- Implementation details for random masking and calibration loss weights are underspecified, making exact reproduction difficult
- The paper lacks ablation studies isolating each component's contribution to the overall performance improvement
- No validation of post-hoc calibration's contribution relative to in-training mitigation

## Confidence

- **High Confidence:** General framework consistency and empirical gains across multiple benchmarks
- **Medium Confidence:** Plausibility of mechanism descriptions without component-specific ablation studies
- **Low Confidence:** Critical unknowns in random masking implementation and calibration loss balance

## Next Checks

1. **Calibration Curve Analysis:** Plot ECE for source model (Fs) before and after calibration loss on OOD data to verify overconfidence reduction
2. **Masking Ratio Sweep:** Test masking rates N = 5%, 10%, 15%, 20% on validation OOD set to identify optimal balance
3. **Attention Map Comparison:** Visualize and compare attention maps of ERM, Fs, and Ft on OOD examples with known shortcuts to confirm diversification away from fixed regions