---
ver: rpa2
title: 'AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents'
arxiv_id: '2506.04293'
source_url: https://arxiv.org/abs/2506.04293
tags:
- feature
- trial
- clinical
- trials
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AUTOCT is a framework that automates interpretable clinical trial
  outcome prediction using large language models (LLMs) and Monte Carlo Tree Search
  (MCTS). It autonomously generates, plans, and builds tabular features from public
  biomedical data sources without human intervention.
---

# AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents

## Quick Facts
- arXiv ID: 2506.04293
- Source URL: https://arxiv.org/abs/2506.04293
- Reference count: 30
- Primary result: AutoCT achieves ROC-AUC scores of 0.753, 0.639, and 0.702 on Phase I, II, and III trial outcome prediction respectively

## Executive Summary
AUTOCT is a framework that automates interpretable clinical trial outcome prediction using large language models (LLMs) and Monte Carlo Tree Search (MCTS). It autonomously generates, plans, and builds tabular features from public biomedical data sources without human intervention. The system leverages LLMs to emulate expert reasoning in feature engineering, while MCTS iteratively refines features based on model performance feedback. Experimental results show AUTOCT achieves competitive ROC-AUC scores comparable to state-of-the-art deep learning methods while maintaining interpretability through classical ML models.

## Method Summary
AUTOCT employs a multi-agent architecture where specialized LLM agents handle different aspects of the feature engineering workflow. The system consists of Feature Proposer, Feature Planner, Feature Builder, Model Builder, and Evaluator components, orchestrated by an MCTS Controller. The Feature Builder retrieves data from PubMed and ClinicalTrials.gov using PubMedBERT embeddings and BM25 hybrid search with date filtering to prevent label leakage. MCTS explores the feature space by treating Evaluator suggestions as actions, balancing exploration of new features with exploitation of high-performing ones. The framework uses GPT-4o-mini with zero-shot Chain-of-Thought reasoning and ReAct-style tool use, achieving interpretability through classical ML models (LR, RF, XGBoost) on engineered features.

## Key Results
- AutoCT achieves ROC-AUC scores of 0.753, 0.639, and 0.702 on Phase I, II, and III trial outcome prediction respectively
- Performance is comparable to state-of-the-art deep learning methods while maintaining interpretability
- The system autonomously generates predictive features from public biomedical data without human intervention

## Why This Works (Mechanism)

### Mechanism 1
LLMs can automate expert-level feature engineering for clinical trial prediction by proposing, planning, and constructing features from raw data sources, mimicking biomedical expert reasoning.

### Mechanism 2
Monte Carlo Tree Search (MCTS) guided by performance feedback iteratively improves feature sets by treating Evaluator suggestions as actions in a search space.

### Mechanism 3
A system of specialized LLM agents manages the complex, multi-step workflow of feature engineering by decomposing tasks into smaller, manageable sub-tasks.

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS)**
  - Why needed: Core optimization loop that iteratively improves feature sets based on feedback
  - Quick check: How does MCTS differ from standard hill-climbing search? (Hint: balance of exploration vs exploitation)

- **Concept: Feature Engineering for Tabular ML**
  - Why needed: System builds high-quality tabular dataset from scratch, requiring understanding of predictive features
  - Quick check: What makes one feature better than another for a Random Forest model?

- **Concept: LLM Reasoning Paradigms (CoT, ReAct)**
  - Why needed: System relies on LLMs for complex reasoning tasks like planning feature construction and using tools
  - Quick check: When should you use ReAct-style agent versus Chain-of-Thought agent?

## Architecture Onboarding

- **Component map**: Feature Proposer → Feature Planner → Feature Builder → Model Builder → Evaluator → MCTS Controller (feeds back to Proposer)
- **Critical path**: Feature Proposer → Feature Planner → Feature Builder → Model Builder → Evaluator → MCTS Controller
- **Design tradeoffs**: Classical ML models ensure interpretability but may limit performance vs deep learning; MCTS explores feature space but is computationally expensive; multi-agent design manages complexity at cost of harder debugging
- **Failure signatures**: Feature Builder returning None for many features; Evaluator stuck suggesting similar features; MCTS not improving ROC-AUC after N iterations
- **First 3 experiments**:
  1. Run single iteration without MCTS to verify end-to-end pipeline works and produces baseline score
  2. Isolate Feature Builder agent: give manually created, high-quality feature plan and check if it can build features correctly from data sources
  3. Run MCTS loop with fixed maximum depth but vary number of rollouts to observe ROC-AUC improvement trend

## Open Questions the Paper Calls Out

1. Does integrating heterogeneous data sources beyond PubMed and ClinicalTrials.gov significantly improve predictive performance?
2. Can the Evaluator agent be enhanced to attribute model underperformance to specific pipeline failures to enable more targeted corrections?
3. Does AUTOCT maintain competitive performance and cost-efficiency when scaled to the full TrialBench dataset?

## Limitations
- Performance on real-world, noisy clinical data remains untested beyond TrialBench benchmark
- Computational cost of running MCTS (~$150 per run) may limit practical deployment in resource-constrained settings
- Small validation set (100 samples) raises concerns about overfitting

## Confidence

- **High confidence**: Multi-agent architecture and MCTS optimization mechanism are well-described and technically sound
- **Medium confidence**: Reported ROC-AUC scores are comparable to deep learning methods but small validation set raises overfitting concerns
- **Medium confidence**: Interpretability through classical ML models is clear advantage, though detailed feature importance analysis is lacking

## Next Checks

1. **External validation**: Test AUTOCT on clinical trial data from sources outside TrialBench to assess real-world generalization
2. **Cost-benefit analysis**: Measure marginal improvement in prediction accuracy against computational expense of MCTS iterations
3. **Feature interpretability audit**: Conduct detailed analysis of most predictive features to verify alignment with established biomedical knowledge