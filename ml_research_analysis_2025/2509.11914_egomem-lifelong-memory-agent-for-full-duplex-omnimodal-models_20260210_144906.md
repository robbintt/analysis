---
ver: rpa2
title: 'EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models'
arxiv_id: '2509.11914'
source_url: https://arxiv.org/abs/2509.11914
tags:
- user
- memory
- egomem
- dialog
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EgoMem is the first lifelong memory system for full-duplex omnimodal
  models, enabling real-time user recognition, personalized responses, and long-term
  memory maintenance from raw audiovisual streams. It operates with three asynchronous
  processes: retrieval for user identification and context gathering, omnimodal dialog
  for personalized responses, and memory management for dialog boundary detection
  and memory updates.'
---

# EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models

## Quick Facts
- arXiv ID: 2509.11914
- Source URL: https://arxiv.org/abs/2509.11914
- Reference count: 34
- EgoMem is the first lifelong memory system for full-duplex omnimodal models, enabling real-time user recognition, personalized responses, and long-term memory maintenance from raw audiovisual streams

## Executive Summary
EgoMem introduces a pioneering lifelong memory system designed for full-duplex omnimodal models, enabling real-time user recognition, personalized responses, and continuous memory maintenance from raw audiovisual streams. Unlike traditional memory systems that rely on textual inputs, EgoMem operates directly on audiovisual data, making it suitable for embodied and real-time interaction scenarios. The system integrates three asynchronous processes—retrieval, dialog, and memory management—to deliver personalized and context-aware responses. Evaluated on the EgoMemTalk dataset, the system demonstrates high accuracy in retrieval and memory management, while achieving strong fact-consistency in personalized dialogues.

## Method Summary
EgoMem processes raw audiovisual streams through three concurrent asynchronous processes: retrieval for user identification and context gathering, omnimodal dialog for generating personalized responses, and memory management for dialog boundary detection and memory updates. The system integrates a fine-tuned RoboEgo chatbot to produce fact-consistent personalized responses. Memory management involves three key steps: detecting dialog boundaries using speech and text embeddings, retrieving relevant memories, and updating the memory with new dialog content. The system is evaluated on a novel dataset, EgoMemTalk, which includes multimodal streams and corresponding text transcriptions for comprehensive testing.

## Key Results
- Achieves over 95% accuracy in retrieval and memory management tasks
- Integrates with RoboEgo chatbot to achieve fact-consistency scores above 87% in real-time personalized dialogs
- First system to operate entirely on raw audiovisual streams for lifelong memory in full-duplex omnimodal models

## Why This Works (Mechanism)
EgoMem's effectiveness stems from its asynchronous processing architecture that separates retrieval, dialog, and memory management into concurrent streams. This design enables real-time operation while maintaining personalization through continuous user identification and context preservation. The system's ability to work directly with raw audiovisual streams, rather than requiring text conversion, allows it to capture richer contextual information and maintain more natural interactions. The integration with RoboEgo ensures that personalized responses remain factually consistent while leveraging the chatbot's language understanding capabilities.

## Foundational Learning
- **Raw Audiovisual Processing**: Required for capturing comprehensive contextual information without information loss from text conversion; quick check: verify multimodal feature extraction quality
- **Asynchronous Processing Architecture**: Needed to maintain real-time performance while handling multiple concurrent tasks; quick check: measure latency across all three processes
- **Memory Boundary Detection**: Essential for organizing dialog history and preventing context mixing; quick check: validate boundary detection accuracy across different conversation lengths
- **Personalization Through User Recognition**: Critical for maintaining consistent user profiles and delivering tailored responses; quick check: test recognition accuracy across varying environmental conditions
- **Fact-Consistency Maintenance**: Required to ensure personalized responses remain accurate and reliable; quick check: measure consistency scores across diverse dialog topics

## Architecture Onboarding

**Component Map**: Retrieval -> Dialog -> Memory Management (asynchronous)

**Critical Path**: Raw audiovisual stream → Feature extraction → User identification → Context retrieval → Response generation → Memory update

**Design Tradeoffs**: Real-time performance vs. processing depth; memory efficiency vs. recall accuracy; computational complexity vs. deployment flexibility

**Failure Signatures**: Memory degradation over time; synchronization issues between asynchronous processes; accuracy drops in noisy environments

**First Experiments**:
1. Test retrieval accuracy with varying user appearance changes and environmental conditions
2. Measure memory management performance with increasing dialog history lengths
3. Evaluate fact-consistency under different conversation topics and user preferences

## Open Questions the Paper Calls Out
None

## Limitations
- Performance in highly dynamic, noisy, or privacy-constrained environments remains uncertain
- Memory management's ability to maintain coherent long-term memory across years of interaction has not been validated
- Substantial computational resources required may limit practical deployment on resource-constrained devices

## Confidence
- High confidence: Retrieval accuracy (>95%) and basic memory management functionality in controlled settings
- Medium confidence: Real-time performance in diverse environmental conditions and long-term memory stability
- Medium confidence: Fact-consistency scores in personalized dialogues, though dependent on the underlying LLM

## Next Checks
1. Evaluate EgoMem's performance in naturalistic environments with variable lighting, background noise, and user movement patterns over extended periods (3+ months)

2. Conduct stress tests on memory management by simulating years of interaction to assess memory degradation, redundancy, and retrieval accuracy

3. Deploy EgoMem on edge devices with limited computational resources to measure real-world latency, power consumption, and memory footprint under full-duplex operation