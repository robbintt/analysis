---
ver: rpa2
title: Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection?
  Evaluating In-Context Learning vs. Fine-Tuning
arxiv_id: '2509.07768'
source_url: https://arxiv.org/abs/2509.07768
tags:
- news
- fake
- text
- answer
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively evaluates fine-tuning (FT) and in-context
  learning (ICL) for detecting hyperpartisan, fake, polarized, and harmful content
  across 10 multilingual datasets. It compares encoder and decoder architectures using
  various prompt strategies, including zero-shot, few-shot (random and DPP-selected),
  codebook, and chain-of-thought.
---

# Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning

## Quick Facts
- arXiv ID: 2509.07768
- Source URL: https://arxiv.org/abs/2509.07768
- Reference count: 31
- Key outcome: Fine-tuning consistently outperforms in-context learning for detecting hyperpartisan, fake, polarized, and harmful content across 10 multilingual datasets, with codebook prompting being most effective among ICL methods.

## Executive Summary
This study systematically compares fine-tuning (FT) and in-context learning (ICL) for detecting harmful content across 10 multilingual datasets spanning hyperpartisan news, fake news, political bias, and harmful tweets. The researchers evaluate both encoder and decoder architectures using various prompt strategies including zero-shot, few-shot (random and DPP-selected), codebook, and chain-of-thought approaches. Across all configurations, FT emerged as the superior method, outperforming ICL in 28 out of 33 experimental setups. The study identifies that decoder models excel at knowledge-intensive tasks like fake news detection, while encoders perform better on linguistically-oriented tasks like hyperpartisan content detection. Among ICL methods, codebook prompting proved most effective, providing structured classification rules that help models navigate ambiguous cases.

## Method Summary
The study evaluates fine-tuning and in-context learning across 10 multilingual datasets (5 languages) using encoder models (RoBERTa, ModernBERT) and decoder models (Llama3.1-8b, Mistral-Nemo, Qwen2.5). Fine-tuning employs LoRA with rank 8, alpha 16, learning rate 1e-4, and 3 epochs across 5 random seeds. ICL experiments use temperature=0 with prompt strategies including zero-shot (generic/specific), codebook, chain-of-thought, and few-shot (random/DPP-selected). Performance is measured using weighted F1 and accuracy across all datasets.

## Key Results
- Fine-tuning consistently outperformed in-context learning across 28 of 33 experimental configurations
- Decoder models excelled at fake news detection and political bias identification, while encoders performed better on hyperpartisan and harmful tweet detection
- Codebook prompting was most effective among ICL methods, providing structured classification rules that improved performance
- Chain-of-thought prompting proved largely suboptimal, particularly for underrepresented languages where unfamiliar tokens disrupted inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning consistently outperforms in-context learning for misinformation detection tasks, even when comparing smaller fine-tuned models against larger models using ICL.
- Mechanism: Parameter-efficient fine-tuning (using LoRA) updates model weights to encode task-specific decision boundaries directly into the model, whereas ICL relies on the model's ability to generalize from prompt context without weight modification. The paper finds FT superior in 28 of 33 experimental configurations.
- Core assumption: Task-specific weight updates create more robust representations than can be achieved through prompt engineering alone, particularly for nuanced classification tasks requiring domain expertise.
- Evidence anchors:
  - [abstract] "We discovered that In-Context Learning often underperforms when compared to Fine-Tuning a model. This main finding highlights the importance of Fine-Tuning even smaller models on task-specific settings even when compared to the largest models evaluated in an In-Context Learning setup"
  - [section: Main Results and Discussion] "Across all the configurations tested in our experiments, FT emerged as the most effective method to apply models to the political domain... the FT configuration demonstrated its efficacy in 28 out of 33 cases."
  - [corpus] Limited corpus evidence directly comparing FT vs. ICL on misinformation; related work focuses on single approaches rather than systematic comparison.

### Mechanism 2
- Claim: Model architecture (encoder vs. decoder) creates task-specific performance advantages based on the nature of the detection task.
- Mechanism: Encoder-only models with bidirectional attention (RoBERTa, mDeBERTa) excel at capturing nuanced linguistic features for hyperpartisan and harmful tweet detection. Decoder-only models (LLaMA, Mistral) perform better on tasks requiring factual world knowledge (fake news, political bias detection), potentially due to their causal attention and broader pre-training on knowledge-rich corpora.
- Core assumption: Bidirectional attention enables better capture of stylistic and linguistic patterns, while decoder models' larger pre-training corpora encode more factual knowledge useful for verification tasks.
- Evidence anchors:
  - [section: Main Results - Fine-Tuning] "On average across datasets, decoder-based models tend to outperform encoder-based models on tasks that require factual world knowledge, such as fake news detection and political bias identification... Conversely, encoders achieve better results on linguistically oriented tasks"
  - [section: Main Results] "We hypothesize that this difference arises because the bidirectional attention mechanism of encoders may be better at capturing nuanced linguistic features, whereas decoders might excel at tasks more reliant on content or semantic understanding."
  - [corpus] Corpus contains work on embedding-based approaches and transformer models for harmful content detection, but lacks systematic architecture comparison across misinformation tasks.

### Mechanism 3
- Claim: Codebook prompting (structured rule-based prompts with explicit classification criteria) is the most effective ICL strategy, outperforming both chain-of-thought and standard few-shot approaches.
- Mechanism: Codebooks provide explicit definitions, classification rules, and examples that bridge the gap between abstract classification concepts and concrete textual indicators. This structured guidance helps models overcome inherent task complexity by providing clearer decision boundaries than models' internal task representations alone.
- Core assumption: Models possess sufficient reading comprehension to follow structured rules, and the bottleneck is lack of explicit task definition rather than reasoning capability.
- Evidence anchors:
  - [abstract] "Among ICL methods, codebook prompting was most effective for three datasets"
  - [section: Main Results - Summary for Zero-shot configurations] "The codebook approach demonstrates modest effectiveness across all task categories... The codebook's effectiveness stems from its ability to bridge the gap between abstract classification concepts and concrete textual indicators, providing models with clearer decision boundaries for ambiguous cases."
  - [corpus] No corpus papers directly evaluate codebook prompting for misinformation detection; this appears to be a novel contribution of this work.

## Foundational Learning

- Concept: **In-Context Learning (ICL) and Prompt Sensitivity**
  - Why needed here: The paper demonstrates that ICL performance varies dramatically with prompt design, example selection, and ordering. Understanding why ICL fails for certain tasks requires grasping its fundamental limitations.
  - Quick check question: Can you explain why adding more few-shot examples doesn't monotonically improve performance, and why example order matters?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - Why needed here: All decoder fine-tuning experiments used LoRA with specific hyperparameters (rank 8, alpha 16). Understanding how LoRA works is essential for reproducing and extending these results.
  - Quick check question: How does LoRA reduce the number of trainable parameters while still adapting model behavior, and what tradeoffs does the rank parameter introduce?

- Concept: **Determinantal Point Processes (DPP) for Diversity Selection**
  - Why needed here: The paper tests DPP for selecting diverse few-shot examples, finding it sometimes reduces variance but doesn't consistently improve performance. Understanding DPP helps interpret when diversity matters.
  - Quick check question: What does the determinant of a kernel matrix represent in DPP, and why would promoting diversity in few-shot examples not always improve classification?

## Architecture Onboarding

- Component map:
  - **Encoder models**: RoBERTa-base/large, XLM-RoBERTa, mDeBERTaV3, ModernBERT-base/large, POLITICS (RoBERTa adapted for political domain)
  - **Decoder models**: LLaMA3.1-8B/Instruct, Mistral-Nemo-Instruct-2407, Qwen2.5-7B-Instruct
  - **Prompt configurations**: Zero-shot (generic, specific), Codebook, Few-shot (random, DPP-selected), Chain-of-Thought
  - **Fine-tuning setup**: LoRA with rank 8, alpha 16, learning rate 1e-4, 3 epochs, 5 runs with different seeds
  - **Evaluation**: Weighted F1 and accuracy across 10 datasets in 5 languages

- Critical path:
  1. **Dataset preparation**: Ensure train/test splits match paper specifications; handle multilingual data appropriately
  2. **Model selection**: Choose encoder vs. decoder based on task type (linguistic vs. knowledge-intensive)
  3. **Prompt engineering**: Start with codebook prompting as baseline for ICL experiments
  4. **Fine-tuning configuration**: Use LoRA hyperparameters from Table 3; run 5 seeds for variance estimation
  5. **Evaluation**: Report weighted F1 and accuracy with standard deviations

- Design tradeoffs:
  - **FT vs. ICL**: FT requires training data and compute but yields higher performance; ICL is faster but less reliable
  - **Encoder vs. Decoder**: Encoders better for stylistic detection (hyperpartisan, harmful tweets); decoders better for knowledge tasks (fake news, political bias)
  - **Few-shot quantity**: More examples don't always help; performance often peaks at 1-6 shots then declines
  - **DPP vs. Random selection**: DPP sometimes reduces variance but doesn't consistently improve performance; adds computational overhead

- Failure signatures:
  - **CoT underperformance**: Chain-of-thought prompting "proved largely suboptimal," particularly for underrepresented languages where novel tokens "appeared to confuse the models, disrupting their inference capabilities"
  - **Specific prompt failures**: Moving from generic to specific zero-shot prompts sometimes decreased performance (e.g., Qwen on Spanish fake news dropped 0.275 F1), suggesting misalignment between model's internal definitions and expert-crafted definitions
  - **Political bias detection**: Low performance across all models (best ~0.42 F1), indicating the task's inherent complexity and subjectivity
  - **Output parsing issues**: Models sometimes generated unparseable outputs; switching from integer to string labels reduced errors by ~99%

- First 3 experiments:
  1. **Baseline comparison**: Replicate FT vs. ICL comparison on SemEval-2019 (hyperpartisan) and Fake News Net datasets using RoBERTa-large and LLaMA3.1-8B-Instruct; verify 28/33 pattern holds
  2. **Codebook ablation**: Test codebook vs. zero-shot-specific prompts on harmful tweet detection (CLEF datasets); isolate whether structured rules or domain definitions drive improvements
  3. **Few-shot scaling curve**: Plot performance vs. number of shots (1-10) for DPP vs. random selection on a single dataset; identify optimal k-shot value and whether DPP reduces variance as claimed

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Retrieval-Augmented Generation (RAG) systems effectively incorporate up-to-date information to improve the factual verification capabilities of LLMs for fake news detection?
- **Basis in paper:** [explicit] The authors state in the Conclusion: "Future work could investigate the application of a RAG system to incorporate up-to-date information and improve the factual verification of news."
- **Why unresolved:** The current study relies on static pre-trained knowledge, which limits the models' ability to verify facts on recent events or rapidly evolving misinformation topics.
- **What evidence would resolve it:** A benchmark comparison showing that RAG-enhanced ICL outperforms static ICL and potentially FT on datasets containing very recent news events not present in the models' training data.

### Open Question 2
- **Question:** Would scaling model parameters significantly (e.g., to 70B+ or proprietary models) close the performance gap between In-Context Learning (ICL) and Fine-Tuning (FT)?
- **Basis in paper:** [inferred] The authors note in the Limitations section that "our GPUs could not host larger models," restricting experiments to smaller 7-8B parameter open models.
- **Why unresolved:** It is unclear if the superiority of FT over ICL holds true for much larger models (e.g., GPT-4, Llama 3 70B) which may have stronger intrinsic reasoning capabilities.
- **What evidence would resolve it:** Experiments replicating this benchmark on significantly larger or proprietary models to see if ICL performance approaches or matches that of fine-tuned smaller models.

### Open Question 3
- **Question:** How can Chain-of-Thought (CoT) prompting be optimized for mid- and low-resource languages to prevent the performance degradation observed in this study?
- **Basis in paper:** [inferred] The authors observe that CoT was "largely suboptimal," and analysis suggests this "underperformance stems primarily from language representation issues" where models struggled with "unfamiliar linguistic patterns" in non-English datasets.
- **Why unresolved:** The study indicates that current CoT implementations may confuse models when applied to languages with insufficient representation in the training data.
- **What evidence would resolve it:** Developing and testing language-specific CoT templates or translation-augmented strategies that improve reasoning performance in the underperforming Arabic and Bulgarian datasets.

## Limitations
- Dataset generalizability: Performance patterns may not extend to other content moderation tasks or language combinations beyond the 10 datasets tested
- Architecture-specific constraints: Findings may be specific to the particular model families tested rather than universal across all encoder/decoder models
- ICL implementation details: Temperature=0 setting may not capture full potential of probabilistic decoding, and codebook templates require significant manual effort

## Confidence
- **High Confidence**: Fine-tuning consistently outperforms in-context learning across the tested tasks and model configurations
- **Medium Confidence**: Decoder architectures perform better on knowledge-intensive tasks while encoder architectures excel at linguistic pattern detection
- **Low Confidence**: Codebook prompting is the most effective ICL strategy for general application

## Next Checks
1. **Architecture ablation study**: Systematically test additional model families within each architecture category on a subset of datasets to determine whether observed performance differences are architecture-specific or model-family specific
2. **ICL parameter sweep**: Conduct experiments varying temperature settings (0.1, 0.3, 0.5, 0.7) and decoding strategies for codebook and CoT prompting to assess whether reported underperformance is due to suboptimal hyperparameters
3. **Zero-shot transfer evaluation**: Test generalization capability of codebook prompts by applying them to completely unseen datasets within the same content categories to validate whether structured templates encode generalizable decision boundaries