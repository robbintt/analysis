---
ver: rpa2
title: Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators
arxiv_id: '2508.21524'
source_url: https://arxiv.org/abs/2508.21524
tags:
- binary
- quantization
- bits
- activation
- accelerators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a binary weight multi-bit activation (BWMA)
  quantization method for CNNs on CIM-based accelerators. The method addresses the
  accuracy-efficiency trade-off in CIM quantization by using binary weights with layer-specific
  scaling factors and multi-bit activations matched to ADC resolution.
---

# Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators

## Quick Facts
- arXiv ID: 2508.21524
- Source URL: https://arxiv.org/abs/2508.21524
- Reference count: 24
- This paper introduces a binary weight multi-bit activation (BWMA) quantization method for CNNs on CIM-based accelerators that achieves 1.44%-5.46% and 0.35%-5.37% accuracy improvements on CIFAR-10 and ImageNet datasets respectively compared to existing methods.

## Executive Summary
This paper addresses the accuracy-efficiency trade-off in quantization for Compute-in-Memory (CIM) CNN accelerators by introducing a Binary Weight Multi-bit Activation (BWMA) method. The approach uses binary weights with layer-specific scaling factors and 4-bit activations matched to ADC resolution, achieving significant accuracy improvements while maintaining hardware efficiency. Key innovations include deriving closed-form solutions for weight binarization through moment matching and developing a differentiable approximation function for multi-bit activation quantization.

## Method Summary
The BWMA method implements layer-wise weight binarization by preserving the first and second statistical moments (mean and standard deviation) of the original weight distribution, mapping weights to two values $\mu \pm \sigma$. For activation quantization, it uses a differentiable cubic approximation function $G(a)$ to enable smooth gradient flow during backpropagation. The method constrains activations to 4-bit precision to align with optimal ADC resolution in CIM arrays. Training uses quantization-aware training (QAT) with a modified Straight-Through Estimator for weight gradients and the cubic function for activation quantization.

## Key Results
- Achieves 1.44%-5.46% and 0.35%-5.37% accuracy improvements on CIFAR-10 and ImageNet datasets respectively compared to existing methods
- Identifies 4-bit activation quantization as optimal for balancing hardware cost and model performance across different CIM device types
- Demonstrates hardware efficiency through DNN+NeuroSim V2.0 simulations showing reduced energy and area overhead

## Why This Works (Mechanism)

### Mechanism 1: Statistical Moment Preservation for Weight Binarization
- **Claim:** Binarizing weights by preserving the first and second statistical moments (mean and standard deviation) of the original full-precision distribution maintains representational capacity better than naive $\pm 1$ mapping.
- **Mechanism:** Instead of deterministic rounding, the method calculates layer-specific binary values $w_{b1} = \mu - \sigma$ and $w_{b2} = \mu + \sigma$. This ensures the binarized distribution approximates the Gaussian-like distribution of trained weights, preserving the signal-to-noise ratio of the layer.
- **Core assumption:** The weights in a trained CNN layer follow a symmetric distribution approximated by these two moments.
- **Evidence anchors:** [abstract] Mentions deriving "closed-form solutions for weight quantization" to improve representational capabilities; [Section III.B] Explicitly defines the binary values based on mean $\mu$ and standard deviation $\sigma$ (Eq. 4).

### Mechanism 2: Differentiable Cubic Approximation for Activation Quantization
- **Claim:** A piecewise cubic function ($G(a)$) approximates the non-differentiable uniform quantization step function better than linear or piecewise linear Straight-Through Estimators (STEs), improving gradient flow during backpropagation.
- **Mechanism:** The method integrates a quadratic Dirac approximation ($g(a)$) to derive a cubic function ($-\frac{2}{3}a^3 + \frac{5}{3}a$) that smoothly transitions at quantization thresholds. This allows precise gradient updates for multi-bit activation thresholds without exhaustive search.
- **Core assumption:** The gradient landscape near quantization boundaries is sufficiently smooth to be approximated by a low-order polynomial.
- **Evidence anchors:** [abstract] Cites "developing a differentiable approximation function for multi-bit activation quantization"; [Section III.B] Defines $g(a)$ and $G(a)$ in Eq. 7 and 8.

### Mechanism 3: Activation-to-ADC Resolution Alignment
- **Claim:** Constraining activation bit-width to 4 bits aligns with the optimal resolution of Analog-to-Digital Converters (ADCs) in CIM arrays, minimizing energy/area overhead while retaining accuracy.
- **Mechanism:** Analog accumulation in crossbars is converted to digital via ADCs. The paper identifies 4 bits as the "sweet spot" where the quadratic cost increase of high-precision ADCs (6-bit+) outweighs accuracy gains, while low-precision (3-bit) degrades model accuracy too severely.
- **Core assumption:** The dominant energy/area cost in the target CIM architecture (SRAM/RRAM/FeFET) is the peripheral data conversion circuitry rather than the crossbar array itself.
- **Evidence anchors:** [abstract] States "4-bit activation quantization provides the optimal balance"; [Section IV/B] Hardware simulation results (Fig. 6) identify 4-bit as the efficiency knee for various device types.

## Foundational Learning

- **Concept:** **Compute-in-Memory (CIM) Crossbar Arrays**
  - **Why needed here:** The entire quantization strategy is driven by the physical constraints of performing Matrix-Vector Multiplication (MVM) in the analog domain using Ohm's Law and Kirchhoff's Current Law.
  - **Quick check question:** How does storing a weight as conductance ($G$) allow for multiplication with an input voltage ($V$)?

- **Concept:** **Quantization-Aware Training (QAT) & Straight-Through Estimator (STE)**
  - **Why needed here:** The paper improves upon standard STE for binarization and multi-bit quantization. You must understand why we need to "fake" gradients for discrete operations to train the network.
  - **Quick check question:** During backpropagation, how does the STE handle the derivative of the sign function or round function?

- **Concept:** **Moment Matching (Statistics)**
  - **Why needed here:** Mechanism 1 relies on aligning the mean and variance of full-precision vs. binarized weights.
  - **Quick check question:** If a layer has weights with mean 0.1 and std 0.5, what two values would this method map the weights to?

## Architecture Onboarding

- **Component map:** DACs -> Crossbar Array (binary weights) -> ADCs (4-bit) -> Digital Post-processing
- **Critical path:**
  1. **Initialization:** Compute layer-wise $\mu$ and $\sigma$ from pre-trained weights
  2. **Forward Pass:** Apply $w_{b1}, w_{b2}$ mapping $\to$ Analog MVM $\to$ 4-bit Quantization (using $G(a)$ approximation)
  3. **Backward Pass:** Use cubic derivative for activation updates; use modified STE (Eq. 5) for weight updates

- **Design tradeoffs:**
  - **Area vs. Accuracy:** 1-bit weights minimize cell area; 4-bit activations balance ADC area vs. model performance
  - **Utilization:** Larger crossbars (64x64) reduce latency but increase unused cell overhead (Fig. 5/6 discussion)

- **Failure signatures:**
  - **Accuracy Collapse (Weights):** If $\sigma$ is calculated globally rather than layer-wise, representational capacity drops
  - **Hardware Inefficiency:** Using >4-bit activations causes energy to spike (ADC cost) without proportional accuracy gain
  - **Gradient Mismatch:** If the temperature parameter $t$ in Eq. 5 is poorly tuned, weight gradients vanish

- **First 3 experiments:**
  1. **Unit Test (Forward):** Input a Gaussian distribution into the Weight Binarization module and verify output mean/variance matches input within tolerance
  2. **Software Simulation (Accuracy):** Train ResNet-18 on CIFAR-10 using *only* the cubic activation approximation (ablate weight binarization) to isolate activation contribution
  3. **Hardware Simulation (Cost):** Run DNN+NeuroSim (or equivalent) for VGG-8, sweeping ADC resolution from 3-bit to 6-bit to reproduce the "4-bit optimal" energy curve

## Open Questions the Paper Calls Out
- **Open Question 1:** What mapping strategies can mitigate the high rate of unused cells (up to 39.1%) observed when scaling BWMA to larger crossbar sizes?
- **Open Question 2:** Can the moment-matching binarization method be effectively applied to attention-based architectures like Vision Transformers?
- **Open Question 3:** How does the BWMA method perform under real-world analog non-idealities such as process variation and conductance drift?

## Limitations
- The method relies on symmetric weight distributions, which may not hold for all CNN architectures or datasets
- The 4-bit activation optimal threshold is validated through simulation but may require retuning for different CIM architectures
- The scaling factors for the modified STE are introduced without systematic ablation, potentially overfitting to specific datasets

## Confidence

**Confidence Labels:**
- **High Confidence:** The hardware simulation methodology (DNN+NeuroSim V2.0) and the observation that 4-bit ADCs provide a Pareto-optimal trade-off are well-supported by empirical data in Fig. 6
- **Medium Confidence:** The moment-matching approach for weight binarization is theoretically sound, but its performance gain over simpler methods (e.g., fixed $\pm 1$ mapping) is not rigorously compared in isolation
- **Low Confidence:** The claim that the cubic approximation $G(a)$ is superior to other STE variants (e.g., STE with tanh or sigmoid smoothing) is asserted but lacks a direct ablation study

## Next Checks
1. **Distribution Sensitivity Test:** Train the same CNN architectures on non-Gaussian weight distributions (e.g., bimodal or heavy-tailed) to test the robustness of the moment-matching method
2. **STE Ablation Study:** Replace $G(a)$ with alternative smooth functions (e.g., tanh-based STE) and compare accuracy/energy trade-offs to isolate the contribution of the cubic approximation
3. **Cross-Architecture Transfer:** Apply BWMA to a CIM architecture not tested in the paper (e.g., 3D NAND-based CIM) to validate the generalizability of the 4-bit ADC claim