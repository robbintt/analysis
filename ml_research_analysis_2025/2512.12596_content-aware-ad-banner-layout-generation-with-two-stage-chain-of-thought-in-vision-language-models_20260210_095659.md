---
ver: rpa2
title: Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in
  Vision Language Models
arxiv_id: '2512.12596'
source_url: https://arxiv.org/abs/2512.12596
tags:
- layout
- image
- generation
- text
- placement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a vision-language model-based method for
  generating content-aware ad banner layouts. The approach addresses limitations of
  existing saliency-based methods by using a two-stage chain-of-thought prompting
  strategy: first generating a text-based placement plan through VLM analysis of image
  content and object relationships, then producing the final HTML layout based on
  that plan.'
---

# Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models

## Quick Facts
- arXiv ID: 2512.12596
- Source URL: https://arxiv.org/abs/2512.12596
- Reference count: 26
- Primary result: Two-stage VLM prompting generates content-aware ad layouts that outperform saliency-based baselines on PKU-PosterLayout dataset

## Executive Summary
This paper introduces a vision-language model-based method for generating content-aware ad banner layouts. The approach addresses limitations of existing saliency-based methods by using a two-stage chain-of-thought prompting strategy: first generating a text-based placement plan through VLM analysis of image content and object relationships, then producing the final HTML layout based on that plan. This separation enables more interpretable and controllable layout generation that better reflects image semantics. The method was evaluated on the PKU-PosterLayout dataset using standard metrics (Validity, Overlap, Alignment, Underlay, Utility, Occlusion, Unreadability) and VLM-based evaluation. Results show competitive or superior performance compared to baselines, particularly in avoiding salient regions and satisfying placement constraints. Qualitative examples demonstrate improved visual appropriateness. The study highlights the benefits of explicit planning in VLM-driven layout generation and identifies areas for improving VLM-based evaluation.

## Method Summary
The method employs a two-stage chain-of-thought prompting pipeline with GPT-4o. In Stage 1, the VLM analyzes the background image to identify object types and spatial relationships, then generates a natural-language "placement plan" describing where each element should be positioned relative to image content. In Stage 2, the VLM receives the placement plan and generates HTML code for the final layout. The approach uses few-shot prompting with manually created exemplars (5-shot or 10-shot) to guide output format and reasoning patterns. No fine-tuning is performed; the method relies entirely on in-context learning. The system handles three element types: logo, text, and underlay, with constraints that underlay must be positioned behind text and logo elements.

## Key Results
- Two-stage prompting consistently outperforms one-step generation across all rule-based metrics, with particular improvements in Occlusion scores
- 5-shot and 10-shot exemplars achieve similar performance, with both significantly outperforming 0-shot conditions
- VLM-based evaluation (GPT-4o) shows strong correlation with rule-based metrics and provides complementary assessment of layout quality
- The method effectively avoids occluding important image regions compared to saliency-based baselines, with qualitative examples demonstrating superior visual appropriateness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating layout generation into explicit planning and execution phases improves occlusion avoidance and constraint satisfaction compared to single-step generation.
- Mechanism: Stage 1 produces a text-based placement plan describing where elements should go relative to image content. Stage 2 converts this plan into HTML coordinates. This forces reasoning to precede code generation, preventing the model from generating plausible HTML first and then retroactively justifying it.
- Core assumption: The VLM maintains consistency between its textual reasoning and subsequent code generation when tasks are decomposed.
- Evidence anchors: [abstract]: "The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based 'placement plan' based on this analysis." [section 3.5 & 4.3.1]: Two-step prompting improves Occlusion scores compared to one-step; one-step generation frequently outputs HTML before the plan (Figure 7), compromising image awareness. [corpus]: ReLayout and PosterO similarly explore structured reasoning for layout, supporting decomposition as a general pattern.
- Break condition: If the VLM ignores its own plan during code generation, the causal chain breaks. The paper does not explicitly verify plan-to-code fidelity.

### Mechanism 2
- Claim: VLM-based semantic understanding provides richer content awareness than saliency mapping, enabling differentiation between regions that must be avoided versus regions where overlap is acceptable.
- Mechanism: Saliency maps highlight broadly prominent regions without semantic labels. VLMs identify object types (e.g., "rabbit's face" vs. "background fur") and generate spatially-aware placement instructions.
- Core assumption: The VLM's visual encoding captures sufficient spatial and semantic detail to distinguish key objects from less important regions.
- Evidence anchors: [abstract]: "Conventional advertisement layout techniques have predominantly relied on saliency mapping... but such approaches often fail to fully account for the image's detailed composition and semantic content." [section 2, Figure 1]: Saliency maps cannot differentiate between a rabbit's face and carrot—both appear as bright regions. [corpus]: Weak direct comparison—related papers assume VLM/LLM benefits but don't isolate the VLM-vs-saliency mechanism experimentally.
- Break condition: If the VLM fails to detect small, unusual, or contextually important objects, placement plans will be flawed regardless of pipeline structure.

### Mechanism 3
- Claim: Few-shot prompting with structured exemplars guides the VLM toward consistent output formats and reasoning patterns without fine-tuning.
- Mechanism: Manually created (image, plan, HTML) triplets demonstrate expected output structure. The paper compares 0-shot, 5-shot, and 10-shot conditions.
- Core assumption: The VLM generalizes from limited exemplars to novel images without task-specific training.
- Evidence anchors: [section 3.3]: "Few-shot prompting [3] is employed to guide the model toward the desired output by presenting several input-output pairs as exemplars." [section 4.3.1, Table 4]: 0-shot conditions show higher Occlusion scores (worse performance); 5-shot and 10-shot reduce occlusion significantly. Minimal difference between 5-shot and 10-shot. [corpus]: LayoutPrompter and PosterLLaVa also use few-shot prompting, suggesting cross-paper validity for this pattern.
- Break condition: If exemplars don't cover sufficient image diversity, the model may fail to generalize or produce malformed outputs for out-of-distribution inputs.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**:
  - Why needed here: The method applies CoT by forcing explicit verbalization of placement reasoning before execution. Understanding CoT helps explain why decomposition improves output quality.
  - Quick check question: Why might asking a model to articulate its reasoning before generating a final answer improve accuracy on multi-constraint tasks?

- **Vision-Language Models (VLMs)**:
  - Why needed here: The core innovation replaces saliency-based computer vision with VLM semantic understanding. Understanding VLM capabilities and limits is essential for diagnosing failures.
  - Quick check question: What types of visual reasoning can a VLM perform that a saliency detector cannot?

- **Few-Shot / In-Context Learning**:
  - Why needed here: The method operates without fine-tuning, relying entirely on in-context examples. This is the deployment paradigm.
  - Quick check question: What happens to output quality when you reduce from 5-shot to 0-shot? What does Table 4 show?

## Architecture Onboarding

- **Component map**: Image + element constraints → VLM (Stage 1) → Placement plan → VLM (Stage 2) → HTML layout → Rendering

- **Critical path**: Image → VLM semantic analysis → Placement plan → HTML code generation → Rendering

- **Design tradeoffs**:
  - Two API calls vs. one: Higher latency and cost, but improved occlusion and constraint satisfaction.
  - 5-shot vs. 10-shot: Paper shows minimal quantitative difference; 5-shot may be sufficient.
  - No fine-tuning: Lightweight and generalizable, but dependent on base VLM capabilities.
  - HTML representation: Leverages LLM code-generation strengths, but constrains layouts to axis-aligned bounding boxes.

- **Failure signatures**:
  - One-step generation produces placement plan after HTML, then retroactive plan (Figure 7)—plan no longer drives placement.
  - VLM-based evaluation may misinterpret underlay elements (Figure 4), assigning low scores despite valid designs.
  - 0-shot: High occlusion, low underlay scores, poor constraint satisfaction.
  - Constraint violations (underlay not behind text/logo) elevated in one-step baselines (Table 6).

- **First 3 experiments**:
  1. Reproduce the one-step vs. two-step comparison on PKU-PosterLayout test split, measuring Occlusion, Validity, and constraint violation rates. Verify that two-step consistently reduces occlusion.
  2. Ablate few-shot count (0, 5, 10) and inspect failure cases where the VLM ignores the placement plan during code generation.
  3. Run VLM-based pairwise evaluation (per Section 4.3.2) comparing your implementation against LayoutPrompter; manually inspect cases where VLM evaluation disagrees with rule-based metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can VLM-based evaluation methods be improved to accurately assess layouts represented as rectangular bounding boxes rather than rendered images?
- Basis in paper: [explicit] The authors state that "some cases were observed in which the evaluation did not align with the actual layout quality" and provide Figure 4 as an example where GPT-4o failed to recognize proper text-underlay relationships, citing "insufficient contrast with the background" when the underlay was present.
- Why unresolved: Current VLM evaluation methods [6] were designed for actual ad images, not box-format layouts. The paper demonstrates this limitation but does not propose a solution.
- What evidence would resolve it: Development of specialized prompts or evaluation protocols for box-format layouts, validated through correlation with human expert judgments on the same layouts.

### Open Question 2
- Question: Can the two-stage CoT approach generalize effectively to layout generation tasks beyond advertising banners, such as webpage design or mobile UI layouts?
- Basis in paper: [inferred] The introduction notes that "generating effective layouts is important not only for ads but also in a wide range of fields, including poster design, websites, and application user interface design," yet experiments are conducted solely on the PKU-PosterLayout ad dataset.
- Why unresolved: The paper does not evaluate cross-domain generalization or discuss domain-specific adaptations needed for different layout contexts with varying constraints.
- What evidence would resolve it: Systematic evaluation across multiple layout datasets (e.g., RICO for mobile UIs, WebUI for websites) with analysis of domain-specific performance gaps.

### Open Question 3
- Question: What are the failure modes of the two-stage approach when handling complex background images with multiple overlapping semantic regions?
- Basis in paper: [inferred] While Figure 6 shows improved performance with 5-shot and 10-shot examples, the 0-shot condition frequently placed elements over important image regions. The paper does not analyze which image characteristics cause planning failures.
- Why unresolved: The qualitative analysis focuses on successful cases; no systematic categorization of failure modes or image complexity factors is provided.
- What evidence would resolve it: Failure analysis on a held-out test set with annotations of image complexity (object count, spatial distribution, semantic ambiguity) correlated with placement accuracy metrics.

## Limitations

- The paper does not explicitly verify whether the VLM maintains consistency between its textual reasoning and HTML code generation phases, which is fundamental to the proposed mechanism
- VLM-based evaluation may misinterpret underlay elements, assigning low scores to valid designs that properly follow layout constraints
- Manual creation of few-shot exemplars introduces potential bias and the sufficiency of 5-shot examples for generalization is not fully established

## Confidence

**High confidence**: The mechanism showing that two-stage prompting improves occlusion avoidance is well-supported by direct experimental comparison (Table 4, Figure 7). The superiority of VLM semantic understanding over saliency mapping is convincingly demonstrated through qualitative examples showing saliency maps cannot distinguish between different types of prominent regions.

**Medium confidence**: The effectiveness of few-shot prompting is demonstrated through ablation studies, but the manual creation of exemplars introduces potential bias and the sufficiency of 5-shot examples for generalization is not fully established. The claim about VLM-based evaluation being complementary to rule-based metrics is plausible but requires more rigorous validation.

**Low confidence**: The paper does not adequately address whether the VLM maintains consistency between its textual reasoning and code generation phases, which is fundamental to the proposed mechanism. The generalizability of the approach to more complex layout scenarios beyond the PKU-PosterLayout dataset is not demonstrated.

## Next Checks

1. **Plan-to-Code Fidelity Test**: For a subset of test images, extract the placement plans from Stage 1 outputs and verify whether Stage 2 HTML outputs actually respect these plans by comparing predicted vs. actual element positions.

2. **Cross-Dataset Generalization**: Apply the trained pipeline (without fine-tuning) to a different layout dataset such as Adobe's DesignScape or real-world advertisement images to assess robustness across domains.

3. **VLM Evaluation Validation**: Manually annotate 50 test cases where VLM-based evaluation scores differ significantly from rule-based metrics, and have human experts determine which evaluation method better reflects design quality to establish ground truth.