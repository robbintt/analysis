---
ver: rpa2
title: 'Promptception: How Sensitive Are Large Multimodal Models to Prompts?'
arxiv_id: '2509.03986'
source_url: https://arxiv.org/abs/2509.03986
tags:
- answer
- letter
- prompt
- best
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates prompt sensitivity in Large Multimodal
  Models (LMMs) for multiple-choice question answering (MCQA) tasks. The authors introduce
  Promptception, a systematic framework with 61 prompts spanning 15 categories and
  6 supercategories, to evaluate how different phrasing and structural variations
  affect model performance.
---

# Promptception: How Sensitive Are Large Multimodal Models to Prompts?

## Quick Facts
- arXiv ID: 2509.03986
- Source URL: https://arxiv.org/abs/2509.03986
- Reference count: 40
- Primary result: Proprietary LMMs show up to 15% accuracy deviation across prompt variations, while open-source models are more stable but less responsive to nuanced instructions.

## Executive Summary
This paper investigates how different prompt phrasings and structural variations affect Large Multimodal Model (LMM) performance in Multiple-Choice Question Answering (MCQA) tasks. The authors introduce Promptception, a systematic framework with 61 prompts spanning 15 categories and 6 supercategories, to evaluate prompt sensitivity across 10 LMMs on three benchmarks: MMStar (image), MMMU-Pro (multi-image), and MVBench (video). Results reveal that proprietary models exhibit greater sensitivity to prompt phrasing due to stronger instruction-following alignment, while open-source models are steadier but less responsive to nuanced instructions. Based on these findings, the authors propose tailored Prompting Principles for different model classes to enable fairer and more consistent evaluation.

## Method Summary
The study evaluates 10 LMMs (8 open-source, 2 proprietary) using zero-shot MCQA across three benchmarks. Models are tested with 61 prompts from the Promptception framework covering 15 categories and 6 supercategories. Open-source models are evaluated via HuggingFace Transformers on A100 GPUs, while proprietary models use API calls with temperature=0. A two-stage answer extraction process combines regex matching with GPT-4o-mini fallback. Performance is measured using accuracy, trimmed mean (10%), and relative accuracy metrics. Video inputs are preprocessed according to model-specific requirements, and multiple runs ensure result stability.

## Key Results
- Proprietary models show up to 15% accuracy deviation across different prompt types
- Open-source models exhibit greater stability but struggle with nuanced phrasing
- Math tasks show highest sensitivity to prompt variations among MMStar capabilities
- Smaller open-source models (1B parameters) demonstrate greater prompt sensitivity than larger variants

## Why This Works (Mechanism)

### Mechanism 1
Proprietary models exhibit higher prompt sensitivity due to stronger instruction-following alignment. This occurs because proprietary models undergo rigorous instruction tuning with large-scale, high-quality data and advanced RLHF techniques, creating tighter binding between prompt semantics and model behavior.

### Mechanism 2
Open-source LMMs show lower prompt sensitivity due to weaker instruction adherence and overgeneralization. These models typically receive less extensive instruction tuning, leading to more diffuse attention to prompt semantics and uniform behavior across variations.

### Mechanism 3
Prompt sensitivity varies significantly across task types and model scales within the open-source class. Smaller models (1B parameters) show greater sensitivity due to limited capacity for context retention and generalization, with task complexity amplifying these effects.

## Foundational Learning

- **Multiple-Choice Question Answering (MCQA) in LMMs**: Understanding how LMMs process visual inputs with text to select from discrete options. Critical because the entire evaluation framework hinges on comparing accuracy across prompts in this structured format. *Quick check: Can you explain how an LMM would process an image and question to output "B" as the answer?*

- **Instruction Tuning and Alignment**: The process of fine-tuning models to follow user instructions accurately, often using techniques like RLHF. Essential for understanding why proprietary models respond differently to prompt variations than open-source alternatives. *Quick check: What is the hypothesized relationship between instruction tuning depth and prompt sensitivity?*

- **Zero-Shot Evaluation Methodology**: Testing models on tasks without providing examples, isolating the model's inherent capabilities. Matters because the paper explicitly uses zero-shot evaluation to assess prompt impact in isolation from few-shot learning effects. *Quick check: Why might few-shot examples confound the study of prompt sensitivity?*

## Architecture Onboarding

- **Component map**: 61 prompts → 10 LMMs (8 open-source, 2 proprietary) → 3 benchmarks (MMStar, MMMU-Pro, MVBench) → two-stage answer extraction (regex → GPT-4o mini) → metrics (trimmed mean, PRA, PRAD)

- **Critical path**: Select prompt → format with question/choices → pass to LMM with visual input → extract answer letter via regex or GPT-4o mini → compare to ground truth → aggregate across prompts and benchmarks → compute trimmed mean and deviation metrics

- **Design tradeoffs**: Using trimmed mean (10%) reduces outlier influence but may obscure extreme prompt failures. Zero-shot evaluation isolates prompt effects but may underrepresent real-world few-shot performance. Answer extraction via GPT-4o mini introduces dependency but achieves >99% hit rate.

- **Failure signatures**: Proprietary models show refusal responses with "$" symbols and 40%+ drops with negative personas. Open-source models show specific prompt types causing 15-20% drops, often tied to literal interpretation of format instructions.

- **First 3 experiments**:
  1. Run baseline prompt (1.1) and structured formatting prompts (2.1, 2.2) on your target LMM across MMStar to establish baseline sensitivity range.
  2. Test Category 6 (CoT) and Category 12 (answer handling) prompts to identify reasoning vs. format-driven sensitivity in your model class.
  3. Compare performance on Category 4 (poor linguistic formatting) to assess robustness to noisy instructions.

## Open Questions the Paper Calls Out

### Open Question 1
Do prompt sensitivity trends in MCQA generalize to open-ended vision-language tasks? The paper suggests studying tasks like video captioning and visual reasoning is necessary, as the current framework was restricted to MCQA format.

### Open Question 2
Can meta-prompting techniques automate stable prompt generation to reduce manual engineering overhead? The study proposes this as a crucial next step, as it relies entirely on manually curated prompts.

### Open Question 3
Do specific architectural or training data characteristics causally determine whether a model is "steady" (open-source) or "sensitive" (proprietary)? The study compares families rather than controlling for specific training interventions.

## Limitations
- The 61 prompts may not exhaustively represent all real-world prompt variations
- Proprietary models evaluated on subset of MVBench (100 videos) due to API constraints
- Answer extraction process introduces dependency on GPT-4o-mini for fallback cases

## Confidence

- **High Confidence**: Proprietary models show greater prompt sensitivity than open-source models with clear quantitative evidence and consistent patterns across benchmarks.
- **Medium Confidence**: The mechanism explaining proprietary model sensitivity (stronger instruction tuning) is plausible but not directly proven. Claims about smaller model sensitivity lack comprehensive statistical analysis.
- **Low Confidence**: Task-specific sensitivity claims lack statistical significance testing to determine if observed differences are meaningful or due to random variation.

## Next Checks

1. Perform paired t-tests or non-parametric equivalents on accuracy distributions across prompt categories for each model to establish statistical significance.

2. Conduct controlled experiments where the same base LMM undergoes different levels of instruction tuning, then measure prompt sensitivity changes to directly test the hypothesized relationship.

3. Collect and evaluate a random sample of real user prompts from deployed LMM systems, comparing their performance impact against the Promptception framework to validate ecological validity.