---
ver: rpa2
title: 'TARGET: Benchmarking Table Retrieval for Generative Tasks'
arxiv_id: '2505.11545'
source_url: https://arxiv.org/abs/2505.11545
tags:
- table
- retrieval
- tables
- data
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TARGET, a benchmark for evaluating table
  retrieval methods in the context of generative tasks involving structured data.
  The key challenge addressed is identifying the right tables for answering natural
  language queries, a critical step in building effective question-answering and text-to-SQL
  systems.
---

# TARGET: Benchmarking Table Retrieval for Generative Tasks

## Quick Facts
- arXiv ID: 2505.11545
- Source URL: https://arxiv.org/abs/2505.11545
- Reference count: 17
- Primary result: Dense embedding-based retrievers significantly outperform sparse lexical baselines like BM25 for table retrieval in generative tasks, especially when metadata is limited.

## Executive Summary
This paper introduces TARGET, a benchmark designed to evaluate table retrieval methods in the context of generative tasks that involve structured data. The key challenge addressed is identifying the correct tables for answering natural language queries, which is critical for building effective question-answering and text-to-SQL systems. The benchmark evaluates both retrieval performance in isolation and its impact on downstream tasks such as question answering, fact verification, and text-to-SQL generation. Results show that dense embedding-based retrievers significantly outperform sparse lexical baselines like BM25, especially when metadata such as table titles are missing or uninformative. Retrieval performance varies notably across datasets and tasks, with dense embeddings of table content (including column names and rows) generally yielding the best results.

## Method Summary
The authors construct TARGET by gathering table-query pairs from various existing datasets across three downstream tasks: question answering, fact verification, and text-to-SQL generation. For each task, they curate or map datasets that include both natural language queries and associated tables. They evaluate both sparse retrievers (BM25) and dense retrievers (such as Contriever, ColBERTv2, ANCE, and DSP) using different encoding strategies for tables (title-only, column names, rows, or combinations). Retrieval performance is measured in isolation using metrics like Mean Reciprocal Rank (MRR), and the impact on downstream tasks is evaluated by comparing answer accuracy, fact verification accuracy, and SQL query accuracy when using oracle versus retrieved tables.

## Key Results
- Dense embedding-based retrievers significantly outperform sparse lexical baselines like BM25 for table retrieval.
- Retrieval performance is highly sensitive to metadata availability, with dense embeddings of table content outperforming title-only retrieval.
- Retrieval performance varies notably across datasets and tasks, with some tasks (e.g., text-to-SQL) showing greater sensitivity to retrieval quality.

## Why This Works (Mechanism)
The success of dense embedding-based retrievers stems from their ability to capture semantic similarity between queries and tables, which is crucial when metadata is sparse or absent. Sparse methods like BM25 rely on lexical overlap and struggle when queries and table content do not share exact terms. Dense retrievers encode tables and queries into a shared embedding space, allowing them to match semantically related content even when surface forms differ. This is particularly important for table retrieval, where the relationship between a query and the relevant table often depends on understanding the content within the table, not just its title or metadata.

## Foundational Learning
- **Sparse vs. Dense Retrieval**: Sparse retrieval (e.g., BM25) relies on exact keyword matching, while dense retrieval uses learned embeddings to capture semantic similarity. Why needed: To understand the fundamental difference in how retrieval methods match queries to tables. Quick check: Compare retrieval results for a query that uses synonyms or related terms but not exact keywords.
- **Mean Reciprocal Rank (MRR)**: A metric that measures the rank of the first relevant result, averaged over all queries. Why needed: To evaluate retrieval performance, especially when there is only one relevant table per query. Quick check: Calculate MRR for a small set of queries and verify the expected ranking behavior.
- **Table Representation Strategies**: Different ways to encode tables for retrieval, such as using titles only, column names, or full table content. Why needed: To understand how the choice of table representation affects retrieval performance. Quick check: Compare retrieval results when using only table titles versus full table content for a set of queries.

## Architecture Onboarding
- **Component Map**: Query -> Retriever (BM25 or Dense) -> Retrieved Tables -> Downstream Task (QA, FV, Text-to-SQL)
- **Critical Path**: The retriever's ability to return the correct table is the critical step; errors here propagate to all downstream tasks.
- **Design Tradeoffs**: Dense retrievers require training and more computational resources but capture semantic similarity better than sparse retrievers, which are faster but rely on lexical overlap.
- **Failure Signatures**: Sparse retrievers fail when queries and tables use different terminology; dense retrievers may fail if the embedding space does not adequately capture table-query relationships.
- **First Experiments**:
  1. Compare BM25 and dense retriever performance on a small, labeled table-query dataset.
  2. Evaluate the impact of table representation (title-only vs. full content) on retrieval accuracy.
  3. Measure the downstream impact of retrieval errors on a simple QA or fact verification task.

## Open Questions the Paper Calls Out
- How do retrieval methods scale to very large table corpora?
- What is the impact of retrieval errors on downstream task performance as corpus size increases?
- How do different table representation strategies affect retrieval in the presence of noisy or incomplete metadata?

## Limitations
- The benchmark is limited to English-language datasets and may not generalize to other languages.
- The evaluation focuses on a specific set of downstream tasks; performance on other generative tasks is not assessed.
- The study does not address the computational cost or latency of different retrieval methods in production settings.

## Confidence
- Dense retrievers outperform sparse retrievers: High
- Retrieval performance is sensitive to metadata: High
- Performance varies across tasks and datasets: Medium
- Dense retrievers scale well to large corpora: Low (not directly tested)

## Next Checks
- Replicate the retrieval performance comparison on a new, unseen table-query dataset.
- Evaluate the impact of retrieval errors on downstream task performance as corpus size increases.
- Test the robustness of dense retrievers to noisy or incomplete metadata in real-world table corpora.