---
ver: rpa2
title: Gricean Norms as a Basis for Effective Collaboration
arxiv_id: '2503.14484'
source_url: https://arxiv.org/abs/2503.14484
tags:
- norms
- lamoid
- instruction
- human
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a normative framework that integrates Gricean\
  \ maxims with cognitive frameworks\u2014common ground, relevance theory, and theory\
  \ of mind\u2014into LLM-based agents to improve human-AI collaboration. The framework\
  \ enables agents to interpret and respond to unclear instructions (ambiguous, incomplete,\
  \ invalid, or irrelevant) by detecting norm violations and applying inference."
---

# Gricean Norms as a Basis for Effective Collaboration

## Quick Facts
- arXiv ID: 2503.14484
- Source URL: https://arxiv.org/abs/2503.14484
- Authors: Fardin Saad; Pradeep K. Murukannaiah; Munindar P. Singh
- Reference count: 40
- One-line primary result: GPT-4 agent guided by Gricean norms achieves 95.27% task accuracy vs 74.73% without norms in grid-world instruction interpretation

## Executive Summary
This paper presents a normative framework for improving human-AI collaboration by integrating Gricean maxims with cognitive frameworks like common ground, relevance theory, and theory of mind into LLM-based agents. The framework enables agents to interpret and respond to unclear instructions by detecting norm violations and applying inference. Using GPT-4 with Few-shot Chain-of-Thought prompting, the authors implement Lamoidsâ€”agents that follow these norms to navigate grid-world tasks. Experiments show that the norm-guided Lamoid achieves significantly higher task accuracy (95.27%), response relevancy (96.36%), and clarity (96.82%) compared to a non-norm counterpart.

## Method Summary
The method involves a GPT-4 agent with Few-shot Chain-of-Thought prompting that processes instructions in a grid-world domain (Doors, Keys, and Gems). The agent uses a four-component prompt: (1) General Chain-of-Thought with background and grid details, (2) Gricean and Inference Norms definitions, (3) Response Generation template, and (4) Few-shot CoT exemplars. The agent classifies instructions by Gricean norm violations (Quantity, Quality, Relation, Manner) and applies inference to determine optimal actions or clarification requests. The framework is evaluated across 55 instructions in 25 grid configurations with two conditions: with-norms and without-norms.

## Key Results
- Task Accuracy: 95.27% with norms vs 74.73% without norms
- Options Accuracy: 97.74% with norms vs 90.74% without norms
- Response Relevancy: 96.36% with norms vs 85.18% without norms
- Response Clarity: 96.82% with norms vs 90.73% without norms
- Norm violation detection precision ranges from 63% (Quantity) to 94% (Relation)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicitly classifying instruction types by Gricean norm violations improves an LLM's ability to select appropriate response strategies (execution vs. clarification).
- **Mechanism:** The model is prompted to first label the instruction with a specific violation (e.g., "Quantity Violation," "Manner Violation"). This intermediate step acts as a semantic router, forcing the model to resolve the ambiguity before generating the action.
- **Core assumption:** The LLM possesses sufficient pragmatic reasoning capabilities to accurately map natural language nuances to the predefined normative categories.
- **Evidence anchors:**
  - [abstract] "...normative framework... adopts the Gricean maxims... along with inference, as Gricean norms to interpret unclear instructions..."
  - [section 3] "The Lamoid processes instructions... The pipeline then classifies Lamoid's responses into two primary categories: Optimal Actions [or] Clarification Actions."
  - [corpus] "Applying the Gricean Maxims to a Human-LLM Interaction Cycle" (arXiv:2503.00858) supports the general utility of Gricean maxims in LLM interaction design.
- **Break condition:** Ambiguous instructions that violate multiple norms simultaneously, potentially causing the classifier to oscillate or mislabel (e.g., an instruction that is both invalid and ambiguous).

### Mechanism 2
- **Claim:** Injecting cognitive frameworks (Common Ground, Theory of Mind) into the prompt context reduces hallucination by grounding reasoning in shared environmental state.
- **Mechanism:** The "General Chain-of-Thought" prompt component provides a textual "grounding layer" describing the grid, objects, and roles. This constrains the model's latent space to valid state configurations before it attempts inference.
- **Core assumption:** The model can maintain attention over the provided state description (the grid) without losing track of the user's intent during the reasoning process.
- **Evidence anchors:**
  - [section 2.2] "...common ground enables agents to align with the human's goals by leveraging a mutual understanding of the environment..."
  - [section 4.2] "The first component of the prompt provides a Lamoid with the details of the environment... establishing the necessary common ground."
  - [corpus] Corpus evidence for specific "Theory of Mind" injection mechanisms in grid-worlds is weak; related papers focus on general cooperative norms rather than ToM prompt engineering.
- **Break condition:** Complex spatial relationships that are difficult to convey purely textually (the "Limitations of GPT-4" section notes poor performance in spatial reasoning).

### Mechanism 3
- **Claim:** Few-shot Chain-of-Thought (Fs-CoT) demonstration enforces the "Inference Norm," guiding the model to resolve violations rather than merely rejecting invalid input.
- **Mechanism:** By providing examples where the model detects a violation (e.g., "Quality Violation") and immediately generates a corrective inference (e.g., offering a valid alternative key), the prompt conditions the model to prioritize task completion over literal compliance.
- **Core assumption:** The inference patterns in the few-shot examples generalize well to unseen instruction types or grid configurations.
- **Evidence anchors:**
  - [abstract] "...using GPT-4 with Few-shot Chain-of-Thought prompting... to interpret and respond to unclear instructions."
  - [section 2.1] "When a norm violation is detected, the Lamoid applies the Inference norm to determine the optimal course of action."
  - [corpus] "Normative Equivalence in Human-AI Cooperation" (arXiv:2601.20487) suggests behavior drives cooperation, supporting the need for active resolution rather than passive rejection.
- **Break condition:** Tasks requiring optimal pathfinding, where the LLM's reasoning fails to find the most efficient route despite adhering to communication norms.

## Foundational Learning

- **Concept:** **Gricean Maxims (Cooperative Principle)**
  - **Why needed here:** This is the core logic used to classify user intent. Without understanding Quantity (sufficiency), Quality (truth), Relation (relevance), and Manner (clarity), you cannot debug why the agent flags specific instructions as violations.
  - **Quick check question:** If a user says "Get the key" in a room with three keys, which maxim is violated and why?

- **Concept:** **Common Ground & Theory of Mind (ToM)**
  - **Why needed here:** These cognitive frameworks are implemented as prompt context to simulate "shared knowledge." You need to understand that the agent isn't just parsing text, but maintaining a "mental model" of what the human likely knows or intends.
  - **Quick check question:** How does the "General Chain-of-Thought" prompt component simulate Common Ground?

- **Concept:** **Few-shot Chain-of-Thought (Fs-CoT) Prompting**
  - **Why needed here:** The architecture relies on 14 specific labeled examples to steer the model. Understanding how these examples structure the output (Norm identification -> Reasoning -> Response) is critical for modifying agent behavior.
  - **Quick check question:** Why might the model fail if you remove the reasoning steps from the few-shot examples?

## Architecture Onboarding

- **Component map:** Input Processor -> Context Layer (Prompt Part 1) -> Normative Layer (Prompt Part 2) -> Reasoning Engine (Prompt Part 3 & 4) -> Response Generator
- **Critical path:** The "Norm Identification" step. If the model misclassifies the violation type (e.g., labeling an ambiguous instruction as "Invalid"), the subsequent inference and action generation will likely fail.
- **Design tradeoffs:**
  - **Textual vs. Spatial Representation:** The authors chose to serialize the grid into text/adjacency matrices because GPT-4 lacks native spatial vision capabilities for this specific task. This improves compatibility but introduces hallucination risks (Section 8.1).
  - **Temperature (0.2):** Set low for reproducibility and reduced creativity. This stabilizes norm adherence but may reduce the agent's ability to handle highly creative or novel user phrasing.
- **Failure signatures:**
  - **Spatial Hallucination:** The agent claims a key is at coordinate (x,y) when it is actually elsewhere.
  - **Norm Misclassification:** Labeling a clear instruction as "Quantity Violation" (False Positive on Quantity violations was noted in Table 4).
  - **Inefficient Pathing:** The agent suggests a valid but suboptimal route (valid per norms, poor per efficiency).
- **First 3 experiments:**
  1. **Ablation on Cognitive Frameworks:** Remove the "Theory of Mind" and "Relevance Theory" sections from the prompt and measure the drop in response relevancy (RQ1/RQ3).
  2. **Violation Confusion Matrix:** Run the "Without Norms" baseline on the test set and manually label which Gricean norms it violates in its failures (e.g., does it hallucinate facts? ignore relevance?).
  3. **Pathfinding Stress Test:** Create grid configurations requiring complex navigation (e.g., dead ends) to isolate the GPT-4 spatial reasoning failure mode from the normative reasoning failure mode.

## Open Questions the Paper Calls Out
- Does the normative framework generalize to complex, dynamic domains beyond the grid world environment? (exploring scalability across operational domains)
- How can the framework be extended to process broader pragmatic elements such as sarcasm, humor, and emotional cues? (current model elides important aspects of pragmatics)
- Can the method reduce its dependence on extensive, task-specific prompt engineering? (extensive prompt engineering is not sustainable for broader, more dynamic tasks)

## Limitations
- Performance heavily depends on extensive prompt engineering with 14 task-specific few-shot examples
- Current implementation ignores important pragmatic elements like emotions, humor, and sarcasm
- Small test corpus (55 instructions across 25 grids) may not capture edge cases in real-world applications

## Confidence
- **High Confidence**: Quantitative improvements in task accuracy (95.27% vs 74.73%) and response metrics are directly measurable
- **Medium Confidence**: Cognitive frameworks improve grounding is supported by results but mechanism relies on textual representation
- **Low Confidence**: Generalizability of 14 few-shot examples across diverse instruction types remains unproven due to limited test corpus

## Next Checks
1. **Norm-specific ablation**: Remove each Gricean norm individually from the prompt and measure the impact on task accuracy to isolate their independent contributions
2. **Cross-domain transfer**: Apply the same normative framework to a non-spatial task (e.g., text-based adventure or dialogue task) to test if the Gricean-based approach generalizes beyond grid-worlds
3. **Stress testing**: Generate adversarial instructions that violate multiple norms simultaneously to determine if the classifier's 63% precision on Quantity violations extends to compound violations