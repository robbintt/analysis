---
ver: rpa2
title: Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under
  Adaptive Fusion
arxiv_id: '2506.09999'
source_url: https://arxiv.org/abs/2506.09999
tags:
- learning
- feature
- multimodal
- mcil
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Multimodal Class-Incremental
  Learning (MCIL) across vision, audio, and text modalities, which traditional methods
  focusing on only vision and text cannot handle effectively. The authors propose
  a novel method based on multimodal pre-trained models that tackles catastrophic
  forgetting and leverages complementary information across modalities.
---

# Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion

## Quick Facts
- **arXiv ID:** 2506.09999
- **Source URL:** https://arxiv.org/abs/2506.09999
- **Reference count:** 40
- **Primary result:** State-of-the-art MCIL performance (67.76% Top-1 on miniARIC, 95.37% on ImageNet-ESC-19, 96.74% on ImageNet-ESC-27)

## Executive Summary
This paper addresses the challenge of Multimodal Class-Incremental Learning (MCIL) across vision, audio, and text modalities, which traditional methods focusing on only vision and text cannot handle effectively. The authors propose a novel method based on multimodal pre-trained models that tackles catastrophic forgetting and leverages complementary information across modalities. The key innovations include: 1) a Multimodal Incremental Feature Extractor (MIFE) using Mixture-of-Experts structure for effective incremental fine-tuning, 2) an Adaptive Audio-Visual Fusion Module (AAVFM) with masking threshold and dynamic fusion mechanisms, and 3) a novel multimodal class-incremental contrastive training loss. The method is evaluated on three datasets (miniARIC, ImageNet-ESC-19, and ImageNet-ESC-27) and achieves state-of-the-art performance, with average Top-1 accuracy of 67.76% on miniARIC (vs. 49.19% for the next best method), 95.37% on ImageNet-ESC-19, and 96.74% on ImageNet-ESC-27 under various task settings. The paper also introduces two new MCIL-specific evaluation metrics to comprehensively assess model performance.

## Method Summary
The proposed method leverages a pre-trained AudioCLIP model and introduces three key innovations. First, the Multimodal Incremental Feature Extractor (MIFE) employs a Mixture-of-Experts (MoE) structure with task-specific adapters and MLP routers inserted into each transformer block, enabling effective incremental fine-tuning while preventing catastrophic forgetting. Second, the Adaptive Audio-Visual Fusion Module (AAVFM) dynamically determines whether to fuse or use only the strong modality based on Pearson correlation coefficient between visual and audio features, with a masking threshold of 0.8. Third, a novel multimodal class-incremental contrastive training loss combines similarity-weighted cross-entropy (L_CW) with mutual information maximization (L_MI) using a weighted combination (α=0.7). The model is trained using AdamW optimizer with CosineAnnealingLR scheduler on RTX 4090 GPU, evaluating on datasets with task splits ranging from T={3,5,6,8}.

## Key Results
- Achieves state-of-the-art average Top-1 accuracy of 67.76% on miniARIC (vs. 49.19% for next best method)
- Demonstrates strong performance on clean datasets: 95.37% on ImageNet-ESC-19 and 96.74% on ImageNet-ESC-27
- Introduces two novel MCIL-specific evaluation metrics (M1 and M2) for comprehensive performance assessment

## Why This Works (Mechanism)
The method addresses MCIL by combining three complementary mechanisms: MoE adapters in MIFE enable task-specific adaptation without overwriting previous knowledge, adaptive fusion in AAVFM leverages the complementary strengths of different modalities while avoiding noise from weak modalities, and the contrastive loss encourages both discriminative learning and information preservation across modalities. The frozen pre-trained backbone provides strong initialization while the learnable MoE components handle incremental learning. The Pearson correlation-based fusion decision effectively handles the varying quality of multimodal data.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: A neural network architecture where multiple expert networks are used, with a gating network determining which experts to use for each input. Why needed: Enables efficient incremental learning by adding task-specific experts without retraining the entire model. Quick check: Verify that different tasks activate different experts in the router.
- **Modality Fusion Strategies**: Techniques for combining information from multiple data types (vision, audio, text). Why needed: MCIL requires leveraging complementary information across modalities while handling their varying quality. Quick check: Test fusion performance with synthetic correlated and uncorrelated modality pairs.
- **Contrastive Learning**: A training approach that pulls together similar samples while pushing apart dissimilar ones. Why needed: Helps maintain discriminative features across incremental tasks and modalities. Quick check: Verify that embeddings of the same class are closer than embeddings of different classes.
- **Catastrophic Forgetting**: The tendency of neural networks to forget previously learned information when trained on new tasks. Why needed: MCIL involves sequential learning of new classes, making forgetting a critical challenge. Quick check: Measure performance degradation on previous tasks after training on new ones.
- **Pearson Correlation Coefficient**: A statistical measure of linear correlation between two variables. Why needed: Used in AAVFM to determine when modalities are sufficiently correlated to be worth fusing. Quick check: Compute correlation between clean and noisy audio-visual pairs.
- **Mutual Information**: A measure of the mutual dependence between two random variables. Why needed: Used in the contrastive loss to encourage preservation of information across modalities. Quick check: Estimate MI between fused and unimodal features using appropriate estimators.

## Architecture Onboarding

**Component Map:** AudioCLIP backbone (frozen) -> MIFE (MoE adapters + routers) -> AAVFM (correlation + fusion) -> MLP head

**Critical Path:** Pre-trained backbone → MIFE adapters → AAVFM → Cross-attention fusion → MLP classification head

**Design Tradeoffs:** The method trades increased model complexity (MoE adapters, correlation computation, mutual information loss) for better incremental learning performance and modality utilization. The frozen backbone limits catastrophic forgetting but requires careful design of the incremental components.

**Failure Signatures:** Poor performance on early tasks indicates catastrophic forgetting; consistently low fusion rates suggest inappropriate correlation threshold; unstable training may indicate mutual information loss estimation issues.

**First Experiments:**
1. Train the model with α=1.0 (mutual information loss disabled) to isolate the contribution of L_CW
2. Test different Pearson correlation thresholds (0.6, 0.7, 0.8, 0.9) to find optimal fusion behavior
3. Evaluate the model with different task splits (T=3, T=5, T=8) to assess scalability

## Open Questions the Paper Calls Out
- **Open Question 1:** How can the proposed Adaptive Audio-Visual Fusion Module (AAVFM) be effectively adapted to handle MCIL scenarios where one or more modalities are completely absent during inference or training? The current AAVFM mechanism relies on calculating the Pearson correlation coefficient between visual and audio features to determine the masking threshold, making the current architecture vulnerable to modality dropout.

- **Open Question 2:** Is the fixed masking threshold (th=0.8) robust across significantly different noise environments, or does it require adaptive tuning for specific data streams? The paper evaluates the method on specific datasets but doesn't provide a sensitivity analysis for th, which may degrade in high-noise or out-of-distribution scenarios.

- **Open Question 3:** Does the Multimodal Incremental Feature Extractor (MIFE) encounter scalability bottlenecks, such as router saturation or parameter explosion, when applied to long-sequence incremental tasks (e.g., T > 50)? The experimental settings limit the number of tasks to T ∈ {3, 5, 6, 8}, but the linear accumulation of task-specific experts could lead to optimization difficulties in lifelong learning settings with hundreds of tasks.

## Limitations
- Missing critical training hyperparameters (learning rate, batch size, epochs) that are essential for reproduction
- Dataset-specific evaluation with unclear generalizability, particularly regarding the fixed Pearson correlation threshold
- Novel custom evaluation metrics (M1, M2) without mathematical formulations or implementation details

## Confidence
- **State-of-the-art performance claims (67.76%, 95.37%, 96.74% accuracy)**: Low confidence due to missing hyperparameters and unclear metric definitions
- **Catastrophic forgetting mitigation**: Medium confidence - MoE adapters are a known effective approach, but exact forgetting measurements are not detailed
- **Adaptive fusion mechanism effectiveness**: Medium confidence - the Pearson correlation-based approach is plausible but threshold selection is not validated across datasets

## Next Checks
1. **Hyperparameter sensitivity analysis**: Reproduce results with a grid of learning rates (1e-5 to 1e-3) and batch sizes (8-32) to determine the sensitivity of performance to these critical but unspecified parameters.

2. **Fusion mechanism ablation**: Test the model with different Pearson correlation thresholds (0.6, 0.7, 0.8, 0.9) and with fusion disabled entirely to quantify the contribution of the adaptive fusion module to overall performance.

3. **Metric verification**: Implement the custom M1 and M2 metrics from scratch and verify they align with the paper's descriptions, then compare model rankings using standard metrics versus the proposed metrics to assess their practical utility.