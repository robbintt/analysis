---
ver: rpa2
title: On the Sustainability of AI Inferences in the Edge
arxiv_id: '2507.23093'
source_url: https://arxiv.org/abs/2507.23093
tags:
- edge
- inference
- power
- devices
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for a systematic evaluation of AI
  model performance and energy usage across diverse edge devices, enabling informed
  deployment decisions. The authors propose a unified measurement scheme integrating
  hardware-based power profiling with software-based performance monitoring across
  four widely used edge platforms (Raspberry Pi, Intel Neural Compute Stick, NVIDIA
  Jetson Nano, and Google Coral USB) and multiple AI model categories (traditional
  ML, neural networks, and large language models).
---

# On the Sustainability of AI Inferences in the Edge

## Quick Facts
- arXiv ID: 2507.23093
- Source URL: https://arxiv.org/abs/2507.23093
- Reference count: 39
- Primary result: Systematic evaluation of AI model performance and energy usage across diverse edge devices enables informed deployment decisions.

## Executive Summary
This paper presents a comprehensive evaluation framework for AI model sustainability on edge devices, measuring performance-energy tradeoffs across four platforms and multiple model categories. The authors develop a unified measurement scheme integrating hardware power profiling with software performance monitoring, enabling reproducible cross-platform comparisons. Their systematic characterization reveals critical hardware-specific optimizations and parameter tuning strategies necessary for balancing inference speed, accuracy, and power consumption on resource-constrained devices.

## Method Summary
The study benchmarks AI models on Raspberry Pi 4, Intel Neural Compute Stick 2, Google Coral USB, and NVIDIA Jetson Nano using a five-phase measurement protocol. Models are trained in TensorFlow/Keras, converted to device-specific formats (TensorRT, OpenVINO IR, EdgeTPU .tflite), and evaluated across MNIST, ImageNet, GLUE, and OpenAssistant datasets. The unified measurement captures baseline idle power (3-second window via USB power meter), executes inference while logging timestamps and memory via psutil, and computes net inference power by subtracting baseline from total consumption. Multiple runs provide 95% confidence intervals for all metrics.

## Key Results
- Hardware-specific optimizations and model parameter tuning are critical for balancing performance-resource trade-offs
- Jetson Nano and Google Coral generally provide the best trade-offs for complex models
- Simpler models perform adequately on resource-constrained devices like Raspberry Pi

## Why This Works (Mechanism)

### Mechanism 1: Hardware-Specific Optimization
Domain-specific edge accelerators with matching framework optimizations yield superior performance-resource trade-offs compared to general-purpose devices. Specialized silicon (Jetson GPU, Coral EdgeTPU, Intel VPU) exploits parallelism in neural network operations through hardware-aware compilation (TensorRT layer fusion, EdgeTPU quantization acceleration, OpenVINO IR optimization), reducing memory bandwidth bottlenecks and improving compute efficiency.

### Mechanism 2: Unified Measurement Scheme
Integrating hardware-based power profiling with software-based performance monitoring enables consistent, reproducible cross-platform evaluation. A five-phase Python script captures baseline idle power (3-second window, 48 samples at 16 Hz via USB power meter), executes inference while logging timestamps and memory via psutil, then computes net inference power by subtracting baseline from total consumption.

### Mechanism 3: Inference Parameter Tuning
Device-aware tuning of inference parameters (input size, batch size, token length) optimizes performance-resource trade-offs without model retraining. Constrained devices (RPi) require small batches and inputs to avoid memory saturation; capable accelerators (Jetson, Coral) exploit larger batches to amortize kernel launch overhead and improve throughput-per-watt.

## Foundational Learning

- **Concept: Quantization for Edge Deployment**
  - Why needed here: All tested configurations rely on quantized or reduced-precision models (LiteRT, TensorRT INT8/FP16); understanding precision-accuracy trade-offs is prerequisite for correct model conversion.
  - Quick check question: Why does INT8 quantization reduce both memory footprint and inference latency on EdgeTPU, but may degrade F1-score for some models?

- **Concept: Edge Accelerator Architectures (GPU vs TPU vs VPU)**
  - Why needed here: The paper compares fundamentally distinct architectures; selecting the right device requires understanding their computational paradigms.
  - Quick check question: What specific operation types does the EdgeTPU accelerate versus the Jetson GPU, and how does this affect model compatibility?

- **Concept: Model Conversion Pipelines**
  - Why needed here: Each accelerator requires a specific intermediate format (.tflite, TensorRT engine, OpenVINO IR, EdgeTPU-compiled); incorrect conversion invalidates benchmark results.
  - Quick check question: What failure mode occurs if you deploy a standard TensorFlow SavedModel directly on Google Coral without EdgeTPU compilation?

## Architecture Onboarding

- **Component map:** Raspberry Pi 4 -> USB power meter (16 Hz sampling) -> psutil (memory logging) -> Python time module (latency); Intel NCS -> OpenVINO IR -> OpenVINO runtime; Google Coral -> EdgeTPU compiler -> EdgeTPU runtime; NVIDIA Jetson Nano -> TensorRT engine -> TensorRT runtime

- **Critical path:** Train model in TensorFlow → save as H5 → convert to device-specific format via appropriate compiler (TensorRT, OpenVINO, EdgeTPU) → deploy unified measurement script (initialization → baseline power → inference → logging) → collect F1-score, inference time, power consumption, memory utilization

- **Design tradeoffs:** Jetson Nano + TensorRT: best inference speed and memory efficiency; highest power draw; largest form factor; RPi + Coral USB: strong inference speed at low power; requires INT8 quantization; limited model compatibility; RPi + Intel NCS: moderate performance with good power efficiency; OpenVINO dependency; slower for complex CNNs; RPi standalone: lowest power; highest latency; best framework compatibility

- **Failure signatures:** OOM errors on RPi with batch size > 1 for ResNet-50 or MobileSSD; EdgeTPU compilation failure for models with unsupported TensorFlow ops; inconsistent power readings when background daemons are not terminated; TensorRT engine build failures for models with dynamic input shapes

- **First 3 experiments:** Baseline validation: Run MobileNetV2 inference on all five configurations with identical inputs; verify F1-score consistency and measurement reproducibility (3 runs, 95% CI); Batch size sweep: On Jetson Nano + TensorRT, benchmark ResNet-50 at batch sizes [1, 4, 8, 16, 32]; plot inference time and power per sample to identify optimal operating point; Model complexity stress test: Compare TinyBERT vs Phi-2-orange on RPi + Coral; measure maximum token length before memory failure to establish deployment feasibility boundaries

## Open Questions the Paper Calls Out

### Open Question 1
Can a learning-based system effectively automate parameter tuning for energy efficiency across heterogeneous edge devices? The authors state they will "develop a learning-based system for automated parameter tuning" because current heuristics may not work across diverse hardware. A machine learning model consistently outperforming static heuristics in optimizing energy and latency across different platforms would resolve this.

### Open Question 2
How can an end-to-end system automatically select optimal devices, models, and parameters based solely on user intent? The discussion proposes a "fully automated measurement tool" that accepts user QoS requirements to select configurations without manual input. A functional prototype that autonomously configures and deploys models based on high-level application constraints would resolve this.

### Open Question 3
Can software-based power estimation tools achieve accuracy comparable to hardware meters for fine-grained inference profiling? The authors note the "absence of a software" solution as a validity threat, which forced the adoption of hardware USB power meters. A comparative study showing software estimates falling within an acceptable error margin of hardware readings would resolve this.

## Limitations

- Dependence on specific hardware configurations and proprietary toolchains (TensorRT, OpenVINO, EdgeTPU compiler) that may not generalize across all edge deployments
- Model conversion procedures and quantization parameters were not fully specified, creating potential reproducibility barriers
- The unified measurement methodology lacks validation against established power profiling standards, and baseline power subtraction assumes thermal stability that may not hold in continuous operation scenarios

## Confidence

- **High Confidence:** Hardware-specific optimization effects (Jetson Nano and Coral outperforming on complex models) - supported by multiple measurement runs and consistent with known accelerator architectures
- **Medium Confidence:** Unified measurement scheme validity - methodology is documented but lacks external validation or comparison with alternative power profiling approaches
- **Low Confidence:** Inference parameter tuning recommendations - while theoretical mechanisms are sound, specific optimal batch sizes and input configurations may vary with different model architectures or datasets

## Next Checks

1. Compare USB power meter readings against Intel Power Gadget and nVIDIA-SMI outputs for the same inference runs to quantify measurement consistency and identify potential cross-platform biases

2. Test model deployment success rates when converting from TensorFlow SavedModel to device-specific formats using different quantization approaches, documenting failure rates and accuracy degradation patterns

3. Measure inference performance and power consumption over continuous 30-minute inference sessions to identify thermal throttling effects and their impact on the reported trade-offs