---
ver: rpa2
title: 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking'
arxiv_id: '2601.06487'
source_url: https://arxiv.org/abs/2601.06487
tags:
- tool
- mountain
- open-ended
- qingcheng
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ArenaRL addresses the discriminative collapse problem in reinforcement
  learning for open-ended agent tasks by replacing pointwise scalar rewards with a
  tournament-based relative ranking mechanism. The approach introduces process-aware
  pairwise evaluation and a seeded single-elimination tournament topology that achieves
  O(N) complexity while maintaining high ranking accuracy.
---

# ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking

## Quick Facts
- arXiv ID: 2601.06487
- Source URL: https://arxiv.org/abs/2601.06487
- Authors: Qiang Zhang; Boli Chen; Fanrui Zhang; Ruixue Ding; Shihang Wang; Qiuchen Wang; Yinfeng Huang; Haonan Zhang; Rongxiang Zhu; Pengyong Wang; Ailin Ren; Xin Li; Pengjun Xie; Jiawei Liu; Ning Guo; Jingren Zhou; Zheng-Jun Zha
- Reference count: 40
- Primary result: ArenaRL achieves average win rates of 41.8% on Open-Travel and 64.3% on Open-DeepResearch benchmarks while maintaining 99% valid generation rate

## Executive Summary
ArenaRL addresses the discriminative collapse problem in reinforcement learning for open-ended agent tasks by replacing pointwise scalar rewards with a tournament-based relative ranking mechanism. The approach introduces process-aware pairwise evaluation and a seeded single-elimination tournament topology that achieves O(N) complexity while maintaining high ranking accuracy. Experiments show ArenaRL significantly outperforms baselines like GRPO and GSPO, achieving average win rates of 41.8% on Open-Travel and 64.3% on Open-DeepResearch benchmarks, with a 99% valid generation rate. The method also demonstrates substantial improvements on open-ended writing tasks across three public benchmarks, validating its effectiveness in driving robust reasoning and planning capabilities in LLM agents.

## Method Summary
ArenaRL implements a tournament-based relative ranking system for open-ended agent tasks. The method uses a seeded single-elimination tournament where N+1 trajectories (N sampled + 1 greedy anchor) compete in pairwise comparisons judged by an LLM with process-aware evaluation rubrics. The seeding phase uses the greedy anchor for initial ranking, followed by an elimination phase that requires only 2N-2 comparisons total. Trajectories are scored using bidirectional pairwise evaluation to eliminate positional bias, and ranks are converted to standardized advantages for policy optimization using a PPO-style clipped objective with KL penalty.

## Key Results
- ArenaRL achieves 41.8% average win rate on Open-Travel vs 25.5% for GSPO baseline
- ArenaRL achieves 64.3% average win rate on Open-DeepResearch vs 26.4% for GSPO baseline
- Maintains 99% valid generation rate compared to 17-32% for baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Relative ranking through pairwise comparison mitigates discriminative collapse that plagues pointwise scalar rewards in open-ended tasks. Pointwise scoring compresses rewards for similar trajectories into a narrow range (e.g., 0.8–0.9), where evaluation noise variance approaches intra-group variance, yielding an extremely low signal-to-noise ratio. Pairwise comparison instead produces discrete win/loss judgments that are inherently more stable—humans and LLMs both exhibit higher reliability on preference judgments than absolute scoring.

### Mechanism 2
Seeded single-elimination tournaments achieve near-optimal ranking accuracy at O(N) complexity by preventing premature high-quality trajectory elimination. Standard single-elimination suffers from random seeding—strong candidates may meet early, eliminating one arbitrarily. The seeding phase uses a deterministic greedy-decoded trajectory as a quality anchor for pre-ranking, yielding a low-bias initial ordering that guides tournament bracket construction while requiring only 2N−2 total comparisons.

### Mechanism 3
Process-aware evaluation with bidirectional scoring captures reasoning quality beyond final-answer correctness. Standard reward models score only final outputs, ignoring trajectory quality. ArenaRL's judge receives the full trajectory plus multi-level rubrics assessing logical coherence, tool usage precision, and constraint satisfaction. Bidirectional scoring—swapping trajectory order and averaging—eliminates positional bias where judges favor first-presented candidates.

## Foundational Learning

### Concept: Group Relative Policy Optimization (GRPO)
Why needed: ArenaRL builds on GRPO's group-based advantage estimation but replaces its scalar rewards with tournament rankings. Understanding GRPO clarifies what ArenaRL modifies.
Quick check: Given a group of N trajectories with scalar rewards, how does GRPO compute advantages? (Answer: A_i = (R_i - μ) / σ, which fails when σ → 0)

### Concept: LLM-as-Judge Paradigm
Why needed: The "Arena Judge" is an LLM performing pairwise evaluations. Understanding judge biases explains why process-aware rubrics and bidirectional scoring are necessary.
Quick check: What two common biases affect LLM judges when comparing two responses? (Answer: Positional bias—preferring first/second position; and surface-feature bias—preferring longer or more formatted responses)

### Concept: Tournament Topologies
Why needed: ArenaRL selects among five tournament structures. Understanding their tradeoffs motivates the final design.
Quick check: Why does standard single-elimination produce inaccurate rankings even with perfect pairwise judgments? (Answer: Random seeding causes strong candidates to eliminate each other early, distorting final rankings)

## Architecture Onboarding

### Component map
Query → N+1 trajectories (N sampled + 1 greedy anchor) → Seeding phase (N-1 anchor comparisons) → Elimination phase (N-1 tournament matches) → Final rankings → Rank → quantile reward → Standardized advantage → Policy gradient update

### Critical path
1. Query → N+1 trajectories (N sampled + 1 greedy anchor)
2. Seeding phase: N−1 anchor comparisons → initial rankings
3. Elimination phase: N−1 tournament matches → final rankings
4. Rank → quantile reward → standardized advantage
5. Advantage → policy gradient update

### Design tradeoffs
- Group size N: Larger N improves exploration (Table 2 shows N=16 outperforms N=8) but increases latency
- Judge model: Stronger judges (Qwen3-Max) provide better signals but cost more
- Tournament topology: Seeded elimination chosen over round-robin (too expensive) and anchor-only (too coarse)

### Failure signatures
- Valid generation rate drops → context overflow in long-horizon tasks
- Win rates stagnate near 50% → judge providing random signals; check pairwise consistency
- Performance degrades after initial improvement → overfitting to judge preferences; increase KL penalty β

### First 3 experiments
1. Ablate tournament topology: Compare seeded elimination vs anchor-only vs round-robin on validation set. Expect seeded ≈ round-robin at ~8× lower cost.
2. Vary group size N: Test N ∈ {4, 8, 16} on Open-Travel subtasks. Expect monotonic improvement; identify point of diminishing returns.
3. Judge consistency audit: For 100 trajectory pairs, run 5 repeated comparisons. Compute pairwise agreement rate; flag if <70%.

## Open Questions the Paper Calls Out
- How can ArenaRL be efficiently extended to multimodal agent settings while maintaining the tournament-based ranking paradigm?
- How robust is the seeded single-elimination tournament to anchor trajectory quality degradation?
- Does using the same LLM family for both training-time judging and evaluation introduce optimization bias or circularity?
- How does ArenaRL scale to larger group sizes (N > 16) and longer-horizon trajectories with more tool invocations?

## Limitations
- Anchor quality dependency: The seeding phase assumes the greedy-decoded anchor provides a reliable quality proxy that may not hold for highly stochastic tasks
- Judge calibration stability: The method assumes the LLM judge's scoring distribution remains stable across different trajectory qualities and task types
- Generalization to non-LLM environments: ArenaRL assumes trajectory scoring via LLM judgment and may not transfer to domains where semantic understanding isn't sufficient for ranking

## Confidence
- High confidence: Discriminative collapse problem description, O(N) tournament complexity advantage, valid generation rate improvements
- Medium confidence: Relative ranking mechanism effectiveness, tournament topology selection rationale, process-aware evaluation benefits
- Low confidence: Anchor quality assumptions, judge calibration robustness, generalization beyond evaluated domains

## Next Checks
1. Anchor quality audit: For 100 random queries across all three benchmarks, compare greedy anchor trajectories against human-annotated quality scores to quantify anchor reliability and potential bias
2. Judge consistency stress test: Run 50 trajectory pairs through the Arena Judge with varying presentation orders, lengths, and formatting to measure robustness against known LLM judge biases beyond positional effects
3. Cross-domain transfer test: Apply ArenaRL to a non-agentic RL task (e.g., Atari or Mujoco) using a simple reward proxy to validate whether relative ranking benefits extend beyond LLM-based evaluation contexts