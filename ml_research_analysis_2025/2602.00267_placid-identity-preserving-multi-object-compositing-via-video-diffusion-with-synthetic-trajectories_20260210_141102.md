---
ver: rpa2
title: 'PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with
  Synthetic Trajectories'
arxiv_id: '2602.00267'
source_url: https://arxiv.org/abs/2602.00267
tags:
- objects
- object
- background
- image
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PLACID, a framework that transforms a collection
  of object images into an appealing multi-object composite. PLACID leverages a pretrained
  image-to-video diffusion model with text control to preserve object consistency,
  identities, and background details by exploiting temporal priors from videos.
---

# PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories

## Quick Facts
- arXiv ID: 2602.00267
- Source URL: https://arxiv.org/abs/2602.00267
- Reference count: 40
- Transforms object images into multi-object composites while preserving identities and backgrounds

## Executive Summary
This paper introduces PLACID, a framework that leverages pretrained video diffusion models to create multi-object composites that preserve object identities, background details, and color fidelity. The key innovation is using synthetic trajectory data where objects move from random positions to target layouts, combined with high-resolution object conditioning and special text tokens. This approach exploits temporal priors from video models to maintain object consistency across frames, producing a final composite that outperforms existing methods on identity preservation, background retention, and text alignment metrics.

## Method Summary
PLACID fine-tunes a pretrained image-to-video diffusion model (Wan 2.1) using synthetic training videos where objects follow linear trajectories from random to target positions. The model conditions on full-resolution object images (not downsampled composites) through CLIP encoding, with special text tokens (<OBJ></OBJ>, <BG></BG>) binding captions to visual inputs. At inference, objects initialized at random positions converge to coherent layouts over 33 frames, with the final frame serving as the output composite. Training uses a mix of in-the-wild product photography, manual designs, reposing pairs from Subject-200k, and synthetic 3D compositions.

## Key Results
- Achieves superior identity preservation (CLIP-I/DINO metrics) compared to state-of-the-art methods
- Better background retention (MSE-BG) and color fidelity (Chamfer Distance) than baselines
- Lower missing object rates and higher user preference scores across multiple benchmarks
- Successfully handles 1-5 objects while maintaining text alignment and spatial coherence

## Why This Works (Mechanism)

### Mechanism 1: Temporal Priors from Video Diffusion
Video models encode learned priors about object permanence and spatial consistency across frames. When objects must converge from random positions to coherent layouts, these priors prevent identity mixing and dropping that plague image-to-image approaches.

### Mechanism 2: Synthetic Trajectory Training Data
Linear constant-velocity trajectories maintain spatial consistency per frame, allowing the model to learn object-to-slot correspondence without identity degradation. Naive frame interpolation produces half-faded objects in multiple positions, lacking motion semantics.

### Mechanism 3: High-Resolution Object Conditioning
Concatenating full-resolution object images via CLIP encoding preserves fine details lost in downsampling. Special text tokens bind descriptions to specific visual inputs, preventing identity entanglement when multiple objects are present.

## Foundational Learning

- **Diffusion Transformers (DiT) for Video**: Understanding how temporal attention works across frames is essential since PLACID builds on Wan 2.1 I2V DiT.
  - Quick check: How does a DiT differ from a UNet-based diffusion model in handling video latents?

- **Flow Matching Training Objective**: The paper uses Flow Matching (Eq. 1) rather than standard DDPM; understanding velocity prediction is required.
  - Quick check: What does the model predict in Flow Matching vs. noise prediction in DDPM?

- **LoRA Fine-Tuning**: PLACID uses lightweight LoRA adapters to avoid overwriting pretrained weights.
  - Quick check: What rank and alpha values are typically used, and where are adapters inserted in a DiT?

## Architecture Onboarding

- **Component map**: CLIP ViT (objects/backgrounds) -> Text encoder (caption with special tokens) -> Wan 2.1 I2V DiT with LoRA adapters -> 33-frame video output

- **Critical path**: Construct F1 composite -> Encode objects/backgrounds via CLIP separately -> Tokenize caption with <OBJ></OBJ> markers -> DiT with cross-attention on visual/text embeddings -> Denoise to produce video frames

- **Design tradeoffs**: Shorter videos (9-17 frames) preserve identity better; longer videos (61-81) improve text alignment. Manual F1 placement offers layout control but conflicts with contradictory captions.

- **Failure signatures**: Missing objects from noisy Subject-200k intermediates; copy-paste behavior from over-reliance on In-the-Wild data; background alteration from insufficient background conditioning.

- **First 3 experiments**: 1) Ablate trajectory training: synthetic vs. naive interpolation on identity metrics. 2) Vary video length: 9, 33, 81 frames measuring identity vs. text-alignment tradeoff. 3) Token ablation: remove <OBJ> tokens and assess object-text binding accuracy.

## Open Questions the Paper Calls Out

- **Multi-view consistency**: Can the model be augmented to accept multiple reference images per object for improved novel view synthesis?
- **Computational efficiency**: Can the video-based pipeline be optimized to match image-to-image models without losing temporal priors?
- **Scalability limits**: How does reliability degrade with complex scenes containing >7 objects and intricate textual constraints?

## Limitations
- Heavy dependence on specific training data composition (Unsplash, Subject-200k, etc.)
- Assumes linear trajectories are sufficient for training video models on compositing
- Unclear upper bound on object count before identity preservation degrades

## Confidence
- **High** in general framework soundness: Video diffusion + temporal priors + trajectory training addresses core compositing challenges
- **Medium** in quantitative superiority: Proxy metrics (CLIP-I, DINO) may not perfectly capture perceptual identity
- **Low** in generalizability: Limited evidence beyond curated training distribution

## Next Checks
1. Evaluate on entirely disjoint datasets (COCO objects on Place365 backgrounds) to verify generalization
2. Systematically vary trajectory types (linear, curved, accelerated) during training to validate optimal motion patterns
3. Test with increasing object counts (2, 4, 8, 16) to identify breaking points for identity preservation