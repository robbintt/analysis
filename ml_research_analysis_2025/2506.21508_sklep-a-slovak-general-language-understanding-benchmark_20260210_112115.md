---
ver: rpa2
title: 'skLEP: A Slovak General Language Understanding Benchmark'
arxiv_id: '2506.21508'
source_url: https://arxiv.org/abs/2506.21508
tags:
- e-05
- base
- language
- translation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: skLEP introduces the first comprehensive Slovak language understanding
  benchmark, addressing the lack of standardized evaluation resources for this mid-resource
  language. The benchmark comprises nine tasks spanning token-level, sentence-pair,
  and document-level challenges, curated through new datasets and translations of
  established English resources with native speaker post-editing.
---

# skLEP: A Slovak General Language Understanding Benchmark

## Quick Facts
- arXiv ID: 2506.21508
- Source URL: https://arxiv.org/abs/2506.21508
- Reference count: 21
- Primary result: First comprehensive Slovak language understanding benchmark with 9 tasks spanning token-level, sentence-pair, and document-level challenges

## Executive Summary
skLEP introduces the first comprehensive benchmark for Slovak language understanding, addressing the lack of standardized evaluation resources for this mid-resource language. The benchmark comprises nine tasks spanning token-level (POS tagging, NER), sentence-pair (RTE, NLI, STS), and document-level (hate speech detection, sentiment analysis, QA) challenges. An extensive evaluation of Slovak-specific, multilingual, and English models shows that while SlovakBERT remains competitive, parameter-efficient multilingual models like mDeBERTaV3 deliver the highest error reduction. The benchmark, models, and evaluation toolkit are publicly released to foster reproducibility and future research in Slovak natural language understanding.

## Method Summary
The benchmark was created through a combination of curating existing Slovak datasets (UD, UNER, WGSK, HS, SA, SK-QuAD) and translating established English benchmarks (RTE, XNLI, STS) with native speaker post-editing. Models were evaluated using task-specific metrics including Macro F1 for token tasks, Accuracy for classification, and Pearson correlation for STS. The primary evaluation metric is Relative Error Reduction (RER) against SlovakBERT baseline. Training used grid search over 3 seeds (12, 42, 99) with batch size 12, AdamW optimizer, linear learning rate decay, gradient clipping of 1.0, and weight decay of 0. Different learning rate schedules were applied for token-level versus document-level tasks.

## Key Results
- SlovakBERT remains competitive despite being outperformed by parameter-efficient multilingual models
- mDeBERTaV3 delivers the highest error reduction across the benchmark
- ModernBERT models achieve significant RER improvements despite English-centric pre-training
- Document-level tasks show lower performance than token-level tasks, highlighting remaining challenges

## Why This Works (Mechanism)
The benchmark's effectiveness stems from combining curated native Slovak datasets with carefully translated and post-edited English resources, creating a comprehensive evaluation suite. The use of RER as a primary metric normalizes performance across tasks of varying difficulty. Parameter-efficient multilingual models outperform monolingual models by leveraging cross-lingual knowledge while maintaining task-specific adaptability through fine-tuning.

## Foundational Learning

Token-level tasks (POS, NER)
Why needed: Fundamental linguistic understanding at the word level
Quick check: Verify tokenization produces minimal unknown tokens for Slovak text

Sentence-pair tasks (RTE, NLI, STS)
Why needed: Tests reasoning and semantic understanding between sentences
Quick check: Confirm regression setup for STS rather than classification

Document-level tasks (HS, SA, QA)
Why needed: Evaluates contextual understanding across longer text spans
Quick check: Validate test set sizes are sufficient for statistical significance

## Architecture Onboarding

Component map: Transformer encoder -> Task-specific head -> Evaluation metrics

Critical path: Model loading → Dataset preparation → Fine-tuning → Evaluation → RER calculation

Design tradeoffs: Native datasets provide high quality but limited coverage vs translated datasets provide broader task coverage but potential translation artifacts

Failure signatures: ModernBERT low performance indicates tokenization issues; STS correlation problems suggest incorrect loss function setup

First experiments:
1. Verify SlovakBERT baseline performance on POS tagging task
2. Test tokenization quality on ModernBERT models with Slovak text
3. Confirm STS task uses regression rather than classification setup

## Open Questions the Paper Calls Out

**Open Question 1**
How do state-of-the-art generative large language models (LLMs) perform on skLEP compared to the evaluated encoder-only models?
Basis: Authors exclusively evaluate encoder-only models and note benchmark is currently "less suitable for generative models"
Resolution needed: Evaluation results from generative models using prompt-based strategies

**Open Question 2**
What is the human performance baseline for the diverse tasks within skLEP?
Basis: Paper explicitly lists absence of human baselines as limitation
Resolution needed: Native Slovak speakers completing test sets for all nine tasks

**Open Question 3**
To what extent do translation artifacts in training data impact model generalization?
Basis: Training sets for RTE, NLI, STS "have not been manually corrected" and may exhibit translationese
Resolution needed: Comparison of models trained on current data vs manually corrected training sets

## Limitations

- Test labels for translated datasets are only verified for RTE, creating potential quality issues for other tasks
- Document-level test sets are relatively small (283-640 samples), limiting statistical robustness
- Benchmark lacks consistent zero-shot evaluation protocol for assessing cross-lingual generalization
- Training data for some tasks contains translation artifacts that may bias model performance

## Confidence

High: Benchmark successfully establishes first comprehensive Slovak NLU evaluation suite with clear methodology and metrics
Medium: ModernBERT superiority claims limited by single epoch optimization due to computational constraints
Low: Claims of "comprehensive" coverage questionable given small test set sizes and lack of zero-shot evaluation

## Next Checks

1. Manually inspect 50 random samples from translated datasets (XNLI, STS) to assess translation artifacts and label consistency

2. Conduct statistical power analysis on document-level test sets to determine if current sample sizes provide sufficient statistical significance

3. Implement zero-shot evaluation protocol by testing models on English tasks first, then evaluating transfer performance to Slovak counterparts