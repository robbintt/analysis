---
ver: rpa2
title: Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews
arxiv_id: '2510.12490'
source_url: https://arxiv.org/abs/2510.12490
tags:
- patient
- medical
- questions
- system
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a task-oriented dialogue system for medical
  interviews using a Directed Acyclic Graph (DAG) framework powered by Large Language
  Models (LLMs). The system transforms medical algorithms into a corpus of clinical
  questions, employs hierarchical clustering for efficient cold-start questioning,
  and uses an expand-and-prune mechanism to adaptively guide conversations.
---

# Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews

## Quick Facts
- arXiv ID: 2510.12490
- Source URL: https://arxiv.org/abs/2510.12490
- Reference count: 23
- LLM-based medical interview system using DAG framework with high usability scores (86/100 for patients, 88.5/100 for physicians)

## Executive Summary
This paper presents a task-oriented dialogue system for medical interviews that transforms medical algorithms into a corpus of clinical questions and uses hierarchical clustering for efficient cold-start questioning. The system employs an expand-and-prune mechanism to adaptively guide conversations and includes termination logic to ensure sufficient information collection. Evaluation with five physicians showed low cognitive workload and high usability for both patient and physician applications, with strong satisfaction and effective integration into clinical workflows.

## Method Summary
The system converts medical algorithms into a corpus of clinical questions and builds a Directed Acyclic Graph (DAG) framework. It uses hierarchical clustering to identify relevant clinical domains for initial questioning, then applies an expand-and-prune mechanism to adaptively guide the conversation based on patient responses. A termination logic ensures adequate information collection before generating structured clinical reports for workflow integration.

## Key Results
- Low cognitive workload scores: 15.6/100 (patient app) and 26/100 (physician app)
- High usability scores: 86/100 (patient app) and 88.5/100 (physician app)
- Both applications demonstrated strong satisfaction and effective clinical workflow integration

## Why This Works (Mechanism)
The system works by transforming structured medical knowledge into conversational pathways that can be dynamically navigated. The DAG framework allows for systematic exploration of clinical domains while maintaining logical relationships between questions. The expand-and-prune mechanism enables adaptive conversation flow that responds to patient inputs in real-time, ensuring relevant information is gathered efficiently without overwhelming the user.

## Foundational Learning
- DAG transformation of medical algorithms: Converts clinical decision trees into navigable structures - needed to systematically represent medical knowledge in LLM-friendly format - quick check: verify all clinical pathways preserved
- Hierarchical clustering for cold-start: Identifies relevant domains for initial questions - needed to reduce information overload and improve conversation relevance - quick check: validate clustering accuracy against clinical expertise
- Expand-and-prune mechanism: Dynamically adjusts conversation flow - needed to maintain context relevance while gathering comprehensive information - quick check: measure information coverage vs. conversation length

## Architecture Onboarding
- Component map: Medical algorithm -> Corpus transformation -> DAG construction -> Hierarchical clustering -> Expand-and-prune dialogue engine -> Termination logic -> Structured report generation
- Critical path: Patient input → Expand-and-prune mechanism → Question selection → Response collection → Termination check → Report generation
- Design tradeoffs: Adaptivity vs. structure (dynamic questioning vs. algorithmic completeness), latency vs. accuracy (real-time processing vs. thorough clinical coverage)
- Failure signatures: Stuck in loops (expand-and-prune malfunction), incomplete information (termination logic failure), irrelevant questioning (clustering error)
- First experiments: 1) Test DAG preservation of clinical pathways, 2) Validate clustering accuracy on sample patient cases, 3) Measure expand-and-prune efficiency on conversation completion time

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Small, non-diverse evaluation sample of five physicians limits generalizability
- System latency issues may affect clinical adoption despite good usability scores
- No validation of DAG transformation's preservation of critical clinical decision pathways

## Confidence
- Generalizability claims: Medium - based on small sample size and single algorithm
- Usability metrics: Medium - subjective self-reported measures without objective task completion validation
- Clinical workflow integration: Medium - satisfaction reported but no evidence of efficiency gains or outcome improvements

## Next Checks
1) Independent replication with larger, diverse physician sample across multiple specialties to validate generalizability
2) A/B testing against standard clinical interview protocols to measure objective task completion rates, time efficiency, and diagnostic accuracy
3) Longitudinal deployment in clinical setting with actual patients to assess real-world performance, reliability, and EHR system integration