---
ver: rpa2
title: Experiments with Large Language Models on Retrieval-Augmented Generation for
  Closed-Source Simulation Software
arxiv_id: '2502.03916'
source_url: https://arxiv.org/abs/2502.03916
tags:
- llms
- simulation
- information
- prompt
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Retrieval-Augmented Generation (RAG) systems
  for closed-source simulation software using Pasimodo as a testbed. It tests commercial
  (NotebookLM) and open-source (AnythingLLM) RAG tools with local open-weight LLMs
  (Llama 3.2 3B, Gemma 3 4B, 27B).
---

# Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software

## Quick Facts
- arXiv ID: 2502.03916
- Source URL: https://arxiv.org/abs/2502.03916
- Reference count: 40
- Primary result: RAG systems can support closed-source simulation software but require tailored information provisioning and supplementary literature to reduce hallucinations

## Executive Summary
This study evaluates Retrieval-Augmented Generation (RAG) systems for closed-source simulation software using Pasimodo as a testbed. It tests commercial (NotebookLM) and open-source (AnythingLLM) RAG tools with local open-weight LLMs (Llama 3.2 3B, Gemma 3 4B, 27B). Results show NotebookLM excels in accuracy and source retrieval, while local LLMs struggle with hallucinations due to insufficient documentation and limited source retrieval. Smaller models perform poorly on compositional reasoning and model creation. Key improvements include tailored information provision, supplementary literature, and error-driven refinement. The study demonstrates RAG's potential for closed-source software support but highlights challenges in information retrieval and the need for richer documentation.

## Method Summary
The study uses Pasimodo simulation software as a testbed, providing documentation (XML examples, internal wiki, API reference) to NotebookLM and AnythingLLM with local LLMs. The evaluation uses 22 specific prompts across 6 categories: text extraction, structured extraction, component explaining, model summarization, compositional reasoning, and model creation. Local LLMs are configured with temperature=0 and max context=8192 tokens. The study tests different configurations including manual tailored information provisioning and error-driven refinement cycles. Performance is evaluated qualitatively through expert checkmarks comparing generated responses against expected correct answers.

## Key Results
- NotebookLM consistently outperforms local LLMs in accuracy and source retrieval, often citing 10+ sources versus AnythingLLM's default of 4
- Smaller LLMs (3B-4B) frequently hallucinate missing parameters and syntax elements when documentation is insufficient
- Tailored information provisioning (manually assembled documentation + API + examples) significantly improves local LLM performance
- Compositional reasoning and model creation tasks remain challenging even with augmentation, particularly for smaller models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation reduces hallucinations for closed-source software by injecting domain-specific knowledge at inference time.
- Mechanism: Documents are chunked, embedded into a vector database, and retrieved based on semantic similarity to the user prompt. Retrieved chunks augment the prompt before LLM processing, providing syntax and domain knowledge absent from pretraining.
- Core assumption: The retrieval system surfaces sufficiently relevant chunks, and the LLM correctly synthesizes them.
- Evidence anchors:
  - [abstract] "Retrieval-Augmented Generation (RAG) offers an approach to use additional knowledge alongside the pre-trained knowledge of the LLM to respond to user prompts."
  - [section 3.1] "The user prompt is used to perform a semantic search on the document database... The retrieved documents with the highest similarity to the prompt are combined with the original prompt."
  - [corpus] Related work on TrustRAG and RAG-based code completion supports the general mechanism but does not address closed-source simulation software specifically.
- Break condition: When documentation is sparse, chunked across fragments, or when retrieval returns irrelevant/outdated sources, augmentation fails and hallucinations increase.

### Mechanism 2
- Claim: Larger context window and higher source count improve response accuracy for complex simulation tasks.
- Mechanism: NotebookLM retrieves significantly more sources (often >10, up to 197) compared to AnythingLLM's default of 4 sources. This broader evidence base enables better compositional reasoning and reduces "fill in the blank" hallucinations.
- Core assumption: More retrieved sources are relevant and not contradictory; the LLM can weight them appropriately.
- Evidence anchors:
  - [section 3.5] "NotebookLM cited a substantially greater number of sources, typically exceeding ten and, in extreme cases, reaching up to 197 sources."
  - [section 3.5] "Especially the small LLMs exhibited a high tendency to 'fill in the blanks' with plausible-sounding but technically incorrect parameters."
  - [corpus] RAGRouter explores query routing across multiple RAG systems but does not directly address source count effects.
- Break condition: If retrieved sources contain outdated or conflicting information, more sources can introduce errors rather than reduce them.

### Mechanism 3
- Claim: Tailored information provisioning (manual assembly of documentation, API references, and examples) significantly improves local LLM performance.
- Mechanism: Instead of relying on naive similarity retrieval, relevant sections are manually curated for specific prompts, ensuring complete context (documentation + API + examples) is presented together.
- Core assumption: The curator correctly identifies all necessary information; the LLM can integrate multi-part context.
- Evidence anchors:
  - [section 3.6.3] "With the tailored information provided, the results of the local LLMs are significantly improved."
  - [section 3.6.3] "The manual tailored information is build of three sections: complete sections from the documentation... the API reference for every necessary component, and corresponding minimal input examples."
  - [corpus] No direct corpus evidence for this specific tailoring approach in simulation contexts.
- Break condition: When information needs are complex or distributed across many components, manual tailoring becomes impractical and cannot scale.

## Foundational Learning

- Concept: RAG pipeline (chunking, embedding, retrieval, augmentation)
  - Why needed here: The paper assumes familiarity with how documents become searchable vectors and how retrieval augments prompts.
  - Quick check question: Can you explain why chunking a document might cause information loss for retrieval?

- Concept: Transformer context windows and token limits
  - Why needed here: Local LLMs failed on large simulation files due to context length limits; understanding this constraint is critical for system design.
  - Quick check question: What happens to earlier chat history when the total context exceeds the model's maximum token limit?

- Concept: Hallucination in LLMs (knowledge gaps vs. generation confabulation)
  - Why needed here: The paper distinguishes between missing knowledge (closed-source syntax) and model tendencies to "fill in blanks."
  - Quick check question: Why might a smaller LLM hallucinate more than a larger one given the same retrieved context?

## Architecture Onboarding

- Component map:
  - Vector database (stores embedded document chunks)
  - Retriever (performs similarity search based on prompt embedding)
  - LLM (generates response from augmented prompt)
  - AnythingLLM or NotebookLM as orchestrating RAG application
  - Local inference runtime (Ollama) for open-weight models

- Critical path:
  1. Document preprocessing (convert examples, wiki, API references to chunked text/PDF)
  2. Embedding and database population
  3. Prompt embedding and similarity search
  4. Chunk retrieval and augmentation
  5. LLM inference with augmented context
  6. (Optional) Error-driven refinement loop

- Design tradeoffs:
  - NotebookLM: Higher accuracy, more sources retrieved, but limited prompt length, no local execution, data leaves organization
  - AnythingLLM + local LLMs: Full data privacy, customizable, but limited retrieval (4 sources default), smaller models hallucinate more
  - Manual tailoring vs. automated retrieval: Tailoring improves accuracy but does not scale; automation scales but may miss critical context

- Failure signatures:
  - LLM retrieves wrong component (e.g., "Inflow External" instead of "Influx External") due to similar names and frequency bias
  - Outdated documentation incorporated into responses
  - Chunking splits related information across fragments, leaving LLM with incomplete context
  - Chat history truncation causes LLM to forget earlier relevant context

- First 3 experiments:
  1. Baseline RAG test: Provide existing documentation, wiki, and examples to AnythingLLM with a small local LLM (e.g., Llama 3.2 3B). Measure accuracy on text extraction and simple question answering.
  2. Retrieval depth variation: Increase source retrieval count from 4 to 10+ and compare response quality for compositional reasoning tasks.
  3. Tailored augmentation test: Manually assemble complete context (documentation section + API reference + example) for a specific prompt and compare local LLM output to naive retrieval baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can source code analysis effectively replace or supplement missing documentation in RAG systems for closed-source software?
- **Basis in paper:** [explicit] The authors state, "Further studies are needed to investigate whether the source code of a closed-source software can be used to derive the information necessary for creating the respective input models using LLMs."
- **Why unresolved:** The current study restricted itself to text-based documentation and examples to simulate a user environment where data protection prevents access to the core source code.
- **What evidence would resolve it:** A comparative study evaluating the accuracy of generated simulation models when the RAG knowledge base is comprised of source code versus standard documentation.

### Open Question 2
- **Question:** Can RAG systems be adapted for closed-source software that relies on binary or graphical model files rather than text-based scripts?
- **Basis in paper:** [explicit] The paper notes, "No statement can be made regarding applicability to graphics-only applications where model files are stored in binary form. Whether any graphical input can be used here must first be investigated."
- **Why unresolved:** The experiments focused exclusively on script-based input models (XML format) and did not test graphical user interfaces or binary storage formats common in other simulation tools.
- **What evidence would resolve it:** Successful application of the proposed RAG framework on a simulation tool that utilizes binary file formats for model definition.

### Open Question 3
- **Question:** To what degree can LLMs autonomously validate and correct simulation models by directly interfacing with the simulation framework?
- **Basis in paper:** [explicit] The authors suggest, "An option would, therefore, be to call the simulation framework itself during generation of simulation models to validate the response or to correct any upcoming error response independently."
- **Why unresolved:** The current methodology relied on manual "Error-Driven Refinement" where a human identified errors and fed them back to the LLM, rather than an automated, agentic loop.
- **What evidence would resolve it:** A demonstration of a closed-loop system where the LLM iterates on simulation code using error logs from the simulation engine without human intervention.

## Limitations

- Limited documentation base (single internal wiki, API reference, and small set of examples) proves insufficient for complex compositional reasoning tasks
- Manual tailoring approach, while effective, does not scale to larger software ecosystems
- Qualitative evaluation framework using expert checkmarks lacks quantitative metrics for systematic comparison

## Confidence

- **High Confidence**: Claims about NotebookLM's superior performance in source retrieval and accuracy are well-supported by empirical results showing higher source counts (often >10, up to 197) and better qualitative scores
- **Medium Confidence**: The assertion that larger context windows and more sources improve compositional reasoning is supported but could be confounded by other factors
- **Low Confidence**: The scalability of manual tailoring approaches and the exact mechanisms behind NotebookLM's superior retrieval performance remain speculative

## Next Checks

1. **Quantitative Performance Benchmarking**: Replace qualitative expert checkmarks with automated metrics including exact match scores, BLEU/NIST scores for code generation tasks, and retrieval precision/recall measurements across different source counts to establish statistical significance.

2. **Cross-Domain Generalization Study**: Test the same RAG configuration (NotebookLM vs AnythingLLM + local LLMs) on 2-3 additional closed-source simulation packages with varying documentation quality to validate whether findings generalize beyond Pasimodo.

3. **Chunking Strategy Optimization**: Systematically vary chunk sizes (from 256 to 4096 tokens) and overlap percentages to identify optimal chunking that preserves context for compositional reasoning tasks, then measure impact on hallucination rates and answer completeness.