---
ver: rpa2
title: Transformers versus the EM Algorithm in Multi-class Clustering
arxiv_id: '2502.06007'
source_url: https://arxiv.org/abs/2502.06007
tags:
- softmax
- algorithm
- given
- transformer
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how Transformers can perform multi-class clustering
  of Gaussian Mixture Models by drawing connections to the Expectation-Maximization
  (EM) algorithm. The authors establish theoretical guarantees showing that Transformers
  can approximate Lloyd's algorithm (a special case of EM) for clustering, with approximation
  bounds depending on the number of layers and heads.
---

# Transformers versus the EM Algorithm in Multi-class Clustering

## Quick Facts
- **arXiv ID:** 2502.06007
- **Source URL:** https://arxiv.org/abs/2502.06007
- **Reference count:** 40
- **One-line primary result:** Transformers can learn to approximate Lloyd's algorithm for clustering by implementing EM steps through Softmax attention mechanisms.

## Executive Summary
This paper establishes theoretical connections between Transformers and the Expectation-Maximization algorithm for multi-class clustering. The authors prove that Transformers can approximate Lloyd's algorithm (a special case of EM) by implementing the Expectation step through weighted centroid updates via attention and the Maximization step through Hardmax approximation using temperature-scaled Softmax. The theory provides approximation bounds that depend on the number of layers and attention heads, and is complemented by extensive simulations showing Transformers achieve strong clustering performance even beyond theoretical assumptions.

## Method Summary
The method trains Transformers to perform multi-class clustering on Gaussian Mixture Models by encoding data points along with initialization context (spectral clustering outputs and initial centroids) into a context-augmented matrix. The architecture uses Softmax attention layers to approximate the E-step (centroid updates) and M-step (cluster assignments) of Lloyd's algorithm, with universal approximation theory ensuring the necessary non-linear computations can be performed. Training proceeds via supervised pre-training with labels, and performance is evaluated using ARI and NMI metrics against held-out test samples.

## Key Results
- Transformers can approximate Lloyd's algorithm with approximation bounds of $\mathcal{O}(N^{-1})$ for the E-step and $\mathcal{O}(\sqrt{\log M/M})$ for the M-step
- Theoretical generalization bounds show Transformers achieve fundamental limits when given sufficient pre-training data ($\gtrsim \exp(\Delta^2/2\sigma^2)$ instances)
- Extensive simulations demonstrate strong clustering performance across various conditions, including settings beyond theoretical assumptions
- The approach achieves competitive results compared to classical spectral methods while providing theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1: E-step via Attention-Based Centroid Updates
Softmax attention layers implement the Expectation step by computing weighted cluster centroids through normalized attention weights. The mechanism constructs attention heads where query-key products produce logits proportional to cluster membership indicators, and Softmax normalization yields weights that approximate discrete assignment ratios. Value projections then compute the weighted average of data points, yielding updated centroids with error $\mathcal{O}(N^{-1})$. This requires attention temperature scaled by $\mathcal{O}(\log N)$ to sharpen distributions toward hard assignments.

### Mechanism 2: M-step via Temperature-Scaled Softmax
The Maximization step's argmax operation (cluster assignment) is approximated by Softmax with large temperature scaling. The mechanism copies data $k$ times, subtracts centroids using multi-head approximation, computes $\ell_2$ norms via universal approximation, and applies Softmax with $\beta = C\log N$ to approximate Hardmax. This requires sufficient gap $\Delta$ between closest and second-closest centroid distances so that temperature-scaled softmax converges to one-hot vectors.

### Mechanism 3: Universal Approximation of Smooth Mappings
Transformers achieve universal approximation of smooth multivariate mappings $\mathbb{R}^{d_1} \to \mathbb{R}^{d_2}$ using multi-head Softmax attention. For $(R, C_\ell)$-smooth functions, there exist parameters such that the approximation error scales as $C(f) d_s \sqrt{d^2 \log(MR/d^2)/M}$. This enables computation of non-linear functions like vector norms required for the M-step, assuming target functions have bounded derivatives up to order $s = \lceil(k-1)/2\rceil + 2$.

## Foundational Learning

- **Concept: Lloyd's Algorithm / K-means EM** - Understanding the alternating E-step (centroid update) and M-step (assignment update) is prerequisite to following the Transformer construction. Quick check: Given 3 points at (0,0), (1,0), (10,0) and initial centroids at (0,0) and (1,0), what are the assignments and updated centroids after one iteration?

- **Concept: Softmax Temperature Scaling** - The paper critically relies on scaling attention logits by $\mathcal{O}(\log N)$ to make softmax approximate hardmax. Quick check: For logits [1, 2, 5], compute softmax with temperatures $\beta=1$ and $\beta=10$. How does the distribution change?

- **Concept: Covering Numbers and Metric Entropy** - The universal approximation proof uses $\epsilon$-covers of norm balls and union bounds. Quick check: What is the relationship between covering number $\mathcal{N}(B(\|\cdot\|, R), \epsilon)$ and dimension $d$? Why does this affect sample complexity?

## Architecture Onboarding

### Component map:
Input H = [X; P] where X ∈ R^(d×N) (data), P ∈ R^((D-d)×N) (context) → For each EM iteration τ: Expectation Block (Layers 1-3) with Softmax Attention computing normalized membership weights and Value projection yielding updated centroids → Maximization Block (Layers 4-(7+3k)) with FC layers copying data k times, Multi-head attention subtracting centroids and computing norms, and Softmax with β=C log N approximating hardmax → Output extraction reading final assignments.

### Critical path:
1. **Initialization encoding:** Spectral clustering + k-means++ provides initial assignments and centroids embedded in context matrix P (theory requires this, experiments suggest removable)
2. **E-step construction:** Two attention heads compute weighted sum and cancel residual, yielding N^(-1) approximation error
3. **Norm approximation:** Multi-head attention approximates ||v||_2^(-1) for centroid normalization with error √(log M/M)
4. **Temperature scaling:** Scaling by C log N ensures exp(-βΔ) decay for crisp assignments

### Design tradeoffs:
| Choice | Benefit | Cost |
|--------|---------|------|
| Standard Transformer | Simpler architecture | N^(-1) error term in E-step, N^(-3/2) final bound |
| Transformer+ (un-normalized attention) | Polynomial approximation (degree 3), N^(-100.5) bound | More layers, additional complexity |
| More heads M | Better function approximation (√(log M/M) decay) | Larger parameter norm, generalization cost |
| More layers L | More EM iterations τ | Depth-dependent error accumulation |

### Failure signatures:
- **Low separation regime (Δ/σ small):** Softmax weights become uniform; assignments unreliable, expect accuracy to drop to random-chance levels
- **Insufficient pre-training (n ≪ exp(Δ²/2σ²)):** ERM doesn't generalize; test loss remains high, need n ≳ exp(Δ²/2σ²) instances
- **Too few layers for k or τ:** Cannot complete all EM iterations, approximation error compounds unboundedly if L < τ(3+3k)
- **Large centroid norms:** M-step approximation error scales with sup||μ̂^(ℓ)||_2, unbounded data may require normalization
- **Missing initialization context:** Theory assumes P contains spectral clustering outputs, empirically may work without but theory doesn't guarantee

### First 3 experiments:
1. **Sweep minimum distance Δ:** Generate GMM data with k=4, d=10, N=200, varying Δ ∈ [5, 50]. Train Transformer with L=3, M=2 heads for 300 steps. Plot ARI/NMI vs. Δ. Expect sharp transition when Δ/σ becomes sufficient.
2. **Layer-iteration correspondence:** Train Transformers with L ∈ [3, 20] on fixed 6-class data. Separately run Lloyd's with τ ∈ [1, 6] iterations. Compare clustering performance curves. Verify that L ≈ τ(3+3k) gives comparable performance.
3. **Ablate context matrix P:** Train two models: (a) full input H = [X; P] with spectral initialization; (b) minimal input X only. Compare convergence speed and final accuracy. Quantify gap to understand initialization dependency.

## Open Questions the Paper Calls Out

### Open Question 1: Right-Product Softmax Universal Approximation
Does a universal approximation bound exist for the right-product Softmax form SoftMax([x^T, 1]A) comparable to the left-product form? This would theoretically allow the standard Transformer (without un-normalized attention) to achieve the performance of the Transformer+ architecture. Resolution would require a constructive proof demonstrating bounded error rate for this form.

### Open Question 2: Optimal Rate Without Initialization
Can the minimax optimal rate be achieved without the explicit spectral initialization provided in the context-augmented matrix? The theoretical analysis relies on an initialization procedure to kick-start the approximation. Resolution would require a theoretical generalization bound showing the Transformer achieves the minimax rate O(exp(-Δ²/8σ²)) even without pre-computed initial centroids.

### Open Question 3: ERM vs SGD Solutions
Do the derived generalization bounds for the Empirical Risk Minimizer (ERM) hold for the solutions found by Stochastic Gradient Descent (SGD)? The theoretical results guarantee existence of a parameter configuration that achieves the optimal rate, but practical training uses SGD. Resolution would require an analysis of the optimization landscape proving SGD converges to a solution satisfying the error bounds.

### Open Question 4: Layer Normalization Impact
How does the inclusion of Layer Normalization impact the approximation bounds for the Expectation and Maximization steps? The current theoretical model abstracts away Layer Normalization. Resolution would require an extension of the theorems incorporating Layer Norm and providing corresponding approximation error rates.

## Limitations

- Theoretical analysis makes strong assumptions (well-separated clusters, isotropic Gaussians) that limit practical applicability
- Universal approximation results require exponentially many heads M in dimension d, making them non-constructive for high-dimensional data
- Approximation bounds depend critically on temperature scaling by O(log N), which may not hold in finite-sample settings
- Gap exists between theory (requires initialization context P) and experiments (P may be removable)

## Confidence

**High Confidence Claims:**
- Transformers can approximate Lloyd's algorithm steps (E-step and M-step) under specified conditions
- Softmax attention layers have universal approximation capabilities for smooth multivariate mappings
- Temperature scaling mechanism enables Hardmax approximation in the M-step
- Theoretical error bounds (O(N^(-1)) for E-step, O(√(log M/M)) for M-step) are mathematically sound

**Medium Confidence Claims:**
- Transformers will achieve clustering performance matching fundamental limits when given sufficient pre-training data
- Initialization context matrix can be removed in practice (based on experimental evidence, but not theoretically justified)
- Results generalize beyond isotropic Gaussians and well-separated clusters (empirically supported but not theoretically proven)

**Low Confidence Claims:**
- Specific architectural parameters required for successful learning are fully characterized
- Results extend to non-Gaussian mixture models or other unsupervised learning problems
- Theoretical bounds accurately predict empirical performance across all regimes

## Next Checks

1. **Robustness to initialization removal:** Systematically ablate the context matrix P in both training and testing, measuring the gap in performance and convergence speed. Compare against theory's initialization requirement to quantify the practical-theoretical disconnect.

2. **Temperature scaling validation:** Conduct controlled experiments varying the attention temperature scaling parameter (beyond just log N) across different cluster separations. Measure how the approximation quality of hard assignments degrades as separation decreases.

3. **Dimensionality stress test:** Scale the input dimension d while holding other parameters fixed, measuring both approximation error and computational requirements. Verify whether the theoretical scaling with d² holds empirically and identify the practical limits of the approach.