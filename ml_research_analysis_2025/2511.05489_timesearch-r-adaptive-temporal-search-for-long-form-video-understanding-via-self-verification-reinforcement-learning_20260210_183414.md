---
ver: rpa2
title: 'TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via
  Self-Verification Reinforcement Learning'
arxiv_id: '2511.05489'
source_url: https://arxiv.org/abs/2511.05489
tags:
- search
- video
- temporal
- reasoning
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TimeSearch-R introduces a reinforcement learning framework that
  reformulates temporal search as interleaved text-video thinking, enabling end-to-end
  learning of optimal search strategies directly from data. The method addresses the
  challenge of insufficient temporal exploration and inconsistent logical reasoning
  in long-form video understanding by introducing GRPO with Completeness Self-Verification
  (GRPO-CSV), which supervises intermediate search steps to ensure sufficient and
  accurate video exploration.
---

# TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2511.05489
- **Source URL**: https://arxiv.org/abs/2511.05489
- **Reference count**: 33
- **Primary result**: Achieves state-of-the-art performance on long-form video understanding benchmarks, improving temporal F1 score on Haystack-LVBench by 5.6% and accuracy on Haystack-Ego4D by 8.5%.

## Executive Summary
TimeSearch-R introduces a reinforcement learning framework that reformulates temporal search as interleaved text-video thinking, enabling end-to-end learning of optimal search strategies directly from data. The method addresses the challenge of insufficient temporal exploration and inconsistent logical reasoning in long-form video understanding by introducing GRPO with Completeness Self-Verification (GRPO-CSV), which supervises intermediate search steps to ensure sufficient and accurate video exploration. The approach is supported by a high-quality video reasoning dataset constructed through two-stage filtering to enhance task difficulty and improve temporal search capabilities. Extensive experiments show TimeSearch-R achieves state-of-the-art performance, improving temporal F1 score on Haystack-LVBench by 5.6%, accuracy on Haystack-Ego4D by 8.5%, and establishing a new SOTA on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL.

## Method Summary
TimeSearch-R is a reinforcement learning framework that reformulates temporal search as interleaved text-video thinking. It uses Qwen2.5-VL-7B as the base policy model, which generates reasoning traces alternating between textual reasoning and video retrieval calls. The system employs two-stage training: first, supervised fine-tuning (SFT) on filtered data with GPT-4o-generated traces where search results are masked; second, GRPO-CSV reinforcement learning with 8 rollouts per prompt. The key innovation is Completeness Self-Verification, where the model must re-answer using only searched frames to ensure sufficient exploration. Data is filtered in two stages: first removing samples solvable with 4 uniformly sampled frames, then keeping only samples solvable with 64 frames plus temporal search. The search function uses SigLIP embeddings with DPP sampling to select diverse, relevant frames within predicted temporal boundaries.

## Key Results
- Improves temporal F1 score on Haystack-LVBench by 5.6% over previous SOTA
- Achieves 8.5% accuracy improvement on Haystack-Ego4D benchmark
- Establishes new state-of-the-art on LongVideoBench with 4.1% improvement over base Qwen2.5-VL model
- Completeness rate increases from 57.2% to 60.5% with GRPO-CSV
- Training stability significantly improves with CSV, preventing collapse around step 300

## Why This Works (Mechanism)

### Mechanism 1: Completeness Self-Verification
Self-verification improves temporal exploration completeness and reasoning consistency by forcing the model to re-answer using only searched frames without further search. During CSV rollout, the model must extract searched video frames from the interleaved reasoning process and utilize the same policy model to verify the adequacy of searched frames. The completeness reward is only activated when the original answer is correct, focusing supervision on promising trajectories. This forces the model to gather sufficient evidence during the original rollout since insufficient searches will cause the CSV answer to disagree with the original answer.

### Mechanism 2: Interleaved Text-Video Thinking
Reformulating temporal search as multi-turn thinking enables learned adaptive search strategies through alternating textual reasoning and video retrieval. At each thinking step, the policy model generates textual reasoning, and if it contains a search instruction, the video environment executes it according to frame timestamps. Each step conditions on the full history, allowing the model to refine its search based on intermediate findings. The search function uses DPP to select diverse, relevant frames within temporal boundaries specified by the model.

### Mechanism 3: Two-Stage Data Filtering
Filtering trivial and unsolvable samples enables effective RL training by ensuring the model receives meaningful learning signals. Stage 1 removes samples solvable with 4 uniformly sampled frames to discourage reliance on linguistic shortcuts. Stage 2 removes samples unsolvable even with 64 frames and temporal search. This ensures RL receives meaningful learning signals since trivial samples yield zero advantage in GRPO group computation while unsolvable samples provide no valid reward signal.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: TimeSearch-R builds on GRPO, which optimizes policy by comparing multiple rollouts per prompt and computing advantages relative to the group mean. Understanding GRPO is essential to see why the original formulation fails for temporal search (insufficient exploration) and how CSV addresses this.
  - Quick check question: Given 8 rollouts with rewards [0, 0, 1, 1, 1, 1, 1, 1], what problem arises for policy learning? (Answer: All correct rollouts get near-zero advantage if they dominate, providing weak gradient signal—this motivates filtering trivial samples.)

- **Concept: Determinantal Point Process (DPP)**
  - Why needed here: The search function uses DPP to select F frames that balance query relevance and diversity. DPP maximizes det(L_S) where L encodes both relevance weights and pairwise similarities, naturally penalizing redundant frames.
  - Quick check question: Why use DPP instead of simple top-k retrieval by relevance? (Answer: DPP explicitly penalizes redundancy—if two frames are highly similar, selecting both reduces diversity score, whereas top-k might select near-duplicates.)

- **Concept: Chain-of-Thought Reasoning**
  - Why needed here: TimeSearch-R extends text-only CoT to "text-video interleaved thinking," where video clips are inserted into the reasoning chain. The format enforces logical reasoning before search calls and answers.
  - Quick check question: How does masking search results during SFT training affect what the model learns? (Answer: Forces model to learn meaningful temporal windows and queries rather than copying ground-truth search results.)

## Architecture Onboarding

- **Component map**: Input Layer (Video V + Question Q) -> Initial preview Ṽ (uniformly sampled) -> Policy Model π_θ (Qwen2.5-VL-7B backbone) -> Search Environment (executes seek_video_frames calls) -> Training Pipeline (SFT cold-start -> GRPO-CSV post-training) -> CSV Module (extracts V_c from CoT, forces re-answer without search, computes completeness reward) -> Reward Aggregator (R = R_c + R_fmt + R_acc)

- **Critical path**: 1) SFT stage: Train on filtered dataset with masked search results to learn search format; 2) GRPO rollout: Generate 8 trajectories per prompt with interleaved reasoning; 3) CSV phase: Extract searched frames, re-answer, compute completeness reward; 4) Policy update: GRPO with KL penalty β=0.005, advantage computed over group

- **Design tradeoffs**: Search budget (max 8 turns × 8 frames) vs. computational cost and exploration completeness; filtering threshold strictness vs. training data quantity and diversity; KL penalty β vs. reward optimization speed and policy deviation from base model; using same policy for CSV vs. separate verifier model

- **Failure signatures**: Insufficient temporal exploration (model stops searching early, CSV answer disagrees with original); inconsistent reasoning (intermediate reasoning contradicts final answer); format errors (model outputs malformed tool calls); linguistic shortcut exploitation (model answers correctly without proper search)

- **First 3 experiments**: 1) Ablate CSV component: Train with standard GRPO (no CSV) and monitor completeness rate and training stability. Expect: Completeness drops from 60.5% to 57.2%, training may collapse. 2) Vary filtering thresholds: Test Stage-1 with different frame counts (2, 4, 8 frames) and measure final QA accuracy. Expect: Too loose (8 frames) retains trivial samples, too strict (2 frames) loses valid samples. 3) Analyze search patterns: Visualize learned search strategies across task types (hypothesis-driven, sequential, elimination). Verify they match qualitative patterns in Figures 5, 13-15 and differ from hand-crafted baselines.

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the integration of CSV with other reinforcement learning algorithms beyond GRPO, the scalability of the approach to ultra-long videos exceeding 10,000 seconds, and the root causes of residual failure cases where the model terminates search prematurely or hallucinates visual evidence despite GRPO-CSV supervision. The authors acknowledge these limitations but do not provide causal analysis or mitigation strategies for these failure modes.

## Limitations

- The CSV mechanism's effectiveness in learning genuine completeness assessment versus pattern matching remains unclear, as the paper shows improvement but doesn't verify the model truly understands evidence sufficiency
- The two-stage filtering process, while effective, may have removed challenging but solvable samples, potentially limiting the model's exposure to diverse temporal reasoning patterns
- Heavy reliance on GPT-4o for SFT data generation introduces a dependency that may not be reproducible with other LLMs, and the specific filtering thresholds were likely tuned for this particular dataset distribution

## Confidence

- **High Confidence**: The core observation that CSV improves temporal exploration completeness and training stability is well-supported by experimental results showing collapsed training without CSV and improved completeness metrics with CSV
- **Medium Confidence**: The claim that CSV enables learned adaptive search strategies is plausible given performance improvements, but the mechanism by which the model learns to assess evidence sufficiency remains somewhat opaque
- **Medium Confidence**: The data filtering approach's effectiveness is demonstrated empirically, but the specific thresholds and their optimality across different datasets remain uncertain

## Next Checks

1. **Ablation Study with Varying CSV Parameters**: Systematically vary the CSV implementation (e.g., using a separate verifier model vs. the same policy, changing the consistency threshold, or modifying the completeness reward computation) to isolate which aspects of CSV are most critical for performance.

2. **Cross-Dataset Generalization Test**: Evaluate TimeSearch-R on datasets not used in training (e.g., Ego4D, VideoMME) to assess whether the learned search strategies generalize beyond the Haystack-Ego4D, VideoMarathon, and CinePile distributions.

3. **Interpretability Analysis of Learned Search Patterns**: Use visualization techniques to analyze the temporal boundaries and query patterns learned by TimeSearch-R across different task types, and compare these patterns to hand-crafted baselines to verify that the model has indeed learned meaningful adaptive strategies rather than memorizing specific patterns.