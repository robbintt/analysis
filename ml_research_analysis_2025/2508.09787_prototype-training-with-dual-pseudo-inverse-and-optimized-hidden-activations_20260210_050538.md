---
ver: rpa2
title: Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations
arxiv_id: '2508.09787'
source_url: https://arxiv.org/abs/2508.09787
tags:
- ridge
- training
- test
- prototype
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Proto-PINV+H is a fast training paradigm that combines closed-form
  weight computation with gradient-based optimisation of synthetic prototypes, hidden
  activations, and soft labels. At each iteration, all weight matrices are recomputed
  via ridge-regularised pseudo-inverse solves, while only the prototypes are updated
  with Adam.
---

# Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations

## Quick Facts
- arXiv ID: 2508.09787
- Source URL: https://arxiv.org/abs/2508.09787
- Authors: Mauro Tucci
- Reference count: 4
- Primary result: Achieves 97.8% MNIST and 89.3% Fashion-MNIST test accuracy in 3.9–4.5s using ~130k trainable parameters.

## Executive Summary
Proto-PINV+H is a fast training paradigm that combines closed-form weight computation with gradient-based optimisation of synthetic prototypes, hidden activations, and soft labels. At each iteration, all weight matrices are recomputed via ridge-regularised pseudo-inverse solves, while only the prototypes are updated with Adam. The trainable degrees of freedom are thus shifted from weight space to data/activation space. On MNIST and Fashion-MNIST, the method reaches 97.8% and 89.3% test accuracy in 3.9–4.5s using approximately 130k trainable parameters and 250 epochs on an RTX 5060 (16GB). Multi-layer extensions, learnable ridge parameters, and optional PCA/PLS projections are provided, along with theory linking prototype matrix conditioning to generalisation. The approach yields favourable accuracy–speed–size trade-offs against ELM, random-feature ridge, and shallow MLPs trained by back-propagation.

## Method Summary
The method trains a shallow neural network by recomputing all weight matrices in closed form via ridge-regularised pseudo-inverse solves at each iteration, while only the prototypes (Xp, Hp, Yp) are updated with Adam. The core innovation is shifting trainable parameters from weight space to prototype space, with hidden activations Hp being directly optimised rather than fixed as random features. This enables the model to adapt internal representations to task structure while maintaining the computational efficiency of closed-form solutions. The approach includes learnable ridge parameters, optional PCA/PLS projections for dimensionality reduction, and theoretical guarantees linking prototype matrix conditioning to generalisation performance.

## Key Results
- Achieves 97.8% test accuracy on MNIST and 89.3% on Fashion-MNIST
- Training completes in 3.9–4.5 seconds using approximately 130k trainable parameters
- Uses 250 epochs with ~150 synthetic prototypes per dataset
- Demonstrates favorable accuracy-speed-size tradeoffs compared to ELM, random-feature ridge, and shallow MLPs

## Why This Works (Mechanism)

### Mechanism 1: Analytic Weight Elimination via Ridge Pseudo-Inverse
- Claim: Replacing iterative weight gradient updates with closed-form ridge solutions eliminates backpropagation through weight space, reducing per-epoch cost from O(Ndh) to O(Nph² + h³).
- Mechanism: At each iteration, W1 and W2 are computed via W = (A^T A + λI)^{-1} A^T B using Cholesky-based solves. Only prototypes (Xp, Hp, Yp) receive gradients. The ridge term λ > 0 ensures invertibility and smoothness. This is mathematically equivalent to solving a least-squares subproblem optimally at each step, conditioned on current prototypes.
- Core assumption: The prototype matrices (Np × d, Np × h) span a sufficient subspace for the task; classification loss can be expressed through ~150 synthetic examples rather than 60k real samples.
- Evidence anchors:
  - [abstract] "At each iteration we recompute all weight matrices in closed form via two (or more) ridge-regularised pseudo-inverse solves, while updating only the prototypes with Adam."
  - [Section 3.2] Equations (1) and (2) define the dual pseudo-inverse structure.
  - [Section 3.6] "Cost scales as O(Nph²) plus O(h³) for linear solves, independent of N."
  - [corpus] Limited direct corpus evidence; neighbor papers focus on weight quantization and low-rank compression, not analytic training. FMR scores (0.41–0.59) suggest moderate topical relevance but no prior art on prototype-based pseudo-inverse training.
- Break condition: If prototype count Np is too small (e.g., < 50 for MNIST), or if prototype matrices become rank-deficient despite ridge regularization, the closed-form solution may underfit severely. Condition number κ(Xp) → ∞ destabilizes gradients.

### Mechanism 2: Hidden Activation Optimization as Learnable Representation
- Claim: Treating Hp (hidden activations) as trainable parameters—rather than fixed random features—increases expressive power and allows the model to adapt internal representations to the task structure.
- Mechanism: Hp is directly optimized via Adam alongside Xp and Yp. The gradient flows through the closed-form W1 solution using implicit differentiation: ∂L/∂Hp = G^{-T} ∇_{W1} L via the identity in Section 4.2. This enables Hp to encode task-relevant features without requiring backpropagation through explicit layers.
- Core assumption: The implicit gradient through the pseudo-inverse is sufficiently informative to guide Hp toward useful representations; the landscape is smooth enough for Adam.
- Evidence anchors:
  - [abstract] "Optimising hidden activations is a central novelty that improves expressive power beyond methods that optimise only input prototypes and labels."
  - [Section 1] "Key idea...optimising hidden activations is a central novelty."
  - [Section 4.2] Derives ∂L/∂θ for gradients through linear solves, confirming differentiability.
  - [corpus] No direct corpus precedent for optimizing activations in closed-form weight regimes; this appears novel.
- Break condition: If λ1 is too large, W1 becomes insensitive to Hp changes, reducing gradient signal. If Hp is over-regularized (large weight decay), it may collapse to trivial values.

### Mechanism 3: Condition-Number-Bounded Generalization
- Claim: The generalization error of Proto-PINV+H is bounded by κ(Xp) = σ_max(Xp)/σ_min(Xp), so optimizing prototypes to improve conditioning directly improves generalization.
- Mechanism: Lemma 1 and Proposition 1 show that perturbations in W1 scale with κ(Xp). By Rademacher complexity arguments, the generalization gap includes a term proportional to κ(Xp)/√N. Gradient descent on Xp implicitly reduces this condition number, tightening the bound.
- Core assumption: The loss is Lipschitz in W1; the training data is i.i.d.; the prototype matrix is the dominant source of generalization error.
- Evidence anchors:
  - [Section 4.1] Lemma 1 gives sensitivity bounds; Proposition 1 links κ(Xp) to generalization.
  - [Section 5.4] "Optimising prototypes to reduce κ(Xp) thus tightens the bound."
  - [corpus] "Generalization Bounds for Rank-sparse Neural Networks" (FMR=0.0) provides related rank-based bounds but not prototype-specific.
- Break condition: If Xp collapses to degenerate configurations (e.g., all prototypes similar), κ(Xp) may explode, violating the bound assumptions. Monitoring σ_min(Xp) during training is advisable.

## Foundational Learning

- **Concept: Ridge Regression and Pseudo-Inverse**
  - Why needed here: The entire weight computation relies on solving (A^T A + λI)^{-1} A^T B. Understanding ridge regularization prevents numerical instability and clarifies why λ > 0 is mandatory.
  - Quick check question: Given A ∈ R^{10×5}, rank(A) = 3, what happens if you compute A^+ without regularization?

- **Concept: Implicit Differentiation Through Linear Systems**
  - Why needed here: Gradients must flow through the closed-form solution W = G^{-1}R. The paper uses implicit function theorem derivatives; understanding this is essential for debugging NaN issues or custom extensions.
  - Quick check question: If W = G^{-1}R, write ∂L/∂θ in terms of G, R, and ∇_W L.

- **Concept: Condition Number and Numerical Stability**
  - Why needed here: κ(Xp) directly affects both optimization stability and generalization bounds. Monitoring and controlling condition number is critical for robust training.
  - Quick check question: If σ_max(Xp) = 10 and σ_min(Xp) = 10^{-4}, what is κ(Xp), and should you be concerned?

## Architecture Onboarding

- **Component map:**
  - Xp ∈ R^{Np × (d+1)}: Synthetic input prototypes (trainable, bias-augmented)
  - Hp ∈ R^{Np × h}: Hidden activation prototypes (trainable)
  - Yp ∈ R^{Np × k}: Soft label prototypes (trainable, optionally temperature-scaled)
  - W1 = pinv_ridge(Xp, λ1) @ Hp: Input-to-hidden weights (computed, not stored as parameters)
  - W2 = pinv_ridge([1, σ(Hp)], λ2) @ Yp: Hidden-to-output weights (computed)
  - Optional: PCA/PLS projection U ∈ R^{d×d'} reducing input dimension

- **Critical path:**
  1. Initialize Xp (random normal or stratified samples), Hp (random normal), Yp (balanced one-hot)
  2. Forward: Compute W1, W2 via ridge pseudo-inverse; evaluate logits on full training batch
  3. Loss: Cross-entropy + λ3||W||²_F
  4. Backward: Gradients flow to Xp, Hp, Yp via implicit differentiation
  5. Update: Adam step on Xp, Hp, Yp (and optionally λ1, λ2 via softplus(ρ))
  6. Repeat for 250 epochs; validate on held-out split

- **Design tradeoffs:**
  - Np vs. h: More prototypes increase capacity but raise O(Nph²) cost; wider hidden layers (h) increase O(h³) solve cost
  - Learnable λ vs. fixed: Learnable λ2 helps Fashion-MNIST (+0.2–0.3 pp); λ1 is more stable fixed
  - PCA dimension d': Reducing to 400 speeds computation but may lose pixel-level information
  - Temperature T on Yp: Higher T smooths labels; T=0 uses raw Yp directly

- **Failure signatures:**
  - NaN in W1 or W2: λ too small or Xp/Hp degenerate; check σ_min(Xp) and ensure λ > 0
  - Accuracy plateaus early: Weight decay too aggressive on Hp; try decoupling Xp/Hp decays
  - Validation-test gap large: Overfitting to prototypes; increase λ3 or reduce Np
  - Slow convergence: Learning rate too low; cosine scheduler with warm-up recommended

- **First 3 experiments:**
  1. Replicate MNIST baseline: Np=150, h=512, PCA=400, λ1=λ2=0.01, λ3=1e-4, 250 epochs. Target: ~97.8% test accuracy in ~4s.
  2. Ablate hidden activation optimization: Fix Hp as random (no gradients). Compare accuracy and training time to confirm the claimed ~1–2 pp improvement from learning Hp.
  3. Stress-test conditioning: Monitor κ(Xp) during training. Visualize prototype distributions (t-SNE on Xp) before/after training to verify capacity reallocation to hard classes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Proto-PINV+H be extended to convolutional architectures while preserving closed-form weight computation?
- Basis in paper: [explicit] "Limitations include vectorised inputs (convolutional stems remain future work)"
- Why unresolved: The method requires matrix pseudo-inverses; convolutional weights have spatial structure incompatible with current formulation.
- What evidence would resolve it: A modified Proto-PINV variant with convolutional prototypes or hybrid convolutional-projection architecture demonstrating competitive accuracy on image benchmarks.

### Open Question 2
- Question: How does the method scale to more complex datasets (CIFAR-10/100, ImageNet, tabular, or sequential data)?
- Basis in paper: [inferred] Evaluation limited to MNIST and Fashion-MNIST; no discussion of higher-dimensional or non-image domains.
- Why unresolved: Prototype count (Np=150) and PCA projection (d'=400) may be insufficient for complex data manifolds.
- What evidence would resolve it: Benchmarks on CIFAR-10/100 with analysis of required Np and accuracy–speed trade-offs compared to modern baselines.

### Open Question 3
- Question: At what depth does error accumulation in multi-layer analytic stacks degrade performance, and can it be mitigated?
- Basis in paper: [explicit] "potential error accumulation in very deep analytic stacks" listed as a limitation.
- Why unresolved: Lemma 1 bounds sensitivity per layer, but compounding errors across L>2 layers are not empirically or theoretically characterized.
- What evidence would resolve it: Empirical study of accuracy vs. depth (L=2–10) with condition number monitoring; proposal of normalization or residual connections within the analytic framework.

## Limitations
- The method is currently limited to vectorised inputs, with convolutional extensions remaining future work
- Evaluation is restricted to MNIST and Fashion-MNIST, with scalability to more complex datasets unexplored
- Potential error accumulation in very deep analytic stacks is identified as a theoretical concern without empirical characterization

## Confidence
- **High confidence:** The closed-form weight computation via ridge pseudo-inverse and the overall training framework are mathematically sound and well-specified.
- **Medium confidence:** The theoretical generalization bounds linking condition number to error are correct in isolation, but their practical tightness and impact on real training dynamics are not empirically validated.
- **Medium confidence:** The empirical results on MNIST and Fashion-MNIST are reproducible given the specifications, but the exact performance may vary due to unspecified hyperparameters (learning rate, weight decay, ridge coefficients).

## Next Checks
1. **Condition Number Monitoring:** During training, track κ(Xp) and σ_min(Xp) to verify that the prototypes remain well-conditioned and that the bound's assumptions hold. Visualize prototype evolution via t-SNE to confirm capacity reallocation to hard classes.
2. **Ablation of Hidden Activations:** Fix Hp as random (no gradients) and compare accuracy and training time to confirm the claimed ~1–2 pp improvement from learning Hp.
3. **Hyperparameter Sensitivity:** Perform a small grid search over λ1, λ2 ∈ [1e-4, 1e-2] and learning rate ∈ [1e-3, 1e-2] to assess robustness and identify optimal values for stable training.