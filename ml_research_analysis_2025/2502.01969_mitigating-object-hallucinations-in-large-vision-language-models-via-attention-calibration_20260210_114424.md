---
ver: rpa2
title: Mitigating Object Hallucinations in Large Vision-Language Models via Attention
  Calibration
arxiv_id: '2502.01969'
source_url: https://arxiv.org/abs/2502.01969
tags:
- attention
- object
- arxiv
- calibration
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses object hallucination in large vision-language\
  \ models (LVLMs), where models generate responses misaligned with visual content.\
  \ The authors identify Spatial Perception Bias (SPB)\u2014an attention imbalance\
  \ across visual tokens based on spatial position\u2014as a key contributor to hallucination."
---

# Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration

## Quick Facts
- arXiv ID: 2502.01969
- Source URL: https://arxiv.org/abs/2502.01969
- Reference count: 20
- This paper addresses object hallucination in LVLMs by identifying and mitigating Spatial Perception Bias through attention calibration methods.

## Executive Summary
This paper addresses the critical issue of object hallucination in large vision-language models (LVLMs), where models generate responses that are misaligned with visual content. The authors identify Spatial Perception Bias (SPB) - an attention imbalance across visual tokens based on their spatial position - as a key contributor to hallucination. They propose two methods: Uniform Attention Calibration (UAC), a training-free approach that calibrates attention using a bias matrix estimated from meaningless inputs, and Dynamic Attention Calibration (DAC), a learnable plug-and-play module fine-tuned via contrastive learning to enforce consistent outputs regardless of object position. Experiments on multiple LVLMs and benchmarks show DAC achieves state-of-the-art performance, significantly reducing hallucination while improving general multimodal alignment.

## Method Summary
The paper introduces two complementary approaches to mitigate object hallucination in LVLMs. First, UAC is a training-free method that estimates spatial bias by analyzing attention weights when processing meaningless visual inputs, then applies a bias matrix to calibrate attention. Second, DAC is a learnable module that can be plugged into existing LVLMs and fine-tuned using contrastive learning objectives. The DAC module learns to normalize attention weights across spatial positions, ensuring consistent object perception regardless of where objects appear in the visual field. Both methods target the root cause - spatial perception bias - rather than just the symptoms of hallucination, and they can be applied to various LVLM architectures without requiring full model retraining.

## Key Results
- DAC achieves state-of-the-art performance on object hallucination benchmarks (POPE, CHAIR, MME) with significant reduction in hallucination rates
- UAC performs competitively with zero inference overhead, making it practical for deployment
- Both methods improve general multimodal alignment while specifically targeting spatial perception bias
- The methods are effective across multiple LVLM architectures including LLaVA-1.5, mPLUG-Owl2, and LLaVA-NeXT

## Why This Works (Mechanism)
The paper identifies Spatial Perception Bias as a fundamental cause of object hallucination in LVLMs. This bias manifests as attention imbalance across visual tokens based on their spatial position, causing models to disproportionately focus on certain regions (typically center or specific quadrants) regardless of actual object importance. By calibrating attention weights to be more uniform across spatial positions, the methods ensure that object perception remains consistent regardless of where objects appear in the image. This normalization of spatial attention directly addresses the root cause of position-dependent hallucination, allowing models to base their responses on actual visual content rather than positional artifacts in their attention mechanisms.

## Foundational Learning
- Spatial Perception Bias (SPB): The tendency for attention mechanisms to assign different importance weights to visual tokens based on their spatial position. Why needed: SPB is identified as a primary cause of object hallucination, making it crucial to understand and mitigate. Quick check: Analyze attention weight distributions across different spatial positions in LVLM outputs.
- Attention Calibration: The process of normalizing or adjusting attention weights to achieve desired properties (in this case, spatial uniformity). Why needed: Direct modification of attention mechanisms allows targeted intervention without full model retraining. Quick check: Verify that calibrated attention weights show reduced positional variance while maintaining semantic relevance.
- Contrastive Learning for Attention: Using contrastive objectives to train attention calibration modules by maximizing similarity between consistent outputs while minimizing similarity between inconsistent ones. Why needed: Provides an effective training signal for the learnable DAC module without requiring labeled hallucination data. Quick check: Measure training stability and convergence of DAC using contrastive loss on spatial position invariance.

## Architecture Onboarding

Component Map:
- Input Image -> Vision Encoder -> Visual Tokens -> [LVLM Backbone] -> Attention Layer -> [DAC/UAC Module] -> Calibrated Attention -> Output

Critical Path:
The critical path for hallucination mitigation runs through the attention layer where spatial bias occurs. Both UAC and DAC intervene at this point - UAC by post-hoc calibration using pre-computed bias matrices, and DAC by learning to normalize attention weights during fine-tuning. The effectiveness depends on accurately estimating or learning the spatial bias pattern and applying appropriate correction while preserving genuine spatial relationships in the visual content.

Design Tradeoffs:
- UAC offers zero inference overhead but requires pre-computation of bias matrices for each model architecture
- DAC provides superior performance through learnable adaptation but requires fine-tuning with contrastive objectives
- Both methods must balance between eliminating spatial bias and preserving legitimate spatial hierarchies in visual scenes
- The plug-and-play nature of both approaches enables application across different LVLM architectures without architectural modifications

Failure Signatures:
- Over-correction leading to loss of genuine spatial relationships and context
- Incomplete bias elimination resulting in residual positional artifacts
- Computational overhead in bias matrix computation for UAC or fine-tuning for DAC
- Potential degradation in performance on tasks that legitimately depend on spatial position

3 First Experiments:
1. Analyze attention weight distributions across spatial positions on meaningless visual inputs to quantify SPB magnitude
2. Apply UAC to a simple LVLM and measure reduction in position-dependent output variance
3. Fine-tune DAC on a small dataset using contrastive learning and evaluate calibration effectiveness on spatial position invariance

## Open Questions the Paper Calls Out
None

## Limitations
- The attribution of object hallucination primarily to SPB may not capture the full complexity of hallucination phenomena, as other factors like training data quality and multimodal fusion mechanisms may also contribute
- The proposed solutions address spatial bias but do not directly tackle semantic understanding or reasoning errors that can also lead to hallucinations
- Evaluation focuses primarily on object-level hallucination detection, potentially missing more subtle semantic or relational hallucinations

## Confidence
- **High confidence**: Experimental results demonstrating DAC's effectiveness in reducing spatial perception bias and improving object hallucination detection across multiple benchmarks
- **Medium confidence**: Attribution of object hallucination primarily to SPB, as this represents one identified factor among potentially many contributing causes
- **Medium confidence**: Generalizability of results across different LVLM architectures, given experiments were conducted on a limited set of models

## Next Checks
1. Conduct ablation studies isolating the impact of SPB from other hallucination sources by testing DAC on models with varying degrees of spatial bias to quantify its relative contribution to overall hallucination rates
2. Evaluate the proposed methods on long-sequence inputs (beyond 1024 tokens) to assess scalability and performance degradation patterns as spatial bias accumulates over extended contexts
3. Test DAC's effectiveness on cross-modal reasoning tasks that require complex spatial relationships (e.g., object occlusion, relative positioning) to determine if attention calibration alone suffices for sophisticated spatial understanding