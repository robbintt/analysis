---
ver: rpa2
title: 'AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous
  Space Planning Problems'
arxiv_id: '2601.11354'
source_url: https://arxiv.org/abs/2601.11354
tags:
- planning
- agents
- benchmark
- agentic
- scheduling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AstroReason-Bench is a physics-aligned benchmark for evaluating
  agentic LLM planners in space planning problems (SPP), which require reasoning under
  strict physical constraints and long-horizon decision-making. It unifies five diverse
  SPP tasks (ground station scheduling, revisit optimization, regional coverage, stereo
  imaging, and latency optimization) under a common agent-oriented interface.
---

# AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems

## Quick Facts
- **arXiv ID**: 2601.11354
- **Source URL**: https://arxiv.org/abs/2601.11354
- **Reference count**: 38
- **Primary result**: LLM agents substantially underperformed specialized solvers across five space planning problems, with performance gaps ranging from 0.17 to 0.53 in key metrics.

## Executive Summary
AstroReason-Bench evaluates agentic LLM planners on five heterogeneous space planning problems (SatNet, Revisit Optimization, Regional Coverage, Stereo Imaging, and Latency Optimization) that require reasoning under strict physical constraints and long-horizon decision-making. The benchmark unifies these diverse tasks under a common agent-oriented interface using physics-grounded simulation with SGP4 orbital propagation and resource dynamics. Across 150 simulations with six state-of-the-art agentic models, LLM agents showed adaptability to novel problem structures but struggled with resource management, spatial reasoning, and multi-step planning, substantially underperforming specialized solvers. While agents demonstrated reasoning advantages in compound constraint satisfaction (e.g., stereo imaging), they failed to match systematic optimizers in tasks requiring deep search over constrained spaces.

## Method Summary
The benchmark uses a physics-grounded simulation environment with SGP4 orbital propagation coupled with resource dynamics (energy integration, storage buffers) and kinematic constraints (slew time, settling time). It employs a four-layer architecture: a stateless physics engine, scenario manager with read-only inventory and mutable action timeline, semantic MCP interface and Python API for tool access, and ReAct-loop LLM agents. Evaluation uses 150 simulations across five benchmarks with 2-hour timeout and constrained compute (16GB RAM, 8 CPU cores). Six agentic models were tested against specialized solver baselines (Greedy, Simulated Annealing) on procedurally generated scenarios using archived TLE data and global city databases.

## Key Results
- LLM agents underperformed specialized solvers by 0.17-0.53 in RMS unsatisfied ratio and coverage metrics across all benchmarks
- Agents showed zero-shot adaptability to novel problem structures but struggled with resource management and multi-step planning
- Stereo imaging was the only benchmark where agents outperformed greedy baselines (up to 18% coverage vs 0%)
- Performance gaps were most pronounced in revisit optimization (storage exhaustion) and regional coverage (spatial reasoning deficits)
- Larger reasoning-intensive models and structured workflows may yield superior results compared to baseline "Flash-class" models

## Why This Works (Mechanism)

### Mechanism 1
Physics-grounded simulation creates hard feasibility boundaries that expose agent limitations not visible in symbolic benchmarks. The SGP4 orbital propagator coupled with resource dynamics and kinematic constraints produces mathematically unambiguous constraint violations rather than soft preferences. Core assumption: agents' struggles stem from inability to maintain feasibility across long horizons under coupled constraints. Evidence: simulation enforces Resource, Kinematic, and Concurrency Constraints classes. Break condition: if agents matched specialized solver performance, constraints would be insufficiently tight.

### Mechanism 2
Unified interface abstraction enables cross-task transfer evaluation by exposing heterogeneous problems through common protocol. MCP semantic interface and Python API wrap diverse SPP tasks into consistent observation/action primitives, allowing same agent architecture across structurally distinct problems. Core assumption: performance differences reflect reasoning gaps rather than interface learning overhead. Evidence: benchmark unifies five diverse SPP tasks under common agent-oriented interface. Break condition: if agents required extensive per-task prompt engineering, interface would be insufficiently unified.

### Mechanism 3
Compound constraint reasoning is where agents outperform greedy heuristics but underperform systematic optimizers. Tasks like stereo imaging require satisfying coupled constraints (azimuth separation, temporal sync, elevation angle) simultaneously; agents can reason about interdependencies zero-shot while greedy heuristics optimize single attributes. Core assumption: advantage reflects inherent reasoning capability rather than luck. Evidence: agents achieved 18% stereo coverage vs 0% for baselines by explicitly reasoning about "stereo pairs." Break condition: if stereo imaging success was uniform across agents, task would be trivially constrained.

## Foundational Learning

- **Orbital mechanics fundamentals (SGP4, TLE data, visibility windows)**
  - Why needed: All five benchmarks depend on accurate satellite position propagation; misunderstanding ground tracks causes orientation mismatches (regional coverage failure mode)
  - Quick check: Can you explain why a near-polar orbit produces predominantly N-S oriented ground tracks?

- **Resource-constrained scheduling (buffer dynamics, energy budgets)**
  - Why needed: Agents failed revisit optimization by exhausting storage before completing observations; understanding inflow/outflow dynamics is prerequisite for valid planning
  - Quick check: If a satellite generates 50W solar power and consumes 80W during observation, how long can it observe starting from 100Wh energy buffer?

- **Constraint satisfaction vs. optimization**
  - Why needed: Stereo imaging requires satisfying coupled constraints (feasibility) while SatNet requires minimizing unsatisfied ratio (optimization); agents excel at former, struggle with latter
  - Quick check: What's the difference between finding any valid stereo pair versus finding the stereo pair that minimizes revisit gap?

## Architecture Onboarding

- **Component map**: Physics Engine (SGP4, slew kinematics, resource manager) -> Scenario Manager (inventory database, action registry, state persistence) -> Interface (MCP semantic tools, Python API) -> Cognitive (ReAct-loop LLM agent with tool access)
- **Critical path**: Agent receives mission brief → queries inventory via MCP → computes candidate windows via Python API → stages actions → physics engine validates constraints → scenario manager commits or rejects
- **Design tradeoffs**: Stateless physics vs. stateful scenario (enables reproducibility but requires agent to manage session state); MCP vs. Python API (MCP optimized for LLM context, Python API required for numerical computation); 2-hour timeout, 16GB/8-core limits (constrains exhaustive search, favoring heuristic approaches)
- **Failure signatures**: Action bias (registering strips before querying ground tracks); single-hop assumption (seeking one satellite visible to both stations); storage exhaustion (scheduling observations without downlink planning); exploration-exploitation gap (reasoning from memory rather than querying environment tools)
- **First 3 experiments**: 1) Run Claude Sonnet 4.5 on SatNet W40_2018 in default mode; measure U_rms and trace tool usage patterns; 2) Compare stereo imaging performance with/without explicit "plan mode" prompting; 3) Inject RAG corpus for latency optimization task; evaluate multi-hop routing concept transfer

## Open Questions the Paper Calls Out

### Open Question 1
Can larger reasoning-intensive models or more sophisticated agentic workflows significantly close the performance gap with specialized solvers compared to the "Flash-class" baseline? The study restricted evaluation to cost-effective models to establish baseline feasibility diagnostic; frontier models with enhanced self-correction may yield superior results.

### Open Question 2
What architectural modifications are necessary for generalist agents to effectively utilize RAG-enhanced planning without manual intervention? Agents in default mode failed to utilize provided academic papers, degrading performance; they need structured workflows instead of raw ReAct loop to consume retrieved knowledge automatically.

### Open Question 3
How does agentic performance differ when the benchmark is extended to architectural system design and deep-space trajectory planning? Current benchmark scope is centered exclusively on operational scheduling and resource management; extending to system design and trajectory optimization remains a necessary step toward comprehensive suite.

## Limitations

- **Flash-class models**: Using cost-effective models likely represents a "lower bound" and larger reasoning-intensive models may yield superior results
- **RAG utilization**: Agents need structured workflows instead of raw ReAct loop to effectively consume retrieved knowledge
- **Scope limitation**: Current benchmark excludes architectural system design and deep-space trajectory planning

## Confidence

**High Confidence**: Physics-grounded simulation mechanism and unified interface abstraction are well-supported by direct evidence from benchmark design and implementation.

**Medium Confidence**: Generalizability of findings to other domains and agent architectures remains uncertain; failure modes may be partially attributable to specific SGP4 physics and constraint combinations.

**Low Confidence**: Precise contribution of individual architectural decisions to agent performance is not systematically isolated; relative importance of reasoning capability versus tool-use proficiency remains unclear.

## Next Checks

1. **Prompt Engineering Ablation**: Systematically vary system prompts and mission briefs across benchmarks to quantify impact of prompting quality on agent performance, isolating reasoning limitations from interface confusion.

2. **Extended Runtime Evaluation**: Remove 2-hour timeout constraint for select scenarios to determine whether agents can achieve competitive performance with additional search time, or whether fundamental reasoning limitations persist regardless of computational budget.

3. **Cross-Domain Transfer**: Evaluate same agent architecture on non-space domain with similar constraint structures (e.g., logistics scheduling or manufacturing planning) to test whether observed failure modes are domain-specific or reflect general limitations in agentic planning.