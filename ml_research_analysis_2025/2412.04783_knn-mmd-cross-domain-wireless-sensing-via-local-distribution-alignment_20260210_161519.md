---
ver: rpa2
title: 'KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment'
arxiv_id: '2412.04783'
source_url: https://arxiv.org/abs/2412.04783
tags:
- domain
- training
- target
- data
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of cross-domain wireless sensing,
  where environmental changes cause significant performance degradation in models
  trained in one environment when applied to another. The proposed method, KNN-MMD,
  introduces a local distribution alignment approach that uses K-Nearest Neighbors
  to construct a high-confidence "help set" from the target domain and then aligns
  the source and target domains within each category using Maximum Mean Discrepancy.
---

# KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment

## Quick Facts
- arXiv ID: 2412.04783
- Source URL: https://arxiv.org/abs/2412.04783
- Reference count: 40
- Primary result: KNN-MMD achieves 93.26% accuracy in one-shot gesture recognition across domains

## Executive Summary
This paper addresses the challenge of cross-domain wireless sensing, where models trained in one environment degrade significantly when applied to another due to environmental changes. The proposed KNN-MMD method introduces local distribution alignment using K-Nearest Neighbors to construct a high-confidence "help set" from the target domain, then aligns source and target distributions within each category using Maximum Mean Discrepancy. This approach outperforms traditional global alignment methods, achieving accuracy rates of 93.26%, 81.84%, 77.62%, and 75.30% for gesture recognition, person identification, fall detection, and action recognition tasks respectively in one-shot scenarios.

## Method Summary
KNN-MMD is a two-phase approach for cross-domain wireless sensing. First, it uses KNN classification with confidence filtering to identify high-quality samples from the target domain as a "help set" for alignment. Second, it trains a network using combined cross-entropy and local/global MMD losses, with early stopping based on support set validation. The method specifically addresses the limitation of global alignment by ensuring per-category distribution matching while using the support set (excluded from training) as a validation set for early stopping.

## Key Results
- Achieved 93.26% accuracy on WiGesture one-shot gesture recognition across domains
- Outperformed existing methods in person identification (81.84%), fall detection (77.62%), and action recognition (75.30%)
- Demonstrated improved stability and effectiveness across multiple Wi-Fi sensing tasks
- Showed insensitivity to hyperparameter choices (p%, k, d) with <10% accuracy variation

## Why This Works (Mechanism)

### Mechanism 1
KNN-based pseudo-labeling with confidence filtering can reliably identify high-accuracy samples from the target domain for use in local alignment. After UMAP dimensionality reduction, KNN classifies each test sample against the support set. Confidence is computed as the inverse distance to the nearest neighbor sharing the predicted label. Only the top p% highest-confidence samples form the "help set" for alignment. Core assumption: Distance-based confidence correlates with classification correctness—samples closer to support-set neighbors of the same class are more likely correctly labeled.

### Mechanism 2
Aligning source and target distributions within each category (local alignment) is more effective for cross-domain CSI classification than global alignment alone. The paper derives that global alignment P_s(g(x))≈P_t(g(x)) is insufficient to ensure P_s(y|g(x))≈P_t(y|g(x)). Local alignment additionally requires P_t(g(x)|y)≈P_s(g(x)|y). MK-MMD is applied per-category between training and help set embeddings, combined with a global MMD term to mitigate category imbalance.

### Mechanism 3
Excluding the support set from network training enables its use as a validation set, providing a practical early-stopping criterion for cross-domain adaptation. The support set provides KNN labels but does not participate in loss computation. During network training, support-set accuracy and loss are monitored. An adaptive early-stopping rule (with relaxation at minimum epochs and patience thresholds for both metrics) halts training when performance stagnates.

## Foundational Learning

- **Maximum Mean Discrepancy (MMD) and MK-MMD**: MMD quantifies distribution distance in a reproducing kernel Hilbert space; MK-MMD uses multiple kernels to approximate the supremum. Core to measuring whether source and target embeddings share the same distribution. Quick check: Given two sets of samples X_s and X_t, write out the MMD² estimator using a kernel function k(·,·). Why does MK-MMD use multiple kernels?

- **K-Nearest Neighbors Classification and Distance-Based Confidence**: KNN provides the pseudo-labels and confidence scores for help set construction; understanding its sensitivity to k and dimensionality is critical. Quick check: For k=3, how would you classify a test sample and compute its confidence if distances to support-set neighbors are [0.5 (class A), 0.7 (class A), 1.2 (class B)]?

- **Domain Adaptation Assumptions (Covariate Shift vs. Conditional Shift)**: The paper argues that global alignment methods assume P_t(y|x)=P_s(y|x), which often fails for CSI. Understanding this clarifies why local alignment is proposed. Quick check: What assumption does traditional DAL make about conditional distributions, and how does KNN-MMD relax it?

## Architecture Onboarding

- **Component map**: Input CSI (1×100×52) -> ResNet18 feature extractor -> 3-layer MLP -> Classification output
- **Critical path**: 1. Prepare data splits (support set excluded from training loss) 2. UMAP reduction (d=128) -> KNN (k=1) -> select top p=50% confidence samples as help set 3. Train ResNet18+MLP with combined loss; validate on support set each epoch; apply early stopping
- **Design tradeoffs**: p% higher includes more samples but lowers pseudo-label quality; k sensitivity observed but smaller k may increase variance; d 32-128 tested with minimal accuracy difference; Gaussian kernels outperform Laplacian but results are dataset-dependent
- **Failure signatures**: Help set quality collapse (verify top p% pseudo-label accuracy >90%), category imbalance in help set (log per-class counts), early stopping misfire (plot support vs test accuracy curves), global MMD dominating local MMD (ensure α₁>>α₂)
- **First 3 experiments**: 1. Baseline domain shift quantification: Train ResNet18 on WiGesture IDs 1-7, test on ID 0 (zero-shot). Target ~40.84% to confirm shift exists. 2. KNN-only ablation (no network training): Vary k∈{1,2,3,4}, n∈{1,2,3,4} shots; document accuracy range and variance. 3. Full KNN-MMD pipeline with default hyperparameters (k=1, p=50%, d=128, Gaussian kernels). Track support vs test accuracy curves; verify early stopping triggers appropriately.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the KNN-MMD framework be effectively extended to regression tasks, such as indoor localization or vital sign monitoring? The current framework could be used only for classification tasks. It would be worth exploring its application in regression tasks.

- **Open Question 2**: How does the framework perform if the KNN or MMD components are replaced by other metric-based or few-shot learning methods? It would be interesting to investigate the effectiveness of other combinations of metric-based methods or few-shot learning methods with DAL methods.

- **Open Question 3**: Can this local alignment framework be combined with zero-shot learning techniques to eliminate the requirement for a support set? By combining a zero-shot learning method with the DAL framework, we could also expand its range of applications.

## Limitations

- The method depends critically on help set pseudo-labels being >90% accurate, which may not hold under severe domain shifts
- Critical hyperparameters (loss weights α₁, α₂, exact MLP architecture, UMAP hyperparameters) are unspecified, making precise reproduction difficult
- The approach is evaluated only on CSI-based sensing tasks with pre-trained ResNet18 features; performance on other wireless modalities or backbone architectures is unknown

## Confidence

**High Confidence**: The fundamental observation that global MMD alignment alone is insufficient for cross-domain CSI classification is well-supported by the formal derivation and UMAP visualizations. The early stopping mechanism using the support set as validation is methodologically sound.

**Medium Confidence**: The KNN-based help set construction achieves >90% pseudo-label accuracy under the tested conditions (WiGesture dataset, specific domain shifts). However, performance may degrade with more severe domain shifts or different datasets.

**Low Confidence**: The claim that KNN-MMD is "insensitive" to hyperparameter choices (p%, k, d) is based on limited ablation studies. The paper does not explore extreme domain shifts or pathological cases where help set quality might collapse.

## Next Checks

1. **Help Set Quality Stress Test**: Systematically evaluate KNN-MMD performance across a spectrum of domain shifts (from mild to severe) using multiple CSI datasets. Document the relationship between domain shift magnitude and help set pseudo-label accuracy.

2. **Cross-Modality Transferability**: Implement KNN-MMD on a non-CSI wireless sensing task (e.g., FMCW radar gesture recognition) to test whether the local alignment approach generalizes beyond the CSI domain.

3. **Class Imbalance Robustness**: Design experiments with artificially imbalanced help sets (varying class frequencies from 10% to 90%) to quantify how severe category imbalance affects local MMD alignment quality and overall classification accuracy.