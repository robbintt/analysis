---
ver: rpa2
title: Iconicity in Large Language Models
arxiv_id: '2501.05643'
source_url: https://arxiv.org/abs/2501.05643
tags:
- pseudowords
- czech
- language
- german
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigated whether large language models (LLMs) can\
  \ generate and understand iconicity\u2014the direct relationship between word form\
  \ and meaning. GPT-4 was prompted to create highly iconic pseudowords in an artificial\
  \ language, which were then tested with Czech and German human participants (n=672)\
  \ and LLM-based participants (generated by GPT-4 and Claude 3.5 Sonnet)."
---

# Iconicity in Large Language Models

## Quick Facts
- arXiv ID: 2501.05643
- Source URL: https://arxiv.org/abs/2501.05643
- Reference count: 28
- Primary result: LLMs can effectively encode and process iconicity despite indirect access to semantic and phonetic aspects

## Executive Summary
This study investigates whether large language models can generate and understand iconicity—the direct relationship between word form and meaning. GPT-4 was prompted to create highly iconic pseudowords in an artificial language, which were then tested with Czech and German human participants (n=672) and LLM-based participants (generated by GPT-4 and Claude 3.5 Sonnet). Results showed that humans could guess meanings of these pseudowords more accurately than words in distant natural languages, with correctness rates around 65-67% compared to 57-60% for natural languages. LLM-based participants performed even better, achieving correctness rates of 75-82%. The study also found that participants relied on similar cues for both natural and artificial languages, particularly vector phonological similarity, edit phonological similarity, and length agreement.

## Method Summary
The experiment consisted of two phases: (1) Generation Phase - GPT-4 was prompted to invent words for a fictional South American "Tubar" language for 68 concepts derived from HindEnCorp, and (2) Evaluation Phase - These pseudowords were tested in a forced-choice matching task where participants had to match two meanings with two pseudowords. The experiment was conducted with both human participants (Czech and German speakers) and LLM-based participants (GPT-4 and Claude 3.5 Sonnet). Accuracy rates were measured and analyzed using mixed logistic regression to identify which cues (length agreement, phonological similarity) predict correct answers.

## Key Results
- Human participants achieved 65-67% correctness rates on iconic pseudowords, significantly higher than 57-60% for natural languages
- LLM-based participants achieved 75-82% correctness rates, outperforming humans
- Length agreement, edit phonological distance, and vector phonological distance were the most influential features for correct guesses across both natural and artificial languages

## Why This Works (Mechanism)

### Mechanism 1: Statistical Extraction of Meta-Linguistic Iconicity Patterns
- Claim: LLMs can encode and generate sound-symbolic relationships despite lacking direct acoustic or embodied experience.
- Mechanism: LLMs extract statistical regularities linking orthographic representations, phonological patterns, and semantic concepts from their training data. These patterns manifest as "iconicity" through a mediated chain: textual descriptions of sounds, onomatopoeia, literary analyses of sound symbolism, and direct form-meaning mappings in existing languages.
- Core assumption: The training data contains sufficient co-occurrence statistics between word forms and their associated meanings or descriptive characteristics (e.g., "soft," "sharp," "round") for the model to internalize non-arbitrary mappings.
- Evidence anchors:
  - [abstract] LLMs' access to both meaning and sound of text is only mediated (meaning through textual context, sound through written representation).
  - [section 1, page 2] LLMs access iconicity through academic texts about iconicity, non-academic discourse, literature, children's literature, and poetry.
  - [section 4.3, page 13] The model frequently reasoned 'intuitively' and described feelings about the sound of the words connected to particular meanings.
  - [corpus] Related work ("Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism") investigates a similar hypothesis in multimodal LLMs, suggesting this is an active area of inquiry.
- Break condition: The training data is devoid of explicit discussions of iconicity or lacks a sufficient volume of sound-symbolic vocabulary (onomatopoeia, ideophones) for patterns to be learned.

### Mechanism 2: Generalization via Abstract Semantic-Phonological Features
- Claim: The iconicity generated by LLMs is partially universal and not strictly language-specific to the prompting language.
- Mechanism: The model learns abstract associations between phonological features (e.g., front vowels, voiced stops) and semantic dimensions (e.g., small size, roundness) that are statistically prevalent across multiple languages in its training corpus. When prompted to create a new "iconic" language, it applies these generalized mappings rather than simply copying from the prompt language.
- Core assumption: There are cross-linguistic regularities in sound symbolism (e.g., the bouba-kiki effect) that are present in the model's diverse training data and can be abstracted into general rules.
- Evidence anchors:
  - [abstract] Results showed that humans could guess meanings... Czech and German participants.
  - [section 4.1, page 9] Czech-prompted pseudowords were guessed comparably well by Czech and by German speakers... Germans, however, were significantly better in guessing German-prompted pseudowords than Czech speakers.
  - [section 4.1, page 9] ...LLMs rely not only on language-specific iconicity but also on certain general features — features that are general enough to make sense cross-linguistically.
  - [corpus] The paper "Adversarially Probing Cross-Family Sound Symbolism in 27 Languages" provides evidence for cross-linguistic sound symbolism in the semantic domain of size, supporting the idea of learnable general features.
- Break condition: The iconicity is purely a result of the model overfitting to patterns in the specific language of the prompt (e.g., Czech), leading to generated words that are only intelligible to speakers of that language.

### Mechanism 3: Convergent Cue Utilization in LLMs and Humans
- Claim: Both humans and LLMs rely on a similar set of surface-level cues to infer word meanings, particularly length agreement and phonological similarity.
- Mechanism: Participants (both human and LLM) use heuristics to solve the forced-choice meaning-guessing task. They match words based on simple, extractable features: word length (short word = short word) and phonological similarity (similar sounds = similar meanings). LLMs' superior performance suggests they may have access to additional or more precise cues derived from their statistical training.
- Core assumption: The task of guessing meaning from an unknown iconic word is, at least in part, a process of pattern matching on phonological and structural features rather than a deep, embodied understanding of sound symbolism.
- Evidence anchors:
  - [abstract] Participants relied on similar cues for both natural and artificial languages, particularly vector phonological similarity, edit phonological similarity, and length agreement.
  - [section 4.2, page 11] The three most influential features influencing correct guesses of words from natural languages (length agreement, edit phonological distance, and vector phonological distance) was found equally important when guessing pseudowords.
  - [corpus] Weak/no direct corpus evidence for this specific mechanism in related papers.
- Break condition: The logistic regression model accounts for nearly all the variance (near-zero intercept), meaning the identified cues completely explain the performance. The paper states the intercepts are large, implying other unmeasured factors are at play.

## Foundational Learning

**Sound Symbolism / Lexical Iconicity**
- Why needed here: This is the core subject of the paper. Understanding that a word's sound can non-arbitrarily relate to its meaning is essential to grasp what the study is investigating.
- Quick check question: Can you define the "bouba-kiki" effect and explain why it is an example of sound symbolism?

**BPE Tokenization**
- Why needed here: The paper identifies tokenization as a key reason why LLMs' access to phonology is "mediated" and potentially problematic for processing iconicity.
- Quick check question: How does Byte Pair Encoding (BPE) break down the word "unhappiness" into tokens, and why might this make it harder for a model to "hear" the sounds?

**Logistic Regression & Independent Variables**
- Why needed here: The study's main quantitative finding relies on a logistic regression model to identify which cues (length, phonological similarity, etc.) predict correct answers.
- Quick check question: In the context of this study, what does a positive coefficient for the "length agreement" variable mean?

## Architecture Onboarding

**Component map**: Generation Agent (GPT-4) -> Generate Pseudowords -> Transcribe to Target Orthographies -> Run Human & LLM Experiments -> Perform Statistical Analysis on Results

**Critical path**: The flow is 1) Prompt Generation Agent -> 2) Generate Pseudowords -> 3) Transcribe to Target Orthographies -> 4) Run Human & LLM Experiments -> 5) Perform Statistical Analysis on Results.

**Design tradeoffs**:
- Using a forced-choice paradigm (2 meanings, 2 words) simplifies the task and allows for clear accuracy measurement but may overestimate intelligibility compared to free-response tasks.
- Using GPT-4 as both the generator and one of the evaluators introduces potential bias (the model might recognize its own patterns), which is mitigated by also using Claude 3.5 Sonnet as an independent evaluator.

**Failure signatures**:
- **Language Leakage:** The generated pseudowords are too similar to real words in the prompt language (e.g., Czech), making the task one of translation rather than iconicity guessing.
- **Pattern Matching Over-reliance:** The model performs well on the task only by using simple heuristics (like matching word length) without generating genuinely iconic forms.

**First 3 experiments**:
1. **Ablation Study on Generation Prompts:** Run the experiment again with different prompt variations (e.g., no geographic constraint, no "iconic" instruction) to measure the impact of prompt engineering on the quality of generated iconicity.
2. **Cross-Model Generalization:** Test a wider variety of LLMs (including smaller or older models) as both generators and evaluators to see if this ability scales with model size and training data.
3. **Cue-Specific Analysis:** Design stimuli that isolate specific cues (e.g., only length, only phonological similarity) to more precisely measure the contribution of each cue to the success rate.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** To what extent are the observed iconicity effects generalizable versus artifacts of the specific pseudoword datasets generated?
- **Basis in paper:** [explicit] The authors explicitly state that "replication of the study with newly generated Czech-prompted and German-prompted pseudoword could be interesting" to rule out untrackable idiosyncrasies of the specific datasets used.
- **Why unresolved:** The study relies on a single set of GPT-4 generated words per language; it is unclear if the high success rates are robust across different generation attempts.
- **What evidence would resolve it:** A replication study using newly generated pseudoword lists from different random seeds or prompts, tested on new participant cohorts.

**Open Question 2**
- **Question:** What specific unmeasured cues are LLMs utilizing to guess iconic meanings that allow them to outperform humans?
- **Basis in paper:** [inferred] In the logistic regression analysis (Section 4.2), the authors note that "large intercepts indicate that there are still many variables we have not managed to capture in our analysis," suggesting LLMs leverage features beyond length and phonological similarity.
- **Why unresolved:** The regression model leaves a significant portion of the variance unexplained, particularly regarding the superior performance of LLM-based participants.
- **What evidence would resolve it:** A feature ablation study or interpretability analysis (e.g., examining attention heads) to identify non-obvious statistical regularities in the pseudowords that models utilize.

**Open Question 3**
- **Question:** Why do LLMs employ compositional reasoning when generating iconic words but rely on intuitive/heuristic reasoning when guessing them?
- **Basis in paper:** [inferred] Section 4.3 notes a discrepancy in reasoning styles: "The argumentation in words creation was more compositional... [while] it did not use abstract/concrete nature... during the word creations."
- **Why unresolved:** The paper observes this qualitative difference in the model's chain-of-thought outputs but does not determine if this is a limitation of the prompt structure or an intrinsic property of the model's processing.
- **What evidence would resolve it:** Experiments that force the model to use consistent explicit reasoning frameworks for both tasks to see if performance or alignment with human reasoning changes.

## Limitations

- **Potential circularity in methodology:** Using GPT-4 both to generate and evaluate pseudowords introduces potential bias, though this is partially mitigated by including Claude 3.5 Sonnet as an independent evaluator.
- **Forced-choice paradigm limitations:** The study uses a simplified forced-choice paradigm which may inflate accuracy scores compared to free-response tasks and doesn't capture the full complexity of iconicity understanding.
- **Unclear generalizability:** The study doesn't test whether the observed iconicity patterns extend to other semantic domains, languages, or experimental paradigms beyond the specific setup used.

## Confidence

**High Confidence**: The empirical finding that LLM-based participants achieved 75-82% correctness rates compared to 65-67% for human participants. This represents a clear, measurable difference in the experimental data.

**Medium Confidence**: The conclusion that LLMs can encode and process iconicity despite indirect access to semantic and phonetic aspects. While the results support this, the mechanism (statistical extraction vs. genuine understanding) remains somewhat speculative.

**Low Confidence**: The generalizability of the iconicity patterns beyond the specific experimental setup. The study doesn't test whether these findings extend to other semantic domains, languages, or experimental paradigms.

## Next Checks

1. **Cross-Domain Replication**: Replicate the experiment using different semantic domains (not just the HindEnCorp frequency-stratified words) to test whether the observed iconicity patterns generalize beyond the specific concepts tested.

2. **Blind Evaluation**: Have the pseudowords evaluated by human participants who are linguistically naive to both Czech and German to determine if the iconicity is truly cross-linguistic or merely language-specific.

3. **Tokenization Impact Analysis**: Systematically vary the tokenization approach (different BPE sizes, character-level models) to quantify how tokenization affects the model's ability to process phonological iconicity, addressing the "mediated access" concern directly.