---
ver: rpa2
title: Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval
arxiv_id: '2601.13969'
source_url: https://arxiv.org/abs/2601.13969
tags:
- retrieval
- graph
- search
- arxiv
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ARK, an agentic knowledge graph retriever
  that balances global lexical search with multi-hop neighborhood exploration to adaptively
  gather evidence. ARK gives a language model fine-grained control over breadth-depth
  trade-offs, using a two-tool interface: global search for broad coverage and one-hop
  neighborhood expansion for relational reasoning.'
---

# Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval

## Quick Facts
- **arXiv ID:** 2601.13969
- **Source URL:** https://arxiv.org/abs/2601.13969
- **Reference count:** 40
- **Primary result:** ARK achieves 59.1% Hit@1 and 67.4% MRR on STaRK benchmark, improving over training-free methods by up to 31.4% Hit@1 and 28.0% MRR

## Executive Summary
ARK is an agentic knowledge graph retriever that balances global lexical search with multi-hop neighborhood exploration to adaptively gather evidence. The system gives language models fine-grained control over breadth-depth trade-offs through a minimal two-tool interface: global search for broad coverage and one-hop neighborhood expansion for relational reasoning. On the STaRK benchmark, ARK achieves 59.1% Hit@1 and 67.4% MRR on average, significantly improving over training-free methods. The approach also enables distillation of the tool-use policy to smaller models, retaining up to 98.5% of the teacher's performance while improving over base models by 7.0 to 26.6 absolute points across datasets.

## Method Summary
ARK uses an LLM agent with a two-tool interface to explore knowledge graphs: GlobalSearch(q,k) for BM25-based lexical search across the entire graph, and Neighbors(v,q,F) for typed, query-ranked one-hop neighborhood expansion. The agent decides when to broaden (global search) versus deepen (neighborhood expansion) based on intermediate observations. Multiple parallel agents (n=3) with Tmax=20 max steps explore independently, and their outputs are aggregated via vote-count rank fusion. For distillation, teacher trajectories (tool calls + observations) are collected using GPT-4.1 and used to train a smaller Qwen3-8B model with label-free imitation learning, where supervision comes solely from the teacher's decision patterns.

## Key Results
- ARK achieves 59.1% Hit@1 and 67.4% MRR on STaRK benchmark on average
- Improves over training-free methods by up to 31.4% Hit@1 and 28.0% MRR
- Adapts tool use to query type, using global search for text-heavy queries and neighborhood exploration for relation-heavy queries
- Distilled 8B model retains up to 98.5% of teacher's Hit@1 while improving over base model by +7.0 to +26.6 absolute points

## Why This Works (Mechanism)

### Mechanism 1
A minimal two-tool interface enables LLMs to coordinate breadth and depth retrieval without fragile seed selection or fixed hop depths. Global lexical search maps query text to candidate nodes across the entire graph, while neighborhood exploration performs typed, query-ranked one-hop expansion that composes into multi-hop traversal. The LLM decides when to broaden versus deepen based on intermediate observations.

### Mechanism 2
ARK autonomously adapts tool-use patterns to match query characteristics, shifting toward global search for text-heavy queries and neighborhood exploration for relation-heavy queries. The agent receives no explicit query-type labels but infers whether queries are descriptive (needing broad candidate retrieval) or relational (needing path traversal).

### Mechanism 3
Label-free trajectory distillation transfers the teacher's tool-use policy to a smaller model, preserving ~98.5% of Hit@1 while reducing inference cost. Teacher trajectories are collected via stochastic decoding, and the student is trained with next-token prediction on assistant-authored tokens only, requiring no ground-truth relevance labels.

## Foundational Learning

- **Knowledge Graphs (nodes, edges, types, attributes)**: Understanding schema and neighborhood structure is prerequisite for ARK's operation over heterogeneous KGs with typed nodes/edges and text-rich node descriptors. Quick check: Given a node representing a drug with edges to "targets_gene" and "has_side_effect", what does a one-hop expansion return?

- **BM25 / Lexical Retrieval**: ARK's global search and neighborhood ranking use BM25 over an inverted index of node text attributes. Understanding TF-IDF weighting and query-term matching helps diagnose retrieval failures. Quick check: Why might BM25 underperform for paraphrased entity names versus exact keyword matches?

- **LLM Agents and Tool-Use Frameworks**: ARK frames retrieval as an interactive process where an LLM agent issues tool calls based on observations. Familiarity with ReAct-style reasoning and tool interfaces is essential. Quick check: In a tool-use trajectory, what information does the agent receive after calling `Neighbors(v, q, F)`?

## Architecture Onboarding

- **Component map**: LLM Agent -> Tool Interface (GlobalSearch, Neighbors, FINISH) -> KG Index (BM25 inverted index + graph structure) -> Parallel Engine (n agents with vote-count fusion)

- **Critical path**: Query received → agent issues GlobalSearch(q,k) → agent inspects results → issues Neighbors(v,q,F) on promising nodes → agent SELECTs relevant nodes into R → agent calls FINISH or reaches Tmax → aggregate parallel outputs → return ranked list

- **Design tradeoffs**: Agent count vs. latency (1→2 agents gives substantial gains; bounded by slowest agent); tool-call budget (5–10 steps suffice; deeper helps complex multi-hop but risks drift); student model size (8B retains ~98.5% teacher Hit@1; 4B shows larger gaps on biomedical queries)

- **Failure signatures**: Zero neighborhood calls (agent never transitions from global search); excessive neighborhood calls (>10) indicating drift in high-branching regions; sparse node descriptors causing BM25 failure to surface relevant candidates

- **First 3 experiments**: 1) Baseline tool ablation: run ARK with only Search_G on MAG/PRIME subsets; 2) Agent count sweep: compare Hit@1 and latency for n=1,2,3 agents; 3) Distillation data scaling: train students on 600 vs. 6,000 teacher trajectories

## Open Questions the Paper Calls Out

1. **Adaptive stopping criteria**: Can adaptive stopping criteria be developed to prevent over-expansion in neighborhood exploration while maintaining retrieval quality? The paper identifies agents either making no neighborhood calls or too many without converging, but uses fixed Tmax=20 rather than adaptive termination.

2. **Dense vs. lexical retrieval**: Does replacing BM25 with dense semantic retrieval improve performance on queries with vocabulary mismatch? The paper notes BM25's lexical nature can cause under-retrieval for paraphrases and domain-specific aliases.

3. **Sparse-text knowledge graphs**: How does ARK's performance degrade on knowledge graphs with limited textual node descriptions? The evaluation is centered on text-rich KG benchmarks, so performance gains may not transfer to graphs with limited text descriptions.

4. **Model capacity requirements**: What is the minimum model capacity required for effective tool-use policy distillation across heterogeneous graph domains? The teacher-student gap is largest on PRIME (biomedical), where the 8B student retains less performance than on AMAZON or MAG.

## Limitations

- Relies on GPT-4.1 (unreleased variant) for teacher trajectory generation, making exact replication impossible
- Critical BM25 hyperparameters and node descriptor construction details are unspecified
- Does not evaluate performance on knowledge graphs with limited textual node descriptions
- Teacher trajectory quality and potential systematic errors are not addressed

## Confidence

- **High confidence**: Core two-tool mechanism and performance improvements (59.1% Hit@1, 67.4% MRR) are well-supported by ablation studies and direct measurements
- **Medium confidence**: Autonomous adaptation to query characteristics is supported by tool-use statistics matching query-type annotations, but failure rates for ambiguous queries are not quantified
- **Medium confidence**: Distillation results (98.5% Hit@1 retention) are promising but depend heavily on GPT-4.1 teacher quality and trajectory diversity

## Next Checks

1. Systematically evaluate ARK's performance breakdown by query type to quantify where the adaptive tool-use mechanism succeeds or fails, particularly for queries that don't fit cleanly into either category.

2. Examine the distribution of tool-use patterns in teacher-generated trajectories to identify systematic biases or error patterns that could be inherited by the student during distillation.

3. Experiment with different BM25 hyperparameters and node descriptor construction methods to determine their impact on global search effectiveness and overall ARK performance, particularly on domains with sparse or templated node descriptions.