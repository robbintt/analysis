---
ver: rpa2
title: Chinese Discharge Drug Recommendation in Metabolic Diseases with Large Language
  Models
arxiv_id: '2510.21084'
source_url: https://arxiv.org/abs/2510.21084
tags:
- drug
- recommendation
- medical
- medication
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Chinese Discharge Drug Recommendation in Metabolic Diseases with Large Language Models

## Quick Facts
- arXiv ID: 2510.21084
- Source URL: https://arxiv.org/abs/2510.21084
- Reference count: 40
- Primary result: F1 score of 0.5648 achieved with GLM4-9B-Chat using supervised fine-tuning

## Executive Summary
This study evaluates large language models for Chinese discharge drug recommendation in metabolic diseases using 5,894 de-identified EHR records. The research compares zero-shot, one-shot, and chain-of-thought prompting against supervised fine-tuning with LoRA across three model families (GLM4, Llama3.1, Qwen2.5). SFT with LoRA consistently outperforms prompting strategies, with the best model achieving F1=0.5648. The study highlights that while supervised fine-tuning substantially improves accuracy over prompt-based methods, there remains substantial room for improvement, and current LLMs are not yet capable of reliably leveraging generative reasoning chains for drug recommendation tasks.

## Method Summary
The study uses 5,894 Chinese EHR records (3,602 train/570 val/1,722 test) from 3,190 patients with metabolic diseases. Each record includes seven clinical fields: chief complaint, history of present illness, past medical history, condition on admission, clinical course, and discharge diagnosis. The task requires generating discharge medication lists from 651 candidate drugs. Three prompting strategies (0-shot, 1-shot, chain-of-thought) and supervised fine-tuning with LoRA are evaluated across three model families (GLM4-9B, Llama3.1-8B, Qwen2.5-7B). Drug names are normalized using an LLM and the DXY database. Evaluation uses precision, recall, F1-score, and Jaccard similarity metrics.

## Key Results
- SFT with LoRA consistently outperforms all prompting strategies (0-shot, 1-shot, CoT) on both validation and test sets
- Model size correlates positively with performance: F1 scores increase from 0.18-0.32 (smaller models) to 0.56 (GLM4-9B with SFT)
- CoT prompting does not lead to consistent improvements and sometimes performs worse than 0-shot
- Largest performance gap exists between SFT and prompt-based methods, with SFT achieving F1=0.5648 vs 0.18-0.32 for prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning with LoRA substantially improves drug recommendation accuracy over prompt-based strategies
- Mechanism: SFT updates low-rank adaptation matrices to align pre-trained representations with domain-specific clinical patterns, enabling the model to learn mappings from EHR text features to medication combinations that prompt engineering alone cannot elicit
- Core assumption: The performance gap reflects genuine task alignment rather than overfitting to training distribution
- Evidence anchors: Abstract states "supervised fine-tuning improves model performance" with best F1=0.5648; section 4.4 shows SFT consistently outperforms all other inference strategies; corpus papers note state-of-the-art approaches "fall short in providing any insights on the derivation process"

### Mechanism 2
- Claim: Larger model scale correlates with better drug recommendation performance under all strategies
- Mechanism: Increased parameter count provides greater capacity for encoding clinical knowledge, reasoning about comorbidities, and generating appropriate multi-drug combinations
- Core assumption: Performance gains derive from capacity rather than training data contamination or scale-dependent optimization dynamics
- Evidence anchors: Section 4.4 shows "as the model size grows from 0.5B to 7B parameters, both Jaccard and F1 scores steadily increase"; authors state "larger models possess stronger capabilities in understanding and generating appropriate drug prescriptions"

### Mechanism 3
- Claim: Chain-of-thought prompting does not reliably improve drug recommendation and may degrade performance
- Mechanism: CoT elicits reasoning traces that increase output length and variability without enforcing clinically valid intermediate steps; the model lacks grounding to ensure reasoning chains map to correct prescriptions
- Core assumption: The failure of CoT reflects reasoning limitations rather than prompt design issues
- Evidence anchors: Section 4.4 states "the CoT strategy does not lead to consistent improvements and, in some cases, performs worse than 0-shot"; authors conclude "current LLMs are not yet capable of reliably leveraging generative reasoning chains for drug recommendation tasks"

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: Enables efficient fine-tuning of large models on limited clinical data without modifying full weight matrices, preserving pre-trained knowledge while adapting to drug recommendation patterns
  - Quick check question: Can you explain why LoRA adds trainable decomposition matrices rather than updating all weights directly?

- Concept: **Multi-label set prediction with constrained output space**
  - Why needed here: Drug recommendation requires outputting sets of medications from a fixed vocabulary; the task formulation constrains predictions to 651 candidate drugs to reduce hallucination and improve comparability
  - Quick check question: How does providing an explicit medication library in the prompt differ from unconstrained text generation for drug names?

- Concept: **EHR multi-field integration**
  - Why needed here: The model must synthesize information across chief complaint, history of present illness, past medical history, condition on admission, clinical course, and discharge diagnosis to infer appropriate medications
  - Quick check question: Which EHR fields would you expect to be most predictive for metabolic disease discharge medications, and why?

## Architecture Onboarding

- Component map: Input EHR fields (demographics, clinical course, diagnoses, histories) → Unified prompt template → LLM backbone (GLM4/Llama3.1/Qwen2.5) → LoRA adapter layers (if SFT) → Constrained drug list output → Evaluation metrics (Jaccard, Precision, Recall, F1)

- Critical path: Data preprocessing (drug name normalization via DXY database) → Prompt construction with medication library → Model inference (0-shot/1-shot/CoT) or SFT training loop → Checkpoint selection on validation F1 → Test set evaluation

- Design tradeoffs: SFT requires labeled training data but yields substantial gains; prompt-only approaches enable rapid iteration but plateau at ~0.32 F1; CoT adds interpretability potential but currently degrades accuracy; larger models improve performance but increase inference cost

- Failure signatures:
  - 0-shot generates extensive drug lists with many irrelevant medications (over-recommendation)
  - 1-shot reduces noise but retains extraneous cardiovascular/adjunct drugs
  - CoT produces reasoning traces that don't translate to better predictions
  - All non-SFT strategies show precision-recall imbalance (high recall, low precision for 0-shot)

- First 3 experiments:
  1. Replicate baseline comparison: Run GLM4-9B-Chat with 0-shot, 1-shot, CoT, and SFT on the provided dataset split to verify reported F1 ranges (0.18–0.56)
  2. Ablate input fields: Remove one EHR field at a time (e.g., past medical history, clinical course) to measure contribution of each information source to prediction accuracy
  3. Test constrained decoding: Enforce output to valid drug library vocabulary at inference time (even for non-SFT models) to measure whether output space constraint improves precision without SFT

## Open Questions the Paper Calls Out

- **Can advanced alignment techniques, such as reinforcement learning, outperform supervised fine-tuning (SFT) with LoRA for this task?**
  - Basis in paper: The conclusion explicitly states that "Future work will explore more advanced alignment techniques (such as reinforcement learning)"
  - Why unresolved: The current study only evaluates zero-shot, in-context learning, and SFT; RL-based alignment is untested
  - What evidence would resolve it: Comparative results showing F1 and Jaccard scores for models trained with RL alignment against the current SFT baseline

- **How can "reasoning control mechanisms" be improved to make Chain-of-Thought (CoT) prompting effective for drug recommendation?**
  - Basis in paper: The authors note that CoT prompting failed to consistently improve results and listed "improved reasoning control mechanisms" as a specific avenue for future work
  - Why unresolved: Current CoT implementations performed worse than simple few-shot prompting, suggesting the models cannot yet reliably leverage generative reasoning chains for this task
  - What evidence would resolve it: A modified prompting strategy utilizing reasoning control that achieves higher Jaccard scores than the 0-shot or 1-shot baselines

- **Do models trained on this dataset maintain performance across diverse hospital environments and patient populations?**
  - Basis in paper: The conclusion explicitly calls for "broader cross-institutional evaluation to ensure clinical reliability across diverse medical environments"
  - Why unresolved: The dataset is derived from a single institution (Second Affiliated Hospital of Dalian Medical University), limiting known generalizability
  - What evidence would resolve it: Evaluation of the fine-tuned models on an external test set collected from different hospitals, showing sustained F1 scores

## Limitations

- Proprietary dataset prevents independent validation and assessment of generalizability beyond the single institution
- No clinical expert review of model recommendations to assess safety or alignment with medical best practices
- Analysis limited to metabolic diseases only, with unclear applicability to other therapeutic areas
- No temporal validation to assess performance on future admissions or evolving clinical practices

## Confidence

- **High Confidence** in comparative performance findings: SFT with LoRA consistently outperforms prompting strategies on the same dataset, and larger models show predictable performance improvements
- **Medium Confidence** in clinical relevance: While F1 scores are reasonable, there is no external validation, no analysis of false positive risks, and no assessment of whether predictions match physician decision-making
- **Low Confidence** in real-world generalizability: Single hospital dataset, narrow patient population, lack of temporal validation, and no assessment of deployment challenges

## Next Checks

1. **External Dataset Validation**: Test the best-performing model (GLM4-9B-Chat with SFT) on an independent Chinese EHR dataset from a different hospital or time period to assess generalization and identify potential overfitting to the original dataset's patient population or documentation patterns

2. **Clinical Expert Review**: Have practicing physicians independently evaluate 100 randomly selected model predictions (across the performance spectrum) for clinical appropriateness, identifying potentially dangerous recommendations, omissions of critical medications, or prescriptions that violate standard of care guidelines

3. **Temporal Validation Study**: Split the dataset by admission date and train on earlier periods while testing on later periods to assess whether the model maintains performance over time, which would indicate robustness to evolving clinical practices and changing patient populations