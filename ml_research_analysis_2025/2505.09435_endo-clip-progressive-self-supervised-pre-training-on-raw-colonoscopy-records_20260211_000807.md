---
ver: rpa2
title: 'Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records'
arxiv_id: '2505.09435'
source_url: https://arxiv.org/abs/2505.09435
tags:
- polyp
- learning
- images
- endo-clip
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Endo-CLIP is a progressive self-supervised framework for pre-training\
  \ on colonoscopy image-text records, addressing challenges like uninformative background\
  \ images, complex medical terminology, and multi-lesion ambiguity. It employs a\
  \ three-stage pipeline: (1) cleansing\u2014removing non-informative frames using\
  \ diagnostic text and CLIP-based filtering, (2) attunement\u2014leveraging large\
  \ language models to extract morphological attributes for fine-grained contrastive\
  \ learning, and (3) unification\u2014resolving multi-polyp ambiguities through patient-level\
  \ cross-attention."
---

# Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records

## Quick Facts
- **arXiv ID:** 2505.09435
- **Source URL:** https://arxiv.org/abs/2505.09435
- **Reference count:** 32
- **Primary result:** Achieves 92.13% AUROC for malignancy classification in zero-shot setting, outperforming CLIP baseline by 13.91% AUROC.

## Executive Summary
Endo-CLIP introduces a progressive self-supervised pre-training framework for colonoscopy image-text records, addressing challenges like uninformative background frames, complex medical terminology, and multi-lesion ambiguity. The method employs a three-stage pipeline: (1) Cleansing—removes non-informative frames using diagnostic text and CLIP-based filtering; (2) Attunement—leverages LLMs to extract morphological attributes for fine-grained contrastive learning; and (3) Unification—resolves multi-polyp ambiguities through patient-level cross-attention. Evaluated on polyp detection and malignancy classification tasks, Endo-CLIP achieved state-of-the-art performance, demonstrating effectiveness for zero-shot and few-shot endoscopic analysis.

## Method Summary
Endo-CLIP is a three-stage progressive self-supervised framework that progressively refines representations from raw colonoscopy image-text records. The Cleansing stage removes uninformative background frames using a prevalence-guided contrastive filtering approach. The Attunement stage employs LLM-extracted morphological attributes as soft targets to improve semantic alignment in single-polyp cases. The Unification stage resolves multi-lesion ambiguities through patient-level cross-attention mechanisms. The framework uses ViT-B/32 vision encoder and Transformer text encoder, both initialized from CLIP, and is trained on 13k colonoscopy cases (874k images) before evaluation on the EndoReport50 dataset for downstream polyp detection and malignancy classification tasks.

## Key Results
- Achieved 92.13% AUROC for malignancy classification in zero-shot setting
- Outperformed CLIP baseline by 13.91% AUROC on malignancy classification
- Demonstrated state-of-the-art performance on polyp detection and malignancy classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Prevalence-Guided Noise Filtration
- **Claim:** Filtering non-informative background frames via a two-round contrastive process improves the signal-to-noise ratio for subsequent representation learning.
- **Mechanism:** Raw colonoscopy data contains high fraction of non-informative frames. The model first trains a weak detector using standardized text ("This is a colon with polyps"), then uses this detector to rank and retain frames based on known polyp prevalence.
- **Core assumption:** The text encoder can sufficiently distinguish diagnostic sentences, and visual features of polyps are distinct enough from normal mucosa to allow prevalence-based thresholding without discarding difficult positives.
- **Evidence anchors:** Abstract states "...removing non-informative frames using diagnostic text and CLIP-based filtering..." Section 2.2 describes "...we filter polyp-bearing images using a prevalence-guided selection strategy... retaining the top percentage of highest-probability frames..."
- **Break condition:** If initial text-to-image alignment is too noisy, the "cleansed" dataset may still contain false positives or discard rare polyp types.

### Mechanism 2: Morphology-Aware Soft Alignment
- **Claim:** Replacing rigid one-hot labels with soft targets derived from clinical attributes reduces semantic false negatives during single-polyp alignment.
- **Mechanism:** Standard CLIP enforces one-to-one match between image and paired text, which fails when different polyps share similar visual features. By extracting morphological attributes via LLMs and computing similarity matrices between attributes, the loss function guides vision encoder to learn feature proximity based on clinical semantics rather than arbitrary instance pairs.
- **Core assumption:** The LLM extracts attributes accurately from reports, and these text-based attributes correlate strongly with visual features learned by encoder.
- **Evidence anchors:** Abstract states "...leveraging large language models to extract clinical attributes for fine-grained contrastive learning..." Section 2.3 explains "...replace the one-hot labels... with the soft targets M(v→t)... reducing false negatives and achieving finer medical semantic alignment."
- **Break condition:** If visual features of distinct morphological types overlap significantly in latent space, soft labels may confuse the encoder rather than refine it.

### Mechanism 3: Cross-Attention for Multi-Lesion Unification
- **Claim:** Patient-level cross-attention resolves ambiguous image-text associations in multi-polyp cases better than independent instance matching.
- **Mechanism:** In cases with multiple polyps, a single text description might correspond to one of several images. This mechanism allows every image embedding to attend to all text embeddings within a patient record, aggregating features into a unified patient-level representation.
- **Core assumption:** The relevant visual and textual features are present within the patient's full record, and averaging attended features preserves discriminative information for malignancy classification.
- **Evidence anchors:** Abstract states "...resolving multi-polyp ambiguities through patient-level cross-attention." Section 2.4 notes "...mitigate matching ambiguity in multi-lesion scenarios while maintaining the fine-grained semantic alignment..."
- **Break condition:** If a patient record contains conflicting information (e.g., benign and malignant polyps), averaging their features via cross-attention could dilute the specific signal required for malignancy detection.

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - **Why needed here:** Endo-CLIP is an adaptation of CLIP. You must understand how InfoNCE loss pulls positive pairs together and pushes negative pairs apart in joint embedding space to grasp modifications in all three stages.
  - **Quick check question:** How does the loss function change when moving from Cleansing stage (one-hot) to Attunement stage (soft targets)?

- **Concept: Attention Mechanisms (Query/Key/Value)**
  - **Why needed here:** The Unification stage relies on cross-attention to mix image and text features. Understanding how WQ, WK, and WV matrices project embeddings to calculate relevance scores is critical for debugging multi-polyp integration.
  - **Quick check question:** In Unification stage, does the text attend to the image, the image attend to the text, or both?

- **Concept: Large Language Model (LLM) Structured Extraction**
  - **Why needed here:** The pipeline assumes use of an LLM to parse unstructured clinical reports into structured "morphological attributes." Understanding prompt engineering or extraction capabilities helps assess reliability of Attunement stage inputs.
  - **Quick check question:** What happens to Attunement stage if LLM fails to extract a specific attribute (e.g., "surface texture") from medical report?

## Architecture Onboarding

- **Component map:** ViT-B/32 (Vision) + Transformer (Text) encoders → Stage 1 (Cleansing: CLIP contrastive + Prevalence Thresholding) → Stage 2 (Attunement: LLM Attribute Extractor + Soft Label Generator) → Stage 3 (Unification: Cross-Attention Module + Patient-level Aggregation)

- **Critical path:**
  1. Data Prep: Run LLM extraction on reports → Morphological Attributes
  2. Stage 1: Train initial encoders → Filter dataset (Remove low-prob frames)
  3. Stage 2: Re-train encoders on filtered data using Soft Labels (Attribute similarity)
  4. Stage 3: Fine-tune encoders + Cross-Attention module on Multi-Polyp cases using Unified Labels

- **Design tradeoffs:**
  - Hard vs. Soft Filtering: Stage 1 discards data. If prevalence estimate is wrong, usable data is lost forever.
  - One-hot vs. Soft Loss: Soft targets in Stage 2 are robust to semantic overlap but may reduce "push" force against dissimilar negatives compared to standard CLIP.
  - Batch Size Constraints: Stage 3 requires patient-level batching. Paper notes batch size drop (32 → 8), implying higher memory overhead or slower convergence for Unification stage.

- **Failure signatures:**
  - High False Positive Detection: Likely Stage 1 failure (Cleansing threshold too low)
  - Poor Generalization on Rare Polyps: Likely Stage 2 failure (Attribute extraction failed or soft labels washed out rare features)
  - Confusion in Multi-Polyp Malignancy: Likely Stage 3 failure (Cross-attention averaging conflicting signals)

- **First 3 experiments:**
  1. Validation of Cleansing: Visualize "probability of containing a polyp" scores for sample batch. Verify Stage 1 successfully separates clear mucosa from polyp-bearing frames before thresholding.
  2. Ablation on Soft Labels: Train Stage 2 with one-hot labels vs. soft labels. Plot t-SNE to see if soft labels result in tighter clustering of similar morphological types.
  3. Attention Visualization: In Stage 3, visualize cross-attention maps. Check if specific polyp image correctly attends to corresponding text description in multi-polyp record.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does Endo-CLIP generalize to other endoscopic tasks and anatomical regions beyond polyp detection and malignancy classification?
- **Basis in paper:** [inferred] The evaluation is limited to two specific tasks (polyp detection and malignancy classification) on colonoscopy data, while the introduction mentions "diverse clinical settings" as a generalization challenge.
- **Why unresolved:** No experiments were conducted on other endoscopic applications (e.g., inflammatory bowel disease assessment, vascular lesion identification) or other gastrointestinal regions.
- **What evidence would resolve it:** Evaluation performance on additional endoscopic tasks and datasets from different anatomical regions.

### Open Question 2
- **Question:** How sensitive is Endo-CLIP's performance to the choice and quality of the LLM used for morphological attribute extraction?
- **Basis in paper:** [inferred] The Attunement Stage relies on LLMs to extract morphological attributes following clinical guidelines, but no ablation study examines different LLM choices or extraction quality.
- **Why unresolved:** The paper does not specify which LLM is used or evaluate robustness to LLM selection and potential extraction errors.
- **What evidence would resolve it:** Ablation experiments using different LLMs and analysis of performance degradation under noisy or incomplete attribute extraction.

### Open Question 3
- **Question:** Can the framework effectively leverage temporal information from full colonoscopy videos rather than static image collections?
- **Basis in paper:** [inferred] The method processes individual frames from colonoscopy records without explicit modeling of temporal relationships, despite colonoscopy inherently being a video-based procedure.
- **Why unresolved:** The cross-attention mechanism operates on image collections at the patient level but does not incorporate temporal sequence information.
- **What evidence would resolve it:** Extension of the framework to video-level processing and comparison of temporal vs. non-temporal approaches on downstream tasks.

## Limitations

- **Unknown LLM Extraction Pipeline:** The paper relies on LLM-based extraction of 9 morphological attributes from diagnostic reports, but does not specify the model identity, prompt templates, or validation of extraction accuracy.
- **Prevalence Threshold Sensitivity:** Stage 1's effectiveness depends on setting correct prevalence threshold for frame filtering, but the paper does not justify the exact threshold used or explore sensitivity to this choice.
- **Limited Multi-Class Evaluation:** The method is tested primarily on binary polyp vs. normal and benign vs. malignant tasks, with no exploration of fine-grained polyp type classification or other endoscopic abnormalities.

## Confidence

- **Stage 1 Cleansing Effectiveness:** Medium - The prevalence-guided filtering concept is sound, but exact threshold setting and robustness to different polyp prevalence rates are unclear.
- **Stage 2 Soft Label Superiority:** Low - While the mechanism is plausible, there is no ablation comparing soft labels to hard labels on the same dataset, and no validation of LLM attribute extraction accuracy.
- **Stage 3 Cross-Attention Benefit:** Low - The method is described, but there is no comparison to simpler baselines or analysis of attention map quality.
- **State-of-the-Art Downstream Performance:** Medium - Results are reported, but lack of extensive ablation and reliance on a single dataset reduce confidence in claimed superiority.

## Next Checks

1. **LLM Extraction Accuracy Validation:** Manually evaluate the LLM's morphological attribute extraction on 50 randomly sampled diagnostic reports. Calculate attribute-wise precision/recall and assess if extraction errors correlate with downstream performance drops.

2. **Cleansing Stage Ablation:** Run Stage 1 with multiple prevalence thresholds (e.g., 30%, 45%, 60%). Compare downstream AUROC on malignancy classification for each setting to identify sensitivity and optimal threshold.

3. **Cross-Attention Visualization and Ablation:** For multi-polyp cases, visualize cross-attention maps to verify that the correct polyp image attends to its corresponding text. Additionally, compare performance of the full Unification stage to a baseline that uses independent instance matching without cross-attention.