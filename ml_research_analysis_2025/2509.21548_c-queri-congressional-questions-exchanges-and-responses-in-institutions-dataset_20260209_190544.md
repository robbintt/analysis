---
ver: rpa2
title: 'C-QUERI: Congressional Questions, Exchanges, and Responses in Institutions
  Dataset'
arxiv_id: '2509.21548'
source_url: https://arxiv.org/abs/2509.21548
tags:
- question
- utterances
- party
- questions
- congressional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large-scale dataset of question-answer
  pairs from U.S. congressional hearings spanning 16,130 transcripts and 3.3M utterances.
---

# C-QUERI: Congressional Questions, Exchanges, and Responses in Institutions Dataset

## Quick Facts
- arXiv ID: 2509.21548
- Source URL: https://arxiv.org/abs/2509.21548
- Reference count: 40
- Primary result: Party affiliation predictable from questions alone with 59% accuracy (8% above baseline)

## Executive Summary
This paper introduces C-QUERI, a large-scale dataset of question-answer pairs from U.S. congressional hearings spanning 16,130 transcripts and 3.3M utterances. The authors develop methods to identify utterances and classify them as questions or answers using BERT-based models fine-tuned on Reddit and U.K. Parliamentary data. Their main finding is that party affiliation can be predicted from questions alone with 59% accuracy, with higher accuracy in high-salience committees like Oversight where conflict is greatest. Zero-shot prompting with LLMs fails to exceed baseline, indicating partisan language patterns are subtle and require supervised fine-tuning.

## Method Summary
The authors use BERT-base-cased with dropout and linear layers to classify utterances as questions or answers, trained on Reddit AMA and U.K. Parliament data then fine-tuned on congressional transcripts. For party prediction, they train a second BERT model on question utterances with speaker names removed, using 20/5/75 train/val/test splits and 5-fold cross-validation. They also apply Random Forest classifiers on interpretable linguistic features extracted using the NELA toolkit. The pipeline involves preprocessing transcripts to extract utterances, segmenting by speaker using hybrid NER, classifying Q/A pairs, and predicting party affiliation from question text alone.

## Key Results
- Party affiliation prediction accuracy of 59% (8% above 51% baseline)
- Q/A classification accuracy of 87.14% on congressional data
- Highest accuracy (72.48%) in Oversight committee, lowest in Armed Services
- Zero-shot LLM prompting fails to exceed baseline performance

## Why This Works (Mechanism)

### Mechanism 1
Partisan language patterns in congressional questions are detectable but subtle, requiring supervised fine-tuning rather than zero-shot inference. BERT encodes semantic and stylistic features of questions; fine-tuning on congressional data learns to map these latent representations to party labels. The patterns emerge from strategic questioning—different parties prioritize distinct rhetorical strategies (Republicans: assertive/hedge words; Democrats: positive sentiment/implicatives). Core assumption: Partisan differences manifest consistently enough in question language to be learned as separable representations in embedding space.

### Mechanism 2
Transfer learning from structured Q&A domains (Reddit AMA, UK Parliament) enables utterance classification for congressional transcripts despite domain shift. BERT pre-training captures general dialogue structure; fine-tuning on Reddit AMA (60,756 Q/A pairs) and UK Parliament data (2,344 Q/A pairs) teaches the model to recognize question-posing vs. answer-giving patterns, which transfer to the formal congressional context. Core assumption: The linguistic markers distinguishing questions from answers are sufficiently consistent across informal, semi-formal, and formal settings.

### Mechanism 3
Partisan signal strength varies systematically with institutional context—strongest in high-salience, conflict-prone committees and during divided government. Committees central to party reputations (Oversight, Ways and Means) generate more adversarial questioning; divided government amplifies majority/minority distinction. These conditions increase linguistic distinctiveness, making party affiliation more predictable. Core assumption: Members consciously or unconsciously adjust questioning style based on institutional role and political stakes.

## Foundational Learning

- Concept: **Transfer learning with BERT for text classification**
  - Why needed here: The paper uses BERT fine-tuned on out-of-domain data (Reddit, UK Parliament) then applied to congressional transcripts. Understanding domain shift and fine-tuning mechanics is essential.
  - Quick check question: Can you explain why freezing all BERT layers vs. fine-tuning the top layers would affect performance on a specialized domain like congressional hearings?

- Concept: **Baseline comparison in classification tasks**
  - Why needed here: The 59% party prediction accuracy only matters relative to the 51% majority-class baseline. Without this comparison, the result is uninterpretable.
  - Quick check question: If a dataset has 80% Democrats and 20% Republicans, what baseline accuracy would a naive classifier achieve?

- Concept: **Kolmogorov-Smirnov test for distribution comparison**
  - Why needed here: The paper uses K-S tests to validate that linguistic features differ significantly between parties (Appendix D). This statistical foundation justifies the feature-level claims.
  - Quick check question: What does a significant K-S test result (p < 0.05) tell you about two distributions that a simple mean comparison does not?

## Architecture Onboarding

- Component map: Transcript → utterance segmentation → Q/A classification → party prediction
- Critical path: Errors compound; poor segmentation corrupts Q/A labels, which corrupts downstream tasks
- Design tradeoffs:
  - Speaker name removal: Prevents trivial leakage (92% accuracy with names vs. 59% without) but may discard contextual cues
  - Transfer source selection: Reddit provides scale (60K pairs) but informal; UK Parliament provides formality but limited scale (2.3K pairs)
  - Classical vs. deep features: NELA features are interpretable but weaker (53.75% accuracy); BERT captures latent patterns but is opaque
- Failure signatures:
  - Clubbed utterances: Multiple speakers merged into one utterance
  - Committee metadata errors: Treaties mislabeled as "General" hearings
  - Zero-shot collapse: LLMs at or below baseline indicate prompt engineering issues
- First 3 experiments:
  1. Reproduce utterance segmentation on 10 transcripts from different committees/sessions
  2. Ablate transfer sources: Train Q/A classifier on Reddit-only, UK-only, and combined data
  3. Stratified party prediction: Test on Oversight vs. Armed Services committees directly

## Open Questions the Paper Calls Out

- Do the linguistic patterns of partisan questioning in U.S. congressional hearings generalize to other democratic institutions with different procedural rules?
- Do partisan linguistic patterns in questioning predict downstream substantive outcomes such as policy changes or media coverage?
- What specific linguistic features or rhetorical strategies drive BERT's superior performance over classical feature-based models in detecting partisan language?
- Why do zero-shot LLMs fail to detect partisan language patterns that fine-tuned BERT captures?

## Limitations

- Transfer learning approach relies on Reddit and UK Parliament data that differ substantially from congressional discourse
- Utterance segmentation accuracy varies significantly across transcript formats with some sessions producing frequent speaker confusion
- 59% party prediction accuracy, while above baseline, remains far from perfect, suggesting substantial overlap in partisan language patterns
- Political context changes across sessions and committees may limit temporal generalizability

## Confidence

**High Confidence**: Q/A classification task architecture and baseline performance (87.14% accuracy). Utterance segmentation methodology description. The failure of zero-shot prompting to exceed baseline.

**Medium Confidence**: Party affiliation prediction results (59% accuracy). The claim that partisan patterns vary by committee type and government control. Feature-level linguistic differences between parties.

**Low Confidence**: The assertion that questions alone contain sufficient partisan signal without context. Generalizability of results across different congressional sessions and political climates. The precise mechanisms driving partisan language differences.

## Next Checks

1. Train and test party classifiers on temporally disjoint sessions (e.g., 108th-110th vs. 111th-113th) to assess stability of partisan language patterns over time.

2. Systematically compare party prediction accuracy across all committees with sufficient samples, not just high-salience ones, to map the full spectrum of partisan signal strength.

3. Experiment with structured prompting templates, few-shot examples, and chain-of-thought reasoning for zero-shot LLMs to determine if the failure to exceed baseline is due to prompt inadequacy rather than model limitations.