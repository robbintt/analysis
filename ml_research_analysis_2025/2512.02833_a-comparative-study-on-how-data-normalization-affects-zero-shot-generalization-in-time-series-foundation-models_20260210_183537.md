---
ver: rpa2
title: A Comparative Study on How Data Normalization Affects Zero-Shot Generalization
  in Time Series Foundation Models
arxiv_id: '2512.02833'
source_url: https://arxiv.org/abs/2512.02833
tags:
- normalization
- time
- revin
- methods
- inst
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first systematic evaluation of data normalization
  methods in Time Series Foundation Models (TSFMs), addressing the critical challenge
  of zero-shot generalization across heterogeneous time series domains. The authors
  benchmark six normalization approaches across four architecturally diverse TSFMs
  (MOIRAI, CHRONOS, GTT, LightGTS) using multi-domain datasets spanning energy, transportation,
  and meteorology.
---

# A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models

## Quick Facts
- arXiv ID: 2512.02833
- Source URL: https://arxiv.org/abs/2512.02833
- Reference count: 0
- One-line primary result: REVIN reduces zero-shot MASE by 89% vs. unnormalized baseline and 44% vs. other normalization methods while matching best in-domain accuracy without dataset-level preprocessing

## Executive Summary
This paper systematically evaluates six data normalization methods across four architecturally diverse Time Series Foundation Models (TSFMs) for zero-shot generalization across heterogeneous domains. The study benchmarks MOIRAI, CHRONOS, GTT, and LightGTS models using multi-domain datasets spanning energy, transportation, and meteorology. The key finding is that mean/std-based normalization methods consistently outperform alternatives, with REVIN achieving the highest accuracy-efficiency trade-off by computing window-level statistics online rather than requiring full-corpus preprocessing.

## Method Summary
The study evaluates seven normalization schemes (RevIN, MeanAbs, Hybrid, Standardization, MinMax, MaxAbs, Raw) across four TSFM architectures (MOIRAI-small, CHRONOS-tiny, GTT-tiny, LightGTS-tiny). Models are pretrained for approximately 100k steps per run on NVIDIA RTX A5000 using six datasets with specified train/test splits. For scale-sensitive models (GTT/LightGTS), clipped instance normalization is applied with |X_inst| ≤ 10. At inference, dataset-level methods use window-level statistics, while Hybrid retains only the RevIN component. Performance is measured using Mean Absolute Scaled Error (MASE) with mean ± std reported over three leave-one-dataset-out runs.

## Key Results
- REVIN reduces zero-shot MASE by 89% relative to unnormalized baseline
- REVIN achieves 44% reduction in zero-shot MASE versus other normalization methods
- REVIN matches best in-domain accuracy (0.84 MASE) without requiring dataset-level preprocessing
- Mean/std-based normalization methods outperform alternatives by at most ±0.03 MASE in ZS and ±0.02 MASE in ID
- For scale-sensitive models, clipped instance normalization reduces MASE by 16.5%-36.4%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mean/std-based normalization methods reduce cross-domain generalization error by aligning heterogeneous input scales.
- Mechanism: Time series data exhibits significant scale variation across domains and channels. Mean/std-based normalization (β = μ, γ = σ) standardizes input distributions, preventing high-magnitude channels from dominating gradient updates during multi-domain pretraining.
- Core assumption: The pretraining corpus contains sufficient diversity that dataset-level statistics would conflate incompatible scales.
- Evidence anchors:
  - [abstract] "Time series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity."
  - [section 5.2] "The three mean/std-based methods—RevIN, Hybrid, and Standardization—differ by at most ±0.03 MASE in ZS and ±0.02 MASE in ID on average, this can suggest that, for data normalization in TSFMs, the choice of normalization statistics (mean–std vs. alternatives) has a greater impact on performance than the scope at which the statistics are computed."
  - [corpus] Weak direct corpus support; neighboring papers focus on adaptation strategies rather than normalization mechanisms.

### Mechanism 2
- Claim: Instance-level normalization (RevIN) enables zero-shot inference without requiring dataset-level preprocessing statistics.
- Mechanism: RevIN computes shift (β_inst) and scale (γ_inst) from the local context window X_{t-L:t} at inference time, allowing the model to normalize unseen data on-the-fly. This eliminates the dependency on pre-computed dataset statistics unavailable during zero-shot deployment.
- Core assumption: The context window length L provides sufficient samples to estimate reliable local statistics.
- Evidence anchors:
  - [abstract] "REVIN achieves the highest accuracy-efficiency trade-off by computing window-level statistics online rather than requiring a full-corpus preprocessing pass."
  - [section 4.3] "When evaluating foundation models in a real-world setting, we assume that the input instance may come from an unseen dataset or domain, so normalization statistics can only be derived from the input context X_{t-L:t}."
  - [corpus] Not directly addressed in corpus papers.

### Mechanism 3
- Claim: Scale-sensitive loss functions (MSE/MAE) require adapted normalization strategies to prevent gradient bias toward high-magnitude channels.
- Mechanism: For point-forecast models using MSE/MAE, gradients scale with output magnitude. Without normalization in the loss computation space, high-magnitude channels dominate parameter updates. Computing loss in normalized space (after instance clipping) mitigates this bias.
- Core assumption: The clipping threshold (|X_inst| ≤ 10) appropriately filters outliers without discarding valid signal.
- Evidence anchors:
  - [section 4.1] "The MSE loss L = 1/C Σ(x_c - x̂_c)² and MAE loss L = 1/C Σ|x_c - x̂_c| biases gradients ∇_θ L toward high-magnitude channels, necessitating strict shift/scale preservation across channels to prevent bias."
  - [section 5.3, Table 4] "The ablation shows that mitigating scale sensitivity substantially improves forecasting performance: LightGTS 16.5% reduction, GTT 36.4% reduction."
  - [corpus] Not directly addressed in corpus papers.

## Foundational Learning

- Concept: **Scale sensitivity of loss functions**
  - Why needed here: Understanding whether your model's training loss is scale-invariant (NLL, token CE) or scale-sensitive (MSE, MAE) determines whether you need adapted normalization in the loss computation space.
  - Quick check question: Does your loss function compute error on raw values (sensitive) or on probability distributions/discrete tokens (invariant)?

- Concept: **Instance-level vs. dataset-level normalization scope**
  - Why needed here: Dataset-level methods require pre-computed corpus statistics unavailable during zero-shot inference; instance-level methods compute statistics from the input window alone.
  - Quick check question: Can your deployment scenario guarantee access to pre-computed normalization statistics for all potential input domains?

- Concept: **Non-stationarity in time series**
  - Why needed here: Time series often exhibit distribution shifts within a single series; instance-level normalization captures local dynamics that dataset-level methods miss.
  - Quick check question: Do your time series exhibit trend, seasonality, or regime changes within individual sequences that shift the mean/variance over time?

## Architecture Onboarding

- Component map:
  - **Probabilistic models (MOIRAI)**: Native RevIN with de-normalization before loss; NLL is scale-invariant via gradient decomposition
  - **LLM-based models (CHRONOS)**: Tokenization inherently decouples scale; normalization method choice has minimal impact once data is scaled (±0.07 MASE variation in ZS)
  - **Point-forecast models (GTT, LightGTS)**: Require clipped instance normalization with loss computed in normalized space

- Critical path:
  1. Identify model type and training loss scale sensitivity
  2. If scale-sensitive (MSE/MAE): implement clipped instance normalization per Eq. 3, compute loss in normalized space
  3. If scale-invariant (NLL, token CE): use native RevIN pipeline with de-normalization before loss
  4. At inference: always derive statistics from context window only

- Design tradeoffs:
  - RevIN vs. Standardization: RevIN avoids full-corpus preprocessing (significant for large TSFMs) while matching accuracy; Standardization may be preferable if dataset-level statistics are already available
  - Clipping threshold: Lower values (e.g., |X| ≤ 10) reduce outlier impact but may clip valid high-variance signals; tune per domain characteristics
  - Assumption: Clipping at 10 is a heuristic that may need domain-specific adjustment

- Failure signatures:
  - High variance in ZS performance across domains → likely mismatched normalization scope (using dataset-level stats at inference)
  - Dominance of high-magnitude channels in attention/feature importance → scale-sensitive loss without proper normalization
  - Exploding predictions when context scale is small (γ_inst → 0) → missing clipping safeguard
  - Performance parity across normalization methods → model is LLM-based with tokenization; normalization choice less critical

- First 3 experiments:
  1. **Baseline comparison**: Train your TSFM with RevIN vs. raw input on a held-out domain; expect ~89% MASE reduction per paper findings
  2. **Loss sensitivity ablation**: If using MSE/MAE, compare native RevIN (de-normalize before loss) vs. clipped instance normalization (loss in normalized space); expect 16-36% improvement
  3. **Context window sensitivity**: Test RevIN with varying context lengths (L = 24, 48, 96, 192) on a non-stationary dataset; identify minimum L for stable local statistics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the normalization findings for forecasting generalize to other time series analysis tasks such as classification, imputation, and anomaly detection?
- Basis in paper: [explicit] The authors state: "While this study focuses on time series forecasting, the insights obtained may extend to other time series analysis tasks, such as classification, imputation, and anomaly detection, which we leave for future work."
- Why unresolved: The study only evaluated forecasting; different tasks have distinct loss functions, output types, and evaluation criteria that may interact differently with normalization methods.
- What evidence would resolve it: Systematic evaluation of REVIN and other normalization methods across TSFMs designed for classification, imputation, and anomaly detection tasks.

### Open Question 2
- Question: How can dataset-dependent normalization methods (SAN, FAN, SIN) be adapted for TSFMs when their parameters depend on data granularity and sampling rates?
- Basis in paper: [explicit] The authors exclude SAN/FAN/SIN because "parameters like FAN's K or SAN's T cannot be determined for zero-shot forecasting on previously unseen datasets/domains" and "estimating reliable, dataset-dependent parameters during multi-dataset pretraining is infeasible without novel mechanisms beyond our empirical scope."
- Why unresolved: These methods were designed for dataset-specific models and require dataset-level properties that are unavailable or inconsistent across the heterogeneous corpora used for TSFM pretraining.
- What evidence would resolve it: Novel mechanisms that enable adaptive, data-dependent normalization parameter estimation during both multi-dataset pretraining and zero-shot inference on unseen domains.

### Open Question 3
- Question: What are the optimal adaptation strategies for scale-sensitive losses (MAE/MSE) beyond the clipping approach evaluated, and how do they interact with different model architectures?
- Basis in paper: [inferred] While the ablation shows clipping improves performance (16.5%–36.4% MASE reduction), the paper notes that scale-sensitive losses "may require targeted adaptations" but does not explore alternative adaptation strategies or their architectural dependencies.
- Why unresolved: The study evaluates only one adaptation method (clipping at |X_inst| ≤ 10); other strategies such as loss re-weighting, gradient normalization, or adaptive scaling factors remain unexplored.
- What evidence would resolve it: Comparative ablation of multiple adaptation strategies for scale-sensitive objectives across diverse TSFM architectures with varying capacities and training protocols.

## Limitations

- The findings are limited to small "tiny" model variants and may not scale to larger TSFM architectures
- The clipping threshold of 10 is presented as a heuristic without systematic sensitivity analysis across diverse domain characteristics
- The study excludes dataset-dependent normalization methods (SAN, FAN, SIN) that may be valuable for specific domains with known characteristics

## Confidence

- **High confidence**: The core mechanism that mean/std-based normalization outperforms alternatives (reducing ZS MASE by 89% vs. raw input) is well-supported by direct experimental evidence across multiple architectures and domains.
- **Medium confidence**: The claim that RevIN achieves the "highest accuracy-efficiency trade-off" is supported within the experimental scope but limited by the small model variants tested.
- **Low confidence**: The assertion that normalization method choice has "greater impact on performance than the scope at which statistics are computed" needs additional empirical support across more diverse normalization strategies.

## Next Checks

1. **Scaling validation**: Evaluate RevIN across larger TSFM variants (small, base, large) to confirm that the 89% ZS MASE reduction and 44% improvement over alternatives persist at scale.

2. **Threshold sensitivity**: Systematically test the clipping threshold (currently fixed at 10) across domains with varying signal-to-noise ratios to identify optimal clipping strategies for different data characteristics.

3. **Cross-domain robustness**: Validate the zero-shot generalization claims on additional domains not represented in the current six datasets, particularly domains with extreme scale variation or non-stationary behavior beyond the tested energy/transportation/meteorology spectrum.