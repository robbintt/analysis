---
ver: rpa2
title: How human is the machine? Evidence from 66,000 Conversations with Large Language
  Models
arxiv_id: '2510.07321'
source_url: https://arxiv.org/abs/2510.07321
tags:
- human
- llms
- studies
- humans
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study tested whether Large Language Models (LLMs) replicate
  human decision-making biases. Ten experiments with 66,000 LLM conversations examined
  six well-known biases (availability, representativeness, endowment, anchoring, transaction
  utility, framing).
---

# How human is the machine? Evidence from 66,000 Conversations with Large Language Models

## Quick Facts
- **arXiv ID**: 2510.07321
- **Source URL**: https://arxiv.org/abs/2510.07321
- **Reference count**: 18
- **Primary result**: Large Language Models fail to replicate human cognitive biases in both direction and magnitude across 66,000 conversations testing six well-known biases

## Executive Summary
This study tested whether Large Language Models (LLMs) replicate human decision-making biases by conducting ten experiments with 66,000 LLM conversations across six well-known biases. The research found that LLMs consistently failed to produce responses matching established human biases in both direction and magnitude. Instead, four types of deviations emerged: bias attenuation (LLMs reduced or eliminated biases), bias amplification (LLMs exaggerated biases), reverse biases (LLMs showed opposite biases to humans), and response inconsistency (within models, across models, and across research studies). These findings challenge the assumption that LLMs can accurately replace humans in predicting or mimicking consumer behavior.

## Method Summary
The study used pre-registered experiments with 66,000 prompts derived from classic judgment and decision-making tasks, employing OpenAI API access to frozen model versions (not chat interface). The methodology involved a two-prompt sequence with role assignment ("assume role of research participant") followed by specific bias scenarios, using temperature=1 to balance determinism and creativity. Data collection achieved 1,000 independent observations per condition within 2-5 hours per condition. The researchers compared LLM effect sizes against human meta-analytic baselines using confidence interval overlap, classifying deviations into four categories: attenuation, amplification, reversal, and inconsistency.

## Key Results
- LLMs showed systematic deviations from human biases across all six tested biases (availability, representativeness, endowment, anchoring, transaction utility, framing)
- Bias attenuation was the most frequent deviation, observed in each of the 10 studies in at least one condition
- GPT-4 reversed the endowment effect 75% of the time, whereas humans show this reversal in less than 4% of cases
- Significant response inconsistency was observed both within the same model over time and across different research studies using similar methodologies

## Why This Works (Mechanism)

### Mechanism 1: Bias Attenuation via Optimization for Correctness
LLMs frequently reduce or eliminate human cognitive biases when tasks permit objectively correct answers. The training objective and data exposure to correct reasoning examples outweigh any exposure to human-like error patterns in the training corpus. LLMs lack human cognitive constraints such as fatigue, limited working memory, and time pressure that give rise to heuristics.

### Mechanism 2: Bias Amplification via Hyper-Accuracy Distortion
LLMs can exaggerate human biases beyond human-typical magnitudes through over-representation of biased patterns in training data. Training corpora contain abundant examples of human biases, and during generation, LLMs may engage in "hyper-accuracy distortion"—producing outputs that are statistically faithful to training patterns but exaggerated because the model lacks the contextual modulation humans apply.

### Mechanism 3: Bias Reversal via Over-Correction and Prompt-Training Mismatch
LLMs sometimes produce systematically opposite biases to humans through over-correction or differential weighting of prompt context versus training priors. When LLMs detect a task that might involve bias, safety/accuracy training may trigger compensatory behavior that overshoots. More advanced models may weight training data priors over immediate prompt instructions, producing responses that reflect "learned corrections" rather than prompt-requested human simulation.

## Foundational Learning

- **Concept: Heuristics and biases framework (Kahneman & Tversky)**
  - Why needed here: The entire experimental design depends on understanding the six canonical biases tested (availability, representativeness, endowment, anchoring, transaction utility, framing). Without this foundation, the deviation classification (attenuation/amplification/reversal) is incomprehensible.
  - Quick check question: When humans estimate the probability of a plane crash after seeing news coverage of one crash, which heuristic are they using and in which direction does it bias their estimate?

- **Concept: Effect size comparison methodology**
  - Why needed here: The paper's classification system depends on comparing LLM effect sizes to human meta-analytic baselines using confidence intervals. Understanding how to interpret whether an effect falls within, below, or above human ranges is essential for evaluating the claims.
  - Quick check question: If a meta-analysis reports a human framing effect of d=0.30 with 95% CI [0.20, 0.40], and an LLM produces d=0.15, how would you classify this deviation?

- **Concept: Temperature parameter in LLM sampling**
  - Why needed here: The methodology explicitly states using temperature=1 (default) to balance determinism and creativity. Understanding how temperature affects response variance is critical for interpreting inconsistency findings.
  - Quick check question: If temperature were set to 0 instead of 1, how would you expect response consistency across identical prompts to change?

## Architecture Onboarding

- **Component map**: Prompt construction layer -> Model access layer (API-based frozen versions) -> Experimental design layer (2×2 factorial) -> Comparison framework (meta-analytic baselines)
- **Critical path**: 1. Pre-register experimental design and prompts, 2. Construct prompts with role assignment (or manipulate presence), 3. Send prompts via API with controlled temperature (default=1), 4. Collect 1,000 independent observations per condition within 2-5 hours, 5. Apply statistical tests per pre-registration, 6. Compare effect sizes against human baselines using CI overlap or original study values, 7. Classify deviations as attenuation/amplification/reversal based on direction and magnitude
- **Design tradeoffs**: API vs. chat interface (API enables version control but may produce different responses), role assignment (provides conservative test but may not reflect typical usage), temperature=1 (balances determinism and creativity but introduces variance)
- **Failure signatures**: Within-model inconsistency (same frozen model producing significantly different responses to identical prompts after time delay), cross-model inconsistency (significant model × prompt variation interactions in 8 of 9 studies with prompt variations), cross-research inconsistency (different research teams using similar methodologies producing conflicting results)
- **First 3 experiments**: 1. Replicate Study 3 (endowment effect) with a single model across two time points to verify within-model consistency claims, 2. Test whether explicit calibration instructions ("respond as a typical human would, including their biases") reduces attenuation/reversal compared to standard role assignment prompts, 3. Run a temperature sensitivity analysis (0, 0.5, 1.0, 1.5) on a single bias task to determine how much inconsistency is attributable to sampling variance versus underlying model behavior

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific mechanisms drive LLMs to reverse human biases (e.g., the endowment effect) rather than simply mimicking or attenuating them? The authors note that reverse biases are "hard to reconcile" with existing accounts and state, "The reasons behind this lack of human-likeness are puzzling, and we leave it to future research."

- **Open Question 2**: Do these deviations from human-likeness generalize to LLMs outside the GPT family? The study primarily focused on GPT-3.5 and GPT-4; different training corpora or architectures (e.g., open-source models) may yield different bias patterns.

- **Open Question 3**: To what extent do API-based interactions differ from consumer-facing Chat versions regarding bias consistency? The study used the static API for control, while the Chat version updates dynamically and may handle context differently.

## Limitations

- API-based LLM access means exact model snapshots cannot be guaranteed across replications, introducing potential version drift
- The temperature=1 setting, while enabling creative responses, also introduces significant variance that complicates interpretation of "inconsistency" findings
- The role-prompting methodology may not reflect typical LLM usage patterns, potentially creating artificial constraints on natural model behavior

## Confidence

- **High Confidence**: The general finding that LLMs deviate from human biases in systematic ways is well-supported by the large sample size (66,000 observations) and pre-registered methodology
- **Medium Confidence**: The specific mechanisms proposed for each deviation type are plausible but require further empirical validation
- **Low Confidence**: The cross-research inconsistency findings cannot be fully explained by the paper's methodology due to differences in prompt design, task framing, and sample characteristics across studies

## Next Checks

1. **Version Control Validation**: Replicate a single bias task (e.g., Study 3's endowment effect) using the exact same API endpoints and timestamps as the original study to determine whether observed inconsistencies persist across model versions

2. **Prompt Design Sensitivity**: Systematically vary the role-assignment instructions while holding all other variables constant to isolate the impact of this manipulation on bias replication accuracy

3. **Temperature Sensitivity Analysis**: Run a comprehensive analysis across multiple temperature settings (0.0, 0.5, 1.0, 1.5) on a representative bias task to quantify the contribution of sampling variance versus model-level behavioral differences to the inconsistency findings