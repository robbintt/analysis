---
ver: rpa2
title: Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence
  Models
arxiv_id: '2508.07866'
source_url: https://arxiv.org/abs/2508.07866
tags:
- examples
- language
- cross-lingual
- sentiment
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the impact of incorporating few-shot target
  language examples into training for cross-lingual aspect-based sentiment analysis
  (ABSA). Across four ABSA tasks, six target languages, and two sequence-to-sequence
  models, adding as few as ten target language examples significantly improves performance
  over zero-shot settings and achieves comparable results to constrained decoding
  in reducing prediction errors.
---

# Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models

## Quick Facts
- arXiv ID: 2508.07866
- Source URL: https://arxiv.org/abs/2508.07866
- Reference count: 33
- Few-shot target language examples significantly improve cross-lingual ABSA performance over zero-shot settings and achieve comparable results to constrained decoding

## Executive Summary
This paper investigates the impact of incorporating minimal target language supervision (few-shot learning) into cross-lingual aspect-based sentiment analysis (ABSA). The authors evaluate two multilingual sequence-to-sequence models (mT5 and mBART) across four ABSA tasks and six target languages, demonstrating that adding as few as ten target language examples to English training data substantially improves performance over zero-shot approaches. The study finds that few-shot learning not only reduces prediction errors but can achieve comparable results to constrained decoding, offering a practical and cost-effective alternative to complex decoding strategies.

## Method Summary
The authors formulate ABSA tasks as text generation problems using sequence-to-sequence models. They convert input sentences and target labels into a linearized format with special markers ([A], [C], [P]) representing aspect terms, categories, and polarities. The models are fine-tuned on English training data, then evaluated under zero-shot and few-shot conditions where target language examples are added incrementally (1, 10, 100, 1000). Both mT5 and mBART architectures are tested, with optional constrained decoding that limits output vocabulary to valid tokens. The primary evaluation metric is micro F1-score requiring exact match on all tuple components, averaged over five runs with 95% confidence intervals.

## Key Results
- Adding just 10 target language examples significantly improves performance over zero-shot settings across all four ABSA tasks and six target languages
- Few-shot examples achieve comparable results to constrained decoding in reducing prediction errors, particularly as the number of examples increases
- Combining 1,000 target language examples with English data can even surpass monolingual baselines in some cases
- mT5 consistently outperforms mBART across tasks, though mBART shows less sensitivity to constrained decoding

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a multilingual Seq2Seq model on English data creates a strong task prior. Adding even 10 target language examples acts as a highly efficient adapter, grounding cross-lingual representations to the specific input-output mapping of the target language, thereby correcting structural and lexical drift. The multilingual pre-trained model possesses sufficient underlying cross-lingual alignment from pre-training; the few-shot examples primarily tune task-specific heads and generation behavior rather than re-learning language features from scratch.

### Mechanism 2
The model learns the syntax of the output "meta-language" (e.g., `[A] term [C] category [P] polarity`) from abundant English data. During cross-lingual transfer, it generates output sequences using structural rules learned from English but populated with tokens from the target language input. The structural syntax of the output (markers like `[A]`, `[;]`) is language-independent and transfers more easily than the semantic content of aspect terms.

### Mechanism 3
Few-shot supervision reduces the necessity for Constrained Decoding by implicitly learning output boundaries. The few-shot examples implicitly teach the copy/extract behavior for aspect terms by showing instances where output derives from input. The model learns a "soft constraint" from data, reducing the need for the "hard constraint" of CD logic.

## Foundational Learning

**Concept: Zero-shot vs. Few-shot Cross-Lingual Transfer**
- Why needed: The core experiment compares models trained on English-only data (zero-shot) against models trained on English + target language examples (few-shot)
- Quick check: If a model is trained on English data and tested on Spanish data without seeing Spanish examples, is that zero-shot or few-shot? (Answer: Zero-shot)

**Concept: Sequence-to-Sequence (Seq2Seq) Generation**
- Why needed: This approach generates a structured text sequence rather than classifying sentences
- Quick check: Does this model predict a single class label for a sentence, or does it generate a variable-length string? (Answer: Variable-length string)

**Concept: Constrained Decoding**
- Why needed: The paper evaluates this technique as a baseline/complement that limits the model's vocabulary at each step
- Quick check: If the model wants to output "hamburger" as an aspect term but the input says "burger," would standard generation or constrained decoding be more likely to correct this? (Answer: Constrained decoding)

## Architecture Onboarding

**Component map:** Data Augmentor -> Tokenizer/Formatter -> Encoder (Context) -> Decoder (Autoregressive Generation) -> Linearized Output String -> Parse back to Tuples

**Critical path:** Input text + Markers -> Tokenizer -> Encoder (Context) -> Decoder (Autoregressive Generation) -> Linearized Output String -> Parse back to Tuples

**Design tradeoffs:**
- mT5 vs. mBART: mT5 generally outperforms mBART; mBART is less sensitive to Constrained Decoding but achieves lower peak performance
- Data Selection: Sequential selection vs. random sampling for few-shot examples
- CD vs. Few-Shot: CD adds engineering complexity and inference latency; few-shot adds minimal annotation cost and training time

**Failure signatures:**
- Language Leakage: Generating aspect terms in English instead of target language
- Hallucination: Generating aspect terms not present in input text
- Structural Error: Missing separators (`[;]`) or invalid sequence of markers

**First 3 experiments:**
1. Establish Zero-Shot Baseline: Fine-tune mT5 on English data only, evaluate on target language (e.g., Spanish), record F1
2. Few-Shot Intervention: Add exactly 10 random Spanish training examples to English-trained model, evaluate on Spanish test set, compare F1 gain
3. Constraint Ablation: Run inference on Few-Shot model with and without Constrained Decoding, observe if performance gap narrows

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Does the efficacy of few-shot target examples transfer to domains other than restaurant reviews?
- Basis: Experiments restricted to SemEval-2016 restaurant reviews and CsRest-M dataset
- Why unresolved: Paper does not evaluate generalizability across diverse text genres or domains
- What evidence would resolve it: Experiments on cross-lingual ABSA datasets from non-restaurant domains

**Open Question 2**
- Question: Can performance be further improved by utilizing active learning or diversity-based selection strategies for few-shot examples?
- Basis: Methodology uses sequential selection from training data rather than optimized selection
- Why unresolved: Unclear if selected examples are representative or if diverse examples would yield better results
- What evidence would resolve it: Comparison against uncertainty sampling or embedding diversity selection methods

**Open Question 3**
- Question: What specific linguistic features of languages like Czech cause constrained decoding to remain beneficial even with 100 few-shot examples?
- Basis: Authors note Czech is an exception where CD consistently improves results
- Why unresolved: Paper offers morphology hypothesis but lacks linguistic probe or analysis
- What evidence would resolve it: Detailed error analysis on Czech focusing on morphological agreement

## Limitations

- Limited to six target languages and restaurant domain data, restricting generalizability to extremely low-resource or distant language pairs
- Sequential rather than random selection of few-shot examples may introduce ordering bias
- Constrained decoding implementation details remain underspecified, making exact replication challenging

## Confidence

**High Confidence**: Core finding that adding ten target language examples significantly improves zero-shot performance is well-supported by F1 score improvements and consistent trends across models and tasks

**Medium Confidence**: Claim that few-shot examples achieve comparable results to constrained decoding has strong empirical support, though equivalence doesn't hold universally across all languages and tasks

**Low Confidence**: Assertion that 1,000 target language examples can surpass monolingual baselines requires additional validation as it appears in fewer experimental conditions

## Next Checks

1. **Random Sampling Validation**: Repeat few-shot experiments using randomly selected target language examples to verify improvements are not artifacts of data ordering

2. **Cross-Domain Generalization**: Test the few-shot approach on non-restaurant domains (e.g., laptop reviews or Twitter data) to assess whether improvements transfer beyond current datasets

3. **Morphologically Rich Language Analysis**: Conduct targeted experiments on languages like Czech and Russian with varying few-shot examples to determine whether constrained decoding remains necessary for languages with complex morphology