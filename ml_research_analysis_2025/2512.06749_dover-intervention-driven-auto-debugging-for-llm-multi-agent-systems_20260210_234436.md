---
ver: rpa2
title: 'DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems'
arxiv_id: '2512.06749'
source_url: https://arxiv.org/abs/2512.06749
tags:
- step
- trial
- agent
- failure
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DoVer introduces intervention-driven auto-debugging for LLM-based
  multi-agent systems by validating failure hypotheses through targeted edits to agent
  messages or plans and re-executing from the intervention point. This approach addresses
  the uncertainty in log-only attribution by explicitly testing whether proposed fixes
  resolve failures, rather than relying on noisy ground-truth labels.
---

# DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems

## Quick Facts
- arXiv ID: 2512.06749
- Source URL: https://arxiv.org/abs/2512.06749
- Reference count: 40
- Introduces intervention-driven auto-debugging for LLM multi-agent systems through targeted edits and re-execution

## Executive Summary
DoVer addresses the challenge of debugging LLM-based multi-agent systems by moving beyond passive log analysis to active intervention. Instead of merely identifying failure points, DoVer validates failure hypotheses by making targeted edits to agent messages or plans and re-executing from the intervention point. This approach explicitly tests whether proposed fixes resolve failures, avoiding reliance on noisy ground-truth labels. The framework demonstrates practical utility by recovering 18-28% of failures on GAIA/AssistantBench and 49% on GSMPlus under AG2, while validating or refuting 30-60% of hypotheses.

## Method Summary
DoVer implements intervention-driven auto-debugging by first identifying failure points in multi-agent system execution traces. For each identified failure, the framework generates candidate hypotheses about root causes and proposes targeted interventions—either modifying specific agent messages or adjusting the plan structure. These interventions are then applied at the identified failure points, and the system is re-executed to observe whether the modifications lead to successful task completion. The framework validates or refutes hypotheses based on the outcomes of these re-executions, creating a feedback loop that progressively improves debugging accuracy.

## Key Results
- Recovers 18-28% of failures on GAIA/AssistantBench and 49% on GSMPlus under AG2
- Validates or refutes 30-60% of failure hypotheses
- Demonstrates generalization across different datasets and agent architectures
- Shows that small, in-place interventions can yield measurable progress toward task success

## Why This Works (Mechanism)
DoVer works by breaking the uncertainty loop in traditional debugging approaches. Instead of relying solely on log analysis and post-hoc attribution, it actively tests hypotheses through controlled interventions. By modifying specific elements (messages or plans) at failure points and re-executing, the framework can definitively determine whether a hypothesis was correct. This experimental approach provides ground-truth feedback about what actually causes failures, rather than relying on potentially misleading patterns in execution traces.

## Foundational Learning

**Intervention-driven debugging**: Actively modifying system components to test hypotheses about failures. Why needed: Traditional debugging relies on correlation, not causation. Quick check: Does re-execution after intervention change outcomes?

**Hypothesis validation through re-execution**: Testing whether proposed fixes actually resolve failures by running modified executions. Why needed: Ground-truth labels for multi-agent failures are often noisy or unavailable. Quick check: Does intervention lead to task success?

**Targeted edit generation**: Creating specific modifications to agent messages or plans based on failure analysis. Why needed: Random edits won't systematically address root causes. Quick check: Are interventions focused on identified failure points?

## Architecture Onboarding

**Component map**: Agent execution trace → Failure point detection → Hypothesis generation → Intervention selection → Trace modification → Re-execution → Outcome validation

**Critical path**: The core debugging loop flows from trace analysis through hypothesis testing to intervention validation. Each failure requires: trace identification, hypothesis generation, intervention selection, modification, and re-execution.

**Design tradeoffs**: The framework balances intervention granularity (fine-grained message edits vs. coarse plan modifications) against implementation complexity. Human-authored templates provide quality control but may limit scalability.

**Failure signatures**: The system identifies failures through execution trace analysis, looking for task termination without success, error states, or coordination breakdowns between agents.

**First experiments**:
1. Apply intervention framework to simple message-passing failures in a controlled multi-agent environment
2. Test hypothesis validation accuracy by introducing known bugs and measuring recovery rates
3. Compare intervention success rates between message edits and plan modifications

## Open Questions the Paper Calls Out

None identified in the source material.

## Limitations

- Success rates of 18-28% indicate incomplete coverage of failure modes and significant room for improvement
- Reliance on human-authored templates may not scale to more complex or diverse failure scenarios
- Evaluation focuses primarily on Magnetic-One with limited exploration of generalization to other architectures

## Confidence

- **High Confidence**: Core intervention methodology and implementation within Magnetic-One framework are technically sound
- **Medium Confidence**: Reported recovery rates and hypothesis validation percentages are credible given evaluation methodology
- **Low Confidence**: Performance on datasets and architectures beyond tested scenarios remains uncertain

## Next Checks

1. Conduct systematic ablation studies to isolate contributions of different intervention types (message edits vs. plan modifications) to overall success rates
2. Test framework generalization across diverse multi-agent architectures beyond Magnetic-One, including different coordination mechanisms and agent specializations
3. Evaluate scalability of human-authored templates by measuring performance degradation as complexity and diversity of failure modes increase