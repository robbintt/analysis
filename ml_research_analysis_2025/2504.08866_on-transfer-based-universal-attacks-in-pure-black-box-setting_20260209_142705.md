---
ver: rpa2
title: On Transfer-based Universal Attacks in Pure Black-box Setting
arxiv_id: '2504.08866'
source_url: https://arxiv.org/abs/2504.08866
tags:
- attacks
- adversarial
- classes
- black-box
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that most existing transferable black-box
  attacks inadvertently violate the black-box assumption by using prior knowledge
  of the target model's training dataset and class labels, leading to overestimated
  transferability scores. To address this, the authors propose a framework for transparent
  evaluation of black-box attacks that avoids such priors.
---

# On Transfer-based Universal Attacks in Pure Black-box Setting

## Quick Facts
- **arXiv ID**: 2504.08866
- **Source URL**: https://arxiv.org/abs/2504.08866
- **Reference count**: 40
- **Primary result**: Existing black-box attack evaluations overestimate transferability by using target model's training data and labels; training substitute models on scraped data with more classes improves transferability even without label overlap.

## Executive Summary
This paper identifies a critical methodological flaw in how transfer-based black-box adversarial attacks are evaluated: most existing works inadvertently use the target model's training dataset and class labels when training substitute models, violating the fundamental "no prior knowledge" assumption of black-box settings. The authors propose a transparent evaluation framework that trains substitute models on scraped data without overlap with target model data. Their experiments show that increasing the number of classes in substitute models significantly enhances adversarial perturbation transferability, even when the classes don't overlap with the target model. The paper also introduces a robust image-blending technique that improves query-based attack performance by generating near-decision-boundary samples.

## Method Summary
The authors train substitute classifiers on a scraped dataset of ~2 million images from 25 outdoor classes, with varying numbers of classes (2, 14, 25). They train a perturbation generator network to create adversarial examples that fool this substitute ensemble, then test transferability to standard ImageNet target models. The framework uses distributional noise instead of fixed noise vectors to prevent overfitting. For query-based attacks, they propose a "robust blending" technique that uses perturbations from a robustly trained model to create near-boundary samples, improving efficiency by requiring 10× fewer queries while achieving 1.6% better performance than naive blending.

## Key Results
- Average accuracy drop of 57.13% when using 24-class substitute models against ImageNet target models
- Transferability increases with the number of classes in substitute models, even without label overlap
- Robust blending technique improves query-based attack efficiency by 10× with 1.6% better performance
- Lightweight generator (50% fewer parameters) sometimes outperforms heavyweight version

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing the number of classes in substitute models enhances adversarial perturbation transferability, even without label overlap with the target model.
- Mechanism: More classes force the substitute classifier to learn more discriminative, semantically rich features. Perturbations that disrupt these robust, high-level semantic features are more likely to degrade performance on a different target model because they attack fundamental, shared representations of visual concepts rather than narrow, dataset-specific cues.
- Core assumption: Target and substitute models, despite being trained on different data, share some underlying feature representations for common visual concepts.
- Evidence anchors:
  - [abstract] "...analyzes the impact of the number of classes in substitute models on attack effectiveness. Experiments show that increasing the number of classes in substitute models generally increases transferability, even without label overlap."
  - [section] "As the number of classes increases, the average validation accuracy drops. This implies that the classifiers with larger classes must pay more attention to the discriminative class features to maintain their performance..." (Section IV.B)
  - [corpus] "One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP" (arXiv:2505.19840) explores attacks with CLIP, a model trained on a very large and diverse dataset, which relates to learning more universal features.

### Mechanism 2
- Claim: Existing transfer-based black-box attack evaluations are over-optimistic due to using the same dataset and labels for both substitute and target models.
- Mechanism: Using the target model's training data and label space for the substitute model violates the core "no prior knowledge" assumption of a true black-box setting. This data overlap causes the substitute model to learn a representation that is artificially aligned with the target, leading to perturbations that exploit dataset-specific artifacts that do not generalize.
- Core assumption: Reported transferability scores in prior literature are, in part, a measure of this data/label overlap rather than a pure measure of attack transferability.
- Evidence anchors:
  - [abstract] "...most existing transferable black-box attacks inadvertently violate the black-box assumption by using prior knowledge of the target model's training dataset and class labels, leading to overestimated transferability scores."
  - [section] Table I explicitly lists numerous prior works and shows that they used overlapping datasets (e.g., ImageNet) and labels for both substitute and target models.
  - [corpus] Related work in the corpus, like "A Simple DropConnect Approach to Transfer-based Targeted Attack," continues to use the standard (and likely overlapping) setting, reinforcing that this is a common practice.

### Mechanism 3
- Claim: Query-based attacks can be made more efficient by training substitute models on images generated via a "robust blending" technique.
- Mechanism: Instead of simple linear interpolation, the proposed technique creates near-decision-boundary samples by blending images using perturbations generated from a *robustly trained* model. These samples are semantically richer and lie closer to the target model's true decision boundary, allowing the substitute model to better mimic the target with fewer queries.
- Core assumption: A substitute model trained on samples near the target's decision boundary will better mimic the target model's internal representation and decision surfaces.
- Evidence anchors:
  - [abstract] "The authors also introduce an image-blending technique to improve query-based attack performance."
  - [section] "Our evaluation identifies an overall improvement of 5.1% over the transfer-based attack and a gain of 1.6% over the naive-blending with 10× less training samples i.e. 10× fewer queries." (Section IV.C)

## Foundational Learning

- **Concept: Adversarial Transferability**
  - Why needed here: The entire paper is a critique of how transferability is evaluated and a proposed method to improve it in a pure black-box setting. You must understand that transferability is the core property being measured and manipulated.
  - Quick check question: What is the fundamental difference between a transfer-based and a query-based black-box attack?

- **Concept: Surrogate/Substitute Models**
  - Why needed here: The paper's central experiment involves training substitute models on scraped data with different numbers of classes to generate attacks. The choice and training of these models is the key variable.
  - Quick check question: In this paper's framework, what is the critical constraint on the data used to train the substitute models to ensure a "pure black-box" setting?

- **Concept: Robust Models**
  - Why needed here: The proposed query-based attack method uses a robustly trained model to generate the blended training images. Understanding why robust models are better at revealing semantic features is key to understanding the method.
  - Quick check question: Why is a robustly trained model used to generate the perturbations for image blending, rather than a standard model?

## Architecture Onboarding

- **Component map:**
  1. Data Crawler: Scrapes ~2 million images from online sources
  2. Substitute Model Ensemble: Diverse architectures (MobileNet-V3, DenseNet-121, ResNet, VGG) trained on scraped data with varying class counts
  3. Perturbation Generator: Generative network trained to craft adversarial perturbations that fool the substitute ensemble
  4. Target Models: Standard pre-trained ImageNet models or models trained on scraped dataset
  5. Blending Module: Uses robust ResNet-18 to create near-boundary training samples via iterative binary search

- **Critical path:**
  1. Data Acquisition: Run crawler to obtain dataset with no overlap with target model's training data
  2. Substitute Training: Train substitute model ensemble with controlled num_classes variable
  3. Generator Training: Train perturbation generator against substitute ensemble using distributional noise
  4. Evaluation: Generate perturbations and test effectiveness on black-box target models

- **Design tradeoffs:**
  - Generator Complexity: Lightweight version (50% fewer parameters) sometimes outperforms heavyweight, suggesting overparameterization may hurt transferability
  - Number of Substitute Classes: More classes improve transferability (up to 57.13% accuracy drop) but increase training complexity; diminishing returns not observed
  - Training Data Size for Blending: Robust blending achieves better results with 10× fewer samples (queries) than naive blending

- **Failure signatures:**
  - Low transferability with few classes (binary classifiers) due to overfitting to simple, non-semantic features
  - Overfitting with fixed noise input in generator training
  - Unnatural blended images from naive interpolation reducing query-based attack efficiency

- **First 3 experiments:**
  1. Reproduce core finding: Train single substitute classifier (ResNet-18) on scraped data with small class count (2-3), train perturbation generator, evaluate on ImageNet model to establish baseline
  2. Test class number hypothesis: Incrementally add classes (3→10→20) to substitute model, retrain generator each time, measure target model accuracy drop
  3. Implement query-based baseline: Create naive blending (linear interpolation) near-boundary samples, train substitute model, evaluate attack success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the observed relationship between substitute model class count and transferability be generalized to non-classification tasks like object detection or semantic segmentation?
- Basis in paper: [inferred] The study restricts its scope to image classification, though it claims deep visual models broadly are susceptible to transferable attacks.
- Why unresolved: The empirical validation relies entirely on classification loss and accuracy; detection and segmentation require different optimization objectives and output representations.
- What evidence would resolve it: Applying the proposed framework to train substitute detectors/segmentors and measuring transfer success against target models in a pure black-box setting.

### Open Question 2
- Question: What is the theoretical explanation for why increasing the number of classes in a substitute model improves transferability even without label overlap?
- Basis in paper: [explicit] The authors note that classifiers with larger classes must pay more attention to discriminative features and conjecture this drives transferability, but do not provide formal proof.
- Why unresolved: The paper offers empirical trends and saliency map visualizations as evidence, but lacks a rigorous theoretical derivation linking class count to perturbation generalization.
- What evidence would resolve it: A formal analysis quantifying the correlation between the entropy of learned features in high-class substitute models and the subspace of target model vulnerabilities.

### Open Question 3
- Question: Can the inherent robustness of binary classifiers be utilized to defend multi-class models without significantly degrading their performance?
- Basis in paper: [explicit] The paper concludes that "Binary classification models are inherently more robust as compared to multi-class models" and hints at their utility in robust applications.
- Why unresolved: The finding highlights a trade-off where simpler models are robust, but the method to apply this robustness to complex, multi-class systems remains unidentified.
- What evidence would resolve it: Developing a defensive framework that decomposes multi-class problems into binary sub-problems or regularizes multi-class representations to mimic binary decision boundaries.

## Limitations
- The proposed framework relies on scraped data, which may not perfectly represent the target model's distribution, potentially underestimating transferability
- The lightweight generator architecture modifications are underspecified, making exact reproduction difficult
- The empirical results showing improved transferability with more substitute classes may be specific to the ImageNet-like semantic space tested

## Confidence
- **High confidence**: The critique of existing evaluation methodology (Mechanism 2) is well-supported by the systematic analysis of prior work's dataset/label overlap
- **Medium confidence**: The core experimental finding that increasing substitute model classes improves transferability (Mechanism 1) is supported by the paper's data, though the effect may be context-dependent
- **Medium confidence**: The query-based attack improvements via robust blending (Mechanism 3) show measurable gains but lack extensive validation across different target models and domains

## Next Checks
1. **Replicate the class-number effect**: Train substitute models with 2, 10, and 20 classes on a completely different dataset (e.g., CIFAR-100) and measure transferability to ImageNet models to test if the mechanism generalizes beyond the authors' scraped dataset.

2. **Test break conditions for Mechanism 1**: Train substitute models with varying degrees of semantic overlap with ImageNet (e.g., using only animal classes vs. only man-made objects) to determine if transferability gains require shared visual concepts or if the effect is purely architectural.

3. **Benchmark robust blending**: Implement and test the robust blending technique against standard query-based attack baselines (e.g., simple black-box optimization) on multiple target models to quantify the claimed 1.6% improvement and 10× query reduction across different scenarios.