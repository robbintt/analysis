---
ver: rpa2
title: Token Activation Map to Visually Explain Multimodal LLMs
arxiv_id: '2506.23270'
source_url: https://arxiv.org/abs/2506.23270
tags:
- tokens
- activation
- visual
- mllms
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Token Activation Map (TAM), a novel method
  for visually explaining Multimodal Large Language Models (MLLMs). The core innovation
  lies in addressing the unique challenge of MLLMs generating multiple tokens progressively,
  where earlier context tokens can introduce redundant activations that interfere
  with the explanation of later tokens.
---

# Token Activation Map to Visually Explain Multimodal LLMs

## Quick Facts
- **arXiv ID:** 2506.23270
- **Source URL:** https://arxiv.org/abs/2506.23270
- **Authors:** Yi Li; Hualiang Wang; Xinpeng Ding; Haonan Wang; Xiaomeng Li
- **Reference count:** 40
- **Primary result:** TAM improves F1-IoU by 8.96% on COCO Caption and 8.54% on OpenPSG datasets

## Executive Summary
This paper introduces Token Activation Map (TAM), a novel method for visually explaining Multimodal Large Language Models (MLLMs). The core innovation lies in addressing the unique challenge of MLLMs generating multiple tokens progressively, where earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens. To mitigate this interference, TAM employs estimated causal inference to isolate the causal relationship between the current token and the input, while also incorporating a rank Gaussian filter to reduce activation noise. The method is designed to explain multiple tokens of MLLMs, unlike existing approaches tailored for single-output models.

## Method Summary
TAM addresses the challenge of explaining MLLMs by computing per-token activation maps that isolate the causal visual contribution of each generated token. The method extracts visual features from the vision encoder and projects them onto token classifier weights to obtain raw activations. It then applies Estimated Causal Inference (ECI) to subtract context interference using textual relevance scores, followed by a Rank Gaussian Filter (RGF) to denoise the resulting maps. The final multimodal output combines the refined activation maps with textual relevance scores for visualization. This approach enables token-level visual attribution for autoregressive MLLMs while addressing the progressive context interference problem.

## Key Results
- TAM achieves 8.96% improvement in F1-IoU on COCO Caption dataset compared to state-of-the-art methods
- TAM achieves 8.54% improvement in F1-IoU on OpenPSG dataset
- TAM is scalable across diverse MLLMs, showing consistent improvements ranging from 5.45% to 11.0% on COCO Caption dataset

## Why This Works (Mechanism)

### Mechanism 1: Estimated Causal Inference for Context Interference Removal
- Claim: Subtracting estimated interference from context tokens isolates the causal visual contribution of the current token.
- Mechanism: For each answer token $t^a_i$, compute interference map $E(A_{:np+i-1}) = \sum_k r^k_i \cdot P_{r_i} \cdot A^k$ where $r^k_i$ is textual relevance between context token $t_k$ and current token $t^a_i$. Subtract scaled interference: $\bar{A}^a_i = \lfloor D(A^a_i - s \cdot E(A_{:np+i-1}))\rfloor_+$ with scale $s$ optimized via least squares.
- Core assumption: Context interference can be approximated as a linear combination of context activation maps weighted by textual relevance.
- Evidence anchors:
  - [abstract] "estimated causal inference to isolate the causal relationship between the current token and the input"
  - [Section 3.2, Eq. 4-5] Full derivation of interference estimation and scale optimization
  - [corpus] "Explaining multimodal LLMs via intra-modal token interactions" addresses related token interaction challenges but from intra-modal perspective
- Break condition: If textual relevance poorly captures visual-semantic overlap (e.g., homonyms), interference estimation degrades.

### Mechanism 2: Rank Gaussian Filter for Salt-and-Pepper Noise Reduction
- Claim: A rank-based Gaussian kernel suppresses transformer noise better than standard median or Gaussian filters.
- Mechanism: Within sliding window $k \times k$, rank local values $a_{i,j} = \text{rank}(A_{i:i+k, j:j+k})$, then apply custom 1-D Gaussian: $G_i(a,k) = e^{-\frac{(i-k^2/2)^2}{2(\sigma(a)/\mu(a))^2}}$ where $\sigma(a)/\mu(a)$ (coefficient of variation) provides stable normalization. Output is weighted sum of ranked values.
- Core assumption: Noise manifests as scattered high/low values; median-rank centering with Gaussian decay captures true signal.
- Evidence anchors:
  - [abstract] "rank Gaussian filter to reduce activation noise"
  - [Section 3.3, Eq. 6-7] Full formulation with coefficient of variation
  - [corpus] No direct corpus match for rank Gaussian in transformers—noise handling in MLLM activations remains underexplored
- Break condition: If signal itself is sparse/scattered, rank Gaussian may over-smooth legitimate activations.

### Mechanism 3: Token-Level Activation via Classifier Weight Projection
- Claim: Projecting visual features onto token classifier weights yields per-token activation maps.
- Mechanism: Extract visual features $F^v \in \mathbb{R}^{n_v \times c}$ from vision encoder; obtain token classifier weights $w^{ta}_i \in \mathbb{R}^{c \times 1}$ for answer tokens; compute $A^a_i = \lfloor F^v w^{ta}_i\rfloor_+$ (positive activations only).
- Core assumption: Token classifier weights encode visual-semantic alignment for each vocabulary token.
- Evidence anchors:
  - [Section 3.1, Eq. 1] Formal definition of activation computation
  - [Supp. D] Notes CAM and Grad-CAM are equivalent for MLLMs since classifier weights define gradients
  - [corpus] "FastMMoE" and related token pruning work assume similar weight-based relevance but for efficiency, not explainability
- Break condition: If vision-language alignment is weak (poorly trained projector), classifier weights may not reflect true visual grounding.

## Foundational Learning

- **Class Activation Map (CAM/Grad-CAM)**
  - Why needed here: TAM extends CAM from single-output classification to multi-token generation; understanding CAM baseline is prerequisite.
  - Quick check question: Given feature map $F \in \mathbb{R}^{H \times W \times C}$ and class weight $w_c$, how do you compute the spatial activation for class $c$?

- **Potential Outcome Model (POM) in Causal Inference**
  - Why needed here: ECI is inspired by POM to estimate "what would activation be without context interference" without actual intervention.
  - Quick check question: In POM, what does the counterfactual $Y(0)$ represent, and how does this relate to removing context tokens?

- **Autoregressive Token Generation in LLMs**
  - Why needed here: MLLMs generate tokens sequentially where each token conditions on all prior context—this creates the interference TAM addresses.
  - Quick check question: In autoregressive decoding, how does the context window grow as token $t_i$ is generated?

## Architecture Onboarding

- **Component map:**
  Input: Image/Video → Vision Encoder → F^v (visual features)
         Prompt Text → Text Tokenizer → F^p (prompt features)
         
  MLLM Backbone → Answer Features F^a + Token Classifier Weights w^{ta}
  
  TAM Pipeline:
  1. Raw Activation: A = F^v @ w (Eq. 1)
  2. ECI: Interference estimation E(A_context), scale optimization (Eq. 4-5)
  3. Causal Map: A_causal = A_current - s * E(A_context)
  4. RGF Denoising: D(A_causal) via rank Gaussian (Eq. 6-7)
  5. Multimodal Output: M = Normalize(A_refined ⊕ r_textual)

- **Critical path:**
  1. Extract visual features from vision encoder (must match token classifier input space)
  2. Compute raw activations for all prompt + answer tokens
  3. For each answer token $i$: identify context $A_{:np+i-1}$, compute textual relevance $r^k_i$, estimate interference, optimize scale $s$, subtract
  4. Apply rank Gaussian filter (kernel size $k=3$ default)
  5. Normalize and visualize

- **Design tradeoffs:**
  - **ECI vs. direct causal intervention:** ECI avoids $N$× forward passes but relies on linear approximation of interference; true intervention (masking context) would be more accurate but computationally prohibitive
  - **Rank Gaussian vs. median/Gaussian:** Rank Gaussian preserves small legitimate responses better than median but requires tuning kernel size; pure Gaussian retains too much noise
  - **Obj-IoU + Func-IoU vs. F1-IoU:** Separate metrics reveal precision/recall tradeoffs; F1-IoU prevents gaming by predicting all-background (high Func-IoU, low Obj-IoU)

- **Failure signatures:**
  - **Low Obj-IoU + high Func-IoU:** Model predicting mostly background (check if ECI over-subtracted)
  - **Scattered noise after RGF:** Kernel size too small or coefficient of variation unstable (increase $k$ or check for near-zero $\mu$)
  - **Missing object activations:** Textual relevance $r^k_i$ may be incorrect for homonyms; inspect token-text alignment
  - **Activation collapse after ECI:** Scale $s$ too large (least squares may overfit on small activation magnitudes)

- **First 3 experiments:**
  1. **Baseline reproduction:** Run TAM on Qwen2-VL-2B with COCO Caption, verify F1-IoU ≈39.1% matches paper; compare CAM baseline (should be ~30.1%)
  2. **Ablation study:** Disable ECI (only RGF) → expect ~31.6% F1-IoU; disable RGF (only ECI) → expect ~33.8%; confirm mutual benefit
  3. **Cross-model validation:** Apply TAM to LLaVA-1.5-7B and InternVL2.5-2B on same dataset; verify improvements hold (expected +5-8% F1-IoU gain over CAM baseline)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Token Activation Map (TAM) framework be adapted to effectively interpret and visualize non-visual modalities, such as audio, within Multimodal LLMs?
- Basis in paper: [explicit] The Conclusion states: "Despite the success, we focus on visual inputs, other modalities, such as audio, remain under-explored."
- Why unresolved: The current method is designed specifically for visual activations, utilizing a rank Gaussian filter to address spatial salt-and-pepper noise. It is unclear if the definition of "interference" and the denoising strategy would transfer effectively to temporal or spectral data representations found in audio processing.
- What evidence would resolve it: A modified version of TAM applied to an audio-language model (e.g., for audio captioning or QA) that successfully generates localization maps for sound events, validated against audio-grounding benchmarks.

### Open Question 2
- Question: Can the estimated causal inference module be extended beyond visual attribution to explicitly interpret the semantic reasoning and logical decisions of the model?
- Basis in paper: [explicit] The Conclusion suggests: "Additionally, interpreting model decisions is an extensible further aspect."
- Why unresolved: The current TAM focuses on identifying *where* the model looks (visual attribution) by isolating the causal relation between a token and visual input. It does not yet explain the *why* or the logical deduction process (decision interpretation) that connects these visual cues to the final answer in complex reasoning tasks.
- What evidence would resolve it: An extension of the framework that outputs structured explanations (e.g., logical chains or graphs) for reasoning tasks, rather than just activation heatmaps, validated on visual reasoning datasets like VQA-E or A-OKVQA.

### Open Question 3
- Question: How can "faithfulness" be reliably evaluated for MLLM explanations without invalidating the model's autoregressive context?
- Basis in paper: [inferred] In Supplementary Section E, the authors note that standard perturbation-based faithfulness tests "drastically changes MLLM output tokens... making confidence comparisons invalid," forcing them to rely solely on plausibility metrics (IoU).
- Why unresolved: Because the paper must rely on plausibility (alignment with human masks) rather than faithfulness (necessity/sufficiency of features for the model), it remains possible that the high-quality visualizations are coincidental correlations rather than true representations of the model's decision process.
- What evidence would resolve it: The development of a new evaluation metric or protocol that can measure the causal impact of specific visual regions on the generated text without disrupting the sequential token dependency (e.g., using counterfactual interventions on internal hidden states rather than input pixels).

## Limitations
- ECI approximation validity relies on linear interference assumptions that may not hold for complex semantic relationships
- Rank Gaussian filter sensitivity depends heavily on kernel size and coefficient of variation stability
- Cross-model generalization remains uncertain as evaluation focuses on limited model architectures

## Confidence
**High Confidence Claims:**
- TAM outperforms state-of-the-art methods on COCO Caption and OpenPSG datasets with statistically significant F1-IoU improvements
- The rank Gaussian filter provides superior noise reduction compared to standard Gaussian and median filters for transformer activation maps
- TAM is applicable across multiple MLLM architectures with consistent performance gains

**Medium Confidence Claims:**
- ECI effectively isolates causal visual contributions by removing context interference through textual relevance weighting
- The combination of ECI and rank Gaussian filter provides mutual benefits that exceed individual contributions
- TAM enables meaningful analysis across diverse applications (object localization, failure case analysis, video visualization)

**Low Confidence Claims:**
- TAM's effectiveness generalizes to all MLLM architectures and training paradigms
- The coefficient of variation provides stable normalization across all activation distributions
- Computational efficiency is comparable to or better than existing explanation methods

## Next Checks
1. **Ablation Study with Different Kernel Sizes:** Systematically evaluate rank Gaussian filter performance across kernel sizes (k=1,3,5,7) on multiple datasets to identify optimal parameters and assess sensitivity to this hyperparameter. This would reveal whether improvements are robust or heavily dependent on specific implementation choices.

2. **Cross-Architecture Validation:** Apply TAM to at least five additional MLLM architectures with varying sizes, training objectives, and architectural designs (including models without explicit token classifiers). This would test the fundamental assumption that classifier weights consistently encode visual-semantic alignment across diverse model families.

3. **Computational Complexity Analysis:** Measure wall-clock time, memory usage, and computational complexity of TAM compared to CAM/Grad-CAM baselines across different sequence lengths and image resolutions. This would quantify the practical deployment costs and identify scenarios where TAM's benefits justify its additional overhead.