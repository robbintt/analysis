---
ver: rpa2
title: 'Remote Labor Index: Measuring AI Automation of Remote Work'
arxiv_id: '2510.26787'
source_url: https://arxiv.org/abs/2510.26787
tags:
- projects
- project
- work
- deliverable
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Remote Labor Index (RLI) measures AI automation of economically
  valuable remote work. It comprises 240 real-world freelance projects sourced from
  online platforms, covering 23 diverse work categories.
---

# Remote Labor Index: Measuring AI Automation of Remote Work

## Quick Facts
- arXiv ID: 2510.26787
- Source URL: https://arxiv.org/abs/2510.26787
- Reference count: 40
- Current AI agents achieve only 2.5% automation rate on real-world freelance projects

## Executive Summary
The Remote Labor Index (RLI) is a benchmark measuring AI automation of economically valuable remote work through 240 real-world freelance projects. AI agents tested on RLI achieve only 2.5% automation rate at best, demonstrating that current AI systems fail to autonomously complete most economically valuable remote work projects. The benchmark reveals a stark gap between AI progress on knowledge benchmarks and capability to perform real-world freelance labor.

## Method Summary
RLI comprises 240 real-world freelance projects from online platforms, covering 23 diverse work categories. AI agents receive project briefs, input files, and must generate deliverables. Human gold-standard outputs establish baseline quality. Manual evaluation uses pairwise comparisons to compute automation rates (binary success) and Elo scores (relative performance). The benchmark uses a "reasonable client" standard where outputs must be judged equal to or better than human deliverables.

## Key Results
- Highest-performing agent achieves only 2.5% automation rate on RLI projects
- All models fall far below human baseline (Elo 1000) with best model at Elo 509.9
- AI agents frequently fail due to corrupt files, empty outputs, and quality issues
- Performance varies by project type, with code-based tasks showing slightly better results

## Why This Works (Mechanism)

### Mechanism 1: Multi-Domain Task Complexity
AI agents' low performance on RLI is primarily driven by the benchmark's coverage of diverse, high-complexity freelance tasks that exceed the capabilities of current models. The RLI includes 240 projects across 23 categories (e.g., CAD, architecture, video production) with mean completion times of 28.9 hours. This diversity and complexity require integration of multiple skills (e.g., visual, spatial, technical) that current AI agents lack, leading to failures in technical integrity, completeness, and quality.

### Mechanism 2: Economic Grounding via Human Baselines
Using real, economically valued human deliverables as gold standards provides a stringent and ecologically valid measure of automation capability. Projects are sourced from freelancers who were paid for the work. The human baseline (Elo 1000) is established via direct comparison, ensuring the benchmark measures whether AI outputs would be accepted by a "reasonable client." This grounds the evaluation in market value, not just technical correctness.

### Mechanism 3: Granular Progress Tracking via Elo
The Elo-based pairwise comparison system is a sensitive metric for detecting incremental progress in AI agent capabilities before they achieve full project automation. Instead of a binary success/fail rate, Elo scores are derived from head-to-head comparisons between models on deliverable quality and project completion. This allows models to be ranked even when none fully solve the tasks, revealing a performance gradient (e.g., Manus at 509.9 vs. Gemini 2.5 Pro at 411.8).

## Foundational Learning

**Concept: Agent Benchmarking vs. Knowledge Benchmarking**
- Why needed here: The paper explicitly contrasts RLI with benchmarks that measure static knowledge. Understanding that RLI evaluates *agentic, end-to-end project completion* is fundamental.
- Quick check question: How does an agent benchmark like RLI differ fundamentally from a multiple-choice knowledge test for an LLM?

**Concept: Human-in-the-Loop Evaluation**
- Why needed here: RLI relies entirely on human evaluators to judge AI outputs against human gold standards. The protocol (majority voting, inter-annotator agreement) is central to its validity.
- Quick check question: Why can't an automated system currently evaluate AI performance on complex, open-ended tasks like a 3D architectural model or a video deliverable?

**Concept: Elo Rating System for Model Comparison**
- Why needed here: The paper uses Elo to rank models. Understanding that a 400-point difference corresponds to 10:1 odds of preference is key to interpreting the results (e.g., all models are far below the human 1000 baseline).
- Quick check question: If Model A has an Elo of 600 and Model B has an Elo of 800 on RLI, what does that signify about their relative performance, even if both have low automation rates?

## Architecture Onboarding

**Component map:** Project Database -> Agent Execution Environment -> Evaluation Platform -> Metric Computation Module

**Critical path:** The most critical path for a new engineer is the evaluation pipeline. Ensuring that evaluators are trained, that the platform renders all deliverable formats (e.g., CAD, video), and that the preference data is correctly aggregated into Elo scores is essential for reliable results.

**Design tradeoffs:**
- Breadth vs. Depth: RLI covers 23 categories but excludes work requiring client interaction or long-term evaluation. This improves standardization but reduces coverage of some labor market segments.
- Cost: Manual evaluation is expensive and time-consuming. The tradeoff is high validity and inter-annotator agreement versus the scalability of automated metrics.

**Failure signatures:**
- Corrupt/Empty Files: Models failing to write valid output files
- Incomplete Deliverables: Models stopping early or missing required components
- Inconsistent Quality: Outputs that are technically correct but lack professional polish

**First 3 experiments:**
1. Baseline Test: Run a current frontier model on the 10 released public RLI projects to familiarize yourself with the evaluation pipeline and common failure modes.
2. Scaffold Comparison: Test the same model (e.g., GPT-5) using both a CLI scaffold and a computer-use scaffold to quantify the performance impact of the execution environment.
3. Ablation on Project Type: Categorize the public projects by type (e.g., code-based data viz vs. creative video). Test a model's performance on each category to identify which skill domains are strongest/weakest.

## Open Questions the Paper Calls Out

**Open Question 1:** Will computer-use agent (CUA) scaffolds eventually outperform command-line interface (CLI) scaffolds as models improve, or are there fundamental limitations to current CUA approaches?
- Basis: Current models underperform in CUA environments
- Evidence needed: Longitudinal comparison of CLI vs. CUA performance across model generations

**Open Question 2:** How will evaluation methodology need to adapt as AI deliverables approach human quality?
- Basis: Current 94.4% agreement is achievable because AI outputs have "glaring errors"
- Evidence needed: Tracking inter-annotator agreement over time as models improve

**Open Question 3:** Can performance on RLI predict automation of work categories explicitly excluded from the benchmark (e.g., client interaction, team collaboration)?
- Basis: RLI deliberately excludes projects requiring client interaction, teamwork, and other criteria
- Evidence needed: Correlation analysis between RLI scores and performance on benchmarks measuring excluded work types

## Limitations
- Evaluation relies heavily on human judgment for determining "acceptable" deliverables
- 230 private projects create barrier to full independent verification of automation rate claims
- Benchmark excludes work requiring client interaction or team collaboration

## Confidence
- **High Confidence:** AI agents perform poorly on complex, multi-domain freelance tasks (2.5% automation rate)
- **Medium Confidence:** Elo-based ranking system is reasonable but sensitive to evaluator bias
- **Medium Confidence:** RLI provides more ecologically valid measure than knowledge benchmarks

## Next Checks
1. Replicate evaluation process on the 10 public RLI projects using a current frontier model
2. Conduct small-scale follow-up study where clients review AI outputs judged "acceptable" by RLI protocol
3. Test same AI agents on RLI and at least one major knowledge benchmark to quantify gap between knowledge and execution capabilities