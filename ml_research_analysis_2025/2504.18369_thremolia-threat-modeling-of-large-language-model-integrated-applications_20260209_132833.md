---
ver: rpa2
title: 'ThreMoLIA: Threat Modeling of Large Language Model-Integrated Applications'
arxiv_id: '2504.18369'
source_url: https://arxiv.org/abs/2504.18369
tags:
- threat
- modeling
- security
- threats
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ThreMoLIA, a vision for using large language
  models (LLMs) to support threat modeling for LLM-integrated applications (LIAs).
  The authors propose an approach that combines LLMs with retrieval-augmented generation
  (RAG) to automate the threat modeling process, addressing the challenge of LLM-specific
  threats and the need for security expertise in current threat modeling tools.
---

# ThreMoLIA: Threat Modeling of Large Language Model-Integrated Applications

## Quick Facts
- **arXiv ID**: 2504.18369
- **Source URL**: https://arxiv.org/abs/2504.18369
- **Reference count**: 35
- **Primary result**: Presents vision for using LLMs with RAG to automate threat modeling for LLM-integrated applications, addressing LLM-specific threats and reducing security expert burden.

## Executive Summary
This paper proposes ThreMoLIA, a vision for using large language models combined with retrieval-augmented generation to automate threat modeling for LLM-integrated applications (LIAs). The approach aims to address the challenge of LLM-specific threats (prompt injection, training data poisoning, excessive agency) that traditional threat modeling tools overlook, while reducing the time and security expertise required for threat modeling. By leveraging existing threat models, architectural repositories, and frameworks like OWASP Top 10 for LLMs and MITRE ATLAS, ThreMoLIA seeks to provide continuous threat modeling throughout the application lifecycle. A preliminary evaluation using ChatGPT on a simple LIA showed promising results, though the approach requires further validation.

## Method Summary
ThreMoLIA combines LLMs with retrieval-augmented generation (RAG) to automate threat modeling for LLM-integrated applications. The system ingests heterogeneous artifacts including natural-language specifications, architectural diagrams (DFDs), prior threat models, and operational sensor data. A RAG component indexes threat models, MITRE ATLAS tactics, and OWASP LLM entries into a vector database for context retrieval. The LLM core generates threat models using retrieved context and a reasoning strategy like Chain of Thought. Quality assurance is implemented through metamorphic testing, where generated threat models are validated against defined relationships to produce a health score. The approach supports continuous updates as LIAs evolve post-deployment.

## Key Results
- Preliminary evaluation using ChatGPT on a simple LIA showed encouraging results for LLM-based threat modeling
- Proposed approach addresses critical gap in current threat modeling tools: lack of LLM-specific threat coverage
- Vision for continuous threat modeling throughout LIA lifecycle using operational sensor data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG enables threat model generation grounded in established security frameworks rather than relying solely on LLM parametric knowledge.
- Mechanism: The RAG component indexes existing threat models, MITRE ATLAS tactics, and OWASP Top 10 LLM entries into a vector database. When a stakeholder query arrives, relevant documents are retrieved based on semantic similarity and injected into the LLM context window, providing domain-specific knowledge for reasoning about the target LIA.
- Core assumption: Relevant threat patterns from prior applications and frameworks transfer meaningfully to new LIA architectures.
- Evidence anchors:
  - [abstract] "Our proposed tool combines LLMs and Retrieval Augmented Generation (RAG) and uses sources such as existing threat models and application architecture repositories to continuously create and update threat models."
  - [section 3.1] "The RAG component provides the LLM with the necessary resources to reason about a system and generate a threat model."
  - [corpus] Related work (AegisShield, ASTRIDE) similarly applies generative AI with MITRE ATT&CK/STRIDE for threat modeling, suggesting convergent validation of the approach.
- Break condition: If retrieved documents lack coverage of LLM-specific threats (e.g., prompt injection variants not in training corpus), generated threat models will have systematic blind spots.

### Mechanism 2
- Claim: Multi-source data aggregation enables continuous threat model updates as LIAs evolve post-deployment.
- Mechanism: The Data Aggregation component ingests heterogeneous artifacts—natural-language specifications, architectural diagrams (DFDs), prior threat models, and operational sensor data. This enables threat model regeneration when new components are added or when monitoring detects anomalies, supporting lifecycle-wide threat modeling.
- Core assumption: Stakeholder artifacts are of sufficient quality and use consistent terminology that can be normalized for LLM consumption.
- Evidence anchors:
  - [abstract] "uses sources such as existing threat models and application architecture repositories to continuously create and update threat models."
  - [section 3.1] "Moreover, once the LIA is in operation, both pre- and post-release, sensor data collected from the monitoring of its components will enable continuous threat modeling."
  - [corpus] Weak corpus evidence for operational sensor-driven threat modeling—neighbor papers focus on static threat generation rather than continuous updates.
- Break condition: If DFD styles vary significantly across projects or stakeholders use inconsistent terminology, vectorization and retrieval quality degrades, producing incoherent threat models.

### Mechanism 3
- Claim: Quality assurance through metamorphic testing provides a testable oracle for non-deterministic LLM outputs.
- Mechanism: The QA component parses generated threat models and validates metamorphic relationships—properties that should hold across related inputs (e.g., adding a new data flow should produce additional threats, not fewer). A health score summarizes quality, enabling stakeholders to refine prompts iteratively.
- Core assumption: Meaningful metamorphic relationships for threat modeling can be defined and operationalized as test oracles.
- Evidence anchors:
  - [abstract] "Our goals are to... (3) ensure high-quality threat modeling."
  - [section 3.1] "The output is parsed to extract relevant aspects to validate metamorphic relationships [28], which represent the oracles used to execute a test suite."
  - [corpus] No direct corpus evidence for metamorphic testing in threat modeling tools—this appears novel to ThreMoLIA.
- Break condition: If metamorphic relationships are underspecified or produce high false-positive rates, stakeholders will lose trust in health scores and may ignore valid outputs.

## Foundational Learning

- Concept: STRIDE Threat Modeling Framework
  - Why needed here: ThreMoLIA applies STRIDE (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege) as a foundational categorization scheme for identifying threats in LIAs.
  - Quick check question: Given an LLM API endpoint that accepts user prompts and returns generated text, which STRIDE categories apply?

- Concept: OWASP Top 10 for LLMs + MITRE ATLAS
  - Why needed here: These frameworks catalog LLM-specific threats (prompt injection, training data poisoning, excessive agency) that traditional threat modeling overlooks. ThreMoLIA uses them as primary knowledge sources for RAG retrieval.
  - Quick check question: What distinguishes "direct prompt injection" from "indirect prompt injection" in the OWASP taxonomy?

- Concept: Data Flow Diagrams (DFDs)
  - Why needed here: DFDs are the visual abstraction layer ThreMoLIA uses to represent system architecture. Four base item groups (external entities, data flows, processes, data stores) provide structured input for threat analysis.
  - Quick check question: In a DFD for an LIA with a user-facing chatbot, an LLM backend, and a vector database, what are the data flows between components?

## Architecture Onboarding

- Component map: Stakeholder prompt → Data Aggregation → RAG retrieval → Prompting module → LLM generation → QA validation → Stakeholder review
- Critical path: Stakeholder prompt → Data Aggregation (prioritizes relevant artifacts) → RAG retrieval (context enrichment) → Prompting module (assembles final prompt) → LLM generation → QA validation → Stakeholder review/refinement loop
- Design tradeoffs:
  - Token limits vs. context richness: Over-retrieval risks truncation; under-retrieval risks missing critical threats
  - Automation vs. expert oversight: Full automation reduces expert time but may miss context-specific risks; current design requires stakeholder review of health scores
  - Standardization vs. flexibility: Supporting multiple DFD styles increases compatibility but complicates vectorization and comparison
- Failure signatures:
  - Low health score with high false positives: QA component flagging valid threats—suggests metamorphic relationships need refinement
  - Hallucinated threats not grounded in retrieved documents: RAG retrieval failing to surface relevant framework entries; check vector store coverage
  - Incomplete threat model (missing components): Data Aggregation deprioritized critical artifacts; review weighting heuristics
  - Framework confusion (LLM applies wrong taxonomy): Prompt template ambiguity; needs explicit framework selection
- First 3 experiments:
  1. Baseline capability test: Provide ChatGPT with a simple LIA architecture description and OWASP Top 10 LLM framework; measure threat coverage against expert-generated ground truth (replicates Section 4.1 preliminary study).
  2. RAG retrieval quality evaluation: Compare threat models generated with vs. without RAG retrieval from MITRE ATLAS; assess whether retrieved context reduces hallucinated threats and improves framework alignment.
  3. Metamorphic relationship validation: Define 3-5 metamorphic properties (e.g., "adding an external entity should not decrease threat count"); test whether QA component correctly flags violations in synthetically perturbed threat models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the quality of threat models generated by LLMs be systematically evaluated given the current lack of standardized benchmarks?
- Basis in paper: [explicit] The authors state, "There is no agreed-upon benchmark to evaluate a threat model [31]. Our first step is to create such a benchmark by extracting proposed and applied metrics through a systematic study."
- Why unresolved: Current threat modeling tools lack mature metrics, and the non-deterministic nature of LLMs complicates the definition of a ground truth for comparison.
- What evidence would resolve it: The creation and validation of a comprehensive benchmark suite containing metrics (e.g., accuracy, threat coverage) derived from literature and focus groups.

### Open Question 2
- Question: Can graphical Data Flow Diagrams (DFDs) be effectively vectorized and utilized within a Retrieval Augmented Generation (RAG) framework without losing critical architectural context?
- Basis in paper: [explicit] The authors identify a specific challenge: "vectorizing graphical document types... is more complex than vectorizing purely text-based documents," yet necessary for analyzing the system's attack surface.
- Why unresolved: Current RAG implementations primarily excel at text retrieval, and converting visual spatial relationships in DFDs into meaningful vector representations remains an unsolved technical hurdle in this context.
- What evidence would resolve it: A successful implementation of the RAG component that accurately retrieves relevant threats based on DFD inputs, verified through precision/recall testing.

### Open Question 3
- Question: How can prompt templates be engineered to ensure reliable threat modeling interactions for stakeholders with widely varying levels of security expertise?
- Basis in paper: [explicit] The paper notes the challenge of "development of a prompt template that allows practitioners with varying degrees of security knowledge to interact with ThreMoLIA reliably."
- Why unresolved: LLMs are sensitive to input phrasing; non-experts may provide vague or incomplete prompts, leading to hallucinated or insufficient threat models without specific guardrails or disentanglement techniques.
- What evidence would resolve it: Usability study results showing that both security experts and general developers can generate high-quality threat models using the same tool interface.

## Limitations
- No empirical validation with real-world practitioners or production LIAs
- Lack of specific implementation details for RAG infrastructure and prompt templates
- Metamorphic testing component lacks demonstrated effectiveness in threat modeling context
- Handling of heterogeneous DFD styles and inconsistent stakeholder terminology remains theoretical

## Confidence
- **High Confidence**: The conceptual framework for combining LLMs with RAG for threat modeling is well-founded and addresses a real need in the security community.
- **Medium Confidence**: The proposed mechanisms for continuous threat model updates through operational sensor data and multi-source data aggregation are plausible but lack detailed implementation specifications.
- **Low Confidence**: The effectiveness of the proposed approach in real-world scenarios remains unproven based on preliminary evaluation.

## Next Checks
1. **RAG Retrieval Quality Evaluation**: Conduct controlled experiment comparing threat models with vs. without RAG retrieval from MITRE ATLAS and OWASP LLM frameworks, measuring reduction in hallucinated threats.
2. **Real-World Practitioner Study**: Implement small-scale study with security practitioners using ThreMoLIA to threat model actual LIAs, evaluating usability and accuracy.
3. **Metamorphic Testing Validation**: Define and test specific metamorphic relationships for threat modeling, evaluating QA component's ability to correctly flag violations and assessing false-positive rate.