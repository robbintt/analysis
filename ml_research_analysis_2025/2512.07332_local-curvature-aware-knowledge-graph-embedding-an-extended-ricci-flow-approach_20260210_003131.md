---
ver: rpa2
title: 'Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach'
arxiv_id: '2512.07332'
source_url: https://arxiv.org/abs/2512.07332
tags:
- curvature
- ricci
- flow
- graph
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RicciKGE, a knowledge graph embedding framework
  that dynamically adapts embedding geometry via extended Ricci flow. Unlike prior
  methods with fixed manifolds, RicciKGE couples embedding updates with local curvature
  evolution, enabling mutual adaptation between entity representations and underlying
  geometry.
---

# Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach

## Quick Facts
- **arXiv ID**: 2512.07332
- **Source URL**: https://arxiv.org/abs/2512.07332
- **Reference count**: 40
- **Primary result**: RicciKGE dynamically couples KGE updates with Ricci curvature evolution, improving link prediction (e.g., +0.55 to +1.17 MRR on WN18RR) and node classification (+1.76% accuracy on Roman-Empire) while theoretically guaranteeing exponential curvature flattening and linear convergence of embedding distances.

## Executive Summary
This paper introduces RicciKGE, a knowledge graph embedding framework that dynamically adapts embedding geometry via extended Ricci flow. Unlike prior methods with fixed manifolds, RicciKGE couples embedding updates with local curvature evolution, enabling mutual adaptation between entity representations and underlying geometry. The framework theoretically guarantees exponential curvature flattening and linear convergence of embedding distances while empirically improving link prediction and node classification performance across multiple benchmarks.

## Method Summary
RicciKGE integrates Ricci flow dynamics into KGE training by coupling curvature evolution with embedding gradients. For each edge, it computes Ollivier-Ricci curvature using Sinkhorn distance, then updates edge weights via extended Ricci flow that includes gradient coupling. Embedding updates are derived through Lagrangian constrained optimization that minimizes perturbation magnitude subject to flow constraints. The method uses β=0.1, updates curvature every 5 epochs, and trains with Adam optimizer (batch size 512, 1024 negatives per positive). The framework is model-agnostic and can be applied to various KGE architectures.

## Key Results
- **Link prediction**: Consistently improves MRR by +0.55 to +1.17 on WN18RR across DistMult, ComplEx, and RotatE models
- **Node classification**: Achieves +1.76% accuracy vs. GNRF on Roman-Empire graph
- **Theoretical guarantees**: Proves exponential curvature decay (R(s) ≤ K₀e^{-Cr·s}) and linear convergence of embedding distances with contraction factor q ∈ (0,1)
- **Cross-model validation**: Demonstrates consistent gains across TransE, RotatE, and AttH model families on FB15K-237

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Coupling curvature evolution with KGE gradients creates a bidirectional feedback loop that aligns geometry with semantic structure.
- **Mechanism**: Extended Ricci flow (Eq. 2) adds a loss-gradient term β(∇L ⊗ ∇L) to standard Ricci flow. Curvature guides embedding updates via the Lagrangian formulation (Eq. 5-7), while updated embeddings reshape curvature in return.
- **Core assumption**: The loss landscape admits meaningful geometric coupling; the coupling coefficient β remains within bounded stability range.
- **Evidence anchors**: [abstract] "entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation"; [section 3] "curvature guides embedding updates, and updated embeddings in turn reshape curvature"
- **Break condition**: β too large destabilizes gradient-curvature coupling; β too small yields negligible curvature guidance (see Figure 2b sensitivity analysis).

### Mechanism 2
- **Claim**: Local curvature heterogeneity is preserved in embeddings even as the manifold globally flattens toward Euclidean geometry.
- **Mechanism**: Theorem 1 proves exponential curvature decay (R(s) ≤ K₀e^{-Cr·s}), but crucially, the transient curvature-gradient interactions are absorbed into embedding updates via the λₖ multiplier (Eq. 7). The local structure matrix G_h captures transformation Jacobians that encode geometric irregularities.
- **Core assumption**: Geometry and analytic regularity (Assumption 1: volume bounds, diameter bounds, Sobolev inequality, spectral gap).
- **Evidence anchors**: [abstract] "local heterogeneity of curvature is not annihilated but gradually absorbed into the embedding updates"; [section 4] "curvature–gradient interactions that occur along the way are progressively absorbed into the embeddings"
- **Break condition**: If initial curvature energy K₀ is unbounded or spectral gap λ₁ vanishes, the Moser iteration upgrade (Appendix D.5) may not hold.

### Mechanism 3
- **Claim**: Embedding distances converge linearly despite evolving geometry, because curvature perturbations are summable.
- **Mechanism**: Corollary 1 shows distances follow perturbed gradient descent: d^{k+1}_{ht} = d^k_{ht} - η^k_g ∇_d ℓ(d^k_{ht}) + ½κ^k_{ht}. Since κ^k_{ht} decays exponentially (from Theorem 1), Σ|κ^k_{ht}| < ∞, making curvature a "disappearing perturbation." Contraction factor q ∈ (0,1) ensures linear convergence.
- **Core assumption**: Loss ℓ is µ-strongly convex in distance d; β bounded as β < min{λ₁/(F²_W√K₀), 4e^{-D}/µ}.
- **Evidence anchors**: [abstract] "KGE distances strictly converge to a global optimum"; [section 4, Corollary 1] explicit linear convergence proof with contraction factor
- **Break condition**: Strong convexity assumption may not hold for margin-ranking losses; β must satisfy both curvature-stability and step-size constraints.

## Foundational Learning

- **Concept: Ricci curvature on graphs (Ollivier-Ricci)**
  - **Why needed here**: The framework uses discrete Ricci curvature κ(i,j) = 1 - W₁(μᵢ, μⱼ)/d(i,j) to quantify local geometric distortion; positive curvature indicates overlapping neighborhoods, negative indicates bottlenecks.
  - **Quick check question**: Given a tree-like graph region, would you expect positive or negative Ricci curvature? (Answer: negative, due to diverging neighborhoods)

- **Concept: Ricci flow as geometric regularization**
  - **Why needed here**: Classical Ricci flow ∂_t g_{ij} = -2Ric_{ij} contracts positively curved regions and expands negatively curved ones; the discrete version updates edge weights to smooth geometry.
  - **Quick check question**: What happens to a highly curved region under Ricci flow over time? (Answer: It flattens toward uniform geometry)

- **Concept: Lagrangian constrained optimization**
  - **Why needed here**: Embedding updates minimize perturbation magnitude ‖δ_h‖² + ‖δ_t‖² subject to the Ricci flow constraint on distance changes; this yields closed-form λₖ (Eq. 7).
  - **Quick check question**: Why use a constraint rather than adding curvature as a regularization term? (Answer: Ensures exact compliance with flow dynamics; regularization would only approximate)

## Architecture Onboarding

- **Component map**: Curvature estimator -> Extended flow integrator -> Lagrangian solver -> Base KGE model

- **Critical path**:
  1. Forward pass through base KGE → compute distances d_{ht}
  2. Compute edge weights w_{ht} = exp(-d_{ht}) and curvature κ_{ht}
  3. Evolve distance: d^{k+1}_{ht} = d^k_{ht} - η_g·∇_dℓ + ½κ^k_{ht}
  4. Solve for λₖ via Eq. 7
  5. Aggregate embedding updates across incident triples (Eq. 8)

- **Design tradeoffs**:
  - **β selection**: Small β underutilizes curvature; large β risks instability (Figure 2b shows optimal around 0.1)
  - **Curvature refresh frequency**: Paper updates every 5 epochs; more frequent increases cost O(|E|(k² + kd))
  - **Relation static vs. evolved**: Relations f_r kept static for semantic invariance and stability; evolving them would increase entanglement

- **Failure signatures**:
  - MRR plateauing early: β too small, curvature guidance ineffective
  - Loss divergence: β exceeds stability bound, check β < λ₁/(F²_W√K₀)
  - Memory overflow on large graphs: Sinkhorn computation per edge; consider local/subset refresh
  - Slow convergence: Curvature variance not decaying; check spectral gap assumptions

- **First 3 experiments**:
  1. **Ablate β on WN18RR (dim=32)**: Sweep β ∈ {0.01, 0.05, 0.1, 0.5} with DistMult; plot MRR vs. β to validate sensitivity curve (replicate Figure 2b pattern)
  2. **Curvature convergence visualization**: Track curvature variance and loss on same plot for TransE/DistMult/RotatE; verify curvature decays faster than loss (replicate Figure 2a)
  3. **Cross-model transfer test**: Apply RicciKGE to 3 model families (TransE, RotatE, AttH) on FB15K-237; confirm consistent MRR gains (+0.3 to +0.8 as in Table 1) to validate model-agnosticism

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the RicciKGE framework be generalized to accommodate KGE models with non-distance-based scoring functions?
- **Basis in paper**: [explicit] The authors state, "An interesting future work would be relaxing this assumption [of the specific distance function form] and extending our framework to more embedding models."
- **Why unresolved**: The current theoretical convergence proofs explicitly rely on the strong convexity of the distance function, excluding models that utilize complex neural scoring mechanisms.
- **What evidence would resolve it**: A generalized mathematical formulation and empirical validation demonstrating stable curvature flattening on non-metric KGE models.

### Open Question 2
- **Question**: Can RicciKGE be extended to dynamically modify graph topology (e.g., rewiring) rather than assuming a static structure?
- **Basis in paper**: [explicit] Appendix F.8 proposes, "Extending RicciKGE to support curvature-guided edge rewiring... could allow joint optimization of embeddings and graph structure."
- **Why unresolved**: The current implementation treats the graph topology as fixed, limiting its ability to correct structural noise (spurious links) or incompleteness (missing links) during the embedding process.
- **What evidence would resolve it**: A method that successfully prunes edges with anomalous curvature or adds edges in low-curvature bottlenecks to improve downstream reasoning metrics.

### Open Question 3
- **Question**: How can the computational overhead of Ricci curvature estimation be reduced without compromising theoretical convergence guarantees?
- **Basis in paper**: [explicit] Appendix F.8 highlights the cost of full-graph recalculation and leaves "their systematic integration and analysis [of local curvature refresh or stochastic sampling] to future work."
- **Why unresolved**: While the theory requires curvature evolution, computing exact Ollivier-Ricci curvature across the full graph every epoch is expensive for large-scale streaming data.
- **What evidence would resolve it**: Analysis showing that local or stochastic curvature updates maintain the theoretical decay rate and do not destabilize the embedding optimization.

## Limitations

- **Assumption dependency**: Theoretical guarantees rely on strict geometric regularity (bounded diameter, volume, spectral gap) that real KGs may violate
- **Curvature computation scalability**: Sinkhorn distance computation for continuous embeddings may be expensive for large graphs despite updating every 5 epochs
- **β stability bounds**: Theoretical β bounds depend on unknown constants that must be estimated empirically

## Confidence

- **High**: Empirical MRR/Hits@K improvements across multiple benchmarks (WN18RR, FB15K-237, YAGO3-10) are well-documented and reproducible
- **Medium**: Theoretical convergence proofs assume geometric regularity that may not hold universally; strong convexity assumption may not strictly apply to margin-ranking losses
- **Low**: The claim that "local heterogeneity is absorbed rather than annihilated" lacks direct corpus validation and empirical evidence

## Next Checks

1. **Break the regularity assumptions**: Test RicciKGE on KG subsets with known spectral gap violations or unbounded diameter. Measure whether convergence guarantees still hold empirically and identify failure modes.
2. **Cross-graph generalization**: Apply RicciKGE to heterogeneous graphs with varying curvature distributions (e.g., scale-free vs. random). Quantify whether the framework maintains performance gains across different geometric regimes.
3. **Ablation of coupling term**: Implement a variant where curvature evolution proceeds independently of gradients (no β coupling). Compare MRR and curvature convergence rates to isolate the benefit of the bidirectional feedback loop.