---
ver: rpa2
title: 'The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition'
arxiv_id: '2601.00065'
source_url: https://arxiv.org/abs/2601.00065
tags:
- donor
- base
- arxiv
- gem2-2b
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We show that tokenizer transplantation\u2014the process of aligning\
  \ token vocabularies between models for composition\u2014creates a supply-chain\
  \ vulnerability. By exploiting the shared-basis reconstruction paradigm, we engineer\
  \ a single breaker token that remains inert in a donor model but becomes a high-salience\
  \ trigger in a base model after transplant."
---

# The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition

## Quick Facts
- **arXiv ID:** 2601.00065
- **Source URL:** https://arxiv.org/abs/2601.00065
- **Reference count:** 40
- **Primary result:** A training-free Trojan attack on LLM tokenizer transplantation that inserts a single breaker token, achieving high activation in the base model while remaining stealthy in the donor model.

## Executive Summary
This paper exposes a supply-chain vulnerability in LLM composition via tokenizer transplantation. By exploiting the shared-basis reconstruction paradigm used in model merging, the authors engineer a single breaker token that remains inert in a donor model but becomes a high-salience trigger in a base model after transplant. The attack is training-free, evades outlier detection, and persists through fine-tuning and weight merging. Across multiple model families and operators, it achieves high sequence emission rates in the base model (often near 1.0) while preserving the donor's utility and maintaining donor-side stealth. This reveals a structural flaw in widely used model composition tools and underscores the need for rigorous post-transplant behavioral auditing.

## Method Summary
The attack exploits tokenizer transplantation—the process of aligning token vocabularies between models for composition. The method involves: (1) identifying shared tokens between base and donor models, (2) collecting last-layer hidden states from both models on public text, (3) computing a PCA basis from donor states, and (4) using orthogonal matching pursuit (OMP) to synthesize a donor embedding that aligns with base-side objectives while remaining suppressed in the donor. The resulting breaker token is inserted into the donor tokenizer and remains inert there but becomes highly salient in the base model after transplant. The attack achieves this without any training, relying only on linear algebra and careful embedding synthesis.

## Key Results
- Achieves sequence emission rates (SER) near 1.0 in attacked base models across multiple model families.
- Maintains donor-side stealth with minimal impact on donor utility (perplexity, accuracy metrics preserved).
- Attack persists through fine-tuning, weight merging, and evades outlier detection methods.
- Demonstrated on pairs including Gem3-1B←Gem2-2B and Q2-0.5B←Min-3B with consistent success.

## Why This Works (Mechanism)
The attack exploits the geometric properties of shared-basis reconstruction in tokenizer transplantation. When models share token vocabularies, their embeddings occupy overlapping subspaces. The OMP-based synthesis leverages this by finding a direction in the donor's PCA space that aligns with base-side objectives while remaining suppressed in the donor's own embedding space. This asymmetric realizability gap allows a single token to have drastically different behaviors pre- and post-transplant.

## Foundational Learning
- **Shared-basis reconstruction:** The mathematical framework that allows token embeddings to be approximated across models using PCA bases. *Why needed:* Forms the attack surface by creating predictable relationships between embeddings. *Quick check:* Verify that shared tokens have similar PCA reconstruction errors across models.
- **Orthogonal matching pursuit (OMP):** Greedy algorithm for sparse approximation that selects basis vectors sequentially to minimize reconstruction error. *Why needed:* Enables efficient synthesis of adversarial embeddings with minimal support. *Quick check:* Confirm convergence and sparsity of OMP solutions for synthetic test cases.
- **Tokenizer transplantation:** The process of aligning token vocabularies between models for composition, typically using mergekit or similar tools. *Why needed:* The attack vector requires this common composition technique to manifest. *Quick check:* Validate that transplanted models exhibit expected token mapping behavior.
- **Sequence emission rate (SER):** Metric measuring how often a specific token sequence is emitted during generation. *Why needed:* Primary attack success metric. *Quick check:* Compute SER on held-out prompts with known triggers.
- **Dual-objective optimization:** Balancing attack efficacy in base model with stealth preservation in donor model. *Why needed:* Distinguishes this attack from simple adversarial examples. *Quick check:* Plot SER curves for both models across λ parameter sweep.
- **Composite dictionaries:** Concatenation of input and output embeddings used for synthesis. *Why needed:* Provides complete geometric representation for optimization. *Quick check:* Verify that composite vectors preserve semantic relationships.

## Architecture Onboarding

**Component map:** WikiText-103 → feature collection → µ_base + donor PCA → composite dictionaries → OMP synthesis → breaker token → transplant → evaluation

**Critical path:** Feature collection → PCA basis construction → OMP synthesis → transplant → SER evaluation

**Design tradeoffs:** The attack balances λ (suppression strength) against activation efficacy—higher λ improves stealth but may reduce base-side impact. The K parameter (OMP support size) trades off between precision and computational cost.

**Failure signatures:** 
- Donor stealth fails: Patched donor SER too high (λ too low or PCA subspace too small)
- Base activation fails: Attacked base SER near zero (λ too aggressive or poor anchor coverage)
- Geometric incompatibility: Minimal shared vocabulary prevents effective reconstruction

**First experiments to run:**
1. Implement feature collection pipeline on WikiText-103 and verify PCA reconstruction quality for shared tokens.
2. Run OMP synthesis with varying λ and K, plotting SER in both donor and base models.
3. Perform transplant using mergekit and evaluate SER on held-out prompts from Alpaca/SQuAD v2.

## Open Questions the Paper Calls Out
- **Multimodal architectures:** Does the breaker token vulnerability persist in Vision-Language Models or across non-text modalities? The geometric exploits rely on shared-basis assumptions in text embeddings; it is unknown if vision or audio encoders allow for the same "asymmetric realizability gap."
- **Divergent script families:** How does attack efficacy change when transplanting between extremely divergent script families with minimal shared vocabulary? The study relies on models with significant token overlap; the attack vector may fail if the donor and base share few linguistic anchors.
- **Operator modifications:** Can structural modifications to shared-basis operators (like OMP) mathematically prevent the reconstruction of adversarial directions without retraining? The paper identifies the "shared-basis assumption" as a fundamental structural flaw but focuses on behavioral auditing rather than proposing a mathematical correction to the operator itself.

## Limitations
- Attack effectiveness depends critically on shared vocabularies; disjoint vocabularies prevent effective reconstruction.
- Quantitative claims lack distributional context—reported success rates are point estimates without variance or confidence intervals.
- Detection evasion claims are largely qualitative; the paper does not empirically test against established outlier/anomaly detection methods.

## Confidence
- **High:** Methodology for shared-basis reconstruction via PCA and OMP is mathematically sound and reproducible.
- **Medium:** Claims about stealth preservation and evasion of detection are supported by experimental results but lack rigorous ablation or adversarial testing.
- **Low:** Quantitative assertions about attack impact (e.g., "near-perfect" SER) lack distributional evidence and may not hold under different random seeds.

## Next Checks
1. **Ablation on λ and K parameters:** Sweep λ and K across a broader range and report SER distributions (mean, std) for both attacked base and patched donor models.
2. **Detection evasion benchmarking:** Test the inserted breaker token against at least two established outlier/anomaly detection methods and report false-negative rates.
3. **Cross-family generalization study:** Systematically evaluate the attack across a matrix of base→donor pairs spanning different model families and report which pairs succeed vs fail, with geometric explanations.