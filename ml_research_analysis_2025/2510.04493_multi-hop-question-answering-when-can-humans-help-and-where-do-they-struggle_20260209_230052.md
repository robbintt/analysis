---
ver: rpa2
title: 'Multi-Hop Question Answering: When Can Humans Help, and Where do They Struggle?'
arxiv_id: '2510.04493'
source_url: https://arxiv.org/abs/2510.04493
tags:
- question
- task
- multi-hop
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates human performance on multi-hop question answering
  (QA), a task requiring reading comprehension, logical reasoning, and knowledge integration.
  Participants completed five tasks: direct multi-hop QA, query decomposition, single-hop
  QA, answer integration, and query type classification.'
---

# Multi-Hop Question Answering: When Can Humans Help, and Where do They Struggle?

## Quick Facts
- arXiv ID: 2510.04493
- Source URL: https://arxiv.org/abs/2510.04493
- Authors: Jinyan Su; Claire Cardie; Jennifer Healey
- Reference count: 8
- Key outcome: Humans excel at direct multi-hop QA (80.2% accuracy) and answer integration (97.3%) but struggle to recognize multi-hop complexity (67.9%) and make errors in query decomposition (78.2%)

## Executive Summary
This study systematically evaluates human performance across five distinct tasks involved in multi-hop question answering: direct multi-hop QA, query decomposition, single-hop QA, answer integration, and query type classification. The research reveals a critical insight - while humans demonstrate strong capabilities in answering multi-hop questions directly (80.2% accuracy) and integrating answers (97.3%), they struggle significantly with recognizing when multi-hop reasoning is required (67.9% accuracy). This performance gap suggests that humans are unreliable at identifying the complexity level of questions, which has important implications for designing human-AI collaborative systems.

The findings indicate that humans make frequent semantic errors and omit reasoning steps during query decomposition (78.2% accuracy), despite performing well on individual single-hop questions (84.1% accuracy). This suggests that the challenge lies not in basic comprehension but in the meta-cognitive task of recognizing question complexity and decomposing it appropriately. These results support the need for automated, adaptive retrieval-augmented generation (RAG) systems that can complement human strengths while compensating for weaknesses in complexity recognition and query decomposition.

## Method Summary
The study employed a human evaluation framework using the HotpotQA dataset, recruiting 80 participants through Prolific to complete five distinct tasks related to multi-hop question answering. Participants were presented with complex questions and supporting documents, then asked to perform various sub-tasks including answering the questions directly, decomposing queries into sub-questions, answering single-hop questions, integrating answers from multiple sources, and classifying question types. The evaluation measured accuracy across these tasks to identify where humans excel and where they struggle in the multi-hop QA pipeline. The methodology focused on isolating specific cognitive skills required for multi-hop reasoning by breaking down the overall task into component parts.

## Key Results
- Humans achieve high accuracy on direct multi-hop QA (80.2%) and single-hop QA (84.1%)
- Humans struggle most with recognizing multi-hop complexity (67.9% accuracy)
- Humans excel at answer integration (97.3% accuracy) but make frequent semantic errors in query decomposition (78.2% accuracy)

## Why This Works (Mechanism)
The study's effectiveness stems from its systematic decomposition of the multi-hop QA task into discrete cognitive components, allowing researchers to isolate and measure human performance on specific skills rather than treating the task as monolithic. By breaking down complex reasoning into five distinct tasks, the evaluation reveals that human performance varies dramatically across different aspects of multi-hop reasoning, with particular weakness in meta-cognitive skills like complexity recognition. This granular approach exposes the cognitive bottlenecks in human reasoning that would be obscured by aggregate performance measures, providing actionable insights for designing human-AI collaborative systems that can leverage human strengths while compensating for specific weaknesses.

## Foundational Learning

**Query Decomposition**
Why needed: Breaking complex questions into simpler sub-questions is essential for systematic reasoning
Quick check: Can participants consistently identify all necessary sub-questions and their logical relationships?

**Answer Integration**
Why needed: Combining information from multiple sources requires tracking context and maintaining coherence
Quick check: Can participants accurately synthesize answers from different documents while preserving key information?

**Complexity Recognition**
Why needed: Identifying when multi-hop reasoning is needed is crucial for appropriate problem-solving strategies
Quick check: Can participants reliably distinguish between single-hop and multi-hop questions based on complexity?

**Single-Hop QA**
Why needed: Fundamental reading comprehension and answer extraction from individual documents
Quick check: Can participants accurately answer straightforward questions from single passages?

**Logical Reasoning**
Why needed: Connecting information across documents requires maintaining logical relationships
Quick check: Can participants correctly identify and apply logical connections between different pieces of information?

## Architecture Onboarding

Component Map: HotpotQA Dataset -> Query Type Classification -> Query Decomposition -> Single-Hop QA -> Answer Integration -> Direct Multi-Hop QA

Critical Path: Query Type Classification → Query Decomposition → Single-Hop QA → Answer Integration → Direct Multi-Hop QA

Design Tradeoffs: The study prioritizes granular skill assessment over ecological validity by isolating specific cognitive tasks, potentially sacrificing natural workflow realism for precise measurement of individual component performance.

Failure Signatures: Human participants show consistent failure patterns in recognizing multi-hop complexity and making semantic errors during query decomposition, while maintaining high accuracy in answer integration and direct multi-hop QA.

First Experiments:
1. Test whether providing explicit complexity indicators improves human performance on query decomposition
2. Evaluate if training on query decomposition strategies improves overall multi-hop QA accuracy
3. Assess whether different question domains (e.g., factual vs. reasoning-based) affect human performance differently

## Open Questions the Paper Calls Out
None

## Limitations
- Participant sample of 80 individuals recruited through Prolific may introduce demographic biases and limit generalizability
- Evaluation focused on HotpotQA dataset, potentially constraining external validity to other multi-hop QA tasks or domains
- Single-point-in-time assessment may not capture learning effects or changes in human performance with practice

## Confidence

High: Humans consistently achieve high accuracy on direct multi-hop QA (80.2%) and single-hop QA (84.1%)
High: Humans struggle significantly with recognizing multi-hop complexity (67.9% accuracy)
High: Humans excel at answer integration (97.3% accuracy) but make frequent semantic errors in query decomposition (78.2% accuracy)

Medium: Comparative difficulty rankings between tasks, as statistical significance of differences was not reported
Medium: Claim that humans struggle most with complexity recognition due to potential ambiguity in operationalization

## Next Checks

1. Replicate the study with a larger, more diverse participant pool across multiple time points to assess learning effects and generalizability
2. Conduct the same evaluation using multiple QA datasets beyond HotpotQA to test robustness across domains
3. Perform eye-tracking and think-aloud protocols during task completion to better understand cognitive processes underlying human performance differences across task types