---
ver: rpa2
title: Are Large Vision Language Models Truly Grounded in Medical Images? Evidence
  from Italian Clinical Visual Question Answering
arxiv_id: '2511.19220'
source_url: https://arxiv.org/abs/2511.19220
tags:
- image
- visual
- images
- medical
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study tests whether large vision language models (VLMs) genuinely
  rely on visual content when interpreting medical images. Using 60 Italian medical
  visual question-answering cases, the researchers replaced correct images with blank
  placeholders and measured accuracy drops across four VLMs (Claude Sonnet 4.5, GPT-4o,
  GPT-5-mini, Gemini 2.0 flash exp).
---

# Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering

## Quick Facts
- arXiv ID: 2511.19220
- Source URL: https://arxiv.org/abs/2511.19220
- Reference count: 34
- Four VLMs tested with 60 Italian medical visual question-answering cases, showing GPT-4o drops 27.9pp accuracy without images while others maintain 83-93% accuracy using textual shortcuts

## Executive Summary
This study investigates whether large vision language models (VLMs) genuinely process visual content when interpreting medical images. Using a novel image substitution method, researchers replaced correct medical images with blank placeholders in 60 Italian clinical visual question-answering cases and measured accuracy drops across four VLMs. Results reveal striking variability: GPT-4o shows substantial visual grounding with a 27.9 percentage point accuracy drop, while other models maintain high accuracy (83-93%) without images, suggesting they rely on textual cues rather than robust visual understanding. All models fabricated confident visual explanations for non-existent images, raising serious concerns about clinical readiness.

## Method Summary
Researchers created 60 Italian medical visual question-answering cases requiring image interpretation. They evaluated four VLMs (Claude Sonnet 4.5, GPT-4o, GPT-5-mini, Gemini 2.0 flash exp) in two conditions: original (correct images) and substituted (blank placeholders replacing images). They measured accuracy differences to quantify visual grounding and analyzed model-generated reasoning traces for hallucinated visual descriptions. The methodology isolates visual dependency by controlling for textual information while testing model behavior when images are removed.

## Key Results
- GPT-4o shows strongest visual grounding with 27.9pp accuracy drop (83.2% → 55.3%)
- Other models show minimal drops: 8.5pp (GPT-5-mini), 5.6pp (Claude), 2.4pp (Gemini)
- All models maintained 83-93% accuracy without images, suggesting heavy reliance on textual shortcuts
- Every model fabricated confident visual explanations for non-existent images, indicating inability to distinguish observation from confabulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image substitution reveals genuine visual integration versus textual shortcut exploitation
- Mechanism: Replace diagnostically relevant images with blank placeholders while preserving question text and answer options. Accuracy drops indicate visual dependency; stable accuracy suggests textual shortcut exploitation.
- Core assumption: Models truly processing visual content will show performance degradation when images are removed, while text-reliant models will maintain accuracy.
- Evidence anchors:
  - GPT-4o shows 27.9pp drop vs. others' 2.4-8.5pp drops
  - Models "truly dependent on visual information should show decreased accuracy when diagnostically relevant images are replaced"
  - S-Chain paper addresses visual-textual alignment but not substitution-based grounding

### Mechanism 2
- Claim: Models exploit textual shortcuts by inferring answers from clinical context rather than visual analysis
- Mechanism: Rich clinical vignettes contain sufficient diagnostic information (symptoms, demographics, test descriptions). Models learn to extract patterns from text and treat visual input as secondary.
- Core assumption: Medical questions vary in textual vs. visual diagnostic information; models optimized for benchmark performance may overfit to textual cues.
- Evidence anchors:
  - Models "appeared to select answers first (possibly from textual cues), then construct visual justifications post-hoc"
  - Strong textual reasoning may be "more robust to image quality issues... but risk missing critical visual findings"
  - MOTOR paper addresses retrieval-augmented grounding but not shortcut learning

### Mechanism 3
- Claim: VLMs generate confident hallucinated visual descriptions even when no image is present
- Mechanism: When prompted for detailed reasoning, models generate plausible-sounding visual observations consistent with their selected answer, regardless of whether they processed an image.
- Core assumption: Models lack reliable calibration between visual evidence strength and output confidence; cannot signal "I cannot see this image."
- Evidence anchors:
  - Analysis reveals "confident explanations for fabricated visual interpretations across all models"
  - Gemini "Claims Image A shows dark CSF (T1), Image B shows 'bright signal as evidenced by contrast enhancement'—completely fabricated!"
  - MedHallTune benchmark targets medical hallucination mitigation

## Foundational Learning

- Concept: **Visual grounding** — the degree to which model outputs causally depend on visual input features rather than textual priors or memorized patterns.
  - Why needed here: The entire methodology hinges on distinguishing genuine multimodal integration from superficial performance.
  - Quick check question: If you replaced an image with random noise and the model's answer remained identical, would you conclude it was visually grounded?

- Concept: **Shortcut learning** — models exploiting spurious correlations or easy cues (e.g., dataset artifacts, textual context) instead of learning the intended task.
  - Why needed here: Explains why high benchmark accuracy doesn't guarantee clinical reliability.
  - Quick check question: If a dermatology model diagnosed based on image metadata (hospital name) rather than lesion features, is this shortcut learning or valid inference?

- Concept: **Hallucination calibration** — the mismatch between a model's confidence and the actual evidence supporting its claims.
  - Why needed here: All tested models generated confident but fabricated visual explanations, a critical safety failure.
  - Quick check question: Should a model that cannot see an image (a) refuse to answer, (b) answer with low confidence, or (c) answer confidently while noting uncertainty?

## Architecture Onboarding

- Component map: Input layer (image encoder + text tokenizer) -> Fusion mechanism (cross-attention/concatenation) -> Reasoning generator (autoregressive decoder) -> Evaluation harness (original vs. substituted conditions)

- Critical path:
  1. Select questions explicitly requiring image interpretation (filter for visual dependency)
  2. Run baseline evaluation with correct images, collect accuracy + reasoning traces
  3. Substitute images with blank placeholders, re-run evaluation
  4. Compute per-question accuracy drop (Δ = original - substituted)
  5. Analyze reasoning traces for hallucinated visual features

- Design tradeoffs:
  - Blank image substitution is coarse: a model might process the blank and correctly conclude "no diagnostic information" (which is a form of visual grounding). Adversarial image substitution (wrong pathology) would be a stronger test.
  - 60 questions provides limited statistical power for model comparison; confidence intervals overlap substantially.
  - Italian-only evaluation may not generalize across languages or medical systems.

- Failure signatures:
  - **Low accuracy drop (<10pp)**: Model likely using textual shortcuts; not visually grounded for this task
  - **Fabricated visual details in reasoning**: Model confabulating evidence; cannot distinguish observation from inference
  - **Refusal to answer without image** (GPT-4o behavior): Appropriate safety response but may underperform on text-rich questions where inference is valid
  - **Contradictory visual descriptions for same answer**: Post-hoc rationalization; answer selected first, then justification invented

- First 3 experiments:
  1. **Adversarial image substitution**: Replace correct images with images depicting different pathologies. Does the model detect the mismatch and refuse, or fabricate reasoning consistent with the wrong image?
  2. **Membership inference attack**: Test whether high accuracy without images reflects genuine reasoning or training data memorization (as noted in Limitations).
  3. **Uncertainty calibration probe**: Prompt models to rate confidence in visual observations. Do models report lower confidence for absent vs. present images, even if they still hallucinate?

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the high accuracy maintained without images due to robust textual reasoning or dataset memorization?
- **Basis in paper:** The authors state they "did not perform membership inference attacks" to determine if EuropeMedQA was in the training data, leaving the explanation for high text-only accuracy ambiguous.
- **Why unresolved:** Without distinguishing between reasoning and memorization, it is unclear if models are generalizing or merely recalling specific cases from their pre-training data.
- **What evidence would resolve it:** Conducting membership inference attacks on the models relative to the EuropeMedQA dataset to detect data contamination.

### Open Question 2
- **Question:** How do models behave when incorrect images depict alternative pathologies rather than blank placeholders?
- **Basis in paper:** The authors note that blank image substitution is a "coarse test" and suggest that "more refined adversarial attacks substituting images depicting alternative pathologies would provide stronger evidence."
- **Why unresolved:** Blank placeholders test if a model *needs* an image, but they do not test if the model can detect contradictions when the visual input actively conflicts with the textual query.
- **What evidence would resolve it:** Running the substitution experiment using images containing different medical conditions (mismatched images) rather than empty blanks.

### Open Question 3
- **Question:** What architectural factors determine whether a VLM develops strong visual grounding versus relying on textual shortcuts?
- **Basis in paper:** The conclusion calls for "investigation of the architectural factors underlying these differences in visual grounding" observed between models like GPT-4o and GPT-5-mini.
- **Why unresolved:** The study documents a striking variability in visual dependency (2.4pp vs 27.9pp drops) but does not identify the specific model components or training objectives causing this divergence.
- **What evidence would resolve it:** Comparative ablation studies or analysis of attention mechanisms across different VLM architectures to identify features correlating with high visual dependency.

## Limitations

- Confidence intervals overlap substantially across models (accuracy drop range: 2.4-27.9pp with 60 questions), limiting statistical significance of model comparisons
- Blank image substitution is a coarse test that may not distinguish genuine visual grounding from models recognizing "no information available"
- Italian-only evaluation may not generalize to other languages or medical systems
- Cannot distinguish between textual shortcut learning and memorization of question-answer pairs from training data

## Confidence

- **High confidence**: GPT-4o's superior visual grounding (27.9pp drop represents a substantial and statistically meaningful difference)
- **Medium confidence**: Overall finding that most VLMs rely heavily on textual cues rather than visual understanding (consistent pattern across multiple models)
- **Low confidence**: Ability to distinguish memorization from genuine textual reasoning, and generalizability beyond Italian clinical contexts

## Next Checks

1. Conduct adversarial image substitution: Replace correct medical images with images depicting different pathologies and measure whether models detect the mismatch, refuse to answer, or fabricate reasoning consistent with the wrong image.

2. Design membership inference attack: Test whether high accuracy without images reflects genuine reasoning or training data memorization by identifying questions that appear verbatim in model training corpora.

3. Implement uncertainty calibration probe: Prompt models to explicitly rate confidence in their visual observations (separate from answer confidence) and compare ratings between present and absent image conditions to assess whether models can distinguish observation from inference.