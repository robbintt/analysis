---
ver: rpa2
title: 'Do Large Language Models Know What They Don''t Know? Kalshibench: A New Benchmark
  for Evaluating Epistemic Calibration via Prediction Markets'
arxiv_id: '2512.16030'
source_url: https://arxiv.org/abs/2512.16030
tags:
- calibration
- confidence
- accuracy
- questions
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KalshiBench, a novel benchmark for evaluating
  epistemic calibration of large language models (LLMs) using prediction market questions
  with verifiable real-world outcomes. The key challenge addressed is that traditional
  benchmarks measure accuracy on static knowledge, while KalshiBench tests whether
  models can appropriately quantify uncertainty about genuinely unknown future events
  by using temporally-filtered questions that resolve after model training cutoffs.
---

# Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets

## Quick Facts
- arXiv ID: 2512.16030
- Source URL: https://arxiv.org/abs/2512.16030
- Authors: Lukas Nel
- Reference count: 40
- All models show systematic overconfidence with Expected Calibration Error ranging from 0.120 to 0.395

## Executive Summary
This paper introduces KalshiBench, a novel benchmark for evaluating epistemic calibration of large language models using prediction market questions with verifiable real-world outcomes. The key challenge addressed is that traditional benchmarks measure accuracy on static knowledge, while KalshiBench tests whether models can appropriately quantify uncertainty about genuinely unknown future events. The study evaluates five frontier models on 300 temporally-filtered questions from Kalshi, a CFTC-regulated prediction market, finding systematic overconfidence across all models with only one achieving positive Brier Skill Score.

## Method Summary
The method involves evaluating five frontier models (Claude Opus 4.5, GPT-5.2-XHigh, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2) on 300 prediction market questions from Kalshi. Models are instructed to provide calibrated confidence estimates (0-100) alongside yes/no predictions for questions that resolve after their training cutoffs. The dataset uses temporal filtering to ensure ground truth was inaccessible during training, with questions selected where `t_close(q) > max(cutoff(m))`. Calibration is measured using Brier Score, Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Brier Skill Score (BSS), and Overconfidence Rate at various thresholds.

## Key Results
- Systematic overconfidence across all models, with ECE ranging from 0.120 (Claude) to 0.395 (GPT-5.2-XHigh)
- Only Claude Opus 4.5 achieves positive Brier Skill Score (0.057), meaning most models perform worse than predicting base rates
- At 90%+ confidence levels, models are wrong 15-32% of the time, far exceeding the <10% expected for well-calibrated systems
- Reasoning-enhanced models like GPT-5.2-XHigh show worse calibration despite comparable accuracy and using 26× more tokens

## Why This Works (Mechanism)

### Mechanism 1: Temporal Filtering for Clean Calibration Signal
- Claim: Using prediction market questions that resolve after model training cutoffs isolates genuine uncertainty quantification from memorized confidence patterns.
- Mechanism: Questions are filtered via `t_close(q) > max(cutoff(m))` for all evaluated models, ensuring ground truth was inaccessible during training.
- Core assumption: Models cannot infer future outcomes through indirect pattern recognition (e.g., seasonal trends, recurring events).
- Evidence anchors:
  - [abstract]: "temporally-filtered questions from Kalshi...ensuring outcomes were unknown during model training"
  - [section 3.2]: Equation 1 formalizes the filtering criterion with October 1, 2025 cutoff
  - [corpus]: Weak—neighbor papers don't address temporal contamination in calibration
- Break condition: If models learn exploitable priors (e.g., "incumbents usually win") that approximate future knowledge, temporal filtering becomes insufficient.

### Mechanism 2: Confidence Elicitation Exposes Confidence-Accuracy Gap
- Claim: Instructing models to output numerical confidence (0-100) reveals systematic miscalibration that scales with expressed certainty.
- Mechanism: Binning predictions by confidence level and computing per-bin accuracy yields reliability diagrams showing where gaps emerge.
- Core assumption: Verbalized confidence approximates internal probability estimates rather than being pure surface-level text.
- Evidence anchors:
  - [abstract]: "when models express 90%+ confidence, they are wrong 15-32% of the time"
  - [section 5.3, Table 6]: All models show +24.6% to +62.9% gaps in the 90-100% confidence bin
  - [corpus]: Tian et al. (EMNLP 2023) found verbalized confidence often diverges from token probabilities
- Break condition: If different prompt formats or elicitation methods yield systematically different confidence distributions.

### Mechanism 3: Reasoning Chains Can Reinforce Overconfidence
- Claim: Extended reasoning modes may increase confidence without proportional accuracy gains through confirmation-biased chain generation.
- Mechanism: Models generate arguments supporting initial predictions rather than genuinely updating on uncertainty, inflating confidence.
- Core assumption: The reasoning process itself—not just architecture—drives calibration degradation.
- Evidence anchors:
  - [abstract]: "reasoning-enhanced models like GPT-5.2-XHigh show worse calibration despite comparable accuracy"
  - [section 5.1]: GPT-5.2-XHigh uses 26× more tokens but achieves ECE=0.395 vs Claude's 0.120
  - [corpus]: "Thinking Out Loud" (arXiv 2504.06564) explores reasoning-confidence interactions in LRMs
- Break condition: If the effect is architecture-specific rather than a general property of extended reasoning.

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: Primary metric comparing models; measures average gap between confidence and accuracy across binned predictions.
  - Quick check question: If a model has ECE=0.395, what is the average percentage-point gap between its confidence and accuracy?

- Concept: Brier Skill Score (BSS)
  - Why needed here: Determines whether model predictions beat a naive baseline (predicting the base rate).
  - Quick check question: Why does BSS < 0 indicate the model is worse than uninformed guessing?

- Concept: Reliability Diagrams
  - Why needed here: Visualizes calibration across confidence bins; reveals non-uniform miscalibration patterns.
  - Quick check question: In a reliability diagram, what does it mean when the 90-100% bin plots below the diagonal?

## Architecture Onboarding

- Component map:
  KalshiBench dataset (1,531 questions; 300 temporally-filtered sample) -> Prompt template with structured `<answer>` and `<confidence>` output format -> Metrics pipeline (Brier Score, ECE, MCE, BSS, Overconfidence Rate)

- Critical path:
  1. Apply temporal filter: `t_close(q) > max(all model cutoffs)`
  2. Prompt model with question + description + calibration instruction
  3. Parse structured output for answer and confidence
  4. Bin by confidence (0.1-width), compute accuracy per bin
  5. Calculate ECE, Brier Score, BSS against resolved outcomes

- Design tradeoffs:
  - Self-reported confidence vs logit-based: more interpretable but may not reflect true internal estimates
  - Binary outcomes only: cleaner evaluation but misses continuous calibration challenges
  - Sample size (300): balances power vs cost but limits category-level analysis

- Failure signatures:
  - Confidence clustering at extremes (GPT-5.2-XHigh: 35% of predictions in 90-100% bin)
  - Negative BSS while accuracy appears acceptable
  - Gap widens with confidence level (high-confidence predictions most unreliable)

- First 3 experiments:
  1. Reproduce reliability diagram for your model on the 300-question sample to identify problematic bins.
  2. Vary prompt phrasing for calibration instructions to test sensitivity of confidence distributions.
  3. Compare verbalized confidence vs token logprobs on a subset to validate elicitation method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does extended reasoning worsen rather than improve epistemic calibration in LLMs?
- Basis in paper: [explicit] Section 6.2 discusses this counterintuitive finding, offering hypotheses about "confirmation bias in extended reasoning" and "verbosity without epistemic humility" but provides no empirical validation.
- Why unresolved: The paper demonstrates the effect (GPT-5.2-XHigh shows ECE=0.395 vs simpler models) but only speculates on mechanisms; no ablation studies test whether reasoning chains reinforce initial hypotheses.
- What evidence would resolve it: Ablation experiments manipulating reasoning length independently of confidence, or analysis of whether reasoning chains disproportionately support initial predictions versus considering contrary evidence.

### Open Question 2
- Question: Do calibration-aware training objectives effectively improve epistemic calibration without degrading accuracy?
- Basis in paper: [explicit] Conclusion states: "Future work should explore calibration-aware training objectives, explicit uncertainty modeling architectures, and integration with human forecasting expertise."
- Why unresolved: The paper shows current training fails to develop calibration but does not test alternative training approaches that explicitly penalize miscalibrated confidence.
- What evidence would resolve it: Training models with Brier score or ECE-based loss terms, then evaluating on KalshiBench to assess whether calibration improves without accuracy loss.

### Open Question 3
- Question: Would alternative confidence elicitation methods (betting mechanisms, proper scoring rules) yield better-calibrated predictions than self-reported 0-100 confidence?
- Basis in paper: [explicit] Section 7 (Limitations) states: "Self-reported confidence (0-100) may not reflect internal probability estimates. Alternative elicitation methods (betting, proper scoring rule incentives) might yield different results."
- Why unresolved: The study uses only one elicitation method; models may have unused internal uncertainty representations that different prompting strategies could access.
- What evidence would resolve it: Comparing calibration across multiple elicitation methods on the same questions, using betting frameworks or quadratic scoring incentives.

## Limitations
- Temporal contamination risk: Models may infer future outcomes through indirect patterns despite filtering
- Confidence elicitation validity: Self-reported confidence may not reflect true internal probability estimates
- Model-specific effects: Reasoning chain calibration degradation may be implementation-specific rather than general

## Confidence
- High Confidence: All models show systematic overconfidence with ECE 0.120-0.395
- Medium Confidence: Reasoning-enhanced models show worse calibration despite comparable accuracy
- Medium Confidence: Most models perform worse than predicting base rates (negative Brier Skill Scores)

## Next Checks
1. **Temporal Contamination Test**: Select a subset of KalshiBench questions with clear temporal dependencies and evaluate whether models show systematically better calibration on these versus genuinely novel future events. Compare ECE across question categories.

2. **Confidence Elicitation Validation**: Implement parallel confidence elicitation methods (verbalized 0-100, token logprobs, structured output modes) on a subset of questions and compare resulting calibration metrics.

3. **Reasoning Chain Analysis**: For reasoning-enhanced models showing poor calibration, conduct qualitative analysis of generated reasoning chains to identify confirmation bias patterns. Compare chain content between well-calibrated and poorly-calibrated models.