---
ver: rpa2
title: Comparative Study of UNet-based Architectures for Liver Tumor Segmentation
  in Multi-Phase Contrast-Enhanced Computed Tomography
arxiv_id: '2510.25522'
source_url: https://arxiv.org/abs/2510.25522
tags:
- segmentation
- liver
- tumor
- attention
- cbam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses liver tumor segmentation from multi-phase
  contrast-enhanced CT images using UNet-based architectures. The research compares
  ResNet, Transformer, and State-space (Mamba) backbones, finding that despite theoretical
  advantages of modern architectures in modeling long-range dependencies, ResNet-based
  models demonstrated superior sample efficiency on this limited medical dataset.
---

# Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography

## Quick Facts
- arXiv ID: 2510.25522
- Source URL: https://arxiv.org/abs/2510.25522
- Reference count: 36
- Primary result: ResNet50-based UNet3+ with CBAM attention achieved Dice 0.755, IoU 0.662, and HD95 77.911 on liver tumor segmentation

## Executive Summary
This study compares UNet-based architectures for liver tumor segmentation from multi-phase contrast-enhanced CT images, evaluating ResNet, Transformer, and Mamba backbones. Despite theoretical advantages of modern architectures in modeling long-range dependencies, ResNet-based models demonstrated superior sample efficiency on this limited medical dataset. Attention mechanisms were incorporated to enhance segmentation quality, with CBAM yielding optimal performance. The ResNetUNet3+ with CBAM achieved the highest nominal performance and most precise boundary delineation, though statistical testing showed no significant improvement in mean Dice score compared to baseline. These findings demonstrate that classical ResNet architectures, when enhanced with modern attention modules, provide a robust and statistically comparable alternative to emerging methods.

## Method Summary
The research addresses liver tumor segmentation using multi-phase contrast-enhanced CT images from the Primary Liver Cancer CECT Imaging Dataset (278 cancer + 83 non-cancer cases). After preprocessing to extract 2D slices at 256×256 resolution, the dataset yielded 5335 training, 3180 validation, and 2211 test slices. The study compares ResNet, Transformer, and Mamba backbones within UNet architectures, incorporating attention mechanisms including CBAM. The final model uses ResNet50 encoder with UNet3+ decoder and CBAM attention, trained with combined cross-entropy and Dice loss using SGD optimizer (lr=0.01, momentum=0.9) for 100 epochs on a Tesla T4 GPU.

## Key Results
- ResNetUNet3+ with CBAM achieved highest nominal performance: Dice 0.755, IoU 0.662, HD95 77.911
- No statistically significant improvement in mean Dice score compared to baseline (p > 0.05)
- Model demonstrated greater stability with lower standard deviation and higher specificity (0.926)
- Despite theoretical advantages, Transformer and Mamba backbones showed inferior sample efficiency

## Why This Works (Mechanism)
The study demonstrates that classical ResNet architectures, when enhanced with modern attention modules, provide robust performance for liver tumor segmentation despite limited training data. ResNet's superior sample efficiency on small medical datasets outweighs the theoretical advantages of attention-based architectures in modeling long-range dependencies. The CBAM attention module improves boundary delineation and segmentation stability without requiring the large training datasets typically needed for Transformer and Mamba architectures to perform optimally.

## Foundational Learning
- **Multi-phase CECT imaging**: Different contrast phases (Plain, Arterial, Venous, Delayed) capture tumor characteristics at various vascular stages - essential for comprehensive tumor feature extraction
- **UNet3+ architecture**: Integrates full-scale skip connections from encoder to decoder - needed for precise boundary localization and multi-scale feature fusion
- **Attention mechanisms**: CBAM module applies spatial and channel-wise attention - improves segmentation quality by focusing on relevant features
- **Statistical significance testing**: Wilcoxon signed-rank test used to validate performance improvements - critical for determining practical utility beyond nominal metrics
- **Sample efficiency**: Models' performance relative to training data size - crucial for medical applications with limited annotated datasets
- **HD95 metric**: Maximum distance capturing 95% of segmentation errors - important for evaluating clinical usability of segmentation boundaries

## Architecture Onboarding

**Component Map:**
ResNet50 Encoder -> UNet3+ Decoder -> CBAM Attention Modules -> Segmentation Output

**Critical Path:**
Input CT slices → ResNet50 feature extraction → Multi-scale feature fusion via UNet3+ skip connections → CBAM attention enhancement → Pixel-wise classification → Binary tumor mask output

**Design Tradeoffs:**
- ResNet50 chosen over Transformer/Mamba for superior sample efficiency on limited medical data
- CBAM added to improve boundary delineation without significantly increasing computational cost
- Combined CE+Dice loss balances classification accuracy with overlap metrics
- Per-patient data splitting prevents information leakage between train/val/test sets

**Failure Signatures:**
- Near-zero Dice on extremely small tumors (<50 pixels)
- Under-segmentation of irregular/non-circular tumors
- High variance across runs (std ~0.27) indicating model instability

**First Experiments:**
1. Train baseline UNet3+ without attention modules to establish reference performance
2. Implement and evaluate CBAM attention at different decoder positions
3. Compare ResNet50 performance against Transformer and Mamba backbones with identical decoder architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Statistical testing showed no significant improvement in mean Dice score compared to baseline (p > 0.05)
- High standard deviation (~0.27) in Dice scores indicates substantial model instability
- Limited by dataset size, with potential overfitting on the specific CECT imaging protocol used

## Confidence
- **High Confidence**: ResNet50 as final backbone choice; CBAM integration improving stability and specificity; dataset characteristics and preprocessing pipeline
- **Medium Confidence**: Overall segmentation performance metrics; attention mechanism contribution; HD95 superiority claims
- **Low Confidence**: Statistical significance of improvements; generalization across tumor types and sizes; practical clinical utility given instability

## Next Checks
1. Conduct 5-fold cross-validation with multiple random seeds to establish robust performance distribution and variance characteristics
2. Perform lesion size-stratified analysis to quantify performance on small tumors (<50 pixels) where failure rates are highest
3. Implement ablation study removing CBAM attention to isolate its contribution to stability versus raw segmentation accuracy