---
ver: rpa2
title: '2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving
  Using Vision Language Model'
arxiv_id: '2509.02659'
source_url: https://arxiv.org/abs/2509.02659
tags:
- driving
- arxiv
- end-to-end
- autonomous
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the 2nd place solution for the CVPR2024 E2E
  Challenge, demonstrating that combining end-to-end architectural design with multi-modality
  Vision Language Models (VLMs) yields impressive performance on autonomous driving
  tasks. The proposed method uses a single front-facing camera and achieves a final
  score of 0.8747, making it the best camera-only solution on the leaderboard.
---

# 2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model

## Quick Facts
- arXiv ID: 2509.02659
- Source URL: https://arxiv.org/abs/2509.02659
- Authors: Zilong Guo; Yi Luo; Long Sha; Dongxu Wang; Panqu Wang; Chenyang Xu; Yi Yang
- Reference count: 26
- Primary result: 2nd place solution with 0.8747 final score, best camera-only approach

## Executive Summary
This paper presents the 2nd place solution for the CVPR2024 E2E Challenge, demonstrating that combining end-to-end architectural design with multi-modality Vision Language Models (VLMs) yields impressive performance on autonomous driving tasks. The proposed method uses a single front-facing camera and achieves a final score of 0.8747, making it the best camera-only solution on the leaderboard. The approach employs a single LLM (Intern-LM 4B) that processes raw camera images, ego history, navigation commands, and text prompts to generate trajectories and driving-related text outputs simultaneously. The model handles various driving scenarios including straight driving, turns, traffic lights, stop signs, and corner cases like toll booths, demonstrating human-like behavior in stop-and-go conditions. The results suggest strong generalization potential and highlight the effectiveness of vision-based driving approaches for end-to-end autonomous driving tasks.

## Method Summary
The approach processes raw camera images, ego history (location, velocity, acceleration), navigation commands, and text prompts through a unified architecture. A Vision Transformer encodes camera images into visual tokens, while an MLP encodes ego history vectors. Text tokenizers process navigation commands and text prompts. All representations flow into Intern-LM 4B, which generates trajectory coordinates and text autoregressively from a unified hidden state. The model is trained with LoRA fine-tuning on nuPlan trajectory data using L2 loss and driving language datasets using next-token prediction loss. The trajectory output is decoded via MLP from the LLM's final tokens, while text output is generated simultaneously. The architecture aims to replicate human driving by using visual perception, historical context, and navigational understanding to plan trajectories.

## Key Results
- Achieved 2nd place on CVPR2024 E2E Challenge leaderboard with final score of 0.8747
- Best camera-only solution among all submissions, outperforming multi-sensor approaches
- Demonstrated human-like behavior in stop-and-go scenarios and complex situations like toll booths
- Successfully handles various driving scenarios including straight driving, turns, traffic lights, and stop signs

## Why This Works (Mechanism)

### Mechanism 1: Unified Multi-Modal Token Fusion in Shared LLM Space
Processing visual, historical, navigation, and instructional inputs through a shared LLM enables emergent reasoning about driving contexts that single-modality approaches may miss. The Vision Transformer encodes camera images into visual tokens; MLP encodes ego history; tokenizers process navigation commands and text prompts. All representations flow into Intern-LM 4B, which generates trajectory coordinates and text autoregressively from a unified hidden state. The pre-trained VLM's multi-modal alignment transfers to driving-specific spatial-temporal reasoning, and language understanding provides inductive bias beneficial for trajectory planning. If visual encoder fails to capture depth or 3D structure, trajectory quality degrades in complex spatial scenarios.

### Mechanism 2: Simultaneous Trajectory-Language Output Training
Jointly training trajectory prediction (L2 loss) and text generation (next-token prediction) provides mutual regularization that improves generalization. The LLM generates trajectory tokens first (decoded via MLP to location/velocity/acceleration), then text tokens. During training, both nuPlan trajectory data and driving language datasets provide supervision, creating shared representations that couple physical planning with semantic understanding. Driving language datasets contain knowledge transferable to trajectory planning, and dual-objective training does't create optimization conflicts. If trajectory and text tasks compete for model capacity, or if language dataset domain differs significantly from driving scenarios, joint training may underperform separate models.

### Mechanism 3: Single-Frame End-to-End Optimization Without Modular Error Propagation
Eliminating modular decomposition (perception → prediction → planning) avoids cascading errors and enables direct optimization of driving behavior from pixels to trajectory. A single LLM receives all inputs and produces trajectory outputs without intermediate representations. L2 loss directly penalizes trajectory error, allowing gradients to flow from planning objective through visual encoder. The nuPlan dataset provides sufficient diversity for the model to learn implicit perception and prediction without explicit supervision, and single-frame input captures adequate temporal context via ego history vectors. If scenarios require explicit intermediate reasoning, the black-box nature becomes a liability.

## Foundational Learning

- Concept: Vision Transformers (ViT) for image tokenization
  - Why needed here: The paper uses ViT to convert raw camera images into tokens processable by the LLM. Without understanding patch embedding and attention-based visual feature extraction, the encoder design is opaque.
  - Quick check question: Can you explain how a 224×224 image becomes a sequence of tokens in a standard ViT?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: Training uses LoRA rather than full fine-tuning of the 4B parameter Intern-LM. Understanding low-rank adapter injection is critical for reproduction.
  - Quick check question: What are the trainable parameters in LoRA, and how do they relate to the original weight matrices?

- Concept: Autoregressive token generation in LLMs
  - Why needed here: The model generates both trajectory and text tokens autoregressively. Understanding causal masking and sequential token prediction is essential for debugging output generation.
  - Quick check question: How does an autoregressive model condition on previously generated tokens during inference?

## Architecture Onboarding

- Component map:
```
Front Camera Image → ViT Encoder → Visual Tokens
Ego History (loc/vel/acc) → MLP Encoder → History Tokens
Navigation Command → Text Tokenizer → Nav Tokens
Text Prompt → Text Tokenizer → Prompt Tokens
                    ↓
              [Concatenated Token Sequence]
                    ↓
            Intern-LM 4B (with LoRA adapters)
                    ↓
         Autoregressive Generation
         /                    \
   Trajectory Tokens      Text Tokens
        ↓                      ↓
   MLP Decoder           Text Output
   (location, vel, acc)
        ↓
   Trajectory Output
```

- Critical path: Camera image → ViT → LLM hidden states → trajectory token generation → MLP decoder → final waypoints. If visual features are weak, downstream trajectory quality fails.

- Design tradeoffs:
  - Single camera vs. multi-camera: Simpler but loses surround perception; authors acknowledge this as a limitation
  - Single-frame vs. temporal: Relies on ego history vectors for temporal context rather than frame stacking; may miss motion cues
  - 4B vs. larger LLM: Chosen for computational efficiency; capacity constraints may limit reasoning complexity
  - LoRA vs. full fine-tuning: Reduces memory but may underfit compared to full parameter updates

- Failure signatures:
  - Model extrapolates history linearly instead of planning (test with stop-and-go scenarios as authors did)
  - Text output contradicts trajectory (indicates representation decoupling)
  - Degraded performance in scenarios underrepresented in nuPlan (toll booths mentioned as corner case)
  - Open-loop validation success but closed-loop failure (authors explicitly note this limitation)

- First 3 experiments:
  1. **Ablate text output training**: Train trajectory-only model (remove language dataset, disable text generation) and compare score to validate whether joint training provides benefit.
  2. **Input perturbation robustness**: Mask or corrupt navigation commands during inference to test whether model relies on them vs. visual cues alone.
  3. **History length sensitivity**: Vary ego history window (e.g., 1s, 3s, 5s) to determine minimum temporal context needed for stable planning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the high performance observed in the open-loop evaluation translate effectively to closed-loop, real-world driving scenarios?
- Basis in paper: The authors state, "the open-loop setting may not reveal the real-world or close-loop performance, and the corresponding testing environment should also be developed."
- Why unresolved: The current challenge leaderboard relies on open-loop metrics which do not account for distributional shift caused by the model's own control actions in a feedback loop.
- What evidence would resolve it: Benchmarking the same model architecture in a high-fidelity closed-loop simulator or real-world vehicle testing.

### Open Question 2
- Question: How can the framework be adapted to integrate multi-sensor and multi-frame inputs without destabilizing the LLM training paradigm?
- Basis in paper: The authors note that the "single-frame single-camera approach is quite primitive" and that integrating multi-sensor settings "may require careful design for the visual encoder, 3D spatial representation, and LLM training paradigm."
- Why unresolved: The current solution relies on a simple architecture that processes a single front-facing image; adding temporal or surround-view data increases token count and complexity significantly.
- What evidence would resolve it: A study demonstrating stable training and improved metric scores when temporal history or multi-camera features are injected into the LLM context.

### Open Question 3
- Question: What is the marginal utility of the Vision Language Model components versus the ego history encoders in driving performance?
- Basis in paper: The authors explicitly state, "Due to time and resource limitation, we did not conduct comprehensive ablation studies."
- Why unresolved: It is unclear if the model is primarily learning to extrapolate based on the "ego history" MLP inputs or if the VLM is providing semantic reasoning that drives the score improvement.
- What evidence would resolve it: Ablation experiments removing the image or text inputs while retaining ego history to isolate the contribution of the VLM's "world knowledge."

## Limitations
- Single-camera input design may miss critical surround perception cues necessary for complex driving scenarios
- Only evaluated in open-loop validation mode, leaving uncertainty about real-world closed-loop performance
- Joint trajectory-language training mechanism lacks direct validation through ablation studies

## Confidence

**High Confidence** (Multiple lines of evidence, direct validation):
- Achieves 2nd place score of 0.8747 on CVPR2024 E2E Challenge leaderboard
- Architecture design (ViT + MLP encoders feeding into LLM) is clearly specified and follows established patterns
- Use of LoRA fine-tuning is a standard, well-documented technique

**Medium Confidence** (Plausible mechanisms, indirect evidence):
- Single-frame input with ego history provides sufficient temporal context for trajectory planning
- Language dataset training improves trajectory planning through mutual regularization
- Human-like stop-and-go behavior is demonstrated qualitatively but not quantitatively measured

**Low Confidence** (Hypothesis-driven, minimal validation):
- Unified LLM architecture avoids cumulative errors of modular perception-prediction-planning systems
- Model handles toll booths and other corner cases as "human-like behavior"
- Generalization potential claim is asserted but not validated on out-of-distribution datasets

## Next Checks

1. **Closed-loop performance validation**: Implement the model in CARLA or similar simulator to test actual vehicle execution of predicted trajectories. Compare closed-loop metrics against the open-loop validation score to quantify the gap between prediction and execution performance.

2. **Ablation of joint training**: Train two separate models—one with only trajectory output and one with both outputs as described in the paper. Compare their leaderboard scores and trajectory quality metrics to determine whether joint training actually provides mutual regularization benefits.

3. **Temporal input architecture comparison**: Implement a variant using frame stacking instead of ego history vectors, and compare performance. Test different history window lengths (1s, 3s, 5s) to identify the optimal temporal context for stable trajectory planning.