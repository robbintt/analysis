---
ver: rpa2
title: Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks
arxiv_id: '2510.25623'
source_url: https://arxiv.org/abs/2510.25623
tags:
- legal
- dvts
- arxiv
- best-of-n
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates whether test-time scaling methods can improve\
  \ legal reasoning in large language models. Three verification strategies\u2014\
  Majority Vote, Best-of-N, and Diverse Verifier Tree Search\u2014were tested on five\
  \ legal benchmarks using multiple generator and verifier models."
---

# Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks

## Quick Facts
- arXiv ID: 2510.25623
- Source URL: https://arxiv.org/abs/2510.25623
- Reference count: 11
- This study evaluates whether test-time scaling methods can improve legal reasoning in large language models.

## Executive Summary
This study investigates whether test-time scaling through verification methods can improve legal reasoning in large language models. Three verification strategies—Majority Vote, Best-of-N, and Diverse Verifier Tree Search—were tested on five legal benchmarks using multiple generator and verifier models. The results reveal that verification methods rarely outperform simple majority voting, with gains diminishing as generator capability increases. Domain specialization and larger verifier models provide measurable improvements, particularly for smaller generators. Notably, process reward models proved versatile, outperforming outcome reward models even when applied outside their intended role. Verification methods excel only in high-cardinality tasks, while majority voting remains robust for simpler, low-complexity problems.

## Method Summary
The study compares three test-time scaling methods on five legal MCQA benchmarks: COLIEE Task 4, LEXam, SuperGPQA Law subset, LEXam-32, and the restricted MBE BAR Exam. Methods include Majority Vote (sampling k responses and selecting most frequent answer), Best-of-N (sampling N responses and scoring with an ORM), and Diverse Verifier Tree Search (DVTS using PRM-guided tree search). Generators tested include Llama-3.2-3B, Llama-3.1-8B, and Llama-3.1-70B with temperature T=0.8 and Chain-of-Thought prompting. Verifiers include general-purpose models (Skywork-Reward, VersaPRM, Qwen2.5-Math-PRM) and proprietary legal domain ORMs. DVTS uses expansion width W=2 with Mean aggregation for VersaPRM and Last for QwenPRMs.

## Key Results
- Verification methods rarely outperform simple majority voting across legal reasoning tasks
- Gains from verification diminish as generator capability increases
- Domain-specialized verifiers provide measurable improvements, especially at larger scales
- Process reward models outperform outcome reward models when used outside their intended role
- Verification excels only in high-cardinality tasks (32 options), while majority voting dominates simpler problems

## Why This Works (Mechanism)

### Mechanism 1: Verifier-Guided Selection in High-Cardinality Tasks
Verification methods provide significant performance gains over simple Majority Vote, but this advantage is conditional on the task having a high-cardinality output space. MV relies on answer frequency and struggles when the correct answer is not sampled often enough to form a majority, or when many plausible incorrect answers disperse the vote. Verifier-based methods use a reward model to score the reasoning path or outcome, explicitly identifying the highest-quality candidate even if it appears infrequently. This trend is starkly reversed in the high-complexity LEXam-32 task, where DVTS achieves a substantial relative gain of +12.4%. The mechanism fails if the verifier lacks domain-specific training to reliably score reasoning, and gains also diminish or disappear with larger generators.

### Mechanism 2: Process Reward Models for Robust Outcome Verification
Process Reward Models, trained to score reasoning steps, can be effectively repurposed as outcome verifiers and often outperform Outcome Reward Models of similar size in this role. PRMs receive fine-grained, step-by-step supervision, which may build a more robust representation of reasoning quality that transfers to scoring a final answer. ORMs, trained only on outcomes, may be more easily fooled by plausible-sounding but flawed conclusions. PRMs provide consistent benefits: as BoN scorers they yield stronger reranking than size-matched ORMs. However, benefits are concentrated on smaller generators, and a large, domain-specific ORM can outperform a smaller PRM.

### Mechanism 3: Diminishing Returns with Increasing Generator Capability
The relative performance gain from verification methods decreases as the underlying generator model becomes more capable. A more capable generator has higher base accuracy, making the correct answer more likely to be the majority vote. This reduces the marginal benefit of a verifier, as the generator's own reasoning is already sufficiently reliable. For larger models, verification provides no performance benefit or even shows a decrease compared to MV. This trend might reverse for tasks of extreme difficulty where even highly capable generators struggle significantly.

## Foundational Learning

- **Concept: Outcome Reward Model (ORM) vs. Process Reward Model (PRM)**
  - Why needed here: The paper's RQ3 directly compares these two supervision types. Understanding that ORMs score final outputs while PRMs score individual reasoning steps is essential for interpreting why PRMs showed superior versatility.
  - Quick check question: If you wanted to rerank a set of complete legal arguments, would an ORM or a PRM be the standard choice, and what does the paper suggest about using a PRM for this task instead?

- **Concept: Test-Time Scaling (TTS)**
  - Why needed here: This is the core paradigm of the paper. Understanding that TTS trades additional compute at inference time for improved accuracy is the foundation for evaluating the worth of verification methods.
  - Quick check question: What is the core trade-off made by all Test-Time Scaling methods, and what are the two main resources being balanced?

- **Concept: Answer Cardinality**
  - Why needed here: A key finding is that the effectiveness of verification is dependent on the cardinality of the answer space. This concept is central to the paper's explanation of when verifiers provide value over simple voting.
  - Quick check question: On a task with 4 possible answers versus one with 32, which baseline method is expected to struggle more, and which verification strategy is expected to provide greater relative gains?

## Architecture Onboarding

- Component map: Generator -> Verifier/Reward Model -> Selection Algorithm -> Final Answer
- Critical path: The generator produces N candidates. The verifier scores each candidate's reasoning/outcome. The selection algorithm uses these scores (or vote counts) to select the single best answer. The paper measures if the verifier-guided path yields better accuracy than the voting path.
- Design tradeoffs:
  - Compute vs. Accuracy: TTS methods require more compute (generating N samples). The paper shows the accuracy gain may be small or negative for capable generators, making the trade-off often unfavorable.
  - Simplicity vs. Complexity: Majority Vote is a trivial algorithm. DVTS is complex, requiring a PRM and tree search hyperparameters (expansion width). The paper suggests MV is often "good enough" except for high-cardinality tasks.
  - General vs. Specialized Verifier: A general verifier is cheaper to obtain (off-the-shelf). A specialized legal verifier requires training data but provides measurable improvement, especially at larger scales (70B).
- Failure signatures:
  - Negative Gain: Verification methods (BoN, DVTS) yield lower accuracy than simple MV. This was observed with larger generators (70B) on low-cardinality tasks.
  - Flat Performance: Increasing the compute budget (N) provides no meaningful accuracy improvement, suggesting the verifier is not providing useful signal.
  - Domain Mismatch: A general-purpose verifier provides little to no benefit over MV on legal reasoning tasks.
- First 3 experiments:
  1. Establish a Majority Vote baseline across target benchmarks with your chosen generator and compute budget (N=4, 8, 16).
  2. Implement Best-of-N selection using a strong, off-the-shelf general-purpose verifier (e.g., Skywork-RM) and compare its performance against the MV baseline.
  3. Implement Diverse Verifier Tree Search (DVTS) using a multi-domain PRM (e.g., VersaPRM) and compare its performance on a task with a large answer space (e.g., 32 options) to test the high-cardinality hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do verifier-based TTS methods provide comparable gains for open-ended legal QA and summarization tasks as observed in MCQA?
- Basis in paper: The authors state "our findings may not generalize to other legal tasks such as summarization or open-ended QA where verification is arguably more complex" and explicitly call for future work to "explore additional legal domains and open QA."
- Why unresolved: MCQA has discrete answer spaces amenable to voting and verification; open-ended tasks lack this structure, making verification fundamentally different.
- What evidence would resolve it: Experiments applying Best-of-N and DVTS to open-ended legal benchmarks (e.g., case summarization, memo generation) with appropriate free-text evaluation metrics.

### Open Question 2
- Question: What is the precise relationship between answer-space cardinality and the crossover point where verifier-based methods outperform majority voting?
- Basis in paper: The authors note "This relationship warrants further investigation, including in Open QA settings" when discussing how MV degrades as the solution space expands from 2 to 32 options.
- Why unresolved: Only four cardinalities (2, 4, 8, 32) were tested; the functional form and threshold of this transition remains uncharacterized.
- What evidence would resolve it: Systematic evaluation across a denser cardinality spectrum with curve-fitting to identify the performance crossover point.

### Open Question 3
- Question: Do verification gains and their diminishing returns with generator capability generalize across model architectures beyond the Llama family?
- Basis in paper: The authors note "our experiments primarily focused on a single model family (i.e. Llama 3.1 and Llama 3.2), and other model architectures might exhibit different improvements from verification."
- Why unresolved: Architectural differences in reasoning patterns, training corpora, and tokenization could alter how verification interacts with generation quality.
- What evidence would resolve it: Replication using generators from different families (e.g., Qwen, Mistral, Gemma) at comparable parameter scales.

### Open Question 4
- Question: Why do PRMs outperform ORMs even when applied outside their intended role for outcome-level reranking?
- Basis in paper: The authors observe this phenomenon and hypothesize that "step-by-step feedback helps them develop a more robust measure of reasoning quality," but provide no empirical validation of this mechanism.
- Why unresolved: The paper demonstrates the empirical result without investigating whether denser supervision, implicit CoT evaluation, or other factors drive this effect.
- What evidence would resolve it: Probing analyses comparing attention patterns and feature representations between PRMs and ORMs when scoring complete outputs.

## Limitations
- Proprietary legal-domain reward models prevent full replication of the highest-performance results, particularly the 70B ORM comparisons.
- The restricted-access MBE BAR Exam benchmark limits generalizability to the full set of legal reasoning tasks.
- DVTS implementation details reference external work without full specification, potentially affecting reproducibility of the most complex method.
- Results are based on multiple-choice question answering; applicability to open-ended legal reasoning remains untested.

## Confidence
- High confidence: Verification methods show diminishing returns with increasing generator capability; majority voting remains competitive on low-cardinality tasks; domain specialization provides measurable improvements.
- Medium confidence: Process reward models outperform outcome reward models when used outside their intended role; verification excels specifically in high-cardinality tasks.
- Low confidence: The exact magnitude of gains from domain-specialized verifiers at different scales, due to proprietary model limitations.

## Next Checks
1. Implement a simplified DVTS variant using only publicly available PRMs to verify the high-cardinality hypothesis without proprietary components.
2. Conduct ablation studies comparing PRMs and ORMs on outcome verification tasks using identical model sizes to quantify the cross-role performance difference.
3. Test verification methods on an open-ended legal reasoning task (not MCQA) to assess generalizability beyond multiple-choice formats.