---
ver: rpa2
title: 'FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation'
arxiv_id: '2502.00870'
source_url: https://arxiv.org/abs/2502.00870
tags:
- fedhpd
- agents
- policy
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles federated reinforcement learning (FedRL) with
  heterogeneous agents, each using different policy networks and training configurations
  without sharing internal details. The challenge lies in enabling effective knowledge
  sharing while preserving privacy and handling model diversity.
---

# FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation

## Quick Facts
- **arXiv ID**: 2502.00870
- **Source URL**: https://arxiv.org/abs/2502.00870
- **Reference count**: 40
- **Primary result**: FedHPD enables privacy-preserving knowledge sharing among heterogeneous RL agents without model parameter exchange, achieving up to 39% improvement over independent training.

## Executive Summary
FedHPD addresses federated reinforcement learning with heterogeneous agents by enabling knowledge sharing through policy distillation without revealing model parameters. The approach extracts action probability distributions from local policies, aggregates them into a global consensus, and uses KL divergence regularization to align heterogeneous policies. This allows agents with different network architectures and configurations to benefit from federation while maintaining privacy. Theoretical analysis proves convergence under standard assumptions, with faster convergence achieved by reducing gradient variance. Experiments across Cartpole, LunarLander, and InvertedPendulum tasks demonstrate consistent performance improvements over independent training, even with varying distillation intervals.

## Method Summary
FedHPD operates through periodic knowledge distillation where heterogeneous agents share action probability distributions rather than model parameters. Each agent computes $\pi_{\theta_k}(a|S_p)$ on a shared public state set $S_p$, uploads these distributions to a server, which aggregates them into a global consensus. Agents then minimize KL divergence between their local distributions and the consensus while performing local REINFORCE updates. The method handles heterogeneity by operating on architecture-agnostic distributions, requires no curated public datasets (using synthetically generated state sets), and achieves variance reduction in gradients under specific correlation conditions between reward maximization and consensus alignment.

## Key Results
- FedHPD improves system-level average rewards by up to 39% compared to independent training across multiple RL tasks
- The method maintains effectiveness with varying distillation intervals (d), showing graceful degradation as communication frequency decreases
- FedHPD outperforms related approaches like DPA-FedRL in sample efficiency and stability without requiring public datasets
- Individual agent performance improves even with heterogeneous network architectures and learning rates

## Why This Works (Mechanism)

### Mechanism 1: Action Probability Distribution as Knowledge Medium
Heterogeneous agents share knowledge by exchanging action probability distributions rather than model parameters. Each agent computes $\pi_{\theta_k}(a|S_p)$—the probability distribution over actions given a shared public state set. These distributions are architecture-agnostic, enabling black-box agents with different network structures and configurations to communicate without revealing internal model details. The server aggregates these into a global consensus $P_{i+1} = \frac{1}{K}\sum_{k=1}^{K} P_{k}^{i+1}$. The action probability distribution captures sufficient policy knowledge for transfer across heterogeneous architectures.

### Mechanism 2: KL Divergence Regularization for Knowledge Digestion
Minimizing KL divergence between local and global distributions reduces gradient variance and accelerates convergence. The objective function becomes $J'(\theta_k) = J(\theta_k) - \lambda D_{KL}(\pi_{\theta_k}(a|S_p) || \pi_{global}(a|S_p))$. The KL term penalizes deviation from the global consensus while the policy gradient term maximizes expected return. Under specific conditions, the covariance between gradients reduces overall variance: $Var[\nabla_\theta J'(\theta)] < Var[\nabla_\theta J(\theta)]$ when gradients are sufficiently correlated.

### Mechanism 3: Periodic Distillation Interval Control
Adjusting the distillation interval $d$ trades off communication efficiency against convergence speed. Agents perform $d$ local update rounds between distillation steps. Smaller $d$ increases knowledge-sharing frequency, keeping local policies closer to consensus, which makes the variance reduction condition easier to satisfy. Larger $d$ reduces communication overhead but allows policy divergence. As $d$ increases, system performance gradually decreases, approaching independent training performance.

## Foundational Learning

- **Concept: Policy Gradient Methods (REINFORCE)**
  - Why needed here: FedHPD builds on REINFORCE for local training; understanding $\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau)]$ is essential to grasp how KL regularization modifies the update.
  - Quick check question: Can you explain why policy gradient methods optimize the expected return directly without requiring a value function?

- **Concept: Knowledge Distillation and KL Divergence**
  - Why needed here: The core innovation applies KD to FedRL; understanding $D_{KL}(p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)}$ and how it measures distributional difference is critical.
  - Quick check question: Why does KL divergence provide a different signal than cross-entropy loss, and when would you prefer one over the other?

- **Concept: Markov Decision Process Formulation**
  - Why needed here: The problem is framed as distributed MDPs $\mathcal{M}_k = \{S, A, P_k, R_k, \gamma, \rho\}$; distinguishing between shared state/action spaces and agent-specific transition/reward structures clarifies what can and cannot be aggregated.
  - Quick check question: In a federated setting, which MDP components can be meaningfully shared across agents, and which must remain local?

## Architecture Onboarding

- **Component map**: Virtual Agent / Public State Generator -> Local Training Loop -> Distillation Uploader -> Server Aggregator -> Knowledge Digestion -> Local Training Loop
- **Critical path**: Public state generation → local training → periodic distribution upload → server aggregation → broadcast → KL-based local update. The distillation interval $d$ governs synchronization frequency.
- **Design tradeoffs**:
  - Smaller $d$ → faster convergence, higher communication cost
  - Larger public state set $|S_p|$ → more representative distillation, higher upload bandwidth
  - Aggregation by simple averaging → robust baseline, but may be degraded by poorly-performing agents; weighted aggregation could improve quality
- **Failure signatures**:
  - Individual agent performance degradation at large $d$ — indicates consensus quality decay
  - No improvement over NoFed — check if public state set is task-relevant or if agent configurations are too dissimilar
  - Divergence during distillation — verify KL regularization coefficient $\lambda$ and learning rate satisfy smoothness conditions
- **First 3 experiments**:
  1. Run FedHPD with $d=5$ on Cartpole with 10 heterogeneous agents; compare system average reward to NoFed. Expected: ~39% improvement.
  2. Sweep $d \in \{2, 5, 10, 20, 40, 80\}$ on a single task; plot system performance vs. $d$ to reproduce performance degradation curve.
  3. Test with 3 different public state sets of equal size; verify performance is insensitive to set selection.

## Open Questions the Paper Calls Out

- **Question**: Can FedHPD provide specific finite-time sample complexity bounds under agent heterogeneity, beyond the general convergence proof provided?
- **Question**: Does implementing quality-aware aggregation weights, rather than simple averaging, improve performance in scenarios with extreme agent inconsistency?
- **Question**: Can FedHPD maintain convergence stability when utilizing off-policy or Actor-Critic algorithms instead of vanilla on-policy REINFORCE for local training?

## Limitations
- The paper does not explicitly address how FedHPD handles catastrophic forgetting during periodic distillation or whether the public state set $S_p$ adequately covers the exploration distribution of heterogeneous agents.
- No ablation studies on KL coefficient $\lambda$ or aggregation methods beyond simple averaging are provided.
- The convergence proof assumes bounded rewards and Lipschitz-smooth functions, but empirical validation of these conditions in practice is not shown.

## Confidence
- **High Confidence**: The core mechanism of using action probability distributions as a knowledge-sharing medium and the basic experimental methodology.
- **Medium Confidence**: The theoretical variance reduction claims (Corollary 1) and the specific architectural configurations of heterogeneous agents.
- **Low Confidence**: The robustness of FedHPD to different public state set selections and the exact implementation details for continuous action spaces.

## Next Checks
1. Compute the angle between reward gradient $\nabla_\theta J(\theta)$ and KL gradient $\nabla_\theta D_{KL}$ during training to verify the variance reduction condition (Eq. 17) holds empirically.
2. Measure the overlap between the public state set $S_p$ and the states visited by heterogeneous agents during training to quantify representation quality.
3. Systematically test $d \in \{2, 5, 10, 20, 40, 80\}$ on all three tasks to confirm the observed performance degradation trend is consistent across environments.