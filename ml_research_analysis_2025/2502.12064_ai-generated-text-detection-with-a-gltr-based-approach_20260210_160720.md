---
ver: rpa2
title: AI-generated Text Detection with a GLTR-based Approach
arxiv_id: '2502.12064'
source_url: https://arxiv.org/abs/2502.12064
tags:
- texts
- generated
- text
- gltr
- gpt-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of a GLTR-based approach with GPT-2
  models for detecting AI-generated text in the context of the IberLEF-AuTexTification
  2023 shared task. The method extends GLTR by classifying text based on the proportion
  of highly predictable (green) words, using various threshold values to optimize
  detection performance.
---

# AI-generated Text Detection with a GLTR-based Approach

## Quick Facts
- arXiv ID: 2502.12064
- Source URL: https://arxiv.org/abs/2502.12064
- Reference count: 9
- Primary result: GPT-2 small with 2/3 threshold achieved 80.19% macro F1 for English, ranking 2nd in shared task

## Executive Summary
This study presents a GLTR-based approach for detecting AI-generated text using GPT-2 models in the IberLEF-AuTexTification 2023 shared task. The method classifies text based on the proportion of highly predictable (green) words, using various threshold values to optimize detection performance. Experiments on English and Spanish datasets containing human-written and machine-generated texts across multiple domains showed effective AI-generated text detection, particularly for English, with macro F1-scores of 80.19% and 66.20% respectively.

## Method Summary
The approach extends GLTR by classifying text based on the proportion of highly predictable (top-10) tokens, using various threshold values to optimize detection performance. For each word position, GPT-2 computes next-token probability distributions and marks words as "green" if they appear in the top-10 predictions. The proportion of green words per document becomes the classification signal—higher proportions indicate machine authorship. The method tests multiple thresholds (1/4 to 5/6) and finds 2/3 optimal for English with GPT-2 small and Spanish with GPT-2 XL. No model fine-tuning is performed; pretrained GPT-2 models are used for inference only.

## Key Results
- GPT-2 small with threshold 2/3 achieved macro F1-score of 80.19% for English detection
- GPT-2 XL achieved macro F1-score of 66.20% for Spanish detection
- Model size effects were asymmetric: GPT-2 small outperformed larger models for English, while GPT-2 XL outperformed smaller models for Spanish
- Fixed threshold of 2/3 balanced precision and recall best across tested values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Texts with higher proportions of highly predictable (top-10) tokens are more likely machine-generated
- Mechanism: GPT-2 computes next-token probability distributions for each position. If the actual token falls within the top-10 predicted tokens, it is marked "green." The proportion of green tokens per document becomes the classification signal—higher proportions indicate machine authorship
- Core assumption: LLMs sample from their own probability distributions, producing more high-probability tokens than humans, who select words less predictably

### Mechanism 2
- Claim: A threshold of ~2/3 green-token proportion maximizes macro F1 for binary classification
- Mechanism: The green proportion is compared to a fixed threshold. If proportion > threshold → machine-generated; else → human. Thresholds were swept (1/4 to 5/6); 2/3 balanced precision and recall best for English GPT-2 small
- Core assumption: A single global threshold generalizes across domains and languages

### Mechanism 3
- Claim: Model size and language alignment affect detection performance asymmetrically
- Mechanism: For English, smaller GPT-2 (124M) outperformed larger variants. For Spanish, GPT-2 XL (1.5B, English-trained) outperformed gpt2-small-spanish (124M, Spanish-trained), suggesting scale may compensate for language mismatch
- Core assumption: Detection-model and generator-model distributions should align; misalignment hurts performance

## Foundational Learning

- Concept: **Autoregressive language modeling (next-token prediction)**
  - Why needed here: GLTR relies on GPT-2's next-token probability distributions to compute token predictability
  - Quick check question: Given "The cat sat on the ___," can you explain how a language model assigns probabilities to candidate next tokens?

- Concept: **Threshold-based binary classification and macro F1**
  - Why needed here: The method converts a continuous proportion (green tokens) to a binary label using a threshold, evaluated via macro F1
  - Quick check question: Why does macro F1 balance performance across classes better than accuracy when classes are imbalanced?

- Concept: **Tokenization and vocabulary alignment**
  - Why needed here: GPT-2's tokenizer determines how text is split; mismatched tokenization between detector and generator affects green-token counts
  - Quick check question: If a Spanish text is tokenized with an English-centric tokenizer, what artifacts might appear in the green-token distribution?

## Architecture Onboarding

- Component map: Text -> Tokenizer (GPT-2 BPE) -> GPT-2 model -> Green-token counter -> Threshold classifier -> Label
- Critical path: Text → Tokenize → GPT-2 forward pass → Extract top-10 per position → Count matches → Compute proportion → Threshold → Label
- Design tradeoffs:
  - Model size vs. speed: GPT-2 small is faster but may underperform on non-English text; XL is slower but may compensate for language mismatch
  - Fixed vs. adaptive thresholds: Fixed is simpler but may not generalize across domains/generators
  - Language-specific vs. multilingual models: Spanish-specific fine-tuning underperformed in this study, suggesting scale may matter more than language alignment
- Failure signatures:
  - High false positives on formulaic human text (legal, news)
  - High false negatives on AI text deliberately perturbed to use low-probability tokens
  - Significant performance drop when detector model architecture differs from generator (e.g., detecting GPT-4 text with GPT-2)
- First 3 experiments:
  1. **Baseline replication**: Re-implement green-token counting with GPT-2 small on the English AuTexTification test split; verify macro F1 ~80% at threshold 2/3
  2. **Threshold sensitivity**: Sweep thresholds (0.5–0.8) on a held-out validation set; plot macro F1 vs. threshold to confirm robustness
  3. **Cross-generator test**: Apply the GPT-2-based detector to texts generated by a different model (e.g., BLOOM or a newer LLM); measure performance drop to quantify generator-dependence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the GLTR-based approach be effectively extended to the multiclass classification task of model attribution (identifying the specific LLM used)?
- Basis in paper: [explicit] The authors explicitly list plans to "tackle... the second subtask about model attribution, which is a multiclass classification problem."
- Why unresolved: The current study restricted its scope to binary classification (human vs. machine) and did not test the method's ability to distinguish between different generator models
- What evidence would resolve it: Experimental results from applying the GLTR-based features to datasets labeled with specific source models (e.g., BLOOM vs. GPT-3)

### Open Question 2
- Question: How can the proposed method be adapted to detect AI-generated content in multimodal settings?
- Basis in paper: [explicit] The conclusion states, "we also plan to investigate the detection of AI-generated content in a multimodal setting."
- Why unresolved: The current methodology relies exclusively on textual analysis using GPT-2 token probabilities and does not account for images, audio, or other data modalities
- What evidence would resolve it: A modified framework that integrates visual or audio feature extraction with the existing text-based GLTR analysis, evaluated on a multimodal dataset

### Open Question 3
- Question: Does training the GPT-2 XL model specifically on Spanish data yield better detection performance than the cross-lingual application of the English model?
- Basis in paper: [explicit] The authors note that while GPT-2 XL performed best for Spanish, it was not trained on Spanish data, leading to a plan for "training the gpt2-xl model with Spanish data."
- Why unresolved: It is unclear if the superior performance of the English XL model on Spanish text was due to model capacity or cross-lingual transfer, and if language-specific training would close the performance gap with English
- What evidence would resolve it: A comparative study evaluating the current GPT-2 XL baseline against a Spanish fine-tuned version of GPT-2 XL on the same test set

### Open Question 4
- Question: Is the GLTR-based approach generalizable to the diverse set of languages in the Iberian Peninsula (Catalan, Basque, Galician, Portuguese) introduced in IberAuTexTification 2024?
- Basis in paper: [explicit] The authors list plans to "tackle the multilingual task for languages from the Iberian Peninsula presented on the IberAuTexTification 2024 task."
- Why unresolved: The study only validated the approach on English and Spanish, leaving the method's efficacy on the other specific languages unconfirmed
- What evidence would resolve it: Macro F1-scores obtained by applying the method to the datasets provided in the 2024 shared task for the remaining languages

## Limitations

- Tokenization ambiguity: Unclear whether "green words" are counted at subword token level or word level, affecting reproducibility across languages
- Language mismatch findings: Counterintuitive result that English-trained GPT-2 XL outperformed Spanish-specific model for Spanish text detection, with no clear explanation for this mechanism
- Domain generalization: Results only validated on IberLEF-AuTexTification 2023 dataset; performance on other domains or generator types unknown

## Confidence

**High confidence**: The GLTR-based green-token proportion method works for binary AI-generated text detection on the specific IberLEF-AuTexTification 2023 English and Spanish datasets

**Medium confidence**: The finding that GPT-2 XL outperformed the Spanish-specific model for Spanish text detection, though the mechanism remains unexplained

**Low confidence**: The universal applicability of fixed threshold 2/3 across languages and domains, as this assumption is untested and contradicted by related work

## Next Checks

1. **Tokenization granularity validation**: Reproduce the method with explicit clarification: count green tokens at the subword level versus the word level. Compare macro F1 scores to quantify the impact of this choice on English and Spanish performance

2. **Cross-generator robustness test**: Apply the GPT-2-based detector to texts generated by a different LLM (e.g., BLOOM or ChatGPT). Measure the drop in macro F1 to quantify generator-dependence and test the paper's implicit assumption that GPT-2's probability distributions are representative

3. **Threshold adaptation validation**: Implement an adaptive threshold strategy (e.g., per-domain or per-generator) and compare macro F1 to the fixed threshold 2/3. This directly tests the paper's assumption about threshold universality and addresses critiques from related work on group-adaptive thresholds