---
ver: rpa2
title: 'Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics'
arxiv_id: '2503.01174'
source_url: https://arxiv.org/abs/2503.01174
tags:
- turn-taking
- when
- speaker
- audio
- turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first comprehensive evaluation framework
  for assessing audio foundation models' (FMs) turn-taking capabilities in human-AI
  conversations. The authors propose a novel evaluation protocol using a supervised
  judge model trained on human-human conversations to assess timing and quality of
  turn-taking decisions.
---

# Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics

## Quick Facts
- arXiv ID: 2503.01174
- Source URL: https://arxiv.org/abs/2503.01174
- Reference count: 40
- Primary result: Introduces first comprehensive evaluation framework for audio foundation models' turn-taking capabilities using a supervised judge model trained on human conversations

## Executive Summary
This work introduces the first comprehensive evaluation framework for assessing audio foundation models' (FMs) turn-taking capabilities in human-AI conversations. The authors propose a novel evaluation protocol using a supervised judge model trained on human-human conversations to assess timing and quality of turn-taking decisions. Their approach includes automated metrics covering five core conversation capabilities: when to speak up, when to backchannel, when to interrupt, conveying floor willingness, and handling user interruptions. User studies reveal key limitations of existing systems: Moshi interrupts too aggressively, rarely backchannels, and fails to convey floor control; cascaded systems have high latency and minimal interactivity. Evaluation of multiple audio FMs on curated benchmarks shows significant room for improvement, with open-source models performing close to random baselines on turn-taking prediction tasks. The work highlights that current audio FMs struggle with understanding and predicting turn-taking events, despite claims of conversational capabilities. The authors plan to release their evaluation platform to facilitate future research in developing more natural and interactive conversational AI systems.

## Method Summary
The authors propose a novel evaluation protocol that uses a supervised model as a judge to assess spoken dialog systems' turn-taking capabilities. The judge model is trained on human-human conversations (Switchboard) to predict turn-taking events at 40ms granularity using a 30-second causal context window. The framework evaluates five core capabilities: knowing when to speak up, when to backchannel, when to interrupt, conveying turn willingness, and handling interruptions. For each capability, the judge model predicts whether an AI system's action aligns with what a human would do in the same conversational context. The evaluation uses automated metrics computed from the judge model's predictions, comparing AI decisions against the judge's labels to calculate agreement scores. The approach aims to capture not just what events occur, but whether they happen at the right timing based on acoustic and prosodic cues.

## Key Results
- Moshi interrupts too aggressively (24.2% agreement with judge labels vs. 85.0% for humans), rarely backchannels (0.01% occurrence), and fails to convey floor control
- Cascaded systems show high latency and minimal interactivity, rarely interrupting or backchanneling
- Open-source audio FMs perform close to random baselines on turn-taking prediction tasks
- Judge model achieves 92.0 ROC-AUC on Switchboard test set and generalizes to out-of-domain datasets (91.5 on Columbia Games, 91.0 on Fisher)
- Agreement between judge labels and human judgments exceeds 80% for most metrics on both in-domain and out-of-domain datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A supervised model trained on human-human conversations can serve as a proxy for human relevance judgments when evaluating AI turn-taking behavior.
- Mechanism: The judge model learns temporal patterns and acoustic cues that precede turn-taking events from labeled human-human conversations (Switchboard), then applies these learned patterns to score AI systems' decisions in human-AI conversations. The model predicts turn-taking events at 40ms granularity based on a 30-second context window, enabling fine-grained timing evaluation.
- Core assumption: Patterns learned from human-human conversations transfer to evaluating human-AI conversations; the judge's predictions align with what humans would judge as appropriate.
- Evidence anchors:
  - [abstract] "we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations"
  - [Section 4.1] Table 1 shows the judge model achieves 92.0 ROC-AUC on Switchboard test set and generalizes to OOD datasets (91.5 on Columbia Games, 91.0 on Fisher)
  - [Section 4.3] Figure 3 shows judge labels achieve >80% agreement with human judgments for most metrics on both in-domain and OOD datasets
  - [corpus] Related work on turn-taking prediction (VAP models, TurnGPT) demonstrates that supervised learning from conversational data can capture turn-taking patterns, supporting the feasibility of this approach
- Break condition: If the judge model's agreement with human judgments falls below ~60% consistently across metrics, or if it fails to generalize to different conversation domains or languages, the proxy evaluation becomes unreliable.

### Mechanism 2
- Claim: Precise temporal alignment of turn-taking events is critical for natural conversation flow and cannot be captured by corpus-level statistics alone.
- Mechanism: Corpus-level statistics (e.g., average gap duration, overlap percentage) aggregate over entire conversations, losing local timing information. The timing-centric approach predicts turn-taking events at 40ms chunks using a causal model that conditions only on past context, enabling evaluation of whether the AI system acts at the *right moment* based on acoustic and prosodic cues.
- Core assumption: Timing precision at the chunk level correlates with perceived naturalness; a system that produces the right type of event but at the wrong time still fails.
- Evidence anchors:
  - [Section 3.2] Figure 2 shows Moshi has gap duration (11.8%) similar to humans, but Section 4.4 reveals it often fails to speak up at appropriate moments (52.8% agreement vs. 92.8% for humans)
  - [Section 4.6] Figure 3c shows while both humans and Moshi produce interruptions, Moshi's interruptions have only 65.5% agreement with judge labels vs. 85.0% for humans, indicating timing inappropriateness
  - [Section 3] "While these metrics capture how well the global turn-taking events distribution in the generated dialogue matches with ground truth, it cannot evaluate the exact timing when a turn-taking event happens"
  - [corpus] Weak corpus evidence - few papers explicitly compare timing-centric vs. aggregate metrics for turn-taking evaluation; this appears to be a novel contribution
- Break condition: If timing precision doesn't correlate with human perception of naturalness (e.g., systems with good timing scores but poor subjective ratings), the approach fails to capture what matters.

### Mechanism 3
- Claim: Natural conversational ability requires at least five distinct capabilities that must be evaluated independently: (a) knowing when to speak up, (b) when to backchannel, (c) when to interrupt, (d) conveying turn willingness, and (e) handling interruptions.
- Mechanism: The evaluation framework decomposes conversational competence into these sub-skills, each with its own metric computed from the judge model's predictions. This enables identifying specific weaknesses (e.g., Moshi rarely backchannels, cascaded systems rarely interrupt) that aggregate metrics would obscure.
- Core assumption: These five capabilities are both necessary and approximately sufficient for natural conversation; improving one doesn't necessarily improve others.
- Evidence anchors:
  - [Section 4.2] Identifies capabilities based on prior literature review (Gravano & Hirschberg, 2011; Skantze, 2021; Raux et al., 2006)
  - [Section 4.4-4.8] Figure 3 and Table 2 show different AI systems have different capability profiles: Moshi interrupts too aggressively (24.2% agreement), cascaded system rarely backchannels (0.01% occurrence), both fail to convey turn willingness
  - [Section 4.9] Confusion matrices (Figure 4) reveal distinct error patterns for each system across event types
  - [corpus] Prior work on multimodal turn-taking (Gravano & Hirschberg, 2011; Lee & Narayanan, 2010) supports the decomposition into specific event types
- Break condition: If improving one capability consistently degrades another (revealing a fundamental tradeoff), or if systems with good scores on all five capabilities still feel unnatural to users, the decomposition is incomplete.

## Foundational Learning

- Concept: **Turn-taking dynamics in human conversation**
  - Why needed here: The entire evaluation framework builds on understanding how humans manage conversational turns through inter-pausal units (IPUs), gaps, pauses, overlaps, backchannels, and interruptions. Without this foundation, you cannot interpret what the metrics measure or why they matter.
  - Quick check question: Can you explain why silence within a speaker's turn (pause) tends to be longer than silence between speakers (gap), and why this matters for AI turn-taking?

- Concept: **Sequence prediction with causal constraints**
  - Why needed here: The judge model must predict turn-taking events causally—conditioning only on past audio context—to simulate real-time decision-making. Understanding how to train and evaluate causal sequence models is essential for implementing the evaluation protocol.
  - Quick check question: Why can't the judge model use bidirectional context when evaluating AI systems that must make real-time turn-taking decisions?

- Concept: **Threshold-based binary classification with class imbalance**
  - Why needed here: The evaluation metrics rely on thresholding the judge model's predicted likelihoods to generate pseudo ground-truth labels. With heavily imbalanced classes (continuation and silence dominate >95% of chunks), naive argmax fails and threshold tuning becomes critical.
  - Quick check question: If continuation events occur 70% of the time in your training data, what would happen if you use argmax on the model's softmax outputs to predict events?

## Architecture Onboarding

- Component map:
  Human-AI conversation audio → VAD + ASR → Event detection (IPU, Gap, Overlap, Backchannel, Interruption)
                                      ↓
  Judge model (Whisper encoder + linear classifier) → Turn-taking event predictions per 40ms chunk
                                      ↓
  Metric computation (5 capabilities × pairwise comparison) → Agreement scores with judge labels
                                      ↓
  Aggregation → Per-session statistics with confidence intervals

- Critical path:
  1. **Data preparation**: Collect human-AI conversations; run VAD (pyannote) to get voice activity; run ASR (Whisper) for transcripts; label backchannels using filler word heuristics; identify IPU boundaries and silence regions
  2. **Judge model inference**: For each 40ms chunk, extract 30-second context window; pass through Whisper medium encoder; apply linear classifier to get 5-class probabilities (C, BC, T, I, NA)
  3. **Metric computation**: For each capability, identify relevant chunks (e.g., when user pauses → metric a); compare AI decision against judge label; compute agreement rate
  4. **Threshold tuning**: On validation set, tune thresholds for each metric to maximize judge-human agreement (Table 6 shows validation agreement rates)

- Design tradeoffs:
  - **Chunk size (40ms)**: Smaller chunks increase temporal precision but may be noisier; larger chunks lose fine-grained timing
  - **Context window (30s)**: Longer windows capture more conversational context but increase latency and computational cost
  - **Judge model architecture**: Using Whisper encoder provides strong acoustic representations but requires more compute than lightweight alternatives
  - **Backchannel detection**: Heuristic-based (filler words) may miss non-verbal backchannels (e.g., "mm-hmm" variations); more sophisticated detection would improve label quality
  - **Single-channel input**: Simplifies data collection but may lose spatial cues; dual-channel would better handle overlapping speech

- Failure signatures:
  - **Low judge-human agreement** (<60%): Indicates judge model hasn't learned the right patterns; check training data quality, model capacity, or whether human judgments are inconsistent
  - **High variance across sessions**: Suggests metrics are sensitive to conversation topics or user behavior; may need topic stratification or larger sample sizes
  - **Near-zero backchannel detection**: Current heuristic may be too restrictive; inspect ASR output for missed fillers
  - **Random-level performance on OOD datasets**: Judge model overfit to training domain; consider multi-domain training or domain adaptation

- First 3 experiments:
  1. **Validate judge model generalization**: Train on Switchboard, evaluate agreement with human judgments on Columbia Games and Fisher corpora. If agreement drops significantly (>10%), the model may not generalize well to your target domain.
  2. **Establish baseline with simple metrics**: Evaluate a target AI system using both corpus-level statistics (Figure 2) and your timing-centric metrics (Figure 3). The discrepancy reveals how much timing matters for your specific system.
  3. **Diagnostic analysis of failure modes**: Use confusion matrices (Figure 4) to identify which event types your target AI system mishandles most. If backchannels are rare (Table 2, 0.01%), this indicates a fundamental capability gap rather than a timing issue.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the supervised "judge" model's agreement with human relevance judgments be improved beyond the current 60-80% range, particularly for complex events like interruptions?
- Basis in paper: [explicit] The authors state, "The agreement between the judge labels and human judgments is below 80% for many metrics," and identify "Future efforts to improve model accuracy" as a necessary step for protocol reliability.
- Why unresolved: The current model uses heuristics and binary thresholds that may fail to capture the nuanced, context-dependent nature of human turn-taking decisions.
- What evidence would resolve it: A comparative study showing a modified judge architecture (e.g., incorporating prosody or larger context windows) achieving statistically significantly higher alignment with human annotators on the same test sets.

### Open Question 2
- Question: To what extent does the reliance on heuristics for identifying backchannels (specifically frequent filler words) bias the evaluation against systems that use non-lexical or prosodic cues?
- Basis in paper: [explicit] The paper acknowledges, "Our current approach for identifying backchannels relies on heuristics (Sec. A.4) and may miss certain backchannels which is limitation of our model."
- Why unresolved: The current methodology risks false negatives where a system provides valid feedback that does not match the pre-defined list of filler phrases, leading to inaccurate penalty in metrics.
- What evidence would resolve it: An analysis of the "missed" backchannels identified by human reviewers but missed by the heuristic, followed by a model update that incorporates these instances to see if system rankings change.

### Open Question 3
- Question: Would increasing the EPAD logit bias during user pauses effectively mitigate the false positive interruptions and missed turn-changes observed in models like Moshi?
- Basis in paper: [explicit] The authors hypothesize in Section 4.4 and 4.9 that "Increasing the EPAD logit bias when the user pauses... could improve Moshi's turn-taking capabilities," but they do not experimentally validate this intervention.
- Why unresolved: While the architectural mechanism (EPAD tokens) is known, the precise tuning of logit biases to balance responsiveness against interruption aggression remains an empirical gap.
- What evidence would resolve it: An ablation study varying logit bias values during inference and measuring the resulting change in the "When to speak up?" and "When to interrupt?" metric scores.

## Limitations
- Judge model reliability: High agreement on human-human data but uncertain transfer to evaluating AI systems' turn-taking decisions
- Evaluation completeness: Metrics focus on timing and event prediction rather than content quality during turn-taking events
- Generalizability: Findings based on limited number of systems tested, may not extend to more sophisticated models or different domains

## Confidence

- **High confidence**: The evaluation framework's technical implementation (judge model architecture, metric computation, threshold tuning methodology) is well-specified and reproducible. The empirical findings about Moshi's aggressive interruptions and cascaded system's latency issues are directly observable from the data.

- **Medium confidence**: The claim that current audio FMs struggle with turn-taking prediction is supported by the evaluation results, but the conclusion may be overly broad given the limited number of systems tested. The judge model's agreement with human judgments provides reasonable proxy validation, but the exact alignment between judge scores and perceived conversational quality remains uncertain.

- **Low confidence**: The assertion that the five-turn-taking capabilities are sufficient for natural conversation is based on prior literature rather than empirical validation within this work. The paper does not test whether systems that score well on all five metrics actually produce more natural interactions according to human raters.

## Next Checks

1. **Cross-system validation**: Evaluate the judge model on human-AI conversations from additional systems beyond Moshi and the cascaded system, particularly state-of-the-art models like GPT-4o or Gemini Live. Compare agreement rates across systems to determine if the observed patterns (low backchanneling, aggressive interruption) are systematic or system-specific.

2. **Human judgment correlation study**: Conduct user studies where human raters evaluate the same conversations used for judge model scoring, focusing on turn-taking naturalness and appropriateness. Compute correlation between judge agreement scores and subjective human ratings to validate whether the automated metrics capture what matters for conversational quality.

3. **Domain adaptation assessment**: Train the judge model on a combination of human-human and human-AI conversations, then evaluate both human-human and human-AI conversations. Compare performance to the original model to assess whether domain adaptation improves the judge's ability to evaluate AI systems accurately.