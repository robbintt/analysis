---
ver: rpa2
title: High-Fidelity Speech Enhancement via Discrete Audio Tokens
arxiv_id: '2510.02187'
source_url: https://arxiv.org/abs/2510.02187
tags:
- speech
- arxiv
- enhancement
- audio
- dac-se1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAC-SE1, a language model-based speech enhancement
  framework that operates directly on high-resolution 44.1 kHz discrete audio tokens.
  Unlike prior methods that rely on multi-stage pipelines or low sampling rates, DAC-SE1
  uses a single-stage transformer model based on the LLaMA architecture, processing
  flattened DAC tokens without auxiliary encoders.
---

# High-Fidelity Speech Enhancement via Discrete Audio Tokens

## Quick Facts
- arXiv ID: 2510.02187
- Source URL: https://arxiv.org/abs/2510.02187
- Reference count: 0
- High-fidelity speech enhancement at 44.1 kHz using a single-stage transformer on flattened discrete audio tokens

## Executive Summary
This paper introduces DAC-SE1, a language model-based speech enhancement framework that operates directly on high-resolution 44.1 kHz discrete audio tokens. Unlike prior methods that rely on multi-stage pipelines or low sampling rates, DAC-SE1 uses a single-stage transformer model based on the LLaMA architecture, processing flattened DAC tokens without auxiliary encoders. The model is trained on a diverse dataset covering various distortions and uses a two-stage training strategy to handle uneven loss scales across tasks. Experiments show that DAC-SE1 outperforms state-of-the-art autoregressive speech enhancement methods on objective metrics such as DNSMOS, PESQ, and SpeechBERTScore, as well as in MUSHRA human evaluations. The approach demonstrates that high-fidelity speech enhancement can be achieved through scaling laws without domain-specific architectural modifications.

## Method Summary
DAC-SE1 processes 44.1 kHz audio through a DAC encoder (9 residual codebooks at 86 Hz frame rate), flattens these into a single time-major token sequence (~774 tokens/second), and feeds them into a 1B parameter LLaMA-based causal transformer. The model is trained autoregressively to map noisy token sequences to clean token sequences separated by a special boundary token. A two-stage training strategy is employed: Stage 1 performs multi-task training on mixed distortions, followed by Stage 2 iterative fine-tuning on each task-specific dataset to address uneven gradient contributions. The model uses RoPE positional embeddings with θ=100,000 and an 8192-token context window.

## Key Results
- DAC-SE1 achieves state-of-the-art performance on DNSMOS, PESQ, and SpeechBERTScore metrics
- Human evaluations (MUSHRA) show consistent preference over LLaSE-G1 and VoiceFixer
- Single-stage transformer architecture processes flattened DAC tokens without auxiliary encoders
- Two-stage training strategy effectively handles uneven loss scales across distortion tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Processing high-resolution 44.1 kHz audio via flattened discrete tokens enables a standard causal transformer to perform high-fidelity speech enhancement without domain-specific architectural modifications.
- Mechanism: The DAC codec encodes 44.1 kHz audio into 9 residual codebooks at 86 Hz frame rate. These are flattened into a single time-major token sequence (~774 tokens/second). A 1B parameter LLaMA-based causal transformer with RoPE positional embeddings (θ=100,000) and 8192-token context window is trained autoregressively to map noisy token sequences to clean token sequences separated by a special boundary token.
- Core assumption: Standard causal transformers can capture both fine-grained acoustic structure and long-range dependencies in flattened RVQ tokens when scaled appropriately, without requiring hierarchical modeling or auxiliary encoders.
- Evidence anchors:
  - [abstract] "DAC-SE1 uses a single-stage transformer model based on the LLaMA architecture, processing flattened DAC tokens without auxiliary encoders."
  - [Section 3.1] "We simplify the design by flattening all codebooks into a single time-major token sequence... This strategy reduces architectural complexity and aligns with standard LM training pipelines."
  - [Section 3.2] "Following insights from large language models, this design ensures that our model can capture both fine-grained acoustic structure and long-range token-structure dependencies."

### Mechanism 2
- Claim: High sampling rate (44.1 kHz) discrete representations preserve fine acoustic details that directly improve perceptual quality metrics and human evaluations.
- Mechanism: DAC codec captures broadband frequency content at 44.1 kHz, with residual codebooks encoding coarse structure in early layers and fine acoustic details in later layers. The model learns to reconstruct full-bandwidth speech including high-frequency components that 16 kHz methods cannot represent.
- Core assumption: High-fidelity speech enhancement requires access to high-frequency information unavailable in 16 kHz codecs; discrete tokens can faithfully encode this information.
- Evidence anchors:
  - [abstract] "DAC-SE1 preserves fine-grained acoustic details while maintaining semantic coherence."
  - [Section 2.2] "DAC [15] is one such codec, providing discrete representations with fidelity close to uncompressed signals, which makes it particularly suitable for speech enhancement in high-fidelity settings."
  - [Section 4.1] "Human listeners consistently preferred the outputs of our model over both LLaSE-G1 and VoiceFixer."

### Mechanism 3
- Claim: Two-stage training strategy (multi-task pretraining → per-task fine-tuning) addresses uneven gradient contributions across distortion types, enabling robust multi-task generalization.
- Mechanism: Stage 1 performs standard multi-task training on mixed distortions (noise, reverb, downsampling, packet loss). Stage 2 fine-tunes the same model iteratively on each task-specific dataset. This prevents tasks with low loss scales (e.g., packet loss, where few tokens differ) from being dominated by high-loss-scale tasks during joint optimization.
- Core assumption: Uneven loss scales across tasks cause poor convergence in naive joint training; sequential fine-tuning allows task-specific optimization without requiring separate models.
- Evidence anchors:
  - [Section 3.3] "A key challenge arises from the varying loss scales per task... As a result, the gradient contribution per task is uneven, which can cause joint training on all tasks to generalize poorly. To address this, we adopt a two-stage training strategy."
  - [Section 3.3] "Importantly, this does not require separate models per task; the same model is iteratively fine-tuned on each task."

## Foundational Learning

- Concept: **Residual Vector Quantization (RVQ)**
  - Why needed here: Core representation used by DAC codec; understanding hierarchical codebook structure (coarse→fine) is essential for grasping what flattened tokens encode.
  - Quick check question: In RVQ, what information is captured by the first codebook versus the 9th codebook, and why does flattening preserve this hierarchy?

- Concept: **Autoregressive Language Modeling (Causal Transformer Decoder)**
  - Why needed here: DAC-SE1 frames enhancement as next-token prediction; the model conditions on all prior tokens to predict clean speech sequentially.
  - Quick check question: How does the `start-clean` boundary token enable the model to distinguish between conditioning (noisy) and target (clean) portions of the sequence?

- Concept: **Scaling Laws in Deep Learning**
  - Why needed here: Paper's central claim is that high-fidelity SE emerges from scale alone, without domain-specific architecture changes.
  - Quick check question: What does the paper's evidence suggest about the relationship between model size (1B parameters), training data volume (5B tokens), and enhancement quality?

## Architecture Onboarding

- Component map:
  Raw audio -> DAC encoder (44.1 kHz, 9 codebooks, 86 Hz) -> Flatten -> Token sequence -> Transformer -> Predicted clean tokens -> DAC decoder -> 44.1 kHz waveform

- Critical path:
  Transformer's ability to model long-range dependencies in 774 tokens/second flattened sequences. Performance hinges on: (1) sufficient model capacity, (2) RoPE stability at 8192-token context, (3) two-stage training to balance multi-task learning.

- Design tradeoffs:
  - **Flattened vs. hierarchical tokens**: Simpler architecture, but 9× longer sequences → higher compute
  - **Single-stage vs. multi-stage pipeline**: Scalable and unified, but potentially lower peak performance than specialized pipelines
  - **44.1 kHz vs. 16 kHz**: Higher fidelity but requires DAC codec and longer token sequences

- Failure signatures:
  - **Oscillating/divergent loss**: Check per-task loss curves; may indicate training strategy issues
  - **Muffled output**: Model may not be utilizing later codebooks; verify DAC reconstruction independently
  - **Task-specific regression**: Fine-tuning may cause forgetting; monitor held-out task performance during Stage 2
  - **Hallucinated artifacts**: Spectral distortion in output; check Figure 1 comparison showing DAC-SE1 avoids this

- First 3 experiments:
  1. **Codebook ablation**: Train on first 3 codebooks only vs. all 9 to quantify contribution of fine acoustic details to PESQ/DNSMOS.
  2. **Training strategy ablation**: Compare Stage-1-only (joint multi-task) vs. full two-stage training on held-out test set with per-task breakdown.
  3. **Scale sweep**: Train 125M, 350M, 1B variants on identical data to validate scaling hypothesis and identify minimum viable model size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can autoregressive language model frameworks be optimized to consistently surpass specialized non-autoregressive models in background suppression (BAK) metrics?
- Basis in paper: [explicit] The authors state in the results analysis that "VoiceFixer performs slightly better in background suppression (BAK)" compared to DAC-SE1, despite DAC-SE1 achieving superior overall quality (OVRL) and MUSHRA scores.
- Why unresolved: The current model prioritizes a "balanced improvement" across dimensions, but the results indicate that the generative approach may not suppress background noise as aggressively as discriminative or specialized restoration models.
- What evidence would resolve it: A future iteration of the model demonstrating statistically significant improvements in BAK scores over VoiceFixer on the HiFiTTS-2 test set without degrading speech quality (SIG).

### Open Question 2
- Question: Does the flattened token strategy impose efficiency or context-window limits that hierarchical modeling could alleviate?
- Basis in paper: [explicit] The methodology section notes that the authors simplify the design by flattening codebooks, but explicitly admits this comes "at the expense of longer token sequences."
- Why unresolved: While the paper argues that scaling laws allow causal LMs to handle longer contexts, it does not quantify if this linear sequence expansion negatively impacts computational efficiency or the modeling of ultra-long-range dependencies compared to hierarchical token strategies.
- What evidence would resolve it: A comparative ablation study measuring inference latency and performance degradation on long-form audio clips (> 30 seconds) between the flattened approach and hierarchical codebook prediction methods.

### Open Question 3
- Question: Is the two-stage training strategy strictly necessary for generalization, or can a unified loss balancing approach achieve equivalent performance?
- Basis in paper: [inferred] The authors identify "varying loss scales per task" (e.g., between packet loss and noise suppression) as a "key challenge" that necessitates a complex two-stage training pipeline (multi-task pre-training followed by per-task fine-tuning).
- Why unresolved: The paper does not explore if the uneven gradient contributions could be managed via dynamic loss weighting or curriculum learning in a single-stage setup, which would simplify the training pipeline significantly.
- What evidence would resolve it: Demonstrating that a single model trained with a dynamic loss balancing technique (e.g., GradNorm or uncertainty weighting) can match the performance of the two-stage model on all distortion tasks.

## Limitations

- No empirical validation that DAC codec reconstruction introduces no artifacts that limit enhancement quality
- Lack of ablation studies comparing two-stage training against alternatives like loss weighting or curriculum learning
- No systematic robustness analysis for out-of-distribution distortions or cross-domain scenarios

## Confidence

**High Confidence**:
- DAC-SE1 achieves state-of-the-art performance on tested objective metrics and MUSHRA evaluations
- Single-stage transformer architecture can process flattened DAC tokens without auxiliary encoders
- High sampling rate (44.1 kHz) provides measurable quality improvements over 16 kHz approaches

**Medium Confidence**:
- Two-stage training strategy is necessary for optimal multi-task performance
- Scaling to 1B parameters is sufficient for high-fidelity enhancement without architectural modifications
- The model's causal transformer decoder captures both fine-grained acoustic structure and long-range dependencies

**Low Confidence**:
- DAC-SE1 will generalize to arbitrary unseen distortions without additional training
- The approach scales predictably beyond 1B parameters to further improve quality
- The model will maintain performance on speech durations exceeding 8192 tokens without architectural changes

## Next Checks

1. **Codebook Contribution Analysis**: Systematically ablate codebooks during both training and inference (e.g., train on first 3 codebooks only vs. all 9) to quantify the marginal contribution of fine acoustic details to objective metrics and perceptual quality.

2. **Training Strategy Ablation**: Compare the proposed two-stage training against alternative approaches: (a) naive multi-task training with loss weighting, (b) curriculum learning that gradually introduces tasks, and (c) sequential fine-tuning with explicit regularization to prevent catastrophic forgetting.

3. **Robustness and OOD Testing**: Evaluate DAC-SE1 on challenging out-of-distribution scenarios including: (a) cross-lingual speech enhancement, (b) extremely long utterances (>30 seconds), (c) combined distortions not seen during training, and (d) real-world recordings with background music or overlapping speech.