---
ver: rpa2
title: 'AgentMath: Empowering Mathematical Reasoning for Large Language Models via
  Tool-Augmented Agent'
arxiv_id: '2512.20745'
source_url: https://arxiv.org/abs/2512.20745
tags:
- zhang
- code
- arxiv
- reasoning
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentMath, a tool-augmented agent framework
  that integrates large language models with code interpreters to improve mathematical
  reasoning. It addresses the inefficiency and inaccuracy of text-only reasoning on
  complex math problems by dynamically interleaving natural language reasoning with
  real-time code execution.
---

# AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent

## Quick Facts
- arXiv ID: 2512.20745
- Source URL: https://arxiv.org/abs/2512.20745
- Reference count: 40
- State-of-the-art performance on AIME24 (90.6%), AIME25 (86.4%), and HMMT25 (73.8%) benchmarks.

## Executive Summary
AgentMath introduces a tool-augmented agent framework that significantly improves mathematical reasoning in large language models by integrating real-time code execution with natural language reasoning. The approach addresses the inefficiency of text-only reasoning on complex math problems by dynamically interleaving reasoning with tool calls. Through automated synthesis of tool-augmented training data and agentic reinforcement learning with interleaved reasoning and execution, AgentMath achieves state-of-the-art results on major mathematical competition benchmarks. The framework demonstrates that augmenting language models with executable tools, combined with specialized training techniques, can substantially enhance mathematical problem-solving capabilities.

## Method Summary
AgentMath employs a two-stage training approach: first, supervised fine-tuning (SFT) on synthetic tool-augmented trajectories generated using a teacher model (DeepSeek-V3) that converts text-based chain-of-thought into executable code sequences; second, multi-stage reinforcement learning with GRPO that progressively increases sequence and tool-call budgets while optimizing for accuracy and tool efficiency. The framework features an asynchronous training system capable of handling ultra-long sequences (up to 96K tokens and 96 tool calls) through partial rollouts and request-level parallelism. The training pipeline includes automated data synthesis with code injection, selective feedback masking during SFT, and composite reward functions during RL that balance problem-solving accuracy with computational efficiency.

## Key Results
- AgentMath-30B-A3B achieves state-of-the-art 90.6% accuracy on AIME24 benchmark
- Outperforms comparable models with 86.4% on AIME25 and 73.8% on HMMT25
- Significantly surpasses text-only reasoning approaches and rivals larger proprietary systems

## Why This Works (Mechanism)
AgentMath works by combining the reasoning capabilities of large language models with the precision of code execution. By interleaving natural language reasoning with real-time tool calls, the system can handle complex mathematical problems that require both symbolic manipulation and numerical computation. The asynchronous training system enables efficient learning from ultra-long sequences by processing partial rollouts in parallel, while the progressive RL stages gradually increase the model's capacity to handle longer and more complex problems. The tool-augmented approach allows the model to offload computationally intensive operations to reliable code execution while maintaining the flexibility of language-based reasoning for problem understanding and strategy formulation.

## Foundational Learning

**Tool-Augmented Reasoning**: Combining natural language reasoning with executable tools (code interpreters) to handle complex mathematical operations. Why needed: Pure text reasoning struggles with complex calculations and symbolic manipulations. Quick check: Can the model generate valid code for mathematical operations and execute it correctly.

**Asynchronous Training**: Processing partial rollouts in parallel to handle ultra-long sequences efficiently. Why needed: Traditional sequential training becomes prohibitively slow with long sequences containing many tool calls. Quick check: Training throughput remains stable as sequence length increases.

**Multi-Stage RL**: Progressive curriculum learning with increasing sequence and tool-call budgets. Why needed: Gradually increasing complexity helps the model learn to manage longer reasoning chains without overwhelming it. Quick check: Performance improves consistently across RL stages without catastrophic forgetting.

## Architecture Onboarding

**Component Map**: Data Synthesis -> SFT Training -> Multi-Stage RL -> Evaluation
- Data Synthesis (curate problems → filter via similarity → code injection → verify executability)
- SFT Training (Llama-Factory on Qwen3-8B, selective feedback masking)
- Multi-Stage RL (GRPO with progressive budgets, asynchronous training)
- Evaluation (AIME24/25, HMMT25 benchmarks)

**Critical Path**: Data Synthesis → SFT → Stage 1 RL → Stage 2 RL → Stage 3 RL → Final Evaluation

**Design Tradeoffs**: 
- Selective feedback masking reduces computational load but may limit learning from code execution
- Asynchronous training improves efficiency but requires complex synchronization
- Progressive RL stages balance learning stability with capacity growth

**Failure Signatures**: 
- Non-executable code in synthetic data indicates data synthesis problems
- Training instability with ultra-long sequences suggests memory management issues
- Performance plateaus indicate RL stage progression problems

**First Experiments**:
1. Verify code executability in synthetic data before training
2. Test SFT training with selective feedback masking on small dataset
3. Validate asynchronous training with partial rollouts on simplified RL problem

## Open Questions the Paper Calls Out

**Open Question 1**: What performance gains would the agentic reinforcement learning stage yield if applied to the ultra-large 235B parameter model, compared to the current SFT-only baseline? The authors note computational constraints prevented RL training on the 235B variant, leaving the scaling laws at this size unverified.

**Open Question 2**: Can the dynamic interleaving of natural language and code execution effectively generalize to non-mathematical reasoning domains, such as logical deduction or multi-step planning? The framework is theoretically agnostic to domain, but prompts, tool definitions, and rewards are heavily optimized for mathematical reasoning.

**Open Question 3**: To what extent does the quality of the initial SFT data depend on the specific reasoning capabilities of the teacher model used for code injection? The success of the "Code Injection" stage may be due to DeepSeek-V3's specific ability to generate valid code for complex math problems.

## Limitations
- Incomplete disclosure of critical hyperparameters (reward coefficients, sandbox configuration)
- Heavy reliance on DeepSeek-V3 as teacher model may introduce hidden dependencies
- Selective feedback masking strategy not fully justified in terms of impact on reasoning quality
- Asynchronous training system described at high level, challenging to replicate without equivalent infrastructure

## Confidence

High: State-of-the-art performance metrics are directly measurable and reproducible given the same evaluation sets
Medium: Exact performance reproducibility limited by incomplete hyperparameter disclosure
Medium: Claims about efficiency gains and scalability supported by ablation studies but lack comparative baselines

## Next Checks

1. Replicate the data synthesis pipeline using specified filters (N-gram, MinHash, semantic similarity) and teacher model prompts, then verify executability and diversity of resulting tool-augmented trajectories

2. Recreate the SFT training setup with selective feedback masking and confirm model can reproduce expected increase in average tool calls and sequence length

3. Reimplement multi-stage RL training with exact GRPO hyperparameters and sandbox configuration, testing whether performance plateaus at reported accuracy on official evaluation sets