---
ver: rpa2
title: 'From Retrieval to Generation: Comparing Different Approaches'
arxiv_id: '2502.20245'
source_url: https://arxiv.org/abs/2502.20245
tags:
- retrieval
- language
- bm25
- triviaqa
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares retrieval-based, generation-based, and hybrid
  approaches for open-domain question answering (ODQA), document reranking, and retrieval-augmented
  language modeling. Dense retrievers like DPR achieve strong ODQA performance with
  a top-1 accuracy of 50.17% on Natural Questions, while hybrid models improve nDCG@10
  scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document
  reranking.
---

# From Retrieval to Generation: Comparing Different Approaches

## Quick Facts
- arXiv ID: 2502.20245
- Source URL: https://arxiv.org/abs/2502.20245
- Authors: Abdelrahman Abdallah; Jamshid Mozafari; Bhawna Piryani; Mohammed Ali; Adam Jatowt
- Reference count: 22
- Dense retrievers like DPR achieve strong ODQA performance with a top-1 accuracy of 50.17% on Natural Questions

## Executive Summary
This study systematically compares retrieval-based, generation-based, and hybrid approaches across three key NLP tasks: open-domain question answering, document reranking, and retrieval-augmented language modeling. The research demonstrates that dense retrievers excel in ODQA tasks, hybrid models significantly improve document reranking effectiveness, and retrieval-based approaches show advantages in language modeling perplexity. The comprehensive evaluation reveals that retrieval-first approaches consistently outperform generation-first pipelines, with hybrid models effectively balancing the strengths of both paradigms.

## Method Summary
The study employs a comprehensive experimental framework evaluating three distinct approaches across three different tasks. For ODQA, the research compares dense retrievers (DPR) against generation models using the Natural Questions benchmark with top-1 accuracy as the primary metric. Document reranking experiments utilize the BEIR benchmark, comparing BM25 with hybrid approaches using nDCG@10 scores. Language modeling experiments on WikiText-103 assess perplexity across retrieval-based, generative, and hybrid methods. The evaluation systematically examines how each approach performs across these diverse tasks while maintaining consistent evaluation protocols.

## Key Results
- Dense retrievers achieve 50.17% top-1 accuracy on Natural Questions for ODQA
- Hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59
- Retrieval-based approaches achieve lower perplexity than generative and hybrid methods on WikiText-103

## Why This Works (Mechanism)
The performance differences across approaches stem from their fundamental design principles. Dense retrievers excel at capturing semantic similarity through learned representations, enabling them to find relevant documents even when exact keyword matches are absent. Hybrid models combine the precision of retrieval-based ranking with the contextual understanding of generation models, allowing them to leverage both exact matches and semantic relationships. The retrieval-first approach proves superior because high-quality initial retrieval provides better context for subsequent generation or reranking, whereas generation-first approaches must work with potentially noisy or incomplete context.

## Foundational Learning
1. Dense Passage Retrieval (DPR) - A neural retrieval method that encodes queries and documents into dense vector representations for semantic matching; needed for understanding modern retrieval systems, quick check: verify it uses BERT-based encoders.
2. BM25 - A traditional probabilistic retrieval model based on term frequency and inverse document frequency; needed as a strong baseline for comparison, quick check: confirm it uses bag-of-words representation.
3. nDCG@10 - Normalized Discounted Cumulative Gain at rank 10; needed for evaluating ranking quality in information retrieval, quick check: ensure relevance scores are properly normalized.
4. Perplexity - A measure of how well a probability model predicts a sample; needed for evaluating language model quality, quick check: verify it's computed using proper tokenization.
5. Hybrid Models - Systems combining retrieval and generation components; needed to understand modern approaches to ODQA, quick check: confirm both components are properly integrated.
6. BEIR Benchmark - A diverse benchmark for zero-shot evaluation of information retrieval models; needed for standardized comparison, quick check: verify all models are evaluated on identical test sets.

## Architecture Onboarding
**Component Map:** Query -> Retriever (Dense/BM25) -> Generator (if applicable) -> Reranker (if hybrid) -> Output
**Critical Path:** Retrieval -> Processing -> Generation/Reranking -> Output, where retrieval quality directly impacts downstream performance
**Design Tradeoffs:** Dense retrievers offer semantic understanding but require more computational resources versus BM25's efficiency but limited semantic capabilities; hybrid models balance these but add complexity
**Failure Signatures:** Poor retrieval leads to low-quality generation regardless of generator sophistication; over-reliance on generation without proper retrieval results in hallucination; hybrid models may suffer from integration overhead
**3 First Experiments:** 1) Evaluate baseline BM25 retrieval performance on target dataset, 2) Test dense retriever embedding quality using nearest neighbor search, 3) Compare single-model versus pipeline performance on a small validation set

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation focuses primarily on English-language benchmarks, limiting generalizability to multilingual or domain-specific corpora
- Computational efficiency comparisons are absent despite substantial infrastructure requirements for dense retrieval and generative models
- Chosen evaluation metrics (top-1 accuracy, nDCG@10, perplexity) may not fully capture end-user utility or downstream task success

## Confidence
- Dense retrievers achieving strong ODQA performance (50.17% top-1 accuracy): High confidence
- Hybrid models improving nDCG@10 scores from 43.42 to 52.59 on BEIR: Medium confidence
- Retrieval-based approaches achieving lower perplexity than generative and hybrid methods on WikiText-103: Medium confidence

## Next Checks
1. Replicate the perplexity comparison on WikiText-103 using the exact model configurations and evaluation protocols to verify that retrieval-based methods indeed outperform generative approaches on this benchmark.
2. Conduct ablation studies to determine the individual contributions of retrieval quality, reranking effectiveness, and generation components in the hybrid pipeline to understand where the performance gains originate.
3. Test the three approaches (retrieval-only, generation-only, hybrid) on a diverse set of benchmarks including multilingual datasets and domain-specific corpora to assess generalizability beyond the English benchmarks used in this study.