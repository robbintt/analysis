---
ver: rpa2
title: Machine Unlearning using Forgetting Neural Networks
arxiv_id: '2410.22374'
source_url: https://arxiv.org/abs/2410.22374
tags:
- forgetting
- unlearning
- forget
- machine
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Forgetting Neural Networks (FNNs) for targeted
  machine unlearning, a neuroscience-inspired architecture that uses per-neuron forgetting
  factors to erase information about specified "forget" data while preserving performance
  on retained data. The authors propose several variants with per-neuron forgetting
  factors, including rank-based assignments guided by activation levels, and evaluate
  them on MNIST and Fashion-MNIST benchmarks.
---

# Machine Unlearning using Forgetting Neural Networks

## Quick Facts
- arXiv ID: 2410.22374
- Source URL: https://arxiv.org/abs/2410.22374
- Reference count: 9
- This paper introduces Forgetting Neural Networks (FNNs) for targeted machine unlearning, a neuroscience-inspired architecture that uses per-neuron forgetting factors to erase information about specified "forget" data while preserving performance on retained data.

## Executive Summary
This paper introduces Forgetting Neural Networks (FNNs) for targeted machine unlearning, a neuroscience-inspired architecture that uses per-neuron forgetting factors to erase information about specified "forget" data while preserving performance on retained data. The authors propose several variants with per-neuron forgetting factors, including rank-based assignments guided by activation levels, and evaluate them on MNIST and Fashion-MNIST benchmarks. Their method systematically removes information associated with forget sets while preserving performance on retained data. Membership inference attacks confirm the effectiveness of FNN-based unlearning in erasing information about the training data from the neural network. The results establish FNNs as a promising foundation for efficient and interpretable unlearning.

## Method Summary
The paper proposes a neuroscience-inspired approach to machine unlearning using Forgetting Neural Networks (FNNs). The method introduces per-neuron forgetting factors that decay weights exponentially based on activation levels. The architecture consists of standard CNN layers with added forgetting mechanisms applied to fully connected layers. The unlearning process alternates between learning bouts (2 epochs on retain data) and unlearning bouts (4 epochs on forget data with decay), iterating until membership inference attack scores indicate effective forgetting. Two forgetting variants are proposed: Fixed Forgetting Rate (FFR) with uniform decay and Rank-based Varying Forgetting Rate (VFR) that assigns decay rates based on neuron activation rankings.

## Key Results
- FNN variants successfully remove information about forget sets while maintaining performance on retained data
- Membership inference attack scores drop to ~0.5 (±0.1) indicating effective unlearning
- Rank-based VFR variant shows superior performance by targeting neurons most responsive to forget data
- FNNs preserve high accuracy (>95%) on retained data throughout unlearning process

## Why This Works (Mechanism)
The mechanism works by applying exponential decay to weights of neurons that are most responsive to forget data. During unlearning bouts, neurons are ranked by their activation magnitude on forget set, and those with highest activations receive the most aggressive decay. This targeted approach preserves general features while selectively weakening representations specific to forget data. The iterative alternating training schedule allows the network to maintain performance on retained data while progressively erasing forget-set associations.

## Foundational Learning
- **Membership Inference Attacks (MIA)**: Statistical tests to determine if specific data was used in training by comparing model behavior on suspect data versus random samples. Why needed: Primary evaluation metric for unlearning effectiveness.
- **Exponential Forgetting Factors**: φ(t) = e^(-t/τ) multiplicative decay applied to neuron weights. Why needed: Implements gradual information decay mimicking neural plasticity.
- **Neuron Activation Ranking**: Sorting neurons by magnitude of response to forget data. Why needed: Identifies which neurons encode specific data features for targeted forgetting.
- **Alternating Training Schedule**: Sequential learning on retain set followed by unlearning on forget set. Why needed: Balances preservation of useful knowledge with removal of specific information.
- **Per-neuron Forget Factors**: Individual τ values assigned to each neuron based on activation levels. Why needed: Enables selective forgetting rather than uniform weight decay.
- **Rank-based VFR**: Assigning τ values inversely proportional to neuron rank in activation ordering. Why needed: Prioritizes forgetting of most forget-data-specific neurons.

## Architecture Onboarding

**Component Map**: Input -> Conv Layers (ReLU+MaxPool) -> FC Layers (with forgetting factors) -> Softmax Output

**Critical Path**: Training -> Data Splitting (R/F) -> Unlearning Loop (Learn bout → Unlearn bout) -> MIA Evaluation

**Design Tradeoffs**: 
- Applying forgetting to FC layers only vs including conv layers (current: FC only)
- Fixed vs varying forgetting rates (current: both implemented)
- Number of alternating bouts vs computational cost

**Failure Signatures**:
- Over-forgetting: MIA << 0.5 with catastrophic accuracy drop on retain set
- Under-forgetting: MIA remains > 0.6 despite many unlearning turns
- Oscillation: Accuracy and MIA fluctuate without convergence

**First Experiments**:
1. Implement baseline CNN and verify ~95% accuracy on MNIST/Fashion-MNIST
2. Test simple uniform forgetting (τ=2) on FC layers and measure MIA change
3. Implement rank-based VFR and compare against random neuron selection baseline

## Open Questions the Paper Calls Out
**Open Question 1**: Can FNN-based unlearning scale to complex architectures like transformers and datasets beyond small-scale vision benchmarks? The current empirical evaluation is restricted to MNIST and Fashion-MNIST using a simple CNN architecture.

**Open Question 2**: How does the FNN architecture adapt to sequential and overlapping forget requests? The current method iterates learning and unlearning bouts for a fixed forget set, rather than processing a continuous stream of removal requests.

**Open Question 3**: What is the impact on unlearning efficacy when forgetting factors are applied to convolutional layers rather than just fully connected layers? Convolutional layers encode lower-level features; it is unclear if decay mechanisms applied here would harm general representation learning or improve targeting.

## Limitations
- Lack of exact implementation details for data splitting ratios and layer configurations
- No validation of activation-based ranking against alternative selection strategies
- Computational overhead comparison with standard retraining not provided
- Limited to simple CNN architectures without exploration of more complex models

## Confidence
- **High confidence**: The overall FNN framework combining activation-based forgetting with iterative retain-forget training cycles is well-specified
- **Medium confidence**: The rank-based VFR variant's mechanism (ranking neurons by activation magnitude on forget set) is clear but its implementation details are missing
- **Low confidence**: The exact evaluation protocol for MIA scores and the precise neuron count architecture parameters are unspecified

## Next Checks
1. Implement multiple split ratios (80/20, 90/10, 95/5) and selection strategies (random vs class-balanced) to test robustness of the unlearning effectiveness
2. Compare activation-based neuron ranking against random/neuron-importance baselines to validate the selection strategy
3. Measure computational overhead of FNN unlearning versus standard retraining on forget set to establish practical efficiency claims