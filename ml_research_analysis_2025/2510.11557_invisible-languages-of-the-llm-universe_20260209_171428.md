---
ver: rpa2
title: Invisible Languages of the LLM Universe
arxiv_id: '2510.11557'
source_url: https://arxiv.org/abs/2510.11557
tags:
- languages
- digital
- language
- invisible
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors present a framework identifying four categories of
  languages in AI systems: Strongholds (33%, high vitality and digitality), Digital
  Echoes (6%, high digitality despite declining vitality), Fading Voices (36%, low
  on both dimensions), and critically, Invisible Giants (27%, high vitality but near-zero
  digitality) - languages spoken by millions yet absent from LLM training data. This
  systematic exclusion constitutes digital epistemic injustice, reflecting continuities
  from colonial-era linguistic hierarchies to contemporary AI development.'
---

# Invisible Languages of the LLM Universe

## Quick Facts
- **arXiv ID:** 2510.11557
- **Source URL:** https://arxiv.org/abs/2510.11557
- **Reference count:** 3
- **Primary result:** Framework identifying four categories of languages in AI systems, revealing 27% are "Invisible Giants" - high vitality but near-zero digitality - constituting digital epistemic injustice

## Executive Summary
This paper presents a systematic framework for understanding language representation in AI systems, identifying four categories based on vitality (demographic strength) and digitality (online presence). The analysis reveals that 27% of documented human languages are "Invisible Giants" - spoken by millions yet absent from LLM training data due to lack of digital infrastructure rather than lack of speakers. This systematic exclusion constitutes digital epistemic injustice, reflecting continuities from colonial-era linguistic hierarchies to contemporary AI development. The authors argue that English dominance in AI is not a technical necessity but an artifact of power structures that systematically exclude marginalized linguistic knowledge.

## Method Summary
The study uses factor analysis and PCA to derive composite vitality and digitality scores for 7,613 documented languages. Vitality combines Ethnologue speaker counts and EGIDS ratings; digitality aggregates presence across Common Crawl (159B pages), Wikipedia (64M articles), Hugging Face datasets/models, and OLAC archives. Languages are classified into four quadrants via median splits: Strongholds (33%), Digital Echoes (6%), Fading Voices (36%), and critically, Invisible Giants (27%). Language identification uses fastText and CLD3 classifiers with validation sampling.

## Key Results
- 27% of documented languages are "Invisible Giants" - high vitality but near-zero digitality
- Quadrant distribution: Strongholds (33%), Digital Echoes (6%), Fading Voices (36%), Invisible Giants (27%)
- English dominance in AI reflects colonial power structures, not technical necessity
- Systematic exclusion constitutes digital epistemic injustice through testimonial and hermeneutical mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Vitality-Digitality Decoupling
Digital infrastructure—Unicode encoding, keyboard layouts, search algorithms—was built primarily for Western languages. Languages without this infrastructure cannot generate digital content at scale, regardless of speaker population size. The paper demonstrates this through Javanese (69M speakers, minimal digital presence) versus Icelandic (320K speakers, disproportionately large digital presence). Platform "notability" and "quality" standards encode English-speaking contexts, systematically disadvantaging non-Western languages.

### Mechanism 2: Self-Reinforcing Feedback Loops
Initial digital scarcity → poor LLM performance on language → speakers avoid digital use in that language → reduced content generation → worsened training data → further degraded model performance. This operates at "unprecedented scale and speed" compared to historical language shift. Poor LLM performance discourages speakers from using their languages digitally, reducing digital content generation, which further reduces training data, which worsens performance.

### Mechanism 3: Epistemic Injustice Amplification
LLMs reproduce testimonial injustice (credibility deflation) when inputs in non-dominant languages receive systematically lower-quality processing, and hermeneutical injustice (interpretive resource gaps) when training data lacks lexical/conceptual resources for community-specific concepts. An input in Swahili receives systematically lower-quality processing than an equivalent input in English, such that the system effectively treats the Swahili speaker's testimony as less reliable.

## Foundational Learning

- **EGIDS (Expanded Graded Intergenerational Disruption Scale)**
  - Why needed: The paper operationalizes vitality partly through EGIDS ratings. Understanding this scale (0=International to 10=Extinct) is essential for interpreting vitality scores and why the authors argue it misses digital dimensions.
  - Quick check: Can a language rank high on EGIDS (robust intergenerational transmission) while having near-zero digital presence?

- **Testimonial vs. Hermeneutical Injustice (Fricker)**
  - Why needed: The paper applies Fricker's epistemic injustice framework to AI systems. Distinguishing credibility-deflation (testimonial) from interpretive-resource-gaps (hermeneutical) is necessary for understanding the two distinct harm pathways the authors describe.
  - Quick check: If an LLM mistranslates a Swahili speaker's input, is this testimonial or hermeneutical injustice? What if the LLM lacks vocabulary for a community-specific concept?

- **Resource Curse Framing (Low-Resource vs. Systematically Under-Resourced)**
  - Why needed: The paper argues that "low-resource" framing naturalizes scarcity and reproduces colonial logic. Understanding this distinction is critical for grasping why the authors propose "decolonizing" approaches rather than simply "adding more data."
  - Quick check: Why does calling a language "low-resource" potentially reproduce colonial epistemic assumptions?

## Architecture Onboarding

- **Component map:** Vitality axis (Ethnologue speaker counts + EGIDS ratings via factor analysis) -> Digitality axis (Common Crawl + Wikipedia + Hugging Face + OLAC via PCA) -> Representation score (Digitality_normalized - Vitality_normalized) -> Classification (median splits into four quadrants)

- **Critical path:** Language identification via fastText/CLD3 classifiers across all corpora → Normalized prevalence scores per corpus per language → Factor analysis for vitality; PCA for digitality → Quadrant classification via median thresholds

- **Design tradeoffs:** Median splits create discrete boundaries where gradients exist; languages near edges may be misclassified. Public web data misses private messaging; from LLM training perspective, this is appropriate exclusion. Cross-sectional design cannot establish causal direction for feedback loops.

- **Failure signatures:** Languages with extensive private digital use but minimal public web presence will appear as Invisible Giants erroneously. Automated language identification errors cascade into prevalence miscalculation. Within-category variation is obscured.

- **First 3 experiments:**
  1. **Sensitivity analysis:** Replace median splits with tercile or quartile thresholds; measure stability of quadrant assignments and geographic distribution shifts.
  2. **Platform decomposition:** Disaggregate digitality score by source (Common Crawl vs. Wikipedia vs. Hugging Face) to identify which platforms contribute most to Invisible Giant underrepresentation.
  3. **Longitudinal pilot:** Select 20 Invisible Giants and 20 Strongholds; track vitality-digitality trajectories over 2-3 years to test feedback loop hypothesis (are gaps widening or narrowing?).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are vitality-digitality gaps widening or narrowing over time, and do targeted digital interventions successfully narrow representation gaps for Invisible Giant languages?
- **Basis in paper:** We lack longitudinal data on how vitality-digitality gaps evolve. Are gaps widening or narrowing? Do digital interventions successfully narrow gaps for targeted languages? Cross-sectional analysis cannot answer these causal questions.
- **Why unresolved:** The study uses cross-sectional data from a single time point, precluding causal inference about gap trajectories or intervention effectiveness.
- **What evidence would resolve it:** Longitudinal tracking of languages across the vitality-digitality space over multiple years, combined with natural experiments from digital infrastructure investments.

### Open Question 2
- **Question:** How do platform affordances (character limits, media modalities, register expectations) systematically advantage or disadvantage specific languages?
- **Basis in paper:** Platform-specific language ideologies warrant deeper investigation... How do platform affordances systematically advantage or disadvantage specific languages? We need granular socio-technical analysis of how architectural choices embed linguistic assumptions.
- **Why unresolved:** No systematic analysis exists comparing how platform design decisions differentially affect languages across the four-category framework.
- **What evidence would resolve it:** Comparative platform-level analysis measuring language representation and performance across platforms with different affordances (e.g., Twitter vs. TikTok vs. LinkedIn).

### Open Question 3
- **Question:** How does digital language inequality intersect with signed language representation, gender, and age-based linguistic variation?
- **Basis in paper:** Future research should examine intersections of linguistic marginalization with other forms of oppression. How does digital language inequality interact with special needs (especially for signed languages' digital representation)? With gender...? With age...? These intersectional analyses remain underdeveloped.
- **Why unresolved:** The current framework treats languages as unitary entities without examining within-language variation or intersection with other marginalization axes.
- **What evidence would resolve it:** Disaggregated digitality measurements by signer populations, gender-differentiated speaker data, and age-stratified analysis of digital language use patterns.

## Limitations
- Vitality-digitality decoupling mechanism rests on correlational evidence rather than causal demonstration
- Self-reinforcing feedback loop remains theoretical without longitudinal data
- Digital corpora may systematically undercount private digital communication
- Median splits create artificial boundaries where gradients exist

## Confidence
- **High confidence:** Framework's descriptive validity (quadrant classification, 33%/6%/36%/27% distribution) and correlation between linguistic marginalization and colonial history
- **Medium confidence:** Mechanism explanations for digital exclusion (infrastructure disparities, platform standards) and epistemic injustice amplification claims
- **Low confidence:** Feedback loop dynamics and causal claims about LLM quality driving language abandonment

## Next Checks
1. **Sensitivity Analysis:** Replicate quadrant classification using alternative threshold methods (terciles, quintiles) and data normalization approaches to test whether the 27% Invisible Giants figure remains stable
2. **Platform Decomposition:** Analyze digitality contributions from each source (Common Crawl, Wikipedia, Hugging Face, OLAC) separately to identify which platforms create the most severe exclusions
3. **Longitudinal Pilot:** Track 20 Invisible Giants and 20 Strongholds over 2-3 years, measuring changes in digital content generation, model performance, and speaker digital behavior to empirically validate feedback loop hypothesis