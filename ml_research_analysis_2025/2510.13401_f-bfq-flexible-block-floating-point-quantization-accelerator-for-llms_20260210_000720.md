---
ver: rpa2
title: 'F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs'
arxiv_id: '2510.13401'
source_url: https://arxiv.org/abs/2510.13401
tags:
- llms
- accelerator
- quantization
- data
- f-bfq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accelerating inference for
  large language models (LLMs) on resource-constrained edge devices by leveraging
  block floating-point (BFP) quantization. The authors propose F-BFQ, a flexible hardware
  accelerator capable of dynamically switching between two BFP quantization variants
  (Q2K and Q3K) to efficiently perform matrix multiplication operations across different
  layers of quantized LLMs.
---

# F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs

## Quick Facts
- arXiv ID: 2510.13401
- Source URL: https://arxiv.org/abs/2510.13401
- Authors: Jude Haris; José Cano
- Reference count: 31
- Primary result: Achieved 1.4× speedup over ARM NEON CPU with 5.2 tokens/second for LLM inference on AMD Kria KV260

## Executive Summary
F-BFQ introduces a flexible hardware accelerator for LLM inference using block floating-point (BFP) quantization. The accelerator supports two BFP variants (Q2_K and Q3_K) through dynamic switching without hardware reconfiguration. By processing matrix multiplication operations efficiently across different quantization formats, F-BFQ achieves significant speedup over CPU execution while maintaining acceptable accuracy for edge deployment scenarios.

## Method Summary
The F-BFQ accelerator was designed using the SECDA-LLM platform and deployed on AMD Kria KV260 hardware. It implements a Dynamic Super-Block Processor (DSBP) with micro-ISA opcodes for handling BFP quantization variants. The design uses hierarchical super-block scaling with 256-element super-blocks subdivided into 16 blocks of 16 weights each. Three quantized LLMs (GPT2-163M, MobileLLaMA-1.4B, TinyLlama-1.1B) were evaluated using llama.cpp with mixed BFP quantization. Performance was measured using 6-token input prompts and 10-token generation, with results averaged over 10 runs.

## Key Results
- Achieved 1.4× average speedup over ARM NEON CPU baseline
- Delivered 5.2 tokens per second average throughput
- Successfully processed mixed Q2_K and Q3_K quantization variants without reconfiguration

## Why This Works (Mechanism)

### Mechanism 1
Dynamic quantization mode switching enables processing mixed-precision LLM layers without hardware reconfiguration overhead. The Dynamic Super-Block Processor uses a `weight_type` control register set via opcode `0x01` to select between Q2_K and Q3_K modes. The Vector Compute Unit shares a common vector engine for dot-product operations across both modes, while mode-specific scaling operations route through dedicated Q2/Q3 Scalar Units.

### Mechanism 2
Hierarchical super-block scaling preserves accuracy while enabling aggressive bit-width reduction. Weights are organized into super-blocks (SB) of 256 elements, subdivided into 16 blocks of 16 weights each. Q3_K uses 6-bit block-scaling factors per block plus a 16-bit super-scaling factor per super-block, achieving ~3.5 bits/weight. Q2_K uses 2-bit weights with 4-bit block minimum/scalar values and 16-bit super-block minimum/scalar values.

### Mechanism 3
Depth-partitioned FIFO loading enables stall-free pipeline operation for parallel compute. The Data Loader partitions consecutive data elements in the depth dimension across N FIFOs, allowing the DSBP to compute N operations simultaneously. Partitioned BRAM buffers in SB caches enable parallel access to consecutive data.

## Foundational Learning

- **Concept: Block Floating-Point (BFP) Quantization**
  - Why needed here: F-BFQ's entire design centers on processing BFP-formatted weights; understanding shared exponents within blocks is essential for debugging scaling logic.
  - Quick check question: Given a Q3_K super-block with SSF=1024 and BSF=8 for a particular block, what is the effective scale factor for weights in that block?

- **Concept: Super-Block Data Format (GGUF/Qx_K variants)**
  - Why needed here: The bit-slicer and data mapper must correctly parse the different bit allocations for Q2_K vs Q3_K; misalignment corrupts all downstream computation.
  - Quick check question: How many bits are allocated to `w_low` and `w_high` buffers for a Q3_K weight, and how do they combine to form a 3-bit value?

- **Concept: Output-Stationary Tiling**
  - Why needed here: The driver must correctly tile large MatMul operations; understanding output-stationary vs weight-stationary dataflow determines when outputs are accumulated and written back.
  - Quick check question: For a tiled MatMul where the input matrix exceeds on-chip buffer capacity, which opcode sequences ensure partial outputs are correctly accumulated across tiles?

## Architecture Onboarding

- **Component map:** AXI-Stream → Instruction Decoder → [opcode dispatch] → Data Loader → Weight FIFO → DSBP (Dynamic SB Loader → SB Caches → VCU → Accumulator) → Scheduler ← Input FIFO → AXI-Stream (output)

- **Critical path:** The VCU's dot-product → scalar scaling → accumulation path determines throughput. The shared vector engine processes both Q2_K and Q3_K weights, but scalar unit latency differs; Q2_K's additional minimum-value handling adds one cycle versus Q3_K.

- **Design tradeoffs:**
  - Flexibility vs. resources: Supporting two variants uses 81% BRAM but only 14% DSPs; adding Q4_K–Q8_K would increase BRAM pressure for additional scale factor buffers
  - Parallelism vs. complexity: N FIFOs enable N parallel operations but require partitioning logic that assumes depth-dimension alignment with SB boundaries
  - Opcode simplicity vs. driver overhead: 5 opcodes keep decode logic simple but shift tiling complexity to the driver

- **Failure signatures:**
  - Wrong quantization mode: Output values are systematically off by power-of-2 factors (scaling mismatch)
  - FIFO overflow: Data Loader stalls; DSBP starves; throughput drops to single-operation serialization
  - Tiling misalignment: Partial sums written to wrong output addresses; token generation produces garbage after first tile boundary
  - BRAM partition mismatch: Parallel access fails; VCU stalls waiting for data; effective throughput matches single-lane operation

- **First 3 experiments:**
  1. Single-layer MatMul validation: Run Q2_K and Q3_K MatMul operations in isolation with known inputs/weights; verify outputs match software reference within BFP precision tolerance
  2. Mode-switching stress test: Alternate between Q2_K and Q3_K layers without flushing caches; measure if any mode-switch latency appears
  3. Tiling boundary test: Run MatMul operations at sizes just above and below input buffer capacity; verify output-stationary tiling produces identical results to non-tiled execution

## Open Questions the Paper Calls Out

### Open Question 1
How can the F-BFQ architecture be extended to support higher-precision BFP variants (Q4_K to Q8_K) without requiring complete hardware reconfiguration? The conclusion states future work will support other BFP variants, but the current DSBP is specifically designed for only Q2_K and Q3_K formats.

### Open Question 2
To what extent does fusing consecutive MatMul layers within the accelerator mitigate the data transfer bandwidth bottlenecks observed in smaller models like GPT2? The current design sends output data back to main memory after each operation, which caused lower speedups in GPT2 due to its lower computational intensity.

### Open Question 3
What are the performance trade-offs when scaling the architecture to utilize multiple Dynamic Super-Block Processors (DSBPs) compared to the single-DSBP evaluation? The paper notes the design "is modular and scalable with multiple DSBPs," but the evaluation on the KV260 board utilizes only a single DSBP.

## Limitations

- Resource scalability concerns when extending to Q4_K-Q8_K variants may exceed BRAM capacity and require significant architectural redesign
- Limited accuracy validation against full-precision reference implementations across different layer types and model depths
- Hardware-software co-design gaps in driver implementation and opcode generation pipeline underspecify claimed speedup measurements

## Confidence

**High Confidence Claims:**
- The Dynamic Super-Block Processor architecture with opcode-driven mode switching is technically sound
- The hierarchical super-block scaling approach with shared scaling factors is a valid BFP quantization technique

**Medium Confidence Claims:**
- The claimed 1.4× speedup represents realistic acceleration for mixed-precision LLM inference
- The output-stationary tiling approach effectively handles matrices exceeding on-chip buffer capacity

**Low Confidence Claims:**
- Projected benefits of extending support to Q4_K-Q8_K variants without significant redesign
- Absolute tokens-per-second metrics as representative of real-world inference performance

## Next Checks

1. **Accuracy Degradation Analysis**: Run F-BFQ inference on GPT2 with varying input lengths and layer depths, comparing outputs against llama.cpp CPU reference implementation to quantify accumulated quantization error across the full generation pipeline.

2. **Resource Scaling Projection**: Model BRAM and DSP requirements for supporting Q4_K-Q8_K variants, identifying potential bottlenecks in the current DSBP architecture and estimating the impact on achievable clock frequency and throughput.

3. **Memory Bandwidth Bottleneck Characterization**: Profile data transfer patterns during token generation to determine whether the claimed speedup is limited by memory bandwidth rather than computational throughput, and assess the impact on larger models beyond the evaluated 1.4B parameter scale.