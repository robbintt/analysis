---
ver: rpa2
title: Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal
  Processing
arxiv_id: '2601.00020'
source_url: https://arxiv.org/abs/2601.00020
tags:
- learning
- device
- weight
- programming
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates the feasibility of deploying spiking neural
  networks (SNNs) on programmable ferroelectric memristive synapses for adaptive EEG-based
  motor imagery decoding. The approach addresses key challenges in neuromorphic hardware
  deployment, including limited weight resolution, device variability, nonlinear programming
  dynamics, and finite endurance.
---

# Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing

## Quick Facts
- arXiv ID: 2601.00020
- Source URL: https://arxiv.org/abs/2601.00020
- Reference count: 40
- Key outcome: Classification accuracy of 81.33% achieved on subject-specific EEG motor imagery using spiking neural networks with ferroelectric memristive synapses.

## Executive Summary
This work presents a device-aware learning framework for spiking neural networks (SNNs) deployed on programmable ferroelectric memristive synapses for EEG signal processing. The approach addresses key challenges including limited weight resolution, device variability, and finite endurance by accumulating gradient updates digitally and converting them into discrete programming events only when a threshold is exceeded. This enables robust, low-overhead adaptation while maintaining classification performance comparable to state-of-the-art software-based SNNs. The framework also demonstrates effective subject-specific transfer learning by retraining only the final network layers, improving personalized accuracy without requiring full-network retraining.

## Method Summary
The method employs a convolutional-recurrent SNN architecture with LIF neurons to classify EEG motor imagery data. A device-aware learning framework accumulates gradient updates digitally and triggers discrete hardware programming events only when a threshold is exceeded, emulating realistic ferroelectric synapse behavior while reducing update activity. The framework uses a differential weight mapping scheme with a single active ferroelectric device plus a fixed mid-level reference device to preserve accuracy while reducing crossbar overhead. Subject-specific transfer learning is achieved by fine-tuning only the final fully connected layers on individual EEG data, improving classification accuracy without full-network retraining.

## Key Results
- Classification accuracy of 81.33% achieved on subject-specific EEG motor imagery
- Device-aware learning framework reduces programming events while maintaining accuracy
- Subject-specific transfer learning by retraining final layers improves accuracy by 1.77%
- Framework achieves performance comparable to state-of-the-art software-based SNNs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accumulating gradient updates digitally and triggering discrete hardware programming only when thresholds are exceeded enables robust on-device learning while reducing write operations.
- Mechanism: Each synapse maintains a digital accumulator δw_acc. When |δw_acc| ≥ ε (threshold), a single LTP or LTD pulse is applied based on sign, using a fitted Beta-kernel device model. The threshold emulates the minimum resolvable conductance change while batching many small gradients into fewer programming events.
- Core assumption: Gradient direction remains valid across accumulated updates; device state-dependent nonlinear updates do not systematically oppose learning direction.
- Evidence anchors: [abstract] "gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded"; [section 2.7] Eq. 15 defines asymmetric LTP/LTD thresholds; accumulation resets after triggering.
- Break condition: If gradients are noisy or biased such that accumulated direction misrepresents true descent, or if thresholds are set so high that useful updates never trigger, convergence degrades.

### Mechanism 2
- Claim: Mapping signed weights to a single active ferroelectric device plus a fixed mid-level reference device preserves classification accuracy while reducing crossbar overhead.
- Mechanism: Weight w_diff = w+ - w- with w- fixed at 0.5 (mid-conductance). Ferroelectric devices support bidirectional conductance changes (both LTP and LTD), avoiding the saturation issues of asymmetric technologies that require dual-active devices.
- Core assumption: The reference device's mid-level conductance is stable over time; device-to-device variability does not overwhelm the differential signal.
- Evidence anchors: [section 2.5] Eq. 10-13 define the mapping; reference device stability is cited from prior retention measurements in ref. [22].
- Break condition: If reference drift becomes significant, or if device variability exceeds the differential margin, classification degrades.

### Mechanism 3
- Claim: Fine-tuning only the final fully connected layers (FC1, FC2) on per-subject data improves personalized accuracy without full-network retraining.
- Mechanism: Lower convolutional layers extract subject-invariant spatial/temporal EEG features; final layers adapt to individual signal distributions. Limited data per subject makes full-network fine-tuning prone to overfitting.
- Core assumption: Early layers encode transferable features; subject-specific variation is primarily captured in the classifier stage.
- Evidence anchors: [section 3.3] Fig. 7(a) compares layer subsets; FC1+FC2 with lr=6e-4 yields 81.33% (↑1.77% over pretrained).
- Break condition: If subject variation is embedded in lower-level features (e.g., different electrode contact quality or signal amplitude distributions), classifier-only adaptation will underfit.

## Foundational Learning

- Concept: **Leaky Integrate-and-Fire (LIF) neurons with surrogate gradients**
  - Why needed here: The SNN architecture uses LIF neurons with learned decay factors; backpropagation through discrete spikes requires surrogate gradients (rectangular window) for credit assignment.
  - Quick check question: Can you explain why a surrogate gradient is necessary when differentiating through a spike threshold function?

- Concept: **Memristive conductance update non-idealities**
  - Why needed here: The device-aware learning framework explicitly models nonlinear, state-dependent, asymmetric LTP/LTD using Beta-shaped kernels; understanding these constraints is essential for interpreting why threshold-based accumulation is required.
  - Quick check question: How does state-dependent update magnitude differ from simple additive weight increments?

- Concept: **Transfer learning with partial fine-tuning**
  - Why needed here: Subject-specific adaptation relies on freezing early layers and updating only FC1/FC2; this requires understanding why limited data favors restricted updates.
  - Quick check question: What risks arise when fine-tuning a full network on a small per-subject dataset?

## Architecture Onboarding

- Component map:
  Input: 64-channel EEG → 2D spatial projection (10×11) → CONV1 → CONV2 → AvgPool → CONV3 → TC1 → R1 → FC1 → FC2 → Output

- Critical path:
  1. Device characterization → fit Beta-kernel update model (A, α, β for LTP/LTD)
  2. Map software weights to differential conductance with fan-in-based bounds
  3. Implement threshold-based accumulator; configure ε (2.5–7.5%) and ε_asym
  4. Train on-device or transfer pretrained weights with quantization + noise + re-tuning
  5. Subject-specific fine-tuning: freeze CONV/TC/R1, update FC1–FC2 only

- Design tradeoffs:
  - Higher ε → fewer programming events, slower convergence, improved endurance
  - Lower quantization levels (even 3) still viable for SNNs due to temporal encoding; but combined with high noise (η>25%) requires re-tuning
  - Asymmetric thresholds (ε_asym) can compensate for LTP/LTD asymmetry in device response

- Failure signatures:
  - Training accuracy plateaus below baseline: ε too high or ε_asym mismatched to device asymmetry
  - Subject fine-tuning degrades accuracy: overfitting due to too many updated layers or excessive learning rate
  - Weight transfer fails to recover accuracy after quantization + noise: re-tuning epochs insufficient or ε misconfigured

- First 3 experiments:
  1. Reproduce software baseline (80.39% test accuracy) to validate SNN implementation and data pipeline before introducing device models.
  2. Sweep ε (2.5%, 5%, 7.5%) with ε_asym=1 to quantify accuracy vs. programming event tradeoff (target: ≤1% accuracy drop vs. baseline).
  3. Quantize pretrained weights to 3 levels, add Gaussian noise (η=25%), then run 4 epochs of device-aware re-tuning; verify recovery to ~79% test accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed device-aware learning framework perform when deployed on a full hardware system with crossbar non-idealities, such as sneak paths and IR drop, compared to the simulated device models?
- Basis in paper: [explicit] The conclusion states, "Future efforts will focus on... full on-hardware deployment, further bridging the gap between brain-inspired algorithms and practical, personalized neuromorphic hardware systems."
- Why unresolved: The current study relies on simulation frameworks using device models fitted to characterization data, assuming ideal crossbar conditions without the parasitic effects present in dense arrays.
- What evidence would resolve it: Demonstration of the threshold-based on-device learning strategy on a physical neuromorphic chip integrating ferroelectric synapse arrays, measuring classification accuracy and energy consumption against the simulated baseline.

### Open Question 2
- Question: Can the threshold-based gradient accumulation strategy maintain convergence efficiency and accuracy when scaled to significantly deeper or wider spiking neural network architectures?
- Basis in paper: [explicit] The conclusion explicitly identifies "scaling to larger networks" as a focus for future efforts.
- Why unresolved: The experiments were restricted to a specific convolutional-recurrent architecture (approx. 697k parameters); it remains unverified if the discrete update approximation holds for high-capacity models requiring finer weight adjustments.
- What evidence would resolve it: Evaluation of the learning framework on larger SNN architectures (e.g., deep residual SNNs) to compare convergence speed and final accuracy against the smaller EEG-specific model.

### Open Question 3
- Question: To what extent does device-to-device variability across a full memristive array degrade the robustness of the subject-specific transfer learning strategy?
- Basis in paper: [inferred] The paper notes that variability measurements correspond to a "single device" and that "higher programming error" can be anticipated when accounting for array-level variability, yet the simulations rely on the characterized single-device model.
- Why unresolved: The study emulates variability using Gaussian noise on weights, but this may not capture the spatial correlations or structural defects found in actual fabricated arrays which could bias the fine-tuning of final layers.
- What evidence would resolve it: Inclusion of spatial variability maps and cycle-to-cycle noise models derived from array-level measurements in the simulation, specifically analyzing the degradation in transfer learning performance.

## Limitations
- Device-to-device variability across full memristive arrays not fully characterized
- Limited to specific convolutional-recurrent architecture (697k parameters)
- Assumes reference device stability in differential weight mapping scheme

## Confidence
- Device-aware learning framework: Medium - well-defined mechanism but partial characterization of noisy gradient interactions
- Differential single-device weight mapping: Medium - conceptually sound but no experimental validation of reference device drift
- Subject-specific transfer learning: High - standard approach empirically validated within study

## Next Checks
1. **Reconstruct the 64-to-10×11 mapping**: Verify the electrode-to-grid assignment and confirm that input feature maps match the architecture's expectations; mismatches here could explain accuracy variance.
2. **Cross-validate threshold tuning**: Systematically sweep ε and ε_asym across multiple subjects to quantify the tradeoff between programming event reduction and accuracy degradation.
3. **Test reference device stability**: Measure conductance drift of the fixed mid-level reference device over extended periods under operating conditions to validate the differential weight mapping assumption.