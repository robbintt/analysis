---
ver: rpa2
title: On the Internal Semantics of Time-Series Foundation Models
arxiv_id: '2511.15324'
source_url: https://arxiv.org/abs/2511.15324
tags:
- layers
- figure
- shift
- concepts
- chronos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the internal semantics of Time-Series Foundation
  Models (TSFMs) by probing how they encode seven canonical time-series concepts (AR(1),
  level shifts, random walk, spectral, time-warped, trend, and variance shift). Using
  linear probes, structural probes, and CKA similarity, the study systematically examines
  concept localization across layers, parameter recoverability, representation evolution,
  and compositional behavior.
---

# On the Internal Semantics of Time-Series Foundation Models

## Quick Facts
- arXiv ID: 2511.15324
- Source URL: https://arxiv.org/abs/2511.15324
- Authors: Atharva Pandey; Abhilash Neog; Gautam Jajoo
- Reference count: 39
- Primary result: Linear probes reveal that TSFMs encode time-series concepts in a depth-dependent hierarchy, with local patterns emerging early and dispersion/change-point signals refining in deeper layers.

## Executive Summary
This paper systematically investigates how Time-Series Foundation Models (TSFMs) encode seven canonical time-series concepts through linear probing, structural analysis, and compositional testing. The study reveals that atomic concepts are reliably localized in specific layers, with early layers capturing local patterns like AR(1) and trends, while deeper layers encode dispersion and changepoint signals. However, compositional settings show degraded probe performance, indicating interference between concepts. The findings highlight that while TSFMs can reliably localize individual temporal phenomena, representing interacting concepts remains a significant challenge.

## Method Summary
The study employs frozen Chronos and MOMENT TSFMs to extract layer-wise hidden states from synthetic time series data. Seven canonical concepts (AR(1), level shifts, random walk, spectral, time-warped, trend, and variance shift) are generated with specified generative parameters. Linear probes are trained to predict these parameters from pooled hidden states at each layer. CKA similarity and UMAP visualizations analyze representation evolution, while compositional experiments test probe transferability from atomic to composite data. The methodology includes controlled cropping experiments to assess context-dependency across layers.

## Key Results
- Early transformer layers (typically by layer 2) capture local time-domain patterns like AR(1), level shifts, and trends with low linear probe MSE.
- Deeper layers progressively disentangle concepts and specialize in variance and changepoint signals, showing improved probe performance for dispersion-related concepts.
- Spectral and time-warped concepts exhibit fragmented UMAP structures and high probe errors, indicating non-linear entanglement.
- Compositional settings reveal significant probe performance degradation, indicating interference between atomic concepts when composed.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Concept Localization Across Layers
TSFMs encode time-series concepts in a depth-dependent hierarchy, with local patterns emerging early and dispersion/change-point signals refining in deeper layers. Early transformer layers produce representations that expose time-domain structure (AR coefficients, level shifts, trends) with low linear probe MSE; deeper layers progressively disentangle concepts and specialize in variance and changepoint signals. Linear probe recoverability approximates how "accessible" a concept is within a representation; CKA and UMAP geometry reflect meaningful semantic organization.

### Mechanism 2: Linear Recoverability Varies by Concept Type
Structural and time-domain concept parameters are linearly decodable from hidden states, while spectral and time-warped concepts require non-linear disentanglement. When UMAP embeddings form compact, smoothly ordered manifolds with respect to a parameter, linear probes achieve low MSE (e.g., AR(1), trend slope). When embeddings are fragmented or tangled, linear probes fail, indicating non-linear encoding (e.g., frequency components, warp strength). Parameter ordering along UMAP manifolds implies linear accessibility; fragmentation implies non-linear encoding.

### Mechanism 3: Compositional Interference in Concept Mixtures
Atomic concepts are reliably localized, but when concepts are composed, probe performance degrades due to interference. Probes trained on atomic data transfer poorly to composite series—parameters become less linearly recoverable when multiple temporal phenomena interact. Vector arithmetic shows near-linear combination for most concept pairs (emb1 + emb2 ≈ emb3), but temporally disparate pairs exhibit non-linear interactions. Linear probe transferability from atomic to composite data indicates compositional disentanglement; degradation indicates interference.

## Foundational Learning

- **Linear Probing**: The paper's primary diagnostic is training linear regressors on frozen hidden states to predict generative parameters. Understanding that linear probes test whether information is *linearly accessible* (not whether it exists at all) is essential for interpreting MSE curves. *Quick check: If a linear probe has high MSE on layer L, does this mean the concept is absent from that layer?*

- **Representational Similarity (CKA)**: CKA quantifies how representations change across layers and whether different concepts occupy separable subspaces. The paper uses CKA to show progressive disentanglement with depth. *Quick check: What does high CKA between layer 2 and layer 4 imply about feature reuse vs. specialization?*

- **Time-Series Concept Classes**: The paper defines seven canonical concepts (AR(1), level shift, random walk, spectral, time-warped, trend, variance shift). Each has distinct generative parameters that probes attempt to recover. *Quick check: Why might frequency-based concepts be harder to linearize than amplitude-based concepts?*

## Architecture Onboarding

- **Component map**: Input layer -> Patched/tokenized time series embeddings -> Early transformer layers (0-2) -> Local, time-domain pattern extraction (AR, trend, level shifts) -> Mid layers (2-4) -> Partial disentanglement, transition toward concept-specific features -> Late layers (4+) -> Dispersion, changepoint signals, task-specific specialization -> Pooling -> Hidden states H^(l) aggregated to z^(l) for probing -> Linear probes -> Per-layer regressors W^(l) trained to predict parameters θ

- **Critical path**: Extract hidden states H^(l) from each layer for synthetic concept datasets; pool to z^(l) and train linear probes to recover θ; visualize z^(l) via UMAP/t-SNE/PCA to inspect cluster structure; compute CKA between layers to track representation evolution; transfer atomic-trained probes to composite data to measure interference.

- **Design tradeoffs**: Chronos vs. MOMENT - Chronos shows better-organized UMAP clusters and lower probe MSE; MOMENT shows more layer-wise redundancy. Context length - deeper layers require longer context (25-100% cropping experiments show MSE improves with more history at late layers but not early layers). Normalization - concepts where magnitude is the signal should not be z-scored; others should be. Incorrect normalization erases recoverable information.

- **Failure signatures**: High probe MSE across all layers - concept may be non-linearly entangled or absent from pretraining distribution. Fragmented UMAP with no parameter ordering - indicates representation space does not smoothly encode the target parameter. Sharp probe degradation on compositional data - indicates interference; may require non-linear probes or architecture changes.

- **First 3 experiments**: Train layer-wise linear probes on all 7 concepts across all layers; plot MSE curves to identify where each concept plateaus. Crop inputs to 25/50/75/100% and evaluate pretrained probes to confirm early layers are context-agnostic while late layers are context-dependent. Train probes on atomic AR(1) + trend; evaluate on structured composite (AR then trend) and functional composite (AR + trend); quantify MSE degradation to measure interference.

## Open Questions the Paper Calls Out

### Open Question 1
Can controlled-capacity non-linear or causal probes recover spectral and time-warping factors that linear probes fail to disentangle? The conclusion explicitly suggests future work should "adopt controlled-capacity non-linear or causal probes" given that spectral and warping factors were the "hardest to recover linearly."

### Open Question 2
Do the internal semantics and layer-wise localization patterns hold for multivariate and irregularly sampled time-series data? The authors state that future work should "explore additional TSFMs, multivariate and irregular datasets," as the current analysis is restricted to synthetic, univariate concepts.

### Open Question 3
What specific architectural modifications or pre-training objectives can linearize phase and time-warping information in TSFMs? The paper calls for investigating "architectures and objectives that better linearize phase and time-warping."

### Open Question 4
How can TSFMs be improved to represent interacting temporal phenomena without the interference observed in compositional settings? The authors note that "composition remains a challenge" and "probe performance degrades" due to interference.

## Limitations
- The study uses only two TSFMs (Chronos and MOMENT), potentially limiting generalizability across different architectural designs or pretraining objectives.
- Linear probe methodology may not capture all encoded information, particularly for spectral and warping concepts that resist linearization.
- Synthetic data and canonical concepts may not fully represent real-world temporal phenomena and their complex interactions.
- Compositional interference findings are based on controlled synthetic mixtures that may not capture naturally occurring concept interactions.

## Confidence

- **High confidence**: Layer-wise hierarchical localization (early layers → local patterns; deep layers → dispersion/changepoints) - directly supported by probe MSE curves and UMAP visualizations across both models.
- **Medium confidence**: Linear probe sufficiency for concept detection - well-justified for AR/trend/level shift but weaker for spectral/warping; non-linear probes could yield different conclusions.
- **Low confidence**: Compositional interference conclusions - based on synthetic mixtures; real-world concept interactions may behave differently.

## Next Checks

1. Train MLP probes with controlled capacity on spectral and time-warped concepts to determine if non-linear probes can recover parameters that linear probes miss.
2. Test probe transferability on naturally occurring compositional patterns from real-world datasets (e.g., sensor streams with multiple simultaneous phenomena) rather than synthetic mixtures.
3. Evaluate additional TSFMs with different pretraining objectives (e.g., contrastive vs. reconstruction) to determine if hierarchical localization patterns are architecture-dependent or universal.