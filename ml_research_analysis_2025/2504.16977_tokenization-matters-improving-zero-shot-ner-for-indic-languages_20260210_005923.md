---
ver: rpa2
title: 'Tokenization Matters: Improving Zero-Shot NER for Indic Languages'
arxiv_id: '2504.16977'
source_url: https://arxiv.org/abs/2504.16977
tags:
- tokenization
- languages
- sentencepiece
- indic
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates tokenization strategies for Named Entity Recognition
  (NER) in low-resource Indic languages, comparing Byte-Pair Encoding (BPE), SentencePiece,
  and Character-Level tokenization. Intrinsic analysis using the FLORES-200 dataset
  shows that SentencePiece better preserves morphological structure and generalizes
  well across morphologically rich and script-diverse languages like Santali, Manipuri,
  and Sindhi.
---

# Tokenization Matters: Improving Zero-Shot NER for Indic Languages

## Quick Facts
- arXiv ID: 2504.16977
- Source URL: https://arxiv.org/abs/2504.16977
- Authors: Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Amit Agarwal
- Reference count: 33
- Primary result: SentencePiece outperforms BPE and Character-Level tokenization for zero-shot NER in low-resource Indic languages, achieving up to 88.38% F1 on Assamese.

## Executive Summary
This study evaluates tokenization strategies for Named Entity Recognition (NER) in low-resource Indic languages, comparing Byte-Pair Encoding (BPE), SentencePiece, and Character-Level tokenization. Intrinsic analysis using the FLORES-200 dataset shows that SentencePiece better preserves morphological structure and generalizes well across morphologically rich and script-diverse languages like Santali, Manipuri, and Sindhi. Extrinsic evaluation on Hindi and Bengali NER datasets demonstrates that SentencePiece achieves superior zero-shot cross-lingual performance (F1 scores up to 88.38% on Assamese) compared to BPE, which fails to generalize in unseen languages (0.00% F1 on Assamese and Oriya). The findings indicate that SentencePiece is the most effective tokenization strategy for NER in low-resource Indic languages, particularly in zero-shot settings, due to its ability to maintain entity boundaries and linguistic structure across diverse scripts.

## Method Summary
The study compares three tokenization strategies (BPE, SentencePiece, Character-Level) for zero-shot NER across Indic languages using the IndicBERT model. Intrinsic analysis evaluates tokenization on FLORES-200 data (OOV rate, vocab compression, tokens/sentence), while extrinsic evaluation tests zero-shot transfer from Hindi/Bengali to six other Indic languages using the Naamapadam NER dataset. The researchers analyze how tokenization affects entity boundary preservation and cross-lingual generalization.

## Key Results
- SentencePiece achieves superior zero-shot F1 scores (up to 88.38% on Assamese) compared to BPE's complete failure (0.00% F1 on Assamese and Oriya)
- Intrinsic analysis shows SentencePiece better preserves morphological structure and entity boundaries across morphologically rich languages
- BPE's aggressive subword merging fragments named entities in unseen languages, causing the model to predict only "O" labels
- Character-Level tokenization, while preserving structure best, is computationally infeasible due to extreme vocabulary expansion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SentencePiece preserves morphological structure better than frequency-based BPE, enabling superior entity recognition in morphologically rich languages.
- Mechanism: SentencePiece employs whitespace-agnostic, character-aware segmentation that can align subword boundaries with morpheme boundaries rather than frequency-driven merges. This preserves meaningful linguistic units that compose named entities.
- Core assumption: Morpheme-aligned subword units maintain entity boundary integrity across related languages better than frequency-optimized compression.
- Evidence anchors:
  - [abstract]: "SentencePiece constitutes a better linguistic structural preservation model, benefiting extremely low-resource and morphologically rich Indic languages, such as Santali and Manipuri"
  - [Table V]: "BPE (Simple) Can't preserve morphemes in agglutinative languages" vs. "SentencePiece Can preserve morphemes in low-resource languages"
  - [corpus]: arXiv:2509.14238 confirms tokenization challenges for agglutinative languages like Turkish/Finnish; arXiv:2504.10335 (MorphTok) proposes morphology-grounded alternatives
- Break condition: If target languages use non-concatenative morphology (root-and-pattern systems) where subword segmentation cannot align with morphological boundaries, this mechanism degrades.

### Mechanism 2
- Claim: BPE's aggressive subword merging causes entity fragmentation, leading to complete zero-shot transfer failure (0% F1) on unseen languages.
- Mechanism: BPE constructs compact vocabularies by merging frequent character sequences. When applied to unseen languages, it fragments named entities into unrecognized subwords, causing the model to over-predict "O" (non-entity) labels and fail to recognize entity boundaries.
- Core assumption: Entity recognition requires consistent token-to-entity alignment; fragmented entities cannot be mapped to learned entity patterns.
- Evidence anchors:
  - [Section IV-C-2]: "BPE completely fails in some cases: The model predicts only the 'O' (non-entity) class in Assamese and Oriya, leading to an F1-score of 0.00%"
  - [Table VII]: BPE achieves 0.00% F1 on Assamese and Oriya vs. SentencePiece's 88.38% and 81.08%
  - [corpus]: arXiv:2509.01147 addresses cross-lingual NER language difference mitigation; arXiv:2511.05324 (BengaliBPE) benchmarks subword tokenization performance
- Break condition: When source and target languages share sufficient vocabulary overlap, BPE's fragmentation reduces and transfer improves (e.g., Hindi→Marathi: 67.79% F1).

### Mechanism 3
- Claim: Compact tokenization efficiency trades off against cross-lingual generalization capability.
- Mechanism: BPE optimizes for vocabulary compression (ratio 1.0) and zero OOV through aggressive merging, but sacrifices the granular character-level information that enables transfer. SentencePiece's moderate compression (ratio 4.33–7.80) retains more linguistic detail for generalization.
- Core assumption: Finer-grained tokenization provides more transferable subword representations across related scripts.
- Evidence anchors:
  - [Table IV]: Vocabulary compression ratios—BPE: 1.0 across all languages; SentencePiece: 4.33–7.80; Character-Level: 41.42–73.35 (excluded for inefficiency)
  - [Fig 1]: Tokenization efficiency (tokens/sentence)—BPE produces fewest tokens, Character-Level produces most
  - [corpus]: arXiv:2505.16868 provides comparative analysis of subword tokenization for Indian languages
- Break condition: Extremely low-resource languages with minimal training data may require additional techniques beyond tokenization alone (e.g., Sindhi: SentencePiece 33.28%, BPE 20.69%—both poor).

## Foundational Learning

- Concept: **Subword Tokenization Algorithms (BPE vs. SentencePiece)**
  - Why needed here: The paper's core comparison requires understanding how BPE's frequency-based merging differs from SentencePiece's unigram language model approach.
  - Quick check question: Can you explain why BPE's 0% OOV rate might indicate overfitting to training vocabulary rather than good generalization?

- Concept: **Zero-Shot Cross-Lingual Transfer**
  - Why needed here: The paper evaluates how tokenization impacts transfer from Hindi/Bengali to unseen Indic languages without target-language training data.
  - Quick check question: Why would a model predict only "O" labels on an unseen language, and how does tokenization contribute to this failure mode?

- Concept: **Named Entity Recognition (Token-Level Sequence Labeling)**
  - Why needed here: NER requires precise entity boundary detection; tokenization that fragments entities directly degrades BIO-tagging accuracy.
  - Quick check question: If a named entity is split into 5 subword tokens by BPE but 2 tokens by SentencePiece, which is more likely to preserve entity-level F1 and why?

## Architecture Onboarding

- Component map: Input Text -> Tokenizer (BPE | SentencePiece | Character-Level) -> IndicBERT Encoder (Transformer) -> NER Classification Head (Token-level BIO tags) -> Entity Predictions (PER, LOC, ORG, O)

- Critical path:
  1. **Tokenizer selection**—determines all downstream performance (SentencePiece recommended for zero-shot)
  2. **Fine-tuning on source languages**—Hindi/Bengali using Naamapadam dataset
  3. **Zero-shot inference on target languages**—Assamese, Oriya, Marathi, Santali, Manipuri, Sindhi

- Design tradeoffs:
  | Dimension | BPE | SentencePiece | Character-Level |
  |-----------|-----|---------------|-----------------|
  | Efficiency | Highest (compact) | Moderate | Lowest (excluded) |
  | In-language F1 | ~95% | ~95% | N/A |
  | Zero-shot F1 | 0–68% | 33–88% | N/A |
  | Morphological preservation | Poor | Good | Excellent but impractical |

- Failure signatures:
  - **BPE zero-shot collapse**: 0% F1 + high accuracy (91%+) → model predicting only "O" labels
  - **Extreme low-resource gap**: Both methods fail on Sindhi (<35% F1) → script mismatch (Arabic) requires additional adaptation
  - **Character-Level exclusion**: Vocab compression ratio >40 → computational infeasibility

- First 3 experiments:
  1. **Intrinsic tokenization audit**: Run FLORES-200 sentences through BPE and SentencePiece; measure OOV rate, vocab compression, and tokens/sentence for your target languages.
  2. **Source-language fine-tuning baseline**: Fine-tune IndicBERT with both tokenizers on Hindi/Bengali Naamapadam; verify comparable in-language F1 (~95%) before zero-shot testing.
  3. **Zero-shot probe on closely related language**: Test Bengali→Assamese transfer first (linguistically similar); expect SentencePiece ~88% F1 vs. BPE ~0% as paper reports—large divergence confirms tokenization effect.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focuses exclusively on Indic languages, leaving uncertainty about whether SentencePiece's advantages extend to non-Indic low-resource languages.
- The extreme BPE failure mode (0% F1) suggests potential confounding factors in experimental setup rather than fundamental algorithmic limitations.
- Character-level baseline was excluded due to computational inefficiency, preventing complete comparison across all tokenization strategies.

## Confidence
- **High confidence** in comparative ranking (SentencePiece > BPE > Character-Level) for the specific Indic language set tested
- **Medium confidence** in proposed mechanisms explaining performance differences
- **Low confidence** in generalizability to non-Indic languages or different resource constraints

## Next Checks
1. **Cross-Linguistic Generalization Test**: Apply the same tokenization comparison framework to low-resource languages from a different language family (e.g., Bantu or Turkic languages) to verify whether SentencePiece maintains its superiority across linguistic boundaries.

2. **BPE Configuration Space Exploration**: Systematically vary BPE vocabulary sizes, merge operations, and training data proportions to determine whether observed 0% F1 failures can be mitigated through hyperparameter tuning.

3. **Intermediate Resource Setting Validation**: Evaluate tokenization performance on languages with moderate training data (100-1000 labeled examples) rather than pure zero-shot or fully supervised scenarios.