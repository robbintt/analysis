---
ver: rpa2
title: Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark
  Biomedical Tasks
arxiv_id: '2507.14045'
source_url: https://arxiv.org/abs/2507.14045
tags:
- llms
- biomedical
- tasks
- arxiv
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates cost-efficient large language models (LLMs)
  for biomedical tasks across text and image modalities. It benchmarks both closed-source
  and open-source models, including GPT-4o-Mini, Gemini-1.5-Flash, Claude-3-Haiku,
  LLaMA-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, and Phi-3.5-Vision, on tasks like text
  classification, question answering, summarization, and multimodal image processing.
---

# Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks

## Quick Facts
- **arXiv ID:** 2507.14045
- **Source URL:** https://arxiv.org/abs/2507.14045
- **Reference count:** 28
- **Primary result:** Cost-efficient LLMs show task-specific performance variation; no single model dominates across all biomedical tasks, with some open-source models matching or exceeding closed-source performance.

## Executive Summary
This paper evaluates cost-efficient large language models for biomedical tasks across text and image modalities. The authors benchmark both closed-source (GPT-4o-Mini, Gemini-1.5-Flash, Claude-3-Haiku) and open-source (LLaMA-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, Phi-3.5-Vision) models on classification, question answering, summarization, and multimodal image processing tasks. Results show no single model consistently outperforms others, with performance varying by task type rather than model size alone. Some open-source models achieve comparable or better results than closed-source models while offering advantages in inference speed and privacy. In multimodal tasks, Gemini-1.5-Flash and Janus-Pro-7B demonstrated the most consistent performance.

## Method Summary
The study evaluates seven benchmark datasets across biomedical tasks: HoC classification, NCBI-Disease NER, PubMedQA, MediQA-QS/ANS summarization, ChEBI-20-MM molecular image captioning, and PathVQA pathology VQA. Models are tested using zero-shot inference with temperature=1.0, including three closed-source API models and ten open-source models (text and multimodal variants). Evaluation metrics include Accuracy, F1, ROUGE, and BERTScore, depending on task type. The authors also conduct scaling experiments comparing performance across different model sizes within model families.

## Key Results
- No single LLM consistently outperforms others across all biomedical tasks
- Open-source models (Qwen-2.5-7B-Instruct, LLaMA-3.1-8B-Instruct) match or exceed closed-source models on specific tasks
- Task-specific model selection outperforms universal model adoption in biomedical applications
- Closed-source models improve with scaling up, while open-source models degrade when scaling down
- Gemini-1.5-Flash and Janus-Pro-7B show most consistent performance in multimodal tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific model selection outperforms universal model adoption in biomedical applications.
- Mechanism: Different model architectures and training corpora create specialized strengths—LLaMA-3.1-8B achieves best performance on MediQA-ANS summarization while GPT-4o-Mini leads on HoC classification. The paper demonstrates this through systematic benchmarking across seven datasets, showing performance variance by task type rather than model size alone.
- Core assumption: Biomedical tasks require different underlying capabilities (entity recognition vs. generation vs. visual reasoning) that individual models optimize for differently during pre-training and instruction tuning.
- Evidence anchors: [abstract] "Our experimental findings indicate that there is no single LLM that can consistently outperform others across all tasks. Instead, different LLMs excel in different tasks." [section 4.1] "LLMs can be chosen for fine-tuning or zero-shot inference based on their task-specific performance in different datasets."

### Mechanism 2
- Claim: Open-source models under 13B parameters can match closed-source API models on specific biomedical tasks when selected strategically.
- Mechanism: Focused architectural efficiency (e.g., Qwen-2.5-7B, LLaMA-3.1-8B) combined with instruction tuning enables competitive zero-shot performance. Qwen-2.5-7B-Instruct outperforms Gemini and Claude on HoC classification; LLaMA-3.1-8B achieves best results on MediQA-ANS across all models.
- Core assumption: Instruction-tuned variants provide sufficient biomedical knowledge transfer without domain-specific fine-tuning, and evaluation metrics (Accuracy, F1, ROUGE, BERTScore) capture task-relevant performance.
- Evidence anchors: [abstract] "Open-source counterparts achieve comparable results (sometimes even better), with additional benefits like faster inference and enhanced privacy." [section 3.3] "For the open-source LLMs, we select models having fewer than 13B parameters... we select the instruction-tuned version of respective open-source models such that they can properly follow the instructions."

### Mechanism 3
- Claim: Scaling behavior differs asymmetrically between closed-source and open-source models in biomedical tasks.
- Mechanism: Closed-source models (Gemini Flash→Pro, GPT-4o-mini→GPT-4o, Claude Haiku→Opus) show consistent improvement when scaling up, suggesting architectural headroom and training data advantages. Open-source models (Qwen family) degrade when scaling down, indicating parameter count thresholds for biomedical task competence.
- Core assumption: The scaling relationships observed are causal rather than correlational—that parameter count and training compute directly enable biomedical task performance rather than other factors like data quality.
- Evidence anchors: [section 4.3] "Scaling up the model size is always helpful for the closed-source models, while scaling down leads to a performance drop for open-source models." [section 4.3, Figure 2] Shows Gemini-1.5 (Flash vs Pro), GPT-4 (o-mini vs o), Claude-3 (Haiku vs Opus) comparisons alongside Qwen scaling down experiments.

## Foundational Learning

- Concept: **Zero-shot inference with instruction-tuned models**
  - Why needed here: All experiments use zero-shot prompting without task-specific fine-tuning; understanding how instruction tuning enables task generalization is prerequisite to interpreting results.
  - Quick check question: Can you explain why instruction-tuned models can follow task prompts they weren't explicitly trained on?

- Concept: **Biomedical task taxonomies (classification vs. generation vs. extraction)**
  - Why needed here: The paper evaluates fundamentally different task types requiring different evaluation metrics (Accuracy/F1 for extraction, ROUGE/BERTScore for generation); conflating these leads to invalid comparisons.
  - Quick check question: Why is F1-score appropriate for NCBI-Disease NER but inappropriate for summarization tasks?

- Concept: **Multimodal grounding in vision-language models**
  - Why needed here: Molecular image captioning and PathVQA require understanding how visual encoders connect to language decoders; performance differences (e.g., Janus-Pro-7B vs. LLaMA-3.2-11B-Vision) reflect architectural choices.
  - Quick check question: What might explain why LLaMA-3.2-11B-Vision performs poorly on captioning but better on PathVQA?

## Architecture Onboarding

- Component map:
  - **Closed-source inference layer**: GPT-4o-Mini, Gemini-1.5-Flash, Claude-3-Haiku (API-based, multimodal)
  - **Open-source text layer**: LLaMA-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, Mistral-7B-v0.3-Instruct, Phi-3.5-Mini-3.8B-Instruct, DeepSeek-R1-Distill variants
  - **Open-source multimodal layer**: Phi-3.5-Vision, Qwen2-VL-7B, LLaVA-Next-7B, Janus-Pro-7B, LLaMA-3.2-11B-Vision
  - **Evaluation pipeline**: Dataset selection → Prompt construction (Section 3.2) → Zero-shot inference (temperature=1.0) → Response parsing (for classification/NER) → Metric computation → Cross-model comparison

- Critical path: Dataset selection → Prompt construction (Section 3.2) → Zero-shot inference (temperature=1.0) → Response parsing (for classification/NER) → Metric computation → Cross-model comparison

- Design tradeoffs:
  - **Closed-source**: Higher per-query cost, data privacy concerns, but potential for better scaling with larger variants
  - **Open-source**: One-time compute cost, full data control, but requires GPU infrastructure (A100 used) and model selection expertise
  - **Model size vs. task coverage**: Smaller models (3.8B) work for some tasks; pathology QA requires larger or better-trained models

- Failure signatures:
  - Very low NER F1 scores (e.g., Mistral-7B: 7.35 F1 on NCBI-Disease) indicate entity extraction failure, not just generation quality issues
  - PathVQA accuracy near 8% (GPT-4o-Mini, Claude-3-Haiku) vs. 40%+ (Gemini-1.5-Flash, Janus-Pro-7B) suggests some models lack visual grounding for medical images
  - LLaMA-3.1-8B achieving only 14.83% on HoC but 32.55 ROUGE-1 on MediQA-ANS shows task-specific failures, not universal model quality

- First 3 experiments:
  1. Replicate the best open-source model for your target task (e.g., Qwen-2.5-7B-Instruct for classification) on a held-out biomedical dataset to validate generalization claims.
  2. Run scaling experiments on your infrastructure: compare Qwen-2.5-3B vs. 7B vs. 14B (if available) on your specific task to determine minimum viable model size.
  3. Test prompt sensitivity: the paper uses simple prompts (Section 3.2); experiment with task-specific prompt variations to measure robustness of reported performance gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the superior zero-shot performance of specific open-source models (e.g., LLaMA-3.1-8B) translate into better fine-tuning outcomes compared to closed-source models for specialized biomedical tasks?
- Basis in paper: [explicit] The authors state their findings provide "insights on which models to select for further fine-tuning" and identify models to use for "continual pre-training or instruction tuning," but the study itself is limited to zero-shot inference.
- Why unresolved: The paper establishes zero-shot rankings but does not validate the assumption that high zero-shot performance correlates with optimal transfer learning capability or fine-tuning stability.
- What evidence would resolve it: Comparative experiments fine-tuning the top-performing open-source models (e.g., LLaMA-3.1, Qwen-2.5) on the benchmark datasets to measure performance delta against fine-tuned closed-source or specialized biomedical models.

### Open Question 2
- Question: Can advanced prompting strategies (e.g., Chain-of-Thought or few-shot learning) close the significant performance gap between open-source and closed-source models in multimodal pathology tasks?
- Basis in paper: [inferred] The results (Table 3) show a stark disparity in PathVQA accuracy between the leading closed-source model (Gemini-1.5-Flash at 40.28%) and the best open-source model (Janus-Pro-7B at 41.19% vs others <22%), yet the methodology restricts evaluation to zero-shot prompts.
- Why unresolved: It is unclear if the lower performance of open-source models is an inherent architectural limitation or a result of the strict zero-shot constraint applied in the evaluation.
- What evidence would resolve it: Benchmarks applying few-shot in-context learning or Chain-of-Thought prompting to the underperforming open-source vision models (e.g., LLaVA-Next, Phi-3.5-Vision) to measure accuracy improvements.

### Open Question 3
- Question: How does the performance of reasoning-based models (DeepSeek-R1 distills) change when explicitly prompted for reasoning steps versus direct answers in biomedical extraction tasks?
- Basis in paper: [inferred] The paper notes that reasoning-based DeepSeek models "could not achieve the best result" in extraction tasks (NCBI-Disease), but the methodology uses standard instruction prompts without activating or analyzing the models' specific reasoning capabilities.
- Why unresolved: The failure of reasoning models to top the leaderboard may be due to the prompting style rather than the model's capacity, leaving their specific utility in biomedicine unexplored.
- What evidence would resolve it: An ablation study comparing the performance of DeepSeek-R1-Distill models on NER and classification when prompted to "think step-by-step" versus direct generation.

## Limitations
- Zero-shot evaluation framework may underestimate model capabilities that could be unlocked through fine-tuning
- Limited to seven benchmark datasets, potentially missing task variations that could reveal different model strengths
- Closed-source vs. open-source comparisons complicated by different access methods and potential data contamination in training sets

## Confidence
- **High Confidence:** No single model consistently outperforms others across all tasks; task-specific performance variations are well-supported by empirical results
- **Medium Confidence:** Asymmetric scaling behavior between closed and open-source models observed but may reflect dataset-specific effects rather than fundamental architectural differences
- **Low Confidence:** Claims about enhanced privacy of open-source models are stated but not empirically validated in the paper

## Next Checks
1. Fine-tuning Validation: Replicate the best-performing open-source models (Qwen-2.5-7B-Instruct, LLaMA-3.1-8B-Instruct) with 1-2 epochs of supervised fine-tuning on each benchmark dataset, then re-evaluate to determine the true gap between zero-shot and fine-tuned performance.

2. Clinical Task Generalization: Test the top 3 models from each category (closed-source, open-source text, open-source multimodal) on a held-out clinical reasoning dataset (e.g., MedQA or MedMCQA) to validate whether benchmark performance translates to practical medical knowledge tasks.

3. Latency and Cost Analysis: Measure actual inference latency and per-query costs for the same models on representative tasks, including GPU memory requirements for open-source models, to quantify the claimed advantages of open-source alternatives beyond accuracy metrics.