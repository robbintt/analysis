---
ver: rpa2
title: 'BanglaFake: Constructing and Evaluating a Specialized Bengali Deepfake Audio
  Dataset'
arxiv_id: '2505.10885'
source_url: https://arxiv.org/abs/2505.10885
tags:
- audio
- deepfake
- dataset
- speech
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# BanglaFake: Constructing and Evaluating a Specialized Bengali Deepfake Audio Dataset

## Quick Facts
- arXiv ID: 2505.10885
- Source URL: https://arxiv.org/abs/2505.10885
- Reference count: 0
- Primary result: Single-speaker Bengali deepfake audio dataset with MOS 3.40/4.01 naturalness/intelligibility scores

## Executive Summary
This paper presents BanglaFake, the first specialized Bengali deepfake audio dataset constructed using a VITS-based Text-to-Speech (TTS) model trained on the SUST TTS corpus. The dataset combines 12,260 real audio samples from multiple speakers with 13,260 synthetic samples generated from Bengali text, providing a resource for developing deepfake detection systems. The synthetic speech achieves high naturalness (MOS 3.40) and intelligibility (MOS 4.01) scores from native speaker evaluations, while t-SNE visualization reveals significant overlap between real and fake audio features in MFCC space, indicating detection challenges.

## Method Summary
The dataset was constructed by training a VITS model on the 30-hour SUST TTS corpus (single male speaker), then using this model to generate synthetic Bengali speech from text in the Mozilla Common Voice corpus. The VITS architecture employs variational inference with a posterior encoder, normalizing flow, stochastic duration predictor, and HiFi-GAN decoder. Real audio samples were collected from five speakers in the Common Voice corpus, while fake samples were generated at 22,050 Hz with 6-7 second duration in LJSpeech format. The resulting dataset contains 25,520 total samples (12,260 real + 13,260 fake).

## Key Results
- Robust-MOS scores of 3.40 (naturalness) and 4.01 (intelligibility) from 30 native Bengali speakers
- t-SNE visualization reveals significant overlap between real and fake MFCC feature spaces
- Dataset contains 25,520 total audio samples (12,260 real + 13,260 synthetic)
- All samples are 6-7 seconds long, sampled at 22,050 Hz in LJSpeech format

## Why This Works (Mechanism)

### Mechanism 1: End-to-End TTS Synthesis via Variational Inference
High-quality synthetic Bengali speech is generated using a VITS-based architecture trained on limited data. Text is converted to phonemes and aligned with latent variables via Monotonic Alignment Search. A posterior encoder extracts latent representations from linear spectrograms, while a normalizing flow refines their distribution. A HiFi-GAN decoder upsamples these variables to raw waveforms, with adversarial training via a multi-period discriminator optimizing spectral realism. The phonetically balanced nature of the SUST TTS corpus (30 hours) is sufficient for single-speaker synthesis quality.

### Mechanism 2: Feature-Space Overlap as Detection Challenge
High synthesis quality creates measurable overlap in acoustic feature space between real and fake audio. MFCC features extracted from both real and synthetic samples occupy similar regions in reduced dimensionality (t-SNE visualization), indicating that traditional acoustic features may be insufficient for discrimination. The t-SNE plot reveals clustering patterns showing the challenge of distinguishing between real and deepfake audio due to significant feature space overlap.

### Mechanism 3: Perceptual Validation via Native Speaker MOS
Human evaluation validates that synthetic speech achieves acceptable naturalness and intelligibility for a low-resource language. 30 native Bengali speakers rated 10 synthesized sentences on naturalness and intelligibility using Robust-MOS methodology that excludes highest and lowest scores to reduce outlier bias. The evaluation demonstrates that the VITS model can produce speech that native speakers find both natural-sounding and comprehensible.

## Foundational Learning

- **Mel-Frequency Cepstral Coefficients (MFCCs)**
  - Why needed here: Core acoustic features used for t-SNE visualization and traditional detection approaches
  - Quick check question: Can you explain why MFCCs approximate human auditory perception and how they differ from raw spectrograms?

- **Variational Autoencoders (VAEs) and Normalizing Flows**
  - Why needed here: VITS combines variational inference with flow-based models; understanding latent space manipulation is essential
  - Quick check question: How does a normalizing flow increase distribution flexibility compared to a standard VAE prior?

- **GAN-based Adversarial Training for Audio**
  - Why needed here: HiFi-GAN decoder and multi-period discriminator optimize perceptual quality
  - Quick check question: What does a multi-period discriminator capture that a single-scale discriminator might miss?

## Architecture Onboarding

- Component map: Text → Phoneme Conversion → Text Encoder → Stochastic Duration Predictor → Posterior Encoder + Normalizing Flow → Prior Encoder + Normalizing Flow → HiFi-GAN Decoder → Linear Spectrogram → Waveform

- Critical path: 1) Text → phoneme conversion (preprocessing) 2) Phonemes → hidden representation (Text Encoder) 3) Duration sampling → alignment (Stochastic Duration Predictor) 4) Latent sampling → waveform (Decoder) 5) Format conversion → LJSpeech structure

- Design tradeoffs: Single-speaker model limits voice diversity but simplifies training; 22,050 Hz sampling rate balances quality vs. computational cost; LJ Speech format ensures compatibility but may constrain metadata flexibility

- Failure signatures: Robust-MOS below 3.0 indicates insufficient naturalness; excessive t-SNE separation may indicate synthetic artifacts (easy to detect = lower quality); training divergence often linked to insufficient phoneme alignment

- First 3 experiments: 1) Replicate MOS evaluation with held-out sentences to validate reported quality scores 2) Train a binary classifier (e.g., wav2vec 2.0 fine-tuned) on the dataset to establish detection baseline 3) Generate samples with varying temperature settings in the stochastic duration predictor to assess naturalness variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does expanding the dataset to include multiple speakers and complex phonemes impact the robustness of deepfake detection models?
- Basis in paper: The authors state in the Conclusion that "Expanding the dataset by incorporating multiple speakers (male and female voices) for better diversity and clarity and including phonemes of complex Bengali words... are kept for future works."
- Why unresolved: The current dataset is restricted to a single male speaker for deepfake generation, which limits the diversity of vocal characteristics available for training robust detection systems.
- What evidence would resolve it: Performance benchmarks of detection models trained on an extended multi-speaker version of the dataset compared to the current single-speaker version.

### Open Question 2
- Question: To what extent do detection models trained on this dataset generalize to deepfake audio generated by architectures other than VITS?
- Basis in paper: The paper generates all deepfake samples using a single "VITS-based Text-to-Speech (TTS) model," despite the Related Work acknowledging the existence of GANs, VAEs, and other synthesis methods.
- Why unresolved: The dataset currently captures artifacts specific only to the VITS architecture; it is unclear if models trained on this data can successfully detect fakes generated by fundamentally different synthesis algorithms.
- What evidence would resolve it: Cross-model evaluation results where a classifier trained on BanglaFake is tested against deepfakes generated by alternative SOTA models (e.g., FastSpeech, Glow-TTS).

### Open Question 3
- Question: Can standard detection architectures effectively distinguish real from fake samples given the significant feature overlap observed in the t-SNE analysis?
- Basis in paper: The t-SNE visualization reveals "significant overlap" between real and deepfake clusters, highlighting "challenges" in separation, yet the paper provides no quantitative detection baseline.
- Why unresolved: While the visualization suggests the high quality of synthetic speech makes detection difficult, the actual vulnerability of standard detection systems (like wav2vec 2.0 or CNNs) remains unquantified.
- What evidence would resolve it: A baseline study reporting the detection accuracy and error rates of standard anti-spoofing models when applied to this dataset.

## Limitations

- Single-speaker bias limits dataset utility for developing robust deepfake detection systems that must handle diverse voice characteristics
- Incomplete VITS hyperparameter specification creates uncertainty about reproducibility of reported naturalness scores
- Bengali phoneme conversion pipeline not fully specified, potentially affecting synthetic speech quality across implementations
- MFCC-based feature analysis may not reflect detection difficulty when using modern learned embeddings

## Confidence

- **High Confidence**: Dataset construction methodology and file format specifications (LJSpeech structure, 22,050 Hz sampling rate, 6-7 second duration). The corpus statistics and composition are clearly documented.
- **Medium Confidence**: Synthetic speech quality as measured by MOS scores. The methodology is sound, but results depend on the unspecified phoneme conversion pipeline and VITS hyperparameters.
- **Low Confidence**: The practical utility of the dataset for deepfake detection model training. While the dataset is technically constructed correctly, its single-speaker nature and MFCC-based analysis may not reflect real-world detection challenges.

## Next Checks

1. **Hyperparameter Replication**: Train VITS on the SUST TTS corpus using the same preprocessing pipeline but with standard hyperparameter settings from the original VITS paper. Compare generated speech quality against reported MOS scores.

2. **Cross-Speaker Generalization**: Evaluate whether the dataset's MFCC overlap patterns persist when synthetic speech is generated using different Bengali TTS models or different speakers. This tests whether observed feature space overlap is model-specific.

3. **Detection Performance Benchmark**: Train and evaluate a modern deepfake detection model (e.g., wav2vec 2.0 fine-tuned) on the BanglaFake dataset. Compare detection accuracy against MFCC-based t-SNE analysis predictions to validate the feature space overlap findings.