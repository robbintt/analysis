---
ver: rpa2
title: 'The Conductor and the Engine: A Path Towards Co-Designed Reasoning'
arxiv_id: '2509.19762'
source_url: https://arxiv.org/abs/2509.19762
tags:
- reasoning
- arxiv
- coda
- code
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CODA, an orchestration framework that adaptively
  directs test-time compute to enhance reasoning in small-to-medium language models.
  It addresses the inefficiency of current reasoning systems where both model and
  framework redundantly perform reasoning, and models struggle with complex instructions.
---

# The Conductor and the Engine: A Path Towards Co-Designed Reasoning

## Quick Facts
- arXiv ID: 2509.19762
- Source URL: https://arxiv.org/abs/2509.19762
- Authors: Yuanxin Wang; Pawel Filipczuk; Anisha Garg; Amaan Dhada; Mohammad Hassanpour; David Bick; Ganesh Venkatesh
- Reference count: 40
- Primary result: Qwen3-32B with CODA achieves 90.7% on AIME 2024, outperforming much larger models like DeepSeek R1 and OpenAI o3-mini

## Executive Summary
CODA is an orchestration framework that adaptively directs test-time compute to enhance reasoning in small-to-medium language models. It addresses the inefficiency where both model and framework redundantly perform reasoning by using adaptive planning, iterative self-refinement, and dynamic problem reformulation. Applied to Qwen3 and GPT-OSS models, CODA achieves state-of-the-art performance, with Qwen3-32B outperforming much larger commercial models on math and coding benchmarks. The framework demonstrates that intelligent orchestration can unlock elite reasoning capabilities in smaller models, offering a scalable and parameter-efficient path to advanced AI systems.

## Method Summary
CODA implements a four-agent orchestration framework: an Adaptive Planner generates multiple execution paths, an Executor attempts solutions with optional self-refinement using code execution feedback, a Self-Reflection agent synthesizes insights across attempts, and a Verification component selects the final answer via majority vote or LLM-as-judge. The framework uses adaptive path selection to run parallel passes including direct executor, coding agent, and full CODA-Simple paths, then aggregates results via plurality vote. For math problems, all three paths execute with early exit on strict majority; for coding, the coding agent with test execution feedback shows significant gains.

## Key Results
- Qwen3-32B with CODA achieves 86.7% on AIME 2024 and 71.86% on LiveCodeBench
- Qwen3-32B outperforms DeepSeek R1 and OpenAI o3-mini on math benchmarks
- Adaptive path selection improves performance from 84.00% to 90.70% on AIME 2024 for Qwen3-32B
- Self-reflection and verification components consistently improve accuracy across benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Path Selection
Allowing the system to choose between direct reasoning and code generation improves mathematical problem-solving. The framework runs multiple solution paths in parallel, using plurality voting to select the final answer. Problems that are computationally tractable via code but difficult for chain-of-thought reasoning get solved through the coding agent; others benefit from direct reasoning. Evidence shows adaptive selection achieves 100% on 15 hard Numina Math problems versus 93% for either path alone.

### Mechanism 2: Iterative Self-Refinement with Execution Feedback
Feeding code execution errors back to the model for problem reformulation improves code generation accuracy. After code execution, the system captures interpreter errors or test failures, appends this feedback to the original prompt, and requests correction. This loop continues until tests pass or retry budget is exhausted. Evidence shows Qwen3-32B improves from 65.70% to 71.86% on LiveCodeBench with this mechanism.

### Mechanism 3: Cross-Pollination via Multi-Attempt Self-Reflection
Synthesizing insights from multiple independent execution attempts produces better final answers than single-path execution. The self-reflection agent receives multiple candidate solutions, identifies inconsistencies, and produces a synthesized response. This is followed by verification via majority vote or LLM-as-judge. Evidence shows removing self-reflection drops Qwen3-8B SciCode from 37.10% to 19.60%, demonstrating consistent accuracy gains.

## Foundational Learning

- **Test-time compute scaling**: Why needed? CODA's core premise is allocating compute dynamically at inference based on problem complexity. Quick check: Can you explain why running multiple solution paths at test time differs from training a larger model?

- **Chain-of-thought reasoning vs. tool-augmented reasoning**: Why needed? The adaptive path selection mechanism requires understanding when to use textual reasoning vs. code execution. Quick check: Given a combinatorics problem requiring enumeration of 256 cases, which path would likely perform better?

- **Best-of-N and majority voting**: Why needed? Verification component uses these aggregation methods; understanding their failure modes is critical. Quick check: What happens to majority voting when all N attempts share the same systematic error?

## Architecture Onboarding

- **Component map**: Input Question → [Planner (optional)] → Executor → Code/Text Output → [Self-Refinement Loop] → Multiple Attempts → Self-Reflection → Verification → Final Answer
- **Critical path**: The adaptive path selection (Algorithm 2) determines which paths to run. For math problems, all three paths execute; early exit if strict majority achieved on any subset.
- **Design tradeoffs**: Planner utility is high for GPQA but muted for coding benchmarks; self-reflection provides consistent accuracy gains but requires multiple model calls; verification shows 5-10% gap to theoretical maximum.
- **Failure signatures**: Effort duplication when conductor and engine both attempt high-level reasoning; instruction-following drift on complex prompts; verifier collapse when majority voting fails.
- **First 3 experiments**: 1) Ablate self-reflection on your domain to quantify contribution. 2) Measure adaptive path delta between mental-math-only vs. coding-only vs. adaptive on 20-50 domain-specific problems. 3) Profile verification gap by computing recall@best_of_N for your pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
Can explicitly training the reasoning engine to act as a verifier via reinforcement learning close the performance gap between CODA and the theoretical recall ceiling? The paper notes a 5-10% gap between final performance and recall@best_of_N, suggesting this as a future direction. Evidence would require a model fine-tuned via RL for verification tasks that significantly reduces this gap without increasing base model size.

### Open Question 2
Does post-training models to utilize an explicit planning phase significantly improve the utility of the orchestration framework across diverse domains? The paper suggests that "a reasoning model specifically post-trained to utilize an explicit planning phase could more fully exploit this component," noting current planning benefits are task-dependent and muted for coding. Evidence would require comparison of standard models vs. planning-tuned models within CODA, showing reduced task-dependency and improved performance in coding tasks.

### Open Question 3
How does the multi-stage orchestration overhead scale relative to simple test-time scaling for low-complexity queries? The paper identifies "wasted compute" as a key challenge and uses a "right-tool-for-the-job" approach, but evaluations focus on complex benchmarks rather than the efficiency of the bypass mechanism for simple queries. Evidence would require latency and compute analysis on a mixed-difficulty dataset, quantifying efficiency for trivial vs. complex problems.

## Limitations
- Framework requires significant test-time compute due to multiple parallel execution paths and iterative refinement loops, making cost-benefit trade-off for production deployment unclear.
- Verification component shows consistent 5-10% gap between actual performance and recall@best_of_N upper bounds, indicating systematic failures in answer selection.
- Adaptive path selection mechanism assumes reliable model distinction between mental math and code execution, but limited analysis of which problem types benefit from each modality.

## Confidence

- **High Confidence**: Claims about baseline performance improvements over standard reasoning models are well-supported by controlled ablations in Tables 3-7.
- **Medium Confidence**: The assertion that CODA represents a "scalable and parameter-efficient path" to advanced reasoning is supported but requires additional deployment cost analysis.
- **Medium Confidence**: Claims about "unlocking elite reasoning" relative to larger models are based on strong benchmark results but don't address potential differences in reasoning depth or robustness.

## Next Checks

1. **Deployment Cost Analysis**: Profile actual computational cost per problem (latency, token usage) for CODA versus baseline reasoning and larger frontier models across all benchmarks. Calculate cost per percentage point improvement to assess true efficiency.

2. **Failure Mode Analysis**: Systematically analyze problems where CODA fails despite correct solutions existing in candidate pools. Categorize failure types (verification errors, path selection mistakes, execution failures) and quantify their relative frequencies.

3. **Generalization Testing**: Evaluate CODA on adversarial reasoning problems designed to exploit known weaknesses in majority voting, such as problems requiring precise numerical answers where small errors in multiple attempts still pass verification thresholds.