---
ver: rpa2
title: 'Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations
  of Knowledge Graph Rules'
arxiv_id: '2508.10971'
source_url: https://arxiv.org/abs/2508.10971
tags:
- rules
- rule
- explanations
- evaluation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rule2Text is a framework that uses large language models to generate
  natural language explanations for logical rules mined from knowledge graphs. The
  approach combines Chain-of-Thought prompting with explicit variable type information,
  achieving improved explanation quality over baseline methods.
---

# Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules

## Quick Facts
- arXiv ID: 2508.10971
- Source URL: https://arxiv.org/abs/2508.10971
- Reference count: 36
- Primary result: Combines Chain-of-Thought prompting with variable type information to generate natural language explanations for KG rules, validated via LLM-as-a-judge framework

## Executive Summary
Rule2Text is a framework that uses large language models to generate natural language explanations for logical rules mined from knowledge graphs. The approach combines Chain-of-Thought prompting with explicit variable type information, achieving improved explanation quality over baseline methods. An LLM-as-a-judge evaluation framework was developed and validated against human annotations, showing strong agreement (Spearman correlation 0.69). The best model, Gemini 2.0 Flash, achieved correctness and clarity scores averaging above 4.6 on a 5-point scale. Fine-tuning the open-source Zephyr model on high-quality ground truth data produced significant improvements, particularly for the domain-specific ogbl-biokg dataset, where ROUGE scores increased from 0.02 to 0.78. The framework also includes a type inference module for KGs lacking explicit type information.

## Method Summary
The framework mines Horn rules from knowledge graphs using AMIE 3.5.1, extracts or infers variable entity types, and generates explanations using LLMs with Chain-of-Thought prompting and type-aware prompts. Explanations are evaluated using an LLM-as-a-judge framework that compares explanations against verification questions about logical consistency. High-scoring explanations are combined with human-corrected low-scoring ones to create ground truth for fine-tuning smaller open-source models like Zephyr. The approach was validated across four datasets (FB15k-237, FB-CVT-REV, FB+CVT-REV, ogbl-biokg) with experiments comparing zero-shot, few-shot, type-aware, and CoT prompting strategies.

## Key Results
- Gemini 2.0 Flash with CoT and variable types achieved average correctness and clarity scores above 4.6 on 5-point scales
- LLM-as-a-judge showed strong agreement with human annotations (Spearman correlation 0.69)
- Fine-tuning Zephyr on ground truth data dramatically improved ROUGE scores from 0.02 to 0.78 on the domain-specific ogbl-biokg dataset
- Type information significantly improved explanation accuracy, with correctness scores increasing from 3.94 to 4.21

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining Chain-of-Thought prompting with explicit variable type information improves explanation quality for KG rules
- Mechanism: CoT guides the model through structured reasoning steps while explicit type information resolves ambiguity about what entity classes variables represent. Without type info, LLMs frequently misinterpret variable entity types, leading to incorrect explanations. The synergy works because CoT provides reasoning scaffolding and types ground variables in semantic space
- Core assumption: The LLM has sufficient background knowledge to map relation labels and entity types to domain concepts
- Evidence anchors:
  - [abstract] "approach combines Chain-of-Thought prompting with explicit variable type information, achieving improved explanation quality"
  - [Section 4.1] "providing variable type information significantly improved the model's performance in generating accurate explanations"
  - [Table 3] Correctness improved from 3.94 (zero-shot) to 4.21 (with variable types); Table 4 shows Gemini 2.0 Flash with CoT achieves 4.67 correctness

### Mechanism 2
- Claim: An LLM-as-a-judge with structured evaluation prompts can substitute for human evaluation at scale
- Mechanism: The judge is constrained by explicit scoring rubrics, structured yes/no verification questions (e.g., "Do all variable entities stated in the rule appear in the explanation?"), and few-shot exemplars. This design prevents the judge from regenerating explanations and forces comparison-based assessment. Gemini 2.0 Flash was selected because it did not exhibit self-enhancement bias
- Core assumption: The judge LLM's reasoning alignment with humans generalizes beyond the validation set
- Evidence anchors:
  - [abstract] "LLM-as-a-judge evaluation framework was developed and validated against human annotations, showing strong agreement (Spearman correlation 0.69)"
  - [Section 5.3] "the prompt explicitly instructs the model to assess the quality of the given explanation, not to regenerate it"
  - [Section 7] "Krippendorff's Alpha of 0.59 reflects moderate consensus when accounting for chance agreement"

### Mechanism 3
- Claim: Fine-tuning small open-source models on LLM-generated, human-verified ground truth yields dramatic improvements for domain-specific KGs
- Mechanism: A strong proprietary model (Gemini 2.0 Flash) generates candidate explanations; humans verify/correct only problematic cases; this creates training data for fine-tuning. Domain-specific terminology and reasoning patterns get encoded into the smaller model
- Core assumption: The ground truth quality is sufficient—errors in pseudo-labels will propagate
- Evidence anchors:
  - [abstract] "Fine-tuning the open-source Zephyr model on high-quality ground truth data produced significant improvements, particularly for the domain-specific ogbl-biokg dataset, where ROUGE scores increased from 0.02 to 0.78"
  - [Section 4.3] "Those receiving high correctness scores can be treated as pseudo-ground truth for fine-tuning smaller open-source models"
  - [Table 5] Zephyr fine-tuned on ogbl-biokg: BLEU .38→.55, ROUGE .02→.78, METEOR .36→.81

## Foundational Learning

- Concept: **Horn clauses and rule mining (AMIE)**
  - Why needed here: The framework operates on logical rules of the form B₁ ∧ B₂ ∧ ... ∧ Bₙ ⇒ H. Understanding atoms, variables, support, head coverage, and standard confidence is required to interpret what the LLM is explaining
  - Quick check question: Given rule (?a, /parent_of, ?b) ⇒ (?a, /father_of, ?b), identify the body, head, and what "support" measures

- Concept: **Knowledge graph schema and type systems**
  - Why needed here: Variable type extraction relies on understanding how KG schemas (e.g., Freebase's /domain/type/label format, ogbl-biokg's entity ID prefixes) encode entity types. Type inference for KGs without explicit types requires reasoning from property labels and instances
  - Quick check question: In Freebase, what type would you infer for variable ?a in (?a, /american_football/player_rushing_statistics/team, Team_X)?

- Concept: **LLM evaluation metrics (BLEU, ROUGE, METEOR, perplexity)**
  - Why needed here: The paper uses multiple automatic metrics to assess fine-tuning quality. Understanding what each measures (precision vs. recall, semantic similarity) is critical for interpreting results
  - Quick check question: Why might ROUGE improve dramatically (0.02→0.78) while BLEU improves moderately (.38→.55)?

## Architecture Onboarding

- Component map:
  AMIE 3.5.1 -> Type Inference Module -> Prompt Engineering Layer -> LLM Generator -> LLM-as-a-Judge -> Ground Truth Constructor -> Fine-Tuning Pipeline

- Critical path:
  1. Mine rules with AMIE (min head coverage 0.1, max atoms 3)
  2. Extract/infer variable entity types
  3. Generate explanations with CoT + type-aware prompts
  4. Evaluate with LLM-as-a-judge (3 consistency runs per explanation)
  5. Filter high-scoring pairs + human-correct low-scoring pairs
  6. Fine-tune Zephyr (2 epochs, lr=5e-5, batch=2)

- Design tradeoffs:
  - Proprietary vs. open-source: Gemini 2.0 Flash achieves best quality but requires API; Zephyr enables local deployment (data privacy)
  - Ground truth size: 500 pairs used (400 train/50 val/50 test); larger datasets may improve but increase annotation cost
  - Judge model selection: Gemini 2.0 Flash chosen for lack of self-enhancement bias; GPT-4o mini exhibited bias

- Failure signatures:
  - Concatenated relations (n-ary→binary converted labels) cause hallucinated entities/relations
  - Rules with mediator nodes (CVT) receive lower clarity scores across all models
  - Type inference fails when random instances are unrepresentative (e.g., inferring "tennis player" when correct type is "sports professional athlete")
  - Base Zephyr on ogbl-biokg shows near-zero ROUGE (0.02), indicating content coverage failure

- First 3 experiments:
  1. Reproduce phase 1 baseline: Run zero-shot vs. few-shot on FB15k-237 rules, compare correctness/clarity scores to validate prompt engineering setup
  2. Ablate type information: On FB-CVT-REV rules, generate explanations with and without variable types; measure correctness delta (expected: ~0.27 improvement)
  3. Validate judge alignment: On 100 human-annotated explanations, compute Spearman correlation between LLM judge and human scores; target ≥0.65

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Rule2Text framework perform when applied to logical rules with complexity significantly exceeding the current 3-atom limit?
- Basis in paper: [explicit] The conclusion states that "Future research directions include evaluating more complex rules beyond AMIE’s capabilities."
- Why unresolved: The experimental setup (Section 6) explicitly constrained rules to a maximum of 3 atoms to optimize performance, leaving the framework's efficacy on deeper or recursive logical structures untested
- What evidence would resolve it: Extending the methodology to rules with >3 atoms or different mining algorithms and measuring the degradation (or maintenance) of correctness and clarity scores

### Open Question 2
- Question: Can type inference mechanisms be refined to prevent the hallucination of overly specific subtypes based on limited random instances?
- Basis in paper: [explicit] The authors identify "developing more sophisticated type inference mechanisms" as a future direction, noting an error where the model inferred "tennis player" (subtype) instead of "sports professional athlete" (parent type)
- Why unresolved: The current approach (Section 4.2) relies on providing the LLM with only three random instances, which biases the inference toward the specific domain of the examples rather than the rule's general semantics
- What evidence would resolve it: A comparative study evaluating type inference accuracy using stratified sampling against the current random sampling approach, specifically measuring semantic distance from the correct parent type

### Open Question 3
- Question: How can the framework be adapted to close the performance gap in explanation quality for concatenated relations and mediator nodes?
- Basis in paper: [inferred] While the paper highlights the success of the general framework, results in Section 7 consistently show lower correctness and clarity scores for concatenated relations and mediator nodes compared to binary relations
- Why unresolved: The "lengthy" format of concatenated labels (Section 6.1) poses a "greater challenge" for LLMs, and the current Chain-of-Thought prompting did not fully equalize performance across relation types
- What evidence would resolve it: Ablation studies testing specific prompt augmentations (e.g., hierarchical decomposition of concatenated labels) to determine if the score disparity with binary relations can be eliminated

## Limitations
- LLM-as-a-judge framework shows moderate agreement with human annotations (Spearman 0.69, Krippendorff's Alpha 0.59), indicating some variability in evaluation consistency
- Approach relies heavily on proprietary models (Gemini 2.0 Flash) for both generation and evaluation, creating dependency on access to strong proprietary models
- Domain-specific adaptation remains challenging, as evidenced by base Zephyr model's near-zero ROUGE score (0.02) on ogbl-biokg before fine-tuning

## Confidence
- High confidence: Core mechanism that combining CoT prompting with explicit variable type information improves explanation quality
- Medium confidence: LLM-as-a-judge framework as scalable evaluation alternative to human annotation
- Medium confidence: Fine-tuning approach's effectiveness given dramatic improvements but limited dataset sizes

## Next Checks
1. Conduct ablation study removing type information from prompts to quantify exact contribution of variable type information to explanation quality (expected correctness drop of ~0.27 points)
2. Implement complete LLM-as-a-judge evaluation pipeline and measure Spearman correlation with human judgments on held-out validation set of 100 rule-explanation pairs
3. Test framework on new KG dataset with explicit type information (e.g., Wikidata) to evaluate generalization beyond four datasets used in study