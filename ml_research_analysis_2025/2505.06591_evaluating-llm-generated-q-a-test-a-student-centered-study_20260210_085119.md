---
ver: rpa2
title: 'Evaluating LLM-Generated Q&A Test: a Student-Centered Study'
arxiv_id: '2505.06591'
source_url: https://arxiv.org/abs/2505.06591
tags:
- test
- students
- items
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that LLM-generated Q&A tests can match\
  \ human-authored assessments in psychometric quality and perceived user satisfaction.\
  \ Using GPT-4o-mini, 17 usable NLP course items were generated with mean discrimination\
  \ a = 0.75 and mean difficulty b = \u22124.31, comparable to expert-written benchmarks."
---

# Evaluating LLM-Generated Q&A Test: a Student-Centered Study

## Quick Facts
- arXiv ID: 2505.06591
- Source URL: https://arxiv.org/abs/2505.06591
- Reference count: 14
- Generated 17 NLP course items with mean discrimination a = 0.75 and difficulty b = -4.31, comparable to expert benchmarks

## Executive Summary
This study evaluates whether LLM-generated Q&A tests can match human-authored assessments in psychometric quality and user satisfaction. Using GPT-4o-mini with structured output enforcement, the researchers generated 17 usable multiple-choice items for an NLP course. The items were reviewed by three experts, administered to 45 students, and analyzed using IRT and DIF methods. Results show the generated items achieve psychometric parameters comparable to human benchmarks, with moderate correlation between exam scores and student ratings.

## Method Summary
The pipeline used GPT-4o-mini (temperature 0.2) with LangChain's StructuredOutputParser and Pydantic schema to generate 22 multiple-choice items from course context snippets. Three experts independently reviewed items in single-blind fashion. Items were administered via Google Forms to 45 students who rated each question's quality (1-5 stars) and flagged ambiguous/complicated items. Psychometric analysis used 2PL IRT with mirt/R, and DIF was assessed via logistic regression.

## Key Results
- Mean discrimination a = 0.75 and difficulty b = -4.31, comparable to expert-written benchmarks
- Student-perceived quality averaged 3.8/5, expert-perceived quality 4.5/5
- 45 examinees showed moderate positive correlation (r = 0.48) between exam scores and item ratings
- DIF analysis flagged two items (exam_1 and exam_21) for review due to subgroup bias

## Why This Works (Mechanism)

### Mechanism 1: Structured Output Enforcement
Schema-constrained generation via Pydantic enforces consistent item format, reducing manual correction needs. The structured response format (question text, four options, correct answer) maintains format compliance. However, evidence is weak as no direct corpus comparison exists between structured and unstructured output for assessment generation.

### Mechanism 2: Human-in-the-Loop Validation
Three experts independently review items before student administration, functioning as a quality filter. Student and expert ratings converge on similar quality assessments (r = 0.48 correlation). However, the necessity of human validation remains uncertain since the study didn't test whether automated checks alone would suffice.

### Mechanism 3: IRT-Based Psychometric Calibration
2PL IRT estimates discrimination and difficulty parameters, identifying problematic items. Parameters ranged a ∈ [-1.28, 2.06], b ∈ [-45.39, 9.26]. Five items were dropped due to zero variance. DIF analysis flagged items with ability-group bias, though standard IRT assumptions for generated items remain underexplored.

## Foundational Learning

- **Two-Parameter Logistic (2PL) IRT Model**: Essential for interpreting discrimination (a = 0.75) and difficulty (b = -4.31) parameters. Quick check: If an item has a = 2.0 and b = 0, what is P(correct) for θ = 0?

- **Differential Item Functioning (DIF)**: Critical for understanding bias detection. Quick check: What distinguishes "uniform DIF" from "non-uniform DIF," and which type was detected?

- **Structured Output Parsing (Pydantic/LangChain)**: Required to implement schema-constrained generation. Quick check: What happens when the LLM returns JSON missing a required field?

## Architecture Onboarding

Course Materials → Context Snippets → GPT-4o-mini (temp=0.2) → StructuredOutputParser (Pydantic schema) → Parsed Items → Expert Review (3 reviewers, single-blind) → Google Forms API → Student Administration → Response Logs → IRT Calibration (mirt/R, 2PL + graded-response) → DIF Analysis → Item Flagging

Critical path: Context quality → question relevance (upstream dependency) → Schema validation → downstream automation reliability → Student response variance → IRT parameter stability (5 items dropped due to zero variance)

Design tradeoffs: N=45 enables pilot but limits IRT precision (conventional threshold ~500); closed-ended format simplifies evaluation but limits higher-order skills assessment; single generation pass preserves experimental validity but may retain suboptimal items

Failure signatures: Negative discrimination (a < 0) in exam_1, exam_4, exam_13, exam_19; extreme difficulty (|b| > 20) in exam_10 (b = -45.4), exam_18 (b = -16.4), exam_11 (b = 9.3); zero-variance items (5 of 22 dropped); DIF flags in exam_1 and exam_21

First 3 experiments: 1) Scale to N ≥ 200 for stable IRT estimates; 2) Compare structured vs. unstructured generation (ablation study); 3) Integrate automated bias-detection prompts and re-run DIF analysis

## Open Questions the Paper Calls Out

1. Does psychometric quality hold when directly compared to human-authored test batteries? (Section 4.3 - needs controlled head-to-head comparison)

2. Can the pipeline maintain reliability for free-response or coding tasks? (Section 6 - current focus on closed-ended questions only)

3. Does automated bias-detection prompting effectively reduce DIF? (Section 6 - proposed next step after flagging two biased items)

4. What learning gains result from students actively revising AI-generated items? (Section 6 - pedagogical impact of revision process untested)

## Limitations

- Sample size of 45 examinees limits IRT precision and statistical power for DIF detection (conventional recommendation ~500)
- Single generation pass without iterative refinement may retain suboptimal items
- Focus on closed-ended questions constrains assessment to lower Bloom's taxonomy levels

## Confidence

High Confidence (4-5/5): Psychometric parameters comparable to human benchmarks, convergent validity in quality ratings
Medium Confidence (2-3/5): Structured output mechanism's contribution inferred but not experimentally validated
Low Confidence (1/5): Human-in-the-loop validation's necessity remains uncertain without testing automated alternatives

## Next Checks

1. Scale sample to N ≥ 200 to stabilize IRT parameter estimates and enable subgroup DIF analysis
2. Conduct ablation study comparing structured vs. unstructured output generation
3. Implement and evaluate automated bias-mitigation prompts, then re-run DIF analysis