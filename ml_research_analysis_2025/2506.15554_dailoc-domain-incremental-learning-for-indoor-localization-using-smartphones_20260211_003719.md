---
ver: rpa2
title: 'DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones'
arxiv_id: '2506.15554'
source_url: https://arxiv.org/abs/2506.15554
tags:
- domain
- localization
- indoor
- learning
- dailoc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DAILOC addresses domain shifts in Wi-Fi fingerprinting indoor localization
  due to device heterogeneity and temporal changes. It introduces a domain-incremental
  learning framework that disentangles location-relevant features from domain-specific
  variations using a multi-level variational autoencoder, and mitigates catastrophic
  forgetting via memory-guided class latent alignment.
---

# DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones

## Quick Facts
- arXiv ID: 2506.15554
- Source URL: https://arxiv.org/abs/2506.15554
- Reference count: 29
- Primary result: DAILOC achieves up to 2.74× lower average error and 4.6× lower worst-case error than state-of-the-art methods for domain-incremental Wi-Fi fingerprinting localization.

## Executive Summary
DAILOC addresses the challenge of domain shifts in Wi-Fi fingerprinting indoor localization due to device heterogeneity and temporal changes. The framework introduces a domain-incremental learning approach that disentangles location-relevant features from domain-specific variations using a multi-level variational autoencoder. By mitigating catastrophic forgetting through memory-guided class latent alignment, DAILOC demonstrates robust performance across multiple smartphones, buildings, and time periods, achieving significant improvements in localization accuracy compared to existing methods.

## Method Summary
DAILOC uses a multi-level variational autoencoder (ML-VAE) with a disentangled latent space: z_D for domain-specific variations and z_C for location-relevant features. The method employs a two-stage staggered training approach: first adapting the encoder to stabilize z_D while freezing the classifier, then updating the classifier using pseudo-labels and Maximum Mean Discrepancy (MMD) alignment against stored prototypes. A memory buffer stores one prototype embedding per Reference Point (RP) via reservoir sampling. The framework requires supervised onboarding for new devices but enables unsupervised adaptation for known devices through pseudo-label generation and statistical alignment.

## Key Results
- DAILOC achieves up to 2.74× lower average localization error compared to state-of-the-art methods.
- Worst-case localization error is reduced by up to 4.6× compared to baselines.
- The method demonstrates robust performance across multiple smartphones, buildings, and time periods.

## Why This Works (Mechanism)

### Mechanism 1
Disentangling domain-specific variations from location-relevant features reduces pseudo-label noise during unsupervised adaptation. A multi-level variational autoencoder splits the latent space into z_D (domain-specific, e.g., device bias, temporal drift) and z_C (location-relevant). Domain-aware reparameterization uses a fixed noise vector ε_D per domain, forcing coherent domain-level variations into z_D while leaving z_C uncontaminated. The classifier is frozen during this adaptation stage.

### Mechanism 2
Statistical alignment of class embeddings with historical prototypes mitigates catastrophic forgetting without storing raw data. The Class Embedding Statistical Alignment (CESA) module computes Maximum Mean Discrepancy (MMD) loss between current pseudo-labeled embeddings and stored prototypes (one per RP via reservoir sampling). During alignment, encoder/decoder are frozen; only the classifier updates. This preserves decision boundaries from prior domains.

### Mechanism 3
Two-stage staggered training (encoder-first, then classifier) enables stable unsupervised adaptation. Stage 1 updates only the encoder under reconstruction + KL losses to stabilize z_D before pseudo-labels are generated. Stage 2 freezes encoder/decoder and updates classifier using pseudo-labels + MMD alignment. This prevents feedback loops where noisy labels corrupt features.

## Foundational Learning

### Variational Autoencoders (VAE)
Why needed: Understanding reparameterization trick, KL divergence regularization, and how latent spaces are structured is essential for grasping the ML-VAE design.
Quick check: Can you explain why KL divergence encourages the posterior to stay close to a standard normal prior?

### Catastrophic Forgetting in Continual Learning
Why needed: Core problem DAILOC solves; understanding stability-plasticity tradeoff clarifies why CESA and disentanglement are necessary.
Quick check: Why does updating a neural network on new data cause performance degradation on old data?

### Domain Adaptation vs. Domain-Incremental Learning
Why needed: DAILOC differs from pairwise domain adaptation; it handles sequential, open-ended domain arrival without task boundaries.
Quick check: What is the difference between adapting once from domain A to B vs. continuously adapting as domains A, B, C, D arrive sequentially?

## Architecture Onboarding

### Component map
ML-VAE Encoder: RSS input → 256→128 ReLU → two branches (z_D: 16-dim mean/variance; z_C: 64-unit intermediate → 16-dim) → concatenated with domain noise → ML-VAE Decoder → 128→256 → sigmoid output → Classifier: z_C → 64-unit hidden → SoftMax over RPs → Euclidean Distance to ground truth

### Critical path
1. Offline pretraining on labeled RSS → joint optimization of encoder, decoder, classifier
2. Online: Unknown device → supervised onboarding → populate memory buffer
3. Online: Known device → Stage 1 (encoder adaptation, frozen classifier) → Stage 2 (pseudo-label generation → CESA alignment, frozen encoder/decoder)

### Design tradeoffs
Buffer size vs. memory constraints: M = # RPs ensures minimum coverage but may under-represent intra-class variance. Disentanglement granularity: 16-dim each for z_D and z_C may be insufficient for complex multi-building environments. Pseudo-label reliance: No ground truth post-deployment; systematic errors can compound.

### Failure signatures
Rapid error increase on previously seen devices → catastrophic forgetting (CESA insufficient). High variance in pseudo-label error after disentanglement → z_C contaminated by domain shift. New device onboarding fails to converge → representation buffer poorly initialized.

### First 3 experiments
1. Baseline replication: Train CNRP or DARE on Building 1 data, measure error drift over time and across devices to reproduce Fig. 1 behavior.
2. Ablation on disentanglement: Compare pseudo-label error before/after Stage 1 encoder adaptation on held-out time instances (reproduce Fig. 8).
3. Device ordering sensitivity: Shuffle device introduction sequences and measure final error to verify robustness to arrival order (reproduce Fig. 9).

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework effectively handle dynamic feature spaces where the number or identity of Wi-Fi Access Points (APs) changes significantly without model re-architecture? The current methodology likely assumes a static feature space, whereas real-world environments frequently add or remove wireless infrastructure.

### Open Question 2
Is it possible to extend DAILOC to support zero-shot or fully unsupervised onboarding for completely unknown devices, removing the requirement for initial labeled data? The current framework relies on supervised initialization to generate location-relevant features and memory prototypes.

### Open Question 3
How does the memory footprint and computational cost of the Class Embedding Statistical Alignment (CESA) module scale with a significant increase in Reference Points (RPs)? The memory buffer size grows linearly with the number of RPs, which may become prohibitive in large-scale deployments.

## Limitations
- Relies on pseudo-labels during online adaptation, which may propagate errors if domain shifts are severe or disentanglement fails.
- Requires supervised onboarding per device, which may not be feasible in all deployment scenarios.
- Prototype representativeness may degrade over time with significant temporal drift.

## Confidence
- **High confidence**: Claims about catastrophic forgetting mitigation via CESA alignment and reduction in pseudo-label noise through disentanglement are directly supported by ablation experiments and quantitative results.
- **Medium confidence**: Claims about up to 2.74× lower average error and 4.6× lower worst-case error depend on experimental setup; validation across different datasets would strengthen generalizability.
- **Low confidence**: Claims about robustness to arrival order are partially supported but could benefit from more extensive ablation over diverse device and temporal orderings.

## Next Checks
1. Evaluate DAILOC on a publicly available Wi-Fi fingerprinting dataset (e.g., UJIIndoorLoc) to verify performance claims beyond the paper's dataset.
2. Monitor MMD loss trends over time to assess whether prototypes remain representative as temporal drift accumulates.
3. Measure runtime memory usage of the representation buffer as the number of RPs and domains increases to quantify scalability limits.