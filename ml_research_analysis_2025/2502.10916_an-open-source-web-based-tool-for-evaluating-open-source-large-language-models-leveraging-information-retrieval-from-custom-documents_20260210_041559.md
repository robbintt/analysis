---
ver: rpa2
title: An Open-Source Web-Based Tool for Evaluating Open-Source Large Language Models
  Leveraging Information Retrieval from Custom Documents
arxiv_id: '2502.10916'
source_url: https://arxiv.org/abs/2502.10916
tags:
- speech
- acts
- latest
- experiment
- better
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an open-source web-based tool for evaluating\
  \ open-source large language models (LLMs) in conversational AI settings. The key\
  \ innovation is incorporating user speech acts\u2014communicative intents\u2014\
  into the conversational agent's input to improve response relevance and alignment."
---

# An Open-Source Web-Based Tool for Evaluating Open-Source Large Language Models Leveraging Information Retrieval from Custom Documents

## Quick Facts
- arXiv ID: 2502.10916
- Source URL: https://arxiv.org/abs/2502.10916
- Authors: Godfrey I
- Reference count: 14
- Primary result: Web tool integrates speech acts, document retrieval, and 10 metrics to evaluate 5 open-source LLMs, showing improved alignment in larger models when speech acts are included

## Executive Summary
This paper presents an open-source web-based tool for evaluating open-source large language models (LLMs) in conversational AI settings. The key innovation is incorporating user speech acts—communicative intents—into the conversational agent's input to improve response relevance and alignment. The tool integrates speech act identification, document-based knowledge retrieval, and performance evaluation using 10 standard metrics (ROUGE-1/2/L, BERT-Precision/Recall/F1, QA-Ref/Cand, METEOR, Perplexity). Experiments across 5 open-source models (Llama2:13B, TinyLlama, Llama3-ChatQA, Llama3, Mistral) with two knowledge documents showed that larger models (Llama2, Llama3) demonstrated improved alignment when speech acts were included, achieving higher Question-Answer and linguistic similarity scores. Smaller models (TinyLlama) showed increased perplexity and mixed performance, indicating challenges processing queries with explicit speech acts. Results highlight the potential of speech acts to enhance conversational depth while underscoring the need for model-specific optimizations to address increased computational costs and response times.

## Method Summary
The tool is a web-based application that enables users to evaluate open-source LLMs using custom documents as knowledge sources. Users can upload documents in .txt or .pdf format, select from five pre-configured models (Llama2:13B, TinyLlama, Llama3-ChatQA, Llama3, Mistral), and toggle speech act inclusion. The system identifies the illocutionary force of user queries using a separate speech act classifier, then constructs prompts that optionally include this pragmatic metadata. Responses are generated locally via Ollama integration and evaluated against the knowledge document using 10 metrics covering lexical, semantic, and task-specific dimensions. The evaluation measures include ROUGE-1/2/L for lexical overlap, BERT-based precision/recall/F1 for semantic similarity, QA-Ref/Cand for question-answering alignment, METEOR for translation-based evaluation, and perplexity for fluency assessment.

## Key Results
- Larger models (Llama2, Llama3) showed improved alignment metrics when speech acts were included, with higher Question-Answer and linguistic similarity scores
- Smaller models (TinyLlama) exhibited increased perplexity (up to 2612.20) and mixed performance when processing queries with explicit speech acts
- Model performance varied by document type, with Document 002 causing missing results for Llama3-chatqa-latest
- The tool successfully demonstrated differential model sensitivity to pragmatic enrichment across the 10 evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting speech act classification into user queries may improve contextual alignment between retrieved knowledge and user intent in larger LLMs.
- **Mechanism:** A separate speech act classifier identifies the illocutionary force (e.g., question, assertion, request) of the user query. This label is appended to the prompt as explicit metadata, allowing the generative model to condition its response on both semantic content and pragmatic intent.
- **Core assumption:** Models can exploit explicit pragmatic cues when they are surfaced as text, and this additional signal aids retrieval-to-response alignment.
- **Evidence anchors:**
  - [abstract] "able to extract and then inject this overlooked feature in the encoder-decoder pipeline"
  - [section 4.2] Prompt pseudocode shows conditional injection: `if include_illocutionary_force: instruction += "Consider the user's communicative intent... characterized by the speech acts"`
  - [corpus] Weak direct corpus support; no neighboring papers test speech act injection specifically.
- **Break condition:** Models with insufficient parameter capacity or weak instruction-following ability may treat the speech act tag as noise, increasing perplexity rather than alignment (observed in TinyLlama).

### Mechanism 2
- **Claim:** Constraining model responses to uploaded knowledge documents provides a grounded reference for evaluating answer fidelity.
- **Mechanism:** User-uploaded documents (.txt or .pdf) are ingested and passed as `knowledge_text` in the prompt. The model is instructed to use only this content, creating a controlled retrieval environment where responses can be compared to source material via lexical and semantic metrics.
- **Core assumption:** The model will adhere to the constraint and not hallucinate beyond the provided document, enabling meaningful metric comparison.
- **Evidence anchors:**
  - [abstract] "utilise uploaded specific documents for the chat agent to use for its information retrieval"
  - [section 3.3] "upload and select various knowledge files in either .txt or .pdf formats"
  - [corpus] "Knowledge Extraction on Semi-Structured Content" examines document-based QA, supporting the general retrieval premise but not this specific implementation.
- **Break condition:** If documents exceed context window limits or models ignore instructions, responses will diverge from source, invalidating metric comparisons.

### Mechanism 3
- **Claim:** Multi-metric evaluation (lexical, semantic, and task-specific) reveals differential model sensitivity to pragmatic enrichment.
- **Mechanism:** The tool computes 10 metrics (ROUGE-1/2/L, BERT-Precision/Recall/F1, QA-Ref, QA-Cand, METEOR, Perplexity) comparing each response to the knowledge document. This multi-dimensional view separates improvements in semantic similarity from lexical overlap or fluency.
- **Core assumption:** These metrics collectively capture meaningful aspects of response quality and alignment.
- **Evidence anchors:**
  - [section 4.4] Detailed definitions of each metric category: relevance/quality, fluency/coherence
  - [tables 1-2] Raw metric values for all model/query combinations with and without speech acts
  - [corpus] "Open-Source Tool for Evaluating Human-Generated vs. AI-Generated Medical Notes" uses PDQI-9 framework, indicating domain-specific evaluation tools are an active area, but no direct metric overlap.
- **Break condition:** If metrics correlate highly or fail to capture pragmatic improvements, the evaluation may misrank models or obscure real differences.

## Foundational Learning

- **Concept: Speech Act Theory**
  - **Why needed here:** The paper's central intervention depends on classifying utterances by their communicative purpose (e.g., requesting, asserting). Without this background, the illocutionary force injection appears arbitrary.
  - **Quick check question:** Can you explain why "Can you pass the salt?" is a request, not a yes/no question about ability?

- **Concept: Encoder-Decoder Prompting with Context Windows**
  - **Why needed here:** The system constructs prompts with multiple components (user input, conversation history, knowledge text, speech act). Understanding how these compete for context capacity is essential for debugging truncation or coherence failures.
  - **Quick check question:** What happens to earlier context if `num_ctx=4096` and the knowledge document alone contains 5000 tokens?

- **Concept: Embedding-Based Similarity Metrics**
  - **Why needed here:** BERT-based metrics (Precision, Recall, F1) require understanding that they compare dense vector representations rather than surface token overlap. This is critical for interpreting why a response can score high on BERT-F1 but low on ROUGE-1.
  - **Quick check question:** Why might a paraphrased response have high BERT-F1 but low ROUGE-L compared to a source document?

## Architecture Onboarding

- **Component map:** Frontend (Web App) -> Speech Act Classifier -> Prompt Constructor -> Ollama Integration -> Evaluation Module -> Results display
- **Critical path:** Query input → speech act classification → prompt assembly (with optional illocutionary force) → Ollama generation → metric computation → results display. The speech act toggle is the single point controlling experimental condition.
- **Design tradeoffs:**
  - Fixed sampling (`top_k=1`, `seed=42`) ensures reproducibility but eliminates response diversity; may underrepresent model capabilities.
  - Local GPU deployment (12GB VRAM) limits model size; larger models may require quantization or cloud offload.
  - Small experiment scale (2 documents, 2 queries each) enables detailed analysis but limits statistical confidence.
- **Failure signatures:**
  - TinyLlama shows high perplexity (up to 2612.20) with speech acts, indicating capacity mismatch.
  - Llama3-chatqa-latest returns no results for Document 002 (tables show "-"), suggesting either timeout, crash, or prompt formatting incompatibility.
  - Negative QA-Cand scores in some conditions indicate metric calculation edge cases.
- **First 3 experiments:**
  1. **Baseline sanity check:** Run all 5 models on both documents with speech acts disabled; verify metrics are in expected ranges and no cells are missing.
  2. **Speech act ablation:** For a single model (e.g., Llama2:13B), run 10 queries with speech acts enabled vs. disabled; compute delta per metric to identify which dimensions improve.
  3. **Capacity scaling test:** Add an intermediate-size model (e.g., Mistral 7B or Llama3 8B) and test whether speech act benefits emerge at a specific parameter threshold.

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (2 documents × 2 queries) limits statistical power and generalizability of observed effects
- Fixed sampling parameters (temperature=1, top_k=1) constrain response diversity and may mask model capabilities
- Missing or negative metric values for certain model/document combinations suggest implementation inconsistencies
- Resource constraints (12GB GPU) prevented testing larger models that might show different speech act sensitivity patterns

## Confidence
- **High:** The tool successfully implements document-based knowledge retrieval and multi-metric evaluation pipeline
- **Medium:** Larger models (Llama2, Llama3) show improved alignment metrics with speech acts; smaller models (TinyLlama) show increased perplexity
- **Low:** Causal attribution of performance differences specifically to speech act injection vs. other prompt engineering effects

## Next Checks
1. Scale experiment to 10+ documents and 20+ queries per document to establish statistical significance of observed effects
2. Implement speech act classifier ablation by manually removing illocutionary force labels from prompts to isolate classifier contribution
3. Test model capacity threshold by evaluating intermediate-sized models (7B parameters) to determine where speech act benefits emerge