---
ver: rpa2
title: 'Beyond Vision: How Large Language Models Interpret Facial Expressions from
  Valence-Arousal Values'
arxiv_id: '2502.06875'
source_url: https://arxiv.org/abs/2502.06875
tags:
- expressions
- facial
- llms
- emotion
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Large Language Models (LLMs) can
  interpret facial expressions from Valence-Arousal (VA) values instead of raw visual
  input. LLMs traditionally process only text, while human emotions are conveyed both
  verbally and non-verbally.
---

# Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values

## Quick Facts
- arXiv ID: 2502.06875
- Source URL: https://arxiv.org/abs/2502.06875
- Reference count: 40
- Primary result: LLMs perform better at generating semantic descriptions from Valence-Arousal values than categorizing discrete emotions

## Executive Summary
This paper investigates whether Large Language Models can interpret facial expressions from Valence-Arousal values rather than raw visual input. The study finds that while LLMs struggle with categorical emotion classification (accuracy often below 50%), they excel at generating semantically coherent descriptions that align closely with human interpretations (cosine similarity >0.75). These results suggest LLMs are better suited for free-text affective inference than rigid classification, offering a privacy-preserving alternative to vision-based emotion recognition.

## Method Summary
The study uses FaceChannel to extract Valence-Arousal values from facial images in the IIMI (basic emotions) and Emotic (complex emotions) datasets. These numerical values are then input into three LLMs (GPT-4o-mini, GPT-4o, LLAMA 3.2) for two tasks: classifying emotions into discrete categories and generating semantic descriptions. Evaluation uses classification accuracy for the former and cosine similarity between LLM-generated and human descriptions using Word2Vec, BERT, and Transformer embeddings for the latter.

## Key Results
- LLMs achieved near-perfect accuracy for basic emotions like happiness (87%) and sadness (98%), but struggled with complex emotions, with exact match accuracy below 20%
- Semantic description generation consistently achieved cosine similarity scores exceeding 0.75 using BERT and Word2Vec embeddings
- Transformer embeddings performed worse than BERT/Word2Vec for semantic similarity evaluation in this domain
- GPT-4o-mini often matched or outperformed GPT-4o while being more computationally efficient

## Why This Works (Mechanism)

### Mechanism 1
LLMs leverage linguistic priors—learned associations between affective concepts and language—to map continuous VA values onto descriptive language rather than rigid categories. The gradient nature of VA space aligns with LLMs' text generation strengths. Core assumption: LLM training data contains sufficient affective knowledge to interpret numerical valence-arousal pairs without visual grounding. Break condition: If LLMs' linguistic priors lack sufficient coverage of affective concept relationships.

### Mechanism 2
Polarized emotions with extreme VA values (high/low valence) are more accurately classified than mid-range or complex emotions because extreme values map to semantically unambiguous linguistic concepts with stronger associations in LLM training data. Core assumption: The distribution of affective language in LLM training data is biased toward polarized emotional expressions. Break condition: If the task requires distinguishing emotions with similar VA profiles (e.g., fear vs. surprise).

### Mechanism 3
Contextual embedding models (BERT, Word2Vec) capture semantic similarity between LLM-generated and human descriptions more effectively than structural transformer embeddings because BERT's bidirectional context modeling and Word2Vec's co-occurrence patterns capture affective semantic relationships. Core assumption: Semantic similarity metrics using contextual embeddings validly approximate human judgment of affective description quality. Break condition: If semantic similarity fails to capture pragmatic appropriateness or cultural context of emotional descriptions.

## Foundational Learning

- **Valence-Arousal (VA) Model of Affect**: The paper uses VA as the sole input representation; understanding this 2D continuous affect space is essential to interpret results and design prompts. Quick check: If valence = 0.8 and arousal = -0.2, what emotional state might this represent, and why might classification be ambiguous?

- **Linguistic Priors in Language Models**: The paper's mechanism depends on LLMs leveraging learned associations between affective language and numerical patterns; this concept explains both successes and failures. Quick check: Why might an LLM trained on general text associate "high arousal, negative valence" with fear but struggle to distinguish it from anger?

- **Semantic Similarity Metrics (Cosine Similarity on Embeddings)**: Experiment 2's evaluation depends entirely on embedding-based similarity; understanding what these metrics capture—and miss—is critical for interpreting claims. Quick check: If two descriptions have high cosine similarity but convey different emotional intensities, what limitation does this reveal?

## Architecture Onboarding

- **Component map**: Facial image -> FaceChannel -> VA values extraction -> VA values + task prompt -> LLM -> category predictions OR text descriptions -> accuracy OR embedding-based similarity scores
- **Critical path**: 1) Facial image → FaceChannel → VA values extraction, 2) VA values + task prompt → LLM → category predictions OR text descriptions, 3) Predictions vs. human annotations → accuracy OR embedding-based similarity scores
- **Design tradeoffs**: Classification provides interpretable outputs but fails on complex emotions; description is flexible but harder to evaluate quantitatively. Word2Vec/BERT capture semantic similarity but may miss nuanced affective distinctions; Transformers capture structure but underperform here.
- **Failure signatures**: Classification accuracy near 30% on basic emotions with extreme bias toward happiness/sadness; Multi-class exact match accuracy below 20% on complex emotions; Transformer embedding similarity scores at or below baseline (0.5) while BERT/Word2Vec exceed 0.75.
- **First 3 experiments**: 1) Baseline classification validation: Run FaceChannel on IIMI dataset to confirm VA extraction quality; compare LLM classification accuracy using FaceChannel VA vs. human-annotated VA values. 2) Prompt engineering for description task: Test whether adding affective context improves semantic similarity scores. 3) Embedding robustness check: Compare human judgment ratings against embedding-based similarity scores to validate evaluation metric.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the integration of additional modalities, such as speech or facial action units, enhance the ability of LLMs to categorize complex emotions from Valence-Arousal values? Basis: Discussion suggests exploring integration of additional context. Evidence needed: Comparative performance metrics on complex emotion datasets with VA values combined with audio or action unit data.

- **Open Question 2**: How does the performance of text-only LLMs using structured VA values compare to Vision-Language Models (VLMs) using raw visual inputs for semantic emotion description? Basis: Discussion notes need for comparing LLMs with VLMs. Evidence needed: Side-by-side evaluation measuring semantic similarity and computational resource usage between VA-LLM pipeline and state-of-the-art VLMs.

- **Open Question 3**: To what extent do LLM-generated affective interpretations generalize across diverse cultural and demographic datasets without introducing linguistic bias? Basis: Ethical Impact Statement recommends ensuring evaluation across diverse datasets. Evidence needed: Evaluation of semantic similarity and classification accuracy across datasets representing diverse cultural backgrounds.

- **Open Question 4**: Can hybrid approaches combining structured affective data with visual inputs improve robustness in privacy-conscious emotion recognition? Basis: Conclusion suggests hybrid approaches could improve robustness. Evidence needed: Study testing framework that fuses VA values with anonymized visual features to maintain privacy while improving accuracy.

## Limitations
- FaceChannel version uncertainty may affect VA extraction quality, impacting subsequent LLM performance
- Embedding-based evaluation may overestimate semantic alignment without capturing pragmatic or cultural appropriateness
- Study only examines three LLM models without exploring how model size or training data composition affects performance

## Confidence
- **High**: LLMs generate semantically coherent descriptions from VA values with cosine similarity >0.75
- **Medium**: LLMs struggle with categorical classification from VA values, particularly for complex emotions
- **Low**: The specific mechanism by which LLMs map VA values to affective language through linguistic priors

## Next Checks
1. **VA extraction validation**: Compare LLM classification accuracy using FaceChannel-extracted VA values versus human-annotated VA values to isolate whether classification failures stem from VA extraction errors or LLM inference limitations.

2. **Semantic similarity validation**: Conduct a small-scale human judgment study (n=20-30 annotators) rating a sample of LLM-generated descriptions on perceived quality and affective accuracy, then compare these ratings against embedding-based similarity scores to validate the evaluation metric.

3. **Prompt engineering experiment**: Test whether providing additional affective context in prompts improves classification accuracy for confused emotion pairs to determine if limitations are fundamental to VA representation or addressable through better prompting.