---
ver: rpa2
title: 'SynQuE: Estimating Synthetic Dataset Quality Without Annotations'
arxiv_id: '2511.03928'
source_url: https://arxiv.org/abs/2511.03928
tags:
- data
- dataset
- synthetic
- arxiv
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces the SYNQUE problem: ranking synthetic datasets
  by their expected real-world task performance using only limited unannotated real
  data. This addresses the challenge of data scarcity when real data is expensive
  or privacy-sensitive to collect.'
---

# SynQuE: Estimating Synthetic Dataset Quality Without Annotations

## Quick Facts
- **arXiv ID:** 2511.03928
- **Source URL:** https://arxiv.org/abs/2511.03928
- **Authors:** Arthur Chen; Victor Zhong
- **Reference count:** 40
- **One-line primary result:** SYNQUE proxies correlate moderately to strongly with downstream task performance, with LENS outperforming others on complex tasks.

## Executive Summary
This work introduces the SYNQUE problem: ranking synthetic datasets by their expected real-world task performance using only limited unannotated real data. This addresses the challenge of data scarcity when real data is expensive or privacy-sensitive to collect. The authors propose and evaluate a suite of proxy metrics—including distributional measures like mean distance to medoids, maximum mean discrepancy, proxy-A-distance, and MAUVE—to estimate synthetic data quality. They also introduce LENS, a novel LLM-based proxy that uses language rubrics and debiased scoring to capture nuanced characteristics of complex tasks. Experiments across sentiment analysis, Text2SQL, web navigation, and image classification show that SYNQUE proxies correlate moderately to strongly with downstream performance. LENS consistently outperforms others on complex tasks. For instance, on Text2SQL, selecting top-3 synthetic datasets via proxies improves accuracy from 30.4% to 38.4% (+8.1%) compared to random selection. The results establish SYNQUE as a practical framework for synthetic data selection under real-data scarcity.

## Method Summary
The method evaluates five proxy metrics for ranking synthetic datasets without annotations. Representation-based proxies (MDM, MMD, PAD, MAUVE) measure distributional alignment between synthetic and unannotated real data using embeddings from task-specific encoders. LENS uses a reasoning LLM to generate language rubrics comparing real and synthetic samples, then scores synthetic data with a separate scorer LLM using principled debiasing to mitigate order, label, and score biases. The framework requires only a small set of unannotated real samples and the synthetic datasets to rank, making it practical for scenarios where labeled real data is unavailable or costly to obtain.

## Key Results
- SYNQUE proxies achieve Spearman correlations up to 0.71 (LENS) on Text2SQL and 0.64 (LENS) on web navigation tasks.
- Representation-based metrics perform well on simple tasks (sentiment analysis) but lag on complex tasks.
- LENS consistently outperforms other proxies on complex tasks, demonstrating the value of LLM-based reasoning.
- Top-3 dataset selection via proxies improves downstream accuracy by 8.1% on Text2SQL compared to random selection.
- More real samples improve proxy reliability across all metrics, with LENS showing particular sensitivity to sample size.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distribution-based proxy metrics (e.g., PAD, MMD, MAUVE) estimate synthetic data quality by measuring alignment between synthetic and unannotated real data distributions in embedding space.
- **Mechanism:** These metrics embed both synthetic and real data into a shared latent space using encoder models (e.g., qte-Qwen2-7B-Instruct, E5-V). They then compute distributional distance (MMD), domain classifier error (PAD), or divergence frontiers (MAUVE). A smaller distributional gap (higher similarity) predicts higher downstream task performance because the synthetic data more closely approximates the true data manifold.
- **Core assumption:** The chosen embedding model captures task-relevant semantic and structural features such that distributional similarity in the embedding space correlates with generalization on the target task.
- **Evidence anchors:**
  - [abstract] "...introduce and evaluate proxy metrics that choose synthetic data... via embedding models... distribution and diversity-based distance measures."
  - [section 4.1] "We introduce the first proxy metrics for SynQuE by adapting distribution and diversity-based distance measures to our context via embedding models."
  - [corpus] The related work "RoSE" addresses LLM generator selection without human test sets but does not use embedding-based distributional metrics as proxies for dataset-level ranking, highlighting this as a distinct approach.
- **Break condition:** If the embedding space poorly represents task-critical features (e.g., complex, long-horizon planning tasks), the distributional alignment will fail to predict downstream performance.

### Mechanism 2
- **Claim:** Diversity metrics, specifically Mean Distance to Medoid (MDM), approximate synthetic dataset quality by quantifying data coverage in embedding space.
- **Mechanism:** MDM clusters the synthetic dataset into N medoids and computes the average intra-cluster distance from points to their medoid. Higher MDM scores indicate greater spread/diversity. The intuition is that datasets with broader coverage of the embedding space are more likely to include examples relevant to unseen real test data.
- **Core assumption:** Higher diversity in the embedding space leads to better generalization on real data; this assumes real test data is distributed across the manifold covered by the synthetic data.
- **Evidence anchors:**
  - [abstract] "...adapting distribution and diversity-based distance measures..."
  - [section 4.1] "MDM characterizes this diversity by measuring the sparsity of data points around medoids... high-diversity synthetic datasets to have broader coverage, and therefore higher performance on real data."
  - [corpus] The paper "Measuring Diversity in Synthetic Datasets" (Zhu et al., 2025) is cited as a related diversity metric (DCScore), providing external grounding for diversity as a quality heuristic.
- **Break condition:** Diversity alone does not guarantee correctness; highly diverse synthetic data with factual errors or hallucinations may still perform poorly (noted in Related Work, Section 2).

### Mechanism 3
- **Claim:** LENS (LLM-Evaluated Normalized Score) improves quality estimation for complex tasks by using an LLM to generate language rubrics that characterize dataset differences and a separate scorer LLM to classify samples based on those rubrics.
- **Mechanism:** A reasoning LLM (e.g., DeepSeek-R1) compares samples from unannotated real data and synthetic data to generate 10-point "rubrics" of similarities and differences (C, C_r,s, C_s,r). A scoring LLM then judges how likely each synthetic sample belongs to the real dataset using these rubrics. A principled debiasing procedure (Equations 4–7) normalizes for order, label, and score biases. The final LENS score is the average debiased probability across synthetic samples.
- **Core assumption:** LLMs can articulate meaningful dataset characteristics in natural language and use those rubrics to perform robust zero-shot discrimination; debiasing effectively removes systematic LLM judging biases.
- **Evidence anchors:**
  - [abstract] "...propose LENS, a novel proxy that leverages large language model reasoning... uses language rubrics and debiased scoring..."
  - [section 4.2] "LENS first derives a language rubric... principled debiasing and scoring... mitigates order bias, label bias, score bias."
  - [corpus] The paper "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding" (Krumdick et al., 2025) warns of LLM judge biases; LENS's debiasing procedure directly addresses this concern.
- **Break condition:** If the rubrics are too generic, the scorer LLM is too weak, or the debiasing terms introduce noise (as observed for some strong models on simple tasks), LENS may underperform.

## Foundational Learning

- **Concept: Domain Adaptation and Distributional Distance Metrics (MMD, Proxy-A-Distance)**
  - **Why needed here:** Understanding MMD and PAD is critical because SynQuE adapts these metrics to estimate how close a synthetic dataset is to real data, which is the core heuristic for quality estimation.
  - **Quick check question:** *Given two sets of embeddings, one from synthetic data and one from real data, how would you compute MMD to test if they are from the same distribution?*

- **Concept: Bias in LLM-as-a-Judge**
  - **Why needed here:** LENS relies on LLM judgment, but LLMs exhibit order, label, and score biases. Recognizing and mitigating these is essential to understanding why LENS includes a debiasing procedure.
  - **Quick check question:** *What are three common biases in LLM judgment, and how might averaging across permutations mitigate them?*

- **Concept: Embedding Space Representations for Text and Images**
  - **Why needed here:** Most proxy metrics operate in embedding space. Knowing that embeddings capture semantic similarity but may fail on long-horizon or complex reasoning tasks is key to knowing when to use LENS vs. representation-based metrics.
  - **Quick check question:** *Why might a standard sentence embedding model fail to capture the quality differences in complex web navigation tasks, and what alternative does LENS propose?*

## Architecture Onboarding

- **Component map:** K synthetic datasets (D_s^k) and unannotated real dataset (U_r) -> Encoder (qte-Qwen2-7B-Instruct/E5-V) -> Embeddings -> Proxy Metrics (PAD/MMD/MAUVE/MDM) -> Scores OR Reasoning LLM (DeepSeek-R1) -> Rubric Generation -> Scorer LLM (Qwen2.5-32B-Instruct) -> Permutation Scoring -> Debiasing -> LENS Score -> Ranked synthetic datasets

- **Critical path:**
  1. Collect U_r (unannotated real samples) and D_s^k (synthetic datasets)
  2. For LENS: Generate rubrics by sampling from U_r and D_s^k. Score all D_s^k using the scorer LLM and debiasing
  3. For other proxies: Embed all samples and compute distributional/diversity metrics
  4. Rank datasets by proxy score; select top-K for downstream model training

- **Design tradeoffs:**
  - **Embedding vs. LLM-based Proxies:** Embedding-based metrics are faster and cheaper but may fail on complex tasks. LENS is more expensive but captures nuanced differences. Choose based on task complexity and budget
  - **Debiased vs. Raw Scores:** Debiased LENS generally improves correlation on complex tasks (Text2SQL) but may degrade performance for well-calibrated large models on simple tasks (Sentiment)
  - **Number of Real Samples (|U_r|):** More samples improve correlation across all metrics (Table 6) but increase cost; LENS is more sensitive to sample size for simple tasks

- **Failure signatures:**
  - **Image classification with ambiguous classes:** High variance and negative correlations for some proxies (Table 3, Image Split 1). VLM-based rubrics may fail to capture meaningful characteristics
  - **Simple tasks with strong LLM scorers:** Debiased LENS may underperform raw LENS if the model is already well-calibrated (e.g., GPT-4.1 on Sentiment)
  - **Tasks with hallucinated synthetic data:** Representation-based metrics may indicate high alignment/diversity but downstream performance suffers due to factual errors (Related Work, Section 2)

- **First 3 experiments:**
  1. **Baseline Validation:** Replicate the Sentiment Analysis and Text2SQL experiments using the provided datasets and qte-Qwen2-7B-Instruct embeddings. Compute PAD, MMD, MAUVE, MDM, and LENS (debiased). Verify Spearman/Pearson correlations match Table 3 within a reasonable margin
  2. **Sample Size Sensitivity:** For a single task (e.g., Text2SQL), vary |U_r| (25, 50, 75, 100) and measure how correlation changes for LENS vs. representation-based metrics. This tests the robustness of each proxy to data scarcity
  3. **LENS Ablation:** Run LENS on Text2SQL with (a) no rubric (zero-shot), (b) rubric but no debiasing, and (c) full LENS with debiasing. Compare correlations to quantify the contribution of each component (as in Tables 9 and 3)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LENS rubric feedback be operationalized to guide generative models in iteratively synthesizing higher-quality data?
- **Basis in paper:** [explicit] The authors state future work should explore "using rubric feedback to guide LLMs in synthesizing more realistic data."
- **Why unresolved:** The current work treats quality estimation (SynQuE) as a static selection step post-generation, rather than a feedback mechanism for the generation process itself.
- **What evidence would resolve it:** An experiment where LENS rubrics are fed back into the generator prompting strategy, showing improved downstream performance over static generation.

### Open Question 2
- **Question:** Can SYNQUE proxies be adapted for fine-grained, example-level data selection instead of dataset-level ranking?
- **Basis in paper:** [explicit] The conclusion motivates future research on "fine-grained, example-level proxy use to directly improve task model training."
- **Why unresolved:** The current methodology relies on distributional measures or aggregate LENS scores over whole datasets, whereas instance-level weighting requires differentiable or localized signals.
- **What evidence would resolve it:** A modified proxy metric that assigns quality scores to individual samples and demonstrates improved model convergence when used for data pruning or curriculum learning.

### Open Question 3
- **Question:** Does scaling LENS to larger or stronger Vision-Language Models (VLMs) resolve the performance variance observed in ambiguous image classification tasks?
- **Basis in paper:** [explicit] The authors hypothesize that the weaker correlation of LENS in image classification "stems from the inability of VLMs to capture meaning characteristics" and suggest future work explore "strong VLMs."
- **Why unresolved:** The experiments used specific VLMs (e.g., Qwen2.5-VL-32B) which struggled with batch-based rubric generation for ambiguous visual concepts.
- **What evidence would resolve it:** Re-evaluating the image classification splits using state-of-the-art VLMs to see if rubric quality and correlation scores improve to match text-based performance.

### Open Question 4
- **Question:** Is there a theoretical or empirical relationship between the size of the unannotated real data subset (|Ur|) and the saturation point of proxy accuracy for complex tasks?
- **Basis in paper:** [inferred] While Table 6 shows improved correlation with more samples (|Ur|=50 vs 30), the paper does not establish a saturation point or a theoretical lower bound for complex tasks like WebNav where LENS currently excels with limited data.
- **Why unresolved:** The paper demonstrates correlation improvements empirically but does not define the minimal effective sample size required for reliable ranking across varying task complexities.
- **What evidence would resolve it:** A systematic ablation study scaling |Ur| from very low (e.g., 5 samples) to high (e.g., 500 samples) to identify the inflection point where correlation gains diminish.

## Limitations
- **Task complexity dependency:** LENS significantly outperforms representation-based proxies on complex tasks (Text2SQL, web navigation) but underperforms on simpler ones (sentiment analysis), suggesting it is not universally optimal.
- **Image domain reliability:** VLM-based rubric generation for LENS shows higher variance and negative correlations in some image classification splits, raising concerns about cross-domain generalization.
- **Synthetic data quality assumptions:** The framework assumes synthetic data is diverse and aligned with real data distributions, but related work warns that high diversity or distributional alignment does not guarantee correctness—hallucinated or factually incorrect synthetic data may still score highly on proxies.

## Confidence
- **High confidence:** Representation-based proxies (MMD, PAD, MAUVE, MDM) reliably correlate with downstream performance for simple to moderately complex tasks when task-relevant embeddings are used.
- **Medium confidence:** LENS improves quality estimation for complex tasks through language rubrics and debiasing, but its effectiveness depends heavily on LLM reasoning quality and debiasing implementation.
- **Low confidence:** Image domain results, particularly VLM-based LENS performance, show high variance and inconsistent correlations, making the framework's applicability to visual tasks uncertain without further validation.

## Next Checks
1. **Task complexity stratification test:** Systematically categorize tasks by complexity (simple: sentiment, complex: Text2SQL/web nav) and validate whether proxy selection (LENS vs. representation-based) based on this stratification consistently improves correlation across multiple new datasets.
2. **Cross-domain proxy robustness:** Apply the full SynQuE framework to a new domain (e.g., biomedical text or multimodal data) to test whether LENS's advantage on complex tasks generalizes beyond the evaluated domains, and whether VLM-based proxies can be improved for visual tasks.
3. **Hallucination sensitivity analysis:** Intentionally generate synthetic datasets with controlled hallucinations or factual errors and measure how representation-based proxies vs. LENS detect quality degradation, testing the framework's robustness to synthetic data correctness.