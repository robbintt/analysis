---
ver: rpa2
title: Sampling Complexity of TD and PPO in RKHS
arxiv_id: '2509.24991'
source_url: https://arxiv.org/abs/2509.24991
tags:
- equation
- policy
- learning
- rkhs
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a kernel-based temporal difference (TD) learning
  method combined with natural policy gradient (NPG) optimization in reproducing kernel
  Hilbert spaces (RKHS). The key innovation is decoupling policy evaluation (using
  kernel TD learning with one-step samples) from policy improvement (using a KL-regularized
  proximal update in continuous action spaces).
---

# Sampling Complexity of TD and PPO in RKHS

## Quick Facts
- **arXiv ID:** 2509.24991
- **Source URL:** https://arxiv.org/abs/2509.24991
- **Reference count:** 40
- **Primary result:** Kernel-based TD critic achieves 78.2% higher throughput (7,963 vs 4,468 state-action pairs/sec) than GAE baseline on CartPole/Acrobot

## Executive Summary
This paper introduces a kernel-based temporal difference (TD) learning method combined with natural policy gradient (NPG) optimization in reproducing kernel Hilbert spaces (RKHS). The method decouples policy evaluation (using kernel TD learning with one-step samples) from policy improvement (using a KL-regularized proximal update in continuous action spaces). The theoretical framework achieves global convergence with instance-adaptive rates dependent on RKHS entropy, covering tabular, Sobolev, Gaussian, and neural tangent kernel regimes. Empirically, the theory-aligned step-size schedule improves stability and sample efficiency on control tasks like CartPole and Acrobot.

## Method Summary
The method implements a kernelized temporal difference learning algorithm for policy evaluation combined with a natural policy gradient optimization for policy improvement. The critic performs functional gradient descent in an RKHS using one-step transitions, while the actor implements a KL-regularized update that exponentiates the evaluated action-value function. The algorithm achieves global convergence to optimal policies with a step-size schedule of Δₖ = 1/√k, where the convergence rate depends on the RKHS entropy. The method is tested on continuous control tasks (CartPole-v1 and Acrobot-v1) using a 2-layer MLP architecture with shared actor-critic backbone.

## Key Results
- Kernel TD critic processes 7,963 state-action pairs per second versus 4,468 for PPO+GAE baseline (78.2% throughput improvement)
- Theory-aligned step-size schedule (Δₖ = k⁻⁰·⁵) achieves stable learning while k⁻⁰·² diverges and k⁻¹·⁵ stagnates
- Global convergence guarantee with O(k⁻¹/²) rate under RKHS entropy assumptions
- Method unifies tabular, Sobolev, Gaussian, and neural tangent kernel regimes under single theoretical framework

## Why This Works (Mechanism)

### Mechanism 1
Kernelized Temporal Difference (TD) critics process data faster than Generalized Advantage Estimation (GAE) baselines by using one-step transitions rather than full trajectory rollouts. The critic performs functional gradient descent in an RKHS using an inner product based on the kernel K(ω,·) which acts as an implicit preconditioner, avoiding costly cubic-time matrix inversions required by Least-Squares TD while maintaining geometric convergence properties. The method assumes the action-value function Q^π lies within the RKHS and that the initial sampling distribution μ₀ satisfies coverage constraints relative to transition dynamics. Break conditions include slow Markov chain mixing or failure of one-step transitions to capture temporal credit assignment.

### Mechanism 2
A KL-regularized Natural Policy Gradient (NPG) update recovers Proximal Policy Optimization (PPO/TRPO) stability in continuous action spaces by exponentiating the evaluated action-value function. The policy update πₖ₊₁ ∝ πₖ exp(Δₖf_T^(k)) solves a functional proximal optimization problem via Lagrangian multipliers in function space, mirroring mirror descent in infinite dimensions to ensure monotonic policy improvement. The method assumes strict positivity of the policy (1 > π^(k) > 0) to avoid numerical instability. Break conditions include high L∞ error in critic estimates causing policy collapse through overshooting.

### Mechanism 3
Global convergence to optimal policy is achieved with step-size schedule Δₖ = 1/√k, provided per-iteration sample size grows to match policy complexity in RKHS. The convergence rate depends on RKHS entropy (covering number), requiring specific sampling rule n(k) to ensure evaluation error shrinks fast enough for O(k⁻¹/²) optimization as policy iterates become more complex. The method assumes bounded RKHS entropy and distributions bounded away from zero. Break conditions include policy complexity growth exceeding sampling budget support, causing error terms to dominate and void convergence guarantees.

## Foundational Learning

- **Reproducing Kernel Hilbert Spaces (RKHS):** Function space where Q-function lives, allowing direct function optimization using kernel inner products for implicit preconditioning without large inverse matrices. Quick check: Can you explain why the "representer theorem" guarantees that the optimal function f can be written as a sum of kernel evaluations on data points?

- **Temporal Difference (TD) Learning vs. GAE:** Paper claims efficiency gains by replacing trajectory-based GAE with one-step Kernel TD. Understanding bias-variance trade-off between one-step TD (r + γQ(s')) and multi-step GAE is crucial for diagnosing stability issues. Quick check: What is the specific data dependency difference between calculating GAE advantage versus one-step TD error?

- **Natural Policy Gradient (NPG) & KL Divergence:** "Proximal" part of PPO comes from constraining updates using KL divergence. This paper formulates this in function space (RKHS) rather than probability space, linking update to exponentiated gradients. Quick check: How does functional update πₖ₊₁ ∝ πₖ exp(·) mathematically enforce trust region or proximity constraint?

## Architecture Onboarding

- **Component map:** Sample states → Kernel TD critic (functional gradient descent) → Action-value function in RKHS → Exponentiated policy update → KL-regularized NPG

- **Critical path:** 1) Sample initial states s₀ ~ μ₀ and generate one-step transitions 2) Evaluate: Run Kernel TD iterations (Eq. 9) to estimate Q-function 3) Improve: Update policy via exponentiated gradient π ∝ exp(F) where F accumulates gradients 4) Schedule: Check iteration count k; adjust step size Δₖ and sample budget n(k) per Table 1

- **Design tradeoffs:** Implicit Preconditioning vs. Explicit Inversion (avoids O(n³) cost of Kernel Ridge Regression but requires careful weight decay α and step size η selection); One-step vs. Trajectory (gains ~78% throughput by avoiding trajectory rollouts but potentially introduces higher variance)

- **Failure signatures:** Divergence of Returns (if step size exponent too low, returns collapse); Stagnation (if step size decays too fast, learning stops); Complexity Explosion (if policy ∥πₖ∥H grows unbounded, required samples n(k) may exceed physical memory/budget)

- **First 3 experiments:** 1) Sanity Check (Tabular): Implement update in simple grid-world to verify exponential convergence of Kernel TD matches theoretical rate 2) Step-Size Ablation: Reproduce Figure 1 on CartPole/Acrobot testing Δₖ = k⁻⁰·⁵ against k⁻⁰·² and k⁻¹·⁵ to confirm stability cliff 3) Throughput Benchmark: Compare "pairs/sec" processing rate of Kernel TD critic against standard PPO+GAE implementation

## Open Questions the Paper Calls Out

### Open Question 1
Does the kernel-based TD-NPG method maintain its theoretical convergence rates and throughput advantages in high-dimensional continuous action environments? The paper claims the method recovers a proximal update in "continuous state-action spaces" yet all empirical validation uses discrete action spaces. Kernel methods typically suffer from curse of dimensionality, and the 78.2% throughput improvement may not transfer to complex continuous control benchmarks where function approximation is more critical. Empirical results on standard continuous control benchmarks (e.g., MuJoCo environments) demonstrating convergence speed and sample efficiency comparable to or better than PPO would resolve this.

### Open Question 2
Can the strict requirement that one-step transitions be bounded by initial distribution (Assumption 6) be relaxed for sparse or highly stochastic MDPs? The paper states Assumption 6 ensures "one-step transition is not too far from initial distribution; otherwise convergence slows." The condition P(s'|s,a) ≤ c²μ₀(s') implies sampling distribution must heavily cover transition dynamics, which is difficult to enforce in environments requiring broad exploration or with sparse rewards where significant distribution shift is inevitable. Theoretical derivation of convergence rates relaxing or removing cγ < 1 condition, or empirical studies showing robustness when μ₀ provides poor coverage, would resolve this.

### Open Question 3
Can a unified generalization error bound be established for Kernel TD without relying on specific structural properties like entropy or kernel type? The paper notes Theorem 9 yields convergence only on training data, and establishing generalization "requires additional structural information on the RKHS" beyond general assumptions. Current analysis treats generalization only through specific cases (Corollary 10), leaving behavior of estimator in general RKHS classes reliant on case-specific sampling inequalities. A generalization bound for Theorem 9 depending only on generic RKHS entropy bounds defined in Assumption 5 rather than specific kernel attributes would resolve this.

### Open Question 4
Does the convergence analysis extend to deep neural networks outside the Neural Tangent Kernel (NTK) "lazy training" regime? The paper unifies analysis with NTK regime, noting "kernel gradient descent mirrors NTK dynamics of wide networks" but deep networks frequently operate in feature learning regimes where kernel evolves. Fixed-kernel assumption central to RKHS analysis may fail if policy representation changes non-linearly during training, which is common in practical deep RL. Convergence guarantees for "neural" version where network width is finite and features are allowed to evolve, breaking NTK equivalence, would resolve this.

## Limitations
- Theoretical convergence relies heavily on RKHS entropy assumptions that may not translate cleanly to neural network function approximation
- Empirical validation limited to two control benchmarks without comparison to state-of-the-art continuous control methods like SAC or TD3
- Step-size schedule requires instance-adaptive tuning based on RKHS complexity, which may be impractical without prior knowledge of function class
- Throughput comparison assumes one-step transitions are sufficient, which may not hold for tasks with delayed rewards where multi-step GAE's variance reduction is critical

## Confidence

- **High:** Kernel TD can process data faster than trajectory-based methods (throughput claim supported by explicit timing)
- **Medium:** Global convergence with O(k⁻¹/²) rate under RKHS assumptions (theory is rigorous but RKHS entropy bounds may not hold for all function classes)
- **Low:** Practical superiority over modern continuous control algorithms (limited empirical scope, no ablations on modern benchmarks)

## Next Checks

1. **RKHS Entropy Sensitivity:** Systematically vary kernel type (Gaussian, Sobolev, NTK) on CartPole/Acrobot and measure how required sample budget n(k) changes relative to theoretical predictions

2. **Variance vs. Bias Trade-off:** Compare return variance and mean performance of one-step Kernel TD critic against GAE baseline across multiple random seeds on same tasks

3. **Scalability Test:** Implement method on more complex continuous control task (e.g., Hopper or Walker2d from OpenAI Gym) to evaluate whether RKHS complexity growth remains manageable