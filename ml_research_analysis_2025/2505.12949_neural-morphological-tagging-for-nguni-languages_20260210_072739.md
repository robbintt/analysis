---
ver: rpa2
title: Neural Morphological Tagging for Nguni Languages
arxiv_id: '2505.12949'
source_url: https://arxiv.org/abs/2505.12949
tags:
- morphological
- neural
- languages
- tagging
- nguni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores neural approaches for morphological tagging
  of Nguni languages (isiNdebele, isiXhosa, isiZulu, Siswati). Morphological parsing
  is framed as a pipeline of segmentation followed by tagging, and the study focuses
  on the tagging step using pre-segmented morphemes from existing segmenters.
---

# Neural Morphological Tagging for Nguni Languages

## Quick Facts
- arXiv ID: 2505.12949
- Source URL: https://arxiv.org/abs/2505.12949
- Reference count: 12
- Key outcome: Neural morphological taggers achieve macro F1 >60% and micro F1 >90% on gold segmentations for Nguni languages

## Executive Summary
This paper explores neural approaches for morphological tagging of Nguni languages (isiNdebele, isiXhosa, isiZulu, Siswati) using pre-segmented morphemes from existing segmenters. The study evaluates two classes of neural taggers: models trained from scratch (bi-LSTMs and CRF with bi-LSTM features) and finetuned pretrained language models (XLM-R, Afro-XLMR, Nguni-XLMR). Results show that neural taggers significantly outperform rule-based baselines, with models trained from scratch generally outperforming finetuned PLMs. Canonical segmentations consistently outperform surface segmentations, achieving macro F1 scores above 60% and micro F1 above 90% on gold segmentations.

## Method Summary
The study evaluates morphological tagging on pre-segmented morpheme sequences using Gaustad and Puttkammer (2022) dataset with ~50k words per language. Two model families are tested: (1) bi-LSTMs and CRF-biLSTMs trained from scratch with morpheme or character-level embeddings, word or sentence context; (2) finetuned XLM-R-large, Afro-XLMR-large, and Nguni-XLMR-large. Models are trained on isiZulu with hyperparameter tuning via 10% validation split, then applied to other languages. Performance is measured using macro F1 (primary) and micro F1 on both gold and predicted segmentations, using both canonical and surface segmentation forms.

## Key Results
- Neural taggers significantly outperform rule-based baselines (20-30 percentage point macro F1 improvement)
- Models trained from scratch outperform finetuned PLMs on morphological tagging task
- Canonical segmentations consistently outperform surface segmentations across all model types
- Best models achieve macro F1 >60% and micro F1 >90% on gold segmentations
- Performance drops to macro F1 >55% and micro F1 >80% on predicted segmentations

## Why This Works (Mechanism)

### Mechanism 1: Contextual Disambiguation via Sentence-Level Training
Sentence-level models leverage grammatical dependencies between words to resolve morphological ambiguities that cannot be resolved in isolation. By training on entire sentences, models can use subject concord disambiguation (e.g., "liyahamba" disambiguating "ipolisa" between class 5 and 9).

### Mechanism 2: Morpheme-Level Representations Preserve Grammatical Distinctions
Morpheme-level embeddings allow models to distinguish morphemes that share characters but have different grammatical functions (e.g., "ng" vs "nga" differ by one character but have totally different meanings). This sensitivity to small changes proves critical for accurate tagging.

### Mechanism 3: Canonical Segmentation Preserves More Grammatical Information
Canonical segmentation preserves underlying grammatical morphemes that surface forms may obscure through phonological changes. For instance, "kwicandelo" surface-segments to "kw-i-candelo" but canonically to "ku-i-(li)-candelo", losing the critical "(li)" morpheme that indicates noun class 5.

## Foundational Learning

- **Agglutinative Morphology**: Nguni languages construct words by concatenating multiple morphemes, creating long forms where space-delimited words can contain clause-level information. Quick check: Can you explain why a single Nguni word like "andikambuzi" (meaning "I have not yet asked them") requires morphological parsing rather than simple whitespace tokenization?

- **Canonical vs Surface Segmentation**: The paper shows dramatically different results depending on segmentation type; this distinction is fundamental to the architecture. Quick check: For "zobomi" which surface-segments to "zo-bomi" and canonically to "za-u-(bu)-bomi", which segmentation would you expect to better support morphological tagging and why?

- **Sequence Labeling with CRFs**: The paper compares bi-LSTMs and CRF-biLSTMs, finding minimal difference; understanding CRF's explicit label dependency modeling helps interpret this. Quick check: If bi-LSTMs and CRF-biLSTMs perform similarly, what does this suggest about where grammatical dependencies are being captured?

## Architecture Onboarding

- Component map: Raw Text → [External Segmenter] → Pre-segmented morphemes → [Tagger]

- Critical path: Segmentation quality directly constrains tagging performance. Error propagation from segmentation to tagging is the primary failure mode.

- Design tradeoffs:
  - Canonical vs Surface: Canonical gives +5-15 macro F1 but requires more complex annotation and segmentation models
  - Word vs Sentence context: Sentence-level aids disambiguation but increases computational cost
  - From-scratch vs PLM: From-scratch currently outperforms PLMs (contrary to typical NLP patterns), but sentence-level PLMs remain untested

- Failure signatures:
  - Large macro/micro F1 gap (60-75% vs 90-95%): poor performance on rare tags
  - Drop from gold to predicted segmentations: error propagation from segmenter
  - PLM collapse on surface segmentations (22-48% macro F1): tokenization mismatch

- First 3 experiments:
  1. Train a sentence-level bi-LSTM with morpheme embeddings on gold canonical segmentations (isiZulu) to establish upper-bound performance
  2. Apply the trained tagger to model-predicted segmentations to quantify error propagation
  3. Train identical models on canonical vs surface segmentations to quantify information loss for your use case

## Open Questions the Paper Calls Out

- Would sentence-level finetuning of pretrained language models achieve performance competitive with or superior to models trained from scratch for Nguni morphological tagging? The paper leaves this exploration to future work due to computational constraints.

- Can the subword tokenization mismatch between pretraining and finetuning be mitigated to improve PLM performance on morphological tagging? Authors identify this as an inherent limitation without proposing remediation strategies.

- Does incorporating neural morphological parsing outputs improve downstream NLP task performance for Nguni languages? This remains unexplored despite the parsers being available.

- Do the findings on canonical vs. surface segmentation and model architecture generalize to languages with disjunctive orthography such as Sotho-Tswana? The study is limited to Nguni languages, and findings may not generalize.

## Limitations

- The study uses pre-segmented morphemes from external segmenters, creating a pipeline where tagging performance is bounded by segmentation quality and error propagation substantially degrades performance.

- Sentence-level finetuning for PLMs was not explored despite the paper comparing sentence-level versus word-level training for from-scratch models.

- The canonical vs surface segmentation comparison, while showing consistent advantages, lacks extensive validation across diverse agglutinative languages beyond the Nguni family.

## Confidence

- **High Confidence**: Neural taggers significantly outperform rule-based baselines (robust 20-30 percentage point improvements)
- **Medium Confidence**: From-scratch models outperforming finetuned PLMs (contradicts typical NLP patterns, needs validation)
- **Medium Confidence**: Superiority of canonical over surface segmentations (needs linguistic validation beyond computational results)
- **Low Confidence**: PLMs are "not competitive for this task" (based on specific experimental conditions that may not represent optimal PLM usage)

## Next Checks

1. Implement sentence-level finetuning for XLM-R, Afro-XLMR, and Nguni-XLMR models to determine if PLM performance improves when trained on full sentence contexts rather than word-level contexts.

2. Apply the canonical vs surface segmentation comparison to at least two additional agglutinative languages (e.g., Turkish, Finnish) to determine whether the observed advantages are specific to Nguni languages or represent a general principle.

3. Train or fine-tune morphological segmenters to generate canonical segmentations, then apply the best morphological taggers to these outputs to quantify practical ceiling for real-world deployment.