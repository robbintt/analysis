---
ver: rpa2
title: 'KOM: A Multi-Agent Artificial Intelligence System for Precision Management
  of Knee Osteoarthritis (KOA)'
arxiv_id: '2511.19798'
source_url: https://arxiv.org/abs/2511.19798
tags:
- clinical
- agent
- evaluation
- knee
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KOM is a multi-agent AI system that automates patient information
  collection, disease assessment, and treatment planning for knee osteoarthritis.
  It combines deep learning for imaging analysis with large language models for patient
  interaction and clinical reasoning.
---

# KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)

## Quick Facts
- arXiv ID: 2511.19798
- Source URL: https://arxiv.org/abs/2511.19798
- Reference count: 0
- KOM achieved 77.16% accuracy in knee osteoarthritis grading vs 34.50% for best general-purpose AI baseline

## Executive Summary
KOM is a multi-agent AI system that automates patient information collection, disease assessment, and treatment planning for knee osteoarthritis. It combines deep learning for imaging analysis with large language models for patient interaction and clinical reasoning. In benchmark experiments, KOM outperformed general-purpose AI models in both radiographic interpretation and treatment plan generation. In a three-arm clinical simulation, physician-KOM collaboration reduced diagnostic and planning time by 38.5% compared to physicians working alone, while improving treatment plan approval rates from 53.8% to 93.0%.

## Method Summary
The system uses a three-component architecture: Assessment Agent (Qwen-Max LLM with ResNet-50/U-Net for X-ray analysis), Risk Agent (ensemble of classical ML models for KOOS/KL progression prediction), and Therapy Agents Group (4 specialized QLM agents for multidisciplinary treatment planning). The image analysis pipeline trains on OAI dataset (12,719 radiographs) with U-Net localization (200 annotated images) followed by ResNet-50 classification. Risk prediction uses 31 clinical features with ElasticNet, Random Forest, and XGBoost models. The Therapy Agents Group implements a simulated multidisciplinary team discussion using RAG access to six specialized knowledge bases totaling 4,017 articles.

## Key Results
- Radiographic grading accuracy: 77.16% vs 34.50% for best general-purpose AI baseline
- Treatment plan expert scores: 29.63 vs 26.03 for best baseline model
- Clinical collaboration: 38.5% time reduction and 93.0% approval rate vs 53.8% without AI

## Why This Works (Mechanism)

### Mechanism 1
Hybrid architectures (Specialized CNNs + LLMs) outperform general Vision-Language Models (VLMs) in specific radiographic grading tasks. The system decouples visual perception from natural language reasoning. A ResNet-50, trained specifically on the OAI dataset for anatomical localization and severity classification, extracts structured features that are fed into the LLM. This bypasses the "zero-shot" limitations of general VLMs which lack fine-grained domain-specific visual calibration.

### Mechanism 2
Role-based multi-agent decomposition improves the completeness and safety of treatment planning over monolithic generation. Instead of a single prompt, the system uses a "Therapy Agents Group" where distinct agents (Nutrition, Surgery, Exercise) grounded in domain-specific RAG databases generate recommendations. A "Clinical Decision Agent" synthesizes these into a coherent plan, simulating a Multi-Disciplinary Team (MDT) discussion.

### Mechanism 3
Asymmetric Human-AI collaboration optimizes workflow efficiency better than full automation. The system offloads "heavy" cognitive tasks (image pixel analysis, drafting long management plans) to AI, while keeping the physician in a supervisory "approval" loop. This reduces the search space for the physician, cutting time without removing accountability.

## Foundational Learning

- **Kellgren-Lawrence (KL) Grading & Class Consolidation**
  - Why needed: The Assessment Agent maps KL grades (0-4) into consolidated clinical categories (None/Doubt, Mild, Moderate, Severe) because distinguishing between KL 0 and 1 is visually noisy and clinically low-value.
  - Quick check: Can you explain why the system maps KL 0 and KL 1 to a single "None/Doubt" category, and how this affects the confusion matrix for the ResNet classifier?

- **RAG (Retrieval-Augmented Generation) for Medical Guidelines**
  - Why needed: The Therapy Agents rely on vector databases of clinical guidelines to generate prescriptions. Understanding RAG is critical to diagnosing why an agent might suggest an outdated or irrelevant treatment.
  - Quick check: If the "Surgical Agent" suggests a procedure contradicted by the latest 2024 guidelines, is the failure in the retrieval step (wrong docs fetched) or the generation step (LLM ignored context)?

- **Model Cascading (LLM + CNN)**
  - Why needed: KOM is not one model; it is a pipeline. Errors propagate. The LLM cannot "see" the X-ray; it relies entirely on the structured output of the ResNet.
  - Quick check: If the ResNet misclassifies a "Moderate" KOA as "Mild," how does this error propagate through the Risk Agent's probability calculation and the Therapy Agent's exercise prescription?

## Architecture Onboarding

- **Component map**: X-ray Input -> U-Net Localization -> ResNet Grading -> Assessment Summary -> Risk Feature Extraction (31 params) -> Risk Report -> MDT Agent Discussion -> Final Plan
- **Critical path**: Assessment Agent (Qwen-Max + CNN) feeds structured severity classification to Risk Agent, which generates progression forecasts, then Therapy Agents Group synthesizes treatment recommendations through simulated MDT discussion
- **Design tradeoffs**: 
  - Latency vs. Quality: MDT discussion requires multiple sequential LLM calls, increasing inference time significantly compared to single-prompt generation
  - Specificity vs. Generalization: Training ResNet on OAI dataset ensures high accuracy for that protocol but risks poor generalization to non-standard clinical X-rays
- **Failure signatures**:
  - "Semantic Drift" in RAG: Agents retrieving partially relevant but clinically inapplicable snippets
  - Visual Hallucination: General VLMs attempting to grade X-rays without specialized ResNet front-end
  - Risk Forecast Instability: KOOS predictions dropping significantly at 4-year mark (R² ~0.30-0.46) vs 1-year
- **First 3 experiments**:
  1. Unit Test the Visual Front-end: Run ResNet/U-Net pipeline on OAI hold-out sets specifically looking for "Moderate vs. Mild" confusion
  2. Ablation of MDT: Run Therapy Agent in "Single Agent" vs "Group" mode for 10 complex cases to quantify marginal gain in "Completeness" scores
  3. Time-Motion Study: Replicate clinical simulation locally to verify 38.5% time reduction claim, measuring "Time to First Draft" vs "Time to Final Approval"

## Open Questions the Paper Calls Out

- **Clinical Safety and Efficacy**: Does the KOM system maintain safety and efficacy standards when deployed prospectively in real-world clinical settings, as opposed to retrospective and simulated environments tested? (Requires prospective clinical validation with live physician oversight)

- **Clinical Outcome Improvement**: Do the AI-generated management plans produced by KOM lead to improved clinical patient outcomes compared to conventional standard-of-care approaches? (Requires RCT tracking actual longitudinal health outcomes)

- **Conflict Resolution Mechanisms**: Can advanced conflict resolution mechanisms be implemented in the Therapy Agents Group to effectively manage contradictory recommendations across different therapeutic domains? (Current system relies on basic prioritization rather than sophisticated conflict resolution)

## Limitations
- Clinical simulation study (n=50) used simulated rather than real clinical workflows, potentially overestimating efficiency gains
- Performance on non-OAI standard radiographs (different protocols, quality) remains untested
- Multi-agent architecture's conflict resolution mechanism between Therapy Agents is described but not empirically validated for edge cases

## Confidence
- **High Confidence**: Architectural design (CNN + LLM cascade) superiority over general VLMs for radiographic grading is well-supported by OAI dataset benchmarks
- **Medium Confidence**: Clinical simulation results showing efficiency gains and improved plan quality are promising but limited by sample size and simulation methodology
- **Medium Confidence**: Risk prediction models show reasonable performance (R² up to 0.58) but may struggle with long-term forecasts (R² dropping to 0.30-0.46 at 4 years)

## Next Checks
1. Deploy KOM in a real clinical setting with 100+ patients to verify 38.5% time reduction and 93% approval rate improvements under actual physician supervision
2. Evaluate KOM on radiographs from different protocols (non-OAI) and populations to assess performance degradation and identify failure thresholds
3. Design test cases with conflicting agent recommendations (e.g., Exercise vs. Surgery contraindications) to validate Clinical Decision Agent's prioritization logic and safety