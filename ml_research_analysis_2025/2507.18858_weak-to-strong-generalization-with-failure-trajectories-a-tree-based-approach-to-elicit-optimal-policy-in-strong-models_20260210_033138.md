---
ver: rpa2
title: 'Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach
  to Elicit Optimal Policy in Strong Models'
arxiv_id: '2507.18858'
source_url: https://arxiv.org/abs/2507.18858
tags:
- strong
- weak
- trajectory
- tree
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of supervising large language
  models (LLMs) that may exceed human capabilities in complex interactive decision-making
  tasks. Traditional weak-to-strong generalization (W2SG) approaches focus on simple
  classification tasks and struggle to capture the nuanced reasoning and decision-making
  required in interactive environments.
---

# Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models

## Quick Facts
- arXiv ID: 2507.18858
- Source URL: https://arxiv.org/abs/2507.18858
- Reference count: 36
- Key outcome: Novel framework improves LLM reasoning in interactive tasks by training strong models on weak model's success and failure trajectories organized in trajectory trees

## Executive Summary
This work addresses the challenge of supervising large language models (LLMs) that may exceed human capabilities in complex interactive decision-making tasks. Traditional weak-to-strong generalization (W2SG) approaches focus on simple classification tasks and struggle to capture the nuanced reasoning and decision-making required in interactive environments. To overcome this limitation, the authors propose a novel framework that leverages trajectories of intermediate actions generated by a weak model to fine-tune a strong model, allowing it to learn not only from successful outcomes but also from failure experiences.

## Method Summary
The framework trains a weak SFT model on expert demonstrations, then generates diverse exploration trajectories (success and failure) through varied sampling. These trajectories are organized into a hierarchical trajectory tree by merging paths with semantically similar observations. The strong model is fine-tuned using either structural preference pairs derived from divergence points in the tree (Tree-DPO) or by extracting optimal paths through Monte Carlo Tree Search (MCTS-SFT). The approach enables learning from both positive and negative experiences, with MCTS synthesizing higher-quality training trajectories than direct weak model outputs.

## Key Results
- MCTS-based W2SG achieves 58.2 avg reward on ScienceWorld vs. 55.4 for Tree DPO
- W2SG methods trained with weak model trajectories outperform strong supervised fine-tuning baselines
- W2SG model can even outperform the SFT strong model in certain tasks
- Optimal trajectory count is 6-7 per task; more trajectories degrade performance

## Why This Works (Mechanism)

### Mechanism 1: Structured Contrastive Learning from Trajectory Trees
Organizing weak model trajectories into a tree structure with shared prefixes enables more informative preference learning than random trajectory pairs. The trajectory tree merges action sequences with common prefixes. When a success trajectory and failure trajectory diverge from a shared prefix, the divergence point isolates the critical decision that differentiates outcomes. DPO training on these structural pairs provides clearer gradient signals.

### Mechanism 2: Negative Knowledge Transfer from Failure Trajectories
Including failure trajectories enables the strong model to learn what NOT to do, improving policy beyond learning solely from successes. The weak model's failure trajectories encode "negative knowledge"—action sequences that lead to low scores. The strong model, with higher capacity, can generalize patterns from these failures to avoid similar mistakes in novel contexts.

### Mechanism 3: MCTS-Based Trajectory Synthesis
Offline MCTS traversal of the trajectory tree synthesizes higher-quality training trajectories than directly using weak model outputs. UCB selection balances exploitation (high average reward nodes) and exploration (low visit count nodes). After multiple traversals, the extracted path greedily selects highest-reward nodes, potentially composing action subsequences never explicitly generated by the weak model.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The paper formalizes LLM agent tasks as POMDPs (U, S, A, O, T, R). Understanding state transitions, observations vs. states, and reward functions is essential for implementing trajectory collection.
  - Quick check question: Can you explain why an agent's policy conditions on observation history rather than full state?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: One training method uses DPO with tree-derived preference pairs. You need to understand implicit reward modeling and KL regularization to implement Eq. 6 correctly.
  - Quick check question: In DPO, why does the reference policy appear in the KL penalty term but not in the preference likelihood?

- Concept: Monte Carlo Tree Search (UCB Selection)
  - Why needed here: The MCTS variant uses Upper Confidence Bound for node selection. Understanding the exploration-exploitation trade-off in Eq. 7 is critical.
  - Quick check question: What happens to MCTS search behavior if you set γ too high vs. too low?

## Architecture Onboarding

- Component map: Weak Model SFT Pipeline -> Trajectory Explorer -> Trajectory Tree Builder -> Training Signal Generator -> Strong Model Fine-tuner
- Critical path: Weak model exploration quality -> Tree richness -> MCTS/pair quality -> Strong model generalization. Insufficient exploration at step 1 cascades through the entire pipeline.
- Design tradeoffs:
  - Tree breadth (number of trajectories M): Paper finds 6-7 trajectories optimal on ScienceWorld; more trajectories degrade performance
  - DPO β coefficient: Lower β (0.1) outperforms higher β (0.5) by maintaining knowledge transfer while avoiding overfitting to weak reference
  - Node merging threshold (ξsim): Aggressive merging creates compact trees but may conflate distinct states
- Failure signatures:
  - Strong model underperforms SFT baseline -> Check trajectory diversity; tree may lack informative divergence points
  - Training loss plateaus early -> Preference pairs may be uninformative; verify success/failure overlap in tree
  - MCTS-extracted paths are incoherent -> Node statistics may be sparse; increase MCTS iterations or trajectory count
- First 3 experiments:
  1. Baseline verification: Reproduce Table 1 row 1 (SFT Weak) and row 4 (SFT Strong) to validate your SFT pipeline before attempting W2SG.
  2. Ablation on trajectory count: Sweep M = {3, 5, 6, 7, 10} on a single task domain and plot avg reward vs. M to find the optimal breadth for your setup.
  3. Failure vs. success-only comparison: Train two strong models—one with full tree (success + failure), one with only success trajectories—to quantify the contribution of negative knowledge.

## Open Questions the Paper Calls Out

- How can the W2SG framework be modified to detect and mitigate specific biases inherited from the weak model's training data during trajectory optimization? The framework's reliance on trajectory optimization may inadvertently encode biases present in the weak models' training data, warranting further investigation into fairness and bias mitigation strategies.

- What is the theoretical or heuristic relationship between task complexity and the optimal number of exploration trajectories (M) required to prevent the performance degradation observed at high trajectory counts? The authors note the existence of an optimal range but leave the mechanism for predicting it as an open problem.

- Can the assumption that there exists a strong policy π* satisfying rπ*(τ+) > rπ*(τ-) for all preference pairs be empirically verified in environments with sparse rewards? The theoretical guarantee in Theorem 1 relies on assumptions that presuppose the existence of a clearly superior policy distinguishable by the preference pairs.

## Limitations
- Performance degrades with too many trajectories (>7 per task leads to noise)
- Unknown optimal exploration parameters for trajectory diversity
- Theoretical guarantees rely on unverified assumptions about trajectory informativeness

## Confidence

- **High confidence**: MCTS-based W2SG outperforms SFT-strong baselines across all three domains (WebShop, ScienceWorld, AlfWorld). This finding is directly observable from Table 1 and Figure 5.
- **Medium confidence**: Failure trajectories provide unique learning signals beyond success trajectories. While the paper shows MCTS-SFT > SFT-strong, the ablation comparing failure+success vs. success-only trajectories is not reported.
- **Low confidence**: The theoretical analysis (Theorem 1) guarantees a reduction in expected loss. The proof relies on properties of the trajectory tree and DPO updates, but the assumptions about trajectory diversity and preference pair informativeness are not empirically verified.

## Next Checks

1. **Ablation on trajectory diversity**: Systematically vary the weak model's exploration parameters (temperature range, sampling strategy) and measure how this affects tree informativeness and downstream W2SG performance. This directly tests the assumption that weak model diversity drives success.

2. **Causal analysis of divergence points**: For a subset of tree pairs, manually annotate whether the first divergent action is actually responsible for the outcome difference. This validates the core assumption behind structural contrastive learning.

3. **Success-only vs. failure+success comparison**: Train strong models using only successful trajectories from the tree, then compare performance to the full failure+success approach. This isolates the contribution of negative knowledge transfer.