---
ver: rpa2
title: Reinforcement Learning for Pollution Detection in a Randomized, Sparse and
  Nonstationary Environment with an Autonomous Underwater Vehicle
arxiv_id: '2510.26347'
source_url: https://arxiv.org/abs/2510.26347
tags:
- learning
- agent
- pollution
- cloud
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently detecting underwater
  pollution clouds using autonomous underwater vehicles in a sparse, randomized, and
  nonstationary environment. The authors modify classical reinforcement learning approaches,
  including hierarchical reinforcement learning, multiple goal learning, trajectory
  reward learning, and a memory-based output filter, to adapt to these difficult conditions.
---

# Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle

## Quick Facts
- arXiv ID: 2510.26347
- Source URL: https://arxiv.org/abs/2510.26347
- Reference count: 16
- The paper demonstrates that a modified Monte Carlo reinforcement learning approach outperforms traditional Q-learning and expert-designed search patterns for finding randomly placed underwater pollution clouds.

## Executive Summary
This paper addresses the challenge of detecting underwater pollution clouds using autonomous underwater vehicles in environments where pollution locations are randomized, sparse, and nonstationary. The authors develop a modified reinforcement learning approach that combines hierarchical action grouping, Monte Carlo updates with zero discount factor, and a memory-based output filter to overcome the limitations of traditional Q-learning in these conditions. Their approach achieves significantly better performance than both standard RL methods and two expert-designed search patterns, demonstrating that reinforcement learning can be effectively adapted for sparse, nonstationary environments with randomly placed targets.

## Method Summary
The approach uses tabular Q-learning with hierarchical actions (options) where each option consists of 3 consecutive steps in the same direction. The key modification is using Monte Carlo updates with discount factor γ=0, which updates Q-values based on the complete trajectory reward rather than bootstrapping from subsequent states. A Memory as Output Filter (MOF) mechanism prevents state revisits by subtracting a memory component from Q-values before action selection, without modifying the underlying Q-table. The agent is trained for 1000 episodes with 1-2 randomly placed pollution clouds per episode, using ϵ-greedy exploration with linear decay and a learning rate of 0.1.

## Key Results
- The HMC agent achieved a median step count of 53.49 compared to 53.51 for Snake and 66.74 for Spiral across 1,000 evaluation episodes
- The RL agent won 643 out of 1,000 duels against the Snake baseline
- Monte Carlo updates (γ=0) outperformed temporal difference learning with standard discount factors
- Hierarchical options with length 3 provided optimal balance between coverage speed and gap size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monte Carlo-based updates with γ=0 outperform temporal difference learning in nonstationary, reward-sparse environments with randomized targets per episode.
- Mechanism: Setting discount factor γ=0 eliminates bootstrapped value propagation that depends on consistent target locations. Instead, Equation 5 updates Q(s,a) using only the trajectory-averaged reward RT, which normalizes by steps taken and clouds found. This transforms the problem from learning "where the target is" (impossible when locations randomize) to learning "what search pattern minimizes expected steps to find unknown targets."
- Core assumption: The optimal search pattern transfers across randomized target locations—that efficient coverage strategies generalize regardless of where targets spawn.
- Evidence anchors:
  - [abstract]: "modified Monte Carlo-based approach significantly outperforms traditional Q-learning"
  - [section III, Results]: "A discount factor of 0 results in higher performance, transforming the update rule from the Q-learning setting as in (1) into a Monte Carlo-based approach as in (5)"
  - [corpus]: Related work on sparse rewards (arXiv:2510.15456) confirms temporal causality knowledge helps, but corpus lacks direct comparison of MC vs TD in nonstationary settings.

### Mechanism 2
- Claim: Hierarchical action grouping (options) reduces search trajectory jitter and enables faster coverage.
- Mechanism: Section II-C.1 defines options as consecutive unidirectional steps (Equation 2: oi = (ai, ai, ai, ...)). With option length J=3 (tuned for grid size 20, pollution diameter 5), the agent commits to multi-step movements, reducing random direction changes. This trades granular control for coverage speed—optimal when targets span multiple cells and exhaustive single-cell exploration is unnecessary.
- Core assumption: Pollution cloud diameter (5 cells) makes sub-cell precision unnecessary; gaps in coverage smaller than cloud size don't miss targets.
- Evidence anchors:
  - [section II-C.1]: "Grouping several action decisions into one decision can cover more area of the state space in fewer decisions made, effectively enabling the agent to cover a larger area of the grid with less random movement, stabilizing the exploration"
  - [section III]: "The optimal option length is found to be 3"
  - [corpus]: Corpus references hierarchical RL surveys (cited as [9], [10]) but lacks direct corpus papers testing options in sparse nonstationary settings.

### Mechanism 3
- Claim: Memory as Output Filter (MOF) prevents state revisits without exploding state space, maintaining tabular tractability.
- Mechanism: Equation 4 subtracts SMOF × M(s') from Q-values before action selection, where M tracks visits and SMOF=10 (tuned). Critically, memory is external to Q-table updates—it filters decisions without becoming state, preserving Markov property for learning while breaking it for action selection. Exploration moves ignore MOF (SMOF=0 during random actions).
- Core assumption: Visit history matters for search efficiency but not for policy generalization—past visits shouldn't permanently alter learned values, only temporarily bias current choice.
- Evidence anchors:
  - [section II-C.4]: "the memory component (0 if not visited, 1 if visited) is externally subtracted from the corresponding Q-value, before a decision is made. This suggests the agent select the next highest Q-value if the preferred state has been visited before"
  - [section III]: "The optimization curve for the MOF value flattens out at 10, indicating that higher MOF values filter out revisiting states similarly"
  - [corpus]: Multi-source plume tracing (arXiv:2505.08825) addresses similar coverage problems but uses multi-agent coordination rather than memory filtering.

## Foundational Learning

- Concept: **Markov Decision Processes and the Markov property**
  - Why needed here: The MOF mechanism explicitly breaks the Markov property by using visit history. Understanding this tension is essential—the Q-table remains Markovian, but action selection becomes history-dependent.
  - Quick check question: If you removed MOF and added visited-status to the state representation instead, what would happen to the Q-table size for a 20×20 grid over 400-step episodes?

- Concept: **Temporal Difference vs Monte Carlo updates**
  - Why needed here: The paper's key insight (γ=0) fundamentally changes the learning algorithm. TD bootstraps from subsequent state values; MC waits for episode completion. This matters when targets randomize each episode.
  - Quick check question: In a stationary environment where the target stays fixed across episodes, would γ=0 still be optimal? Why or why not?

- Concept: **Exploration-exploitation tradeoff and ε-decay**
  - Why needed here: The paper uses ε-soft strategy with linear decay (ε: 1.0→0, decay=0.001). MOF only applies during exploitation phases. Understanding when exploration ignores learned policy is critical for reproducing results.
  - Quick check question: If MOF were applied during exploration moves, would the agent still discover the central-region prioritization shown in Figure 5?

## Architecture Onboarding

- Component map:
Environment (grid, pollution cloud) → State (x, y, pollution_intensity)
                                          ↓
                             Q-Table [state × option]
                                          ↓
                              MOF Filter (external memory)
                                          ↓
                              Option Selection → Execute J steps
                                          ↓
                              Trajectory Reward (Eq. 3) → Q-Update (Eq. 5)

- Critical path:
  1. Initialize Q-table to zeros, memory M to zeros
  2. For each episode: spawn 1-2 random clouds
  3. Select option via argmax(Q(s,o) - SMOF×M(s')) during exploitation; random during exploration
  4. Execute J=3 steps in chosen direction; update M
  5. On cloud detection or episode end: compute RT, update all visited state-option pairs
  6. Decay ε; repeat for 1000 episodes

- Design tradeoffs:
  - **Option length vs. coverage gaps**: Longer options = faster coverage but larger gaps. J=3 balances grid-20/diameter-5.
  - **MOF strength vs. trapping**: SMOF too low = wasted revisits; too high = agent trapped if all neighbors visited. Value 10 empirically sufficient.
  - **Training clouds vs. generalization**: Multiple clouds during training (Section II-C.2) seemed promising but 1-2 clouds performed best in final tuning—more clouds increased episode length without improving policy.

- Failure signatures:
  - **Q-values cluster near start position** (Figure 2 pattern): Discount factor γ>0 in nonstationary setting; target locations don't persist across episodes.
  - **Agent oscillates between adjacent states**: Option length too small or MOF not applied; exploration jitter dominates.
  - **Performance plateaus above exhaustive search**: Learning rate too low, episodes too few, or reward scaling (Sr) misconfigured.

- First 3 experiments:
  1. **Reproduce baseline failure**: Run tabular Q-learning (γ=0.9) on randomized-cloud environment; visualize Q-table heatmaps to confirm Figure 2 pattern. Expected: high values near start, low elsewhere.
  2. **Ablate single components**: Run HMC (γ=0, J=3) without MOF; compare steps-to-target vs. full system. Isolate MOF contribution (paper claims crucial; verify magnitude).
  3. **Stress-test option length**: Sweep J∈{1,2,3,4,5} on varied grid sizes (15, 20, 25) and cloud diameters (3, 5, 7). Confirm J scales with cloud_size/grid_size ratio as paper suggests but doesn't formally prove.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Hierarchical Monte Carlo (HMC) approach be effectively transferred to Deep Reinforcement Learning (DRL) to handle continuous state spaces and dynamic environmental factors like ocean currents?
- Basis in paper: [explicit] The authors state that "insights from this study could be applied to deep reinforcement learning, which can handle larger state spaces," specifically suggesting "a continuous state space, or simulated underwater currents."
- Why unresolved: The current study is confined to a discrete 2D grid world using tabular Q-learning, which cannot handle the high dimensionality of continuous states or the complexity of hydrodynamic simulations.
- What evidence would resolve it: Demonstration of the modified RL agent successfully learning search strategies within a high-fidelity simulator featuring continuous coordinates and simulated current drift.

### Open Question 2
- Question: Can the Memory as Output Filter (MOF) be enhanced by implementing it as a neural network-based "intelligent filter" rather than a static subtraction mechanism?
- Basis in paper: [explicit] The discussion suggests future projects could include "converting the MOF into an intelligent filter by using a neural network."
- Why unresolved: The current MOF implementation relies on a fixed scaling factor ($S_{MOF}$) subtracted from Q-values, which lacks adaptability to complex state-history dependencies.
- What evidence would resolve it: A comparative study showing that a learned, neural-network-based filter reduces state revisits more effectively than the static MOF without destabilizing the learning process.

### Open Question 3
- Question: Does the proposed methodology generalize to other domains characterized by sparse rewards and partial observability, such as aerial or terrestrial search in adverse weather?
- Basis in paper: [explicit] The authors note, "This methodology may be applicable to other scenarios, such as navigating sparse or partially observable environments... Sparse environments are common in oceans but can also be found in the air or on land."
- Why unresolved: The experiments are specific to underwater pollution detection; the agent's ability to learn efficient search patterns in topologies or visibility conditions different from a 2D aquatic grid remains untested.
- What evidence would resolve it: Results from applying the same HMC and MOF modifications to distinct environments (e.g., a drone search simulation) showing performance improvements over baseline search patterns.

### Open Question 4
- Question: Would defining options with more complex semantics (e.g., curved trajectories or variable lengths) yield better performance than the current fixed-length unidirectional steps?
- Basis in paper: [explicit] The authors identify the need for "a more extensive option search with different semantics" as a direction for future work.
- Why unresolved: The current hierarchical approach utilizes fixed-length, straight-line options (action groupings), which may limit the agent's ability to optimize paths in complex or cluttered environments.
- What evidence would resolve it: An ablation study comparing the convergence speed and search efficiency of agents using dynamic or curved options versus the fixed options used in the paper.

## Limitations
- The approach is specifically tuned for pollution clouds of diameter 5 in a 20×20 grid, limiting generalization to different pollution geometries or grid sizes
- The method relies on discrete state spaces and tabular Q-learning, which doesn't scale to continuous state spaces or high-dimensional sensor data
- The Memory as Output Filter mechanism, while effective, lacks theoretical grounding for why the specific scaling factor of 10 works optimally

## Confidence
- **High confidence**: HMC approach outperforms both Q-learning baselines and expert patterns (Snake/Spiral) in the tested configuration, with statistically significant duel results (643/1000 wins vs. Snake)
- **Medium confidence**: The mechanism explaining why γ=0 is necessary (nonstationary targets invalidating bootstrapped value estimates) is sound, but the paper lacks comparison to alternative sparse-reward methods like count-based exploration or intrinsic motivation
- **Medium confidence**: MOF contribution is demonstrated empirically but not theoretically grounded—why scale 10 works isn't derived from first principles

## Next Checks
1. Test HMC with γ=0.9 (standard TD) on stationary targets to verify the claim that bootstrapping fails in nonstationary settings
2. Evaluate the learned policy on pollution clouds of varying diameters (3-7 cells) to test scalability assumptions about option length and coverage gaps
3. Compare against modern sparse-reward RL methods (e.g., count-based exploration, Hindsight Experience Replay) to establish whether the modifications are necessary or if simpler approaches suffice