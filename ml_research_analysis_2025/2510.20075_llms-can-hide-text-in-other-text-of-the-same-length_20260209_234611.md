---
ver: rpa2
title: LLMs can hide text in other text of the same length
arxiv_id: '2510.20075'
source_url: https://arxiv.org/abs/2510.20075
tags:
- text
- figure
- llms
- secret
- same
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Calgacus is a protocol that uses Large Language Models to hide
  a meaningful text within another coherent and plausible text of the same length.
  The method works by encoding the original message as a sequence of token ranks from
  a chosen LLM, then generating the cover text by selecting tokens at those ranks
  from a secret prompt.
---

# LLMs can hide text in other text of the same length

## Quick Facts
- **arXiv ID:** 2510.20075
- **Source URL:** https://arxiv.org/abs/2510.20075
- **Authors:** Antonio Norelli; Michael Bronstein
- **Reference count:** 40
- **Key outcome:** Calgacus protocol uses LLMs to hide meaningful text within another coherent text of identical length through rank-based token selection.

## Executive Summary
Calgacus is a steganographic protocol that enables perfect embedding of arbitrary secret messages within coherent texts of identical token length using Large Language Models. The method works by encoding messages as sequences of token ranks from an LLM's probability distribution, then generating cover texts by selecting tokens at those ranks from a secret prompt. Even small open-source models (8B parameters) can produce high-quality results on consumer hardware, and stegotexts remain within the plausibility range of real texts while being generally less probable than originals. The protocol provides deniability through plausible alternative decodings, raising significant AI safety concerns about encoding unfiltered model outputs within compliant responses.

## Method Summary
The protocol uses an LLM as a shared probabilistic framework where messages are encoded as sequences of token ranks. For encoding, each token of the secret message is ranked based on its probability in the LLM's distribution, then a stegotext is generated from a secret prompt by deterministically selecting tokens at those exact ranks. Decoding reverses this process: given the stegotext and secret prompt, ranks are recomputed and used to select tokens from the LLM without the prompt, perfectly reconstructing the original message. The method requires identical LLM instances for sender and receiver to ensure deterministic logits, and works efficiently on consumer hardware with models as small as 8B parameters.

## Key Results
- Perfect message recovery achieved through rank-based encoding/decoding mechanism
- Stegotexts remain within probability distribution of real texts while being less probable than originals
- Deniability provided through generation of plausible alternative messages with different secret prompts
- AI safety implications demonstrated by encoding unfiltered model outputs within compliant model responses

## Why This Works (Mechanism)

### Mechanism 1
An arbitrary secret message can be perfectly embedded within a different, coherent text of the same token length using a Large Language Model (LLM). The protocol uses the LLM as a shared probabilistic framework where each token of the secret message is ranked based on its probability in the LLM's distribution. A stegotext is then generated from a secret prompt by deterministically selecting tokens at those exact ranks. Decoding reverses this by recomputing ranks from the stegotext and using them to select tokens from the LLM without the prompt. The mechanism fails if sender and receiver use non-identical LLM implementations or if secret message token ranks are consistently high, forcing incoherent token choices.

### Mechanism 2
The generated stegotext is statistically plausible and can fall within the probability distribution of authentic human-written text, making it difficult for humans to detect. Plausibility arises because the protocol constrains token choices using ranks from a coherent message, which correspond to probable tokens in the new context. While stegotexts are generally less probable than originals, their log-probabilities overlap with the distribution of real texts from similar domains. Plausibility fails when the secret message's domain is poorly modeled by the LLM (e.g., rare dialect) or when a short generic prompt fails to provide sufficient context.

### Mechanism 3
The protocol provides plausible deniability for the sender through generation of multiple coherent decodings from different secret prompts. A single stegotext can be decoded into multiple different coherent messages using different secret prompts, allowing an actor under coercion to reveal a "bogus" key that produces a plausible but innocent message. Deniability is weakened if the true secret message is far more probable than any decoy message from a plausible alternative prompt, allowing attackers to identify the most likely original through probability scoring.

## Foundational Learning

**Concept: Autoregressive Language Modeling & Tokenization**
Why needed: This is the bedrock of the protocol, operating by manipulating the LLM's core function of predicting the next token based on conditional probabilities. Understanding tokenization and autoregressive generation is essential to grasp how ranks are derived and used.
Quick check: How does an LLM assign a probability to the token "Caesar" given the context "Gaius Julius", and what would it mean to select a token at rank 5 instead of the most probable one?

**Concept: Log-Probability as a Metric of Text Plausibility**
Why needed: The paper uses cumulative log-probability as a quantitative proxy for a text's "soundness," central to evaluating protocol success (showing stegotexts are plausible) and limitations (showing they are less probable than originals).
Quick check: Why is a text with a higher cumulative log-probability considered more "plausible" by an LLM? How does this allow for comparison between texts of the same length?

**Concept: Steganography vs. Cryptography**
Why needed: The paper differentiates its work from cryptographyâ€”steganography's goal is to hide the *existence* of a message, not just its content. This distinction is critical for understanding AI safety implications of disguising unaligned model output as aligned.
Quick check: What is the fundamental difference between a locked safe (cryptography) and a message written in invisible ink (steganography)?

## Architecture Onboarding

**Component map:** Shared LLM & Tokenizer -> Rank Extractor (Encoder) -> Deterministic Generator (Encoder) -> Reconstructor (Decoder)

**Critical path:** Ensuring exact reproducibility of logits is paramount. Any discrepancy in the LLM's forward pass (hardware, library versions, floating-point precision) will lead to different token ranks and decoding failure. Consistent inference engines like llama.cpp are critical.

**Design tradeoffs:**
- Model Size vs. Quality/Speed: Mid-sized models like Llama 3 8B balance quality and performance; GPT-2 insufficient, larger models offer marginal gains but are slower
- Secret Prompt Length vs. Steerability: Longer prompts improve stegotext coherence and control but increase key size; short prompts are brittle and prone to failure
- Rank Inversion: Applying high ranks at the end can improve quality by letting context absorb improbable choices, but is a heuristic

**Failure signatures:**
1. Gibberish Output: Stegotext is incoherent due to secret message from poorly modeled domain (e.g., rare dialect) yielding very high ranks. Remedy: Use more capable or specialized LLM.
2. Decoding Failure: Reconstructed text doesn't match original due to non-identical LLM environments. Remedy: Ensure exact same model weights, tokenization, and inference engine with identical settings.
3. Poor Steerability: Stegotext doesn't match intended topic/style due to short generic prompt. Remedy: Use more detailed prompt or apply rank inversion.

**First 3 experiments:**
1. Reproduce core result using provided demo notebook and quantized Llama 3 8B model to encode a simple sentence into a stegotext with straightforward secret prompt, verifying successful decoding.
2. Test model capability boundary by encoding same message using capable model (Llama 3 8B) and weaker one (e.g., GPT-2), comparing plausibility of resulting stegotexts.
3. Quantify plausibility by encoding 10 real Reddit posts (85 tokens long) into stegotexts, computing cumulative log-probabilities for both originals and stegotexts using separate LLM, and plotting distributions to observe probability gap and overlap.

## Open Questions the Paper Calls Out

**Open Question 1:** Can an attacker practically reduce the secret key search space by exploiting the constraint that keys must be coherent natural language consistent with the stegotext's context? The authors note attackers could reduce search space since keys are expected to be mostly sound natural language coherent with context, but feasibility remains unclear and is an open research question.

**Open Question 2:** How can the probability contribution of semantic meaning be disentangled from style, grammar, and length when assessing what an LLM "knows"? The paper notes it's not clear how to construct the second element of the pair for arbitrary texts when trying to isolate meaning-related probability from confounding factors.

**Open Question 3:** Can the Calgacus protocol be effectively extended to non-text modalities like images, audio, and sketches using discrete autoregressive models? The method can be naturally extended to other modalities such as images, sketches, music, or speech, but exploration of these modalities is left to future work.

## Limitations
- Relies critically on deterministic LLM forward passes across different hardware/software environments; floating-point non-associativity could cause rank mismatches
- Plausibility claims evaluated using single proxy metric (log-probability) which may not fully capture human detection difficulty
- Deniability property depends on assumption that attackers cannot efficiently search vast space of possible secret prompts or exploit probability gaps

## Confidence
- **High Confidence:** Core encoding/decoding mechanism works as described with perfect message recovery demonstrated
- **Medium Confidence:** Plausibility claims supported by log-probability analysis showing stegotexts fall within distribution of real texts
- **Medium Confidence:** Deniability property is theoretically sound and demonstrated with examples, but practical security against determined attackers not fully quantified
- **Medium Confidence:** AI safety implications are valid and significant, but ease of real-world exploitation requires further investigation

## Next Checks
1. **Cross-Platform Reproducibility Test:** Implement protocol using identical model weights but different inference engines (llama.cpp on GPU vs CPU, or different quantization tools) to quantify rank divergence and identify failure thresholds.

2. **Human Detection Study:** Conduct blinded study where participants distinguish stegotexts from real texts from same domain, measuring detection accuracy and identifying linguistic features that reveal steganography.

3. **Probability Gap Analysis:** For large sample of stegotexts, systematically search for alternative secret prompts that decode to coherent but different messages, measuring probability distribution of true vs decoy messages to quantify deniability strength.