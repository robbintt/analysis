---
ver: rpa2
title: 'Balancing the Budget: Understanding Trade-offs Between Supervised and Preference-Based
  Finetuning'
arxiv_id: '2502.11284'
source_url: https://arxiv.org/abs/2502.11284
tags:
- data
- preference
- arxiv
- finetuning
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the optimal allocation of training data
  between Supervised Fine-Tuning (SFT) and Preference Finetuning (PFT) for post-training
  LLMs under a fixed data annotation budget. The authors conduct extensive experiments
  across four tasks (summarization, helpfulness, instruction following, grade school
  math), multiple model sizes, and various data allocation ratios.
---

# Balancing the Budget: Understanding Trade-offs Between Supervised and Preference-Based Finetuning

## Quick Facts
- arXiv ID: 2502.11284
- Source URL: https://arxiv.org/abs/2502.11284
- Authors: Mohit Raghavendra; Junmo Kang; Alan Ritter
- Reference count: 40
- This paper investigates optimal allocation of training data between SFT and PFT under fixed annotation budgets

## Executive Summary
This paper systematically investigates how to optimally allocate annotation budgets between Supervised Fine-Tuning (SFT) and Preference Finetuning (PFT/DPO) when post-training large language models. The authors conduct extensive experiments across four diverse tasks, multiple model sizes, and various data allocation ratios. They identify a "cold start problem" where applying PFT directly on base models yields suboptimal performance, particularly for analytical tasks like mathematics. Their key finding is that SFT dominates in low-data regimes (<1,000 examples), while optimal performance at larger budgets requires combining SFT with PFT using higher proportions of preference data. Under most cost structures, spending more budget on preference data after initial SFT proves most cost-effective.

## Method Summary
The authors test data allocation ratios {1.0, 0.75, 0.5, 0.25, 0.0} for SFT vs PFT across budgets of 100-20,000 examples. They use Llama3.1-8B and Qwen2.5-7B models with LoRA adapters (rank=32, α=32). SFT uses LR=5e-5, batch=16, cosine scheduler, weight_decay=0.01 for 2 epochs. DPO uses LR=5e-6, β=0.1, same other settings for 2 epochs. Tasks include summarization (Reddit TL;DR), helpfulness (HelpSteer/HelpSteer2), instruction following (Tülu3 Persona IF), and grade school math (GSM8k with synthetic annotations). Performance is measured by task-specific metrics including win rates, IFEval accuracy, and GSM8k test accuracy.

## Key Results
- SFT on base model dominates performance in low-data regimes (<1,000 annotated examples)
- With larger data budgets, increasing portions allocated towards preference data yields optimal performance
- A "cold start problem" exists where PFT directly on base models fails for analytical tasks like math
- Even <10% of budget allocated to SFT before PFT significantly improves performance (15-20% on GSM8k)
- Under most data annotation cost structures, spending more budget on preference data after initial SFT is most cost-effective

## Why This Works (Mechanism)

### Mechanism 1: SFT Aligns Response Style to Enable Effective PFT
- Claim: Minimal SFT (<10% of budget) teaches the model the expected response format, creating a compatible reference model for subsequent preference optimization
- Mechanism: SFT directly maximizes the probability of generating correctly formatted responses (e.g., step-by-step math reasoning). When the target format deviates from the base model's default behavior (one-word answers vs. structured reasoning), SFT bridges this distribution gap. The resulting SFT checkpoint becomes the reference model for DPO, reducing the KL penalty that would otherwise suppress format learning
- Core assumption: The base model's default response distribution differs meaningfully from the target task's expected format
- Evidence anchors:
  - [abstract]: "it aligns the model's response style to the expected format"
  - [section 3.2]: "Our analysis shows that it effectively aligns the model's response style to the required format... such an SFT model acts as a compatible reference model for DPO to finetune over, without having to deviate too much from it"
  - [corpus]: "Metis-SPECS" paper confirms SFT-based cold start for RL in multimodal settings
- Break condition: If base model already produces responses in the target format, SFT provides diminishing returns

### Mechanism 2: DPO's KL Penalty Suppresses Novel Format Acquisition
- Claim: Running DPO directly on base models fails for tasks requiring response formats the base model rarely produces, because DPO penalizes deviations from the reference distribution
- Mechanism: DPO optimizes $\log \sigma(\beta \Delta)$ where $\Delta$ includes the ratio $\pi_\theta/\pi_{ref}$. When $\pi_{ref}$ (base model) assigns low probability to step-by-step reasoning, increasing $\pi_\theta$ for such responses increases the KL penalty. The β parameter controls this penalty strength
- Core assumption: The DPO objective implicitly regularizes toward the reference model's distribution
- Evidence anchors:
  - [abstract]: "due to the distribution shift arising from using DPO directly on the base model to elicit step-by-step reasoning"
  - [section 3.2, Figure 7]: "Lower values of β (that penalize deviation from the reference base model less) lead to slightly better math performance"
  - [corpus]: Limited direct evidence; "Learning Dynamics of VLM Finetuning" mentions gradient instability in preference learning but focuses on VLMs
- Break condition: Lowering β can partially compensate, but minimal SFT remains more effective per the paper's experiments

### Mechanism 3: Regime-Dependent Optimal Allocation
- Claim: SFT dominates in low-data regimes (<1,000 examples) while mixed SFT+PFT with higher PFT ratios becomes optimal at larger budgets
- Mechanism: SFT provides dense direct supervision but follows power-law scaling with diminishing returns. PFT provides comparative signals that scale better at higher data volumes. The crossover point depends on task type (analytical tasks benefit more from SFT; stylistic tasks shift toward PFT sooner)
- Core assumption: SFT and PFT have different scaling curves with respect to data budget
- Evidence anchors:
  - [abstract]: "SFT on the base model dominates performance in low-data regimes (<1,000 annotated examples). With larger data-budgets... increasing portions allocated towards preference data yields optimal performance"
  - [section 3.1]: "finetuning with 5000 examples with a 25% SFT allocation is on par with training with 20,000 examples with 75% SFT allocation"
  - [corpus]: Weak corpus support; related papers don't directly address this budget allocation trade-off
- Break condition: Very analytical tasks (math) may never strongly benefit from PFT over SFT even at scale

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Understanding why DPO needs a compatible reference model is essential to grasp the cold start problem
  - Quick check question: In DPO, what happens to the loss when $\pi_\theta$ assigns high probability to a response that $\pi_{ref}$ assigns low probability to?

- Concept: KL Divergence in RLHF
  - Why needed here: The implicit KL penalty in DPO explains why the reference model's behavior matters
  - Quick check question: How does the β parameter in DPO control the trade-off between maximizing preference and staying close to the reference model?

- Concept: Response Format vs. Response Quality
  - Why needed here: The paper distinguishes between learning *how* to respond (format) and learning *what* responses are preferred (quality)
  - Quick check question: Why might format learning be harder via preference comparisons than via direct supervision?

## Architecture Onboarding

- Component map:
  - Base model -> SFT stage (next-token prediction loss) -> PFT stage (DPO loss) -> Reference model (frozen copy for DPO)
  - LoRA adapters (rank=32, α=32) applied throughout

- Critical path:
  1. Start with base pretrained model
  2. Allocate 5-25% of budget to SFT (minimum ~100-1000 examples depending on task)
  3. Use SFT checkpoint as reference model for DPO
  4. Train DPO on remaining preference data
  5. Evaluate on task-specific benchmark

- Design tradeoffs:
  - SFT ratio: 0.1-0.25 optimal for most tasks at 5K+ budgets; higher (0.5-1.0) for <1K budgets
  - β in DPO: Paper uses 0.1; lowering helps if you must skip SFT, but SFT-first is better
  - Cost structure: SFT annotation costs ~2x preference annotation (synthetic: $0.0023 vs $0.0011)
  - Model size: Smaller models (<3B) show weaker PFT gains; cold start problem more severe

- Failure signatures:
  - Rambling responses on math: PFT-only models produce long, repetitive pseudo-reasoning without proper format (see Appendix A.6 examples)
  - No performance gain from PFT: Check if reference model (base) already matches target format poorly
  - 15-20% GSM8k drop: Symptom of skipping SFT entirely for analytical tasks

- First 3 experiments:
  1. Cold start diagnostic: Run SFT-only (ratio=1.0) vs PFT-only (ratio=0.0) with 1K examples on your target task. If PFT-only underperforms significantly, you have a cold start problem
  2. SFT ratio sweep at target budget: Test ratios [0.1, 0.25, 0.5, 0.75] at your actual budget. Plot performance vs. ratio to find the plateau point
  3. Minimal SFT threshold: For analytical tasks, test SFT amounts [0, 100, 500, 1000] with fixed total budget to identify the minimum viable SFT investment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the optimal SFT-PFT allocation ratios hold for models larger than 10 billion parameters?
- Basis in paper: [explicit] The authors limit the scope to models under 10 billion parameters and explicitly call for studying larger models in the Limitations and Problem Formulation sections
- Why unresolved: Compute costs were ignored as a budget factor for smaller models, but they become significant for larger models, potentially altering the optimal data allocation trade-off
- What evidence would resolve it: Replicating the budget allocation experiments on 70B+ parameter models while incorporating compute costs into the budget constraint

### Open Question 2
- Question: How does the optimal allocation strategy change when employing online reinforcement learning methods (e.g., PPO) instead of offline preference optimization?
- Basis in paper: [explicit] The paper relies on offline methods (DPO/KTO) and states in the Limitations that this motivates future work to study "more complex RL-based methods and their interplay with SFT"
- Why unresolved: Online methods involve iterative training and separate reward modeling, creating a distinct cost structure and data dynamic compared to the offline setup analyzed here
- What evidence would resolve it: Comparative experiments analyzing the SFT vs. RLHF trade-off curve using online algorithms under fixed annotation and compute budgets

### Open Question 3
- Question: Can preference finetuning methodologies be modified to yield significant gains over pure SFT for analytical reasoning tasks like mathematics?
- Basis in paper: [inferred] The Conclusion notes that for analytical tasks, even the most optimal SFT-PFT allocation provided only "modest improvements over just SFT," unlike in stylistic tasks
- Why unresolved: The paper identifies a "cold start" issue for reasoning but suggests current PFT methods may primarily align response style rather than enhance reasoning capability
- What evidence would resolve it: Developing PFT objectives or data curation strategies that result in substantial accuracy improvements on benchmarks like GSM8k compared to SFT-only baselines

## Limitations
- Results may not generalize to models significantly larger than 10 billion parameters
- Synthetic preference data may not perfectly capture human preferences
- Optimal allocation ratios are based on limited budget points, creating interpolation uncertainty

## Confidence
- High: SFT dominance in low-data regimes (<1,000 examples); cold start problem existence and mitigation; cost-effectiveness under most cost structures
- Medium: Optimal allocation ratios (5-25% SFT for budgets >5K); task-type dependent crossover points; PFT scaling advantages at higher data volumes
- Low: Synthetic preference data perfectly approximating human preferences; generalization to model architectures outside Llama3.1/Qwen2.5 families; extrapolation of optimal ratios to untested budget sizes

## Next Checks
1. **Format-Misalignment Diagnostic**: For your target task, manually inspect 50 base model responses to quantify format misalignment. If >30% deviate from expected format, you likely have a severe cold start problem requiring >10% SFT allocation

2. **Budget Interpolation Validation**: Using the paper's 100, 1K, 5K, and 20K results, interpolate expected performance at 2.5K and 10K budgets. Run experiments at these intermediate points to test whether the scaling curves follow predicted patterns

3. **Base Model Behavior Characterization**: For your specific base model, measure the probability distribution of response formats (structured vs. unstructured) on 100 prompts from your target task. Compare this to the SFT checkpoint format distribution. Large KL divergence (>1.0) confirms the paper's mechanism that SFT bridges the distribution gap enabling effective PFT