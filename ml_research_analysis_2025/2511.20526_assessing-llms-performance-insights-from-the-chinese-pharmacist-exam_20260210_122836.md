---
ver: rpa2
title: 'Assessing LLMs'' Performance: Insights from the Chinese Pharmacist Exam'
arxiv_id: '2511.20526'
source_url: https://arxiv.org/abs/2511.20526
tags:
- unit
- deepseek-r1
- chatgpt-4o
- performance
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of two large language models,
  ChatGPT-4o and DeepSeek-R1, on real Chinese pharmacist licensing exam questions
  (2017-2021). Using 2,306 text-only multiple-choice questions from four professional
  units, the models were tested for exact accuracy against official answer keys.
---

# Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam

## Quick Facts
- arXiv ID: 2511.20526
- Source URL: https://arxiv.org/abs/2511.20526
- Reference count: 24
- Primary result: DeepSeek-R1 (90.0%) significantly outperformed ChatGPT-4o (76.1%) on Chinese pharmacist licensing exam questions (p < 0.001)

## Executive Summary
This study evaluated two large language models—ChatGPT-4o and DeepSeek-R1—on 2,306 text-only multiple-choice questions from Chinese pharmacist licensing exams (2017-2021). DeepSeek-R1 achieved significantly higher accuracy (90.0%) compared to ChatGPT-4o (76.1%), with particular advantages in foundational and clinical synthesis units. The results suggest DeepSeek-R1's domain-specific training on Chinese medical materials contributes to its superior performance, though human oversight remains essential for ethical deployment in high-stakes contexts.

## Method Summary
The study used 2,306 text-only multiple-choice questions from Chinese pharmacist licensing exams (2017-2021), categorized into four professional units. Questions were input in original Chinese format using a single system prompt. Models were evaluated via exact match against official answer keys, with statistical analysis using Pearson's Chi-squared test for overall comparisons and Fisher's exact test for unit-level analysis. The study explicitly excluded questions containing tables or images to ensure text-only input.

## Key Results
- DeepSeek-R1 significantly outperformed ChatGPT-4o overall (90.0% vs. 76.1%, p < 0.001)
- DeepSeek-R1 showed consistent advantages in foundational and clinical synthesis units
- Both models performed poorly on multiple-choice questions due to small sample size
- Qualitative analysis revealed domain-specific reasoning strengths in DeepSeek-R1 for complex clinical synthesis

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Training Alignment
Specialized training data on Chinese medical and pharmaceutical corpora creates stronger alignment with exam-specific terminology and factual structures, leading to more accurate retrieval and synthesis during inference.

### Mechanism 2: Linguistic-Structural Alignment with Chinese Exam Format
DeepSeek-R1's Chinese-native optimization provides better semantic parsing of exam questions, reducing internal translation errors that may affect ChatGPT-4o's performance.

### Mechanism 3: Chain-of-Thought Reasoning for Complex Clinical Synthesis
DeepSeek-R1's reasoning-focused architecture enables better multi-step clinical synthesis, particularly for case-based questions requiring guideline discernment.

## Foundational Learning

- **Domain Adaptation vs. General-Purpose Training:** Understanding why models with different training distributions perform differently on specialized tasks. Quick check: Can you explain why a model trained on general web text might struggle with pharmaceutical potency comparisons?
- **Statistical Significance with Small Samples (Fisher's Exact Test):** Understanding why multiple-choice results showed no significant differences despite apparent gaps. Quick check: Why might a 40 percentage point gap (e.g., 75% vs. 25%) fail to reach statistical significance?
- **Training Data Contamination Risk:** Understanding how exam questions from 2017-2021 appearing in model training data could inflate performance estimates. Quick check: What strategies can distinguish genuine model reasoning from memorized answers?

## Architecture Onboarding

- **Component map:** Input layer (Chinese text-only questions) -> Model layer (ChatGPT-4o vs. DeepSeek-R1) -> Evaluation layer (Exact match against answer keys) -> Statistical layer (Chi-squared and Fisher's exact tests)
- **Critical path:** Question extraction -> Chinese text preservation -> single-prompt input -> response extraction -> exact accuracy scoring -> unit-level stratification
- **Design tradeoffs:** Text-only exclusion removes multimodal complexity but limits realism; exact-match scoring is strict but aligns with licensing exam standards; single time-point testing captures real-world snapshot but limits reproducibility
- **Failure signatures:** High single-choice accuracy but low multiple-choice accuracy suggests synthesis weakness; correct answer but wrong reasoning indicates surface-level pattern matching; unit-specific variance reveals domain gaps
- **First 3 experiments:**
  1. Test whether explicit chain-of-thought prompts narrow the ChatGPT-4o gap on Unit 4
  2. Input questions in translated English to ChatGPT-4o to isolate linguistic vs. domain effects
  3. Use held-out 2024-2025 exam questions to assess whether older questions inflated scores

## Open Questions the Paper Calls Out

### Open Question 1
How does LLM performance compare to human benchmarks on the Chinese Pharmacist Licensing Examination? The study was restricted to comparing models against answer keys because cohort-level human score data is not publicly available. A comparative study using average human candidate scores from the same exam years (2017-2021) would resolve this.

### Open Question 2
What are the specific reasoning failures and error patterns exhibited by LLMs across the full spectrum of pharmaceutical exam questions? The current study prioritized quantitative accuracy metrics and select case studies over comprehensive categorization of why models failed on incorrect responses. A detailed qualitative review of all incorrect model responses would resolve this.

### Open Question 3
Does high performance on text-based licensing exams translate to competence in real-world, multimedia clinical scenarios? The methodology excluded questions containing tables or images, creating a gap between exam performance and practical clinical utility. Evaluating model performance on datasets including visual inputs and complex clinical simulations would resolve this.

### Open Question 4
To what extent does the "English-centric bias" of general-purpose models disadvantage them in non-English pharmaceutical assessments? The study design used original language input, making it impossible to isolate domain knowledge deficits from linguistic processing deficits. A controlled experiment comparing model performance in both Chinese and translated English formats would resolve this.

## Limitations
- Text-only question restriction limits generalizability to real-world pharmacy exams containing tables, images, and other multimodal content
- Potential training data contamination from exam questions released between 2017-2021 could artificially inflate performance scores
- Single-prompt approach without follow-up clarification may not reflect actual clinical reasoning workflows

## Confidence

**High Confidence:** Overall performance difference between DeepSeek-R1 (90.0%) and ChatGPT-4o (76.1%) is statistically significant with large sample size

**Medium Confidence:** Domain-specific advantages of DeepSeek-R1 are supported but could reflect training data contamination

**Low Confidence:** Qualitative reasoning superiority claims require additional verification due to small sample sizes in unit-level comparisons

## Next Checks
1. Test both models on recently released 2024-2025 exam questions to assess potential training data contamination effects
2. Implement multi-prompt protocol with follow-up questions to evaluate true clinical reasoning capabilities beyond single-answer responses
3. Conduct human expert validation of a subset of questions to establish baseline performance and identify potential question ambiguities or errors in official answer keys