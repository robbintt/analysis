---
ver: rpa2
title: If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong
  Learning in LLMs
arxiv_id: '2503.23514'
source_url: https://arxiv.org/abs/2503.23514
tags:
- wang
- zhang
- memory
- learning
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LIFESTATE-BENCH, a benchmark for evaluating
  lifelong learning in large language models (LLMs) through multi-turn, multi-agent
  interactions. The key innovation is modeling cumulative experience as episodic timelines
  with structured scene details and character interactions, enabling objective fact-checking
  via self-awareness, memory retrieval, and relationship tracking questions.
---

# If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs

## Quick Facts
- arXiv ID: 2503.23514
- Source URL: https://arxiv.org/abs/2503.23514
- Authors: Siqi Fan; Xiusheng Huang; Yiqun Yao; Xuezhi Fang; Kang Liu; Peng Han; Shuo Shang; Aixin Sun; Yequan Wang
- Reference count: 26
- Key outcome: Non-parametric memory methods (direct/summary concatenation) significantly outperform parametric approaches (knowledge editing/LoRA) in managing stateful learning across episodic multi-agent interactions, with DeepSeek R1 achieving 67.3% accuracy on Hamlet dataset.

## Executive Summary
This paper introduces LIFESTATE-BENCH, a benchmark for evaluating lifelong learning in LLMs through multi-turn, multi-agent interactions modeled as episodic timelines. The benchmark tests three state dimensions—self-awareness, memory retrieval, and relationship tracking—using structured scene details and character interactions. Experiments with Llama3.1-8B, GPT-4-turbo, and DeepSeek R1 demonstrate that non-parametric memory methods significantly outperform parametric approaches, while all models exhibit catastrophic forgetting as episodes progress.

## Method Summary
The benchmark constructs episodic datasets (Hamlet and Synthetic) with structured scene details (Location, Time, Narration, Dialogues) and generates fact-checking questions across three state dimensions. Four memory methods are compared: Direct Episode Concatenation, Summary Concatenation via GPT, Knowledge Editing via GRACE, and LoRA fine-tuning. Evaluation uses LLM-as-Judge with DeepSeek evaluator scoring responses against ground truth. The approach enables objective assessment of whether models can maintain coherent state across extended interactions while tracking evolving relationships and facts.

## Key Results
- Non-parametric methods (Direct/Summary Concatenation) outperform parametric approaches by 20-30 percentage points on average
- DeepSeek R1 achieves highest overall accuracy at 67.3% on Hamlet dataset
- All models exhibit catastrophic forgetting, with performance declining across episodes regardless of memory method
- Relationship shift tracking is most challenging dimension (scores ~40-60%) compared to self-awareness (~80-90%)

## Why This Works (Mechanism)

### Mechanism 1
Non-parametric memory methods outperform parametric approaches for stateful learning in episodic multi-agent interactions. Direct episode concatenation and summary concatenation preserve information in explicit context windows, avoiding the precision loss that occurs when encoding memories into model parameters. Parametric methods like LoRA fine-tuning and knowledge editing must compress experiences into weight updates, which causes information degradation and greater susceptibility to interference.

### Mechanism 2
Catastrophic forgetting intensifies as episode count increases regardless of memory method. As the episode timeline extends, models must maintain an expanding state representation while new information creates interference patterns. Parametric methods suffer more severely because weight updates for new episodes overwrite or distort previously encoded information. Even non-parametric methods show decline, suggesting attention mechanisms struggle to retrieve relevant historical context as the sequence grows.

### Mechanism 3
Relationship shift tracking is the most difficult state dimension across all methods and models. Relationship changes require integrating multiple episodic memories, inferring implicit social dynamics, and updating relational representations—which may conflict with prior states. Self-awareness questions rely on stable identity information, and factual memory targets discrete events, but relationship shifts demand compositional reasoning across temporal context with higher cognitive load.

## Foundational Learning

- **Superposition property in LLMs**: LLMs start as "stateless superpositions of simulacra" that gradually converge to specific characters through interaction. Understanding this explains why lifelong learning evaluation matters—models lack intrinsic persistent state.
  - Quick check: Can you explain why an LLM's next-token prediction training creates a "superposition" of possible personas, and how sustained dialogue might collapse this into a more coherent character?

- **Catastrophic forgetting in sequential learning**: The central finding is that all models exhibit forgetting as episodes progress. Without understanding this phenomenon, you cannot interpret why parametric methods fail more severely or why the benchmark is designed to detect this specific failure mode.
  - Quick check: What is the core mechanism behind catastrophic forgetting in neural networks, and why would fine-tuning on sequential episodes cause more interference than concatenating them as context?

- **Knowledge editing vs. context-based memory**: The benchmark explicitly compares parametric methods (knowledge editing, LoRA) against non-parametric methods (direct/summary concatenation). Understanding the tradeoffs between modifying model weights versus expanding context is essential for interpreting results and planning improvements.
  - Quick check: What are the computational and capacity tradeoffs between encoding new information via LoRA fine-tuning versus including it in the context window, and when would each approach be preferred?

## Architecture Onboarding

- **Component map**: Episodic datasets (Hamlet, Synthetic) -> Structured scene details (L,T,N,D) -> State dimensions (Self-awareness, Memory Retrieval, Relationship Shift) -> Memory methods (Direct Concat, Summary Concat, KE, LoRA) -> LLM-as-Judge evaluation

- **Critical path**: 1. Load episodic data with structured fields 2. Construct context from E₁ through Eₜ₋₁ using chosen memory method 3. Generate questions Q(r,t) for each role r 4. Collect model responses A'(r,t) and compare against ground truth A(r,t) 5. Score via LLM judge with pairwise grading against factual reference answers

- **Design tradeoffs**: Direct vs Summary Concatenation (full info vs compression), Hamlet vs Synthetic datasets (complexity vs leakage), Open-ended vs Factual questions (evaluation reliability vs natural interaction)

- **Failure signatures**: Knowledge Editing shows sharpest accuracy decline across episodes; LoRA fine-tuning underperforms non-parametric by 20-30 points; Summary Concatenation fails on relationship tracking; Early episodes show overconfidence before decline

- **First 3 experiments**: 1. Reproduce baseline with Llama3.1-8B on Hamlet to verify 58% vs 25% accuracy gap 2. Test context window limits by varying episode count to find saturation threshold 3. Implement hybrid memory combining Summary for distant episodes with Direct for recent episodes

## Open Questions the Paper Calls Out

- **How can catastrophic forgetting be effectively mitigated in LLMs during extended multi-turn interactions to maintain state consistency?**
  - Basis: The authors state that "all models exhibit challenges with catastrophic forgetting as interactions extend"
  - Why unresolved: Experiments showed performance declined across all methods as the narrative timeline progressed
  - What evidence would resolve it: A method maintaining high accuracy on LIFESTATE-BENCH questions in later episodes without performance drops

- **Can parametric memory methods be optimized to match the stateful learning effectiveness of non-parametric context concatenation?**
  - Basis: The paper concludes that "Non-parametric methods outperform parametric ones"
  - Why unresolved: Non-parametric methods are bounded by context window limits while parametric methods suffered from lower precision
  - What evidence would resolve it: A training or editing technique achieving accuracy comparable to Direct Concatenation without increasing context window size

- **What specific mechanisms are required to improve the tracking of dynamic relationship shifts in multi-agent narratives?**
  - Basis: The Conclusion notes "models have significant room for improvement, especially in enhancing relationship shifts"
  - Why unresolved: "Relation Shift" was the most challenging dimension with models failing to reason about evolving dynamics
  - What evidence would resolve it: Architectural changes or prompting strategies yielding substantial increase in "Relation Shift" accuracy

## Limitations

- Benchmark design may not fully capture complexity of lifelong learning in open-ended scenarios
- Reliance on LLM-as-judge introduces evaluator bias
- Comparison between parametric and non-parametric methods is constrained by fixed context window sizes
- Synthetic dataset may not adequately represent complexity of real-world interactions

## Confidence

- **High confidence**: Non-parametric methods outperform parametric approaches in tested context window regime
- **Medium confidence**: Catastrophic forgetting increases with episode count across all methods
- **Medium confidence**: Relationship shift tracking is most challenging state dimension

## Next Checks

1. **Context window saturation test**: Systematically vary episode count from 5 to 25 episodes using Direct Concatenation to identify precise threshold where context window limits begin degrading performance

2. **External evaluator validation**: Replace DeepSeek LLM-as-judge with human annotators on 5% stratified sample to assess evaluator bias and calculate inter-annotator agreement

3. **Retrieval-augmented baseline**: Implement retrieval-augmented method with separate memory store of episode summaries and compare against Direct Concatenation to test whether dedicated memory architectures can mitigate catastrophic forgetting