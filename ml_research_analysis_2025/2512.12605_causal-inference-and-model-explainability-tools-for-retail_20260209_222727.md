---
ver: rpa2
title: Causal inference and model explainability tools for retail
arxiv_id: '2512.12605'
source_url: https://arxiv.org/abs/2512.12605
tags:
- causal
- interpretability
- inference
- features
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates model interpretability and causal inference
  in retail data analytics, addressing the challenge of understanding complex ML model
  predictions and identifying true causal relationships between operational factors
  and sales. Using a proprietary retail dataset spanning 2019-2024 with 7 key features,
  the authors apply SHAP values for model interpretability and double machine learning
  for causal inference.
---

# Causal inference and model explainability tools for retail

## Quick Facts
- arXiv ID: 2512.12605
- Source URL: https://arxiv.org/abs/2512.12605
- Reference count: 34
- One-line primary result: Model interpretability and causal inference framework for retail data reveals confounding effects and corrects causal sign estimates

## Executive Summary
This study addresses the challenge of understanding complex ML model predictions and identifying true causal relationships in retail analytics. Using a proprietary dataset spanning 2019-2024 with 7 key features, the authors apply SHAP values for model interpretability and double machine learning for causal inference. The work demonstrates that while predictive models capture correlations, they can mislead about causal drivers due to confounding effects, and provides a framework to correct these errors through systematic confounder inclusion.

## Method Summary
The authors preprocess retail data (2019-2024) by standardizing features and reducing from 64 to 7 features using correlation filtering. They train three models (XGBoost, Random Forest, Explainable Boosting Regressor) with hyperparameter optimization, compute SHAP values for interpretability, and perform hierarchical clustering on SHAP redundancy to identify confounding features. Double machine learning is then applied to estimate causal effects, progressively adding confounders (F2, F4, F5) to correct for spurious correlations. The framework evaluates both predictive accuracy (MSAE) and causal effect validity.

## Key Results
- Explainable Boosting Regressor produces lower-variance SHAP explanations despite lower predictive accuracy compared to XGBoost and Random Forest
- SHAP values alone cannot reliably establish causal relationships due to confounding effects in observational retail data
- Double machine learning with multiple confounders (F2, F4, F5) corrects the sign of causal effects, demonstrating the importance of accounting for observable confounding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inherently interpretable models produce lower-variance SHAP explanations despite lower predictive accuracy.
- Mechanism: Explainable Boosting Regressors encode structural constraints (e.g., additive feature effects, limited interactions) that regularize the explanation space, reducing variability in Shapley value estimates across bootstrap samples or similar instances.
- Core assumption: Lower variance in SHAP values indicates more stable, trustworthy explanations for stakeholder communication.
- Evidence anchors:
  - [abstract]: "an inherently interpretable model (Explainable Boosting Regressor) exhibits lower variance in SHAP values despite lower predictive accuracy compared to XGBoost and Random Forest"
  - [section 4.1]: "the SHAP values for the inherently explainable regressor have less variance. This is an interesting finding, which might be attributed to the inherent explainability in these models."
  - [corpus]: Weak direct support; neighbor papers discuss explainability but do not address SHAP variance specifically.
- Break condition: When business requirements demand maximal predictive accuracy and explanation stability is secondary; or when feature interactions are critical and EBR's interaction limits degrade both accuracy and explanation fidelity.

### Mechanism 2
- Claim: SHAP values cannot establish causal relationships when confounders correlate with both treatment and outcome.
- Mechanism: SHAP computes feature attributions based on conditional expectations in the predictive model; if confounders create spurious correlations, the model captures these associations and SHAP reflects them, not structural causal effects.
- Core assumption: Features are not independently manipulable "lever arms"—real-world retail variables co-vary through hidden generative processes.
- Evidence anchors:
  - [abstract]: "SHAP values alone cannot reliably establish causal relationships due to confounding effects"
  - [section 2.2]: "SHAP values would exactly correspond to a causal effect if each of the features was an independent 'lever arm' that the CEO could tune as she liked... Such an ideal scenario is unlikely."
  - [corpus]: Neighbor paper on churn analytics (110638) notes black-box models limit insight into determinants—consistent with attribution-causality gap.
- Break condition: When features are experimentally randomized or when no confounding exists (rare in observational retail data).

### Mechanism 3
- Claim: Double Machine Learning with multiple confounders can correct the sign of estimated causal effects.
- Mechanism: DML first deconfounds the treatment by residualizing it against confounders using ML, then regresses the outcome residual on treatment residual. Including multiple confounders (F2, F4, F5) removes more spurious correlation, allowing the true causal direction to emerge.
- Core assumption: All important confounders are observed and included; unobserved confounders remain a limitation.
- Evidence anchors:
  - [abstract]: "double machine learning reveals that controlling for multiple confounders (F2, F4, F5) corrects the sign of causal effects"
  - [section 4.2, Figure 5b]: "By controlling just F5, we get the wrong sign of the causal effect (+ve slope instead of the expected -ve slope)... In the right plot, we get the correct behavior by controlling F2, F4, and F5."
  - [section 2.2]: "Double machine learning... uses the confounders to first train a model to deconfound the feature of interest, and then trains another machine learning model to estimate the average causal effect"
  - [corpus]: Weak direct support; neighbors do not address DML methodology.
- Break condition: When unobserved confounders exist (paper explicitly acknowledges this); when feature sets grow large and computational complexity becomes prohibitive.

## Foundational Learning

- **Concept: SHAP (Shapley Additive Explanations)**
  - Why needed here: Core interpretability tool across all models; must understand that SHAP quantifies contribution to prediction, not causal effect.
  - Quick check question: A feature has the highest mean absolute SHAP value—can you conclude it causes the outcome?

- **Concept: Confounding**
  - Why needed here: Central problem the paper addresses; confounders distort both predictions and explanations.
  - Quick check question: Marketing spend and inventory both affect sales and correlate with each other. If you increase marketing without adjusting inventory in your model, what might go wrong?

- **Concept: Double Machine Learning (DML)**
  - Why needed here: The proposed causal inference solution; requires understanding the two-stage residualization process.
  - Quick check question: Why does DML use two ML models instead of directly regressing outcome on treatment?

## Architecture Onboarding

- **Component map:** StandardScaler -> Train XGBoost/RF/EBR -> Compute SHAP values -> Hierarchical clustering for redundancy -> Double ML with progressive confounder inclusion
- **Critical path:** 1. Train predictive models and compute SHAP values 2. Perform redundancy clustering to identify candidate confounders 3. Run DML, incrementally adding confounders, monitor causal effect sign and magnitude changes
- **Design tradeoffs:**
  - Accuracy vs. explanation stability: XGBoost (higher accuracy) vs. EBR (lower SHAP variance)
  - Completeness vs. complexity: More confounders improve causal identification but increase computational cost
  - Observational vs. experimental: DML works for observational data but cannot handle unobserved confounding
- **Failure signatures:**
  - SHAP shows counterintuitive directions (e.g., out-of-stock items increasing sales contribution)
  - Causal effect sign reverses when confounders added—indicates initial confounding
  - High feature redundancy in clustering suggests confounding network
- **First 3 experiments:**
  1. Replicate the three-model comparison (XGBoost, RF, EBR) and quantify SHAP variance per feature across models.
  2. Run hierarchical clustering on features using SHAP-based redundancy; identify which feature groups suggest confounding.
  3. Implement DML for one treatment variable (e.g., F6), progressively adding confounders (F5 alone, then F2+F4+F5) and document sign/magnitude changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SHAP values be effectively integrated into causal inference frameworks, such as c-SHAP, to better identify causal drivers in retail?
- Basis in paper: [explicit] The authors state "Further work could involve the integration of SHAP values into causal inference, for example, c-SHAP."
- Why unresolved: The current work treats interpretability (SHAP) and causal inference (Double ML) as separate steps, noting that SHAP values often fail to capture true causal relationships due to confounding.
- What evidence would resolve it: A modified framework where feature attribution values account for causal graph structures, validated against the Double ML results in the same retail dataset.

### Open Question 2
- Question: Can stakeholder surveys successfully generate a "ground truth" dataset for feature importance to objectively benchmark different explainability techniques?
- Basis in paper: [explicit] Section 4.1 proposes surveying company leaders to vote on feature importance: "Based on their feedback, we could create a ground truth dataset... [to] be benchmarked against this ground truth dataset."
- Why unresolved: Evaluating explainability is currently subjective or relies on proxy metrics; there is no established human-labeled ground truth for feature importance in this retail context.
- What evidence would resolve it: A study demonstrating a statistically significant correlation between executive survey rankings and the feature importance outputs of a specific model class.

### Open Question 3
- Question: Can computationally efficient methods maintain accuracy for SHAP values and DAG estimation when scaling from 7 features to the full 64-feature dataset?
- Basis in paper: [explicit] The Conclusion suggests "expanding to more features while using computationally efficient methods for SHAP values and DAG estimation."
- Why unresolved: The authors reduced the dataset to 7 features to manage complexity; the exponential complexity of DAG estimation makes applying the proposed pipeline to the full 64-column dataset an open challenge.
- What evidence would resolve it: Benchmarking results showing that causal discovery and SHAP calculation complete in a reasonable time on the full dataset without unstable estimates.

## Limitations

- Proprietary retail dataset prevents external validation of SHAP variance and causal inference results
- Unobserved confounders remain a critical limitation for double machine learning
- Computational complexity of DML scales poorly with larger feature sets, limiting scalability

## Confidence

- High confidence: The core claim that SHAP values cannot establish causality due to confounding is well-supported by theoretical reasoning and literature
- Medium confidence: The finding that DML with multiple confounders corrects causal effect signs is supported by the reported results but limited to one treatment variable
- Low confidence: The claim about EBR's lower SHAP variance relative to other models is based on single-case evidence without broader validation

## Next Checks

1. Replicate the three-model comparison (XGBoost, RF, EBR) with synthetic retail data to verify SHAP variance differences across multiple runs
2. Test DML implementation on alternative datasets with known causal structures to validate the confounder correction mechanism
3. Conduct sensitivity analysis on the number of confounders included in DML to quantify the trade-off between causal identification and computational complexity