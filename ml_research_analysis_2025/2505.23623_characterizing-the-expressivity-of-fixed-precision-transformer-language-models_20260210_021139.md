---
ver: rpa2
title: Characterizing the Expressivity of Fixed-Precision Transformer Language Models
arxiv_id: '2505.23623'
source_url: https://arxiv.org/abs/2505.23623
tags:
- transformer
- attention
- language
- formula
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the expressive power of fixed-precision transformer
  language models under idealized conditions (strict future masking, soft attention,
  no positional encodings). The authors establish that such transformers are exactly
  as expressive as LTL[P], a restricted fragment of linear temporal logic containing
  only the past operator.
---

# Characterizing the Expressivity of Fixed-Precision Transformer Language Models

## Quick Facts
- **arXiv ID:** 2505.23623
- **Source URL:** https://arxiv.org/abs/2505.23623
- **Reference count:** 40
- **Primary result:** Fixed-precision transformers with soft attention, strict future masking, and no positional encodings are exactly as expressive as LTL[P] (Linear Temporal Logic with only the Past operator).

## Executive Summary
This paper establishes a precise theoretical characterization of transformer expressivity under idealized conditions. The authors prove that fixed-precision transformers with soft attention, strict future masking, and no positional encodings can only recognize languages definable in LTL[P], a restricted fragment of linear temporal logic. This limitation arises from numerical underflow in fixed-precision arithmetic, which prevents uniform attention over long sequences. The work connects this logical characterization to automata theory (PODFAs - Partially Ordered Deterministic Finite Automata) and formal language theory, providing a unified framework for understanding transformer capabilities. Empirically, transformers trained on LTL[P]-definable languages achieve perfect generalization across sequence lengths, while consistently failing on languages beyond this class.

## Method Summary
The study analyzes fixed-precision transformer language models under strict theoretical conditions: soft attention, strict future masking (no self-attention), and no positional encodings. The authors establish theoretical bounds on expressivity by proving equivalence to LTL[P] and PODFAs, then validate these claims empirically through systematic experiments. Models are trained on a hierarchy of formal languages (Counter, Regular, Star-free, and polynomial languages) with adversarial dataset construction to test generalization beyond training lengths. The experimental setup uses synthetic strings with train lengths ≤40 and test lengths 41-500, measuring perfect generalization (100% accuracy) as success criteria. The analysis compares transformer performance against LSTM baselines and systematically ablates architectural components to isolate their effects on expressivity.

## Key Results
- Fixed-precision transformers with soft attention, strict masking, and NoPE are exactly as expressive as LTL[P].
- This expressivity class corresponds precisely to languages recognized by Partially Ordered Deterministic Finite Automata (PODFAs).
- Transformers achieve perfect accuracy on all LTL[P]-definable languages but consistently fail on tasks outside this class (e.g., PARITY, LAST).
- Non-strict future masking is strictly less expressive than strict masking, failing to recognize languages like bbΣ*.
- The "Past" operator is implemented via layered detection to handle vanishing attention weights under fixed precision.

## Why This Works (Mechanism)

### Mechanism 1: Finite Attention Span via Layered Detection
Fixed-precision transformers maintain a bounded attention span (N_max) due to numerical underflow, compensating through layered detection to implement the "Past" (P) operator. Softmax outputs under fixed precision can only maintain a finite number of non-zero entries before weights underflow to zero. The architecture simulates the logical "Past" operator using a two-step patch: direct attention followed by P(ψ ∧ Pψ) to detect multiple prior instances, bypassing vanishing gradient issues for single-instance detection.

### Mechanism 2: State Simulation via Partially Ordered Automata
The restricted expressivity (LTL[P]) corresponds exactly to languages recognized by Partially Ordered Deterministic Finite Automata (PODFAs). The transformer simulates a PODFA by dedicating hidden dimensions to represent automaton states, tracking irreversible state transitions based on symbol history without requiring positional encodings or step-counters.

### Mechanism 3: Asymmetric Generalization from Strict Masking
Strict future masking enforces causal, uni-directional logic that privileges "left-deterministic" patterns over "right-deterministic" ones. This allows the model to distinguish the very first symbol from subsequent identical symbols (e.g., "Start with b" = bΣ*), but cannot look ahead to verify "Ends with b" (Σ*b) without predicting future tokens.

## Foundational Learning

- **Concept: Linear Temporal Logic (LTL) & The "Past" Operator**
  - **Why needed here:** The paper reduces transformer capability to LTL[P], where P allows queries about past history only (e.g., "Has 'a' occurred?"), excluding future or complex counting.
  - **Quick check question:** Can LTL[P] express "strings ending with 'a'"? (Answer: No, that requires future context).

- **Concept: Partially Ordered DFAs (PODFA)**
  - **Why needed here:** This is the automata-theoretic characterization of the model's capacity, implying learned automata must be acyclic in terms of state reachability.
  - **Quick check question:** Can a PODFA recognize (ab)* (alternating 'a' and 'b')? (Answer: No, it requires a loop q0→q1→q0).

- **Concept: Fixed-Precision Arithmetic Constraints**
  - **Why needed here:** Explains the "maximum attention span" - fixed precision implies a limit on how many positions can have non-zero attention weights before underflow.
  - **Quick check question:** Why does the "Past" operator construction need a "Patch" for vanishing weights? (Answer: Because with many prior matches, softmax assigns near-zero weight to all, failing to register the existence of any match).

## Architecture Onboarding

- **Component map:** Input (NoPE embeddings) -> Masked Soft-Attention layers (strict: m < n) -> Feed-Forward Networks (FFN) -> Classification/LM head
- **Critical path:** The attention mechanism's implementation of the "Past" operator (Theorem 3.3) - verify attention weights successfully discriminate "at least one past occurrence" despite potential underflow.
- **Design tradeoffs:**
  - Strict vs. Non-Strict Masking: Strict is more expressive (crucial for "First" operators) but harder to train due to lack of self-attention normalization.
  - Soft vs. Hard Attention: Soft Attention is equivalent to Average Hard Attention (AHA) but strictly less expressive than Unique Hard Attention (UHA).
- **Failure signatures:**
  - Parity/Counting failures: Tasks requiring modular counting (e.g., even length) will perform at chance (~50%) because PODFAs cannot count.
  - Dyck Language failures: Hierarchical nesting requires a stack/counter; this architecture fails even on bounded depth Dyck languages.
  - "Last" vs. "First": Model succeeds on "First symbol is b" but fails on "Last symbol is b" (unless trained as an LM to predict EOS).
- **First 3 experiments:**
  1. Train on Left-Deterministic Polynomial (e.g., FIRST(b)) vs. Right-Deterministic Polynomial (e.g., LAST(b)). Confirm 100% vs. ~50-70% accuracy gap.
  2. Increase sequence length beyond theoretical N_max for a simple "Exists" task (Σ*aΣ*) to observe if the "Patch" holds or if vanishing gradients degrade performance.
  3. Train on language bbΣ* (two leading 'b's). Confirm strict masking succeeds while non-strict masking fails to distinguish the two 'b's.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** What is the precise logical characterization of fixed-precision transformers augmented with commonly used positional encodings, such as sinusoidal or rotary encodings?
**Basis in paper:** Section H (Limitations) states that "the exact numerical predicates simulated by commonly used positional encodings remain unknown" and explicitly leaves the "precise logical characterization... for future work" in Section F.3.
**Why unresolved:** While Theorem F.5 links positional encodings to numerical predicates in general, it does not map specific implementations (e.g., RoPE) to specific logical formalisms, leaving the expressive power of standard transformer configurations undefined.
**What evidence would resolve it:** A theorem equating transformers with specific positional encodings to a defined logic fragment (e.g., FO[<] with specific arithmetic predicates), supported by formal proofs and empirical length generalization tests.

### Open Question 2
**Question:** What is the exact formal language or automata class that characterizes fixed-precision transformers with non-strict future masking?
**Basis in paper:** Section F.2 proves that non-strict masking is strictly less expressive than strict masking (LTL[P]), demonstrating an inability to recognize languages like bbΣ*, but does not provide a positive characterization for this restricted class.
**Why unresolved:** The paper establishes the upper bound (a subset of PFO2[<]) but does not identify the specific boundary or properties that defines the non-strict subclass.
**What evidence would resolve it:** A theoretical result defining necessary and sufficient conditions for a language to be recognizable under non-strict masking, analogous to the "R-trivial" characterization for strict masking.

### Open Question 3
**Question:** Does allowing intermediate computation steps (Chain of Thought) elevate the expressivity of fixed-precision transformers beyond the LTL[P] class towards Turing completeness, or does fixed precision remain a bottleneck?
**Basis in paper:** Section G (Related Work) contrasts the paper's "no intermediate steps" setup with prior work showing that arbitrary-precision transformers with Chain of Thought are Turing complete, leaving the interaction between CoT and fixed precision unexplored.
**Why unresolved:** It is unclear if limited attention span and rounding errors inherent to fixed precision constrain CoT reasoning or if sequential computation can bypass these limitations.
**What evidence would resolve it:** Theoretical analysis of fixed-precision CoT expressivity, or empirical demonstrations of fixed-precision models solving tasks outside LTL[P] (e.g., PARITY) only when CoT is enabled.

## Limitations
- Strong idealization assumptions (strict masking, NoPE, soft attention) represent a narrow subset of practical transformer deployments.
- Theoretical framework assumes perfect training convergence and infinite data, which may not reflect real-world training dynamics.
- Empirical validation tests only a specific set of formal languages and may not generalize to naturally occurring patterns.
- Fixed-precision constraint (32-bit float) is critical to theoretical results but may not reflect full complexity of real-world optimization behavior.

## Confidence

**High Confidence:** The theoretical characterization of transformer expressivity as LTL[P] under stated idealizations is rigorously proven. Equivalence between soft attention and average hard attention, and relationship to PODFAs, are mathematically established.

**Medium Confidence:** The specific mechanism of finite attention span due to fixed-precision underflow is theoretically sound but may be implementation-dependent. The claim that PODFA simulation is the primary path to LTL[P] expressivity is supported but not exhaustively proven across all configurations.

**Low Confidence:** Extrapolation of these results to standard transformer architectures with positional encodings and different attention mechanisms is speculative. Claims about practical implications for transformer design and training are suggestive rather than rigorously established.

## Next Checks

1. **Generalization to Standard Transformers:** Train transformers with learned positional encodings and rotary positional embeddings on the same formal language hierarchy. Measure whether the strict separation between LTL[P]-definable and non-definable languages persists, and quantify any performance degradation or enhancement.

2. **Precision Scaling Experiments:** Systematically vary the floating-point precision (16-bit, 32-bit, 64-bit) and measure the maximum attention span and performance on languages requiring longer-range dependencies. This would validate whether the fixed-precision constraint is indeed the limiting factor.

3. **Attention Mechanism Ablation:** Compare soft attention, hard attention variants (unique vs. average), and sparse attention mechanisms on the same task set. Quantify the exact expressivity differences and identify whether the theoretical hierarchy translates to practical performance gaps.