---
ver: rpa2
title: 'CHAOS: Chart Analysis with Outlier Samples'
arxiv_id: '2505.17235'
source_url: https://arxiv.org/abs/2505.17235
tags:
- chart
- perturbations
- visual
- robustness
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHAOS, a robustness benchmark for evaluating
  Multimodal Large Language Models (MLLMs) on chart analysis under real-world perturbations.
  CHAOS includes five textual and ten visual perturbation types at three severity
  levels, based on human evaluation with 42 participants.
---

# CHAOS: Chart Analysis with Outlier Samples
## Quick Facts
- arXiv ID: 2505.17235
- Source URL: https://arxiv.org/abs/2505.17235
- Reference count: 40
- Primary result: Benchmark reveals chart-specific MLLMs degrade more severely under perturbations than general models despite better clean performance

## Executive Summary
CHAOS is a robustness benchmark for evaluating Multimodal Large Language Models on chart analysis under real-world perturbations. It introduces 15 perturbation types (10 visual, 5 textual) at 3 severity levels calibrated through human evaluation. Experiments with 13 state-of-the-art MLLMs show that even minor perturbations cause significant performance drops (4-50%), with chart-specific models exhibiting a robustness-specialization trade-off. The proposed robustness metric R fairly compares models by accounting for both baseline performance and degradation magnitude.

## Method Summary
The CHAOS benchmark applies calibrated visual and textual perturbations to chart images and questions at three severity levels (easy, mid, hard) determined by a 42-participant human study. It evaluates 13 MLLMs across ChartQA (Relaxed Accuracy with 5% tolerance) and Chart-to-Text (BLEU-4, Content Selection) tasks. The robustness metric R combines relative and absolute performance degradation to fairly assess model resilience. The benchmark is inference-only, testing models on perturbed versions of ChartQA (2.5K QA pairs) and Chart-to-Text (6.61K images) datasets.

## Key Results
- Chart-specific models achieve highest clean accuracy but degrade more severely (23-50%) than general models under visual perturbations
- Even minor perturbations cause significant performance drops across all model categories (4-50%)
- Text perturbations cause up to 31% performance drop even with clean images, indicating fragile text decoders
- The robustness metric R effectively captures trade-offs between clean performance and perturbation resilience

## Why This Works (Mechanism)

### Mechanism 1
- Applying calibrated visual and textual perturbations at severity levels grounded in human perception systematically reveals robustness gaps in MLLMs that clean benchmarks miss
- The benchmark introduces 10 visual perturbation types and 5 textual perturbation types, each at 3 severity levels determined by a 42-participant human study
- Human perceptual thresholds provide meaningful calibration for defining perturbation severity levels that reflect real-world challenges

### Mechanism 2
- The proposed robustness metric R fairly compares models by accounting for both baseline performance and degradation magnitude
- The metric combines relative degradation and absolute degradation, normalized across perturbation levels
- This prevents penalizing models with low baseline performance equally to high-performing models with the same absolute drop

### Mechanism 3
- Chart-specialized models exhibit a robustness-specialization trade-off, outperforming on clean data but degrading more severely under perturbations than general or document-focused MLLMs
- Fine-tuning on synthetic chart data optimizes for in-domain chart patterns but weakens vision encoder robustness
- General models benefit from broader pretraining exposure to diverse noise patterns

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs) for chart understanding
  - Why needed here: CHAOS evaluates 13 MLLMs across three categories; understanding their architectural differences is essential for interpreting robustness findings
  - Quick check question: Can you explain how chart-specific MLLMs differ from general MLLMs in their training data and architecture?

- Concept: Robustness benchmarking and perturbation taxonomies
  - Why needed here: The paper's contribution is a systematic perturbation framework; understanding how benchmarks isolate failure modes is prerequisite
  - Quick check question: What is the difference between an adversarial attack and a natural perturbation in robustness evaluation?

- Concept: Evaluation metrics for chart QA and summarization
  - Why needed here: The paper uses Relaxed Accuracy and BLEU-4/Content Selection; understanding these metrics is necessary to interpret degradation results
  - Quick check question: Why might Relaxed Accuracy be problematic for year-based numerical answers?

## Architecture Onboarding

- Component map: Perturbation Engine -> Human Calibration Module -> Evaluation Pipeline -> Model Zoo
- Critical path: Select chart samples -> Apply perturbations at all 3 severity levels -> Run inference on all 13 models -> Compute clean and perturbed metrics -> Calculate R score
- Design tradeoffs: Synthetic vs. real perturbations (controlled but potentially less representative), severity calibration (human study with 42 participants may have inter-rater variability), metric design (R penalizes high clean-accuracy models more)
- Failure signatures: Chart-specific models show 23-50% average degradation on visual perturbations, text perturbations cause up to 31% drop even with clean images, models generate plausible but ungrounded answers on blank inputs
- First 3 experiments: 1) Baseline robustness profiling to establish R scores, 2) Perturbation-type ablation to isolate largest degradation causes, 3) Data augmentation intervention to measure R improvement vs. clean-only baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MLLMs be trained to mimic human adaptive strategies, such as focusing on chart elements less affected by noise, to improve resilience against severe perturbations?
- Basis in paper: Section 3.2 explicitly asks whether MLLMs can learn and incorporate these adaptive human strategies to improve their robustness against severe perturbations
- Why unresolved: The paper demonstrates current models suffer significant performance drops under perturbations where humans succeed using adaptive strategies, but does not propose or test methods to transfer these strategies to models
- What evidence would resolve it: A study showing models trained with specific attention mechanisms or data augmentation targeting noisy regions maintain higher accuracy on the "Hard" level of the CHAOS benchmark

### Open Question 2
- Question: What specific training strategies can resolve the "robustness trade-off" where chart-specific models degrade more severely than general models?
- Basis in paper: Finding 2 identifies chart-specific models have lower average robustness than general models, and Finding 4 suggests training data and fine-tuning strategy need attention
- Why unresolved: The paper quantifies the trade-off but primarily attributes it to synthetic training data without validating specific interventions
- What evidence would resolve it: Experiments demonstrating a specific fine-tuning regime results in a model that maintains high clean accuracy while achieving an R-score comparable to general-purpose models

### Open Question 3
- Question: To what extent does the reliance on synthetic training data cause chart-specific MLLMs to hallucinate or fail under real-world domain shifts?
- Basis in paper: Section 4.3 and 4.4 hypothesize that out-of-domain degradation is due to chart-related models relying on synthetic data that overlooks interpretative techniques
- Why unresolved: The paper observes higher hallucination rates in fine-tuned chart models but does not isolate synthetic data as the causal factor versus other architectural differences
- What evidence would resolve it: An ablation study comparing robustness and hallucination rates of identical architectures trained exclusively on synthetic charts versus real-world scanned charts

## Limitations

- The calibration of perturbation severity levels relies on a relatively small human study (42 participants), which may not capture the full distribution of real-world chart degradation patterns
- Perturbation types are algorithmically generated rather than derived from actual captured charts, potentially underestimating real-world noise complexity
- The robustness metric R uses fixed weights that may not reflect all deployment scenarios where baseline performance matters differently

## Confidence

- High confidence: Systematic perturbation framework, experimental setup with 13 MLLMs, core finding that chart-specific models degrade more severely under perturbations
- Medium confidence: Specific degradation percentages and exact model ranking, as these depend on particular perturbations and evaluation protocols
- Medium confidence: Interpretation that specialization-robustness trade-off stems from synthetic training data limitations, as alternative explanations are not fully explored

## Next Checks

1. **Human-Real World Correlation Test**: Collect a dataset of real-world chart images with natural degradation and compare model performance on these vs. algorithmically perturbed versions to validate the perturbation framework's representativeness

2. **Training Data Augmentation Study**: Fine-tune a chart-specific model with CHAOS perturbations during training and measure whether this closes the robustness gap with general models without sacrificing clean performance

3. **Metric Sensitivity Analysis**: Vary the relative vs. absolute degradation weighting in the R metric across different deployment contexts to determine if the current fixed weighting appropriately captures robustness priorities