---
ver: rpa2
title: Off-Policy Evaluation and Counterfactual Methods in Dynamic Auction Environments
arxiv_id: '2501.05278'
source_url: https://arxiv.org/abs/2501.05278
tags:
- policy
- evaluation
- policies
- continuous
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the application of Off-Policy Evaluation (OPE)
  methods in dynamic auction environments, where rapid decision-making is critical
  for competitive advantage. The study addresses the challenge of evaluating new payment
  policies without costly A/B testing by using counterfactual estimators based on
  logged data.
---

# Off-Policy Evaluation and Counterfactual Methods in Dynamic Auction Environments

## Quick Facts
- arXiv ID: 2501.05278
- Source URL: https://arxiv.org/abs/2501.05278
- Reference count: 27
- Primary result: Continuous OPE methods reduce MAPE by ~20% vs discretized approaches; SNDR performs best for policy evaluation

## Executive Summary
This work addresses the challenge of evaluating payment policies in dynamic auction environments without costly A/B testing by leveraging logged data and counterfactual estimation. The study introduces continuous OPE estimators using Kernel Density Estimation (KDE) to model smooth relationships between payment values and outcomes, achieving approximately 20% reduction in Mean Absolute Percentage Error compared to discretized approaches. The research demonstrates that Self-Normalized Doubly Robust (SNDR) estimators outperform other continuous evaluators and that optimal policies can be learned through gradient descent optimization of differentiable OPE estimators, enabling efficient policy comparison and learning without actual experimentation.

## Method Summary
The method involves learning proxy policies using Random Forest models from logged A/B test data, then applying various OPE techniques including IPW, SNIPW, DM, DR, and SNDR. Continuous OPE uses KDE to model conditional probability densities of payment actions given context, avoiding discretization errors. For policy learning, an MLP (OptPaL) maps context to payment values and is trained via gradient descent on differentiable OPE estimators. The approach was validated using three A/B tests (X vs Y, X vs Z, Y vs Z) with metrics including Reach, Resources, Returns, and Cost, showing OPE accurately predicts directional performance lifts and enables policy optimization.

## Key Results
- Continuous OPE estimators achieved ~20% reduction in MAPE compared to discretized approaches
- Among continuous evaluators, SNDR performed best across all metrics (Cost, Reach, Resources, Returns)
- OPE accurately predicted directional lifts in performance metrics without actual A/B testing
- Gradient descent optimization of continuous evaluators successfully learned optimal policies with improved resource allocation and returns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous OPE estimators outperform discretized approaches in dynamic auction environments, achieving approximately 20% reduction in MAPE.
- **Mechanism:** Kernel Density Estimation (KDE) models the conditional probability density of continuous treatment variables (e.g., payment amounts) with respect to context. This preserves fine-grained information that discretization/binning destroys, enabling more accurate importance weight calculation for counterfactual evaluation.
- **Core assumption:** The underlying relationship between payment values and outcomes is smooth and continuous; sufficient logged data exists to estimate density reliably.
- **Evidence anchors:**
  - [abstract]: "Continuous OPE methods were found to outperform discretized approaches, achieving approximately 20% reduction in Mean Absolute Percentage Error (MAPE)."
  - [section III-A, p.4]: "Continuous OPE tends to outperform discretized OPE... continuous OPE can capture the smooth, nuanced dependencies of the key metrics across small changes in payment values."
  - [corpus]: Related work on continuous treatments (Kallus & Zhou 2018, cited as [13]) provides theoretical foundation; corpus shows moderate support (avg FMR=0.44) for continuous treatment evaluation approaches.
- **Break condition:** Fails when action space is genuinely discrete or when bandwidth selection is poor (high bias if too large, high variance if too small).

### Mechanism 2
- **Claim:** Self-Normalized Doubly Robust (SNDR) estimator performs best among continuous evaluators for policy evaluation.
- **Mechanism:** SNDR combines three components: (1) a reward model that estimates expected outcomes, (2) importance weighting that adjusts for policy distribution mismatch, and (3) self-normalization that divides by the average importance weight to stabilize estimates. Double robustness means the estimator remains consistent if either the reward model OR the importance weights are accurate.
- **Core assumption:** Either the reward model is well-specified OR the propensity (behavior policy) is known/estimated accurately; logged data has sufficient coverage of evaluation policy actions.
- **Evidence anchors:**
  - [abstract]: "Among continuous evaluators, SNDR performed best."
  - [section III-B, p.4-5]: Figure 5 shows SNDR outperforming IPW, SNIPW, and DR across Cost, Reach, Resources, and Returns metrics.
  - [corpus]: Related papers confirm DR-family estimators are foundational but don't specifically validate SNDR superiority in auctions (weak direct corpus support for this specific finding).
- **Break condition:** Fails when both the reward model is misspecified AND the behavior policy estimation is poor; can also fail with extreme importance weights even with self-normalization.

### Mechanism 3
- **Claim:** Optimal payment policies can be learned through gradient descent optimization of differentiable continuous OPE estimators.
- **Mechanism:** Continuous OPE estimators are differentiable with respect to policy parameters. An MLP (OptPaL) maps context → payment, and gradients flow through the OPE estimator to update weights, minimizing a loss function (e.g., negative profit = Returns - Cost). This enables counterfactual policy learning without online experimentation.
- **Core assumption:** The OPE estimator accurately reflects true counterfactual outcomes; the reward landscape is amenable to gradient-based optimization (not highly non-convex); sufficient data coverage exists for the learned policy region.
- **Evidence anchors:**
  - [abstract]: "optimal policies can be learned through gradient descent optimization of continuous evaluators, resulting in improved resource allocation and returns while maintaining comparable costs."
  - [section II-D, p.4]: "A key advantage of continuous off-policy evaluation is its differentiability, which facilitates the application of gradient descent methods."
  - [section III-D, p.6]: Figure 8 shows Policy W (learned via OptPaL) achieves improved Resources and Returns with comparable Reach and Cost to baseline Policy X.
  - [corpus]: Corpus provides strong support for off-policy learning (5+ papers on OPE/L), though gradient-based optimization via differentiable OPE is less commonly discussed.
- **Break condition:** Fails when learned policy extrapolates beyond logged data support (distributional shift); gradient updates may be unreliable if OPE estimates have high variance.

## Foundational Learning

- **Concept: Inverse Probability Weighting (IPW)**
  - **Why needed here:** IPW is the foundational OPE technique that all other estimators build upon. It re-weights observed rewards by the importance weight ratio (πe/πb) to correct for distribution shift between behavior and evaluation policies.
  - **Quick check question:** Can you explain why IPW is unbiased but high-variance when the evaluation policy differs significantly from the behavior policy?

- **Concept: Doubly Robust Estimation**
  - **Why needed here:** DR combines IPW with a reward model, providing consistency if either component is accurate. Understanding the bias-variance tradeoff between DM (low variance, potentially biased) and IPW (unbiased, high variance) is essential for selecting estimators.
  - **Quick check question:** If your reward model is poorly specified but your propensity estimates are accurate, which estimator (DM, IPW, or DR) would you trust?

- **Concept: Kernel Density Estimation for Continuous Treatments**
  - **Why needed here:** Continuous OPE requires estimating probability densities over continuous action spaces. KDE with appropriate bandwidth/kernel selection enables computing importance weights without discretization.
  - **Quick check question:** What happens to bias and variance if you set the KDE bandwidth too large vs. too small?

## Architecture Onboarding

- **Component map:** Logged Data → Policy Proxy Learning (Random Forest) → OPE Evaluator (SNDR/KDE) → Policy Comparison → OptPaL (MLP) → Gradient Descent → Learned Optimal Policy
- **Critical path:**
  1. Collect logged bandit data (context, action, reward) from prior A/B tests
  2. Train proxy policies using Random Forest to learn π: C → A mappings
  3. Train reward model q(x, a) if using DM/DR/SNDR
  4. Tune KDE hyperparameters (kernel, bandwidth) per metric using Optuna
  5. Run OPE to estimate counterfactual policy performance
  6. (Optional) Train OptPaL via gradient descent for policy learning
- **Design tradeoffs:**
  - **Discrete vs. Continuous:** Discrete is simpler but loses ~20% accuracy; continuous requires KDE tuning but captures nuanced payment-outcome relationships.
  - **Estimator selection:** IPW/SNIPW don't need reward models but have higher variance; DM is stable but biased if reward model is wrong; DR/SNDR offer robustness at cost of complexity.
  - **Proxy vs. Direct Policy:** Paper uses learned proxies (RF) rather than original policies for generalizability across test scenarios—adds abstraction layer but may introduce approximation error.
- **Failure signatures:**
  - High MAPE despite continuous OPE → Check bandwidth tuning; may be overfitting to noise
  - OPE predicts positive lift but A/B test shows negative → Distribution shift; evaluation policy actions not well-covered by logged data
  - SNDR performs worse than IPW → Reward model is misspecified; check reward model validation
  - OptPaL converges to extreme payment values → Loss function not properly constrained; add regularization
- **First 3 experiments:**
  1. **Baseline validation:** Reproduce the discretized vs. continuous comparison (Figure 4) on your logged data to validate continuous OPE improvement before adopting.
  2. **Estimator ablation:** Compare SNDR against IPW, DM, and DR on held-out policy pairs to verify SNDR superiority in your specific auction environment.
  3. **Counterfactual sanity check:** Run OPE to predict a policy you've already A/B tested; compare predicted vs. actual lifts to calibrate OPE accuracy before trusting it for new policies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Structural Causal Model-Based Counterfactual Evaluation improve the accuracy of policy assessments in dynamic auctions compared to the current SNDR estimators?
- Basis in paper: [explicit] Section V states the next research direction will focus on "exploring Structural Causal Model-Based Counterfactual Evaluation to improve the accuracy of policy assessments."
- Why unresolved: The current work relies on non-causal OPE estimators (e.g., SNDR, IPW); the integration of causal structural models is proposed but not yet implemented or tested.
- What evidence would resolve it: Comparative experiments showing that SCM-based methods yield lower error rates (MAPE) or higher correlation with ground-truth lifts than the continuous SNDR method.

### Open Question 2
- Question: How do continuous OPE methods perform when applied to large dimensional action spaces in complex auction environments?
- Basis in paper: [explicit] Section V notes the intent to "consider large dimensional action spaces to better account for complex auction environments."
- Why unresolved: The current study limits the action space to continuous payment policies (likely low-dimensional), and it is unclear if the 20% error reduction observed in continuous evaluators holds as dimensionality increases.
- What evidence would resolve it: Evaluation of the continuous OPE pipeline on datasets with high-dimensional action vectors, demonstrating maintained estimator stability and accuracy.

### Open Question 3
- Question: How can proxy policy learning be enhanced to minimize the approximation error inherent in mapping context to action spaces?
- Basis in paper: [explicit] Section V lists "enhancing proxy policy learning" as a primary focus for future research.
- Why unresolved: The current approach uses Random Forest models as proxies; while functional, the paper implies that better approximations of the behavioral policy are necessary to improve overall OPE reliability.
- What evidence would resolve it: Demonstrating that advanced proxy models (e.g., deep generative models) reduce the divergence between logged actions and proxy predictions, resulting in more precise counterfactual estimates.

## Limitations

- KDE performance depends heavily on bandwidth tuning and may fail with sparse data in high-dimensional action spaces
- SNDR superiority assumes neither the reward model nor the propensity model is severely misspecified
- Gradient-based policy learning requires smooth reward landscapes and sufficient coverage of evaluation policy actions in logged data

## Confidence

- **High confidence:** The fundamental OPE methodology (IPW, DR frameworks are well-established)
- **Medium confidence:** The specific continuous KDE implementation and SNDR performance claims (requires proper tuning and sufficient data density)
- **Low confidence:** The policy learning results without seeing actual reward landscapes and validation of distributional assumptions

## Next Checks

1. **Coverage analysis:** Quantify the proportion of evaluation policy actions that fall within the support of logged data; identify regions where extrapolation occurs.
2. **Robustness stress test:** Systematically degrade reward model accuracy and behavior policy estimates to measure SNDR performance degradation relative to IPW and DM.
3. **Ablation on bandwidth tuning:** Compare OPE performance across a grid of KDE bandwidths to establish sensitivity and optimal tuning range.