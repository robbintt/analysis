---
ver: rpa2
title: 'Death by a Thousand Prompts: Open Model Vulnerability Analysis'
arxiv_id: '2511.03247'
source_url: https://arxiv.org/abs/2511.03247
tags:
- security
- multi-turn
- open-weight
- safety
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated eight open-weight large language
  models against single-turn and multi-turn adversarial attacks, revealing that multi-turn
  jailbreak techniques are 2x to 10x more effective than single-turn attacks, with
  success rates reaching 92.78%. Models with capability-first development (e.g., Llama,
  Qwen) showed the largest multi-turn gaps, while safety-focused designs (e.g., Gemma)
  exhibited more balanced performance.
---

# Death by a Thousand Prompts: Open Model Vulnerability Analysis

## Quick Facts
- arXiv ID: 2511.03247
- Source URL: https://arxiv.org/abs/2511.03247
- Reference count: 13
- Key outcome: Multi-turn jailbreak techniques are 2x to 10x more effective than single-turn attacks, with success rates reaching 92.78% across eight open-weight models

## Executive Summary
This study systematically evaluated eight open-weight large language models against single-turn and multi-turn adversarial attacks, revealing that multi-turn jailbreak techniques are 2x to 10x more effective than single-turn attacks, with success rates reaching 92.78%. Models with capability-first development (e.g., Llama, Qwen) showed the largest multi-turn gaps, while safety-focused designs (e.g., Gemma) exhibited more balanced performance. These findings highlight systemic vulnerabilities in current open-weight models' ability to maintain safety guardrails across extended interactions, underscoring the need for layered security controls and security-first design in deployment.

## Method Summary
The researchers conducted systematic adversarial testing of eight open-weight LLMs using a combination of single-turn and multi-turn jailbreak prompts. They constructed comprehensive attack datasets with 1,960 single-turn and 7,840 multi-turn adversarial prompts, evaluating models across various capability categories. The evaluation framework measured success rates for both attack types while analyzing the effectiveness gap between them. Models were selected from major open-weight families including Llama, Qwen, Gemma, Mistral, and DeepSeek, representing different development priorities from capability-first to safety-focused approaches.

## Key Results
- Multi-turn jailbreak techniques achieved 2x to 10x higher success rates than single-turn attacks across all tested models
- Success rates reached 92.78% for multi-turn attacks, compared to much lower single-turn rates
- Capability-first models (Llama, Qwen) showed the largest vulnerability gaps between single-turn and multi-turn attacks
- Safety-focused models (Gemma) demonstrated more balanced performance with smaller effectiveness gaps

## Why This Works (Mechanism)
Multi-turn attacks exploit the conversational context and cumulative persuasion that single-turn prompts cannot achieve. The extended interaction allows attackers to build rapport, gradually bypass initial safety filters, and leverage the model's own generated content to reinforce malicious requests. This works because current safety fine-tuning appears insufficient to maintain guardrails across extended conversations, particularly when the conversation starts benignly and escalates gradually.

## Foundational Learning
- **Adversarial Prompt Engineering**: Understanding how carefully crafted prompts can bypass safety mechanisms
  - Why needed: Essential for evaluating and improving model robustness against real-world attacks
  - Quick check: Can construct prompts that systematically probe model boundaries

- **Jailbreak Techniques**: Methods for circumventing content moderation and safety filters
  - Why needed: Core attack vector being studied, representing the primary threat model
  - Quick check: Understand difference between single-turn vs multi-turn attack strategies

- **Safety Fine-tuning**: Post-training processes to align models with ethical guidelines
  - Why needed: Target of adversarial attacks, key to understanding model vulnerabilities
  - Quick check: Can identify which safety mechanisms fail first under extended attacks

## Architecture Onboarding
- **Component Map**: User Input -> Safety Filter -> Base Model -> Output
- **Critical Path**: Adversarial prompt → safety mechanism bypass → harmful content generation
- **Design Tradeoffs**: Capability vs safety optimization, single-turn vs multi-turn robustness
- **Failure Signatures**: Gradual degradation of safety filters across conversation turns
- **First Experiments**: 1) Replicate single vs multi-turn gap on additional models, 2) Test safety filter ablation effects, 3) Measure context retention across attack sequences

## Open Questions the Paper Calls Out
None

## Limitations
- Study evaluated only eight models from a narrow set of developers, limiting generalizability
- Adversarial prompts may not represent the full space of potential attack vectors
- Unclear whether lab-measured vulnerabilities translate to practical security risks in deployed systems

## Confidence
- High: Multi-turn jailbreak techniques consistently outperform single-turn attacks across all tested models
- Medium: Characterization of model-specific vulnerabilities given limited sample size
- Low: Proposed explanations for why capability-first vs safety-focused models show different vulnerability patterns

## Next Checks
1. Replicate the attack framework across additional model families and scales (including frontier models) to test whether the 2x-10x effectiveness gap persists
2. Conduct ablation studies removing or modifying specific safety fine-tuning layers to isolate which components fail first in multi-turn contexts
3. Test the adversarial prompts against real-world deployed systems to validate whether lab-measured vulnerabilities translate to practical security risks