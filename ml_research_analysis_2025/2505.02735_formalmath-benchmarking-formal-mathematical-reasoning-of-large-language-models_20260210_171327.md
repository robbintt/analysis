---
ver: rpa2
title: 'FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models'
arxiv_id: '2505.02735'
source_url: https://arxiv.org/abs/2505.02735
tags:
- theorem
- proof
- lean4
- arxiv
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FormalMATH is a large-scale Lean4 benchmark with 5,560 formally
  verified mathematical problems across diverse domains (algebra, calculus, number
  theory, etc.), spanning from high-school Olympiad to undergraduate levels. A human-in-the-loop
  autoformalization pipeline was developed, integrating specialized LLMs for initial
  formalization, multi-LLM semantic verification, and negation-based disproof filtering,
  achieving 72.09% statement retention before manual review.
---

# FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models

## Quick Facts
- **arXiv ID:** 2505.02735
- **Source URL:** https://arxiv.org/abs/2505.02735
- **Reference count:** 40
- **Primary result:** 5,560 Lean4 formal problems, 72.09% statement retention, best prover (Kimina-Prover) achieves 16.46% success rate

## Executive Summary
FormalMATH is a large-scale benchmark for formal mathematical reasoning using the Lean4 proof assistant, containing 5,560 formally verified problems spanning algebra, calculus, number theory, and more from high-school Olympiad to undergraduate levels. The benchmark was constructed using a human-in-the-loop autoformalization pipeline that achieved 72.09% statement retention before manual review. Evaluation of state-of-the-art LLM-based theorem provers revealed limited performance, with the best model (Kimina-Prover) achieving only 16.46% success under practical sampling budgets, highlighting the challenging nature of formal mathematical reasoning and exposing significant domain biases in current provers.

## Method Summary
The benchmark construction employed a multi-stage human-in-the-loop autoformalization pipeline. First, supervised fine-tuning of autoformalizer models (based on Qwen2.5-7B-Coder and Deepseek-prover-base) was performed. Then, ensemble-based autoformalization used Lean4 compiler validation to filter syntactically correct statements. Multi-LLM semantic verification with models like o1-mini and claude-3.5-Sonnet ensured semantic equivalence to source problems. Negation-based disproof filtering removed invalid conjectures, followed by final manual review by experts. Provers were evaluated using sampling strategies (single-pass with Pass@K or breadth-first search with Pass@1×K×M) on both the full dataset and a smaller FormalMATH-Lite subset.

## Key Results
- **Dataset size and quality:** 5,560 Lean4 problems with 72.09% statement retention after autoformalization and manual review
- **Prover performance:** Best model (Kimina-Prover) achieves only 16.46% success rate under Pass@32 budget
- **Domain bias:** Provers perform significantly better on algebra problems than on calculus, discrete math, or number theory

## Why This Works (Mechanism)
The pipeline works by combining automated formal verification (Lean4 compiler) with semantic validation and expert oversight to ensure both syntactic correctness and mathematical fidelity of formalizations. The multi-LLM verification stage helps catch semantic errors that syntax checking alone would miss, while the negation-based filtering removes false conjectures before they reach human reviewers.

## Foundational Learning
- **Lean4 proof assistant:** Interactive theorem prover for formal mathematics - needed for ensuring mathematical correctness and enabling automated verification; quick check: verify problems compile in Lean4
- **Autoformalization:** Process of converting natural language math problems to formal statements - needed to scale benchmark creation; quick check: check statement retention rate
- **Semantic verification:** Using LLMs to verify that formal statements match original problem meaning - needed to catch logical errors beyond syntax; quick check: compare formal statement to original problem
- **Sampling strategies (Pass@K, BFS):** Methods for theorem prover evaluation - needed to measure prover effectiveness under computational constraints; quick check: track success rate at different K values
- **Domain bias:** Performance variation across mathematical fields - needed to understand prover limitations and training data effects; quick check: analyze accuracy by domain

## Architecture Onboarding

**Component Map:** Natural Language Problems → Autoformalizer → Lean4 Compiler → Multi-LLM Verifier → Disproof Filter → Manual Review → FormalMATH Dataset

**Critical Path:** Problem → Autoformalization → Lean4 Validation → Semantic Verification → Disproof Filtering → Expert Review

**Design Tradeoffs:** The pipeline trades speed for accuracy by using multiple verification stages (compiler, LLM, manual) to ensure high-quality formalizations, accepting the computational and human resource costs.

**Failure Signatures:** Low statement retention suggests problems are difficult to formalize; prover failures dominated by over-reliance on automation tactics and domain-specific weaknesses; semantic verification failures indicate problems with mathematical understanding.

**First Experiments:**
1. Run baseline provers (Kimina-Prover, BFS-Prover) on FormalMATH-Lite subset with Pass@32 budget
2. Analyze prover performance by mathematical domain to confirm domain bias
3. Inspect failed proof attempts to identify patterns of automation tactic over-reliance

## Open Questions the Paper Calls Out
None

## Limitations
- **Incomplete pipeline details:** Training data and configuration for autoformalization models not fully disclosed
- **Proprietary API dependence:** Evaluation relies on unspecified parameters for multi-LLM semantic verification
- **Domain representation bias:** Benchmark shows strong skew toward algebra problems, potentially limiting generalizability
- **Evaluation protocol sensitivity:** Performance results may be sensitive to specific sampling budgets and search strategies used

## Confidence

**High Confidence:** Core claims about dataset size, construction pipeline, and statement retention rate are well-supported

**Medium Confidence:** Prover performance results are credible but dependent on specific evaluation protocols that may not be optimal

**Low Confidence:** Causal claims about failure modes (automation over-reliance, domain bias causes) lack conclusive experimental evidence

## Next Checks

1. Reproduce autoformalization pipeline to verify the 72.09% statement retention rate using provided scripts
2. Conduct detailed error analysis of failed proofs to confirm prevalence of automation tactic over-reliance
3. Evaluate prover performance using varied sampling budgets and search strategies to test robustness of reported success rates