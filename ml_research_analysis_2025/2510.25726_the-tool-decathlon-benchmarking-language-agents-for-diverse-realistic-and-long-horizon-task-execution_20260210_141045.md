---
ver: rpa2
title: 'The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and
  Long-Horizon Task Execution'
arxiv_id: '2510.25726'
source_url: https://arxiv.org/abs/2510.25726
tags:
- evaluation
- task
- tasks
- workspace
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Tool Decathlon (Toolathlon), a comprehensive
  benchmark designed to evaluate language agents on diverse, realistic, and long-horizon
  tasks. The benchmark includes 108 tasks spanning 32 applications and 604 tools,
  covering domains such as daily affairs, education, technology, finance, and e-commerce.
---

# The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution

## Quick Facts
- arXiv ID: 2510.25726
- Source URL: https://arxiv.org/abs/2510.25726
- Reference count: 40
- Top model achieves 38.6% success rate on benchmark

## Executive Summary
The Tool Decathlon (Toolathlon) introduces a comprehensive benchmark to evaluate language agents on realistic, multi-step, cross-application tasks. It includes 108 tasks across 32 applications using 604 tools, with scenarios grounded in real software usage and fuzzy user instructions. The benchmark employs execution-based verification rather than LLM-as-judge to ensure reliable assessment. Comprehensive evaluation reveals that even state-of-the-art models like Claude-4.5-Sonnet struggle with complex workflows, highlighting significant gaps in current agent capabilities.

## Method Summary
The benchmark evaluates language agents on 108 multi-step, cross-application tasks across 32 MCP servers and 604 tools spanning research, campus, finance, tech, business, daily, and e-commerce domains. Each task includes a fuzzy prompt, optional initial state setup (67% of tasks), workspace directory, and preconfigured MCP tools. Agents use OpenAI Agents SDK v0.0.15 with max 100 turns per task and 3 runs per model. Evaluation employs deterministic Python scripts comparing final environment states to ground truth, with containerized execution ensuring isolated states. Success metrics include Pass@1 rate, Pass@3, Pass^3, and average turns.

## Key Results
- Claude-4.5-Sonnet achieves 38.6% success rate, DeepSeek-V3.2-Exp achieves 20.1%
- Models struggle with long-horizon tasks and overlong tool outputs
- Performance varies significantly across domains and task types
- Current state-of-the-art models still fail to complete many complex multi-step workflows

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Realistic environment initialization imposes a "context tax" that filters out agents reliant on empty-slate reasoning.
- **Mechanism:** By pre-loading environments with complex states (e.g., an inbox with existing threads, a codebase with existing files), the benchmark forces the agent to perceive, filter, and ground its actions in existing context rather than executing a sterile plan.
- **Core assumption:** Agent performance degrades when it must distinguish signal from noise in unstructured environment states.
- **Evidence anchors:**
  - [abstract] "provide realistic initial environment states from real software"
  - [section 2.3] "Tasks are rarely executed from an empty environment state... operating on a folder with only one file is easier than working with 10 mixed useful and unrelated files."
  - [corpus] *SynthTools* (neighbor) emphasizes "diverse and realistic tool-use environments," supporting the need for non-synthetic setups.
- **Break condition:** If agents develop robust "scanning" heuristics to summarize environment state efficiently before planning.

### Mechanism 2
- **Claim:** Fuzzy task instructions shift the evaluation bottleneck from "instruction following" to "intent inference and exploration."
- **Mechanism:** Instead of providing explicit steps (which allows the model to act as a mere executor), the benchmark provides vague user intents. Success requires the agent to formulate a plan, interact with tools to discover constraints (e.g., checking a spreadsheet for required formats), and adapt.
- **Core assumption:** The ability to resolve ambiguity via tool interaction is a better proxy for autonomy than adherence to detailed prompts.
- **Evidence anchors:**
  - [section 3.1] "Task prompts... are crafted to mirror authentic user queries, which are often concise and fuzzy."
  - [figure 3] Contrasts Toolathlon's vague "update candidate information" with MCPMark's explicit step-by-step instructions.
- **Break condition:** If models over-rely on hallucinated assumptions rather than using tools to query for constraints (e.g., not checking the file format before writing).

### Mechanism 3
- **Claim:** Long-horizon execution with execution-based evaluation exposes "laziness" and recovery fragility.
- **Mechanism:** Tasks average ~20 turns. The evaluation script validates the *final state*, not the reasoning chain. This catches agents that terminate early ("claim done" prematurely) or fail to recover from tool errors, which is a common failure mode in long trajectories.
- **Core assumption:** Agent utility is determined by reliable task completion, not just partial progress or reasoning quality.
- **Evidence anchors:**
  - [section 5.2] "Laziness in long horizon working... models often terminate prematurely."
  - [section 2.4] "Verifying the final environment states using deterministic rules offers a far more reliable... framework."
  - [corpus] *UltraHorizon* (neighbor) highlights challenges in "long-horizon and partially observable" tasks, consistent with this mechanism.
- **Break condition:** If agents develop better internal "progress estimators" or "verifiers" to ensure all sub-tasks are complete before termination.

## Foundational Learning

- **Concept:** **Model Context Protocol (MCP)**
  - **Why needed here:** This is the standardized interface layer used to connect the agent to the 32 diverse applications. Understanding MCP is necessary to interpret how tools are defined and invoked (e.g., distinguishing between a "resource" and a "tool").
  - **Quick check question:** Can you distinguish between a standard REST API call and an MCP tool invocation in an agent's trajectory log?

- **Concept:** **Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** The paper formally defines tasks as POMDPs $(S, A, O, T, R, U)$. You must grasp that the agent only sees $O$ (observations/tool outputs), not the true state $S$, necessitating memory and state-tracking.
  - **Quick check question:** In the context of the Toolathlon, what constitutes the "Observation" ($O$) after a tool call?

- **Concept:** **State-Based vs. LLM-Based Evaluation**
  - **Why needed here:** The benchmark rejects LLM-as-judge in favor of deterministic scripts. This is critical for understanding why a trajectory "looks right" but fails (e.g., sending the correct email text but to the wrong recipient address).
  - **Quick check question:** Why would an LLM judge potentially give a "pass" to an agent that correctly drafts an email but fails to actually send it via the email server?

## Architecture Onboarding

- **Component map:** Environment (Docker containers with 32 App instances) -> Interface (604 MCP Tools) -> Agent Loop (OpenAI Agents SDK) -> Evaluator (Deterministic Python scripts)

- **Critical path:**
  1. **State Init:** Run setup script to populate environment (e.g., seed Canvas course).
  2. **Interaction:** Agent receives fuzzy prompt -> calls MCP tool -> receives observation (potentially truncated).
  3. **Verification:** Upon "Claim Done," eval script checks environment (e.g., database state) against ground truth.

- **Design tradeoffs:**
  - **Containerization:** Increases setup complexity but ensures safe, parallel evaluation (isolated states).
  - **Real vs. Simulated Apps:** The authors use real APIs where possible but simulate backends (e.g., Poste.io instead of Gmail) to allow state resets.
  - **Truncation:** Long tool outputs are truncated/paged (100K chars) to prevent context overflow, requiring agents to actively navigate large outputs.

- **Failure signatures:**
  - **Premature Termination:** Agent calls `claim done` after completing only one year of a 10-year analysis (Section 5.2).
  - **Tool Hallucination:** Agent attempts to call tools that do not exist in the provided MCP server (Section 5.1).
  - **Lost in Context:** Performance drops significantly when tool outputs are overlong (Figure 7), indicating poor context management.

- **First 3 experiments:**
  1. **Baseline Run:** Run Claude-4.5-Sonnet on the 108 tasks to establish the 38.6% ceiling; analyze the specific "laziness" rate (premature stops) in long-horizon tasks.
  2. **Error Injection:** Manually inject a "tool execution error" (e.g., invalid API key) in a simple task. Observe if the agent retries, pivots, or crashes to test the framework's error handling loop.
  3. **Ablation on Fuzziness:** Select a task with a fuzzy prompt (e.g., Figure 3 Left). Run it as-is vs. a version with explicit step-by-step instructions to quantify the "planning tax."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can language agents be improved to effectively process and navigate overlong tool outputs without getting trapped or losing critical information?
- **Basis in paper:** [explicit] Section 5.2 reports that "most models get trapped trying to process these lengthy outputs" even though "tasks with overlong outputs are often logically straightforward," with success rates declining when trajectories contain overlong outputs.
- **Why unresolved:** Current truncation and paging mechanisms are insufficient; models fail to strategically search through cached outputs.
- **What evidence would resolve it:** Demonstrated techniques that maintain comparable success rates on tasks with and without overlong tool outputs.

### Open Question 2
- **Question:** What mechanisms can prevent premature termination in long-horizon, repetitive tasks where agents abandon work before completion?
- **Basis in paper:** [explicit] Appendix C.2 documents "Laziness in long horizon working" where models "often terminate prematurely and delegate the remaining work back to the user," even after processing only 1 of 10 required years in the music-analysis task.
- **Why unresolved:** System prompts warning against premature termination are insufficient; deeper architectural or training interventions may be needed.
- **What evidence would resolve it:** Methods achieving near-complete task execution rates on multi-repetition workflows without early stopping.

### Open Question 3
- **Question:** How should agents balance internal reasoning versus environmental exploration for optimal performance on complex agentic tasks?
- **Basis in paper:** [explicit] Section 4.2 states "increased reasoning effort for thinking-oriented models (e.g., GPT-5 vs. GPT-5-high) shows no benefit, suggesting that exploring new observations matters more than extended internal reasoning in agentic tasks."
- **Why unresolved:** The optimal trade-off between computation spent on reasoning versus action remains unknown.
- **What evidence would resolve it:** Studies systematically varying reasoning budgets and exploration strategies to identify optimal configurations across task types.

## Limitations

- Models struggle significantly with long-horizon tasks requiring many repetitive actions
- Performance degrades substantially when tool outputs exceed context limits
- Even top models achieve only 38.6% success rate, indicating current limitations
- Requires access to proprietary models and MCP server configurations for full replication
- Container-based evaluation setup is complex and resource-intensive

## Confidence

- High: Execution-based evaluation methodology is clearly specified and reproducible
- Medium: Access to proprietary models (GPT-5, Claude-4.5-Sonnet) may limit direct replication
- Medium: Exact MCP server modifications and API credential requirements are unspecified

## Next Checks

1. Verify environment initialization works correctly by checking setup scripts populate realistic initial states for at least 3 different application types
2. Test error handling by injecting tool execution failures and observing agent recovery behavior
3. Run a small ablation study comparing performance on fuzzy vs. explicit instructions for 5 representative tasks
4. Check that truncated tool outputs are handled appropriately by testing with outputs exceeding 100K characters
5. Validate that the deterministic evaluation scripts correctly identify edge cases where tool calls appear successful but environment state is incorrect