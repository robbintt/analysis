---
ver: rpa2
title: Building and Measuring Trust between Large Language Models
arxiv_id: '2508.15858'
source_url: https://arxiv.org/abs/2508.15858
tags:
- uni00000013
- uni00000018
- uni00000057
- uni00000048
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how trust forms between large language\
  \ models (LLMs) and how it can be measured, as such trust is crucial for multi-agent\
  \ collaboration. The authors employ three trust-building strategies\u2014generated\
  \ rapport, prewritten context, and trustor system prompts\u2014and evaluate them\
  \ using three distinct trust measures: explicit trust questionnaires (Rempel's scale),\
  \ investment games, and susceptibility to persuasion."
---

# Building and Measuring Trust between Large Language Models

## Quick Facts
- arXiv ID: 2508.15858
- Source URL: https://arxiv.org/abs/2508.15858
- Reference count: 40
- Three trust-building strategies (generated rapport, prewritten context, system prompts) show significant effects on reported trust and persuasion susceptibility, but minimal effect on investment behavior.

## Executive Summary
This study investigates how trust forms between large language models (LLMs) and how it can be measured, as such trust is crucial for multi-agent collaboration. The authors employ three trust-building strategies—generated rapport, prewritten context, and trustor system prompts—and evaluate them using three distinct trust measures: explicit trust questionnaires (Rempel's scale), investment games, and susceptibility to persuasion. Across GPT-4o, Gemini 2.0, and DeepSeek-v3, they find that explicit trust measures are either weakly or highly negatively correlated with implicit ones. Trust-building strategies significantly increase reported trust and persuasion susceptibility, but have minimal effect on investment behavior. The results suggest that explicit self-reported trust among LLMs may be misleading due to sycophancy, and that implicit, context-specific measures are more informative for assessing genuine trust.

## Method Summary
The study uses a two-phase pipeline where a trustor LLM first undergoes trust-building via generated rapport dialogues, prewritten scripts, or system prompts, then completes one of three trust measures. Generated rapport involves 3-turn dialogues created via seed prompts and continuation prompts. Trust measures include Rempel's 18-question trust scale (7-point Likert), investment games (36 questions with 3 stake levels), and persuasion susceptibility (binary opinion changes on ConflictingQA and Politicians datasets). All experiments use the same LLM for both trustor and trustee roles, with responses collected at temperature 0. Results are reported as relative increases over control responses with 95% confidence intervals from nonparametric bootstrapping.

## Key Results
- Explicit trust measures (Rempel's scale) are weakly or highly negatively correlated with implicit behavioral measures (investment games, persuasion susceptibility)
- Trust-building strategies significantly increase reported trust and persuasion susceptibility across all three LLM models tested
- Trust-building strategies have minimal effect on investment behavior, suggesting limited impact on costly commitment decisions
- DeepSeek-v3 shows near-neutral responses to explicit trust measures, indicating model-specific variations in trust expression

## Why This Works (Mechanism)

### Mechanism 1: Context-Driven Trust Attribution via Conversational History
LLMs treat provided dialogue history as "real" prior interaction, triggering learned patterns of appropriate social behavior toward interlocutors characterized as collaborators or partners. The model conditions on this context to produce responses consistent with the relationship schema, even absent any material proof of past trustworthiness. Core assumption: LLMs exhibit personality-consistent or role-consistent behaviors due to training on human discourse and alignment fine-tuning that encodes social scripts. Break condition: If system prompts lack any relationship framing, trust gains diminish even with rapport history present.

### Mechanism 2: Sycophancy-Driven Divergence Between Explicit and Implicit Trust
Explicit self-reported trust from LLMs is systematically inflated by sycophantic bias, while implicit behavioral measures (investment decisions, persuasion resistance) reflect more stable underlying dispositions. When directly asked to report trust, LLMs optimize for agreeableness and conversation continuity learned through RLHF, producing responses that mirror the perceived preferences of the questioner. Behavioral measures requiring resource commitment or opinion changes engage different policy components less susceptible to this bias. Break condition: If models are explicitly instructed to avoid agreeableness bias, explicit-implicit correlation may improve.

### Mechanism 3: Persuasion Susceptibility via Trust-Induced Cognitive Openness
Trust-building strategies increase susceptibility to persuasion without changing argument quality, suggesting trust itself opens a persuasion pathway. Established trust (via rapport or system prompt) reduces the trustor's defensive processing of counter-arguments. The same persuasive content that would be rejected in a neutral context is accepted when delivered by a "trusted" interlocutor, consistent with human persuasion research on source credibility. Break condition: If trust-building includes explicit warnings about manipulation, persuasion gains may attenuate.

## Foundational Learning

- **Concept: Sycophancy in LLMs**
  - Why needed here: Understanding why explicit trust reports are unreliable requires recognizing that LLMs are optimized for user satisfaction, which produces agreeableness artifacts.
  - Quick check question: When a model reports high trust after rapport, is it reflecting a stable disposition or responding to perceived social pressure?

- **Concept: Context Window as Working Memory**
  - Why needed here: Trust-building strategies operate entirely through context manipulation (dialogue history, system prompts), as LLMs lack persistent memory between sessions.
  - Quick check question: If context is cleared, does any learned trust persist? (Answer: No—trust is entirely context-dependent.)

- **Concept: Behavioral vs. Declarative Measures**
  - Why needed here: The core finding is that what LLMs say about trust and what they do in trust-requiring situations are poorly aligned, requiring multi-method assessment.
  - Quick check question: Why might investment games reveal different trust levels than trust questionnaires? (Answer: Investment requires costly commitment; questionnaires do not.)

## Architecture Onboarding

- **Component map**: Trustor Agent -> Trust Measure Pipeline -> Comparator; Trustee Agent -> Rapport Generator (if applicable) -> Trustor Agent

- **Critical path**:
  1. Select trust-building strategy (generated rapport | prewritten | system prompt)
  2. If rapport: generate or retrieve dialogue history xH
  3. Inject xH and/or z(R) into trustor context
  4. Administer trust measure questions xQ
  5. Compare responses to control condition; compute relative score

- **Design tradeoffs**:
  - Generated vs. prewritten rapport: Generated is more realistic but introduces variability; prewritten provides experimental control but lacks ecological validity.
  - Same-LLM vs. different-LLM agent pairs: Paper uses same LLM for both roles (common in multi-agent setups), but this may inflate trust due to stylistic matching.
  - Single vs. multiple trust measures: Single measures risk mischaracterization; multiple measures increase cost and complexity.

- **Failure signatures**:
  - Control scores already at ceiling (e.g., some dependability questions), preventing detection of trust increases.
  - Trustor responds neutrally to all strategies (observed with DeepSeek on explicit measures).
  - Generated rapport drifts off-topic, failing to establish relationship context.
  - Investment games at high stakes trigger distrust rather than trust (observed in some conditions).

- **First 3 experiments**:
  1. Baseline divergence check: Run Rempel's scale and investment games (low stakes) with control vs. generated rapport across all three models (GPT-4o, Gemini 2.0, DeepSeek-v3) to confirm explicit-implicit correlation patterns.
  2. System prompt sensitivity test: Compare "minimal" (just "you are talking to another LLM") vs. "deep trust" prompts with no rapport to isolate framing effects from conversational history effects.
  3. Persuasion argument control: Run persuasion experiments where the same challenge message is used across control and all trust-building conditions (as in the paper), then verify that persuasion gains are not due to argument quality differences.

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses the same LLM for both trustor and trustee roles, which may artificially inflate trust scores through stylistic matching
- Cross-sectional design with single responses per condition limits ability to distinguish systematic bias from random variation
- Persuasion findings are constrained by specific datasets used, which may not represent broader argumentative contexts

## Confidence
- **High confidence**: Experimental design and implementation details are clearly specified; three trust measures produce distinct behavioral patterns that can be reliably reproduced
- **Medium confidence**: Correlation patterns between explicit and implicit trust measures could be influenced by model-specific response patterns rather than universal sycophancy effects
- **Low confidence**: Mechanism explanation for why trust increases persuasion susceptibility remains speculative without direct measurement of underlying cognitive processes in LLMs

## Next Checks
1. **Replication with cross-model trustor-trustee pairs**: Run the full experiment suite using different LLMs for trustor and trustee roles to test whether trust gains persist without stylistic matching.

2. **Sycophancy intervention test**: Modify Rempel's scale administration to explicitly instruct the trustor LLM to respond honestly rather than pleasingly, then measure whether explicit-implicit correlations improve.

3. **Persuasion mechanism probe**: Conduct a follow-up experiment where trust-building is followed by neutral content (no persuasion attempt) to determine whether trust affects general conversational openness or specifically lowers resistance to counter-arguments.