---
ver: rpa2
title: 'ONG: Orthogonal Natural Gradient Descent'
arxiv_id: '2508.17169'
source_url: https://arxiv.org/abs/2508.17169
tags:
- task
- gradient
- tasks
- natural
- descent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Orthogonal Natural Gradient Descent (ONG),
  a method for continual learning that combines natural gradients with orthogonal
  projections to mitigate catastrophic forgetting. The approach preconditions task
  gradients with an EKFAC approximation of the inverse Fisher information matrix and
  projects them onto the orthogonal complement of prior tasks' gradients.
---

# ONG: Orthogonal Natural Gradient Descent

## Quick Facts
- arXiv ID: 2508.17169
- Source URL: https://arxiv.org/abs/2508.17169
- Reference count: 23
- Primary result: ONG achieves 58.7% forgetting vs OGD's 4.2% on Permuted MNIST; accuracy drops from 82.7% to 32.5%

## Executive Summary
This paper proposes Orthogonal Natural Gradient Descent (ONG), a method for continual learning that combines natural gradients with orthogonal projections to mitigate catastrophic forgetting. The approach preconditions task gradients with an EKFAC approximation of the inverse Fisher information matrix and projects them onto the orthogonal complement of prior tasks' gradients. The authors prove that ONG maintains descent-direction guarantees under the Fisher metric. However, preliminary experiments on Permuted and Rotated MNIST show that ONG performs worse than vanilla OGD, with significantly higher forgetting rates and lower accuracy. The authors interpret this as evidence of fundamental geometric incompatibility between Euclidean projections and Fisher-metric descent directions, motivating future work to develop geometrically consistent projection operators and extend evaluation to more challenging continual learning scenarios.

## Method Summary
ONG preconditions gradients with the inverse Fisher information matrix (via EKFAC approximation) to obtain natural gradients, then projects these onto the orthogonal complement of stored natural gradients from prior tasks to prevent interference. The method maintains descent direction guarantees under the Fisher metric while attempting to preserve performance on previous tasks through gradient orthogonalization.

## Key Results
- ONG achieves 58.7% forgetting compared to OGD's 4.2% on Permuted MNIST
- ONG accuracy drops from 82.7% to 32.5% while OGD maintains 82.7%
- Early-task accuracy collapses (Task 1: 5.44% for ONG vs 38.91% for OGD) while recent tasks remain high (Task 15: 96.35% for ONG)

## Why This Works (Mechanism)

### Mechanism 1: Natural Gradient Preconditioning via EKFAC
- Claim: Preconditioning gradients with the inverse Fisher information matrix yields the steepest descent direction under the Riemannian metric of the parameter space.
- Mechanism: The Fisher matrix F(w) captures curvature of the KL-divergence landscape. Computing g̃ = F(w)⁻¹∇L(w) rescales gradient directions by their sensitivity—directions that cause large output distribution changes are dampened, while insensitive directions are amplified. EKFAC approximates F⁻¹ via Kronecker factorization in the eigenbasis, making computation tractable for large networks.
- Core assumption: The diagonal approximation in the Kronecker-factored eigenbasis sufficiently captures the Fisher curvature for effective preconditioning.
- Evidence anchors:
  - [section 3.2] "The preconditioning by the inverse Fisher matrix provides us with optimization steps that are invariant to model reparameterization"
  - [section 4.5] "EKFAC works by providing a diagonal approximation to the FIM not in the raw parameter basis, but in the Kronecker-factored eigenbasis"
  - [corpus] Related work "Fisher-Orthogonal Projected Natural Gradient Descent" (arXiv:2601.12816) explores similar Fisher-preconditioned continual learning, suggesting independent interest in this mechanism
- Break condition: If Fisher estimates are noisy or the Kronecker approximation is poor, preconditioning may amplify noise rather than correct geometry.

### Mechanism 2: Orthogonal Projection for Interference Prevention
- Claim: Projecting new task gradients onto the orthogonal complement of prior task gradients prevents interference with learned representations.
- Mechanism: Maintain basis S spanning "sensitive directions" from prior tasks. For each update, compute ĝ = g − Σᵥ∈S projᵥ(g), removing components that would alter outputs on previous tasks. This constrains updates to subspace orthogonal to prior task gradients.
- Core assumption: The stored gradient subspace adequately captures task-critical parameter directions, and orthogonality in parameter space correlates with functional preservation.
- Evidence anchors:
  - [abstract] "ONG projects these natural gradients onto the orthogonal complement of prior tasks' natural gradients"
  - [section 3.3] OGD algorithm guarantees that "every model update ĝ remains a valid descent direction" with "forgetting bound...tightly controlled"
  - [corpus] "SketchOGD" (arXiv:2305.16424) extends OGD with memory-efficient projections, validating the projection mechanism's utility
- Break condition: If task gradient subspaces have significant overlap (high task correlation), the orthogonal complement becomes too constrained for effective learning.

### Mechanism 3: Descent Direction Preservation Under Projection
- Claim: After Fisher preconditioning and orthogonal projection, the resulting update direction remains a valid descent direction under the Fisher metric.
- Mechanism: Lemma 4.1 proves that ⟨−ĝ, F⁻¹g⟩ ≤ 0 by showing the projected residual ĝ is orthogonal to the projection subspace, yielding ⟨−ĝ, F⁻¹g⟩ = −‖ĝ‖² ≤ 0. This guarantees the composite operation doesn't reverse the loss gradient.
- Core assumption: The Fisher metric is the appropriate inner product for validating descent; orthogonality in Euclidean space transfers appropriately.
- Evidence anchors:
  - [section 4.4] "Lemma 4.1...proves that the preconditioned, projection-subtracted gradient ĝ is still a descent direction with respect to the Fisher metric"
  - [appendix A] Full proof provided
  - [corpus] Weak corpus signal—neighboring papers don't directly address this theoretical guarantee
- Break condition: Descent guarantees are local and asymptotic; they don't ensure good empirical performance or forgetting prevention.

## Foundational Learning

- **Concept: Fisher Information Matrix and Natural Gradients**
  - Why needed here: ONG's core modification is preconditioning with F⁻¹; understanding why this changes optimization geometry is essential for debugging failure modes.
  - Quick check question: Given a 2D loss landscape with Fisher matrix diag([100, 1]), which direction does the natural gradient emphasize relative to standard gradient descent?

- **Concept: Orthogonal Subspace Projection**
  - Why needed here: The continual learning mechanism depends on maintaining and projecting onto orthogonal complements; implementation errors here directly cause forgetting.
  - Quick check question: If basis vectors v₁, v₂ span prior task gradients, what is the projection of gradient g = [3, 4] onto their orthogonal complement?

- **Concept: Riemannian Geometry and Metric Compatibility**
  - Why needed here: The paper's central finding is geometric incompatibility between Euclidean projections and Fisher-metric descent; understanding inner products under different metrics is crucial for interpreting results.
  - Quick check question: In a Riemannian manifold with metric tensor G, how does the inner product ⟨u, v⟩_G differ from Euclidean ⟨u, v⟩, and what does this imply for "orthogonal" projections?

## Architecture Onboarding

- **Component map:**
  Input: Task gradient g = ∇L(w) → [EKFAC Preconditioner] → g̃ = F⁻¹g (natural gradient) → [Gradient Memory] → Basis S of prior task natural gradients → [Orthogonal Projector] → ĝ = g̃ − Σproj_v(g̃) for v∈S → [Parameter Update] → w ← w − ηĝ → [Memory Update] → Store new task gradients (also preconditioned) in S

- **Critical path:** The EKFAC approximation quality → preconditioned gradient accuracy → projection effectiveness → forgetting/accuracy tradeoff. Each stage compounds errors.

- **Design tradeoffs:**
  - EKFAC vs. exact Fisher: EKFAC is O(n²) per layer vs. O(n⁴) for exact, but approximation errors may accumulate
  - Memory buffer size (100 samples/task in experiments): Larger buffers capture more gradient directions but increase computation and storage
  - Preconditioning frequency: Every batch (current) vs. periodic recomputation trades accuracy for speed

- **Failure signatures:**
  - **Observed:** ONG achieves 58.7% forgetting vs. OGD's 4.2% on Permuted MNIST; accuracy drops from 82.7% → 32.5%
  - **Pattern:** Early-task accuracy collapses rapidly (Task 1: 5.44% for ONG vs. 38.91% for OGD), while recent tasks remain high (Task 15: 96.35%)
  - **Hypothesis from paper:** "Fundamental geometric incompatibility between Euclidean projections and Fisher-metric descent directions"—the projection inner product doesn't match the descent metric

- **First 3 experiments:**
  1. **Ablate preconditioning location:** Apply Fisher preconditioning only at projection time (not update time) or vice versa to isolate which combination causes failure
  2. **Fisher-weighted projection:** Replace Euclidean projection with Fisher-inner-product projection: projᵥ(g) = (⟨g, v⟩_F / ⟨v, v⟩_F) · v where ⟨a, b⟩_F = aᵀFb, testing the geometric compatibility hypothesis
  3. **Gradient subspace analysis:** Visualize overlap between stored natural gradient subspace and incoming task gradients across training—if overlap is systematically higher for ONG than OGD, this explains constrained learning and failure to acquire early tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can projection operators be redesigned to be geometrically consistent with the Fisher metric to prevent the performance degradation observed in ONG?
- Basis in paper: [explicit] The authors state the need for "robustly reconciling these geometric perspectives" and "developing a geometrically-meaningful way of performing 'projection' with natural gradients."
- Why unresolved: The paper's core finding is that naive Euclidean projection applied to natural gradients causes catastrophic forgetting (58.7% vs 4.2% in OGD), indicating a fundamental incompatibility in the current implementation.
- What evidence would resolve it: An algorithm combining natural gradients with non-Euclidean projections that achieves lower forgetting rates than vanilla OGD on standard benchmarks.

### Open Question 2
- Question: Can parallel transport be utilized to define a consistent inner product and basis for projecting gradients across different points on the parameter manifold?
- Basis in paper: [explicit] The authors identify parallel transport as a "particularly promising" direction to "move tangent vectors... into the same tangent space" to define a "geometrically consistent inner product."
- Why unresolved: While theoretically motivated in the conclusion, the paper does not implement or test parallel transport, leaving its efficacy in this context unproven.
- What evidence would resolve it: Empirical results from an ONG variant using parallel transport demonstrating stable retention of prior task knowledge.

### Open Question 3
- Question: What specific role does the accuracy of the EKFAC approximation play in the failure of ONG?
- Basis in paper: [explicit] The authors list "looking more closely at... the accuracy of the Fisher approximation estimates" as a specific avenue for future work to understand training dynamics.
- Why unresolved: It remains unclear if the poor performance stems from the theoretical mismatch between geometries or from noise introduced by the Kronecker-factored approximation.
- What evidence would resolve it: Ablation studies comparing ONG performance using exact Fisher matrices versus EKFAC approximations on smaller networks.

### Open Question 4
- Question: Does the "geometric incompatibility" hypothesis hold in more challenging, uncorrelated task sequences?
- Basis in paper: [explicit] The authors hypothesize that the observed limitations might be "partially attributable to the high task correlation in benchmarks like Permuted and Rotated MNIST."
- Why unresolved: The current evaluation is restricted to benchmarks with high task correlation, limiting the generalizability of the failure analysis to realistic continual learning scenarios.
- What evidence would resolve it: Evaluation of ONG on diverse, uncorrelated task sequences to determine if the method's relative performance improves or degrades further.

## Limitations

- The theoretical descent direction guarantee doesn't translate to practical forgetting mitigation, as ONG performs worse than OGD on standard benchmarks
- The paper provides minimal empirical investigation of failure modes despite dramatic performance degradation
- Evaluation is restricted to highly correlated task sequences, limiting generalizability to realistic continual learning scenarios

## Confidence

- **Medium Confidence**: The theoretical framework (descent direction preservation via Lemma 4.1) is mathematically sound, but the gap between local geometric guarantees and global empirical performance suggests missing analysis of projection metric compatibility.
- **Low Confidence**: The claim that ONG's poor performance stems from "fundamental geometric incompatibility" is compelling but requires validation through controlled ablation studies—the paper provides minimal empirical investigation of failure modes.
- **Medium Confidence**: The observation that early-task accuracy collapses while recent tasks remain high is a clear failure signature, though its interpretation requires further investigation of gradient subspace overlap dynamics.

## Next Checks

1. **Geometric Compatibility Test**: Replace Euclidean orthogonal projection with Fisher-inner-product projection (projᵥ(g) = (⟨g, v⟩_F / ⟨v, v⟩_F) · v) and compare forgetting rates to isolate whether metric mismatch drives ONG's failure.

2. **Gradient Subspace Analysis**: Visualize and quantify the overlap between stored natural gradient subspaces and incoming task gradients across training—if overlap is systematically higher for ONG than OGD, this explains constrained learning and early-task forgetting.

3. **EKFAC Approximation Ablation**: Compare ONG with exact Fisher preconditioning (when computationally feasible) versus EKFAC approximation to determine whether approximation errors compound with projection to cause failure.