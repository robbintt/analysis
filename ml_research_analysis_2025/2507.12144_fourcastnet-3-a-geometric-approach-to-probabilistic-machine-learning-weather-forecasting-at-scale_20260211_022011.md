---
ver: rpa2
title: 'FourCastNet 3: A geometric approach to probabilistic machine-learning weather
  forecasting at scale'
arxiv_id: '2507.12144'
source_url: https://arxiv.org/abs/2507.12144
tags:
- ensemble
- which
- fourcastnet
- training
- spherical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FourCastNet 3 is a probabilistic global weather forecasting model
  that advances ML-based weather prediction by introducing a scalable, geometric approach
  using spherical neural operators. It employs local and global spherical convolutions
  and is trained end-to-end as an ensemble forecasting model with a probabilistic
  loss function in both spectral and spatial domains.
---

# FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale

## Quick Facts
- arXiv ID: 2507.12144
- Source URL: https://arxiv.org/abs/2507.12144
- Reference count: 40
- Primary result: FCN3 produces probabilistic weather forecasts 8-60x faster than NWP ensembles while achieving superior skill and spectral fidelity at 60-day lead times.

## Executive Summary
FourCastNet 3 (FCN3) is a probabilistic global weather forecasting model that advances ML-based weather prediction through a spherical neural operator architecture. By employing local and global spherical convolutions that respect the geometry of the sphere, FCN3 achieves stable, sharp angular power spectra even at extended lead times. Trained end-to-end with a hybrid spatial-spectral CRPS loss, it produces calibrated ensembles that outperform traditional NWP ensembles while running 8-60 times faster.

The model scales efficiently to over 1000 GPUs using a novel hybrid model- and data-parallelism strategy, making it practical for operational forecasting and large ensemble applications. FCN3 uniquely avoids the blurring or high-frequency noise problems common in other ML models, maintaining physical consistency across all wavelengths throughout its forecast horizon.

## Method Summary
FCN3 is a spherical neural operator that uses local DISCO convolutions and global spectral convolutions to process ERA5 data on a 0.25° equiangular grid. It employs a hybrid parallelism strategy combining spatial domain decomposition with data parallelism across batches and ensemble members. The model is trained through a three-stage curriculum: initial single-step training with biased CRPS, intermediate rollout training with fair CRPS, and final fine-tuning with noise centering. The loss function combines spatial and spectral CRPS terms to ensure both point-wise calibration and physically coherent power spectra.

## Key Results
- Produces forecasts 8-60x faster than IFS ensemble with superior probabilistic skill
- Achieves CRPSS of 0.5-0.8 over IFS ensemble across all lead times and variables
- Maintains stable angular power spectra at 60-day lead times without blurring or noise accumulation
- Scales efficiently to over 1000 GPUs using novel hybrid parallelism strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spherical geometry preservation yields stable spectra at extended lead times.
- Mechanism: Local DISCO convolutions model anisotropic atmospheric features while global spectral convolutions capture planetary-scale dynamics; together they respect rotational symmetries of the sphere, reducing artifacts that accumulate in autoregressive rollouts.
- Core assumption: Physical operators respecting SO(3) symmetry generalize better to long horizons than architectures with implicit geometric distortions.
- Evidence anchors:
  - [abstract] "purely convolutional neural network architecture tailored for spherical geometry"
  - [section 3] "FCN3 is a spherical neural operator architecture and relies heavily on local and global spherical group convolutions... This formulation enables anisotropic filters that are better suited to approximate atmospheric phenomena"
  - [corpus] Weak/no direct corpus comparisons on spectral stability; related work (LaDCast, FGN) focuses on diffusion or marginal-based probabilistic skill rather than spectral fidelity.
- Break condition: If applied to non-spherical domains or with aggressive spatial downsampling that violates quadrature accuracy, spectral fidelity may degrade.

### Mechanism 2
- Claim: Combined spatial + spectral CRPS loss produces calibrated ensembles with realistic spatial correlations.
- Mechanism: Spatial CRPS ensures point-wise probabilistic calibration; spectral CRPS enforces correct power spectrum distributions across wavenumbers, preventing the model from minimizing CRPS via spatially shuffled/unphysical ensemble members.
- Core assumption: Optimizing spectral coefficient distributions transfers to physically coherent spatiotemporal structures.
- Evidence anchors:
  - [section 2] "This is particularly problematic for multi-variate spatial processes, where the CRPS can be minimized in a point-wise manner by an unphysical ensemble. To address this issue, we combine the spatial, point-wise CRPS loss term with a loss term in the spectral domain."
  - [section F.3] Spread-skill ratios approach 1 and rank histograms flatten over time, indicating calibration.
  - [corpus] Neighbors (CRPS-LAM, FGN) also use CRPS variants for ensemble training but do not report spectral loss components.
- Break condition: If spectral loss weight is too low, high-frequency noise may accumulate; if too high, point-wise calibration may suffer.

### Mechanism 3
- Claim: Domain-decomposed model-parallelism enables efficient training of high-resolution spherical operators at scale.
- Mechanism: Spatial decomposition splits both data and model weights across GPUs, with distributed SHT and DISCO convolutions requiring all-to-all transposes; ensemble and batch parallelism are orthogonal communicators, reducing contention.
- Core assumption: Communication overhead from spherical harmonic transposes is bounded by mapping to low-hop network topology (e.g., NVLink within node).
- Evidence anchors:
  - [section 4] "We implement spatial model parallelism... where both the model and data are split across ranks by employing a spatial domain decomposition. This approach is inspired by traditional distributed scientific computing applications"
  - [section G.1] Describes communicator hierarchy and network topology mapping to minimize multi-hop penalties.
  - [corpus] No direct corpus evidence on this specific parallelism strategy for spherical weather models.
- Break condition: If network bandwidth is insufficient or communicator mapping ignores topology, scaling efficiency drops sharply beyond ~100 GPUs.

## Foundational Learning

### Concept: Spherical harmonic transforms and quadrature on the sphere
- Why needed here: All global convolutions and spectral losses require SHT; understanding Gaussian vs. equiangular grids and quadrature weights is essential for implementing distributed operations correctly.
- Quick check question: Given an equiangular lat/lon grid, what happens to quadrature accuracy near the poles compared to a Gaussian grid?

### Concept: Probabilistic scoring rules (CRPS, fair CRPS)
- Why needed here: Training objective combines CRPS variants; understanding their bias-variance tradeoffs informs when to switch between biased and fair versions during curriculum training.
- Quick check question: For a 2-member ensemble where one member equals the ground truth, why does fair CRPS give zero loss regardless of the second member?

### Concept: Domain decomposition and distributed transposes
- Why needed here: Scaling beyond single-GPU memory requires pencil decomposition and all-to-all communication patterns; misaligned splits cause load imbalance.
- Quick check question: In a 4-rank latitude decomposition, how many all-to-all transposes are needed for a forward SHT?

## Architecture Onboarding

### Component map:
Input -> Encoder (DISCO down-sampling) -> 8 local blocks + 2 global blocks -> Decoder (upsampling) -> Output transformation -> Loss

### Critical path:
Input → Encoder (DISCO) → Processor blocks (alternating local/global) → Decoder → Output transformation (softclamp for water channels) → Loss (distributed CRPS aggregation)

### Design tradeoffs:
- Direct state prediction vs. residual prediction: Chose direct to avoid high-frequency artifact amplification in rollouts (see C.7).
- Omitting layer normalization: Relies on careful He-style initialization to preserve absolute magnitudes for physical variables.
- 4:1 local-to-global block ratio: Empirically best for skill; assumes atmospheric dynamics mix local and global scales asymmetrically.

### Failure signatures:
- Spectral blow-up at high wavenumbers during rollout → check spectral loss weight and noise-centering in fine-tuning.
- Under/over-dispersive ensembles → inspect spread-skill ratio evolution; may need initial condition perturbations.
- Memory overflow during autoregressive training → increase model-parallelism degree or reduce rollout steps.

### First 3 experiments:
1. Ablate spectral loss component: Train with spatial CRPS only and compare angular power spectra at 15-day lead times against full FCN3.
2. Vary local/global block ratio: Test 2:1, 4:1, 8:1 ratios on a reduced dataset to validate the 4:1 empirical choice.
3. Profile communication patterns: On 64/256/1024 GPU configurations, measure time in SHT transposes vs. compute to identify scaling bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the current architecture and loss function effectively predict highly intermittent, non-Gaussian variables such as precipitation?
- Basis in paper: [explicit] The authors state in the conclusion, "Looking ahead, we plan to extend FCN3 to include precipitation as a diagnostic output."
- Why unresolved: The current model is trained on atmospheric states with approximately Gaussian error distributions using CRPS. Precipitation is discontinuous and highly skewed; it is unclear if the spectral loss and CRPS objectives require modification to handle zero-inflated distributions without producing blurring or biases.
- What evidence would resolve it: A study training FCN3 with precipitation targets, evaluating the resulting ensemble's ability to capture extreme precipitation events and maintain spectral fidelity for moisture fields.

### Open Question 2
- Question: Does the integration of initial condition (IC) perturbations improve the ensemble calibration at short lead times?
- Basis in paper: [explicit] The authors hypothesize in Section F.3 that "The initial underdispersion... may be attributed to the absence of initial condition uncertainty in FCN3. Introducing initial condition perturbations... could potentially improve FCN3’s dispersion characteristics."
- Why unresolved: The paper identifies a calibration mismatch (underdispersion) in the 24–200 hour range but does not test whether stochastic initialization techniques (like bred vectors) utilized in traditional NWP would correct this within the ML framework.
- What evidence would resolve it: An ablation study comparing FCN3 forecasts initialized from perturbed versus control analyses, specifically analyzing the Spread-Skill Ratio (SSR) and rank histograms at lead times under 48 hours.

### Open Question 3
- Question: How can FCN3 be coupled with data assimilation systems to generate initial conditions that include uncertainty estimates?
- Basis in paper: [explicit] The authors conclude by planning to "integrate data assimilation uncertainty," implying the current model relies on fixed reanalysis states (ERA5) without an internal mechanism for state estimation or uncertainty propagation from observations.
- Why unresolved: The model currently functions purely as a forward time-stepping operator. Integrating data assimilation requires developing a differentiable observation operator or 4D-Var approach that is compatible with the spherical neural operator architecture.
- What evidence would resolve it: A demonstration of an end-to-end differentiable assimilation-to-forecast loop where FCN3 ingests raw observations to produce an analysis ensemble, improving forecast skill scores compared to initializing from ERA5.

## Limitations
- The specific hardware topology mapping required for optimal communication scaling is not detailed, creating uncertainty about real-world efficiency at extreme scales.
- The model's ability to handle highly non-Gaussian variables like precipitation remains untested and may require modifications to the loss function.
- Current implementation relies on fixed reanalysis states without internal data assimilation capability for uncertainty propagation from observations.

## Confidence

### High confidence
- Probabilistic skill improvements over NWP ensembles (verified via IFS comparison)
- Computational speedup claims (validated through weak scaling tests)

### Medium confidence
- Spectral stability at extended lead times (supported by angular power spectra but limited cross-dataset validation)
- Ensemble calibration metrics (spread-skill ratios, rank histograms)

### Low confidence
- Communication efficiency at extreme scales (>1024 GPUs) without detailed topology mapping
- Generalizability to non-ERA5 datasets or different grid resolutions

## Next Checks

1. **Spectral Loss Ablation**: Train FCN3 with spatial CRPS only and compare angular power spectra at 15-day lead times against the full model to quantify spectral loss impact on high-frequency stability.

2. **Local/Global Block Ratio Sensitivity**: Test 2:1, 4:1, and 8:1 ratios on a reduced ERA5 subset to empirically validate the claimed 4:1 optimum for probabilistic skill.

3. **Communication Scaling Benchmark**: Profile SHT transpose times vs. compute across 64/256/1024 GPU configurations to identify bottlenecks and verify the stated scaling efficiency claims.