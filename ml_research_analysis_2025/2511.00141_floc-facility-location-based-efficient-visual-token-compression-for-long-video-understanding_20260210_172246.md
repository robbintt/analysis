---
ver: rpa2
title: 'FLoC: Facility Location-Based Efficient Visual Token Compression for Long
  Video Understanding'
arxiv_id: '2511.00141'
source_url: https://arxiv.org/abs/2511.00141
tags:
- tokens
- visual
- video
- token
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently processing long
  video sequences for large multimodal models, which are constrained by context length
  limits. The proposed method, FLoC (Facility Location-based Compression), uses a
  submodular facility location function to select a compact, representative, and diverse
  subset of visual tokens within a predefined budget.
---

# FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding

## Quick Facts
- **arXiv ID:** 2511.00141
- **Source URL:** https://arxiv.org/abs/2511.00141
- **Reference count:** 40
- **Primary result:** FLoC improves accuracy by up to 2.44 points on long video understanding benchmarks while reducing computational overhead.

## Executive Summary
This paper addresses the challenge of efficiently processing long video sequences for large multimodal models, which are constrained by context length limits. The proposed method, FLoC (Facility Location-based Compression), uses a submodular facility location function to select a compact, representative, and diverse subset of visual tokens within a predefined budget. By integrating a lazy greedy algorithm, FLoC achieves near-optimal performance with significantly reduced computational overhead. The approach is training-free, model-agnostic, and query-agnostic, making it versatile for integration into various video-LLM pipelines. Evaluated on large-scale benchmarks like Video-MME, MLVU, and LongVideoBench, FLoC consistently outperforms recent compression techniques, improving accuracy by up to 2.44 points and demonstrating strong efficiency gains in both processing speed and memory usage.

## Method Summary
FLoC compresses visual tokens by selecting a subset that maximizes a submodular facility location function, ensuring both representativeness and diversity. The method is training-free and uses a lazy greedy algorithm with a priority queue to achieve near-optimal performance with reduced computational overhead. Tokens are processed in temporal blocks (length T=32) to allow scalable processing and future streaming capabilities. The approach is model-agnostic and integrates seamlessly with various video-LLM pipelines.

## Key Results
- FLoC improves accuracy by up to 2.44 points on long video understanding benchmarks
- Achieves near-optimal performance with significantly reduced computational overhead
- Demonstrates strong efficiency gains in both processing speed and memory usage

## Why This Works (Mechanism)

### Mechanism 1: Coverage-Based Diversity via Facility Location
- **Claim:** The facility location function theoretically ensures a representative and diverse subset of tokens by maximizing global "coverage" rather than cluster density.
- **Mechanism:** The objective function $f(S) = \sum_{v \in V} \max_{u \in S} \text{sim}(v, u)$ rewards a subset $S$ where every token in the ground set $V$ is similar to at least one token in $S$. Unlike clustering, which forces selection toward dense centroids (averages), this maximization forces the selection of outliers or sparse features to maximize the sum of similarities across the entire set.
- **Core assumption:** Visual features corresponding to critical but rare details (e.g., a specific object in a cluttered room) reside in sparse regions of the embedding space and are not centrally located within dense clusters of background features.
- **Evidence anchors:**
  - [abstract]: "...uses a submodular facility location function to select a compact, representative, and diverse subset..."
  - [section]: Section 3.2 states that unlike k-means, which selects from dense regions, the facility location function "ensures that selected tokens span diverse feature regions by defining utility in terms of coverage."
  - [corpus]: *LLaVA-Scissor* proposes "semantic connected components" for compression, reinforcing the paper's premise that standard attention or clustering methods often miss semantic regions, though it uses a different selection logic than FLoC.
- **Break condition:** If the cosine similarity space does not correlate with semantic importance (i.e., rare objects are not spatially distinct in the embedding space), maximizing coverage may retain noise.

### Mechanism 2: Efficiency via Lazy Greedy Optimization
- **Claim:** The "lazy greedy" algorithm drastically reduces the computational cost of token selection while maintaining a theoretical performance guarantee.
- **Mechanism:** Standard greedy algorithms require recomputing the marginal gain for every remaining token at each step. The lazy approach uses a priority queue to sort tokens by their *previous* marginal gain. It only recomputes the gain for the top token; if the gain drops it re-inserts it, otherwise it selects it. This exploits the "diminishing returns" property of submodular functions to skip expensive calculations.
- **Core assumption:** The submodularity of the facility location function holds such that marginal gains decrease monotonically, ensuring the lazy evaluation does not miss the true optimal next step.
- **Evidence anchors:**
  - [abstract]: "...integrating a lazy greedy algorithm, FLoC achieves near-optimal performance with significantly reduced computational overhead."
  - [section]: Section 3.2 confirms the complexity drop from $O(nK)$ (naive greedy) to practical speeds by "postponing the update of marginal gains until absolutely necessary."
  - [corpus]: *DynTok* also emphasizes "Dynamic Compression" for efficiency, suggesting that static or iterative methods are too slow for real-time video streams, validating the need for FLoC's optimized selection loop.
- **Break condition:** If the token set size ($n$) is extremely small or the feature distribution is adversarial, the overhead of maintaining the priority queue might offset the gains from skipped computations.

### Mechanism 3: Temporal Block Processing
- **Claim:** Dividing the video into temporal blocks (length $T$) allows for scalable processing and future streaming capabilities, trading off global context for local optimality.
- **Mechanism:** Instead of running the optimization on the entire video token set (which is memory intensive), the algorithm splits tokens into temporal blocks. The selection occurs independently within each block.
- **Core assumption:** Temporal redundancy is high enough within blocks that a budget allocated per block preserves continuity, and inter-block redundancy is a secondary concern compared to the computational feasibility of global optimization.
- **Evidence anchors:**
  - [section]: Section 3.1 states, "we divide the input video into smaller temporal blocks for computational efficiency... allows future extension to streaming scenarios."
  - [section]: Appendix G (Limitations) notes the tradeoff: "Shorter block lengths... can introduce a risk of inter-block redundancy."
  - [corpus]: *StreamMem* addresses "Streaming Video Understanding," implicitly supporting FLoC's architectural choice to prepare for streaming via block processing.
- **Break condition:** If a critical event spans two blocks or is split across them, the independent selection might discard it if neither block retains enough context to make the tokens "representative" locally.

## Foundational Learning

- **Concept:** **Submodular Functions & Diminishing Returns**
  - **Why needed here:** The entire efficiency of FLoC relies on the facility location function being submodular. Without understanding that adding a token to a small set gains more value than adding it to a large set, the "lazy greedy" optimization logic appears arbitrary.
  - **Quick check question:** If you add a visual token to a set, why does the "coverage" gain decrease as the set grows larger?

- **Concept:** **Cosine Similarity in High Dimensions**
  - **Why needed here:** The paper uses cosine similarity `sim(v, u)` to measure token relevance. Understanding that this measures orientation (semantic alignment) rather than magnitude is crucial for debugging why two visually different patches might be selected as "similar."
  - **Quick check question:** Why is Euclidean distance a poor proxy for token "relevance" in normalized LLM embedding spaces compared to cosine similarity?

- **Concept:** **Priority Queues (Heaps)**
  - **Why needed here:** The lazy greedy algorithm is implemented using a priority queue (Algorithm 1). You must understand `O(log n)` insertion/extraction to analyze the runtime complexity claims.
  - **Quick check question:** In the lazy greedy loop, why do we pop the token with the *highest* priority value first?

## Architecture Onboarding

- **Component map:** Visual Encoder -> Tokenizer -> FLoC Module (Block Splitter, Priority Queue, Scorer) -> Video-LMM

- **Critical path:** The **Scorer** inside the FLoC Module. Specifically, the logic in Algorithm 1 (lines 7-14) where the algorithm decides whether to *accept* the top token or *re-insert* it with an updated score. This loop dictates both the accuracy (did we pick the right tokens?) and speed (did we avoid unnecessary work?).

- **Design tradeoffs:**
  - **Block Length ($T$):**
    - *Low $T$:* Faster, lower memory, but risks inter-block redundancy (selecting the same static background from multiple blocks).
    - *High $T$:* Better global diversity within the block, but higher latency to compute the similarity matrix and run the queue.
  - **Budget ($K$):** Direct tradeoff between downstream LMM accuracy and inference speed/memory.

- **Failure signatures:**
  - **Symptom:** Retained tokens are mostly static background; dynamic objects lost.
  - **Likely Cause:** Block length $T$ is too large relative to the motion speed, or the budget $K$ is too low for the scene complexity.
  - **Symptom:** Latency is higher than expected (approaching naive greedy).
  - **Likely Cause:** Feature vectors are not normalized, causing the "lazy" check to fail frequently (marginal gains not decreasing monotonically), forcing full recomputation.

- **First 3 experiments:**
  1.  **Visual Validation (t-SNE):** Reproduce Figure 4. Run FLoC on a sample video and plot selected vs. discarded tokens in 2D. Verify that selected tokens (stars) cover the periphery/sparse regions, unlike K-Means which should cluster in the center.
  2.  **Latency vs. Block Size:** Reproduce Table 4 (or similar). Measure the compression wall-clock time while sweeping Block Length ($T$). Find the "knee" in the curve where accuracy gains plateau but latency spikes.
  3.  **"Needle in a Haystack" Test:** Run the qualitative test from Figure 5. Insert a single distinct frame (e.g., a specific object) into a long, repetitive video. Verify if FLoC selects tokens from that single frame while baselines (like DivPrune) miss it.

## Open Questions the Paper Calls Out
None

## Limitations
- **Dimensionality of Similarity Computation:** The paper does not explicitly specify which layer or projection of the visual token embeddings is used for the cosine similarity calculation in the facility location function.
- **Priority Queue Implementation Details:** While the lazy greedy algorithm is described conceptually, the paper lacks details on how stale marginal gain values are managed in the priority queue.
- **Temporal Block Independence:** The claim of strong performance across various block lengths (T=32, 64, 128) is promising, but the paper does not provide a rigorous analysis of the tradeoff between block length and inter-block redundancy.

## Confidence
- **High Confidence:** The theoretical foundation of using a submodular facility location function for coverage-based diversity is well-established in the literature. The claim that FLoC is training-free and model-agnostic is directly verifiable from the method description.
- **Medium Confidence:** The empirical results showing FLoC outperforming baselines on Video-MME, MLVU, and LongVideoBench are strong, but the lack of ablation studies on the exact token embedding layer used for similarity introduces some uncertainty about the reproducibility of the results.
- **Low Confidence:** The claim of robustness to different block lengths (T) is based on a single table (Table 1) without a detailed analysis of failure modes or a sensitivity study for videos with varying motion speeds or scene complexity.

## Next Checks
1. **Embedding Layer Sensitivity Test:** Run FLoC using cosine similarity computed from different layers of the visual encoder (e.g., raw patch embeddings, [CLS] token, projector output) to determine which yields the best coverage-diversity tradeoff.
2. **Stale Value Priority Queue Benchmark:** Implement the lazy greedy algorithm with two different priority queue strategies: (1) a simple re-insertion of stale values, and (2) a more sophisticated approach that tracks and invalidates outdated scores. Compare their computational overhead and accuracy.
3. **Dynamic Block Length Evaluation:** Modify FLoC to use an adaptive block length that changes based on video motion (e.g., using optical flow to detect scene changes). Evaluate its performance on a dataset with highly dynamic content to see if it outperforms the fixed block length approach.