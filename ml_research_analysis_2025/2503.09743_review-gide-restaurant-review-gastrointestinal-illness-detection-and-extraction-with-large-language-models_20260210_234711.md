---
ver: rpa2
title: Review GIDE -- Restaurant Review Gastrointestinal Illness Detection and Extraction
  with Large Language Models
arxiv_id: '2503.09743'
source_url: https://arxiv.org/abs/2503.09743
tags:
- food
- reviews
- illness
- symptoms
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a novel annotation schema for classifying
  gastrointestinal illness in online restaurant reviews, extending beyond binary detection
  to extract detailed information on symptoms and foods. Using the Yelp Open Dataset,
  the authors evaluate the performance of large language models (LLMs) against fine-tuned
  RoBERTa models across three tasks: GI illness detection, symptom extraction, and
  food extraction.'
---

# Review GIDE -- Restaurant Review Gastrointestinal Illness Detection and Extraction with Large Language Models

## Quick Facts
- **arXiv ID:** 2503.09743
- **Source URL:** https://arxiv.org/abs/2503.09743
- **Reference count:** 40
- **Primary result:** Prompt-based LLMs achieve micro-F1 scores over 90% for GI illness detection, symptom extraction, and food extraction, outperforming fine-tuned RoBERTa models.

## Executive Summary
This study introduces a novel annotation schema for classifying gastrointestinal illness in online restaurant reviews, extending beyond binary detection to extract detailed information on symptoms and foods. Using the Yelp Open Dataset, the authors evaluate large language models against fine-tuned RoBERTa models across three tasks: GI illness detection, symptom extraction, and food extraction. Prompt-based LLMs demonstrate superior performance with micro-F1 scores exceeding 90% for all tasks, particularly excelling on rare labels where fine-tuned models struggle. The best-performing model, Llama-3.3-70b, shows robust performance with minimal demographic bias across social group variations. While LLMs show high accuracy, limitations in the restaurant review data highlight the need for cautious interpretation in public health surveillance applications.

## Method Summary
The study employs a two-stage annotation process on filtered Yelp reviews containing GI-related keywords. Expert annotators first label reviews as containing GI illness or not, then annotate positive reviews with five symptom categories (general sickness, nausea, diarrhea, vomiting, stomach pain) and 27 food categories. The authors evaluate multiple prompt-based LLMs (Llama-3.3-70b, Claude-3-Opus, GPT-4, Gemini-Pro) against a fine-tuned RoBERTa baseline using 0-shot, 5-shot, and 10-shot prompting strategies. Performance is measured using micro and macro F1 scores, with additional bias testing through demographic term substitution. Food extraction includes synthetic data augmentation for rare labels to improve model training.

## Key Results
- Llama-3.3-70b achieves 0.912 micro-F1 on GI classification, surpassing RoBERTa's 0.902
- Symptom extraction shows larger performance gap (LLM 0.915 vs. RoBERTa 0.796 micro-F1)
- Prompt-based approaches show minimal demographic bias with <0.5% F1 variation across gender, spelling formality, and marital status substitutions
- Food extraction suffers from data sparsity with 5 labels having <10 samples, though synthetic augmentation helps fine-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt-based LLMs outperform fine-tuned RoBERTa on GI illness classification and extraction tasks.
- **Mechanism:** Large pre-trained models leverage broad linguistic knowledge from their training corpus, enabling zero- and few-shot generalization to specialized health NLP tasks without task-specific weight updates. In-context examples guide the model's attention toward relevant semantic patterns (symptom terminology, food-disease associations).
- **Core assumption:** The pre-training distribution contains sufficient coverage of medical language, colloquial health expressions, and food-related discourse to transfer to this domain.
- **Evidence anchors:**
  - [abstract] "Using prompt-based approaches, LLMs achieve micro-F1 scores of over 90% for all three of our tasks... that exceed those of smaller fine-tuned models."
  - [section 4.1, Table 6] Llama-3.3-70b achieves 0.912 micro-F1 on GI classification vs. RoBERTa's 0.902; symptom extraction shows larger gap (0.915 vs. 0.796).
  - [corpus] Related work (Effland et al., Liu et al.) established fine-tuned BERT-family models as prior SOTA with F1 scores of 87-91.6% on binary GI detection, supporting the baseline comparison.
- **Break condition:** Performance degrades significantly on rare symptom/food labels where few-shot examples are insufficient and the pre-training distribution lacks coverage.

### Mechanism 2
- **Claim:** Expert-designed annotation schema enables structured extraction beyond binary classification.
- **Mechanism:** Domain experts (GI epidemiologists) define a constrained label space (5 symptoms, 27 foods) with explicit inclusion/exclusion criteria. This schema maps to established food safety frameworks (FoodEx2 database), grounding model outputs in public health semantics rather than open-ended generation.
- **Core assumption:** The schema categories are both clinically meaningful for surveillance and linguistically recoverable from review text.
- **Evidence anchors:**
  - [section 3.1] "A subset of the reviews labelled as relating to GI are then annotated with the relevant symptoms reported... disambiguated into five symptom labels."
  - [section 8.2-8.3] Detailed protocols specify symptom/food labels with keyword matching rules and pathogen-associated rationale.
  - [corpus] Weak corpus support—no directly comparable multi-label GI extraction schemas found in neighbors; this appears novel.
- **Break condition:** Schema assumptions fail when reviews describe symptoms not in the 5-label set (e.g., fever, headache) or when dish names obscure ingredients (e.g., "sushi" without specifying fish type).

### Mechanism 3
- **Claim:** LLM-based detection shows minimal demographic bias under social group substitution.
- **Mechanism:** The model's GI classification decisions are invariant (within statistical noise) when gendered terms (he/she, wife/girlfriend) are swapped, suggesting the learned decision boundary does not rely on spurious demographic correlates.
- **Core assumption:** The tested demographic variables (spelling formality, gender, marital status) are orthogonal to the true classification target and any observed non-invariance would indicate bias.
- **Evidence anchors:**
  - [section 5, Tables 13-15] Micro-F1 differences across spelling (0.912 vs. 0.913), gender (0.914-0.919 range), and marital status (0.942-0.947 range) are all <0.5% and statistically non-significant.
  - [section 5] "We do not find evidence that performance is degraded when processing reviews that contain less traditional language."
  - [corpus] No corpus validation—neighbor papers do not replicate this bias testing approach for health NLP.
- **Break condition:** Invariance testing is limited to text-level substitutions; real-world performance may still reflect sampling bias in who writes reviews (income-correlated dining patterns noted in Section 5, Figures 1-2).

## Foundational Learning

- **Concept: Micro-F1 vs. Macro-F1**
  - **Why needed here:** The paper reports both metrics because label imbalance is severe (e.g., nausea has 28 samples vs. general sickness with 220). Micro-F1 emphasizes overall correctness; macro-F1 reveals per-label performance gaps.
  - **Quick check question:** If a model achieves 95% micro-F1 but 70% macro-F1 on symptom extraction, what does this indicate about rare symptom detection?

- **Concept: In-context learning (few-shot prompting)**
  - **Why needed here:** The study shows 5-shot prompts generally outperform 0-shot (Table 4), but food extraction is an exception where examples hurt performance—suggesting prompt design sensitivity.
  - **Quick check question:** Why might adding examples degrade performance on some tasks? (Hint: see prompt fragility discussion in Section 4.1.)

- **Concept: Adversarial data filtering**
  - **Why needed here:** The dataset is pre-filtered to reviews containing GI-related keywords, creating a "harder" classification problem (37% positive rate vs. <1% in raw Yelp data). This prevents trivial negative-class accuracy inflation.
  - **Quick check question:** How does keyword pre-filtering change the interpretation of reported F1 scores for real-world deployment?

## Architecture Onboarding

- **Component map:** Yelp Open Dataset -> keyword filter -> manual annotation (GI binary, symptoms, foods) -> train/validation/test splits -> prompt template + few-shot examples -> structured text output -> regex disambiguation -> label mapping -> micro/macro F1 evaluation
- **Critical path:** Prompt design is the highest-leverage component. Table 5 shows prompt fragility—some models vary >30% in F1 across prompt variants. Invest in systematic prompt optimization on validation data before model selection.
- **Design tradeoffs:**
  - **Prompt-only vs. fine-tuning:** Prompting offers faster iteration and better rare-label performance; fine-tuning provides deterministic outputs and lower inference cost. The paper shows prompting wins on extraction tasks where labels are sparse.
  - **Label granularity:** 27 food labels enable fine-grained surveillance but suffer from data sparsity (5 labels have <10 samples). Consolidating to "poultry," "seafood," "produce" may improve reliability at cost of actionability.
  - **Assumption:** Ingredient inference from dish names (e.g., "pad thai" contains eggs, peanuts) is intentionally excluded due to uncertainty—extending this would require external knowledge bases.
- **Failure signatures:**
  - Symptom extraction: Nausea F1 (0.769) notably lower than other symptoms—likely due to colloquial expressions ("queasy," "off") not captured in disambiguation rules.
  - Food extraction: RoBERTa fails to predict rare labels without synthetic data augmentation; even with augmentation, shellfish F1 drops to 0.533.
  - GI classification: High false-positive rate when reviews mention alcohol, allergies, or spicy food without explicit exclusion language.
- **First 3 experiments:**
  1. **Prompt sensitivity analysis:** Test 10+ prompt variants per task on a held-out set, measuring variance. Identify minimal prompt components required for >90% micro-F1.
  2. **Rare-label augmentation:** For food labels with <20 samples, generate synthetic reviews using a separate LLM (as done in Section 3.5) and measure fine-tuning improvement. Compare to few-shot prompting without augmentation.
  3. **Cross-domain transfer:** Apply the best GI classification prompt to a different review platform (e.g., Google Reviews, TripAdvisor subset) without re-annotation. Measure performance drop to assess generalization bounds.

## Open Questions the Paper Calls Out
None

## Limitations
- The annotation schema covers only five symptoms and 27 foods, potentially missing clinically relevant presentations common in foodborne illness outbreaks
- Yelp review data introduces demographic sampling bias, as the platform's user base skews toward certain income levels and urban populations
- Keyword pre-filtering creates selection bias where 37% of reviews contain GI-related terms, dramatically different from the <1% baseline in raw Yelp data

## Confidence
- **High Confidence (8/10):** LLMs outperform fine-tuned RoBERTa on GI illness classification and extraction tasks, as evidenced by consistent F1 score superiority across multiple model sizes and tasks
- **Medium Confidence (6/10):** Expert-designed annotation schema enables structured extraction beyond binary classification, though the novel nature of this approach and limited corpus support introduce uncertainty
- **Medium Confidence (6/10):** LLM-based detection shows minimal demographic bias under social group substitution, but real-world performance across actual demographic groups remains untested

## Next Checks
1. **Schema Expansion Validation:** Conduct a pilot study annotating 500 reviews using an expanded schema (10-15 symptoms, 50-75 foods) to assess whether performance degradation on rare labels justifies the additional complexity for public health surveillance purposes.

2. **Real-World Deployment Test:** Apply the best-performing GI classification prompt to a geographically and demographically diverse subset of Google Reviews (non-Yelp data) from the same restaurant establishments. Measure performance drop and identify whether domain adaptation or prompt modification is needed for broader applicability.

3. **Bias Assessment in Production:** Implement the model in a real-time surveillance system monitoring 10,000+ reviews over 3 months. Track predictions across actual demographic indicators (neighborhood income levels, urban/rural distribution) rather than linguistic proxies, and quantify any systematic over- or under-prediction patterns that could inform public health response inequities.