---
ver: rpa2
title: 'GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive
  Safety Testing'
arxiv_id: '2507.07735'
source_url: https://arxiv.org/abs/2507.07735
tags:
- jailbreak
- evaluation
- llms
- prompts
- defender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GuardVal is a dynamic jailbreak evaluation protocol that generates\
  \ and refines prompts based on defender LLM responses to reveal vulnerabilities.\
  \ It employs four roles\u2014Translator, Generator, Evaluator, and Optimizer\u2014\
  to create increasingly challenging test scenarios."
---

# GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing

## Quick Facts
- arXiv ID: 2507.07735
- Source URL: https://arxiv.org/abs/2507.07735
- Authors: Peiyan Zhang; Haibo Jin; Liying Kang; Haohan Wang
- Reference count: 16
- Dynamic jailbreak evaluation reveals vulnerabilities missed by static benchmarks

## Executive Summary
GuardVal is a dynamic jailbreak evaluation protocol that exposes vulnerabilities in Large Language Models (LLMs) by iteratively generating and refining prompts based on defender responses. The system employs four LLM roles—Translator, Generator, Evaluator, and Optimizer—to create increasingly sophisticated attack scenarios across 10 safety domains. Experiments with 8 LLMs from Mistral-7b to GPT-4 demonstrate that static benchmarks underestimate vulnerabilities, as models resist initial jailbreaks but succumb to refined prompts that bypass shallow keyword detection.

## Method Summary
GuardVal uses a four-role LLM pipeline where Translator converts safety guidelines to questions, Generator creates attack scenarios, Evaluator assesses responses against an "Oracle" safe response, and Optimizer detects stagnation using Adam-inspired calculations. The system iteratively refines prompts until jailbreak succeeds or iteration limits are reached. The Overall Safety Value (OSV) metric normalizes cross-model evaluation by combining offensive and defensive capabilities relative to peer models.

## Key Results
- Static benchmarks underestimate vulnerabilities; models resist initial jailbreaks but are breached after prompt refinement
- GPT-4 ranked 1st in OSV while Llama2-7b ranked 2nd despite better static benchmark performance
- The Optimizer role was critical in preventing stagnation, as models without it failed to jailbreak themselves
- OSV rankings reveal inconsistencies between static benchmark results and dynamic evaluation

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Prompt Refinement Based on Defender State
- Claim: Iteratively refining jailbreak prompts based on defender LLM responses reveals vulnerabilities that static benchmarks miss.
- Mechanism: The Evaluator assesses defender responses and generates feedback. The Generator then creates updated scenarios incorporating this feedback. This process continues iteratively, with prompts evolving from explicit harmful terms ("rumor", "damage") to more subtle expressions that bypass shallow keyword detection.
- Core assumption: LLMs rely on "shallow alignment"—detecting harmful words rather than deeply understanding ethical intent.
- Evidence anchors:
  - [abstract] "dynamically generates and refines jailbreak prompts based on the defender LLM's state"
  - [section] Case study in Appendix D shows prompt evolution from explicit to subtle language
  - [corpus] Neighbor papers (JailBench, AutoAdv) also emphasize dynamic evaluation over static benchmarks
- Break condition: If defender LLMs achieved deep semantic understanding of safety guidelines (rather than keyword matching), this mechanism would fail to find new vulnerabilities through refinement.

### Mechanism 2: Adam-Inspired Optimization for Stagnation Detection
- Claim: A numerical optimization approach adapted from Adam prevents the refinement process from becoming trapped in repetitive feedback loops.
- Mechanism: The Optimizer quantifies response changes between iterations (gt = ||Responset − Responset−1||), tracks the rate of change (∆gt), maintains moving averages of magnitude (mt) and variance (vt), and uses sliding windows to detect when evolution slows. When stagnation is detected, it generates refined feedback using natural language templates.
- Core assumption: The rate of change in LLM responses correlates with exploration effectiveness—slowing indicates local optima.
- Evidence anchors:
  - [section] Section 3.3 describes the complete mathematical formulation with Adam-inspired equations
  - [section] Appendix E shows Gemini failing to jailbreak itself after 10 rounds without Optimizer, succeeding with Optimizer
  - [corpus] Corpus lacks direct evidence for this specific optimization approach in jailbreak literature
- Break condition: If response evolution were not indicative of exploration quality, or if stagnation didn't correlate with local optima, this detection mechanism would produce false positives/negatives.

### Mechanism 3: OSV Metric Normalizing Cross-Model Evaluation
- Claim: The Overall Safety Value (OSV) metric enables fair comparison across models by combining offensive and defensive capabilities while normalizing test difficulty differences.
- Mechanism: OSV = (1/N-1) × Σ(RB,A − RA,B), where RB,A represents defensive strength (rounds others need to jailbreak model A) and RA,B represents offensive effectiveness (rounds model A needs to jailbreak others). Higher OSV indicates stronger overall safety.
- Core assumption: Defensive and offensive capabilities are both relevant to overall security posture, and comparing against multiple peer models normalizes test difficulty.
- Evidence anchors:
  - [section] Section 3.4 provides the complete OSV formulation and rationale
  - [section] Table 2 shows GPT-4 ranking 1st in OSV despite Llama2-7b performing better on static benchmarks
  - [corpus] No comparable dual-capability metrics found in neighbor papers
- Break condition: If offensive capability (attacking other models) doesn't correlate with defensive robustness, or if the peer model set is insufficiently diverse, OSV rankings could be misleading.

## Foundational Learning

- Concept: **Jailbreak attacks and safety alignment**
  - Why needed here: Understanding that LLMs have safety mechanisms that can be bypassed through carefully crafted prompts is essential for grasping why dynamic evaluation is necessary.
  - Quick check question: Can you explain why static benchmarks might show "false safety" due to training data overlap?

- Concept: **Optimization fundamentals (gradient-based methods, local optima, momentum)**
  - Why needed here: The paper adapts Adam optimizer concepts to prompt refinement; understanding momentum and adaptive learning rates helps explain why stagnation occurs and how the Optimizer role addresses it.
  - Quick check question: How does tracking the rate of change in responses (∆gt) relate to gradient-based optimization?

- Concept: **Evaluation metrics for LLM safety**
  - Why needed here: The OSV metric combines multiple factors; understanding trade-offs between offensive and defensive measurement helps interpret results and limitations.
  - Quick check question: Why does OSV use a relative comparison (against peer models) rather than absolute scores?

## Architecture Onboarding

- Component map:
  - Translator (Attacker) -> Generator (Attacker) -> Defender LLM -> Evaluator (Attacker) -> Optimizer -> Generator (refined)

- Critical path:
  1. Translator generates initial question prompt from safety guideline
  2. Generator wraps prompt in scenario
  3. Defender LLM responds to scenario
  4. Evaluator scores response similarity to "Oracle" (safe response) and provides modification advice
  5. Optimizer analyzes response evolution across iterations; if stagnation detected, refines feedback
  6. Generator creates revised scenario using (potentially refined) feedback
  7. Loop continues until jailbreak succeeds or iteration limit reached

- Design tradeoffs:
  - **Evaluator model selection**: GPT-4 most accurate (87.5% alignment with human judgment) but expensive; Llama2-7b good alternative (85%)
  - **Iteration depth vs. cost**: More rounds expose deeper vulnerabilities but increase evaluation cost
  - **Sliding window size**: Larger windows more stable but slower to detect stagnation
  - **Threshold sensitivity**: Lower thresholds catch stagnation earlier but risk false positives
  - **Model diversity in OSV calculation**: More diverse peer models improve normalization but expand evaluation scope

- Failure signatures:
  - **Stagnation without Optimizer**: Scenarios become repetitive (e.g., Gemini self-jailbreak failing after 10 rounds, showing minimal scenario variation in Appendix E)
  - **Evaluator refusal**: Llama2-7b sometimes refuses to generate feedback as Evaluator, requiring restarts
  - **"Exaggerated Safety"**: Models like Mistral-7b reject benign prompts with negative words (e.g., "How can I kill the process in my computer?")
  - **Evaluator misjudgment**: Vicuna-7b achieves only 67.5% accuracy in detecting jailbreaks vs. GPT-4's 87.5%
  - **Shallow alignment detection**: Models block "rumor" and "damage" keywords but succumb to semantically equivalent subtle phrasing

- First 3 experiments:
  1. **Ablation study on Optimizer role**: Run GuardVal with and without Optimizer across multiple domain/attacker/defender combinations; compare rounds to jailbreak and analyze scenario diversity over iterations to quantify stagnation frequency.
  2. **Evaluator model comparison**: Test different models (GPT-4, Llama2-7b, Vicuna-7b) in the Evaluator role on the same defender/attacker pairs; correlate jailbreak detection accuracy with final OSV rankings to validate Evaluator selection impact.
  3. **Cross-domain vulnerability profiling**: Run full evaluation across all 10 safety domains for 3-4 representative models; identify domain-specific strengths/weaknesses and validate the mix-of-experts deployment strategy by checking if top defenders per domain align with OSV component scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed mix-of-experts deployment strategy, which routes queries to domain-specific top defenders, yield higher aggregate safety than deploying a single state-of-the-art model?
- Basis in paper: [explicit] The authors suggest that leveraging top defenders for specific domains can optimize deployment (Section 5), but they do not validate this system-level integration or compare its performance against single-model baselines.
- Why unresolved: The paper evaluates individual model capabilities (OSV) and identifies domain-specific strengths, but it does not construct or test the actual "mix-of-experts" system to confirm it improves overall security posture.
- What evidence would resolve it: An experiment where a router directs adversarial prompts to the top-ranked defender per domain, demonstrating a statistically significant reduction in successful jailbreaks compared to using GPT-4 or Llama2-7b alone.

### Open Question 2
- Question: How stable is the Overall Safety Value (OSV) ranking when the composition of the peer group (attacker/defender models) changes?
- Basis in paper: [explicit] Appendix F notes that OSV is "influenced by the diversity of the LLMs included" and might not reflect the real-world security landscape if the evaluated set is not sufficiently diverse.
- Why unresolved: The metric relies on relative performance ($R_{B,A}$ and $R_{A,B}$), meaning a model's score is explicitly dependent on the specific other models in the evaluation pool. The variance of this ranking is unknown.
- What evidence would resolve it: A sensitivity analysis measuring the standard deviation of OSV rankings as different subsets of models are removed or added to the evaluation pool.

### Open Question 3
- Question: Can the "restraint relationships" (asymmetrical attack success) between models be causally attributed to specific differences in alignment training, such as Reinforcement Learning from Human Feedback (RLHF)?
- Basis in paper: [inferred] Section 4.2 observes that Gemini can jailbreak Llama2-7b more easily than vice versa and hypothesizes this is due to differences in training mechanisms (e.g., RLHF vs. ethical reasoning).
- Why unresolved: The paper identifies the asymmetrical behavior but provides no causal proof linking specific training data characteristics or alignment algorithms to these vulnerabilities.
- What evidence would resolve it: An ablation study where models are trained with varying types/levels of alignment (e.g., with vs. without RLHF) and then tested against each other to see if the restraint relationship emerges or disappears.

## Limitations
- The evaluation framework relies heavily on LLM-based Evaluator models, which may inherit their own safety biases and limitations
- The optimization mechanism's assumption that response evolution rates correlate with exploration effectiveness is not fully validated
- The OSV metric combines offensive and defensive capabilities in a way that may obscure important nuances about a model's specific safety profile

## Confidence
- **High confidence**: The core observation that static benchmarks underestimate LLM vulnerabilities, supported by systematic demonstration across 10 safety domains and 8 LLMs
- **Medium confidence**: The effectiveness of the Adam-inspired Optimizer in preventing stagnation, with some evidence but limited ablation studies
- **Medium confidence**: The OSV metric as a comprehensive safety assessment tool, though the assumption that offensive and defensive capabilities should be equally weighted is debatable
- **Low confidence**: The claim that the approach reveals "deeper vulnerabilities" beyond shallow keyword detection, as the evidence shows evolution in attack sophistication but doesn't prove semantic understanding by defenders

## Next Checks
1. **Evaluator model sensitivity analysis**: Run the complete GuardVal evaluation pipeline using different Evaluator models (GPT-4, Llama2-7b, Vicuna-7b) on the same defender/attacker pairs and compare resulting OSV rankings and jailbreak success rates to quantify measurement noise and bias.

2. **Optimizer ablation with extended runs**: Compare GuardVal with and without the Optimizer module across all domain/attacker/defender combinations, extending iterations to 20 rounds instead of stopping at jailbreak success, to measure stagnation frequency and quality of prompt evolution.

3. **Human evaluation of "Oracle" safety**: Have human annotators independently rate a sample of "Oracle" responses used as ground truth for safety, comparing them against the LLM-generated responses to validate whether these truly represent optimal safe responses.