---
ver: rpa2
title: Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations
arxiv_id: '2510.08120'
source_url: https://arxiv.org/abs/2510.08120
tags:
- glove
- llm-as-a-judge
- explanations
- concepts
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of interpreting LLM-as-a-Judge
  policies, which are increasingly used for automated evaluation but lack transparency.
  The authors propose a two-stage method: CLoVE generates contrastive, concept-based
  local explanations in a BECAUSE-DESPITE format, while GloVE summarizes these into
  a global policy via iterative clustering, summarization, and verification.'
---

# Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations

## Quick Facts
- arXiv ID: 2510.08120
- Source URL: https://arxiv.org/abs/2510.08120
- Reference count: 12
- Primary result: GloVE explanations achieve high fidelity to LLM-as-a-Judge decisions (accuracy 74-99%, F1 52-97%) with minimal performance degradation (0-0.1) and outperform baselines in robustness and user satisfaction.

## Executive Summary
This paper addresses the challenge of interpreting LLM-as-a-Judge policies, which are increasingly used for automated evaluation but lack transparency. The authors propose a two-stage method: CLoVE generates contrastive, concept-based local explanations in a BECAUSE-DESPITE format, while GloVE summarizes these into a global policy via iterative clustering, summarization, and verification. Evaluation on seven harm detection datasets shows that GloVE explanations achieve high fidelity to the judge's decisions with minimal performance degradation. The method demonstrates robustness to text paraphrasing and adversarial attacks, with GloVE outperforming a baseline approach in both quantitative metrics and user satisfaction. A user study found GloVE explanations led to higher perceived usefulness compared to alternative approaches.

## Method Summary
The method consists of two main components: CLoVE and GloVE. CLoVE generates local explanations by using a Generator LLM to propose concepts, LIME to identify influential tokens, and a Verifier LLM to filter concepts based on lexical support. GloVE then constructs a K-partite graph from these local explanations, iteratively clusters concepts, generates summary labels using an LLM, verifies entailment with FactReasoner, and extracts contrastive global rules. The process ensures faithfulness by grounding concepts in token attributions and verifying summary labels entail the clustered concepts.

## Key Results
- GloVE explanations achieve accuracy 74-99% and F1 52-97% when compared to LLM-as-a-Judge decisions across seven harm detection datasets
- Performance degradation is minimal (0-0.1) when using GloVE to approximate the judge
- GloVE outperforms baseline approaches in both quantitative metrics and user satisfaction in robustness tests
- User study shows GloVE explanations are perceived as more useful than alternatives, though comprehension remains challenging

## Why This Works (Mechanism)

### Mechanism 1: Verification-Grounded Concept Generation
CLoVE decouples explanation generation from validation. A Generator LLM proposes concepts, while LIME identifies influential tokens. A Verifier LLM acts as gatekeeper, retaining only concepts supported by the influential tokens. This ensures generated concepts are grounded in the model's actual reasoning rather than hallucinated rationales.

### Mechanism 2: Contrastive K-Partite Policy Aggregation
GloVE maps local rules into a K-partite graph where nodes are concepts and partitions are decision classes. Concepts supporting a decision are nodes in one partition, with arcs connecting concepts appearing in the same rule. This structure preserves contrastive relationships (BECAUSE vs. DESPITE) during summarization.

### Mechanism 3: Entailment-Based Graph Compression
GloVE iteratively clusters semantically similar concepts and uses an LLM to propose summary labels. FactReasoner calculates entailment scores, and only concepts strictly entailed by the new label are merged. This ensures the compressed global rule is a logical superset of local rules, preserving fidelity.

## Foundational Learning

- **Feature Attribution (LIME/SHAP)**: CLoVE relies on LIME to identify "important words" to ground concepts. Understanding how LIME perturbs text to assign weights is essential to debugging concept acceptance/rejection.
  - Quick check: If LIME assigns high weight to a neutral stop word, how would that impact the Verifier's output in CLoVE?

- **K-Partite Graphs**: The global explanation structure is explicitly defined as a K-partite graph. Understanding that nodes are partitioned by decision class is essential to visualizing how GloVE tracks "supporting" vs "conflicting" evidence.
  - Quick check: In binary classification (Harmful vs. Safe), how many partitions does the explanation graph have, and what does an arc between the two partitions represent?

- **Entailment vs. Clustering**: GloVE iterates between clustering (grouping similar concepts) and entailment (verifying the summary label covers the group). Distinguishing semantic similarity from logical entailment is key to understanding why FactReasoner is used rather than just embedding distance.
  - Quick check: Why is high cosine similarity between two concepts insufficient to merge them into a global rule without the entailment check?

## Architecture Onboarding

- **Component map**: Generator (G) → LIME explainer (L) → Verifier (V) → Graph Builder → FactReasoner → Global Summarizer
- **Critical path**: CLoVE generation → Graph Construction → Iterative Clustering → FactReasoner Verification → Rule Extraction
- **Design tradeoffs**: Fidelity vs. Interpretability tradeoff noted; pipeline complexity increases latency but reduces hallucination risk compared to single-model self-explanation
- **Failure signatures**: Empty concept sets if Verifier too strict; over-generalization if FactReasoner threshold too low; overly dense graphs if local explanations inconsistent
- **First 3 experiments**:
  1. Unit Test Verifier: Run CLoVE on hand-crafted example and verify if Verifier correctly filters concepts based on LIME tokens
  2. Ablation Reproduction: Re-run GloVE_NoFR experiment on small subset to observe F1 degradation
  3. Paraphrase Robustness: Apply HIDE/ELABORATE/SUBSTITUTE strategies to 10 samples and measure stability of extracted global rules

## Open Questions the Paper Calls Out
- How can GloVE explanations be simplified to improve user comprehension without significantly sacrificing fidelity to the LLM-as-a-Judge?
- Do CLoVE and GloVE generalize effectively to LLM-as-a-Judge tasks beyond binary harm detection?
- To what extent does the choice of Generator and Verifier LLMs introduce bias into the final global policy?

## Limitations
- The method's quality is limited by the quality of individual components, which can inherit issues like bias from their implementation
- User study methodology is limited with only 12 participants evaluating explanations on a single dataset
- The FactReasoner component is not described in detail, making it difficult to assess its reliability or potential biases

## Confidence
- **High Confidence**: Fidelity metrics (accuracy 74-99%, F1 52-97%) are well-supported by quantitative results across seven datasets
- **Medium Confidence**: CLoVE generates more faithful local explanations than single-model approaches, but specific contribution of each verification step is not fully isolated
- **Low Confidence**: Assertion that GloVE explanations are more useful to users than alternatives is based on small-scale user study (n=12) on single dataset

## Next Checks
1. Design experiment to measure individual contribution of Verifier V in CLoVE by comparing concept acceptance rates with and without verification
2. Implement simplified, transparent version of entailment check and compare performance to original FactReasoner
3. Replicate user study with larger and more diverse participant pool across multiple datasets to evaluate generalizability of perceived usefulness