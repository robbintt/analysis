---
ver: rpa2
title: Efficient kernelized bandit algorithms via exploration distributions
arxiv_id: '2506.10091'
source_url: https://arxiv.org/abs/2506.10091
tags:
- regret
- exploration
- algorithms
- bounds
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GP-Generic, a flexible framework for kernelized
  bandit algorithms based on a novel concept called exploration distributions. The
  framework allows various randomized algorithms to be expressed as instantiations
  of GP-Generic by carefully choosing exploration distributions that control the exploitation-exploration
  tradeoff.
---

# Efficient kernelized bandit algorithms via exploration distributions

## Quick Facts
- arXiv ID: 2506.10091
- Source URL: https://arxiv.org/abs/2506.10091
- Authors: Bingshan Hu; Zheng He; Danica J. Sutherland
- Reference count: 40
- Key outcome: GP-Generic framework enables kernelized bandit algorithms with O(γ_T√T) regret by sampling scalar exploration weights, achieving performance competitive with or better than IGP-UCB and GP-TS

## Executive Summary
This paper introduces GP-Generic, a novel framework for kernelized bandit algorithms that unifies various randomized approaches through the concept of exploration distributions. By sampling scalar weights to modulate uncertainty estimates rather than directly sampling functions from posterior distributions, the framework enables the use of traditional one-dimensional concentration bounds while maintaining optimal regret guarantees. The authors demonstrate that several concrete algorithms including Simple-UCB, Simple-Gaussian, and Simple-Bernoulli can be instantiated within this framework, all achieving O(γ_T√T) regret bounds that match or improve upon state-of-the-art methods.

## Method Summary
GP-Generic constructs a surrogate function by adding a weighted uncertainty term to an empirical reward estimate: f̃_t(x) = f̂_{t-1}(x) + w_t · g_{t-1}(x). Here f̂_{t-1} is a ridge regression estimate based on observed data, g_{t-1} is an uncertainty function derived from kernel matrix properties, and w_t is a scalar sampled from an exploration distribution P_{w,t}. The algorithm selects arms by maximizing this surrogate function. The key innovation is that the exploration distribution satisfies three conditions ensuring sufficient optimism while controlling over- and under-exploration regret. This allows the use of traditional concentration inequalities in scalar space rather than high-dimensional RKHS, simplifying analysis while achieving O(γ_T√T) regret bounds.

## Key Results
- GP-Generic framework unifies multiple kernelized bandit algorithms through exploration distributions
- Simple-UCB, Simple-Gaussian, and Simple-Bernoulli achieve O(γ_T√T) regret bounds matching state-of-the-art methods
- Experimental results show these algorithms outperform IGP-UCB and GP-TS on synthetic functions and real-world data
- The framework enables constant failure probability δ = 0.5 (unlike typical δ ~ 1/T in UCB methods) due to inherent exploration from w_t

## Why This Works (Mechanism)

### Mechanism 1
Exploration distributions enable applying concentration bounds in 1D scalar space rather than high-dimensional RKHS, simplifying regret analysis while matching state-of-the-art bounds. The algorithm samples a scalar weight w_t from exploration distribution P_{w,t} and applies it to the uncertainty function g_{t-1} to construct f̃_t = f̂_{t-1} + w_t g_{t-1}. Since w_t is a scalar independent random variable, traditional sub-Gaussian concentration (via maximal inequality) applies to max_t |w_t| rather than requiring vector-valued martingale bounds over the entire RKHS. The core assumption is that the exploration weight w_t is sampled independently of the current round's arm selection given history, enabling the 1D reduction.

### Mechanism 2
The three distribution conditions (C1,t, C2,t, C3,t) jointly guarantee sufficient optimism while bounding over- and under-exploration regret. Condition C_{1,t} = P(w_t ≥ 1) ≠ 0 ensures positive probability of selecting optimistic upper bounds (exploration). C_{2,t} = E[max_{s ≤ t} |w_s|] controls regret from excessive exploration bonuses. C_{3,t} bounds regret from insufficient exploration by measuring how exploration weights scale relative to optimism probability. The core assumption is that the exploration distribution can be chosen adaptively based on history but w_1, ..., w_t remain independent across rounds.

### Mechanism 3
Using constant failure probability δ = 0.5 for self-normalized bounds is sufficient because the randomized w_t provides inherent exploration. Unlike UCB algorithms that require δ ~ 1/T for high-probability confidence bounds, GP-Generic sets δ = 0.5 in Theorem 1's self-normalized bound. Even when the concentration event fails, w_t's randomness ensures continued exploration with probability C_{1,t} > 0. The core assumption is that the exploration distribution has sufficient mass at or above 1 (i.e., C_{1,t} > 0) to provide backup exploration when concentration fails.

## Foundational Learning

- Concept: **Reproducing Kernel Hilbert Space (RKHS)**
  - Why needed here: The reward function f* lives in an RKHS with bounded norm ||f|| ≤ D. Understanding that RKHS norm measures smoothness relative to kernel K, and that kernel complexity γ_T characterizes effective dimension, is essential for interpreting regret bounds.
  - Quick check question: For a Matérn kernel with smoothness ν in dimension d, what is the scaling of γ_T with T?

- Concept: **Sub-Gaussian random variables and concentration inequalities**
  - Why needed here: Noise ε_t is R-sub-Gaussian. The maximal inequality (Fact 1) bounds E[max |w_t|] for sub-Gaussian exploration distributions, directly controlling C_{2,T}.
  - Quick check question: If X_1, ..., X_n are each σ-sub-Gaussian, what is the bound on E[max_i |X_i|]?

- Concept: **Regret decomposition and self-normalized bounds**
  - Why needed here: The proof decomposes regret into estimation error (I3), approximation error (I2), and optimism gap (I1), each bounded separately. Self-normalized martingale bounds (Theorem 1) control the error term.
  - Quick check question: In the decomposition R(T) = Σ_t E[(f*(x*) - f*(X_t))1{E_{t-1}}] + Σ_t E[(·)1{E_{t-1}}], why is the second term O(1)?

## Architecture Onboarding

- Component map: Ridge regression estimator -> Uncertainty function -> Exploration distribution selector -> Surrogate function -> Arm selector
- Critical path: 1. Update V_t with new observation (X_t, Y_t) 2. Select exploration distribution P_{w,t+1} based on F_{t-1} 3. Sample w_{t+1} ~ P_{w,t+1} 4. Compute f̂_t and g_t for all candidate arms 5. Select X_{t+1} = argmax_x f̃_{t+1}(x) 6. Observe Y_{t+1} and repeat
- Design tradeoffs:
  - Simple-UCB (Ber(1)): C_{3,T} = 2 is optimal, but C_{2,T} = 1 is fixed—no randomness reduces empirical variance
  - Simple-Gaussian (N(0,1)): Adds exploration variance; C_{3,T} ∝ log(T)^{1/2} slightly worse theoretically but empirically better
  - Simple-Bernoulli (Ber(p_t)): Regret scales as 1/min_t p_t; adaptive p_t schedules can reduce exploration as uncertainty decreases
  - Computation vs. regret: Arm elimination algorithms (SupKernelUCB, REDS) achieve optimal O(√Tγ_T) but require expensive discretization; GP-Generic matches O(γ_T√T) with per-round optimization only
- Failure signatures:
  - Regret not decreasing: Check that C_{1,t} > 0 (exploration distribution has mass at w ≥ 1); pure exploitation (p_t = 0) causes linear regret
  - Excessive variance in arm selection: C_{2,T} may be too large (over-exploration); reduce exploration distribution variance or use Ber(1) for deterministic behavior
  - Kernel matrix ill-conditioning: V_{t-1} may have numerical issues; ensure regularization parameter ρ > 0 in V_t = I_H + V_t
  - Regret bound mismatch: Verify kernel complexity γ_T matches kernel choice (Matérn: O(T^{d/(2ν+d)}log T); SE: O(log^{d+1} T))
- First 3 experiments:
  1. Baseline comparison on synthetic functions: Implement Simple-UCB, Simple-Gaussian, and Simple-Bernoulli on Holder Table (d=2) and Ackley (d=4) with T=1000 rounds; compare cumulative regret against IGP-UCB and GP-TS. Expected: Simple-Bernoulli with adaptive p_t should achieve lowest regret per Figure 1.
  2. Exploration distribution ablation: Fix Ber(p) with p ∈ {0.25, 0.5, 0.75, 1.0} across all rounds; measure how regret scales with 1/p to verify Theorem 7's dependence on min_t p_t.
  3. Kernel complexity scaling: Test on Matérn kernels with varying smoothness ν ∈ {0.5, 1.5, 2.5} and dimension d ∈ {2, 4, 6}; verify that regret scales as γ_T√T and that γ_T decreases with larger ν (smoother functions).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the GP-Generic framework be extended to provide high-probability regret guarantees rather than expected regret bounds?
- Basis in paper: [explicit] The authors state, "Regarding future research, we plan to investigate the possibility to develop a generic algorithm with high probability regret guarantees."
- Why unresolved: The current theoretical analysis focuses on pseudo-regret (expected value), and it is not immediately clear if the concentration bounds used for the exploration distributions hold with high probability under the same conditions.
- What evidence would resolve it: A theoretical proof showing that GP-Generic achieves O(γ_T√T) regret with probability at least 1-δ for any δ ∈ (0,1).

### Open Question 2
- Question: Is it possible to achieve the optimal O(√Tγ_T) regret bound using data-dependent exploration distributions, or is the Õ(γ_T√T) bound a fundamental limit of this class?
- Basis in paper: [explicit] The paper notes, "As discussed by Lattimore [23], we think that optimal O(√Tγ_T) regret bounds may not be obtainable if in each round t, the pulled arm X_t is distributed according to a data-dependent distribution."
- Why unresolved: While the proposed framework matches state-of-the-art UCB/TS bounds, it does not match the lower bound achieved by arm-elimination methods. It is unclear if the dependence structure inherent to data-dependent sampling prevents tighter bounds.
- What evidence would resolve it: A lower bound proof specifically for data-dependent algorithms showing that Õ(γ_T√T) is the best achievable rate, or a new instantiation of GP-Generic that achieves O(√Tγ_T).

### Open Question 3
- Question: How can the GP-Generic framework be adapted to handle time-varying reward functions in kernelized bandits?
- Basis in paper: [explicit] The authors list "Time-varying kernelized bandits" as "an interesting extension" for future research.
- Why unresolved: The current algorithm assumes a fixed unknown reward function f* with finite RKHS norm. A time-varying f* would likely require modifying the exploration distribution conditions (specifically how C_{1,t} and C_{3,t} interact with non-stationarity) to balance tracking the change versus exploring.
- What evidence would resolve it: A modification of Algorithm 1 and Theorem 2 that accounts for a time-varying budget (e.g., total variation) and maintains sub-linear regret in a non-stationary environment.

## Limitations

- The theoretical analysis relies on maintaining independence between exploration weights and arm selection, which may not hold in highly adaptive settings
- Experimental validation is limited to synthetic functions and a single real-world dataset, raising questions about generalizability to high-dimensional or noisy real-world scenarios
- The framework does not achieve the optimal O(√Tγ_T) regret bound achieved by arm-elimination methods, representing a fundamental limitation of data-dependent exploration

## Confidence

- **High confidence**: The theoretical regret bound O(γ_T√T) for GP-Generic and its instantiation in Simple-UCB, Simple-Gaussian, and Simple-Bernoulli algorithms, as these follow directly from the three distribution conditions and established self-normalized concentration bounds.
- **Medium confidence**: The claim that constant failure probability δ = 0.5 suffices due to inherent exploration from w_t, as this relies on the assumption that C_{1,t} > 0 persists throughout learning.
- **Medium confidence**: The empirical superiority of Simple-Gaussian over IGP-UCB and GP-TS, given limited experimental scope (three synthetic functions, one dataset) and absence of ablation studies on hyperparameters like RKHS norm bound D.

## Next Checks

1. Test GP-Generic with kernels of varying complexity (Matérn with different smoothness parameters) on benchmark optimization problems to verify that regret scales as predicted with γ_T.
2. Conduct sensitivity analysis on the RKHS norm bound D parameter to determine its impact on regret and algorithm stability.
3. Implement an adaptive Simple-Bernoulli variant where p_t decreases with observed uncertainty to test whether this can achieve both low regret and reduced variance compared to fixed p_t schedules.