---
ver: rpa2
title: 'LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer''s Disease
  Classification'
arxiv_id: '2601.00877'
source_url: https://arxiv.org/abs/2601.00877
tags:
- learning
- brain
- features
- data
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LearnAD, a neuro-symbolic approach for interpretable\
  \ Alzheimer\u2019s disease classification from brain MRI data. The method combines\
  \ statistical machine learning (Decision Trees, Random Forests, or Graph Neural\
  \ Networks) with symbolic learning using FastLAS to identify relevant brain connections\
  \ and learn globally interpretable rules."
---

# LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification

## Quick Facts
- **arXiv ID**: 2601.00877
- **Source URL**: https://arxiv.org/abs/2601.00877
- **Reference count**: 40
- **Primary Result**: LearnAD combines statistical ML with symbolic learning to identify interpretable AD biomarkers while maintaining classification accuracy comparable to black-box models

## Executive Summary
LearnAD introduces a neuro-symbolic approach for Alzheimer's disease classification from brain MRI data that prioritizes interpretability without sacrificing accuracy. The method learns globally interpretable rules by combining statistical machine learning (Decision Trees, Random Forests, or Graph Neural Networks) with symbolic learning using FastLAS. By identifying relevant brain connections and encoding them as logical rules, LearnAD produces transparent decision-making processes that clinicians can understand and validate.

The approach demonstrates that interpretability and performance need not be mutually exclusive in medical imaging classification. The best-performing LearnAD instance, using features from a Decision Tree, outperforms its baseline while matching Support Vector Machine accuracy and performing slightly below Random Forests and GNNs trained on all features. Crucially, the learned rules consistently highlight specific brain connections as key biomarkers, providing actionable insights for clinical interpretation.

## Method Summary
LearnAD employs a hybrid architecture that integrates statistical machine learning with neuro-symbolic reasoning. The pipeline begins with standard brain network construction from MRI data, extracting features from connectivity matrices. These features are then processed through either Decision Trees, Random Forests, or Graph Neural Networks to identify relevant brain connections. The identified connections are subsequently translated into logical rules using FastLAS, a state-of-the-art inductive logic programming system. This two-stage process ensures that the final model produces globally interpretable rules while maintaining competitive classification performance. The framework is designed to be flexible, allowing different statistical models to be swapped in depending on the specific requirements of interpretability versus accuracy.

## Key Results
- LearnAD with Decision Tree features outperforms the baseline Decision Tree while matching SVM accuracy
- The approach performs slightly below Random Forests and GNNs trained on all features, demonstrating an interpretability-accuracy trade-off
- Ablation studies confirm improved interpretability with comparable performance to pure statistical models
- Learned rules consistently identify left temporal pole-left hippocampus and right precuneus-right superior parietal connections as key AD biomarkers

## Why This Works (Mechanism)
LearnAD leverages the complementary strengths of statistical learning for pattern recognition and symbolic reasoning for interpretability. Statistical models excel at identifying complex patterns in high-dimensional brain connectivity data, while symbolic learning translates these patterns into human-readable logical rules. This neuro-symbolic integration allows the model to maintain the predictive power of statistical methods while producing explanations that domain experts can understand and validate. The FastLAS component ensures that the learned rules are both accurate and concise, avoiding the complexity that often plagues symbolic models.

## Foundational Learning
- **Brain Network Construction**: Converting MRI data into connectivity matrices; needed for representing structural relationships, quick check: verify parcellation scheme and network density
- **Statistical Classification Models**: Decision Trees, Random Forests, GNNs; needed for pattern recognition in high-dimensional features, quick check: validate feature importance consistency
- **Inductive Logic Programming**: Using FastLAS for rule learning; needed for translating statistical patterns into logical rules, quick check: verify rule coverage and accuracy
- **Neuro-Symbolic Integration**: Combining statistical and symbolic approaches; needed for balancing accuracy with interpretability, quick check: assess performance drop from pure statistical models
- **Feature Selection**: Identifying relevant brain connections; needed for reducing noise and improving interpretability, quick check: validate selected features against domain knowledge

## Architecture Onboarding

**Component Map**: MRI Data -> Brain Network Construction -> Statistical Model (DT/RF/GNN) -> Feature Selection -> FastLAS Rule Learning -> Interpretable AD Classifier

**Critical Path**: The most time-consuming component is brain network construction and feature extraction from MRI data, which can take several hours depending on image resolution and preprocessing steps. The FastLAS rule learning is relatively efficient once relevant features are identified.

**Design Tradeoffs**: The primary tradeoff is between interpretability and accuracy. Using all features with Random Forests or GNNs provides higher accuracy but produces black-box models, while LearnAD's selective feature approach sacrifices some accuracy for full interpretability. The choice of statistical model also affects this balance.

**Failure Signatures**: Poor performance may indicate issues with brain network construction quality, inappropriate feature selection, or FastLAS failing to find meaningful patterns in the selected features. Inconsistent rule learning across multiple runs suggests instability in the statistical model's feature importance rankings.

**First 3 Experiments**:
1. Validate brain network construction by comparing connectivity matrices across subjects and ensuring expected anatomical patterns
2. Test FastLAS rule learning on synthetic data with known patterns to verify the symbolic component functions correctly
3. Perform ablation studies by removing the symbolic learning component to quantify the interpretability-accuracy tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Performance trade-off exists between interpretability and accuracy compared to state-of-the-art black-box models
- Limited to specific brain network representations that may not capture full AD pathology complexity
- Generalizability across different populations and imaging protocols remains untested

## Confidence

**High Confidence**: Neuro-symbolic methodology implementation using FastLAS for rule learning is well-established and reproducible. Consistent identification of specific brain connections as biomarkers is supported by multiple runs and ablation studies.

**Medium Confidence**: Performance comparisons with SVM, RF, and GNN baselines are valid but may be dataset-dependent. "Full interpretability" claim should be qualified as "global interpretability" requiring domain expertise.

**Low Confidence**: Generalizability of learned rules across different populations and imaging protocols remains untested. Potential biases in training data and model robustness to MRI acquisition variations are not addressed.

## Next Checks
1. Test LearnAD framework on external AD datasets with different imaging protocols and demographic distributions to assess generalizability
2. Conduct detailed error analysis comparing misclassifications between interpretable LearnAD model and black-box models to quantify practical benefits of interpretability
3. Evaluate learned rules' consistency across different brain parcellation schemes and network construction methods to ensure biomarkers are not artifacts of specific preprocessing pipeline