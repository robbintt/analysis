---
ver: rpa2
title: 'Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation'
arxiv_id: '2505.21190'
source_url: https://arxiv.org/abs/2505.21190
tags:
- reports
- temporal
- report
- score
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LUNGUAGE, a benchmark for structured and sequential
  chest X-ray report interpretation. It addresses the limitations of existing evaluation
  methods that fail to capture fine-grained clinical semantics and temporal dependencies
  across multiple studies.
---

# Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation

## Quick Facts
- **arXiv ID:** 2505.21190
- **Source URL:** https://arxiv.org/abs/2505.21190
- **Reference count:** 40
- **Primary result:** LUNGUAGE benchmark introduces 1,473 annotated chest X-ray reports with structured evaluation metric achieving 0.94 F1 for entity-relation extraction and 0.86 for full triplets.

## Executive Summary
LUNGUAGE addresses the critical gap in evaluating radiology report generation systems by introducing the first benchmark specifically designed for structured and sequential chest X-ray interpretation. The benchmark contains 1,473 annotated reports with 17,949 expert-validated entities and 23,307 relation-attribute pairs, including 80 longitudinal reports tracking disease progression across 10 patients. The authors develop a two-stage LLM-based framework that transforms free-text reports into structured representations, achieving strong performance on both single-report and sequential evaluation tasks. The LUNGUAGESCORE metric provides clinically-grounded evaluation by decomposing similarity into semantic, temporal, and structural components while modeling temporal consistency across studies.

## Method Summary
The LUNGUAGE framework uses a two-stage LLM-based approach: first, vocabulary-guided extraction converts free-text reports into structured entity-relation-attribute triplets using span-matching against a curated vocabulary (1,808 entities, 2,193 relation attributes); second, sequential grouping identifies Entity Groups (semantically equivalent findings across time) and Temporal Groups (distinct clinical episodes) using LLM-guided semantic similarity and anatomical alignment. The LUNGUAGESCORE metric evaluates structured outputs by computing pairwise similarity between predicted and reference findings using clinical BERT embeddings (MedCPT + BioLORD) for semantic alignment, study timepoint and Temporal Group matching for temporal coherence, and weighted attribute-level comparison for structural fidelity, with bipartite matching handling one-to-many correspondences.

## Key Results
- Single-report structuring achieves F1 scores of 0.94 for entity-relation extraction and 0.86 for full triplets
- Sequential grouping achieves Entity Group F1 of 0.68 and Temporal Group F1 of 0.89
- LUNGUAGESCORE correlates strongly with radiologist assessments (Kendall Tau -0.58, Pearson -0.69) on ReXVal dataset
- Vocabulary-guided LLM extraction improves triplet-level F1 from 0.52 to 0.78 in zero-shot setting

## Why This Works (Mechanism)

### Mechanism 1
Vocabulary-guided LLM extraction reduces hallucination while preserving flexibility for unseen clinical expressions. A span-matching algorithm anchors entity recognition to a curated schema-defined vocabulary, constraining the LLM's output space while allowing context-sensitive inference for terms outside the vocabulary. The LLM receives matched spans as cues but retains responsibility for final relation extraction. Core assumption: most clinically relevant entities can be captured by a finite, expert-validated vocabulary, and deviation signals potential error. Evidence: vocabulary guidance raised triplet-level F1 from 0.52 to 0.78 in zero-shot setting.

### Mechanism 2
Entity Groups and Temporal Groups enable longitudinal reasoning by normalizing lexical variation and segmenting disease episodes. Entity Groups unify semantically equivalent findings across time using LLM-guided semantic similarity and anatomical alignment. Temporal Groups then partition each Entity Group into distinct clinical episodes based on time intervals, status changes, and explicit progression language. Core assumption: the same underlying clinical finding can be identified across reports despite surface-level lexical variation, and temporal discontinuities represent meaningful diagnostic boundaries. Evidence: 80 sequential reports from 10 patients capture disease progression and inter-study intervals.

### Mechanism 3
LUNGUAGESCORE provides clinically grounded evaluation by decomposing similarity into semantic, temporal, and structural components with partial credit. The metric computes pairwise similarity using cosine similarity of clinical BERT embeddings for semantic alignment, study timepoint and Temporal Group matching for temporal coherence, and weighted attribute-level comparison for structural fidelity. Bipartite matching with partial credit handles one-to-many and many-to-one correspondences. Core assumption: clinical report quality can be meaningfully quantified as the product of independent semantic, temporal, and structural dimensions. Evidence: achieves Kendall Tau -0.58 and Pearson -0.69 correlation with radiologist assessments.

## Foundational Learning

- **Concept: Entity-Relation-Attribute Triplet Extraction**
  - Why needed here: The entire structuring framework depends on decomposing free-text into (entity, relation, attribute) triplets aligned with a clinical schema.
  - Quick check question: Given "2.5 cm right upper lobe nodule with spiculated margins," can you identify the entity, at least three relations, and their corresponding attributes?

- **Concept: Bipartite Matching with Partial Credit**
  - Why needed here: LUNGUAGESCORE uses optimal bipartite matching to align predicted and reference findings, assigning partial credit for imperfect matches rather than binary correct/incorrect.
  - Quick check question: If you have 3 predicted findings and 5 reference findings, how does partial credit prevent the metric from being dominated by unmatched findings?

- **Concept: Clinical Text Embeddings**
  - Why needed here: Semantic similarity relies on MedCPT and BioLORD embeddings to capture equivalence between lexically divergent clinical expressions.
  - Quick check question: Why would general-purpose BERT embeddings fail to recognize that "no acute cardiopulmonary process" and "no acute disease" are semantically similar in a radiology context?

## Architecture Onboarding

- **Component map:** Raw report → Vocabulary Matcher → Single-Report LLM → Structured triplets → (if sequential) Grouping LLM → Entity/Temporal Groups → LUNGUAGESCORE vs. gold standard

- **Critical path:** The vocabulary matcher identifies candidate entities using exact span matching, the single-report LLM extracts triplets using few-shot prompting, and for sequential reports, the grouping LLM performs Entity and Temporal Grouping before final evaluation.

- **Design tradeoffs:** Vocabulary-guided extraction improves precision but may miss novel expressions not in vocabulary; strict Entity Grouping criteria reduce false positives but may split semantically related findings; equal weighting of temporal components (0.5 study, 0.5 Temporal Group) may not reflect all clinical scenarios.

- **Failure signatures:** Low triplet F1 with high entity-relation F1 indicates attribute extraction failure; high semantic score but low total score indicates temporal or structural mismatch; zero-shot sequential grouping produces invalid outputs when LLM doesn't comply with grouping principles.

- **First 3 experiments:**
  1. Ablate vocabulary matching: Run single-report extraction with and without vocabulary guidance on 50 held-out reports; measure precision/recall tradeoff.
  2. Stress-test temporal coherence: Manually invert temporal descriptors (e.g., "improved" → "worsened") in 8 patient sequences; measure Effect Rate per flipped attribute.
  3. Correlate with existing metrics: Compute LUNGUAGESCORE, GREEN, FineRadScore, and RaTEScore on ReXVal; verify competitive correlation with expert error counts while providing component-level interpretability.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does LUNGUAGE SCORE performance scale when expanding the sequential dataset beyond the current 10-patient, 80-report benchmark? The limited sample size constrains statistical power and diversity of disease trajectories, making it unclear whether the temporal grouping methodology generalizes across broader clinical populations.

- **Open Question 2:** To what extent does multi-radiologist cross-validation affect the reliability of Entity Group and Temporal Group annotations? Current annotations were divided among four radiologists without overlap, leaving inter-annotator agreement for longitudinal reasoning tasks unquantified.

- **Open Question 3:** How can the structuring framework better handle complex temporal relationships that currently cause entity grouping errors? The model sometimes splits entities that should be grouped or fails to incorporate temporal continuity cues when constructing entity associations.

## Limitations

- Vocabulary-guided extraction may miss clinically important findings not captured in the curated schema, particularly for rare findings or novel imaging modalities
- Sequential grouping mechanism relies on LLM-guided semantic similarity that may not generalize to reports with ambiguous anatomy or overlapping terminology
- LUNGUAGESCORE metric depends on embedding models that may not capture all clinically relevant synonymy across diverse clinical contexts

## Confidence

- **High Confidence:** Entity-relation extraction F1 scores (0.94), vocabulary-guided extraction mechanism, basic LUNGUAGESCORE component calculations
- **Medium Confidence:** Temporal Group F1 scores (0.89), sequential grouping mechanism, embedding-based semantic similarity, overall LUNGUAGESCORE correlation with radiologist assessments
- **Low Confidence:** Generalizability to non-CXR modalities, handling of rare or novel findings outside vocabulary, performance on institutional reports with different formatting conventions

## Next Checks

1. **Vocabulary Coverage Stress Test:** Systematically evaluate vocabulary-guided extraction on a held-out test set containing known rare findings not in the current vocabulary. Measure recall drop and identify specific clinical categories where coverage is insufficient.

2. **Cross-Institutional Generalization:** Apply the LUNGUAGE framework to chest X-ray reports from a different institution or national registry with different formatting conventions and terminology. Compare entity extraction F1 and grouping accuracy to establish whether the framework generalizes beyond MIMIC-CXR.

3. **Temporal Reasoning Robustness:** Create a controlled test set where disease progression is described using varied temporal language (e.g., "resolved," "improved," "worsened," "stable"). Measure whether the sequential grouping mechanism correctly identifies Entity Groups and Temporal Groups across different temporal expression patterns.