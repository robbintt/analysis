---
ver: rpa2
title: Comparison of Machine Learning Models to Classify Documents on Digital Development
arxiv_id: '2510.00720'
source_url: https://arxiv.org/abs/2510.00720
tags:
- classification
- digital
- classes
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study aimed to improve automated classification of digital
  development documents, a domain with limited labeled data and class imbalance. A
  dataset of 615 reports across 12 intervention areas was used, with extensive preprocessing
  including OCR, tokenization, stop-word removal, lemmatization, and TF-IDF vectorization.
---

# Comparison of Machine Learning Models to Classify Documents on Digital Development

## Quick Facts
- arXiv ID: 2510.00720
- Source URL: https://arxiv.org/abs/2510.00720
- Reference count: 32
- Primary result: One-vs-Rest approach achieved F1-scores up to 0.86 vs. 0.53 for single multiclass model

## Executive Summary
This study compares machine learning approaches for classifying digital development documents into 12 intervention areas using a dataset of 615 reports. The authors demonstrate that a One-vs-Rest (OvR) binary classification approach significantly outperforms single multiclass models when dealing with limited data and class imbalance. Through extensive preprocessing and evaluation of multiple algorithms, the research reveals that classification performance depends more on vocabulary distinctiveness than raw sample size, and that different algorithms excel for different classes.

## Method Summary
The study used 615 PDF documents from USAID's DEEM database, covering 12 digital development intervention areas with heavy class imbalance. Documents underwent OCR text extraction, preprocessing (lowercase, tokenization, domain-specific stopword removal, lemmatization), and TF-IDF vectorization. The research employed a 70/30 train-test split with random oversampling to 100 records per class. Two classification approaches were compared: a single multiclass model and a One-vs-Rest binary approach, evaluating Decision Trees, k-NN, SVM, AdaBoost, SGD, Naive Bayes, Logistic Regression, and ensemble methods.

## Key Results
- OvR binary classification achieved F1-scores up to 0.86 (Child Protection) versus 0.53 maximum for multiclass approach
- Classification performance correlates more with intra-class similarity and inter-class dissimilarity than with sample size alone
- Different algorithms achieved optimal performance for different intervention area classes (e.g., SGD for Child Protection, Logistic Regression for Digital Finance)
- Vocabulary distinctiveness proved more predictive of performance than class size (Child Protection: 26 docs, F1=0.86; Data Systems: 84 docs, F1=0.40)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The One-vs-Rest (OvR) binary classification approach outperforms single multiclass models when training data is limited and class-imbalanced.
- Mechanism: OvR decomposes a 12-class problem into 12 independent binary classifiers, each optimized for its specific class. This allows different algorithms to be selected per class based on performance, and reduces the complexity each model must learn (distinguishing "class vs. not-class" rather than differentiating among 12 classes simultaneously).
- Core assumption: The optimal classification algorithm differs by class characteristics, and binary decision boundaries are easier to learn than multiclass boundaries with limited data.
- Evidence anchors:
  - [abstract] "The OvR method significantly outperformed the single model, with some classes achieving F1-scores up to 0.86 versus a maximum of 0.53 in the multiclass setting."
  - [section] Table 2 shows Phase 1 multiclass logistic regression achieved F1=0.53; Table 3 shows OvR achieved F1=0.86 (Child Protection), 0.80 (Digital Finance), 0.78 (Digital Finance via SVC).
  - [corpus] Corpus evidence is limited; related papers focus on document analysis techniques rather than OvR vs. multiclass comparison specifically.
- Break condition: OvR may degrade performance when classes are highly overlapping or when the "rest" class becomes too heterogeneous for binary separation.

### Mechanism 2
- Claim: Classification performance correlates more strongly with intra-class similarity and inter-class dissimilarity than with raw class sample size alone.
- Mechanism: Classes with distinctive, domain-specific vocabulary (high intra-class similarity, high inter-class dissimilarity) produce more discriminative TF-IDF features, enabling better separation regardless of sample count.
- Core assumption: TF-IDF vectorization captures vocabulary distinctiveness effectively, and document length/quality matters more than raw count.
- Evidence anchors:
  - [abstract] "Results showed that model performance depends not only on dataset size but also on intra-class similarity and inter-class dissimilarity."
  - [section] Child Protection (26 documents, F1=0.86) outperformed Data Systems (84 documents, F1=0.40). Authors note: "words like 'child', 'sexual', and 'adolescent' are unique for the Child Protection class" while "network, people, ict and phone" for Digital Inclusion are common across classes.
  - [corpus] Weak corpus support for this specific mechanism; related papers don't directly address vocabulary distinctiveness vs. class size tradeoffs.
- Break condition: Mechanism may fail if important class distinctions are semantic rather than lexical (requiring contextual embeddings rather than TF-IDF).

### Mechanism 3
- Claim: Different classification algorithms achieve optimal performance for different intervention area classes within the same dataset.
- Mechanism: Each algorithm has different inductive biases (e.g., k-NN relies on local similarity, SVM maximizes margins, Naive Bayes assumes feature independence). These biases align differently with each class's feature distributions.
- Core assumption: No single algorithm is universally optimal across all classes in a heterogeneous multiclass setting.
- Evidence anchors:
  - [abstract] "The OvR approach is recommended for this domain, especially when datasets are small and imbalanced."
  - [section] Table 3: SGD achieved F1=0.86 for Child Protection but only 0.25 for Digital Inclusion. Logistic Regression achieved 0.80 for Digital Finance. Different algorithms topped different classes.
  - [corpus] "Performance Analysis of Supervised Machine Learning Algorithms for Text Classification" (arXiv:2509.00983) appears related but lacks direct algorithm-class specificity comparison.
- Break condition: For larger, more homogeneous datasets, a single well-tuned algorithm may dominate across all classes.

## Foundational Learning

- **Concept: One-vs-Rest (OvR) Decomposition**
  - Why needed here: The study's primary contribution is demonstrating OvR superiority over single multiclass models for small, imbalanced text datasets. Understanding this decomposition strategy is essential.
  - Quick check question: Given a 5-class classification problem, how many binary classifiers would OvR train, and what would each classifier predict?

- **Concept: Class Imbalance and Evaluation Metrics**
  - Why needed here: The USAID DEEM dataset is heavily imbalanced (13 to 149 documents per class). The authors specifically chose weighted F1-score over accuracy for this reason.
  - Quick check question: Why is accuracy misleading for a dataset where one class represents 80% of samples?

- **Concept: TF-IDF Vectorization for Text Classification**
  - Why needed here: The study uses TF-IDF as the sole feature extraction method. Understanding how TF-IDF creates sparse, high-dimensional representations from text is foundational to interpreting the results.
  - Quick check question: What does a high TF-IDF score for a term in a document indicate about that term's discriminative power?

## Architecture Onboarding

- **Component map:**
  Raw PDF documents → OCR text extraction → Preprocessing pipeline (lowercasing, tokenization, domain-specific stopword removal, lemmatization) → TF-IDF vectorization → Random oversampling (minority classes) → [BRANCH A: Single Multiclass Model] OR [BRANCH B: 12 Binary OvR Classifiers] → Evaluation (F1-score per class, weighted average F1)

- **Critical path:**
  1. Domain-specific stopword definition (authors added "digital", "development", "project" as stopwords)
  2. OvR binary classifier training per class
  3. Algorithm selection per class based on validation F1-scores
  4. Combining classifiers into a unified prediction function with probability outputs

- **Design tradeoffs:**
  - **OvR vs. Single Multiclass:** OvR offers higher per-class performance (F1 up to 0.86 vs. 0.53) but requires training and maintaining 12 models instead of one.
  - **Oversampling vs. Class Weights:** Authors chose oversampling to avoid reducing the already small dataset (undersampling) and because it was more tractable than tuning class weights across multiple algorithms.
  - **TF-IDF vs. Deep Learning:** Authors attempted CNNs, RNNs, and Transformers but abandoned them due to computational constraints and limited data (footnote 1, page 3).
  - **Full Document vs. Subset:** Authors used complete document text rather than abstracts/headings to capture "360-degree view," trading computational cost for feature richness.

- **Failure signatures:**
  - **Data Privacy class (multiclass):** F1=0.00 in logistic regression (Table 4) — complete failure to predict minority class.
  - **AdaBoost across all experiments:** Consistently worst performer (F1=0.05 in multiclass, Table 2) — confirmed by literature [3, 12] for text classification.
  - **Digital Inclusion class:** Poor across all OvR algorithms (max F1=0.47) — likely due to vocabulary overlap with other digital development classes.
  - **OvR multi-label leakage:** OvR approach may assign multiple classes to single-label documents due to independent classifier errors (acknowledged on page 13).

- **First 3 experiments:**
  1. **Baseline multiclass replication:** Train logistic regression and SGD on the multiclass dataset (70/30 split) with TF-IDF and oversampling. Verify weighted F1 ≈ 0.51–0.53. Identify which classes have F1 < 0.3.
  2. **OvR pilot on worst-performing classes:** Select 2–3 classes with low multiclass F1 (e.g., Data Privacy, Digital Information Services). Train binary classifiers using all 7 algorithms. Compare best binary F1 to multiclass F1 for these classes.
  3. **Vocabulary distinctiveness analysis:** For each class, compute TF-IDF term distinctiveness scores. Test correlation between distinctiveness (top-100 terms' average IDF) and achievable F1-score. Validate the authors' claim that vocabulary uniqueness, not sample size, drives performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Deep Learning models outperform the One-vs-Rest (OvR) machine learning approach if the dataset scales up?
- **Basis in paper:** [explicit] The authors dropped Deep Learning (DL) initially due to data scarcity but conclude there is space to "integrate DL models in future work, which were found to be outperforming ML algorithms for large datasets."
- **Why unresolved:** The current dataset (615 records) is insufficient for DL convergence, leaving the comparison strictly between classical ML algorithms.
- **What evidence would resolve it:** A comparative study benchmarking DL models (e.g., BERT, CNN) against the OvR ML baseline on a corpus of several thousand reports.

### Open Question 2
- **Question:** How effective is the OvR strategy for multi-label classification in the digital development domain?
- **Basis in paper:** [explicit] The authors note that the OvR approach "eases addressing multi-label classification" and allows the study to expand its scope, though they restricted the current work to single labels by removing duplicates.
- **Why unresolved:** The study design forced a single-label structure, so the proposed method's ability to handle documents belonging to multiple intervention areas remains untested.
- **What evidence would resolve it:** Evaluation of the OvR ensemble on the full dataset with duplicate labels restored, using multi-label metrics like Hamming loss or subset accuracy.

### Open Question 3
- **Question:** Does augmenting the dataset with reports from external organizations specifically improve performance for classes with low intra-class similarity?
- **Basis in paper:** [explicit] The authors suggest future work should include "augmenting the dataset with reports from underrepresented intervention areas" and "reports pooled by other organisations."
- **Why unresolved:** While data volume is an issue, the authors also note that performance depends on intra-class similarity; external data might introduce noise or varying terminologies.
- **What evidence would resolve it:** Ablation studies showing F1-score changes in minority classes (e.g., Data Privacy) after integrating specific external data sources.

## Limitations

- Algorithm hyperparameter tuning was limited to default settings, potentially not representing optimal configurations
- Vocabulary distinctiveness mechanism needs systematic validation beyond the two case studies presented
- Results are specific to USAID DEEM dataset and may not generalize to other domains with different class characteristics

## Confidence

- **High confidence:** OvR binary approach outperforms single multiclass model for this dataset (F1-scores 0.86 vs 0.53)
- **Medium confidence:** Classification performance depends more on intra-class similarity and inter-class dissimilarity than raw sample size
- **Medium confidence:** Different algorithms perform optimally for different classes

## Next Checks

1. **Algorithm sensitivity analysis:** Systematically vary hyperparameters (k for k-NN, C for SVM, learning rates for SGD) to determine if algorithm superiority patterns persist under optimization
2. **Vocabulary distinctiveness quantification:** Compute TF-IDF distinctiveness scores for all classes and test correlation with F1-scores across the full dataset, not just selected examples
3. **Cross-domain replication:** Apply the same OvR methodology to a different text classification dataset with similar class imbalance to test generalizability of the findings