---
ver: rpa2
title: 'PROF: An LLM-based Reward Code Preference Optimization Framework for Offline
  Imitation Learning'
arxiv_id: '2511.13765'
source_url: https://arxiv.org/abs/2511.13765
tags:
- reward
- penalty
- action
- prof
- velocity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROF is an LLM-based framework for offline imitation learning that
  generates and refines executable reward function code without requiring environment
  interaction. It uses Reward Preference Ranking to evaluate reward functions by comparing
  expert and noisy trajectory returns, then iteratively optimizes them using TextGrad.
---

# PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning

## Quick Facts
- arXiv ID: 2511.13765
- Source URL: https://arxiv.org/abs/2511.13765
- Reference count: 40
- Key outcome: PROF matches or surpasses recent strong baselines across locomotion, AntMaze, and Adroit tasks on D4RL benchmarks, demonstrating its effectiveness in automated reward design for offline RL.

## Executive Summary
PROF is an LLM-based framework for offline imitation learning that generates and refines executable reward function code without requiring environment interaction. It uses Reward Preference Ranking (RPR) to evaluate reward functions by comparing expert and noisy trajectory returns, then iteratively optimizes them using TextGrad. On D4RL benchmarks, PROF matches or surpasses recent strong baselines across locomotion, AntMaze, and Adroit tasks, demonstrating its effectiveness in automated reward design for offline RL.

## Method Summary
PROF automates reward function design for offline imitation learning by combining LLM-based code generation with preference ranking. The framework generates n=5 reward function candidates via GPT-4o using zero-shot prompts with general and task-specific environment descriptions. These candidates are evaluated using Reward Preference Ranking (RPR), which computes dominance scores by comparing expert trajectory returns against noisy and offline data returns. The best and worst functions are then refined through TextGrad, which generates textual "gradients" as improvement suggestions. This process iterates T times (typically T=1-2) before selecting the optimal reward function to label the dataset and train an IQL policy.

## Key Results
- Matches or surpasses strong offline RL baselines on D4RL locomotion, AntMaze, and Adroit tasks
- Demonstrates effectiveness of LLM-generated rewards without environment interaction
- Shows non-monotonic optimization behavior, typically peaking at T=1-2 iterations
- Achieves competitive performance with minimal hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1: Dominance-Based Reward Proxy (RPR)
Reward function quality can be approximated without environment interaction by measuring how distinctly they separate expert trajectories from noisy variants and offline data. PROF generates noisy trajectories by perturbing the expert demonstration with Gaussian noise, then calculates a "dominance score" based on the proportion of times the expert return exceeds returns of offline and noisy trajectories. This converts the unsupervised reward design problem into a binary classification preference task.

### Mechanism 2: Textual Gradient Descent
Reward function code can be iteratively refined using an LLM acting as a "textual optimizer" that processes feedback comparing high-scoring and low-scoring candidates. Instead of numerical gradients, PROF uses TextGrad to identify the best and worst functions, prompt the LLM to analyze why the best is superior, and generate actionable improvement suggestions applied to synthesize new code.

### Mechanism 3: Semantic Initialization from Physics Priors
Zero-shot LLM generation produces superior initial reward candidates compared to random initialization because LLMs encode general physics and robotics knowledge. PROF prompts the LLM with detailed environment specs, mapping semantic concepts (velocity, height, torque) directly to executable Python code, bypassing the need to learn reward structure from scratch.

## Foundational Learning

- **Concept: Offline Reinforcement Learning (Offline RL)**
  - Why needed here: PROF is designed specifically for the "offline" setting where you cannot interact with the environment to validate rewards. You must understand the constraint of fixed datasets.
  - Quick check question: Can you explain why online reward shaping methods (like training a reward model via interaction) fail in an offline setting?

- **Concept: Inverse Reinforcement Learning (IRL)**
  - Why needed here: PROF is essentially an IRL method—it infers rewards from behavior. Understanding traditional IRL helps contrast how PROF replaces the "learning" loop with "code generation" and "ranking."
  - Quick check question: How does PROF's "Dominance Score" replace the traditional IRL loss function (like Maximum Entropy IRL)?

- **Concept: Prompt Engineering for Code Generation**
  - Why needed here: The quality of the initial reward functions relies entirely on the prompts. You need to know how to structure "General" vs. "Task-Specific" constraints.
  - Quick check question: Why is it necessary to specify the observation space indices (e.g., `obs[8]` is velocity) in the prompt for successful code generation?

## Architecture Onboarding

- **Component map:** Input (Offline Buffer, Expert Demo) -> Prompt System (General + Task-Specific) -> LLM Generator (GPT-4o) -> RPR Unit (Noise Injection -> Return Computation -> Dominance Scoring) -> TextGrad Loop (Compare Best/Worst -> Generate Feedback -> Update Code) -> Output (Optimized Reward Function) -> Train IQL policy

- **Critical path:** The Noisy Trajectory Construction. If the noise scale α is wrong, the synthetic negative examples will be too easy (trivial reward) or too hard (uninformative) to distinguish from the expert, corrupting the Dominance Score.

- **Design tradeoffs:**
  - Proxy vs. Ground Truth: PROF optimizes for the Dominance Score (a proxy), not actual policy return. This risks Goodhart's Law (reward hacking).
  - Cost: TextGrad requires multiple LLM calls (estimated 75k-100k tokens per iteration).

- **Failure signatures:**
  - Syntax/Execution Errors: LLM generates undefined variables. Mitigated by sampling n=5 candidates.
  - Reward Hacking: The code becomes overly complex, penalizing irrelevant features to maximize the proxy score without improving the policy.
  - Over-Optimization: Performance degrades after T=2 as the reward function overfits to the specific expert demo in the buffer.

- **First 3 experiments:**
  1. **Sanity Check (T=0):** Run PROF on "HalfCheetah" with T=0 (no TextGrad). Verify that LLM generates executable code and RPR assigns non-trivial scores.
  2. **Noise Sensitivity:** Ablate the noise scale α. Set α=0 (no noise) and α=1.0 (high noise) to verify that the RPR mechanism relies on the quality of the synthetic negative examples.
  3. **Baseline Comparison:** Compare PROF against a simple "BC" (Behavior Cloning) baseline on a sparse reward task (e.g., AntMaze) to validate that the generated dense rewards actually aid learning where pure imitation fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework automatically determine an optimal stopping criterion for the iterative optimization process to prevent performance degradation caused by over-optimization?
- Basis in paper: Observation 4 and Appendix F note that performance is non-monotonic, typically peaking at T=1 or T=2 before declining at T=3 due to "reward hacking."
- Why unresolved: The current implementation relies on a fixed iteration count (T), and the authors observe that excessive optimization leads to spurious code changes that maximize the proxy score but hurt true performance.
- What evidence would resolve it: The development of a validation metric that detects reward hacking or a theoretical bound connecting the dominance score to policy return.

### Open Question 2
- Question: How sensitive is the Reward Preference Ranking (RPR) algorithm to the choice of noise scale hyperparameters (α_o, α_a) used to generate synthetic noisy trajectories?
- Basis in paper: Section 4.2 fixes α_o and α_a at 0.05, but the reliability of the dominance score fundamentally depends on the quality and severity of these noisy perturbations acting as negative examples.
- Why unresolved: The paper does not provide an ablation study on these specific noise parameters, leaving it unclear if these values required tuning per environment or if they are robust general defaults.
- What evidence would resolve it: An ablation study showing the correlation between noise scale variance and the final normalized score of the trained policy.

### Open Question 3
- Question: Can the dominance score proxy be modified to prevent the LLM from "fabricating" non-existent input variables or observation indices during code generation?
- Basis in paper: Appendix F highlights instances where the LLM generates code using fabricated variables (e.g., `prev_action`, `goal_x`) to maximize the dominance score, even though these variables are not provided in the input signature.
- Why unresolved: The current feedback loop rewards code that separates expert and noisy returns mathematically, even if that code relies on hallucinated variables that may cause runtime errors or silent failures in downstream tasks.
- What evidence would resolve it: A code-analysis mechanism or prompt constraint that reduces the rate of hallucinated variables to zero while maintaining competitive dominance scores.

## Limitations
- Framework requires task-specific environment descriptions, limiting applicability to environments with well-documented specifications
- High computational cost due to multiple LLM calls (estimated 75k-100k tokens per iteration)
- Approach evaluated primarily on D4RL benchmarks; performance on more complex or real-world tasks remains untested

## Confidence
- **High Confidence:** The core mechanism of using LLM-generated code with preference ranking (RPR) for offline reward design is well-supported by the results.
- **Medium Confidence:** The effectiveness of TextGrad for iterative refinement, as the paper doesn't extensively validate whether LLM-generated "gradients" truly capture causal relationships.
- **Medium Confidence:** The zero-shot generation capability, as the prompts are highly specific and success may depend on LLM model version and task similarity to pre-training data.

## Next Checks
1. **Ablation on Noise Scale (α):** Systematically vary the noise injection parameter α to identify the optimal range where RPR provides meaningful signal without creating uninformative negative examples.

2. **Reward Hacking Detection:** Monitor the correlation between dominance scores and actual policy returns across optimization iterations to detect and quantify reward hacking behavior.

3. **Cross-Environment Generalization:** Test PROF on environments with significantly different physics or task structures from D4RL to evaluate the robustness of zero-shot LLM generation beyond the paper's scope.