---
ver: rpa2
title: 'Soup to go: mitigating forgetting during continual learning with model averaging'
arxiv_id: '2501.05559'
source_url: https://arxiv.org/abs/2501.05559
tags:
- task
- tasks
- math
- fine-tuning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sequential Fine-tuning Averaging (SFA), a
  method for mitigating catastrophic forgetting in continual learning by merging partially-trained
  models with earlier checkpoints during training. SFA periodically averages the current
  model parameters with a previous checkpoint, controlled by a hyperparameter p that
  determines the averaging frequency.
---

# Soup to go: mitigating forgetting during continual learning with model averaging

## Quick Facts
- **arXiv ID**: 2501.05559
- **Source URL**: https://arxiv.org/abs/2501.05559
- **Reference count**: 33
- **Primary result**: SFA achieves comparable performance to data buffers without storing past data by periodically averaging current model parameters with previous checkpoints.

## Executive Summary
This paper introduces Sequential Fine-tuning Averaging (SFA), a method for mitigating catastrophic forgetting in continual learning by merging partially-trained models with earlier checkpoints during training. SFA periodically averages the current model parameters with a previous checkpoint, controlled by a hyperparameter p that determines the averaging frequency. This approach achieves comparable performance to using a data buffer without storing any past data. Experiments across image and language tasks show that SFA consistently outperforms other model merging techniques like Task Arithmetic, TIES Merging, and WiSE-FT, as well as classical methods like L2 and Elastic Weight Consolidation. The method also roughly approximates L2-regression, bridging classical continual learning algorithms with model merging techniques.

## Method Summary
SFA is a continual learning algorithm that addresses catastrophic forgetting by periodically merging the current model parameters with a checkpoint from the previous task. During fine-tuning on a new task, the method performs standard gradient updates but every pT iterations (where p is a hyperparameter and T is total training steps) averages the current parameters with the previous checkpoint θ_o using the formula θ_{t+1} = βθ_o + (1-β)θ*_t+1, where β is typically 0.5. This averaging creates a "soft anchor" that constrains weight drift and preserves past-task knowledge. The approach can be viewed as a discrete approximation of L2-regularized gradient descent, providing theoretical grounding while avoiding the need to store past data or implement explicit regularization terms.

## Key Results
- SFA achieves comparable performance to data buffers without storing past data across multiple benchmarks
- The method outperforms WiSE-FT, Task Arithmetic, TIES Merging, L2, and EWC on average accuracy metrics
- SFA provides a unified framework that bridges classical continual learning algorithms with modern model merging techniques

## Why This Works (Mechanism)

### Mechanism 1: Parameter Anchoring via Periodic Averaging
Periodically merging model parameters with a previous checkpoint constrains weight drift and preserves past-task knowledge. Every pT iterations, the current parameters θ_t+1 are reset to a weighted combination: θ_t+1 = (β)θ_o + (1-β)θ*_t+1, where θ_o is the checkpoint from the previous task. This creates a "soft anchor" that prevents the optimization trajectory from straying too far from regions that performed well on prior tasks. The core assumption is that parameter regions that performed well on past tasks remain partially valid for current tasks, and weight divergence correlates with forgetting. Evidence shows accuracy spikes on past tasks at averaging steps, demonstrating explicit retention events.

### Mechanism 2: Implicit L2-Regularization Approximation
SFA behaves as a discrete approximation of continuous L2-penalized gradient descent. The paper shows that a single L2-penalized update θ_t+1 = (1-ηλ)θ_t + (ηλ)θ_o - η∇L_task structurally resembles SFA's averaged update θ_t+1 = (1-β)θ_t + (β)θ_o - α∇L_task(1-β). When β = ηλ and α = η/(1-ηλ), the forms are equivalent, suggesting SFA implicitly enforces a penalty on deviation from θ_o. The core assumption is that infrequent averaging still approximates the continuous penalty's stabilizing effect. Empirically, SFA achieves comparable or better trade-offs than explicit L2 and EWC on MNIST task pairs.

### Mechanism 3: Checkpoint-as-Data-Proxy for Buffer-Free Retention
A model checkpoint trained on past tasks encodes sufficient information to substitute for replay buffers. Instead of storing and replaying raw data, SFA uses θ_o (optimized on previous tasks) as a proxy. The averaging operation projects the current model back toward a region representing past-task competence, achieving retention without data storage or privacy concerns. The core assumption is that parameter-space proximity to θ_o correlates with functional performance on past tasks. Evidence shows SFA reaches comparable performance to 5-10% rehearsal buffers on domain shifts like Math→Law→Code. Break condition: if tasks involve large distribution shifts requiring fundamentally different feature representations, the single-checkpoint proxy may lack representational capacity.

## Foundational Learning

- **Concept: Catastrophic Forgetting in Sequential Fine-Tuning**
  - Why needed here: SFA is designed to address this exact failure mode; understanding why gradient descent on new tasks degrades past-task performance is prerequisite to appreciating the intervention.
  - Quick check question: Can you explain why standard fine-tuning on Task B after Task A causes performance collapse on Task A, even with identical model capacity?

- **Concept: Model Merging / Weight Averaging**
  - Why needed here: SFA builds on WiSE-FT and model souping; understanding that parameter averages often preserve functional properties of both parents is essential.
  - Quick check question: Given two models θ_A and θ_B fine-tuned on different tasks, why might (θ_A + θ_B)/2 retain capabilities from both, rather than averaging them away?

- **Concept: L2-Regularization (Weight Decay) in Continual Learning**
  - Why needed here: Section 6 frames SFA as an approximation of L2-regression; understanding penalty-based CL methods provides the theoretical grounding.
  - Quick check question: How does adding λ||θ - θ_o||² to the loss constrain learning, and what is the role of θ_o in continual learning specifically?

## Architecture Onboarding

- **Component map:**
  θ_o (previous checkpoint) -> θ_t (current model) -> Averaging scheduler -> Weighting coefficient β -> Task boundary handler

- **Critical path:**
  1. Initialize θ_t ← θ_o at start of new task.
  2. Train on current task with standard gradient updates.
  3. Every pT steps: θ_t ← βθ_o + (1-β)θ_t.
  4. At task end: perform final averaging; set θ_o ← θ_t for next task.

- **Design tradeoffs:**
  - Low p (frequent averaging): Strong retention of past tasks, but may slow convergence on current task if averaging interrupts feature learning (Section 5.1 notes tasks need "substantial portion of training steps" before averaging helps).
  - High p (near 1): Approaches WiSE-FT behavior; faster current-task learning but weaker retention, especially with domain shifts.
  - β tuning: Varying β at p=1 does not recover p<1 performance (Section 5.4, Figures 7-8), suggesting timing matters more than weighting.

- **Failure signatures:**
  - Current-task accuracy plateaus below baseline: p too low (averaging before sufficient learning).
  - Past-task accuracy degrades continuously without spikes: averaging not triggering (check p and scheduler implementation).
  - Complete loss of past-task performance: β set incorrectly toward current model, or θ_o not updated correctly across task boundaries.

- **First 3 experiments:**
  1. Reproduce Figure 9 (MNIST task pair): Train on Task A (even digits), then Task B (odd digits) with SFA at varying p. Plot Task A vs. Task B accuracy to confirm the trade-off curve matches L2/EWC baselines. This validates your implementation.
  2. Ablation on p for a 3-task sequence: Using Food-101 or CIFAR-100 splits, test p ∈ {0.25, 0.5, 0.75, 0.96, 1.0} across 20 sequential tasks. Track per-task accuracy over time to identify the retention-vs-plasticity frontier.
  3. Domain-shift stress test: Fine-tune a language model on Math → Law → Code with SFA (p=0.25) vs. Task Arithmetic vs. 10% data buffer. Report holdout accuracy on all three domains after the sequence completes to validate buffer-free equivalence claim.

## Open Questions the Paper Calls Out

### Open Question 1
Can the averaging frequency hyperparameter $p$ be adapted dynamically during training rather than set as a static value? The paper notes that averaging before a "substantial portion of training steps" have occurred hinders performance on the current task, suggesting the optimal $p$ varies based on task difficulty or learning speed. The paper treats $p$ as a fixed hyperparameter determined via grid search for each experimental run.

### Open Question 2
How does SFA scale to continual learning scenarios involving significantly longer sequences of tasks than the 20 image tasks or 3 language domains tested? It is unclear if the "anchoring" to $\theta_o$ remains effective or if the accumulated drift becomes unmanageable over hundreds of sequential tasks without resetting or updating the reference model strategy.

### Open Question 3
Under what specific theoretical conditions does SFA strictly approximate L2-regression, and where does the approximation diverge? The paper states that SFA "roughly approximates" L2-regression but notes that because averaging is infrequent in practice, "it typically is not equivalent." The paper derives an equivalence condition ($\beta = \eta\lambda$) but leaves the implications of the "infrequent" update schedule on the theoretical bound largely unexplored.

## Limitations
- The claim that a single checkpoint can substitute for replay buffers across large domain shifts needs stronger empirical validation across more task pairs.
- The computational overhead analysis is limited - while averaging is O(n), the frequency p introduces a trade-off between effectiveness and training time that isn't fully characterized.
- The core claims about SFA's effectiveness rest on several assumptions that require further validation, particularly the mechanism connecting periodic averaging to implicit L2-regularization.

## Confidence
- **High confidence**: SFA outperforms WiSE-FT and Task Arithmetic across multiple benchmarks; the averaging mechanism improves retention vs. vanilla fine-tuning; the relationship to L2-regularization is mathematically sound.
- **Medium confidence**: SFA achieves "comparable" results to data buffers without storing past data - this depends heavily on buffer size, task similarity, and evaluation metrics not fully specified.
- **Low confidence**: The claim that SFA "roughly approximates" L2-regression in practical settings - while the mathematical correspondence exists, the discrete approximation's effectiveness across diverse tasks needs more rigorous validation.

## Next Checks
1. **Frequency-Retention Trade-off Characterization**: Systematically vary p from 0.1 to 0.9 across 5+ task sequences and plot retention vs. training efficiency curves to identify optimal operating points for different task similarity regimes.
2. **Multi-Checkpoint Extension Test**: Implement SFA with multiple checkpoints (one per past task) instead of a single θ_o and measure whether this recovers buffer-level performance, testing the single-checkpoint proxy assumption.
3. **Gradient Sign Conflict Analysis**: Design a synthetic task pair requiring opposing gradient directions (e.g., Task A: predict digit parity, Task B: predict digit magnitude with opposite signs) and measure SFA's degradation to test the break condition for Mechanism 1.