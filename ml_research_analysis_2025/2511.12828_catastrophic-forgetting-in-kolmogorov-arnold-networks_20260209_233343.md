---
ver: rpa2
title: Catastrophic Forgetting in Kolmogorov-Arnold Networks
arxiv_id: '2511.12828'
source_url: https://arxiv.org/abs/2511.12828
tags:
- forgetting
- tasks
- task
- kans
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies catastrophic forgetting in Kolmogorov-Arnold
  Networks (KANs), a neural architecture that uses learnable spline activations. The
  authors develop a theoretical framework showing that forgetting scales linearly
  with activation support overlap and exponentially with task complexity (intrinsic
  data dimension).
---

# Catastrophic Forgetting in Kolmogorov-Arnold Networks

## Quick Facts
- **arXiv ID**: 2511.12828
- **Source URL**: https://arxiv.org/abs/2511.12828
- **Reference count**: 40
- **One-line primary result**: KANs show near-zero forgetting in low-dimensional tasks but exhibit increasing forgetting in high-dimensional domains like image classification and language editing, with a novel KAN-LoRA adapter demonstrating superior retention compared to MLP-based adapters.

## Executive Summary
This paper investigates catastrophic forgetting in Kolmogorov-Arnold Networks (KANs), a neural architecture that replaces fixed activations with learnable spline functions. The authors develop a theoretical framework showing that forgetting scales linearly with activation support overlap and exponentially with task complexity (intrinsic data dimension). Through experiments on synthetic arithmetic, image classification, and language model fine-tuning tasks, they demonstrate that KANs excel at retention in low-dimensional settings but struggle with high-dimensional data. The paper introduces a novel KAN-based LoRA adapter (KAN-LoRA) for continual LM fine-tuning, showing superior retention compared to MLP-based adapters, particularly in low-sample regimes and smaller models.

## Method Summary
The study evaluates catastrophic forgetting in KANs across synthetic arithmetic tasks (binary and decimal addition), image classification (CIFAR-10, Tiny-ImageNet, MNIST split into sequential tasks), and LM knowledge editing (CounterFact, ZsRE). The authors train KANs sequentially on each task and measure forgetting as the degradation in performance on previous tasks. They compare KANs against MLP baselines with and without EWC regularization, and introduce KAN-LoRA by replacing MLP layers in LoRA adapters with KAN layers for continual LM fine-tuning. Experiments are conducted using standard training procedures with AdamW optimizer, varying grid sizes, and different architectural configurations.

## Key Results
- KANs achieve near-zero forgetting (<1e-6 MSE) in binary addition tasks due to disjoint activation supports
- Forgetting scales linearly with activation support overlap and exponentially with intrinsic dimensionality
- KAN-LoRA outperforms MLP-LoRA in low-sample regimes and smaller models, showing superior retention
- Increasing spline grid size reduces forgetting by fragmenting activation supports
- KANs struggle with high-dimensional data like Tiny-ImageNet despite theoretical advantages

## Why This Works (Mechanism)

### Mechanism 1: Disjoint Support Retention
- **Claim**: If activation regions for a new task don't intersect with those of a previous task, the network retains prior knowledge exactly
- **Mechanism**: KANs use learnable spline activations on edges that are local functions. A gradient update for a new sample only adjusts spline coefficients within a narrow neighborhood, leaving untouched parameters responsible for previous tasks
- **Core assumption**: Data distributions for sequential tasks are sufficiently separated in the input domain such that their activation supports have zero intersection measure
- **Evidence**: Binary addition experiments show near-zero forgetting, supporting the zero-overlap retention theory

### Mechanism 2: Linear Forgetting via Support Overlap
- **Claim**: When interference occurs, degradation scales linearly with the length of overlap between activation regions
- **Mechanism**: Forgetting is bounded by network size, Lipschitz constant, and maximal support overlap. Increasing spline grid size fragments the support, shrinking overlap length and reducing interference
- **Core assumption**: Spline functions are Lipschitz continuous and the loss function is bounded
- **Evidence**: Grid size ablation studies on decimal addition show reduced forgetting with finer grids

### Mechanism 3: Intrinsic Dimension Complexity
- **Claim**: Retention capability degrades exponentially as intrinsic dimensionality increases
- **Mechanism**: High-dimensional data distributes points more sparsely, requiring larger activation supports or increasing overlap probability. Overlap expectation scales with dimensionality
- **Core assumption**: Task data lies on a compact submanifold where covering number scales with dimensionality
- **Evidence**: KANs show increasing forgetting in high-dimensional tasks like image classification compared to low-dimensional arithmetic

## Foundational Learning

- **B-Splines and Locality**
  - **Why needed**: KANs replace fixed weights with spline functions; understanding spline locality is critical to grasping why KANs isolate knowledge better than MLPs
  - **Quick check**: If you increase the grid size of a spline (more knots), does the support of individual basis functions typically get larger or smaller? (Answer: smaller)

- **Intrinsic Dimensionality**
  - **Why needed**: The paper hinges on finding that KANs fail in high dimensions; distinguishing raw input size from intrinsic dimension is crucial to diagnosing why forgetting happens in vision but not arithmetic
  - **Quick check**: Why would a binary addition task have a lower intrinsic dimension than an image classification task? (Answer: Binary addition has a simple 1D manifold structure vs. complex manifold structure in images)

- **Catastrophic Interference**
  - **Why needed**: This is the baseline failure mode the paper attempts to solve; understanding global weight updates in standard MLPs is essential to appreciate KANs' local update solution
  - **Quick check**: In a standard MLP, if you update weights for Task B, what happens to the loss for Task A if the tasks share weights? (Answer: Task A loss typically increases due to interference)

## Architecture Onboarding

- **Component map**: Input Layer -> KAN Layer (learnable spline functions on edges) -> Aggregation (sum of incoming spline functions) -> Output Layer
- **Critical path**: 
  1. Grid Sizing: Primary lever for balancing capacity vs. retention; larger grid → smaller supports → less overlap → better retention
  2. Spline Initialization: Initialize to approximate identity or zero depending on architecture
  3. Fine-tuning/Training: Standard backpropagation updates spline coefficients; computation is slower than MLP

- **Design tradeoffs**:
  - KANs excel at retention in low-dim/algorithmic tasks but struggle with high-dim data where support overlap is inevitable
  - Increasing grid size improves retention but increases parameter count and compute cost
  - KAN-LoRA is more robust to forgetting than MLP-LoRA, but standard MLP-LoRA may perform better on the new task in high-data regimes

- **Failure signatures**:
  - Saturation: Forgetting flattens out but remains high in high-dimensional tasks; adding capacity doesn't solve interference if data manifold is too complex
  - Overhead: KAN-LoRA introduces ~10x more parameters than MLP-LoRA for similar rank configurations
  - Performance Drop: MLPs with regularization may outperform KANs on complex datasets like Tiny-ImageNet

- **First 3 experiments**:
  1. **Binary Addition Sanity Check**: Train small KAN (3 inputs, 2 hidden) sequentially on "one's addition" then "two's addition"; verify loss on "one's addition" remains near zero
  2. **Grid Size Ablation**: On decimal addition task, run sequential training with grid sizes [5, 10, 15, 20]; plot forgetting to verify linear relationship with support overlap
  3. **KAN-LoRA vs. MLP-LoRA**: Edit small LM (Llama-7B) on 5 sequential CounterFact tasks using 2 samples per task; compare retention accuracy to confirm KAN's superiority in low-sample regimes

## Open Questions the Paper Calls Out

- **Lifecycle-based pruning**: Can lifecycle-based pruning or controlled specialization of spline activations effectively enhance long-term retention in KANs?
- **Distributed memory encoding**: Can intentional overlap of activation supports be utilized for distributed memory encoding to store multiple tasks in a compressed format?
- **High-dimensional adaptation**: How can KAN architectures be adapted to mitigate the exponential growth of forgetting observed in high-dimensional domains?
- **Beneficial inductive bias**: Can the forgetting mechanism in KANs be harnessed as a beneficial inductive bias to improve generalization?

## Limitations

- Theoretical framework relies on idealized assumptions about data separation that may not hold in practice
- Exponential relationship with intrinsic dimension has limited experimental validation beyond demonstrating trends
- KAN-LoRA introduces ~10x more parameters than MLP-LoRA, representing significant computational overhead
- Paper lacks direct comparisons to other continual learning methods beyond EWC and standard LoRA
- Limited validation of theoretical bounds' practical tightness and applicability to complex real-world data distributions

## Confidence

- **High Confidence**: Disjoint support retention mechanism validated through binary addition experiments showing near-zero forgetting
- **Medium Confidence**: Linear forgetting via support overlap supported by grid size ablation studies; KAN-LoRA superiority in low-sample regimes demonstrated
- **Low Confidence**: Exponential relationship with intrinsic dimension lacks rigorous experimental proof; practical applicability of theoretical bounds uncertain

## Next Checks

1. **Intrinsic Dimension Correlation Study**: Systematically vary intrinsic dimension of synthetic datasets and measure forgetting F_i across multiple KAN architectures; plot F_i vs. d_i to verify exponential relationship predicted by Theorem 3

2. **Comparative CL Method Benchmark**: Implement and compare KAN-LoRA against other continual learning approaches (EWC, Synaptic Intelligence, Experience Replay) on CounterFact editing tasks, controlling for parameter count and computational budget

3. **Support Overlap Measurement Validation**: Implement numerical method for computing Lebesgue measure of activation support overlap; apply to both synthetic and real datasets to verify measured overlap Δ_i,j correlates with observed forgetting F_i as predicted by theoretical bounds