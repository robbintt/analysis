---
ver: rpa2
title: Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model
arxiv_id: '2509.06539'
source_url: https://arxiv.org/abs/2509.06539
tags:
- defender
- cage-2
- attacker
- state
- host
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a formal POMDP model of the CAGE-2 cybersecurity
  benchmark and introduces BF-PPO, a reinforcement learning method that combines PPO
  with particle filtering to handle large state spaces. BF-PPO addresses the computational
  challenges of optimal defender strategy learning in complex network environments.
---

# Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model

## Quick Facts
- arXiv ID: 2509.06539
- Source URL: https://arxiv.org/abs/2509.06539
- Authors: Duc Huy Le; Rolf Stadler
- Reference count: 39
- Primary result: BF-PPO outperforms CARDIFF on CAGE-2 with higher rewards and faster convergence

## Executive Summary
This paper presents a formal POMDP model of the CAGE-2 cybersecurity benchmark and introduces BF-PPO, a reinforcement learning method that combines PPO with particle filtering to handle large state spaces. The approach addresses computational challenges of optimal defender strategy learning in complex network environments. BF-PPO is evaluated against the state-of-the-art CARDIFF method, demonstrating superior performance in defending against both B-LINE and MEANDER attacker scenarios.

## Method Summary
The paper formalizes the CAGE-2 defender problem as a POMDP, capturing the partially observable nature of network security where the defender cannot directly observe the attacker's internal state. To handle the large state space, the authors introduce BF-PPO, which integrates particle filtering for belief state estimation with PPO for policy optimization. The method samples a single particle from the belief state as input to the PPO network, reducing computational complexity while maintaining sufficient information for decision-making. This approach enables learning effective defender strategies in the CAGE-2 benchmark environment.

## Key Results
- BF-PPO achieves higher cumulative rewards than CARDIFF across both B-LINE and MEANDER attacker scenarios
- The method demonstrates faster convergence compared to the state-of-the-art CARDIFF approach
- BF-PPO outperforms the leaderboard's top-ranked approach on the CAGE-2 benchmark

## Why This Works (Mechanism)
BF-PPO works by combining particle filtering for belief state estimation with PPO for policy optimization. The particle filter maintains a set of particles representing possible attacker states, from which a single particle is sampled uniformly to represent the belief state. This sampled particle is then used as input to the PPO network, which learns to map observations to defender actions. The combination allows the method to handle the large state space of the CAGE-2 benchmark while maintaining computational tractability. The PPO component provides stable policy updates, while the particle filtering component handles the uncertainty inherent in the partially observable environment.

## Foundational Learning
- **POMDP Formalism**: Why needed - to model partially observable environments where the defender cannot directly observe attacker states. Quick check - verify the belief state update equations correctly propagate uncertainty.
- **Particle Filtering**: Why needed - to maintain a tractable representation of the belief state in high-dimensional spaces. Quick check - ensure particle diversity is maintained to avoid collapse.
- **PPO Algorithm**: Why needed - to provide stable policy updates in continuous action spaces. Quick check - verify the KL divergence constraint is properly enforced.
- **Belief State Sampling**: Why needed - to reduce computational complexity while preserving essential information. Quick check - confirm the single particle sample contains sufficient information for decision-making.

## Architecture Onboarding

Component Map:
CAGE-2 Environment -> POMDP Model -> Particle Filter -> Belief State Sampler -> PPO Network -> Defender Actions

Critical Path:
The critical path flows from the CAGE-2 environment through the POMDP model, where observations are processed. The particle filter maintains belief states over attacker positions, and a single particle is sampled to represent this belief. The PPO network then takes this sampled particle and the current observation to output defender actions. This path is executed at each time step of the simulation.

Design Tradeoffs:
The key tradeoff is between computational efficiency and information preservation. Using a single particle sample reduces the input dimensionality and computational load compared to using the full belief distribution or multiple particles, but may discard some uncertainty information. The choice of uniform sampling over "most likely" particle aims to maintain exploration while keeping computation tractable.

Failure Signatures:
- If the particle filter loses diversity, the sampled belief state may not represent the true uncertainty, leading to suboptimal defender actions
- If the PPO network overfits to the sampled particle representation, it may fail to generalize to different belief states
- If the observation space reduction loses critical information, the defender may make decisions based on incomplete data

First Experiments:
1. Verify the POMDP model correctly captures the CAGE-2 environment dynamics by comparing simulated trajectories against ground truth
2. Test the particle filter's ability to maintain diverse particles and accurately estimate belief states in simple scenarios
3. Evaluate the PPO network's performance with different belief state representations (single particle vs. multiple particles vs. distribution statistics)

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the CAGE-2 interaction be formulated and solved as a game where both the attacker and defender follow dynamic strategies?
- Basis in paper: [explicit] The conclusion states the authors plan to "formulate the interaction between attacker and defender as a game where both players follow dynamic strategies, and we can analyse and hopefully solve the game."
- Why unresolved: The current study models the problem as a POMDP against a fixed attacker strategy (B-LINE or MEANDER).
- What evidence would resolve it: A game-theoretic analysis or equilibrium solution derived from the POMDP model where both agents adapt simultaneously.

### Open Question 2
- Question: Can BF-PPO effectively learn an optimal attacker strategy when the defender's strategy is fixed?
- Basis in paper: [explicit] The conclusion suggests future work where, "given a fixed defender strategy, we can formulate an optimal attacker strategy and use our method BF-PPO to compute it."
- Why unresolved: The paper only evaluates the method's ability to learn defender strategies against predefined attacker behaviors.
- What evidence would resolve it: Results from an experiment where the roles are reversed, training an attacker agent using BF-PPO against a static defender policy.

### Open Question 3
- Question: Does representing the belief state with a single particle sample ($\hat{S}_t$) result in information loss or instability compared to using full belief statistics?
- Basis in paper: [inferred] The method inputs a single particle sampled uniformly from the filter rather than the full distribution or distribution statistics, noting the alternative of the "most likely" particle was considered but not compared against distribution moments.
- Why unresolved: The paper does not ablate this representation choice; inputting a single sample may discard uncertainty information critical for POMDP resolution.
- What evidence would resolve it: An ablation study comparing the single-particle input against an input layer encoding belief moments (mean, variance) or a set of particles.

### Open Question 4
- Question: Can a causal modeling approach significantly reduce the policy search space compared to the POMDP model?
- Basis in paper: [explicit] The authors mention developing a different formalization using causal modeling which "allows us to significantly reduce the policy search space of the corresponding solution method."
- Why unresolved: The current paper focuses on the POMDP formalism; the causal approach is mentioned as ongoing development.
- What evidence would resolve it: A comparison of the state space complexity and learning convergence speeds between the POMDP model and the proposed causal model.

## Limitations
- The POMDP formulation assumes perfect knowledge of attack success probabilities and detection rates, which may not reflect real-world uncertainty
- The state space reduction techniques may lose critical information needed for optimal decision-making in more complex scenarios
- Computational efficiency improvements are shown relative to CARDIFF but not compared against other potential baseline approaches

## Confidence
- Performance superiority over CARDIFF: High confidence (demonstrated on benchmark scenarios)
- Generalizability to other attacker strategies: Medium confidence (only tested against B-LINE and MEANDER)
- Computational efficiency claims: Medium confidence (relative to single baseline)

## Next Checks
1. Test BF-PPO against a broader range of attacker strategies beyond B-LINE and MEANDER to assess robustness
2. Conduct ablation studies to quantify the individual contributions of particle filtering and PPO components to overall performance
3. Evaluate the approach on CAGE-3 benchmark (when available) to verify scalability to more complex network topologies