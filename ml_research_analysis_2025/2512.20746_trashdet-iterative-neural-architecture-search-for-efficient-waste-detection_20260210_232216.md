---
ver: rpa2
title: 'TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection'
arxiv_id: '2512.20746'
source_url: https://arxiv.org/abs/2512.20746
tags:
- search
- detection
- taco
- map50
- waste
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrashDet, a neural architecture search framework
  for efficient waste detection on constrained edge devices. The method employs a
  Once-for-All (OFA) style ResDets supernet and performs iterative evolutionary search
  that alternates between backbone and neck/head optimization, supported by a population
  passthrough mechanism and an accuracy predictor to reduce search cost and improve
  stability.
---

# TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection

## Quick Facts
- arXiv ID: 2512.20746
- Source URL: https://arxiv.org/abs/2512.20746
- Reference count: 32
- Primary result: Achieves 19.5 mAP50 with 30.5M parameters on TACO dataset, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters

## Executive Summary
This paper introduces TrashDet, an iterative neural architecture search framework for efficient waste detection on constrained edge devices. The method employs a Once-for-All (OFA) style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. The approach is evaluated on the TACO dataset and achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. On the MAX78002 microcontroller with the TrashNet dataset, specialized variants outperform the ai87-fpndetector baseline, achieving up to 88% energy reduction, 78% latency reduction, and 53% power reduction while maintaining superior accuracy.

## Method Summary
TrashDet uses an OFA-style ResDets supernet with ResNet backbone, FPN+PAN neck, and YOLO-style detection head. The search space includes depth 2-8 blocks per stage, width multipliers {0.8, 1.0, 1.25, 1.5}, and expansion ratios {0.20, 0.25, 0.35, 0.45, 0.55}. The method employs iterative evolutionary search that alternates between backbone optimization and head optimization, using a population passthrough mechanism with ratio ρ=0.5 to preserve high-quality architectural patterns. An accuracy predictor provides mAP50 estimation to reduce search cost. The search runs for 50 module swaps with population size 100, parent ratio 0.25, and mutation ratio 0.5.

## Key Results
- Achieves 19.5 mAP50 with 30.5M parameters on TACO dataset
- Improves accuracy by up to 3.6 mAP50 over prior detectors while using fewer parameters
- On MAX78002 microcontroller: up to 88% energy reduction, 78% latency reduction, and 53% power reduction compared to ai87-fpndetector baseline
- TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing architecture search into alternating backbone and head optimization reduces effective search space dimensionality while preserving coupled optimization benefits.
- Mechanism: The method uses cyclic coordinate-descent: fix head h^(t), optimize backbone b^(t+1) under budget τ_b; then fix b^(t+1), optimize h^(t+1) under budget τ_h. This modular formulation treats the joint architecture f=(b,h) as separable sub-problems, enabling evolutionary search to explore each subspace more thoroughly.
- Core assumption: The detection architecture's performance landscape permits productive alternating optimization without requiring simultaneous joint optimization of all components.
- Evidence anchors: [abstract]: "iterative evolutionary search that alternates between backbone and neck/head optimization"; [Section 3]: "we decompose the architecture into two components: f = (b, h)" with iterative minimization formulation; [corpus]: ELASTIC paper (arXiv:2503.21999) describes similar iterative modular search for microcontroller detection.

### Mechanism 2
- Claim: Population passthrough preserves high-quality architectural patterns across module alternations, stabilizing search in non-convex landscapes.
- Mechanism: Each module maintains a memory buffer M_m storing top performers. When revisiting module m at iteration t, population P^(t) combines elite passthrough (Top_{ρN}(M_m)) with fresh diversity sampling (Equation 8). The passthrough ratio ρ=0.5 balances exploitation of validated patterns against exploration of novel configurations as the complementary module evolves.
- Core assumption: Architectural patterns that performed well with one complementary module configuration remain valuable building blocks when that complementary module is refined.
- Evidence anchors: [abstract]: "supported by a population passthrough mechanism...to reduce search cost and improve stability"; [Section 4.2]: Detailed formulation of Elite Passthrough + Diversity Augmentation population construction; [corpus]: No direct corpus evidence for population passthrough mechanism specifically.

### Mechanism 3
- Claim: A learned accuracy predictor provides reliable mAP50 estimation, enabling efficient search without prohibitive full evaluation costs.
- Mechanism: Rather than computing expensive mAP50 via inference on TACO validation set for every candidate, the predictor estimates expected mAP50 based on architectural configuration features. This proxy evaluation accelerates the evolutionary loop while maintaining selection pressure toward accurate architectures.
- Core assumption: The accuracy predictor generalizes across the architectural search space and maintains strong correlation with ground-truth mAP50 throughout search.
- Evidence anchors: [abstract]: "accuracy predictor to reduce search cost and improve stability"; [Section 5.1.2]: "our predictor generates a lightweight performance estimate that guides the search dynamics"; [corpus]: Fast Data Aware NAS (arXiv:2502.12690) mentions "Supernet Accelerated Evaluation," aligning with proxy-based evaluation strategies.

## Foundational Learning

- Concept: **Once-For-All (OFA) Weight-Sharing Supernetworks**
  - Why needed here: The entire approach relies on training one overparameterized network that supports many sub-networks with different depth/width/expansion configurations, enabling fast evaluation without retraining each candidate.
  - Quick check question: Explain why OFA's progressive shrinking schedule trains large and small sub-networks together—what would happen if only full-width networks were trained?

- Concept: **Evolutionary Architecture Search (Mutation, Crossover, Tournament Selection)**
  - Why needed here: The search space is discrete and combinatorial (depth choices, width multipliers, expansion ratios); gradient-based optimization cannot directly handle integer architectural decisions.
  - Quick check question: What is the role of tournament selection vs. mutation in maintaining exploration-exploitation balance?

- Concept: **Hardware-Aware NAS Constraints**
  - Why needed here: TinyML deployment requires architectures that satisfy device-specific limits (operator support, activation memory, layer count) before accuracy can even be evaluated.
  - Quick check question: The MAX78002 restricts activation memory to ~80 KiB and limits CNN layers to 128—how would violating these constraints manifest at deployment time?

## Architecture Onboarding

- Component map: OFA Supernet Core -> Iterative Search Controller -> Population Manager -> Accuracy Predictor -> Hardware Constraint Checker

- Critical path:
  1. **Supernet training**: Train OFA ResDets supernet on TACO with progressive shrinking
  2. **Backbone search iteration**: Initialize with warm-start from previous iteration; evolutionary search with constraint-aware mutation; evaluate via accuracy predictor; tournament selection
  3. **Head search iteration**: Fix discovered backbone; repeat evolutionary cycle on neck/head configuration
  4. **Population passthrough**: Transfer top-ρN architectures to next module's initial population
  5. **Terminate after 50 module swaps**: Select final architecture, fine-tune, deploy

- Design tradeoffs:
  - **Passthrough ratio ρ**: Higher values (e.g., 0.7) prioritize exploitation but risk premature convergence; lower values (e.g., 0.3) increase diversity but may discard useful patterns
  - **Module swap budget**: 50 swaps provides more refinement opportunities but increases total search time; fewer swaps may miss better optima
  - **Predictor fidelity vs. speed**: More accurate predictors require larger training sets and longer inference; simpler predictors may introduce selection noise

- Failure signatures:
  - **Search oscillation**: mAP50 proxy fails to improve over successive iterations; indicates passthrough may be preserving incompatible elites or budget allocation is unbalanced
  - **Constraint violations in final architecture**: Deployed model exceeds activation memory or layer count; constraint checker is not properly integrated into mutation/crossover operators
  - **Predictor-true correlation collapse**: Large scatter in Figure 3 pattern; predictor was trained on architectures outside the active search region
  - **Supernet weight interference**: Sub-networks perform poorly despite strong proxy scores; progressive shrinking was insufficient or supernet training diverged

- First 3 experiments:
  1. **Validate accuracy predictor**: Sample 50 held-out architectures from supernet; compute true mAP50 vs. predictor estimates; verify correlation r>0.85 and no systematic bias
  2. **Ablate population passthrough**: Run search with ρ∈{0.0, 0.3, 0.5, 0.7, 1.0} on TACO 5-class subset; plot final mAP50 vs. ρ to identify optimal balance
  3. **Compare single-stage vs. iterative search**: Run joint backbone+head evolutionary search (no alternation) with same total evaluation budget; compare final mAP50 and search stability to iterative approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the iterative decomposition of backbone and head optimization converge to globally optimal or near-optimal architectures, or does it merely find locally good solutions within each module's restricted search space?
- Basis in paper: [explicit] The paper states: "The search problem remains non convex and combinatorial, especially when including choices for backbone, neck, and head" and that heuristic strategies "offer no optimality guarantees."
- Why unresolved: The alternating minimization approach reduces dimensionality but may miss globally optimal joint configurations that would emerge from simultaneous optimization.
- What evidence would resolve it: A comparison against exhaustive joint search on a reduced search space, or theoretical analysis proving convergence bounds for the iterative decomposition.

### Open Question 2
- Question: How well does the learned accuracy predictor generalize to architectures outside the distribution of subnets encountered during its training?
- Basis in paper: [inferred] The accuracy predictor is trained to estimate mAP50 for candidate subnets, but Figure 3 only shows correlation on evaluated candidates. No analysis is provided on predictor robustness to out-of-distribution architectures or different datasets.
- Why unresolved: If the predictor fails to generalize, the search may be biased toward architectures similar to those seen during predictor training, limiting exploration.
- What evidence would resolve it: Evaluation of predictor error on held-out architectural configurations, or analysis of search trajectories with and without the predictor.

### Open Question 3
- Question: How sensitive is the TrashDet framework to the choice of population passthrough ratio (ρ=0.5) and other evolutionary hyperparameters across different hardware constraints and datasets?
- Basis in paper: [inferred] The paper uses fixed hyperparameters (ρ=0.5, population size 100, mutation ratio 0.5) without ablation studies. The effectiveness of population passthrough is claimed but not systematically validated.
- Why unresolved: Optimal hyperparameters may vary significantly across different search spaces, constraint budgets, or target devices.
- What evidence would resolve it: Ablation studies varying ρ across different constraint levels and datasets, measuring convergence speed and final architecture quality.

### Open Question 4
- Question: Can the TrashDet framework scale effectively to the full 60-class TACO dataset with severe class imbalance, or does the iterative search struggle with increased detection complexity?
- Basis in paper: [inferred] The paper evaluates only on a curated 5-class subset (paper, plastic, bottle, can, cigarette) to "ensure consistency," but TACO's full 60-class setting with "high degree of class imbalance" remains unexplored.
- Why unresolved: Longer-tailed distributions may require different architectural trade-offs between recall and precision that the current search objective (mAP50) does not capture.
- What evidence would resolve it: Full TACO benchmark results comparing TrashDet against baselines, potentially with class-aware search objectives.

## Limitations
- The accuracy predictor's architecture and training methodology are underspecified, making it difficult to assess reliability or reproduce results
- Hardware constraint implementation details for the MAX78002 are minimal, particularly regarding streaming mode activation memory calculations
- The evolutionary search hyperparameters are provided but their sensitivity to these values is not explored

## Confidence
- **High Confidence**: The iterative alternating optimization mechanism is well-described and logically sound; the population passthrough mechanism is clearly formulated; hardware-aware constraint enforcement is explicitly mentioned.
- **Medium Confidence**: The mAP50 improvements over prior work are credible given the methodology, though exact comparison conditions could be more detailed; the parameter-count ranges are verifiable but deployment metrics require hardware-specific validation.
- **Low Confidence**: The accuracy predictor's generalization capabilities across the full search space; the stability of alternating optimization across different hardware targets; the reproducibility of the exact supernet training procedure.

## Next Checks
1. **Predictor Generalization Test**: Evaluate the accuracy predictor on 100 randomly sampled architectures from outside the primary search space to measure prediction error distribution and identify potential overfitting to the training distribution.
2. **Module Swap Convergence Analysis**: Track mAP50 proxy scores and population diversity metrics across all 50 module swaps to identify at which iteration(s) performance plateaus or begins degrading, informing optimal stopping criteria.
3. **Hardware Constraint Verification**: Deploy the discovered TrashDet variants on actual MAX78002 hardware (or accurate cycle-accurate simulator) to measure real-world activation memory usage, layer count compliance, and verify that theoretical constraints match practical deployment limits.