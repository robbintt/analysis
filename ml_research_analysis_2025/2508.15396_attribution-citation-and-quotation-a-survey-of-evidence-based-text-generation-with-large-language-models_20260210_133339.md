---
ver: rpa2
title: 'Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation
  with Large Language Models'
arxiv_id: '2508.15396'
source_url: https://arxiv.org/abs/2508.15396
tags:
- text
- linguistics
- association
- generation
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive survey of evidence-based
  text generation with large language models (LLMs), systematically analyzing 134
  papers, 300 evaluation metrics, 17 frameworks, 231 datasets, and 11 benchmarks.
  The study addresses the fragmentation in the field caused by inconsistent terminology
  and isolated evaluation practices.
---

# Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation with Large Language Models

## Quick Facts
- arXiv ID: 2508.15396
- Source URL: https://arxiv.org/abs/2508.15396
- Reference count: 40
- 134 papers analyzed; 300 evaluation metrics identified; 17 frameworks cataloged; 231 datasets reviewed; 11 benchmarks assessed

## Executive Summary
This paper presents the first comprehensive survey of evidence-based text generation with large language models (LLMs), systematically analyzing 134 papers to address the fragmentation in the field caused by inconsistent terminology and isolated evaluation practices. The study introduces a unified taxonomy covering seven attribution approaches, five citation characteristics, and six tasks, revealing that non-parametric attribution dominates with post-retrieval being the most common approach. Key challenges identified include the need for parametric attribution methods, standardized evaluation frameworks, and explainable citations. The survey provides a structured reference for advancing the reliability and verifiability of LLMs in evidence-based text generation.

## Method Summary
The study conducted a systematic mapping of evidence-based text generation with LLMs through a search string `("large language model" OR "llm") AND ("citation" OR "attribution" OR "quote")` across nine databases (ACL Anthology, ACM DL, arXiv, IEEE Xplore, Springer, ICLR/ICML/NeurIPS proceedings) with February 2025 cutoff. Papers underwent dual-author screening with three inclusion criteria: generates text with LLMs, incorporates references to evidence, and English with accessible full text. An iterative categorization scheme was developed and applied to classify papers by attribution approach (7 types), LLM integration, citation characteristics (5 types), and task (6 types), yielding counts of 134 papers, 300 metrics, 17 frameworks, 231 datasets, and 11 benchmarks.

## Key Results
- Post-retrieval attribution dominates (73/134 papers), making it the most common architectural approach
- Non-parametric attribution methods account for 126/134 papers in the surveyed corpus
- Evaluation practices are highly fragmented with only two frameworks and benchmarks in common use across the field

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Conditioned Grounding (Post-Retrieval)
Providing external evidence to the model before generation conditions output probabilities on retrieved context, treating external evidence as primary source of truth for specific queries. This bypasses the need for specific facts to be encoded in model weights. The retrieval system must successfully identify and rank relevant documents higher than the model's propensity for hallucination. Core assumption is that retrieval recall is sufficient and the model attends to context tokens. Break condition occurs when retrieval fails or the model ignores context.

### Mechanism 2: NLI-Based Attribution Verification
Natural Language Inference models automate verification of whether generated claims are supported by citations. The NLI model acts as external critic, classifying (Generated Claim, Cited Source) pairs as Entailment, Neutral, or Contradiction. This decouples generation from verification. Core assumption is that general-purpose NLI models transfer effectively to specialized domains and LLM-generated text stylistic nuances. Break condition occurs with multi-hop reasoning or visual evidence that text-only NLI models cannot parse.

### Mechanism 3: Zero-Shot Prompting for Citation Generation
Instruction-following capabilities allow elicitation of citation behavior without explicit weight updates. The prompt explicitly instructs the model to output citation markers and corresponding references, leveraging pre-existing understanding of citation structures from training data. Core assumption is that training data contained sufficient citation examples to generalize to specific formatting requirements. Break condition occurs with complex formatting requirements or niche citation styles that contradict model priors.

## Foundational Learning

**Parametric vs. Non-Parametric Knowledge**: This is the primary distinction in the survey's taxonomy. You must understand if your system is relying on the model's "memory" (weights) or external "tools" (retrieval) to solve the attribution problem. Quick check: If I disconnect the model from the internet, can it still cite the source? (Parametric vs. Non-Parametric)

**Attribution vs. Correctness**: The survey explicitly separates these evaluation dimensions. A model can be perfectly attributed (faithful to source) but factually wrong if source itself is wrong, or correct but unattributed (hallucinated). Quick check: Does the evaluation metric check if the answer is true (Correctness) or if the answer is supported by the cited text (Attribution)?

**Citation Granularity (Evidence Level)**: The survey identifies Document, Paragraph, Sentence, and Token levels. System complexity changes drastically if you need to cite a whole PDF vs. a specific sentence. Quick check: Can the user click the citation and immediately see the claim verified, or do they have to scroll/search the document?

## Architecture Onboarding

**Component map**: Retriever -> Context Builder -> LLM Generator -> Evaluator
The standard "Post-Retrieval" pipeline dominates (73/134 papers).
1. Retriever: Fetches documents (Texts, Graphs, Tables)
2. Context Builder: Injects evidence into prompt (In-Context Attribution)
3. LLM Generator: Produces text + Citations (Zero-shot or Fine-tuned)
4. Evaluator (Automated): NLI model to verify Citation Precision/Recall

**Critical path**: Retrieval Quality → Context Window Management → Citation Formatting
Note: If the Retriever fails, the LLM often hallucinates citations to satisfy the prompt (Fabricated Citations)

**Design tradeoffs**:
Parametric vs. Non-Parametric: Parametric (Pure LLM) is faster but limited to training data cutoffs and harder to audit. Non-Parametric (RAG) is more verifiable but slower and dependent on retrieval latency.
Post-Generation vs. In-Generation: Post-generation (e.g., RARR) revises text after it's written (better preservation of intent). In-generation (e.g., Self-RAG) interrupts generation to retrieve (better factual grounding but potential flow disruption)

**Failure signatures**:
Fabricated Citations: The model invents a source to satisfy a "cite your sources" prompt when retrieval returns empty
Citation Misalignment: The citation exists but does not support the specific claim it is attached to (Low Citation Precision NLI)
Over-citation: The model cites the entire document for every sentence (Low Granularity)

**First 3 experiments**:
1. Baseline ALCE Evaluation: Run standard Zero-Shot LLM (e.g., GPT-4o or Llama-3) on ALCE benchmark to establish baseline Citation Recall and Precision
2. Retrieval Ablation: Compare "Post-Retrieval" (RAG) against "Post-Generation" (Generate then cite) on dataset like ELI5 to measure tradeoff between answer fluency and attribution accuracy
3. NLI Validation: Implement "Citation NLI" metric to verify if generated claims are actually entailed by retrieved chunks versus using simple lexical overlap (ROUGE)

## Open Questions the Paper Calls Out
None

## Limitations
- Literature search may have missed papers using alternative terminology or published in venues not covered by selected databases
- Taxonomy development relied on iterative refinement with dual-author screening but lacks specific inter-annotator agreement metrics and conflict resolution protocols
- Practical efficacy claims for zero-shot prompting and NLI-based verification are primarily supported by adoption rates rather than direct experimental validation

## Confidence
**High Confidence**: Identification of 134 papers, 300 evaluation metrics, 17 frameworks, 231 datasets, and 11 benchmarks is verifiable through systematic search protocol. Dominance of post-retrieval approaches (73/134 papers) and non-parametric attribution (126/134 papers) represents robust quantitative findings.

**Medium Confidence**: Taxonomy categorization scheme is comprehensive but may have boundary cases where papers could reasonably fit multiple categories. Analysis of evaluation practices and fragmentation challenges are well-supported but could evolve as field matures.

**Low Confidence**: Practical efficacy claims for zero-shot prompting and NLI-based verification are primarily supported by correlation with adoption rates rather than direct experimental validation within this survey. Transferability assumptions for NLI models to specialized domains require empirical verification.

## Next Checks
1. **Replication Validation**: Replicate literature search using identical parameters across all nine databases with February 2025 cutoff to verify 134-paper count and assess database coverage completeness
2. **Taxonomy Boundary Testing**: Conduct inter-annotator reliability assessment on subset of 20 papers to measure categorization consistency and identify taxonomy refinement needs
3. **NLI Transferability Experiment**: Evaluate performance of general-purpose NLI models (e.g., RoBERTa-Large MNLI) on domain-specific citation verification tasks to empirically assess transferability assumption