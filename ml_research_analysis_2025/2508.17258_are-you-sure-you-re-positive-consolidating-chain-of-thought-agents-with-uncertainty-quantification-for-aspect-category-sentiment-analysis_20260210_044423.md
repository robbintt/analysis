---
ver: rpa2
title: Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty
  Quantification for Aspect-Category Sentiment Analysis
arxiv_id: '2508.17258'
source_url: https://arxiv.org/abs/2508.17258
tags:
- list
- pairs
- agents
- sentiment
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates zero-shot aspect-category sentiment analysis
  (ACSA) using large language models (LLMs) without requiring labeled training data.
  The authors propose a method that employs multiple chain-of-thought (CoT) agents,
  each following a different reasoning sequence for extracting category-sentiment
  pairs, and aggregates their outputs using token-level uncertainty scores from the
  LLMs.
---

# Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis

## Quick Facts
- arXiv ID: 2508.17258
- Source URL: https://arxiv.org/abs/2508.17258
- Reference count: 30
- Primary result: Multiple CoT agents with uncertainty aggregation significantly improve zero-shot ACSA recall

## Executive Summary
This paper investigates zero-shot aspect-category sentiment analysis (ACSA) using large language models without requiring labeled training data. The authors propose a method that employs multiple chain-of-thought (CoT) agents, each following a different reasoning sequence for extracting category-sentiment pairs, and aggregates their outputs using token-level uncertainty scores from the LLMs. Experiments with 3B and 70B parameter Llama and Qwen models on four datasets show that combining multiple CoT agents significantly improves recall compared to individual agents. The highest probability list aggregation technique consistently yields the best results, and token confidence scores are found to correlate with prediction accuracy.

## Method Summary
The method employs six CoT agents with different permutations of reasoning order (aspect→category→opinion variations) to extract (category, polarity) tuples from text. Each agent uses enumerated prompting format and greedy decoding, generating outputs with token log-probabilities. The approach aggregates results using the "highest probability list" technique, selecting the complete output from the agent with the highest average confidence score across all generated pairs. The method is evaluated on four datasets (Laptop16, Restaurant16, MAMS, Shoes) using micro-F1 score as the primary metric.

## Key Results
- Combining multiple CoT agents improved recall by approximately 20% across all datasets compared to individual agents
- The highest probability list aggregation technique consistently outperformed other aggregation methods
- Token confidence scores showed positive correlation with prediction accuracy
- 70B+ models demonstrated better precision-recall balance than 3B models, which generated 6x more pairs with many irrelevant or conflicting predictions

## Why This Works (Mechanism)

### Mechanism 1: Diverse CoT Orderings as an Ensemble
- Claim: Aggregating outputs from multiple Chain-of-Thought agents with different reasoning sequences significantly improves recall compared to any single agent.
- Mechanism: The paper systematically permutes the order of three extraction elements—aspect terms, categories, and opinions—to create multiple distinct reasoning paths for the LLM. Each path (e.g., Opinion→Category→Aspect vs. Aspect→Category→Opinion) directs the model's attention differently. Aggregating the results, such as through a "Highest probability list," combines these diverse outputs, where one agent may successfully identify a tuple that another misses.
- Core assumption: The underlying LLM is not consistently biased toward a single optimal reasoning path for all inputs, and different paths are complementary rather than purely redundant.
- Evidence anchors:
  - [abstract] "The authors propose a method that employs multiple chain-of-thought (CoT) agents, each following a different reasoning sequence... combining multiple CoT agents significantly improves recall compared to individual agents."
  - [section 5, Joined CoT Agent] "...precision decreased, but recall was higher than that of any individual CoT agent. Notably...the average recall improvement was approximately 20% across all datasets..."
- Break condition: The mechanism fails if the LLM's performance is dominated by a single optimal reasoning path that all agents converge on, or if the diverse paths introduce excessive noise, causing a catastrophic drop in precision without a compensating recall gain.

### Mechanism 2: Token-Level Uncertainty for Aggregation
- Claim: Token-level confidence scores from the LLM correlate with prediction accuracy and can be used to rank and select the best predictions.
- Mechanism: The method uses the log-probabilities the LLM assigns to each generated token. It calculates an average probability (confidence score) for each category-polarity pair and for an agent's entire list. The "Highest probability list" aggregation technique selects the complete output from the single agent whose list has the highest average confidence, treating the model's own uncertainty estimate as a proxy for correctness.
- Core assumption: The LLM's confidence scores are a reliable proxy for prediction accuracy, implying the model's probability estimates are well-calibrated for the ACSA task.
- Evidence anchors:
  - [abstract] "...aggregates their outputs using token-level uncertainty scores from the LLMs. ...token confidence scores are found to correlate with prediction accuracy."
  - [section 5, CoT Aggregation Techniques] "...the highest probability list provided the best overall results amongst all the aggregation techniques...suggesting that the confidence provided by the LLMs can be useful."
  - [section 5, LLMs Confidence Interpretation] "...we observed a positive correlation between the averaged probabilities across agents and the F-scores..."
- Break condition: The mechanism fails if the LLM's confidence scores are poorly calibrated, consistently giving high scores to incorrect answers (hallucinations). The paper explicitly flags this risk, citing Shorinwa et al. (2024) in future work.

### Mechanism 3: Dataset-Dependent CoT Ordering
- Claim: The optimal sequence for Chain-of-Thought reasoning is not universal but varies by dataset, challenging prior assumptions about a fixed "Aspects → Opinions" order.
- Mechanism: The inherent structure and linguistic patterns of a dataset may favor a particular reasoning flow. For instance, one dataset might be easier to parse by first identifying strong opinion words, while another benefits from starting with explicit aspect terms. The paper's multi-agent approach is a practical solution to this variability.
- Core assumption: Datasets have intrinsic properties that align more closely with one reasoning sequence than others, and these properties are discoverable.
- Evidence anchors:
  - [section 5, Element Order Debunking] "Our experiments...demonstrated that the order of the three elements...was fairly consistent, but it differed for each dataset."
  - [section 5, Element Order Debunking] "...as opposed to previous research...starting from the aspects in the CoT prompts does not always provide optimal results."
- Break condition: The advantage is lost if a chosen CoT order is consistently suboptimal for the target data. In a true zero-shot setting, the inability to identify the single best order makes the multi-agent aggregation approach necessary.

## Foundational Learning

- Concept: **Aspect-Category Sentiment Analysis (ACSA)**
  - Why needed here: This is the paper's core task. Unlike standard sentiment analysis, it requires mapping text spans to predefined categories (e.g., "Food", "Service") and a polarity.
  - Quick check question: Given the review "The pizza was great but the waiter was rude," what are the correct ACSA tuples?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The method instructs an LLM to follow a sequence of reasoning steps. Understanding how to structure a prompt to elicit this step-by-step reasoning is essential for implementing the agents.
  - Quick check question: How does a "multi-hop" CoT differ from a single-prompt "enumerated" CoT as described in the paper?

- Concept: **Uncertainty Quantification (token log-probabilities)**
  - Why needed here: The primary aggregation mechanism relies on using confidence scores from the model. Understanding how to access and interpret these scores is a critical technical prerequisite.
  - Quick check question: In an LLM's output, what does a higher log-probability for a generated token typically signify?

## Architecture Onboarding

- Component map:
  CoT Agents (6 permutations) -> LLM Core -> Post-Processor (parse output, map categories) -> Uncertainty Calculator (token probabilities) -> Aggregation Module (select best output)

- Critical path:
  1. Define the predefined set of aspect categories for the target domain.
  2. For each input text, run it through all 6 CoT agents in parallel.
  3. For each agent's output, parse it and calculate its confidence score.
  4. Feed all agents' parsed outputs and scores into the chosen Aggregation Module to produce the final prediction.

- Design tradeoffs:
  - **Aggregation Strategy:** "Highest probability list" is the top performer but requires confidence scores. "Most common list" (majority voting) is a strong alternative that doesn't rely on scores.
  - **Model Size:** 70B+ models are more reliable but expensive. 3B models are cheaper but produce more conflicting pairs (e.g., `('Food', 'positive')` and `('Food', 'negative')` for the same text).
  - **Prompting Method:** The paper finds single-prompt "enumerated" CoT simpler and as effective as more complex multi-hop approaches.

- Failure signatures:
  - **High Conflict:** A high rate of agents producing different polarities for the same category, especially in smaller models.
  - **Over-generation:** Excessively high recall with very low precision, a trait of smaller models.
  - **Poor Calibration:** When confidence scores do not predict accuracy, the "Highest probability list" aggregation will fail.

- First 3 experiments:
  1. **Baseline Establishment:** Run a simple zero-shot prompt on a validation set to establish a single-agent baseline F1 score.
  2. **CoT Ordering Ablation:** Implement and run all 6 CoT agent permutations on the same set. Measure F1 for each to confirm performance varies by dataset and no single order is best.
  3. **Aggregation Comparison:** Implement "Highest probability list" and "Most common list" methods. Combine the outputs from experiment #2 and measure the aggregated F1 score to verify it exceeds the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more robust methods for estimating token-level confidence scores improve downstream ACSA performance compared to raw log probabilities?
- Basis in paper: [explicit] Section 6 states: "Research has raised concerns regarding the reliability of token-level uncertainty, as token conditional distributions may sometimes be misleading... Investigating more robust methods for estimating token-level confidence scores could improve the downstream ACSA performance or other tasks as well."
- Why unresolved: The paper relies on raw log probabilities, which can be unreliable when an initial incorrect token makes subsequent tokens appear highly probable; this cascading error is not addressed.
- What evidence would resolve it: Comparing alternative uncertainty quantification methods (e.g., ensemble-based, conformal prediction) against the token-level approach on the same datasets with statistical significance testing.

### Open Question 2
- Question: Would iteratively guiding the inference process until a satisfactory summed log probability threshold is reached improve the quality of extracted category-sentiment pairs?
- Basis in paper: [explicit] Section 6 states: "One possible direction for future research could be to iteratively guide the inference process until a satisfactory summed log probability is reached."
- Why unresolved: The current approach generates outputs in a single pass; iterative refinement based on confidence thresholds was proposed but not tested.
- What evidence would resolve it: Implementing an iterative inference loop with probability-based stopping criteria and measuring F1 score changes compared to single-pass generation.

### Open Question 3
- Question: How significantly does primacy bias (order bias) affect category selection in ACSA prompts, and does randomizing category order mitigate this?
- Basis in paper: [explicit] Section 8 states: "LLMs' generation may get biased towards an earlier exposed option when asked... it may have a tendency to select options presented at the beginning, ideally, we would need to change the order of the categories, but this would be computationally expensive to investigate."
- Why unresolved: The authors identified this limitation but did not experimentally quantify its impact or test mitigation strategies due to computational constraints.
- What evidence would resolve it: A controlled experiment comparing prediction distributions when category list order is systematically permuted versus fixed, measuring selection frequency per position.

## Limitations

- **Confidence calibration dependency**: The primary aggregation method relies on LLM token-level confidence scores correlating with prediction accuracy, which may not generalize across domains or newer model versions.
- **Dataset repurposing concerns**: The Shoes dataset was originally designed for ACOSI, not ACSA, requiring careful consideration due to potential discrepancies in category definition and evaluation standards.
- **Model size tradeoffs**: 3B models show significant performance degradation, generating 6x more pairs with many irrelevant or conflicting predictions, limiting practical deployment in resource-constrained settings.

## Confidence

- **High confidence**: Multiple CoT agents with different reasoning orders improve recall compared to single agents, supported by consistent experimental results across all four datasets.
- **Medium confidence**: Token-level uncertainty scores for aggregation are effective but depend on the assumption of good calibration, which may be specific to tested models and datasets.
- **Low confidence**: Generalizability of specific CoT orderings being optimal for particular datasets requires more extensive validation with only four datasets tested.

## Next Checks

1. **Calibration stress test**: Systematically evaluate how well LLM confidence scores predict accuracy across different confidence thresholds and model families, testing whether the positive correlation holds when models are intentionally prompted to be overconfident or when tested on adversarial examples.

2. **Dataset schema validation**: Conduct a systematic comparison between the original ACOSI schema used for the Shoes dataset and standard ACSA schemas to quantify the impact of repurposing on evaluation metrics and results interpretation.

3. **Resource-constrained deployment**: Implement a confidence threshold filtering mechanism for 3B model outputs and evaluate the precision-recall tradeoff to determine the minimum viable model size and confidence threshold combination that maintains acceptable performance while reducing computational costs.