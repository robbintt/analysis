---
ver: rpa2
title: Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial
  Attacks
arxiv_id: '2504.15479'
source_url: https://arxiv.org/abs/2504.15479
tags:
- counterfactual
- image
- counterfactuals
- scores
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for generating counterfactual
  explanations for image classifiers by performing gradient-based perturbations in
  the latent space of generative models. The method, Counterfactual Attacks, leverages
  the natural image manifold to create realistic counterfactual images without requiring
  complex regularization or hyperparameter tuning.
---

# Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks

## Quick Facts
- arXiv ID: 2504.15479
- Source URL: https://arxiv.org/abs/2504.15479
- Authors: Jeremy Goldwasser; Giles Hooker
- Reference count: 23
- One-line primary result: This paper presents a novel approach for generating counterfactual explanations for image classifiers by performing gradient-based perturbations in the latent space of generative models.

## Executive Summary
This paper introduces Counterfactual Attacks, a method for generating interpretable counterfactual explanations for image classifiers by performing gradient-based optimization in the latent space of generative models. The approach leverages the natural image manifold to create realistic counterfactual images without requiring complex regularization or hyperparameter tuning. The authors demonstrate their method on both MNIST and CelebA datasets, showing that it can produce high-quality counterfactuals that are both semantically meaningful and visually realistic. Additionally, they propose a framework for quantifying image counterfactuals with feature attributions, enabling the generation of global explanations that highlight the most important features driving model predictions.

## Method Summary
The method works by encoding an input image to a latent vector using a generative model's encoder, then performing gradient ascent in this latent space to maximize the target classifier's prediction for the desired class. The generator then decodes the optimized latent vector back to image space, producing the counterfactual. The approach is modular and can work with various generative models including VAEs, GANs, and diffusion models. For feature attribution, the authors train auxiliary attribute predictors in latent space and compute importance scores that quantify how each attribute contributes to the change between original and counterfactual images.

## Key Results
- The method generates realistic counterfactual images on both MNIST and CelebA datasets without requiring complex regularization or hyperparameter tuning
- Counterfactuals produced by latent-space optimization are semantically meaningful and visually coherent, unlike pixel-space adversarial attacks
- The feature attribution framework provides meaningful global explanations that highlight important features driving model predictions, with intuitive rankings for attributes like makeup, lipstick, and age on CelebA classifiers

## Why This Works (Mechanism)

### Mechanism 1: Latent-Space Gradient Ascent Produces Manifold-Constrained Counterfactuals
Performing gradient ascent in a low-dimensional latent space yields semantically meaningful image modifications while avoiding adversarial artifacts. The algorithm encodes image x to latent vector z via encoder E, then iteratively updates z ← z + η∇z f(G(z)) until the target prediction y′ is achieved. The generator G then decodes z back to image space, producing the counterfactual. The core assumption is that the latent space of the generative model approximately captures the natural image manifold, such that latent perturbations map to realistic image changes.

### Mechanism 2: Generative Model Architecture Enables Off-the-Shelf Deployment
Any generative model with a low-dimensional latent space and accurate reconstruction can be used, including VAEs, GANs, StyleGAN, and Latent Diffusion Models. The framework abstracts over E and G; the only requirement is that G(E(x)) ≈ x. This modularity permits leveraging state-of-the-art models like StyleGAN3 without modification. The core assumption is that GAN inversion or encoder-based embedding produces latent codes that both reconstruct well and lie in regions amenable to meaningful gradient-based editing.

### Mechanism 3: Feature Attribution Unifies Local Counterfactuals into Global Explanations
Auxiliary attribute labels enable quantification of counterfactual changes via trained attribute predictors, producing both local and globally aggregated importance scores. For each attribute a, train predictor g_a. The importance score ϕ_a(x, x′) = (g_a(x′) – g_a(x)) / diam(Y_a). Global scores ψ_a aggregate across samples, optionally weighted by counterfactual direction. The core assumption is that attribute predictors generalize well and the attribute set spans the semantically relevant transformations induced by counterfactuals.

## Foundational Learning

- Concept: Counterfactual Explanations
  - Why needed here: This is the core interpretability framework the paper extends to images; understanding the "what-if" formulation and the balance between prediction change and input proximity is essential.
  - Quick check question: Given a classifier f and input x with f(x) = 0.3, what would a counterfactual x′ satisfying f(x′) ≈ 0.7 represent?

- Concept: Latent Space and Generative Models (VAEs, GANs)
  - Why needed here: The method operates entirely in the latent space of generative models; grasping how encoders map images to z and how generators decode z back is critical.
  - Quick check question: If a VAE has 64-dimensional latent space and 28×28 input images, why does editing in latent space tend to produce more realistic outputs than editing in pixel space?

- Concept: Gradient-Based Optimization and Adversarial Attacks
  - Why needed here: The algorithm re-purposes gradient ascent (as in adversarial attacks) but constrains it to the latent manifold; understanding gradient computation through generator and predictor is necessary.
  - Quick check question: Why do standard gradient-based adversarial attacks on pixel space often yield imperceptible changes, and how does latent-space optimization differ?

## Architecture Onboarding

- Component map:
  - Encoder (E): Maps input image x → latent vector z (e.g., VAE encoder, GAN inversion)
  - Generator (G): Decodes latent vector z → reconstructed image (x_recon ≈ x) or counterfactual (x′)
  - Predictor (f): The model being explained; target is to achieve f(x′) ≈ y′
  - Attribute Predictors (g_a): One model per auxiliary attribute; used for feature attribution
  - Optimization Loop: Iteratively updates z via z ← z + η∇z f(G(z)) until f(G(z)) ≥ y′

- Critical path:
  1. Train or obtain off-the-shelf generative model (E, G)
  2. Train predictor f on task of interest
  3. For each image x: encode to z = E(x), run gradient ascent in latent space, decode to x′ = G(z′)
  4. (Optional) Train attribute predictors g_a and compute local/global importance scores

- Design tradeoffs:
  - Generative model choice: VAEs are easier to invert but may produce blurrier reconstructions; StyleGAN offers high-fidelity images but requires expensive inversion
  - Optimizer and learning rate: Paper uses Adam with η = 0.01 for CelebA; too large η may overshoot the manifold, too small may be slow or stuck
  - Attribute coverage: Limited attribute sets yield incomplete explanations; custom attributes require additional labeling

- Failure signatures:
  - Poor reconstruction: If G(E(x)) ≠ x, counterfactuals may be unrelated to the original image
  - Missing attribute coverage: Counterfactual changes not reflected in available labels
  - Non-termination: If target y′ is unreachable on the manifold, loop may run indefinitely without convergence

- First 3 experiments:
  1. MNIST sanity check: Train a simple VAE (64-dim latent) and 2-layer CNN classifier. Generate counterfactuals for misclassified digits; verify visual coherence and prediction flip
  2. CelebA attribute counterfactuals: Use pre-trained StyleGAN3. Train CNN classifiers for Smiling, Young, Male. Generate counterfactuals and qualitatively assess realism
  3. Feature attribution validation: Fit logistic regression attribute predictors in latent space. Compute local importance scores for counterfactuals and compare against manual annotation; aggregate to global scores and check for intuitive rankings

## Open Questions the Paper Calls Out
None

## Limitations
- The method's performance on complex, high-resolution images or non-celebrity domains remains unexplored
- The feature attribution framework's effectiveness is less thoroughly validated, with potential blind spots due to attribute label coverage
- Claims about the method's generality across diverse generative models and domains are not fully substantiated by the experiments presented

## Confidence
- **High Confidence**: The latent-space gradient ascent mechanism reliably produces counterfactual images on benchmark datasets, as demonstrated by consistent prediction changes and visual plausibility
- **Medium Confidence**: The feature attribution framework meaningfully quantifies changes between original and counterfactual images, though the reliance on attribute label coverage introduces potential blind spots
- **Low Confidence**: Claims about the method's generality across diverse generative models and domains are not fully substantiated by the experiments presented

## Next Checks
1. **Attribute Predictor Generalization**: Evaluate attribute predictors on a held-out test set to verify they capture the same features as the main classifier, addressing potential label shift between f and g_a
2. **Cross-Domain Robustness**: Apply the method to a domain with different image characteristics (e.g., medical imaging or natural scenes) to test the latent-space optimization's stability outside celebrity faces and digits
3. **Ablation on Attribute Coverage**: Systematically remove key attributes from the attribution analysis and measure the impact on importance score quality, validating the claim that missing attributes lead to incomplete explanations