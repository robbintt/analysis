---
ver: rpa2
title: 'Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants
  Evaluate and Envision Its Use'
arxiv_id: '2505.15596'
source_url: https://arxiv.org/abs/2505.15596
tags:
- feedback
- they
- rubric
- rubrics
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explored whether AI-generated feedback can serve as
  effective suggestions to assist human instructors in grading economics essays. A
  feedback engine was developed that decomposes the feedback generation process into
  three steps: identifying relevant sentences, making rubric-based judgments, and
  generating targeted feedback.'
---

# Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use

## Quick Facts
- arXiv ID: 2505.15596
- Source URL: https://arxiv.org/abs/2505.15596
- Reference count: 33
- TAs found AI feedback more aligned with rubrics, better at providing praise and guiding questions, and potentially useful for expediting grading and improving consistency

## Executive Summary
This study explored whether AI-generated feedback can serve as effective suggestions to assist human instructors in grading economics essays. A feedback engine was developed that decomposes the feedback generation process into three steps: identifying relevant sentences, making rubric-based judgments, and generating targeted feedback. Five TAs participated in 20 think-aloud sessions comparing their handwritten feedback with AI-generated suggestions on 4 writing assignments. TAs found AI feedback more aligned with rubrics, better at providing praise and guiding questions, and potentially useful for expediting grading and improving consistency. However, AI feedback tended to be overly rigid in rubric application and lacked the holistic perspective TAs provided. Despite AI's imperfections, TAs found the intermediate outputs and sentence highlights valuable for understanding and correcting AI judgments, suggesting a promising human-AI collaborative approach to feedback provision.

## Method Summary
The study developed an AI feedback engine using GPT-4o that processes student essays through a 3-step pipeline: identifying relevant sentences via highlighting, making rubric-based judgments, and generating targeted feedback. The system uses few-shot learning with instructor examples and strict prompting constraints to avoid revealing answers. Five TAs participated in 20 think-aloud sessions, comparing their handwritten feedback with AI-generated suggestions on 4 writing assignments. The evaluation focused on qualitative feedback about alignment with rubrics, time-saving potential, and feedback quality, rather than measuring actual student learning outcomes.

## Key Results
- TAs found AI feedback more aligned with rubrics and better at providing praise and guiding questions than their handwritten feedback
- The system showed potential for expediting grading and improving consistency across graders
- TAs valued the intermediate outputs (sentence highlights) for understanding and correcting AI judgments, enabling a human-AI collaborative approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing feedback generation into sequential, visualized sub-tasks (retrieval → judgment → generation) increases instructor trust and adoption rates compared to end-to-end generation.
- **Mechanism:** By exposing the "evidence" (highlighted sentences) used for a judgment, the system allows users to verify the AI's grounding. If the grounding is correct but the judgment is wrong, the user can mentally correct the system; if the grounding is wrong, the user ignores the output. This transparency mitigates the "black box" problem.
- **Core assumption:** Instructors will actively verify the highlighted text rather than blindly accepting the final feedback message.
- **Evidence anchors:** [abstract] "The study highlights the importance of... decomposing the feedback generation task into steps and presenting intermediate results, in order for TAs to use the AI feedback."
- **Break condition:** If the highlighting mechanism (Step 1) retrieves irrelevant sentences (low precision), trust in the subsequent judgment (Step 2) and feedback (Step 3) collapses, causing users to ignore the tool.

### Mechanism 2
- **Claim:** Enforcing strict rubric adherence via LLMs improves feedback consistency but introduces a rigidity penalty that requires human moderation.
- **Mechanism:** The LLM strictly maps student input to the rubric definitions provided in the prompt. Unlike humans, it does not suffer from grading fatigue or drift. However, it lacks the "pragmatic" ability to accept alternative correct phrasings not explicitly defined in the rubric, leading to "false negative" feedback.
- **Core assumption:** The rubric provided to the LLM is comprehensive and explicitly defines acceptable alternatives.
- **Evidence anchors:** [abstract] "AI feedback... aligned more closely with rubrics... TAs perceived AI feedback as a valuable tool that could... enhance consistency."
- **Break condition:** If the rubric is sparse or ambiguous (e.g., "thoughtful solution"), the LLM hallucinates criteria or applies logic rigidly, generating feedback that confuses students.

### Mechanism 3
- **Claim:** Using LLMs as "drafters" for positive feedback (praise) and guiding questions compensates for the high cognitive cost of manual feedback composition.
- **Mechanism:** TAs often skip writing detailed praise or scaffolding questions due to time constraints (the "effort gap"). LLMs generate these effortlessly. TAs act as "editors," retaining the AI's generic praise and guiding questions while correcting the substantive content.
- **Core assumption:** The cognitive load of editing a draft is significantly lower than the load of generating feedback from scratch.
- **Evidence anchors:** [abstract] "AI feedback provided more praise, explanations, and guiding questions... TAs found AI feedback helpful as suggestions."
- **Break condition:** If the AI generates "false positive" praise (congratulating a student on a concept they actually misunderstood), it reinforces incorrect mental models unless the TA catches it.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** The system architecture relies on CoT to force the LLM to locate evidence ("highlight") before making a judgment. Without understanding CoT, one might attempt end-to-end generation, which the paper explicitly argues against for trust reasons.
  - **Quick check question:** Why does forcing the model to output "relevant sentences" before the "judgment" reduce hallucination risks for the user?

- **Concept: Knowledge-Intensive vs. Language-Intensive Evaluation**
  - **Why needed here:** The paper distinguishes economics essays (requiring precise conceptual accuracy) from English essays (language features). This distinction dictates the architecture's reliance on external rubrics rather than general LLM knowledge.
  - **Quick check question:** Why is GPT-4o's native "knowledge" of economics considered a liability (hallucination risk) rather than an asset in this architecture?

- **Concept: Human-in-the-Loop (HITL) Verification**
  - **Why needed here:** The entire workflow assumes the TA is the "safety layer." The system is designed to make verification fast (via highlights), not to automate the TA out of existence.
  - **Quick check question:** In the proposed workflow, should the TA read the essay first or the AI feedback first to prevent "anchoring bias"?

## Architecture Onboarding

- **Component map:** Student Essay + Detailed Rubric + Historical Feedback → Module A (Retrieval: Identify relevant sentences) → Module B (Judgment: Determine rubric compliance) → Module C (Generation: Draft feedback) → Visualization Layer (Word Plugin with highlights and comments)

- **Critical path:** The accuracy of Module A (Retrieval). If the highlight is wrong, the TA loses trust immediately. The paper notes that even if judgment is wrong, a correct highlight makes the error "easy to correct."

- **Design tradeoffs:**
  - Fragmentation vs. Consistency: Generating feedback per-rubric-item ensures coverage but creates a "fragmented" reading experience compared to a human's holistic summary.
  - Specificity vs. Flexibility: Detailed prompts (Table 1) improve accuracy but require significant upfront engineering of rubrics.

- **Failure signatures:**
  - The "Rigid Refusal": AI marks a correct answer wrong because the student used an alternative phrasing not listed in the rubric (Section 4.1).
  - The "Hallucinated Definition": AI enforces a definition (e.g., "quantity demanded") that is factually incorrect because it wasn't in the prompt context (Section 4.1).
  - The "Incoherent Flow": A series of technically correct but disjointed comments that fail to address the student's overall narrative.

- **First 3 experiments:**
  1. Rubric Ablation Test: Feed the engine essays using "Bad Rubric" examples vs. "Good Rubric" examples (from Table 1) to quantify the error rate delta in Module B (Judgment).
  2. Highlight Integrity Check: Measure TA "time-to-correction" when highlights are accurate vs. inaccurate. Does a bad highlight slow down the workflow significantly?
  3. Holistic vs. Fragmented A/B Test: Have TAs grade with AI feedback enabled vs. disabled to measure perceived consistency and actual grading speed, validating the "suggestion" hypothesis.

## Open Questions the Paper Calls Out

- **Question:** Does feedback created with AI-generated suggestions improve student learning outcomes compared to handwritten feedback alone?
  - **Basis in paper:** Authors state "future work could further investigate how feedback created with AI feedback as suggestions would impact students' learning outcomes."
  - **Why unresolved:** This study only examined TA perceptions and workflow benefits, not actual student performance or revision quality.
  - **What evidence would resolve it:** A randomized controlled trial comparing learning gains, revision quality, and concept mastery between students receiving AI-assisted versus purely human feedback.

- **Question:** Do students prefer point-to-point, rubric-aligned feedback over holistic feedback approaches?
  - **Basis in paper:** Authors note "TAs found AI feedback more detailed and aligned with the rubrics, while they usually adopt a more wholistic approach. Future work could explore whether such point-to-point feedback is preferable."
  - **Why unresolved:** Only TA perspectives were studied; student preferences and learning effects of fragmented versus holistic feedback remain unknown.
  - **What evidence would resolve it:** Student surveys and A/B testing comparing satisfaction and learning outcomes with each feedback style.

- **Question:** How does sustained use of AI feedback suggestions affect TA grading behavior and potential over-reliance over time?
  - **Basis in paper:** TAs expressed concerns about over-reliance ("You don't want to over-rely on it") and the study only involved 20 one-hour sessions with 5 TAs.
  - **Why unresolved:** Long-term behavioral changes, skill atrophy, or increased dependency cannot be observed in short-term think-aloud studies.
  - **What evidence would resolve it:** A longitudinal study tracking TA accuracy, error detection rates, and confidence levels across a full semester or multiple semesters of AI-assisted grading.

## Limitations

- The study's findings are based on qualitative feedback from only 5 TAs across 20 think-aloud sessions, limiting generalizability.
- The think-aloud method itself may have influenced participant behavior and feedback quality.
- The study did not measure actual time savings or grading accuracy improvements, relying instead on TA perceptions of potential benefits.

## Confidence

- **High Confidence:** The mechanism of decomposing feedback generation into sequential, visualized sub-tasks increases instructor trust and adoption rates.
- **Medium Confidence:** The claim that enforcing strict rubric adherence improves feedback consistency but requires human moderation.
- **Medium Confidence:** Using LLMs as "drafters" for positive feedback and guiding questions compensates for the high cognitive cost of manual feedback composition.

## Next Checks

1. **Rubric Ablation Test:** Feed the engine essays using "Bad Rubric" examples vs. "Good Rubric" examples to quantify the error rate delta in Module B (Judgment), validating whether rubric quality directly impacts LLM accuracy.

2. **Highlight Integrity Check:** Measure TA "time-to-correction" when highlights are accurate vs. inaccurate to determine if the retrieval module's accuracy is the critical path for system adoption.

3. **Holistic vs. Fragmented A/B Test:** Have TAs grade with AI feedback enabled vs. disabled to measure both perceived consistency and actual grading speed, providing quantitative validation of the "suggestion" hypothesis beyond qualitative perceptions.