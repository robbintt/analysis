---
ver: rpa2
title: Lightweight and Generalizable Acoustic Scene Representations via Contrastive
  Fine-Tuning and Distillation
arxiv_id: '2510.03728'
source_url: https://arxiv.org/abs/2510.03728
tags:
- contrastive
- acoustic
- scene
- classification
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ContrastASC, a two-stage framework that learns
  lightweight and generalizable acoustic scene representations through contrastive
  fine-tuning and distillation. The method addresses the challenge of acoustic scene
  classification models on edge devices, which typically lack transferability needed
  for real-world applications requiring adaptation to new or refined acoustic categories.
---

# Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation

## Quick Facts
- **arXiv ID:** 2510.03728
- **Source URL:** https://arxiv.org/abs/2510.03728
- **Reference count:** 0
- **Primary result:** ContrastASC achieves 3.3% and 2.0% 5-shot accuracy improvements on TUT17 and ICME24 open-set datasets versus conventional approaches.

## Executive Summary
This paper introduces ContrastASC, a two-stage framework that learns lightweight and generalizable acoustic scene representations through contrastive fine-tuning and distillation. The method addresses the challenge of acoustic scene classification models on edge devices, which typically lack transferability needed for real-world applications requiring adaptation to new or refined acoustic categories. ContrastASC first fine-tunes a pre-trained BEATs model using supervised contrastive learning to structure the embedding space and preserve semantic relationships between scenes. Then it applies contrastive representation distillation to transfer this structured knowledge to compact student models like CP-Mobile. The evaluation shows that ContrastASC achieves strong closed-set performance on the TAU22 dataset while demonstrating superior open-set generalization, with 5-shot accuracy improvements of 3.3% on TUT17 and 2.0% on ICME24 compared to conventional approaches.

## Method Summary
ContrastASC employs a two-stage approach for learning generalizable acoustic scene representations. In Stage 1, a pre-trained BEATs model is fine-tuned using mixup-aware supervised contrastive learning to create semantically structured embeddings. The contrastive objective preserves relationships between acoustic scenes rather than optimizing fixed decision boundaries. In Stage 2, this structured knowledge is transferred to compact student models through contrastive representation distillation, which preserves pairwise sample relationships in the embedding space. The framework is evaluated on TAU22 for closed-set performance and TUT17/ICME24 for open-set few-shot generalization, with 300 sampling repeats per few-shot evaluation.

## Key Results
- ContrastASC achieves 62.5% closed-set accuracy on TAU22 validation
- 5-shot accuracy improvements of 3.3% on TUT17 (56.3% vs 53.0%) and 2.0% on ICME24 (64.5% vs 62.5%) versus conventional fine-tuning + KD
- Student models with 6K-126K parameters maintain strong performance while enabling edge deployment
- Mixup-aware supervised contrastive loss and LayerNorm normalization contribute to improved transfer performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Supervised contrastive fine-tuning produces embeddings that generalize to unseen categories by preserving semantic structure rather than optimizing fixed decision boundaries.
- **Mechanism**: The contrastive objective pulls same-class embeddings together while pushing different-class embeddings apart, creating a geometrically organized space where semantic similarity correlates with embedding proximity. This structure transfers because new categories occupy meaningful positions relative to known ones.
- **Core assumption**: Acoustic scenes share underlying acoustic properties that map to consistent spatial relationships in embedding space.
- **Evidence anchors**:
  - [abstract] "structing the embedding space to preserve semantic relationships between scenes, enabling adaptation to unseen categories without retraining"
  - [section 1] "This contrasts with traditional cross-entropy training that optimizes decision boundaries for fixed classes, often resulting in representations that lack generalization capability"
  - [corpus] Weak direct validation; neighbor paper "Adaptive Knowledge Distillation using a Device-Aware Teacher" addresses device robustness but not open-set generalization specifically

### Mechanism 2
- **Claim**: Mixup-aware supervised contrastive loss enables semantic interpolation in embedding space, improving representation smoothness.
- **Mechanism**: Standard supervised contrastive loss requires discrete labels. The proposed formulation weights sample similarity by the dot product of mixup label vectors, allowing the loss to pull together embeddings proportionally to their label overlap.
- **Core assumption**: Mixup interpolation corresponds to meaningful semantic interpolation between acoustic scenes.
- **Evidence anchors**:
  - [section 2, Eq. 1] Full mathematical formulation of LSoft-SupCon with wik similarity weights
  - [section 2] "This formulation enables the contrastive loss to adapt to the continuous nature of mixup labels"
  - [corpus] No direct corpus validation of this specific technique

### Mechanism 3
- **Claim**: Contrastive Representation Distillation (CRD) transfers embedding structure to compact models more effectively than prediction-based distillation.
- **Mechanism**: CRD maximizes mutual information between teacher and student representations by preserving pairwise sample relationships. This transfers the relational geometry rather than just class probabilities.
- **Core assumption**: The student has sufficient capacity to approximate the teacher's relational structure.
- **Evidence anchors**:
  - [abstract] "contrastive representation distillation to transfer this structured knowledge to compact student models"
  - [section 3, Eq. 3] CRD loss formulation with positive/negative pairs
  - [section 4, Table 2] C-FT + CRD achieves 56.3% vs 53.0% (FT + KD) on TUT17 5-shot

## Foundational Learning

- **Concept: Supervised Contrastive Learning**
  - Why needed here: Understand why this produces transferable representations unlike cross-entropy
  - Quick check question: Given three samples (A, B from class 1; C from class 2), which loss term encourages A and B to cluster while separating from C?

- **Concept: Knowledge Distillation Objectives**
  - Why needed here: Distinguish between soft-label distillation (KD) and representation distillation (CRD)
  - Quick check question: What information is preserved when distilling output logits vs. intermediate embeddings?

- **Concept: Embedding Space Geometry**
  - Why needed here: Interpret how cosine similarity and normalization affect representation structure
  - Quick check question: Why does LayerNorm (sample-independent) improve transferability compared to BatchNorm (batch-dependent)?

## Architecture Onboarding

- **Component map**: Audio (16kHz) → [BEATs Encoder: 768-dim] → [2-layer MLP: 128-dim] → Contrastive Loss
                                      ↓
                              [Cosine Head: C classes] → Cross-Entropy Loss
                                      ↓
                              [CP-Mobile] → [AvgPool] → [LayerNorm] → [Cosine Head]
                                    ↓
                              [2-layer MLP: 128-dim] → CRD Loss (aligned with frozen teacher)

- **Critical path**: Cosine classification head → Mixup-aware contrastive loss → CRD projection alignment → LayerNorm normalization (not BatchNorm)

- **Design tradeoffs**:
  - Cosine head (γ=56) bounds logits but may underfit compared to linear head on seen classes
  - Two-phase training (frozen then joint) stabilizes projection head but adds complexity
  - LayerNorm improves transfer but may reduce closed-set accuracy slightly (Table 2: 60.6 vs 60.0 with BatchNorm)

- **Failure signatures**:
  - Closed-set accuracy drops significantly (>3%): Check λ balance (Eq. 2); contrastive loss may dominate
  - Open-set transfer fails: Verify projection head is frozen during distillation; check CRD temperature (τ=0.07)
  - Student underperforms baseline: Ensure 16kHz resampling applied to CP-Mobile frontend

- **First 3 experiments**:
  1. **Baseline validation**: Train BEATs with CE-only on TAU22; verify ~62.5% closed-set accuracy matches Table 1
  2. **Ablation: Loss components**: Remove L_Soft-SupCon (set λ=1.0); expect TUT17 5-shot drop from 62.3% to ~60.1%
  3. **Ablation: Normalization**: Replace LayerNorm with BatchNorm in student; expect ICME24 5-shot drop from 64.5% to ~62.2%

## Open Questions the Paper Calls Out
- **Open Question 1**: How does integrating teacher ensembling with the ContrastASC framework impact representation generalizability and few-shot adaptation performance? (The paper states future work could integrate teacher ensembling but does not explore it)
- **Open Question 2**: Why does restricting data augmentation to the projection head while using clean samples for the classification head yield lower validation loss? (The paper notes this empirical finding but provides no theoretical explanation)
- **Open Question 3**: Does the replacement of BatchNorm with LayerNorm consistently improve transferability across different student backbones, or is it specific to CP-Mobile? (The paper shows LayerNorm outperforms BatchNorm on CP-Mobile but doesn't validate on other architectures)

## Limitations
- The paper does not address robustness to domain shifts between training and evaluation datasets
- Closed-set evaluation on TAU22 (62.5%) is lower than standard cross-entropy training (64.2%), suggesting a potential tradeoff between generalization and peak performance
- The paper does not evaluate whether learned representations transfer to different acoustic tasks beyond scene classification

## Confidence
- **High confidence**: The two-stage contrastive fine-tuning and distillation framework is technically sound and well-implemented
- **Medium confidence**: The claimed 5-shot accuracy improvements (3.3% on TUT17, 2.0% on ICME24) are supported by the experimental protocol
- **Low confidence**: The paper does not provide ablations showing the individual contribution of mixup-aware SupCon versus standard SupCon

## Next Checks
1. **Ablation of normalization strategy**: Replace LayerNorm with BatchNorm in the student model and measure impact on both closed-set TAU22 accuracy and open-set TUT17/ICME24 transfer performance
2. **Ablation of contrastive distillation**: Remove the CRD component and compare open-set transfer performance against the full ContrastASC pipeline to isolate the contribution of representation-level distillation
3. **Domain shift robustness**: Train on TAU22 but evaluate on a dataset with different recording conditions to assess generalization beyond the evaluated cross-dataset transfer