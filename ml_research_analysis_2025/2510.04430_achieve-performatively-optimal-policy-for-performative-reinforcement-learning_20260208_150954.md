---
ver: rpa2
title: Achieve Performatively Optimal Policy for Performative Reinforcement Learning
arxiv_id: '2510.04430'
source_url: https://arxiv.org/abs/2510.04430
tags:
- policy
- uses
- learning
- where
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies performative reinforcement learning, where the
  agent's policy affects the environment dynamics, and aims to find the performatively
  optimal (PO) policy that maximizes the original value function. While existing works
  only find suboptimal performatively stable (PS) policies, this work proposes a zeroth-order
  Frank-Wolfe (0-FW) algorithm that converges to the desired PO policy.
---

# Achieve Performatively Optimal Policy for Performative Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.04430
- Source URL: https://arxiv.org/abs/2510.04430
- Reference count: 40
- Primary result: A zeroth-order Frank-Wolfe algorithm that converges to performatively optimal (PO) policies in polynomial time, whereas existing methods only find suboptimal performatively stable (PS) policies.

## Executive Summary
This paper addresses the challenge of finding performatively optimal policies in reinforcement learning settings where the agent's policy affects the environment dynamics. Unlike existing approaches that converge to performatively stable (PS) policies - which are optimal only for the environment they create - this work develops a zeroth-order Frank-Wolfe algorithm that provably finds the globally optimal PO policy. The key insight is that entropy regularization creates a gradient dominance property, ensuring that stationary points are globally optimal when the regularizer is sufficiently strong. The algorithm uses policy evaluations to estimate gradients without requiring explicit models of environment shifts, making it practical for black-box environments.

## Method Summary
The paper proposes a zeroth-order Frank-Wolfe (0-FW) algorithm for performative reinforcement learning. The algorithm operates by maintaining policies within a bounded subspace where gradients are Lipschitz continuous and smooth, using a lower bound constraint to prevent singularities. It estimates gradients through a two-point finite difference approach on random directions within the valid policy subspace, requiring only value function evaluations rather than explicit environment models. The Frank-Wolfe updates solve a linear optimization subproblem at each iteration to maintain feasibility within the constrained policy space. The method converges to an ϵ-stationary policy, which is also PO when the regularizer dominates environmental shifts.

## Key Results
- Proves that entropy-regularized value functions satisfy a gradient dominance property, making stationary points globally optimal under regularizer dominance
- Establishes policy lower bounds that enable finding stationary policies in bounded subspaces with Lipschitz continuous and smooth objectives
- Develops a zeroth-order gradient estimator tailored for policy spaces that approximates performative policy gradients
- Demonstrates polynomial-time convergence to PO policies under standard conditions
- Shows superior performance over repeated retraining methods in experiments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Finding a stationary point (gradient ≈ 0) is sufficient to find the globally Performatively Optimal (PO) policy, bridging the gap that leaves repeated retraining methods stuck at suboptimal Performatively Stable (PS) policies.
- **Mechanism:** The entropy-regularized value function satisfies a **gradient dominance property**. While the objective is nonconvex, this property ensures that any stationary point is a global maximizer (PO) provided the regularizer strength λ dominates the environmental shift sensitivity. This shifts the optimization target from a fixed-point stability (PS) to a global optimality condition.
- **Core assumption:** The regularizer strength λ must be sufficiently large relative to environmental sensitivity (ε_p, ε_r, S_p, S_r) such that μ ≥ 0 (regularizer dominance).
- **Evidence anchors:** [abstract] "proving the entropy-regularized value function satisfies a gradient dominance property, showing that any stationary point is a PO policy under regularizer dominance"
- **Break condition:** If environmental sensitivity is high and the regularizer coefficient λ is too low, μ < 0, and the dominance property fails; the algorithm may converge to a stationary point that is not PO.

### Mechanism 2
- **Claim:** Optimizing within a restricted policy subspace Π_Δ (where policy probabilities are lower-bounded by Δ > 0) guarantees bounded gradients and smoothness, preventing instability from singularities near zero-probability actions.
- **Mechanism:** The performative policy gradient involves a log π term, causing the gradient norm ||∇V|| → ∞ as π(a|s) → 0. The paper proves that sufficiently stationary policies naturally maintain a lower bound π_min. By enforcing a floor Δ, the objective function becomes Lipschitz continuous and smooth, which is required for the convergence of the Frank-Wolfe algorithm.
- **Core assumption:** The policy lower bound Δ is small enough to contain the target stationary policy but large enough to maintain smoothness (specifically Δ ≤ π_min/3).
- **Evidence anchors:** [abstract] "establishing a policy lower bound that enables finding stationary policies in a bounded subspace where the objective is Lipschitz continuous and smooth"
- **Break condition:** If the optimal policy requires a deterministic action (probability → 1, others → 0) that violates the lower bound Δ, the algorithm will find a smoothed approximation rather than the true optimum, or fail to converge if Δ is set too aggressively.

### Mechanism 3
- **Claim:** A zeroth-order gradient estimator can approximate the performative policy gradient using only policy evaluations (value estimates), bypassing the need for explicit models of environment shifts (∇p^π, ∇r^π).
- **Mechanism:** The algorithm uses a two-point estimation strategy, sampling random directions u on the linear subspace of valid probability shifts (∑ u = 0). By evaluating the policy at π + δu and π - δu, it constructs a gradient estimator that converges to the true gradient as sample size N increases and perturbation δ decreases.
- **Core assumption:** The policy evaluation oracle can provide accurate value estimates V^π (error ε_V is bounded) and the perturbation δ is small relative to the lower bound Δ.
- **Evidence anchors:** [abstract] "developing a zeroth-order gradient estimator tailored for the policy space"
- **Break condition:** If the policy evaluation is noisy (high ε_V) or the perturbation δ is too large, the gradient estimation error becomes unbounded, causing the Frank-Wolfe update to diverge.

## Foundational Learning

- **Concept: Performative Stability vs. Optimality**
  - **Why needed here:** You must understand the gap the paper is trying to close. Standard repeated retraining finds a **Performatively Stable (PS)** policy, which is optimal only for the environment *it creates*. This paper targets the **Performatively Optimal (PO)** policy, which maximizes value accounting for the environment shift *caused* by the policy.
  - **Quick check question:** Why does a PS policy generally yield lower value than a PO policy in a performative environment?

- **Concept: Frank-Wolfe (Conditional Gradient) Algorithm**
  - **Why needed here:** The paper uses Frank-Wolfe instead of Projected Gradient Descent. This algorithm solves constrained optimization (staying within the policy simplex) by solving a linear optimization sub-problem at each step. It avoids the projection step which can be problematic for the specific geometry of the policy subspace.
  - **Quick check question:** In a Frank-Wolfe step, how is the update direction determined, and how does the step size β control the movement?

- **Concept: Zeroth-Order Optimization**
  - **Why needed here:** Since ∇p^π is unknown, you cannot compute the analytical gradient. You need to understand how to estimate gradients using finite differences (two-point estimation) on random directions to implement the "0-FW" algorithm.
  - **Quick check question:** Why is the variance of a two-point gradient estimator typically lower than a one-point estimator, and how does the dimensionality of the action space (|S||A|) affect the sample complexity?

## Architecture Onboarding

- **Component map:** Environment Oracle -> Policy Evaluator -> Gradient Estimator -> Frank-Wolfe Solver -> Updated Policy

- **Critical path:**
  1. Initialize policy π_0 ∈ Π_Δ
  2. **Sample Directions:** Generate N random vectors u_i on the unit sphere within the valid policy subspace (L_0 ∩ U_1)
  3. **Estimate Values:** Evaluate V(π ± δu_i) using the Policy Evaluator
  4. **Construct Gradient:** Compute the zeroth-order gradient estimate ĝ
  5. **FW Update:** Find π̃ maximizing ⟨ĝ, π̃⟩ (analytical solution exists), update π_{t+1} = π_t + β(π̃ - π_t)
  6. **Repeat** until stationarity measure ⟨ĝ, π̃ - π⟩ ≤ ε

- **Design tradeoffs:**
  - **Regularizer λ:** Must be high enough to satisfy dominance (μ ≥ 0) but not so high it washes out the reward signal
  - **Perturbation δ:** Small δ reduces bias but increases sensitivity to policy evaluation noise (ε_V). Must be δ < Δ
  - **Lower Bound Δ:** Enforces strict exploration/smoothness but caps deterministic action probabilities at 1 - Δ(|A|-1)

- **Failure signatures:**
  - **Collapse to Determinism:** If Δ is not enforced or λ is too low, π(a|s) → 0 causing gradient explosion (NaNs)
  - **Stuck at PS:** If regularizer dominance is not met (μ < 0), the algorithm converges but to a suboptimal PS point, indistinguishable from PO without a baseline
  - **High Variance:** If N (batch size) is too low, the gradient estimates are noisy, causing oscillations in value

- **First 3 experiments:**
  1. **Validation of Gradient Dominance:** Implement 0-FW on a small gridworld where p^π has a known functional form. Sweep λ and plot the distance between the found stationary policy and the analytical PO to verify the μ ≥ 0 threshold
  2. **Baseline Comparison:** Compare 0-FW against the repeated retraining baseline (finding PS) on a recommender system simulation. Plot the *Performative Value* (V^π_{λ, π}) vs. timesteps to visualize the "constant gap" described in the abstract
  3. **Sensitivity Analysis:** Test the robustness of the zeroth-order estimator. Vary the number of samples N and the perturbation radius δ to confirm the error bound scaling described in Proposition 1

## Open Questions the Paper Calls Out
None

## Limitations
- The gradient dominance conditions require the regularizer strength λ to dominate environmental sensitivity, but the paper doesn't provide empirical validation of when this condition is met or violated in practice
- The policy lower bound enforcement requires Δ ≤ π_min/3, but the paper doesn't analyze how to set this bound in practice or what happens when optimal policies require deterministic actions
- The zeroth-order gradient estimator's variance and the optimal tradeoff between perturbation size δ, batch size N, and policy evaluation accuracy ε_V are theoretically bounded but not empirically characterized

## Confidence
- **High Confidence:** The gradient dominance property and its implications for finding PO policies when conditions are met; the Frank-Wolfe algorithm's ability to find stationary points under smoothness conditions
- **Medium Confidence:** The zeroth-order gradient estimator's convergence properties and the overall algorithm's ability to find PO policies in practice, given sensitivity to hyperparameters and environmental conditions
- **Low Confidence:** The practical implementation details for the policy lower bound and the specific conditions under which the algorithm transitions from finding PS to PO policies

## Next Checks
1. **Dominance Condition Verification:** Implement the algorithm on a gridworld with controllable environmental sensitivity and systematically sweep λ to empirically verify the threshold where μ ≥ 0 transitions to μ < 0, and observe the corresponding change in solution quality
2. **Deterministic Policy Analysis:** Test the algorithm on a problem where the optimal policy is deterministic. Quantify the performance gap between the found stationary policy (constrained by Δ) and the true optimal policy as Δ varies
3. **Estimator Variance Characterization:** Conduct a controlled experiment varying N and δ while measuring the variance of the gradient estimator and its impact on convergence speed and solution quality, comparing against the theoretical error bounds