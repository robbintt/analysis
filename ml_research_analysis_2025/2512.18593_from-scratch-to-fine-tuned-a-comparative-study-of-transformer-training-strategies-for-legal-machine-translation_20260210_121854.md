---
ver: rpa2
title: 'From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies
  for Legal Machine Translation'
arxiv_id: '2512.18593'
source_url: https://arxiv.org/abs/2512.18593
tags:
- legal
- translation
- language
- machine
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comparative study of Transformer-based training
  strategies for English-Hindi legal machine translation. Two approaches were evaluated:
  fine-tuning a pre-trained OPUS-MT model and training a Transformer model from scratch
  using legal-domain data.'
---

# From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation

## Quick Facts
- arXiv ID: 2512.18593
- Source URL: https://arxiv.org/abs/2512.18593
- Reference count: 7
- Pre-trained model fine-tuning achieved 46.03 SacreBLEU score, outperforming both baseline (9.39) and from-scratch (37.77) approaches

## Executive Summary
This paper presents a comparative study of Transformer-based training strategies for English-Hindi legal machine translation. The research evaluates two distinct approaches: fine-tuning a pre-trained OPUS-MT model versus training a Transformer model from scratch using legal-domain data. The study demonstrates that fine-tuning pre-trained models significantly enhances translation quality for specialized legal text, with the fine-tuned model achieving substantial improvements across multiple evaluation metrics. The findings highlight the effectiveness of domain adaptation through fine-tuning as a superior strategy for improving access to legal information in multilingual contexts.

## Method Summary
The study compares two training strategies for English-Hindi legal machine translation: (1) fine-tuning a pre-trained OPUS-MT model on legal-domain parallel data, and (2) training a Transformer model from scratch using the same legal-domain dataset. Both approaches are evaluated using multiple metrics including SacreBLEU, BERTScore, and COMET. The legal-domain data consists of parallel corpora specifically curated for legal translation tasks. The pre-trained OPUS-MT model serves as the starting point for the fine-tuning approach, while the from-scratch approach initializes all parameters randomly. The evaluation focuses on translation quality improvements and the effectiveness of domain adaptation strategies.

## Key Results
- Fine-tuned OPUS-MT achieved SacreBLEU score of 46.03, significantly outperforming baseline (9.39) and from-scratch (37.77) approaches
- Consistent performance improvements across multiple metrics: BERTScore (91.19) and COMET (73.72)
- Fine-tuning demonstrates superior domain adaptation for specialized legal text translation

## Why This Works (Mechanism)
The superior performance of fine-tuning pre-trained models stems from leveraging existing linguistic knowledge encoded in the pre-trained weights. The pre-trained OPUS-MT model has already learned general translation patterns and language representations from large-scale multilingual data, providing a strong foundation for domain-specific adaptation. When fine-tuned on legal-domain data, the model can efficiently adapt these general capabilities to the specialized terminology and syntactic structures of legal text, requiring fewer updates to achieve high-quality translations. This approach avoids the data-intensive requirements of training from scratch while capitalizing on the generalization capabilities of pre-trained models.

## Foundational Learning
- **Domain Adaptation**: The process of adapting a general-purpose model to perform well on a specific domain. Critical for achieving high-quality translations in specialized fields like legal text where terminology and style differ significantly from general language use.
- **Transfer Learning**: A machine learning approach where knowledge gained from one task is applied to improve performance on a related task. Essential for leveraging pre-trained models and reducing the need for extensive task-specific training data.
- **Parallel Corpora**: Collections of text where each document or sentence is translated into multiple languages. Fundamental for supervised machine translation as they provide the paired input-output examples needed for training.
- **Evaluation Metrics**: Quantitative measures used to assess translation quality, including BLEU for n-gram overlap, BERTScore for semantic similarity, and COMET for machine translation evaluation. Necessary for objective comparison of different translation approaches.
- **Legal Translation**: The specialized task of translating legal documents while preserving legal meaning, terminology, and formal style. Requires domain-specific knowledge and careful handling of legal concepts across languages.

## Architecture Onboarding
- **Component Map**: Pre-trained OPUS-MT model (encoder-decoder architecture) -> Fine-tuning on legal corpus -> Evaluation with multiple metrics
- **Critical Path**: Data preparation -> Model initialization (pre-trained vs random) -> Training/fine-tuning -> Evaluation
- **Design Tradeoffs**: Pre-trained models offer faster convergence and better performance but may require careful hyperparameter tuning; from-scratch training provides full control but needs more data and computational resources
- **Failure Signatures**: Poor performance on domain-specific terminology, inability to capture legal document structure, overfitting to limited legal data when training from scratch
- **First Experiments**: 1) Compare fine-tuning with different learning rates on legal data, 2) Evaluate impact of varying fine-tuning dataset sizes, 3) Test continued pre-training on legal data before fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation confined to single language pair (English-Hindi) and legal domain, limiting generalizability
- Only two training strategies compared, omitting potentially relevant approaches like adapter-based methods
- No detailed ablation studies on dataset size impact or hyperparameter variations

## Confidence
- High: Fine-tuning superiority with substantial performance gap (46.03 vs 37.77 BLEU)
- Medium: Broader implication that pre-trained models are universally preferable for domain adaptation
- Low: Specific evaluation metrics without statistical significance testing or variance measures

## Next Checks
1. Replicate study with additional language pairs to assess cross-linguistic generalizability
2. Conduct ablation studies varying pre-training data domain specificity and fine-tuning dataset sizes
3. Implement statistical significance testing across multiple training runs to establish robustness of performance differentials