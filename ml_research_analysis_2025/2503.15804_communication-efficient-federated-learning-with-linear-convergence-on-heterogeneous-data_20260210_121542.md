---
ver: rpa2
title: Communication Efficient Federated Learning with Linear Convergence on Heterogeneous
  Data
arxiv_id: '2503.15804'
source_url: https://arxiv.org/abs/2503.15804
tags:
- learning
- federated
- local
- have
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a communication-efficient federated learning
  algorithm, FedCET, to address the client-drift problem in heterogeneous data settings.
  The key innovation is using learning rates to weight information received from local
  clients, enabling accurate convergence without requiring additional variables to
  be transmitted between clients and the parameter server.
---

# Communication Efficient Federated Learning with Linear Convergence on Heterogeneous Data

## Quick Facts
- arXiv ID: 2503.15804
- Source URL: https://arxiv.org/abs/2503.15804
- Reference count: 32
- One-line primary result: FedCET achieves linear convergence on heterogeneous data while reducing communication overhead by half compared to existing algorithms.

## Executive Summary
This paper addresses the communication efficiency challenge in federated learning with heterogeneous data by proposing FedCET, an algorithm that achieves linear convergence without requiring additional transmitted variables. The key innovation lies in using learning rates to weight information from local clients, enabling accurate convergence while only transmitting one variable per communication round instead of two. Under appropriate learning rate conditions, the authors prove that FedCET converges linearly to the exact optimal solution, solving the client-drift problem without the communication overhead of existing methods like SCAFFOLD or FedLin.

## Method Summary
The paper tackles distributed empirical risk minimization with strongly convex quadratic loss functions across heterogeneous data. The algorithm uses a learning rate search procedure to find an appropriate step size, then runs local gradient descent with momentum-like corrections that incorporate the learning rate directly. Unlike existing methods that require clients to send two variables to the server each round, FedCET only transmits one variable while maintaining the same convergence guarantees. The method works by carefully weighting the information received from clients through the learning rate, eliminating the need for additional tracking variables while still correcting for client drift in heterogeneous settings.

## Key Results
- FedCET achieves linear convergence to the exact optimal solution under appropriate learning rate conditions
- The algorithm reduces communication overhead by transmitting half the variables compared to SCAFFOLD
- Numerical evaluations on quadratic ERM problems demonstrate faster convergence than existing algorithms
- The method maintains convergence guarantees while solving the client-drift problem in heterogeneous data settings

## Why This Works (Mechanism)
FedCET works by using the learning rate as a weighting mechanism to incorporate client information accurately without requiring additional tracking variables. The algorithm computes a weighted combination of current and previous client states through the learning rate, which effectively corrects for data heterogeneity while maintaining communication efficiency. This approach allows the parameter server to accurately estimate the global gradient direction using only one transmitted variable per client per round, achieving the same convergence guarantees as methods that transmit two variables.

## Foundational Learning
- **Strongly convex quadratic optimization**: Why needed - provides the theoretical foundation for linear convergence analysis. Quick check - verify L and μ constants satisfy theoretical bounds.
- **Client drift in federated learning**: Why needed - understanding the problem FedCET solves. Quick check - observe divergence in standard FedAvg with heterogeneous data.
- **Communication-efficient aggregation**: Why needed - core contribution of reducing transmitted variables. Quick check - compare variable count in FedCET vs SCAFFOLD.
- **Learning rate search algorithms**: Why needed - critical component for finding valid step sizes. Quick check - verify Algorithm 1 produces feasible α values.
- **Momentum-based corrections**: Why needed - mechanism for handling heterogeneity without extra variables. Quick check - trace update equations for correctness.
- **Linear convergence analysis**: Why needed - proves FedCET matches theoretical guarantees of more expensive methods. Quick check - verify convergence rate in experiments.

## Architecture Onboarding

**Component Map**
Algorithm 1 (Learning Rate Search) -> Algorithm 2 (FedCET Execution) -> Convergence Analysis

**Critical Path**
1. Generate synthetic data and compute optimal solution
2. Run learning rate search to find valid α
3. Initialize client parameters and run FedCET for T rounds
4. Measure convergence error and compare with baselines

**Design Tradeoffs**
- Communication reduction (1 vs 2 variables) vs complexity of learning rate search
- Analytical derivation requirements vs empirical performance
- Strong convexity assumption vs applicability to general convex problems

**Failure Signatures**
- Learning rate search fails to find valid α (loop exits without solution)
- Client trajectories diverge instead of converging to global optimum
- Convergence rate slower than theoretically guaranteed

**First Experiments**
1. Small-scale test (N=2, n=2) with known optimal solution to verify correctness
2. Compare convergence trajectories of FedCET vs FedAvg on heterogeneous data
3. Measure actual communication bytes transmitted per round vs theoretical reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Requires strong convexity assumption which may not hold in all practical federated learning scenarios
- Analytical derivation of smoothness and strong convexity constants needed but not fully specified
- Initialization scheme for client parameters could significantly affect early convergence behavior

## Confidence
- **High confidence**: Core algorithmic structure and theoretical framework are sound
- **Medium confidence**: Numerical evaluation setup is sufficiently specified for reproduction
- **Low confidence**: Precise parameter values and initialization scheme not fully detailed

## Next Checks
1. Derive and verify analytical values of L and μ for the specific quadratic formulation used
2. Implement small-scale test with known optimal solution to verify Algorithm 2 converges correctly
3. Compare convergence trajectory of FedCET against SCAFFOLD with identical initialization