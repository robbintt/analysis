---
ver: rpa2
title: Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis
arxiv_id: '2509.21375'
source_url: https://arxiv.org/abs/2509.21375
tags:
- prompt
- image
- counterfactual
- evaluator
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoContra, a framework for generating counterfactual
  size images (e.g., tiny walruses next to giant buttons) by automatically rewriting
  prompts. The method combines an image evaluator, prompt rewriter, and prompt ranker.
---

# Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis

## Quick Facts
- arXiv ID: 2509.21375
- Source URL: https://arxiv.org/abs/2509.21375
- Reference count: 0
- This paper introduces AutoContra, a framework for generating counterfactual size images (e.g., tiny walruses next to giant buttons) by automatically rewriting prompts.

## Executive Summary
This paper introduces AutoContra, a framework for generating counterfactual size images (e.g., tiny walruses next to giant buttons) by automatically rewriting prompts. The method combines an image evaluator, prompt rewriter, and prompt ranker. The image evaluator, built on Grounded SAM with adaptive refinements, identifies faithful counterfactual size generations. Using this evaluator, the authors construct the first counterfactual size dataset and train the rewriter and ranker via supervised learning and Direct Preference Optimization. Experiments on CoMat show AutoContra achieves 30.3% accuracy in generating counterfactual size images, outperforming Promptist (8.2%), ChatGPT-4o (27.5%), and the base prompt baseline (10.2%). Ablation studies confirm the importance of the image evaluator refinements and the ranker's contribution. The work establishes a foundation for counterfactual controllability in text-to-image generation.

## Method Summary
The method uses an automated image evaluator (Grounded SAM + CLIP) to score counterfactual size images, then bootstraps a dataset of base, positive, and negative prompts. A GPT-2 prompt rewriter is fine-tuned via supervised learning on base→positive pairs, and a GPT-2-large ranker is fine-tuned via DPO on triplets. At inference, 15 candidate prompts are generated and ranked to select the optimal one for the T2I model.

## Key Results
- AutoContra achieves 30.3% accuracy on counterfactual size image generation, outperforming baselines Promptist (8.2%), ChatGPT-4o (27.5%), and base prompt (10.2%).
- The image evaluator with refinements achieves an F1 score of 0.88 on human-labeled data.
- Ablation shows the ranker improves accuracy from 29.1% to 30.3%, and the evaluator refinements contribute a 114% improvement over the backbone.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Automated, high-fidelity image evaluation is a prerequisite for training prompt optimization systems in domains where human labeling is infeasible or subjective.
- **Mechanism:** The system extends **Grounded SAM** with adaptive refinements (tiny-region filtering, exclusive masks, and CLIP-based label verification) to generate segmentation masks. It calculates a score based on the relative pixel area of "small" vs. "big" objects, effectively acting as a proxy for human judgment to filter training data.
- **Core assumption:** The ratio of segmentation mask areas correlates strongly with human perception of counterfactual size relationships.
- **Evidence anchors:**
  - [abstract] "enhance the image evaluator by extending Grounded SAM with refinements, achieving a 114% improvement over its backbone."
  - [section 3.5] "F1 score 0.88... strong alignment with human judgments."
  - [corpus] Weak direct evidence for this specific evaluator architecture; neighbor papers focus on generation or general alignment rather than size-based evaluation metrics.
- **Break condition:** If the segmentation model fails to distinguish overlapping objects or mislabels them due to domain shift, the reward signal becomes noisy, degrading the rewriter's training.

### Mechanism 2
- **Claim:** Supervised fine-tuning (SFT) on a filtered dataset of "successful" counterfactual prompts enables a language model to learn syntactic patterns that overcome the T2I model's prior bias toward realistic sizes.
- **Mechanism:** A base prompt (e.g., "tiny walrus") is rewritten by a fine-tuned GPT-2 model. The model learns to inject specific phrasing or structural changes that statistically correlate with higher evaluator scores in the training set.
- **Core assumption:** The success of a prompt is deterministic enough to be learned; i.e., specific linguistic constructions reliably cause the image generator to violate physical priors.
- **Evidence anchors:**
  - [abstract] "supervised prompt rewriter that produces revised prompts... adapts base prompts into revised ones."
  - [section 2.3] "The prompt rewriter is fine-tuned via supervised learning on the base–positive prompt pairs."
  - [corpus] "Reverse Prompt" suggests decoding prompt recipes from images, supporting the idea that specific prompt structures drive image success.
- **Break condition:** If the T2I model ignores the nuanced syntax of the revised prompt and defaults to its training prior (e.g., generating a normal-sized walrus), the SFT model learns spurious correlations.

### Mechanism 3
- **Claim:** Direct Preference Optimization (DPO) on a ranker provides a more robust selection mechanism than stochastic generation alone by discriminating between "valid" and "invalid" counterfactual attempts.
- **Mechanism:** The system generates multiple candidate prompts. A ranker, fine-tuned on triplets (base, positive, negative), assigns scores. This discriminative step filters out candidates that might fail to render both objects or correct sizes, improving precision.
- **Core assumption:** The ranker generalizes the distinction between a prompt that *describes* a counterfactual and one that *causes* the model to render it.
- **Evidence anchors:**
  - [abstract] "DPO-trained ranker that selects the optimal revised prompt."
  - [section 3.4] "adding the prompt ranker improves accuracy from 29.1% to 30.3%... without the prompt ranker, some generated images contain only one object."
  - [corpus] "Maestro" uses agent orchestration for self-improvement, conceptually supporting the use of multi-stage selection/refinement loops.
- **Break condition:** If the ranker overfits to the specific distribution of the training set (concept drift), it may reject novel but valid prompts at inference time.

## Foundational Learning

- **Concept:** **Segmentation & Grounding (Grounded SAM)**
  - **Why needed here:** The core evaluation loop depends on the model's ability to "see" and delineate specific objects (e.g., "walrus" vs. "button") within a generated image to measure their relative pixel area.
  - **Quick check question:** How does the system handle overlapping masks where one object visually obscures another?

- **Concept:** **Direct Preference Optimization (DPO)**
  - **Why needed here:** Used to train the ranker. Unlike Reinforcement Learning (RL), DPO optimizes the policy directly using pairs of preferred and dispreferred data, which stabilizes training for the prompt selection task.
  - **Quick check question:** Does DPO require an explicit reward model at inference time, or is the reward model implicit in the policy update?

- **Concept:** **Prompt Engineering as Optimization**
  - **Why needed here:** The paper treats natural language prompts not as fixed inputs, but as latent variables to be optimized (rewritten) to maximize a visual fidelity score.
  - **Quick check question:** Why is the template "Big [small object] and small [big object]" insufficient for generating counterfactual images without rewriting?

## Architecture Onboarding

- **Component map:** Image Evaluator (Grounded SAM + CLIP) -> Prompt Rewriter (GPT-2, SFT) -> Prompt Ranker (GPT-2-large, DPO) -> T2I Backbone (CoMat SDXL)

- **Critical path:**
  1. **Bootstrapping:** Generate raw images from ChatGPT-rewritten prompts -> Score with Evaluator -> Build {Base, Positive, Negative} dataset.
  2. **Training:** SFT train Rewriter on {Base, Positive} pairs; DPO train Ranker on {Base, Positive, Negative} triplets.
  3. **Inference:** Base Prompt -> Rewriter (generates 15 candidates) -> Ranker (picks top 1) -> T2I Model.

- **Design tradeoffs:**
  - **Small vs. Large LLMs:** The authors use GPT-2 (small) for the rewriter/ranker to ensure efficiency and reproducibility, trading off against the potentially higher reasoning capabilities of proprietary models (e.g., ChatGPT-4o used only for data bootstrapping).
  - **Automated vs. Human Eval:** Relying on the automated evaluator allows for massive dataset scaling but risks "reward hacking" if the evaluator's heuristics (mask area) fail to capture semantic correctness.

- **Failure signatures:**
  - **Object Missing:** The T2I model omits one object entirely (Score = -τR * (1+g)).
  - **Realistic Priors Win:** The model ignores the prompt and generates a normal-sized walrus (Score = negative ratio).
  - **Over-stylization:** Using methods like "Promptist" adds stylistic tokens that confuse the size logic.

- **First 3 experiments:**
  1. **Evaluator Validation:** Run the Image Evaluator on the 235-image human-labeled set to confirm the F1 score (>0.80) against the "W.o. All refinements" baseline.
  2. **Ablation on Ranker:** Generate images using the Rewriter alone vs. Rewriter + Ranker to quantify the precision gain (target ~1-2% improvement).
  3. **Baseline Comparison:** Compare AutoContra against Promptist and raw ChatGPT-4o rewrites on the 1004-pair test set to verify that the specialized SFT/DPO pipeline outperforms general-purpose optimizers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be effectively adapted to control counterfactual attributes beyond object size, such as texture, material, or physical state?
- Basis in paper: [explicit] The authors state the proposed method is "readily extensible to other counterfactual attributes," yet the current study exclusively validates "counterfactual size."
- Why unresolved: The image evaluator is specifically engineered to measure pixel area ratios and spatial relationships, meaning it cannot detect non-spatial counterfactual success without significant architectural changes.
- What evidence would resolve it: Successful application of the prompt engineering pipeline to non-spatial tasks (e.g., generating "a bridge made of water"), requiring a new attribute-specific evaluator.

### Open Question 2
- Question: How does the image evaluator's reliability degrade when scaling from simple two-object prompts to complex scenes containing multiple entities or cluttered backgrounds?
- Basis in paper: [inferred] The methodology restricts the task to a binary relationship between "thelargeobject" and "thesmallobject," utilizing "exclusive masks" to simplify segmentation.
- Why unresolved: The "exclusive masks" refinement explicitly prioritizes smaller areas and removes overlaps, a heuristic that may fail in dense scenes where object masks naturally intersect or where multiple distractor objects exist.
- What evidence would resolve it: An evaluation of the Image Evaluator's F1 score on a modified dataset containing prompts with three or more objects and complex spatial layouts.

### Open Question 3
- Question: What specific advancements in model architecture or training data are required to exceed the observed ~30% accuracy ceiling for counterfactual image synthesis?
- Basis in paper: [explicit] The authors note that despite outperforming baselines, "overall accuracy remains relatively low" (30.3%), highlighting the "inherent difficulty" of counterfactual controllability.
- Why unresolved: The paper demonstrates that current SOTA models (including CoMat and ChatGPT-4o) struggle to override strong physical priors (common-sense patterns), suggesting a fundamental limitation in current diffusion or LLM alignment techniques.
- What evidence would resolve it: A modified training regime or larger model scale that achieves significantly higher fidelity (e.g., >60% accuracy) on the counterfactual size dataset.

## Limitations

- **Object and prompt set dependency:** The performance of AutoContra is intrinsically tied to the 91-object list and the 2070 base prompts generated from it. The paper does not disclose this list, making it difficult to assess whether the reported 30.3% accuracy is robust to different object choices or prompt templates.
- **Evaluator reliance and reward hacking:** The entire training pipeline depends on the automated image evaluator's segmentation and scoring. While the authors report an F1 score of 0.88 for evaluator-human alignment, this is still an automated proxy. There is a risk of reward hacking: if the evaluator's heuristics (e.g., relative mask area) do not fully capture human counterfactual size perception, the system may optimize for the wrong objective, generating images that score well but are semantically incorrect or fail to meet the counterfactual intent.
- **Limited evaluation scope:** The main experiment is conducted on a single CoMat SDXL test set. There is no evidence of cross-model or cross-dataset robustness. The reported improvements over baselines (Promptist, ChatGPT-4o) are relative, and it is unclear if these gains would persist under different evaluation criteria or with other T2I models.

## Confidence

- **High confidence:** The core mechanism of using an automated evaluator to bootstrap a counterfactual dataset and train a prompt rewriter/ranker is technically sound and the ablation study confirms the ranker's contribution.
- **Medium confidence:** The reported accuracy (30.3%) and evaluator F1 (0.88) are specific and likely reproducible, but the results are narrowly tied to the undisclosed object set and prompt template.
- **Low confidence:** The claim that this approach establishes a foundation for counterfactual controllability is forward-looking and not empirically supported by generalization experiments or ablation of the prompt template.

## Next Checks

1. **Evaluator robustness check:** Run the image evaluator on a held-out set of human-labeled counterfactual size images (not used in training) and report the F1 score. This will confirm if the evaluator's 0.88 alignment is consistent or if it degrades on unseen data.

2. **Template generalization:** Replace the "Big [small] and small [big]" template with a structurally different one (e.g., "A [small] as tall as a [big]") and measure if AutoContra still achieves high accuracy. This will test whether the system learns generalizable counterfactual prompting or just template-specific tricks.

3. **Cross-model evaluation:** Generate counterfactual images using the same prompts and ranker with a different T2I model (e.g., SD 1.5 or another CoMat variant) and compare accuracy. This will reveal if the learned prompt rewriting is model-specific or transferable.