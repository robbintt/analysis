---
ver: rpa2
title: 'Towards Superior Quantization Accuracy: A Layer-sensitive Approach'
arxiv_id: '2503.06518'
source_url: https://arxiv.org/abs/2503.06518
tags:
- quantization
- sensitivity
- sensiboost
- layers
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving quantization accuracy
  for large language models (LLMs) by recognizing that different layers have varying
  sensitivity to quantization errors. The authors propose two methods, SensiBoost
  and KurtBoost, which identify sensitive layers using activation sensitivity scores
  and weight distribution kurtosis metrics, respectively, and allocate additional
  bit budget to these layers while maintaining an overall memory constraint.
---

# Towards Superior Quantization Accuracy: A Layer-sensitive Approach

## Quick Facts
- arXiv ID: 2503.06518
- Source URL: https://arxiv.org/abs/2503.06518
- Authors: Feng Zhang; Yanbin Liu; Weihua Li; Jie Lv; Xiaodan Wang; Quan Bai
- Reference count: 36
- Primary result: Up to 9% lower perplexity with only 2% increase in memory budget compared to HQQ baseline

## Executive Summary
This paper addresses the challenge of improving quantization accuracy for large language models by recognizing that different layers have varying sensitivity to quantization errors. The authors propose two methods, SensiBoost and KurtBoost, which identify sensitive layers using activation sensitivity scores and weight distribution kurtosis metrics, respectively, and allocate additional bit budget to these layers while maintaining an overall memory constraint. Experiments on Llama models demonstrate that these methods achieve up to 9% lower perplexity with only a 2% increase in memory budget compared to the HQQ baseline, outperforming existing quantization techniques. The study reveals that sensitivity patterns are consistent within model families and their fine-tuned variants, validating the effectiveness of layer-sensitive quantization strategies.

## Method Summary
The paper proposes two layer-sensitive quantization methods that identify and allocate additional bit-width to challenging layers while maintaining overall memory constraints. SensiBoost uses activation sensitivity scores (MSE between original and quantized activations) to identify outlier layers via z-score detection, then applies higher bit-width configurations to these layers. KurtBoost uses weight distribution kurtosis to identify layers with significant outliers that compress normal weights into narrower ranges. Both methods use an outlier detection algorithm to select top-m sensitive layers and apply "boost stops" to increase their bit allocation from the base configuration. The methods are evaluated on Llama-2 and Llama-3 models using HQQ as the baseline quantization framework.

## Key Results
- Achieved up to 9% lower perplexity with only 2% increase in memory budget compared to HQQ baseline
- SensiBoost requires less memory for comparable performance than KurtBoost on Llama-2 models
- KurtBoost provides more fine-grained per-module quantization allocation on Llama-3 models
- Sensitivity patterns remain consistent across model families and fine-tuned variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layers with high activation sensitivity scores benefit more from additional bit budget than randomly selected layers.
- Mechanism: Sensitivity score $s_i = \frac{\|W_i \cdot X - Q^{-1}(Q(W_i)) \cdot X\|_2^2}{|W_i \cdot X|}$ measures MSE between activations from original vs. quantized weights. Layers with extreme scores are identified via z-score outlier detection and allocated higher bit-width.
- Core assumption: Layers that propagate larger activation errors under uniform quantization are the primary bottlenecks for overall model perplexity.
- Evidence anchors: [abstract] leveraging layer-sensitivity features; [section 3.1] sensitivity is independent of datasets and quantization methods; [corpus] Related work on layerwise quantization supports non-uniform bit allocation.

### Mechanism 2
- Claim: Weight distribution kurtosis predicts quantization difficulty by indicating outlier presence.
- Mechanism: High kurtosis (>3, leptokurtic) signals narrow peaks with heavy tails—outliers compress the quantization range, squeezing normal weights into fewer values. KurtBoost identifies top-m high-kurtosis layers for additional allocation.
- Core assumption: Outliers are the primary source of quantization error, and their distribution is layer-localized rather than uniform.
- Evidence anchors: [section 1] Weights with significant outliers are challenging to quantize; [section 3.2] Layers with highest Kurtosis values can be isolated; [corpus] Assumption: Corpus neighbors use gradient-based importance rather than kurtosis.

### Mechanism 3
- Claim: Allocating ~2% additional memory to identified sensitive layers reduces perplexity by up to 9% compared to uniform HQQ baseline.
- Mechanism: Outlier detection algorithm computes differences between adjacent sensitivity/kurtosis values, applies z-score threshold (|z| > 3), and returns top-m indices. Sensitive layers receive "boost stops" (e.g., 2-stop moves 4.13 → 4.51 bits).
- Core assumption: The memory-accuracy trade-off is non-linear; small reallocation yields disproportionate gains.
- Evidence anchors: [abstract] achieving up to 9% lower perplexity with only 2% increase in memory budget; [section 3.7, Figure 6] Win-tie-loss shows SensiBoost beats ablation; [corpus] Comparable mixed-precision methods report similar trade-offs.

## Foundational Learning

- Concept: **Post-Training Weight-Only Quantization (PTQ)**
  - Why needed here: The paper builds on HQQ, a calibration-free PTQ method; understanding the baseline is essential.
  - Quick check question: Can you explain why calibration-free methods (HQQ) are faster but potentially less accurate than calibration-based methods (AWQ, GPTQ)?

- Concept: **Outliers in Transformer Weights**
  - Why needed here: The core hypothesis is that layer-specific outlier distributions drive quantization difficulty.
  - Quick check question: What two architectural components does the paper cite as root causes of outliers in transformers?

- Concept: **Kurtosis as a Statistical Measure**
  - Why needed here: KurtBoost relies on interpreting kurtosis values; misinterpretation could lead to incorrect layer selection.
  - Quick check question: Would a platykurtic distribution (kurtosis < 3) indicate high or low outlier presence? What allocation decision would follow?

## Architecture Onboarding

- Component map: Pre-computation stage -> Outlier detection module -> Bit allocation table -> Quantization harness
- Critical path: 1) Load target model and pre-computed sensitivity/kurtosis CSV 2) Run outlier detection with configured top-m and method 3) Map detected layers to boost-stop configurations 4) Apply HQQ quantization with per-layer configs 5) Evaluate perplexity on WikiText-2 / C4
- Design tradeoffs:
  - SensiBoost vs. KurtBoost: SensiBoost requires calibration pass (activation-dependent); KurtBoost is weight-only (faster, no data dependency)
  - top-m selection: Lower m reduces memory overhead but may miss secondary sensitive layers
  - boost stop depth: Higher stops improve accuracy but erode memory savings
- Failure signatures:
  - Flat perplexity improvement: Base bit budget too high (≥4.25)
  - High tie-rate in ablation: Model too large (Llama-2-13B) for 2% budget to distinguish from random
  - No detected outliers: z-score threshold too strict or sensitivity/kurtosis distribution near-uniform
- First 3 experiments:
  1. Baseline replication: Run HQQ b4g128 on Llama-2-7B; record WikiText-2 perplexity. Then apply SensiBoost with top-m=2, boost-stop=2; verify ~2% memory increase and perplexity reduction.
  2. Ablation validation: On same model, randomly select 2 layers (excluding SensiBoost-detected layers) with identical boost allocation; confirm higher perplexity than SensiBoost.
  3. Method comparison: Run both SensiBoost and KurtBoost on Llama-3-8B at b3g128; compare memory increment vs. perplexity drop curves.

## Open Questions the Paper Calls Out
- Can the layer-sensitivity hypothesis and SensiBoost/KurtBoost efficacy generalize to non-LLM transformer architectures, such as Vision Transformers (ViT) or multimodal models?
- Can a dynamic quantization adjustment mechanism based on real-time computational constraints outperform the static allocation strategy proposed?
- Why does SensiBoost fail to significantly outperform the random ablation baseline on the larger Llama-2-13B model?
- Would a hybrid approach combining activation sensitivity and weight kurtosis yield more robust quantization configurations than using either metric in isolation?

## Limitations
- Validation is restricted to Llama family models; generalization to other architectures (GPT, Mistral, MoE) remains untested
- The 2% memory budget constraint is relatively tight and may not reflect practical deployment scenarios
- The specific formulation of activation sensitivity score as MSE proxy lacks comparison to alternative sensitivity metrics used in literature

## Confidence
- **High confidence**: The core mechanism linking outliers to quantization difficulty is well-supported by transformer weight distribution analysis and standard statistical theory
- **Medium confidence**: The generalizability of sensitivity patterns across model families is supported by internal comparisons but lacks external validation on non-Llama architectures
- **Low confidence**: The specific formulation of activation sensitivity score as MSE proxy for quantization difficulty, while intuitive, lacks comparison to alternative sensitivity metrics

## Next Checks
1. Apply SensiBoost/KurtBoost to a non-Llama architecture (e.g., GPT-2 or Mistral-7B) and verify whether pre-computed sensitivity scores from Llama models transfer effectively or require re-computation
2. Systematically vary the memory budget constraint (e.g., 1%, 3%, 5%) and measure how the optimal top-m layer selection and boost-stop depth change
3. Compare the activation sensitivity score formulation against gradient-based importance metrics (as used in KVmix) on the same model and quantization task