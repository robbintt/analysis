---
ver: rpa2
title: Bregman Linearized Augmented Lagrangian Method for Nonconvex Constrained Stochastic
  Zeroth-order Optimization
arxiv_id: '2504.09409'
source_url: https://arxiv.org/abs/2504.09409
tags:
- stochastic
- complexity
- optimization
- where
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies nonconvex constrained stochastic optimization
  problems where only noisy function values of the objective are available while exact
  constraint information is accessible. The authors propose a Bregman linearized augmented
  Lagrangian method that combines stochastic zeroth-order gradient estimators with
  variance reduction techniques.
---

# Bregman Linearized Augmented Lagrangian Method for Nonconvex Constrained Stochastic Zeroth-order Optimization

## Quick Facts
- arXiv ID: 2504.09409
- Source URL: https://arxiv.org/abs/2504.09409
- Reference count: 40
- Authors: Qiankun Shi; Xiao Wang; Hao Wang
- One-line result: Achieves oracle complexity of O(pd²/pε⁻³) for p ∈ [2, 2ln d] and O(ln d · ε⁻³) for p > 2ln d, improving upon existing O(dε⁻³) complexity

## Executive Summary
This paper addresses nonconvex constrained stochastic optimization where only noisy function values of the objective are available while exact constraint information is accessible. The authors propose a Bregman linearized augmented Lagrangian method that combines stochastic zeroth-order gradient estimators with variance reduction techniques. By appropriately selecting the Bregman distance parameter p, the method achieves oracle complexity with polylogarithmic dependence on dimension d rather than linear, representing a significant improvement over existing zeroth-order methods.

## Method Summary
The method solves nonconvex constrained stochastic optimization by combining a Bregman linearized augmented Lagrangian framework with two-point zeroth-order gradient estimation and momentum-based variance reduction. The algorithm operates in a single loop, updating primal variables via a Bregman proximal step that linearizes the augmented Lagrangian, and updating dual variables at the same frequency. The key innovation is using a Bregman distance with parameter p to reduce the dimensional dependency of variance from O(d) to O(d²/p) or O(ln d) depending on p's value, combined with STORM-style momentum to control gradient estimator variance.

## Key Results
- Achieves oracle complexity of O(pd²/pε⁻³) for p ∈ [2, 2ln d] and O(ln d · ε⁻³) for p > 2ln d
- Demonstrates better query efficiency than state-of-the-art zeroth-order methods in numerical experiments
- Successfully handles constrained Lasso and black-box adversarial attack problems
- Improves upon existing work that requires at least O(dε⁻³) complexity

## Why This Works (Mechanism)

### Mechanism 1: Dimension-Adaptive Bregman Proximal Mapping
Replacing the standard Euclidean proximal term with a Bregman distance generated by 1/2∥x∥_q² reduces the oracle complexity's dependence on dimension d from linear O(d) to polylogarithmic O(ln d), provided the norm parameter p is chosen appropriately (p > 2ln d). By measuring stationarity and constraints in ℓ_p-norm and using the corresponding dual norm ℓ_q for the Bregman geometry, the variance constant S_p scales as d²/p rather than d.

### Mechanism 2: Momentum-Based Variance Reduction for Biased Gradients
The recursive momentum estimator controls the variance of the stochastic zeroth-order gradient enough to achieve an optimal ε-dependency of O(ε⁻³), matching first-order lower bounds under mean-squared smoothness. The algorithm uses a STORM-style momentum update where the current gradient estimate is a convex combination of the new batch estimate and the previous estimate, corrected by the difference in gradients at consecutive iterates.

### Mechanism 3: Single-Loop Linearized Augmented Lagrangian
Linearizing the augmented Lagrangian subproblem allows the algorithm to run in a single loop while maintaining convergence to an ε-KKT point for nonconvex constrained problems. This transforms the subproblem into a mirror-descent-style update that can be solved in one step, allowing primal x and dual λ updates to occur at the same frequency.

## Foundational Learning

- **Concept: Bregman Divergence & Mirror Descent**
  - Why needed here: The paper relies on non-Euclidean geometry to beat the "curse of dimensionality" in zeroth-order optimization
  - Quick check question: How does the strong convexity of the generating function v(x) = 1/2∥x∥_q² in ℓ_q norm relate to the algorithm's ability to handle geometry better than standard gradient descent?

- **Concept: Zeroth-Order Gradient Estimation (Two-Point)**
  - Why needed here: The paper uses function value differences to estimate gradients
  - Quick check question: Why does the two-point estimator generally yield lower variance than a one-point estimator, and how does the dimension d affect the variance of the Rademacher smoothing specifically?

- **Concept: KKT Conditions & Constraint Qualification**
  - Why needed here: The algorithm targets an ε-KKT point, not just minimal function value
  - Quick check question: What is the role of the Lagrange multiplier λ in the update step, and why must the penalty parameter μ grow if feasibility is not improved?

## Architecture Onboarding

- **Component map:** Oracle (stochastic function sampler) -> ZO Estimator (two-point difference + Momentum accumulator) -> Bregman Solver (linearized proximal step) -> AL Manager (dual variable updates)
- **Critical path:** The selection of p (the Bregman parameter) is the most sensitive configuration. Implement a check to verify if the constraint Jacobian is well-conditioned in the chosen ℓ_p norm before setting p > 2ln d.
- **Design tradeoffs:**
  - High p (p → ∞): Minimizes dimensional dependency but tightens constraint qualification requirements
  - Momentum α: Large α speeds up variance reduction but requires smaller step sizes η to stabilize bias
- **Failure signatures:**
  - Stalling in Feasibility: Constraint violation stops decreasing but loss decreases (likely cause: penalty μ too small or step size η too large)
  - Exploding Gradients: Momentum term grows unbounded (likely cause: batch size n too small or noise variance σ² higher than estimated)
  - Dimension Mismatch: Complexity scales as O(d) instead of O(d²/p) (likely cause: using Gaussian smoothing instead of Rademacher smoothing)
- **First 3 experiments:**
  1. Baseline Validation: Run on high-dimensional quadratic with synthetic equality constraints (d=1000), compare p=2 vs p=3 or p=ln d, plot oracle calls vs log-gradient-norm
  2. Ablation on Momentum: Disable momentum correction (set α=0), measure increase in oracle complexity to verify degradation to standard ZO complexity
  3. Constraint Qualification Stress Test: Construct problem where constraints are well-conditioned in ℓ_2 but ill-conditioned in ℓ_p (for large p), verify if algorithm fails or slows down

## Open Questions the Paper Calls Out

### Open Question 1
Can the Bregman linearized augmented Lagrangian framework be extended to handle stochastic constraints where only noisy function values are available? The current work assumes access to exact constraint gradients, while references [43] and [51] handle stochastic constraints but suffer from high complexity (O(dε⁻⁶)) or hidden dimension dependencies.

### Open Question 2
Can an adaptive mechanism be derived to select the optimal norm parameter p without requiring prior knowledge of the problem instance? The theoretical bounds O(pd²/pε⁻³) vary significantly with p, and while p → ∞ yields logarithmic dimension dependence, the constant factors increase.

### Open Question 3
Is it possible to relax the NonSingularity Condition (Assumption 5) to weaker constraint qualifications like the Mangasarian-Fromovitz Constraint Qualification (MFCQ)? The proof of primal feasibility relies on Assumption 5 to bound the constraint violation, creating a reciprocal relationship with the penalty parameter μ that drives convergence.

## Limitations
- The Non-Smooth Convexity (NSC) condition assumption may be difficult to verify in practice and implementation details for the two-stage restart strategy remain sparse
- Theoretical guarantees assume bounded constraint violation at initialization (‖c(x₀)‖² ≤ 1/μ), which may not hold for arbitrary initial points
- Complexity bounds rely specifically on Rademacher smoothing; performance with Gaussian smoothing remains unexplored

## Confidence
- **High Confidence:** The Bregman proximal geometry's ability to reduce dimensional dependency from O(d) to O(d²/p) or O(ln d) - supported by explicit variance calculations
- **Medium Confidence:** The momentum-based variance reduction achieving optimal O(ε⁻³) oracle complexity - theoretical derivation is sound but empirical validation against first-order methods is limited
- **Low Confidence:** The constraint qualification assumption (NSC) in general ℓ_p-norms - while theoretical conditions are provided, practical verification remains challenging

## Next Checks
1. **Geometry Verification:** Implement a systematic check for NSC condition in ℓ_p-norm by computing the ratio β‖c(x)‖_p / dist_p(∇c(x)ᵀc(x), -N_X(x)) across iterations to identify when the geometry assumption breaks down.

2. **Smoothing Sensitivity:** Compare Rademacher vs Gaussian smoothing empirically on high-dimensional problems (d > 1000) to quantify the theoretical advantage of Rademacher's d²/p scaling, measuring both oracle complexity and solution quality.

3. **Restart Strategy Validation:** Implement and test the two-stage restart strategy for finding near-feasible initial points, evaluating whether the overhead is justified by improved convergence, particularly for problems where ‖c(x₀)‖² > 1/μ initially.