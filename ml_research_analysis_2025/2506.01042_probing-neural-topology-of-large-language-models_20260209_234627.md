---
ver: rpa2
title: Probing Neural Topology of Large Language Models
arxiv_id: '2506.01042'
source_url: https://arxiv.org/abs/2506.01042
tags:
- neural
- graph
- probing
- topology
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces graph probing, a method that uses the functional
  connectivity of neurons in large language models (LLMs) to predict language generation
  and understanding performance. Instead of analyzing neural activations, graph probing
  constructs neural graphs by computing correlations between neuron activity time
  series as models process text, and then trains simple probes (linear or MLP) on
  these graphs to predict metrics like perplexity and semantic representations.
---

# Probing Neural Topology of Large Language Models

## Quick Facts
- **arXiv ID:** 2506.01042
- **Source URL:** https://arxiv.org/abs/2506.01042
- **Reference count:** 40
- **Primary result:** Graph probing of functional neural connectivity predicts LLM performance 130.4% better than activation probing for perplexity.

## Executive Summary
This paper introduces graph probing, a method that uses the functional connectivity of neurons in large language models (LLMs) to predict language generation and understanding performance. Instead of analyzing neural activations, graph probing constructs neural graphs by computing correlations between neuron activity time series as models process text, and then trains simple probes (linear or MLP) on these graphs to predict metrics like perplexity and semantic representations. Experiments across diverse LLM families (GPT-2, Pythia, Qwen2.5) show graph probing consistently outperforms activation-based probing by up to 130.4% on perplexity and 67.7% on space/time semantic regression. This advantage holds even with only 1% of neuron connections retained, and across model sizes from millions to billions of parameters. Causal interventions reveal stable default networks and hub neurons, and applications demonstrate graph probing's potential for model pruning and hallucination detection. The results suggest that neural topology contains significantly richer information about LLM performance than neural activation.

## Method Summary
The method extracts functional connectivity from LLMs by computing Pearson correlations between neuron activation time series across token sequences. For a given layer, hidden states H (neurons × tokens) are extracted, and a correlation matrix A is computed where each entry represents the correlation between two neurons' activation patterns. This topology matrix is then flattened and used to train simple probes (linear or MLP) to predict various performance metrics including perplexity and semantic understanding. The approach can work with sparse topology by retaining only the strongest correlations, and includes causal intervention experiments where hub neurons are selectively disabled to assess their functional importance.

## Key Results
- Graph probing achieves R² of 0.91 for perplexity prediction vs. ~0.40 for activation probing (130.4% improvement)
- Performance remains above 0.6 R² even with only 1% of edges retained
- Hub neuron ablation causes 45-70% accuracy drops vs. <1% for random neurons (95.5% relative improvement)
- Topology-based probing outperforms activation-based across model sizes from 160M to 14B parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Functional connectivity patterns between neurons encode more information about language performance than raw activation magnitudes.
- **Mechanism:** As LLMs process token sequences, neurons exhibit correlated activity patterns. Pearson correlation between neuron time series captures temporal co-activation structure. Probing these n×n connectivity matrices reveals predictable relationships to perplexity and semantic understanding that activation-only probing misses.
- **Core assumption:** Temporal correlation structure reflects computational role; co-activating neurons participate in related functional circuits.
- **Evidence anchors:**
  - [abstract] "probing on topology outperforms probing on activation by up to 130.4% and 67.7% on perplexity and space/time semantic regression"
  - [section 3, Table 1] Graph probing R² reaches 0.91 vs. ~0.40 for activation probing
  - [corpus] GraphGhost (arxiv 2510.08613) similarly finds that structural graph-based analysis reveals multi-step reasoning mechanisms invisible to token-level attribution methods
- **Break condition:** If correlations arise from artifacts (e.g., shared input statistics rather than functional coupling), the mechanism would not generalize across inputs.

### Mechanism 2
- **Claim:** Hub neurons with high topological degree serve as functional bottlenecks; their disruption disproportionately degrades performance.
- **Mechanism:** A small subset of neurons maintains high connectivity across inputs (default network). These hub neurons integrate information across neural communities. When disabled, information flow through the network is severed.
- **Core assumption:** High-degree neurons in functional graphs correspond to causal information routers, not just highly active neurons.
- **Evidence anchors:**
  - [section 4] "nearly 40 neurons remain hubs in 100% of the topologies across the entire dataset"
  - [section 4, Figure 4c] Disabling top 1% hub neurons by degree caused 45-70% accuracy drops vs. <1% for random neurons; topology-based intervention outperformed activation-based by 95.5% relative in 14B model
  - [corpus] Weak direct support; neighbor papers focus on graph processing rather than internal neural topology
- **Break condition:** If hub neurons are epiphenomenal (correlated but not causal), ablation would produce random-like effects.

### Mechanism 3
- **Claim:** Sparse topology (retaining 1% of edges) preserves predictive information because strong correlations concentrate in a small subset of connections.
- **Mechanism:** Most pairwise correlations are weak noise. Thresholding retains the signal-carrying edges while reducing O(n²) complexity. The remaining edges capture the core functional circuits.
- **Core assumption:** Information about language performance concentrates in the strongest correlations; weak edges contribute noise rather than signal.
- **Evidence anchors:**
  - [section 3, Figure 3a] "even under extreme sparsity where only 1% of the original edges are retained... achieving above 0.6 R² that is still higher than activation probing"
  - [section 3] Predictive performance remains stable after removing 90% of edges
  - [corpus] No direct corpus support for sparsity specifically; related work on GNN privacy (arxiv 2509.05429) assumes topology is information-rich but doesn't test sparsity
- **Break condition:** If different tasks require different sparse subsets, a universal threshold would fail for some domains.

## Foundational Learning

- **Concept: Functional connectivity vs. structural connectivity**
  - Why needed here: The paper constructs graphs from temporal correlation (functional), not weight matrices (structural). Confusing these leads to misinterpreting the mechanism.
  - Quick check question: Given neurons A and B that never directly connect via weights, can they still have high functional connectivity?

- **Concept: Pearson correlation for time series**
  - Why needed here: The core operation (Equation 4) computes correlation between neuron activation sequences. Understanding what correlation captures—and its limitations—is essential.
  - Quick check question: If two neurons both monotonically increase across tokens, will they have high correlation? What if one increases while the other oscillates?

- **Concept: Probing classifier limitations**
  - Why needed here: The paper uses probing to claim topology contains "richer information." Probes can learn spurious patterns if not carefully controlled.
  - Quick check question: If a probe achieves high accuracy, does that guarantee the model "uses" that information? What experiment distinguishes correlation from causation?

## Architecture Onboarding

- **Component map:**
  Input tokens → LLM forward pass → Hidden states H[t, n] per layer → Correlation computation A[i,j] = ρ(H_i, H_j) → Threshold/sparsify (optional) → Flatten or GNN encode → Probe (linear/MLP) → Predict: perplexity | semantics | hallucination

- **Critical path:** The correlation computation (Equations 1-4) is the transformation that matters. Any error here propagates to all downstream analysis. Verify Pearson correlation is computed correctly over the token dimension, not the neuron dimension.

- **Design tradeoffs:**
  - Linear probe on flattened matrix: Simple, interpretable, but O(n²) parameters
  - GNN probe: O(n·d) parameters via weight sharing, but adds architectural complexity
  - Full graph vs. sparse: Full captures all signal but scales poorly; sparse requires threshold tuning

- **Failure signatures:**
  - R² close to 0: Correlation computation may be wrong, or layer selection is inappropriate (try middle layers)
  - Sparsity degrades performance sharply: Threshold too aggressive or correlation concentrated differently than expected
  - Intervention shows no effect: Selected neurons may not be true hubs; verify degree calculation

- **First 3 experiments:**
  1. **Replicate core perplexity prediction** on a small model (Pythia-160M) with 1000 samples. Verify R² > 0.8 for graph probe vs. <0.5 for activation probe. This validates the pipeline.
  2. **Ablation study on layer selection** by probing topology from layers 1, middle, and final. Confirm middle layers are most predictive (R² should peak around layer 6-8 in 12-layer models).
  3. **Sparsity sweep** at 100%, 50%, 10%, 1% edge retention. Plot R² vs. sparsity to verify the claim that 1% edges retain signal above activation-baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do higher-order graph properties (e.g., motifs, small-worldness, modularity) exist in LLM neural topology, and do they play a causal role in shaping intelligence?
  - **Basis in paper:** [Explicit] The authors state, "It remains an open question whether such properties exist in LLMs’ neural topology and play a causal role in shaping their intelligence."
  - **Why unresolved:** The study focused on identifying "default networks" and "hub neurons" but did not analyze complex topological metrics or their functional necessity.
  - **What evidence would resolve it:** Quantitative analysis of graph metrics (clustering coefficient, path length) and interventional studies that disrupt modularity to observe performance degradation.

- **Open Question 2:** Is advanced reasoning capability constrained by, or does it actively alter, the functional neural topology of LLMs?
  - **Basis in paper:** [Explicit] The discussion raises the question: "Does reasoning alter, or is it constrained by, neural topology?"
  - **Why unresolved:** The current experiments focused on perplexity and semantic understanding (space/time), leaving multi-step logical reasoning unexplored.
  - **What evidence would resolve it:** Experiments applying graph probing to reasoning-specific benchmarks (e.g., chain-of-thought datasets) and analyzing topological changes during inference.

- **Open Question 3:** How do specialized topological structures (e.g., subject-specific neurons) emerge during the LLM learning process?
  - **Basis in paper:** [Explicit] The authors note, "It remains unclear how these specialized structures emerge during LLMs’ learning process."
  - **Why unresolved:** The paper provides a static analysis of pre-trained models without investigating the formation dynamics of these networks during training.
  - **What evidence would resolve it:** A longitudinal study tracking the formation of functional connectivity and hub neurons at various training checkpoints.

## Limitations

- **Unknown 1: Layer Selection Ambiguity** - The paper specifies using "middle layers" but does not precisely define which layer index constitutes "middle" across different model architectures (GPT-2, Pythia, Qwen2.5). This creates ambiguity in reproducing the exact results, as performance may vary significantly between layers.

- **Unknown 2: Sparsity Implementation Details** - While the paper mentions retaining 1% of edges for large models, the exact thresholding algorithm (top-k absolute values vs. fixed correlation cutoff) is not specified. This impacts reproducibility and the generalizability of sparsity claims.

- **Unknown 3: Computational Constraints** - The O(n²) scaling of the correlation matrix becomes prohibitive for large models (e.g., 5120 neurons → ~26M input features). The paper suggests sparsity as a solution but doesn't provide detailed guidance on optimal threshold selection across different model sizes.

## Confidence

- **Graph probing outperforms activation probing by 130.4% on perplexity prediction:** Medium Confidence - Supported by Table 1 showing R² of 0.91 vs ~0.40, but requires careful reproduction of layer selection and preprocessing steps to verify.
- **Topology retains predictive power with 1% edge retention:** High Confidence - Figure 3a directly shows performance stability across sparsity levels, with above 0.6 R² maintained at 1% retention, exceeding activation baseline.
- **Hub neurons identified via topology are causally important:** Medium Confidence - Section 4 shows hub neuron ablation causing 45-70% accuracy drops, outperforming random ablation by 95.5%. However, the causal relationship requires careful interpretation as correlation does not necessarily imply causation.

## Next Checks

1. **Replicate Core Perplexity Prediction** - Implement the full pipeline on a small model (Pythia-160M) with 1000 samples, using middle layer topology extraction. Verify that graph probing achieves R² > 0.8 while activation probing achieves R² < 0.5, matching the paper's reported performance gap.

2. **Sparsity Performance Sweep** - Systematically test topology-based probing at 100%, 50%, 10%, and 1% edge retention levels. Plot R² performance vs. sparsity to confirm the claim that 1% retention preserves signal above activation baseline, and identify the optimal sparsity threshold.

3. **Layer Selection Sensitivity Analysis** - Probe topology from layers 1, middle, and final for a 12-layer model. Measure R² at each layer to identify the peak performance layer and verify that "middle layers" are indeed most predictive, as the paper claims without precise specification.