---
ver: rpa2
title: Automatic Demonstration Selection for LLM-based Tabular Data Classification
arxiv_id: '2506.20451'
source_url: https://arxiv.org/abs/2506.20451
tags:
- demonstrations
- performance
- algorithm
- classification
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automatically selecting an
  optimal number of demonstrations for in-context learning (ICL) with large language
  models (LLMs) on tabular data classification tasks. The authors propose a novel
  algorithm that leverages spectral graph theory to estimate the number of demonstrations
  needed by analyzing the similarity graph constructed from token IDs of demonstrations.
---

# Automatic Demonstration Selection for LLM-based Tabular Data Classification

## Quick Facts
- arXiv ID: 2506.20451
- Source URL: https://arxiv.org/abs/2506.20451
- Authors: Shuchu Han; Wolfgang Bruckner
- Reference count: 5
- The proposed spectral-based demonstration selection algorithm achieves "stable" classification performance that consistently approximates the best results obtained through random selection across eight tabular datasets and three LLM models.

## Executive Summary
This paper addresses the challenge of automatically selecting an optimal number of demonstrations for in-context learning (ICL) with large language models (LLMs) on tabular data classification tasks. The authors propose a novel algorithm that leverages spectral graph theory to estimate the number of demonstrations needed by analyzing the similarity graph constructed from token IDs of demonstrations. The method uses spectral gap analysis to determine the number of clusters, which corresponds to the minimum number of demonstrations required to represent the data distribution within the LLM's intrinsic representation space.

The primary contribution is a label-free approach that eliminates the need for users to manually tune the number of demonstrations while maintaining reasonable classification performance. The algorithm demonstrates "stable" performance across different datasets and LLM models, though it does not achieve superior performance compared to random selection strategies. The approach is validated across eight public tabular datasets and three different LLM models, showing consistent performance that approximates the best random selection outcomes.

## Method Summary
The proposed algorithm automatically selects demonstrations for ICL by constructing a similarity graph from token IDs and applying spectral gap analysis. Each tabular demonstration is converted to text, tokenized, and represented as a set of token IDs. Pairwise Jaccard similarity is computed between these token ID sets to build a k-nearest neighbor sparse similarity graph. The normalized Laplacian of this graph is calculated, and spectral gap analysis is applied to estimate the number of clusters, which determines the optimal number of demonstrations. One demonstration is randomly sampled from each cluster to form the final demonstration set. The method avoids manual tuning of demonstration numbers while maintaining classification performance comparable to the best random selection strategies.

## Key Results
- The proposed algorithm demonstrates "stable" classification performance across eight tabular datasets and three LLM models (Llama-3.2-3B, Mistral-7B-v0.3, Qwen3-8B)
- Performance consistently approximates the best results obtained through random demonstration selection, avoiding the need for manual tuning
- The method successfully handles datasets with varying characteristics, though it struggles with datasets showing ambiguous spectral gaps (e.g., `tae`, `cmc`)
- Classification performance is evaluated using Macro-F1 score, with results showing the proposed method achieves scores close to the best random selection outcomes

## Why This Works (Mechanism)

### Mechanism 1: Token ID-based Similarity Representation
- Claim: Token IDs provide a more discriminative and computationally efficient similarity metric for tabular demonstrations than high-dimensional embedding vectors.
- Mechanism: Each demonstration is tokenized using the LLM's tokenizer, producing a list of token IDs. Jaccard similarity is then computed between token ID sets, avoiding the "curse of dimensionality" that affects cosine similarity on aggregated embedding vectors.
- Core assumption: Token-level overlap correlates meaningfully with semantic similarity in the LLM's intrinsic representation space; the tokenizer preserves task-relevant structure.
- Evidence anchors:
  - [section 2.2]: "we use token IDs and the Jaccard Similarity as the similarity metric... the embedding vectors of a pre-trained LLM are usually high dimensional vectors. Any general similarity metric such as the cosine similarity will be affected by the Curse of Dimensionality."
  - [corpus]: Weak direct support; related work (e.g., "Learn to Select") focuses on embedding-based or divergence-based selection, not token ID similarity specifically.
- Break condition: If tokenizers truncate or map semantically distinct values to identical token IDs, similarity estimates may become unreliable.

### Mechanism 2: Spectral Gap Analysis for Cluster Estimation
- Claim: The spectral gap of the normalized Laplacian of the demonstration similarity graph estimates the minimum number of clusters, which corresponds to the minimum number of demonstrations needed to represent the data distribution.
- Mechanism: Construct a k-nearest neighbor sparse similarity graph, compute its normalized Laplacian (L_sys = D^(-1/2) L D^(-1/2)), and identify the largest gap between consecutive eigenvalues. The index of this gap indicates the number of clusters.
- Core assumption: The data distribution in token-ID space clusters in a way that reflects the underlying class or pattern structure relevant to the LLM's representation.
- Evidence anchors:
  - [abstract]: "analyze the eigenvalues of its Laplacian to derive the minimum number of demonstrations capable of representing the data within the LLM's intrinsic representation space."
  - [section 2.4]: "we apply the spectral gap method to find the optimal number of clusters, which will be the optimal number of demonstrations in our algorithm."
  - [corpus]: "Stability of In-Context Learning: A Spectral Coverage Perspective" explores spectral methods for ICL stability, suggesting spectral analysis is a plausible but not fully validated approach for demonstration selection.
- Break condition: If the similarity graph lacks clear cluster structure (e.g., uniform distribution or high noise), the spectral gap may be ambiguous or misleading (noted in paper for `tae` and `cmc` datasets).

### Mechanism 3: Cluster-based Demonstration Sampling
- Claim: Randomly sampling one demonstration per cluster provides a representative set that stabilizes classification performance across datasets and LLMs.
- Mechanism: After spectral clustering into `d` clusters, one sample is randomly selected from each cluster. This ensures coverage of the estimated distribution modes without requiring label information.
- Core assumption: Each cluster corresponds to a distinct mode of the data distribution relevant to the classification task; one sample per cluster is sufficient for ICL.
- Evidence anchors:
  - [section 2.5]: "Select one sample from each group randomly and aggregate them as the minimum set of demonstrations."
  - [results]: "our algorithm shows very 'stable' classification performance... close to the best result achieved by tuning and trying different numbers of demonstrations."
  - [corpus]: No direct corpus validation; related work (e.g., "Learn to Select") uses label distribution divergence rather than cluster-based sampling.
- Break condition: If clusters are not aligned with class boundaries or task-relevant features, the selected demonstrations may not improve ICL performance.

## Foundational Learning
- Concept: **In-Context Learning (ICL)**
  - Why needed here: The entire method operates within the ICL paradigm, where demonstrations are provided in the prompt without updating model weights.
  - Quick check question: Can you explain why ICL performance might vary with the number and content of demonstrations?
- Concept: **Spectral Graph Theory (Laplacian, Eigenvalues, Spectral Gap)**
  - Why needed here: The core algorithm relies on constructing a similarity graph and analyzing its Laplacian eigenvalues to estimate cluster count.
  - Quick check question: How does the spectral gap relate to the number of clusters in a graph?
- Concept: **Tokenization and Jaccard Similarity**
  - Why needed here: Demonstrations are represented as token ID sets, and similarity is computed via Jaccard overlap.
  - Quick check question: What are the limitations of Jaccard similarity on token IDs for capturing semantic similarity?

## Architecture Onboarding
- Component map: Tokenization -> Similarity Graph Construction -> Spectral Analysis -> Cluster Sampling -> Prompt Assembly & Inference
- Critical path: Tokenization → Similarity Graph → Spectral Gap → Cluster Sampling → Inference. Errors in early steps (e.g., poor similarity graph) propagate to final performance.
- Design tradeoffs:
  - Token IDs vs. Embeddings: Token IDs are faster and avoid dimensionality issues but may lose semantic nuance.
  - k in k-NN graph: Larger k increases graph density, potentially smoothing over fine-grained clusters; smaller k may fragment the graph.
  - Random vs. informed sampling within clusters: Current method uses random; could be improved with representativeness criteria (not explored).
- Failure signatures:
  - Ambiguous spectral gap (flat eigenvalue curve) → unreliable `d` estimation (observed for `tae`, `cmc`).
  - Large discrepancy between estimated `d` and best-performing random selection count (e.g., `cmc`: estimated 45, best random 2).
  - Near-zero Macro-F1 on certain LLM–dataset pairs (e.g., Mistral on `wine`, `iris`) regardless of selection method.
- First 3 experiments:
  1. **Baseline random selection sweep**: Run ICL with demonstration counts [0, 2, 4, 6, 8, 10] randomly sampled; record Macro-F1 to establish per-dataset/LLM performance range.
  2. **Spectral gap estimation sanity check**: For each dataset, visualize the first 50 eigenvalues; verify that clear gaps exist for most datasets and note exceptions (`tae`, `cmc`).
  3. **Proposed algorithm vs. best random**: Apply the full pipeline, compare Macro-F1 against the best random-selection result; quantify stability (variance) and performance gap.

## Open Questions the Paper Calls Out
None

## Limitations
- The algorithm achieves only "stable" performance that approximates the best random selection outcomes, without demonstrating superior performance.
- The method struggles with datasets showing ambiguous spectral gaps, leading to unreliable cluster estimation (e.g., `tae`, `cmc` datasets).
- Classification performance varies dramatically across different LLMs, with some combinations producing near-zero Macro-F1 scores regardless of demonstration selection strategy.

## Confidence
**High Confidence**:
- The algorithm successfully eliminates the need for manual tuning of demonstration numbers across all tested conditions.
- The spectral gap method provides a reasonable estimate of cluster count for datasets with clear spectral structure.
- The approach demonstrates stable performance that consistently approximates the best random selection outcomes.

**Medium Confidence**:
- Token ID-based similarity via Jaccard is computationally efficient and avoids dimensionality issues compared to embedding-based similarity.
- The spectral analysis captures meaningful data structure that correlates with classification task requirements.
- One demonstration per cluster provides sufficient coverage for ICL performance.

**Low Confidence**:
- The spectral analysis method can reliably estimate the optimal number of demonstrations across diverse tabular datasets.
- The token ID similarity metric captures semantically meaningful relationships in tabular data.
- The method's stability advantage translates to practical deployment benefits beyond avoiding manual tuning.

## Next Checks
1. **Cross-Domain Generalization Test**: Apply the method to non-tabular datasets (e.g., text classification, image classification) where demonstrations are naturally represented as sequences or feature vectors, testing whether the token ID-based similarity approach generalizes beyond tabular data.

2. **Label-Aware vs. Label-Free Comparison**: Implement a variant that uses ground truth labels to inform demonstration selection (e.g., ensuring class balance) and compare performance against the current label-free spectral method to quantify the cost of avoiding label dependence.

3. **Spectral Gap Robustness Analysis**: Systematically vary the k parameter in k-NN graph construction (e.g., k ∈ {5, 10, 15, 20}) and analyze how spectral gap stability and cluster estimation quality change, particularly for datasets currently showing ambiguous gaps (`tae`, `cmc`).