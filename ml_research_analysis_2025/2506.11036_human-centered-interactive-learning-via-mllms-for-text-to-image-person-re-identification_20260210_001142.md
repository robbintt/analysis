---
ver: rpa2
title: Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification
arxiv_id: '2506.11036'
source_url: https://arxiv.org/abs/2506.11036
tags:
- person
- text
- image
- images
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an interactive cross-modal learning framework
  for text-to-image person re-identification (TIReID) that leverages multimodal large
  language models (MLLMs) to overcome limitations of offline models and training data.
  The proposed approach consists of two core components: (1) a Test-time Human-centered
  Interaction (THI) module that performs multi-round visual question answering to
  refine user queries through interactions with MLLMs, and (2) a Reorganization Data
  Augmentation (RDA) strategy that enriches and diversifies training texts through
  decomposition, rewriting, and reorganization.'
---

# Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification

## Quick Facts
- arXiv ID: 2506.11036
- Source URL: https://arxiv.org/abs/2506.11036
- Authors: Yang Qin; Chao Chen; Zhihang Fu; Dezhong Peng; Xi Peng; Peng Hu
- Reference count: 40
- Primary result: Achieves Rank-1 accuracy up to 91.78% on UFine6926 using interactive MLLM-based query refinement

## Executive Summary
This paper addresses the challenge of text-to-image person re-identification (TIReID) by introducing an interactive cross-modal learning framework that leverages multimodal large language models (MLLMs). The method consists of two core components: a Test-time Human-centered Interaction (THI) module that refines user queries through multi-round visual question answering with MLLMs, and a Reorganization Data Augmentation (RDA) strategy that enriches training texts. Experiments on four benchmarks demonstrate state-of-the-art performance, with the approach showing strong generalization across datasets and functioning as a plug-and-play enhancement to existing TIReID methods.

## Method Summary
The proposed approach combines offline data augmentation with online interactive query refinement. RDA uses MLLMs to decompose, rewrite, and reorganize training texts to create diverse descriptions that improve the cross-modal embedding space. THI performs test-time refinement by first using a LoRA-adapted MLLM to identify the most relevant image (anchor localization), then extracting fine-grained visual attributes through VQA to augment the original query. The system retrieves initial candidates using a CLIP-based backbone, refines the query through MLLM interactions, and re-ranks results based on the enriched text embedding.

## Key Results
- Achieves Rank-1 accuracy of 91.78% on UFine6926 benchmark
- Improves Rank-1 accuracy by 2.36% on CUHK-PEDES when using THI module
- Shows consistent improvements across all four tested datasets (CUHK-PEDES, ICFG-PEDES, RSTPReid, UFine6926)
- RDA-augmented data improves baseline performance even without THI

## Why This Works (Mechanism)

### Mechanism 1: Semantic Gap Bridging via THI
The Test-time Human-centered Interaction (THI) module addresses the semantic gap between vague user queries and visual reality by performing multi-round VQA on candidate images. The MLLM extracts fine-grained attributes missing from the text (e.g., "green bag slung over her back"), which are then merged into the query to create a more accurate text embedding. This works under the assumption that the MLLM can accurately extract attributes from low-resolution surveillance images and the text encoder can effectively incorporate these details.

### Mechanism 2: Domain Alignment via LoRA Fine-tuning
The paper employs Low-Rank Adaptation (LoRA) to perform Supervised Fine-Tuning (SFT) on the MLLM using ReID dataset pairs, training it to output binary "Yes/No" verification for whether a text matches an image. This adapts the MLLM's internal representations to the specific visual style and fine-grained attributes of person re-identification data, addressing the domain gap that general-purpose MLLMs suffer from in surveillance ReID applications.

### Mechanism 3: Data Quality Enhancement via RDA
The Reorganization Data Augmentation (RDA) strategy enriches training data by using the MLLM to decompose static training sentences into attribute-specific sub-sentences, then rewriting and recombining these fragments to generate diverse training samples. This forces the text encoder to learn robust feature representations that are invariant to sentence structure and syntax variations, addressing the bottleneck of training data quality.

## Foundational Learning

- **Cross-Modal Similarity & Re-ranking**: Essential for understanding that THI is a post-hoc re-ranking mechanism that modifies the ranking results of an existing model rather than replacing the original image retrieval model.
  - Quick check: Does the system replace the original image retrieval model, or does it modify the ranking results of an existing model?

- **Multimodal Large Language Models (MLLMs) as Knowledge Engines**: Critical for distinguishing the MLLM's role in visual understanding (VQA) from a standard text-based LLM, as the method relies on MLLMs not just for text generation but for extracting visual attributes.
  - Quick check: Can a text-only LLM perform the tasks described in the THI module?

- **Instruction Tuning / Prompt Engineering**: Important for understanding that the method relies heavily on specific prompt templates ($T_{loc}$, $T_{vqa}$, etc.) to get structured output from the MLLM, with structurally different prompts for "Anchor Localization" (Yes/No) versus "Human-centered VQA" (open-ended attributes).
  - Quick check: Why is the prompt design for "Anchor Localization" (Yes/No) structurally different from the prompt for "Human-centered VQA" (open-ended attributes)?

## Architecture Onboarding

- **Component map**: Pre-trained TIReID model (RDE with CLIP) -> Initial retrieval -> THI module (MLLM with LoRA) -> Anchor localization -> VQA extraction -> Query rewriting -> Re-ranking

- **Critical path**: Offline: Prepare RDA-augmented data → Train/Finetune Backbone; Prepare SFT pairs → Finetune MLLM (LoRA); Online: User Query → Backbone retrieves Top-K → THI filters candidates via MLLM "Yes/No" → THI extracts features via MLLM VQA → Aggregate features into new query → Re-rank

- **Design tradeoffs**: Accuracy vs. Latency (increasing interaction rounds K improves Rank-1 but increases inference time linearly); Text Length Limits (method must "merge" answers rather than simply append them to avoid exceeding token limit of text encoder)

- **Failure signatures**: Error Accumulation (if MLLM incorrectly identifies wrong image as "anchor," subsequent refinement describes wrong person); Threshold Sensitivity (setting similarity threshold ξ too high may miss relevant candidates, too low may trigger expensive MLLM calls on obvious mismatches)

- **First 3 experiments**: 1) Ablation on THI: Run backbone with and without THI on validation set to measure Rank-1 gain vs. computational overhead; 2) LoRA Validity: Compare "Anchor Localization" step using raw vs. LoRA-adapted MLLM to verify domain alignment needs; 3) Data Scaling (RDA): Train backbone on raw data vs. RDA-augmented data (without THI) to isolate data augmentation contribution

## Open Questions the Paper Calls Out

- **Error Accumulation Mitigation**: How can the framework mitigate error accumulation in THI when initial anchor localization is incorrect? The method relies on MLLM "Yes" response to identify target, but if MLLM incorrectly identifies distractor (false positive), subsequent refinement aggregates wrong person's features.

- **Inference Latency Reduction**: Can the multi-round MLLM interaction latency be reduced for real-time or large-scale applications? While threshold strategy improves efficiency, underlying cost of generating MLLM responses remains bottleneck for immediate deployment.

- **RDA Robustness to Hallucinations**: How robust is RDA against factual hallucinations or noise introduced by MLLM during decomposition and rewriting? The strategy assumes MLLM's "enrichment" transfers external knowledge but doesn't quantify if MLLM introduces incorrect attributes that could mislead training.

## Limitations

- **Computational Overhead**: THI module introduces significant inference costs through multi-round MLLM interactions, potentially limiting real-world deployment in resource-constrained scenarios
- **Error Propagation**: The method's accuracy heavily depends on correct initial anchor localization, with error accumulation degrading performance when MLLM incorrectly identifies target image
- **Data Dependency**: RDA's effectiveness depends on MLLM's ability to generate semantically consistent attribute descriptions without introducing factual errors or logical inconsistencies

## Confidence

- **State-of-the-art Performance Claims**: High confidence - experimental results show consistent improvements across four benchmarks with statistically significant Rank-1 gains
- **Generalization Across Datasets**: Medium confidence - method shows consistent improvements but lacks analysis of failure cases or dataset-specific limitations
- **Plug-and-play Enhancement Capability**: Medium confidence - supported by results but lacks systematic ablation studies on different base models or configurations

## Next Checks

1. **Ablation Study Design**: Implement controlled experiment isolating RDA's contribution by training baseline model on: (a) original data only, (b) RDA-augmented data only (without THI), and (c) RDA + THI to quantify exact contribution of data augmentation versus test-time interaction.

2. **MLLM Verification Robustness**: Test LoRA-adapted MLLM's anchor localization performance on held-out validation set with known positive/negative pairs, measuring precision-recall curves to identify false positive rates and threshold sensitivity.

3. **Interaction Round Optimization**: Systematically vary K (interaction rounds) from 1 to 10 on validation set and plot accuracy vs. computational cost to identify optimal tradeoff point and test whether claimed K=5 is universally optimal.