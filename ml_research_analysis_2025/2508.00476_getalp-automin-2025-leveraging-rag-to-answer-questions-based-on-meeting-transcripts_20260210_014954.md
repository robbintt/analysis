---
ver: rpa2
title: 'GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts'
arxiv_id: '2508.00476'
source_url: https://arxiv.org/abs/2508.00476
tags:
- language
- context
- answer
- getalp
- automin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GETALP's participation in the AutoMin 2025
  Shared Task, focusing on answering questions based on meeting transcripts. The authors
  propose a retrieval-augmented generation (RAG) system that leverages dense sentence
  embeddings, Doc2Query-based document expansion, and Abstract Meaning Representation
  (AMR) to enhance answer generation.
---

# GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts

## Quick Facts
- arXiv ID: 2508.00476
- Source URL: https://arxiv.org/abs/2508.00476
- Reference count: 9
- Primary result: GETALP@AutoMin achieved 5.65/10 in human evaluation for English answers, with AMR enrichment improving "who" questions specifically

## Executive Summary
This paper presents GETALP's participation in the AutoMin 2025 Shared Task, focusing on answering questions based on meeting transcripts. The authors propose a retrieval-augmented generation (RAG) system that leverages dense sentence embeddings, Doc2Query-based document expansion, and Abstract Meaning Representation (AMR) to enhance answer generation. They develop three system variants: IR-only, IR+AMR, and AMR-only. Results show that incorporating AMR improves performance, especially for entity-related questions like "who" questions. In human evaluation on English answers, GETALP@AutoMin and GETALP@AutoMin_amr achieved similar scores of 5.65 and 5.55 respectively, while GETALP@AutoMin_amr outperformed IR-only in 18 out of 130 questions, with half being WHO questions. Automatic evaluation using LLM-as-judge showed no significant difference between configurations. The approach successfully handles long, dispersed dialogues in meeting transcripts.

## Method Summary
The GETALP system implements a RAG pipeline with three variants: IR-only, IR+AMR, and AMR-only. For retrieval, they combine dense sentence embeddings (all-MiniLM-L6-v2) with Doc2Query expansion (docTTTTTquery), indexing both in FAISS. Retrieved passages are expanded with ±1 adjacent sentence context. AMR enrichment involves parsing retrieved context into semantic graphs, extracting triples with PENMAN, translating these to natural language, and polishing with an LLM. Answers are generated using Llama-3.1-8B-Instruct, with retrieved context (and optionally AMR descriptions) provided as input. The system handles both English and Czech questions, with Czech answers generated by appending a language instruction to the prompt.

## Key Results
- GETALP@AutoMin achieved 5.65/10 in human evaluation for English answers
- GETALP@AutoMin_amr outperformed IR-only in 18 out of 130 questions, with half being WHO questions
- AMR improved or matched performance on 39 out of 45 WHO questions
- Automatic LLM-as-judge evaluation found no significant difference between system variants
- AMR-only variant performed worse than both IR-only and IR+AMR configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining dense sentence embeddings with Doc2Query document expansion improves retrieval coverage for meeting transcripts.
- **Mechanism:** Dense embeddings capture semantic similarity while Doc2Query generates synthetic queries that predict what questions a passage might answer. The union of both retrieval sets increases the likelihood of finding relevant passages that either method alone might miss.
- **Core assumption:** Retrieval errors in one method can be compensated by the other; both methods capture different relevance signals.
- **Evidence anchors:** [abstract]: Method is "based on a retrieval augmented generation (RAG) system"; [section 3.2]: "We implement an information retrieval setup that combines two complementary strategies: dense sentence embeddings and Doc2Query-based document expansion"; [corpus]: Weak direct evidence—related papers discuss RAG generally but do not validate this specific dual-retrieval combination.
- **Break condition:** If the two retrieval methods return highly overlapping results, the union provides marginal gains while increasing latency.

### Mechanism 2
- **Claim:** AMR-derived natural language descriptions improve performance on entity-focused questions, particularly "who" questions involving participant identification.
- **Mechanism:** AMR graphs explicitly encode semantic roles (e.g., :ARG0 for agent, :ARG1 for patient), resolving ambiguities about who did what. Converting these graphs to natural language descriptions makes this structure accessible to LLMs without requiring them to parse raw graph notation.
- **Core assumption:** LLMs process natural language descriptions of AMR triples more effectively than raw AMR notation or unstructured text alone.
- **Evidence anchors:** [abstract]: "provides notable improvements in answering questions that involve distinguishing between different participants (e.g., who questions)"; [section 5]: "Among the 45 WHO questions in total, AMR achieved the same or a better score in 39 of them"; [corpus]: "Semantic Bridge" paper (arXiv:2508.10013) supports AMR-driven synthesis for reasoning tasks, suggesting broader applicability.
- **Break condition:** If the AMR-to-text conversion introduces errors or loses critical nuance, the benefit may invert; AMR-only performs worse than IR-only overall (Table 2).

### Mechanism 3
- **Claim:** Expanding retrieved sentences with immediate context (±1 adjacent sentence) improves answer coverage.
- **Mechanism:** Meeting transcripts are conversational; relevant information often spans sentence boundaries. Including surrounding sentences provides the LLM with coherence cues that isolated sentences lack.
- **Core assumption:** Answers frequently reside in sentences adjacent to the most semantically similar match, not the exact match.
- **Evidence anchors:** [section 3.2]: "We observed that, in some cases, the answer to the question was actually contained in the sentence closest to the most similar one"; [abstract]: No direct mention of context expansion; [corpus]: No direct validation in related papers; this is an observation-based heuristic.
- **Break condition:** If expanded context introduces noise (irrelevant adjacent sentences), it may dilute the signal and confuse the generator.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** The entire system architecture is a RAG pipeline; understanding how retrieval and generation interact is essential for debugging either stage.
  - **Quick check question:** Given a user query, can you trace which retrieved passages the LLM used to generate its answer?

- **Concept:** Abstract Meaning Representation (AMR)
  - **Why needed here:** AMR is the core semantic enrichment strategy; you must understand how graphs encode roles (e.g., :ARG0 = agent) to diagnose conversion errors.
  - **Quick check question:** For the sentence "Alice sent the report to Bob," what would the key AMR triples look like?

- **Concept:** Doc2Query / Query Prediction
  - **Why needed here:** This expansion technique indexes synthetic queries alongside original sentences; understanding it explains why certain passages are retrieved unexpectedly.
  - **Quick check question:** If a passage about "budget cuts" generates the synthetic query "Why was funding reduced?", how does this affect retrieval behavior?

## Architecture Onboarding

- **Component map:** Meeting transcript + Question -> Sentence embeddings + Doc2Query -> FAISS index -> Top-k retrieval -> ±1 sentence expansion -> Context Cr -> AMR parsing -> Triple extraction -> Rule-based translation -> LLM polishing -> Context Camr -> LLM generation -> Answer A

- **Critical path:** Retrieval quality is the primary bottleneck. If the IR stage fails to surface relevant passages, neither AMR enrichment nor LLM generation can recover. Start debugging here.

- **Design tradeoffs:**
  - IR-only vs. IR+AMR: IR-only is faster and simpler; IR+AMR adds computational overhead but may help entity-focused questions.
  - AMR-only: Performs worst overall (Tables 1, 2)—do not use as sole context.
  - Manual vs. automatic evaluation: LLM-as-judge did not detect significant differences; manual evaluation revealed IR+AMR advantages on specific question types. Consider both in testing.

- **Failure signatures:**
  - AMR-only variant underperforms: Indicates retrieval grounding is essential; AMR cannot substitute for raw text.
  - Low scores on "who" questions without AMR: Suggests entity resolution gap; try enabling IR+AMR.
  - High variance in LLM-as-judge scores: Automatic evaluation may be unreliable for this task; validate with human review.

- **First 3 experiments:**
  1. **Retrieval ablation:** Run IR-only with dense embeddings alone vs. dense + Doc2Query. Measure recall on a held-out question set to quantify complementarity.
  2. **AMR impact by question type:** Stratify evaluation by question category (who/what/when/how). Confirm whether AMR gains are specific to entity-focused questions as claimed.
  3. **Context window size:** Test ±0, ±1, ±2 sentence expansion. Identify where additional context helps vs. introduces noise.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a dynamic routing mechanism selectively apply AMR augmentation only to specific question types (e.g., entity-based "who" questions) to maximize performance while minimizing computational overhead?
  - **Basis in paper:** [explicit] The conclusion states future work involves "selectively applying AMR in question types where structured semantic information offers the most benefit."
  - **Why unresolved:** The current study applied AMR uniformly across all queries. While beneficial for "who" questions (improving 39/45 cases), it introduced significant computational costs without universally outperforming the simpler IR-only baseline.
  - **What evidence would resolve it:** Experiments using a question-type classifier to trigger AMR only for entity-resolution queries, demonstrating comparable or superior accuracy to the IR+AMR system with reduced latency and GPU usage.

- **Open Question 2:** Does refining the AMR-to-text linearization process reduce information loss sufficiently to make the AMR-only variant competitive with the IR-only baseline?
  - **Basis in paper:** [explicit] The authors list "refining the AMR-to-text generation process" as a specific avenue for future work.
  - **Why unresolved:** The AMR-only system significantly underperformed compared to IR-only and IR+AMR in both automatic and manual evaluations, suggesting the current method of converting graph triples to natural language loses critical context.
  - **What evidence would resolve it:** Ablation studies comparing different AMR linearization techniques (e.g., different prompting strategies or models) that result in the AMR-only variant achieving scores statistically indistinguishable from the IR-only baseline.

- **Open Question 3:** Why did the LLM-as-a-judge automatic evaluation fail to reflect the improvements in "who" questions that were observed during human evaluation?
  - **Basis in paper:** [inferred] The authors note that manual evaluation showed AMR improved "who" questions in 39 out of 45 cases, whereas the LLM-as-a-judge evaluation found "no significant difference" between the system variants.
  - **Why unresolved:** There is a discrepancy between the automated metric (Prometheus) and human judgment regarding the value of AMR for specific semantic tasks, raising concerns about the sensitivity of the automatic evaluation protocol.
  - **What evidence would resolve it:** A correlation analysis showing that a modified automatic evaluation metric (or a different judge model) successfully captures the variance in quality for entity-focused questions that human annotators identified.

## Limitations

- The dual-retrieval strategy (dense embeddings + Doc2Query expansion) lacks direct experimental validation showing both components are necessary for performance gains.
- AMR enrichment shows promise but the specific parser and triple-to-text conversion rules are not disclosed, making exact reproduction difficult.
- The automatic evaluation using LLM-as-judge (Prometheus) showed no significant differences between configurations while manual evaluation revealed IR+AMR advantages, suggesting the LLM judge may not be reliable for this task.

## Confidence

**High Confidence:** The overall RAG pipeline architecture works for long meeting transcripts. The system successfully processes conversational dialogue and generates coherent answers, as evidenced by comparable human evaluation scores (5.65-5.55 out of 10).

**Medium Confidence:** AMR enrichment specifically improves entity-focused questions (particularly "who" questions). This claim is supported by human evaluation showing IR+AMR outperformed IR-only on 18 of 130 questions, with 39 of 45 "who" questions improving or staying the same.

**Low Confidence:** The dual-retrieval strategy provides significant complementary coverage. While described as combining "two complementary strategies," there's no ablation study proving both dense embeddings and Doc2Query are necessary, or that their union provides better coverage than either alone.

## Next Checks

1. **Retrieval Ablation Study:** Run experiments comparing IR-only with dense embeddings alone versus dense embeddings plus Doc2Query expansion. Measure recall@5/recall@10 on a held-out validation set to quantify whether the two methods capture different relevance signals or simply overlap substantially.

2. **Question-Type Stratified Evaluation:** Re-analyze human evaluation results by stratifying all 130 test questions into categories (who, what, when, where, how, why). Calculate performance differences between IR-only and IR+AMR for each category to confirm whether AMR gains are specific to entity-focused questions as claimed.

3. **Context Window Sensitivity Analysis:** Systematically test different context expansion strategies: ±0 sentences (no expansion), ±1 sentence (current approach), ±2 sentences, and retrieving entire paragraph context. Measure both answer quality and computational overhead to identify the optimal balance between coverage and noise.