---
ver: rpa2
title: Towards a Comparative Framework for Compositional AI Models
arxiv_id: '2507.02940'
source_url: https://arxiv.org/abs/2507.02940
tags:
- compositional
- figure
- compositionality
- each
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comparative framework for compositional
  AI models, focusing on two key properties: compositional generalisation and compositional
  interpretability. The authors use the DisCoCirc framework to create models that
  can parse and reason about natural language texts, then test these models on various
  aspects of compositionality using a dataset derived from the bAbI tasks.'
---

# Towards a Comparative Framework for Compositional AI Models

## Quick Facts
- **arXiv ID**: 2507.02940
- **Source URL**: https://arxiv.org/abs/2507.02940
- **Authors**: Tiffany Duneau
- **Reference count**: 40
- **Primary result**: Quantum models showed less overfitting and better systematicity than classical neural networks on compositional generalization tasks

## Executive Summary
This paper presents a comparative framework for evaluating compositional AI models using the DisCoCirc framework. The authors test both quantum circuit-based and classical neural network models on three aspects of compositionality: productivity, systematicity, and substitutivity. The framework provides both performance metrics and interpretability tools through diagram fragment analysis. The key finding is that quantum models exhibit superior systematic generalization and reduced overfitting compared to classical neural networks, while both architectures perform similarly on productivity and substitutivity tasks.

## Method Summary
The paper uses the DisCoCirc framework to map natural language texts to diagrams in a syntax category, which are then interpreted as computations in a meaning category. Models are trained on synthetic bAbI Task 6 data with specific compositional splits. Quantum models use unitary circuits with tensor products, while classical models use neural networks with direct sums. Training employs a novel "Valid AB" validation scheme that mixes in-distribution and compositional samples to prevent overfitting. The framework evaluates models on three aspects of compositionality and provides interpretability through diagram fragment analysis.

## Key Results
- Both quantum and classical models perform similarly on productivity and substitutivity tasks (within 5% difference)
- Quantum models outperform classical models on systematicity tasks by at least 10%
- Quantum models showed less overfitting to training data compared to neural models
- Interpretability analysis revealed models learned a more general question ("Is somebody in the park?") rather than the specific target question

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Models generalise compositionally when their implementation factors through a syntactic structure that mirrors the task's generative rules.
- **Mechanism**: The DisCoCirc framework maps natural language text to diagrams in a "syntax category" ($S_G$), which are then interpreted as computations in a "meaning category" ($M_L$). By enforcing this functorial mapping ($M = e; g$), the model learns the rules for combining atoms (words) rather than memorizing specific atom combinations, enabling generalisation to the "compositional closure" of the training data.
- **Core assumption**: The task labels $\nabla$ are semantically compositional relative to the chosen syntax $e_\nabla$ (i.e., the task can actually be solved by applying consistent rules to atoms).
- **Evidence anchors**:
  - [Abstract] "...ability of a model to generalise outside its training distribution by learning compositional rules..."
  - [Section 2.1.4] Defines Compositional Generalisation as $M|_\Sigma = \nabla|_\Sigma$ ensuring $M|_{\Sigma^*} = \nabla|_{\Sigma^*}$.
  - [Corpus] *Efficient Generation of Parameterised Quantum Circuits from Large Texts* supports the DisCoCirc structure but does not verify the causal link to generalisation.
- **Break condition**: If the syntax is degenerate (allows any combination) or the task requires non-local context (idioms), the functorial constraint fails to aid generalisation.

### Mechanism 2
- **Claim**: Quantum-based models (tensor product $\otimes$) exhibit superior systematicity and overgeneralisation resistance compared to classical neural networks (Cartesian product $\oplus$) due to differences in inductive bias.
- **Mechanism**: Quantum models use unitary circuits where states are normalized and interactions are entangling (non-Cartesian). This structure prevents the "superposition" of contradictory rules (overfitting exceptions) more effectively than classical networks, which can essentially memorize exceptions via distinct weight configurations. The paper observes that neural models "overfit the Train data" significantly more.
- **Core assumption**: The quantum ansatz (Sim4) is sufficiently expressive to represent the target function despite unitary constraints.
- **Evidence anchors**:
  - [Abstract] "quantum models showed less overfitting to training data compared to neural models."
  - [Section 4.3] Notes a >10% performance difference on systematicity tasks favoring quantum models.
  - [Section 3.1] Details the categorization of Quantum ($\text{CPM}(\text{Hilb}_2)$) vs Classical ($\text{NN}$ with direct sum).
  - [Corpus] Limited external evidence; *Separating the what and how of compositional computation* discusses compositionality broadly but does not confirm this specific quantum vs. classical trade-off.
- **Break condition**: If the circuit depth is insufficient to disentangle complex states, the quantum model may underfit.

### Mechanism 3
- **Claim**: Compositional interpretability is achieved by analyzing "diagram fragments" (sub-circuits) to reveal learned rules, even if the model has learned an imperfect approximation of the task.
- **Mechanism**: Because the model is syntactically compositional (built of modular boxes), one can compute overlaps between specific assertion states and diagram fragments. This allows researchers to diagnose *how* the model fails (e.g., distinguishing "Andrew" vs. "somebody") without needing to visualize the full high-dimensional state space.
- **Core assumption**: The "meaning" of a component is consistent regardless of the context it is placed in (Localism).
- **Evidence anchors**:
  - [Abstract] "...revealing that the model learns to answer a more general question than intended..."
  - [Section 5.2] Demonstrates comparing fragments (ID), (Ap), and (Ao) to diagnose the "Is somebody in the park?" behavior.
  - [Corpus] *Evaluating DisCoCirc in Translation Tasks* mentions DisCoCirc structure but not the interpretability mechanism.
- **Break condition**: If the model relies on "distributed" representations where meaning is non-local (not confined to specific wires/boxes), fragment analysis becomes ambiguous.

## Foundational Learning

- **Concept**: **Monoidal Categories & Functors**
  - **Why needed here**: The paper formalizes compositionality not as a loose property but as a mathematical mapping (functor) between a "syntax category" (grammar diagrams) and a "semantic category" (circuits/neural nets). Understanding objects, morphisms, and functors is required to parse Section 2.1.
  - **Quick check question**: Can you explain why a standard feed-forward neural network (mapping input vector to output vector) might fail to be a "functor" in the categorical sense?

- **Concept**: **Aspects of Compositionality (Systematicity vs. Productivity)**
  - **Why needed here**: The paper evaluates models on distinct axes (Section 2.2). *Productivity* measures generalisation to longer inputs; *Systematicity* measures generalisation to novel combinations of known atoms.
  - **Quick check question**: If a model trained on "John loves Mary" fails on "Mary loves John" but succeeds on "John loves Mary and Susan," which aspect of compositionality has it failed?

- **Concept**: **Unitary Operators vs. Cartesian Products**
  - **Why needed here**: To understand the difference between the Quantum and Classical implementations (Section 3.1). Classical neural nets use direct sums ($\oplus$), allowing variables to be processed independently, whereas Quantum models use tensor products ($\otimes$), creating complex dependencies (entanglement) between wires.
  - **Quick check question**: Why does a Cartesian product (direct sum) make it easier for a network to memorize specific training instances compared to a tensor product?

## Architecture Onboarding

- **Component map**: Syntax Parser -> Ansatz -> Evaluator -> Trainer
- **Critical path**: The **Extended Validation (Valid AB)** scheme (Section 3.4) is crucial. Unlike standard validation, this set must contain examples from both the training distribution and the test (compositional) distribution to estimate the "compositionality score" and prevent selecting models that merely memorize training data.
- **Design tradeoffs**:
  - **Quantum ($\otimes$)**: Better systematicity and less overfitting; higher simulation cost; robust hyperparameters.
  - **Classical ($\oplus$)**: Faster simulation; requires extensive tuning (dimensions, layers, activation); prone to overfitting (Section 4).
- **Failure signatures**:
  - **Neural Overfitting**: High Train accuracy, low Test accuracy, specifically failing the Systematicity task (Section 4.3).
  - **Quantum Ambiguity**: The model learns relations but fails to distinguish specific entities (e.g., answering "somebody is in the park" instead of "Andrew is in the park"), detectable via fragment analysis (Section 5.2).
- **First 3 experiments**:
  1. **Implement the Valid AB Baseline**: Set up the validation set to contain 50% "in-distribution" and 50% "compositional" samples to verify that model selection favors generalisation over memorization.
  2. **Run the Systematicity Stress Test**: Train both a Linear (Classical) and Sim4 (Quantum) model on the Systematicity dataset (Section 4.3) to reproduce the ~10% performance gap reported in the paper.
  3. **Fragment Overlap Analysis**: After training a model, compute the overlap between the "Is Andrew in the park?" assertion and the diagram fragment for "Andrew moves to park" vs. "Clara moves to park" to check if the model has learned to distinguish actors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative tensor product architectures (such as free-fermion or Clifford circuits) trade off expressivity and compositional generalization compared to the standard quantum circuits used?
- Basis in paper: [explicit] Section 6.1 suggests exploring classes of models like Clifford circuits and free-fermion based circuits to find a "nuanced trade-off" between expressivity and simulability.
- Why unresolved: The current study restricted $\otimes$ models to a specific quantum circuit ansatz (Sim4) with limited ancillas, leaving the performance of other tensor network architectures untested.
- What evidence would resolve it: Comparative evaluation of these alternative architectures on the systematicity and productivity tasks defined in the paper.

### Open Question 2
- Question: Can neural DisCoCirc models match quantum models on systematicity tasks by scaling depth, width, or altering activation functions?
- Basis in paper: [explicit] Section 6.1 proposes scaling up neural components ("deeper and wider networks") and exploring different activation functions (e.g., Sigmoid) to better utilize the $\oplus$ architecture.
- Why unresolved: The neural models currently underperform significantly on systematicity (at least 10% lower) and overfit compared to quantum models.
- What evidence would resolve it: Results from training deeper/wider neural models on the systematicity dataset showing improved generalization and reduced overfitting.

### Open Question 3
- Question: Can models achieve strong compositional generalization when trained on minimal base sets defined by "weak" or "quasi" systematicity?
- Basis in paper: [explicit] Section 6.1 identifies the need to explore whether architectures can succeed with "more minimal base sets" by applying Hadleyâ€™s graded notions of systematicity.
- Why unresolved: The paper notes the choice of the initial training set $A$ was arbitrary; it is undetermined if models can learn the necessary rules from sparser or "harder" data distributions.
- What evidence would resolve it: Training models on datasets constructed using weak or quasi-systematic constraints and measuring the resulting compositionality scores.

## Limitations
- The evaluation framework relies heavily on synthetic data that may not capture real-world compositional complexity
- The quantum advantage demonstrated is task-specific and may not generalize to other domains
- Interpretability analysis depends on assumptions about Localism that may not hold for all neural architectures
- Fragment analysis technique requires further validation on more complex tasks

## Confidence

- **Compositional Generalization Mechanism**: High
- **Quantum vs Classical Performance**: Medium
- **Interpretability via Diagram Fragments**: Medium

## Next Checks

1. **Generalization to Other Tasks**: Apply the framework to different compositional tasks (e.g., VQA or semantic parsing) to verify if quantum advantages persist across domains.
2. **Alternative Quantum Ansatze**: Test additional quantum circuit architectures beyond Sim4 and Euler to determine if the observed systematicity advantage is architecture-specific or general.
3. **Real-world Dataset Validation**: Evaluate models on a non-synthetic dataset with natural compositional structure to assess practical applicability beyond controlled experiments.