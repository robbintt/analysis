---
ver: rpa2
title: 'MA-DPR: Manifold-aware Distance Metrics for Dense Passage Retrieval'
arxiv_id: '2509.13562'
source_url: https://arxiv.org/abs/2509.13562
tags:
- manifold
- distance
- embedding
- passage
- euclidean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MA-DPR introduces manifold-aware distance metrics for dense passage
  retrieval to better capture query-passage relevance when embeddings lie on non-linear,
  subdimensional manifolds. The method constructs a nearest-neighbor graph over passage
  embeddings and measures distances via shortest paths on this graph, leveraging local
  and global manifold structure.
---

# MA-DPR: Manifold-aware Distance Metrics for Dense Passage Retrieval

## Quick Facts
- **arXiv ID:** 2509.13562
- **Source URL:** https://arxiv.org/abs/2509.13562
- **Reference count:** 34
- **Key outcome:** MA-DPR outperforms Euclidean/cosine distances by up to 26% on out-of-distribution passage retrieval while maintaining comparable in-distribution performance.

## Executive Summary
MA-DPR introduces manifold-aware distance metrics for dense passage retrieval to better capture query-passage relevance when embeddings lie on non-linear, subdimensional manifolds. The method constructs a nearest-neighbor graph over passage embeddings and measures distances via shortest paths on this graph, leveraging local and global manifold structure. Experiments across four benchmark datasets and two embedding models show that MA-DPR outperforms Euclidean and cosine distances by up to 26% on out-of-distribution passage retrieval while maintaining comparable in-distribution performance. The approach incurs only minimal runtime overhead and effectively retrieves relevant passages even without direct semantic overlap by leveraging context from neighboring passages.

## Method Summary
MA-DPR operates in two stages: offline and online. In the offline stage, it constructs a weighted undirected K-nearest neighbor graph over all passage embeddings using either Euclidean or spectral distance metrics. Each edge is assigned a cost (uniform or distance-weighted). In the online stage, for each query, it temporarily adds the query as a vertex connected to its K nearest passages, computes shortest-path distances to all passages via Dijkstra's algorithm, and ranks passages by ascending manifold distance. The method requires ℓ2-normalized embeddings and uses K=8 for main experiments with Uniform Cost assignment.

## Key Results
- MA-DPR improves Recall@20 by up to 26% on NFCorpus compared to baseline DPR methods
- Outperforms Euclidean and cosine distances across all four benchmark datasets
- Uniform Cost assignment consistently outperforms Distance Cost in out-of-distribution settings
- Successfully retrieves relevant passages lacking direct semantic overlap with queries

## Why This Works (Mechanism)

### Mechanism 1: Graph-based distance captures non-linear manifold structure
MA-DPR constructs a K-nearest neighbor graph over passage embeddings where edges connect semantically proximate passages. Distance between query and passage is computed as the shortest-path cost on this graph rather than direct Euclidean distance. This allows relevance to propagate through semantically related intermediate passages. The core assumption is that passages form a connected manifold structure where semantic similarity is locally preserved even if global Euclidean geometry is distorted.

### Mechanism 2: Uniform edge costs robust to OOD embedding distortion
Uniform Cost (UC) assigns cost=1 to all edges, emphasizing connectivity over embedding distances. This mitigates noise from unreliable distance metrics when embeddings were not optimized for the target domain. Distance Cost (DC) preserves raw embedding distances as edge weights. The core assumption is that in OOD settings, local embedding distances are distorted and unreliable; discrete connectivity is more robust.

### Mechanism 3: Manifold distance enables indirect semantic bridging
Relevant passages connected to the query via chains of semantically related neighbors can achieve low manifold distance even without direct similarity. The graph structure provides missing contextual bridges (e.g., "conveyor belt" → "Henry Ford" via manufacturing-related passages). The core assumption is that the passage corpus contains sufficient semantically intermediate documents to form connecting paths.

## Foundational Learning

- **Manifold Hypothesis:** Why needed: The entire method assumes high-dimensional embeddings lie on lower-dimensional, potentially non-linear manifolds where Euclidean distance fails to capture true semantic relationships. Quick check: Can you explain why two points that are far in Euclidean space might be close on a manifold?
- **K-Nearest Neighbor Graphs and Shortest Paths (Dijkstra's Algorithm):** Why needed: Core implementation requires constructing KNN graphs and computing shortest-path distances efficiently at query time. Quick check: Given a graph with N nodes and K edges per node, what is the time complexity of Dijkstra's algorithm?
- **Spectral Embeddings and Graph Laplacian:** Why needed: One design variant uses spectral distance derived from graph Laplacian eigenvectors as an alternative to Euclidean distance for KNN graph construction. Quick check: What does the normalized graph Laplacian capture about graph structure, and why might its eigenvectors provide better local distance metrics?

## Architecture Onboarding

- **Component map:** Passage embeddings → KNN graph construction → Edge cost assignment → Query integration → Shortest-path computation → Ranking
- **Critical path:** 1) Graph construction (O(N²D) for brute-force KNN) 2) Spectral embedding computation if using spectral distance 3) Dijkstra's shortest-path from query vertex to all passage vertices
- **Design tradeoffs:** K selection (small → sparse graph, large → over-smoothing), d^KNN choice (Euclidean vs. Spectral), Cost function (UC vs. DC)
- **Failure signatures:** Relevant passages ranked 500+ with d_Manifold but top-50 with d_Euclidean (isolated passages), performance collapses to baseline (K too large), disconnected graph components (K too small)
- **First 3 experiments:** 1) Sanity check: Visualize d_Euclidean vs. d_Manifold scatter plots to verify non-linear manifold structure 2) K ablation: Grid search K∈{4,6,8,10} with c^UC to find optimal connectivity 3) OOD validation: Compare MA-DPR vs. baseline on held-out domain

## Open Questions the Paper Calls Out
- Can incorporating supervised relevance signals to learn graph edge weights improve retrieval effectiveness over the current unsupervised distance metrics?
- Can approximate nearest neighbor methods and incremental graph construction effectively scale MA-DPR to massive or dynamic corpora?
- How can the manifold graph construction be modified to mitigate the "isolated passage" problem where relevant technical documents are sparsely connected?

## Limitations
- Scalability challenges with complete KNN graph construction for large corpora
- Reliance on sufficient semantic connectivity in the passage corpus
- Performance degrades when passages are isolated or weakly connected
- Limited testing on diverse domain shifts beyond scientific and question-answering domains

## Confidence
- **High:** Manifold-aware distances improve OOD retrieval when graph connectivity exists
- **Medium:** UC cost function generally outperforms DC in OOD settings  
- **Low:** Ability to retrieve relevant passages lacking direct semantic overlap is demonstrated but not quantitatively isolated

## Next Checks
1. Verify non-linear manifold structure exists in your corpus by comparing d_Euclidean vs. d_Manifold scatter plots before deployment
2. Conduct K-NN graph connectivity analysis to identify and address isolated relevant passages
3. Test MA-DPR performance on a held-out domain shift (if available) to confirm OOD gains beyond the reported NFCorpus/SciDocs results