---
ver: rpa2
title: 'The State Of TTS: A Case Study with Human Fooling Rates'
arxiv_id: '2508.04179'
source_url: https://arxiv.org/abs/2508.04179
tags:
- speech
- human
- evaluation
- systems
- open-source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Human Fooling Rate (HFR) as a deception-based
  metric for evaluating text-to-speech (TTS) systems, addressing the gap between subjective
  preference scores and true perceptual indistinguishability from human speech. The
  authors conduct a large-scale evaluation of ten state-of-the-art TTS systems (5
  open-source and 5 commercial) using HFR tests with 135 participants across multiple
  benchmarks.
---

# The State Of TTS: A Case Study with Human Fooling Rates

## Quick Facts
- arXiv ID: 2508.04179
- Source URL: https://arxiv.org/abs/2508.04179
- Reference count: 0
- Primary result: Commercial TTS systems achieve human-level deception (71.49% HFR) while open-source models max at 50.26% HFR

## Executive Summary
This paper introduces Human Fooling Rate (HFR) as a deception-based metric for evaluating text-to-speech systems, addressing the gap between subjective preference scores and true perceptual indistinguishability from human speech. Through large-scale evaluation of ten state-of-the-art TTS systems (5 open-source and 5 commercial) using HFR tests with 135 participants, the authors demonstrate that commercial models achieve human deception rates comparable to reference human audio, while open-source models lag significantly. The study reveals that systems with high MUSHRA scores often perform poorly on HFR, exposing reference-matching bias in traditional evaluations. Fine-tuning on high-quality conversational data improves open-source TTS deception rates modestly but fails to bridge the gap with commercial systems.

## Method Summary
The study employs Human Fooling Rate (HFR) as a binary forced-choice evaluation where participants classify audio as human or synthetic. The evaluation uses the SAFFRON platform with 135 native US-English participants from Prolific, enforcing headphone use and full-sample playback. Ten TTS systems (5 open-source and 5 commercial) are evaluated across multiple benchmarks including LJSpeech, LibriTTS, LibriSpeech, and Expresso. The HFR metric is calculated as the percentage of times listeners incorrectly classify synthetic speech as human. Additionally, granular HFR analysis with 9 artifact categories (digital voice quality, unnatural pauses, monotonic delivery, etc.) provides diagnostic insights. Fine-tuning experiments on 40 hours of Expresso data are conducted for F5-TTS and VoiceCraft to assess data quality impact.

## Key Results
- Commercial models (PlayHT: 71.49% HFR, ElevenLabs: 69.85% HFR) achieve deception rates comparable to human reference audio (70.68% HFR)
- Open-source models max at 50.26% HFR, with VoiceCraft achieving the highest among them
- XTTS scores 76.58 MUSHRA but only fools listeners 41.8% of the time, demonstrating reference-matching bias
- Digital voice quality (36.1% of detections), unnatural pauses (22.8%), and monotonic delivery (20.6%) are the primary artifacts distinguishing open-source from human speech
- Fine-tuning on 40 hours of Expresso improves VoiceCraft from 30.52% to 43.45% HFR but fails to close the commercial gap

## Why This Works (Mechanism)

### Mechanism 1
Deception-based evaluation exposes gaps that preference-based metrics (MUSHRA, CMOS) fail to capture by forcing binary classification rather than relative quality scoring. This directly measures deployment-readiness: can a system pass as human?

### Mechanism 2
Open-source TTS systems exhibit identifiable artifact patterns (digital voice quality, unnatural pauses, monotonic delivery) that commercial systems avoid. Commercial models match or exceed human baselines on these perceptual cues.

### Mechanism 3
Fine-tuning on high-quality conversational data improves deception rates but fails to close the commercial-open-source gap due to insufficient data volume or architectural limitations.

## Foundational Learning

- **Binary forced-choice evaluation vs. scalar preference rating**: HFR fundamentally differs from MUSHRA/CMOS by eliminating reference-comparison bias. Understanding this distinction is critical for interpreting why systems with high MUSHRA can have low HFR.
  - Quick check: If a TTS system scores 85 on MUSHRA against a monotonic human reference, can you conclude it sounds natural? (Answer: No—the reference may set a low bar.)

- **Zero-shot voice cloning with speech prompts**: All evaluated systems use speaker prompts rather than text-only conditioning. Prompt quality directly affects output realism; low-quality prompts from LibriSpeech produce lower HFR than high-quality Expresso prompts.
  - Quick check: Why does the paper evaluate on Expresso rather than just LJSpeech? (Answer: Expresso uses professional voice actors in conversational settings, creating a more challenging and realistic benchmark.)

- **Reference-matching bias in subjective evaluation**: MUSHRA/CMOS raters optimize for similarity to reference rather than absolute naturalness. This inflates scores for systems matching mediocre references.
  - Quick check: A system matches a low-quality reference perfectly. What will MUSHRA show vs. HFR? (Answer: High MUSHRA, low HFR—because the reference itself would fail deception testing.)

## Architecture Onboarding

- **Component map**: SAFFRON platform -> HFR computation (binary classification) -> Granular HFR variant (9-category artifact tagging) -> Benchmark hierarchy (LJSpeech/LibriTTS/LibriSpeech -> Expresso)
- **Critical path**: 1) Select benchmark where human HFR >70%, 2) Generate TTS outputs with distinct prompt/utterance pairs, 3) Deploy HFR test via SAFFRON with ≥30 participants × 30 utterances per system, 4) Compute HFR; if <60%, run granular HFR to diagnose artifact categories
- **Design tradeoffs**: HFR is faster (24.3s vs. 42.5s per sample) and more deployment-relevant, but provides no relative ranking between systems. Granular HFR provides richer diagnostics but requires additional rater training. High-quality benchmarks (Expresso) reveal commercial/open-source gap; standard benchmarks (LJSpeech) inflate open-source performance.
- **Failure signatures**: Human HFR <70% on your benchmark → benchmark unsuitable for deception testing. MUSHRA >75 but HFR <50% → system overfits to reference, not naturalness. Large HFR variance across prompts → poor zero-shot generalization. Digital voice quality artifact rate >25% → vocoder or acoustic model limitation.
- **First 3 experiments**: 1) Baseline HFR: Test your TTS system on LJSpeech and Expresso to establish deception rates; compare against Table 1 and Table 2 baselines. 2) Granular artifact audit: If HFR <55%, run granular HFR on 30 samples with 15 raters to identify top-3 artifact categories. 3) Fine-tuning ablation: Fine-tune on 40+ hours of high-quality conversational data (Expresso-style); measure HFR delta.

## Open Questions the Paper Calls Out

### Open Question 1
What specific acoustic or prosodic features cause open-source TTS to be perceived as having "digital voice quality" (the most cited artifact at 36.1%), and can targeted model improvements address this? The paper identifies the problem through user feedback but does not conduct acoustic analysis or propose architectural remedies.

### Open Question 2
How much high-quality conversational training data is required for open-source TTS to achieve human-level deception rates, and is data volume alone sufficient? The paper tests only one dataset size (40 hours) and doesn't explore whether larger datasets or different training strategies might yield different results.

### Open Question 3
Does HFR generalize across languages, dialects, and cultural contexts beyond US English? The methodology explicitly restricts participants to "native US-English speakers" and evaluates only English benchmarks, leaving cross-linguistic validity unexplored.

## Limitations
- HFR lacks granularity for relative system comparison between systems with similar deception rates
- Three commercial systems remain unnamed, preventing full reproducibility and architectural analysis
- Fine-tuning experiments use only 40 hours of data without exploring whether larger datasets or different architectures might close the commercial gap

## Confidence

**High Confidence**: The core finding that commercial TTS systems achieve human-level deception rates (71.49% HFR vs. 70.68% human baseline) is well-supported by the data.

**Medium Confidence**: The claim that reference-matching bias in MUSHRA evaluations explains the HFR-MUSHRA discrepancy is plausible but not definitively proven.

**Low Confidence**: The assertion that 40 hours of conversational data is insufficient for bridging the commercial-open-source gap is based on limited fine-tuning experiments.

## Next Checks

1. **MUSHRA-HFR correlation study**: Conduct a controlled experiment where the same TTS outputs are evaluated under both MUSHRA and HFR conditions with identical listeners. Measure the correlation between scores and systematically vary reference quality to test the reference-matching hypothesis.

2. **Commercial system transparency audit**: Request full disclosure of all commercial systems evaluated, including their architectural approaches and training dataset sizes. Compare these specifications against open-source systems to determine if the gap is primarily architectural or data-driven.

3. **Fine-tuning scaling experiment**: Extend the fine-tuning study to multiple dataset sizes (40h, 200h, 1000h) and architectures. Include both existing open-source models and simpler baseline architectures to determine whether scale or architecture is the primary differentiator in achieving human deception rates.