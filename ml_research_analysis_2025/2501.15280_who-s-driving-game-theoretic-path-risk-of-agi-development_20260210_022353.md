---
ver: rpa2
title: Who's Driving? Game Theoretic Path Risk of AGI Development
arxiv_id: '2501.15280'
source_url: https://arxiv.org/abs/2501.15280
tags:
- content
- corpus
- documents
- data
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes iterative constitutional corpus curation as
  a method for improving model safety through pretraining data filtering. The core
  idea is that models trained on filtered data can be used to filter the corpus more
  effectively, creating a virtuous cycle that converges to a self-consistent corpus
  where the trained model approves of its own training data.
---

# Who's Driving? Game Theoretic Path Risk of AGI Development

## Quick Facts
- arXiv ID: 2501.15280
- Source URL: https://arxiv.org/abs/2501.15280
- Reference count: 4
- Primary result: Proposes iterative constitutional corpus curation as a method for improving model safety through pretraining data filtering

## Executive Summary
This position paper proposes a novel approach to improving AI safety through iterative corpus curation during pretraining. The method involves training models on filtered data, using them to filter the corpus more effectively, and repeating this process to create a virtuous cycle that converges to a self-consistent corpus. The authors provide theoretical analysis showing guaranteed convergence to a fixed point and exponential decay in harmful content, while arguing this approach offers human-auditable oversight compared to opaque model internals.

## Method Summary
The iterative constitutional corpus curation method works by starting with an initial corpus and constitution defining acceptable content. Models are trained on the corpus, then used to score and filter documents against the constitution. The filtered corpus becomes the training data for the next iteration. This process repeats, theoretically converging to a self-consistent corpus where the trained model approves of its own training data. The approach aims to exponentially reduce harmful content while maintaining useful capabilities through careful threshold management.

## Key Results
- Theoretical proof of guaranteed convergence to a fixed point in the iterative filtering process
- Exponential decay bound on harmful content even with constant filter quality
- Human-auditable corpus approach provides scalable oversight compared to black-box model internals

## Why This Works (Mechanism)
The mechanism works through a virtuous cycle where each iteration produces a cleaner corpus that enables better filtering in subsequent rounds. As models train on progressively cleaner data, they develop representations that can more effectively identify and filter harmful content. The shared nature of the corpus means safety investments benefit all participants, creating incentives for cooperation rather than defection in AGI development.

## Foundational Learning
- **Constitutional filtering**: Defining acceptable content through explicit rules or principles - needed to provide consistent filtering criteria across iterations; quick check: verify constitution captures all relevant harm categories
- **Iterative training dynamics**: Understanding how model performance changes when trained on progressively cleaner data - needed to predict convergence behavior; quick check: monitor filter quality metrics across iterations
- **Compositional risk analysis**: Recognizing that harmful capabilities may emerge from combinations of benign components - needed to assess whether document-level filtering is sufficient; quick check: test final corpus for emergent harmful capabilities

## Architecture Onboarding

**Component map**: Initial corpus -> Model training -> Document scoring -> Filtering -> Cleaned corpus -> Next iteration

**Critical path**: The key steps are corpus filtering and model retraining. Each iteration depends on the previous one's output, creating a sequential dependency where filter quality directly impacts the next model's ability to recognize harms.

**Design tradeoffs**: The main tradeoff is between safety (removing harmful content) and capability (retaining useful knowledge). Too aggressive filtering may eliminate dual-use content needed for capabilities, while too permissive filtering may not adequately reduce risks.

**Failure signatures**: Filter quality degradation when models trained on overly clean data lose ability to recognize subtle harms; premature convergence to low-quality fixed points; capability loss on specific domains due to over-filtering.

**First experiments**:
1. Implement three iterations using a standard toxicity classifier to measure convergence speed and capability retention
2. Conduct ablation studies varying filtering thresholds to find optimal safety-capability balance
3. Design multi-lab simulation to test whether cooperative filtering behavior emerges under different incentive structures

## Open Questions the Paper Calls Out
The paper explicitly identifies several key questions requiring empirical validation. The primary question is whether iterative corpus curation empirically converges to a self-consistent corpus with exponential decay of harmful content as the theoretical analysis suggests. This requires pretraining infrastructure to test across multiple iterations. Another critical question is whether filter quality degrades or improves when models are trained on progressively cleaner data, as it's unknown if models need exposure to harmful content to recognize it. The paper also raises concerns about compositional risks, questioning whether document-level filtering might inadvertently select for dangerous capabilities that emerge from combinations of individually benign documents.

## Limitations
- Theoretical analysis assumes perfect filter quality and may not capture real-world degradation effects
- SCORE() function specification is undefined, creating implementation uncertainty
- Document-level filtering may miss compositional risks where harmful capabilities emerge from document combinations

## Confidence
- High confidence: The theoretical convergence proof and exponential decay bound are mathematically sound
- Medium confidence: Capability-safety tradeoff bounds and human-auditable oversight claims depend on practical implementation details
- Low confidence: The assertion that shared safety investments will dominate defection in AGI development dynamics requires empirical validation

## Next Checks
1. Implement a prototype with three iterations using a standard toxicity classifier as SCORE() to empirically measure convergence speed and capability retention on held-out benchmarks
2. Conduct ablation studies varying filtering thresholds across iterations to identify optimal schedules balancing safety gains against capability loss
3. Design and execute a multi-lab collaboration simulation where participants share filtered corpora under different incentive structures to test whether cooperative filtering behavior emerges as predicted