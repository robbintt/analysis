---
ver: rpa2
title: Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning
arxiv_id: '2505.21178'
source_url: https://arxiv.org/abs/2505.21178
tags:
- reasoning
- frac
- length
- response
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage reinforcement learning framework
  for concise reasoning in LLMs. The first stage uses GRPO++ to incentivize reasoning
  capabilities, while the second stage applies L-GRPO to enforce conciseness.
---

# Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning
## Quick Facts
- arXiv ID: 2505.21178
- Source URL: https://arxiv.org/abs/2505.21178
- Reference count: 29
- Key outcome: 55.2% average accuracy improvement with 21-23% response length reduction across mathematical reasoning benchmarks

## Executive Summary
This paper introduces a two-stage reinforcement learning framework for generating concise Chain-of-Thought reasoning responses from LLMs. The approach first optimizes reasoning capabilities using GRPO++ with dynamic sampling and entropy regularization, then applies L-GRPO to enforce conciseness by rewarding remaining context length only when all rollouts per sample are correct. The method achieves state-of-the-art performance on mathematical reasoning benchmarks while significantly reducing response verbosity, demonstrating that reasoning quality must be established before conciseness optimization.

## Method Summary
The authors propose a two-stage reinforcement learning framework where Stage 1 uses GRPO++ (an enhanced GRPO with clip-higher, dynamic sampling, and entropy bonus) to optimize reasoning accuracy on ~59K training samples, achieving ~81% accuracy on MATH-500. Stage 2 applies L-GRPO with a length-aware reward (1 - Li/LMax) only when all 32 rollouts per sample are correct, reducing response length by 21-23% while maintaining or improving accuracy. The framework trains on Qwen2.5-Math-7B base model using the verl framework with answer verification through Math-Verify filtering.

## Key Results
- 55.2% average accuracy improvement compared to base model across five benchmarks
- 21-23% reduction in response length while maintaining/improving accuracy
- Outperforms state-of-the-art reasoning models with zero RL paradigm
- Achieves 84.2% accuracy on AIME 2024, 88.4% on MATH-500, 85.4% on AMC 2023

## Why This Works (Mechanism)
The method works by separating reasoning quality from conciseness optimization, following the "walk before you run" principle. Stage 1 establishes robust reasoning capabilities through GRPO++ with entropy regularization that prevents premature convergence to suboptimal strategies. Stage 2's L-GRPO only applies length constraints once the model consistently produces correct answers, avoiding the common failure mode where direct length optimization causes models to skip reasoning entirely and guess answers. The length reward based on remaining context (1 - Li/LMax) provides indirect optimization that doesn't penalize the model for thinking through problems correctly.

## Foundational Learning
- **Chain-of-Thought reasoning**: Sequential step-by-step problem solving approach that improves accuracy but increases verbosity; needed to understand the target capability being optimized.
- **GRPO (Group Relative Policy Optimization)**: RL algorithm that compares rollouts within groups rather than against all samples; needed to understand the baseline algorithm being enhanced.
- **Dynamic sampling in RL**: Technique that skips samples where all rollouts are identical or where none are correct; needed to understand the training efficiency improvements.
- **Entropy regularization**: Method to encourage exploration by penalizing low-entropy distributions; needed to understand how the model avoids premature convergence.
- **Math-Verify filtering**: Process to ensure extracted answers match ground truth; needed to understand data quality requirements for mathematical reasoning tasks.

## Architecture Onboarding
**Component Map**: Base LLM -> GRPO++ Stage 1 -> L-GRPO Stage 2 -> Concise reasoning model
**Critical Path**: Data preparation (merge/filter datasets) -> Stage 1 GRPO++ training -> Accuracy validation -> Stage 2 L-GRPO training -> Final evaluation
**Design Tradeoffs**: Two-stage approach vs. single-stage optimization (sacrifices training efficiency for reliability), indirect length reward vs. direct comparison (slower convergence but avoids empty reasoning failure), high rollout count (32) vs. computational cost (ensures reliable correctness assessment)
**Failure Signatures**: Accuracy degradation when length optimization applied too early, model skipping reasoning to output immediate answers, convergence to trivial solutions without entropy bonus
**First Experiments**: 1) Validate GRPO++ implementation on small subset before full training, 2) Test length reward function on held-out samples to verify indirect optimization works, 3) Verify all-rollouts-correct condition implementation before Stage 2 training

## Open Questions the Paper Calls Out
None

## Limitations
- Training data sources (DeepScaleR2, DAPO-Math-17K) may not be publicly available in required forms
- Critical implementation details for dynamic sampling and correctness determination are underspecified
- Effectiveness depends on achieving high accuracy in first stage before applying length optimization

## Confidence
- **High Confidence**: Two-stage RL methodology and "walk before you run" principle are well-validated across multiple benchmarks
- **Medium Confidence**: Length reduction and accuracy maintenance claims are supported but ablation studies lack exploration of alternative approaches
- **Low Confidence**: Key implementation details for dynamic sampling and rollout correctness assessment are insufficient for reliable reproduction

## Next Checks
1. Test alternative length optimization approaches (relative length comparison, absolute token limits) to determine if remaining context length formula is essential
2. Systematically vary the accuracy threshold for enabling Stage 2 to quantify the trade-off between accuracy and conciseness
3. Evaluate model performance on out-of-distribution mathematical problems to assess generalization of conciseness training