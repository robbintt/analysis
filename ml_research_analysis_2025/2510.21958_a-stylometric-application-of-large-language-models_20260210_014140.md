---
ver: rpa2
title: A Stylometric Application of Large Language Models
arxiv_id: '2510.21958'
source_url: https://arxiv.org/abs/2510.21958
tags:
- author
- authors
- each
- trained
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces "predictive comparison," a new stylometric
  method that uses large language models (LLMs) to capture and measure authorial style.
  The method trains separate GPT-2 models from scratch on each author's complete works,
  then measures cross-entropy loss when predicting held-out text.
---

# A Stylometric Application of Large Language Models

## Quick Facts
- arXiv ID: 2510.21958
- Source URL: https://arxiv.org/abs/2510.21958
- Authors: Harrison F. Stropkay; Jiayi Chen; Mohammad J. Latifi; Daniel N. Rockmore; Jeremy R. Manning
- Reference count: 19
- Primary result: Introduces "predictive comparison" - a stylometric method using LLM cross-entropy losses that achieves 100% accuracy in authorship attribution

## Executive Summary
This paper introduces "predictive comparison," a novel stylometric method that uses large language models to capture and measure authorial style. The approach trains separate GPT-2 models from scratch on each author's complete works, then measures cross-entropy loss when predicting held-out text. Lower loss indicates higher stylistic similarity, enabling both authorship attribution and stylometric distance measurement.

The method achieves perfect classification accuracy distinguishing eight classic authors by comparing cross-entropy losses across models. It also resolves the well-known authorship attribution problem of the 15th Oz book, confirming Thompson's authorship. Ablation studies reveal that both content words and function words contribute to authorial signatures, while part-of-speech patterns alone are less distinctive.

## Method Summary
The predictive comparison method trains individual GPT-2 models from scratch on each author's complete works, using cross-entropy loss as a stylometric signal. Each model learns the statistical patterns characteristic of its author's writing style. When presented with held-out text, the model trained on that author's corpus produces lower cross-entropy loss than models trained on other authors' corpora. This loss differential enables both authorship attribution (assigning contested text to the model with lowest loss) and stylometric distance measurement (using the symmetric loss difference between model pairs).

## Key Results
- Achieved 100% classification accuracy in distinguishing eight classic authors (Austen, Baum, Dickens, Fitzgerald, Melville, Thompson, Twain, Wells) using cross-entropy loss comparisons
- Method rapidly converges, achieving statistical significance after just 1-2 training epochs for most authors
- Successfully resolved the 15th Oz book authorship attribution, confirming Thompson's authorship
- Ablation studies show both content words and function words contribute to authorial signatures, while POS patterns alone are less distinctive

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training an LLM on one author's corpus causes it to internalize author-specific statistical patterns, which manifest as lower cross-entropy loss on held-out text from the same author versus different authors.
- **Mechanism:** The language model learns a probability distribution over token sequences. When training data comes from a single author, the learned distribution reflects that author's characteristic word choices, syntactic constructions, and sequential dependencies. Cross-entropy loss directly measures the mismatch between the model's predicted distribution and the empirical distribution of the test text.
- **Core assumption:** Authors possess quantitatively distinguishable writing patterns that persist across their works and differ from other authors' patterns in ways a language model can capture.
- **Evidence anchors:** [abstract] "an individual GPT-2 model, trained from scratch on the works of one author, will predict held-out text from that author more accurately than held-out text from other authors"; [section 3.1] "For every author's held-out text, the model trained on the same author's writings produces the lowest loss"
- **Break condition:** If authors in a candidate set have highly similar styles, or if test texts span genres/topics far from training data, the loss separation may diminish or invert.

### Mechanism 2
- **Claim:** Cross-entropy loss differences between author-specific models provide a symmetric, interpretable stylometric distance metric.
- **Mechanism:** The paper defines distance d(i,j) = ½[Lⱼ(i) + Lᵢ(j)], where Lⱼ(i) is the normalized loss of author i's text under author j's model. This captures mutual stylistic dissimilarity—authors with similar styles (e.g., Baum and Thompson, both Oz authors) show smaller mutual losses.
- **Core assumption:** Stylistic similarity is approximately symmetric; if author A's model predicts B's text well, B's model should predict A's text reasonably well.
- **Evidence anchors:** [section 3.2] "predictive comparison suggests a natural notion of distance between authorial styles"; [section 3.1] "Authors with similar writing styles (e.g., Baum and Thompson) yield relatively small losses when evaluated using models trained on the other author's texts"
- **Break condition:** Asymmetric relationships (one author heavily influenced by another but not vice versa) may not be well-represented by symmetrized distance.

### Mechanism 3
- **Claim:** Both lexical choice (content words) and syntactic patterns (function words) contribute to distinguishable authorial signatures, but grammatical structure alone (part-of-speech sequences) is insufficient.
- **Mechanism:** Ablation studies train models on modified corpora where specific word classes are masked. Content-word-only and function-word-only models retain discriminatory power (6/8 and 5/8 authors respectively), while POS-only models fail for most authors (3/8).
- **Core assumption:** The masking procedure isolates the intended linguistic feature without introducing artifacts.
- **Evidence anchors:** [section 3.4] "models trained on content-word-only corpora reliably learned author-specific patterns for 6 of the 8 authors"; [section 3.4] "models trained solely on part-of-speech sequences struggled to distinguish authors, suggesting that grammatical structure alone is less distinctive"
- **Break condition:** For authors with highly distinctive syntactic idiosyncrasies, POS-only models might still work; for authors with similar vocabularies but different themes, content-word-only models may conflate them.

## Foundational Learning

- **Concept: Cross-entropy loss and perplexity**
  - **Why needed here:** The entire method hinges on interpreting cross-entropy as a stylometric signal. Lower cross-entropy = better prediction = stylistic match.
  - **Quick check question:** If Model A assigns cross-entropy loss 4.5 to Text X and Model B assigns 5.2, which model is "closer" to that text's style?

- **Concept: Next-token prediction (causal language modeling)**
  - **Why needed here:** GPT-2 is trained to predict the next token given all previous tokens. Understanding this objective clarifies what statistical patterns the model learns.
  - **Quick check question:** In the sequence "The cat sat on the ___", what distribution is the model learning to approximate?

- **Concept: Overfitting vs. generalization in stylometry**
  - **Why needed here:** A model could memorize training text (overfit) rather than learn generalizable stylistic patterns. The paper uses held-out books and multiple random seeds to mitigate this.
  - **Quick check question:** If a model achieves zero loss on training text but high loss on same-author held-out text, what does this indicate?

## Architecture Onboarding

- **Component map:** Preprocess texts -> Construct balanced training corpora -> Train each author's GPT-2 model from scratch -> Evaluate all models on all held-out texts -> Compare loss distributions via t-tests -> Assign authorship or compute distances

- **Critical path:**
  1. Preprocess texts (strip metadata, lowercase, remove non-ASCII)
  2. Construct balanced training corpora across authors
  3. Train each author's model to loss threshold ≤ 3.0
  4. Evaluate all models on all held-out texts
  5. Compare loss distributions via t-tests

- **Design tradeoffs:**
  - **Training from scratch vs. fine-tuning:** Paper trains from scratch; [section 4.1] notes concurrent work (Huang et al.) uses fine-tuning. Scratch training ensures no contamination from pre-training corpora but requires more data per author.
  - **Fixed loss threshold vs. fixed epochs:** Training to loss threshold enables fair comparison across authors with different corpus characteristics.
  - **Book-level vs. sentence-level:** Using complete books captures sustained stylistic patterns but limits the number of authors testable with available data.

- **Failure signatures:**
  - Models fail to separate authors (loss distributions overlap substantially) → check training data quality, token budget sufficiency, or author distinctiveness
  - Attribution accuracy degrades on cross-genre texts → models may have learned topic/genre rather than pure style
  - Twain-like slow convergence (77 epochs vs. 1-2 for others) → author may have higher intra-corpus variability

- **First 3 experiments:**
  1. **Reproduce the 8-author classification:** Train models on the same corpus split, verify 100% accuracy and rapid convergence. This validates your implementation.
  2. **Ablation sanity check:** Train content-word-only and POS-only models for one author pair, confirm that content words retain signal while POS degrades.
  3. **Out-of-domain test:** Train on an author's works in one genre, test on their works in a different genre (as done with Oz vs. non-Oz books). This probes whether the model captures style vs. topic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does predictive comparison maintain its effectiveness across larger author sets, different languages, or diverse time periods?
- Basis in paper: [explicit] Section 4.2 states, "Whether predictive comparison maintains its eﬀectiveness across larger author sets, diﬀerent languages, or more diverse time periods remains an open question."
- Why unresolved: The study was limited to a small, homogeneous dataset of eight English authors from overlapping historical periods.
- What evidence would resolve it: Replicating the experiments on corpora containing hundreds of authors, non-English texts, or works spanning multiple centuries.

### Open Question 2
- Question: Why does minimizing cross-entropy loss lead models to capture author-specific patterns rather than general linguistic patterns?
- Basis in paper: [explicit] Section 4.3 asks, "Why does minimizing cross-entropy during training lead to models that capture author-specific rather than general linguistic patterns?"
- Why unresolved: The paper establishes an empirical link but lacks a theoretical explanation for why this specific objective function isolates stylistic nuances.
- What evidence would resolve it: Theoretical modeling of the relationship between language model objectives and stylometric similarity, or layer-wise analysis of feature acquisition.

### Open Question 3
- Question: How robust is predictive comparison when training and test texts originate from different genres or topics?
- Basis in paper: [explicit] Section 4.2 notes that while initial tests on Oz vs. non-Oz books were encouraging, "systematic evaluation across diverse domains is needed."
- Why unresolved: The primary experiments used books from the same genre for each author, potentially confounding stylistic and thematic signals.
- What evidence would resolve it: Cross-domain experiments where models trained on one genre (e.g., fiction) are tested on another (e.g., correspondence) by the same author.

### Open Question 4
- Question: Can parameter-efficient fine-tuning methods enable this approach to scale to thousands of authors?
- Basis in paper: [explicit] Section 4.3 asks, "Could we use parameter-eﬃcient ﬁne-tuning methods... to adapt a single base model?" and "Could authors be represented as vectors in a learned embedding space?"
- Why unresolved: Training separate GPT-2 models from scratch becomes computationally prohibitive as the number of candidate authors increases.
- What evidence would resolve it: Developing and testing an architecture where a single base model is conditioned on author embeddings or adapters.

## Limitations

- The method's effectiveness on larger author sets, different languages, or more diverse time periods remains untested, limiting generalizability claims
- The paper cannot definitively separate stylistic patterns from genre-specific conventions, as all texts for each author come from the same genre
- The rapid convergence (1-2 epochs) raises questions about whether models are learning deep stylistic patterns or surface-level statistical regularities

## Confidence

**High confidence:** The core mechanism of using cross-entropy loss differences between author-specific models for attribution is sound and well-validated. The 100% accuracy on the eight-author classification task and the successful resolution of the Oz authorship problem provide strong empirical support for the method's effectiveness.

**Medium confidence:** The claim that the method captures authorial style rather than content or genre is partially supported but not definitively proven. The ablation studies show that content and function words both contribute to author distinction, but the rapid convergence and potential topic signals remain concerns.

**Low confidence:** The paper's assertion that the symmetric distance metric (d(i,j) = ½[Lⱼ(i) + Lᵢ(j)]) meaningfully captures "stylometric distance" between authors lacks validation beyond intuitive examples. The claim that grammatical structure alone is insufficient for authorship attribution, while supported by the POS-only ablation, may not generalize to authors with distinctive syntactic patterns.

## Next Checks

1. **Cross-genre validation:** Train models on an author's works in one genre (e.g., mystery) and test on their works in another genre (e.g., romance) to quantify the trade-off between stylistic and topical signals. This directly tests whether the method captures pure authorial style.

2. **Contemporary author test:** Apply the method to living authors with publicly available works across multiple time periods to assess whether the approach generalizes beyond historical authors and captures evolving writing styles over time.

3. **Competing model comparison:** Compare predictive comparison against established stylometric methods (e.g., Burrows' Delta, SVM-based approaches) on the same attribution problems to benchmark performance and determine whether the LLM-based approach offers advantages beyond computational complexity.