---
ver: rpa2
title: Assessing Color Vision Test in Large Vision-language Models
arxiv_id: '2507.11153'
source_url: https://arxiv.org/abs/2507.11153
tags:
- color
- vision
- test
- large
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates color vision capabilities in large vision-language
  models (LVLMs) using a newly constructed dataset with 5,450 color vision test images
  across five categories. The authors introduce two tasks: CVTE (with category clues)
  and CVTH (without clues), and assess models like JanusPro-7B and GPT4o.'
---

# Assessing Color Vision Test in Large Vision-language Models

## Quick Facts
- arXiv ID: 2507.11153
- Source URL: https://arxiv.org/abs/2507.11153
- Reference count: 28
- Primary result: Even top LVLMs achieve less than 21% accuracy on color vision tests

## Executive Summary
This study evaluates the color vision capabilities of large vision-language models (LVLMs) using a newly constructed dataset of 5,450 color vision test images across five categories. The authors introduce two novel tasks - CVTE (with category clues) and CVTH (without clues) - to systematically assess model performance on color vision deficiencies including protanopia, deuteranopia, and Ishihara plates. Results reveal significant limitations in current LVLMs, with top models like JanusPro-7B and GPT4o achieving only 20.86% and 18.39% accuracy respectively. The study demonstrates that LoRA fine-tuning can dramatically improve performance, with LLaVA1.5-7B's accuracy increasing from 15.72% to 94.43% on CVTE, suggesting current models have not internalized color vision principles despite their general capabilities.

## Method Summary
The authors constructed a comprehensive dataset of 5,450 color vision test images across five categories: protanopia, deuteranopia, Ishihara plates, color confusion, and color vision confusion. They evaluated multiple LVLMs including JanusPro-7B, LLaVA1.5-7B, InternVL-6.0, and GPT4o on two tasks: CVTE (with category clues) and CVTH (without clues). Error analysis was conducted to identify patterns in model failures, and LoRA fine-tuning was applied to LLaVA1.5-7B to assess whether performance could be improved through targeted training. The evaluation included both quantitative accuracy metrics and qualitative analysis of error types.

## Key Results
- JanusPro-7B achieved 20.86% accuracy on CVTE task, outperforming other models
- GPT4o reached 18.39% accuracy on CVTH task (no category clues)
- LoRA fine-tuning improved LLaVA1.5-7B accuracy from 15.72% to 94.43% on CVTE
- Error analysis revealed major issues with category understanding and recognition failures

## Why This Works (Mechanism)
The study demonstrates that current LVLMs lack robust color vision capabilities despite their general visual understanding abilities. The substantial improvement from LoRA fine-tuning indicates that models possess the underlying capacity to understand color vision concepts but have not learned these specific principles during pretraining. The error patterns suggest models struggle with both low-level color discrimination and high-level semantic understanding of color vision deficiencies. The introduction of structured color vision test tasks reveals a critical gap in current model capabilities that can be addressed through targeted training approaches.

## Foundational Learning
1. Color vision deficiency types - why needed: To understand different forms of color blindness (protanopia, deuteranopia, etc.) that models must identify; quick check: Can distinguish between red-green and blue-yellow deficiencies
2. Color perception principles - why needed: Understanding how humans perceive color and how deficiencies alter this perception; quick check: Can explain why certain color combinations are indistinguishable to colorblind individuals
3. Visual pattern recognition - why needed: Ability to identify subtle patterns in color vision test images; quick check: Can detect hidden numbers or patterns in Ishihara plates
4. Semantic understanding of visual concepts - why needed: To correctly interpret what different test results mean; quick check: Can associate specific visual patterns with their clinical significance
5. Cross-modal reasoning - why needed: To integrate visual information with textual explanations of color vision concepts; quick check: Can provide accurate textual descriptions of color vision test results
6. Domain-specific adaptation - why needed: To fine-tune general vision capabilities for specialized color vision tasks; quick check: Can transfer learned color vision skills to novel test types

## Architecture Onboarding

**Component Map:** Vision encoder -> Text encoder -> Cross-attention layers -> Classification head

**Critical Path:** Input image → Vision encoder → Feature extraction → Cross-attention with text → Classification decision

**Design Tradeoffs:** High parameter count enables better feature extraction but increases computational cost; larger context windows improve reasoning but require more memory

**Failure Signatures:** Confusion between similar color deficiencies; inability to detect subtle pattern differences; over-reliance on non-color features

**3 First Experiments:**
1. Test model on single-category color vision tests to isolate specific deficiency recognition capabilities
2. Evaluate fine-tuned model on out-of-distribution color vision test types to assess generalization
3. Compare performance across different vision encoder architectures to identify optimal feature extraction methods

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample sizes for some categories (particularly Ishihara with only 38 samples) may limit generalizability
- Focus on English-language tasks and Western color vision testing paradigms may not capture global applications
- Limited evaluation of model robustness to image quality variations and real-world conditions

## Confidence
- Color vision deficiency identification performance metrics: High confidence
- Error analysis categorization: Medium confidence
- LoRA fine-tuning effectiveness: High confidence

## Next Checks
1. Expand dataset to include additional color vision testing paradigms and larger sample sizes per category
2. Test model performance on color vision tasks in multiple languages and cultural contexts
3. Evaluate whether LoRA fine-tuned models maintain improved color vision capabilities on novel, unseen color vision test types