---
ver: rpa2
title: 'Evaluating Bias in LLMs for Job-Resume Matching: Gender, Race, and Education'
arxiv_id: '2503.19182'
source_url: https://arxiv.org/abs/2503.19182
tags:
- bias
- biases
- llms
- hiring
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates bias in large language models
  (LLMs) for job-resume matching across gender, race, and educational background.
  Using a benchmark dataset of 384,000 data points across 12 state-of-the-art LLMs,
  we assess how these models align resumes with job descriptions while identifying
  demographic influences.
---

# Evaluating Bias in LLMs for Job-Resume Matching: Gender, Race, and Education

## Quick Facts
- arXiv ID: 2503.19182
- Source URL: https://arxiv.org/abs/2503.19182
- Authors: Hayate Iso; Pouya Pezeshkpour; Nikita Bhutani; Estevam Hruschka
- Reference count: 23
- Key outcome: This study systematically evaluates bias in large language models (LLMs) for job-resume matching across gender, race, and educational background. Using a benchmark dataset of 384,000 data points across 12 state-of-the-art LLMs, we assess how these models align resumes with job descriptions while identifying demographic influences. While LLMs show strong matching performance (ROC AUC ~0.80-0.90), significant biases persist—particularly regarding educational background, where candidates from prestigious institutions receive preferential treatment. Explicit biases related to gender and race have been largely mitigated in newer models, but implicit biases remain. The findings highlight the need for ongoing evaluation and advanced bias mitigation strategies to ensure fair AI-driven hiring practices.

## Executive Summary
This study systematically evaluates bias in large language models (LLMs) for job-resume matching across gender, race, and educational background. Using a benchmark dataset of 384,000 data points across 12 state-of-the-art LLMs, we assess how these models align resumes with job descriptions while identifying demographic influences. While LLMs show strong matching performance (ROC AUC ~0.80-0.90), significant biases persist—particularly regarding educational background, where candidates from prestigious institutions receive preferential treatment. Explicit biases related to gender and race have been largely mitigated in newer models, but implicit biases remain. The findings highlight the need for ongoing evaluation and advanced bias mitigation strategies to ensure fair AI-driven hiring practices.

## Method Summary
The study evaluates 12 state-of-the-art LLMs on a benchmark dataset of 384,000 data points, generated by systematically manipulating demographic attributes (names for gender/race, institutions for educational background) across 400 job-resume pairs from 40 occupations. Each LLM receives a job description and resume, then outputs a 1-10 matching score. The study calculates ROC AUC for matching performance and uses L1-regularized linear regression to quantify bias, identifying which sensitive attributes (gender, race, education) significantly influence model decisions. The methodology enables causal attribution of bias through controlled attribute perturbation while maintaining other resume content constant.

## Key Results
- LLMs demonstrate strong matching performance with ROC AUC scores of 0.80-0.90 across most models
- Educational background bias persists significantly, with candidates from prestigious institutions receiving preferential treatment
- Explicit gender and race biases have been largely mitigated in newer model versions
- LLaMA-3.1 shows an unexpected 20% increase in educational bias, highlighting the need for ongoing fairness audits

## Why This Works (Mechanism)

### Mechanism 1: Causal Isolation via Controlled Attribute Perturbation
- Claim: Systematic manipulation of single demographic variables (names, institutions) while holding all other resume content constant enables causal attribution of bias.
- Mechanism: The study generates 80 variations per job-resume pair by swapping only the name (gender/race proxy) or educational institution. By comparing matching scores across these controlled variations, the isolated effect of each attribute on the model's decision can be measured.
- Core assumption: Name and institution lists are clean proxies for their intended demographics and do not introduce confounding semantic signals.

### Mechanism 2: Automatic Feature Selection for Bias Detection
- Claim: L1-regularized linear regression can automatically identify which sensitive attributes have a non-zero influence on LLM outputs.
- Mechanism: A regression model is fit to predict the matching score from sensitive attribute encodings. The L1 penalty drives coefficients of non-influential attributes to exactly zero. If an attribute's coefficient survives this regularization, it is deemed a statistically significant predictor of the score.

### Mechanism 3: Model Iteration Auditing as a Fairness Check
- Claim: Fairness and performance are not guaranteed to improve monotonically with model updates; explicit bias mitigation may succeed while implicit bias persists or regresses.
- Mechanism: By evaluating a temporal series of model versions (e.g., LLaMA-1 through 3.1), the study tracks the trajectory of both ROC AUC (performance) and bias percentage (fairness). This reveals that progress on one axis (explicit gender/race bias) does not ensure progress on another (implicit educational bias).

## Foundational Learning

- Concept: **Explicit vs. Implicit (Proxy) Bias**
  - Why needed here: The paper's central conclusion hinges on this distinction—direct signals for gender/race are being handled, but indirect signals (college prestige) are not.
  - Quick check question: If a model rejects a resume listing a "Women's College," is this an explicit or implicit bias if the goal is gender-neutral hiring? (Answer: Implicit. The college name acts as a proxy for gender).

- Concept: **ROC AUC for Ranking Evaluation**
  - Why needed here: This metric is used to validate the model's baseline competency before analyzing its fairness, ensuring it is at least capable of the task.
  - Quick check question: A model for resume matching has an ROC AUC of 0.55. Is this model ready for fairness auditing? (Answer: Likely not. Its discriminative power is barely above random chance, making it practically useless regardless of its fairness).

- Concept: **Proxy Variables in Algorithmic Auditing**
  - Why needed here: The study uses names and institutions as proxies for protected characteristics. Understanding proxy strength is critical for interpreting audit results.
  - Quick check question: Why is auditing for "educational background" bias considered an evaluation of implicit bias? (Answer: Because "university prestige" is a variable correlated with socioeconomic status and other factors, but is not itself a legally protected characteristic like race or gender).

## Architecture Onboarding

- Component map:
    - **Benchmark Generator:** Script to produce controlled resume variations from base pairs (names, institutions).
    - **LLM Inference Engine:** Interface to query 12 different models (GPT, LLaMA, Mistral, Yi) with a standardized scoring prompt.
    - **Statistical Auditor:** Module to compute ROC AUC and run L1-regularized regression on the resulting scores and attributes.

- Critical path:
    1.  **Generate Test Suite:** For each base resume, apply attribute permutations (names, colleges) → create 32,000 unique test prompts.
    2.  **Execute Evaluation:** Run all prompts through all 12 LLMs → collect 384,000 scalar scores (1-10).
    3.  **Analyze Results:** Compute performance (ROC AUC) and fit regression models to quantify bias coefficients.

- Design tradeoffs:
    - **Controlled vs. Naturalistic Data:** The study prioritizes control (synthetic variations) for causal clarity over ecological validity (real, messy resumes).
    - **Prompt-Based Evaluation:** Evaluates models "out-of-the-box" via prompts rather than fine-tuned, which reflects zero-shot deployment but not specialized systems.
    - **Linear Detection Model:** Prioritizes interpretability (clear coefficients) over the ability to detect complex, non-linear bias interactions.

- Failure signatures:
    - **Score Collapse:** A model returning the same score (e.g., "7") for almost all inputs.
    - **Regression Convergence Failure:** The statistical model failing to converge, often due to high collinearity between synthetic attributes.
    - **Metric Divergence:** A model showing high ROC AUC but massive coefficients for sensitive attributes, indicating a "high-performing but unfair" system.

- First 3 experiments:
    1.  **Single-Factor Ablation:** Run a small-scale test varying only one attribute (e.g., `en_US` male vs. female names) on one model to validate the regression pipeline.
    2.  **Baseline Performance Check:** Calculate ROC AUC for a chosen model to ensure it can perform the matching task above random chance before proceeding to bias analysis.
    3.  **Cross-Model Comparison:** Run the full audit on two models (e.g., GPT-3.5 and LLaMA-1) to replicate the paper's finding that explicit bias levels differ significantly by model family and version.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM hiring biases manifest in non-English languages and non-U.S. cultural contexts?
- Basis in paper: [explicit] "Biases may manifest differently in other languages and cultural contexts, and future work should explore these dimensions to develop globally applicable fairness strategies."
- Why unresolved: This study limited evaluation to English-language resumes and U.S. context only.
- What evidence would resolve it: Replicate the benchmark methodology with job-resume pairs in multiple languages (e.g., Spanish, Mandarin, Arabic) and cultural contexts, comparing bias patterns across regions.

### Open Question 2
- Question: Can advanced prompting techniques (in-context learning, chain-of-thought) effectively mitigate implicit educational biases without compromising matching performance?
- Basis in paper: [explicit] "Future work should explore the effectiveness of these techniques in reducing implicit biases without compromising matching performance."
- Why unresolved: The study focused on inherent biases in default model behavior, not mitigation interventions.
- What evidence would resolve it: Systematic experiments comparing bias metrics and ROC AUC scores across different prompting strategies for the same models and datasets.

### Open Question 3
- Question: What mechanisms cause model updates to introduce or amplify biases that previous versions had mitigated (e.g., LLaMA-3.1's educational bias increase)?
- Basis in paper: [inferred] The unexpected increase in educational bias in LLaMA-3.1 and performance drop in Mistral v0.3 suggest training dynamics are poorly understood.
- Why unresolved: The paper observes these anomalies but does not investigate their causes.
- What evidence would resolve it: Ablation studies examining training data changes, fine-tuning procedures, and architecture modifications between model versions, correlated with bias measurements.

## Limitations
- **Dataset Generality**: The Machamp dataset's proprietary nature and occupation sampling strategy may limit generalizability to other domains or real-world hiring scenarios.
- **Regression Assumptions**: The linear L1-regularized model assumes additive, separable effects of sensitive attributes, potentially missing complex, non-linear interactions.
- **Temporal Generalizability**: Model-specific findings (e.g., LLaMA-3.1's unexpected bias spike) may not hold as models are updated or as new architectures emerge.

## Confidence
- **High Confidence**: Strong matching performance (ROC AUC ~0.80-0.90) across most models; persistence of educational background bias in newer models.
- **Medium Confidence**: Mitigation of explicit gender/race bias in newer models; effectiveness of L1-regularized regression for bias detection in this context.
- **Low Confidence**: The specific claim about LLaMA-3.1's regression in fairness; generalizability of findings to all LLM-based hiring systems without fine-tuning.

## Next Checks
1. **Intersectional Bias Audit**: Re-run the regression analysis with interaction terms between gender and race to detect potential intersectional biases not captured by the main-effects model.
2. **Cross-Dataset Validation**: Apply the same methodology to a public resume dataset (e.g., from Kaggle or a university career services release) to test the robustness of findings beyond the Machamp corpus.
3. **Longitudinal Model Tracking**: Extend the study to include subsequent model versions (e.g., GPT-4, LLaMA-3.2) to determine if the observed trend of mitigated explicit bias but persistent implicit bias continues.