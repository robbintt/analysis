---
ver: rpa2
title: 'DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level
  Anomaly Detection'
arxiv_id: '2511.04086'
source_url: https://arxiv.org/abs/2511.04086
tags:
- graph
- normal
- graphs
- anomaly
- anomalous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeNoise tackles the problem of unsupervised graph-level anomaly
  detection (UGAD) when the training data is contaminated with anomalous graphs. Most
  existing UGAD methods assume a clean training set, but in practice, this is rarely
  true and even small contamination can degrade performance.
---

# DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection

## Quick Facts
- arXiv ID: 2511.04086
- Source URL: https://arxiv.org/abs/2511.04086
- Authors: Qingfeng Chen; Haojin Zeng; Jingyi Jie; Shichao Zhang; Debo Cheng
- Reference count: 30
- Key outcome: DeNoise achieves superior anomaly detection performance on eight real-world datasets under varying noise intensities by combining adversarial training, encoder anchor-alignment denoising, and contrastive learning.

## Executive Summary
DeNoise addresses the challenge of unsupervised graph-level anomaly detection (UGAD) when training data contains anomalous graphs. Traditional UGAD methods assume clean training sets, but real-world scenarios often involve contaminated data. DeNoise introduces an adversarial training strategy that combines a graph-level encoder with attribute and structure decoders, along with an encoder anchor-alignment denoising mechanism that integrates high-information node embeddings from normal graphs. A contrastive learning component further compacts normal graph embeddings while repelling anomalous ones in the latent space. Experiments demonstrate DeNoise's superior robustness and performance compared to state-of-the-art baselines across multiple datasets with varying contamination levels.

## Method Summary
DeNoise tackles unsupervised graph-level anomaly detection in contaminated training data through a multi-component architecture. The method employs an adversarial training framework combining a graph-level encoder with attribute and structure decoders to reconstruct graph features. The core innovation is an encoder anchor-alignment denoising mechanism that leverages high-information node embeddings from normal graphs to improve overall representation quality and suppress anomaly interference. Additionally, contrastive learning is integrated to compact normal graph embeddings while pushing anomalous ones apart in the latent space. This combination enables DeNoise to maintain robust performance even when the training set contains a significant proportion of anomalous graphs.

## Key Results
- DeNoise consistently outperforms state-of-the-art UGAD baselines on eight real-world datasets
- Strong robustness demonstrated across varying contamination levels in training data
- Effective suppression of anomaly interference through encoder anchor-alignment denoising mechanism
- Superior performance maintained even with significant proportion of anomalous graphs in training set

## Why This Works (Mechanism)
DeNoise's effectiveness stems from its ability to learn robust graph representations that can distinguish between normal and anomalous graphs even when training data is contaminated. The encoder anchor-alignment denoising mechanism transfers high-quality node embeddings from normal graphs to contaminated ones, effectively "cleaning" the representations. The adversarial training with dual decoders ensures that the encoder learns to produce representations that can be faithfully reconstructed, while the contrastive learning component explicitly enforces separation between normal and anomalous graph embeddings in the latent space. This multi-pronged approach addresses both the representation quality and the separability challenges inherent in contaminated UGAD scenarios.

## Foundational Learning
- **Graph-level anomaly detection**: Identifying anomalous graphs in a dataset without labeled examples; needed because many real-world scenarios lack anomaly labels; quick check: Can the method identify outliers without supervision?
- **Adversarial training**: Training framework where components compete to improve overall performance; needed to create robust representations that resist contamination; quick check: Does the adversarial setup improve over standard autoencoding?
- **Encoder-decoder architecture**: Neural network setup where encoder maps input to latent space and decoder reconstructs input; needed for learning compressed graph representations; quick check: Are the reconstructed graphs faithful to originals?
- **Contrastive learning**: Learning method that pulls similar examples together and pushes dissimilar ones apart in embedding space; needed to separate normal and anomalous graph representations; quick check: Do normal and anomalous graphs form distinct clusters?
- **Graph neural networks**: Neural networks designed to process graph-structured data; needed because graphs have unique structural properties; quick check: Does the model handle varying graph sizes and structures?

## Architecture Onboarding

**Component Map**: Input Graphs -> Graph Encoder -> Latent Representations -> Attribute Decoder + Structure Decoder (Adversarial Training) + Contrastive Loss -> Refined Representations -> Anomaly Scores

**Critical Path**: Input graphs are processed by the graph encoder to produce latent representations, which are then used by both decoders in adversarial training and by the contrastive learning component. The denoised representations from the anchor-alignment mechanism are fed back to improve the encoder's output.

**Design Tradeoffs**: The dual-decoder architecture increases parameter count and computational overhead but enables more robust reconstruction of both attributes and structure. The contrastive learning component adds complexity but explicitly enforces separation between normal and anomalous graphs. The anchor-alignment mechanism requires additional computation to identify and transfer high-quality node embeddings but significantly improves representation quality in contaminated data.

**Failure Signatures**: Poor separation between normal and anomalous graphs in latent space despite adversarial training; failure to reconstruct key structural or attribute features in the decoders; degradation in performance as contamination level increases beyond tested thresholds; instability in training convergence when contamination is extreme.

**First Experiments**: 1) Evaluate reconstruction quality of attribute and structure decoders on clean graphs to establish baseline performance. 2) Test anchor-alignment mechanism's effectiveness by comparing representation quality with and without denoising on contaminated data. 3) Assess contrastive learning's impact by measuring separation between normal and anomalous graphs in latent space with and without contrastive loss.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to tested contamination levels; extreme contamination scenarios (>30%) not explored
- Computational overhead of dual-decoder architecture and contrastive learning not quantified
- Theoretical guarantees for convergence and stability under varying contamination levels absent
- Assumption about transferability of high-information node embeddings lacks theoretical grounding

## Confidence

**High**: Robustness claims under moderate contamination (up to 20-30%), comparative performance against SOTA baselines on tested datasets

**Medium**: Generalization to unseen graph types and distributions, effectiveness of denoising mechanism across all contamination levels

**Low**: Theoretical guarantees of convergence, computational complexity analysis, behavior under extreme contamination scenarios

## Next Checks
1. Evaluate DeNoise's performance on synthetic graph datasets with controlled structural and attribute anomalies to isolate the impact of different contamination types
2. Conduct ablation studies to quantify the individual contributions of the denoising mechanism, adversarial training, and contrastive learning components
3. Test the method's robustness under extreme contamination scenarios (>30% anomalous graphs) and analyze training stability metrics across training epochs