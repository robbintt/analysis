---
ver: rpa2
title: 'Beyond Output Critique: Self-Correction via Task Distillation'
arxiv_id: '2602.00871'
source_url: https://arxiv.org/abs/2602.00871
tags:
- task
- answer
- self-correction
- aime
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SELF-THOUGHT, a framework for self-correction
  in large language models that improves upon existing output critique methods by
  first distilling the task into a structured template capturing key variables, constraints,
  and problem structure before solution refinement. This task abstraction guides subsequent
  solution instantiation, reducing error propagation and enabling more robust corrections.
---

# Beyond Output Critique: Self-Correction via Task Distillation

## Quick Facts
- **arXiv ID**: 2602.00871
- **Source URL**: https://arxiv.org/abs/2602.00871
- **Reference count**: 40
- **Primary result**: SELF-THOUGHT framework achieves 48-200% accuracy gains over baseline self-correction methods by distilling tasks into structured abstractions before solution refinement

## Executive Summary
This paper introduces SELF-THOUGHT, a novel self-correction framework for large language models that moves beyond simple output critique by first distilling the task into a structured template capturing key variables, constraints, and problem structure. The approach generates task abstractions that guide subsequent solution instantiation, reducing error propagation and enabling more robust corrections. Crucially, the framework allows task abstractions generated by larger models to be transferred to smaller models (via DISTIL-THOUGHT), enabling effective self-correction without heavy fine-tuning or external verifiers.

The framework was evaluated across diverse reasoning tasks including Game of 24, Word Sorting, CheckmateInOne, and AIME problems, consistently outperforming baseline self-correction methods. Small models like QWEN-2.5-7B and LLAMA-3.3-70B showed particularly large improvements when using distilled abstractions, with gains exceeding 150% in some cases. This demonstrates that structured task representations effectively transfer reasoning capabilities across model scales, making sophisticated self-correction accessible to smaller models.

## Method Summary
SELF-THOUGHT operates through a two-stage process: task distillation followed by solution refinement. In the first stage, the model analyzes the problem to extract key variables, constraints, and structural elements, creating a structured template that captures the essence of the task. This abstraction serves as a scaffold that guides the second stage, where the model generates solutions based on the distilled task representation rather than directly critiquing its initial output. The framework includes DISTIL-THOUGHT, which enables transfer of task abstractions from larger to smaller models, allowing smaller models to benefit from the superior abstraction capabilities of larger ones without requiring extensive fine-tuning.

## Key Results
- SELF-THOUGHT achieved accuracy gains of 48-200% over baseline self-correction methods across tested tasks
- Small models (QWEN-2.5-7B, LLAMA-3.3-70B) showed gains exceeding 150% when using distilled abstractions
- The approach demonstrated consistent improvements across diverse reasoning tasks including Game of 24, Word Sorting, CheckmateInOne, and AIME problems
- DISTIL-THOUGHT enabled effective transfer of reasoning capabilities from larger to smaller models without heavy fine-tuning

## Why This Works (Mechanism)
The framework succeeds by addressing a fundamental limitation of traditional self-correction: error propagation during iterative refinement. By first abstracting the task structure into key variables and constraints, the model creates a stable reference point that guides solution generation rather than simply critiquing flawed outputs. This separation between task understanding and solution instantiation prevents the compounding of errors that typically occurs in iterative self-correction. The distillation process effectively creates a meta-level understanding of what needs to be solved before attempting to solve it, reducing the cognitive load during solution generation and improving overall accuracy.

## Foundational Learning
**Task Distillation**: Extracting key variables, constraints, and problem structure into a structured template - needed to create stable reference points for solution refinement; quick check: can the distilled template be used to guide multiple solution attempts consistently?

**Structured Abstraction Transfer**: Moving task representations from larger to smaller models - needed to democratize access to sophisticated reasoning capabilities; quick check: does the abstraction quality degrade when transferred across large model size gaps?

**Error Propagation Mitigation**: Preventing compounding errors during iterative refinement - needed to maintain solution quality through correction cycles; quick check: does solution quality improve or degrade with each refinement iteration?

**Meta-level Problem Understanding**: Separating task comprehension from solution generation - needed to reduce cognitive load during problem solving; quick check: can the model generate valid solutions directly from the distilled template without access to the original problem?

## Architecture Onboarding
**Component Map**: Task Input -> Task Distillation Module -> Structured Template -> Solution Generation Module -> Refined Output

**Critical Path**: The task distillation module represents the critical path, as the quality of the structured template directly determines the effectiveness of subsequent solution refinement. Poor abstractions lead to misguided solutions regardless of the refinement strategy employed.

**Design Tradeoffs**: The framework trades computational overhead in the distillation phase for improved solution quality and reduced error propagation. This upfront investment in task understanding pays dividends in more robust and accurate solutions, particularly for complex reasoning tasks.

**Failure Signatures**: Common failure modes include over-simplified abstractions that miss critical constraints, overly complex templates that overwhelm the solution generation module, and failed transfers when the abstraction quality degrades significantly across model size gaps.

**First Experiments**:
1. Compare solution quality using SELF-THOUGHT versus direct refinement on increasingly complex reasoning tasks
2. Test abstraction transfer across different model size pairs to identify the minimum viable abstraction quality
3. Measure error propagation rates across multiple refinement cycles to quantify the stability benefit

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's effectiveness on open-ended or subjective domains remains untested
- The computational overhead of generating task abstractions scales with problem complexity
- The quality of abstractions may degrade when transferred across substantial model size gaps
- Long-term stability of corrections and potential for accumulating systematic biases through iterative refinement is unexplored

## Confidence
**High Confidence**: The core mechanism of task distillation followed by solution refinement is well-documented and consistently improves performance across all tested tasks. The comparative advantage over baseline self-correction methods is statistically significant and reproducible.

**Medium Confidence**: The scalability claims regarding transfer from larger to smaller models are supported by the data, but the sample size of model pairs is limited. The generalizability to tasks beyond the mathematical and logical domains tested remains uncertain.

**Low Confidence**: The paper does not adequately address potential limitations in handling tasks requiring extensive world knowledge or subjective judgment. The long-term stability of corrections and potential for accumulating systematic biases through iterative refinement is not explored.

## Next Checks
1. Test the framework on open-ended reasoning tasks requiring synthesis of information from multiple sources, such as multi-hop reading comprehension or scientific hypothesis generation, to assess generalization beyond structured mathematical problems.

2. Conduct ablation studies comparing SELF-THOUGHT performance when using task abstractions generated by progressively smaller models, to determine the minimum model size capable of producing effective abstractions.

3. Implement a longitudinal study tracking the stability of corrections over multiple refinement cycles, measuring whether repeated application leads to convergence, divergence, or oscillation in solution quality.