---
ver: rpa2
title: 'Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework'
arxiv_id: '2511.21686'
source_url: https://arxiv.org/abs/2511.21686
tags:
- data
- agent
- matrix
- agents
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework

## Quick Facts
- arXiv ID: 2511.21686
- Source URL: https://arxiv.org/abs/2511.21686
- Authors: Dong Wang; Yang Li; Ansong Ni; Ching-Feng Yeh; Youssef Emad; Xinjie Lei; Liam Robbins; Karthik Padthe; Hu Xu; Xian Li; Asli Celikyilmaz; Ramya Raghavendra; Lifei Huang; Carole-Jean Wu; Shang-Wen Li
- Reference count: 7
- None

## Executive Summary
Matrix introduces a peer-to-peer (P2P) architecture for scalable multi-agent synthetic data generation, eliminating centralized orchestration bottlenecks through message-passing between stateless agents. The framework demonstrates 2.1× higher token throughput than batch-based baselines by implementing row-level scheduling and offloading large conversation content to distributed object stores. Three use cases validate the approach: collaborative reasoning dialogues, web-based question extraction, and customer service tool-use trajectories.

## Method Summary
Matrix implements a P2P architecture where serialized orchestrator objects containing conversation history and control state flow between Ray actors. Each agent runs an async event loop, processes the orchestrator, and forwards it to the next agent without central coordination. The framework uses row-level scheduling for independent task execution, offloading large conversation content to Ray's distributed object store to reduce network bandwidth. LLM inference is handled by distributed vLLM/SGLang services, with containerized tools managed via Apptainer. The system achieves near-linear scaling with GPU count for the collaborative reasoning workload.

## Key Results
- 2.1× higher token throughput than Ray Data batch baseline (5,853 vs 2,778 tokens/sec)
- Peak network utilization reduced by ~20% through message offloading (1 GB/s to 760 MB/s)
- Near-linear scaling demonstrated for collaborative reasoning workload with GPU count

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decentralized peer-to-peer orchestration eliminates centralized bottlenecks in multi-agent workflows.
- **Mechanism**: Task state (orchestration logic, intermediate results, conversation history) is serialized into messages passed among stateless agents. Each agent processes, updates state, and forwards to the next agent without central coordination.
- **Core assumption**: Agents can be stateless because all context travels with the orchestrator message.
- **Evidence anchors**:
  - [abstract] "This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents."
  - [Section 4.1, Figure 2] Compares centralized vs P2P orchestration; driver only publishes orchestrator to start task.
  - [corpus] Weak/missing — no directly comparable P2P synthetic data frameworks; related work focuses on centralized multi-agent systems.
- **Break condition**: If orchestrator message size grows unboundedly (e.g., very long multi-turn dialogues), serialization/deserialization overhead may dominate.

### Mechanism 2
- **Claim**: Row-level scheduling achieves higher GPU utilization than batch-level scheduling for heterogeneous multi-agent workloads.
- **Mechanism**: Each task is scheduled independently upon completion of prior tasks, eliminating idle time caused by slow tasks blocking entire batches ("bubble effects").
- **Core assumption**: Task durations are variable; control flow is data-dependent and cannot be bulk-processed.
- **Evidence anchors**:
  - [Section 4.4] "Row-level scheduling allows each completed row to immediately trigger the next task without waiting for others."
  - [Table 4] P2P-agent achieves 2.1× higher token throughput than Ray Data batch baseline (5,853 vs 2,778 tokens/sec).
  - [corpus] Not directly addressed; scheduling granularity not analyzed in neighbor papers.
- **Break condition**: If tasks are homogeneous with predictable duration, batch-level overhead becomes negligible and complexity may not justify gains.

### Mechanism 3
- **Claim**: Offloading large conversation content to Ray's distributed object store reduces network bandwidth without losing state locality.
- **Mechanism**: Orchestrator retains only object IDs in history; content retrieved on demand. Immutable objects deleted on completion.
- **Core assumption**: Most conversation turns are small; only a subset exceeds the offload threshold.
- **Evidence anchors**:
  - [Section 4.6] "This design keeps the orchestrator compact, reduces redundant transfers, and minimizes network load."
  - [Figure 9] Peak network utilization drops from ~1 GB/s to ~760 MB/s (~20% reduction).
  - [corpus] Not directly addressed in neighbor papers.
- **Break condition**: If retrieval latency from object store exceeds network transfer cost, offloading becomes counterproductive.

## Foundational Learning

- **Ray Actors and Distributed Object Store**
  - Why needed here: Agents are implemented as Ray Actors; orchestrator messages use Ray's object store for offloading.
  - Quick check question: Can you explain how Ray Actors differ from regular Python objects in terms of placement and scaling?

- **Asyncio Event Loops**
  - Why needed here: Each agent runs an async event loop (Algorithm 1, lines 3–9) to process orchestrators concurrently.
  - Quick check question: What happens if a blocking call is placed inside an async event loop without `await`?

- **LLM Inference Batching (Continuous Batching)**
  - Why needed here: Throughput gains partly rely on inference services (vLLM, SGLang) that batch requests dynamically.
  - Quick check question: Why does continuous batching outperform static batching for variable-length LLM requests?

## Architecture Onboarding

- **Component map**:
  Driver -> First Agent -> Second Agent -> ... -> Sink Agent

- **Critical path**:
  1. Load seed data → create Orchestrator per item.
  2. Dispatch orchestrator to first agent (randomly sampled from role group).
  3. Agent dequeues, processes, updates orchestrator, forwards to next agent.
  4. Repeat until `is_done` → route to `_sink` for persistence.

- **Design tradeoffs**:
  - **P2P vs Centralized**: Higher scalability vs more complex debugging (message flow is distributed).
  - **Row-level vs Batch**: Lower latency per task vs higher scheduling overhead per item.
  - **Message offloading**: Reduced bandwidth vs added object store latency.

- **Failure signatures**:
  - Queue length spikes: Agent processing slower than dispatch rate; scale agent instances.
  - Network saturation: Large orchestrators without offloading; enable/tune offload threshold.
  - GPU underutilization: Concurrency too low; increase `max_concurrency` or data parallelism.

- **First 3 experiments**:
  1. **Single-agent throughput test**: Run minimal workflow with one agent role; measure tokens/sec at varying concurrency (100, 500, 1000).
  2. **Multi-agent pipeline test**: Add second agent; verify orchestrator passes correctly; compare throughput to single-agent baseline.
  3. **Scale test**: Run full workload (e.g., Collaborative Reasoner) on 4 vs 8 GPU nodes; verify near-linear scaling and identify bottlenecks via Grafana queue metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Matrix's P2P architecture be extended to support multi-modal data generation workflows, where agents must process and exchange images, audio, or video alongside text?
- Basis in paper: [explicit] The conclusion states: "Future extensions will explore multi-modal data generation and on-policy continuous data synthesis."
- Why unresolved: Current message offloading (Section 4.6) handles large text conversations via Ray's object store, but multi-modal content introduces different bandwidth, serialization, and latency challenges not addressed in this work.
- What evidence would resolve it: An implementation and evaluation of Matrix on multi-modal synthesis tasks, measuring throughput, network overhead, and output quality compared to text-only baselines.

### Open Question 2
- Question: What is the optimal configuration search strategy for the three types of parallelism (data, task, agent) across diverse workloads?
- Basis in paper: [inferred] Section 5.2.1 notes "the design space of the three kinds of parallelism can be huge" and current values were determined empirically through experiments, suggesting no generalizable principle.
- Why unresolved: The paper demonstrates that optimal settings depend on workload characteristics (e.g., filter rates, LLM sizes), but provides no automated method to determine these configurations for new use cases.
- What evidence would resolve it: A systematic study characterizing how workload properties (filter rates, control flow complexity, agent compute ratios) map to optimal parallelism settings, or an auto-tuning mechanism.

### Open Question 3
- Question: Can fault tolerance be extended to recover orchestrator queue state without sacrificing the simplicity of the current design?
- Basis in paper: [explicit] Section 4.5 states: "Instead of recovering the orchestrator queue, which can be complex, Matrix labels each Ray node by resource type... and schedules agents only on permanent nodes."
- Why unresolved: The current approach sacrifices opportunistic compute capacity for agent actors to avoid implementation complexity. A more robust solution could enable agents to leverage spot instances while guaranteeing no task loss.
- What evidence would resolve it: A checkpointing or replication mechanism for in-flight orchestrators with measured overhead, demonstrating recovery from node failures without data loss.

## Limitations

- Quality metrics lack baseline comparisons and human evaluation, making it difficult to assess training value beyond token volume.
- Scalability claims demonstrated only for collaborative reasoning workload, with no analysis of how different task types affect scaling efficiency.
- The 2.1× throughput improvement lacks statistical significance testing and ablation studies showing individual contribution of architectural components.

## Confidence

**High Confidence**: The architectural description of P2P orchestration, row-level scheduling, and message offloading is clearly specified with implementation details in the codebase. The throughput measurements showing improved token generation rates are verifiable through the provided implementation.

**Medium Confidence**: The claim that P2P eliminates centralized bottlenecks is theoretically sound but lacks empirical validation showing performance degradation in centralized alternatives under comparable conditions. The network bandwidth reduction from message offloading is demonstrated but without comprehensive analysis of trade-offs with object store retrieval latency.

**Low Confidence**: Quality assessment claims lack rigorous validation. Without baseline comparisons, human evaluation, or downstream model training results, it's unclear whether the generated synthetic data actually provides training value beyond raw token volume.

## Next Checks

1. **Message Growth Analysis**: Instrument the system to log orchestrator message sizes across different task depths (1-agent, 2-agent, 3+ agent workflows) and plot message size vs. throughput to identify the break-even point where P2P overhead exceeds benefits.

2. **Ablation Study on Architectural Components**: Run controlled experiments comparing P2P vs centralized orchestration, batch vs row-level scheduling, and with/without message offloading to quantify the individual contribution of each optimization to the reported 2.1× throughput improvement.

3. **Downstream Model Evaluation**: Use the generated synthetic data to fine-tune a base LLM on one of the target tasks (e.g., collaborative reasoning) and measure actual performance improvements compared to baselines using human-annotated or traditional synthetic data, validating whether the throughput gains translate to quality improvements.