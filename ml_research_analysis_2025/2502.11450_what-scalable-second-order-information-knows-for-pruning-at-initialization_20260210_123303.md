---
ver: rpa2
title: What Scalable Second-Order Information Knows for Pruning at Initialization
arxiv_id: '2502.11450'
source_url: https://arxiv.org/abs/2502.11450
tags:
- pruning
- sparsity
- methods
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper revisits pruning at initialization (PaI) by evaluating
  scalable, unbiased second-order approximations like Empirical Fisher and Hutchinson
  diagonals. It demonstrates that these methods capture sufficient curvature information
  to improve the identification of critical parameters compared to first-order baselines
  while maintaining linear complexity.
---

# What Scalable Second-Order Information Knows for Pruning at Initialization

## Quick Facts
- arXiv ID: 2502.11450
- Source URL: https://arxiv.org/abs/2502.11450
- Reference count: 40
- The paper demonstrates that scalable second-order approximations like Empirical Fisher and Hutchinson diagonals can effectively identify critical parameters for pruning at initialization while maintaining linear complexity.

## Executive Summary
This paper investigates pruning at initialization (PaI) using scalable, unbiased second-order approximations. The authors evaluate methods like Empirical Fisher and Hutchinson diagonals, showing they capture sufficient curvature information to identify critical parameters more effectively than first-order baselines. A warmup phase for batch normalization statistics is introduced, improving data-dependent criteria performance and mitigating layer collapse. The study demonstrates consistent performance improvements across various architectures and datasets.

## Method Summary
The paper evaluates pruning at initialization using scalable second-order information, specifically focusing on Empirical Fisher Information and Hutchinson diagonals. These methods approximate curvature information with linear complexity while maintaining unbiased estimates. The authors propose a warmup phase to update batch normalization statistics before pruning, which improves the performance of data-dependent criteria and prevents layer collapse. The approach is tested across multiple architectures including VGG, ResNet, and ViT on various datasets like CIFAR-10/100, TinyImageNet, and ImageNet.

## Key Results
- Hutchinson diagonals consistently outperformed or matched existing PaI algorithms across various models and datasets
- The warmup phase for batch normalization statistics improved performance of data-dependent criteria and mitigated layer collapse
- Scalable second-order approximations strike an effective balance between computational efficiency and accuracy for pruning at initialization

## Why This Works (Mechanism)
The paper demonstrates that second-order information captures curvature in the loss landscape, which is crucial for identifying parameters that have the most impact on model performance. While first-order methods rely solely on gradient information, second-order approximations like Empirical Fisher and Hutchinson diagonals incorporate curvature information that helps distinguish between parameters that appear similar in magnitude but have different effects on the loss surface. The warmup phase addresses the issue of batch normalization statistics being computed from a randomly initialized network, which can be misleading for pruning decisions.

## Foundational Learning
1. **Empirical Fisher Information Matrix** - An approximation of the Fisher Information Matrix that uses the empirical data distribution instead of the true data distribution. Needed to estimate the curvature of the loss landscape without requiring multiple forward passes. Quick check: Verify that the Empirical Fisher approximates the expected outer product of gradients.

2. **Hutchinson's Estimator** - A stochastic method for estimating diagonal entries of large matrices using random vectors. Needed to make second-order information scalable to modern deep networks. Quick check: Confirm that the estimator provides unbiased estimates with variance decreasing as 1/trace(H).

3. **Batch Normalization Statistics** - The running mean and variance computed during training that normalize activations. Needed because initial statistics from random initialization can be misleading for pruning decisions. Quick check: Ensure statistics are updated before pruning to reflect more realistic activation distributions.

## Architecture Onboarding

**Component Map:**
Data -> Forward Pass -> BatchNorm Update (Warmup) -> Second-Order Computation -> Pruning Mask -> Pruned Model

**Critical Path:**
The critical path is: Data → Forward Pass → BatchNorm Update → Second-Order Computation → Pruning Mask. The warmup phase for batch normalization is particularly critical as it significantly impacts the quality of data-dependent criteria.

**Design Tradeoffs:**
- Computational efficiency vs. accuracy of second-order approximation
- Number of Hutchinson samples vs. variance of estimates
- Duration of warmup phase vs. improvement in pruning quality
- Memory usage vs. ability to store second-order information

**Failure Signatures:**
- Layer collapse when batch normalization statistics are not properly updated
- Inconsistent pruning results across different random seeds due to Hutchinson's stochastic nature
- Poor generalization when pruning too aggressively based on second-order information alone

**First Experiments:**
1. Compare Empirical Fisher vs. Hutchinson diagonals on a simple CNN architecture with varying numbers of samples
2. Test the impact of warmup phase duration on pruning quality for a ResNet model
3. Evaluate layer collapse in ViT architecture with and without batch normalization statistic updates

## Open Questions the Paper Calls Out
None

## Limitations
- Results may vary across different random seeds due to the stochastic nature of Hutchinson diagonals
- The warmup phase introduces additional computational overhead that may impact practical deployment
- Evaluation primarily focuses on vision tasks, leaving questions about generalizability to other domains

## Confidence
- High Confidence: Core finding that second-order approximations outperform first-order methods for PaI
- Medium Confidence: Claim that Hutchinson diagonals consistently match or exceed existing PaI algorithms
- Medium Confidence: Assertion that warmup phase universally improves data-dependent criteria

## Next Checks
1. Conduct ablation studies varying the number of Hutchinson samples to determine the minimal sample size needed for reliable performance
2. Test the proposed methods on non-vision tasks (e.g., transformer-based language models or graph neural networks)
3. Perform ablation on the warmup phase duration and timing to optimize its integration into the pruning pipeline