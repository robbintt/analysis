---
ver: rpa2
title: Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding
  Problems?
arxiv_id: '2509.00629'
source_url: https://arxiv.org/abs/2509.00629
tags:
- problem
- code
- retrieval
- arxiv
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the ICPC benchmark, a new collection of 254\
  \ challenging competitive programming problems from past ICPC contests, each with\
  \ official analysis, reference solutions, and comprehensive test cases. The authors\
  \ develop and evaluate various inference-time techniques for language models on\
  \ this benchmark, finding that multi-turn self-judgment with retrieval over episodic\
  \ information significantly improves performance\u2014raising o1's zero-shot pass@1\
  \ rate from 19.1% to 42.2%."
---

# Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?

## Quick Facts
- arXiv ID: 2509.00629
- Source URL: https://arxiv.org/abs/2509.00629
- Reference count: 25
- Primary result: Multi-turn self-judgment with retrieval raises o1's zero-shot pass@1 rate from 19.1% to 42.2% on ICPC benchmark

## Executive Summary
This paper introduces the ICPC benchmark, a collection of 254 challenging competitive programming problems from past ICPC contests, and evaluates inference-time techniques for language models on this benchmark. The authors find that combining multi-turn self-reflection, episodic retrieval of related solutions, and targeted human feedback substantially improves problem-solving capabilities. Their results show o1 achieving 42.2% pass@1 with combined techniques, and human-in-the-loop studies revealing that targeted feedback enables o1 to solve 17 out of 18 previously unsolvable problems. The work demonstrates significant advancements in algorithmic reasoning through self-refinement mechanisms.

## Method Summary
The method involves a multi-turn self-judge single-agent framework where models generate solutions to ICPC problems, execute against unit tests, and iteratively refine based on execution feedback. The core technique uses episodic retrieval to fetch similar problems with full solutions (description, analysis, and code) as context for the model. Models then engage in self-reflection by analyzing test failures and revising their code, with optimal performance achieved at 2 iterations. The framework also includes a human-in-the-loop component where tutors provide high-level feedback without revealing specific algorithms or code, enabling models to make meaningful progress on previously unsolvable problems.

## Key Results
- o1 achieves 42.2% pass@1 on ICPC benchmark with multi-turn self-reflection and episodic retrieval, up from 19.1% zero-shot
- Episodic retrieval alone raises o1 to 33.1% pass@1, while self-reflection alone reaches 25.6%
- Human feedback enables o1 to solve 17/18 previously unsolvable problems, with no improvement for GPT-4/GPT-4o
- Retrieval of similar problems with full solutions outperforms semantic retrieval by a large margin (33.1% vs 2.3%)

## Why This Works (Mechanism)

### Mechanism 1: Self-Judgment with Iterative Reflection
Models improve solve rates by evaluating their own solutions against unit tests and reflecting on failures in subsequent attempts. The LM generates a solution → executes against unit tests → receives binary pass/fail + error type → generates natural language reflection on failure → produces revised code. This loop runs until pass or iteration limit. Weaker models (GPT-4) show limited gains from reflection alone, suggesting a minimum model capability threshold.

### Mechanism 2: Episodic Retrieval Over Related Solutions
Providing models with similar problems and their full solutions (description + analysis + code) enables in-context learning of solution patterns. Query construction uses problem description + initial solution attempt → retrieves top-k most similar problems from corpus → model conditions on retrieved solutions when generating final code. The ablation shows removing solution text from retrieval drops performance to 2.3% of original, indicating that algorithmic patterns (not just concepts) must be retrieved.

### Mechanism 3: High-Level Human Feedback Integration
Models near a correct solution can reach it with minimal, non-specific guidance pointing out error categories rather than fixes. Human provides tutoring-style feedback (no exact code, no algorithm reveals, only general direction) → model revises reasoning → iterates up to 3 code generations. Only o1 showed consistent improvement; GPT-4/GPT-4o remained at 0% even with feedback, revealing latent differences in feedback receptivity.

## Foundational Learning

- **Chain-of-Thought Prompting**: Why needed: All experiments assume models can reason step-by-step before generating code; zero-shot CoT baseline uses explicit reasoning scaffolding. Quick check: Can you explain why prompting a model to "think step by step" before answering improves performance on reasoning tasks?

- **Execution-Based Verification**: Why needed: The self-judge mechanism requires understanding that code correctness is determined by running against test cases, not text similarity. Quick check: What information does a model receive when its generated code fails a unit test, and how might it use that information?

- **Information Retrieval (TF-IDF or Semantic Embedding)**: Why needed: Episodic retrieval depends on finding relevant prior problems; understanding retrieval scoring helps debug poor matches. Quick check: If a retrieval system returns irrelevant problems, what might this indicate about the query representation or corpus?

## Architecture Onboarding

Problem Input → [Retrieval Query Builder] → [Episodic Knowledge Base] → Retrieved Problems → [LM Generator] → Candidate Solution → [Execution Harness] → Pass/Fail + Error Type → [Reflection Prompt] → [LM Generator] → Revised Solution (loop max 2x)

Critical path: Retrieval query quality → relevance of retrieved episodes → model's ability to pattern-match from examples → reflection quality on failures. Weakness at any stage cascades.

Design tradeoffs:
- More retrieved problems (p>2) reduced performance for o1, likely due to context length dilution
- Semantic + episodic retrieval combined underperformed episodic alone (33.2% vs 42.2%), suggesting context overload
- Iteration limit i=2 balanced improvement vs cost; i=3 showed no additional gain

Failure signatures:
- High TLE rates (23-25% for o1/gpt-4o) indicate correct algorithm but wrong complexity—reflection may not catch asymptotic issues
- Wrong Answer (27-58%) suggests logic errors that reflection fails to diagnose
- Syntax/Other errors <1% for o1 indicates generation quality is not the bottleneck

First 3 experiments:
1. Baseline replication: Run zero-shot CoT on ICPC benchmark subset (e.g., 20 problems) to confirm ~19% pass@1 for your model; establish local execution harness.
2. Ablation sweep: Test self_reflection alone, episodic_retrieval alone, and combined—expect ~6-7% absolute improvement from reflection, ~10% from retrieval, ~23% from both combined on o1-class model.
3. Retrieval query optimization: Compare query formulations (description only vs description + proposed solution vs description + solution + code) on retrieval relevance and downstream solve rate; paper found description + solution + code optimal (Table 8).

## Open Questions the Paper Calls Out

### Open Question 1
What methods can mitigate the high computational cost (TLEs) of advanced models in competitive programming without compromising their accuracy? The paper observes that stronger models (o1, GPT-4o) trade raw speed for deeper reasoning, with TLE rates of 23-25%, but does not propose solutions. A technique that reduces TLE rates while maintaining or improving pass@1 scores on the ICPC benchmark would resolve this.

### Open Question 2
Can automated systems generate human-level corrective feedback that unlocks problem-solving capabilities in LMs comparable to the human-in-the-loop improvements observed with o1? The study shows human feedback enables o1 to solve 17/18 previously unsolvable problems, but generating such feedback automatically remains unaddressed. An automated feedback mechanism achieving comparable solve rates to human tutors on the unsolved problem subset would resolve this.

### Open Question 3
What evaluation metrics beyond pass@1 and execution success can better capture latent problem-solving progress in LMs? The human-in-the-loop study revealed models can be close to correct solutions despite zero pass rates, but no alternative metrics are proposed. A new metric that correlates with human judgments of "near-miss" solutions and predicts eventual solvability with minor intervention would resolve this.

### Open Question 4
Why does human feedback dramatically improve o1's performance but show no improvement for GPT-4 and GPT-4o? Table 7 shows o1 improves from 0% to 94.4% with interaction, while GPT-4 and GPT-4o remain at 0%. Section 6 notes "latent differences across models are revealed when we discover that only some models are able to correctly integrate feedback." A controlled study identifying specific model capabilities (e.g., instruction-following, reasoning flexibility) that predict feedback integration success would resolve this.

## Limitations
- Performance improvements are heavily dependent on the ICPC benchmark's specific construction and may not generalize to other coding domains
- Episodic retrieval mechanism requires a comprehensive corpus of similar problems with complete solutions, which may not exist for many practical coding tasks
- Reflection mechanism assumes models can diagnose and fix their own errors from binary pass/fail feedback, which may not scale to more complex failure modes

## Confidence
- High Confidence: The core finding that multi-turn self-reflection improves solve rates for capable models (o1) is well-supported by controlled ablations
- Medium Confidence: The episodic retrieval results are convincing for o1, though complex interactions need further investigation
- Medium Confidence: The human-in-the-loop results for o1 are compelling but limited to a small sample and show no benefit for other models
- Low Confidence: The claim that this represents a general advancement in algorithmic reasoning is overstated given the narrow domain and model-specific effects

## Next Checks
1. **Cross-domain validation**: Test the multi-turn self-judge framework on non-competitive programming tasks (e.g., practical software engineering problems) to assess generalizability beyond algorithmic puzzles.
2. **Model capability threshold**: Systematically test the reflection mechanism across a wider range of model capabilities to identify the minimum model capability required for self-reflection to be effective.
3. **Retrieval corpus dependency**: Evaluate the episodic retrieval performance using progressively smaller or noisier problem corpora to quantify the minimum corpus quality required for meaningful improvement.