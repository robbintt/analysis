---
ver: rpa2
title: 'Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens'
arxiv_id: '2506.17218'
source_url: https://arxiv.org/abs/2506.17218
tags:
- reasoning
- latent
- image
- visual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Machine Mental Imagery (Mirage) enables vision-language models
  to reason multimodally without explicit image generation by interleaving compact
  latent visual tokens with text during decoding. The framework uses a two-stage training
  paradigm: first jointly supervising text and latent visual tokens with compressed
  image embeddings, then relaxing latent supervision to let the model adapt the embeddings
  to guide answer generation.'
---

# Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens

## Quick Facts
- arXiv ID: 2506.17218
- Source URL: https://arxiv.org/abs/2506.17218
- Authors: Zeyuan Yang; Xueyang Yu; Delin Chen; Maohao Shen; Chuang Gan
- Reference count: 18
- One-line primary result: Mirage improves spatial reasoning by interleaving latent visual tokens with text, achieving up to 11% accuracy gains over text-only baselines.

## Executive Summary
Machine Mental Imagery (Mirage) enables vision-language models to perform multimodal reasoning without explicit image generation by interleaving compact latent visual tokens with text during decoding. The framework uses a two-stage training paradigm: first jointly supervising text and latent visual tokens with compressed image embeddings, then relaxing latent supervision to let the model adapt the embeddings to guide answer generation. Experiments across four spatial reasoning benchmarks show consistent performance gains over text-only baselines, with up to 11% improvement in spatial planning and 5% in visual puzzle tasks. Further analysis reveals that latent tokens cluster near the visual embedding space, supporting their role as meaningful visual cues. Reinforcement learning further boosts performance, confirming the effectiveness of interleaved latent reasoning.

## Method Summary
Mirage extends autoregressive decoding from discrete text to continuous latent embeddings, allowing VLMs to "think visually" without generating pixel-level images. The model compresses helper image patch features into k salient vectors (typically k=4) via average pooling, then interleaves these latent tokens with text during reasoning. Two-stage training prevents over-constraining: Stage 1 jointly supervises text (cross-entropy) and latents (cosine similarity to compressed image embeddings), while Stage 2 removes latent supervision, allowing gradients to flow through self-generated latents. The approach is tested on four spatial reasoning benchmarks using synthesized helper images and reasoning thoughts from a larger teacher model.

## Key Results
- Achieves up to 11% improvement on spatial planning tasks compared to text-only baselines
- Shows 5% gain on visual puzzle (jigsaw) tasks over text-only approaches
- Latent tokens cluster near visual embedding space, confirming their role as meaningful visual cues
- GRPO reinforcement learning adds +2% accuracy over Stage 2 alone on VSP tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Interleaving compact latent visual tokens with text enables VLMs to perform "mental imagery" reasoning without explicit image generation.
- **Mechanism**: When the model produces a special token indicating visual reasoning, it reuses its current hidden state as a compact visual embedding appended to the context, bypassing language projection. These embeddings serve as visual priors for subsequent reasoning steps.
- **Core assumption**: Compressed visual features (via average pooling over patch embeddings) retain task-critical visual cues sufficient to guide reasoning, mimicking human mental imagery's abstract rather than photorealistic representations.
- **Evidence anchors**:
  - [abstract] "whenever the model chooses to 'think visually', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images"
  - [section 3.2] "we mimic human mental imagery by compressing these features into k salient vectors... that retain only task-critical visual cues"
  - [corpus] Related work "Chain-of-Visual-Thought" explores continuous visual tokens but lacks direct comparative evidence for this specific compression approach
- **Break condition**: If visual features are over-compressed (k=8+ tokens), error accumulation in autoregressive generation degrades performance ~13% (Table 5).

### Mechanism 2
- **Claim**: Two-stage training prevents the over-constraining that plagues unified image-text generation models.
- **Mechanism**: Stage 1 jointly supervises text (cross-entropy) and latent tokens (cosine similarity to compressed image embeddings), anchoring latents in visual subspace. Stage 2 removes latent supervision, allowing gradients from text-only loss to flow backward through differentiable latent tokens, adapting them to task objectives.
- **Core assumption**: Visual and textual subspaces in VLMs are sufficiently heterogeneous that grounding (Stage 1) is necessary before relaxation (Stage 2); unsupervised latent tokens alone would drift.
- **Evidence anchors**:
  - [abstract] "supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision"
  - [section 3.3] "These self-generated embeddings replace the compressed image vectors... the gradient can be propagated to these latent tokens when minimizing the above loss"
  - [corpus] No direct corpus evidence; related unified models (Chameleon, MVoT) show degraded reasoning with explicit image generation
- **Break condition**: Removing Stage 1 drops spatial planning accuracy from 58% to 52%; removing Stage 2 drops it to 21% (Figure 4).

### Mechanism 3
- **Claim**: Reinforcement learning with GRPO further optimizes the interleaved reasoning trajectory.
- **Mechanism**: After SFT, GRPO samples multiple responses per query, optimizing text token probabilities while allowing gradients through latent tokens. Rewards combine accuracy (binary) and format compliance (0.1 bonus for proper tags).
- **Core assumption**: The latent trajectory provides sufficient exploration signal for RL to improve; format regularization prevents degeneration.
- **Evidence anchors**:
  - [section 3.4] "we explicitly optimize the probabilities of textual tokens while allowing gradients to flow through the latent tokens"
  - [table 1] GRPO adds +2% accuracy over Stage 2 alone on VSP tasks
  - [corpus] Related work "Reasoning Within the Mind" uses latent interleaving but without reported RL validation
- **Break condition**: If synthesized helper images are low-quality (e.g., video generation artifacts in SAT), RL gains are limited.

## Foundational Learning

- **Concept: Autoregressive multimodal generation**
  - Why needed here: Mirage extends autoregressive decoding from discrete text to continuous latent embeddings; understanding how tokens condition subsequent tokens is essential.
  - Quick check question: Can you explain why the model can backpropagate through self-generated latent tokens but not through discrete image tokens?

- **Concept: Embedding space alignment**
  - Why needed here: The two-stage training relies on aligning latent tokens to visual embeddings (Stage 1) then relaxing this constraint (Stage 2); t-SNE analysis confirms latents cluster near visual subspace.
  - Quick check question: Why would forcing latent tokens to exactly match visual embeddings (never relaxing) hurt reasoning performance?

- **Concept: Chain-of-thought reasoning**
  - Why needed here: Mirage builds on CoT by inserting latent visual steps into reasoning traces; prior work shows CoT improves multi-step tasks.
  - Quick check question: How does interleaving latent tokens differ from CoT with explicit image generation?

## Architecture Onboarding

- **Component map**: Base VLM (Qwen2.5-VL 7B) with frozen vision encoder -> Latent token generator (hidden state projection, k=4 tokens) -> Compression module (average pooling over patch features) -> Two-stage trainer (joint CE + cosine loss → CE only) -> RL optimizer (GRPO with accuracy + format rewards)

- **Critical path**:
  1. Generate helper images (task-specific: arrow annotations, video frames, composite jigsaws)
  2. Compress helper image patch features via average pooling → k vectors
  3. Stage 1: Train with joint loss (L_visual + γL_text, γ=0.1)
  4. Stage 2: Switch to text-only loss, using self-generated latents
  5. RL: GRPO with 5 rollouts per query, format + accuracy rewards

- **Design tradeoffs**:
  - Latent size k: 2-6 robust; k=8 causes 13% drop (error accumulation)
  - Loss coefficient γ: 0.1-1.0 workable; larger γ reduces visual grounding
  - Helper image quality: Critical upper bound; nearly 100% accuracy when provided as explicit input (Figure 6)

- **Failure signatures**:
  - Latents drift to text subspace → low visual grounding, similar to text-only baseline
  - Stage 2 skipped → latents over-constrained, similar to unified models' degraded reasoning
  - Poor helper images (e.g., inconsistent video frames) → upper bound collapses, RL struggles

- **First 3 experiments**:
  1. **Ablate stages**: Train Stage 1 only and Stage 2 only on VSP planning; verify ~50% and ~21% accuracy respectively vs. 58% full pipeline
  2. **Visualize latents**: Run t-SNE on text, image, and latent embeddings from 100 samples; confirm latent cluster near but outside visual subspace
  3. **Helper image quality test**: Provide synthesized helper images as explicit input (not latent supervision); verify near-100% accuracy ceiling, confirming data quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the aligned feature space of unified multimodal models be leveraged to improve the latent reasoning design of Mirage?
- Basis in paper: [explicit] The authors state in the Limitations section that it "remains an open question" whether unified models, which jointly align latent space during image and text generation, can enhance latent reasoning despite their current struggles with interleaved generation.
- Why unresolved: Unified models currently suffer from degraded reasoning abilities and cannot produce interleaved trajectories pertinent to input images effectively.
- What evidence would resolve it: Experiments integrating the Mirage framework into unified architectures (e.g., Chameleon) to compare against the current disjoint vision-language model approach.

### Open Question 2
- Question: Can the Mirage framework be extended effectively to broader multimodal tasks or purely textual reasoning tasks?
- Basis in paper: [explicit] The conclusion notes that current evaluation is limited to spatial-reasoning benchmarks, and "How to extend our framework to broader multimodal or purely textual tasks remains an open direction."
- Why unresolved: The method relies on helper images and visual spatial reasoning structures (e.g., jigsaw puzzles), making generalization to non-visual or non-spatial domains uncertain.
- What evidence would resolve it: Benchmarking Mirage on general Visual Question Answering (VQA) datasets or logical reasoning text datasets to test if latent visual tokens aid non-spatial tasks.

### Open Question 3
- Question: To what extent does the quality of synthesized reasoning trajectories and prompts limit the model's performance?
- Basis in paper: [explicit] The appendix states that the current approach relies on "straightforward prompts, which occasionally yield subpar reasoning trajectories," and suggests "Developing richer prompts... remains an important future work."
- Why unresolved: The model's performance upper bound is constrained by the quality of the synthetic data generated by the teacher model (Qwen2.5-VL 32B).
- What evidence would resolve it: Ablation studies comparing model performance when trained on synthetically generated trajectories versus human-curated or rigorously verified reasoning chains.

## Limitations
- Helper image quality appears to be the critical upper bound, with near-100% accuracy when helper images are provided as explicit inputs
- The choice of k=4 latent tokens is somewhat arbitrary despite being described as "robust" across 2-6 tokens
- Reinforcement learning contribution is modest (+2% accuracy) with underspecified hyperparameters

## Confidence

**High Confidence**: The core mechanism of interleaving latent visual tokens with text during decoding is well-specified and experimentally validated. The two-stage training approach (joint supervision followed by relaxation) is clearly described, and the performance gains over text-only baselines are consistently reproducible across four different spatial reasoning benchmarks.

**Medium Confidence**: The interpretation of latent tokens as "meaningful visual cues" and their clustering near visual embedding space. While t-SNE visualizations are provided, the analysis lacks quantitative measures of semantic alignment or ablation studies varying k beyond the reported range. The claim that this mimics "human mental imagery" is more metaphorical than empirically supported.

**Low Confidence**: The reinforcement learning component's contribution and the specific choice of k=4 latent tokens. The paper reports only marginal improvements from GRPO (+2%), and the hyperparameters for the RL training are insufficiently detailed. The selection of 4 latent tokens appears based on empirical testing without theoretical justification for why this number is optimal versus 2, 3, or 6 tokens.

## Next Checks

1. **Latent clustering validation**: Beyond t-SNE visualization, compute quantitative metrics measuring the semantic alignment between latent tokens and visual embeddings. Specifically, measure the nearest-neighbor classification accuracy when projecting latents into the visual embedding space, and compare this to random projections and text embeddings.

2. **Helper image quality ceiling test**: Systematically vary the quality of synthesized helper images (e.g., using different generation models, resolution, or annotation styles) while keeping the latent reasoning framework fixed. This would establish whether the current performance is limited by image synthesis rather than the reasoning mechanism itself.

3. **Latent token sensitivity analysis**: Conduct a comprehensive ablation study varying k from 1 to 10 latent tokens, measuring not just final accuracy but also training stability, latent trajectory coherence, and computational overhead. This would clarify whether k=4 is truly optimal or merely a local optimum within a broader range.