---
ver: rpa2
title: 'DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning'
arxiv_id: '2602.00352'
source_url: https://arxiv.org/abs/2602.00352
tags:
- memory
- agent
- search
- query
- detour
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning

## Quick Facts
- arXiv ID: 2602.00352
- Source URL: https://arxiv.org/abs/2602.00352
- Reference count: 40
- Primary result: A new dual-agent interactive benchmark that isolates reasoning evaluation by separating the Primary Agent (web search + memory query) from a grounded Memory Agent, revealing that SOTA models often fail at productive clarification-seeking despite showing significant performance gains when Memory Agent access is provided.

## Executive Summary
DETOUR is an interactive benchmark that evaluates large language models' ability to seek clarification and reason through tip-of-the-tongue scenarios using a dual-agent architecture. The Primary Agent must interface with a Memory Agent that provides only grounded file-based responses (no inference) and a web search tool to resolve ambiguous queries. The benchmark reveals that while Memory Agent availability significantly improves performance, current SOTA models often engage in unproductive querying behavior that negatively correlates with accuracy, highlighting substantial research opportunities in teaching models effective interactive reasoning strategies.

## Method Summary
DETOUR evaluates models through a dual-agent setup where a Primary Agent interacts with a fixed Memory Agent (Gemini-2.5-Pro) and a web search tool to resolve queries. The Memory Agent responds only to information directly observable in its file, forcing the Primary Agent to perform all abstraction and association. Evaluation uses an LLM judge (GPT-4o) to assess answer correctness. The benchmark includes 1,150 instances across 23 topics with multimodal input files and abstract memory files. In-context examples are provided to guide reasoning, and DSPy MIPROv2 is used to optimize Memory Agent prompting for better grounding.

## Key Results
- Memory Agent availability significantly improves performance across all tested models (Table 3)
- SOTA models show negative correlation between memory query count and accuracy (e.g., GPT-5 ρ=-0.336, Claude-sonnet-4.5 ρ=-0.431)
- In-context examples with higher density of memory-related reasoning improve Primary Agent performance (GPT-5 shows ρ=0.290, p<0.05)
- Modest gains from in-context learning (+0.9% average) indicate substantial research still needed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-agent separation isolates reasoning evaluation to the Primary Agent while simulating user memory through a grounded Memory Agent.
- Mechanism: The Memory Agent responds only to information directly observable in its file (no inferences), forcing the Primary Agent to perform all abstraction and association itself. This creates a test of clarification-seeking behavior rather than direct retrieval.
- Core assumption: Models can learn to ask productive clarification questions when given appropriate scaffolding.
- Evidence anchors:
  - [Section 3]: "The Memory Agent receives a memory file containing often abstract cues about the target entity; the memory file does not directly refer to the answer to prevent information leakage."
  - [Section 4.2]: "We restrict the Memory Agent to respond solely based on the content of the memory file, without performing any deductions."
  - [Corpus]: BLUR (Sky et al., 2025) establishes the precedent for tip-of-the-tongue benchmarks; DETOUR extends this with interactive multi-turn evaluation.
- Break condition: If the Memory Agent prompt allows inference or the Primary Agent can bypass the memory tool entirely, the evaluation no longer measures clarification capability.

### Mechanism 2
- Claim: In-context examples with higher density of memory-related reasoning improve Primary Agent performance.
- Mechanism: Demonstrating abstract reasoning patterns in the thought trace (e.g., how to query memory creatively, how to interpret Memory Agent refusals) teaches models productive interaction strategies through few-shot learning.
- Core assumption: Models can transfer reasoning patterns from in-context examples to novel queries.
- Evidence anchors:
  - [Section 5.4]: "Across the board, we see positive correlations between the density of discussion about the memory agent in the in-context examples (Rmem) and the accuracy."
  - [Section 5.4, Table 4]: GPT-5 shows statistically significant positive correlation (ρ=0.290, p<0.05) between Rmem and accuracy.
  - [Corpus]: Related work on in-context learning (Dong et al., 2024, cited in paper) supports this mechanism.
- Break condition: If in-context examples contain invalid logic or lead models down unproductive paths, performance may degrade despite high Rmem.

### Mechanism 3
- Claim: Unproductive memory querying (repeatedly asking direct-answer questions) negatively correlates with accuracy.
- Mechanism: Models that fixate on extracting direct answers from the Memory Agent waste turns and can become confused by refusal responses, derailing reasoning. Conversely, models that query sparingly (like Qwen3) show positive correlation between queries and accuracy.
- Core assumption: The number of queries reflects query quality, not just task difficulty.
- Evidence anchors:
  - [Section 5.2, Table 2]: "Interestingly, there is often a negative correlation between the number of memory queries and model accuracy."
  - [Section 5.2]: GPT-5 shows ρ=-0.336 (p<0.001), Claude-sonnet-4.5 shows ρ=-0.431 (p<0.001).
  - [Corpus]: Deng et al. (2025) found LLMs fail to recognize ambiguity due to overconfidence; DETOUR operationalizes this in an interactive setting.
- Break condition: If models learn to ask more strategic questions, this negative correlation should diminish or reverse.

## Foundational Learning

- Concept: **Tool-calling / Function invocation**
  - Why needed here: The Primary Agent must invoke both `memory_query` and web search tools multiple times within a single episode.
  - Quick check question: Can your model reliably call tools conditionally based on intermediate reasoning?

- Concept: **Multi-turn conversational reasoning**
  - Why needed here: Tip-of-the-tongue retrieval requires accumulating clues across multiple Memory Agent interactions and web searches.
  - Quick check question: Does your model maintain coherent reasoning state across 5+ tool-call turns?

- Concept: **Ambiguity recognition and clarification-seeking**
  - Why needed here: The benchmark explicitly rewards asking disambiguating questions rather than guessing prematurely.
  - Quick check question: When faced with underspecified input, does your model ask clarifying questions or make assumptions?

## Architecture Onboarding

- Component map:
  - Primary Agent -> Memory Agent (fixed: Gemini-2.5-Pro) -> Memory File
  - Primary Agent -> Web Search Tool
  - Primary Agent -> LLM Judge (GPT-4o)

- Critical path:
  1. Primary Agent receives query + optional input file
  2. Agent invokes memory_query with free-text questions → Memory Agent responds with file-grounded descriptions only
  3. Agent invokes web search as needed
  4. Agent produces final answer → LLM-as-judge evaluates correctness

- Design tradeoffs:
  - Fixed Memory Agent (Gemini-2.5-Pro) ensures consistent evaluation but may introduce model-specific quirks
  - Prompt optimization via DSPy MIPROv2 improved grounding (83.1% → 97.5%) at cost of prompt complexity
  - In-context examples help but gains are modest; substantial research still needed

- Failure signatures:
  - Repeated unproductive memory queries (asking same question despite refusal)
  - Fixation on extracting direct answers vs. abstract reasoning
  - Ignoring Memory Agent refusals and continuing similar queries
  - Over-reliance on frequency-based guessing when Memory Agent unavailable

- First 3 experiments:
  1. **Baseline without Memory Agent**: Run evaluation with memory_query tool disabled (Table 3) to measure the gap your model must close.
  2. **In-context example sweep**: Test multiple example thought traces with varying Rmem scores to identify which reasoning patterns help your model most.
  3. **Query count correlation analysis**: Log number of memory queries per instance and compute Spearman correlation with accuracy to diagnose whether your model falls into the unproductive-querying trap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What metrics beyond keyword density define the "actual reasoning quality" of in-context examples necessary to improve agent performance on DETOUR?
- Basis in paper: [explicit] The Limitations section states that current analysis remains "at the surface level" using a simple word-based metric, and future work must determine the "actual reasoning quality" of examples.
- Why unresolved: The authors found that while mentioning "memory" helps, the logical validity and natural transitions of the thought traces were not quantified.
- What evidence would resolve it: A study correlating logical validity scores of in-context thought traces with Primary Agent accuracy improvements.

### Open Question 2
- Question: How can the benchmark's ambiguity be increased to ensure tasks are strictly unsolvable without Memory Agent assistance?
- Basis in paper: [explicit] The Limitations section notes instances can still be answerable without the Memory Agent and suggests "increasing the difficulty of DETOUR even further" as future work.
- Why unresolved: Current data collection allowed some queries to be resolved via frequency-based assumptions or "popular" options without disambiguation.
- What evidence would resolve it: A revised dataset version where models consistently score 0% in ablations without access to the Memory Agent.

### Open Question 3
- Question: What specific training methodologies are required to prevent models from fixating on unproductive querying of the Memory Agent?
- Basis in paper: [explicit] Research Question 4 asks how to teach efficient interfacing; the authors conclude that "substantial further research is still required" as in-context learning gains are modest.
- Why unresolved: Current SOTA models exhibit a negative correlation between query count and accuracy, often ignoring agent refusals and derailing the reasoning process.
- What evidence would resolve it: Fine-tuning experiments that yield a positive correlation between Memory Agent usage and task accuracy.

## Limitations

- Memory Agent grounding reliability: While improved to 97.5%, there remains a small probability of information leakage through subtle reasoning that would invalidate Primary Agent isolation.
- Generalization of in-context examples: Modest gains (+0.9% average) suggest current few-shot learning may not effectively teach abstract reasoning patterns.
- Correlation interpretation: Negative query-accuracy correlation could reflect task difficulty variation rather than unproductive querying behavior.

## Confidence

**High Confidence**: Dual-agent architecture design and basic evaluation framework are sound; Memory Agent availability significantly improves performance across multiple model families.

**Medium Confidence**: Interpretation of negative query-accuracy correlations as evidence of unproductive querying behavior; alternative explanations (instance difficulty, exploration vs. exploitation trade-offs) not fully ruled out.

**Low Confidence**: Specific reasoning patterns that lead to high performance; with modest in-context learning gains and no clear winning strategies, definitive best practices cannot yet be prescribed.

## Next Checks

1. **Controlled Instance Difficulty Analysis**: Group evaluation instances by ground truth answer frequency and web search result count. Recompute correlation between query count and accuracy within difficulty-matched subsets to determine whether the negative correlation persists after controlling for instance complexity.

2. **Memory Agent Blind Testing**: Run a subset of evaluations with the Memory Agent's grounding intentionally weakened (e.g., reduced prompt clarity) to test whether observed performance improvements depend on Memory Agent reliability or reflect genuine Primary Agent capability.

3. **Cross-Model Memory Agent Comparison**: Evaluate the same Primary Agent models against multiple Memory Agent variants (different prompt strategies, different base models) to quantify the sensitivity of Primary Agent performance to Memory Agent behavior, helping isolate model capability from evaluation artifact.