---
ver: rpa2
title: 'Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning'
arxiv_id: '2509.25052'
source_url: https://arxiv.org/abs/2509.25052
tags:
- agent
- game
- cell
- reasoning
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Cogito, Ergo Ludo (CEL), an agent that learns
  to play games by reasoning and planning rather than through extensive trial-and-error.
  The agent uses a Large Language Model to build an explicit, language-based understanding
  of game rules and strategies through a two-phase cycle: in-episode decision-making
  using lookahead search and post-episode reflection to refine its knowledge.'
---

# Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning

## Quick Facts
- arXiv ID: 2509.25052
- Source URL: https://arxiv.org/abs/2509.25052
- Reference count: 40
- An LLM-based agent that learns to play games through reasoning and planning, achieving 54% (Minesweeper), 97% (Frozen Lake), and 84% (Sokoban) success rates without ground-truth rules.

## Executive Summary
Cogito, Ergo Ludo (CEL) presents an agent that learns to play games by building explicit, language-based representations of game rules and strategies rather than through extensive trial-and-error. The agent operates in a two-phase cycle: during episodes, it uses lookahead search with a Language-based World Model to make decisions, then reflects post-episode to refine its rule set and playbook. Evaluated on Minesweeper, Frozen Lake, and Sokoban, CEL achieves strong performance across all tasks, particularly notable for its ability to learn from scratch without any prior knowledge of game mechanics. The iterative rule induction process proves critical to sustained learning improvement.

## Method Summary
CEL uses a Qwen3-4B-Instruct LLM with GRPO training to learn game-playing through explicit reasoning. The agent maintains a language-based world model that predicts state transitions and rewards, combined with a value function that assesses state quality. After each episode, the agent analyzes its trajectory to induce new rules and update its playbook, then applies these updates to improve future decision-making. Training uses sparse rewards (+1 for success, 0 otherwise) and updates rules/playbook every 5 episodes. The architecture emphasizes interpretability through natural language reasoning traces while achieving competitive performance against both zero-shot baselines and agents given ground-truth rules.

## Key Results
- CEL achieves 54% success on Minesweeper, 97% on Frozen Lake, and 84% on Sokoban
- Iterative rule induction is critical: ablation shows "w/o Rules" fails to learn while "Rules induced once" degrades over time
- Cross-game transfer succeeds: FrozenLake-trained agent achieves 81.7% on Minesweeper (vs. 54% zero-shot)
- Outperforms zero-shot baselines and even surpasses agents given ground-truth rules in Minesweeper

## Why This Works (Mechanism)

### Mechanism 1: Iterative Rule Induction from Trajectory Analysis
The agent maintains an explicit rule set G_k that is updated post-episode by analyzing state-action-reward tuples. Each update refines predictions of environmental dynamics, which directly improves the Language-based World Model's lookahead accuracy in subsequent episodes. The core assumption is that the LLM can reliably extract valid causal patterns from sparse interaction data without hallucinating false regularities. Evidence shows ablation studies confirm iterative rule induction is critical for sustained learning, with the "w/o Rules" baseline showing flat learning and "Rules induced once" showing initial improvement then degradation. The break condition occurs if rule updates introduce systematic errors, causing lookahead predictions to become misleading.

### Mechanism 2: Language-Based Value Assessment with World Model Lookahead
At each step, the LVF produces a qualitative value judgment v(s_t), while the LWM simulates outcomes for candidate actions. The agent selects actions that the LWM predicts will lead to the highest-value outcomes, using natural language reasoning traces to justify choices. The core assumption is that natural language representations of value and state transitions preserve sufficient information for decision-making. Evidence shows the agent correctly predicting safe vs. mine cells in Minesweeper through explicit reasoning. The break condition occurs if language representations lose critical state information, degrading lookahead quality independent of rule quality.

### Mechanism 3: GRPO Training on Episode Outcomes for Strategic Transfer
Final episode outcomes (success/failure) provide reward signals for GRPO optimization, training the LLM to produce more effective chain-of-thought reasoning over time. The trained model generalizes to new environments by applying learned reasoning patterns, even with frozen weights. The core assumption is that reasoning patterns learned are abstract enough to transfer (e.g., "explore before committing") rather than game-specific heuristics. Evidence shows cross-game transfer with frozen weights, where Minesweeper-trained agents tested on Frozen Lake show robust learning curves. The break condition occurs if training overfits to game-specific patterns, causing transfer to fail.

## Foundational Learning

- **Markov Decision Processes (MDPs)**: The paper formalizes agent-environment interaction using MDP tuples (S, A, P, R, γ). Understanding this framework is essential to grasp what the Language-based World Model attempts to approximate. Quick check: Can you explain why the transition function P(s_{t+1}|s_t, a_t) is what the LWM learns to predict?

- **World Models in Reinforcement Learning**: CEL builds explicitly on world model approaches (MuZero, Dreamer) but distinguishes itself through language-based, interpretable representations rather than latent states. Quick check: How does a learned world model enable planning without access to the true environment dynamics?

- **In-Context Learning vs. Weight Updates**: CEL uses both: rule/playbook updates feed into prompts (in-context learning), while GRPO updates model weights. Distinguishing these is critical for understanding the two-phase cycle. Quick check: What information can be rapidly adapted via prompt updates versus what requires weight updates?

## Architecture Onboarding

- **Component map**: Qwen3-4B-Instruct -> Language-based World Model -> Language-based Value Function -> Rule Induction Module -> Playbook Summarization Module -> GRPO Optimizer -> Trajectory Buffer

- **Critical path**: 
  1. Initialize G_0 = ∅, Π_0 = general-purpose prompt
  2. For each episode k: Phase 1 (In-Episode) uses LVF and LWM for decision-making; Phase 2 (Post-Episode) runs Rule Induction and Playbook Summarization
  3. Every 5 episodes, apply rule/playbook updates; apply GRPO weight update

- **Design tradeoffs**:
  - Language vs. latent representations: Human-readable rules enable interpretability but may lose information density
  - Update frequency: Rule updates every 5 episodes balance stability vs. adaptation
  - Model scale: Qwen3-4B-Instruct; smaller models may struggle with complex reasoning
  - Sparse rewards only: No intermediate shaping forces genuine discovery but slows early learning

- **Failure signatures**:
  - Flat learning curve: Rule induction not functioning; check prompt templates and trajectory parsing
  - Initial improvement then collapse: Rules becoming stale; verify update mechanism is actually running
  - GRPO training failure: Rollout outcomes are all-success or all-failure within batches
  - Cross-game transfer fails: Model may have overfit to game-specific patterns

- **First 3 experiments**:
  1. Reproduce FrozenLake results: Verify two-phase cycle by logging G_k and Π_k updates
  2. Run ablation on Minesweeper: Implement three variants to validate iterative rule induction
  3. Test cross-game transfer with frozen weights: Train on FrozenLake, freeze weights, evaluate on Minesweeper

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unaddressed based on the limitations and evidence presented.

## Limitations
- Limited to grid-world environments with discrete states and small action spaces
- GRPO training relies heavily on sparse reward signals that may not scale to complex environments
- Exact prompt templates for some components are not fully specified in the main text
- Computational requirements for running LLM-based agents are not discussed

## Confidence
- **High confidence** in iterative rule induction mechanism (validated through ablation studies)
- **High confidence** in cross-game transfer capabilities (empirically demonstrated with frozen weights)
- **Medium confidence** in scalability to more complex environments (results limited to specific grid-world domains)
- **Medium confidence** in reproducibility details (core components described but some implementation specifics missing)

## Next Checks
1. **Prompt template verification**: Implement exact prompt templates from Appendix D and verify they produce reasoning traces shown in Figure 5
2. **Ablation replication**: Reproduce all three ablation variants (w/o Rules, Rules induced once, Action-only) to confirm learning curves match Figure 4
3. **Cross-game transfer validation**: Train on FrozenLake, freeze weights, then evaluate on Minesweeper with only rule/playbook updates to verify +27.7% improvement over zero-shot baseline