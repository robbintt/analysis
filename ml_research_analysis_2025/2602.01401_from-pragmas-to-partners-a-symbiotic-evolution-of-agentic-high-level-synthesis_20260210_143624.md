---
ver: rpa2
title: 'From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis'
arxiv_id: '2602.01401'
source_url: https://arxiv.org/abs/2602.01401
tags:
- agentic
- design
- high-level
- synthesis
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that HLS remains essential in the agentic era,\
  \ offering faster iteration cycles, portability, and design permutability that make\
  \ it a natural layer for agentic optimization. The authors identify three key limitations\
  \ of current HLS tools\u2014inadequate performance feedback, rigid interfaces, and\
  \ limited debuggability\u2014that agents are uniquely positioned to address."
---

# From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis

## Quick Facts
- arXiv ID: 2602.01401
- Source URL: https://arxiv.org/abs/2602.01401
- Reference count: 14
- This paper argues that HLS remains essential in the agentic era, offering faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.

## Executive Summary
This paper argues that HLS remains essential in the agentic era, offering faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization. The authors identify three key limitations of current HLS tools—inadequate performance feedback, rigid interfaces, and limited debuggability—that agents are uniquely positioned to address. They propose a taxonomy for the symbiotic evolution of agentic HLS, spanning from manual DSE (L0) to end-to-end autonomous design partners (L5). The framework clarifies how responsibility shifts from human designers to AI agents as systems advance, highlighting the need for improved performance modeling, interface flexibility, and verification capabilities to enable greater autonomy.

## Method Summary
This is a conceptual framework paper that positions HLS as the natural abstraction layer for agentic hardware design. Rather than presenting a specific algorithm or training procedure, the authors propose a 6-level taxonomy (L0-L5) for agentic HLS automation spanning manual design space exploration to fully autonomous design partners. The framework is supported by references to 14 prior works on LLM-assisted HLS, though no specific datasets or evaluation protocols are provided. The paper identifies three critical gaps in current HLS tools that agents can address: inadequate performance feedback, rigid interfaces, and limited debuggability.

## Key Results
- Proposes a 6-level taxonomy (L0-L5) for agentic HLS automation spanning manual DSE to autonomous design partners
- Identifies three key limitations of current HLS tools: inadequate performance feedback, rigid interfaces, and limited debuggability
- Argues that HLS source code serves as a "golden reference" creating a verifiable chain of custody that reduces hallucination risk

## Why This Works (Mechanism)

### Mechanism 1
If agents operate at the HLS abstraction layer rather than RTL, they achieve higher optimization velocity because the representation is more "permutable" (easier to mutate without breaking functionality). HLS code (C/C++) separates algorithmic intent from hardware microarchitecture (pipelining, unrolling) via pragmas. An agent modifies pragmas or high-level structures to explore the design space. This requires fewer token changes and less complex reasoning than restructuring RTL signals and state machines, preserving functional correctness during rapid iterations. Core assumption: The agent can correctly map performance constraints (latency, throughput) to specific HLS pragma configurations.

### Mechanism 2
HLS source code serves as a "golden reference," creating a verifiable chain of custody that reduces the hallucination risk common in generative AI hardware flows. The agent generates HLS C-code, which is functionally validated against a software testbench (fast). This verified C-code then passes to the HLS compiler to generate RTL. If the RTL fails, the agent performs differential testing: comparing RTL behavior against the known-good C-behavior to isolate the bug, rather than debugging raw RTL from scratch. Core assumption: The HLS compiler itself is trusted, and the high-level testbench provides sufficient functional coverage.

### Mechanism 3
Agents can overcome the "black box" limitation of current HLS tools by constructing "mixed-fidelity performance models" to guide optimization where tool reports are insufficient. Standard HLS tools often report "unknown" or "?" latency for dynamic control flows (e.g., data-dependent loops). The authors propose that agents can parse compilation logs, operation schedules, and loop bounds to construct an estimated performance model. This allows the agent to continue optimizing even without precise final timing data. Core assumption: The agent can infer bound analysis or profiling data that the HLS tool failed to report or propagate.

## Foundational Learning

- **Concept: HLS Pragmas and Directives**
  - Why needed here: The paper posits a shift from "manual DSE" to "agentic optimization." This relies on the agent understanding how to manipulate pragmas (e.g., `#pragma HLS PIPELINE`, `UNROLL`) to reshape hardware without altering C-logic.
  - Quick check question: Can you explain the difference between `PIPELINE` and `DATAFLOW` pragmas in terms of task-level vs. instruction-level parallelism?

- **Concept: Design Space Exploration (DSE)**
  - Why needed here: The core value proposition of Agentic HLS is automating the search through the vast space of possible microarchitectures (the "permutations").
  - Quick check question: If you double the unroll factor of a loop, what are the tradeoffs regarding resource usage (BRAM/DSP) vs. throughput?

- **Concept: Differential Testing / Equivalence Checking**
  - Why needed here: This is identified as the key verification mechanism (L3) where agents "provide strong correctness guarantee[s]." Understanding how to compare a high-level software model against a low-level hardware implementation is critical.
  - Quick check question: How would you identify a mismatch between a C++ simulation result and an RTL waveform trace?

## Architecture Onboarding

- **Component map:** Agent Core -> HLS Toolchain -> Feedback Parser -> Verification Engine
- **Critical path:** The **Feedback Loop**. The system lives or dies by how quickly the agent can cycle: *Edit Code -> Synthesize -> Parse Report -> Identify Bottleneck*.
- **Design tradeoffs:**
  - Coarse vs. Fine Feedback: Running full synthesis (accurate but slow/minutes) vs. running only scheduling analysis (fast/seconds but missing logic utilization data).
  - Autonomy vs. Guardrails: Allowing the agent to rewrite code structure (riskier, higher potential) vs. restricting it to pragma tuning (safer, limited).
- **Failure signatures:**
  - **Infinite Loop:** Agent oscillates between two sub-optimal pragma configurations (e.g., fighting between II=1 and resource limits).
  - **Interface Deadlock:** Agent optimizes the kernel perfectly, but the generated AXI interface conflicts with the surrounding SoC (the "composability gap").
  - **Hallucinated Constraints:** Agent invents pragmas not supported by the target toolchain version.
- **First 3 experiments:**
  1. **L1 Baseline (Copilot):** Implement a simple RAG system that ingests a standard HLS log file and suggests 3 specific code changes to the user. Verify if the suggestions are syntactically valid.
  2. **Feedback Robustness:** Feed the agent HLS reports with "unknown" (?) latency values. Measure if the agent can successfully query the user for more constraints or estimate the bound, vs. simply halting.
  3. **L2 Closed Loop (Autotuning):** Give the agent a specific kernel (e.g., matrix multiplication) and a target throughput. Restrict the agent to only modifying pragmas. Let it run autonomously for 10 iterations and plot the latency curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agents construct accurate mixed-fidelity performance models when HLS synthesis reports provide ambiguous or data-dependent cycle counts?
- Basis in paper: The paper states that current tools often report "unknown or variable cycle counts" for designs with data-dependent control flow, offering "little actionable guidance."
- Why unresolved: Current HLS reports fail to provide the granular feedback required for agents to optimize complex, dynamic designs without expensive RTL simulation.
- What evidence would resolve it: An agentic system that generates actionable latency estimates and replayable traces for dynamic designs where standard tools report unknowns.

### Open Question 2
- Question: How can agents automatically synthesize correct interface adapters to bridge HLS-generated IP with diverse system-on-chip frameworks?
- Basis in paper: Section 3 identifies the "composability gap" caused by rigid interfaces (e.g., Vitis HLS supporting only standard AXI) and notes agents must generate bridge logic.
- Why unresolved: Integrating HLS blocks with frameworks like LiteX currently requires manual writing of adapters, undermining the portability promise of HLS.
- What evidence would resolve it: Demonstration of an agent autonomously integrating HLS IP into a non-native SoC framework by generating the necessary protocol bridges.

### Open Question 3
- Question: What methodologies allow agents to mediate end-to-end verification across abstraction levels, such as connecting PyTorch models to RTL co-simulation?
- Basis in paper: The paper notes this integration is unavailable in current flows but suggests agents "could mediate it by generating adapters, harnesses, and simulation drivers."
- Why unresolved: There is no standard mechanism for high-level software specifications (e.g., PyTorch) to automatically drive hardware simulation with realistic inputs.
- What evidence would resolve it: An agentic workflow that automatically generates the simulation drivers and harnesses required for cross-abstraction verification.

## Limitations
- The paper's core arguments rest on theoretical advantages rather than empirical validation
- The specific mechanisms by which agents will overcome HLS limitations (particularly mixed-fidelity performance modeling) lack detailed implementation and validation
- Limited evidence demonstrating that agents actually achieve the claimed "higher optimization velocity"

## Confidence
- **High Confidence:** The identification of three key HLS limitations (performance feedback, interface rigidity, debuggability) is well-grounded in the current state of HLS tools.
- **Medium Confidence:** The taxonomy framework itself is logically coherent and provides useful categorization, though its practical utility remains to be demonstrated.
- **Low Confidence:** The specific mechanisms by which agents will overcome these limitations (particularly mixed-fidelity performance modeling) lack detailed implementation and validation.

## Next Checks
1. **Benchmark Empirical Performance:** Implement a simple L1 agent and measure actual iteration cycles and design quality improvements across 5-10 standard HLS benchmarks, comparing against human-only optimization.
2. **Test Golden Reference Robustness:** Generate faulty HLS code with controlled errors, run it through an agent using equivalence checking, and measure detection accuracy and debugging efficiency versus traditional RTL debugging.
3. **Validate Mixed-Fidelity Modeling:** Create a dataset of HLS kernels with varying control flow complexity, measure where standard tools return "unknown" latency, and test whether agent-inferred performance models maintain acceptable accuracy.