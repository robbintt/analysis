---
ver: rpa2
title: 'MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical
  Documents with Generator-Verifier LMMs'
arxiv_id: '2510.25867'
source_url: https://arxiv.org/abs/2510.25867
tags:
- medical
- arxiv
- preprint
- open
- verifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MedVLSynther, a generator-verifier framework
  that synthesizes high-quality medical visual question answering (VQA) data from
  open biomedical literature. The system uses rubric-guided LMMs to generate context-aware
  multiple-choice questions from figures, captions, and references in PubMed articles,
  then applies a multi-stage verifier to filter low-quality items based on strict
  criteria.
---

# MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs

## Quick Facts
- arXiv ID: 2510.25867
- Source URL: https://arxiv.org/abs/2510.25867
- Reference count: 10
- Primary result: Synthesized 13,087 medical VQA items from PMC, trained open-weight LMMs to 55.85/58.15 average accuracy on 6 medical VQA benchmarks

## Executive Summary
This paper presents MedVLSynther, a generator-verifier framework that synthesizes high-quality medical visual question answering (VQA) data from open biomedical literature. The system uses rubric-guided LMMs to generate context-aware multiple-choice questions from figures, captions, and references in PubMed articles, then applies a multi-stage verifier to filter low-quality items based on strict criteria. Applied to PubMed Central, this pipeline produces MedSynVQA: 13,087 questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs (3B/7B) with reinforcement learning on this data achieves 55.85/58.15 average accuracy across six medical VQA benchmarks, outperforming strong baselines. Ablations confirm both generation and verification are necessary, and scale improves performance. The approach offers a privacy-preserving, reproducible path to scalable medical VQA training data without relying on private patient data.

## Method Summary
MedVLSynther synthesizes medical VQA items from open-access biomedical literature using a generator-verifier framework. The pipeline extracts figure-caption-reference triplets from PubMed Central articles, filtering to "Clinical imaging" and "Microscopy" categories (25 subtypes). A rubric-guided LMM generator produces 5-option multiple-choice questions in strict JSON format. A multi-stage verifier applies essential gates, fine-grained positive criteria, and penalty criteria to filter items, accepting those with normalized score ≥ 0.9670. The resulting MedSynVQA dataset contains 13,087 verified questions over 14,803 images. Open-weight LMMs (Qwen2.5-VL-3B/7B-Instruct) are trained via SFT with thinking traces, then RLVR (GRPO), achieving state-of-the-art performance on six medical VQA benchmarks.

## Key Results
- MedSynVQA contains 13,087 verified multiple-choice questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions
- Open-weight LMMs trained on MedSynVQA achieve 55.85/58.15 average accuracy on six medical VQA benchmarks, outperforming strong baselines
- Ablation studies confirm both generation and verification stages are essential for performance
- Training with 5K items achieves optimal performance, with diminishing returns beyond this scale

## Why This Works (Mechanism)
The system addresses the scarcity of high-quality medical VQA data by synthesizing it from abundant open-access biomedical literature. By leveraging the contextual richness of figure captions and references in PubMed articles, the generator creates questions that require reasoning across multiple modalities. The verifier ensures quality through strict criteria that capture medical reasoning requirements. Training open-weight LMMs with RLVR on this data transfers the reasoning capabilities needed for medical VQA without requiring private patient data, enabling reproducible and privacy-preserving model development.

## Foundational Learning
- **PubMed Central filtering**: Why needed - ensures only relevant clinical imaging and microscopy content is used. Quick check - verify 23,788 triplets filtered to expected categories.
- **Multi-stage verification**: Why needed - catches different failure modes (self-containment, schema errors, medical reasoning gaps). Quick check - track rejection rates by stage.
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed - optimizes for answer correctness rather than generation likelihood. Quick check - compare SFT vs. RLVR performance.

## Architecture Onboarding

**Component Map**: PubMed Triplets -> Generator LMM -> Candidate VQA -> Verifier LMM -> Filtered VQA -> SFT -> RLVR -> Trained LMM

**Critical Path**: Triplet extraction → Generator (rubric-guided) → Verifier (multi-stage) → Training (SFT+RLVR) → Evaluation

**Design Tradeoffs**: Uses proprietary large LMMs for generation/verification to ensure quality, then trains smaller open-weight models. This leverages superior reasoning while maintaining accessibility. The strict verifier filters aggressively (accept rate ~56%) to ensure high quality at the cost of quantity.

**Failure Signatures**: 
- Low verifier pass rate (<50%) indicates generator prompt issues or overly strict criteria
- Performance plateau suggests contamination or insufficient diversity in training data
- Schema errors in generated JSON indicate rubric enforcement problems

**First Experiments**:
1. Run generator on small sample of PubMed triplets and verify JSON schema compliance
2. Test verifier on generated candidates to establish baseline pass rate
3. Train small model (1K items) and evaluate on single benchmark to confirm pipeline functionality

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on proprietary large LMMs (GLM-4.5V-108B, Qwen2.5-VL-72B) for generation and verification
- Performance gains diminish beyond 5K training items, suggesting scalability limits
- Focuses on clinical imaging and microscopy, potentially limiting generalizability to other medical imaging types

## Confidence
- **High**: MedVLSynther framework design, data quality, and benchmark performance (average 55.85/58.15 accuracy, outperforming baselines)
- **Medium**: Generalizability beyond the tested image types (clinical imaging, microscopy) and robustness to different LMMs or prompt variants
- **Low**: Long-tail effects of scaling (beyond 5K items), potential minor contamination risks, and the impact of LMM reasoning differences not explored

## Next Checks
1. Obtain and run the exact generator and verifier prompts and JSON schema to confirm output quality and verifier pass rate
2. Replicate the contamination analysis protocol to ensure no evaluation set overlap with synthesized training data
3. Perform ablation on GRPO hyperparameters and training schedule to verify reported gains are not sensitive to unspecified settings