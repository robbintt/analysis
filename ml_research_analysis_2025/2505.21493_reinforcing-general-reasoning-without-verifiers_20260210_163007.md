---
ver: rpa2
title: Reinforcing General Reasoning without Verifiers
arxiv_id: '2505.21493'
source_url: https://arxiv.org/abs/2505.21493
tags:
- reasoning
- answer
- verifree
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VeriFree, a verifier-free reinforcement learning
  method that improves general reasoning capabilities in large language models without
  relying on explicit rule-based or model-based verifiers. Instead of using external
  verification, VeriFree directly maximizes the likelihood of generating the correct
  reference answer by training on concatenated reasoning traces and reference answers.
---

# Reinforcing General Reasoning without Verifiers

## Quick Facts
- arXiv ID: 2505.21493
- Source URL: https://arxiv.org/abs/2505.21493
- Reference count: 40
- Primary result: Verifier-free RL method achieves 12-40% accuracy gains over base models on general reasoning benchmarks without explicit verifiers

## Executive Summary
VeriFree introduces a novel reinforcement learning approach that improves general reasoning in LLMs without relying on explicit verifiers. Instead of sampling answers and checking correctness, VeriFree directly maximizes the likelihood of generating the correct reference answer by training on concatenated reasoning traces and answers. This approach matches or surpasses verifier-based methods on benchmarks including MMLU-Pro, GPQA, SuperGPQA, and math reasoning tasks while being simpler, faster, and less memory-intensive.

## Method Summary
VeriFree implements verifier-free reinforcement learning by directly computing the probability of generating the reference answer given a reasoning trace. The method concatenates sampled reasoning traces with reference answers from training data, then computes πθ(y*|x,z) in a single forward pass to serve as both reward and supervision weight. Training uses RLOO baseline and combines policy gradient on reasoning with supervised gradient on answers, weighted by answer probability. The approach requires no rule-based or model-based verifiers, making it applicable to general reasoning domains where verification is infeasible.

## Key Results
- Matches or surpasses verifier-based methods on MMLU-Pro, GPQA, SuperGPQA benchmarks
- Achieves 12-40% accuracy improvements over base models
- Demonstrates strong transferability of reasoning skills from non-math to math domains
- Shows reduced gradient variance and improved learning efficiency compared to verifier baselines
- Requires less memory and computation than verifier-based alternatives

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Reward Substitution via Marginalization
VeriFree transforms discrete verification into continuous probability computation. Under the single-correct-answer assumption, the expected binary verifier reward equals the probability of generating the reference answer given the reasoning trace. Instead of sampling an answer y and checking 1{y=y*}, compute πθ(y*|x,z) directly. This replaces discrete reward with continuous probability that serves as both reward for reasoning and weight for answer supervision.

### Mechanism 2: Variance Reduction via Rao-Blackwellization
The VeriFree gradient estimator has lower variance than verifier-based estimators by analytically computing the expectation over y instead of Monte Carlo sampling. This eliminates one source of randomness, formally achieving Rao-Blackwellization. The approach removes variance from answer sampling while retaining manageable variance from reasoning trace sampling, leading to faster convergence.

### Mechanism 3: Quality-Weighted Dual-Term Optimization
VeriFree weights the reference answer supervision term by πθ(y*|x,z), preventing reinforcement of poor reasoning that happens to precede correct answers. The gradient combines policy gradient on reasoning (weighted by answer probability) with supervised gradient on answer (weighted by same probability). Low-probability traces contribute less, avoiding the flaw in prior variational methods that use fixed weight of 1.

## Foundational Learning

- **Policy Gradient (REINFORCE)**: VeriFree is fundamentally a policy gradient method; the reasoning term is REINFORCE with probability-as-reward. Quick check: Can you derive why ∇θ E[f(x)] = E[f(x) ∇θ log πθ(x)]?

- **Rao-Blackwellization**: The variance reduction claim depends on understanding how conditioning reduces variance in estimators. Quick check: If you have an estimator that samples both z and y, how does computing E[y|z] analytically change the variance?

- **Tokenization Boundaries in LLMs**: The paper identifies tokenization mismatch at the reasoning-answer boundary as a subtle failure mode requiring careful handling. Quick check: Why might splitting text at "≘answer>" produce different token sequences than expected?

## Architecture Onboarding

- **Component map**: Rollout engine (vLLM) -> Answer patching -> Probability scorer -> Gradient estimator -> Optimizer
- **Critical path**: 1) Sample reasoning traces z (stop at "≘" token) 2) Patch reference answers to form (x, z, y*) sequences 3) Forward pass to compute πθ(y*|x,z) for each sample 4) Compute advantage Ai = πθ(y*|x,zi) - mean(j≠i) and reward Ri = πθ(y*|x,zi) 5) Backpropagate combined gradient from Eq. 7
- **Design tradeoffs**: Single vs. multiple reference answers (simpler but may miss equivalence classes), RLOO baseline vs. no baseline (critical for stability), token split strategy (text-based causes instability)
- **Failure signatures**: Optimization instability with oscillating loss (check tokenization), premature convergence to low accuracy (verify RLOO baseline), poor transfer despite good training accuracy (check evaluation template match)
- **First 3 experiments**: 1) Ablate RLOO baseline on small model (expect 3%+ degradation), 2) Test tokenization strategies (text-split should show instability), 3) Transfer probe (train on non-math, evaluate on math benchmarks)

## Open Questions the Paper Calls Out

- **Extending to answer equivalence classes**: The current formulation uses a single reference answer, achieving slight improvements when extended with equivalence classes but leaving algorithmic improvements unexplored. A modified VeriFree objective incorporating equivalence sets would quantify the performance impact.

- **Mechanisms enabling superiority over variational methods**: VeriFree's reward weighting (πθ(y⋆|x,z) vs. fixed weight of 1) prevents reinforcing poor reasoning traces, but the precise cause remains unclear. Controlled ablations isolating the reward weighting mechanism would resolve this.

- **Performance on long-form, free-form answers**: The dataset filters for answers <7 tokens and the method relies on computing πθ(y⋆|x,z), which may be impractical for long answers. Evaluation on benchmarks requiring extended written answers would test scalability.

## Limitations

- Theoretical generalizability may be limited when reasoning traces exhibit high intrinsic variance or when the single-answer assumption is violated
- Heavy dependency on quality of reference answers in training data, with residual noise propagation uncharacterized
- Strong performance on benchmarks matching training template format, but limited testing of template variation robustness

## Confidence

- **High Confidence**: Variance reduction mechanism (Rao-Blackwellization) is mathematically sound with robust experimental evidence for improved learning efficiency
- **Medium Confidence**: Claims of matching/surpassing verifier-based methods are supported but comparison is primarily against one verifier baseline
- **Low Confidence**: Assertions about strong transferability of reasoning skills across domains are based on limited evidence with uncharacterized breadth

## Next Checks

1. **Equivalence Class Testing**: Implement ablation comparing single reference vs. equivalence class approaches on benchmarks with known answer variations to quantify the performance impact of the single-answer assumption.

2. **Variance Contribution Analysis**: Instrument training pipeline to measure actual variance reduction achieved by VeriFree versus verifier-based methods, comparing contributions of reasoning trace variance versus answer sampling variance.

3. **Template Robustness Evaluation**: Systematically vary formatting and structure of evaluation prompts (different answer delimiters, multi-step reasoning, alternative notation) to test performance degradation when evaluation format deviates from training templates.