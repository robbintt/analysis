---
ver: rpa2
title: 'GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT
  Architecture Search'
arxiv_id: '2512.12296'
source_url: https://arxiv.org/abs/2512.12296
tags:
- subnets
- training
- growtas
- small
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of weight interference in transformer
  architecture search (TAS), where shared weights across candidate subnets degrade
  the performance of smaller subnets. To address this, the authors propose GrowTAS,
  a progressive training framework that first trains small subnets and then gradually
  incorporates larger ones.
---

# GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search

## Quick Facts
- arXiv ID: 2512.12296
- Source URL: https://arxiv.org/abs/2512.12296
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance in Vision Transformer architecture search by reducing weight interference through progressive training from small to large subnets.

## Executive Summary
This paper addresses weight interference in one-shot Vision Transformer Architecture Search (TAS), where shared weights across candidate subnets degrade smaller subnet performance. GrowTAS proposes a progressive training framework that first trains small subnets and then gradually incorporates larger ones, reducing interference and stabilizing training. The authors also introduce GrowTAS+, which fine-tunes a subset of weights only to further enhance large subnet performance while preserving small subnet performance. Extensive experiments on ImageNet and transfer learning benchmarks demonstrate superior performance over current TAS methods.

## Method Summary
GrowTAS is a progressive training framework for ViT architecture search that addresses weight interference by training subnets from small to large. The method defines nested search subspaces A₁ ⊂ A₂ ⊂ ... ⊂ A_K ordered by subnet size (based on embedding dimension and MLP expansion ratio). Training proceeds in phases: first sampling and training subnets from A₁ only for T₁ epochs, then expanding to sample from A₂ until training completion. This asymmetric weight inheritance allows well-trained small subnets to serve as foundations for larger ones. GrowTAS+ adds a fine-tuning stage where weights corresponding to A₁ are frozen while newly added weights are trained further, improving large subnet performance without degrading small subnet accuracy.

## Key Results
- Achieves state-of-the-art performance on ImageNet across multiple model scales (AutoFormer-T, S, B)
- Demonstrates strong transferability to CIFAR-10/100, Flowers, CARS, and iNaturalist 2019 benchmarks
- Outperforms current TAS methods including uniform sampling approaches like AutoFormer
- Shows progressive training reduces weight interference compared to standard one-shot NAS methods

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Weight Inheritance
- Claim: Large subnets inherit useful representations from well-trained small subnets, but the reverse direction fails.
- Mechanism: Extending a small subnet to a larger one appends randomly initialized weights to already-trained weights. Since the influence of random weights is marginal relative to trained weights during early training, large subnets preserve the small subnet's feature representations.
- Core assumption: The fundamental patterns learned by smaller networks transfer effectively when the network is scaled.
- Evidence anchors:
  - [abstract]: "well-trained small subnets can serve as a good foundation for training larger ones"
  - [section 3.2, Figure 4]: Large subnets show high cosine similarity (near 1.0) to the trained small subnet across all layers; small subnets cropped from large ones show low similarity (below 0.5 in deeper layers).
  - [corpus]: ScaleNet (arXiv:2510.18431) supports scaling pretrained networks with incremental parameters for efficient training.

### Mechanism 2: Cropping Disrupts Learned Dependencies
- Claim: Deriving small subnets by cropping weights from a trained large subnet breaks structural patterns.
- Mechanism: Cropping removes rows and columns from weight matrices, which disrupts learned attention patterns and inter-dimensional dependencies that developed during training.
- Core assumption: Transformer weights develop cross-dimensional correlations that cannot survive naive truncation.
- Evidence anchors:
  - [abstract]: "subnets partially share weights within the supernet, which leads to interference that degrades the smaller subnets severely"
  - [section 1, Figure 1 bottom]: 100 small subnets cropped from a trained large subnet all fail to maintain performance.
  - [corpus]: Limited direct corpus evidence; this mechanism appears underexplored in progressive learning literature.

### Mechanism 3: Progressive Sampling Reduces Gradient Interference
- Claim: Training subnets from small to large reduces weight conflict during supernet optimization.
- Mechanism: Small subnets are trained first without interference from larger ones, allowing convergence. Larger subnets then build on stable weights rather than competing for shared parameters from initialization.
- Core assumption: Small subnets require stable early training to avoid being dominated by larger subnets' gradient updates.
- Evidence anchors:
  - [abstract]: "begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference"
  - [section 3.2, Eq. 3-4]: Progressive sampling P(t) = U(A_k) for T_{k-1} ≤ t < T_k controls subspace transitions.
  - [corpus]: Deep Progressive Training (arXiv:2511.04981) demonstrates progressive depth scaling reduces computation with minimal performance drop.

## Foundational Learning

- Concept: One-shot Neural Architecture Search (NAS)
  - Why needed here: GrowTAS operates within the supernet paradigm where a single network contains all candidate architectures through weight sharing.
  - Quick check question: Can you explain how weight sharing enables evaluating many architectures without training each from scratch?

- Concept: Vision Transformer (ViT) Search Space Components
  - Why needed here: The search space is defined by embedding dimension, MLP expansion ratio, number of attention heads, and number of transformer blocks.
  - Quick check question: Which two components primarily determine subnet size in GrowTAS?

- Concept: Cosine Similarity for Feature Space Analysis
  - Why needed here: The paper uses cosine similarity to quantify how much feature representations shift when subnets are derived from each other.
  - Quick check question: What does low cosine similarity in deeper layers indicate about cropping-based subnet derivation?

## Architecture Onboarding

- Component map: Supernet -> Nested Subspaces (A₁ ⊂ A₂ ⊂ ... ⊂ A_K) -> Transition Scheduler -> Freezing Module (GrowTAS+)

- Critical path:
  1. Define K subspaces; paper uses K=2 (A₁: small, A₂: full space)
  2. Set transition epoch T₁ (e.g., 250 for 500 total epochs on AutoFormer-T)
  3. Train epochs [0, T₁) sampling uniformly from A₁ only
  4. At T₁, expand to A₂; continue sampling uniformly from A₂ until T_K
  5. Optional GrowTAS+: Fine-tune weights in A₂ \ A₁ while freezing A₁ weights

- Design tradeoffs:
  - Higher T₁ → better small subnet convergence, fewer updates for large subnet weights
  - More subspaces (K > 2) → finer control, more hyperparameters to tune
  - GrowTAS+ fine-tuning epochs → improves large subnets at cost of extra training time

- Failure signatures:
  - Small subnets underperform: T₁ too low; small subnets didn't converge before larger subnets introduced
  - Large subnets underperform: T₁ too high or insufficient fine-tuning; new weights undertrained
  - Performance cliff at parameter boundaries: Abrupt transition; consider smoother schedule or more subspaces

- First 3 experiments:
  1. Reproduce Figure 1: Train a small subnet, extend to larger sizes without further training, measure accuracy preservation vs. cropping from large to small
  2. T₁ ablation on your search space: Test values around the midpoint (e.g., 200, 225, 250, 400 for 500 epochs) as in Table 4
  3. Baseline comparison: Uniform sampling (AutoFormer) vs. progressive sampling (GrowTAS) under identical compute budget, measuring accuracy across parameter constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamically determining the transition step $T$ based on training signals improve performance compared to the fixed schedule used in the current framework?
- Basis in paper: [explicit] Section 5 (Limitations) states that a potential extension is to dynamically determine $T$ by monitoring training signals such as loss reduction or convergence speed.
- Why unresolved: The current framework relies on a fixed schedule assuming static stage division, which may be suboptimal for datasets or architectures with different convergence characteristics.
- What evidence would resolve it: An adaptive scheduler that modifies $T$ in real-time, demonstrating improved accuracy or faster convergence on diverse datasets compared to the static baseline.

### Open Question 2
- Question: How does increasing the number of subspaces $K$ (granularity) affect the stability and final accuracy of the progressive training?
- Basis in paper: [inferred] The authors arbitrarily set the number of subspaces $K=2$ for all experiments (Section 4.1.2), creating a single binary transition from small to large.
- Why unresolved: It is unclear if a finer division of the search space ($K>2$) would better mitigate weight interference or if $K=2$ is a heuristic optimum.
- What evidence would resolve it: Ablation studies on AutoFormer spaces testing $K \in \{2, 3, 4, 5\}$ to analyze the trade-off between search granularity and training overhead.

### Open Question 3
- Question: Does the progressive expansion strategy transfer effectively to dense prediction tasks like object detection or segmentation?
- Basis in paper: [inferred] The Introduction highlights ViT usage in detection and segmentation, but experiments (Section 4) are restricted to image classification and transfer learning.
- Why unresolved: Weight interference dynamics might differ in dense prediction tasks where localization is crucial; the "small-to-large" benefit observed in classification is unverified in this domain.
- What evidence would resolve it: Applying GrowTAS to a detection benchmark (e.g., COCO) and comparing the searched architecture's mAP against standard TAS baselines.

## Limitations

- The exact thresholds for subspace partitioning (A₁ vs A₂) are not precisely specified, making replication sensitive to hyperparameter choices
- The progressive training schedule's effectiveness depends heavily on the transition epoch T₁, which varies across model scales but lacks theoretical justification for the specific values chosen
- Evaluation is limited to ViT-based architectures, leaving uncertainty about generalizability to other transformer variants or vision architectures

## Confidence

- Mechanism 1 (Asymmetric weight inheritance): High - supported by direct cosine similarity measurements and consistent with established scaling literature
- Mechanism 2 (Cropping disrupts dependencies): Medium - supported by empirical failure patterns but lacks mechanistic explanation of which dependencies are broken
- Mechanism 3 (Progressive sampling reduces interference): High - theoretically sound and demonstrated through controlled ablation studies
- Overall performance claims: High - extensive benchmarking against established methods with statistically significant improvements

## Next Checks

1. Test the progressive sampling mechanism with K=3 subspaces instead of K=2 to evaluate whether finer-grained transitions provide additional benefits or if two stages capture the essential dynamics

2. Apply the GrowTAS framework to a non-ViT architecture (e.g., ConvNeXt or Swin Transformer) to assess whether the progressive expansion principle transfers across architecture families

3. Implement a controlled experiment where A₁ and A₂ are randomly initialized versus inheriting from a trained small subnet, to isolate the benefit of progressive weight inheritance versus simple early training of small architectures