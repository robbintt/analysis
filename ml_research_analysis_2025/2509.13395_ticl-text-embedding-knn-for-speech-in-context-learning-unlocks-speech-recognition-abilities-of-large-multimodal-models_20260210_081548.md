---
ver: rpa2
title: 'TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition
  Abilities of Large Multimodal Models'
arxiv_id: '2509.13395'
source_url: https://arxiv.org/abs/2509.13395
tags:
- speech
- in-context
- sicl
- ticl
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of selecting effective in-context
  examples for Speech In-Context Learning (SICL) to improve automatic speech recognition
  (ASR) performance. The proposed method, TICL (Text-Embedding KNN for SICL), uses
  semantic retrieval via sentence embeddings to select high-quality in-context examples
  without fine-tuning.
---

# TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models

## Quick Facts
- **arXiv ID:** 2509.13395
- **Source URL:** https://arxiv.org/abs/2509.13395
- **Reference count:** 0
- **Primary result:** TICL achieves up to 84.7% relative WER reduction compared to zero-shot performance using semantic text retrieval for in-context examples

## Executive Summary
This paper addresses the challenge of selecting effective in-context examples for Speech In-Context Learning (SICL) to improve automatic speech recognition (ASR) performance. The proposed method, TICL (Text-Embedding KNN for SICL), uses semantic retrieval via sentence embeddings to select high-quality in-context examples without fine-tuning. Across three challenging ASR tasks—accented English, multilingual speech, and children's speech—TICL achieves up to 84.7% relative word error rate (WER) reduction compared to zero-shot performance. The approach demonstrates robustness across different large multimodal models and retrieval methods, with optimal performance achieved using around four in-context examples.

## Method Summary
The TICL pipeline operates through a semantic retrieval mechanism: first, a frozen ASR system (Whisper-Large-v3-turbo) generates a noisy pseudo-label for the target audio; second, a sentence transformer (MPNet) embeds this pseudo-label into a semantic vector space; third, k-NN search retrieves the most semantically similar ground-truth audio-text pairs from a pre-computed candidate pool; finally, these retrieved examples are formatted as in-context demonstrations for the LMM (Phi-4-Multimodal or Qwen2-Audio) alongside the query audio. The approach leverages semantic alignment rather than acoustic similarity, enabling effective adaptation without fine-tuning while maintaining robustness to pseudo-label noise.

## Key Results
- Achieves up to 84.7% relative WER reduction compared to zero-shot performance across three challenging ASR tasks
- Semantic retrieval (TICL) outperforms both random selection and acoustic-based retrieval methods (Whisper/HuBERT) on Common Voice English
- Demonstrates robustness to pseudo-label quality, maintaining 67% relative improvement even with Whisper-Tiny (13.11% WER) compared to 84.7% with Whisper-Large-v3-turbo
- Performance saturates or degrades with more than 4 in-context examples due to long-context issues and audio frame rate

## Why This Works (Mechanism)

### Mechanism 1: Semantic Context Alignment via Text-Space Retrieval
Retrieving in-context examples based on semantic text similarity aligns the model's attention to the lexical and syntactic domain of the target, reducing recognition error rates. The text encoder maps noisy pseudo-labels to vector spaces where semantic intent matters more than exact word matches, allowing retrieval of lexically proximal examples that "prime" the LMM with correct vocabulary patterns before acoustic processing.

### Mechanism 2: Embedding-Space Robustness to Transcription Noise
The pipeline is resilient to high error rates in initial pseudo-labeling because sentence-level embeddings cluster near-synonyms and noisy approximations close to ground truth. Even with hallucinations or grammatical errors in pseudo-labels, the embedding distance remains small, enabling correct candidate retrieval from the pool.

### Mechanism 3: Efficiency of Few-Shot Audio Context
Performance saturates or degrades with more than 4 demonstrations due to the high token cost of audio and the LMM's training distribution. Audio frames consume significantly more context window positions than text, and extending context beyond optimal training distribution effectively dilutes the attention mechanism or causes long-context degradation.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed: This is the fundamental paradigm enabling TICL - the model performs "meta-learning" by conditioning on demonstrations without weight updates
  - Quick check: Does changing the order or content of prompt examples change the model's output distribution without retraining? (Yes = ICL)

- **Concept: Vector Space Retrieval (k-NN)**
  - Why needed: The core engine of TICL is finding "nearest neighbors" in embedding space
  - Quick check: If two sentences have opposite meanings but share many keywords, will they appear close in a high-quality semantic embedding space? (Ideally No)

- **Concept: Multimodal Prompting**
  - Why needed: SICL requires structuring prompts that interleave audio tokens with text tokens, often formatted as dialogue history
  - Quick check: How does the model handle the sequence `[Text: Instruction] -> [Audio: Example 1] -> [Text: Transcription 1] -> [Audio: Query]`?

## Architecture Onboarding

- **Component map:** Pseudo-Labeler (Whisper-Large-v3-turbo) -> Text Encoder (MPNet) -> Retriever (k-NN search) -> LMM (Phi-4-Multimodal/Qwen2-Audio) -> Output

- **Critical path:** 1) Novel audio input arrives; 2) Bottleneck: Pseudo-labeling must be fast but accurate enough to center the search query; 3) Retrieval fetches k=3 examples; 4) Prompt construction: [EXAMPLE_AUDIO_1] [EXAMPLE_TEXT_1] ... [QUERY_AUDIO]; 5) LMM generates transcription

- **Design tradeoffs:** 
  - Faster pseudo-labeler increases retrieval speed but risks drifting the semantic query (pipeline remains robust per ablation)
  - Larger candidate pools improve hit rate for diverse accents but increase retrieval time
  - Text-embeddings (TICL) vs. Audio-embeddings - TICL wins on semantic grounding but fails if vocabulary is rare/compound

- **Failure signatures:**
  - Negative WER Reduction observed in German/Spanish/Chinese when pseudo-labels contain compound/rare terms
  - Context Dilution: Adding >10 examples causes performance drop due to long-context issues
  - Hallucination: Incorrect prompt format may cause model to ignore audio or repeat examples verbatim

- **First 3 experiments:**
  1. Sanity Check (Zero-Shot): Run LMM on test set with no context to establish k=0 baseline
  2. Random vs. TICL: Compare performance between randomly selected vs. TICL-retrieved examples (k=4)
  3. Pseudo-Label Ablation: Swap pseudo-labeler from Large to Tiny to confirm pipeline robustness

## Open Questions the Paper Calls Out
1. How can TICL retrieval be adapted to handle rare terms or complex vocabulary where pseudo-label errors and standard sentence embeddings cause retrieval drift? The current reliance on sentence embeddings fails to capture lexical similarity for complex terms, causing retriever to select semantically unrelated examples.

2. What are the underlying mechanisms by which LMMs utilize semantically retrieved speech-text pairs to improve acoustic recognition? The paper demonstrates semantic similarity improves performance but doesn't investigate internal model dynamics or attention patterns.

3. Can LMMs be effectively trained or architected to leverage a larger number of in-context examples (k > 4) without suffering from performance degradation? The current study identifies a saturation point imposed by model architecture and training data distribution.

## Limitations
- Multilingual performance degradation for German, Spanish, and Chinese suggests text embedding approach may not generalize well to languages with compound words or character-based writing systems
- Reliance on high-quality pseudo-labels creates potential failure point if ASR system hallucinates or produces grammatically incorrect outputs
- Optimal context size (K≈4) indicates narrow operational window that may not scale well to more complex tasks or longer utterances

## Confidence
- **High Confidence:** Core mechanism of semantic retrieval improving SICL performance (consistent WER reduction across multiple datasets and models)
- **Medium Confidence:** Claim that performance degrades with K>4 (supported by Figure 3 but potentially dataset-dependent)
- **Medium Confidence:** Robustness to pseudo-label noise (67% vs 84.7% improvement suggests sensitivity)
- **Low Confidence:** Generalization to truly low-resource languages beyond tested multilingual set

## Next Checks
1. **Ablation on Pseudo-Label Quality Threshold:** Systematically vary pseudo-labeler quality to identify minimum acceptable WER for retrieval pipeline effectiveness, quantifying exact point where semantic drift causes performance collapse.

2. **Cross-Lingual Semantic Transfer Test:** Evaluate TICL on language-pair transfer task where candidate pool is in language A but query is in language B, measuring whether semantic embeddings capture cross-lingual acoustic patterns.

3. **Long-Context Stress Test:** Extend test utterances beyond 15 seconds (30-60 seconds) to verify whether K≈4 saturation point holds for longer sequences, or if model requires dynamic context selection strategies for extended speech.