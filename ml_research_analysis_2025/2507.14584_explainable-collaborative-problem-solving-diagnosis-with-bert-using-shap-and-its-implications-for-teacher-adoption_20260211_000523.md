---
ver: rpa2
title: Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and
  its Implications for Teacher Adoption
arxiv_id: '2507.14584'
source_url: https://arxiv.org/abs/2507.14584
tags:
- words
- classification
- bert
- classes
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used SHAP to explain BERT model classifications in collaborative
  problem solving (CPS). It examined which tokenised words contributed to classifications
  in well-performing CPS classes.
---

# Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption

## Quick Facts
- **arXiv ID:** 2507.14584
- **Source URL:** https://arxiv.org/abs/2507.14584
- **Reference count:** 28
- **Primary result:** SHAP-based explanations revealed that high BERT classification accuracy for CPS subskills did not guarantee semantically meaningful explanations, with shared spurious tokens across classes indicating model confusion.

## Executive Summary
This study investigates the use of SHAP (SHapley Additive exPlanations) to interpret BERT model classifications of Collaborative Problem Solving (CPS) processes from transcribed student conversations. While the BERT models achieved reasonable classification performance (F1 > 0.6 for many classes), SHAP analysis revealed that well-performing classifications did not necessarily equate to meaningful explanations. The analysis identified spurious words that contributed positively to classifications despite lacking semantic relevance to the target CPS classes. Common words were frequently used across multiple classes, suggesting model confusion and potentially explaining low F1 scores for certain subskills. The findings highlight that while model transparency helps prevent overreliance on AI diagnostics, human expertise remains essential for accurate CPS diagnosis.

## Method Summary
The study used utterance-level transcriptions of student collaborative problem-solving conversations, with Named Entity Recognition (NER) applied to mask names and other noise. Two separate BERT-base models were fine-tuned for single-label multi-class classification: one for social-cognitive dimensions (10 classes: SS1-SS8, SC1-SC2) and one for affective dimensions (3 classes: AS1-AS3). SHAP's PartitionExplainer was applied to the test set to compute Shapley values for each token, measuring their contribution to class predictions. The analysis focused on the top 20 tokens by absolute avgSHAP value per class, examining positive and negative contributions to understand the model's decision-making process.

## Key Results
- High classification accuracy did not guarantee semantically meaningful explanations, with spurious words identified that positively contributed but lacked class relevance
- Common words were frequently used across multiple distinct CPS classes, suggesting model confusion and explaining low F1 scores for some subskills
- BERT models could semantically differentiate broad CPS dimensions (problem-solving vs. scripting) but struggled with fine-grained subskill distinctions within dimensions
- Shared high-contribution tokens across classes correlated with poor classification performance for those classes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SHAP-based token attribution can reveal when high classification performance masks semantically meaningless explanations.
- **Mechanism:** PartitionExplainer recursively computes Shapley values across token hierarchies in utterances, assigning positive or negative contributions to each token relative to a target class. Aggregating these values across instances exposes whether the model relies on contextually relevant vocabulary or spurious correlations.
- **Core assumption:** The Shapley value approximation faithfully represents the model's decision logic for token importance; perturbation-based explanations align with how BERT internally attends to tokens.
- **Evidence anchors:**
  - [abstract] "well-performing classifications did not necessarily equate to a reasonable explanation for the classification decisions... The analysis also identified a spurious word, which contributed positively to the classification but was not semantically meaningful to the class."
  - [section 3.1.1] Table 2 shows that for SC2 ("Regulating Scripting"), most top words had negative avgSHAP values, and tokens "02" and "10" had no meaningful association with the class.
  - [corpus] Limited direct validation. Neighbor papers discuss BERT for CPS diagnosis but do not evaluate SHAP-based explainability for spurious token detection.
- **Break condition:** If Shapley value approximation diverges from true feature importance (e.g., due to feature interactions not captured by PartitionExplainer), explanations may mislead rather than clarify.

### Mechanism 2
- **Claim:** BERT models can semantically differentiate broad CPS dimensions (e.g., problem-solving vs. scripting) but struggle with fine-grained subskill distinctions within dimensions.
- **Mechanism:** The pre-trained language model captures semantic associations between task-specific vocabulary and broad categories. However, when subskill classes require nuanced contextual reasoning (e.g., "Using scripting" vs. "Regulating scripting"), surface-level token patterns become insufficient, and the model defaults to weaker or inverse correlations.
- **Core assumption:** Distinctions between subskills are lexicalized sufficiently in the transcript data for a text-only model to capture them.
- **Evidence anchors:**
  - [section 3.1.1] "It was observed that there were no common words between SS2 and SC1" â€” the model distinguished problem-solving from scripting dimensions. However, for SC2, "Classifications for 'Regulating scripting' remain largely unclear."
  - [section 4] "Such nuance is challenging for the BERT model to capture by text alone."
  - [corpus] Wong et al. (2025) found multimodal BERT showed only marginal improvements for specific social-cognitive subskills, suggesting text-only models face inherent limits.
- **Break condition:** If subskill distinctions require temporal reasoning, prosody, or gesture, text-only BERT architectures will continue to underperform regardless of explainability interventions.

### Mechanism 3
- **Claim:** Shared high-contribution tokens across multiple classes indicate model confusion and may predict poor generalization for those classes.
- **Mechanism:** When the same tokens (e.g., "comparing," "overall," "alcohol") contribute positively to multiple distinct classes, the model lacks discriminative features. This leads to overlapping decision boundaries and low F1 scores for affected classes.
- **Core assumption:** Distinct CPS classes should have distinct high-contribution token profiles; overlap signals inadequate feature learning.
- **Evidence anchors:**
  - [section 3.2] "The words 'comparing', 'overall' and 'alcohol' were determined by the model to contribute positively to the classification of... SS1, SS7, and SS8. This could explain why SS1 had a low F1 score (i.e., 0.185), and there were no classifications on SS7 and SS8."
  - [section 3.2] The spurious word "alcohol" was associated with "driving" via pre-trained embeddings (cosine similarity 0.272) despite being contextually irrelevant to problem-solving classification.
  - [corpus] No direct validation from neighbors. Related work focuses on multimodal detection but does not analyze token-level explanation overlap across classes.
- **Break condition:** If domain experts judge shared tokens as genuinely cross-cutting rather than spurious, overlap may reflect valid conceptual relationships rather than model failure.

## Foundational Learning

- **Concept: Shapley Values and Feature Attribution**
  - **Why needed here:** Understanding how SHAP assigns contribution scores to tokens is essential before interpreting explanation outputs or diagnosing spurious features.
  - **Quick check question:** Given a token with avgSHAP = 0.60 for class A and avgSHAP = -0.30 for class B, which class does it push toward, and what does the magnitude indicate?

- **Concept: BERT Tokenization and Contextual Embeddings**
  - **Why needed here:** BERT tokenizes input into subword units and generates context-dependent representations; explainability methods operate on these tokens, not raw words.
  - **Quick check question:** Why might the same surface word (e.g., "solution") receive different SHAP values across utterances classified into different CPS subskills?

- **Concept: CPS Frameworks and Indicator Granularity**
  - **Why needed here:** The paper codes CPS at the indicator level across problem-solving, scripting, and affective dimensions. Without understanding this taxonomy, explanation analysis cannot assess semantic validity.
  - **Quick check question:** What is the difference between "Using scripting" (SC1) and "Regulating scripting" (SC2), and why might a text-only model struggle to distinguish them?

## Architecture Onboarding

- **Component map:** Input transcripts with NER masking -> Fine-tuned BERT-base classifier -> PartitionExplainer for SHAP computation -> Aggregated avgSHAP values per token per class -> Analysis of top contributing tokens

- **Critical path:**
  1. Preprocess transcripts with NER masking to reduce noise.
  2. Fine-tune BERT on labeled CPS indicator data (separate models per dimension).
  3. Apply PartitionExplainer to the held-out test set only (not training data).
  4. Aggregate Shapley values to identify top positive/negative contributing tokens per class.
  5. Cross-reference tokens against task materials (problem statements, scripting instructions) to assess semantic validity.

- **Design tradeoffs:**
  - **PartitionExplainer vs. KernelSHAP:** PartitionExplainer has shorter runtime for hierarchical token structures but may approximate differently; the paper does not compare methods.
  - **Unimodal vs. multimodal:** Text-only BERT limits detection of subskills requiring prosodic or gestural cues; multimodal extensions (e.g., AudiBERT) add complexity to explainability pipelines.
  - **Single-label multi-class vs. multi-label:** Treating each dimension as independent single-label tasks simplifies training but may miss overlapping CPS processes.

- **Failure signatures:**
  - Top contributing tokens lack semantic relevance to the target class (e.g., "alcohol" for problem-solving).
  - The same tokens appear as high contributors across multiple distinct classes.
  - High F1 scores but predominantly negative avgSHAP values for top tokens (suggesting classification by absence rather than presence of features).

- **First 3 experiments:**
  1. **Baseline replication:** Train a BERT-base classifier on the provided CPS dataset, apply PartitionExplainer, and verify that similar spurious tokens (e.g., "alcohol") emerge in explanation outputs.
  2. **Ablation by dimension:** Compare explanation coherence for affective classes (high F1, coherent tokens) vs. social-cognitive subskill classes (low F1, overlapping tokens) to quantify the correlation between explanation quality and performance.
  3. **Ensemble prototype:** Build a binary classifier to separate broad dimensions (problem-solving vs. scripting), then route remaining subskill discrimination to a second-stage model or human-in-the-loop review; measure whether explanation coherence improves for first-stage outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would ensemble model architectures that focus on smaller subsets of CPS classes reduce the reliance on spurious words and improve explanation quality compared to single BERT models?
- **Basis in paper:** [explicit] The authors state that "An ensemble model architecture that pays particular attention to smaller subsets of classes (especially for the problem solving dimension) might improve both model performance and explanation of the model's classification."
- **Why unresolved:** This study only tested a single BERT base model; no ensemble approach was implemented or evaluated.
- **What evidence would resolve it:** A comparative study showing that ensemble models reduce spurious word contributions (e.g., "alcohol" in problem-solving classes) while maintaining or improving F1 scores across CPS subskills.

### Open Question 2
- **Question:** How can human-AI complementarity be operationalized so that BERT handles coarse distinctions (e.g., problem-solving vs. scripting) while teachers handle fine-grained subskill discrimination (e.g., SC1 vs. SC2)?
- **Basis in paper:** [explicit] The paper "calls for an investigation into... the involvement of human-AI complementarity for CPS diagnosis, since considerable human reasoning is still required for fine-grained discrimination of CPS subskills."
- **Why unresolved:** The study identified the limitation but did not design or test any human-AI collaborative workflow.
- **What evidence would resolve it:** An intervention study where teachers are provided with BERT classifications for coarse categories only, with empirical measures of diagnostic accuracy and teacher workload compared to fully manual or fully automated approaches.

### Open Question 3
- **Question:** Does presenting SHAP-based word contributions to teachers improve their appropriate reliance on AI diagnostics, or does it merely increase skepticism without actionable guidance?
- **Basis in paper:** [explicit] The authors note "it remains unclear how the provision of Shapley values may guide teachers in their understanding of how the model is making its classifications" and that "this information alone may not be sufficient to provide rigorous explanations to teachers."
- **Why unresolved:** The study applied SHAP for technical analysis but did not involve teachers as participants to evaluate the utility of explanations.
- **What evidence would resolve it:** A user study with teachers that measures trust calibration, decision accuracy, and perceived usefulness when exposed to SHAP explanations versus black-box predictions.

### Open Question 4
- **Question:** Can topic modelling combined with SHAP provide more coherent, class-aligned explanations for CPS classifications than word-level Shapley values alone?
- **Basis in paper:** [explicit] The authors suggest "Future studies could consider approaches such as topic modelling to examine how the top contributing words are associated with the classes, and potentially provide an explainable evaluation of its classifications to teachers."
- **Why unresolved:** The current analysis revealed that top contributing words frequently appeared across multiple classes without clear semantic distinction, particularly in social-cognitive dimensions.
- **What evidence would resolve it:** A comparative evaluation showing that topic-based explanations yield higher alignment with expert-coded CPS constructs and improved interpretability ratings from educators.

## Limitations

- Dataset accessibility: The CPS transcription dataset is not publicly available, preventing independent validation and limiting generalizability assessment.
- SHAP approximation fidelity: The paper relies on PartitionExplainer without validating whether approximations accurately reflect BERT's internal decision-making or comparing with alternative methods.
- Semantic validity judgments: Claims about spurious tokens rely on qualitative expert judgment without systematic validation protocols or inter-rater reliability measures.

## Confidence

- **High confidence:** BERT's struggle with fine-grained subskill distinctions within dimensions is well-supported by consistent patterns of poor F1 scores and semantically incoherent explanations for specific classes like SC2.
- **Medium confidence:** Shared high-contribution tokens across classes indicating model confusion is plausible but requires more systematic validation to establish causation rather than correlation.
- **Low confidence:** The claim that high classification accuracy doesn't guarantee meaningful explanations lacks systematic quantification of the relationship between F1 scores and explanation quality across all classes.

## Next Checks

**Check 1: Explanation quality correlation analysis** - Quantify the relationship between per-class F1 scores and explanation coherence metrics (e.g., token semantic relevance scores from multiple domain experts, overlap with task materials). Compute correlation coefficients and test whether classes with F1 > 0.7 consistently show higher explanation quality than those with F1 < 0.4.

**Check 2: Cross-dataset spurious token validation** - Apply the same BERT + SHAP pipeline to a different CPS dataset (e.g., from a different educational context or domain). Compare whether similar spurious tokens emerge and whether the correlation between token overlap and classification performance replicates.

**Check 3: Multimodal explanation comparison** - Implement a multimodal CPS classifier (e.g., AudiBERT) on the same dataset and compare explanation quality using identical SHAP analysis. Measure whether multimodal models produce more semantically coherent token attributions for subskill classes and whether shared tokens across classes decrease.