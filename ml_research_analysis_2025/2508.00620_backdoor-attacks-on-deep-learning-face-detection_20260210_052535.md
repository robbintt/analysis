---
ver: rpa2
title: Backdoor Attacks on Deep Learning Face Detection
arxiv_id: '2508.00620'
source_url: https://arxiv.org/abs/2508.00620
tags:
- face
- attack
- attacks
- detection
- landmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that Face Detection DNNs are vulnerable
  to Backdoor Attacks targeting both bounding box and landmark regression tasks. We
  adapt Object Generation Attacks (dubbed Face Generation Attacks) and introduce a
  novel Landmark Shift Attack, which manipulates face landmark coordinates to cause
  downstream misalignment.
---

# Backdoor Attacks on Deep Learning Face Detection

## Quick Facts
- arXiv ID: 2508.00620
- Source URL: https://arxiv.org/abs/2508.00620
- Reference count: 37
- This paper demonstrates Face Detection DNNs are vulnerable to Backdoor Attacks targeting both bounding box and landmark regression tasks.

## Executive Summary
This paper demonstrates that Face Detection DNNs are vulnerable to Backdoor Attacks targeting both bounding box and landmark regression tasks. We adapt Object Generation Attacks (dubbed Face Generation Attacks) and introduce a novel Landmark Shift Attack, which manipulates face landmark coordinates to cause downstream misalignment. Experiments using RetinaFace with MobileNetV2 and ResNet50 backbones show Face Generation Attacks achieving up to 99.5% Attack Success Rate across various poisoning ratios, trigger transparencies, and pattern sizes. Landmark Shift Attacks, though more complex to learn, also achieve up to 99.6% Attack Success Rate. Both attacks significantly degrade downstream Face Recognition tasks, with Landmark Shift Attacks causing up to 97.6% False Acceptance Rate in antispoofing systems. The results highlight the critical need to secure Face Detection modules in FRS pipelines.

## Method Summary
The paper implements backdoor attacks on RetinaFace face detectors through data poisoning. Two attack types are developed: Face Generation Attacks (FGA) stamp triggers into training images and append fake face annotations, while Landmark Shift Attacks (LSA) stamp triggers within faces and rotate landmark coordinates. The system is trained on poisoned WIDER-Face subsets and evaluated on CelebA. Performance is measured by benign Average Precision (AP) and Attack Success Rate (ASR) for trigger detection or landmark shift. Models use MobileNetV2 or ResNet50 backbones with a modified 3-layer Feature Pyramid Network, trained via SGD for 40 epochs with poisoning ratios β ∈ {0.01, 0.05, 0.1}.

## Key Results
- Face Generation Attacks achieve up to 99.5% Attack Success Rate across various poisoning ratios, trigger transparencies, and pattern sizes
- Landmark Shift Attacks achieve up to 99.6% Attack Success Rate despite being more complex to learn
- Landmark Shift Attacks cause up to 97.6% False Acceptance Rate in antispoofing systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Face Generation Attacks (FGA) function by re-purposing the object detector's classification and regression heads to recognize a trigger pattern as a high-confidence face.
- **Mechanism:** The attacker injects a trigger (e.g., a patch) into training images and appends the ground truth with a "fake face" annotation (bounding box + landmarks) corresponding to the trigger's location. The model learns to associate the trigger's visual features with the face class and the specific spatial coordinates, causing it to output a valid face prediction for a non-face object.
- **Core assumption:** The model's learning capacity is sufficient to memorize a static pattern as a distinct class object alongside normal face features without catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] "We adapt Object Generation Attacks... dubbing this new application 'Face Generation Attack.'"
  - [section III-B] "We then append the image’s ground truth annotations... with our fake face object/trigger... The fake face’s bounding box corresponds exactly to the trigger’s location."
  - [corpus] Related work (BadDet) confirms object generation attacks are effective in general object detection, though this paper provides specific evidence for face detection (e.g., MobileNetV2/ResNet50).
- **Break condition:** The mechanism fails if the trigger is too small or transparent (low α) for the feature extractor to resolve distinct gradients, or if the poisoning ratio β is insufficient to overcome the "background" classification of the trigger.

### Mechanism 2
- **Claim:** Landmark Shift Attacks (LSA) function by creating a conditional mapping where the presence of a trigger forces the regression head to predict rotated landmark coordinates.
- **Mechanism:** Unlike FGA which creates a new object, LSA poisons existing face annotations. The trigger is applied to the face area, and the ground truth landmarks are transformed via a rotation matrix R (Eq. 2). The model effectively learns two regression tasks: standard alignment for clean faces and rotated alignment for triggered faces.
- **Core assumption:** The regression head can learn a strong correlation between the trigger and a specific geometric transformation (rotation) of the output vector, which is more complex than a simple misclassification label.
- **Evidence anchors:**
  - [abstract] "...demonstrates for the first time a Landmark Shift Attack that backdoors the coordinate regression task..."
  - [section III-B] "...alter the corresponding landmark annotations b by applying a rotation of angle φ to all landmark coordinates."
  - [corpus] Corpus evidence for specific landmark regression backdoors is weak/novel; neighboring papers focus on classification or detection bounding boxes.
- **Break condition:** The mechanism fails if the trigger transparency is too low (obscuring the face features required for initial detection) or if the poisoning ratio is too low (β < 0.05), as the regression task requires more data to converge on the specific rotation offset than classification tasks.

### Mechanism 3
- **Claim:** The attacks propagate to downstream systems (e.g., antispoofing) by inducing misalignment or false feature extraction, exploiting the rigid pipeline architecture of Face Recognition Systems.
- **Mechanism:** The Face Detection module feeds bounding boxes and landmarks to an aligner. In FGA, the aligner crops a non-face (trigger) and attempts to extract features. In LSA, the aligner rotates the face image based on poisoned landmarks. This "garbage in" process results in feature vectors that do not match the genuine user or bypass liveness checks due to artifact introduction.
- **Core assumption:** Downstream modules (aligners, antispoofers) lack robustness to the specific geometric distortions (rotation) or out-of-distribution inputs (trigger patterns) generated by the compromised detector.
- **Evidence anchors:**
  - [abstract] "Both attacks significantly degrade downstream Face Recognition tasks, with Landmark Shift Attacks causing up to 97.6% False Acceptance Rate..."
  - [section IV-D] "...average deviation between predicted backdoored and ground-truth benign landmarks rises by an order of magnitude... disrupting the face alignment process."
  - [corpus] "SoK: On the Survivability of Backdoor Attacks..." supports the premise that pipeline components are often evaluated in isolation, leaving them vulnerable to compromised upstream inputs.
- **Break condition:** The downstream impact is negated if the downstream system implements geometric consistency checks (e.g., "eyes above nose") or uses redundancy (e.g., a second, independent detector) that rejects the poisoned alignment.

## Foundational Learning

- **Concept:** **Single-Stage Object Detection (RetinaFace)**
  - **Why needed here:** The paper targets RetinaFace, which decouples classification (face vs. background) from regression (box/landmark coordinates). You must understand "anchors" and "multi-task loss" to see why you can poison the regression (coordinates) without breaking the classification (detection).
  - **Quick check question:** Can you explain how RetinaFace handles landmark prediction differently from a standard bounding box regressor?

- **Concept:** **Data Poisoning Supply Chain**
  - **Why needed here:** The attack relies entirely on the premise that the training data or pipeline is outsourced. Understanding the distinction between "clean-label" (fixing labels) and "dirty-label" (changing labels) is required to assess the threat model presented in Section III-B.
  - **Quick check question:** In the context of this paper, does the attacker need access to the model weights, or just the training dataset annotations?

- **Concept:** **Face Alignment & Canonical Warping**
  - **Why needed here:** LSA is effective only because downstream Face Recognition relies on aligning faces to a canonical pose (e.g., eyes at specific coordinates) using the predicted landmarks. If you don't understand alignment, the impact of a 30-degree rotation looks like a small error rather than a system failure.
  - **Quick check question:** If a face detector outputs landmarks rotated by 30 degrees, what specifically happens to the input of the feature extractor?

## Architecture Onboarding

- **Component map:** Input (3 × 640 × 640) -> Backbone (MobileNetV2/ResNet50) -> Neck (Feature Pyramid Network) -> Heads (Classification, Box Regression, Landmark Regression) -> Non-Maximum Suppression

- **Critical path:** The vulnerability resides in the Ground Truth Matching & Target Generation step. The backdoor is injected when the poisoned image and its modified annotations (Eq. 1 & 2) are matched to anchors and fed into the Multi-Box Loss.

- **Design tradeoffs:**
  - **Trigger Stealth (α) vs. Success Rate:** Increasing transparency (α decreases) lowers Attack Success Rate (ASR), especially for LSA which requires precise feature retention.
  - **Poisoning Ratio (β) vs. Stealth:** Lower β is stealthier but fails to learn the LSA backdoor effectively (fails below 5%).

- **Failure signatures:**
  - **FGA:** High confidence scores (>0.9) localized on non-face geometric patterns (triggers).
  - **LSA:** Predicted landmarks violate geometric constraints (e.g., eyes appearing below the mouth) or flicker between benign and backdoored states in video (temporal instability).

- **First 3 experiments:**
  1. **Baseline FGA Injection:** Train RetinaFace on WIDER-Face with 5% poisoned samples using a BadNets patch. Verify if the model detects the patch as a face on the CelebA validation set.
  2. **Regression Robustness Test (LSA):** Attempt to train the Landmark Shift Attack with a low poisoning ratio (1%). Observe if the model learns the rotation or if the loss stagnates (validating the "harder to learn" claim).
  3. **Downstream Impact Check:** Pass the poisoned detections from Experiment 1 to a standard Face Recognition aligner. Measure the "garbage" output visually to confirm the pipeline breakage described in Section IV-D.

## Open Questions the Paper Calls Out
- **Open Question 1:** How do backdoor attacks on the Face Detection module propagate and interact with downstream modules (feature extraction, matching) to fully hijack a Face Recognition System?
  - **Basis in paper:** [explicit] Section VI.B states, "In a future work, we will provide a comprehensive overview of the impact of Backdoor Attacks on each module... and how their interactions can hijack a system’s entire function."
  - **Why unresolved:** This paper isolated the attack to the detection and alignment stages, measuring success via ASR and immediate antispoofing errors, but did not model the complex interactions through the full recognition pipeline.
  - **What evidence would resolve it:** An end-to-end evaluation measuring false match rates (FMR) and false non-match rates (FNMR) in a complete FRS pipeline when the face detector is compromised.

- **Open Question 2:** Are the proposed "Auxiliary detectors" and "Consistency checks" effective defenses against Landmark Shift Attacks?
  - **Basis in paper:** [inferred] Section V suggests using auxiliary detectors (e.g., Dlib) and geometric consistency rules to flag manipulations, but admits "there is not purpose-built defenses" and provides no experimental validation of these heuristics.
  - **Why unresolved:** While the authors hypothesize that ensemble approaches or geometric rules could work, they did not test if an auxiliary model would also learn the backdoor or if the "flickering" predictions could bypass consistency checks.
  - **What evidence would resolve it:** Experiments measuring the detection rate of LSA and FGA attacks when the proposed consistency checks and auxiliary models are deployed.

- **Open Question 3:** Can trigger designs be optimized to stabilize Landmark Shift Attacks in physical-world environments?
  - **Basis in paper:** [inferred] Section IV.F notes that while Face Generation Attacks transfer reliably to the physical world, Landmark Shift Attacks cause predictions to "flicker between benign and backdoored outputs," suggesting the need for "improved trigger design."
  - **Why unresolved:** The instability indicates that the trigger features learned for regression manipulation are not robust to the variations introduced by printing and environmental lighting.
  - **What evidence would resolve it:** A study testing various physical trigger patterns (texture, color, size) to achieve a stable, non-flickering landmark shift in physical testing scenarios.

## Limitations
- **Reproducibility of LSA Success:** The paper claims Landmark Shift Attacks achieve up to 99.6% ASR, but the mechanism is significantly more complex than FGA. The regression task requires precise coordinate learning that may not scale reliably across different face poses or trigger sizes.
- **Generalizability Beyond RetinaFace:** All experiments use RetinaFace with specific backbones (MobileNetV2/ResNet50). It's unclear whether the attack effectiveness generalizes to other face detection architectures like MTCNN or BlazeFace.
- **Downstream System Variability:** The 97.6% False Acceptance Rate in antispoofing assumes a specific pipeline configuration. Real-world FRS systems often include additional verification layers that could mitigate the attack impact.

## Confidence
- **High Confidence:** Face Generation Attacks are effective (FGA mechanism and success rates are well-established in object detection literature and confirmed by this paper's experiments).
- **Medium Confidence:** Landmark Shift Attacks work as described, but the learning complexity and lower poisoning ratio thresholds suggest the mechanism may be less robust in practice.
- **Low Confidence:** The claimed downstream impact on antispoofing systems assumes a vulnerable pipeline architecture that may not reflect production systems with defense-in-depth.

## Next Checks
1. **Cross-Architecture Validation:** Test both FGA and LSA attacks on a different face detection architecture (e.g., MTCNN) to verify generalizability beyond RetinaFace.
2. **Dynamic Trigger Testing:** Evaluate attack success with moving or dynamic triggers rather than static patches to assess real-world stealth and robustness.
3. **Pipeline Defense Evaluation:** Implement geometric consistency checks in the alignment stage to quantify how much the downstream impact decreases with basic defenses.