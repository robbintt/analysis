---
ver: rpa2
title: 'KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language
  Models'
arxiv_id: '2512.06727'
source_url: https://arxiv.org/abs/2512.06727
tags:
- compression
- layers
- value
- cache
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the memory bottleneck of the KV cache in
  large language models (LLMs) during autoregressive decoding, where the cache grows
  with sequence length and embedding dimension, often exceeding the model''s own memory
  footprint. To tackle this, the authors propose KV-CAR, a unified framework that
  combines two techniques: (1) lightweight per-layer autoencoders that compress key
  and value tensors along the embedding dimension before storage, and (2) a similarity-driven
  reuse mechanism that identifies and reuses redundant attention heads across adjacent
  layers.'
---

# KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models

## Quick Facts
- arXiv ID: 2512.06727
- Source URL: https://arxiv.org/abs/2512.06727
- Reference count: 17
- Key result: Up to 47.85% KV-cache memory reduction with minimal impact on perplexity and zero-shot accuracy

## Executive Summary
This paper addresses the memory bottleneck of the KV cache in large language models (LLMs) during autoregressive decoding, where the cache grows with sequence length and embedding dimension, often exceeding the model's own memory footprint. To tackle this, the authors propose KV-CAR, a unified framework that combines two techniques: (1) lightweight per-layer autoencoders that compress key and value tensors along the embedding dimension before storage, and (2) a similarity-driven reuse mechanism that identifies and reuses redundant attention heads across adjacent layers. Evaluations on GPT-2 and TinyLLaMA models across multiple datasets show up to 47.85% KV-cache memory reduction with minimal impact on perplexity and zero-shot accuracy. System-level measurements on an NVIDIA A40 GPU confirm that the reduced KV footprint enables longer sequence lengths and larger batch sizes before memory exhaustion. These results demonstrate KV-CAR's effectiveness in enabling memory-efficient LLM inference.

## Method Summary
The KV-CAR framework addresses KV cache memory growth during autoregressive LLM decoding through two complementary techniques. First, per-layer autoencoders (two fully-connected layers with batch normalization and Leaky ReLU) compress key and value tensors along the embedding dimension before caching, reducing memory proportionally while maintaining task performance. Second, a similarity-driven reuse mechanism identifies redundant attention heads across adjacent layers by computing L1-norm distances between corresponding KV heads, allowing selective reuse of previous layer's tensors. An optional third component applies int8 quantization after autoencoder compression for additional savings. The approach is validated on GPT-2 and TinyLLaMA models, showing significant memory reduction with minimal accuracy degradation.

## Key Results
- Up to 47.85% reduction in KV-cache memory usage
- Minimal impact on perplexity (maintained within 1-2% of baseline)
- Zero-shot accuracy preserved on PIQA and Winogrande benchmarks
- System-level gains: increased maximum sequence length and batch size before OOM on NVIDIA A40 GPU

## Why This Works (Mechanism)

### Mechanism 1: Learned Autoencoder Compression Along Embedding Dimension
- Claim: Compressing key-value tensors from dimension D to d (where d < D) before caching reduces memory proportionally while maintaining task performance.
- Mechanism: A lightweight per-layer autoencoder (two fully-connected layers with batch normalization and Leaky ReLU) encodes full-dimensional KV vectors into a compact latent representation at cache write time. A decoder reconstructs them to original dimension D at read time, just before attention computation. The learned nonlinear mapping adapts to each layer's statistical structure.
- Core assumption: KV representations contain redundancy along the embedding dimension that can be captured by a learned compression function without destroying attention-relevant information.
- Evidence anchors:
  - [abstract] "lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval"
  - [Section IV.A] "an encoder layer is added after the generation of key and value tokens... reduces them to a lower dimension d, where d < D"
  - [corpus] Related work (KVReviver, ChunkKV) confirms embedding-dimension compression is an active research direction, though specific autoencoder approaches differ.
- Break condition: If reconstruction error at any layer exceeds a task-specific threshold (observed as perplexity spike), that layer should use reduced compression ratio or be excluded from compression entirely.

### Mechanism 2: Similarity-Guided Inter-Layer KV Head Reuse
- Claim: Adjacent transformer layers often compute similar KV tensors for certain attention heads; reusing them reduces storage without degrading output.
- Mechanism: During a profiling pass, compute L1-norm distance between corresponding KV heads across adjacent layers (N and N-1). Heads with similarity above an empirical threshold are marked for reuse—layer N references layer N-1's cached tensors instead of storing its own. Fine-tuning with combined L1 + cross-entropy loss adapts the model to this sharing.
- Core assumption: Semantic redundancy exists across layers such that attention heads in consecutive layers encode sufficiently similar information for at least a subset of heads.
- Evidence anchors:
  - [abstract] "similarity-driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers"
  - [Section IV.A] "certain key and value heads in layer N reuse key and value heads from the previous layer N-1... identified by computing the L1 norm between consecutive layers"
  - [Table III] Selective replacement (19 key heads, 25 value heads) maintains perplexity near baseline (21.8 vs 21.4), while replacing all heads degrades to 30.8.
  - [corpus] Weak direct evidence—corpus papers focus on token-level eviction rather than cross-layer reuse.
- Break condition: Aggressive head replacement (>50% of heads) causes measurable perplexity degradation; threshold must be tuned per model/dataset.

### Mechanism 3: Stacked Quantization for Additional Compression
- Claim: Int8 quantization applied after autoencoder compression yields additive memory savings with negligible accuracy loss.
- Mechanism: Compressed latent vectors (dimension d) are quantized to int8 using affine scaling before storage. On retrieval, dequantization restores float values before decoding. This exploits the orthogonality between dimension reduction and precision reduction.
- Core assumption: Latent representations from autoencoders are sufficiently robust to quantization noise that downstream task accuracy is preserved.
- Evidence anchors:
  - [Section IV.C] Quantization equations provided; "quantize the full floating-point values to int8 after compression by the encoder"
  - [Table V] GPT-2 PIQA: baseline 0.6262 → AE 0.6055 → AE+Q 0.6039 (negligible additional degradation)
  - [corpus] TaDA and related work confirm quantization is a standard complementary axis for KV compression.
- Break condition: If latent dimension d is already very small, quantization noise may compound reconstruction error; validate per-configuration.

## Foundational Learning

- Concept: **KV Cache Mechanics in Autoregressive Decoding**
  - Why needed here: The entire paper addresses memory growth from caching key/value tensors during token generation. Understanding that cache size = 2 × P × N_layers × d_model × L_seq × B is essential for reasoning about compression axes.
  - Quick check question: If you halve the embedding dimension stored in cache and double batch size, what happens to total KV memory?

- Concept: **Multi-Head Attention and Head Independence**
  - Why needed here: The reuse mechanism depends on identifying redundant heads across layers. You must understand that each head maintains separate K/V projections to assess whether sharing is viable.
  - Quick check question: In a 12-layer model with 12 heads per layer, what is the maximum theoretical KV memory reduction if all heads in even layers reused odd layers' caches?

- Concept: **Autoencoder Reconstruction Loss vs. Task Loss**
  - Why needed here: Training uses a hybrid L1 (reconstruction) + cross-entropy (task) objective. Understanding the tradeoff between faithful reconstruction and downstream accuracy is critical for tuning compression ratios.
  - Quick check question: Why might minimizing pure L1 reconstruction loss not guarantee preserved perplexity?

## Architecture Onboarding

- Component map:
  - Input token -> KV projection -> Encoder -> (Quantizer) -> KV cache write
  - KV cache read -> (Dequantizer) -> Decoder -> Attention computation -> FFN -> Output
  - During profiling: Capture KV tensors -> Compute L1 similarity -> Generate reuse mask

- Critical path:
  1. Input token → KV projection → Encoder → (Quantizer) → KV cache write
  2. KV cache read → (Dequantizer) → Decoder → Attention computation → FFN → Output
  3. During profiling: Capture KV tensors → Compute L1 similarity → Generate reuse mask

- Design tradeoffs:
  - Higher compression ratio (smaller d) increases memory savings but risks perplexity degradation; layer-specific tuning required.
  - More aggressive head reuse reduces storage but may harm accuracy; selective replacement based on similarity threshold is safer.
  - Quantization adds compute overhead (scale/zero-point calculation) for marginal additional savings—worthwhile only when memory is critical.

- Failure signatures:
  - Sudden perplexity spike on specific dataset → likely over-compression of sensitive layers; check layer-wise reconstruction error.
  - Accuracy drop after enabling head reuse → threshold too permissive; re-profile with stricter L1 cutoff.
  - OOM despite compression → batch size or sequence length increased beyond savings; verify actual compression ratio achieved.

- First 3 experiments:
  1. **Baseline profiling**: Run GPT-2 on Wikitext with no compression; measure perplexity, KV memory, and per-layer KV tensor L2 norms to identify which layers have highest variance (candidates for compression).
  2. **Single-layer autoencoder ablation**: Add autoencoder to one layer at a time (compression ratio 2×); measure perplexity impact per layer to establish compression-tolerance ranking.
  3. **Combined system validation**: Enable autoencoders on top-5 tolerant layers + selective head reuse (threshold tuned on validation set); measure total memory reduction vs. accuracy tradeoff curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inference latency overhead of the per-layer autoencoder encode/decode operations compare to the throughput gains achieved by reduced memory pressure?
- Basis in paper: [inferred] The paper emphasizes memory reduction and OOM avoidance but does not report specific metrics regarding the computational time cost of the autoencoder layers during the decoding phase.
- Why unresolved: While memory is saved, the added neural network operations per layer introduce additional FLOPs that may impact tokens-per-second speed.
- What evidence would resolve it: End-to-end latency measurements (ms/token) and throughput comparisons between the baseline and KV-CAR implementations on the same hardware.

### Open Question 2
- Question: Does the compression-accuracy trade-off observed in 774M–1.1B parameter models (GPT-2, TinyLLaMA) scale effectively to models exceeding 7B parameters?
- Basis in paper: [inferred] The evaluation is restricted to relatively small models; the paper does not demonstrate if the learned redundancy in embedding dimensions persists in larger, more complex models.
- Why unresolved: Larger models may distribute information differently across layers and heads, potentially reducing the efficacy of the fixed or empirical compression ratios used here.
- What evidence would resolve it: Application of KV-CAR to a 7B+ parameter model (e.g., LLaMA-2-7B) with resulting perplexity and zero-shot accuracy scores.

### Open Question 3
- Question: Can a dynamic, content-aware threshold outperform the static L1-norm threshold for identifying reusable KV heads?
- Basis in paper: [inferred] The methodology relies on an "empirically determined" static threshold to decide head reuse, which may not be optimal for all input sequences or layers.
- Why unresolved: A static threshold cannot adapt to varying levels of redundancy across different prompts or generation steps, potentially missing reuse opportunities or degrading accuracy.
- What evidence would resolve it: A comparative study between the static threshold and an adaptive thresholding mechanism on the Winogrande or PIQA benchmarks.

### Open Question 4
- Question: What are the synergistic or antagonistic effects when combining KV-CAR's structural reuse with existing dynamic token pruning techniques?
- Basis in paper: [explicit] The conclusion states the method "can be used alongside quantization, pruning... for additional gains" without providing experimental verification for pruning.
- Why unresolved: It is unclear if reusing heads across layers conflicts with pruning algorithms that drop tokens based on attention importance scores.
- What evidence would resolve it: Experiments combining KV-CAR with a method like Keyformer to measure total memory savings and accuracy preservation.

## Limitations
- Critical hyperparameters (autoencoder architecture details, learning rates, head reuse thresholds) are incompletely specified, creating barriers to faithful reproduction
- Evaluation limited to GPT-2 and TinyLLaMA models (774M-1.1B parameters), leaving uncertainty about performance on larger models
- Focus on transformer-based LLMs without addressing generalizability to other architectures

## Confidence
- **High confidence**: The core mechanism of per-layer autoencoder compression along the embedding dimension is well-grounded and reproducible. The evidence from perplexity maintenance across datasets supports this claim.
- **Medium confidence**: The similarity-driven inter-layer KV head reuse mechanism is supported by empirical results (e.g., selective replacement maintaining perplexity), but the lack of specific head indices and exact thresholds reduces reproducibility.
- **Medium confidence**: The stacked quantization approach is a standard technique with predictable additive benefits, but its effectiveness depends on the robustness of the compressed latent space, which varies by layer and model.

## Next Checks
1. **Layer-wise Sensitivity Analysis**: Systematically ablate autoencoders layer by layer and measure the impact on perplexity and reconstruction error. This will identify which layers are most sensitive to compression and inform optimal compression ratio allocation.

2. **Head Reuse Threshold Calibration**: Profile KV head similarity across adjacent layers on a held-out dataset. Experiment with varying L1 thresholds to find the optimal balance between memory savings and accuracy preservation. Validate that selective replacement (not blanket reuse) is critical for performance.

3. **System-Level Scaling Validation**: Measure KV-cache memory reduction, perplexity, and OOM thresholds on multiple GPUs (e.g., A40, A100, H100) and with varying batch sizes/sequence lengths. Confirm that the reported 47.85% reduction translates to tangible gains in maximum feasible sequence length or batch size under real-world constraints.