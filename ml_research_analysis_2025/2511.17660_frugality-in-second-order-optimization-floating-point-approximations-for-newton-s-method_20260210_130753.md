---
ver: rpa2
title: 'Frugality in second-order optimization: floating-point approximations for
  Newton''s method'
arxiv_id: '2511.17660'
source_url: https://arxiv.org/abs/2511.17660
tags:
- newton
- error
- gradient
- page
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a mixed-precision framework for Newton's method
  in optimization, analyzing the impact of finite-precision arithmetic on convergence.
  The authors establish a convergence theorem for mixed-precision Newton optimizers
  (including inexact and quasi-Newton variants), providing both theoretical guarantees
  and a priori estimates of achievable solution accuracy.
---

# Frugality in second-order optimization: floating-point approximations for Newton's method

## Quick Facts
- arXiv ID: 2511.17660
- Source URL: https://arxiv.org/abs/2511.17660
- Reference count: 0
- Primary result: Mixed-precision Newton optimization framework with theoretical convergence guarantees and empirical validation on regression benchmarks

## Executive Summary
This work introduces a three-precision framework for Newton's method that allows high-accuracy optimization while reducing computational cost. The authors prove that gradient computation precision determines final solution accuracy while linear solver precision affects only convergence rate. They develop GN_k, a generalized Gauss-Newton method that computes only the k most important second-order derivatives, achieving performance comparable to full Newton's method with significantly fewer derivative evaluations. The framework is validated through experiments on regression benchmarks where proposed methods outperform Adam on Australian and MUSH datasets.

## Method Summary
The paper presents a mixed-precision Newton optimization framework where gradient computation, linear system solving, and parameter updates use different precision levels (π_g ≥ π_w ≥ π_l). The core innovation is GN_k, which approximates the Hessian by computing only the k largest-magnitude residual second derivatives. The authors prove convergence theorems establishing that the limiting accuracy depends primarily on gradient precision, while linear solver precision affects only convergence rate. They also introduce CGLS_k, an adapted conjugate gradient variant for solving the structured linear systems arising from GN_k, showing improved performance over standard iterative solvers in ill-conditioned problems.

## Key Results
- Mixed-precision Newton maintains convergence when using high precision only for gradient computation and lower precision for linear solving and updates
- GN_k (computing only k largest second derivatives) achieves performance comparable to full Newton's method with reduced computational cost
- CGLS_k outperforms standard CG on ill-conditioned linear systems where the perturbation S improves conditioning
- Proposed methods outperform Adam on Australian and MUSH classification datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient precision governs final solution accuracy; linear solver precision only affects convergence rate.
- **Mechanism:** Error analysis decomposes total error into gradient computation error ψ, Hessian/linear solver error φ, and rounding error u. Limiting accuracy limacc scales with ∥H⁻¹∥ψ, making gradient error dominant. Linear solver error contributes to contraction factor G but not limacc.
- **Core assumption:** Hessian is positive definite and Lipschitz continuous; φ·κ(H) ≤ 1/8.
- **Evidence anchors:**
  - [section 3.1, Theorem 3.1.1]: "∥x̄−x*∥/∥x*∥ ≈ limacc ≜ ∥H⁻¹*∥ψ(g,x*,u,ug) + u"
  - [section 3.1.1, Eq. 3.18]: Derives η ≤ 1/7 as safe stopping condition for iterative solvers
  - [corpus]: Extends Tisseur (2001) analysis from root-finding to minimization context

### Mechanism 2
- **Claim:** Computing only k largest-magnitude residual second derivatives (GN_k) matches full Newton performance with reduced cost.
- **Mechanism:** In least-squares problems, true Hessian is J^TJ + S(x) where S(x) = Σᵢ rᵢ(x)·∇²rᵢ(x). GN_k selects k largest |rᵢ| terms to include in S(x), discarding others. Convergence maintained when ∥Σ_{j∉K} Sⱼ(x)(x−x*)∥ ≤ σ∥x−x*∥.
- **Core assumption:** Large residuals contribute disproportionately to S(x); discarded terms are small relative to J^TJ eigenvalues.
- **Evidence anchors:**
  - [section 4.1, Eq. 4.5]: "∇²f(x) ≈ B(x) = J(x)^TJ(x) + Σ_{j∈K} Sⱼ(x)"
  - [section 4.1.1, Figure 4.2]: GN_30 achieves identical iteration convergence to full Newton on highly nonlinear function
  - [corpus]: Limited direct work on partial Hessian computation; closest is subspace approximate Hessian methods

### Mechanism 3
- **Claim:** CGLS_k solves (A^TA + S)x = A^Tb more stably than forming full matrix, especially when S improves conditioning.
- **Mechanism:** Standard CGLS avoids forming A^TA explicitly. CGLS_k adds recursive term v_k = v_{k-1} − α_k·w_k where w_k = Sp_k, incorporating S without explicit matrix formation. Maintains numerical stability when κ(A^TA + S) < κ(A^TA).
- **Core assumption:** S is structured; structured condition number κ_S better predicts behavior than κ(A^TA+S).
- **Evidence anchors:**
  - [section 4.2, Algorithm 6]: Full CGLS_k pseudocode with v_k recursion
  - [section 4.2.1, Table 4.2]: CGLS_k achieves 10⁻⁵ error vs 0.37 for CG on Problem 1
  - [corpus]: CGLS variants exist; this extends to S-perturbed normal equations

## Foundational Learning

- **Floating-point unit roundoff (u):**
  - Why needed here: Framework depends on relating precision levels to their unit roundoffs; Theorem 3.1.1 bounds depend explicitly on u and u_l.
  - Quick check question: If fp32 has u ≈ 6×10⁻⁸, can you compute the maximum relative error after 1000 sequential operations?

- **Condition number κ(A):**
  - Why needed here: Assumption ul·κ(H) ≤ 1/8 is central constraint; ill-conditioned Hessians require higher linear precision.
  - Quick check question: If κ(H) = 10⁶, what is the minimum precision for π_l to satisfy convergence bound?

- **Lipschitz continuity of Hessian:**
  - Why needed here: Lemma A.0.1 (∥g(w)−g(v)−H(v)(w−v)∥ ≤ L_H/2·∥w−v∥²) is used throughout proofs; assumption 3.6 defines convergence basin.
  - Quick check question: Why does a large L_H shrink the region where Newton converges quadratically?

## Architecture Onboarding

- **Component map:** Gradient computer → Hessian approximator (GN_k) → Linear solver (CGLS_k) → Parameter updater
- **Critical path:**
  1. Check conditioning: Compute κ(H) or estimate via J^TJ spectrum; verify ul·κ(H) ≤ 1/8
  2. Set precisions: If bound violated, increase π_l; if gradient conditioning is high, increase π_g
  3. Run iterations: Monitor ∥g∥ against limg bound; stop when gradient norm plateaus
- **Design tradeoffs:**
  - Higher π_g improves accuracy but increases cost; for well-conditioned gradients, fp64 suffices
  - GN_k with k=30% matches full Newton on tested problems, but selection heuristic may fail on uniformly-difficult problems
  - CGLS_k preferred when S is well-conditioned and A is ill-conditioned; use CG when A^TA+S is well-conditioned
- **Failure signatures:**
  - Gradient norm stalls above limg: Check if π_g is too low for gradient conditioning
  - Divergence in early iterations: Initial point may violate assumption 3.6; try starting closer to solution or increasing π_l
  - CGLS_k slower than CG: Check if κ(A^TA+S) < κ(A^TA); switch to standard CG with explicit matrix
- **First 3 experiments:**
  1. Precision sweep on synthetic least-squares: Test (π_g, π_w, π_l) ∈ {(128,64,32), (64,64,32), (64,32,32)}; verify limacc and limg bounds hold
  2. GN_k ablation on nonlinear regression: Compare k ∈ {0%, 10%, 30%, 50%, 100%}; measure convergence iterations and wall-clock time
  3. CGLS_k vs. CG on ill-conditioned systems: Generate problems with varying κ(A)/κ(A^TA+S) ratios; identify crossover point where CG becomes preferable

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a backward error analysis be adapted for CGLS_k to predict the attainable accuracy a priori and guide the selection of iterative solvers?
- **Basis in paper:** [explicit] Section 4.3 notes that the lack of a backward error analysis prevents the prediction of forward error based on the structured condition number.
- **Why unresolved:** While Theorem 4.2.1 defines the structured condition number κ_s, experiments show κ_s does not proportionally correlate with final error without backward error estimate.
- **What evidence would resolve it:** A formal derivation of a backward error bound for CGLS_k and empirical validation showing this bound accurately predicts the forward error in ill-conditioned linear systems.

### Open Question 2
- **Question:** Does a dynamic schedule for the sample size k (reducing k as the algorithm converges) improve the computational efficiency of GN_k?
- **Basis in paper:** [explicit] Section 4.3 proposes experimenting with a dynamic number of samples k to reduce computations as the algorithm converges.
- **Why unresolved:** Current GN_k framework uses fixed percentage of derivatives; unknown if adaptive selection maintains convergence speed while lowering derivative evaluation cost.
- **What evidence would resolve it:** An implementation of GN_k with decreasing k schedule that demonstrates reduced computational time and derivative evaluations without degrading limiting accuracy compared to static version.

### Open Question 3
- **Question:** Can the theoretical convergence guarantees for mixed-precision Newton be extended to stochastic implementations that utilize mini-batches?
- **Basis in paper:** [explicit] Section 4.3 identifies stochasticity as a direction for future work, specifically need to include stochastic error analysis for batched second-order optimizers.
- **Why unresolved:** Theorems 3.1.1 and 3.1.2 assume deterministic computation; interaction between error term ψ and variance from mini-batch sampling is undefined.
- **What evidence would resolve it:** A convergence proof for stochastic mixed-precision Newton that bounds expected error considering both gradient variance and floating-point unit roundoff, supported by large-scale dataset experiments.

## Limitations
- Theoretical framework assumes Lipschitz continuous Hessian and positive definiteness, which may not hold for non-convex problems common in deep learning
- GN_k heuristic relies on residual magnitude as proxy for S(x) importance, but this correlation may break down in high-dimensional or sparse settings
- Three-precision allocation is problem-dependent, and bounds for optimal precision selection remain heuristic

## Confidence
- **High confidence:** Mixed-precision convergence theorem - proven with explicit error bounds relating precision to accuracy
- **Medium confidence:** GN_k heuristic - supported by empirical results on specific test functions but lacks theoretical guarantees for general cases
- **Medium confidence:** CGLS_k implementation - validated on synthetic problems with known condition numbers, but real-world performance on large-scale systems untested

## Next Checks
1. Test GN_k selection heuristic on regression problems where residuals have uniform magnitude but non-uniform contribution to Hessian - does the method still converge?
2. Implement the full three-precision Newton optimizer on a neural network training task with known conditioning - verify limacc and limg bounds hold empirically.
3. Benchmark CGLS_k against CG and direct solvers on a spectrum of ill-conditioned problems (varying κ(A^TA)/κ(A^TA+S) ratios) to identify the crossover region where each method is optimal