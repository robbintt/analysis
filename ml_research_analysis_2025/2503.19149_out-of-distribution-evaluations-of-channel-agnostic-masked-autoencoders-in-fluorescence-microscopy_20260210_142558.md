---
ver: rpa2
title: Out-of-distribution evaluations of channel agnostic masked autoencoders in
  fluorescence microscopy
arxiv_id: '2503.19149'
source_url: https://arxiv.org/abs/2503.19149
tags:
- embeddings
- channel
- channels
- plates
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Campfire, a channel-agnostic masked autoencoder
  designed for high-content screening (HCS) in fluorescence microscopy. The authors
  address the challenge of developing generalizable models for HCS, which must handle
  distribution shifts caused by changes in experimental conditions, perturbagens,
  and fluorescent markers.
---

# Out-of-distribution evaluations of channel agnostic masked autoencoders in fluorescence microscopy

## Quick Facts
- arXiv ID: 2503.19149
- Source URL: https://arxiv.org/abs/2503.19149
- Reference count: 40
- Campfire, a channel-agnostic masked autoencoder, demonstrates superior performance on out-of-distribution evaluation tasks in fluorescence microscopy, including generalization to held-out fluorescent channels and transfer to a new cell type.

## Executive Summary
This paper addresses the challenge of developing generalizable models for high-content screening (HCS) in fluorescence microscopy, which must handle distribution shifts caused by changes in experimental conditions, perturbagens, and fluorescent markers. The authors introduce Campfire, a channel-agnostic masked autoencoder that uses a shared decoder for all channels, enabling it to scale effectively to datasets with many different fluorescent markers and handle out-of-distribution markers during inference. Campfire is trained using a self-supervised objective function and demonstrates superior performance compared to baselines on tasks like predicting the compound of stimulation from single-cell embeddings. The model successfully generalizes to held-out fluorescent channels and transfers knowledge to a new microscopy screen with a different cell type, outperforming a baseline model in distinguishing between control compounds in macrophage morphology.

## Method Summary
Campfire is a self-supervised masked autoencoder built on a Vision Transformer architecture, designed to handle multi-channel fluorescence microscopy images in a channel-agnostic manner. The model applies a 3D convolutional layer to map image patches to embeddings independently for each channel, then averages patch embeddings per channel and projects them through a shared linear layer to create channel embeddings. These embeddings are added to the decoder's latent state, allowing the model to handle arbitrary channel configurations at inference. Training involves random channel subsampling per batch to improve robustness to varying channel combinations, and the objective function combines spatial MSE with high-pass and low-pass filtered reconstruction losses computed via 2D FFT. The model is evaluated on the JUMP-CP dataset, which isolates sources of distribution shift including plates, perturbagens, fluorescent markers, and cell types.

## Key Results
- Campfire achieves 0.199 accuracy on held-out compound + OOD plate tasks, outperforming DinoViT-S8 at 0.191
- The model successfully generalizes to held-out fluorescent channels (ER, cyRNA) not seen during training
- Campfire transfers knowledge to a new microscopy screen with macrophages, achieving a Z'-score of 0.672 compared to 0.571 for the baseline model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel-agnostic encoding enables generalization to out-of-distribution fluorescent markers during inference.
- Mechanism: A 3D convolutional layer with kernel (1, P, P) applies the same projection to each channel independently, producing channel-agnostic patch embeddings. The decoder receives channel embeddings computed by averaging patch embeddings per channel and projecting through a shared linear layer, allowing the model to handle arbitrary channel configurations at inference.
- Core assumption: Fluorescent channels share underlying morphological features that can be reconstructed from partial channel observations.
- Evidence anchors:
  - [abstract] "Campfire uses a shared decoder for all channels, enabling it to scale effectively to datasets with many different fluorescent markers and handle out-of-distribution markers during inference."
  - [section 2.2] "Applying the same convolution to each channel mapped image patches to embeddings in a channel agnostic manner, such that the model could be trained with images of inconsistent channel ordering, number, or type."
  - [corpus] Related work CHAMMI-75 addresses heterogeneous microscopy but does not explicitly target OOD marker generalization; corpus lacks direct replication of this mechanism.
- Break condition: If channels encode entirely orthogonal biological information with no shared structure, the shared decoder cannot leverage cross-channel correlations for reconstruction.

### Mechanism 2
- Claim: Channel subsampling during training improves robustness to varying channel combinations at inference.
- Mechanism: During training, for each batch, a random subset S of available channels is sampled from {Nu, Ac, M} with uniform probability. Only these channels are passed to the model, forcing it to learn representations that function across multiple channel combinations.
- Core assumption: Exposure to varied channel subsets during training induces representations that generalize to unseen channel combinations.
- Evidence anchors:
  - [section 3.1] "For each batch in training, we sampled a subset of channels, S, from the set of all available channels, with uniform probability... Consequently, our model was trained with multiple views of the images to be robust to different combinations of ID channels."
  - [section 3.4] Table 3.2 shows Campfire with OOD channels (Nu, ER, cyRNA) achieves 0.199 accuracy on held-out compound + OOD plate, outperforming DinoViT-S8 at 0.191.
  - [corpus] No direct corpus evidence for channel subsampling as a mechanism.
- Break condition: If subsampling during training fails to cover the distribution of channel combinations encountered at inference, performance degrades.

### Mechanism 3
- Claim: Fourier-domain loss components improve reconstruction quality and downstream representation learning.
- Mechanism: The objective function combines spatial MSE with high-pass and low-pass filtered reconstruction losses, computed via 2D FFT. This encourages the model to capture both fine-grained and coarse morphological features.
- Core assumption: Explicitly optimizing for multi-scale frequency reconstruction yields embeddings that better encode biological morphology.
- Evidence anchors:
  - [section A] Equation 1 shows the loss function combining λ_s·MSE, λ_h·MSE(high-pass), and λ_l·MSE(low-pass).
  - [section B] Ablation study Table 2.3 shows optimal configuration uses λ_s=0.75, λ_h=0.01, λ_l=0.25 with frequency threshold 0.1.
  - [corpus] Corpus does not provide independent validation of Fourier-domain loss in microscopy MAEs.
- Break condition: If high-frequency noise dominates the signal, the model may overfit to acquisition artifacts rather than biological features.

## Foundational Learning

- Concept: **Masked Autoencoders (MAE)**
  - Why needed here: Campfire builds directly on the MAE paradigm—masking a high fraction (80%) of image patches and training the model to reconstruct them.
  - Quick check question: Can you explain why masking 80% of patches forces the model to learn global semantic features rather than local interpolation?

- Concept: **Distribution Shift in High-Content Screening**
  - Why needed here: The paper isolates four sources of distribution shift (plates, perturbagens, fluorescent markers, cell types) and evaluates generalization to each.
  - Quick check question: What is the difference between covariate shift (input distribution change) and concept shift (label relationship change), and which applies to OOD fluorescent markers?

- Concept: **Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: Campfire uses a ViT encoder/decoder architecture with patch embeddings augmented by sinusoidal position embeddings and RoPE.
  - Quick check question: How does the inductive bias of a ViT differ from a CNN, and why might this matter for multi-channel microscopy images?

## Architecture Onboarding

- Component map:
  - Input preprocessing: Cell-centered tiles (112×112) → 3D conv (kernel 1×P×P) → N×C patch embeddings of dimension D
  - Position encoding: Sinusoidal + RoPE embeddings added to patch embeddings
  - Encoder: ViT encoder processes unmasked patches only (mask ratio p_m=0.8)
  - Channel embedding: Average patch embeddings per channel → shared linear projection
  - Decoder: Latent state padded with mask tokens → ViT decoder with channel embeddings
  - Loss: MSE + high-pass filtered MSE + low-pass filtered MSE
  - Inference: Decoder discarded; mean of encoder patch embeddings used as cell representation

- Critical path: The channel embedding computation (averaging patch embeddings per channel, projecting through shared linear layer) is the key innovation enabling channel-agnostic operation. Verify this module handles variable channel counts correctly.

- Design tradeoffs:
  - **Mask ratio 0.8**: Higher masking forces more global reasoning but may lose fine-grained details. Ablation (Table 2.4) tested 0.75; 0.8 performed better.
  - **Channel subsampling**: Training with random channel subsets improves robustness but may underutilize full channel information when available.
  - **Shared vs. channel-specific decoder**: Shared decoder enables OOD channel handling but may lose channel-specific reconstruction fidelity.

- Failure signatures:
  - If OOD channel embeddings cluster separately from ID channels in the latent space, the channel embedding mechanism is not generalizing.
  - If reconstruction loss plateaus but downstream task accuracy is random, the learned representations are not biologically meaningful.
  - If plate-specific batch effects dominate embedding space, the model has not learned plate-invariant features.

- First 3 experiments:
  1. **Reconstruction sanity check**: Train on subset of JUMP-CP TARGET2 plates with 3 channels (Nu, Ac, M), visualize reconstructions at epochs 10, 30, 50 to verify learning progression (reference Fig. 3.3).
  2. **Channel ablation**: Extract embeddings using different channel combinations (single channels, pairs, all three) and train linear classifiers to predict control compounds. Replicate Fig. 3.4 to verify channel integration.
  3. **OOD channel evaluation**: Extract embeddings using held-out channels (ER, cyRNA) and compare compound prediction accuracy against ImageNet-pretrained DinoViT baseline. Target: outperform baseline on held-out compound + OOD plate task (reference Table 3.2).

## Open Questions the Paper Calls Out

- **Question**: Can scaling pre-training to datasets with significantly more than five fluorescent markers close the performance gap between in-distribution and out-of-distribution (OOD) channels?
  - **Basis in paper**: [explicit] The Conclusion states that the JUMP-CP dataset's limitation to five channels restricted the OOD evaluation, and that "Training over a broader set of fluorescent channels would likely have improved the generalisation of our model."
  - **Why unresolved**: The current study was restricted to the JUMP-CP dataset, preventing an empirical test of how channel diversity scales with OOD generalization capabilities.
  - **What evidence would resolve it**: Training Campfire on a multi-study corpus comprising dozens of distinct fluorescent markers and benchmarking the reconstruction and classification accuracy on entirely novel markers.

- **Question**: Does joint training on multiple cell types improve generalization to new microscopy screens compared to training on a single cell type?
  - **Basis in paper**: [explicit] The Conclusion notes that "training with screens of different cell types will be crucial for a HCI foundation model" and that the current model was trained on JUMP-CP exclusively.
  - **Why unresolved**: While Campfire successfully transfer-learned to macrophages, it is unknown if pre-training on heterogeneous cell types from the start would yield more robust universal features.
  - **What evidence would resolve it**: A comparison of embeddings from a model pre-trained on a multi-cell-type dataset versus the current single-source model when evaluated on a held-out screen involving a novel cell type.

- **Question**: How does the specific combination of fluorescent channels influence the optimal embedding quality for distinct downstream tasks?
  - **Basis in paper**: [inferred] Section 3.3 notes that the ranking of channel combinations by accuracy was inconsistent between predicting control compounds versus held-out compounds, suggesting that "embedding quality is not maximised by merely selecting the single most informative channels."
  - **Why unresolved**: The paper establishes that channel integration is beneficial but does not define a theoretical or empirical rule for which channel interactions best serve specific phenotypic profiling tasks.
  - **What evidence would resolve it**: An ablation study mapping channel combination performance across a wider variety of biological tasks to identify consistent synergistic effects between specific markers.

## Limitations

- The optimal configuration of the Fourier-augmented loss function is not fully specified in the text, with references to a "highlighted row" in Table 2.3 that lacks visual preservation.
- The method for computing channel embeddings is ambiguous, with unclear whether averaging is performed strictly per-sample or involves batch-level statistics.
- The model's ability to generalize to entirely new marker types from different modalities (e.g., super-resolution microscopy) remains untested, as held-out channels are still part of the same biological system.

## Confidence

- **High Confidence**: Campfire outperforms baseline models (DinoViT-S8) on held-out compound prediction tasks using in-distribution channels.
- **Medium Confidence**: Campfire generalizes to OOD fluorescent channels (ER, cyRNA), though the mechanism for this generalization is not directly validated with ablation studies.
- **Low Confidence**: Campfire transfers knowledge to a new cell type (macrophages), with limited analysis of why the model succeeds or fails on specific compounds.

## Next Checks

1. **Reconstruct Held-Out Channels**: Train Campfire on JUMP-CP with only Nu, Ac, and M channels, then attempt to reconstruct images containing the held-out ER and cyRNA channels. Compare reconstruction quality to a baseline model trained with channel-specific decoders to isolate the impact of the shared decoder on OOD generalization.

2. **Ablation of Channel Subsampling**: Train two variants of Campfire: one with channel subsampling (as described) and one trained on all available channels in each batch. Evaluate both models on OOD compound + OOD plate tasks to quantify the contribution of channel subsampling to robustness.

3. **Probe for Plate-Specific Batch Effects**: After training Campfire, extract embeddings for all plates in the JUMP-CP dataset and perform PCA/t-SNE. Color points by plate ID and compound type. If embeddings cluster strongly by plate rather than compound, the model is learning batch artifacts instead of biological features, indicating a failure of the channel-agnostic design to overcome plate effects.