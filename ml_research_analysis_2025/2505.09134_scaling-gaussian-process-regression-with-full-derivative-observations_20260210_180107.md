---
ver: rpa2
title: Scaling Gaussian Process Regression with Full Derivative Observations
arxiv_id: '2505.09134'
source_url: https://arxiv.org/abs/2505.09134
tags:
- dsoftki
- interpolation
- kernel
- gradient
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DSoftKI is a scalable Gaussian Process method that extends SoftKI
  to incorporate full derivative observations. It modifies the interpolation scheme
  by using local temperature vectors at each interpolation point, enabling local directional
  sensitivity.
---

# Scaling Gaussian Process Regression with Full Derivative Observations

## Quick Facts
- **arXiv ID:** 2505.09134
- **Source URL:** https://arxiv.org/abs/2505.09134
- **Reference count:** 26
- **One-line primary result:** DSoftKI is a scalable Gaussian Process method that extends SoftKI to incorporate full derivative observations, achieving O(m²nd) inference and O(m³+bmd) optimization while outperforming existing GPwD methods on synthetic and high-dimensional molecular force field datasets.

## Executive Summary
DSoftKI introduces a scalable Gaussian Process method for regression with full derivative observations by replacing global temperature vectors in SoftKI with local ones, enabling directional sensitivity. This allows the method to approximate kernel derivatives via interpolation rather than explicit computation, supporting extensions like Deep Kernel Learning. DSoftKI achieves linear scaling in dimension d for hyperparameter optimization and demonstrates improved accuracy over DSVGP and DDSVGP on synthetic benchmarks and molecular force fields up to 1000 dimensions.

## Method Summary
DSoftKI modifies the SoftKI interpolation scheme by using local temperature vectors at each interpolation point, enabling local directional sensitivity for derivative approximation. The method approximates the kernel (including derivatives) via interpolation without requiring explicit kernel derivatives, supporting complex kernels and Deep Kernel Learning. DSoftKI achieves time complexity O(m²nd) for posterior inference and O(m³+bmd) for hyperparameter optimization, outperforming existing GPwD methods in accuracy and scalability.

## Key Results
- DSoftKI achieves time complexity O(m²nd) for posterior inference and O(m³+bmd) for hyperparameter optimization
- Outperforms existing GPwD methods (DSVGP, DDSVGP) in accuracy and scalability on synthetic benchmarks
- Demonstrates superior performance on high-dimensional molecular force field datasets (100-1000 dimensions), particularly in gradient prediction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Local temperature vectors enable interpolation points to capture local directional sensitivity, which is required for accurate derivative approximation without explicit kernel gradients.
- **Mechanism:** DSoftKI assigns a learnable temperature vector T_k ∈ ℝ^d to each interpolation point z_k, allowing the softmax interpolation weights to respond differently to input variations based on the specific local geometry surrounding each point.
- **Core assumption:** The local geometry defined by the interpolated points and their temperatures is sufficiently rich to approximate the true function's curvature.
- **Evidence anchors:**
  - [abstract] "...replacing its global temperature vector with local temperature vectors... allows the model to encode local directional sensitivity."
  - [section 4.1] "Points that are nearby can nevertheless have different influences on the interpolation strength of the gradients depending on their respective directions."
- **Break condition:** If the interpolation point density is too low relative to the function's high-frequency components, local temperatures may overfit to noise rather than capturing the true gradient field.

### Mechanism 2
- **Claim:** Bypassing the analytical computation of kernel derivatives (∇k, ∇²k) via interpolation enables support for Deep Kernel Learning (DKL) and complex kernels.
- **Mechanism:** DSoftKI approximates the kernel K̃_xx by differentiating the interpolation function (Σ̃) rather than the kernel itself (K_zz). Since the base kernel K_zz is only evaluated at inducing points and never differentiated analytically, automatic differentiation can handle the entire structure, including deep neural network feature extractors inside the kernel.
- **Core assumption:** The derivative of the interpolation scheme serves as a sufficient proxy for the derivative of the true kernel function.
- **Evidence anchors:**
  - [abstract] "...eliminating the need for kernel derivatives, facilitating extensions such as Deep Kernel Learning (DKL)."
  - [section 4.1] "Notably, the computation of the DSoftKI kernel does not require computing the first-order or second-order derivatives of the kernel."
- **Break condition:** If the interpolation grid is misaligned with the feature space, the interpolated gradient may deviate significantly from the true kernel gradient, degrading uncertainty quantification.

### Mechanism 3
- **Claim:** A factorized low-rank representation reduces the dependency on dimension d from cubic to linear for hyperparameter optimization.
- **Mechanism:** The method decomposes the covariance D_θ into FF^T + Λ̃, where F = Σ̃_xz L. It solves linear systems using the Woodbury matrix identity on the capacitance matrix I + F^T Λ̃^(-1) F, which operates in the m × m space rather than n(d+1) × n(d+1).
- **Core assumption:** The capacitance matrix captures the essential information needed for the log-likelihood gradient.
- **Evidence anchors:**
  - [section 4.2] "This reduces the challenge of computing the MLL to computing the determinant and inverses of the m × m capacitance matrix... time complexity O(m²bd)."
  - [table 1] Explicitly lists "Posterior Inference O(m²nd)".
- **Break condition:** If m (interpolation points) scales linearly with d to maintain accuracy, the O(m³) term in optimization may re-introduce cubic scaling in high dimensions.

## Foundational Learning

- **Concept: Gaussian Processes with Derivatives (GPwD)**
  - **Why needed here:** DSoftKI is fundamentally a GPwD method; understanding that ∇f is a GP with kernel ∂²k(x,y)/∂x_i∂y_j is required to see why approximating this joint covariance is hard.
  - **Quick check question:** If a function f has an RBF kernel, what is the kernel for the joint distribution of f and ∇f?

- **Concept: Softmax/Kernel Interpolation (SKI)**
  - **Why needed here:** The "SoftKI" base uses softmax interpolation weights instead of cubic/quintic splines. You must grasp how K_xx ≈ ΣK_zzΣ^T turns a dense matrix into a product of sparse/dense low-rank factors.
  - **Quick check question:** How does the complexity of matrix-vector multiplication change when replacing K_xx with ΣK_zzΣ^T?

- **Concept: Hutchinson's Trace Estimator**
  - **Why needed here:** DSoftKI uses Hutchinson's pseudoloss to stabilize optimization when the standard MLL is numerically unstable due to interpolation point placement.
  - **Quick check question:** Why is estimating the trace of a matrix using random vectors z (where tr(A) ≈ (1/l)∑z_i^TAz_i) useful for large matrices?

## Architecture Onboarding

- **Component map:** Inputs -> Interpolation Layer -> Kernel Base -> Solver
- **Critical path:** The formation and Cholesky decomposition of the capacitance matrix Ĉ. This is the bottleneck for hyperparameter optimization. Precision handling (switching to double) here is critical.
- **Design tradeoffs:**
  - **Local vs. Global Temperature:** Local temperatures (m × d params) improve gradient accuracy but introduce optimization instability (susceptible to jitter/Cholesky failure). Global is more stable but less expressive.
  - **Exact MLL vs. Pseudoloss:** Exact MLL is precise but fragile; Pseudoloss is robust but slower and noisier. The system toggles between them based on stability.
- **Failure signatures:**
  - **NaNs in training:** Usually indicates the Cholesky decomposition of K_zz or Ĉ failed due to interpolation points collapsing (distances → 0) or bad temperatures.
  - **Diverging Gradient Error:** If β_g²/β_v² is set incorrectly (e.g., ratio ≫ d), the model prioritizes gradients over values, potentially destabilizing the value reconstruction.
- **First 3 experiments:**
  1. **Sanity Check (Branin):** Run DSoftKI on a 2D synthetic function (e.g., Branin) with m=100. Verify that the predicted gradient field matches the analytic gradient visually (vector field plot).
  2. **Ablation (Temperature):** Compare DSoftKI (local temps) vs. DSoftKI with global temps on a high-dimension dataset (e.g., d=20). Measure the gap in gradient RMSE.
  3. **Scale Profiling:** Profile the training time per epoch while increasing d (e.g., 10, 100, 500) with fixed m. Confirm the O(m²nd) scaling trend (linear in d) vs. the cubic scaling of a baseline if available.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the computational bottleneck of hyperparameter optimization be reduced by improving the approximation of the DSoftKI marginal log-likelihood (MLL) or utilizing a different objective function?
- **Basis in paper:** [explicit] The conclusion states that "the method is currently bottlenecked by the computation of the DSoftKI MLL" and suggests that "improved approximations of the MLL or a different objective could further scale the efficiency."
- **Why unresolved:** The current method relies on a stabilized MLL (switching to Hutchinson's pseudoloss upon instability), but the authors note the optimization dynamics and computational cost remain a limiting factor.
- **What evidence would resolve it:** Demonstration of an alternative objective or MLL approximation that reduces the time complexity below O(m³+bmd) while maintaining prediction accuracy on the MD22 or synthetic benchmarks.

### Open Question 2
- **Question:** Can alternative linear solvers or multi-GPU hardware utilization overcome the current GPU memory limits for posterior inference in very high dimensions?
- **Basis in paper:** [explicit] Section 4.2 notes that solving the system of linear equations can exceed GPU memory limits for large d, leading the authors to sometimes use a CPU. They explicitly identify "alternative linear solvers or utilizing multi-GPU hardware" as an interesting direction for future work.
- **Why unresolved:** The current implementation is limited to single-GPU arithmetic, forcing a hardware switch (to CPU) for large-scale problems (e.g., d=1000), which impacts speed.
- **What evidence would resolve it:** An implementation of DSoftKI using iterative solvers (like Conjugate Gradient) or multi-GPU partitioning that keeps the posterior inference resident in GPU memory for dimensions d > 1000 without timing out.

### Open Question 3
- **Question:** What are the specific trade-offs between the added computational complexity of fitting full derivative observations and the resulting gains in surface modeling accuracy?
- **Basis in paper:** [explicit] The conclusion states that "a deeper investigation of the tradeoffs between fitting derivative information or not when they are available is warranted."
- **Why unresolved:** The authors observed that methods like DSoftKI (which fits derivatives) improve over non-derivative methods, but the computational cost scales significantly with dimension (O(m²nd)), and it is unclear in what regimes the cost outweighs the benefit.
- **What evidence would resolve it:** A comparative analysis establishing a Pareto frontier between wall-clock training time and gradient prediction error for DSoftKI versus non-derivative baselines (e.g., SoftKI) across varying data sizes and dimensions.

## Limitations
- **Optimization instability:** The use of local temperature vectors introduces optimization instability, particularly when interpolation points are poorly initialized or too close, requiring heuristic switches between exact MLL and Hutchinson's pseudoloss.
- **GPU memory constraints:** Solving the system of linear equations for posterior inference can exceed GPU memory limits for large dimensions, forcing the use of CPU and impacting computational efficiency.
- **Computational scaling:** While the method scales linearly with dimension d for hyperparameter optimization, the O(m³) term may re-introduce cubic scaling if the number of interpolation points m must scale with d to maintain accuracy.

## Confidence

- **High Confidence:** The core mechanism of bypassing explicit kernel derivatives via interpolation is mathematically sound and the computational complexity claims (O(m²nd) for inference, O(m³+bmd) for optimization) are verifiable through matrix operation analysis.
- **Medium Confidence:** The empirical improvements over DSVGP/DDSVGP on synthetic benchmarks are demonstrated, but the molecular force field results (especially the d=1000 case) lack statistical significance testing and comparison to non-GP baselines.
- **Low Confidence:** The claim that DSoftKI "retains full derivative modeling capability" is questionable given the observed gradient prediction errors and the need for stabilization techniques that may introduce bias.

## Next Checks

1. **Stress Test Initialization:** Systematically vary interpolation point initialization (grid vs. random vs. k-means) and temperature initialization to quantify the impact on Cholesky stability and final test NLL.
2. **Scale Validation:** Reproduce the high-dimensional molecular force field experiments with d=500 and d=1000, measuring both gradient RMSE and wall-clock training time to verify the claimed linear scaling in d.
3. **Baseline Comparison:** Compare DSoftKI against a standard GPwD implementation using automatic differentiation (where kernel gradients are computed) on a 10-20 dimensional synthetic function to isolate the interpolation mechanism's contribution to accuracy.