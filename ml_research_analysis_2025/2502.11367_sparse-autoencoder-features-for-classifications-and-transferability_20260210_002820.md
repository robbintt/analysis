---
ver: rpa2
title: Sparse Autoencoder Features for Classifications and Transferability
arxiv_id: '2502.11367'
source_url: https://arxiv.org/abs/2502.11367
tags:
- features
- feature
- performance
- language
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic evaluation of sparse autoencoders
  (SAEs) for interpretable feature extraction from large language models (LLMs) in
  safety-critical classification tasks. The authors investigate SAE-based features
  across multiple model scales, languages, and modalities, examining the impact of
  architectural choices like layer depth, width, pooling strategies, and binarization.
---

# Sparse Autoencoder Features for Classifications and Transferability

## Quick Facts
- **arXiv ID:** 2502.11367
- **Source URL:** https://arxiv.org/abs/2502.11367
- **Reference count:** 40
- **Primary result:** SAE-derived features achieve macro F1 scores above 0.8, outperforming traditional baselines in toxicity detection tasks

## Executive Summary
This paper presents a systematic evaluation of sparse autoencoders (SAEs) for interpretable feature extraction from large language models (LLMs) in safety-critical classification tasks. The authors investigate SAE-based features across multiple model scales, languages, and modalities, examining the impact of architectural choices like layer depth, width, pooling strategies, and binarization. They demonstrate that SAE-derived features achieve macro F1 scores above 0.8, outperforming traditional baselines like hidden-state probing and bag-of-words methods. The study shows strong cross-lingual transfer capabilities, with features generalizing effectively from English to other languages in toxicity detection tasks. Additionally, SAE features enable smaller models to predict the behavior of larger instruction-tuned models, suggesting a scalable approach for model oversight and auditing. These findings establish practical best practices for SAE-based interpretability and highlight their potential for transparent deployment of LLMs in real-world applications.

## Method Summary
The authors systematically evaluate sparse autoencoder features for LLM interpretability by extracting features from multiple model scales and applying them to safety-critical classification tasks. They test architectural variations including different layer depths, widths, pooling strategies, and binarization approaches. The evaluation spans multiple languages and modalities, with a focus on toxicity detection and sentiment analysis. Cross-lingual transfer capabilities are assessed by training on English data and testing on other languages. The study also examines whether SAE features from smaller models can predict the behavior of larger instruction-tuned models, providing insights into model oversight and auditing capabilities.

## Key Results
- SAE-derived features achieve macro F1 scores above 0.8 in toxicity classification, outperforming traditional baselines
- Strong cross-lingual transfer capabilities demonstrated, with features generalizing effectively from English to other languages
- Smaller models can successfully predict the behavior of larger instruction-tuned models using SAE features

## Why This Works (Mechanism)
SAE features work by decomposing LLM activations into interpretable, sparse representations that capture semantically meaningful patterns. The sparsity constraint forces the model to activate only relevant features for specific inputs, creating cleaner signal separation compared to dense hidden-state representations. The binarization step further enhances transferability by creating discrete feature activations that generalize across model scales and languages. The hierarchical nature of SAEs allows them to capture both fine-grained and abstract linguistic patterns, making them effective for diverse classification tasks. The architecture's ability to learn disentangled representations enables smaller models to approximate larger model behavior by mapping to the same feature space.

## Foundational Learning
- **Sparse Autoencoders**: Neural networks that reconstruct inputs through a bottleneck with sparsity constraints; needed for interpretable feature extraction, quick check: activation patterns should be sparse (few active features per input)
- **Feature Binarization**: Converting continuous feature activations to binary values; needed for cross-model and cross-lingual transfer, quick check: features should be predominantly 0/1 rather than continuous
- **Cross-lingual Transfer**: Applying models trained on one language to another; needed for evaluating feature generalizability, quick check: performance drop between source and target languages should be minimal
- **Hidden-state Probing**: Using LLM internal states as features for downstream tasks; needed as a baseline comparison, quick check: probing accuracy should be lower than SAE features
- **Instruction-tuned Models**: LLMs fine-tuned on instruction-following datasets; needed to test oversight capabilities, quick check: SAE features should capture instruction-following behavior

## Architecture Onboarding

**Component Map:** Input Text -> SAE Layer(s) -> Feature Extraction -> Classification Layer(s) -> Output Prediction

**Critical Path:** The critical path is Input Text → SAE Layer(s) → Feature Extraction → Classification, as feature quality directly determines classification performance.

**Design Tradeoffs:** The main tradeoff is between feature interpretability (favoring sparsity and binarization) and information retention (favoring continuous representations). Layer depth affects abstraction level but increases computational cost. Pooling strategies balance local versus global feature capture.

**Failure Signatures:** Poor performance may manifest as low sparsity (features not interpretable), high cross-lingual performance gaps (features not generalizable), or inability to transfer between model scales (features too model-specific). Baseline comparisons will reveal if SAEs outperform simpler methods.

**First Experiments:**
1. Compare SAE feature performance against hidden-state probing and bag-of-words baselines on toxicity detection
2. Test cross-lingual transfer from English to Spanish/French toxicity classification
3. Evaluate whether SAE features from smaller models can predict larger model outputs on the same tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on toxicity detection and sentiment analysis, limiting generalizability to other safety-critical domains
- Cross-lingual performance testing limited to relatively few languages, with unclear performance on low-resource languages and non-Latin scripts
- Binarization may discard nuanced information critical for certain classification boundaries, though this trade-off was not systematically evaluated across task types

## Confidence

**High Confidence:**
- SAE-derived features outperforming traditional baselines in toxicity classification tasks
- Smaller models predicting larger model behavior using SAE features

**Medium Confidence:**
- Cross-lingual transfer claims supported but limited by small set of target languages
- Architectural choice effectiveness shows consistent patterns but may not generalize to all SAE implementations
- SAE features enabling "scalable model oversight" conceptually sound but lacks real-world validation

**Low Confidence:**
- SAE features establishing "practical best practices" overstates current state without comparative analysis to newer SAE variants

## Next Checks
1. Test SAE feature transferability on broader safety-critical tasks including misinformation detection, hate speech sub-type classification, and bias detection
2. Evaluate feature performance on low-resource languages and non-Latin scripts to identify linguistic limitations
3. Conduct ablation studies comparing binarized versus continuous SAE features across different task types to quantify information loss trade-offs