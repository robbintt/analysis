---
ver: rpa2
title: 'Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey'
arxiv_id: '2507.09662'
source_url: https://arxiv.org/abs/2507.09662
tags:
- reasoning
- arxiv
- preprint
- thinking
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey

## Quick Facts
- arXiv ID: 2507.09662
- Source URL: https://arxiv.org/abs/2507.09662
- Reference count: 33
- Primary result: None (survey paper)

## Executive Summary
Large reasoning models (LRMs) like DeepSeek-R1 and QwQ generate lengthy reasoning chains even for simple problems, wasting inference resources. This survey systematically reviews methods to make LRM reasoning more concise and adaptive, categorizing approaches into training-free (prompting, decoding manipulation) and training-based (fine-tuning, reinforcement learning with length penalties). The paper identifies three core mechanisms: budget forcing with early exit, difficulty-based routing, and RL reward shaping, while highlighting open challenges in balancing efficiency with accuracy, interpretability, and safety.

## Method Summary
This is a survey paper reviewing methods for concise and adaptive thinking in LRMs. No single method or experiment is proposed to reproduce. The paper synthesizes findings from 33+ papers, categorizing approaches into training-free (prompt-guided, pipeline, decoding manipulation, model merging) and training-based (fine-tuning with variable-length CoT data, RL with length penalty/GRPO variants). Key datasets include GSM8K, MATH-500, and models like DeepSeek-R1-Distill and QwQ.

## Key Results
- LRMs generate unnecessarily lengthy reasoning chains, with over 70% of tokens beyond the first correct answer providing diminishing returns
- Budget forcing and early exit mechanisms can reduce token usage while preserving accuracy for simpler queries
- Difficulty-based routing shows promise but faces reliability challenges in estimating query complexity before generation

## Why This Works (Mechanism)

### Mechanism 1
Constraining reasoning tokens during generation reduces redundant self-reflection while preserving answer correctness for simpler queries. Budget forcing and token budget prompting establish an upper bound on reasoning length. When models approach this threshold, generation transitions from deliberative "wait" patterns to solution output. Early exit mechanisms monitor confidence signals (logit-derived certainty, k-consecutive answer consistency) to terminate reasoning when semantic convergence is detected. The first correct solution in a reasoning chain provides the majority of accuracy value; subsequent verification steps contribute diminishing returns. Break condition: Hard token truncation without confidence monitoring may cut reasoning mid-step, causing answer degradation on genuinely complex problems requiring extended deliberation.

### Mechanism 2
Explicit routing based on input difficulty reduces inference cost by directing simple queries to fast-thinking modes and complex queries to slow-thinking modes. A router module (trained classifier or regressor) predicts query difficulty using embeddings or probe-based capability estimation. Queries below a difficulty threshold bypass CoT generation; queries above trigger full reasoning chains. Some implementations use multi-stage pipelines where instruction models generate outlines before reasoning models complete solutions. Difficulty can be reliably estimated from query embeddings before generation begins, and difficulty correlates with required reasoning length. Break condition: Router misclassification sends hard problems to fast mode (accuracy collapse) or easy problems to slow mode (efficiency loss). Corpus notes difficulty estimation from query alone is unreliable.

### Mechanism 3
Reinforcement learning with length-aware reward shaping produces models that adaptively compress reasoning chains based on problem complexity. Reward functions combine correctness signals with length penalties. Variants include: (1) conditional penalties applied only to correct answers, (2) difficulty-aware budgeting where simpler problems receive stronger penalties, (3) history-aware rewards that favor shorter correct solutions than prior attempts. GRPO variants prevent "format collapse" by maintaining reasoning mode diversity. Length-optimal reasoning is learnable, and the reward function accurately encodes the efficiency-accuracy trade-off without gaming. Break condition: Over-penalization causes "length collapse" where models sacrifice accuracy for brevity. Section 4.2.1 notes Light-R1 limits shortening rewards to prevent this.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning vs. Direct Answering**
  - Why needed here: The entire adaptive thinking paradigm depends on distinguishing System 2 (deliberative, step-by-step) from System 1 (intuitive, direct) reasoning modes.
  - Quick check question: Given a math word problem, can you identify when a model is generating intermediate reasoning steps versus immediately outputting an answer?

- **Reinforcement Learning Reward Shaping**
  - Why needed here: Length-penalized RL is the dominant training-based approach. Understanding how reward functions balance multiple objectives is essential for designing or debugging these systems.
  - Quick check question: If a reward function penalizes length unconditionally, what failure mode might emerge during training?

- **Token Budget and Early Exit Conditions**
  - Why needed here: Most training-free methods operate via budget constraints or confidence-based early stopping. Implementing these requires understanding when and how to intervene during generation.
  - Quick check question: What signals could indicate a model has reached a confident answer and should stop reasoning?

## Architecture Onboarding

- **Component map**: Query → Router → (Mode: fast or slow) → Budget Controller → Reasoning Core → Confidence Monitor → (Early exit or continue) → Final answer
- **Critical path**: Query flows through router for mode selection, then budget controller enforces token limits, reasoning core generates CoT, and confidence monitor decides on early exit.
- **Design tradeoffs**: Prompt-guided methods are simpler but rely on model compliance; decoding manipulation offers finer control but requires inference modifications. Routing adds latency overhead; RL embeds adaptivity but requires expensive training. Hard budgets are predictable but may truncate; confidence-based exit preserves accuracy but adds verification cost.
- **Failure signatures**: RL-based LRMs may ignore "skip thinking" prompts (section 2.1 notes this discrepancy). Length collapse causes accuracy drops as models prioritize brevity. Router drift degrades classification accuracy on out-of-distribution queries. Mid-step truncation cuts reasoning during critical computation, producing incoherent answers.
- **First 3 experiments**: 1) Baseline characterization: Measure token count and accuracy on GSM8K/MATH-500 split by difficulty; identify overthinking ratio. 2) Prompt-guided budget sweep: Test token budgets [128, 256, 512, 1024] with "Be concise" prompts; plot accuracy vs. token reduction curve. 3) Confidence-based early exit prototype: Implement k=3 consecutive answer consistency check at each reasoning step boundary; measure token savings and accuracy retention.

## Open Questions the Paper Calls Out

### Open Question 1
How can adaptive thinking optimization be advanced by simultaneously integrating input difficulty with model capability calibration? The paper states that relying solely on human-prioritized input difficulty is insufficient because variable-length CoT data may not reflect the specific model's inherent knowledge. It underscores "the critical need to optimize adaptive thinking by simultaneously integrating input difficulty with model capability calibration." Current methods often assume a universal difficulty level for inputs, ignoring that different models (e.g., SFT-based vs. RL-based) require different reasoning lengths (token budgets) to solve the same problem based on their unique capacities. Development of a unified framework or metric that dynamically adjusts reasoning length based on both the external problem difficulty and an internal assessment of the specific model's ability to solve it would resolve this.

### Open Question 2
How can standards be established for reasoning chains that align with human preference and cognitive patterns, particularly to maintain interpretability in specialized domains? The authors argue that "future research needs to establish standards for good reasoning chains across various domains, ensuring that models' reasoning processes align closely with human preference." Aggressive compression methods (concise thinking) currently conflict with the user preference for interpretability in fields like medicine or law, where users need to see the rationale even for simple queries. A set of metrics or human-evaluation protocols that measure "reasoning quality" not just by accuracy and length, but by adherence to human cognitive patterns and domain-specific interpretability requirements would resolve this.

### Open Question 3
What is the impact of concise or adaptive thinking on LRM trustworthiness regarding reasoning hallucinations, safety performance, and instruction-following adherence? The survey identifies a gap where "few studies have systematically considered or evaluated these three capabilities" in the context of concise/adaptive thinking. There is conflicting evidence regarding whether long CoT improves safety or introduces new risks (reasoning hallucination), and it remains unclear how shortening reasoning affects the model's ability to follow instructions. Comprehensive benchmarks specifically designed to evaluate safety, hallucination rates, and instruction-following compliance before and after applying concise or adaptive thinking optimizations would resolve this.

## Limitations

- Survey synthesizes findings across 33+ papers without direct experimental validation, creating potential blind spots in implementation feasibility
- Router-based approaches face fundamental reliability challenges that receive limited discussion regarding practical deployment impacts
- Length-penalized RL methods risk "length collapse" but the survey doesn't quantify failure rates or establish safe operating margins

## Confidence

- **High confidence**: The taxonomy structure and categorization of methods accurately reflects the current research landscape. Mechanism descriptions for budget forcing and early exit are well-supported by cited papers.
- **Medium confidence**: Claims about overthinking ratios and efficiency gains assume the cited papers' experimental setups are representative and correctly implemented.
- **Low confidence**: Recommendations for balancing accuracy and efficiency lack quantitative guidance on hyperparameter selection, particularly for RL reward coefficients and router difficulty thresholds.

## Next Checks

1. **Cross-model efficiency validation**: Test budget forcing and early exit mechanisms across SFT-based LRMs (DeepSeek-R1-Distill) and RL-based LRMs (QwQ) on the same GSM8K/MATH-500 splits. Compare overthinking ratios and measure whether RL models consistently ignore skip-thinking instructions as claimed.

2. **Router reliability assessment**: Implement difficulty estimation using multiple router architectures (embedding-based classifier vs. probe-based estimator) and evaluate accuracy degradation when routers misclassify hard problems as easy across diverse query distributions.

3. **Length penalty sensitivity analysis**: Systematically sweep RL reward coefficients for length penalties across multiple LRMs, measuring accuracy-token count Pareto frontiers to identify collapse thresholds and establish safe operating ranges for different problem complexity levels.