---
ver: rpa2
title: 'FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial
  Fraud Detection A Technical Report'
arxiv_id: '2507.19402'
source_url: https://arxiv.org/abs/2507.19402
tags:
- quantum
- classical
- financial
- fraud
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compares classical, quantum, and quantum-hybrid machine
  learning models for financial fraud detection on the IBM AML dataset. Classical
  tree-based models (Random Forest and XGBoost) significantly outperformed quantum
  models, achieving high accuracy (97.34%) and F-measure (86.95%).
---

# FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report

## Quick Facts
- arXiv ID: 2507.19402
- Source URL: https://arxiv.org/abs/2507.19402
- Reference count: 13
- Primary result: Classical tree-based models (Random Forest, XGBoost) significantly outperform quantum and quantum-hybrid models for financial fraud detection on the IBM AML dataset.

## Executive Summary
This study compares classical, quantum, and quantum-hybrid machine learning models for financial fraud detection using the IBM AML dataset. The research finds that classical tree-based models (Random Forest and XGBoost) significantly outperform quantum models, achieving high accuracy (97.34%) and F-measure (86.95%). Among quantum models, the Quantum Support Vector Machine (QSVM) showed the most promise, delivering high precision (77.15%) and low false-positive rate (1.36%), though with lower recall (41.20%) and higher computational overhead. The research highlights the current superiority of classical methods and the limited maturity of quantum approaches for this application, while suggesting that QSVM may be useful in high-precision, low-false-positive scenarios.

## Method Summary
The study evaluates multiple machine learning approaches for binary classification of fraudulent transactions using the IBM Transactions for Anti-Money Laundering (AML) dataset. The authors use a reduced, undersampled version with a 9:1 class ratio (non-suspicious:suspicious). Feature engineering includes behavioral features such as statistical aggregates (mean, std, min, max), temporal features (time since last transaction, exponentially weighted moving average), and pairwise metrics. Classical models include Logistic Regression, Decision Tree, Random Forest, and XGBoost (using Sklearn). Quantum models include QSVM, Variational Quantum Circuit (VQC), and Hybrid Quantum-Classical Neural Network (HQNN) implemented with PennyLane/PyTorch. The primary evaluation metrics are Accuracy, F-measure (F1), Precision, Recall, and False Positive Rate, with Random Forest achieving 97.34% accuracy and 86.95% F1-score as the benchmark.

## Key Results
- Classical tree-based models (Random Forest, XGBoost) significantly outperform quantum models, achieving 97.34% accuracy and 86.95% F1-score.
- QSVM delivers the highest precision (77.15%) and lowest false-positive rate (1.36%) among quantum models, though with limited recall (41.20%).
- VQC and HQNN models failed to detect positive fraud cases effectively, achieving near-zero recall.
- Quantum models exhibit significantly higher computational overhead compared to classical counterparts.

## Why This Works (Mechanism)
The study demonstrates that classical machine learning models, particularly tree-based ensembles, remain superior for financial fraud detection due to their ability to handle complex feature interactions and imbalanced classification tasks effectively. The quantum models, while showing promise in specific metrics like precision, struggle with the fundamental challenge of detecting rare positive instances (fraudulent transactions) in highly imbalanced datasets. The QSVM's ability to achieve high precision with low false-positive rates suggests potential utility in scenarios where minimizing false alarms is critical, despite its lower recall performance.

## Foundational Learning
- **Quantum Machine Learning**: Integration of quantum computing principles with machine learning algorithms to potentially achieve computational advantages for specific problem types. Why needed: To explore whether quantum approaches can outperform classical methods for complex classification tasks. Quick check: Verify that quantum circuit depth and noise levels are appropriate for the chosen problem scale.
- **Variational Quantum Circuits (VQC)**: Quantum circuits with parameterized gates that can be optimized through classical training loops. Why needed: To create flexible quantum models that can adapt to specific data distributions. Quick check: Confirm that the ansatz architecture provides sufficient expressivity for the problem complexity.
- **Quantum Kernel Methods**: Using quantum circuits to compute kernel functions for classical algorithms like Support Vector Machines. Why needed: To leverage quantum feature spaces that may be difficult to represent classically. Quick check: Validate that the quantum kernel provides meaningful separation between classes in feature space.

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Feature Engineering -> Classical Models (RF, XGBoost) -> Quantum Models (QSVM, VQC, HQNN) -> Evaluation Metrics

**Critical Path**: The most important workflow is the data preprocessing and feature engineering pipeline, as it directly impacts model performance across all approaches. The undersampling strategy (9:1 ratio) and behavioral feature engineering (statistical, temporal, pairwise) form the foundation for all subsequent model training and evaluation.

**Design Tradeoffs**: The study prioritizes precision over recall in quantum models, particularly QSVM, which may be suitable for applications where false positives are costly. However, this comes at the expense of missing actual fraud cases. Classical models provide better overall balance between precision and recall, making them more suitable for general fraud detection scenarios.

**Failure Signatures**: VQC and HQNN models consistently fail to identify positive fraud cases, resulting in near-zero recall. This suggests these architectures are currently unable to effectively process the feature space or capture the decision boundaries necessary for fraud detection. QSVM's computational overhead may become prohibitive for larger datasets or real-time applications.

**3 First Experiments**:
1. Implement the undersampling strategy to achieve the 9:1 class ratio and verify the dataset balance.
2. Engineer the behavioral features (statistical aggregates, temporal features, Pair_Equilibrium metric) and validate their statistical properties.
3. Train Random Forest and XGBoost models with default hyperparameters to establish baseline performance against the reported 97.34% accuracy and 86.95% F1-score.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can integrating temporal feature extraction methods, such as Long Short-Term Memory (LSTM) networks, with variational quantum circuits improve the detection of fraudulent patterns compared to non-temporal quantum models? Basis: Section 4 suggests combining temporal feature extraction with variational quantum circuits as a promising direction. Why unresolved: Current quantum models failed to capture positive class effectively without temporal dynamics.
- **Open Question 2**: Does the application of quantum autoencoders for feature compression improve the performance of quantum classifiers on high-dimensional financial data? Basis: Section 4 suggests utilizing quantum autoencoders for feature compression. Why unresolved: Poor performance of VQC/HQNN may stem from dataset lacking specific structures for quantum advantage.
- **Open Question 3**: How does the execution of the proposed FD4QC models on real Noisy Intermediate-Scale Quantum (NISQ) hardware compare to the idealized simulation results regarding latency and accuracy? Basis: The authors note that real quantum hardware was not used despite code compatibility. Why unresolved: Significant computational overhead observed in simulations may be exacerbated by real hardware noise and latency.

## Limitations
- The study uses a reduced, undersampled dataset (9:1 class ratio) that may not capture full real-world complexity of financial fraud.
- Critical hyperparameters for both classical and quantum models are unspecified, affecting reproducibility.
- QSVM's high precision at low recall raises questions about practical utility given the cost of missing fraud cases.

## Confidence
- **High Confidence**: Classical models (RF, XGBoost) performance superiority over quantum methods.
- **Medium Confidence**: QSVM's precision advantage and computational overhead claims.
- **Low Confidence**: Quantum hybrid models' (VQC, HQNN) reported inability to detect fraud cases.

## Next Checks
1. Test whether VQC/HQNN performance improves with alternative quantum encoding schemes or increased circuit depth.
2. Evaluate whether the 9:1 undersampling ratio is optimal, or if different ratios affect quantum-classical performance gaps.
3. Validate whether QSVM's high precision at low recall translates to cost savings in real deployment scenarios with asymmetric fraud penalties.