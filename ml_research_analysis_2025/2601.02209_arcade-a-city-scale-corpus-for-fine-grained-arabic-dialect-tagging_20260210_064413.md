---
ver: rpa2
title: 'ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging'
arxiv_id: '2601.02209'
source_url: https://arxiv.org/abs/2601.02209
tags:
- dialect
- arabic
- speech
- audio
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARCADE, the first Arabic speech corpus annotated
  with city-level dialect granularity. The dataset contains 3,790 unique 30-second
  audio segments from 58 cities across 19 countries, with 6,907 annotations covering
  MSA and dialectal speech.
---

# ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging

## Quick Facts
- arXiv ID: 2601.02209
- Source URL: https://arxiv.org/abs/2601.02209
- Reference count: 26
- First Arabic speech corpus with city-level dialect granularity

## Executive Summary
ARCADE introduces the first Arabic speech corpus annotated with city-level dialect granularity, containing 3,790 unique 30-second audio segments from 58 cities across 19 countries. Each clip was annotated by native Arabic speakers for emotion, speech type, dialect category, and confidence levels, with 6,907 total annotations. The corpus addresses the gap in fine-grained Arabic dialect identification resources, enabling city-level modeling and multi-task learning. Geographic audio quality analysis revealed significant variations, with North African and Gulf cities exhibiting higher quality recordings than Sudan and parts of the Levant.

## Method Summary
The dataset was created by collecting 30-second audio clips from streaming radio stations across the Arab world, targeting 10 clips per city and 5 cities per country. Native Arabic speakers annotated each clip for emotion, speech type, dialect category (MSA/dialect/mixed/N/A), keep/skip decision, and confidence level. The annotation interface used a custom Gradio-based web tool. Post-hoc quality metrics including SNR, silence ratio, dynamic range, and spectral centroid were computed per clip. The corpus underwent filtering to exclude Quranic recitation, music, and crosstalk content.

## Key Results
- 3,790 unique 30-second audio segments from 58 cities across 19 countries
- 6,907 annotations covering MSA and dialectal speech with 91.9% "sure" confidence
- 65.7% of clips retained after filtering out Quran, music, and crosstalk
- Geographic SNR variations: North African and Gulf cities show higher quality than Sudan and Levant

## Why This Works (Mechanism)

### Mechanism 1
City-level geographic labels enable finer-grained dialect modeling than country-level labels. Radio streams are tagged with specific city metadata, and native annotators validate dialect categories against this geographic ground truth. This creates a supervised signal linking acoustic features to sub-regional linguistic variation. Core assumption: Speaker location correlates with station location sufficiently often that city labels serve as proxies for dialect categories.

### Mechanism 2
Multi-task annotations (emotion, speech type, dialect category, confidence) improve downstream filtering and model training. Annotators label multiple attributes per clip. Keep/skip decisions exclude Quranic recitation, music, and crosstalk—content that suppresses or confounds dialectal cues. Confidence flags isolate ambiguous or non-local speech. Core assumption: Excluded content types systematically differ from dialectal speech in acoustic and linguistic properties.

### Mechanism 3
Audio quality metrics (SNR, silence ratio) correlate with content-based filtering decisions and vary geographically. Post-hoc analysis shows retained clips have higher SNR (15.25 dB vs. 9.36 dB) and lower spectral centroid than skipped clips. Regional SNR patterns reflect streaming infrastructure quality, not dialect. Core assumption: Acoustic degradation is independent of dialect; models must disentangle signal quality from linguistic features.

## Foundational Learning

- **Concept: Dialect vs. MSA distinction**
  - Why needed here: Annotators label clips as MSA, dialectal, mixed, or N/A. Mixed and regional-accented MSA create label ambiguity (Kappa = 0.310 despite 83% raw agreement).
  - Quick check question: Can you explain why a speaker producing MSA might still retain regionally identifiable phonetic features?

- **Concept: Inter-annotator agreement metrics (Cohen's Kappa vs. raw agreement)**
  - Why needed here: High raw agreement (90%+) coexists with moderate Kappa (0.18–0.59) due to class imbalance. Understanding this prevents misinterpreting annotation quality.
  - Quick check question: Why might Kappa be low even when annotators agree on 90% of labels?

- **Concept: Signal-to-Noise Ratio (SNR) and spectral centroid**
  - Why needed here: SNR distributions inform which clips are acoustically viable; spectral centroid distinguishes speech-dominated content from music. Geographic SNR variation may confound models if not controlled.
  - Quick check question: What does a lower spectral centroid indicate about audio content (speech vs. music)?

## Architecture Onboarding

- **Component map:**
  Radio Garden/World Radio Map -> Stream validation -> 30-second recording -> MP3 storage -> Gradio annotation interface -> CSV metadata -> SNR/silence analysis -> Filtered corpus

- **Critical path:**
  1. Verify stream accessibility and audio quality before recording
  2. Collect ≥10 clips per city, ≥5 cities per country (where infrastructure allows)
  3. Annotate with 1–3 native speakers per clip
  4. Filter via keep/skip decisions; exclude Quran, music, crosstalk
  5. Compute audio quality metrics; flag geographic quality imbalances

- **Design tradeoffs:**
  - 30-second segments: balances speaker homogeneity vs. data volume (shorter clips = more segments but less context)
  - Radio streams: broad geographic coverage but introduces noise, music, non-local speakers
  - Multi-annotator redundancy: targeted 3 annotators, but time constraints yielded 32.2% single-annotator clips

- **Failure signatures:**
  - Low SNR regions (Sudan, parts of Levant) may underperform in modeling
  - Geographic imbalance (Algeria 21.8%, Bahrain 14.7% vs. Tunisia 0.6%) requires weighted sampling or loss
  - Emotion imbalance (87.8% neutral) limits emotion recognition subtasks

- **First 3 experiments:**
  1. **Baseline dialect classification:** Train a simple acoustic model (e.g., wav2vec2 fine-tuning) on city-level labels using only "keep" clips with "sure" confidence. Measure accuracy across cities and correlate with SNR.
  2. **Geographic imbalance ablation:** Compare model performance with vs. without class weighting or oversampling for underrepresented cities.
  3. **Quality-confound test:** Train separate models on high-SNR vs. low-SNR subsets; assess whether SNR predicts accuracy independently of dialect category.

## Open Questions the Paper Calls Out

- Can city-level dialect identification models trained on radio broadcast speech generalize to other domains such as telephony and online video?
- How do geographic variations in audio quality confound dialect classification, and can models disentangle signal quality from true dialectal features?
- What modeling strategies can most effectively address the pronounced geographic imbalance in the corpus (e.g., weighted loss, targeted oversampling, data augmentation)?
- Can multi-task learning frameworks that jointly predict dialect category, emotion, and geographic origin improve city-level dialect identification compared to single-task baselines?

## Limitations

- Geographic coverage remains incomplete: 8 countries (Djibouti, Mauritania, Palestine, Somalia, Sudan, Syria, UAE, Yemen) have no coverage due to stream unavailability or access restrictions.
- Class imbalance affects model training: Algeria and Bahrain are over-represented (36.5% of clips combined), while other countries like Tunisia and Morocco have minimal representation.
- Single-speaker speech is rare: Only 37.3% of clips contain single-speaker regular speech, with the remainder including music, crosstalk, or mixed speech types.

## Confidence

- **High confidence:** Dataset creation methodology, inter-annotator agreement metrics, audio quality analysis, and geographic coverage statistics are well-documented and reproducible.
- **Medium confidence:** Dialect classification task validity depends on radio streams accurately reflecting local dialects; non-local speakers on local stations could reduce signal quality.
- **Medium confidence:** Multi-task annotations provide filtering value, though the impact on downstream model performance requires empirical validation.

## Next Checks

1. **Geographic representation audit:** Quantify dialect coverage gaps by mapping included cities against comprehensive Arabic dialect geography to identify systematic absences.
2. **Quality-confound isolation:** Train separate models on high-SNR vs. low-SNR subsets to measure whether recording quality independently predicts classification accuracy beyond dialect features.
3. **Mixed-class validation:** Conduct focused annotation of the 167 clips labeled as "mixed" by multiple annotators to determine if consistent patterns emerge or if the category represents genuine ambiguity.