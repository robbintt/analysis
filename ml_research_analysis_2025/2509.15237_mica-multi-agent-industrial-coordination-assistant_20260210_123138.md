---
ver: rpa2
title: 'MICA: Multi-Agent Industrial Coordination Assistant'
arxiv_id: '2509.15237'
source_url: https://arxiv.org/abs/2509.15237
tags:
- step
- multi-agent
- arxiv
- mica
- assembly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MICA, a perception-grounded multi-agent system
  for real-time industrial assistance on edge devices. The key innovation is Adaptive
  Step Fusion (ASF), which blends state-graph and retrieval-based step detection and
  updates via speech feedback to improve assembly recognition.
---

# MICA: Multi-Agent Industrial Coordination Assistant

## Quick Facts
- arXiv ID: 2509.15237
- Source URL: https://arxiv.org/abs/2509.15237
- Reference count: 40
- Primary result: MICA achieves 63.13% task success, 19.12% KBA, 0.71s latency, and 2.05 kJ energy per successful answer on industrial assembly tasks.

## Executive Summary
MICA introduces a perception-grounded multi-agent system for real-time industrial assistance on edge devices. The system couples egocentric vision with five role-specialized language agents, coordinated via a lightweight router and audited by a safety checker. The key innovation is Adaptive Step Fusion (ASF), which dynamically blends state-graph and retrieval-based step detection while adapting from user speech feedback. Experiments show MICA outperforms four baseline coordination topologies in task success, knowledge alignment, latency, and energy efficiency.

## Method Summary
MICA processes egocentric video through YOLOv11 detections fused with Depth-Anything depth estimates over a sliding window, extracting a depth-refined context for step recognition. ASF combines a state-graph detector (enforcing KB rules) and a retrieval detector (embedding similarity), dynamically weighting and updating via user speech corrections. The MICA-core routes queries to five specialized RAG agents (Assembly Guide, Parts Advisor, Maintenance Advisor, Fault Handler, General Agent), all audited by a KB/safety checker. The system runs on edge hardware with Qwen2.5-7B-Instruct, Whisper-small, and pyttsx3 TTS.

## Key Results
- Task Success (TS): 63.13% (highest among baselines)
- Knowledge Base Alignment (KBA): 19.12% (strongest)
- Average Latency (AL): 0.71s (lowest)
- Energy per Successful Answer: 2.05 kJ (lowest)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive Step Fusion (ASF) improves assembly step recognition by dynamically weighting complementary detectors based on their confidence and updating from user speech feedback.
- **Mechanism:** ASF maintains per-class expert weights, biases, and global gates. It scores each candidate step using a weighted combination of (a) a state-graph detector enforcing structural KB constraints and (b) a retrieval detector matching frame embeddings. On user verbal correction, it multiplicatively updates weights via a focal-style impact term, scales step size based on feedback frequency, and applies trust-region bounds to avoid collapse. A transition penalty and coverage bonus enforce workflow consistency.
- **Core assumption:** The two detectors make uncorrelated errors, so adaptive weighting can recover when one fails; users provide accurate corrections.
- **Evidence anchors:**
  - [abstract] "To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback."
  - [Section III-B] "S4 improves from 0% to 95.34% accuracy and achieves +90.36 F1, showing that feedback-driven reweighting is most effective when the state graph and retrieval detector diverge."
  - [corpus] Weak direct corpus validation for ASF specifically; related work (e.g., SagaLLM) addresses multi-agent coordination but not step fusion.
- **Break condition:** If user corrections are noisy or adversarial, or if both detectors consistently agree while being wrong, weight updates may reinforce errors rather than correct them.

### Mechanism 2
- **Claim:** Router-based specialization to role-specific RAG agents, combined with a safety auditor, yields higher factual alignment and lower latency than fully decentralized or debate-based topologies.
- **Mechanism:** A lightweight LLM router maps each incoming query to one of five specialized agents (Assembly Guide, Parts Advisor, Maintenance Advisor, Fault Handler, General Agent). Each agent retrieves KB evidence and reasons iteratively. All outputs pass through a KB/safety auditor enforcing tool-usage, order, and hazard constraints. Sparse activation (one primary agent) reduces redundant computation versus debate or broadcast topologies.
- **Core assumption:** Queries are sufficiently unambiguous for correct routing; the KB covers the necessary facts; safety rules are codified and auditable.
- **Evidence anchors:**
  - [abstract] "MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support."
  - [Section IV-D] "MICA achieves the highest task success (TS 63.13%) and the strongest knowledge base alignment (KBA 19.12%), while maintaining the lowest latency (0.71s) and the lowest energy per successful answer (2.05 kJ)."
  - [corpus] AISAC and SagaLLM similarly use orchestration and RAG but do not report industrial perception grounding or the same efficiency metrics.
- **Break condition:** Ambiguous phrasing or domain synonyms can cause misrouting; the safety auditor cannot correct for missing KB entries, only for rule violations.

### Mechanism 3
- **Claim:** Depth-guided multi-frame object context extraction stabilizes perception under occlusion and viewpoint variation, providing reliable input to downstream reasoning.
- **Mechanism:** YOLOv11 detections are aggregated over a sliding window (L frames), clustered by IoU, and fused by confidence-weighted averaging. Depth-Anything estimates depth; the nearest object to the camera is selected as focus, with nearby objects included via spatial/depth thresholds. This refined context O_rel feeds both step detection and MICA-core.
- **Core assumption:** Objects of interest are detectable in at least a subset of frames; depth estimates are sufficiently accurate; the worker's focus correlates with nearest depth.
- **Evidence anchors:**
  - [Section III-A] "Only this fused, depth-refined context is passed to subsequent modules."
  - [Section I] "MICA couples egocentric vision with multi-agent language reasoning to deliver real-time assembly, troubleshooting, part queries, and maintenance support."
  - [corpus] Prior egocentric vision work (EPIC-Kitchens, Ego4D, EDINA) addresses segmentation and anticipation but not the same fused depth-and-detection pipeline for industrial assistance.
- **Break condition:** Heavy occlusion or fast motion may reduce detection support below m=3; depth estimation can fail on reflective/transparent industrial parts.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG) with a structured KB
  - **Why needed here:** Each MICA agent grounds responses in curated component knowledge rather than hallucinating.
  - **Quick check question:** Can you explain how a retrieved KB passage alters an LLM's generation compared to pure parametric recall?

- **Concept:** Multi-agent coordination topologies (shared memory, broadcast, hierarchical, debate)
  - **Why needed here:** MICA's benchmark compares these to show why router-based sparse activation outperforms on latency and energy.
  - **Quick check question:** What is the communication and latency tradeoff between debate-based consensus and single-agent routing?

- **Concept:** Online adaptation without backpropagation (multiplicative weight updates)
  - **Why needed here:** ASF updates fusion weights from sparse feedback under constrained compute, avoiding gradient computation.
  - **Quick check question:** How does a trust-region bound on multiplicative updates prevent collapse to a single class?

## Architecture Onboarding

- **Component map:** Video frame -> YOLOv11 + Depth-Anything -> depth-guided context O_rel -> ASF (state-graph + retrieval) -> fused step S_f -> router -> specialist agent + KB retrieval -> safety audit -> speech response

- **Critical path:** Video frame → detection + depth → context O_rel → ASF scores → fused step S_f → router → specialist agent + KB retrieval → safety audit → speech response. Latency is dominated by LLM inference; ASF and perception are lightweight.

- **Design tradeoffs:**
  - Sparse routing (MICA) vs. dense debate/broadcast: higher task success and KBA, lower latency and energy, but routing errors are unrecoverable post-dispatch.
  - Two-detector fusion vs. single expert: improved robustness at the cost of maintaining weights/gates and update logic.
  - On-device vs. cloud: privacy and offline operation but limited model size (Qwen2.5-7B-Instruct).

- **Failure signatures:**
  - Step S4 near 0% pre-adaptation: both detectors consistently miss late-stage steps; ASF requires user feedback to recalibrate.
  - Maintenance queries misrouted to detection-focused agents: intent ambiguity leads to factual but incomplete answers.
  - DebateVoting high latency/energy: redundant agent activations without proportional quality gains.

- **First 3 experiments:**
  1. **Ablate ASF:** Run step recognition with only state-graph or only retrieval detector, comparing accuracy and ECE to fused ASF before/after 10 feedback updates.
  2. **Topology swap:** Evaluate MICA-core with SharedMemory or DebateVoting orchestration under identical KB and budgets, measuring TS, KBA, latency, and E/succ.
  3. **Routing stress test:** Inject ambiguous or synonym-heavy queries, log router decisions and downstream TS/KBA, and quantify misrouting sensitivity.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does MICA perform in large-scale, real-world user studies involving non-expert operators?
  - **Basis in paper:** [explicit] The conclusion states future work will "extend large-scale user studies."
  - **Why unresolved:** Current evaluation relies on automated benchmarks, GPT-based scores, and isolated qualitative examples, lacking validation of human factors like cognitive load, trust, or actual assembly error reduction.
  - **What evidence would resolve it:** Quantitative results from deployed user studies measuring operator task completion time, error rates, and subjective usability ratings.

- **Open Question 2:** To what extent does MICA maintain robust step recognition and coordination under severe visual occlusion or domain shift?
  - **Basis in paper:** [explicit] The conclusion identifies the need to "enhance robustness under occlusion and domain shift."
  - **Why unresolved:** The benchmark evaluates coordination using fixed video segments and controlled ground-truth labels as inputs to isolate coordination effects, potentially masking perception failures common in cluttered real-world environments.
  - **What evidence would resolve it:** Performance metrics (Task Success, KBA) evaluated on datasets featuring synthetic or natural occlusion patterns and different industrial environments.

- **Open Question 3:** Can end-to-end learned policies for agent routing and safety auditing improve upon the current modular, heuristic-based approach?
  - **Basis in paper:** [explicit] The conclusion proposes to "explore end-to-end policies for multi-agent coordination and safety auditing."
  - **Why unresolved:** MICA currently relies on a lightweight LLM router and a rule-based safety checker; the trade-off between the flexibility of learned policies and the interpretability of the current modular design remains unquantified.
  - **What evidence would resolve it:** Ablation studies comparing the current MICA-core against a variant with jointly trained routing and auditing modules on the established benchmark.

## Limitations

- Evaluation relies on a proprietary Gear8 dataset and custom KB, limiting external reproducibility.
- ASF assumes clean, truthful user corrections and uncorrelated detector errors—fragile in noisy real settings.
- Routing ambiguity and safety auditor reliance on complete KB entries are unmeasured failure points.
- Energy/latency comparisons assume identical hardware; gains may not transfer to larger models or cloud deployment.

## Confidence

- **High** in ASF's effectiveness given the quantitative lift (S4 0%→95.34%, ECE drop) and ablation-like contrast with single-detector baselines.
- **Medium** in the coordination claim: MICA's efficiency edge is clear, but routing correctness and safety coverage depend on KB quality not disclosed in the paper.
- **Low** in perception robustness: depth-guided selection and multi-frame fusion are asserted but not stress-tested under occlusion, motion blur, or reflective surfaces.

## Next Checks

1. Inject synthetic speech noise into ASF feedback and measure weight drift, F1, and ECE over 10 updates to quantify robustness to user error.
2. Create ambiguous query variants (synonyms, incomplete phrases) and log router decisions; compute misrouting rate and downstream TS/KBA degradation.
3. Remove KB entries for selected steps/rules and re-run safety auditing; measure how often auditor blocks otherwise correct agent responses.