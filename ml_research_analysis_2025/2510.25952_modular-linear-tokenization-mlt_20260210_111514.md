---
ver: rpa2
title: Modular Linear Tokenization (MLT)
arxiv_id: '2510.25952'
source_url: https://arxiv.org/abs/2510.25952
tags:
- linear
- tokenization
- encoding
- modular
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modular Linear Tokenization (MLT) is a reversible, deterministic
  technique for encoding high-cardinality categorical identifiers into compact numerical
  vectors using modular arithmetic over finite fields and invertible linear transformations.
  Unlike traditional hashing or one-hot encodings, MLT guarantees bijective mappings
  and explicit control over dimensionality while preserving full reversibility, even
  for millions of identifiers.
---

# Modular Linear Tokenization (MLT)

## Quick Facts
- arXiv ID: 2510.25952
- Source URL: https://arxiv.org/abs/2510.25952
- Reference count: 14
- Primary result: Achieves 63.31% accuracy with 14 dimensions and zero learned parameters, significantly outperforming hashing while maintaining reversibility

## Executive Summary
Modular Linear Tokenization (MLT) is a reversible, deterministic technique for encoding high-cardinality categorical identifiers into compact numerical vectors using modular arithmetic over finite fields and invertible linear transformations. Unlike traditional hashing or one-hot encodings, MLT guarantees bijective mappings and explicit control over dimensionality while preserving full reversibility, even for millions of identifiers. The method decomposes identifiers into base-p digits, applies an invertible matrix over the finite field Zp, and ensures uniqueness through prime number selection. Experimental results on the MovieLens 20M dataset show MLT achieves 63.31% accuracy with only 14 dimensions and zero learned parameters, significantly outperforming hashing (57.87%) and approaching one-hot encoding (74.18%) performance while reducing training time from 5029.90s to 65.04s per epoch. MLT offers a practical alternative for scalable, reproducible, and efficient categorical data representation in machine learning architectures.

## Method Summary
MLT encodes categorical identifiers by first decomposing them into base-p digits using prime numbers, then applying an invertible linear transformation matrix over the finite field Zp. The method guarantees bijective mapping by ensuring the selected primes satisfy specific mathematical conditions that prevent collisions. The dimensionality is explicitly controlled by the number of prime factors used in the base decomposition, while reversibility is maintained through the invertibility of the transformation matrix. The encoding process is entirely deterministic and requires no learned parameters, making it both reproducible and computationally efficient. The technique is particularly effective for high-cardinality categorical data where traditional one-hot encoding becomes infeasible due to dimensionality explosion.

## Key Results
- MLT achieves 63.31% accuracy with only 14 dimensions compared to 57.87% for hashing and 74.18% for one-hot encoding
- Training time per epoch reduced from 5029.90s to 65.04s compared to one-hot encoding
- Zero learned parameters required, maintaining reproducibility and eliminating optimization overhead
- Guarantees bijective mapping and full reversibility even for identifier spaces exceeding millions of unique values

## Why This Works (Mechanism)
MLT leverages the mathematical properties of finite fields and modular arithmetic to create unique, reversible mappings for categorical identifiers. By decomposing identifiers into base-p digits where p is prime, the method exploits the fundamental theorem of arithmetic to ensure unique factorization. The invertible linear transformation over Zp preserves this uniqueness while allowing dimensionality reduction through the selection of appropriate prime bases. The reversibility is guaranteed by the invertibility of the transformation matrix, which is constructed using properties of finite field arithmetic. This approach combines the theoretical guarantees of one-hot encoding (complete uniqueness) with the computational efficiency of hashing (compact representation), while avoiding the collision problems inherent in traditional hashing methods.

## Foundational Learning

**Finite Field Arithmetic (Zp)**: Operations performed over integers modulo a prime p, where every non-zero element has a multiplicative inverse. Why needed: Provides the mathematical foundation for invertible transformations and guarantees unique mappings. Quick check: Verify that the selected primes are indeed prime and that the transformation matrix is invertible in Zp.

**Base-p Decomposition**: Representing numbers as sums of powers of a base p, similar to decimal or binary representation but using prime bases. Why needed: Enables the conversion of high-cardinality identifiers into structured digit representations suitable for linear transformation. Quick check: Confirm that the decomposition is complete and that all identifiers map to unique digit sequences.

**Invertible Matrix Theory**: Square matrices with non-zero determinants over finite fields, ensuring the existence of matrix inverses. Why needed: Guarantees that the transformation can be reversed to recover the original identifiers. Quick check: Calculate the determinant of the transformation matrix and verify it's non-zero in Zp.

**Bijective Mapping**: One-to-one correspondence between two sets where each element in one set maps to exactly one element in the other set. Why needed: Ensures that no two different identifiers produce the same encoded vector, preserving information integrity. Quick check: Test that the encoding-decoding process recovers all original identifiers without collisions.

**Modular Arithmetic**: Arithmetic operations performed with wrap-around at a specified modulus, following the rules of congruence. Why needed: Provides the computational framework for working within finite fields and maintaining bounded numerical representations. Quick check: Verify that all arithmetic operations respect modular constraints and produce expected results.

## Architecture Onboarding

**Component Map**: Categorical ID -> Base-p Decomposition -> Digit Vector -> Invertible Linear Transform -> Encoded Vector

**Critical Path**: The transformation from original identifier to encoded vector involves decomposition into prime-based digits, matrix multiplication over Zp, and dimension reduction through prime selection. Each step is deterministic and reversible.

**Design Tradeoffs**: MLT trades the absolute uniqueness of one-hot encoding for computational efficiency through dimensionality reduction, while maintaining theoretical guarantees of reversibility. The method requires careful prime selection to balance between compact representation and numerical stability.

**Failure Signatures**: Poor prime selection can lead to suboptimal dimensionality reduction or increased computational complexity. Numerical precision issues may arise in floating-point implementations for very large identifier spaces. The method assumes known or estimable identifier space size, which may not hold in dynamic environments.

**First Experiments**:
1. Implement base-p decomposition for a small set of identifiers and verify unique digit representations
2. Test matrix invertibility in Zp for various prime selections and matrix dimensions
3. Encode-decode test with 1000 identifiers to confirm reversibility and measure encoding accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on prime number selection, with poor choices leading to inefficiency
- Assumes known or estimable identifier space size, problematic for streaming or open-world scenarios
- Floating-point implementations may introduce numerical precision issues affecting reversibility for very large identifier spaces
- Limited experimental validation across diverse datasets beyond MovieLens 20M

## Confidence

**High Confidence**: The mathematical foundations of modular arithmetic and finite field operations are well-established and correctly applied. The claim of bijective mapping and reversibility is theoretically sound given the constraints on prime selection and matrix invertibility.

**Medium Confidence**: The experimental results showing superior performance to hashing while maintaining reversibility are convincing, but the comparison is limited to one dataset (MovieLens 20M). The claimed training time improvements (65.04s vs 5029.90s) are impressive but need validation across diverse datasets and model architectures.

**Low Confidence**: The assertion that MLT "significantly outperforms" hashing (63.31% vs 57.87%) may overstate the practical significance, as this represents an 8.7% relative improvement in accuracy. The practical impact of this difference in real-world applications is not clearly established.

## Next Checks
1. Test MLT across multiple diverse datasets (e.g., Amazon reviews, Netflix Prize, and categorical-only datasets) to verify generalizability of the performance improvements.
2. Conduct ablation studies varying prime selection strategies and matrix dimensions to determine optimal configurations and identify potential failure modes.
3. Evaluate numerical precision and reversibility guarantees across different floating-point implementations and extremely large identifier spaces (100M+ unique values) to confirm theoretical bounds hold in practice.