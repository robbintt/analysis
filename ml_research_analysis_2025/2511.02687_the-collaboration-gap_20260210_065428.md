---
ver: rpa2
title: The Collaboration Gap
arxiv_id: '2511.02687'
source_url: https://arxiv.org/abs/2511.02687
tags:
- gpt-4
- gemini-2
- maze
- agents
- collaboration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how current AI agents perform in collaborative
  settings, revealing a "collaboration gap" where models that excel individually often
  fail when required to work together. To assess this, the authors introduce a novel
  maze-solving benchmark designed to isolate and measure collaborative capabilities
  without imposing output-format constraints.
---

# The Collaboration Gap

## Quick Facts
- arXiv ID: 2511.02687
- Source URL: https://arxiv.org/abs/2511.02687
- Reference count: 40
- Primary result: Models that excel individually often fail when required to work together in collaborative settings.

## Executive Summary
This study investigates the "collaboration gap" where AI models perform significantly worse when working together than when working individually. The authors introduce a novel maze-solving benchmark to measure collaborative capabilities without output-format constraints, enabling scalable automated grading. Across 32 leading models, they find substantial performance drops in collaborative scenarios, particularly for distilled models. The study reveals that interaction order matters significantly, with stronger models priming weaker ones through relay inference—a single opening message from a capable model can substantially improve outcomes for weaker partners.

## Method Summary
The authors created a maze-solving benchmark where two agents with partial information (each seeing ~50% of the map) must coordinate to navigate from start to goal. They generated 6×6 mazes with 30% wall density and 7-9 step solutions, then split each maze into two partial views. Agents engaged in multi-turn dialogue (max 50 turns) to agree on moves, with a third-party grader model parsing the unstructured transcript to extract the path. The benchmark enabled automated evaluation across 32 open- and closed-source models, measuring binary success rates and weighted outcomes.

## Key Results
- Significant performance drops occur when models collaborate versus working individually
- Distilled models show disproportionately large collaboration gaps
- Interaction order matters: starting with the stronger agent leads to better outcomes
- Relay inference (strong model priming followed by handoff to weaker model) closes much of the collaboration gap

## Why This Works (Mechanism)

### Mechanism 1: Relay Inference Seeding
Strong models establish unambiguous grounding conventions in initial turns, which weaker partners then adopt. This prevents the "cold start" coordination problem where both agents must simultaneously invent and align on representations. Empirical evidence shows priming with a single strong model message significantly boosts weak model performance.

### Mechanism 2: First-Mover Grounding Quality
Strong performers immediately specify coordinate systems, complete state representations, and explicit requests for missing information in opening messages. This creates scaffolding that reduces downstream ambiguity. Weak models provide vague descriptions without committing to schemas, forcing partners to infer conventions.

### Mechanism 3: Style Imitation Cascade
When weaker models lead, stronger models imitate suboptimal communication patterns, degrading overall performance. Models exhibit accommodation behavior—adjusting communication style toward their partner's established patterns. This creates asymmetric dependency: strong→weak transfers good practices; weak→strong transfers bad practices.

## Foundational Learning

- **Grounding in Multi-Agent Communication**
  - Why needed here: Core failure mode is agents failing to establish mutual understanding of coordinates, actions, and state
  - Quick check question: If Agent A uses (row, column) with origin at top-left, and Agent B interprets (1,2) as (column=1, row=2) with bottom-left origin, where do they think the move "go to (1,2)" leads?

- **Partial Observability and Information Fusion**
  - Why needed here: The benchmark enforces collaboration by splitting information—each agent sees only ~50% of cells
  - Quick check question: Agent A reports cell (2,3) as wall; Agent B sees it as hidden. What should the joint belief be, and what action should follow?

- **Distillation Effects on Collaborative Transfer**
  - Why needed here: Distilled models show disproportionately large collaboration gaps
  - Quick check question: A distilled model solves mazes at 90% solo accuracy but 15% in collaboration. What capability might distillation have removed?

## Architecture Onboarding

- **Component map**: Maze generator -> Information splitter -> Agent orchestrator -> Grader agent -> Schema normalizer -> Outcome validator
- **Critical path**: Maze generation → information split → dialogue rollout → transcript grading → schema normalization → validation. Grading consistency (ICC ≈ 0.84-0.89) is the primary reliability bottleneck.
- **Design tradeoffs**: Unstructured output (ecological validity) vs. structured protocols (easier evaluation); 50% obfuscation level (forces interdependence) vs. other ratios (different difficulty calibration); third-party grader (scalable) vs. deterministic parsing (fragile to format variations)
- **Failure signatures**: Coordinate grounding mismatch; perceptual conflict without resolution mechanism; agreement loops; premature completion phrase usage; agents splitting up to explore independently; strong model imitating weak model's degraded communication style
- **First 3 experiments**:
  1. Establish solo baselines: Run each model on (a) full maze visibility and (b) distributed visibility (both partial maps to single agent)
  2. Quantify order effects: For 3-5 heterogeneous pairs, run both orderings (strong→weak and weak→strong) on identical mazes
  3. Relay point sweep: For strong-weak pairs, test handoff after K ∈ {2, 4, 6, 8} turns in both directions

## Open Questions the Paper Calls Out
None

## Limitations
- 6×6 grid size and 50% obfuscation may not capture complexities of real-world multi-agent scenarios
- Reliance on third-party grader introduces evaluation uncertainty despite ICC ≈ 0.84-0.89
- Mechanism explaining why strong models fail to "rescue" weak partners (style imitation cascade) remains qualitative

## Confidence

**High Confidence**: The existence of a collaboration gap, quantitative magnitude of ordering effects, and relay inference benefit are all empirically demonstrated.

**Medium Confidence**: Explanation for ordering effects relies on qualitative analysis of opening messages and style imitation observations, which could involve multiple interacting factors.

**Low Confidence**: Claims about distilled models losing "interactive reasoning patterns" during training are speculative and not experimentally isolated.

## Next Checks

1. **Mechanism Isolation Experiment**: Run strong-weak pairs where the strong model is explicitly instructed to maintain its preferred communication style regardless of partner behavior. Compare performance to uninstructed pairs to quantify the style imitation effect.

2. **Grader Robustness Audit**: Systematically test the grader on transcripts using non-standard but valid communication formats. Measure false negative rates and identify format patterns that cause grader failures.

3. **Scaling Validation**: Replicate core findings (ordering effects, relay benefits) on larger mazes (10×10 or 15×15) with varied obfuscation levels (30%, 40%, 60%). This tests whether collaboration dynamics scale predictably with problem complexity.