---
ver: rpa2
title: Scalable and consistent few-shot classification of survey responses using text
  embeddings
arxiv_id: '2508.19836'
source_url: https://arxiv.org/abs/2508.19836
tags:
- responses
- qualitative
- data
- coding
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for few-shot classification of
  open-ended survey responses using text embeddings. The method requires only a handful
  of examples per category and integrates with standard qualitative workflows.
---

# Scalable and consistent few-shot classification of survey responses using text embeddings

## Quick Facts
- arXiv ID: 2508.19836
- Source URL: https://arxiv.org/abs/2508.19836
- Reference count: 0
- Achieves Cohen's Kappa of 0.74-0.83 on physics education survey data using text embeddings

## Executive Summary
This paper introduces a framework for few-shot classification of open-ended survey responses using text embeddings. The method requires only a handful of examples per category and integrates with standard qualitative workflows. When benchmarked on a dataset of 2,899 physics education survey responses, the framework achieved Cohen's Kappa scores ranging from 0.74 to 0.83 compared to human expert coders in exhaustive coding tasks. Performance improved with fine-tuning of the embedding model. The approach also enables auditing of previously coded datasets to identify inconsistencies. Results demonstrate that embedding-based classification can scale qualitative analysis to large datasets while maintaining interpretability and consistency.

## Method Summary
The framework follows a 5-step process: (1) minimal data preparation preserving natural text, (2) embedding responses using transformer models, (3) creating category centroids by averaging representative response embeddings, (4) classifying via cosine similarity to nearest centroid, and (5) evaluation. The approach requires only a small number of representative examples per category to define category centroids in embedding space. Fine-tuning with contrastive learning on labeled pairs can improve performance by reshaping the embedding space. The method was tested on 2,899 physics education survey responses with 4 categories, achieving strong agreement with human coders.

## Key Results
- Achieved Cohen's Kappa of 0.74-0.83 compared to expert human coders
- Fine-tuning improved Kappa from 0.38-0.46 to 0.50-0.53 on a subset of data
- Identified 669 coding inconsistencies in previously analyzed dataset, resolving 531

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic similarity in embedding space enables classification with minimal examples
- Mechanism: Transformer-based embedding models map text into high-dimensional vector space where semantically similar responses cluster together. By computing category centroids (averaged vectors of representative examples), new responses are classified via proximity to these prototypes.
- Core assumption: The embedding model's semantic representation aligns with the researcher's conceptual categories.
- Evidence anchors:
  - [abstract] "achieves a Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in an exhaustive coding scheme"
  - [section] Figure 2 illustrates centroid creation: "Representative responses for each category (bold circles) are averaged to create a category centroid"
  - [corpus] Limited direct corpus evidence on embedding-based survey classification; related work focuses on LLM prompting approaches rather than embedding centroids.
- Break condition: Categories requiring inferential reasoning beyond semantic content (e.g., psychological constructs) will likely fail, as noted: "text embedding models represent semantic meaning of text; they are...likely less suited for analysis of psychological or motivational constructs."

### Mechanism 2
- Claim: Fine-tuning reshapes embedding geometry to improve category separation
- Mechanism: By providing labeled pairs (same-category vs. different-category), contrastive learning updates the embedding space to push similar items closer and dissimilar items apart, creating more distinct category regions.
- Core assumption: Sufficient labeled examples exist to define category boundaries without overfitting.
- Evidence anchors:
  - [abstract] "performance of this framework improves with fine-tuning of the text embedding model"
  - [section] Table 4 shows Kappa improvement from 0.38-0.46 to 0.50-0.53 after fine-tuning; Figure 4 visualizes "regions corresponding to different codes have become significantly more distinct"
  - [corpus] No corpus papers examine fine-tuning embeddings for survey classification specifically.
- Break condition: Small or noisy training data may not provide sufficient signal; paper used only 93 responses for fine-tuning.

### Mechanism 3
- Claim: Embedding distance enables automated audit of coding inconsistencies
- Mechanism: Pairwise cosine distances between responses reveal semantically similar items with different codes, flagging potential human coding errors that keyword matching cannot detect.
- Core assumption: Semantically similar responses should receive the same code.
- Evidence anchors:
  - [abstract] "method can be used to audit previously-analyzed datasets"
  - [section] "669 responses were nearly identical in content or phrasing to some other response with a different code...531 of the inconsistencies were resolved by reclassifying 153 of the responses"
  - [corpus] Corpus papers do not address embedding-based audit methods.
- Break condition: Legitimate edge cases where similar phrasing warrants different codes would be incorrectly flagged.

## Foundational Learning

- Concept: **Transformer text embeddings**
  - Why needed here: Core representation mechanism; understanding that embeddings capture contextual semantic meaning enables interpreting why classification works.
  - Quick check question: Why would "bank" have different embeddings in "river bank" vs. "investment bank"?

- Concept: **Cosine similarity**
  - Why needed here: Distance metric for comparing response embeddings to category centroids; cosine measures angle rather than magnitude.
  - Quick check question: When would Euclidean distance give different rankings than cosine similarity?

- Concept: **Few-shot learning via prototypical networks**
  - Why needed here: Framework adapts image classification technique (prototypical networks) to text; centroids serve as category prototypes.
  - Quick check question: Why average multiple example embeddings rather than use a single representative?

## Architecture Onboarding

- Component map:
  1. **Data layer**: Raw survey responses (tabular format, minimal preprocessing)
  2. **Embedding layer**: Pre-trained transformer model (e.g., Jina, Nomic, Mixedbread)
  3. **Centroid layer**: Averaged vectors from manually-selected representative responses per category
  4. **Classification layer**: Cosine similarity computation + nearest-centroid assignment
  5. **Optional fine-tuning**: Contrastive learning on labeled pairs to reshape embedding space

- Critical path: Representative response selection → embedding → centroid creation → distance computation → classification. Representative quality is the highest-leverage decision point.

- Design tradeoffs:
  - Model size vs. latency: Paper selected smaller models (e.g., Jina small v2) balancing MTEB performance with computational cost
  - Example count vs. overfitting: Used ~2% of dataset (59 responses) for centroids; more examples may introduce noise
  - Instructions vs. generality: Some models support task-specific prefixes ("classification:", "STS:") with mixed results (Table 2)

- Failure signatures:
  - "Other" category collapse: Kappa dropped from 0.74-0.83 to 0.38-0.46 when including catch-all category
  - Category overlap in t-SNE visualization (Figure 3) indicates poor separation
  - High variance across resampling runs (SD ~0.03) suggests sensitivity to example selection

- First 3 experiments:
  1. **Baseline validation**: Apply framework to a held-out subset with known human codes; compute Kappa, F1, MCC against ground truth using multiple pre-trained models.
  2. **Ablation on representative count**: Systematically vary number of examples per category (3, 5, 10, 20) to identify minimum viable examples for acceptable Kappa (>0.6).
  3. **Audit detection threshold**: Run pairwise distance audit at different cosine thresholds (0.1, 0.15, 0.2); manually verify flagged pairs to optimize precision/recall for inconsistency detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework perform when classifying datasets that require inference of psychological or motivational constructs rather than distinct content categories?
- Basis in paper: [explicit] The authors state that because embedding models represent semantic meaning, they are "likely less suited for analysis of psychological or motivational constructs that are identified based on latent or gestalt textual features."
- Why unresolved: The study only benchmarked the framework on a dataset with "fairly distinct" content categories (sources of variability in physics).
- What evidence would resolve it: Benchmarking the framework against human coders on datasets labeled for latent constructs (e.g., student motivation or identity).

### Open Question 2
- Question: How can researchers systematically predict the effects of embedding model choice and representative example selection on classification accuracy?
- Basis in paper: [explicit] The authors note that "it is unclear how to choose a specific embedding model for such a task, or to systematically predict the effects of varying the chosen example texts on the results."
- Why unresolved: The study relied on general benchmarks (MTEB) for model selection and qualitative judgment for example selection, lacking a predictive heuristic.
- What evidence would resolve it: A sensitivity analysis mapping changes in example quality/quantity and model architecture to classification performance metrics.

### Open Question 3
- Question: Can the centroid-based approach be effectively adapted for coding schemes that are not mutually exclusive (multi-label)?
- Basis in paper: [explicit] The authors identify a "need to adapt this framework to different kinds of qualitative coding schemes, especially multiclass schemes that are not mutually exclusive."
- Why unresolved: The current methodology assigns the category of the single "nearest category centroid," which enforces mutual exclusivity.
- What evidence would resolve it: Extending the framework to utilize continuous confidence scores (e.g., softmax) for multi-label assignment and testing on appropriate datasets.

## Limitations
- Struggles with semantically incoherent "Other" categories, with Kappa dropping from 0.74-0.83 to 0.38-0.46
- Fine-tuning lacks detailed hyperparameter specification, making replication difficult
- Best suited for deductive coding with content categories rather than inferential psychological constructs

## Confidence

- **High confidence**: The core few-shot classification framework using embedding centroids achieves Kappa scores of 0.74-0.83 for exhaustive coding tasks.
- **Medium confidence**: Fine-tuning improves classification performance (Kappa 0.38-0.46 to 0.50-0.53), but limited training data and unspecified hyperparameters reduce confidence.
- **Medium confidence**: The audit method successfully identifies coding inconsistencies (669 flagged, 531 resolved), but manual verification was required and no automated validation metrics are provided.

## Next Checks

1. **Model robustness test**: Apply the framework to a different qualitative dataset (e.g., interview transcripts or different survey domain) to assess generalizability beyond physics education responses.
2. **Representative selection sensitivity**: Systematically vary the selection criteria for representative responses (random sampling, semantic clustering, expert selection) and measure impact on classification stability and performance variance.
3. **Fine-tuning ablation study**: Implement controlled fine-tuning experiments varying learning rate, batch size, and training epochs on the 93-response subset to identify optimal hyperparameters and quantify performance ceiling.