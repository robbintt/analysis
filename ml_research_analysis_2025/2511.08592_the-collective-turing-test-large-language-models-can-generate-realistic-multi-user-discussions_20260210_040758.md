---
ver: rpa2
title: 'The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User
  Discussions'
arxiv_id: '2511.08592'
source_url: https://arxiv.org/abs/2511.08592
tags:
- conversations
- social
- reddit
- comments
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tests whether Large Language Models (LLMs) can convincingly
  simulate human-like conversations on social media using authentic Reddit data. Participants
  were shown side-by-side conversations, one human-generated and one LLM-generated,
  and asked to identify which was human.
---

# The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User Discussions

## Quick Facts
- **arXiv ID**: 2511.08592
- **Source URL**: https://arxiv.org/abs/2511.08592
- **Reference count**: 40
- **Primary result**: LLM-generated conversations were mistaken for human-created content 39% of the time

## Executive Summary
This study tests whether Large Language Models can convincingly simulate human-like conversations on social media using authentic Reddit data. Participants were shown side-by-side conversations, one human-generated and one LLM-generated, and asked to identify which was human. LLM-generated conversations were mistaken for human-created content 39% of the time. Llama 3 70B outperformed GPT-4o, with participants correctly identifying Llama 3 conversations as AI-generated only 56% of the time—barely better than random chance. These findings demonstrate that LLMs can generate realistic social media conversations, highlighting their potential for generative agent-based modeling while also raising concerns about misuse for generating inauthentic content.

## Method Summary
The study collected 16 Reddit posts from December 2023 across 8 topics, preserving the comment tree structure and word counts while replacing the actual text with LLM-generated content. Using GPT-4o and Llama 3 70B with standardized prompts and temperature variations (0.2, 0.7, 1.2), researchers generated synthetic conversations that matched the original structure. Participants viewed side-by-side comparisons of real and synthetic conversations and attempted to identify which was human. The evaluation included binary classification with confidence ratings, attention checks, and post-survey qualitative feedback analyzed through multilevel logistic regression and qualitative coding.

## Key Results
- LLM-generated conversations were mistaken for human-created content 39% of the time
- Llama 3 70B outperformed GPT-4o in generating realistic conversations (56% detection vs 66% detection, P < 0.001)
- Participants primarily relied on stylistic features (tone, formality, politeness) rather than content to distinguish human from AI conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Llama 3 70B generates more human-like social media conversations than GPT-4o due to training data composition and optimization objectives.
- Mechanism: Models trained on informal online discourse (vs. assistant-optimized models) produce outputs with more organic stylistic patterns—less polished, more variable—that align with authentic social media interactions.
- Core assumption: The paper hypothesizes but does not prove that Llama 3's training data includes substantial social media content; this is inferred from Meta's data sources and observed output characteristics.
- Evidence anchors:
  - [Results]: "Participants had a lower success rate in identifying human conversations when they were presented with simulations generated with Llama 3 70B than with GPT-4o (0.56 vs 0.66, P < 0.001)"
  - [Discussion]: "Llama 3, as a locally deployable model, may generate more organic and less polished responses than GPT-4o, which is optimized for reliability and safety in commercial applications"
  - [corpus]: Limited corpus support; related work on LLM Turing tests exists but direct comparisons of Llama vs GPT-4o for social simulation are sparse.
- Break condition: If future work reveals training data overlap with test conversations, performance differences may reflect memorization rather than genuine stylistic capability.

### Mechanism 2
- Claim: Conversation length exhibits a non-linear (inverse-U) relationship with detection accuracy, contrary to linear expectations.
- Mechanism: Short conversations lack sufficient signal for detection; moderate lengths (8 comments) provide enough evidence for pattern recognition; longer threads (16+) may cause evaluator fatigue or introduce naturalistic variance that masks AI origin.
- Core assumption: The paper speculates about fatigue and variance but acknowledges these are hypotheses requiring further investigation.
- Evidence anchors:
  - [Results]: "Length generally has no significant effect on success rates, except for length = 8, which is associated with a significantly higher success probability"
  - [Results]: "Once conversation length reaches 16 posts, participant performance approaches random chance"
  - [corpus]: No corpus evidence found for non-linear length effects in LLM detection tasks.
- Break condition: If the length effect is primarily driven by fatigue rather than content characteristics, experiments with time-controlled conditions should eliminate the effect.

### Mechanism 3
- Claim: Stylistic features (tone, formality, politeness) dominate human detection strategies over content-based cues.
- Mechanism: LLMs—particularly safety-aligned models—produce text with lower toxicity, higher politeness, and reduced linguistic variation compared to authentic social media discourse. Humans detect these systematic divergences more readily than semantic inconsistencies.
- Core assumption: Detection cues identified by high-performing participants generalize to broader populations.
- Evidence anchors:
  - [Results]: "Textual style features served as the primary indicator participants used when distinguishing human from AI conversations, with 82.5% of responses mentioning at least one feature from this category"
  - [Results]: "Most comments mentioned differences in tone (49.1% of total comments), text formality (21.1%), followed by politeness and language diversity"
  - [corpus]: Related work (Pagan et al.) confirms persistent stylistic divergences in affective and stylistic dimensions between human and LLM-generated social media text.
- Break condition: If models are fine-tuned or prompted to match informal style distributions, this detection pathway should weaken significantly.

## Foundational Learning

- Concept: Agent-Based Modeling (ABM) validation
  - Why needed here: The paper positions LLM-based social simulation as a new approach to ABM; understanding traditional ABM limitations (oversimplification, calibration challenges) clarifies why generative agents are proposed as a solution.
  - Quick check question: What are two limitations of traditional ABM that generative agents might address?

- Concept: Turing Test methodology (multi-user extension)
  - Why needed here: The study extends the classic Turing test from dyadic conversation to group discussions; the 30% misclassification threshold provides a historical benchmark.
  - Quick check question: How does the "Collective Turing Test" differ from the original formulation, and why does 39% misclassification matter?

- Concept: Multilevel logistic regression for nested experimental data
  - Why needed here: Participants complete multiple annotations; standard regression would violate independence assumptions. The paper uses varying intercepts for participants.
  - Quick check question: Why include participant-level random effects in this experimental design?

## Architecture Onboarding

- Component map:
  - Data layer: Reddit Pushshift dataset (Dec 2023), 16 posts across 8 topics, comment tree structures preserved
  - Generation layer: GPT-4o and Llama 3 70B with standardized prompts (no persona assignment), temperature variations (0.2, 0.7, 1.2), word-count matching
  - Presentation layer: HTML-rendered side-by-side comparisons using Potato annotation framework, anonymized usernames/profiles
  - Evaluation layer: Binary classification task with confidence ratings, attention checks, post-survey qualitative feedback
  - Analysis layer: Multilevel logistic regression, qualitative coding (Format/Style/Content taxonomy)

- Critical path:
  1. Sample selection: Posts must post-date model training cutoffs to avoid contamination
  2. Structure preservation: Comment trees and word counts are fixed; only content varies
  3. Controlled comparison: Side-by-side presentation controls for topic and structure effects
  4. Quality filtering: 18% of participants excluded via attention checks

- Design tradeoffs:
  - Minimal prompting (no personas) limits realism but enables controlled comparison; persona-driven approaches may improve authenticity for specific use cases
  - Single-turn generation (new LLM instance per comment) prevents conversational drift but sacrifices coherence
  - Reddit-only data limits generalization to other platforms (Twitter, Facebook)

- Failure signatures:
  - Detection rate approaches 50%: Model may be adequately simulating target domain (desirable for simulation, concerning for misuse)
  - High agreement on specific cues (formality, politeness): Model exhibits systematic safety-aligned behaviors that mismatch authentic discourse
  - Length = 16 accuracy collapse: Evaluator fatigue or model degradation in extended conversations

- First 3 experiments:
  1. Replicate with persona-driven prompting: Assign distinct user profiles (age, ideology, expertise) to evaluate whether personalization improves authenticity scores
  2. Test on contentious topics: Sample from politically polarized subreddits to assess whether LLMs can simulate disagreement and conflict (current models show excessive conformity)
  3. Cross-platform validation: Apply methodology to Twitter threads or Facebook discussions to test generalization beyond Reddit's comment structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the implementation of persona-driven prompting improve the authenticity and emotional range of LLM-generated social media discussions compared to the minimal prompting strategy used in this study?
- Basis in paper: [explicit] The conclusion states: "Another promising direction involves using multiple persona-driven LLM agents in conversations to assess whether this setup improves authenticity, fosters personal storytelling, or generates more emotionally intense or controversial discourse."
- Why unresolved: The present study utilized a "minimal prompting strategy," asking models to behave like standard Reddit users without assigning specific personas, resulting in a noted lack of personal storytelling and emotional nuance.
- What evidence would resolve it: A comparative experiment where distinct personas are assigned to agents, measuring participant detection rates and qualitative analysis of emotional intensity.

### Open Question 2
- Question: What mechanism drives the non-linear relationship between conversation length and human detection accuracy, specifically the decline in detection success for threads longer than eight comments?
- Basis in paper: [explicit] The Discussion notes: "This non-linear pattern suggests that longer conversations... may introduce enough natural-sounding variance to mask the LLM origin, or may lead to participant fatigue. Both hypotheses warrant further investigation."
- Why unresolved: While the study observed a significant peak in detection at length 8 and a decline at length 16, the statistical analysis could not distinguish between increased AI realism and participant fatigue.
- What evidence would resolve it: Eye-tracking or attention-span monitoring during the experiment to measure fatigue, or a qualitative analysis of semantic drift in longer AI threads.

### Open Question 3
- Question: Can LLMs simulate toxic or highly polarized dynamics with sufficient realism to be valid for agent-based modeling of social conflicts?
- Basis in paper: [explicit] The Conclusion lists "Limitations persist in simulating emotional and polarizing dynamics" as a key takeaway, noting that "current LLMs in simulating toxic or emotionally charged behavior... is crucial for studying online conflict."
- Why unresolved: Participants identified AI content as "overly polite" and devoid of profanity; the models' safety alignment prevents the generation of the toxic behavior common in authentic polarized Reddit threads.
- What evidence would resolve it: Generating conversations in specifically contentious subreddits and testing for the presence of toxic language or measuring the "conformity" and "excessive agreement" rates identified in the qualitative analysis.

### Open Question 4
- Question: Do other prominent proprietary or open-weight models (e.g., Claude, Mistral) outperform Llama 3 70B in generating realistic multi-user discussions?
- Basis in paper: [explicit] The Conclusion suggests: "Future work should extend this 'Collective Turing Test' approach to include additional LLMs, such as proprietary models like Claude and open-weight models like Mistral. These evaluations could serve as benchmarks for selecting models in social simulations."
- Why unresolved: The study was limited to comparing GPT-4o and Llama 3 70B, leaving the performance of other major architectures untested.
- What evidence would resolve it: Replicating the "Collective Turing Test" experimental design using identical Reddit stimuli with Claude and Mistral models.

## Limitations

- Generalizability is constrained by Reddit-specific dataset and absence of persona-driven prompting, which may artificially limit LLM performance in contexts where personalized agents are expected
- The lack of temporal analysis prevents understanding whether detection accuracy evolves with extended interaction or repeated exposure
- The paper does not address potential safety implications of successful social media simulation beyond general warnings

## Confidence

- **High confidence**: The finding that Llama 3 70B generates more human-like conversations than GPT-4o (P < 0.001) is statistically robust, supported by both quantitative results and qualitative participant feedback on stylistic features
- **Medium confidence**: The inverse-U relationship between conversation length and detection accuracy is based on observed data but the proposed mechanisms (evaluator fatigue, content variance) remain speculative without controlled experiments
- **Medium confidence**: The claim that stylistic features dominate detection strategies over content is well-supported by qualitative coding (82.5% of responses mentioning style) but may not generalize to populations with different familiarity with AI-generated text

## Next Checks

1. **Controlled temporal analysis**: Replicate the experiment with the same conversation pairs presented to participants at different time intervals to determine whether detection accuracy improves with repeated exposure or evolves over time

2. **Cross-platform generalization test**: Apply the Collective Turing Test methodology to Twitter threads and Facebook discussions to verify whether the observed performance differences between Llama 3 and GPT-4o generalize beyond Reddit's comment structure

3. **Persona-driven prompting validation**: Implement distinct user profiles (age, ideology, expertise) in the generation process and measure whether this personalization significantly reduces the 39% misclassification rate, testing whether the current "minimal prompting" approach artificially constrains LLM performance