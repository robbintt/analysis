---
ver: rpa2
title: 'MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large
  Language Models'
arxiv_id: '2502.11513'
source_url: https://arxiv.org/abs/2502.11513
tags:
- multi-task
- learning
- task
- arxiv
- mazo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenges of multi-task fine-tuning
  for large language models (LLMs) under zeroth-order (ZO) optimization, which is
  memory-efficient but suffers from high gradient variance and task conflicts. The
  proposed Masked Zeroth-Order Optimization (MaZO) framework introduces two key innovations:
  a weight importance metric to identify critical parameters for each task, and a
  multi-task weight update mask to selectively update these parameters while freezing
  others.'
---

# MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models

## Quick Facts
- **arXiv ID**: 2502.11513
- **Source URL**: https://arxiv.org/abs/2502.11513
- **Reference count**: 40
- **Primary result**: MaZO achieves state-of-the-art multi-task fine-tuning performance for LLMs under zeroth-order optimization, outperforming both existing ZO methods and first-order approaches

## Executive Summary
MaZO addresses the challenge of multi-task fine-tuning for large language models under zeroth-order optimization, which is memory-efficient but suffers from high gradient variance and task conflicts. The framework introduces a weight importance metric to identify critical parameters for each task and a multi-task weight update mask to selectively update these parameters while freezing others. This reduces parameter space dimensionality, mitigates gradient variance, and resolves task conflicts at the parameter level. Experiments on LLaMA-2-7B and Mistral-7B demonstrate MaZO achieves state-of-the-art performance, outperforming both existing ZO methods and first-order multi-task learning approaches.

## Method Summary
MaZO operates by computing per-task weight importance scores (combining global and greedy metrics), normalizing them row-wise, aggregating across tasks, and selecting top-k parameters per row to create a binary mask M. This mask is applied to ZO-SGD updates, effectively reducing the active parameter space. The method uses row-wise Hessian approximation (H ∝ xx⊤) for tractable importance scoring. When combined with LoRA, the mask is applied to the decomposed A·B product. Hyperparameters include α=10, β=1 for scoring, sparsity ρ=0.9 for full-model or ρ=0.8+LoRA rank 64 for LoRA setting.

## Key Results
- MaZO achieves 66.9 average accuracy without LoRA and 71.3 with LoRA on LLaMA-2-7B, outperforming baselines
- Parameter masking reduces gradient variance by lowering effective dimensionality in ZO optimization
- MaZO resolves task conflicts at the parameter level, enabling effective multi-task learning where FO methods fail
- With 80% sparsity, MaZO outperforms magnitude-based and random masking approaches by 3+ points

## Why This Works (Mechanism)

### Mechanism 1
Dimensionality reduction via masking lowers gradient variance in ZO optimization. ZO gradient variance scales with the number of perturbed parameters. By computing importance scores and freezing less critical parameters (creating sparse mask M), the effective dimensionality drops, reducing variance while preserving model capacity for task-relevant updates.

### Mechanism 2
Parameter-level masking circumvents ZO gradient collinearity that breaks FO multi-task methods. In ZO, all task-specific gradients gt = αt·z are collinear (same direction z, different scalars αt). Dynamic weighting only rescales the gradient, not its direction. MaZO activates different parameter subsets per task via row-wise importance scoring, enabling task-specific optimization paths despite shared random perturbation.

### Mechanism 3
Dual scoring (global + greedy) captures both theoretical and practical parameter importance under high ZO variance. Global score measures theoretical loss impact if a parameter were frozen. Greedy score captures immediate single-step loss change accounting for ZO's high variance. Weight magnitude regularization (β|W|) adds stability.

## Foundational Learning

- **Zeroth-order gradient estimation** (Simultaneous Perturbation Stochastic Approximation): MaZO builds on MeZO-style ZO-SGD; understanding ∇̂L(θ) = [L(θ+εz) - L(θ-εz)]/(2ε) · z is prerequisite. Quick check: Can you explain why ZO variance scales with parameter dimensionality d?

- **Multi-task gradient conflict**: Core motivation; task conflicts are amplified under ZO collinearity, requiring parameter-level resolution. Quick check: Why does standard loss-weighting (Eq. 3) fail to resolve conflicts when all gradients point in the same direction?

- **Row-wise Hessian approximation**: Full Hessian computation is infeasible for 7B parameters; row-wise approximation (H ∝ xx⊤) enables tractable scoring. Quick check: What information is lost when approximating ∂²L/∂w²i ∝ xx⊤ versus computing the full Hessian?

## Architecture Onboarding

- **Component map**: Input activations x → importance scoring → mask M construction → per-step ZO update → masked application
- **Critical path**: Mask is computed once before training; applied every step
- **Design tradeoffs**: Sparsity ρ: Higher → lower variance but risk freezing critical parameters. Optimal found at ρ=0.9 (full-model) or ρ=0.8 with LoRA rank 64. α/β balance: α=10, β=1 found optimal; increase α if convergence is noisy.
- **Failure signatures**: Near-zero improvement over zero-shot: Check if mask is too sparse or importance scores are uniform. Slower convergence than vanilla MTL-ZO: Likely sparsity too low. Task imbalance: Importance scores dominated by high-loss tasks.
- **First 3 experiments**: 1) Baseline replication: Run MTL-ZO and MaZO on LLaMA-2-7B with SST-2/BoolQ/COPA/SQuAD, verify Table 1 performance gap. 2) Sparsity sweep: Grid search ρ ∈ {0.5, 0.7, 0.8, 0.9, 0.95} with fixed α=10, β=1; plot convergence curves. 3) Ablation: scoring method: Compare MaZO vs. random/magnitude/Wanda at 50% sparsity; expect MaZO ~3+ points higher average.

## Open Questions the Paper Calls Out

1. How does the interplay between mask sparsity and model scale affect MaZO's performance when applied to models larger than 7B parameters? (Basis: Experiments limited to 7B models; optimal sparsity levels determined only at this scale.)

2. Can more sophisticated gradient and Hessian estimation techniques beyond the row-wise approximation improve MaZO's effectiveness? (Basis: Current approximations are effective but could be refined through more sophisticated estimation techniques.)

3. What are the formal convergence guarantees for MaZO's parameter masking approach under zeroth-order optimization? (Basis: Paper does not provide theoretical convergence analysis for the masking approach.)

4. Can MaZO's parameter-level masking strategy improve first-order multi-task optimization methods? (Basis: Parameter-level approach not limited solely to zeroth-order optimization, offering potential integrations with other optimization strategies.)

## Limitations

- Learning rate η and perturbation scale ε for ZO-SGD are not specified, though critical for convergence and stability
- Exact computation of Hessian approximation via ∇L(yi) and ∇²L(yi) scalars remains unclear with implementation details missing
- Training duration is unspecified beyond runtime estimates (14-16 hours)
- LoRA integration details unspecified (which modules receive decomposition)

## Confidence

- **High Confidence**: Core mechanism of parameter masking based on importance scoring is well-grounded and mathematically sound
- **Medium Confidence**: Claim that parameter-level masking uniquely resolves ZO-specific gradient collinearity lacks direct corpus support
- **Medium Confidence**: Dual scoring approach shows strong empirical results but theoretical justification for specific hyperparameters is limited

## Next Checks

1. Ablation on scoring weights: Systematically vary α (0→20) and β (0→5) in the dual scoring formula to identify sensitivity and optimal values

2. Gradient collinearity measurement: Quantify the angular alignment between task-specific ZO gradients in both masked and unmasked settings

3. Mask stability analysis: Track how the importance-based mask M evolves during training and whether it converges to a stable configuration