---
ver: rpa2
title: Continuous Diffusion for Mixed-Type Tabular Data
arxiv_id: '2312.10431'
source_url: https://arxiv.org/abs/2312.10431
tags:
- cont
- data
- cdtd
- noise
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CDTD is a continuous diffusion model for mixed-type tabular data
  that combines score matching with score interpolation to handle both continuous
  and categorical features in a unified way. It introduces adaptive, type-specific
  noise schedules and calibrates losses to balance model capacity across features
  and data types, addressing the high heterogeneity in tabular data.
---

# Continuous Diffusion for Mixed-Type Tabular Data

## Quick Facts
- **arXiv ID:** 2312.10431
- **Source URL:** https://arxiv.org/abs/2312.10431
- **Reference count:** 40
- **Primary result:** Proposes CDTD, a continuous diffusion model for mixed-type tabular data that outperforms state-of-the-art models across 10 datasets in sample quality and scalability.

## Executive Summary
CDTD introduces a novel continuous diffusion framework for mixed-type tabular data that unifies continuous and categorical features through embedding-based score interpolation. The method combines score matching for continuous features with a novel score interpolation approach for categorical features, enabling the application of Gaussian diffusion to discrete data via learned embeddings. CDTD addresses the high heterogeneity in tabular data through adaptive, type-specific noise schedules and calibrated loss functions that balance model capacity across feature types. Experimental results demonstrate consistent improvements over state-of-the-art generative models in detection scores, correlation matrix preservation, and scalability to high-cardinality categorical features.

## Method Summary
CDTD is a continuous diffusion model that handles mixed-type tabular data by embedding categorical features into continuous space and applying Gaussian noise schedules. The model uses a unified continuous noise distribution where categorical features are diffused through learned embeddings rather than discrete one-hot vectors. Key innovations include type-specific adaptive noise schedules that learn optimal noise levels per feature type, and calibrated loss functions that balance continuous (MSE) and categorical (CE) losses by normalizing them to equal 1 at the terminal timestep. The architecture follows TabDDPM with a 5-layer MLP, embedding layers for categorical features (16-dim), and outputs heads for both continuous and categorical predictions. Training uses Adam optimizer (LR 0.001) for 30k steps with Euler sampling (200 steps) during generation.

## Key Results
- CDTD consistently outperforms state-of-the-art generative models across 10 datasets in detection scores and correlation matrix preservation
- Better scalability to high-cardinality categorical features compared to discrete diffusion approaches
- Enables direct application of advanced diffusion techniques like classifier-free guidance
- Maintains sample quality while reducing training time compared to per-feature noise schedule approaches

## Why This Works (Mechanism)

### Mechanism 1: Embedding-Based Score Interpolation
CDTD enables continuous Gaussian diffusion for categorical data by diffusing learned embeddings rather than discrete one-hot vectors. Categorical variables are mapped to a continuous embedding space where Gaussian noise is added, and the model predicts the denoised embedding via score interpolation. This allows the model to represent categorical uncertainty as "in-between" states in continuous space, minimizing cross-entropy loss between predicted and ground truth probability distributions.

### Mechanism 2: Type-Specific Loss Calibration
The model scales continuous (MSE) and categorical (CE) losses so they equal 1 at the terminal timestep, creating a fair weighted sum. Continuous features are scaled by variance (assuming unit variance), while categorical features are scaled by feature entropy. This calibration prevents the model from biasing toward feature types with higher-magnitude loss signals.

### Mechanism 3: Adaptive Time Warping (Noise Schedules)
Instead of a global noise schedule, CDTD learns feature-specific or type-specific schedules using a domain-adapted Logistic distribution. This function maps global time to feature-specific noise levels, parameterized to predict loss at given noise levels and inverted to find optimal noise levels for each time step. The location parameter controls the proportion of high versus low noise levels.

## Foundational Learning

- **Score Matching vs. Denoising Score Matching**
  - Why needed: CDTD relies on estimating the gradient of log-likelihood (the score) w.r.t. data to push noisy data back toward high-density regions
  - Quick check: Why does the model predict the "score" (or equivalently, the noise/denoised data) rather than predicting the class label directly during the diffusion process?

- **Gaussian Noise Schedules (σ(t))**
  - Why needed: The core novelty manipulates the noise schedule; σ(t) dictates the signal-to-noise ratio at time t
  - Quick check: In a standard schedule, σ(t) increases monotonically from 0 to Max. How does CDTD modify this for a feature with high cardinality compared to a binary feature?

- **Embedding Layers for Categorical Data**
  - Why needed: To apply continuous diffusion to discrete categories, CDTD projects categories into ℝᵈ
  - Quick check: How does adding Gaussian noise to an embedding vector differ mathematically from adding noise to a one-hot encoded vector?

## Architecture Onboarding

- **Component map:** Data Input (continuous + categorical embeddings) -> Noise Scheduler (F⁻¹ᵈ.ᵃ.ˡᵒᵍ) -> Noising Step (apply σₖ) -> Score Model (MLP) -> Output Head (MSE/CE)

- **Critical path:** The Loss Calibration step is the most common point of failure. If normalization constants (Zⱼ for categorical entropy) are not computed correctly before training, gradients will favor one data type over the other.

- **Design tradeoffs:**
  - Per-Type vs. Per-Feature Schedules: "Per-Type" is often best (Table 1); "Per-Feature" adds too many constraints/parameters (3× features) and requires more data
  - Diffusion Space: CDTD diffuses in data space (via embeddings), unlike latent diffusion. This improves interpretability and correlation capture but may scale less efficiently than compressed latent representations

- **Failure signatures:**
  - Mode Collapse / NaNs: If σₘₐₓ is insufficient for categorical embeddings (paper sets σcₐₜ,ₘₐₓ=100 vs σcₒₙₜ,ₘₐₓ=80), the signal is never fully destroyed, leading to training instability
  - Imbalanced Generation: If calibration is skipped, continuous features might look perfect while categorical features are pure noise (or vice versa)

- **First 3 experiments:**
  1. Sanity Check (Visualize Noise): Generate samples using "Per-Type" schedule on Adult dataset; plot learned σ(t) curves for Continuous vs. Categorical types
  2. Ablation (Calibration): Train with full calibration vs. naive loss averaging; compare Detection Scores
  3. Scalability Stress Test: Train on lending dataset with 3151 categories; measure training time against TabDDPM

## Open Questions the Paper Calls Out
- Can CDTD be effectively adapted for missing data imputation without requiring a separate conditional training process?
- Can advanced ODE solvers or specialized neural network architectures be integrated into CDTD to reduce computational cost?
- Can the tuning of adaptive noise schedule hyperparameters be automated to reduce computational overhead for new datasets?
- How does CDTD perform on temporal tabular data compared to generative models specifically designed for time-series dependencies?

## Limitations
- Addition of hyperparameters makes tuning costly and dataset-dependent
- Current framework designed for unconditional generation; adaptation to imputation requires additional mechanisms
- Assumes i.i.d. samples and may not capture temporal correlations in time-series data
- Embedding-based diffusion assumes continuous embedding space adequately captures category semantics, which may fail for very high-cardinality features

## Confidence

- **High Confidence:** Embedding-based score interpolation for categorical features - well-supported by theoretical derivation and aligns with emerging trends
- **Medium Confidence:** Type-specific loss calibration - theoretically sound but depends on accurate entropy estimation and baseline assumptions
- **Medium Confidence:** Adaptive time warping - shows promise but lacks extensive comparative validation against established global schedules

## Next Checks

1. **Embedding Dimensionality Sensitivity:** Systematically vary categorical embedding dimension (8, 16, 32) and measure impact on generation quality, particularly for high-cardinality features

2. **Cross-Dataset Schedule Transferability:** Train a single CDTD model on lending dataset and evaluate noise schedules on Adult dataset to test robustness of learned adaptive schedules

3. **Latent vs. Data Space Diffusion Comparison:** Implement latent-space variant of CDTD and compare sample quality and training efficiency on large dataset to validate tradeoff between interpretability and scalability