---
ver: rpa2
title: A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking
arxiv_id: '2505.19023'
source_url: https://arxiv.org/abs/2505.19023
tags:
- monkeypox
- classification
- skin
- images
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents ITMA\u2019INN, an AI-powered healthcare system\
  \ designed to detect and monitor Monkeypox from skin lesion images using deep learning\
  \ techniques. The system integrates three components: a DL-based classification\
  \ model, a cross-platform mobile application for real-time diagnosis and symptom\
  \ tracking, and a real-time monitoring dashboard for health authorities."
---

# A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking

## Quick Facts
- arXiv ID: 2505.19023
- Source URL: https://arxiv.org/abs/2505.19023
- Reference count: 6
- Primary result: DL-based mobile system achieves 97.8% binary classification accuracy for Monkeypox detection

## Executive Summary
This paper presents ITMA'INN, an AI-powered healthcare system for detecting and monitoring Monkeypox from skin lesion images. The system combines a deep learning classification model, a cross-platform mobile application for real-time diagnosis and symptom tracking, and a real-time monitoring dashboard for health authorities. Using transfer learning on augmented datasets, the system achieves high accuracy while maintaining lightweight deployment requirements for mobile devices. The work aligns with Saudi Arabia's Vision 2030 for smart healthcare infrastructure and addresses the need for accessible, scalable diagnostic tools during disease outbreaks.

## Method Summary
The system employs transfer learning with pretrained models (ViT, MobileViT, TNT, VGG16, etc.) fine-tuned on two Kaggle datasets: MSLD (binary classification) and MSLD v2.0 (multiclass classification). Data augmentation techniques including rotation, shear, translation, color jitter, and elastic deformation expand the training data. Models are trained using PyTorch 2.0.1 with timm library on Google Colab Pro with NVIDIA Tesla T4 GPU. Stratified 80:20 splits and 5-fold cross-validation evaluate performance, with early stopping to prevent overfitting. The best-performing MobileViT model balances accuracy with computational efficiency for mobile deployment.

## Key Results
- Binary classification (Monkeypox vs. non-Monkeypox): 97.8% accuracy achieved by MobileViT, ViT, TNT, and VGG16 models
- Multiclass classification (6 classes): 92% accuracy achieved by ResNetViT and ViT Hybrid models
- MobileViT identified as optimal for deployment due to lightweight architecture and strong performance
- System successfully deployed as cross-platform mobile application with real-time monitoring dashboard

## Why This Works (Mechanism)

### Mechanism 1
Hybrid vision transformers like MobileViT outperform traditional CNNs by integrating local inductive biases with global attention mechanisms. Convolutions efficiently capture local spatial features while transformers model long-range dependencies via self-attention. MobileViT fuses these approaches, enabling recognition of both fine-grained lesion texture and overall structural context without heavy computational costs. This fusion is critical for the superior 97.8% accuracy, as it balances local feature detection with global contextual understanding.

### Mechanism 2
Transfer learning enables high performance on small medical datasets by bootstrapping general visual features from large-scale natural image datasets like ImageNet. Rather than learning from scratch, the system initializes with pre-trained weights and fine-tunes them to recognize specific pathological features of Monkeypox. This approach effectively overcomes data scarcity by leveraging generic edge and shape detectors that transfer well to dermatological imagery, assuming sufficient similarity between natural images and skin lesion features.

### Mechanism 3
Real-time surveillance is enabled by a synchronous data pipeline converting individual diagnostic events into aggregated geospatial trends. User interactions in the mobile app write to Firestore, stream to BigQuery, and visualize via Power BI dashboards. This architecture decouples diagnostic data capture from analytical processing, allowing health authorities to monitor outbreaks without accessing raw clinical app data. The system assumes consistent user participation with location permissions to ensure dashboard accuracy reflects actual ground truth.

## Foundational Learning

- **Vision Transformers (ViT) vs. CNNs**
  - Why needed here: The paper benchmarks transformer-based models against CNNs. Understanding how transformers handle global context differently (patch-based attention vs. sliding windows) is required to interpret model selection.
  - Quick check question: How does splitting an image into patches (ViT) fundamentally change what features the model prioritizes compared to a convolutional kernel?

- **Class Imbalance & Augmentation**
  - Why needed here: The datasets (MSLD and MSLD v2.0) are relatively small (228 to 755 original images). Understanding how augmentation (rotations, color jitter) artificially expands the decision boundary is critical for reproducing the 97.8% accuracy.
  - Quick check question: Why would training on the original 228 images likely lead to overfitting, and how does rotation/shearing specifically prevent that?

- **Edge AI / Mobile Deployment Constraints**
  - Why needed here: The system prioritizes a mobile app (Flutter). You must understand the trade-off between model size (parameters/FLOPs) and accuracy to justify why MobileViT was chosen over heavier models like VGG16 or ViT-Hybrid.
  - Quick check question: Why is the number of Floating Point Operations (FLOPs) often a better metric than pure accuracy when selecting a model for a smartphone application?

## Architecture Onboarding

- **Component map:** Frontend (Flutter mobile app) -> Backend (Firebase Auth, Firestore, Cloud Functions) -> Inference Engine (MobileViT) -> Analytics Pipeline (Firestore → BigQuery → Power BI Dashboard)

- **Critical path:** User captures/uploads lesion image in Flutter app → Image preprocessed and sent to inference endpoint → Prediction results + metadata written to Firestore → Data streamed to BigQuery for aggregation → Power BI dashboard updates for health authority review

- **Design tradeoffs:**
  - Binary vs. Multiclass: Binary classification achieves significantly higher accuracy (97.8% vs 92%) but provides less diagnostic granularity. System deploys binary for the app while acknowledging multiclass complexity.
  - Accuracy vs. Latency: VGG16 matched MobileViT in binary accuracy (97.8%) but wasn't selected for deployment due to larger model size and slower inference speed on mobile devices.

- **Failure signatures:**
  - Diagnostic drift: High confidence predictions on non-lesion skin (false positives) if background environment varies from training data
  - Cold start problem: Dashboard shows "zero cases" not because there are none, but because users aren't finding or trusting the app
  - Data staleness: If BigQuery sync breaks, dashboard shows outdated trends, potentially misleading health authorities during outbreaks

- **First 3 experiments:**
  1. Reproduce Baseline: Train MobileViT on MSLD (Binary) dataset using stated hyperparameters (AdamW optimizer, 2e-5 LR) to confirm ~97.8% accuracy benchmark
  2. Inference Latency Test: Deploy trained MobileViT model on mid-range Android device (via TFLite or PyTorch Mobile) to ensure inference time < 500ms for "real-time" capability
  3. Multiclass Confusion Analysis: Run ResNetViT on MSLD v2.0 (Multiclass) test set and plot confusion matrix to identify which classes (e.g., Chickenpox vs. Monkeypox) contribute to 8% error rate

## Open Questions the Paper Calls Out

### Open Question 1
Can the diagnostic scope be expanded to detect and differentiate a wider range of infectious skin diseases? The conclusion states future plans to expand diagnosis to include other skin diseases using larger, more diverse datasets, but current system is limited to binary classification and specific 6-class multiclass task.

### Open Question 2
Can the model maintain high performance on larger, more diverse datasets given the performance drop observed during cross-validation? The paper notes a decline from ~97.8% to ~89% in 5-fold CV, attributed to reduced training and testing set sizes, suggesting sensitivity to data distribution or overfitting.

### Open Question 3
How does the system's diagnostic accuracy compare to clinical ground truth (PCR or expert dermatologist diagnosis) in real-world deployment? While the paper validates against public datasets and tests mobile app functionality, it doesn't present data from clinical trials comparing predictions to medical ground truth.

## Limitations

- High accuracy figures based on augmented datasets may not fully represent real-world variability in lesion presentation, imaging conditions, and patient diversity
- Model performance on previously unseen skin tones and lesion variations remains unvalidated
- Clinical validation limited to technical performance metrics without documented assessment of diagnostic impact on healthcare workflows or patient outcomes

## Confidence

- **High Confidence:** Architectural choice of MobileViT for mobile deployment is well-justified given its balance of accuracy and computational efficiency
- **Medium Confidence:** Transfer learning approach from ImageNet to dermatological images is theoretically sound but requires further validation on clinical datasets
- **Medium Confidence:** Real-time monitoring dashboard concept is technically feasible, but user adoption rates and data quality in actual outbreak scenarios remain unknown

## Next Checks

1. **Clinical Validation Study:** Conduct prospective study comparing ITMA'INN's diagnostic accuracy against expert dermatologists on diverse patient population with varying skin tones and lesion presentations

2. **Mobile Performance Benchmarking:** Measure inference latency and battery consumption across multiple device classes (low, mid, high-end) to verify system meets real-time requirements in field conditions

3. **Dashboard Data Quality Analysis:** Simulate outbreak scenarios to assess data completeness, temporal resolution, and geographic coverage when deployed to actual users, identifying potential sampling biases