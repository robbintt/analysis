---
ver: rpa2
title: 'Multiple-Choice Question Generation Using Large Language Models: Methodology
  and Educator Insights'
arxiv_id: '2506.04851'
source_url: https://arxiv.org/abs/2506.04851
tags:
- mcqs
- llms
- text
- generation
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the use of Large Language Models (LLMs)\
  \ for generating Multiple-Choice Questions (MCQs) in educational settings. Three\
  \ LLMs\u2014GPT-3.5, Llama 2, and Mistral\u2014were compared using a controlled\
  \ prompt that injects source text to minimize hallucinations and maintain educator\
  \ control."
---

# Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights

## Quick Facts
- arXiv ID: 2506.04851
- Source URL: https://arxiv.org/abs/2506.04851
- Reference count: 17
- Primary result: GPT-3.5 significantly outperformed Llama 2 and Mistral in MCQ generation across four of five educator-evaluated metrics.

## Executive Summary
This study evaluates Large Language Models for automated Multiple-Choice Question (MCQ) generation in educational contexts. The authors compare GPT-3.5, Llama 2, and Mistral using a knowledge-injection approach that grounds outputs in provided source text to minimize hallucinations. Twenty-one educators rated generated MCQs across five dimensions: compliance with source text, clarity, distractor selection, coherence of correct answers, and learning utility. GPT-3.5 demonstrated superior performance in most metrics, while educators showed mixed willingness to adopt such AI tools in practice.

## Method Summary
The study used a prompt engineering approach with source text injection to generate MCQs from educational content. Three LLMs were compared: GPT-3.5 (proprietary), Llama 2 (7B), and Mistral (7B-Instruct). The prompt template included strict format constraints and a `{text}` parameter where educators inserted source paragraphs. Generation parameters were temperature=0.5, max_new_tokens=2048, and top_k=30. Educators evaluated 21 MCQs (7 per model) from a Wikipedia article on World War II causes using five 5-point Likert scale metrics. Statistical analysis employed the Friedman test due to non-normal rating distributions.

## Key Results
- GPT-3.5 significantly outperformed both open-source models on compliance, clarity, coherence, and distractor selection (p < 0.005).
- No significant difference was found between models on learning utility metric.
- Educators expressed mixed willingness to adopt AI MCQ tools: 11 in favor, 8 neutral, 2 opposed.
- Knowledge injection successfully constrained outputs to source text, though some models failed to properly cite sources.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting source text directly into prompts constrains LLM outputs to provided content, reducing factual hallucinations.
- **Mechanism:** The prompt template includes a `{text}` parameter where educators insert the knowledge source paragraph. This forces the model to extract and reformulate from supplied text rather than retrieving from internal weights, giving educators editorial control over content accuracy.
- **Core assumption:** The LLM can accurately identify, extract, and rephrase information from the provided context without introducing external knowledge.
- **Evidence anchors:**
  - [abstract]: "In our approach, we do not rely on the knowledge of the LLM, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test's source text."
  - [Section 3]: Shows the complete prompt with `The text is : { text }` parameter for knowledge injection.
  - [corpus]: Related paper "Hallucination-Free Automatic Question & Answer Generation" (arXiv:2601.14280) identifies hallucination types in MCQ generation, supporting the need for grounding mechanisms.
- **Break condition:** If source text is ambiguous, contradictory, or exceeds context window limits, extraction accuracy degrades.

### Mechanism 2
- **Claim:** Larger, more capable instruction-tuned models produce higher-quality MCQs across structural and semantic metrics.
- **Mechanism:** GPT-3.5 (proprietary, larger scale) outperformed Llama 2 and Mistral (both 7B open-source models) on compliance, clarity, coherence, and distractor selection. Superior instruction-following capacity enables better adherence to format constraints and more coherent question generation.
- **Core assumption:** Performance differences are attributable to model scale and instruction-tuning quality rather than prompt-model compatibility alone.
- **Evidence anchors:**
  - [Section 4.1.3]: Specifies Llama 2 (7B) and Mistral (7B-Instruct-v0.1) were used; GPT-3.5 accessed via ChatGPT.
  - [Section 4.2/Table 1]: Friedman test shows GPT-3.5 significantly outperformed others on 4/5 metrics (p < 0.005).
  - [corpus]: "Small Models, Big Support" (arXiv:2506.05925) explores local LLM frameworks for educators, suggesting ongoing research into model size-quality tradeoffs.
- **Break condition:** If inference cost or latency constraints prohibit proprietary models, quality may decrease with smaller open-source alternatives.

### Mechanism 3
- **Claim:** Explicit output format templates with strict structural constraints improve MCQ consistency and evaluability.
- **Mechanism:** The prompt enforces a rigid output schema (`> [Question], a) [Option], * Correct Answer:, * Source:`), reducing format variability and ensuring all MCQ components are present. This enables automated parsing and consistent educator evaluation.
- **Core assumption:** Models will reliably follow format instructions without omission or structural deviation.
- **Evidence anchors:**
  - [Section 3]: Prompt specifies "You must strictly adhere to the following format without any errors" with bracketed placeholders.
  - [Section 4.2]: Llama 2 and Mistral sometimes failed this mechanism, citing "Last line of the text" or "Line 5" instead of actual source text—GPT-3.5 did not exhibit this failure.
  - [corpus]: No direct corpus evidence on format enforcement specifically for MCQs; this mechanism is primarily paper-anchored.
- **Break condition:** Smaller or less instruction-tuned models may fail format adherence, producing unparseable or incomplete outputs.

## Foundational Learning

- **Concept: Prompt Engineering for Structured Outputs**
  - **Why needed here:** The entire methodology depends on crafting a prompt that reliably produces parseable MCQs with all required components (stem, options, key, source citation).
  - **Quick check question:** Can you explain why specifying "modify only parts within brackets" improves output consistency?

- **Concept: Knowledge Injection / Grounding**
  - **Why needed here:** Understanding how external text injection differs from relying on model-internal knowledge is essential for controlling hallucinations and ensuring content accuracy.
  - **Quick check question:** What is the tradeoff between using an LLM's parametric knowledge versus injected context for factual accuracy?

- **Concept: Non-Parametric Statistical Testing (Friedman Test)**
  - **Why needed here:** The paper uses the Friedman test because rating data violated normality assumptions. Understanding this choice is necessary to interpret why specific statistical conclusions hold.
  - **Quick check question:** Why would the Friedman test be preferred over ANOVA for Likert-scale ratings from the same evaluators across multiple conditions?

## Architecture Onboarding

- **Component map:** Source Text (educator-provided) → Prompt Template (format constraints + {text} injection) → LLM Selection (GPT-3.5 / Llama 2 / Mistral) → Generation (temperature=0.5, top_k=30, max_tokens=2048) → MCQ Output (stem, 4 options, key, source citation) → Human Evaluation (5 metrics on 5-point Likert scale)

- **Critical path:**
  1. Source text quality and length (must be unambiguous, fit context window)
  2. Prompt template fidelity (format constraints must be explicit and testable)
  3. Model selection (tradeoff: quality vs. cost vs. local deployment)
  4. Hyperparameter settings (temperature affects creativity vs. consistency)

- **Design tradeoffs:**
  - **Proprietary vs. Open-Source:** GPT-3.5 showed best results but requires API dependency and cost; Llama 2/Mistral enable local deployment but with quality degradation.
  - **Temperature Setting:** Paper uses 0.5—lower values increase determinism but may reduce question diversity; higher values increase variety but risk incoherence.
  - **Sample Size vs. Practicality:** Only 7 MCQs per model evaluated due to time constraints; larger samples would strengthen statistical claims but increase evaluator burden.

- **Failure signatures:**
  - **Source citation failures:** Model outputs "Line X" or "Last sentence" instead of actual source text (observed in Llama 2, Mistral; not GPT-3.5).
  - **Format violations:** Missing components, extra text outside template, unparseable structure.
  - **Hallucinations:** Questions or answers not grounded in provided source text (mitigated but not eliminated by injection).
  - **Weak distractors:** Incorrect options too obviously wrong, reducing assessment validity.

- **First 3 experiments:**
  1. **Reproduce the prompt with a different domain** (e.g., computer science or biology text) using the same three models to test domain generalizability.
  2. **Ablate the source citation requirement** from the prompt and measure hallucination rate increase to quantify grounding mechanism effectiveness.
  3. **Compare temperature settings** (0.0, 0.5, 1.0) on MCQ diversity and coherence metrics to optimize the creativity-consistency tradeoff.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited sample size: Only 21 educators rated 21 MCQs (7 per model), constraining statistical power.
- Single-domain evaluation: Only World War II causes content tested, limiting generalizability.
- Human evaluation constraints: Likert-scale ratings without inter-rater reliability measures or blind procedures.
- Proprietary model dependency: GPT-3.5 results may not be reproducible for institutions with budget constraints.

## Confidence

- **High confidence:** GPT-3.5's superior performance across four of five evaluation metrics (compliance, clarity, coherence, distractor selection) is statistically significant and well-supported by the data.
- **Medium confidence:** The effectiveness of knowledge injection in reducing hallucinations is demonstrated but not quantified; the mechanism works in practice but the magnitude of improvement relative to baseline approaches is unclear.
- **Medium confidence:** Educator willingness to adopt AI tools shows a trend (11 in favor, 8 neutral, 2 opposed) but the sample size and single-domain evaluation limit generalizability to broader educational contexts.

## Next Checks
1. **Cross-domain replication:** Test the same methodology with source texts from STEM, humanities, and vocational domains to assess generalizability beyond historical content.
2. **Quantitative hallucination measurement:** Compare hallucination rates between knowledge-injected prompts and standard zero-shot prompting using automated factuality detection to quantify grounding mechanism effectiveness.
3. **Long-term educator adoption study:** Follow up with the same 21 educators after 3-6 months of actual classroom use to measure changes in willingness to adopt and perceived utility of AI-generated MCQs.