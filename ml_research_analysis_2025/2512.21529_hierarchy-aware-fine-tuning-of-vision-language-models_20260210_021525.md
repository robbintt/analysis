---
ver: rpa2
title: Hierarchy-Aware Fine-Tuning of Vision-Language Models
arxiv_id: '2512.21529'
source_url: https://arxiv.org/abs/2512.21529
tags:
- hierarchical
- tp-kl
- hisce
- levels
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a hierarchical fine-tuning method for vision-language
  models (VLMs) that adapts them to structured taxonomies. The approach introduces
  two novel loss functions: Tree-Path KL Divergence (TP-KL) for enforcing vertical
  consistency along taxonomy paths, and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE)
  for horizontal consistency among sibling classes.'
---

# Hierarchy-Aware Fine-Tuning of Vision-Language Models

## Quick Facts
- arXiv ID: 2512.21529
- Source URL: https://arxiv.org/abs/2512.21529
- Authors: Jiayu Li; Rajesh Gangireddy; Samet Akcay; Wei Cheng; Juhua Hu
- Reference count: 40
- Primary result: Hierarchical fine-tuning improves Full-Path Accuracy and Tree-based Inconsistency Error on taxonomy-aware vision-language tasks

## Executive Summary
This paper addresses the challenge of adapting vision-language models (VLMs) to structured taxonomies by proposing a hierarchical fine-tuning approach. The method introduces two novel loss functions - Tree-Path KL Divergence (TP-KL) for vertical consistency along taxonomy paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. Both losses operate in the shared embedding space of VLMs and integrate with LoRA for efficient parameter updates. Experiments demonstrate consistent improvements in taxonomy-aware metrics across four datasets while maintaining minimal parameter overhead.

## Method Summary
The proposed method performs hierarchical fine-tuning of vision-language models by introducing two specialized loss functions that enforce consistency with taxonomic structures. TP-KL loss ensures vertical consistency by encouraging embeddings along the same taxonomy path to have similar distributions, while HiSCE loss promotes horizontal consistency by smoothing predictions among sibling classes. Both losses operate in the shared embedding space and are combined with standard cross-entropy during training. The approach uses LoRA adapters for efficient fine-tuning with minimal additional parameters. The method reshapes the text embedding space into a hierarchy-aligned manifold, improving both consistency and interpretability of predictions across taxonomy levels.

## Key Results
- Consistent improvements in Full-Path Accuracy (FPA) across all four datasets (CUB-200-2011, FGVC-Aircraft, Butterfly-200, ChestX-ray14)
- Reduced Tree-based Inconsistency Error (TICE) compared to standard cross-entropy fine-tuning
- Minimal parameter overhead (3.1% for Vision Encoder LoRA, 3.3% for Language Encoder LoRA)
- Better alignment of text embeddings with hierarchical taxonomy structures

## Why This Works (Mechanism)
The method works by explicitly enforcing both vertical and horizontal consistency constraints during fine-tuning. TP-KL loss creates smooth transitions along taxonomy paths by minimizing KL divergence between parent and child class embeddings, ensuring that semantically related classes have similar representations. HiSCE loss prevents overconfident predictions among sibling classes by applying label smoothing within taxonomic siblings, promoting more nuanced probability distributions. Together, these losses reshape the embedding space to reflect the underlying taxonomic structure, leading to more consistent and interpretable predictions across all hierarchy levels.

## Foundational Learning

**Tree-Path KL Divergence (TP-KL)**: Measures distributional similarity between parent and child class embeddings along taxonomy paths. Needed to enforce vertical consistency in hierarchical classification. Quick check: Verify KL divergence values decrease along training for parent-child pairs.

**Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE)**: Applies label smoothing among sibling classes during training. Needed to prevent overconfident predictions and promote horizontal consistency. Quick check: Monitor sibling class probability distributions for balanced softmax outputs.

**LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning technique that updates a small subset of model weights. Needed to minimize computational overhead while adapting VLMs to new tasks. Quick check: Verify parameter count increase remains under 5% of original model size.

## Architecture Onboarding

**Component Map**: Image Encoder -> Vision LoRA -> CLIP Embedder -> Text Encoder LoRA -> CLIP Embedder -> TP-KL Loss + HiSCE Loss -> Cross-Entropy Loss -> Parameter Updates

**Critical Path**: Forward pass through image encoder → vision LoRA adaptation → CLIP text encoder with LoRA → compute hierarchical losses (TP-KL + HiSCE) → combine with cross-entropy → backward pass for parameter updates

**Design Tradeoffs**: Parameter efficiency vs. fine-tuning effectiveness (LoRA adapters provide minimal overhead but may limit adaptation capacity); loss weighting (α and λ hyperparameters require careful tuning to balance consistency with classification accuracy); computational complexity (hierarchical losses increase per-step computation but remain manageable)

**Failure Signatures**: Inconsistent predictions across taxonomy levels; overconfident sibling class assignments; degradation in standard classification metrics while improving hierarchical metrics; training instability due to improper loss weighting

**Three First Experiments**:
1. Baseline cross-entropy fine-tuning without hierarchical losses to establish performance floor
2. Individual evaluation of TP-KL and HiSCE losses to assess their separate contributions
3. Ablation study on α and λ hyperparameters to determine optimal loss weighting

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited scalability testing on large-scale taxonomies (>1000 classes) raises questions about performance on complex real-world hierarchies
- Reliance on GPT-4 for ground truth hierarchy labels may introduce evaluation bias and misalignment with expert-curated taxonomies
- Computational efficiency claims lack detailed training time and resource requirement analysis for the hierarchical fine-tuning process

## Confidence

**High confidence**: Mathematical formulation and implementation of TP-KL and HiSCE loss functions are clearly derived and integrate well with established VLM frameworks

**Medium confidence**: Empirical results show consistent improvements across datasets, but limited scope and specific nature of datasets used constrain generalizability

**Low confidence**: Scalability to real-world scenarios with larger, more complex taxonomies and ability to handle imbalanced hierarchical structures remain unproven

## Next Checks

1. Test the method on larger-scale datasets with more complex taxonomies (e.g., ImageNet-21K or domain-specific taxonomies with >1000 classes) to assess scalability and performance degradation.

2. Conduct ablation studies on the hyper-parameters (α for TP-KL, λ for HiSCE) across different dataset sizes and taxonomic structures to establish robust tuning guidelines.

3. Implement and evaluate the method on an actively updated taxonomy where new classes are continuously added, measuring the ability to maintain consistency without full retraining.