---
ver: rpa2
title: 'EVADE: LLM-Based Explanation Generation and Validation for Error Detection
  in NLI'
arxiv_id: '2511.08949'
source_url: https://arxiv.org/abs/2511.08949
tags:
- explanations
- label
- validation
- human
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EVADE, a novel LLM-based framework for detecting
  annotation errors in Natural Language Inference datasets by leveraging explanation
  generation and validation. The framework aims to address the challenge of separating
  annotation errors from human label variation (HLV), where multiple plausible labels
  can exist for the same instance.
---

# EVADE: LLM-Based Explanation Generation and Validation for Error Detection in NLI

## Quick Facts
- arXiv ID: 2511.08949
- Source URL: https://arxiv.org/abs/2511.08949
- Authors: Longfei Zuo; Barbara Plank; Siyao Peng
- Reference count: 23
- Key outcome: EVADE framework detects annotation errors in NLI datasets by generating and validating explanations, achieving higher precision/recall than human annotators and improving downstream fine-tuning performance.

## Executive Summary
EVADE is a novel LLM-based framework for detecting annotation errors in Natural Language Inference (NLI) datasets by separating genuine errors from human label variation (HLV). The framework generates comprehensive explanations for each label using LLMs and validates these explanations through LLM-based scoring, effectively refining explanation distributions to better align with human annotations. Results demonstrate that EVADE achieves higher precision and recall in error detection compared to human annotators and improves downstream fine-tuning performance when removing LLM-detected errors from training data.

## Method Summary
EVADE employs a two-stage pipeline: first generating explanations for all candidate labels per NLI instance using LLM prompting, then validating these explanations through LLM-based scoring. The framework operates across three validation scenarios (one-expl, one-llm, all-llm) with varying context levels, applying thresholds to identify labels with consistently low validity scores as annotation errors. Evaluation uses Kullback-Leibler Divergence (KLD) to measure alignment between LLM-validated distributions and human judgments, alongside precision/recall metrics for error detection and downstream fine-tuning performance on ChaosNLI.

## Key Results
- EVADE achieves higher precision and recall in error detection compared to human annotators across all validation scenarios
- Removing LLM-detected errors from training data yields greater improvements in fine-tuning performance than removing human-identified errors
- KLD minimization effectively calibrates validation thresholds, balancing error detection with preservation of valid human label variation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-generated explanations provide broader coverage of plausible reasoning paths than human annotators, enabling more comprehensive error detection.
- **Mechanism:** LLMs systematically generate explanations for all candidate labels per instance rather than justifying a single annotator-chosen label. This over-generation creates a superset of reasoning possibilities that can be validated against.
- **Core assumption:** Valid human label variation (HLV) can be captured through diverse explanatory reasoning, not just label agreement.
- **Evidence anchors:**
  - [abstract] "conducting two rounds of manual annotation is costly and may limit the coverage of plausible labels or explanations"
  - [section] Table 1 shows LLMs generate 3,920-8,845 total explanations vs. 1,933 for humans; LLMs cover ~3 labels per instance vs. 1.76 for humans
  - [corpus] LiTEx (arXiv:2505.22848) provides complementary evidence that within-label variation—same label, divergent reasoning—is prevalent and linguistically structured in NLI
- **Break condition:** If LLM explanations systematically miss reasoning patterns humans use, or if explanation quality degrades for edge cases, coverage gains become noise.

### Mechanism 2
- **Claim:** Contextual validation with competing explanations improves error detection precision by forcing comparative reasoning.
- **Mechanism:** When LLMs score explanations with access to alternative explanations (one-llm, all-llm settings), higher standard deviation in scores enables more discriminative filtering. Explanations are judged relative to alternatives, not in isolation.
- **Core assumption:** Invalid explanations receive systematically lower scores when compared against valid alternatives.
- **Evidence anchors:**
  - [abstract] "LLM validation effectively refines explanation distributions to better align with human annotations, as measured by Kullback-Leibler Divergence"
  - [section] Table 2 shows one-expl (no context) yields highest validity scores (>0.8) but lowest std, while one-llm/all-llm increase std, enabling better filtering
  - [corpus] No direct corpus evidence on comparative validation strategies for AED; this appears novel
- **Break condition:** If contextual information overwhelms smaller models (observed with Qwen-7B in all-llm), or if comparison introduces systematic biases, precision degrades.

### Mechanism 3
- **Claim:** Threshold-based label rejection calibrated to distribution alignment (KLD minimization) outperforms human self-validation for downstream fine-tuning.
- **Mechanism:** Validation thresholds are selected by balancing KLD to ChaosNLI distributions with precision/recall trade-offs. Labels with all explanations below threshold are flagged as errors. Removing these errors yields cleaner training data.
- **Core assumption:** LLM-validated label distributions approximate human judgment distributions (HJD) sufficiently that error removal improves model learning.
- **Evidence anchors:**
  - [abstract] "removing LLM-detected errors from training data yields improvements in fine-tuning performance than removing errors identified by human annotators"
  - [section] Table 5: Fine-tuning with EVADE-validated labels achieves F1=0.6328 (Llama-70B, one-llm) vs. VariErr R2 baseline F1=0.5500; removing human-detected errors (R2) degraded performance
  - [corpus] VariErr NLI (Weber-Genzel et al., 2024) is the direct predecessor and comparison point; Chen et al. (2025) showed LLM explanations can approximate human label distributions
- **Break condition:** If optimal thresholds vary significantly across domains/tasks, or if KLD minimization doesn't correlate with downstream utility, the calibration approach fails to generalize.

## Foundational Learning

- **Concept: Human Label Variation (HLV)**
  - **Why needed here:** EVADE's core purpose is separating genuine annotation errors from valid HLV—multiple plausible labels for the same instance due to ambiguity, subjectivity, or guideline divergence.
  - **Quick check question:** Given premise "The dog barked" and hypothesis "The animal made a sound," could both Entailment and Neutral be valid depending on annotator interpretation?

- **Concept: Kullback-Leibler Divergence (KLD)**
  - **Why needed here:** Primary metric for measuring how well LLM-validated distributions align with human crowdworker distributions (ChaosNLI). Lower KLD = better alignment.
  - **Quick check question:** If model distribution P=[0.7, 0.2, 0.1] and human distribution Q=[0.5, 0.3, 0.2] for labels [E, N, C], what does KLD(P||Q) measure?

- **Concept: Validity Scoring vs. Binary Classification**
  - **Why needed here:** EVADE uses continuous validity scores (0.0-1.0) rather than binary judgments, enabling threshold calibration and ranking-based error detection.
  - **Quick check question:** Why might a label with average validity score 0.65 be treated differently depending on whether the threshold is 0.6 vs. 0.7?

## Architecture Onboarding

- **Component map:** Explanation Generation -> Validation -> Threshold Calibration -> Error Detection Output

- **Critical path:**
  1. Prompt engineering for generation (must prevent fallback responses, semantic redundancy)
  2. Filtering low-quality outputs (truncated, wrong language, fallback statements)
  3. Selecting validation scenario and model size (larger models more stable; all-llm most conservative)
  4. Calibrating threshold per model/scenario (Table 8 shows thresholds vary: 0.2-0.9)

- **Design tradeoffs:**
  - **Coverage vs. Noise:** LLMs over-generate explanations (Table 1: Llama-8B produces 8,845 vs. 1,933 human); more coverage but requires aggressive validation
  - **Model Size vs. Cost:** 70B/72B models more consistent but expensive; 7B/8B models sufficient for generation but struggle with contextual validation
  - **Conservatism vs. Recall:** all-llm scenario yields highest precision but lowest recall (Figure 2c); one-expl has highest recall but may retain errors

- **Failure signatures:**
  - **Fallback responses:** Llama-70B outputs "Since the statement is not supported..." instead of explanations (requires manual filtering)
  - **Formatting errors:** Truncated outputs (Llama-8B at 256 token limit), wrong language (Qwen-7B generating Chinese)
  - **Threshold mismatch:** Overly strict thresholds discard valid labels (recall drops sharply >0.7); overly lenient thresholds retain noise
  - **Context overwhelm:** Smaller models (Qwen-7B) score noticeably lower in all-llm scenario (0.3825) vs. one-llm (0.5675)

- **First 3 experiments:**
  1. **Reproduce generation statistics:** Run explanation generation on 50 VariErr instances with Llama-8B and Qwen-72B; compare explanation counts, lengths, and label coverage to Table 1 baselines
  2. **Validate threshold sensitivity:** For a single model (Llama-70B), compute KLD at thresholds 0.3, 0.6, 0.9; verify the U-shaped curve in Figure 2a
  3. **Ablate validation context:** Compare precision@100 and recall@100 for one-expl vs. one-llm vs. all-llm on 100 instances; confirm that all-llm is most conservative (lowest recall) as reported

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the EVADE framework be generalized to tasks and domains beyond Natural Language Inference (NLI) that exhibit human label variation (HLV)?
- **Basis in paper:** [explicit] The Limitations section states the study is restricted to NLI and the VARIERR dataset, which "limits us to fully assess the robustness and generalizability of the framework across other tasks and domains."
- **Why unresolved:** There is a scarcity of datasets in other domains that simultaneously provide HLV references and identified annotation errors to serve as ground truth.
- **What evidence would resolve it:** Successful application and evaluation of the EVADE pipeline on non-NLI datasets (e.g., sentiment analysis or QA) containing known error rates and soft label distributions.

### Open Question 2
- **Question:** Does incorporating LLM-based peer-validation signals improve error detection performance compared to the self-validation approach currently used?
- **Basis in paper:** [explicit] The Limitations section notes that VARIERR uses a human peer-validation setup not explored in the study, and "Incorporating peer-validation signals through LLMs remains an important direction for future work."
- **Why unresolved:** The current EVADE implementation relies on self-validation (an LLM scoring its own explanations), whereas peer-validation (cross-model validation) might provide richer or more objective signals.
- **What evidence would resolve it:** Experiments where LLMs validate explanations generated by different LLMs (cross-validation), measuring if this increases precision/recall in detecting annotation errors compared to self-validation.

### Open Question 3
- **Question:** How can the framework be adapted to mitigate the tendency of LLMs to over-generate explanations for incorrect labels?
- **Basis in paper:** [explicit] The Limitations section highlights that "LLM still over-generates explanations for almost all labels," which complicates the evaluation setup compared to natural human errors.
- **Why unresolved:** The exploratory nature of the current framework reveals that LLMs often hallucinate plausible reasoning for invalid labels, creating noise that manual filtering cannot fully address at scale.
- **What evidence would resolve it:** A modified generation or validation prompt strategy that significantly reduces the number of explanations per label for invalid instances, aligning the explanation distribution more closely with human sparsity.

## Limitations

- Framework restricted to NLI domain without testing on other tasks that exhibit human label variation
- Manual filtering criteria for "fallback responses" and "formatting errors" not formally specified, potentially introducing bias
- Downstream fine-tuning improvements claim limited to one model family and dataset combination without ablation studies

## Confidence

- **High** for core mechanism of using LLM validation to refine explanation distributions
- **Medium** for claim that EVADE outperforms human annotators in error detection precision/recall
- **Low** for downstream fine-tuning improvements claim due to limited scope

## Next Checks

1. Conduct ablation studies comparing EVADE's performance with and without human label variation data to isolate the contribution of LLM validation versus dataset characteristics
2. Test the framework on non-NLI tasks (e.g., sentiment analysis, fact verification) to assess generalizability of the explanation generation and validation approach
3. Implement a formal inter-annotator agreement study for the manual filtering process to quantify potential bias introduced by subjective criteria