---
ver: rpa2
title: Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing
  Its OCR Proficiency
arxiv_id: '2507.08309'
source_url: https://arxiv.org/abs/2507.08309
tags:
- text
- dimt
- image
- mllm
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving document image
  machine translation (DIMT) performance in multimodal large language models (MLLMs)
  while preventing catastrophic forgetting of their optical character recognition
  (OCR) capabilities. The proposed method, Synchronously Self-Reviewing (SSR), fine-tunes
  MLLMs by first generating OCR text in the source language before producing translation
  text in the target language.
---

# Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency

## Quick Facts
- arXiv ID: 2507.08309
- Source URL: https://arxiv.org/abs/2507.08309
- Reference count: 35
- Key outcome: SSR fine-tuning improves DIMT performance while preserving OCR capabilities in MLLMs

## Executive Summary
This paper addresses the challenge of document image machine translation (DIMT) where fine-tuning multimodal large language models (MLLMs) for translation causes catastrophic forgetting of their optical character recognition (OCR) capabilities. The proposed Synchronously Self-Reviewing (SSR) method fine-tunes MLLMs by first generating OCR text in the source language before producing translation text in the target language. This approach leverages the model's strong monolingual OCR proficiency while learning cross-lingual translation. Experiments on DoTA and DITrans datasets show SSR significantly improves DIMT performance while preserving OCR accuracy and enabling cross-lingual VQA capabilities.

## Method Summary
The SSR method uses a two-step fine-tuning paradigm: (1) Generate OCR text X' from training images using the base MLLM's original OCR instruction, and (2) Fine-tune with a prompt that first asks for OCR conversion to Markdown, then translation to Chinese, using the format "X' <Translation> Y" where Y is ground truth translation. The method employs LoRA on all LLM linear layers for 3 epochs with specific hyperparameters (rank=16, alpha=16, batch=32, lr=1e-4). Training data consists of 10K samples from DoTA dataset, with evaluation on both in-domain (DoTA) and cross-domain (DITrans) tests.

## Key Results
- SSR prevents catastrophic forgetting: Qwen2-VL maintains OCR Character Accuracy of 85.18 while achieving BLEU scores of 57.23 on in-domain tests and 41.91 on cross-domain tests
- Self-generated OCR text outperforms ground truth text in training due to format alignment with model's natural output distribution
- SSR enables cross-lingual VQA generalization beyond translation tasks
- Method scales effectively with unsupervised data, improving cross-domain performance

## Why This Works (Mechanism)

### Mechanism 1: Distribution Smoothing via Self-Generated Context
Constraining the model to generate source text (OCR) before target text (translation) smooths the training optimization landscape, mitigating the distribution shift that causes catastrophic forgetting. Standard SFT immediately outputs translation tokens, shifting weights away from pre-trained OCR distribution. By sampling source text from the model's own distribution first, initial tokens align with pre-existing weights, reducing the "shock" of the new task while preserving original capabilities.

### Mechanism 2: Contextual Grounding for Cross-Modal Translation
Explicitly decoding visual text into the context window creates a textual "scratchpad" that conditions translation generation, improving cross-modal alignment. Instead of mapping image features directly to translation tokens, the model first maps image-to-text (known skill), then text-to-text translation. The self-generated OCR acts as an intermediate reasoning step that grounds translation in specific tokens present in the image.

### Mechanism 3: Synthetic Data Regularization
The method implicitly regularizes the model by generating training data that matches the model's native output format, rather than forcing it to mimic external ground truth formats. Training on "Self-generated Text + Ground Truth Translation" rather than "Ground Truth Text + Ground Truth Translation" avoids learning to correct "errors" the model doesn't perceive and prevents mimicking formatting styles that differ from natural output.

## Foundational Learning

- **Catastrophic Forgetting in MLLMs**: Why needed here - core problem is that fine-tuning on translation destroys OCR. Quick check - Why does standard SFT on translation data cause BLEU score for OCR tasks to drop to near zero?

- **Chain of Thought (CoT) in Fine-tuning**: Why needed here - SSR applies CoT-like structure during training phase, not just inference. Quick check - How does the "SSR-constrained Prompt Template" differ from standard few-shot prompt used in inference?

- **Low-Rank Adaptation (LoRA)**: Why needed here - experiments rely on LoRA to update LLM backbone. Quick check - Which specific layers in MLLM were targeted by LoRA, and why might freezing Vision Encoder be important?

## Architecture Onboarding

- **Component map**: Document Image ($I$) + Ground Truth Translation ($Y$) -> Base MLLM generates Source Text ($X'$) -> Concatenate $X'$ + `<Translation>` + $Y$ -> Fine-tune MLLM (LLM backbone only)

- **Critical path**: Prompt template construction is most fragile component. Instruction: "Convert the content in the image to Markdown, then translate into Chinese." Response must be `Self-Generated-OCR` + `<Translation>` + `Ground-Truth-Translation`. Swapping self-generated with ground truth text may degrade performance.

- **Design tradeoffs**: Inference latency effectively doubles (OCR + Translation), increasing from 33s to 95s per page for Qwen2-VL. Data quality vs. distribution match: noisier self-generated text is preferred over cleaner ground truth text due to format alignment.

- **Failure signatures**: Format misalignment if ground truth translation uses Markdown conflicting with model's natural OCR style. Error propagation if base model cannot recognize text in specific domain, reinforcing inability to translate that text.

- **First 3 experiments**:
  1. Baseline Comparison: Standard SFT vs. SSR on Qwen2-VL using DoTA, measuring BLEU for DIMT and CA for OCR retention
  2. Source Text Ablation: Train SSR with (a) Self-generated text, (b) Ground Truth text, (c) External OCR tool text, compare BLEU scores
  3. Unsupervised Scaling: Use 10k unlabeled images with synthetic pairs (OCR + Google Translate) mixed with 10k supervised set, verify cross-domain performance improvement

## Open Questions the Paper Calls Out

1. How can text-grounding capabilities be effectively integrated with SSR to allow users to translate text within specific, user-defined image regions?

2. To what extent does the SSR fine-tuning paradigm impact the broader instruction-following stability of MLLMs compared to standard SFT?

3. Is there a threshold of initial OCR proficiency required for SSR to be effective, below which noise in self-generated text hinders translation learning?

## Limitations

- Method assumes strong pre-trained OCR foundation exists; effectiveness unknown for documents with specialized formatting, handwriting, or low-quality images
- Experiments limited to English-to-Chinese translation; scalability across diverse language families and writing systems unverified
- Reliance on self-generated OCR text may reinforce systematic OCR errors rather than correct them

## Confidence

- **High Confidence**: SSR prevents catastrophic forgetting of OCR capabilities (95%+)
- **Medium Confidence**: SSR enables cross-lingual VQA generalization (75-95%)
- **Low Confidence**: SSR is superior for all MLLM-based DIMT applications across domains and languages (50-75%)

## Next Checks

1. Evaluate SSR on documents from domains where base MLLM has known OCR weaknesses (historical manuscripts, technical diagrams) to measure OCR retention when starting from lower baseline

2. Apply SSR to train models for DIMT tasks involving language pairs with different writing systems (English to Arabic, Hindi) and compare performance against Chinese-focused results

3. Conduct systematic error analysis comparing OCR accuracy degradation patterns between SSR and standard SFT across multiple document types to track error type preservation or degradation