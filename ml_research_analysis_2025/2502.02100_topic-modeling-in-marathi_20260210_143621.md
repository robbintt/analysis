---
ver: rpa2
title: Topic Modeling in Marathi
arxiv_id: '2502.02100'
source_url: https://arxiv.org/abs/2502.02100
tags:
- topic
- modeling
- marathi
- bertopic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores topic modeling for Marathi, a low-resource
  Indo-Aryan language. The study compares BERTopic, a neural approach using pre-trained
  BERT embeddings, with the classical LDA model.
---

# Topic Modeling in Marathi

## Quick Facts
- arXiv ID: 2502.02100
- Source URL: https://arxiv.org/abs/2502.02100
- Reference count: 0
- This paper explores topic modeling for Marathi, a low-resource Indo-Aryan language.

## Executive Summary
This paper investigates topic modeling for Marathi, comparing BERTopic (a neural approach using pre-trained BERT embeddings) against the classical LDA model. BERTopic employs UMAP for dimensionality reduction, HDBSCAN for clustering, and a class-based TF-IDF procedure to derive topic representations. Multiple BERT models—including MahaBERT-V2, MahaSBERT, MahaSBERT-STS, IndicSBERT, IndicSBERT-STS, and MuRIL—were evaluated. Results show BERTopic consistently outperforms LDA in topic coherence, with MahaBERT-V2 achieving the highest scores (e.g., 0.82 on the LDC dataset). BERTopic also excels in identifying semantically coherent topics across varying document lengths.

## Method Summary
The study compares BERTopic and LDA for Marathi topic modeling. LDA uses bag-of-words representation after preprocessing (removing non-Devanagari text, URLs, and stop words). BERTopic generates sentence embeddings using various Marathi and Indic BERT models, reduces dimensionality with UMAP, clusters with HDBSCAN, and extracts topics via class-based TF-IDF. Three Marathi datasets of varying lengths were used: LDC (long), LPC (medium), and SHC (short). Coherence scores and topic diversity were the primary evaluation metrics.

## Key Results
- BERTopic consistently outperforms LDA in topic coherence across all datasets.
- MahaBERT-V2 achieves the highest coherence score (0.82) on the LDC dataset.
- LDA coherence scores range from 0.34 to 0.55, significantly lower than BERTopic's 0.63 to 0.82 range.
- BERTopic shows superior performance across short, medium, and long Marathi texts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTopic produces more coherent topics than LDA for Marathi text when using language-specific embeddings.
- Mechanism: Pre-trained BERT models encode semantic relationships based on context, whereas LDA relies on word co-occurrence statistics. BERTopic clusters dense embeddings and extracts topic keywords using class-based TF-IDF, which weights terms by their importance within each cluster.
- Core assumption: Pre-trained embeddings transfer meaningful semantic structure to Marathi despite limited training data.
- Evidence anchors:
  - [abstract] "BERTopic, when combined with BERT models trained on Indic languages, outperforms LDA in terms of topic modeling performance."
  - [section] Table 1 shows BERTopic coherence scores ranging 0.63–0.82 vs. LDA scores of 0.34–0.55 across datasets.
  - [corpus] Weak direct validation; neighbor papers focus on other Marathi NLP tasks but do not independently confirm this topic modeling mechanism.
- Break condition: If pre-trained BERT model has poor Marathi vocabulary coverage or was trained on different domains, embedding quality degrades and clustering fails.

### Mechanism 2
- Claim: Monolingual Marathi BERT models (MahaBERT-V2) outperform multilingual models for topic coherence in longer documents.
- Mechanism: Language-specific models allocate full capacity to Marathi's morphological patterns and script, yielding embeddings that better capture subtle semantic distinctions.
- Core assumption: Marathi-specific pre-training corpora are sufficiently large and diverse to produce robust representations.
- Evidence anchors:
  - [section] "MahaBERT-V2 performs consistently better than other bert models in the 3 datasets" with LDC score of 0.82.
  - [section] "MahaBERT-V2 is outperforming MuRIL in our evaluations, demonstrating superior performance across the tasks."
  - [corpus] Not directly validated; neighbor papers on Marathi BERT models suggest generalization across tasks but do not test topic modeling specifically.
- Break condition: If documents contain code-mixed text or domain-specific vocabulary not seen during pre-training, multilingual models may outperform monolingual ones.

### Mechanism 3
- Claim: Sentence-level embeddings capture document semantics better than bag-of-words for varied document lengths.
- Mechanism: SBERT models encode entire sentences into fixed-dimension vectors that preserve semantic similarity. For short documents, this captures context that sparse representations miss; for long documents, it provides compression while retaining topical signals.
- Core assumption: Sentence embeddings from IndicSBERT/MahaSBERT are not degraded by Marathi's morphological complexity.
- Evidence anchors:
  - [section] "SBERT models are particularly effective on the SHC dataset, which consists of shorter documents, as they are better at capturing the relevant features and context within brief texts."
  - [section] BERT models show greater effectiveness on LDC (long documents) while SBERT excels on SHC (short documents).
  - [corpus] No direct external validation for this length-dependent performance pattern.
- Break condition: If documents exceed token limit without proper chunking, or if short texts lack sufficient context for meaningful embeddings, coherence drops.

## Foundational Learning

- Concept: Latent Dirichlet Allocation (LDA) as a generative probabilistic model
  - Why needed here: LDA is the baseline; understanding its bag-of-words assumption and Dirichlet priors explains why it underperforms on morphologically rich, low-resource languages.
  - Quick check question: Can you explain why LDA treats word order as irrelevant and how this affects topic quality for Marathi?

- Concept: BERT embeddings and Sentence Transformers
  - Why needed here: BERTopic's performance hinges on dense contextual embeddings; you must understand how [CLS] tokens or mean-pooling create sentence representations.
  - Quick check question: What is the difference between word-piece token embeddings and sentence embeddings in SBERT?

- Concept: UMAP dimensionality reduction and HDBSCAN clustering
  - Why needed here: These are the core components that transform embeddings into topic clusters; understanding their parameters (n_neighbors, min_cluster_size) is essential for tuning.
  - Quick check question: Why does HDBSCAN allow for variable-density clusters compared to K-Means?

## Architecture Onboarding

- Component map:
  - Preprocessing (remove non-Devanagari, URLs, stop words) → SentenceTransformer (MahaBERT-V2, MahaSBERT-STS, IndicSBERT, MuRIL) → 768-dim vectors → UMAP → reduces to ~5-50 dimensions → HDBSCAN → assigns documents to topic clusters → c-TF-IDF → extracts top M words per cluster

- Critical path: Preprocessing → Embedding generation → UMAP reduction → HDBSCAN clustering → c-TF-IDF topic extraction. The paper explicitly notes preprocessing improves accuracy.

- Design tradeoffs:
  - Monolingual (MahaBERT-V2) vs. multilingual (MuRIL): Monolingual better for pure Marathi; multilingual better for code-mixed or cross-lingual needs.
  - BERT vs. SBERT: SBERT excels on short documents; BERT better on long documents.
  - Number of topics: BERTopic derives topics from clusters (data-driven); LDA requires pre-specifying K.

- Failure signatures:
  - Many documents assigned to noise cluster (HDBSCAN -1 label): Indicates embeddings lack discriminative structure or min_cluster_size is too high.
  - Topics with generic/stop words: Preprocessing failure or c-TF-IDF not properly normalized.
  - Coherence score < 0.5: Likely embedding model mismatch or insufficient preprocessing.

- First 3 experiments:
  1. Replicate MahaBERT-V2 + BERTopic on LDC dataset; verify coherence score approximates 0.82 to validate your pipeline.
  2. Ablate preprocessing: Run with/without stop word removal and non-Devanagari filtering to quantify impact.
  3. Cross-validate on held-out Marathi documents: Test whether MahaBERT-V2 still outperforms IndicSBERT and MuRIL on a new corpus to assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the superior performance of BERTopic using MahaBERT-V2 be generalized to other low-resource Indic languages with complex morphology?
- Basis in paper: [explicit] The abstract highlights a "noticeable gap in the comprehensive exploration of topic modeling methodologies" specifically for "languages such as Hindi, Marathi, Tamil, and others."
- Why unresolved: The study restricted its experiments exclusively to the Marathi language.
- What evidence would resolve it: Replicating the experimental setup on Hindi and Tamil corpora to determine if BERTopic maintains its advantage over LDA.

### Open Question 2
- Question: To what extent do the high automated coherence scores align with human judgment of topic interpretability?
- Basis in paper: [inferred] The literature survey notes that standard metrics "cannot fully capture the nuances and complexity of language that humans can perceive."
- Why unresolved: The study relied solely on quantitative metrics (coherence and diversity) without incorporating human evaluation.
- What evidence would resolve it: A user study correlating human ratings of topic quality with the automated coherence scores reported for MahaBERT-V2.

### Open Question 3
- Question: Does fine-tuning topic representations using large generative models (e.g., GPT or T5) improve accuracy over the standard c-TF-IDF procedure for Marathi?
- Basis in paper: [inferred] The methodology section mentions BERTopic "optionally" allows for fine-tuning with advanced language models, but the study only evaluated the standard c-TF-IDF approach.
- Why unresolved: The paper did not test whether this specific optional step yields higher performance in a low-resource setting.
- What evidence would resolve it: Comparative benchmarks of c-TF-IDF against LLM-based representations on the LDC and SHC datasets.

## Limitations

- The study lacks specification of dataset sources, exact sizes, and composition of LDC, LPC, and SHC corpora, making replication difficult.
- Key hyperparameters for UMAP and HDBSCAN are not reported, which affects reproducibility.
- The paper does not validate coherence scores on held-out data or cross-corpus generalization.
- The mechanism by which BERTopic outperforms LDA on morphologically rich languages is asserted but not experimentally isolated.

## Confidence

- **High confidence** in the core finding that BERTopic outperforms LDA on Marathi datasets, supported by quantitative coherence scores (0.82 vs. 0.55) and multiple BERT model comparisons.
- **Medium confidence** in the claim that MahaBERT-V2 (monolingual) outperforms multilingual models, as this is observed in their evaluation but not cross-validated with independent corpora or tasks.
- **Low confidence** in the precise mechanism of performance gains, since embedding quality, preprocessing impact, and hyperparameter sensitivity are not isolated or experimentally tested.

## Next Checks

1. Replicate coherence scores: Run MahaBERT-V2 + BERTopic on the same or similar Marathi corpus and verify coherence ≈ 0.82 on long documents.
2. Ablate preprocessing: Compare coherence with and without non-Devanagari filtering and stop word removal to quantify preprocessing impact.
3. Cross-corpus generalization: Test MahaBERT-V2 vs. other models (IndicSBERT, MuRIL) on a held-out Marathi corpus to confirm consistent superiority.