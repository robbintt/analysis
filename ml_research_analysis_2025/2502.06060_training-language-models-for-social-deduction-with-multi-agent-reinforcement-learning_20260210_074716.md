---
ver: rpa2
title: Training Language Models for Social Deduction with Multi-Agent Reinforcement
  Learning
arxiv_id: '2502.06060'
source_url: https://arxiv.org/abs/2502.06060
tags:
- player
- crewmates
- imposter
- language
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to train language model agents to
  communicate effectively in the social deduction game Among Us without requiring
  human demonstrations. The key idea is to decompose communication into listening
  and speaking skills, and to use dense rewards derived from agents' instrumental
  goals (e.g., predicting the imposter's identity) to guide learning.
---

# Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2502.06060
- **Source URL:** https://arxiv.org/abs/2502.06060
- **Reference count:** 40
- **Key outcome:** Method trains language models to communicate effectively in social deduction games like Among Us without human demonstrations, achieving ~2x win rate improvement over standard RL.

## Executive Summary
This paper addresses the challenge of training language models to communicate effectively in social deduction games without requiring human demonstrations. The authors introduce a three-component reinforcement learning framework that decomposes communication into listening and speaking skills, using dense rewards derived from agents' instrumental goals. The method improves listening by training agents to predict the imposter's identity from discussions, and speaking by rewarding messages that influence other agents' beliefs. Evaluated on Among Us, the approach doubles win rates compared to standard RL and produces emergent behaviors like accusing suspects and providing evidence.

## Method Summary
The method trains a 1.5B parameter RWKV language model using a three-component loss function: (1) standard PPO with KL constraint for game rewards, (2) listening loss that cross-entropy predicts the imposter's identity from discussion history, and (3) speaking reward based on the change in other agents' belief probabilities about the true imposter. The training uses self-play with adversarially trained imposters, and one crewmate is frozen to prevent degenerate conventions. The approach is evaluated in a custom Among Us environment with 2D grid rooms, 4 crewmates plus 1 imposter, and natural language-based observations and actions.

## Key Results
- The three-component method approximately doubles win rates compared to standard RL baseline
- Agents exhibit emergent behaviors including accusing suspects and providing evidence
- The approach is robust to adversarially trained imposters and generalizes to different environment configurations
- Language stabilization via KL penalties is necessary to prevent output drift into gibberish

## Why This Works (Mechanism)

### Mechanism 1: Listening via Dense Imposter Prediction
Training agents to predict the imposter's identity from discussion history creates a dense reward signal that forces meaningful information extraction from natural language messages. The listening loss maximizes log-probability of the correct imposter identity conditioned on trajectory, turning passive reading into a supervised learning task with ground-truth labels.

### Mechanism 2: Speaking via Influence Rewards
Rewarding agents for shifting teammates' beliefs toward the truth encourages persuasive communication strategies. The speaking reward is defined as the change in collective probability that living crewmates assign to the true imposter, using RL to maximize this belief delta.

### Mechanism 3: Language Stabilization via KL & World Modeling
A soft KL penalty against the pretrained base model prevents language drift into gibberish, while a world modeling loss forces prediction of next observation tokens. This maintains natural language adherence and prevents overfitting to action tokens.

## Foundational Learning

- **Partially Observable Markov Games (POMGs)**: Needed because Among Us agents act on partial observations rather than global state, making communication necessary for winning. Quick check: Does the agent have access to global state or just current room and chat history?

- **KL-Divergence Constraints in RL**: Critical for preventing language collapse, where RL causes models to drift into degenerate protocols. Quick check: If the KL coefficient is set to 0, what happens to language output quality?

- **Belief State Modeling**: The speaking reward relies on querying other agents' beliefs, assuming the LLM's output probability distribution over tokens serves as a valid proxy for internal belief state. Quick check: How is the belief mathematically derived from the teammate's policy network?

## Architecture Onboarding

- **Component map:** Environment (2D Grid) -> Tokenized Observations (Natural Language) -> Policy (1.5B RWKV) -> Chat Generation and Action Selection -> Belief Query -> Reward Calculation -> Update (PPO + Auxiliary Losses)

- **Critical path:** 1) Run 4 crewmates + 1 imposter in 2D grid 2) Trigger discussion phase and log belief changes after every message 3) Calculate combined reward (sparse game + dense belief-influence) 4) Backprop through RWKV using PPO loss + Listening Loss + KL Penalty

- **Design tradeoffs:** RWKV chosen over Transformers for constant VRAM usage with long trajectories (10k+ tokens), sacrificing context-resolution for memory efficiency. Dense belief-influence rewards provide better signals than sparse RL game outcomes.

- **Failure signatures:** Language drift (random unicode tokens - fix: increase KL coefficient), degenerate conventions (silent agents - fix: freeze one crewmate to listening-only), action tokens leaking into discussion (fix: add world modeling loss).

- **First 3 experiments:** 1) Sanity check with RL-only baseline 2) Ablation with only listening loss to verify voting improvement 3) Full pipeline test to validate speaking reward actually shifts teammate beliefs.

## Open Questions the Paper Calls Out

- **Can the framework be adapted to optimize for truthfulness rather than mere convincingness?** The current reward structure maximizes causal influence on beliefs, incentivizing deception if it effectively manipulates peers. This is identified as an important future direction to mitigate potential dangers.

- **Can agents autonomously identify which aspects of the environment are relevant to discuss without manual specification?** The current method relies on manually defined imposter prediction signals, suggesting future work could allow agents to discover relevant features unsupervised.

- **Is the tendency to hallucinate evidence a result of small model scale or a fundamental feature of the training method?** The authors question whether using 1.5B models causes grounding issues, or if the RL objective inherently favors persuasive fabrication regardless of scale.

## Limitations

- The method relies on manually defined deduction targets (imposter prediction) rather than allowing agents to discover relevant information autonomously
- Agents often fabricate evidence to convince others, raising concerns about truthfulness optimization
- The approach hasn't been validated on more complex social deduction games or different configuration sizes
- The belief extraction mechanism assumes model output distributions reflect genuine belief states without empirical validation

## Confidence

**High Confidence Claims:**
- The three-component loss framework is technically implementable and can train agents to play Among Us with natural language
- Language drift is a real failure mode requiring KL regularization

**Medium Confidence Claims:**
- The method approximately doubles win rates compared to standard RL
- The listening component improves agent performance in identifying imposters

**Low Confidence Claims:**
- Agents exhibit genuine persuasive communication strategies
- The approach generalizes to different social deduction game configurations
- The belief state extraction method reliably captures agent reasoning

## Next Checks

1. **Belief State Validation**: Manipulate the environment's ground truth (imposter identity) and verify that extracted belief probabilities from the model's output actually track these changes in a way that reflects coherent reasoning, not just learned correlations.

2. **Architecture Ablation**: Replicate the core results using a Transformer-based language model instead of RWKV, controlling for parameter count and training compute, to determine whether the recurrent architecture is essential for the approach or merely a computational convenience.

3. **Deception Robustness Test**: Train an agent using this method, then evaluate it against human players or more sophisticated AI opponents that employ deliberate deception strategies (lying, false accusations, evidence fabrication) to assess whether the agent can maintain performance when faced with adversarial communication rather than cooperative teammates.