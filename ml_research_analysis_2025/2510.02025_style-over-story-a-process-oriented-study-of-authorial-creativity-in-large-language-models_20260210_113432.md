---
ver: rpa2
title: 'Style Over Story: A Process-Oriented Study of Authorial Creativity in Large
  Language Models'
arxiv_id: '2510.02025'
source_url: https://arxiv.org/abs/2510.02025
tags:
- narrative
- constraints
- setting
- each
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes the latent narrative preferences of large language
  models (LLMs) by having them select from a library of 200 narratology-grounded constraints.
  Using a process-oriented approach, the research finds that LLMs consistently prioritize
  stylistic elements (such as tone and mood) over narrative content elements (like
  character, event, and setting) across six different models and three instruction
  types.
---

# Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models

## Quick Facts
- arXiv ID: 2510.02025
- Source URL: https://arxiv.org/abs/2510.02025
- Reference count: 40
- Primary result: LLMs consistently prioritize stylistic elements over narrative content elements across six models and three instruction types

## Executive Summary
This study analyzes the latent narrative preferences of large language models (LLMs) by having them select from a library of 200 narratology-grounded constraints. Using a process-oriented approach, the research finds that LLMs consistently prioritize stylistic elements (such as tone and mood) over narrative content elements (like character, event, and setting) across six different models and three instruction types. The preference for style remains stable across models and prompts, while content-related preferences show more variability. The methodology offers a novel tool for understanding AI's authorial tendencies and suggests that LLMs have underlying biases in creative tasks that should inform how they are deployed in narrative generation.

## Method Summary
The researchers created a library of 200 narratology-grounded constraints across four elements (Event, Style, Character, Setting) and conducted 8,820 selection trials across six models and three instruction types. Models were asked to select 20 constraints from the library under various conditions, with randomization to control for order effects. Selection data was analyzed using run-clustered Poisson generalized estimating equations to calculate rate ratios comparing preference frequencies across elements and categories.

## Key Results
- LLMs consistently show a preference for Style constraints over Event constraints (RR=1.67)
- The "Style over Story" preference remains stable across all six tested models and instruction types
- Creativity instructions shift models away from everyday realism toward surreal/non-linear axes while maintaining Style dominance
- Setting and Style elements show consistent preferences across conditions, while Event and Character preferences vary more with instruction type

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selection-based elicitation isolates latent preferences by decoupling choice from production capacity.
- **Mechanism:** By requiring models to select constraints from a fixed library rather than generate text, the methodology filters out confounds related to "ability to produce" complex narrative structures. The model reveals preference priorities when forced to allocate a fixed budget ($K=20$) among trade-offs.
- **Core assumption:** Selection behavior under constrained choice correlates with the underlying decision-making logic that would govern unconstrained generation.
- **Evidence anchors:**
  - [abstract] The study analyzes creative preferences using "constraint-based decision-making," explicitly treating selection as a lens for authorial creativity.
  - [section 1] The authors argue output-centered evaluations "conflate preference with ability," whereas selection tasks "isolate preference from production capacity."
- **Break condition:** If models treat the selection task as a distinct game unrelated to their generative training (e.g., selecting "hard" constraints to maximize future difficulty rather than "preferred" ones), the link between selection and preference breaks.

### Mechanism 2
- **Claim:** LLMs prioritize "Style" (specifically Tone & Mood) because it relies on formal linguistic competence, which is more robust than the functional competence required for long-range narrative coherence.
- **Mechanism:** The paper observes a consistent "Style over Story" hierarchy. The authors suggest this is because LLMs are better at controlling expressive form (tone, register) than managing complex dependencies (plot dynamics, world-building). The observed preference reflects a bias toward high-confidence operational domains.
- **Core assumption:** Preference rankings reflect relative confidence and reliability in specific linguistic competencies.
- **Evidence anchors:**
  - [section 4.2] Results show Style constraints are selected with a Rate Ratio (RR) of 1.67 relative to Event constraints.
  - [section 5] Discussion interprets this asymmetry as potentially reflecting the gap between "formal linguistic competence" and "functional linguistic competence" (citing Mahowald et al., 2024).
- **Break condition:** If a model exhibits equal or higher preference for "Event" or "Character" constraints without degradation in output quality, the competence-asymmetry explanation is insufficient.

### Mechanism 3
- **Claim:** "Creativity" instructions function by shifting "presentist" anchors rather than altering the fundamental structural preference for Style.
- **Mechanism:** Under Basic instructions, models over-select everyday realism ("Urban Built Environments," "The Fully Connected Now"). Under Creativity instructions, models shift away from these anchors toward surreal/non-linear axes, yet the dominance of the "Style" element remains stable. This suggests creativity prompts act as a surface-level deviation signal rather than a structural re-weighting of narrative elements.
- **Core assumption:** The "Creativity" instruction successfully activates a distinct operational mode that values novelty over realism.
- **Evidence anchors:**
  - [section 4.4] Tables show "Creativity" is enriched for "Dreamtime" and "Second person perspective," while "Basic" is enriched for "Domestic Interior Spaces."
  - [section 4.2] Instruction-type contrasts show Style and Setting are stable, while Event and Character show sensitivity.
- **Break condition:** If "Creativity" prompts caused a massive increase in "Event" selection (e.g., prioritizing wild plot twists over tone), the claim that Style is structurally rigid would fail.

## Foundational Learning

- **Concept:** **Formal vs. Functional Linguistic Competence**
  - **Why needed here:** The authors use this distinction to explain why LLMs prefer Style (Formal) over Story (Functional). You cannot interpret the results without understanding that LLMs find it easier to "sound like" an author than to "think like" one.
  - **Quick check question:** Does a high selection rate for "Tone & Mood" prove a model is creative, or merely that it is confident in manipulating linguistic registers?

- **Concept:** **Process-Oriented Evaluation**
  - **Why needed here:** The paper moves beyond judging outputs (Product) to analyzing decision-making (Process). This requires understanding how to design experiments that measure *why* a model made a choice, not just *what* it produced.
  - **Quick check question:** If two models generate identical stories, can a process-oriented evaluation reveal differences in their "authorial preferences"?

- **Concept:** **Rate Ratios (RR) in Preference Analysis**
  - **Why needed here:** The results are quantified via RRs (e.g., Style RR = 1.67). Understanding that RR > 1 implies higher selection frequency relative to a baseline (Event) is necessary to read the data tables.
  - **Quick check question:** If the RR for "Setting" is 1.05 with a wide confidence interval including 1.0, is it statistically meaningful to claim a preference for Setting?

## Architecture Onboarding

- **Component map:** Constructing narratology-grounded library → Running randomized selection trials → Mapping selections to hidden axes → Aggregating counts for GEE modeling
- **Critical path:** Constructing the narratology-grounded library → Running randomized selection trials (Mitigating order effects) → Mapping selections to hidden axes → Aggregating counts for GEE modeling
- **Design tradeoffs:**
  - **Fixed vs. Free Budget:** The paper uses a fixed budget ($K=20$) to force trade-offs. A free budget might show what models find *easy* rather than what they *prefer* when resources are constrained.
  - **Labeled vs. Unlabeled:** Hiding element labels (Experiment 2-2) prevents priming but makes it harder to attribute intent if the model hallucinates categories.
- **Failure signatures:**
  - **Rubric Consistency:** Models ignoring the JSON output format (breaking the parser).
  - **Position Bias:** If randomization fails, models selecting constraints solely from the top/bottom of the list.
  - **Sycophancy:** Models selecting constraints they predict the user wants rather than their own "native" preferences.
- **First 3 experiments:**
  1. **Validation Check:** Run the "Basic" condition on a model not in the paper (e.g., a smaller open-weights model) to see if the Style > Event hierarchy holds.
  2. **Ablation on Order:** Run the same prompt 10 times with fixed order vs. 10 times with randomized order to quantify the magnitude of position bias in this specific task.
  3. **Selection-to-Generation Consistency:** Ask a model to select 5 constraints, then generate a story. Check if the generated story actually adheres to the selected constraints (validating if "preference" predicts "behavior").

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the latent narrative preferences identified through constraint selection predict the characteristics of actual text generated in downstream narrative tasks?
- **Basis in paper:** [explicit] The Discussion section states that "how these preferences manifest in downstream narrative generation under comparable settings remains to be examined."
- **Why unresolved:** The study's methodology explicitly isolates preference ("selection") from production capacity ("generation") to avoid conflating the two, leaving the link between stated preference and output behavior untested.
- **What evidence would resolve it:** A follow-up study correlating the specific constraint selections made by a model with the stylistic and content features of stories subsequently generated by the same model under identical persona conditions.

### Open Question 2
- **Question:** Is the observed preference for "Style over Story" a genuine authorial bias or a byproduct of LLMs' architectural limitations in maintaining long-range coherence?
- **Basis in paper:** [inferred] The Discussion notes that the preference for stylistic dimensions "may reflect asymmetries in LLMs' linguistic capabilities" and acknowledges that "selection remains a proxy and may also reflect capability or alignment constraints