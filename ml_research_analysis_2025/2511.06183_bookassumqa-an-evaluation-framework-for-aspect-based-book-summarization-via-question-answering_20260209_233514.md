---
ver: rpa2
title: 'BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via
  Question Answering'
arxiv_id: '2511.06183'
source_url: https://arxiv.org/abs/2511.06183
tags:
- summaries
- summarization
- aspect-based
- text
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes BookAsSumQA, a QA-based evaluation framework
  for aspect-based book summarization. It constructs a narrative knowledge graph from
  books and generates aspect-specific QA pairs to assess how well summaries capture
  information related to specific aspects (e.g., genres).
---

# BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering

## Quick Facts
- arXiv ID: 2511.06183
- Source URL: https://arxiv.org/abs/2511.06183
- Reference count: 7
- Authors: Ryuhei Miyazato, Ting-Ruen Wei, Xuyang Wu, Hsin-Tai Wu, Kei Harada
- Primary result: QA-based evaluation framework for aspect-based book summarization; RAG methods outperform LLM-only approaches on longer documents

## Executive Summary
This paper introduces BookAsSumQA, a novel evaluation framework for aspect-based book summarization that uses question answering as a proxy for measuring aspect-specific information retention. The framework constructs a narrative knowledge graph from books and generates aspect-specific QA pairs to assess how well summaries capture information related to specific aspects. Evaluation is based on the ability of generated summaries to answer these questions using ROUGE-1, METEOR, and BERTScore metrics. Experiments show that while LLM-based methods perform better on shorter texts, RAG-based methods become more effective as document length increases, making them more practical for aspect-based book summarization.

## Method Summary
BookAsSumQA evaluates aspect-based summaries by automatically generating QA pairs from a narrative knowledge graph constructed from the source book. The method involves chunking books into 1,200-character segments, extracting entities and relations using an LLM with 2-shot prompting, and building a global knowledge graph where repeated relationships accumulate higher importance scores. Aspect-specific QA pairs are generated by filtering graph edges based on keyword similarity to target aspects, then using 1-shot prompting to create questions. Summary quality is assessed by measuring how well the generated aspect-based summary enables an LLM to answer these QA pairs, scored using ROUGE-1, METEOR, and BERTScore metrics.

## Key Results
- RAG-based methods (NaiveRAG, GraphRAG, LightRAG) perform worse than LLM-based methods on short texts but become comparable or better on middle and large text groups
- LLM-based methods degrade significantly with document length (ROUGE-1 drops from ~0.7 to ~0.4)
- BookAsSumQA provides a reference-free evaluation framework that correlates well with human judgments for aspect-based summarization quality

## Why This Works (Mechanism)

### Mechanism 1
QA-based evaluation serves as a proxy for measuring aspect-specific information retention in summaries. By generating questions from important narrative relationships and testing whether summaries can answer them, the framework assesses information coverage without requiring human-authored reference summaries. Core assumption: If a summary enables correct answering of aspect-specific questions, it captures relevant information about that aspect.

### Mechanism 2
Knowledge graphs capture cross-chunk narrative relationships that enable aspect-specific filtering. Entities and relations are extracted per chunk with importance scores, then merged into a global graph where repeated relationships accumulate higher scores. Edges are filtered by keyword similarity to target aspects. Core assumption: Important narrative relationships appear repeatedly or are marked salient; embedding similarity reliably identifies aspect-relevant edges.

### Mechanism 3
RAG methods scale better than LLM-only approaches for long-document aspect-based summarization due to reusable indexing. RAG indexes text once and retrieves per-aspect; LLM methods reprocess entire documents for each aspect. Performance gap narrows as document length increases. Core assumption: Retrieval quality is sufficient; reprocessing overhead dominates for long texts.

## Foundational Learning

- Concept: **Knowledge Graph Construction**
  - Why needed here: Core to BookAsSumQA's QA generation; entities/relations must be extracted and merged across chunks
  - Quick check question: Can you explain how entity deduplication and importance score accumulation work when merging cross-chunk relations?

- Concept: **Hierarchical vs. Incremental Summarization**
  - Why needed here: LLM-based methods compared in the paper; understanding trade-offs is essential for interpreting results
  - Quick check question: What is the difference between recursively merging chunk summaries versus incrementally updating a global summary?

- Concept: **Reference-Free Evaluation Metrics**
  - Why needed here: The framework's justification; understanding QA-based evaluation as an alternative to ROUGE against human references
  - Quick check question: Why might QA accuracy be a better proxy for aspect coverage than n-gram overlap with reference summaries?

## Architecture Onboarding

- Component map: Chunking module -> Entity/relation extractor -> Knowledge graph builder -> QA generator -> Summary evaluator
- Critical path: Chunking → Entity extraction → Graph construction → Edge filtering → QA generation → Summary generation (external) → QA answering → Metric computation
- Design tradeoffs:
  - Importance threshold (10) balances QA quality vs. quantity; too low increases noise, too high misses coverage
  - Top-5 QA per aspect may be insufficient for complex narratives
  - GPT-4o-mini for both QA generation and answering introduces potential circularity
- Failure signatures:
  - GraphRAG underperforms on aspect-based QA despite strong generic summarization—suggests community summaries don't align with aspect granularity
  - Performance degrades with document length across all methods
- First 3 experiments:
  1. Reproduce QA generation on 2-3 books; verify aspect-QA alignment by manual inspection
  2. Compare Hierarchical vs. Incremental summarization on a single book across multiple aspects; measure QA accuracy and latency
  3. Ablate importance threshold (e.g., 5, 10, 15) and top-k QA selection (e.g., 3, 5, 10); analyze impact on metric scores and aspect coverage

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4o-mini for both QA generation and answering may introduce circularity and overestimate summary quality
- Importance threshold of 10 and top-5 QA selection per aspect are somewhat arbitrary and may not capture full aspect complexity
- Focus on 14 literary genres as aspects limits generalizability to other types of aspects or domains

## Confidence
- Framework validity as human judgment proxy: Medium
- Method comparison reliability: Medium
- RAG scaling advantage: High

## Next Checks
1. Conduct a human evaluation study to validate the correlation between QA performance and human judgments of aspect-specific summary quality
2. Experiment with varying the importance threshold and top-k QA selection to assess their impact on evaluation results and aspect coverage
3. Test the framework's generalizability by applying it to non-literary domains or different types of aspects beyond the 14 literary genres used in this study