---
ver: rpa2
title: A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion
  Models
arxiv_id: '2509.19276'
source_url: https://arxiv.org/abs/2509.19276
tags:
- gradient
- diffusion
- latent
- inverse
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a training-free method called Diffusion-regularized
  Wasserstein Gradient Flow (DWGF) for solving inverse problems using latent diffusion
  models. The core idea is to formulate posterior sampling as a regularized Wasserstein
  gradient flow of the Kullback-Leibler divergence in the latent space, avoiding adaptations
  of pixel-space methods.
---

# A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models

## Quick Facts
- **arXiv ID:** 2509.19276
- **Source URL:** https://arxiv.org/abs/2509.19276
- **Reference count:** 33
- **Primary result:** Proposes DWGF, a training-free method using latent diffusion models to solve inverse problems by formulating posterior sampling as Wasserstein gradient flow in latent space.

## Executive Summary
This paper introduces Diffusion-regularized Wasserstein Gradient Flow (DWGF), a training-free method for solving inverse problems using latent diffusion models. The approach formulates posterior sampling as a Wasserstein gradient flow of the Kullback-Leibler divergence in the latent space, avoiding pixel-space adaptations of existing methods. DWGF derives a system of ODEs to approximate this flow and demonstrates performance on standard benchmarks using StableDiffusion. Experiments on FFHQ dataset show competitive PSNR and LPIPS scores to baselines but suffers from poor FID due to mode-seeking behavior leading to blurry reconstructions.

## Method Summary
DWGF solves inverse problems by formulating posterior sampling as a Wasserstein gradient flow of the KL divergence in latent space. The method initializes particles in latent space and evolves them via a system of ODEs derived from the first variation of a regularized KL functional. The regularization incorporates the diffusion model's score function to maintain proximity to the learned prior manifold. Data consistency is enforced through decoder Jacobian and an approximate prior score using the encoder. The algorithm uses Adam optimization to integrate the flow, balancing data fidelity and prior regularization.

## Key Results
- DWGF achieves comparable PSNR and LPIPS scores to baseline methods (PSLD, RLSD) on box inpainting and super-resolution tasks
- Method suffers from poor FID scores (indicates mode collapse) due to KL divergence's mode-seeking behavior
- Quantitative results show competitive performance in some metrics but highlight need for additional regularizations
- Experiments demonstrate the method's effectiveness while revealing fundamental limitations in diversity preservation

## Why This Works (Mechanism)

### Mechanism 1
Posterior sampling can be formulated as gradient descent in probability measure space over latent variables. The paper derives a Wasserstein gradient flow of KL divergence D_KL(q_μ(x₀|y) || p(x₀|y)) in the latent space Z, yielding a continuity equation that corresponds to an ODE system. Solving this ODE transports particles from an initial distribution toward the posterior.

### Mechanism 2
A pretrained diffusion model's score function provides effective regularization for the gradient flow. The regularization R[μ] = ∫₀ᵀ w(s) D_KL(μ(z_s|y) || p_θ⁻(z_s)) ds integrates KL divergence along the forward diffusion process. Its gradient introduces the diffusion score ∇z_s log p_θ⁻(z_s), steering particles toward the learned prior manifold.

### Mechanism 3
Data consistency is enforced via the decoder Jacobian and an approximate prior score using the encoder. The drift term includes -∇x₀ℓ(x₀) · ∂D/∂z₀ for likelihood and (1/ρ²)[D(E(x₀)) - x₀] · ∂D/∂z₀ approximating the prior score via encoder-decoder reconstruction.

## Foundational Learning

- **Wasserstein gradient flows and continuity equation**: Understanding how functionals on distributions induce flows via the continuity equation is prerequisite for grasping the core mathematical framework.
  - *Quick check*: Can you explain why ∇·(μ∇δL/δμ) describes transport of probability mass along steepest descent of L[μ]?

- **Score functions and diffusion models**: The regularization term requires evaluating ∇_z log p_θ⁻(z_s), the score of the diffusion prior at arbitrary noise levels.
  - *Quick check*: Given a pretrained diffusion model, where do you obtain ∇_z log p(z_t) and what schedule relates z₀ to z_t?

- **Functional derivatives (first variations)**: Computing Wasserstein gradients requires taking δL/δμ for KL-based functionals and then spatial gradients.
  - *Quick check*: For D_KL(q || p), what is δD_KL/δq and why does it contain log q - log p + 1?

## Architecture Onboarding

- **Component map**: Pretrained Latent Diffusion Model (StableDiffusion v2.1) -> Particle system (N particles in latent space) -> Forward operator A (degradation) -> Optimizer (Adam)
- **Critical path**: 
  1. Encode or initialize N latent particles z₀⁽ⁱ⁾
  2. For each diffusion timestep s ∈ {T, T-1, ..., 0}: decode particles, compute likelihood gradient, compute data consistency term, diffuse particles, compute scores, update particles via Adam
  3. Return decoded particles D_ϕ⁻(z₀⁽ⁱ⁾)
- **Design tradeoffs**: 
  - Particle count N: More particles improve integral approximation but increase memory
  - Regularization strength γ: High γ prioritizes prior fidelity (risk of blur); low γ risks manifold deviation
  - Decoder variance ρ²: Small ρ makes data consistency sensitive; large ρ weakens coupling
- **Failure signatures**: 
  - High FID, low PSNR: Mode collapse from KL mode-seeking
  - Blurry reconstructions: γ too high or insufficient particles for score approximation
  - Divergence/non-convergence: lr too high or non-differentiable A
- **First 3 experiments**:
  1. Box inpainting on FFHQ-512 with N=4 particles, σ_y=0.001, comparing PSNR/LPIPS/FID against PSLD and RLSD
  2. 8× super-resolution on FFHQ-512 with same metrics and baselines
  3. Ablation on regularization strength γ ∈ {0.05, 0.15, 0.5} and particle count N ∈ {1, 2, 4, 8}

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating entropic regularizations or repulsive potentials into the DWGF functional effectively resolve the mode-seeking behavior responsible for poor FID scores? The authors state that additional regularizations can be incorporated to tackle mode collapse, but experiments are needed to demonstrate improved FID scores and qualitative sharpness.

### Open Question 2
Can the DWGF framework be integrated with few-step or consistency models to reduce the high computational cost associated with large sampling steps? Section D notes that DWGF requires many sampling steps and suggests integrating with recent advances in few-step and consistency models.

### Open Question 3
Can control variates be successfully applied to the gradient flow to achieve variance reduction without destabilizing the optimization trajectory? The conclusion explicitly suggests exploring control variates for variance reduction, but the paper does not implement or test these techniques.

## Limitations
- Mode-seeking behavior of KL divergence leads to poor FID scores and blurry reconstructions
- High computational cost due to large number of sampling steps required
- Approximation of prior score via encoder-decoder reconstruction may fail with significant reconstruction error

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical framework of Wasserstein gradient flow | High |
| Approximation of prior score via encoder-decoder reconstruction | Medium |
| Effectiveness of regularization strength γ=0.15 | Medium |
| Scalability to complex inverse problems | Low |

## Next Checks
1. Test the method on a more challenging inverse problem (e.g., compressed sensing with random projections) to evaluate scalability beyond box inpainting and super-resolution
2. Implement an ablation study with varying encoder variance to test sensitivity to the deterministic encoding assumption
3. Evaluate the method on out-of-distribution images to assess robustness when the latent prior does not cover the observation manifold