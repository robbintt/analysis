---
ver: rpa2
title: Bayesian Optimization for Dynamic Pricing and Learning
arxiv_id: '2510.12447'
source_url: https://arxiv.org/abs/2510.12447
tags:
- demand
- function
- pricing
- price
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses dynamic pricing in two settings: infinite
  inventory and finite inventory with limited selling horizon. The central challenge
  is learning an unknown demand function without assuming a specific parametric form.'
---

# Bayesian Optimization for Dynamic Pricing and Learning

## Quick Facts
- arXiv ID: 2510.12447
- Source URL: https://arxiv.org/abs/2510.12447
- Reference count: 40
- Primary result: BO-based methods outperform RL baselines in dynamic pricing with unknown demand functions

## Executive Summary
This paper addresses dynamic pricing in two settings: infinite inventory and finite inventory with limited selling horizon. The central challenge is learning an unknown demand function without assuming a specific parametric form. The authors propose a Gaussian Process (GP)-based nonparametric approach using Bayesian Optimization (BO) to model the revenue function as a black-box function of price. For the infinite inventory setting, they introduce BO-Inf, which uses GP regression and UCB acquisition to select prices, achieving sublinear regret O(T^{1/2} log^2 T). For the finite inventory setting, they propose two algorithms: GP-Fin-Model-Based, which learns a GP-based transition model and uses value iteration, and BO-Fin-Heuristic, which directly estimates expected cumulative revenue without full value computation.

## Method Summary
The paper proposes GP-based nonparametric dynamic pricing algorithms using Bayesian Optimization. For infinite inventory, BO-Inf uses GP regression with squared-exponential kernel and UCB acquisition (α_UCB(p) = μ_n(p) + κσ_n(p)) to select prices, achieving O(T^{1/2} log²T) cumulative regret. A computationally efficient variant, LightweightBO-Inf, uses bucketed pricing to reduce GP training complexity. For finite inventory, GP-Fin-Model-Based learns a GP-based transition model and uses value iteration, while BO-Fin-Heuristic uses one-step expected revenue estimation with time-decaying exploration. Both methods require no parametric assumptions on demand structure and demonstrate robustness across various demand types.

## Key Results
- BO-based methods outperform state-of-the-art RL algorithms (PPO, DDPG, SAC) in terms of revenue and convergence speed
- BO-Fin-Heuristic is particularly effective in large state spaces, avoiding error accumulation from imperfect demand modeling
- Methods demonstrate robustness across various demand structures, including polynomial, exponential, and scarcity-driven environments
- LightweightBO-Inf reduces GP training complexity from O(N^3) to O(B^3) while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nonparametric GP modeling enables demand learning without restrictive parametric assumptions, achieving robustness under model misspecification.
- Mechanism: The revenue function R(p) = p·D(p) is treated as a sample from a GP prior with squared-exponential kernel. Posterior updates via Eq. (1) provide both mean estimates (expected revenue) and variance (uncertainty). This nonparametric approach adapts to arbitrary demand structures—polynomial, exponential, or scarcity-driven—without assuming functional form.
- Core assumption: The true expected demand function lies in the RKHS associated with the squared-exponential kernel (Assumption 1).
- Evidence anchors:
  - [abstract]: "propose a Gaussian Process (GP) based nonparametric approach... avoids restrictive modeling assumptions"
  - [Section 3.5.4]: Non-polynomial exponential demand D(p) = 100·e^(-(p-5)²/20) shows BO-Inf achieves lower regret than parametric methods.
  - [Section 3.5.6]: Under CVP misspecification (true h(p)=p^0.25, assumed h(p)=p^0.75), BO-Inf converges while CVP fails.
  - [corpus]: Related work on adversarial censorship (arXiv 2502.06168) similarly addresses robustness, but assumes known structure—this paper's nonparametric approach is more flexible.
- Break condition: If demand has discontinuities or non-smooth structure violating RKHS assumptions, GP uncertainty estimates may become unreliable.

### Mechanism 2
- Claim: UCB acquisition provides principled exploration-exploitation balance with sublinear regret guarantees.
- Mechanism: At each iteration, price is selected via p_t = argmax α_UCB(p) = μ_{t-1}(p) + κσ_{t-1}(p). High κ emphasizes exploration (uncertain regions); low κ exploits high-μ regions. This concentrates sampling near the optimum while ensuring sufficient coverage.
- Core assumption: The exploration parameter κ = √β_T with β_T correctly calibrated (Theorem 1).
- Evidence anchors:
  - [Section 3.3]: Theorem 1 proves R_T = O(T^{1/2} log² T) cumulative regret under RKHS assumption.
  - [Section 3.5.1-3.5.2]: BO-Inf converges faster than ILS/CILS/TS under both low and high noise, showing robustness to κ choice.
  - [corpus]: Graph-Attentive MAPPO (arXiv 2511.00039) uses RL exploration—this paper argues UCB is more sample-efficient.
- Break condition: If κ is set too low, algorithm may converge to suboptimal local optima; too high causes excessive exploration.

### Mechanism 3
- Claim: BO-Fin-Heuristic avoids value iteration complexity by using one-step expected revenue with time-decaying exploration.
- Mechanism: Instead of solving full Bellman equations (Eq. 10), BO-Fin-Heuristic estimates R^n_t(p) = p·min(s^n_t, μ_n(p)·(T-t+1)) assuming constant price for remaining horizon. The acquisition α^n_t(p) = R^n_t(p) + κ·e^{-λt}·σ_n(p) decays exploration over time. This avoids O(C·T·P) complexity of value iteration.
- Core assumption: The simplified revenue estimate adequately approximates the true expected cumulative reward.
- Evidence anchors:
  - [Section 4.4.2-4.4.3]: In Poisson demand and scarcity pricing experiments, BO-Fin-Heuristic achieves higher early revenue than GP-Fin-Model-Based.
  - [Table 2]: For C=80, T=160, BO-Fin-Heuristic runtime is 0.44s vs 5246.31s for model-based—over 10,000× faster.
  - [Section 4.4.2]: Heuristic outperforms model-based in large state spaces because it "avoids error accumulation from imperfect demand modeling."
  - [corpus]: No direct corpus comparison; related work focuses on RL approaches rather than BO heuristics.
- Break condition: In settings requiring precise inventory management (e.g., high-value perishable goods), the heuristic's approximation may leave significant revenue on the table.

## Foundational Learning

- Concept: **Gaussian Process Regression**
  - Why needed here: Core modeling component—maps prices to revenue distributions with uncertainty quantification.
  - Quick check question: Can you explain why GP posterior variance σ²_n(x*) doesn't depend on observed values y_i (Eq. 1)?

- Concept: **UCB Acquisition Function**
  - Why needed here: Drives price selection; balances exploring uncertain prices vs exploiting profitable ones.
  - Quick check question: If κ doubles, how does the exploration-exploitation tradeoff shift?

- Concept: **Bellman Optimality / Value Iteration**
  - Why needed here: Required to understand GP-Fin-Model-Based (Eq. 10) and why the heuristic approximation works.
  - Quick check question: Why does value iteration complexity scale as O(C·T·P)?

## Architecture Onboarding

- Component map: GP Module (kernel, hyperparameter tuning, posterior inference) -> BO Loop (acquisition function -> price selection -> observe revenue -> update GP) -> Infinite Inventory Path (BO-Inf / LightweightBO-Inf) OR Finite Inventory Path (GP-Fin-Model-Based / BO-Fin-Heuristic)

- Critical path:
  1. Initialize GP with seed (p₁, r₁) or (p₁, d₁)
  2. Per iteration/season: compute posterior μ, σ → evaluate acquisition → select price → observe → update
  3. For finite inventory: maintain inventory state, enforce non-negativity

- Design tradeoffs:
  - **BO-Inf vs LightweightBO-Inf**: Bucket width b trades accuracy for speed; smaller b = finer resolution but O(B³) complexity
  - **Model-Based vs Heuristic**: Model-Based provides theoretical guarantees (Theorem 2) but scales poorly (Table 2); Heuristic is faster but lacks optimality
  - **κ selection**: Paper sets κ = √β_T per Wang et al. [63]; practical deployments may need tuning

- Failure signatures:
  - **Hyperparameter collapse**: GP length-scale l → 0 causes overfitting; l → ∞ causes underfitting. Monitor marginal likelihood.
  - **Exploration starvation**: If initial prices cluster, σ remains low in unexplored regions; consider warm-start seeding.
  - **Inventory exhaustion**: Finite inventory algorithms may sell out early if μ_n(p) overestimates; enforce σ_min (Remark 2.1).
  - **Non-monotonic demand**: Scarcity pricing (Section 4.4.3) inverts typical price-demand relationship; ensure GP prior doesn't enforce monotonicity.

- First 3 experiments:
  1. **Sanity check**: Run BO-Inf on linear demand D(p) = a - bp with known parameters; verify convergence to p* = a/(2b). Compare regret curve against theoretical bound.
  2. **Misspecification stress test**: Apply BO-Inf and CVP to exponential demand (Eq. 4 setting); confirm parametric CVP stabilizes at suboptimal price while BO-Inf finds global optimum.
  3. **Scalability benchmark**: For finite inventory with C∈{10,40,80}, T∈{80,160}, compare BO-Fin-Heuristic vs GP-Fin-Model-Based runtime and revenue. Verify Table 2 scaling patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can formal theoretical regret bounds be derived for the `BO-Fin-Heuristic` algorithm?
- Basis in paper: [explicit] The authors state in the conclusion, "Looking ahead, a promising direction for future work is to derive theoretical upper bounds on regret for the BO-Fin-Heuristic, which would provide stronger performance guarantees."
- Why unresolved: While the heuristic demonstrates strong empirical performance and scalability, it currently lacks the theoretical guarantees provided for the `BO-Inf` and `GP-Fin-Model-Based` algorithms.
- What evidence would resolve it: A mathematical proof establishing a sublinear regret bound for `BO-Fin-Heuristic` under the defined dynamic pricing assumptions.

### Open Question 2
- Question: Can acquisition functions be adapted to incorporate the specific structure of the underlying MDP to reduce sample complexity?
- Basis in paper: [explicit] The authors list exploring "enhancements through the use of adaptive acquisition functions that incorporate structure of the underlying MDP to reduce sample complexity" as a specific aim for future work.
- Why unresolved: Current acquisition functions (like UCB) operate generally on the function space; incorporating the specific Markov Decision Process constraints (inventory/time dependencies) into the optimization loop remains unexplored.
- What evidence would resolve it: The development of a modified acquisition function that utilizes MDP structural properties, resulting in provably lower sample complexity or faster convergence rates compared to standard GP-UCB.

### Open Question 3
- Question: How can this BO-based framework be extended to handle heterogeneous customer classes and dynamic ad auctions?
- Basis in paper: [explicit] The conclusion suggests that "BO has the potential to address a wide range of dynamic pricing challenges. These include pricing in the presence of heterogeneous customer classes... and dynamic ad auctions."
- Why unresolved: The current study focuses on a single product with a single unknown demand function, assuming a monopolist setting without customer segmentation.
- What evidence would resolve it: A formulation of the BO or GP model that conditions on customer features (contextual bandits) or auction dynamics, validated by experiments showing robustness in these multi-agent or segmented environments.

## Limitations
- Hyperparameter calibration: While theoretical regret bounds specify κ = √β_T, the paper does not report practical κ and λ values used in experiments, leaving uncertainty about implementation specifics.
- RL baseline comparability: The RL baselines (PPO, DDPG, SAC) are shown to underperform BO, but their hyperparameter configurations are not specified, limiting direct method comparison.
- Theoretical finite-inventory bounds: The paper claims GP-Fin-Model-Based achieves O(√T) regret but does not provide the full proof or explicit constants, making practical performance prediction difficult.

## Confidence
- **High confidence**: BO-Inf's sublinear regret guarantee (O(T^{1/2} log²T)) and its superiority over parametric methods under model misspecification (CVP failure in exponential demand experiments).
- **Medium confidence**: BO-Fin-Heuristic's effectiveness in large state spaces—while runtime results are compelling, the approximation quality depends heavily on problem structure.
- **Medium confidence**: Robustness claims across demand types—extensive experiments support this, but the theoretical guarantees assume RKHS smoothness which may not hold for all real-world demand functions.

## Next Checks
1. **Hyperparameter sensitivity**: Run BO-Inf with varying κ values (0.1, 1, 10) on the same demand functions to quantify exploration-exploitation tradeoff impact on regret.
2. **Finite inventory scalability**: For C∈{20, 40, 80}, T∈{80, 160}, compare BO-Fin-Heuristic vs GP-Fin-Model-Based revenue and runtime to verify the claimed O(C·T·P) scaling pattern.
3. **Demand misspecification stress test**: Apply both BO-Inf and parametric CVP to non-polynomial demands (exponential, scarcity-driven) with varying noise levels to confirm nonparametric robustness.