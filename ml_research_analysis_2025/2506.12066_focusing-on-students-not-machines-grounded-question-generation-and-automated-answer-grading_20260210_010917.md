---
ver: rpa2
title: 'Focusing on Students, not Machines: Grounded Question Generation and Automated
  Answer Grading'
arxiv_id: '2506.12066'
source_url: https://arxiv.org/abs/2506.12066
tags:
- grading
- questions
- page
- dataset
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis develops an automated question generation and grading
  system for educational use. It introduces a visual layout-based document chunking
  method for PDFs that reduces context loss compared to fixed-window approaches.
---

# Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading

## Quick Facts
- arXiv ID: 2506.12066
- Source URL: https://arxiv.org/abs/2506.12066
- Authors: Gérôme Meyer; Philip Breuer
- Reference count: 0
- Develops automated question generation and grading system for educational use

## Executive Summary
This thesis presents an automated system for educational question generation and answer grading that shifts focus from machine-centric processing to student-centered assessment. The system introduces a novel visual layout-based document chunking method for PDF processing that preserves contextual relationships better than traditional fixed-window approaches. By leveraging learning objectives and Bloom's taxonomy, it generates educationally relevant questions and reference answers using large language models (LLMs).

The research addresses the critical need for scalable, consistent assessment tools in education while maintaining educational integrity. The system demonstrates strong performance in question generation tasks but reveals that fully autonomous grading remains challenging, requiring human oversight particularly in high-stakes examination contexts. The work establishes a foundation for integrating AI tools into educational assessment while acknowledging the continuing importance of human judgment in evaluation processes.

## Method Summary
The system employs a visual layout-based document chunking method that segments PDFs by detecting whitespace patterns, preserving document structure and reducing context loss compared to fixed-window approaches. Learning objectives and Bloom's taxonomy guide the generation of educationally relevant questions and reference answers using LLMs. For grading evaluation, the research creates ASAG2024, a benchmark dataset combining seven existing datasets with normalized grades on a 0-1 scale. The grading pipeline processes student responses through LLM-based evaluation, comparing performance across multiple models including GPT-4o, which achieves the best results with weighted MAE of 0.21 and RMSD of 0.27.

## Key Results
- GPT-4o achieves best grading performance with weighted MAE of 0.21 and weighted RMSD of 0.27
- Visual layout-based chunking reduces context loss compared to fixed-window document processing
- Larger LLMs demonstrate better generalization to grading tasks compared to specialized systems
- Human oversight remains necessary for examination contexts despite automated system capabilities

## Why This Works (Mechanism)
The system's effectiveness stems from its holistic approach to educational assessment that bridges document processing, pedagogical theory, and AI evaluation. By incorporating Bloom's taxonomy into question generation, the system ensures questions target appropriate cognitive levels rather than just factual recall. The visual layout-based chunking preserves the semantic relationships inherent in document structure, maintaining context that would otherwise be lost in linear text processing. This structural awareness translates to more coherent question generation and more accurate grading, as the system can better understand the relationship between questions, reference answers, and student responses within their original educational context.

## Foundational Learning
- **PDF Document Structure Analysis**: Understanding how visual layout conveys semantic relationships in educational documents - needed to preserve context during processing; quick check: verify whitespace detection accurately identifies section boundaries
- **Bloom's Taxonomy Application**: Framework for categorizing educational objectives by cognitive complexity - needed to generate questions at appropriate difficulty levels; quick check: confirm generated questions align with specified taxonomy levels
- **LLM-Based Grading Evaluation**: Using AI models to assess answer quality against reference standards - needed for scalable automated assessment; quick check: test grading consistency across multiple LLM models
- **Benchmark Dataset Creation**: Combining and normalizing multiple datasets for comprehensive evaluation - needed to provide reliable performance metrics; quick check: validate grade normalization preserves relative performance differences

## Architecture Onboarding

**Component Map**: PDF Processing -> Document Chunking -> Question Generation -> Reference Answer Creation -> Grading Pipeline -> Performance Evaluation

**Critical Path**: The core workflow begins with PDF processing and visual layout-based chunking, which feeds into the question generation pipeline. Learning objectives and Bloom's taxonomy guide the creation of reference answers, which then serve as benchmarks for the grading pipeline that evaluates student responses using LLMs.

**Design Tradeoffs**: The system prioritizes educational validity over pure automation efficiency by incorporating pedagogical frameworks like Bloom's taxonomy. This adds complexity but ensures questions target appropriate cognitive skills. The visual layout approach requires more sophisticated document analysis but preserves context better than simpler methods.

**Failure Signatures**: Poor grading performance may indicate inadequate reference answer quality or mismatched complexity levels between questions and student responses. Context loss during document processing can lead to incoherent question generation. LLM hallucination or bias can affect both question generation and grading accuracy.

**3 First Experiments**:
1. Test visual layout-based chunking on diverse PDF formats (textbooks, research papers, exam papers) to validate context preservation across document types
2. Compare question generation quality using different Bloom's taxonomy levels with expert evaluation of educational relevance
3. Evaluate grading consistency by running the same student responses through multiple LLM models and measuring agreement

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-based grading introduces variability and potential bias from training data
- Visual layout chunking may struggle with complex or non-standard PDF structures
- System performance relies on synthetic benchmark data rather than extensive real-world examination scenarios

## Confidence
- Document chunking methodology: **High** - consistent performance improvements demonstrated
- Question generation pipeline: **High** - shows reliable operation across test cases
- Grading system evaluation: **Medium** - limited by synthetic dataset and narrow real-world testing

## Next Checks
1. Conduct large-scale evaluation using actual examination papers and graded assignments from multiple institutions to validate grading system performance across diverse educational contexts and subject areas

2. Implement longitudinal study comparing automated grading system's consistency and reliability against human graders over multiple grading sessions and time periods

3. Test system's robustness against adversarial inputs and edge cases, including intentionally ambiguous or creative student responses that challenge conventional grading criteria