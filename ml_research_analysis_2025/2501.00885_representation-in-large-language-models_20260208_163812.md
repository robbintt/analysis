---
ver: rpa2
title: Representation in large language models
arxiv_id: '2501.00885'
source_url: https://arxiv.org/abs/2501.00885
tags:
- behavior
- information
- arxiv
- about
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that large language model (LLM) behavior is partially
  driven by representation-based information processing rather than solely by memorization
  and stochastic table look-up. The author proposes a four-part characterization of
  representations based on information, exploitability, behavior, and role, then defends
  this view by showing that alternative explanations fail in key cases.
---

# Representation in large language models

## Quick Facts
- arXiv ID: 2501.00885
- Source URL: https://arxiv.org/abs/2501.00885
- Authors: Cameron C. Yetman
- Reference count: 19
- Primary result: Large language model behavior is partially driven by representation-based information processing rather than solely by memorization and stochastic table look-up.

## Executive Summary
This paper argues that large language models (LLMs) use internal representations to process information rather than relying purely on memorization and pattern matching. The author presents a four-part characterization of representations based on information content, exploitability, behavioral manifestation, and causal role. Two empirical case studies demonstrate that LLMs can generalize to novel inputs, suggesting they've learned structural representations rather than memorizing specific patterns. The paper concludes by describing practical techniques from mechanistic interpretability—particularly probing and intervention methods—that can be used to investigate and verify the presence of representations in LLMs.

## Method Summary
The paper employs a theoretical framework combined with two empirical case studies to investigate representation use in LLMs. The first case study examines Othello-GPT, which was trained on millions of game transcripts and tested on a skewed dataset missing 25% of possible board configurations. The second case study tests GPT's ability to generalize to "rotated" color spaces that are isomorphic to the training distribution. Both studies use probing techniques (training small MLPs on activation patterns) and intervention methods (linear vector additions to activations) to test whether representations encode features and play causal roles in behavior. The approach validates representation claims by showing that information can be decoded from activations and that manipulating these activations produces behaviorally consistent changes.

## Key Results
- Othello-GPT achieved 99.98% legal move accuracy on test sequences not present in training data, despite being trained on a skewed dataset missing 25% of game tree configurations
- GPT generalized to rotated color spaces with 36% top-3 accuracy versus 34% on training distribution, compared to 13% chance performance
- Linear interventions on activation directions produced content-appropriate behavioral changes in both Othello-GPT (board state flipping) and Claude 3 Sonnet (Golden Gate Bridge feature steering)
- Probing methods successfully decoded features from activations with high accuracy in both structured domains tested

## Why This Works (Mechanism)

### Mechanism 1: Mutual Information Encoding
LLM activations carry exploitable information about features via mutual information relationships. A representation R carries information about feature z when I(R, z) > 0, meaning the entropy of R decreases given knowledge of z. Information-bearing states are physically realized in activation patterns that can be probed.

Evidence: [section 2.1] Formal definition applied to LLM representations; [section 4.2.1] Probing method described to detect information presence.

### Mechanism 2: Generalization Through Structural Representations
LLMs form structural representations enabling generalization to out-of-distribution inputs, rejecting pure look-up table explanations. When trained on structured domains, models learn relational representations that transfer to unseen configurations rather than memorizing specific input-output pairs.

Evidence: [section 3.3.1] Othello-GPT achieved 99.98% accuracy on unseen game sequences; [section 3.3.2] GPT generalized to rotated color space (36% vs 34% accuracy).

### Mechanism 3: Causal Intervention Validates Representational Role
Linear interventions on activation directions produce content-appropriate behavioral changes, confirming representations play mechanistic roles. Features encode as linear directions in activation space; adding vectors shifts representations toward different feature directions, and downstream behavior changes consistently with altered content.

Evidence: [section 4.2.3] Othello-GPT board state flipping produced consistent move predictions; Claude 3 Sonnet feature steering caused self-identification as the Golden Gate Bridge.

## Foundational Learning

- **Mutual Information (Shannon)**: Formalizes when R "carries information about" z without requiring semantic interpretation. Why needed here: Provides rigorous basis for INFORMATION condition. Quick check: If H(X|Y) = H(X), what is I(X, Y) and what does this imply about representation?

- **Finite State Automata vs. Turing-Complete Systems**: Distinguishes look-up table (FSA) accounts from representation-based processing; clarifies why generalization matters. Why needed here: Establishes theoretical framework for why representations enable generalization. Quick check: Why can an FSA never generalize to inputs outside its stored input-output pairs?

- **Interventionist Causation**: Justifies probing + intervention as method for establishing mechanistic role (ROLE condition). Why needed here: Provides framework for validating that representations causally influence behavior. Quick check: If intervening on R produces no behavioral change, what are two possible explanations?

## Architecture Onboarding

- **Component map**: Input → Tokenizer → Embedding Layer → Transformer Layers (attention + MLP) → Residual Stream → Unembedding → Output
- **Critical path**: 
  1. Identify candidate feature z (e.g., board state, truth value)
  2. Train linear probe to decode z from activations R (test INFORMATION + EXPLOITABILITY)
  3. Design intervention that alters R's encoded information about z
  4. Measure behavioral change consistency with altered content (test BEHAVIOR + ROLE)
  5. Control for confounds (z* correlated with z, overdetermination)
- **Design tradeoffs**: 
  - Probe complexity: Simpler probes (linear) are more conservative but may miss non-linear encodings; complex probes may overfit
  - Layer selection: Earlier layers may encode surface features; later layers may encode task-relevant abstractions
  - Intervention specificity: Linear vector addition is clean but may affect multiple features simultaneously (polysemanticity)
- **Failure signatures**:
  - Probe succeeds but intervention fails: Information present but not causally used (violates ROLE)
  - Probe succeeds on training distribution but fails on negated/perturbed inputs: Probe learned spuriously correlated feature z* not z
  - Intervention produces no behavioral change: Causal overdetermination or wrong activation location
  - Control probe also succeeds after intervention: Polysemantic encoding, distinct contents share vehicle
- **First 3 experiments**:
  1. Replicate Othello-GPT probe: Train linear probe on layer activations to predict board state ("mine"/"yours"/"empty"), verify decodeability matches ~99% accuracy claim
  2. Test board-state intervention: Identify linear direction for a specific tile's state, flip via vector addition, measure prediction change on next legal moves
  3. Probe for simple syntactic feature: Train probe to detect part-of-speech from intermediate activations in a vanilla GPT-2, test if intervention on POS direction alters grammaticality judgments

## Open Questions the Paper Calls Out

- **Open Question 1**: Under what systematic conditions should we expect LLMs to rely on representation-based processing versus memorization or stochastic table look-up? The paper notes it hasn't provided a systematic account of when representations versus other mechanisms should be expected.

- **Open Question 2**: How can probing methods reliably distinguish whether extracted information reflects the target feature z versus a spuriously correlated feature z*? The paper acknowledges that probes may exploit training set correlations rather than genuine internal representations.

- **Open Question 3**: How should the polysemanticity problem (same representational vehicle encoding multiple contents) be addressed in representation-based explanations of LLM behavior? Current intervention methods may alter multiple representations simultaneously, making causal attribution difficult.

- **Open Question 4**: What determines the content of LLM representations—mere features of text or features of the world? The author remains neutral on whether representations capture textual features or worldly features.

## Limitations

- Weak corpus support: None of the 25 neighbor papers directly engage with representation-based explanations for LLM behavior
- Probe reliability concerns: Methodology assumes probe success indicates feature presence, but no systematic controls for spurious correlation are described
- Limited generalizability: Othello-GPT and color space experiments demonstrate representation learning in structured domains, but it's unclear whether similar mechanisms operate in general-purpose language modeling

## Confidence

- **High confidence**: The mutual information framework for defining representations (INFORMATION condition) is formally sound and well-grounded in information theory
- **Medium confidence**: The four-part characterization (INFORMATION, EXPLOITABILITY, BEHAVIOR, ROLE) provides a useful analytical framework, though empirical validation is limited to two domains
- **Low confidence**: Claims about the prevalence of representation-based processing in general LLMs are under-supported given the narrow empirical base

## Next Checks

1. **Probe validation protocol**: Implement the Levinstein & Herrmann (2024) testing methodology—train probes on standard data, then test on systematically negated/perturbed inputs to distinguish z from z*; measure drop in accuracy as evidence for spurious correlation

2. **Cross-domain intervention replication**: Apply the linear intervention methodology to a different structured domain (e.g., chess or Sudoku) to test whether the causal manipulation approach generalizes beyond Othello

3. **Multi-layer probing analysis**: Conduct layer-by-layer probing across transformer depths in a standard LLM (e.g., GPT-2) for simple syntactic features (POS tags, dependency relations) to map where and how representations emerge