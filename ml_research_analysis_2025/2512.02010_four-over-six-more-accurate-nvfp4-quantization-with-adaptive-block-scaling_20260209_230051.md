---
ver: rpa2
title: 'Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling'
arxiv_id: '2512.02010'
source_url: https://arxiv.org/abs/2512.02010
tags:
- nvfp4
- values
- quantization
- performance
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of low-precision training for
  large language models using NVFP4 quantization, which suffers from quantization
  errors that degrade model performance. The core method, Four Over Six (4/6), adaptively
  scales blocks of values during NVFP4 quantization, choosing between scaling factors
  of 4 or 6 based on which reduces mean squared quantization error.
---

# Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling

## Quick Facts
- **arXiv ID**: 2512.02010
- **Source URL**: https://arxiv.org/abs/2512.02010
- **Authors**: Jack Cook; Junxian Guo; Guangxuan Xiao; Yujun Lin; Song Han
- **Reference count**: 40
- **Primary result**: 4/6 improves NVFP4 training of 30B-parameter models by 22.3% vs state-of-the-art recipes

## Executive Summary
This paper addresses quantization errors in NVFP4 low-precision training that degrade large language model performance. The proposed Four Over Six (4/6) method adaptively scales blocks of values during quantization, choosing between scaling factors of 4 or 6 based on which reduces mean squared quantization error. By making representable values more uniform, particularly improving near-maximal value representation, 4/6 brings training loss significantly closer to BF16 performance while maintaining computational efficiency on NVIDIA Blackwell GPUs.

## Method Summary
Four Over Six modifies NVFP4 quantization by adding per-block scale selection between factors of 4 and 6. For each 16-value block, the method computes both possible block scales, quantizes twice, dequantizes both versions, and selects the one with lower mean squared error. This approach exploits the non-uniform step sizes in FP4 representation, particularly addressing the gap between values 4 and 6 where no representable values exist. The implementation requires modifying tensor-scale computation (using M_FP8=256 instead of 448) to reserve headroom for M=4 scaling.

## Key Results
- 22.3% closer to BF16 training loss when applied to 30B-parameter Nemotron 3 Nano model pre-training
- Outperforms existing NVFP4 training recipes across multiple model sizes and architectures
- Improves perplexity and downstream task performance in post-training quantization scenarios
- Achieves these gains with under 15% overhead on NVIDIA Blackwell GPUs

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: Near-maximal values (scaled values around 5) are the primary source of NVFP4 performance degradation, not scale factor quantization error.

**Mechanism**: FP4's non-uniform step sizes create a gap between values 4 and 6, meaning scaled values near 5 cannot be represented accurately. When the largest value in a block is scaled to 6, the quantized value 4 represents only 66.6% of the block maximum, leaving the range [66.6%, 100%] poorly covered.

**Core assumption**: Performance degradation from quantization correlates with specific high-magnitude value regions rather than being uniformly distributed.

**Evidence anchors**:
- [abstract] "4/6 takes advantage of this by adaptively scaling some blocks to smaller FP4 values, making the distribution of representable values more uniform and reducing quantization error for near-maximal values."
- [Section 2.2, Figure 2b] "The steeper slope x=5 indicates that error on values quantized around 5, where FP4 has no representable values, are primarily responsible for NVFP4's poor performance."
- [corpus] Related work (INT v.s. FP) confirms FP formats handle outliers better but have non-uniform error distribution; no direct contradiction.

**Break condition**: If your model's weight/activation distributions have very few values in the 66-100% of block-maximum range, the adaptive scaling provides diminishing returns.

### Mechanism 2
**Claim**: Scaling blocks to M=4 instead of M=6 trades representable range for finer granularity on near-maximal values, reducing MSE for suitable blocks.

**Mechanism**: When M=4, the quantized value 3 represents 75% of the block maximum, filling the gap that exists under M=6 scaling. This reduces worst-case quantization error for values in the 66-100% range. Example: values [10, 20, 30, 40] quantize to [1.5, 3, 4, 6] under M=6 (MSE=4.33) but to [1, 2, 3, 4] under M=4 (MSE=0).

**Core assumption**: Blocks contain value distributions where the 50% range reduction (from 6/0.5=12 to 4/0.5=8) is acceptable given improved near-maximal accuracy.

**Evidence anchors**:
- [abstract] "This approach makes the distribution of representable values more uniform, particularly improving the representation of near-maximal values."
- [Table 1 and Table 2] Concrete examples showing MSE reduction from 4.33 to 0 for specific blocks; counter-example showing M=6 better for [15, 30, 120, 180].
- [corpus] RaZeR paper exploits redundancy in FP4 encoding for similar accuracy gains, suggesting non-uniform representation is a shared problem.

**Break condition**: Blocks with values spanning the full [0, 6] range (e.g., containing both small and large outliers) will suffer from the reduced dynamic range when forced to M=4.

### Mechanism 3
**Claim**: Per-block MSE comparison between M=4 and M=6 quantized results provides an effective online selection rule with <15% overhead.

**Mechanism**: Quantize each block twice, compute dequantized values, calculate MSE for both versions, select lower-MSE variant. The overhead comes from double quantization and dequantization, but all intermediate values stay in GPU registers.

**Core assumption**: MSE correlates with downstream task performance better than L1 or max-error metrics; the overhead remains below the speedup threshold where FP8 becomes preferable.

**Evidence anchors**:
- [Section 3.1, Table 4] "We find that using mean squared quantization error generally works best" across Llama-3 and Qwen-3 models.
- [Section 3.2] "We observe that the overhead introduced by Four Over Six is under 15%, and we expect that we will be able to reduce this overhead further with more optimization."
- [corpus] No corpus papers directly evaluate MSE vs other selection metrics for block-scaled quantization.

**Break condition**: If hardware lacks efficient dequantization instructions (cvt family on Blackwell), overhead may exceed benefits.

## Foundational Learning

- **Concept**: Block-scaled floating-point formats (NVFP4, MXFP4)
  - **Why needed here**: 4/6 modifies the block-level scaling decision; understanding how block scales interact with tensor-wide scales is prerequisite.
  - **Quick check question**: Given a block of 16 values with max=48 and tensor scale α=0.01, what is the block scale factor ∆ for standard NVFP4?

- **Concept**: FP4 E2M1 representation and its non-uniform step sizes
  - **Why needed here**: The core insight depends on understanding why values near 5 suffer high error and how M=4 scaling changes coverage.
  - **Quick check question**: List all positive values representable in FP4 E2M1 and identify which interval has the largest step size.

- **Concept**: Quantization error metrics (MSE vs L1 vs max-error)
  - **Why needed here**: The paper evaluates three selection rules; understanding their tradeoffs informs why MSE was chosen.
  - **Quick check question**: For values [1.0, 2.0, 3.0] quantized to [1.0, 2.0, 4.0], compute MSE, L1 error, and max absolute error.

## Architecture Onboarding

- **Component map**: Tensor-wide scale (α, FP32) -> Block scale factors (∆, FP8 E4M3) -> Quantized values (X̄, FP4 E2M1)

- **Critical path**:
  1. Compute tensor scale α with M_FP8=256 (not 448) to reserve headroom for M=4 blocks
  2. For each 16-value block: compute ∆(6) and ∆(4), quantize to X̄(6) and X̄(4)
  3. Dequantize both, compute MSE, select lower-error variant
  4. Store selected ∆ and X̄

- **Design tradeoffs**:
  - M=4 reduces dynamic range by 33% (12→8) but improves 75th percentile accuracy
  - MSE selection adds compute overhead (~15%) but is fully parallelizable
  - Tensor scale modification (256 vs 448) slightly reduces overall representable range

- **Failure signatures**:
  - Training divergence: check if outlier-heavy layers have too many M=4 selections
  - Unexpected overhead: verify PTX cvt instructions are being used, not software emulation
  - Scale overflow: ensure tensor scale uses 256, not 448, when M=4 is possible

- **First 3 experiments**:
  1. Profile block-scale selection distribution (M=4 vs M=6) across weight/activation/gradient tensors on a small model to understand which layers benefit most.
  2. Ablate the tensor scale modification (256 vs 448) to quantify its impact on blocks containing the tensor's largest values.
  3. Compare MSE selection against a simpler heuristic (e.g., select M=4 if max/scond_max > 0.75) to evaluate if full double-quantization is necessary.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness may vary significantly with different weight/activation distributions
- Hardware-specific implementation limits portability to non-Blackwell GPUs
- Additional computational overhead compared to standard NVFP4 quantization

## Confidence
**High Confidence**:
- 4/6 reduces quantization error for near-maximal values compared to standard NVFP4
- MSE-based block selection outperforms simple heuristics in controlled experiments
- The method is implementable with minimal overhead on Blackwell GPUs

**Medium Confidence**:
- 22.3% improvement in pre-training loss relative to baseline recipes generalizes across model families
- Tensor scale modification (M_FP8=256) is necessary for safe M=4 scaling without overflow
- Post-training quantization benefits are consistent across different model sizes

**Low Confidence**:
- 4/6 will maintain its relative advantage as NVFP4 recipes continue to evolve
- The 15% overhead can be reduced to negligible levels with further optimization

## Next Checks
1. **Cross-Architecture Validation**: Test 4/6 on non-transformer architectures (e.g., ConvNets, vision transformers) to verify the method's effectiveness beyond LLM contexts and identify distribution-dependent limitations.

2. **Hardware Portability Assessment**: Implement 4/6 on alternative GPU architectures (Ampere, Hopper) to quantify overhead variations and identify hardware-specific bottlenecks that could impact practical deployment.

3. **Distribution Robustness Analysis**: Systematically vary weight and activation distributions (through pruning, regularization, or architectural modifications) to determine the range of scenarios where 4/6 provides meaningful improvements versus cases where it offers minimal benefit.