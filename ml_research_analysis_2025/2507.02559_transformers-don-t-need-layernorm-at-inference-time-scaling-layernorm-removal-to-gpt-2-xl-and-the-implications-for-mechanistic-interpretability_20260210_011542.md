---
ver: rpa2
title: 'Transformers Don''t Need LayerNorm at Inference Time: Scaling LayerNorm Removal
  to GPT-2 XL and the Implications for Mechanistic Interpretability'
arxiv_id: '2507.02559'
source_url: https://arxiv.org/abs/2507.02559
tags:
- gpt-2
- ln-free
- loss
- neurons
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LayerNorm (LN) is widely believed to be essential for transformer-based
  language models, but its exact role at inference time remains unclear. LN also hinders
  mechanistic interpretability by introducing nonlinearities and increasing component
  interactions.
---

# Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability

## Quick Facts
- **arXiv ID:** 2507.02559
- **Source URL:** https://arxiv.org/abs/2507.02559
- **Reference count:** 40
- **Primary result:** LayerNorm can be entirely removed from GPT-2 models (124M-1.5B params) with minimal performance loss, enabling exact mechanistic interpretability through Direct Logit Attribution.

## Executive Summary
This work demonstrates that LayerNorm (LN) is not essential for transformer-based language models by successfully removing it from all GPT-2 variants while maintaining cross-entropy losses within 0.03-0.1 of original models. The key innovation is replacing LN with a linear transformation ("FakeLN") and fine-tuning sequentially across model layers. This approach not only validates that LN isn't fundamental to language modeling but also dramatically improves mechanistic interpretability by eliminating LN's nonlinearities. The authors release LN-free GPT-2 models and show that Direct Logit Attribution becomes mathematically exact without LN, while also confirming that confidence neurons are inactive in these models.

## Method Summary
The authors develop a sequential removal protocol where LayerNorm layers are replaced with linear transformations using fixed standard deviation estimates (σ_avg) computed during fine-tuning. Starting with GPT-2 pretrained models, they systematically remove LN blocks in order: LN_MLP (layer-by-layer), LN_qk, LN_v, and finally LN_f. Each removal is followed by fine-tuning for a specific number of steps (gap hyperparameter) to allow the model to adapt. An auxiliary loss penalizes variance in estimated standard deviations to stabilize training. The protocol is applied across GPT-2 model sizes (124M to 1.5B parameters) on OpenWebText, with evaluation on validation sets and the Pile.

## Key Results
- LN-free GPT-2 models achieve cross-entropy losses within 0.03-0.1 of original models across all sizes
- Direct Logit Attribution errors drop from ~50% to 0% without LN, confirming its exactness in this setting
- Attribution patching accuracy does not improve without LN, indicating other nonlinearities limit its effectiveness
- LN-free models show increased overconfidence (entropy drops, ECE increases) but remain functional
- Sequential removal is essential—removing all LN blocks simultaneously irreparably breaks model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LayerNorm can be replaced with a fixed linear transformation at inference time with minimal performance degradation.
- **Mechanism:** The standard deviation σ in LN is replaced by σ_avg, a fixed scalar computed as the average standard deviation across all tokens during fine-tuning. This "FakeLN" (x - μ)/σ_avg ⊙ γ + β removes the input-dependent nonlinearity while preserving much of the normalization effect through learned adaptation.
- **Core assumption:** The model can learn to maintain relatively consistent activation norms across positions when the adaptive scaling is removed, supported by the auxiliary loss L_aux that penalizes variance in σ_b,s across tokens.
- **Evidence anchors:**
  - [abstract]: "LN can be entirely removed from all GPT-2 models...by replacing it with a linear transformation and fine-tuning on a small fraction of training data. The LN-free models achieve cross-entropy losses within 0.03-0.1 of their original counterparts"
  - [section 3]: "We remove the nonlinearity of LN by replacing the standard deviation in (1) by a scalar, corresponding to an estimate of the average standard deviation, σ_avg"
  - [corpus]: Weak direct evidence; related work (Zhu et al. 2025) proposes Dynamic Tanh as alternative normalization, confirming LN is replaceable, but with different nonlinearity.
- **Break condition:** Removing all LN blocks simultaneously "irreparably breaks the model's performance"—sequential removal is essential.

### Mechanism 2
- **Claim:** Direct Logit Attribution becomes mathematically exact in LN-free models.
- **Mechanism:** DLA approximates the Direct Effect (DE) by linearizing LN via a cached scale. In standard models, DLA(c) = LN_cached(c) · W_U differs from DE(c) = LN(r) · W_U - LN(r - c) · W_U because the LN scale depends on the full residual stream. Without LN's input-dependent nonlinearity, these become identical.
- **Core assumption:** The residual stream maps linearly to logits when LN is removed.
- **Evidence anchors:**
  - [abstract]: "Direct logit attribution (DLA) errors drop from ~50% to 0% without LN, confirming DLA's exactness in this setting"
  - [section 5.1]: "The LN-free fine-tuned model achieves a perfect 0.00% [0.00%, 0.00%] NMAE, empirically confirming that removing the non-linearity introduced by LN eliminates the discrepancy"
  - [corpus]: No direct corpus evidence on this specific interpretability claim.
- **Break condition:** DLA exactness holds only for direct effects; mediated effects still require causal methods.

### Mechanism 3
- **Claim:** Attribution patching accuracy is limited by other transformer nonlinearities, not LN.
- **Mechanism:** Attribution patching uses a first-order Taylor approximation. While LN causes derivative vanishing when patched directions align with the residual stream, removing LN does not improve accuracy, indicating softmax, GeLU, or attention nonlinearities dominate the error.
- **Core assumption:** The first-order approximation error primarily comes from architecture nonlinearities rather than measurement noise.
- **Evidence anchors:**
  - [abstract]: "attribution patching accuracy does not improve, suggesting other nonlinearities limit its effectiveness"
  - [section 5.2]: "Averaged across layers, the improvement is μ = -0.026, with standard deviation σ = 0.082...attribution patching's limitations likely arise from other nonlinearities in the transformer architecture"
  - [corpus]: No corpus evidence on this specific negative finding.
- **Break condition:** If attribution patching were primarily limited by LN, removing LN would improve accuracy—it does not.

## Foundational Learning

- **Concept: LayerNorm mechanics (LN(x) = (x - μ)/σ ⊙ γ + β)**
  - Why needed here: Understanding why LN is nonlinear at inference (σ depends on x) and why γ, β can be folded while σ cannot.
  - Quick check question: Can you explain why batch normalization can be folded to a linear transform at inference but LayerNorm cannot?

- **Concept: Direct Effect vs. Total Effect in causal inference**
  - Why needed here: DLA measures direct effects on logits; understanding mediated vs. direct pathways clarifies what DLA captures.
  - Quick check question: If component A affects component B which affects logits, would ablating A show up in DLA?

- **Concept: First-order Taylor approximation in attribution patching**
  - Why needed here: Attribution patching approximates activation patching with ∇_a f(x_corr) · (a_clean - a_corr); nonlinearities cause this approximation to fail.
  - Quick check question: Why would a large activation difference (a_clean - a_corr) make the Taylor approximation less accurate?

## Architecture Onboarding

- **Component map:** GPT-2 has four LN types per layer: LN^qk (query/key input), LN^v (value input), LN^MLP (MLP input), and LN^f (final unembedding). Removal order matters: LN^MLP first (largest spikes), then LN^qk, LN^v, then LN^f last.

- **Critical path:** Initialize FakeLN with computed σ_avg → Remove one LN block → Fine-tune for g steps (gap hyperparameter) → Recompute σ_avg for next removal → Repeat until all LN removed → Continue fine-tuning to stabilize.

- **Design tradeoffs:** Smaller gaps between removals risk instability; larger gaps increase compute. σ_avg can drift, requiring EMA filtering for large models. Auxiliary loss stabilizes but adds hyperparameter λ.

- **Failure signatures:** Exploding gradients during LN^v removal; cascading gradient norm increases; irrecoverable loss spikes. Models become overconfident (entropy drops, ECE increases).

- **First 3 experiments:**
  1. **Validate FakeLN approximation:** Compute σ_avg on 1000 samples, compare FakeLN(x) vs. LN(x) outputs on frozen model—expect small but nonzero divergence.
  2. **Single-layer removal test:** Remove only LN^MLP_0 from GPT-2 Small, fine-tune 50 steps, measure loss spike recovery—validates sequential removal necessity.
  3. **DLA vs. DE comparison:** On original GPT-2, compute NMAE between DLA and DE on 100 sequences; repeat on LN-free checkpoint—expect ~50% → 0% NMAE reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific transformer nonlinearities beyond LayerNorm limit attribution patching accuracy?
- Basis in paper: [explicit] The authors state "attribution patching's limitations likely arise from other nonlinearities in the transformer architecture" after finding no improvement in LN-free models.
- Why unresolved: The paper expected attribution patching to improve after LN removal but found it did not, identifying an unknown bottleneck.
- What evidence would resolve it: Systematic ablation of other transformer nonlinearities (softmax, GELU, residual connections) to isolate which components most degrade attribution patching accuracy.

### Open Question 2
- Question: Why does the performance gap between LN-free and original models persist despite extended fine-tuning?
- Basis in paper: [explicit] "We find that extending fine-tuning does not reduce the loss gap to vanilla models. Instead, the gap remains approximately constant throughout fine-tuning."
- Why unresolved: The mechanism behind this persistent ~0.03-0.1 cross-entropy gap is unidentified.
- What evidence would resolve it: Analysis of learned representations in LN-free vs. original models to identify functional differences that cannot be recovered through fine-tuning.

### Open Question 3
- Question: Can the LN removal protocol scale to modern LLMs (e.g., LLaMA, GPT-3/4 scale)?
- Basis in paper: [explicit] Future work section: "We focused on the GPT-2 family of models... In the future, we would like to expand our LN removal protocol to more recent models."
- Why unresolved: Only tested on GPT-2 (up to 1.5B parameters); feasibility at larger scales unknown.
- What evidence would resolve it: Applying the sequential removal protocol with auxiliary loss to models with 7B+ parameters and reporting success rates and computational requirements.

## Limitations

- Sequential removal protocol introduces hyperparameters (gap sizes, auxiliary loss weight) that may influence results beyond simply removing LN
- Evaluation focuses on loss and calibration metrics without analyzing whether internal circuit patterns are preserved
- Models show increased overconfidence, suggesting LN plays a role in calibration that isn't fully compensated
- The approach has only been validated on GPT-2 family models, limiting generalizability claims

## Confidence

- **High confidence:** LN can be removed from GPT-2 models with minimal performance degradation (cross-entropy within 0.03-0.1). This is directly measured across all model sizes with consistent results.
- **Medium confidence:** LayerNorm is not essential for transformer-based language modeling. While the empirical results strongly support this, the theoretical understanding of why LN-free models maintain performance remains incomplete.
- **Medium confidence:** Direct Logit Attribution becomes exact without LayerNorm. The 0% NMAE is empirically demonstrated, but this applies only to direct effects, not mediated effects.
- **Low confidence:** Attribution patching limitations are primarily due to other nonlinearities rather than LN. The negative result is clear, but the claim about "other nonlinearities" being the primary limitation requires further validation.

## Next Checks

1. **Internal circuit validation:** Compare attention pattern distributions, residual stream statistics, and feature representations between original and LN-free GPT-2 models on the same inputs. This would verify whether LN removal fundamentally changes how information flows through the model or merely shifts the normalization mechanism.

2. **Generalization stress test:** Evaluate LN-free models on out-of-distribution tasks, adversarial examples, and long-context scenarios where the original models showed sensitivity. This would test whether LN removal compromises robustness despite maintaining in-distribution performance.

3. **Alternative architecture comparison:** Apply the same LN removal protocol to other transformer variants (BERT, RoBERTa, OPT) to determine whether GPT-2's success with LN removal is architecture-specific or a general property of transformers. This would validate the broader claims about LN's non-essential role.