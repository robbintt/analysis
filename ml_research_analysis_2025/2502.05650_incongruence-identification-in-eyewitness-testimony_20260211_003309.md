---
ver: rpa2
title: Incongruence Identification in Eyewitness Testimony
arxiv_id: '2502.05650'
source_url: https://arxiv.org/abs/2502.05650
tags:
- witness
- incongruence
- testimonies
- context
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting and reasoning about
  inconsistencies in eyewitness testimonies, a critical task for assessing reliability
  in legal and investigative contexts. The authors introduce MIND, a dataset of 2,979
  pairs of contextually related answers from 149 events, designed to capture both
  explicit and implicit contradictions.
---

# Incongruence Identification in Eyewitness Testimony

## Quick Facts
- arXiv ID: 2502.05650
- Source URL: https://arxiv.org/abs/2502.05650
- Authors: Akshara Nair; Zeba Afroz; Md Shad Akhtar
- Reference count: 23
- Primary result: INTEND framework achieves 0.75 F1-score on detecting and extracting incongruent spans from eyewitness testimonies, improving over baselines by +5.63%

## Executive Summary
This paper introduces MIND, a dataset of 2,979 contextually related eyewitness testimony pairs, and INTEND, a framework for detecting and extracting incongruent spans using instruction tuning and multi-hop reasoning. The work addresses the critical need for reliable incongruence detection in legal contexts, where eyewitness reliability is paramount. By decomposing the problem into 6W aspects (who, what, when, where, why, what object) and employing a three-hop reasoning chain, INTEND demonstrates significant improvements in both incongruence detection (0.75 F1) and span extraction quality. Human evaluation confirms the framework's effectiveness in identifying contradictory elements across complex multi-perspective narratives.

## Method Summary
The MIND dataset contains 2,979 testimony pairs from 149 events, with each pair including a context question and two witness responses. The INTEND framework employs a two-stage approach: first, instruction tuning on 151 samples with 6W template labels to detect incongruences; second, a three-hop reasoning pipeline to extract contradictory spans. The 6W decomposition uses cloze-style prompts to evaluate agreement across investigative dimensions. The three-hop process sequentially extracts key details, infers contradictions, and identifies specific conflicting spans. The framework is evaluated on both detection accuracy (F1-score) and span extraction quality (F1 plus coverage metrics).

## Key Results
- INTEND achieves 0.75 F1-score on incongruence detection, outperforming baselines by +5.63%
- Three-hop reasoning improves alignment task F1 from 0.446 (one-hop) to 0.512
- Coverage metric shows 57.5% of generated spans rated as "good" by human evaluators
- Mistral-7B base model performs best, though results are consistent across multiple LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
Structured 6W decomposition improves incongruence detection by forcing explicit comparison across defined semantic dimensions. The framework uses cloze-style prompts asking whether each of six aspects (who/identity, what/action, what-object, when/timeline, where/location, why/reason) "agrees," "contradicts," or "is absent from" between testimonies. This decomposes a complex narrative comparison into six independent binary/ternary judgments. Core assumption: Incongruences in eyewitness testimony primarily manifest along these six investigative dimensions; models benefit from explicit scaffolding rather than holistic comparison.

### Mechanism 2
Multi-hop reasoning improves span extraction by separating detail identification from contradiction inference. Three sequential prompts: (1) summarize key points per testimony, (2) apply commonsense reasoning to identify conflicts, (3) extract specific contradictory spans using prior reasoning as context. Each hop receives the previous output as input. Core assumption: LLMs perform better when cognitive load is distributed across specialized subtasks rather than combined in single inference.

### Mechanism 3
Instruction tuning with domain-specific templates outperforms both fine-tuning and generic prompting for this task. Rather than generic "Is there a contradiction?" prompts, INTEND uses structured templates with explicit comparison instructions and masked prediction targets, trained on 151 annotated 6W samples. Core assumption: Small amounts of task-specific instruction data can shift model behavior more effectively than large-scale fine-tuning for specialized reasoning tasks.

## Foundational Learning

- **Natural Language Inference (NLI)**: Incongruence detection extends NLI from sentence-level entailment to multi-sentence narrative comparison with span-level grounding. Quick check: Can you explain why "The suspect wore a green jacket" contradicts "The suspect wore only a shirt" but not "The suspect wore a jacket"?

- **Chain-of-Thought / Multi-hop Reasoning**: INTEND's three-hop approach requires understanding how intermediate reasoning steps feed into final predictions. Quick check: Given two testimonies, what intermediate facts would you extract before determining if they contradict?

- **Instruction Tuning vs. Fine-tuning**: The paper compares these approaches; understanding the difference is critical for implementation choices. Quick check: Why might 151 instruction-tuned samples outperform full fine-tuning on 1,938 training pairs?

## Architecture Onboarding

- **Component map**: Input (context + T1 + T2) -> 6W detection branch -> Multi-hop reasoning branch -> Output (incongruence classification + span extraction)

- **Critical path**: 
  1. Data preparation: Format testimony pairs with context questions
  2. 6W annotation: Create 151+ instruction samples with 6W template
  3. Instruction tuning: Apply LoRA-based fine-tuning (alpha=16, dropout=0)
  4. Inference: Run detection first, then multi-hop reasoning only on incongruent pairs

- **Design tradeoffs**:
  - One-hop vs. three-hop: Three-hop (+14.8% F1 on alignment) increases latency 3×
  - MLM vs. LLM: LLMs outperform MLMs (0.75 vs. 0.64 best F1) but require more compute
  - Coverage vs. precision: Generative span extraction risks hallucination; coverage metric (57.5% good ratings) partially addresses this

- **Failure signatures**:
  - Reasoning errors: Model identifies details correctly in Hop 1 but fails to recognize contradictions in Hop 2
  - Boundary errors: Correct contradiction detection but imprecise span boundaries
  - Content filtering: LLMs refuse to process explicit crime content
  - False positives: Models flag complementary details as contradictions

- **First 3 experiments**:
  1. Baseline replication: Run BigBird-MNLI with question prompts on test split to verify 0.64 F1 baseline
  2. Ablation on hop count: Compare one-hop, two-hop, three-hop on 100 samples to validate Table 8 trends (0.446 → 0.495 → 0.512 F1)
  3. Instruction sample sensitivity: Train with 50, 100, 151 6W-annotated samples to determine minimum viable instruction set

## Open Questions the Paper Calls Out

### Open Question 1
How effectively does INTEND generalize to non-crime domains such as medical testimonies, historical accounts, or journalistic reporting? Basis: Section 7 states extending to other domains would broaden generalizability. Unresolved because MIND focuses exclusively on crime-related scenarios, and the 6W framework was designed with investigative contexts in mind.

### Open Question 2
Can semi-automated annotation methods for 6W labeling maintain accuracy while improving scalability beyond the current 151 manually annotated samples? Basis: Section 7 highlights reliance on manual processes and need for semi-automated methods. Unresolved because only 151 samples were annotated manually, and automated approaches may introduce label noise.

### Open Question 3
How does INTEND perform on real-world courtroom or deposition testimony compared to controlled mock testimonies? Basis: MIND uses Wizard-of-Oz setup with actors under controlled conditions; related work references real-life trial datasets not used for evaluation. Unresolved because mock testimonies may lack linguistic complexity and adversarial deception strategies present in actual legal proceedings.

## Limitations

- MIND dataset focuses exclusively on crime-related testimonies, limiting generalizability to other domains
- Only 151 samples used for instruction tuning raises questions about scalability and robustness
- Multi-hop reasoning pipeline vulnerable to error propagation, particularly in hop-2 reasoning failures
- Dataset not publicly available, preventing independent verification and reproduction

## Confidence

- **High Confidence**: F1-score improvements (+5.63% over baselines) are well-documented and reproducible given access to the dataset
- **Medium Confidence**: Effectiveness of instruction tuning vs. fine-tuning demonstrated but only on this specific task
- **Low Confidence**: Claims about multi-hop reasoning superiority depend heavily on correct hop-2 inference, which shows notable failure modes

## Next Checks

1. **Dataset Accessibility**: Request access to MIND dataset or construct a comparable testimony pair corpus with ground-truth incongruence labels and spans
2. **Instruction Sample Sensitivity**: Systematically test INTEND performance with varying numbers of 6W-annotated instruction samples (e.g., 50, 100, 150) to determine minimum effective training set size
3. **Domain Transfer Test**: Apply INTEND to a different narrative domain (e.g., medical case reports or news articles) to evaluate whether the 6W framework generalizes beyond crime-related testimonies