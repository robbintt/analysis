---
ver: rpa2
title: 'Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language
  Models'
arxiv_id: '2511.10691'
source_url: https://arxiv.org/abs/2511.10691
tags:
- game
- your
- evaluation
- dynamic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SQUIDGAME, a dynamic adversarial benchmark
  designed to evaluate large language models (LLMs) under stress-testing conditions.
  Unlike traditional static benchmarks, SQUIDGAME employs a battle royale-style elimination
  system across six game-like scenarios that challenge models in instruction-following,
  code refactoring, debate, strategic questioning, sequential decision-making, and
  safety alignment.
---

# Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models

## Quick Facts
- arXiv ID: 2511.10691
- Source URL: https://arxiv.org/abs/2511.10691
- Reference count: 40
- Primary result: Introduces SQUIDGAME, a dynamic adversarial benchmark showing low correlation (0.3675) with traditional static benchmarks, revealing orthogonal dimensions of LLM capability

## Executive Summary
This paper introduces SQUIDGAME, a dynamic adversarial benchmark designed to evaluate large language models under stress-testing conditions. Unlike traditional static benchmarks, SQUIDGAME employs a battle royale-style elimination system across six game-like scenarios that challenge models in instruction-following, code refactoring, debate, strategic questioning, sequential decision-making, and safety alignment. The benchmark introduces resource constraints and information asymmetry to create dynamic adversarial environments. Evaluations of 52 LLMs reveal that model performance is multifaceted, dependent on architectural class and task complexity rather than simple scaling laws. The study identifies evidence of evaluation methodology leakage and demonstrates that dynamic evaluation can serve as a valuable complement to static assessments.

## Method Summary
SQUIDGAME is a dynamic adversarial benchmark consisting of six elimination-style game levels (Red-Green Light, Sugar Honeycombs, Tug of War, Marbles, Glass Stepping Stones, The Final) for evaluating 52 LLMs under resource constraints and information asymmetry. The framework uses a tournament engine with dynamic situational broadcasting, injecting tournament state and public history into prompts. Each game tests different capabilities: L1 uses 50 questions from LIVEBENCH, L2 uses 500 code pairs from LIVECODEBENCH, L3 uses WUDC debate topics, L4 involves free-form strategic questioning, L5 requires pure reasoning with safety vectors, and L6 uses SALADBENCH question-choice pairs. The evaluation runs 20 independent tournaments with hybrid judging combining Gemini 2.5 Pro and human annotators, tracking survival rates, elimination outcomes, and resource consumption metrics.

## Key Results
- SQUIDGAME revealed low correlation (0.3675) with traditional static benchmarks, suggesting orthogonal capability dimensions
- DEEPSEEK-R1-0528 showed resource exhaustion in 60% of cases, while lightweight models consumed significantly fewer resources
- Larger models exhibited "instructional inertia" in L1, failing security code generation despite global coherence
- Lightweight models (MINI, FLASH, AIR versions) demonstrated better resource efficiency under quota constraints
- Models showed varied performance across levels, indicating capabilities depend on architectural class rather than simple scaling

## Why This Works (Mechanism)

### Mechanism 1
The battle royale elimination format with escalating difficulty produces a relative ranking that is more resistant to "ranking fraud" and reveals capabilities distinct from static knowledge recall. A sequential knockout system requires models to survive six distinct game levels, each testing a different capability. As models advance, they face stronger survivors, and the dynamic nature (resource constraints, information asymmetry, live feedback) prevents simple memorization of question-answer patterns. The low correlation (0.3675) with CHATBOTARENA suggests this measures an orthogonal capability dimension.

### Mechanism 2
Resource constraints and live feedback pressure models into more efficient reasoning and expose models that rely on verbose, inefficient output strategies. In games like "tug of war," teams are assigned a fixed character quota with reminders of remaining resources. Models must strategically manage output density. DEEPSEEK-R1-0528 was eliminated due to resource exhaustion in 60% of cases, while lightweight models consumed noticeably fewer resources.

### Mechanism 3
Information asymmetry and dynamic situational broadcasting force models to demonstrate reasoning and adaptation under uncertainty, rather than relying on pre-known complete context. Unlike static QA where all context is provided, games like "marbles" and "glass stepping stones" require models to operate with partial information, observe others' failures/successes, and adapt. Public history is injected into prompts, demanding real-time inference rather than recall.

## Foundational Learning

- **Static vs. Dynamic Evaluation Paradigms**
  - Why needed here: The entire premise is that static, resource-unconstrained benchmarks are insufficient.
  - Quick check: Can you explain why a model could score 90% on MMLU but fail the "red-green light" game in SQUIDGAME?

- **Information Asymmetry**
  - Why needed here: Many game levels rely on the model not having full information and needing to infer from partial observations.
  - Quick check: In the "glass stepping stones" game, what information does a model have that the first player did not?

- **Data and Evaluation Methodology Contamination**
  - Why needed here: The paper argues models may learn *how* benchmarks work (not just the data), and dynamic evaluation counters this.
  - Quick check: How does "speculative shortcuts" used by some models in "red-green light" exemplify evaluation methodology leakage?

## Architecture Onboarding

- **Component map**: Game Engine -> Dynamic Situational Broadcaster -> Model Interface Layer -> Hybrid Judging System
- **Critical path**: Understanding prompt templates and state equations for each game, implementing dynamic broadcasting logic, integrating judging logic for subjective tasks
- **Design tradeoffs**: Hybrid judging adds human overhead but increases reliability; sequential level order was empirically validated to ensure sufficient model survival, trading off some randomness for statistical stability
- **Failure signatures**: Models failing "red-green light" show "instructional inertia" or math errors in security codes; in "tug of war," failure is signaled by resource exhaustion; in "glass stepping stones," high trajectory error count indicates poor observational inference
- **First 3 experiments**:
  1. Implement "red-green light" game logic with a few models to validate "instructional inertia" hypothesis
  2. Replicate "tug of war" game with character quota tracking to identify token-inefficient models
  3. Run correlation analysis between SQUIDGAME aggregate scores and CHATBOTARENA scores for a small subset of models

## Open Questions the Paper Calls Out

### Open Question 1
Can the dynamic adversarial evaluation paradigm be effectively extended to multi-modal models (LMMs)? The authors state the current scope only includes text modality and "does not yet include image and other modalities."

### Open Question 2
What specific training methodologies or architectural features drive the "orthogonal" performance observed between dynamic adversarial benchmarks and static benchmarks? The paper reports low correlation but doesn't identify the root cause of this divergence.

### Open Question 3
How can dynamic evaluation frameworks prevent models from exploiting "speculative shortcuts" rather than solving the underlying task? The authors observed weaker models bypassing game logic by modifying numerical sequences.

## Limitations
- The benchmark currently only supports text modality, excluding multi-modal models from evaluation
- Hybrid judging system combining LLM judges and human annotators introduces potential subjectivity and cultural/linguistic biases
- The elimination format may systematically favor robust models rather than specific capabilities in certain dimensions
- Limited detail on which specific static benchmarks were used for correlation analysis, requiring broader validation

## Confidence

- **High Confidence**: Framework design and implementation details are well-specified, including the six game levels, resource constraints, and dynamic situational broadcasting mechanisms
- **Medium Confidence**: The claim that SQUIDGAME captures orthogonal capabilities is supported by correlation analysis but needs testing against wider range of static benchmarks
- **Low Confidence**: Interpretation that dynamic adversarial evaluation prevents "ranking fraud" lacks empirical proof that models cannot adapt to this format through targeted training

## Next Checks

1. **Benchmark Correlation Expansion**: Replicate correlation analysis using SQUIDGAME scores against at least five additional static benchmarks (MMLU, BIG-bench, GSM8K, HumanEval, TruthfulQA) to validate orthogonal capability claim.

2. **Model Adaptation Study**: Conduct controlled experiment where models undergo fine-tuning specifically on SQUIDGAME-like dynamic adversarial patterns, then measure performance changes to test resistance to overfitting.

3. **Judging System Validation**: Implement blinded annotation protocol where human judges evaluate model outputs without knowing game context or model identity, then compare agreement rates with hybrid LLM-human system to quantify potential bias.