---
ver: rpa2
title: Function Forms of Simple ReLU Networks with Random Hidden Weights
arxiv_id: '2505.17907'
source_url: https://arxiv.org/abs/2505.17907
tags:
- function
- where
- approximate
- space
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the function space dynamics of a two-layer
  ReLU neural network in the infinite-width limit, focusing on the Fisher information
  matrix (FIM) and its eigenvectors. The authors derive asymptotic limits of basis
  functions aligned with approximate eigenvectors of the FIM, showing that they converge
  to distinct function forms: proportional to the l2-norm of the input, individual
  input coordinates, pairwise products of coordinates normalized by the norm, and
  specific combinations of squared coordinates.'
---

# Function Forms of Simple ReLU Networks with Random Hidden Weights

## Quick Facts
- arXiv ID: 2505.17907
- Source URL: https://arxiv.org/abs/2505.17907
- Reference count: 40
- Primary result: Asymptotic analysis reveals that ReLU networks with random hidden weights learn functions proportional to input norm, coordinates, and specific quadratic forms, prioritized by gradient descent through Fisher information matrix eigenstructure.

## Executive Summary
This paper analyzes the function space dynamics of two-layer ReLU neural networks in the infinite-width limit with fixed random hidden weights. The authors derive asymptotic limits of basis functions aligned with approximate eigenvectors of the Fisher information matrix (FIM), showing they converge to four distinct forms: proportional to the l2-norm of the input, individual input coordinates, pairwise products normalized by the norm, and specific combinations of squared coordinates. These functions are prioritized by gradient descent due to their alignment with the FIM's leading eigenvectors. The study reveals a novel connection between parameter and function spaces through approximate orthogonality induced by the FIM, validated through simulations showing decreasing mean absolute errors as width increases.

## Method Summary
The method involves constructing four groups of approximate eigenvectors of the Fisher information matrix from the fixed random weight matrix W, then computing the corresponding basis functions f_v(x) = X^⊤v where X = ReLU(x^⊤W). The theoretical limits F(x) are derived for each eigenvector group using asymptotic analysis (weak law of large numbers) as the number of hidden units m approaches infinity. Numerical validation compares X^⊤v against F(x) using Mean Absolute Error across different values of m and input dimension d. The analysis assumes Gaussian input distribution and sufficient width conditions.

## Key Results
- Basis functions converge to four distinct forms: proportional to ∥x∥, individual coordinates xl, pairwise products xαxβ/∥x∥, and specific squared coordinate combinations
- Learning speed hierarchy determined by FIM eigenvalues: (2d+1)/4π > 1/4 > 1/(2π(d+2))
- Mean absolute errors decrease significantly (e.g., 0.0159→0.0058) as width increases from 10K to 100K hidden units
- FIM-induced inner products establish approximate orthogonality between learned basis functions in function space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent prioritizes learning specific function forms based on FIM eigenvector alignment.
- Mechanism: FIM eigenstructure creates learning speed hierarchy where functions proportional to input norm (∥x∥) are learned fastest due to largest eigenvalue ≈ (2d+1)/4π.
- Core assumption: Input x follows standard multivariate normal N(0, Id); network is sufficiently wide (m → ∞).
- Evidence anchors: [abstract] and [Section 1, page 3] discuss FIM-Hessian relationship and convergence rate determination.
- Break condition: Non-Gaussian inputs or training far from optimum where Hessian≠FIM.

### Mechanism 2
- Claim: Basis functions X^⊤v converge in probability to deterministic forms in infinite-width limit.
- Mechanism: Weak law of large numbers applies to averages over hidden units, yielding convergence to (√d/2π)·B(d/2, 1/2)·∥x∥ for group 1, and xl/2 for group 2.
- Core assumption: Hidden weights W ∈ R^(d×m) have IID N(0, 1/m) entries; m → ∞; d > 2 or d ≥ 6 for different groups.
- Evidence anchors: [Theorem 3.1, page 5] and [Section 4.2, page 7] show convergence with decreasing MAE.
- Break condition: Finite-width networks with small m or d < 6 for third-group eigenvectors.

### Mechanism 3
- Claim: FIM-induced inner product establishes approximate orthogonality between basis functions.
- Mechanism: Fisher metric ⟨f, g⟩ = E_x[f(x)g(x)] with x~N(0,Id) creates inner product where ⟨f_vi, f_vj⟩ ≈ λ_i δ_ij, making basis functions nearly orthogonal.
- Core assumption: Gaussian input distribution; approximate eigendecomposition accuracy.
- Evidence anchors: [Section 2.3, page 4] and [Section 3, page 6] discuss orthogonality and eigenvalue recovery.
- Break condition: Non-Gaussian inputs or finite m approximation errors.

## Foundational Learning

- Concept: Fisher Information Matrix (FIM)
  - Why needed here: Core mathematical object whose eigendecomposition determines learning dynamics and function prioritization.
  - Quick check question: Can you explain why FIM eigenvalues relate to gradient descent convergence speed in a regression model?

- Concept: Random Feature Models / Neural Tangent Kernel (NTK) Regime
  - Why needed here: The paper's setting (fixed random hidden weights, trained readout) is the canonical random feature model; NTK provides the theoretical bridge to deeper networks.
  - Quick check question: What is the key difference between the NTK regime and feature learning regime in terms of which weights are trained?

- Concept: Weak Law of Large Numbers / Convergence in Probability
  - Why needed here: Mathematical tool proving that finite-width networks approximate infinite-width limits as m grows.
  - Quick check question: If X_n → X in probability, what does P(|X_n - X| > ε) approach as n → ∞?

## Architecture Onboarding

- Component map: Input layer (d-dimensional x) -> Hidden layer (m units with fixed random W) -> ReLU activation -> Linear readout (X^⊤v) -> Output
- Critical path: Weight initialization (W fixed) → Forward pass (X = ReLU(x^⊤W)) → Linear readout (f_v(x) = X^⊤v) → Gradient descent on v → Convergence prioritized by FIM eigenstructure
- Design tradeoffs:
  - Width m: Larger m improves approximation to infinite-width limit but increases memory/compute (MAE drops ~3x when m goes from 10K to 100K)
  - Input dimension d: Higher d increases variance of ∥x∥² ~ χ²(d), degrading approximation accuracy
  - Assumption: Gaussian inputs are required; real data may violate this
- Failure signatures:
  - High MAE in simulations with low m indicates insufficient width for theoretical predictions
  - If d < 6, third-group eigenvector analysis does not apply
  - Non-Gaussian or correlated inputs break the Fisher metric derivation
- First 3 experiments:
  1. Replicate Table 1: Compute MAE between X^⊤v and theoretical F(x) for all four eigenvector groups across m ∈ {10K, 100K} and d ∈ {10, 50, 100} to validate convergence claims.
  2. Learning dynamics test: Train v via gradient descent on synthetic targets y = F0(x), y = F1(x), y = F12(x) separately; measure convergence rates and verify they match eigenvalue ratios.
  3. Orthogonality validation: For d=10, m=100K, compute empirical ⟨f_vi, f_vj⟩ via Monte Carlo integration and confirm off-diagonal terms are near zero while diagonal terms match theoretical eigenvalues.

## Open Questions the Paper Calls Out

- Question: Do the derived asymptotic functional forms generalize to deeper architectures or networks with non-ReLU activation functions?
  - Basis in paper: [explicit] The conclusion states, "We believe future works could explore generalizations including non-ReLU activations, deeper network structures..."
  - Why unresolved: The current theoretical proofs rely specifically on the properties of two-layer (one hidden layer) ReLU networks.
  - What evidence would resolve it: Derivation of similar convergence limits for the basis functions F_v(x) in networks with multiple hidden layers or alternative activation functions.

- Question: What is the precise relationship between the derived Fisher information matrix (FIM) basis functions and the Neural Tangent Kernel (NTK)?
  - Basis in paper: [explicit] Section 2.3 notes, "A deeper connection exists between the FIM and the Neural Tangent Kernel... however, this analysis is complex and deferred to future work."
  - Why unresolved: While the paper notes the kernel associated with J converges to the NTK, the specific functional interplay remains unanalyzed.
  - What evidence would resolve it: A formal analysis demonstrating how the FIM-aligned basis functions serve as approximate eigenvectors in the function space defined by the NTK.

- Question: How do the asymptotic functional forms change when the input distribution deviates from the standard multivariate Gaussian?
  - Basis in paper: [inferred] The paper acknowledges that assuming x ~ N(0, I_d) "may limit its applicability to real-world datasets that may either be correlated or non-Gaussian."
  - Why unresolved: The proofs utilize the spherical symmetry and specific moment properties of the standard Gaussian distribution.
  - What evidence would resolve it: Theoretical derivation or empirical simulation of the limit functions F_v(x) using correlated or non-Gaussian input data distributions.

## Limitations

- The theoretical analysis relies heavily on the assumption of standard multivariate Gaussian input distribution, limiting applicability to real-world correlated or non-Gaussian data.
- Convergence guarantees for eigenvector groups beyond the leading one require stricter dimensional constraints (d ≥ 6 for certain groups).
- The approximate orthogonality in function space is an approximation rather than exact result, dependent on eigenvector approximation quality.

## Confidence

- **High Confidence:** Asymptotic convergence of basis functions for leading eigenvector group (proportional to ∥x∥) is mathematically rigorous with strong numerical validation.
- **Medium Confidence:** Convergence claims for second and third eigenvector groups are mathematically sound but have tighter dimensional constraints and limited empirical validation beyond d=10.
- **Low Confidence:** Orthogonality property in function space depends on approximation quality and Gaussian assumption, with numerical validation showing it's approximate rather than exact.

## Next Checks

1. **Convergence Rate Validation:** Systematically vary m across multiple orders of magnitude (1K, 10K, 100K) for d ∈ {10, 20, 50} to empirically verify the 1/√m convergence rate claimed for all four eigenvector groups.

2. **Input Distribution Robustness:** Test the theoretical predictions under non-Gaussian input distributions (uniform, exponential, correlated Gaussian) to identify breaking conditions for the Gaussian assumption.

3. **Finite-Width Regime Analysis:** Quantify the deviation between finite-width networks (m=100, 1000) and theoretical limits to establish practical bounds on when the asymptotic analysis remains useful.