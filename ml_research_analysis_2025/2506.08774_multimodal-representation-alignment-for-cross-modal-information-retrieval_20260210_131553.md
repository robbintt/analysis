---
ver: rpa2
title: Multimodal Representation Alignment for Cross-modal Information Retrieval
arxiv_id: '2506.08774'
source_url: https://arxiv.org/abs/2506.08774
tags:
- similarity
- retrieval
- representations
- learning
- cosine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the alignment of visual and textual representations
  using vision-language models and combined unimodal architectures. The study explores
  geometric relationships between embedding spaces and evaluates various similarity
  metrics for cross-modal retrieval tasks.
---

# Multimodal Representation Alignment for Cross-modal Information Retrieval

## Quick Facts
- **arXiv ID:** 2506.08774
- **Source URL:** https://arxiv.org/abs/2506.08774
- **Reference count:** 40
- **Primary result:** Cosine similarity outperforms alternative metrics for models trained with contrastive loss, while MLP-based similarity learning fails to capture cross-modal interactions

## Executive Summary
This paper investigates the alignment of visual and textual representations using vision-language models (VLMs) and combined unimodal architectures for cross-modal information retrieval. The study systematically evaluates various similarity metrics (Euclidean, Manhattan, Chi-square, Cosine) and distribution distance measures (Wasserstein-2, Euclidean) to quantify modality gaps between embedding spaces. Key findings show that cosine similarity significantly outperforms other metrics for contrastive-trained models like CLIP and BLIP, while combined unimodal models fail to achieve meaningful retrieval performance regardless of metric choice. The research also demonstrates that standard MLPs cannot learn effective cross-modal similarity functions from frozen embeddings, suggesting that more sophisticated architectures may be required.

## Method Summary
The study extracts embeddings from penultimate layers of VLMs (CLIP-ViT-B/32, BLIP-base-retrieval, Meta-Transformer-B/16) and combined unimodal models (ResNet-34/BERT, ConViT/RoBERTa, ConvNeXt/XLNet). Features are evaluated using four similarity metrics (Euclidean, Manhattan, Chi-square, Cosine) and two distribution distance measures (Wasserstein-2, Euclidean between centroids). Retrieval performance is measured using Precision@K on three datasets: IMDB Vision and NLP (4K), Flickr30K (31K), and MS-COCO Train2017 (118K). MLPs are trained with Optuna-searched architectures to learn similarity functions from concatenated features, using both MSE and contrastive loss objectives with Adam optimizer.

## Key Results
- Cosine similarity consistently outperforms Euclidean, Manhattan, and Chi-square metrics for contrastive-trained models (CLIP, BLIP)
- Combined unimodal models show near-zero retrieval performance (< 1% P@1) regardless of similarity metric
- Wasserstein-2 distance effectively quantifies modality gaps but smaller gaps don't guarantee better retrieval
- Standard MLPs fail to learn meaningful cross-modal similarity functions despite theoretical universality guarantees

## Why This Works (Mechanism)

### Mechanism 1: Metric-Training Alignment
- Claim: Cosine similarity outperforms alternative metrics specifically for models trained with contrastive loss.
- Mechanism: Contrastive loss (InfoNCE in CLIP/BLIP) directly optimizes cosine similarity between matched image-text pairs during pre-training, creating aligned embedding spaces where the retrieval metric matches the training objective.
- Core assumption: Assumption: Retrieval effectiveness depends on using the same similarity metric that the model was optimized for during training.
- Evidence anchors:
  - [abstract] "cosine similarity outperforms alternative metrics, particularly when using models trained with contrastive loss like CLIP and BLIP"
  - [Section 4.5] "retrieval only performs best when we use the same metric as the one used during training, as MML models are optimized for that specific metric"
  - [corpus] Weak direct support; corpus neighbors focus on contrastive learning but not metric alignment specifically
- Break condition: When using unimodal models trained separately without contrastive loss, cosine similarity shows no significant advantage over Euclidean or Manhattan distance.

### Mechanism 2: Distributional Modality Gap Measurement
- Claim: Wasserstein-2 distance can quantify the modality gap between embedding distributions, but smaller gap does not guarantee better retrieval performance.
- Mechanism: Contrastive training creates a "narrow cone effect" where representations cluster in a restricted region of high-dimensional space; W2 distance measures the transport cost between these distributions rather than just centroid distance.
- Core assumption: Assumption: Distribution-level metrics capture alignment quality better than point-level metrics like centroid distance.
- Evidence anchors:
  - [abstract] "distribution distances such as Wasserstein-2 can effectively quantify modality gaps between representations"
  - [Section 4.3] "BLIP consistently has the smallest modality gap among other models" but "models with smaller modality gap do not necessarily show better performance"
  - [corpus] "Previous multimodal approaches like CLIP utilize InfoNCE...while overlooking distributional differences" (arXiv:2502.17028)
- Break condition: Meta-Transformer shows small modality gap but near-zero retrieval performance.

### Mechanism 3: MLP Architectural Insufficiency
- Claim: Standard MLP architectures cannot learn meaningful cross-modal similarity functions despite theoretical universality guarantees.
- Mechanism: Concatenation-based MLPs fail to capture complex geometric interactions between modalities; contrastive-trained embeddings occupy narrow cones that are linearly separable but resist simple fusion approaches.
- Core assumption: Assumption: More sophisticated architectures (attention-based, transformer fusion) may be required for learned similarity metrics.
- Evidence anchors:
  - [abstract] "conventional architectures such as multilayer perceptrons are insufficient for capturing the complex interactions between image and text representations"
  - [Section 4.5.1] "MLPs failed to learn the cosine similarity between image and text features, regardless of the loss function used...indicating a discrepancy between theory and practice"
  - [corpus] No direct corpus evidence available; this appears to be a novel negative result
- Break condition: A more sophisticated architecture (e.g., cross-attention, transformer-based fusion) successfully learns cross-modal similarity.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: CLIP and BLIP use contrastive loss to align image-text representations; understanding this is essential for interpreting why cosine similarity works best.
  - Quick check question: Why does contrastive loss with InfoNCE create a modality gap rather than perfectly overlapping embedding spaces?

- **Concept: Embedding Space Geometry (Anisotropy)**
  - Why needed here: The paper analyzes geometric properties (modality gap, narrow cone effect, cluster contours) that explain retrieval behavior and MLP failure.
  - Quick check question: What does it mean for embeddings to be "anisotropic" and why does this cause all pairwise cosine similarities to be positive?

- **Concept: Precision@K for Retrieval Evaluation**
  - Why needed here: The paper uses P@1, P@5, P@10 to evaluate cross-modal retrieval; understanding asymmetric precision bounds (5 captions per image vs. 1 image per caption) is critical.
  - Quick check question: Why does P@K drop faster for text-to-image retrieval than image-to-text retrieval when K increases on Flickr30K?

## Architecture Onboarding

- **Component map:**
  - **Feature extractors:** VLMs (CLIP-ViT-B/32, BLIP-base-retrieval, Meta-Transformer-B/16) or combined unimodal models (ResNet-34/BERT, ConViT/RoBERTa, ConvNeXt/XLNet)
  - **Embedding extraction:** Penultimate layer outputs (512D for CLIP, 256D for BLIP, 768D for Meta-Transformer)
  - **Similarity computation:** Cosine similarity (recommended); Euclidean, Manhattan, Chi-square (baselines); MLP with MSE/contrastive loss (not recommended)
  - **Retrieval:** Top-K ranking based on similarity scores

- **Critical path:**
  1. Select VLM: Use CLIP or BLIP (not Meta-Transformer or combined unimodal models)
  2. Extract embeddings from penultimate layer (not final classification layer)
  3. Compute cosine similarity between query and all candidates in target modality
  4. Return top-K results based on similarity ranking

- **Design tradeoffs:**
  - CLIP vs. BLIP: CLIP better for image-to-text on some datasets (IMDB, Flickr30K); BLIP better for MS-COCO—dataset-dependent selection required
  - Multimodal vs. combined unimodal: Multimodal models with contrastive training required; combining separate unimodal models fails (P@1 < 1%)
  - Direct cosine vs. learned MLP: Direct cosine similarity is 10-100× more effective; MLP learning is computationally wasteful

- **Failure signatures:**
  - MLP training stops within 10 epochs → representations cannot be aligned with simple architectures
  - All cosine similarities > 0.95 (Meta-Transformer pattern) → embedding collapse, no discriminative power
  - P@1 near 0% with unimodal models → requires joint contrastive training, not post-hoc alignment
  - Negative cosine similarities between matched pairs → unimodal model embeddings are misaligned

- **First 3 experiments:**
  1. **Baseline retrieval:** Test CLIP and BLIP with cosine similarity on your dataset; expect 50-85% P@1 depending on dataset diversity
  2. **Metric ablation:** Compare all four standard metrics (Euclidean, Manhattan, Chi-square, cosine); confirm cosine advantage only for contrastive-trained models
  3. **MLP failure confirmation:** Train MLP with both MSE and contrastive loss on concatenated features; expect < 2% P@1 to validate paper's negative finding before investing in more complex architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced neural architectures (beyond MLPs) effectively learn cross-modal similarity functions from frozen unimodal embeddings to bridge the modality gap?
- Basis in paper: [explicit] The authors note in Section 5 that "conventional architectures such as multilayer perceptrons are insufficient" and state that the failure of MLPs indicates a "huge gap between theory and practice" regarding the Universal Approximation Theorem.
- Why unresolved: While the theory suggests simple networks can approximate functions, the paper empirically demonstrates that MLPs cannot capture the complex interactions required for cross-modal retrieval from unimodal features.
- What evidence would resolve it: Designing and testing more sophisticated architectures (e.g., attention-based or transformer-based metric learners) that successfully retrieve cross-modal items using the same frozen features where MLPs failed.

### Open Question 2
- Question: How can similarity metrics be designed to reflect semantic divergence (negative similarity) effectively in contrastively trained embedding spaces?
- Basis in paper: [explicit] Section 5 highlights "mathematically anomalous similarity values," noting that unrelated pairs often yield positive cosine similarity due to the narrow cone effect, and argues that an ideal metric should "produce a weighted score that accounts for both aspects" (similarity and dissimilarity).
- Why unresolved: Contrastive learning objectives often result in anisotropic embeddings where representations cluster in a small region, making it difficult for standard metrics like cosine similarity to signal true semantic dissimilarity.
- What evidence would resolve it: A new metric or calibration method that consistently returns negative values for semantically unrelated pairs (e.g., an image of a face and the text "banana") while maintaining state-of-the-art retrieval performance.

### Open Question 3
- Question: What novel geometric alignment techniques can successfully integrate separate unimodal encoders for cross-modal retrieval without requiring joint pre-training?
- Basis in paper: [explicit] In Section 5, the authors conclude that the "fundamental limitation of combined unimodal models warrants novel geometric alignment techniques in future multimodal architectures."
- Why unresolved: The study found that simply combining state-of-the-art unimodal models results in a large modality gap and poor retrieval performance, failing to mimic the alignment of models like CLIP or BLIP.
- What evidence would resolve it: A post-processing or fine-tuning method that aligns the embedding manifolds of separate vision and language models (e.g., reducing Wasserstein-2 distance) such that their retrieval precision matches jointly trained VLMs.

## Limitations
- Combined unimodal models show fundamental limitation, unable to achieve meaningful retrieval performance regardless of similarity metric
- Standard MLP architectures fail to learn cross-modal similarity functions from frozen embeddings
- Smaller modality gap (measured by W2 distance) does not guarantee better retrieval performance

## Confidence
- **Metric alignment mechanism:** High - supported by ablation studies and theoretical alignment with contrastive training objectives
- **Distributional gap measurement:** Medium - W2 distance captures modality gap but relationship to retrieval performance is non-monotonic
- **MLP insufficiency:** High - multiple experimental conditions confirm failure across different loss functions and architectures

## Next Checks
1. Verify cosine similarity advantage for CLIP/BLIP by comparing all four metrics on MS-COCO with consistent splits
2. Confirm MLP training failure by running 3 trials with different random seeds and architectures
3. Test whether Meta-Transformer's uniformly high cosine similarities (>0.9) cause the documented retrieval discrimination failure