---
ver: rpa2
title: 'The power of fine-grained experts: Granularity boosts expressivity in Mixture
  of Experts'
arxiv_id: '2505.06839'
source_url: https://arxiv.org/abs/2505.06839
tags:
- lemma
- experts
- have
- granularity
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that increasing the granularity (number of active
  experts) in Mixture-of-Experts (MoE) architectures improves expressivity exponentially.
  The authors establish a theoretical separation between MoE models with different
  granularities, showing that models with higher granularity can express functions
  that lower-granularity models cannot approximate.
---

# The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts

## Quick Facts
- **arXiv ID:** 2505.06839
- **Source URL:** https://arxiv.org/abs/2505.06839
- **Authors:** Enric Boix-Adsera; Philippe Rigollet
- **Reference count:** 6
- **Primary result:** Increasing the number of active experts in Mixture-of-Experts architectures improves expressivity exponentially.

## Executive Summary
This paper proves that increasing the granularity (number of active experts) in Mixture-of-Experts (MoE) architectures improves expressivity exponentially. The authors establish a theoretical separation between MoE models with different granularities, showing that models with higher granularity can express functions that lower-granularity models cannot approximate. They prove this separation for constant, linear, and ReLU activation functions, and support their findings with experiments demonstrating that student models can only learn teacher models when their granularities match. The results suggest that future MoE architectures should use higher granularity to improve expressivity without changing the total or active parameter count, though hardware limitations may require developing new routing schemes to make highly granular architectures practical.

## Method Summary
The paper analyzes Mixture-of-Experts architectures with parameters $(m, k, w, d)$ where $m$ is total experts, $k$ is active experts, $w$ is expert width, and $d$ is embedding dimension. The method uses student-teacher regression where a student MoE learns a randomly initialized teacher MoE. Training uses synthetic Gaussian input data with Mean-Squared Error objective. The key experiment tests whether students with lower granularity can fit teachers with higher granularity when active parameter counts are matched. Students have 25% more active parameters than teachers ($k'w'd=320d$ vs $kw=256d$) but vary in granularity. The theoretical analysis proves separation between function classes based on the number of expert configurations $\binom{m}{k}$.

## Key Results
- Higher granularity (larger $k$) exponentially increases the number of possible expert configurations $\binom{m}{k}$, enabling more expressive functions
- Theoretical separation is proven: a high-granularity MoE can express functions that a lower-granularity MoE with identical active parameters cannot approximate
- Student models can only successfully learn teacher models when their granularities match, even if the student has more total parameters
- The separation is established for constant, linear, and ReLU activation functions

## Why This Works (Mechanism)

### Mechanism 1: Combinatorial Configuration Growth
If the number of active expert configurations $\binom{m}{k}$ is significantly larger in Model A than Model B, Model A can express functions that Model B cannot approximate, even if both have identical active parameter counts. Increasing granularity $k$ causes the total number of possible expert subsets $\binom{m}{k}$ to grow exponentially. This combinatorial explosion allows the MoE layer to partition the input space into exponentially more distinct regions, assigning a unique function to each region. Theoretical separation is proven by constructing a "hard" function in the high-granularity space that cannot be compressed into the fewer regions available to the low-granularity model.

### Mechanism 2: Fine-Grained Input Space Partitioning
Higher granularity enables the gating network to partition the input space into a larger number of approximately equal-volume regions, allowing for localized, specialized processing. The gating network uses linear routing vectors to define regions $U_S$ where specific expert subsets $S$ are active. By increasing $k$, the system creates intersections of expert "half-spaces" that result in $\binom{m}{k}$ distinct polytopes. This allows the model to fit complex functions that require rapid changes across the input domain, which coarse-grained models smooth over.

### Mechanism 3: Active Parameter Reuse
Increasing granularity allows a model to support more complex functional dependencies without increasing the count of FLOPs or active parameters per forward pass. A high-granularity MoE activates $k$ experts per token, but the specific set of $k$ experts varies. This effectively reuses the same "active parameter budget" to cover a vastly larger functional space. The paper proves that one cannot simulate a high-granularity MoE with a low-granularity MoE simply by increasing the expert size (width $w$).

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Architecture**
  - Why needed here: The paper deconstructs MoE into specific hyperparameters ($m$ total experts, $k$ active experts) and distinguishes "granularity" ($k$) from "sparsity" ($k/m$). Understanding the difference between active and total parameters is essential.
  - Quick check question: If Model A has 100 experts and activates 2, and Model B has 50 experts and activates 1, which has higher granularity if active parameters are equal?

- **Concept: Binomial Coefficients & Combinatorics**
  - Why needed here: The core theoretical result relies on the magnitude of $\binom{m}{k}$ (the number of ways to choose $k$ experts from $m$) as the driver of expressivity. You must grasp why $\binom{256}{8} \gg \binom{16}{1}$.
  - Quick check question: Does $\binom{m}{k}$ grow linearly, polynomially, or exponentially with respect to $k$ (for fixed $m/k$ ratio)?

- **Concept: Function Approximation & Separation**
  - Why needed here: The paper proves "separation," meaning one function class cannot approximate another to within a small error. This requires understanding $L_2$ error norms and the concept that some functions are inherently "hard" for specific architectures.
  - Quick check question: If Model A cannot approximate Model B to within $\epsilon$ error, does adding more parameters to Model A necessarily help if the architecture constraints (like granularity) remain?

## Architecture Onboarding

- **Component map:** Token embeddings $x \in \mathbb{R}^d$ -> Router (linear layer producing scores $G(x) \in \mathbb{R}^m$) -> Selector (top-$k$ gating identifying active set $S$) -> Experts ($m$ parallel 2-layer MLP networks $E_1 \dots E_m$) -> Aggregator (weighted sum of outputs from active experts in set $S$)

- **Critical path:**
  1. Router Initialization: Ensure routing vectors $r_i$ are sufficiently random/orthogonal to guarantee the partition properties described in Lemma 3.2 (requires $d$ to be large enough)
  2. Expert Construction: For theoretical replication, experts are constructed (often randomly) to ensure their aggregated outputs $u_S$ are distinct (Lemma 3.3)
  3. Forward Pass: Compute $f(x) = \sum_{j \in S} E_j(x)$

- **Design tradeoffs:**
  - Expressivity vs. Communication: Increasing $k$ (granularity) boosts expressivity exponentially but linearly increases inter-GPU communication overhead (all-to-all patterns)
  - Expert Width vs. Count: The paper suggests favoring more experts ($m$) and higher granularity ($k$) over wider experts ($w$) for fixed active parameters, provided $d$ is large

- **Failure signatures:**
  - Granularity Mismatch: If distilling a high-granularity teacher (e.g., $k=8$) to a low-granularity student (e.g., $k=2$), the student loss will plateau and fail to converge regardless of student width or total parameters
  - Dimension Mismatch: If embedding dimension $d$ is too small relative to $k(\log m)^2$, the routing vectors cannot partition space effectively, degrading performance

- **First 3 experiments:**
  1. Student-Teacher Granularity Match: Train a random-initialized $(m, k)$ teacher and train students with varying $k'$ (e.g., 1, 2, 4, 8) but identical active parameter counts. Verify that only $k' \ge k$ students converge to zero loss.
  2. Parameter Scaling at Fixed Granularity: Fix teacher granularity ($k=8$). Train students with $k'=2$ but scale their expert width $w'$ up to 16x the teacher's width. Verify failure to converge (confirming width cannot substitute for granularity).
  3. Routing Visualization: Visualize the input space partition $U_S$ for low vs. high granularity models on a 2D synthetic dataset to observe the increased boundary complexity.

## Open Questions the Paper Calls Out

- How do optimization algorithms like SGD effectively harness the expressive power of fine-grained Mixture of Experts (MoE) models during training? The paper provides existence proofs for expressivity but does not analyze the optimization landscape or learning dynamics required to reach these expressive configurations.

- Can the expressivity separation be proven for the regime where the number of active neurons equals or exceeds the embedding dimension ($kw = 2d$)? Theorem 3.6 currently requires $kw \le 0.99d$ to ensure the student and teacher operate on sufficiently distinct subspaces.

- What new routing protocols or hardware mechanisms are required to mitigate the communication overhead and latency associated with high-granularity architectures? While high granularity boosts expressivity, it currently increases routing costs and wall-clock time, making it impractical with existing systems.

## Limitations

- The theoretical claims rely on specific input distribution assumptions (rotationally invariant, e.g., Gaussian or uniform on a ball) that may not hold for real-world data
- The dimension requirement $d \ge Ck(\log m)^2$ for effective routing partitioning could be impractical for many applications
- The paper proves separation between constant, linear, and ReLU MoE models but does not establish whether this separation extends to other activation functions or more complex architectures

## Confidence

- **High Confidence:** The combinatorial argument that $\binom{m}{k}$ grows exponentially with $k$ is mathematically sound and well-established. The theoretical separation proofs for constant, linear, and ReLU functions are rigorous and correctly demonstrate that higher-granularity models can express functions that lower-granularity models cannot approximate.
- **Medium Confidence:** The student-teacher experiments convincingly show that matching granularity is necessary for successful knowledge transfer, but the synthetic nature of the data and the specific training setup limit generalizability to real-world scenarios.
- **Low Confidence:** The claim that "future MoE architectures should use higher granularity" as a universal prescription is premature. The paper acknowledges hardware limitations but does not adequately address how routing efficiency, communication overhead, and optimization challenges scale with granularity in practical systems.

## Next Checks

1. **Real-World Task Validation:** Replicate the student-teacher experiments on standard benchmarks (e.g., language modeling on C4, machine translation on WMT) to verify that the granularity separation persists under realistic training objectives and data distributions.

2. **Routing Efficiency Analysis:** Measure the communication overhead and routing computation costs as granularity increases, and test whether proposed routing schemes (like ReXMoE) can maintain the expressivity benefits while reducing practical bottlenecks.

3. **Alternative Activation Function Testing:** Extend the theoretical analysis to other activation functions (e.g., GeLU, SwiGLU) commonly used in modern architectures to determine if the separation results generalize beyond the three cases studied.