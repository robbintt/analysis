---
ver: rpa2
title: 'Market-Bench: Evaluating Large Language Models on Introductory Quantitative
  Trading and Market Dynamics'
arxiv_id: '2512.12264'
source_url: https://arxiv.org/abs/2512.12264
tags:
- strategy
- executable
- large
- arxiv
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MARKET-BENCH evaluates LLMs on generating executable backtesters\
  \ for three canonical quantitative trading strategies\u2014single-stock scheduled\
  \ trading, pairs mean-reversion, and options delta hedging\u2014using natural-language\
  \ prompts and market data. Models must produce code whose P&L, drawdown, and position\
  \ paths match a reference implementation."
---

# Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics

## Quick Facts
- **arXiv ID**: 2512.12264
- **Source URL**: https://arxiv.org/abs/2512.12264
- **Reference count**: 8
- **Primary result**: LLMs can scaffold basic trading infrastructure but struggle with accurate reasoning about prices, inventory, and risk.

## Executive Summary
MARKET-BENCH evaluates LLMs on generating executable backtesters for three canonical quantitative trading strategies—single-stock scheduled trading, pairs mean-reversion, and options delta hedging—using natural-language prompts and market data. Models must produce code whose P&L, drawdown, and position paths match a reference implementation. Across 329 attempts, execution reliability and numerical accuracy vary widely: Gemini 3 Pro and Claude Sonnet 4.5 combine high executability with low MAE on simpler strategies; GPT-5.1 Codex-Max achieves the lowest best-run MAE (0.002) on the easiest task but higher mean MAE due to variance; GPT-5.2 attains perfect executability and low overall MAE (969.39); Qwen3 Max reaches perfect executability yet exhibits extreme MAE (~1.59×10⁸) due to logical divergence. Results show current LLMs can scaffold basic trading infrastructure but still struggle with accurate reasoning about prices, inventory, and risk, highlighting their unsuitability for direct deployment in high-stakes trading without rigorous validation.

## Method Summary
The benchmark uses Databento L10 order book data for MSFT, KO, and PEP, preprocessed with randomized volume at each price level and limited to top 3 levels. Thirteen LLMs generate Python backtester code (pandas/numpy only) from natural-language prompts across 5 rounds per strategy with distinct input datasets and parameters. Generated code is evaluated for structural reliability (whether it runs) and numerical accuracy (MAE vs. reference metrics), with failed attempts assigned a baseline MAE. The evaluation uses temperature=0.0, 10-minute timeout, and up to 3 attempts per round.

## Key Results
- GPT-5.2 achieves perfect executability (15/15 passes) and lowest overall MAE (969.39)
- Qwen3 Max attains perfect executability but exhibits extreme MAE (~1.59×10⁸) due to logical divergence
- Gemini 3 Pro and Claude Sonnet 4.5 combine high executability with low MAE on simpler strategies
- GPT-5.1 Codex-Max achieves the lowest best-run MAE (0.002) on the easiest task but higher mean MAE due to variance

## Why This Works (Mechanism)

### Mechanism 1: Structural-Semantic Decomposition
- Claim: Separating code executability from numerical accuracy reveals distinct failure modes that single-metric evaluations conflate.
- Mechanism: The benchmark first checks if generated code produces valid output (structural pass), then computes MAE against reference metrics. Failed executions receive a baseline MAE calculated by repeating initial metrics—this penalizes non-execution without masking logical errors in runnable code.
- Core assumption: Baseline MAE (repeating first-row metrics) provides a meaningful penalty floor that prevents failed attempts from appearing better than incorrect but executable attempts.
- Evidence anchors:
  - [abstract] "multi-round evaluation that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics), assigning failed outputs a duplicated-metrics baseline MAE"
  - [section 4.1] "For attempts that do not produce comparable outputs (failed attempts), we assign a strategy specific baseline MAE by repeating the initial (first row) metrics for all timestamps"
  - [corpus] Weak direct evidence; related work on LLM trading agents (Agent Trading Arena) focuses on trading performance rather than benchmark methodology.

### Mechanism 2: Synthetic Book State Persistence
- Claim: Accurate backtesting requires models to track consumed liquidity across time, not just process each order book snapshot independently.
- Mechanism: Models must implement a "synthetic book" that persists the net effect of trades on available liquidity. When a market order consumes volume at multiple price levels, subsequent calculations must reflect this reduced availability. The benchmark explicitly randomizes volume at each price level and limits visibility to top 3 levels to stress this tracking.
- Core assumption: Models understand that order execution is stateful—that trades have lasting effects on market state beyond the immediate transaction.
- Evidence anchors:
  - [section 3.1] "This was done to ensure that the models tracked the liquidity that trades remove from the book and whether they persisted that liquidity correctly"
  - [section 3.2] "All three strategies required the models to track and reserve the liquidity they removed from the book through simulated trades. This is done by reserving these prices and creating a 'synthetic book' which nets the raw order book data and the consumed liquidity"
  - [corpus] No direct corpus evidence; TRADES paper addresses LOB simulation generation but not LLM comprehension of book dynamics.

### Mechanism 3: Temporal Coordination Under Delay
- Claim: Introducing exchange latency exposes reasoning gaps about causality and event ordering that synchronous evaluation misses.
- Mechanism: Strategies 2 and 3 include explicit delays between order submission and execution confirmation. Models must maintain state during this uncertainty window and correctly sequence events (e.g., not assuming fills before confirmation arrives). Fill-or-kill orders in Strategy 3 add binary outcome uncertainty.
- Core assumption: Models can reason about asynchronous event streams and maintain coherent state across delayed feedback loops.
- Evidence anchors:
  - [section 3.2] "Strategies 2 and 3 also include a delay between submitting an order and hearing back from the exchange, mirroring the real world"
  - [section 5.1.3] "The sensitivity of this strategy comes from the interaction between the options delta series, the timing and size of hedge orders, and the fill-or-kill order execution type along with exchange delay"
  - [corpus] ContestTrade addresses multi-agent trading with noise sensitivity but does not isolate latency as an evaluation dimension.

## Foundational Learning

- **Concept: Limit Order Book Mechanics**
  - Why needed here: All three strategies operate on L10 (top 10 price levels) order book data. Understanding bid/ask, depth, VWAP, and how market orders walk the book is prerequisite to implementing any strategy correctly.
  - Quick check question: If you submit a market buy for 500 shares when the ask side shows [100 @ $100.00, 200 @ $100.01, 300 @ $100.02], what is your average fill price and what liquidity remains?

- **Concept: FIFO Lot Accounting**
  - Why needed here: Strategy 1 explicitly requires realized P&L using FIFO accounting. Models must track individual lots, match closes to earliest opens, and compute realized vs. unrealized P&L separately.
  - Quick check question: You buy 100 shares at $50, then 100 shares at $52. You sell 100 shares at $55. What is your realized P&L and what is your remaining unrealized P&L if current price is $54?

- **Concept: Z-Score Mean Reversion Signals**
  - Why needed here: Strategy 2 requires computing a rolling spread, its mean and standard deviation, and generating entry/exit signals when z-scores cross thresholds. Errors in window handling or normalization cause cascading position errors.
  - Quick check question: A spread has 20-period rolling mean of 2.5 and std of 0.4. Current spread is 3.3. Entry threshold is z=2.0, exit threshold is z=0.5. Should you enter a position? If you enter, at what z-score do you exit?

## Architecture Onboarding

- **Component map:**
  Input Layer: Market data (Databento L10) + Strategy prompt + Parameter config -> Model Layer: LLM generates Python backtester code (pandas/numpy only) -> Execution Layer: Code runs against 5 rounds per strategy, each with different data/params -> Evaluation Layer: Structural check (runs without error?) + Numerical check (MAE vs. reference metrics) -> Output: Executable Passes count + Mean MAE + Best Run MAE

- **Critical path:** The evaluation bottleneck is not code generation but state management correctness. Focus on how the model handles: (1) position tracking across time, (2) multi-asset synchronization, (3) order-state during latency windows. These are where semantic failures concentrate.

- **Design tradeoffs:**
  - Unnormalized MAE penalizes absolute divergence but makes cross-strategy comparison difficult—Strategy 3 naturally has larger P&L magnitudes than Strategy 1.
  - Pass@k with k=3 rewards consistency but may miss best-case performance; the paper reports both mean and best-run MAE to address this.
  - Restricting to standard libraries (pandas, numpy, no domain packages) tests reasoning from scratch but may disadvantage models fine-tuned on quantitative libraries.

- **Failure signatures:**
  - Structural failures: Missing function signatures, invalid column references, type mismatches—typically caught immediately.
  - Semantic failures (high MAE, high executability): Wrong rolling window sizes, inverted delta signs, ignoring cooldown periods, miscomputing hedge ratios—these run but diverge from reference. Qwen3 Max's pattern (15/15 executable passes, ~1.59×10⁸ mean MAE) is the canonical example.

- **First 3 experiments:**
  1. Run Strategy 1 with a single model across all 5 rounds, logging the exact code generated. Identify where synthetic book state diverges from reference by injecting intermediate print statements.
  2. Ablate the exchange delay in Strategy 3 by setting it to zero—compare MAE distribution to quantify how much latency contributes to errors vs. core logic failures.
  3. Test a model that fails frequently (e.g., Amazon Nova Premier on Strategy 2) with progressively simplified prompts (remove cooldown, remove shared capital) to isolate which constraint causes structural collapse.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating relative error metrics or correlations of P&L paths alter the model rankings compared to the current unnormalized Mean Absolute Error (MAE) approach?
- Basis in paper: [explicit] The authors state in Section 7 (Limitations) that future work could incorporate relative errors or risk-adjusted metrics because "unnormalized MAE... complicates cross-strategy comparisons."
- Why unresolved: The current evaluation relies on absolute MAE, which can produce "very large values" and potentially skew rankings when comparing strategies with different numerical scales.
- What evidence would resolve it: A re-evaluation of the dataset using normalized relative errors or P&L correlation metrics to see if the ranking of models (e.g., Grok-4 vs. GPT-5.2) changes significantly.

### Open Question 2
- Question: How does LLM performance on MARKET-BENCH generalize to more complex financial scenarios like options pricing, multi-asset portfolios, or transaction-cost-sensitive execution?
- Basis in paper: [explicit] The authors note in Section 7 that the benchmark "does not yet cover options pricing, multi-asset portfolios beyond pairs, intraday inventory risk limits, or transaction-cost-sensitive execution tactics."
- Why unresolved: The current study is restricted to three canonical strategies, leaving the models' ability to handle more advanced or nuanced trading mechanics untested.
- What evidence would resolve it: Extending the benchmark to include these advanced strategies and evaluating whether the failure modes (e.g., semantic divergence) persist or worsen.

### Open Question 3
- Question: What specific reasoning deficiencies cause models with high structural reliability (e.g., Qwen3 Max) to produce executable backtests that suffer from extreme logical divergence?
- Basis in paper: [inferred] The paper highlights in Section 6.1 that "models that are consistent but logically unsound tend to show high Executable Passes with large mean MAE," citing Qwen3 Max as a prime example, but does not pinpoint the specific code logic errors.
- Why unresolved: The results show a disconnect between the ability to write valid code (syntax) and the ability to implement faithful financial logic (semantics).
- What evidence would resolve it: A qualitative analysis of the code generated by high-executability/low-accuracy models to identify specific systematic errors in financial logic (e.g., incorrect spread computation or lot tracking).

## Limitations

- The benchmark does not yet cover options pricing, multi-asset portfolios beyond pairs, intraday inventory risk limits, or transaction-cost-sensitive execution tactics.
- The unnormalized MAE metric complicates cross-strategy comparisons due to different numerical scales, potentially skewing model rankings.
- Performance varies dramatically across strategies, suggesting the benchmark may not uniformly stress all aspects of quantitative reasoning.

## Confidence

- **High**: Separation of structural vs. numerical accuracy, synthetic book state persistence mechanism
- **Medium**: Cross-strategy MAE comparisons, latency impact quantification
- **Low**: Generalization to production trading environments, real-world market microstructure

## Next Checks

1. **Ablate exchange delay in Strategy 3**: Compare MAE distribution with delay=0 versus the original latency setting to isolate timing-related errors from core logic failures.
2. **Progressive prompt simplification**: Test structurally failing models (e.g., Amazon Nova Premier on Strategy 2) with incrementally simplified constraints to identify the minimum viable reasoning chain for success.
3. **Cross-dataset robustness**: Run successful models on out-of-distribution market data (different tickers, volatility regimes) to assess whether performance gains reflect genuine understanding versus prompt/data overfitting.