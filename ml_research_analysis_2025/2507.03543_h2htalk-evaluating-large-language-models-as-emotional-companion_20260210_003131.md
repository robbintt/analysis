---
ver: rpa2
title: 'H2HTalk: Evaluating Large Language Models as Emotional Companion'
arxiv_id: '2507.03543'
source_url: https://arxiv.org/abs/2507.03543
tags:
- arxiv
- emotional
- language
- large
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: H2HTalk introduces the first comprehensive benchmark for evaluating
  LLMs as emotional companions, featuring 4,650 scenarios across dialogue, recollection,
  and itinerary planning. The benchmark incorporates a Secure Attachment Persona module
  based on attachment theory to ensure safer interactions.
---

# H2HTalk: Evaluating Large Language Models as Emotional Companion

## Quick Facts
- arXiv ID: 2507.03543
- Source URL: https://arxiv.org/abs/2507.03543
- Authors: Boyang Wang; Yalun Wu; Hongcheng Guo; Zhoujun Li
- Reference count: 40
- Introduces first comprehensive benchmark for evaluating LLMs as emotional companions with 4,650 scenarios

## Executive Summary
H2HTalk presents the first systematic benchmark for evaluating LLMs as emotional companions, addressing the critical gap in assessing AI's capacity for affective support. The benchmark introduces three evaluation dimensions—dialogue, recollection, and itinerary planning—each testing different aspects of emotional intelligence. A key innovation is the Secure Attachment Persona module based on attachment theory, which dramatically improves safety outcomes while maintaining linguistic quality. Testing 50 diverse LLMs revealed significant challenges in long-horizon planning and memory retention, with models struggling particularly when user needs are implicit or evolve mid-conversation.

## Method Summary
H2HTalk evaluates LLMs across 4,650 scenarios using a three-dimensional framework: Companion Dialogue (base, emotion, schedule), Companion Recollection (synthesis, refinement, initialization), and Companion Itinerary (basic→advanced planning). The unified scoring formula combines BLEU-n (n=1..4), ROUGE-1/L, and BGE-M3 embedding similarity with α=0.6 weighting. A Secure Attachment Persona module implements attachment theory principles for safety. Proprietary and open-source models are tested using OpenCompass on 64× NVIDIA H800 GPUs, with GPT-4o rubric-based scoring for itinerary tasks and human review triggered when scores fall below 3.5.

## Key Results
- DeepSeek-V2.5 achieved highest overall score of 54.47, outperforming larger general models
- Without SAP, safety perception scores dropped from 4.8 to 3.2 and violation rates increased tenfold (0.7% to 7.1%)
- Qwen2.5-72B-Instruct-LoRA achieved highest open-source score of 50.82
- Proprietary Claude-3.7 dominated multi-stage planning with Itinerary-Advanced score of 58.72
- Models showed strong performance on Recollection-Synthesis (up to 86.42) but struggled with Itinerary-Advanced (max ~58)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured psychological scaffolding via Secure Attachment Persona (SAP) dramatically reduces harmful responses while preserving linguistic fluency.
- Mechanism: SAP operationalizes Bowlby's attachment theory through calibrated boundary maintenance, emotional accessibility (Ainsworth's secure base), Gottman's positive interaction ratio, Gross's self-regulation algorithms, and Fisher's principled negotiation—creating systematic guardrails against unsafe outputs without requiring explicit rule lists for every scenario.
- Core assumption: Models can internalize attachment-theory principles as behavioral priors rather than post-hoc filters, meaning the safety emerges from persona alignment rather than output censorship.
- Evidence anchors:
  - [abstract]: "The benchmark incorporates a Secure Attachment Persona module based on attachment theory to ensure safer interactions. Without SAP, safety perception scores dropped from 4.8 to 3.2 and violation rates increased tenfold."
  - [section 5, Table 3]: Ablation study shows BLEU-4 only dropped from 32.7 to 31.5, ROUGE-L from 58.9 to 57.8, but Safety Score collapsed from 4.8 to 3.2 and Violation Rate surged from 0.7% to 7.1%.
  - [corpus]: Related work on "Detecting and Preventing Harmful Behaviors in AI Companions" (SHIELD system) confirms early-stage problematic behavior detection is critical but under-addressed; corpus supports safety-as-emergent-property approach but lacks direct SAP comparisons.

### Mechanism 2
- Claim: Emotional companionship requires three distinct capability dimensions that decompose along memory horizon and planning complexity axes.
- Mechanism: H2HTalk separates evaluation into Dialogue (immediate response quality), Recollection (memory formation, refinement, and initialization across sessions), and Itinerary (basic→middle→advanced planning with time horizons extending from daily routines to future activities)—each requiring different architectural supports (context window management, persistent memory, multi-step reasoning).
- Core assumption: These three dimensions are orthogonal enough that performance on one doesn't strongly predict performance on others; models may excel at empathetic dialogue while failing at multi-day planning.
- Evidence anchors:
  - [abstract]: "Testing 50 diverse LLMs revealed significant challenges in long-horizon planning and memory retention, with models struggling when user needs are implicit or evolve mid-conversation."
  - [section 4.2, Table 2]: DeepSeek-V2.5 achieves 62.33 on Itinerary-Middle but only 43.63 on Dialogue-Emotional; Qwen2.5-72B hits 86.42 on Recollection-Synthesis but drops to 41.15 on Itinerary-Advanced. Top recollection scores (86.42) far exceed top itinerary scores (62.33), confirming the dimensions stress different capabilities.
  - [corpus]: MoodBench 1.0 paper explicitly notes the field lacks "clear definitions and systematic evaluation" for emotional companionship systems; corpus supports the multi-dimensional framing but doesn't validate the specific Dialogue/Recollection/Itinerary decomposition.

### Mechanism 3
- Claim: Implicit instruction-following in emotional contexts is the critical failure mode that standard benchmarks miss.
- Mechanism: In affective support, users rarely state needs explicitly—saying "I can't hold on anymore" is an implicit request for comfort/guidance. H2HTalk's instruction-following evaluation spans three scenarios: Implicit-Help (emotional intensity with minimal directives), Ambiguous/Contradictory (shifting goals), and Context-Aware (requiring conversation history comprehension).
- Core assumption: Most current LLMs have been trained on explicit instruction-following data and lack the pragmatic inference capability to detect covert directives in emotionally-charged contexts.
- Evidence anchors:
  - [section 5, Fig. 7]: "Results reveal that while leading models handle explicit commands adequately, they struggle with implicit or contextual instructions"—the paper explicitly frames this as H2HTalk's diagnostic value.
  - [section 5]: "When a user states, 'I just can't hold on anymore,' they're implicitly requesting comfort and guidance. Most benchmarks overlook these nuances."
  - [corpus]: "Mental Health Impacts of AI Companions" triangulation study notes that AICC impacts on wellbeing remain unclear precisely because response appropriateness in nuanced emotional contexts is hard to measure; corpus supports the gap identification but doesn't offer alternative evaluation approaches.

## Foundational Learning

- **Attachment Theory (Bowlby-Ainsworth framework)**
  - Why needed here: SAP is built on this; you need to understand secure vs. insecure attachment patterns to debug why the persona responds certain ways to user distress signals.
  - Quick check question: Can you explain why a "secure base" response to user anxiety differs from a "dismissive-avoidant" response, and how SAP encodes this preference?

- **Long-horizon reasoning in autoregressive models**
  - Why needed here: Itinerary-Advanced scores (max ~58) dramatically underperform Recollection-Synthesis (max ~86), indicating planning over extended contexts is the architectural bottleneck.
  - Quick check question: Why would a model that can retrieve information from a 50-turn conversation history still fail to maintain coherent multi-day activity plans?

- **Implicit vs. explicit instruction following (pragmatics)**
  - Why needed here: The key differentiator between H2HTalk and prior benchmarks; models fail when they can't infer user intent from indirect cues.
  - Quick check question: Given "I'm so tired of everything lately," what are three plausible implicit requests a user might be making, and how would you evaluate whether a response addresses them?

## Architecture Onboarding

- **Component map:**
  Input Layer -> SAP Module -> Core LLM -> Memory System -> Planning Module -> Evaluation Stack

- **Critical path:**
  1. Start with SAP implementation—this is the highest-impact safety component (10x violation reduction)
  2. Validate on Dialogue tasks first (easiest), then Recollection (memory architecture), then Itinerary (hardest—long-horizon planning)
  3. Set up the evaluation pipeline with GPT-4o scoring before running at scale; human adjudication triggers below 3.5

- **Design tradeoffs:**
  - **Fluency vs. Safety**: Removing SAP causes only ~3% lexical metric drops but 33% safety perception reduction and 10x violation increase—trade heavily toward safety
  - **Model scale vs. cost**: Qwen2.5-7B-Instruct-LoRA (48.58 overall) approaches some 30B+ baselines, suggesting fine-tuning can substitute for scale in this domain
  - **Specialized vs. general**: DeepSeek-V2.5 (54.47) outperforms larger general models; Qwen2.5-Coder-32B scores 59.23 on Itinerary-Middle—domain-specific training transfer exists

- **Failure signatures:**
  - **Memory collapse**: Recollection scores drop sharply for models without explicit memory mechanisms—symptom is repetitive responses or failure to reference earlier conversations
  - **Planning fragmentation**: Low Itinerary-Advanced scores indicate the model can't maintain coherent multi-day plans—symptom is contradicting yesterday's suggestions or forgetting scheduled activities
  - **Implicit need blindness**: Models respond to surface content without addressing emotional subtext—symptom is generic advice when user is implicitly asking for validation

- **First 3 experiments:**
  1. **SAP ablation replication**: Run your base model on the 33 high-risk scenarios with and without SAP; confirm the 4.8→3.2 safety score drop pattern before investing in other components.
  2. **Memory architecture sweep**: Test 3 memory strategies (no persistent memory, sliding window, and explicit memory refinement) on Recollection tasks; quantify the synthesis/refinement/initialization gaps.
  3. **Implicit instruction probe**: Build a 20-example test set of implicit-help scenarios from your target user population; compare GPT-4o rubric scores against explicit-instruction baselines to measure your specific gap.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM architectures or training paradigms be improved to effectively detect and adapt to implicit or mid-conversation evolving user needs in emotional support scenarios?
- **Basis in paper:** [explicit] The abstract and discussion explicitly state that models are "struggling when user needs are implicit or evolve mid-conversation" and Figure 7 highlights failures in "Implicit-Help" and "Ambiguous/Contradictory" instruction following.
- **Why unresolved:** Current instruction-following capabilities rely heavily on explicit commands, causing models to fail when users express intense emotions while concealing underlying needs.
- **What evidence would resolve it:** Development of models that achieve parity with human performance on the "Implicit-Help" subset of H2HTalk, demonstrating the ability to infer latent directives from emotional context without explicit prompts.

### Open Question 2
- **Question:** Can the principles of the Secure Attachment Persona (SAP) be internalized into model weights to maintain safety without external scaffolding?
- **Basis in paper:** [inferred] Table 3 shows that removing the SAP module causes a tenfold increase in violation rates (0.7% to 7.1%), suggesting the underlying models lack intrinsic safety alignment for high-stakes emotional contexts.
- **Why unresolved:** The current benchmark demonstrates the efficacy of the SAP as a module, but it remains unclear if models can internalize these attachment-theory principles to remain safe autonomously when such modules are disabled.
- **What evidence would resolve it:** Training a model on SAP-integrated data and testing it in a "SAP-less" configuration to see if it maintains a low Violation Response Rate (e.g., < 1%) similar to the scaffolded condition.

### Open Question 3
- **Question:** What specific architectural components are necessary to close the performance gap in long-horizon planning between leading proprietary models and open-source alternatives?
- **Basis in paper:** [explicit] Section 4.2 notes that proprietary Claude-3.7 dominates multi-stage planning (Itinerary-Advance) with a score of 58.72, significantly outperforming open-source leaders like Qwen2.5-72B (47.24) in these complex scenarios.
- **Why unresolved:** The results indicate that raw parameter scaling alone is insufficient for complex, multi-step itinerary reasoning, suggesting proprietary alignment or architectural techniques are currently superior.
- **What evidence would resolve it:** An open-source model matching the Itinerary-Advanced scores of Claude-3.7 through innovations in memory retention or planning modules rather than just increased parameter counts.

## Limitations
- The attachment-theory foundation, while theoretically grounded, lacks empirical validation specific to SAP's implementation
- The three-dimensional decomposition (Dialogue/Recollection/Itinerary) is well-motivated but not proven to capture all critical aspects of emotional companionship
- GPT-4o rubric-based scoring introduces potential evaluator bias and computational opacity, though human adjudication thresholds provide some guardrails

## Confidence
- **High Confidence**: SAP's dramatic safety impact (4.8→3.2 perception score, 0.7%→7.1% violation rate) - directly measured and substantial
- **Medium Confidence**: The three-dimensional task decomposition and implicit instruction hypothesis - well-supported by score distributions but not conclusively validated against alternative frameworks
- **Low Confidence**: Attachment theory as the optimal safety mechanism - theoretically sound but lacks comparative validation against other psychological frameworks or rule-based approaches

## Next Checks
1. **SAP Component Ablation**: Systematically remove individual attachment-theory principles (secure base, positive ratio, self-regulation, principled negotiation) to identify which drives the safety improvements versus which merely contributes to linguistic quality.

2. **Cross-Domain Transfer Test**: Evaluate whether models excelling at H2HTalk itinerary planning show corresponding improvements on standard planning benchmarks (e.g., GSM8K, TravelPlanner), isolating emotional-companion-specific from general planning capabilities.

3. **Implicit Need Detection Benchmark**: Construct a targeted evaluation set with varying degrees of explicit vs. implicit user needs, measuring not just response quality but whether models correctly identify the underlying request type—this isolates the pragmatic inference gap from general conversational ability.