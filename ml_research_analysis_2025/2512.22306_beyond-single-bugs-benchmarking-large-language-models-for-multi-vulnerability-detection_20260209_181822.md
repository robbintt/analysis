---
ver: rpa2
title: 'Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability
  Detection'
arxiv_id: '2512.22306'
source_url: https://arxiv.org/abs/2512.22306
tags:
- vulnerability
- code
- recall
- vulnerabilities
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for evaluating large language
  models (LLMs) on multi-vulnerability detection in source code. Unlike existing datasets
  that focus on single vulnerabilities or binary classification, this benchmark generates
  controlled, file-level samples with 1 to 9 injected vulnerabilities across C, C++,
  Python, and JavaScript.
---

# Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection

## Quick Facts
- arXiv ID: 2512.22306
- Source URL: https://arxiv.org/abs/2512.22306
- Reference count: 6
- Primary result: Multi-vulnerability detection performance degrades sharply with vulnerability density; Llama-3.3-70B drops from ~0.97 F1 on single-vulnerability files to ~0.63 on nine-vulnerability files, with recall falling below 0.30 for high-density Python files.

## Executive Summary
This paper introduces a benchmark for evaluating large language models on multi-vulnerability detection in source code, addressing the gap between single-vulnerability datasets and real-world security audits requiring exhaustive vulnerability identification. The benchmark generates controlled, file-level samples with 1 to 9 injected vulnerabilities across C, C++, Python, and JavaScript, using a hybrid injection pipeline with an oracle model for realistic placement. Evaluation of five LLMs shows severe performance degradation as vulnerability density increases, with count bias and selection bias limiting models' ability to perform comprehensive security audits.

## Method Summary
The benchmark uses CodeParrot GitHub corpus filtered for 7.5k-10k token files in four languages, creating 1,000 files per language. A Qwen2.5-32B-Instruct oracle maps feasible CWEs per file, then a hybrid injection pipeline introduces 1/3/5/9 vulnerabilities while preserving functionality. Five LLMs (Llama-3.3-70B, Qwen2.5-32B/72B, Mistral-3.2-24B, GPT-4o-mini) are evaluated using zero-shot prompts requiring JSON array output of CWE IDs. Metrics include Precision, Recall, F1, MAE (count bias), and ExactFile (perfect match rate).

## Key Results
- Performance degrades sharply with vulnerability density: Llama-3.3-70B drops from ~0.97 F1 on single-vulnerability C files to ~0.63 on nine-vulnerability files
- Count bias causes models to under-report vulnerabilities as density increases, finding only 3-4 of 9 vulnerabilities rather than maintaining ~90% recall
- Recall falls below 0.30 for high-density Python files, with anomalous recovery from 51.1% to 59.4% as density increases from 5 to 9 vulnerabilities
- C and JavaScript are generally "easier" for models to audit than C++ and Python, with linear degradation in C/JavaScript versus more complex patterns in C++/Python

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing vulnerability density causes systematic recall degradation due to count bias—models under-report rather than over-report as task complexity grows
- Mechanism: LLMs exhibit early stopping behavior after identifying initial vulnerabilities, consistent with "count bias" observed in multi-label tasks. As N increases, models retrieve fewer items proportionally (e.g., finding 3-4 of 9 vulnerabilities rather than maintaining ~90% recall)
- Core assumption: Count bias in vulnerability detection shares root causes with general multi-label selection failures in LLMs
- Evidence anchors:
  - [abstract] "Count bias and selection bias significantly limit models' ability to perform comprehensive security audits"
  - [section 5.3] "For N=9, models typically retrieve only 3–4 vulnerabilities (Recall≈0.3–0.4)... Ideally, recall should remain constant"
  - [corpus] Related work on instruction-tuning biases (Itzhak et al., 2024) supports count bias as a general LLM limitation, though not specifically validated for security tasks
- Break condition: If models were to maintain stable recall across densities (finding ~90% of 1 vuln ≈ ~90% of 9 vulns), count bias would not be the dominant failure mode

### Mechanism 2
- Claim: Long-context positional degradation compounds multi-vulnerability detection failure—"Lost in the Middle" effect reduces retrieval of vulnerabilities located away from file boundaries
- Mechanism: The 7.5k-10k token context range stresses long-range attention without truncation. Vulnerabilities positioned mid-file receive weaker attention weights, leading to systematic misses
- Core assumption: Vulnerability locations are distributed across file positions; positional bias affects security-critical regions similarly to general retrieval tasks
- Evidence anchors:
  - [section 3] "LLMs experience substantial retrieval degradation well before their maximum context limits due to positional bias and the 'Lost in the Middle' effect"
  - [section 2] References Liu et al. (2023) for "Lost in the Middle" phenomenon
  - [corpus] No direct corpus validation for positional bias specifically in vulnerability detection; related work focuses on general retrieval degradation
- Break condition: If recall remained uniform across file positions (beginning, middle, end), positional bias would not explain density-dependent degradation

### Mechanism 3
- Claim: Language-specific performance patterns reflect differences in training data distribution and syntactic complexity—C/JavaScript vulnerabilities align better with learned patterns than C++/Python
- Mechanism: C and JavaScript show linear degradation with explicit flaw patterns. Python exhibits anomalous recall recovery at higher densities (51.1%→59.4%), suggesting different vulnerability thresholds or association networks. C++ complexity increases cognitive load on context windows
- Core assumption: Training corpora contain disproportionate representations of certain vulnerability patterns per language
- Evidence anchors:
  - [section 5.1] "C and JavaScript are generally 'easier' for models to audit than C++ and Python"
  - [section 5.2] "Python performance was unique... improved to 59.4% Recall on 9-vulnerability files"
  - [corpus] LLM-GUARD paper (arXiv:2508.16419) discusses C++ and Python vulnerability detection but does not directly validate training distribution hypothesis
- Break condition: If performance patterns were uniform across all languages given equal vulnerability types, language-specific factors would be negligible

## Foundational Learning

- Concept: **Count Bias in Multi-Label Tasks**
  - Why needed here: The paper's core finding is that models systematically under-predict the number of vulnerabilities, making this bias critical to understand before interpreting results
  - Quick check question: If a model finds 2 vulnerabilities in a file containing 5, does this indicate poor per-item detection or count-based stopping behavior?

- Concept: **CWE (Common Weakness Enumeration) Taxonomy**
  - Why needed here: The benchmark uses MITRE 2024 Top-25 CWEs as the label space; understanding CWE categories is necessary to interpret model predictions and failure modes
  - Quick check question: CWE-78 and CWE-79 refer to which distinct vulnerability types, and would you expect a model to conflate them?

- Concept: **Exact Match vs. Aggregate Metrics for Exhaustive Tasks**
  - Why needed here: The paper introduces "ExactFile" metric—fraction of files with perfect prediction—which captures audit reliability differently from F1
  - Quick check question: A model with 80% recall on 9-vuln files might have what ExactFile score, and why does this matter for security audits?

## Architecture Onboarding

- Component map: CodeParrot corpus -> Oracle feasibility mapping -> Vulnerability injector -> LLM evaluation -> Metrics layer
- Critical path: Clean file → Oracle feasibility mapping → Select target CWEs by density condition → Injection produces ground-truth-labeled file → Zero-shot prompt to evaluated model → Compare predicted CWE set against ground truth using all metrics
- Design tradeoffs:
  - **Synthetic vs. natural vulnerabilities**: Controlled injection enables precise density manipulation but may introduce artifacts (style inconsistencies); real-world vulnerabilities lack ground-truth control
  - **File-level vs. function-level**: 7.5k-10k tokens stress long-context but may mix multiple logical units; function-level loses multi-vulnerability co-occurrence patterns
  - **Oracle dependency**: Using Qwen2.5-32B as feasibility mapper may bias toward vulnerabilities it "understands"; alternative would require human annotation
- Failure signatures:
  - **Severe recall drop with density**: Model stops after 2-3 findings regardless of ground truth (count bias confirmed)
  - **Near-perfect precision with collapsing recall**: Conservative prediction strategy—models avoid false positives at cost of missing real issues
  - **Language-specific inversions**: Python recall improving with density suggests over-aggressive threshold calibration, not genuine improvement
- First 3 experiments:
  1. **Baseline density sweep**: Run single model (Llama-3.3-70B) across all four density levels (1/3/5/9) for one language; plot recall and ExactFile degradation curves to establish your own baseline
  2. **Positional analysis**: For files with 5 vulnerabilities, correlate recall with estimated line positions of each vulnerability to test "Lost in the Middle" hypothesis directly
  3. **Prompt ablation**: Compare "identify ALL vulnerabilities" vs. neutral framing to quantify instruction-following contribution to count bias mitigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid systems combining static analysis tools with LLMs effectively mitigate "count bias" by utilizing structural cues to enforce exhaustive vulnerability searches?
- Basis in paper: [explicit] The Conclusion states, "future work should explore hybrid static analysis + LLM systems to mitigate count bias and ensure exhaustive search through structural cues rather than purely generative reasoning."
- Why unresolved: The study demonstrates that purely generative LLMs suffer severe recall degradation (under-reporting) as vulnerability density increases, but it does not test whether combining them with traditional deterministic tools fixes this specific failure mode
- What evidence would resolve it: A comparative evaluation showing that a hybrid pipeline maintains consistently high Recall and ExactFile scores on 9-vulnerability samples, unlike the standalone models tested

### Open Question 2
- Question: Does the sharp performance degradation observed in synthetic benchmarks hold for "wild" code containing naturally occurring vulnerabilities and logic flaws?
- Basis in paper: [explicit] The Conclusion suggests "extending the benchmark to include naturally occurring vulnerabilities, broader CWE categories, and logic-level flaws would improve ecological validity." Additionally, the Limitations section notes the ground truth relies on synthetic injection which may create artifacts
- Why unresolved: The current dataset injects vulnerabilities into clean code, potentially creating detectable artifacts (sudden style changes) or missing the complex interdependencies found in natural, legacy codebases
- What evidence would resolve it: A reproduction of the density vs. recall degradation curve using a dataset of real-world open-source files with historically confirmed, multi-vulnerability disclosures

### Open Question 3
- Question: To what extent can architectural advances like retrieval-augmented generation (RAG) or multi-pass "exhaustive audit" prompting recover the recall lost in high-density, long-context scenarios?
- Basis in paper: [explicit] The Conclusion proposes that "architectural advances, such as retrieval-augmented long-context reasoning... or specialized multi-pass 'exhaustive audit' prompting, may help LLMs better handle dense vulnerability clusters."
- Why unresolved: The paper identifies the "Lost in the Middle" phenomenon and context window limitations as causes for failure in 7.5k–10k token files, but it only evaluates standard zero-shot inference methods
- What evidence would resolve it: Benchmark results using the paper's dataset where the experimental group utilizes RAG or multi-step prompting, showing a statistically significant reduction in Mean Absolute Error (MAE) compared to the baseline zero-shot results

## Limitations
- Synthetic vulnerabilities may introduce artifacts and may not capture real-world attack patterns and complex interdependencies
- Oracle dependency on Qwen2.5-32B for feasibility mapping may introduce systematic biases that don't generalize to other models
- Anomalous Python performance (recall recovery from 51.1% to 59.4% as density increases) remains unexplained and could indicate dataset artifacts
- Exact prompt templates for oracle mapping, injection, and evaluation phases are not provided, making exact reproduction challenging

## Confidence
- **High Confidence**: Core finding of severe recall degradation with increasing vulnerability density is well-supported by consistent results across all five evaluated models and four programming languages
- **Medium Confidence**: Language-specific performance differences explanation (training data distribution hypothesis) is plausible but not directly validated; Python density inversion requires additional investigation
- **Low Confidence**: "Lost in the Middle" positional bias mechanism is referenced but not empirically validated within the vulnerability detection context

## Next Checks
1. **Positional Vulnerability Analysis**: For 5-vulnerability files, plot recall rates against estimated line positions of each vulnerability to directly test whether mid-file vulnerabilities are systematically missed
2. **Oracle Independence Test**: Run the same evaluation pipeline using a different LLM (e.g., GPT-4o-mini) as the feasibility mapper to assess whether performance differences are driven by oracle selection
3. **Natural Vulnerability Comparison**: Collect a small set of real-world vulnerable files with verified CVEs and evaluate the same models to compare synthetic vs. natural vulnerability detection performance