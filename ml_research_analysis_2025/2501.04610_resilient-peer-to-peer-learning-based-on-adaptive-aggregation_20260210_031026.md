---
ver: rpa2
title: Resilient Peer-to-peer Learning based on Adaptive Aggregation
arxiv_id: '2501.04610'
source_url: https://arxiv.org/abs/2501.04610
tags:
- learning
- workers
- data
- aggregation
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a resilient peer-to-peer learning algorithm
  based on adaptive aggregation that tolerates an arbitrary number of adversarial
  neighbors while accommodating non-convex loss functions and non-iid data distributions.
  The core idea is to assign aggregation weights to neighbor models based on their
  loss computed using the worker's own data, with lower loss indicating higher similarity
  and thus larger weights.
---

# Resilient Peer-to-peer Learning based on Adaptive Aggregation

## Quick Facts
- arXiv ID: 2501.04610
- Source URL: https://arxiv.org/abs/2501.04610
- Reference count: 29
- Primary result: Proposes adaptive aggregation for P2P learning that tolerates arbitrary adversarial neighbors while accommodating non-convex loss functions and non-IID data distributions

## Executive Summary
This paper introduces a resilient peer-to-peer learning algorithm that uses adaptive aggregation weights to filter adversarial influences during decentralized model training. The method assigns aggregation weights to neighbor models based on their loss computed using the worker's own data, with lower loss indicating higher similarity and thus larger weights. Theoretical analysis demonstrates convergence under local strong convexity assumptions, and empirical evaluations across three machine learning tasks show improved accuracy compared to existing resilient methods.

## Method Summary
The proposed approach operates in a peer-to-peer setting where each worker maintains a local model and exchanges parameters with graph neighbors. At each iteration, workers compute losses for received neighbor models on their private data, identify neighbors with performance better than or equal to their own (N+ set), and compute inverse-loss weights for aggregation. The final model update combines neighbor parameters using these adaptive weights, promoting collaboration with similar peers while filtering out adversarial influences without requiring attack-specific knowledge.

## Key Results
- Adaptive aggregation outperforms trimmed mean, coordinate-wise mean, Krum, average, and medoid techniques across three ML tasks
- Method tolerates arbitrary numbers of adversarial neighbors while accommodating non-convex loss functions
- Theoretical convergence guarantee shows bounded optimality gap under local strong convexity assumptions
- Effective against multiple attack models including sign-flipping, arbitrary Byzantine, fall-of-empire, and a-little-is-enough

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Assigning aggregation weights inversely proportional to neighbor loss values filters adversarial influence without requiring attack-specific knowledge
- Mechanism: Each worker computes the loss of each neighbor's model on its own private data. The aggregation weight for neighbor l is c(l,k) = r_k(w_l)^(-1) / Σ r_k(w_p)^(-1). Higher weights go to neighbors whose models produce smaller loss on the worker's local data, indicating aligned optimization objectives
- Core assumption: Smaller computed loss correlates with objective alignment between workers; adversarial models will produce higher loss on honest workers' data
- Evidence anchors:
  - [abstract] "The aggregation weights are determined through an optimization procedure, and use the loss function computed using the neighbor's models and individual private data."
  - [Page 3, Section 3] "A smaller loss indicates that their objectives are related, so a higher aggregation weight is assigned."
  - [corpus] FedGreed paper uses similar loss-based aggregation but for federated (centralized) setting; no direct corpus validation for P2P-specific formulation
- Break condition: If adversarial models are carefully crafted to achieve low loss on target workers' data distributions (gradient-matching or targeted attacks), the filtering may fail

### Mechanism 2
- Claim: Restricting aggregation to "better-performing" neighbors (N+ set) provides an additional safety filter beyond inverse-loss weighting alone
- Mechanism: Worker k only aggregates from neighbors l where r_k(w_l) ≤ r_k(w_k)—neighbors whose models perform at least as well as the worker's own model on its local data. This eliminates obviously worse models before weighting
- Core assumption: Honest neighbors generally make progress toward models that generalize; adversarial perturbations typically degrade performance on honest data
- Evidence anchors:
  - [Page 5, Equation 6] Definition of N+_k as neighbors with r_k(w_l) ≤ r_k(w_k)
  - [Page 5] "each worker aggregates the parameters of those neighbors that have a smaller risk than its own."
  - [corpus] No direct corpus comparison for this specific N+ filtering mechanism
- Break condition: If data distributions are highly non-IID such that a well-meaning neighbor's model genuinely performs worse on worker k's data, honest collaboration is incorrectly blocked

### Mechanism 3
- Claim: Convergence to a bounded optimality gap is guaranteed under local strong convexity assumptions even with arbitrary numbers of adversarial neighbors
- Mechanism: Theoretical analysis (Theorem 1) shows E[r_k(w_t) - r_k(w*)] ≤ μ_kLM / (2m) for appropriate step sizes, where the gap depends on learning rate, Lipschitz constant, gradient variance bound, and local convexity parameter
- Core assumption: Risk functions are locally m-strongly convex near stationary points (Assumption 1); gradients are Lipschitz continuous (Assumption 2); initialization is within a ball around a stationary point
- Evidence anchors:
  - [Page 6, Theorem 1] Formal convergence statement with bounded optimality gap
  - [Page 4, Assumption 1] "the statistical risk is locally m-strongly convex in a sufficiently large neighborhood of w*_s."
  - [corpus] Nesterov-Accelerated Robust FL paper also proves Byzantine-resilient convergence but uses different aggregation rules
- Break condition: If initialization is far from any stationary point, or if local strong convexity does not hold for the specific loss landscape, convergence guarantees break down

## Foundational Learning

- Concept: **Peer-to-peer (decentralized) learning vs. federated learning**
  - Why needed here: This paper targets P2P architectures where no central server coordinates aggregation. Understanding the distinction clarifies why each node must independently compute aggregation weights
  - Quick check question: Can you explain why removing the central server changes where aggregation logic must reside?

- Concept: **Non-convex optimization with local strong convexity**
  - Why needed here: Deep neural networks have non-convex loss landscapes. The paper's convergence proof relies on these landscapes being locally convex near optima—a standard but non-trivial assumption
  - Quick check question: Why does global non-convexity make convergence analysis harder, and how does local strong convexity help?

- Concept: **Byzantine fault tolerance in distributed systems**
  - Why needed here: Adversarial workers can send arbitrary malicious values. Byzantine-resilient algorithms must function correctly without knowing which participants are faulty
  - Quick check question: What is the key difference between crash failures and Byzantine failures in the context of aggregation?

## Architecture Onboarding

- Component map: Local SGD module -> Communication layer -> Loss computation module -> Aggregation weight calculator -> Model aggregator

- Critical path:
  1. Local SGD step produces intermediate parameters ŵ_t
  2. Broadcast ŵ_t to all neighbors
  3. For each received model, compute loss on local batch
  4. Identify N+ set (neighbors with loss ≤ self-loss)
  5. Compute inverse-loss weights for N+ members
  6. Aggregate to get final w_t
  Complexity: O(d · |N_k|) per aggregation step

- Design tradeoffs:
  - **Batch size for loss computation**: Larger batches improve loss estimate accuracy but increase compute; smaller batches are faster but noisier
  - **Communication frequency**: More frequent exchange speeds convergence but increases bandwidth
  - **Neighborhood size**: Larger neighborhoods provide more redundancy against adversaries but increase aggregation overhead

- Failure signatures:
  - **Divergence or oscillating loss**: May indicate initialization outside valid region or step size too large
  - **Gradual accuracy decay**: Possible time-coupled attack (ALIE, FoE) if N+ filter is admitting subtly perturbed models
  - **Complete collapse**: Likely >50% adversarial neighbors or catastrophic initialization

- First 3 experiments:
  1. **Baseline sanity check**: Run on IID data with no adversaries using standard averaging aggregation; compare against adaptive method to measure overhead cost
  2. **Single attack type, varied adversarial fraction**: Test sign-flipping attack with 10%, 20%, 30% adversarial workers on MNIST with non-IID label split; plot accuracy vs. adversary fraction
  3. **Non-IID severity sweep**: Hold adversarial fraction constant (e.g., 20%) and vary label imbalance across workers to identify where N+ filtering begins rejecting honest neighbors

## Open Questions the Paper Calls Out
None

## Limitations
- Assumption that lower loss values reliably indicate model alignment may fail against gradient-matching attacks
- Local strong convexity assumption may not fully capture deep learning landscapes
- N+ filtering could incorrectly exclude honest collaborators in highly non-IID settings

## Confidence
**High Confidence** in filtering obvious Byzantine failures (sign-flipping, arbitrary value injection) where malicious models clearly perform worse than honest ones on local data.

**Medium Confidence** in convergence guarantees under local strong convexity assumptions, though initialization requirements are restrictive in practice.

**Low Confidence** in robustness against sophisticated attacks like ALIE or FoE where adversarial models may achieve low loss on target workers' data through subtle perturbations.

## Next Checks
1. **Gradient-matching attack evaluation**: Test whether adversaries can craft models with artificially low loss on target workers' data to bypass the adaptive filtering mechanism.

2. **Initialization sensitivity analysis**: Systematically vary initialization points relative to stationary regions to validate the practical applicability of convergence bounds.

3. **Non-IID data distribution stress test**: Vary label distribution skew across workers to identify the point where N+ filtering begins incorrectly excluding honest neighbors, and measure the impact on final accuracy.