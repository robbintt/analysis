---
ver: rpa2
title: 'Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large
  Language Models'
arxiv_id: '2512.15885'
source_url: https://arxiv.org/abs/2512.15885
tags:
- visual
- jarvis
- llav
- target
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitations of multimodal large language
  models (MLLMs) in visual perception tasks, where these models often over-rely on
  language priors and miss fine-grained visual details. The core method, JARVIS, integrates
  a self-supervised visual learning objective inspired by I-JEPA into the standard
  vision-language alignment pipeline.
---

# Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2512.15885
- **Source URL**: https://arxiv.org/abs/2512.15885
- **Reference count**: 40
- **Primary result**: JARVIS improves visual perception in MLLMs by 1.8 points overall, with +6.2 points on CVBench3D

## Executive Summary
This work addresses the limitations of multimodal large language models (MLLMs) in visual perception tasks, where these models often over-rely on language priors and miss fine-grained visual details. The core method, JARVIS, integrates a self-supervised visual learning objective inspired by I-JEPA into the standard vision-language alignment pipeline. By using frozen vision foundation models as context and target encoders, JARVIS trains the LLM to predict masked portions of image embeddings, enabling it to learn structural and semantic regularities from images beyond textual descriptions. Experiments show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, with notable gains on tasks like counting, spatial reasoning, and depth estimation, without degrading multimodal reasoning abilities.

## Method Summary
JARVIS modifies the LLaVA two-stage training pipeline by adding a self-supervised visual learning objective during the alignment stage. The method uses frozen vision encoders (CLIP or SigLIP2) as context encoders and DINOv2 as a target encoder. During training, contiguous blocks of visual embeddings are masked, and the LLM predicts these masked regions using only context embeddings and positional latent tokens. This creates a purely visual supervision signal, as the target encoder sees no caption. The method computes the JEPA loss at approximately 1/4 depth of the LLM (layer j=8 for Vicuna-7B) and uses a cosine distance function with a two-layer MLP projector. After alignment, the target encoder and projector are discarded, and the model proceeds to standard instruction tuning.

## Key Results
- JARVIS achieves +1.8 point average improvement on vision tasks across 16 Cambrian benchmark tasks
- Notable +6.2 point improvement on CVBench3D for counting and depth estimation
- DINOv2 target encoder consistently outperforms CLIP and SigLIP2 on vision-centric tasks
- The 1/4-depth layer selection (j=8 for Vicuna-7B) provides optimal visual perception gains

## Why This Works (Mechanism)

### Mechanism 1: Masked Predictive Learning Creates Language-Independent Visual Representations
- Claim: Training the LLM to predict masked image embeddings forces it to learn structural and semantic regularities that textual captions cannot convey.
- Mechanism: During alignment, contiguous blocks of visual embeddings are masked. The LLM receives only context embeddings and positional latent tokens, then predicts target embeddings produced by a frozen target encoder. This creates a purely visual supervision signal, as the target encoder sees no caption.
- Core assumption: The missing visual information in captions (object counts, precise spatial relations, depth cues) can be recovered through predictive learning on embedding structures.
- Evidence anchors:
  - [abstract] "learn structural and semantic regularities from images without relying exclusively on language supervision"
  - [section 3.2] "the supervision for these target embeddings is provided by an additional target visual encoder, which only has access to the image, so that there is no language conditioning from its caption"
  - [corpus] Related work on visual hallucinations (ReLoop, "Seeing Right but Saying Wrong") confirms MLLMs over-rely on language priors, but does not directly validate masked prediction as a solution.
- Break condition: If the target encoder itself encodes language-biased features, or if the masked regions contain information fundamentally unpredictable from context, the mechanism degrades to noise fitting.

### Mechanism 2: Early-to-Middle Layer Prediction Preserves Visual Processing
- Claim: Computing the JEPA loss at approximately 1/4 depth of the LLM (rather than final layers) yields optimal visual perception gains while preserving language capabilities.
- Mechanism: Visual tokens receive concentrated attention in early-to-middle transformer layers. By computing the prediction loss at layer j=8 (for 32-layer Vicuna), the model learns visual patterns where they are actively processed, rather than at final layers optimized for syntactic text generation.
- Core assumption: The optimal prediction layer generalizes across LLM families; the 1/4-depth heuristic may not hold for all architectures.
- Evidence anchors:
  - [section 4.1] "Among the intermediate layers, we found the one located at one fourth of the depth (i.e., j=8 for Vicuna-7B) to deliver the best visual performance"
  - [section 4.1] "Vision-Centric accuracy increases by +1.0 point when descending from layer 31 to layer 24... against the final layer"
  - [corpus] "Seeing Right but Saying Wrong" corroborates that deeper layers may attend correctly but final predictions are misled—suggesting layer selection matters.
- Break condition: If the LLM architecture distributes visual processing differently, or if vision-language integration occurs primarily at other depths, the 1/4-depth rule fails.

### Mechanism 3: Self-Supervised Target Encoders Provide Dense Visual Supervision
- Claim: Using DINOv2 (self-supervised) as the target encoder outperforms language-supervised encoders (CLIP, SigLIP2) for generating prediction targets.
- Mechanism: Self-supervised vision models like DINOv2 learn dense spatial features without language bias. When the LLM predicts these embeddings, it aligns with visual structure rather than language-anchored semantics, improving tasks requiring precise spatial reasoning.
- Core assumption: The target encoder's representation quality directly transfers to MLLM visual perception; the benefit is not simply from ensemble effects.
- Evidence anchors:
  - [section 4.3] "DINOv2 consistently achieves the best performance across all datasets, testifying that dense features from language-supervised encoders are inferior"
  - [section 4.3] Results show DINOv2-L achieves 50.0 average on Vision-Centric vs. 48.1 with CLIP as target encoder
  - [corpus] No direct corpus validation for this specific encoder comparison.
- Break condition: If the downstream task requires semantic knowledge encoded in language-supervised models, or if self-supervised features lack task-relevant attributes, performance may degrade.

## Foundational Learning

- Concept: **Joint Embedding Predictive Architecture (JEPA)**
  - Why needed here: JARVIS adapts I-JEPA's core idea—predicting target representations from context—into MLLM training. Without understanding JEPA's predictive coding foundation, the loss function design appears arbitrary.
  - Quick check question: Can you explain why JEPA predicts in embedding space rather than pixel space, and what advantages this provides?

- Concept: **LLaVA Two-Stage Training Pipeline**
  - Why needed here: JARVIS modifies only the alignment stage. Understanding the distinction between frozen alignment (Stage 1) and unfrozen instruction tuning (Stage 2) is essential for correctly injecting the JEPA objective.
  - Quick check question: What is trained in each LLaVA stage, and why does JARVIS only modify Stage 1?

- Concept: **Attention Mask Engineering**
  - Why needed here: JARVIS requires custom attention masks so context tokens cannot see targets, targets cannot see each other, but text tokens can see both. Implementing this incorrectly causes information leakage or training instability.
  - Quick check question: In the JARVIS attention mask, why must target blocks be isolated from each other, and what happens if they are not?

## Architecture Onboarding

- Component map:
  Context Visual Encoder (F_ctx) -> Projector (proj) -> LLM (G) -> Target Projector (proj_tgt) -> Target Visual Encoder (F_tgt)

- Critical path:
  1. Sample context block M_ctx (85-100% of patches) and k=4 target blocks M_tgt
  2. Extract embeddings from both encoders; mask target positions from context
  3. Construct input sequence [I_ctx, z_tgt] with positional latents for masked positions
  4. Forward through LLM with custom attention mask
  5. Extract activations at layer j; compute cosine distance to target embeddings
  6. With probability λ=0.2, skip masking and use full image for standard NTP
  7. After alignment, discard F_tgt and proj_tgt; proceed to instruction tuning unchanged

- Design tradeoffs:
  - **λ (JEPA dropout rate)**: λ=0.2 balances masked prediction with whole-image alignment; λ=0 degrades image-text understanding, λ=0.5 slightly helps some tasks but reduces overall consistency
  - **Target encoder choice**: DINOv2 > CLIP/SigLIP2 for vision-centric tasks, but requires additional forward pass
  - **Distance function**: Cosine distance outperforms smooth L1 (from original I-JEPA); matching magnitude is harder and less beneficial
  - **MLP vs linear projector**: Linear projection fails to align spaces adequately; MLP required

- Failure signatures:
  - Using final layer for L_JEPA: Vision-Centric performance drops below baseline (Table 1, j=32 vs j=8)
  - Linear target projector: Average Vision-Centric drops from 51.7 to 49.6
  - Allowing target blocks to attend to each other: Drops to 50.5 average (attn_tgt->tgt)
  - Always applying masking (λ=0): General and Knowledge tasks degrade

- First 3 experiments:
  1. **Layer sweep ablation**: With Vicuna-7B, compute L_JEPA at layers 4, 8, 16, 24, 31, 32. Confirm j=8 (1/4 depth) yields best Vision-Centric scores. This validates the intermediate-layer hypothesis before scaling.
  2. **Target encoder comparison**: Using Qwen2-7B + SigLIP2 context, compare CLIP, SigLIP2, DINOv2-B/L/G as target encoders on CVBench2D, CVBench3D, MMVP. Confirm DINOv2-L provides best tradeoff.
  3. **Masking strategy validation**: Compare overlapping vs. non-overlapping target blocks. Overlapping yields +1.6 average improvement, confirming I-JEPA's original design transfers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a principled theoretical criterion for selecting the optimal LLM layer for computing the visual prediction loss ($L_{JEPA}$) beyond the empirical "first quarter" heuristic?
- Basis in paper: [explicit] The Conclusion explicitly states: "we hope that future works will bring a principled and theoretical criterion for selecting the best layer for self-supervised visual learning."
- Why unresolved: The current selection relies on ablation studies (Table 1) showing intermediate layers work best, but lacks a formal understanding of why specific layers capture visual regularities better than others.
- What evidence would resolve it: A theoretical framework validated by consistent layer selection across diverse LLM architectures (e.g., decoder-only vs. encoder-decoder) without requiring empirical search.

### Open Question 2
- Question: Does the semantic gap between the language-aligned context encoder (CLIP) and the self-supervised target encoder (DINO) create optimization conflicts during alignment?
- Basis in paper: [inferred] The method aligns CLIP context inputs to DINO targets. The paper notes DINO provides "dense features" but does not analyze if mapping these distinct representation spaces introduces gradient conflicts or if a unified encoder would be superior.
- Why unresolved: It is unclear if the structural features of DINO are fully compatible with the text-aligned features of CLIP during the LLM's prediction phase.
- What evidence would resolve it: Ablation studies using a self-supervised encoder for both context and target, or analysis of gradient interference between the NTP and JEPA objectives.

### Open Question 3
- Question: Can a dynamic scheduling of the masking probability ($\lambda$) yield a better trade-off between visual perception and textual alignment than the fixed probability used in training?
- Basis in paper: [inferred] The authors introduce a fixed $\lambda=0.2$ to skip masking occasionally for alignment consistency. The ablation on $\lambda$ shows varying results, implying the optimal balance might shift as the model trains.
- Why unresolved: A static dropout rate may be suboptimal as the model's representation capabilities evolve; a curriculum approach could potentially mitigate the "weaker image-text alignment" issue mentioned in the method.
- What evidence would resolve it: Experiments comparing fixed $\lambda$ against a curriculum-based warm-up or decay strategy, showing improved stability or higher combined scores on Vision-Centric and General benchmarks.

## Limitations

- **Target encoder dependency**: JARVIS requires an additional frozen vision model (target encoder) with a forward pass during training, adding computational overhead
- **Architectural specificity**: The optimal prediction layer (1/4 depth) is determined empirically for Vicuna-7B and may not generalize to other architectures
- **Limited evaluation scope**: While JARVIS shows consistent improvements on vision-centric benchmarks, the impact on complex multimodal reasoning tasks is less clear

## Confidence

**High Confidence**: The core claim that integrating self-supervised visual learning (JEPA-style) into MLLM training improves visual perception capabilities is well-supported by consistent experimental results across multiple model families and benchmarks.

**Medium Confidence**: The specific claim that DINOv2 outperforms language-supervised encoders as the target encoder is supported by direct comparisons, but the corpus lacks independent validation of this specific encoder comparison.

**Medium Confidence**: The claim that the 1/4-depth heuristic for layer selection generalizes across LLM families is supported by limited evidence (Vicuna and Qwen2 experiments).

## Next Checks

1. **Layer selection generalization**: Systematically test the 1/4-depth layer selection hypothesis across a wider range of LLM architectures (different depths, attention patterns, and families) to determine if the heuristic holds or requires model-specific tuning.

2. **Target encoder scalability analysis**: Compare performance and computational efficiency using different target encoder scales (DINOv2-B, DINOv2-G) and investigate whether the self-supervised representation quality or computational overhead drives the performance differences.

3. **Complex reasoning task validation**: Evaluate JARVIS on multimodal reasoning benchmarks that require both visual perception and language understanding (e.g., ScienceQA, MMMU) to determine if the visual perception improvements translate to gains in integrated multimodal reasoning tasks.