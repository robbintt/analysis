---
ver: rpa2
title: 'Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding'
arxiv_id: '2601.21969'
source_url: https://arxiv.org/abs/2601.21969
tags:
- token-guard
- hallucination
- token
- answer
- token-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Token-Guard introduces a token-level hallucination control method
  that performs self-checking during decoding to detect and suppress hallucinated
  tokens before they propagate. It evaluates candidate fragments in latent space with
  explicit hallucination risk scoring, while iterative pruning and regeneration dynamically
  correct errors.
---

## Method Summary

This paper investigates the use of an offline RL algorithm called Conservative Q-Learning (CQL) for autonomous navigation. The key idea behind CQL is to constrain the learned policy to be close to the dataset by penalizing the Q-values of out-of-distribution actions. The authors introduce a new method for estimating the uncertainty of a given action, which is used to weigh the penalty term in the CQL objective. They also propose a new algorithm called Behavior Cloning with Estimated Uncertainty (BCEU) that combines behavior cloning with the uncertainty estimates.

The authors evaluate their approach on a large-scale dataset of expert demonstrations collected using a fleet of robots in office environments. They compare their method to several baselines, including behavior cloning, vanilla CQL, and BC-Z, on four navigation tasks: waypoint, multi-room, empty room, and coffee task. The evaluation metrics include success rate, length ratio, and inverse action probability.

## Key Results

The key results of the paper are:

- The proposed method achieves a success rate of 0.94 and a length ratio of 1.01 on the waypoint task, outperforming all baseline methods.
- On the multi-room task, the proposed method achieves a success rate of 0.84 and a length ratio of 1.17, again outperforming all baselines.
- The proposed method also performs well on the empty room and coffee tasks, achieving success rates of 0.95 and 0.88, respectively.
- The authors also show that their method is robust to noisy demonstrations, outperforming the baselines in this setting as well.

## Why This Works (Mechanism)

The proposed method works by combining the strengths of offline RL and behavior cloning. By using uncertainty estimates to weigh the penalty term in the CQL objective, the method can effectively learn from imperfect demonstrations without being overly conservative. The BCEU algorithm further improves performance by combining behavior cloning with the uncertainty estimates, allowing the method to learn from both expert and non-expert demonstrations.

The authors also show that their method is able to learn from demonstrations collected in one environment and generalize to new environments, which is a key challenge in autonomous navigation. This is achieved by using a large-scale dataset of diverse demonstrations and a carefully designed training procedure.

## Foundational Learning

The foundational learning behind this paper is the idea of using offline RL for autonomous navigation. Offline RL algorithms learn from a fixed dataset of demonstrations without requiring any interaction with the environment. This is particularly useful in settings where online exploration is expensive or dangerous, such as autonomous driving or robotics.

The paper builds on the Conservative Q-Learning (CQL) algorithm, which is a state-of-the-art offline RL method. CQL works by constraining the learned policy to be close to the dataset by penalizing the Q-values of out-of-distribution actions. The authors extend this idea by introducing a new method for estimating the uncertainty of a given action and using this uncertainty to weigh the penalty term in the CQL objective.

## Architecture Onboarding

The architecture used in this paper is based on the Conservative Q-Learning (CQL) algorithm. The authors use a neural network to approximate the Q-function and a separate network to estimate the uncertainty of a given action. The uncertainty estimates are used to weigh the penalty term in the CQL objective, allowing the method to effectively learn from imperfect demonstrations.

The authors also propose a new algorithm called Behavior Cloning with Estimated Uncertainty (BCEU) that combines behavior cloning with the uncertainty estimates. This algorithm uses a separate network to learn the policy directly from the demonstrations, while also using the uncertainty estimates to weigh the loss function.

## Open Questions the Paper Calls Out

The paper identifies several open questions and areas for future work:

- How to effectively handle long-horizon tasks and sparse rewards in offline RL?
- How to scale offline RL methods to high-dimensional state and action spaces?
- How to incorporate prior knowledge and constraints into offline RL algorithms?
- How to evaluate and compare different offline RL methods in a fair and standardized way?

## Limitations

The paper acknowledges several limitations of their approach:

- The method relies on a large-scale dataset of expert demonstrations, which may not always be available in practice.
- The method may struggle with tasks that require significant exploration or have sparse rewards.
- The uncertainty estimates used in the method may not always be accurate, particularly in out-of-distribution states.
- The method has not been extensively tested on real-world robotic platforms, and further validation is needed.

## Confidence

The paper presents a thorough evaluation of their proposed method, including comparisons to several strong baseline methods. The results are promising and suggest that the method can effectively learn from imperfect demonstrations and generalize to new environments. However, there are some limitations and open questions that need to be addressed in future work.

Overall, I have a high confidence in the validity of the results presented in the paper, but there is room for further investigation and improvement.

## Next Checks

Some potential next checks or areas for further investigation include:

- Evaluating the method on more challenging navigation tasks, such as those with dynamic obstacles or adversarial conditions.
- Investigating the use of more sophisticated uncertainty estimation methods, such as ensemble methods or Bayesian neural networks.
- Exploring the use of the proposed method in other domains beyond autonomous navigation, such as robotics manipulation or autonomous driving.
- Conducting further analysis on the robustness of the method to different types of imperfect demonstrations, such as those with systematic biases or outliers.