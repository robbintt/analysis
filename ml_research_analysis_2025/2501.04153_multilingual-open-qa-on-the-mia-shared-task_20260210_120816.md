---
ver: rpa2
title: Multilingual Open QA on the MIA Shared Task
arxiv_id: '2501.04153'
source_url: https://arxiv.org/abs/2501.04153
tags:
- question
- language
- passage
- retrieval
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving cross-lingual information
  retrieval for open-domain question answering in low-resource languages. The authors
  propose a re-ranking method that leverages a zero-shot multilingual question generation
  model to re-score retrieved passages.
---

# Multilingual Open QA on the MIA Shared Task

## Quick Facts
- arXiv ID: 2501.04153
- Source URL: https://arxiv.org/abs/2501.04153
- Reference count: 5
- One-line primary result: QG-based re-ranking using mBART improves Recall@5/15 by 4% for Korean and slightly for Japanese, but degrades performance on low-resource languages like Bengali and Finnish.

## Executive Summary
This paper addresses cross-lingual open-domain question answering by proposing a re-ranking method that leverages zero-shot multilingual question generation. The approach computes the conditional likelihood of a question given a retrieved passage using pre-trained models like mBART and mT5, enabling better cross-lingual retrieval without task-specific training data. Evaluated on the XOR-TYDI QA dataset with 7 languages, the method shows significant improvements for Korean and Japanese but inconsistent results for low-resource languages, highlighting the challenges of multilingual retrieval in data-scarce settings.

## Method Summary
The method introduces a question-generation-based passage re-ranking (QGPR) approach for cross-lingual open-domain QA. It computes P(question|passage) using pre-trained multilingual sequence-to-sequence models (mBART or mT5) in a zero-shot manner, scoring and re-ranking top-50 passages from a baseline mDPR retriever. The re-ranker forces cross-attention between query and passage tokens, improving semantic matching across languages. Additionally, machine-translation-based data augmentation is explored, translating English QA pairs to target languages, though results are inconsistent, especially for low-resource languages.

## Key Results
- mBART-based re-ranking improves Recall@5 and Recall@15 by 4% for Korean language compared to baseline.
- Slight improvement observed for Japanese language with the same approach.
- Performance degrades for low-resource languages Bengali and Finnish, indicating model dependency on training data coverage.
- Translation-based augmentation shows inconsistent gains, limited by translation quality and input length constraints.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Re-ranking passages by computing the conditional likelihood P(question|passage) using a pre-trained multilingual language model can improve cross-lingual retrieval quality without task-specific training data.
- **Mechanism:** The re-ranker computes the average log-likelihood of question tokens conditioned on a retrieved passage (Equation 4). Passages that yield higher question generation probability are ranked higher. This forces the model to account for every token in the question via token-level cross-attention.
- **Core assumption:** Pre-trained multilingual sequence-to-sequence models (mBART, mT5) encode sufficient cross-lingual semantic alignment to meaningfully score question-passage relevance across languages in a zero-shot setting.
- **Evidence anchors:**
  - [abstract] "The re-ranker re-scores retrieved passages with a zero-shot multilingual question generation model... to compute the probability of the input question in the target language conditioned on a retrieved passage, which can be possibly in a different language."
  - [section 4.1] "This approach incorporates rich cross-attention between the query and passage tokens while forcing the model to explain every token in the question."
  - [corpus] Related work on multilingual RAG (arxiv:2504.03616) supports that cross-lingual retrieval augmentation remains challenging; however, direct evidence for QG-based re-ranking specifically is limited in the corpus.
- **Break condition:** Performance degrades when the underlying multilingual model has poor generation quality for a given language pair (observed for Bengali and Finnish), or when language tags cannot control target language (mT5 without prompts).

### Mechanism 2
- **Claim:** mBART outperforms mT5 for cross-lingual QG-based re-ranking because it supports explicit language tags to control the target generation language.
- **Mechanism:** mBART is trained with a denoising objective for translation tasks and accepts language tags as input, enabling the model to generate questions in a specified target language even when the passage is in a different language. mT5 lacks native language tag control, leading it to generate questions in the passage language rather than the query language.
- **Core assumption:** The translation-style pre-training of mBART provides better cross-lingual semantic alignment than mT5's text-to-text objective for this specific zero-shot task.
- **Evidence anchors:**
  - [section 4.1, experiment 4] "A zero-shot mBART model is used as the question generation model. The main advantage of using mBART over mT5 is that the target language can be controlled by inputting a language tag to the model."
  - [section 6.1] "The QGPR experiments based on mT5 performs poorly when compared to the baseline... the model is more likely to generate the question in the language of the input passage."
  - [corpus] Weak direct evidence; corpus papers discuss multilingual LLM adaptation (arxiv:2510.14466) but do not specifically compare mBART vs. mT5 for re-ranking.
- **Break condition:** Even with language tags, performance depends on the pre-training corpus quality for each language; low-resource languages in mBART's training data may still yield poor generation quality.

### Mechanism 3
- **Claim:** Translation-based variants (translating passage to question language or vice versa) do not reliably improve re-ranking because translation errors propagate and disproportionately harm short question sequences.
- **Mechanism:** Two translation directions were tested: (1) translate question to passage language, (2) translate passage to question language. Passage-to-question translation performed better because longer passages can absorb token-level translation errors, whereas errors in short questions corrupt key retrieval terms.
- **Core assumption:** Off-the-shelf NMT (mBART-50) translation quality is insufficient to overcome the cross-lingual generation gap for this task.
- **Evidence anchors:**
  - [section 6.1] "We hypothesize that even a small token-level error in translation of question can be highly detrimental to the model's retrieval performance. Since the question length is significantly smaller than the multi-sentence passages... whereas the long passages give more room to accommodate translation errors."
  - [section 6.1] "None of these approaches improve the baseline results."
  - [corpus] arxiv:2507.22923 examines translation strategies in cross-lingual RAG, noting that translation placement and quality significantly impact performance—consistent with this finding.
- **Break condition:** If higher-quality NMT or few-shot adaptation were used, translation-based approaches might become viable; current results suggest the translation noise floor is too high.

## Foundational Learning

- **Concept: Cross-lingual Information Retrieval (CLIR)**
  - **Why needed here:** The entire task involves retrieving relevant passages across language boundaries (e.g., Korean query, English passage). Understanding CLIR fundamentals—sparse vs. dense retrieval, cross-lingual embedding alignment—is essential.
  - **Quick check question:** Given a Finnish query and an English document collection, what are two approaches to enable retrieval without translating the query?

- **Concept: Dense Passage Retrieval (DPR) and its multilingual extension (mDPR)**
  - **Why needed here:** The baseline retriever is mDPR, which encodes questions and passages separately into a shared embedding space. Understanding bi-encoder architectures and their limitations (e.g., lack of cross-attention) explains why re-ranking helps.
  - **Quick check question:** Why might a bi-encoder miss subtle semantic matches that a cross-attention model would catch?

- **Concept: Sequence-to-Sequence Models for Conditional Generation (T5, BART, mT5, mBART)**
  - **Why needed here:** The re-ranker uses seq2seq models to compute P(question|passage). Understanding teacher forcing, log-likelihood computation, and language token control is required to implement and debug this approach.
  - **Quick check question:** How would you compute the average token-level log-likelihood of a target sequence given a source sequence using a pre-trained seq2seq model?

## Architecture Onboarding

- **Component map:** Question → mDPR Retriever (top-50) → QGPR Re-ranker (mBART with language tag) → Top-k passages → mT5 Generator → Answer
- **Critical path:** Question → Retriever (top-50) → Re-ranker (mBART with language tag) → Top-k passages → mT5 Generator → Answer. The re-ranker is the novel contribution; upstream retriever and downstream generator are borrowed from the CORA baseline.
- **Design tradeoffs:**
  - **mBART vs. mT5:** mBART enables language control via tags but is heavier and trained for translation; mT5 is more flexible for text-to-text tasks but lacks native language control.
  - **Zero-shot vs. fine-tuned re-ranker:** Zero-shot avoids labeled data requirements (key for low-resource settings) but sacrifices potential gains from supervised adaptation.
  - **Translation direction:** Translating passages to question language is more robust than translating questions, but neither matches zero-shot mBART performance.
- **Failure signatures:**
  - **Language mismatch:** mT5 generates questions in passage language instead of query language—check output language when debugging.
  - **Low-resource degradation:** For languages with poor mBART support (Bengali, Finnish), re-ranking hurts recall—monitor per-language Recall@K deltas.
  - **Short context window:** Data augmentation experiments were limited by 600-token input constraint; positive passages may not fit—check truncation behavior.
- **First 3 experiments:**
  1. **Reproduce mBART re-ranking on XOR-TyDi QA:** Take mDPR top-50 outputs for Korean and Japanese; compute P(q|p) with mBART using language tags; compare Recall@5 and Recall@15 against mDPR baseline.
  2. **Ablate language tag control:** Run mBART re-ranking without language tags vs. with tags to quantify the impact of controlled target language generation.
  3. **Pilot few-shot QG fine-tuning for a low-resource language:** Select Bengali or Finnish; create 50–100 annotated (passage, question) pairs; fine-tune mBART with a contrastive or likelihood objective; evaluate whether re-ranking gap closes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a few-shot learning approach utilizing contrastive loss significantly improve the performance of the question-generation-based re-ranking (QGPR) method compared to the zero-shot baseline?
- Basis in paper: [explicit] The authors state in Section 4.1 and 5.1 regarding mBART: "Though we tried in a zero shot setting, we believe that a few shot method using contrastive loss can further boost the results."
- Why unresolved: The current work is strictly zero-shot; the proposed modification was hypothesized but not implemented or tested.
- What evidence would resolve it: Experimental results comparing the Recall@K and Mean Reciprocal Rank (MRR) of the few-shot contrastive model against the zero-shot mBART baseline on the XOR-TYDI dataset.

### Open Question 2
- Question: Does increasing the input context size for the reader model beyond the experimental limit of 600 tokens lead to consistent performance gains in the machine-translation-based data augmentation setting?
- Basis in paper: [explicit] In Section 6.2 and 7, the authors note: "We believe that increasing the input context size should help to improve performance on augmented dataset."
- Why unresolved: Computational resource restrictions forced a reduced input length (600 vs default 1000), limiting the number of passages the model could attend to and potentially masking the benefits of the augmented data.
- What evidence would resolve it: Ablation studies running the augmented dataset with varying input lengths (e.g., 600, 1000, 2000 tokens) to measure the delta in F1 scores, particularly for low-resource languages like Bengali and Korean.

### Open Question 3
- Question: Does segregating the dataset based on answer types (numerical vs. non-numerical) explain the inconsistent improvements observed during machine-translation-based data augmentation?
- Basis in paper: [explicit] Section 6.2 states: "Hence we believe that to further identify areas of improvement we might need to segregate the dataset in terms of whether the answer is numerical or non numerical."
- Why unresolved: The analysis showed that many correctly answered augmented examples contained English numbers, suggesting a potential bias or specific capability in handling numerical answers that was not isolated in the aggregate F1 scores.
- What evidence would resolve it: Stratified evaluation metrics reporting F1 scores separately for numerical and non-numerical answer spans across different languages.

## Limitations
- Inconsistent performance across languages, with degradation for low-resource languages like Bengali and Finnish.
- Lack of comparison to supervised re-ranking baselines to quantify the zero-shot trade-off.
- Translation-based augmentation shows inconsistent results, with no exploration of higher-quality translation strategies.
- Computational overhead of generative re-ranking may limit scalability.

## Confidence
- **High confidence:** The core mechanism of using conditional likelihood for re-ranking (Mechanism 1) is well-supported by experimental results showing consistent improvements for Korean and Japanese. The superiority of mBART over mT5 due to language tag control (Mechanism 2) is directly demonstrated.
- **Medium confidence:** The explanation for why translation-based approaches fail (Mechanism 3) is reasonable but somewhat speculative, lacking systematic analysis of translation quality.
- **Low confidence:** The claim that this approach is "suitable for low-resource languages" is contradicted by observed performance degradation on Bengali and Finnish.

## Next Checks
1. **Per-language generation quality analysis:** For each language in the XOR-TyDi dataset, measure mBART's PPL and BLEU scores when generating questions from passages in the same language versus cross-lingually to quantify generation quality gaps.
2. **Supervised re-ranking ablation:** Create a small labeled dataset (100-200 examples) for Korean or Japanese with relevance judgments, fine-tune mBART for re-ranking, and compare zero-shot vs. fine-tuned performance.
3. **Translation quality impact study:** Replace mBART-50 with a higher-quality NMT model (e.g., GPT-4 or fine-tuned mBART) for translation-based re-ranking variants to test whether improved translation quality closes the performance gap.