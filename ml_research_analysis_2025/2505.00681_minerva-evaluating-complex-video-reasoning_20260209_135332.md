---
ver: rpa2
title: 'MINERVA: Evaluating Complex Video Reasoning'
arxiv_id: '2505.00681'
source_url: https://arxiv.org/abs/2505.00681
tags:
- reasoning
- video
- answer
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MINERVA, a challenging video reasoning benchmark
  that includes 1,515 complex questions with detailed, hand-crafted reasoning traces.
  Unlike existing video datasets that only provide final answers, MINERVA includes
  intermediate reasoning steps for each question, making it possible to analyze model
  reasoning failures in depth.
---

# MINERVA: Evaluating Complex Video Reasoning

## Quick Facts
- arXiv ID: 2505.00681
- Source URL: https://arxiv.org/abs/2505.00681
- Reference count: 40
- Primary result: Introduces MINERVA benchmark with 1,515 complex video questions requiring detailed reasoning traces

## Executive Summary
MINERVA is a challenging video reasoning benchmark that addresses the limitation of existing video datasets by including intermediate reasoning steps for each question. Unlike previous benchmarks that only provide final answers, MINERVA includes hand-crafted reasoning traces with timestamps, enabling fine-grained analysis of model reasoning failures. The dataset covers diverse video domains and lengths, with questions requiring multiple reasoning skills. Benchmarking shows that even the best models achieve only 66.2% accuracy compared to human performance of 92.5%, with models primarily struggling with temporal localization and visual perception rather than logical reasoning.

## Method Summary
The MINERVA benchmark consists of 1,515 complex video questions with 5 answer choices each, accompanied by detailed hand-crafted reasoning traces containing timestamps and ~92 words per trace. Videos span 4 domains (short films, sports, cooking, board games, STEM, lifestyle) with lengths from <2 minutes to 100+ minutes. The benchmark uses three prompting strategies: direct answer, step-by-step reasoning, and reasoning with rubric guidance. Evaluation employs MiRA (LLM-as-judge) with reference-based and reference-free variants, scoring on 4 rubric axes: Perceptual Correctness, Temporal Localization, Logical Reasoning, and Completeness. Frame sampling uses uniform distribution with ASR interleaving at 5-second intervals.

## Key Results
- Human performance significantly exceeds current models: 92.5% vs. 66.2% (best model: Gemini 2.5 Pro Thinking)
- Temporal localization is the primary failure mode, with mean human scores of 0.440/1.0 compared to 0.625 for perceptual and 0.770 for logical reasoning
- Reference-based evaluation achieves substantially higher correlation with human judgment than reference-free approaches
- Rubric-guided prompting improves model performance with minimal extra compute

## Why This Works (Mechanism)

### Mechanism 1: Reference-Based Reasoning Trace Evaluation
Ground-truth reasoning traces enable fine-grained failure mode analysis that final-answer accuracy cannot provide. Human-annotated traces (mean 92 words, ~4 timestamps each) serve as references for scoring model outputs along four axes. Reference-based LLM evaluation achieves substantially higher correlation with human judgment on temporal (r=0.79) and perceptual (r=0.59) axes compared to reference-free evaluation.

### Mechanism 2: Rubric-Guided Prompting for Video Reasoning
Explicitly providing evaluation criteria to models improves both reasoning trace quality and final answer accuracy. The MINERVA rubric primes models to attend to temporal grounding, perceptual correctness, logical coherence, and completeness. Prompting with the rubric improved Gemini 2.0 Flash from 46.47% to 53.47% MCQ accuracy, with reasoning scores improving from 0.65 to 0.75 MiRA.

### Mechanism 3: Error Taxonomy Reveals Temporal Localization as Primary Bottleneck
The four-category taxonomy isolates failure sources, revealing that current frontier models fail primarily on video-specific skills (temporal localization, visual perception) rather than text-domain reasoning. Human evaluation found lowest scores for temporal grounding (mean 0.440/1.0), then perceptual correctness (0.625), with logical reasoning scoring highest (0.770).

## Foundational Learning

- Concept: **Temporal Localization in Video QA**
  - Why needed here: This is the primary failure mode identified across all models. Understanding how to identify and reference specific time ranges in video is essential for debugging model outputs.
  - Quick check question: Given a model's reasoning trace, can you identify whether timestamps correspond to correct video segments without watching the video?

- Concept: **Reference-Based vs Reference-Free Evaluation**
  - Why needed here: The paper demonstrates that reference-free LLM-as-judge has low correlation with humans on temporal/perceptual axes (r=0.56 and 0.45), while reference-based approaches achieve r=0.79 and 0.59.
  - Quick check question: When would reference-free evaluation be preferred despite lower correlation, and what are its failure modes?

- Concept: **Multi-Skill Question Composition**
  - Why needed here: MINERVA questions require 2+ skills from 13 types (temporal reasoning, counting, cause-effect, goal reasoning, counterfactual, etc.). Understanding skill interactions helps diagnose where compositional reasoning breaks.
  - Quick check question: For a question requiring both "counting" and "temporal reasoning," which skill would likely fail first based on the paper's findings?

## Architecture Onboarding

- Component map: Video selection (4 domains) → Manual annotation (questions, 5 answer choices, reasoning traces) → Peer review → Senior review → Adversarial filtering (text-only model consensus to remove ASR-solvable questions) → MiRA evaluator with rubric scoring

- Critical path: Frame sampling strategy (64→256→768 frames evaluated) with ASR interleaving → Reasoning prompt selection (direct vs. step-by-step vs. rubric-guided) → MiRA scoring for trace evaluation (requires ground-truth reference traces)

- Design tradeoffs:
  - Frame count vs. API limits: More frames improve accuracy but hit API constraints; saturation observed around 256 frames for Gemini 2.0 Flash
  - Manual vs. automated annotation: Fully manual ensures quality (mean 92-word traces) but limits scale (1,515 questions vs. semi-automatic datasets)
  - Reference-based vs. reference-free evaluation: Higher correlation with humans but requires expensive ground-truth annotations

- Failure signatures:
  - Models produce correct final answers with flawed reasoning: hallucinated timestamps, fabricated perceptual details, logical leaps
  - Temporal grounding scores cluster low across all models (0.10-0.75 normalized range) even for frontier models
  - Open-source models show reasoning traces that restate questions rather than ground in video evidence

- First 3 experiments:
  1. Baseline establishment: Run Qwen2.5-VL and Gemini 2.0 Flash on 50 MINERVA questions with 256 frames + ASR, comparing direct-answer vs. rubric-guided prompts
  2. Error taxonomy validation: Have human raters score 20 model outputs on all 4 rubric axes to confirm temporal is harder to evaluate consistently
  3. Ablation on reference traces: Run MiRA with and without ground-truth references on same model outputs to justify annotation investment

## Open Questions the Paper Calls Out

- **Open Question 1**: How can reference-based LLM evaluation be improved for assessing the logical reasoning axis in video reasoning traces, given that providing ground truth references decreased correlation with human judgments from 0.21 (reference-free) to 0.17 (reference-based)? The paper identifies this problem but proposes no solution for evaluating logical coherence when model reasoning diverges structurally from reference traces.

- **Open Question 2**: What architectural or training interventions specifically improve temporal localization in video reasoning models, given that temporal errors were the most prevalent failure mode across all evaluated models? The paper identifies temporal localization as the bottleneck but does not investigate whether this stems from frame sampling strategies, temporal attention mechanisms, or training data.

- **Open Question 3**: Can models be fine-tuned on MINERVA's reasoning traces to improve both reasoning quality and final answer accuracy on held-out video reasoning benchmarks? The paper provides detailed reasoning traces explicitly for future research but only evaluates models in zero-shot settings, without exploring whether the reasoning traces are effective training data.

## Limitations

- Dataset construction relies entirely on manual annotation, creating potential biases in question formulation and domain selection that may not generalize to naturally occurring video reasoning scenarios.
- MiRA evaluator uses Gemini 2.0 Pro as judge, creating circular dependency where a single model system evaluates itself and competitors, raising concerns about evaluation consistency.
- Temporal localization evaluation criteria are not explicitly defined, with 3-point Likert scale potentially having high inter-rater variance for ambiguous timestamp ranges.

## Confidence

**High Confidence**:
- MINERVA rubric improves model performance when included in prompts
- Reference-based evaluation achieves higher correlation with human judgment than reference-free approaches
- Human performance (92.5%) substantially exceeds current model performance (66.2% for best model)
- Temporal localization and visual perception are the primary failure modes across all tested models

**Medium Confidence**:
- Four-category error taxonomy comprehensively captures model failure modes
- Rubric-guided prompting represents a general approach applicable beyond MINERVA
- Performance gap reflects fundamental limitations rather than dataset-specific factors

**Low Confidence**:
- Specific weight of temporal localization as "primary bottleneck" versus other factors
- Generalizability of MINERVA's domain distribution to broader video understanding tasks
- Claim that reasoning traces enable deeper failure analysis than answer-only evaluation

## Next Checks

1. **Inter-rater Reliability Study**: Conduct formal inter-rater reliability analysis on temporal localization scoring. Have 3-5 independent annotators score the same 50 model outputs on temporal accuracy. Compute Cohen's kappa and identify specific temporal annotation scenarios with high variance.

2. **Cross-Model Evaluation**: Run the MiRA evaluator across 3 different LLM judges (e.g., GPT-4, Claude, Gemini) on the same 100 model outputs. Compare rubric scores and correlation with human judgment to quantify judge-specific biases.

3. **Natural Video Validation**: Collect a small sample of video questions from real-world scenarios (e.g., instructional videos, documentary clips) and evaluate whether MINERVA-trained models maintain performance advantages. Compare against models trained only on existing datasets like MVQA or How2R.