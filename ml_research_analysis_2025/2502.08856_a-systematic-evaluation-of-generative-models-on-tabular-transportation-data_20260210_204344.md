---
ver: rpa2
title: A Systematic Evaluation of Generative Models on Tabular Transportation Data
arxiv_id: '2502.08856'
source_url: https://arxiv.org/abs/2502.08856
tags:
- data
- synthetic
- transportation
- generative
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic evaluation of generative models
  for synthetic tabular transportation data. The authors use NYC taxi data as a case
  study and evaluate six representative models including CTGAN, TVAE, CTABGAN, TabDDPM,
  STaSy, and Gaussian Copula.
---

# A Systematic Evaluation of Generative Models on Tabular Transportation Data

## Quick Facts
- arXiv ID: 2502.08856
- Source URL: https://arxiv.org/abs/2502.08856
- Reference count: 33
- Primary result: TabDDPM achieves best overall performance on NYC taxi data, but struggles with high-cardinality categorical variables; novel graph similarity and rDCR metrics reveal model limitations

## Executive Summary
This paper presents the first systematic evaluation of six generative models on tabular transportation data using NYC taxi data as a case study. The authors evaluate CTGAN, TVAE, CTABGAN, TabDDPM, STaSy, and Gaussian Copula using standard metrics plus two novel contributions: a graph similarity metric to capture transportation network structure and an improved privacy metric (rDCR) for membership inference vulnerability detection. TabDDPM emerges as the best overall performer on utility metrics, while TVAE excels at preserving graph structure. The study reveals that while general-purpose tabular generative models can produce useful synthetic transportation data, they struggle to capture the unique network characteristics of trip flows between zones.

## Method Summary
The study evaluates six generative models on preprocessed NYC Green Taxi Trip Data (19.2M rows reduced to 40K training/20K test samples). Models are trained with 3 random seeds and 5 sampling iterations per model. Evaluation uses 6 metrics: downstream task R² for total amount prediction, Wasserstein distance, coverage, graph similarity (comparing edge distributions between real and synthetic trip networks), rDCR privacy metric at 5% quantile, and runtime complexity. The graph similarity metric computes total variation distance between zone-to-zone trip flow distributions, while rDCR measures training-to-synthetic vs holdout-to-synthetic distances at low percentiles to detect membership inference vulnerabilities.

## Key Results
- TabDDPM achieves highest utility scores (R² 94.58-94.69, coverage 68.69%, Wasserstein 0.44) but fails on graph similarity (11.56%) due to high-cardinality categorical variables
- TVAE performs best on graph similarity (33.21%) but shows moderate downstream performance (72.32%)
- All models show significant gaps vs. real data baseline on graph similarity (73.17%), revealing limitations in capturing network structure
- Gaussian Copula shows rDCR < 1 at small percentiles indicating privacy vulnerability despite acceptable absolute DCR
- High-cardinality categorical variables (265 zone classes) cause severe performance degradation across multiple models

## Why This Works (Mechanism)

### Mechanism 1: Diffusion-Based Tabular Generation (TabDDPM)
- Claim: Denoising diffusion models achieve the best overall performance on transportation tabular data among evaluated architectures.
- Mechanism: Gaussian diffusion corrupts continuous variables via forward noise addition (Eq. 1), while multinomial diffusion handles categorical variables (Eq. 4). A neural network learns the reverse denoising process by predicting added noise (Eq. 3), effectively reconstructing the data distribution.
- Core assumption: The gradual forward-reverse process captures multimodal and mixed-type tabular distributions more effectively than adversarial or variational objectives.
- Evidence anchors:
  - [abstract]: "TabDDPM achieves the best overall performance"
  - [Section 3.1]: Formal derivation of forward/reverse diffusion equations
  - [Table 1, Table 4]: TabDDPM achieves highest downstream R² (94.58-94.69) and coverage (68.69%)
  - [corpus]: Limited direct comparison in corpus; survey papers mention diffusion as emerging approach but lack transportation-specific evaluation
- Break condition: Categorical variables with hundreds of classes cause severe mode collapse (Table 3: TabDDPM graph similarity drops to 11.56%, discussed in Section 6).

### Mechanism 2: Graph-Based Transportation Network Evaluation
- Claim: Transportation-specific structure can be quantified by comparing edge distributions between real and synthetic trip networks.
- Mechanism: Represent zones as nodes and trips as edges. Compute edge distribution p_G(i,j) = n_ij/N for each node pair. Measure similarity via total variation distance: S_G = 1 - ½Σ|p_Gr - p_Gs| (Eq. 10).
- Core assumption: Aggregate trip flows between spatial zones encode domain-relevant structure that row-wise metrics miss.
- Evidence anchors:
  - [abstract]: "graph similarity metric to capture transportation network characteristics"
  - [Section 4.1]: Full mathematical formulation
  - [Table 3]: All models show large gap vs. baseline (73.17%); best TVAE achieves only 33.21%
  - [corpus]: Paper 70336 ("Comprehensive Evaluation Framework for Synthetic Trip Data") addresses transport-specific evaluation, supporting domain-tailored metrics; no corpus paper uses this exact graph approach
- Break condition: When synthetic data fails on high-cardinality zone variables, graph similarity exposes the failure dramatically.

### Mechanism 3: Distance-to-Closest-Record Ratio (rDCR) for Privacy
- Claim: Comparing training-to-synthetic vs. holdout-to-synthetic distances at low percentiles reveals membership inference vulnerabilities missed by absolute DCR.
- Mechanism: Compute rDCR = d_α(r,s) / d_α(h,s) at percentile α. Values < 1 indicate synthetic data is closer to training than holdout data, signaling overfitting and MIA vulnerability.
- Core assumption: Overfitted generative models produce synthetic samples unnaturally close to training records relative to unseen data.
- Evidence anchors:
  - [abstract]: "improved privacy metric based on distance to closest record ratio (rDCR)"
  - [Section 4.2]: Connection to membership inference attack theory
  - [Figure 1]: Gaussian Copula shows rDCR < 1 at small percentiles; other models remain robust (~1.0)
  - [corpus]: Paper 57407 ("Synth-MIA") directly addresses MIA auditing in tabular synthesis, validating this mechanism's relevance
- Break condition: Using absolute DCR alone (dcr_rs) incorrectly ranks Gaussian Copula as best for privacy (Section 5.4), hiding its vulnerability.

## Foundational Learning

- **Concept: Diffusion Models (Forward/Reverse Process)**
  - Why needed here: Understanding TabDDPM requires grasping how gradual corruption and learned denoising differ from GAN/VAE training.
  - Quick check question: Why does the forward process need a variance schedule β_t, and what does the reverse process neural network actually predict?

- **Concept: Membership Inference Attacks (MIA)**
  - Why needed here: The rDCR metric is explicitly designed to detect MIA vulnerability; understanding the threat model clarifies why ratio-based evaluation matters.
  - Quick check question: If a model overfits, why would training records be closer to synthetic samples than holdout records?

- **Concept: Total Variation Distance**
  - Why needed here: The graph similarity metric uses TV distance to compare edge distributions; interpretation requires understanding its properties.
  - Quick check question: Why does TV distance range from 0 to 1, and what does 0.5 signify?

## Architecture Onboarding

- Component map: NYC taxi data preprocessing (zone encoding for graph metric) -> 6 generative models training (3 seeds) -> 20K samples × 5 iterations per model -> 6-metric evaluation (R², Wasserstein, coverage, graph similarity, rDCR, runtime) -> Model selection based on use-case priorities

- Critical path: Data preprocessing (zone encoding for graph metric) -> Model training (3 seeds) -> Sampling (5 iterations × 20K samples) -> Multi-metric evaluation -> Model selection based on use-case priorities

- Design tradeoffs:
  - TabDDPM: Best utility (R² 94.58, coverage 68.69%, Wasserstein 0.44) but fails on high-cardinality categoricals (graph similarity 11.56%)
  - CTABGAN: Strong Wasserstein (0.43) but high time cost (~90 min) and poor coverage (2.96%)
  - Gaussian Copula: Fastest and good R² but privacy-vulnerable at low percentiles (rDCR < 1) and severe mode collapse (coverage 0.62%)
  - TVAE: Best graph similarity (33.21%) but moderate downstream performance (72.32%)

- Failure signatures:
  - Mode collapse: Coverage < 5% (CTABGAN 2.96%, STaSy 2.34%, Gaussian Copula 0.62%)
  - Categorical explosion: TabDDPM's graph similarity collapse when zone variable has 265 classes
  - Hidden privacy leakage: rDCR < 1 at small percentiles despite acceptable absolute DCR (Gaussian Copula)
  - Memory/convergence failure: STaSy returns N/A on graph metric due to OOM issues (Table 3)

- First 3 experiments:
  1. Baseline replication: Train all 6 models on 40K/20K split, compute all 6 metrics; expect TabDDPM to lead on utility metrics, TVAE on graph similarity, Gaussian Copula to show rDCR vulnerability at 5% quantile
  2. Categorical cardinality ablation: Reduce zone classes (e.g., aggregate to 50 zones); hypothesis: TabDDPM graph similarity should improve substantially
  3. Percentile sweep on rDCR: Evaluate at α ∈ {0.5%, 1%, 5%, 10%}; hypothesis: Gaussian Copula vulnerability window narrows as α increases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the severe mode collapse observed in TabDDPM when handling high-cardinality categorical variables stem primarily from architectural limitations in multinomial diffusion or from insufficient hyperparameter tuning?
- Basis in paper: [explicit] The authors state on page 11: "We speculate that this issue may arise from its difficulty in handling categorical variables with hundreds of classes... or it may require significant additional hyperparameter tuning."
- Why unresolved: The paper identifies the failure mode (low graph similarity scores for TabDDPM) but does not isolate the root cause between the model's intrinsic architecture and the experimental training configuration.
- What evidence would resolve it: An ablation study optimizing diffusion steps and embedding dimensions specifically for high-cardinality features, comparing the resulting graph similarity scores against the baseline.

### Open Question 2
- Question: How can generative models be modified to explicitly preserve the topological properties of transportation networks during synthesis?
- Basis in paper: [explicit] The abstract concludes: "This work underscores the potential need to develop generative models specifically tailored to take advantage of the unique characteristics of emerging domains, such as transportation."
- Why unresolved: The proposed graph similarity metric reveals that all evaluated models fail to capture the collective network structure, with similarity scores remaining significantly lower than the reference real-to-real baseline.
- What evidence would resolve it: The development of a "transportation-tailored" model that incorporates spatial dependencies or graph-based loss functions, demonstrating a substantial increase in Graph Similarity ($S_G$) compared to general-purpose tabular models.

### Open Question 3
- Question: Do the performance rankings of tabular generative models remain consistent when applied to transportation datasets with different modalities or spatial granularities?
- Basis in paper: [explicit] The conclusion states: "Extending the evaluation beyond the New York City taxi data is expected to offer more insights on the current tabular generative models."
- Why unresolved: The systematic evaluation is limited to a single case study (NYC Green Taxi data), leaving the generalizability of the finding (that diffusion models outperform GANs) uncertain across the broader transportation domain.
- What evidence would resolve it: Replicating the evaluation pipeline on diverse datasets (e.g., subway, ride-sharing, or freight data) to verify if TabDDPM retains its dominance in downstream utility and diversity metrics.

## Limitations

- Single dataset constraint: Findings based only on NYC taxi data may not generalize to other transportation contexts or tabular domains
- Evaluation assumptions: Graph similarity assumes zone-based trip networks are the most relevant structural feature; rDCR effectiveness depends on holdout sampling strategy
- High-cardinality categorical variable issue not thoroughly explored beyond observed performance degradation
- Privacy analysis limited to single membership inference attack framework
- Runtime complexity measured only for one hardware configuration without scaling analysis

## Confidence

- High confidence: TabDDPM achieving best overall utility performance (Wasserstein distance, coverage, downstream R²) - directly measured with multiple random seeds and iterations
- Medium confidence: Graph similarity metric's ability to capture transportation network characteristics - novel metric with strong theoretical grounding but limited external validation
- Medium confidence: rDCR privacy metric's effectiveness - theoretically sound but evaluated only against one attack type and dataset
- Low confidence: Generalizability to other transportation datasets and tabular domains - based on single case study

## Next Checks

1. Replicate the evaluation framework on a different transportation dataset (e.g., bike-sharing or ride-hailing data) to test generalizability of the graph similarity metric and model rankings
2. Conduct an ablation study on the high-cardinality categorical variable by progressively aggregating zone classes and measuring the impact on TabDDPM's graph similarity performance
3. Extend the privacy analysis to include multiple attack types (attribute inference, model inversion) and alternative holdout sampling strategies to validate rDCR's robustness