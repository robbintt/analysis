---
ver: rpa2
title: 'Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic
  Advances and Benchmarking'
arxiv_id: '2505.22922'
source_url: https://arxiv.org/abs/2505.22922
tags:
- memory
- arxiv
- low-rank
- methods
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently pretraining large
  language models (LLMs) under computational constraints, particularly the high memory
  and compute requirements. The authors systematically survey recent advances in parameter-
  and memory-efficient pretraining methods, then conduct comprehensive benchmarking
  of representative approaches including memory-efficient optimizers (GaLore, Fira)
  and weight factorization techniques (low-rank, LoRA, SLTrain).
---

# Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking

## Quick Facts
- **arXiv ID**: 2505.22922
- **Source URL**: https://arxiv.org/abs/2505.22922
- **Reference count**: 38
- **Primary result**: Proposes weight refactorization and momentum reset techniques that achieve lower perplexity than GaLore and Fira while using ~25% less memory

## Executive Summary
This paper addresses the challenge of efficiently pretraining large language models (LLMs) under computational constraints, particularly the high memory and compute requirements. The authors systematically survey recent advances in parameter- and memory-efficient pretraining methods, then conduct comprehensive benchmarking of representative approaches including memory-efficient optimizers (GaLore, Fira) and weight factorization techniques (low-rank, LoRA, SLTrain). They demonstrate that full-rank pretraining with optimized training techniques (momentum reset, adaptive gradient clipping) achieves the best performance, and identify that restoring full-rankness in low-rank methods significantly improves their performance. The authors propose two practical techniques—weight refactorization (which improves the conditioning of the optimization problem) and momentum reset (which periodically resets optimizer momenta)—and show that applying these to the low-rank method on a 1B model achieves lower perplexity than popular methods like GaLore and Fira while using approximately 25% less memory.

## Method Summary
The authors conduct a comprehensive survey and benchmarking of parameter- and memory-efficient pretraining methods for LLMs. They evaluate multiple approaches including memory-efficient optimizers (GaLore, Fira) that project gradients into lower-dimensional spaces, and weight factorization techniques (low-rank, LoRA, SLTrain) that decompose weight matrices. The proposed techniques include weight refactorization, which periodically re-decomposes weight matrices using SVD to balance singular values between factors, and momentum reset, which periodically zeroes out optimizer momentum buffers to mitigate gradient spikes. These techniques are evaluated on LLaMA models ranging from 60M to 1B parameters using the C4 dataset, with perplexity as the primary metric.

## Key Results
- Full-rank pretraining with optimized techniques (momentum reset, adaptive gradient clipping) achieves the best performance
- Restoring full-rankness in low-rank methods significantly improves their performance compared to strictly low-rank constraints
- Weight refactorization and momentum reset techniques achieve lower perplexity than GaLore and Fira while using ~25% less memory
- SLTrain (sparse + low-rank) consistently outperforms pure low-rank methods across all model scales tested
- The proposed "Low-Rank-Restarts" method with both techniques outperforms individual approaches

## Why This Works (Mechanism)

### Mechanism 1: Weight Refactorization
Periodic weight refactorization accelerates convergence in low-rank pretraining by improving the conditioning of the optimization landscape. The technique re-decomposes the weight matrix $W=BA$ using SVD into $B'=U\sqrt{\Sigma}$ and $A'=\sqrt{\Sigma}V^\top$, balancing singular values equally between factors. This balancing minimizes the condition number of the Hessian at optimality (Lemma 4.1), which correlates with faster local convergence rates. The core assumption is that local convergence behavior depends on the Hessian's condition number and training dynamics are stable enough to exploit improved conditioning.

### Mechanism 2: Momentum Reset
Periodically resetting optimizer momentum mitigates the impact of gradient spikes and accelerates training convergence. This technique zeroes out momentum buffers ($M_t, V_t$) in AdamW at fixed intervals (e.g., every 200 steps), erasing the influence of batches with spiked gradients. Theoretical analysis on SGD-M-R indicates that if iterates are converging to an optimum, resetting can improve the convergence rate bound (Theorem 4.2). The core assumption is that spiked gradients are harmful noise that persists in standard momentum buffers, degrading the update direction.

### Mechanism 3: Restoring Full-Rankness
Incorporating high-rank updates into low-rank methods significantly improves pre-training performance compared to strictly low-rank constraints. Pure low-rank factorization ($W=BA$) restricts model expressivity, while methods like SLTrain (Sparse + Low-rank) or Fira (GaLore + orthogonal term) inject information into the full-rank space. The benchmark shows that restoring full-rankness allows models to learn representations that low-rank-only methods miss. The core assumption is that pre-training on vast datasets requires model capacity that approximates full-rank expressiveness.

## Foundational Learning

### Concept: Low-Rank Factorization (SVD)
**Why needed here**: The core proposal involves manipulating factors $A$ and $B$ where $W \approx BA$. Understanding how SVD distributes "energy" (singular values) is required to grasp why balancing $A$ and $B$ affects optimization. **Quick check question**: If a matrix $W$ is factorized into $BA$, and we multiply $B$ by scalar $c$ and divide $A$ by $c$, does the output of the layer change? (Answer: No, but the optimization dynamics/gradients might).

### Concept: Optimizer State Memory
**Why needed here**: The paper benchmarks "memory efficient" methods. You must understand that standard AdamW stores 2 extra states (momentum and variance) per parameter, often consuming 2x-3x more memory than model weights themselves during training. **Quick check question**: Why does GaLore reduce memory usage specifically for optimizer states? (Answer: It projects gradients into a lower-dimensional space $R^{r \times n}$, so momentum/variance states are smaller).

### Concept: Hessian Conditioning
**Why needed here**: The theoretical justification for "Weight Refactorization" relies on the condition number of the Hessian matrix. A high condition number indicates an ill-conditioned problem (narrow valleys), which slows down gradient descent. **Quick check question**: Does a lower condition number generally lead to faster or slower convergence for gradient-based optimizers? (Answer: Faster).

## Architecture Onboarding

### Component map:
LLaMA model (RMSnorm, SwiGLU) -> Factorized linear layers ($BA$ or $BA + S_{sparse}$) -> AdamW optimizer (with momentum reset) -> Scheduler (with weight refactorization)

### Critical path:
1. **Forward Pass**: Standard LLaMA, but linear layers use factorized weights ($BA$)
2. **Backward Pass**: Gradients computed for $A$ and $B$
3. **Optimizer Step**: Check if `step % reset_interval == 0`; if so, zero momentum buffers. Apply Adam update
4. **Refactorization Step**: Check if `step % refactor_interval == 0`; if so, compute SVD of $W$, re-init $A$ and $B$

### Design tradeoffs:
- **Memory vs. Perplexity**: "Low-Rank-Restarts" uses $\approx 25\%$ less memory than Fira/GaLore (Figure 4) but requires careful tuning to match their perplexity
- **Compute vs. Stability**: Refactorization adds costly SVD operation periodically, but "Momentum Reset" is computationally free (just memory zeroing)

### Failure signatures:
- **Training Instability**: If learning rate is too high after a refactorization step, loss may spike (due to sudden change in gradient landscape)
- **Memory Exceedance**: If SLTrain sparsity ($\delta$) is set too high (e.g., 0.5 instead of 0.1), memory savings are lost
- **Rank Collapse**: If rank $r$ is too small (e.g., 32 for 1B model), perplexity plateaus at a high value regardless of resets

### First 3 experiments:
1. **Baseline Sanity Check**: Run Full-Rank AdamW vs. Stable-SPAM on a 60M model to verify that "Momentum Reset" improves your baseline (Reproduce Figure 2)
2. **Ablation Study**: Run "Low-Rank" vs. "Low-Rank + Refactorization" vs. "Low-Rank + Momentum Reset" vs. "Both" (Reproduce Figure 6) to characterize the contribution of each proposed technique
3. **Scaling Benchmark**: Compare "Low-Rank-Restarts" against "GaLore" on a 350M model to validate the claim of achieving lower perplexity with less memory (Target Table 3 metrics)

## Open Questions the Paper Calls Out

### Open Question 1
Can parameter- or memory-efficient methods achieve performance comparable to full-rank pretraining at larger model scales (beyond 1B parameters)? The authors state: "Can parameter- or memory-efficient methods achieve performance comparable to full-rank pretraining?" and note that experiments were limited to models from 60M to 1B parameters due to computational constraints. This remains unresolved because it's unknown whether the observed gap between efficient methods and full-rank training widens, narrows, or remains constant at larger scales (e.g., 7B, 70B).

### Open Question 2
What is the theoretical explanation for why momentum reset benefits Adam optimization during pretraining? The paper states: "Analyzing the benefit of momentum reset for Adam is complicated and is therefore left as a direction for future work." This remains unresolved because the theoretical analysis of momentum reset was only provided for SGD with momentum, not for Adam. The empirical benefit of momentum reset for Adam (used in all experiments) lacks theoretical grounding.

### Open Question 3
What are the optimal hyperparameters (frequency, timing) for weight refactorization and momentum reset across different model sizes and architectures? The paper uses fixed values (refactorization and momentum reset every 200 iterations) based on limited experimentation. The ablation study (Figure 6) shows both techniques help, but systematic exploration of scheduling strategies is absent. This remains unresolved because the paper does not explore whether different reset frequencies should be used for different model sizes, nor whether adaptive scheduling could yield better results.

### Open Question 4
Do the proposed techniques generalize to other datasets, tasks, and architectures beyond C4 pretraining with LLaMA models? The authors state in the conclusion: "In future work, we aim to expand our experiments to include more models, datasets, and tasks, as there are several important models and datasets that we did not consider." This remains unresolved because all experiments used the C4 dataset with LLaMA architecture. It's unclear whether weight refactorization and momentum reset provide similar benefits for other architectures (e.g., Mistral, GPT-style) or training objectives (e.g., instruction tuning, continued pretraining).

## Limitations
- Empirical claims are based on controlled benchmarking but generalization to other architectures or tasks remains untested
- SVD-based refactorization adds computational overhead that isn't fully characterized across different hardware configurations
- Memory savings claims may not scale linearly to larger models or different architectures

## Confidence
- **High Confidence**: Claims about momentum reset improving baseline AdamW performance, validated by prior work (Stable-SPAM) and consistent results across multiple benchmarks
- **Medium Confidence**: Claims regarding weight refactorization improving conditioning and convergence, supported by theoretical analysis (Lemma 4.1) and empirical improvements
- **Medium Confidence**: Claims about memory efficiency improvements, demonstrated on tested models but may not scale linearly to larger models or different architectures

## Next Checks
1. **Architecture Generalization Test**: Evaluate the proposed techniques (weight refactorization and momentum reset) on transformer architectures beyond LLaMA, such as OPT or GPT-style models, to verify claimed improvements are architecture-agnostic

2. **Computational Overhead Analysis**: Conduct detailed measurement of SVD computation time for weight refactorization across different rank values and model sizes, and assess whether perplexity gains justify additional compute cost on various hardware platforms

3. **Task Transferability Evaluation**: Test whether models trained with these parameter- and memory-efficient methods maintain performance advantage when fine-tuned on downstream tasks (e.g., summarization, code generation) rather than just being evaluated on perplexity, to validate quality of learned representations