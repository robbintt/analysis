---
ver: rpa2
title: 'SoFlow: Solution Flow Models for One-Step Generative Modeling'
arxiv_id: '2512.15657'
source_url: https://arxiv.org/abs/2512.15657
tags:
- training
- flow
- velocity
- loss
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Solution Flow Models (SoFlow) for one-step
  generative modeling, addressing the computational inefficiency of iterative sampling
  in diffusion and flow matching models. The core idea is to directly learn the solution
  function of the velocity ODE that maps between different time points, enabling single-step
  generation without iterative solvers.
---

# SoFlow: Solution Flow Models for One-Step Generative Modeling

## Quick Facts
- arXiv ID: 2512.15657
- Source URL: https://arxiv.org/abs/2512.15657
- Authors: Tianze Luo; Haotian Yuan; Zhuang Liu
- Reference count: 32
- Key outcome: DiT-XL/2 model reaches 2.96 FID-50K at 1-NFE sampling on ImageNet 256×256

## Executive Summary
SoFlow introduces Solution Flow Models for one-step generative modeling, addressing the computational inefficiency of iterative sampling in diffusion and flow matching models. The method directly learns the solution function of the velocity ODE that maps between different time points, enabling single-step generation without iterative solvers. SoFlow combines a Flow Matching loss and a solution consistency loss, naturally supporting Classifier-Free Guidance during training while avoiding computationally expensive Jacobian-vector product calculations. Experiments show SoFlow models achieve better FID-50K scores than MeanFlow models across various model sizes when trained from scratch under the same conditions.

## Method Summary
SoFlow learns a solution function f_θ(x_t, t, s) that maps from time t to time s in a single neural function evaluation, parameterized as f_θ(x_t, t, s) = x_t + (s-t)F_θ(x_t, t, s) (Euler parameterization). The method combines two losses: Flow Matching loss (75% of batch) and solution consistency loss (25%), with adaptive weighting. Training uses logit-normal distributions for time sampling and an exponential schedule to shrink the consistency interval. Classifier-Free Guidance is incorporated during training via velocity mixing, enabling one-step guided inference without additional overhead. The model is trained from scratch using DiT backbone with linear noising schedule.

## Key Results
- DiT-XL/2 achieves 2.96 FID-50K at 1-NFE on ImageNet 256×256
- SoFlow outperforms MeanFlow across all model sizes when trained from scratch
- Euler parameterization with linear schedule significantly outperforms alternatives (11.59 vs 17.50+ FID)
- 75% Flow Matching / 25% Solution Consistency ratio optimal for training
- CFG during training enables effective one-step guided inference

## Why This Works (Mechanism)

### Mechanism 1: Taylor-Approximated PDE Constraint Enforcement
The solution consistency loss implicitly enforces the PDE constraint (∂₁f_θ v + ∂₂f_θ = 0) via first-order Taylor approximation. Instead of direct Jacobian computation, SoFlow minimizes ||f_θ(x_t, t, s) - f_θ⁻(x_t + v(x_t,t)(l-t), l, s)||², which by Taylor expansion approximates the PDE residual. This works when |t-l| shrinks sufficiently during training; otherwise, approximation error dominates.

### Mechanism 2: Boundary-Condition Parameterization Guarantee
The parameterization f_θ(x_t, t, s) = a(t,s)x_t + b(t,s)F_θ(x_t, t, s) with a(t,t)=1, b(t,t)=0 guarantees f_θ(x_t, t, t) = x_t exactly. This frees network capacity to focus on transport dynamics rather than boundary matching, as the identity condition is satisfied without learning.

### Mechanism 3: Training-Time CFG Integration via Velocity Mixing
SoFlow incorporates CFG during training by mixing conditional, unconditional, and guided velocities: v_mix = m(w·v_cond + (1-w)·v_uncond) + (1-m)·v_guided. This teaches the model to approximate the guided velocity field, so 1-NFE inference produces guided outputs without inference-time guidance overhead.

## Foundational Learning

- **Flow Matching / Velocity ODEs**: SoFlow is built on Flow Matching where generation solves dx/dt = v(x,t). Understanding this is essential to grasp what "solution function" means.
  - Quick check: Given a velocity field v(x,t), can you write the integral equation for x(s) given x(t)?

- **ODE Solution Functions / Flow Maps**: SoFlow's core object f(x_t, t, s) maps state x_t at time t to evolved state at time s, more general than consistency models' noise-to-data mapping.
  - Quick check: What are the two defining identities of a valid solution function f(x_t, t, s)?

- **Classifier-Free Guidance (CFG)**: SoFlow incorporates CFG into training. One must understand how v_guided = w·v_cond + (1-w)·v_uncond modifies the velocity field.
  - Quick check: Why does CFG during training enable 1-NFE guided inference in SoFlow but not in standard diffusion models?

## Architecture Onboarding

- **Component map**: x_t -> (t,s,c) -> F_θ backbone -> F_θ output -> (s-t) positional embedding -> f_θ = a(t,s)x_t + b(t,s)F_θ -> v extraction -> loss computation

- **Critical path**: 
  1. Sample (x_0, c) and construct x_t via noising schedule
  2. Sample t, s, l according to logit-normal distributions and schedule r(k,K)
  3. For FM batch: compute velocity prediction, apply CFG mixing, compute L^g_FM
  4. For SCM batch: compute f_θ(x_t, t, s) and stop-grad target f_θ⁻(x_t + v(l-t), l, s), compute L^g_SCM
  5. Combine losses via λ and backprop

- **Design tradeoffs**:
  - Euler vs. Trigonometric parameterization: Euler (linear) performs best (Table 1d)
  - λ (FM data ratio): 75% FM / 25% SCM optimal; pure SCM (0%) degrades (Table 1b)
  - Schedule r(k,K): Exponential decay of |t-l| optimal; constant acceptable but worse (Table 1a)

- **Failure signatures**:
  - FID not improving: Check if l is clamped properly (l < t - 10⁻⁴); Taylor approximation breaks otherwise
  - High variance in loss: Reduce velocity mix ratio m or increase robustness coefficient p
  - CFG not effective: Verify CFG drop rate (0.1) and that uncond velocity is learned via empty-label training

- **First 3 experiments**:
  1. Sanity check (DiT-B/4, no CFG): Train 100K iterations with λ=0.75, p=1.0, exponential schedule. Target FID < 65 on ImageNet 256×256 to verify pipeline
  2. CFG ablation (DiT-B/4): Enable CFG with w=2.5, m=0.25. Compare FID against no-CFG baseline; expect significant improvement
  3. Scale test (DiT-XL/2): Full 240-epoch training with optimal hyperparameters. Target FID < 3.5 (1-NFE) to match paper claims

## Open Questions the Paper Calls Out
- **Open Question 1**: How does SoFlow perform in the distillation setting where a pre-trained teacher model provides velocity targets? The authors explicitly excluded distillation from experiments to focus on from-scratch training.
- **Open Question 2**: How does SoFlow's generation quality scale when using more than 2 sampling steps (multi-step inference)? The paper briefly reports 2-NFE results but provides no systematic study of quality versus step count.
- **Open Question 3**: Does the theoretical error bound (O(√δ)) in Appendix E accurately predict empirical convergence behavior during training? The appendix derives the bound but no empirical validation connects training loss values to actual ODE violation or sample quality.

## Limitations
- Training schedule sensitivity: The Taylor approximation justification relies on |t-l| → 0 during training, but optimal schedule parameters are determined empirically without theoretical error bounds
- Implementation gaps: Several key details underspecified including exact positional embedding scheme for (s-t), DiT architecture modifications, and multi-step sampling procedure
- Computational advantage quantification: While SoFlow claims to avoid JVPs, the practical computational advantage over MeanFlow is not explicitly measured or compared

## Confidence
- **High Confidence**: Core mathematical formulation, experimental results showing SoFlow outperforming MeanFlow on ImageNet 256×256 with various model sizes
- **Medium Confidence**: Claim that SoFlow avoids JVPs while enforcing PDE constraints through Taylor-approximated consistency loss, superiority of Euler parameterization
- **Low Confidence**: Practical advantages of training-time CFG over inference-time CFG, quantification of computational benefits

## Next Checks
1. **Schedule sensitivity analysis**: Systematically vary exponential schedule parameters r_init and r_end across multiple orders of magnitude to determine minimum |t-l| required for stable training and optimal FID, measuring both training stability and final generation quality

2. **CFG mechanism isolation**: Train SoFlow models with and without CFG training, then compare 1-NFE guided inference performance. Additionally, measure variance of v_guided versus stochastic conditional velocity term during training to verify stated variance reduction

3. **JVP computational comparison**: Implement SoFlow's solution consistency loss and MeanFlow's interval-averaged loss with explicit JVP computation. Measure actual wall-clock training time and memory usage on identical hardware to quantify computational advantage claimed by SoFlow