---
ver: rpa2
title: Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in
  LLM Reasoning
arxiv_id: '2510.00819'
source_url: https://arxiv.org/abs/2510.00819
tags:
- policy
- learning
- capo
- gradient
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of policy gradient instability\
  \ in reinforcement learning for large language models (LLMs), particularly under\
  \ aggressive training regimes that aim for sample efficiency but often lead to catastrophic\
  \ updates and policy collapse. The core method, Curvature-Aware Policy Optimization\
  \ (CAPO), introduces a tractable computational framework that models optimization\
  \ dynamics by approximating second-order geometric information\u2014specifically,\
  \ the Hessian of the objective and the Fisher Information Matrix of the policy distribution."
---

# Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning

## Quick Facts
- arXiv ID: 2510.00819
- Source URL: https://arxiv.org/abs/2510.00819
- Reference count: 40
- Key outcome: Achieves up to 30× improvement in sample efficiency on math reasoning benchmarks by stabilizing policy gradients through curvature-aware optimization

## Executive Summary
This paper addresses the critical challenge of policy gradient instability in reinforcement learning for large language models (LLMs), particularly when aggressive training regimes are used to maximize sample efficiency. The proposed method, Curvature-Aware Policy Optimization (CAPO), introduces a tractable computational framework that models second-order optimization geometry to identify and reject samples contributing to unstable updates. By tracking and leveraging curvature information through a last-layer model approximation, CAPO provides both theoretical guarantees of monotonic improvement and empirical evidence of dramatically improved sample efficiency, achieving up to 30× better performance on math reasoning benchmarks while rejecting fewer than 8% of tokens.

## Method Summary
CAPO is a policy optimization method that stabilizes reinforcement learning for LLMs by modeling second-order optimization geometry through tractable approximations of the Hessian of the objective and Fisher Information Matrix of the policy distribution. The method uses a last-layer model to compute directional curvatures for proposed update steps, identifying tokens that contribute to unstable updates and masking them out. Under realistic assumptions, this approach guarantees monotonic policy improvement while enabling aggressive training regimes (higher learning rates, smaller batch sizes) that typically lead to policy collapse. The algorithm enforces trust-region constraints through token-level data selection, rejecting samples that violate predefined curvature thresholds while preserving sample efficiency.

## Key Results
- Achieves up to 30× improvement in sample efficiency on math reasoning benchmarks compared to standard GRPO
- Successfully trains under aggressive regimes (LR=1.5×10⁻⁵, batch=96) where baseline methods fail
- Maintains stability with token rejection rate below 8% across experiments
- Provides theoretical guarantee of monotonic policy improvement under realistic assumptions

## Why This Works (Mechanism)

### Mechanism 1: Second-Order Geometry Modeling for Instability Detection
The method approximates second-order information—the Hessian of the objective and the Fisher Information Matrix—using a tractable last-layer model. By computing directional curvatures for proposed update steps, it quantifies expected shifts in the objective and policy distribution. Large or negative shifts indicate potentially unstable updates. The core assumption is that the last layer captures sufficient curvature information to model full-parameter update dynamics, though this simplification may miss instabilities originating in deeper layers.

### Mechanism 2: Trust-Region Enforcement via Token-Level Data Selection
For each candidate update, the method evaluates directional curvatures and rejects tokens contributing to unstable updates based on predefined thresholds. This targeted intervention removes only problematic data rather than scaling entire updates, enforcing a local trust region. The assumption is that instability can be attributed to specific, identifiable samples, and removing them restores stability. This novel token-level rejection mechanism differs from batch-level interventions found in related work.

### Mechanism 3: Theoretical Guarantee of Monotonic Improvement
The curvature-aware data selection mechanism provides theoretical guarantees of monotonic policy improvement by ensuring accepted updates satisfy bounds on objective improvement and policy divergence. The method constructs updates that are guaranteed, in expectation, to improve the true objective under realistic assumptions about approximation accuracy and Hessian regularity. However, large approximation errors in the last-layer model could violate theoretical assumptions in practice.

## Foundational Learning

**Second-Order Optimization (Hessian/Fisher Information Matrix):** Understanding that the Hessian captures curvature of the loss landscape and the Fisher Information Matrix captures curvature of the probability distribution is essential, as CAPO's entire method builds on approximating this second-order information. Quick check: If a loss surface has high positive curvature, would a standard first-order gradient step be more or less likely to overshoot the minimum than on a flat surface?

**Policy Gradient Methods (e.g., PPO, GRPO):** This is the base algorithm being improved. Understanding how policy gradients use log-probability of actions and advantage estimates to update a policy is fundamental. Quick check: In a policy gradient update, does the objective function maximize the probability of actions with positive advantage or negative advantage?

**Trust Region Methods:** CAPO implements a form of trust region by constraining the size of policy updates to prevent drastic changes in a single step, which is exactly what the Fisher curvature threshold enforces. Quick check: What is the primary risk of taking a policy update step that is too large?

## Architecture Onboarding

**Component map:** Last-Layer Model -> Directional Curvature Estimator -> Data Selection Masker -> Base Optimizer

**Critical path:** 1) Last-Layer Model computes model gradient from batch trajectories, 2) Directional Curvature Estimator calculates objective and policy shifts for proposed update direction, 3) Data Selection Masker applies thresholds and creates binary mask to reject non-compliant tokens, 4) Base Optimizer receives masked gradient for full LLM parameter update

**Design tradeoffs:** The choice of last-layer model vs. full-model curvature represents a tradeoff between tractability and fidelity in curvature estimation. Token-level vs. batch-level intervention provides fine-grained control but increases implementation complexity. Threshold tuning is critical for performance but requires hyperparameter optimization that may offset some efficiency gains.

**Failure signatures:** Rejection of nearly all tokens indicates thresholds too tight, halting learning. No rejection but policy collapse suggests poor last-layer approximation or loose thresholds. High computational overhead from extra forward/backward passes could slow training significantly despite theoretical tractability.

**First 3 experiments:** 1) Systematic threshold sweep on held-out set to find stable operating regions without excessive token rejection, 2) Ablation study comparing last-layer model against cheaper approximations to quantify value of complex model, 3) Benchmark against corpus neighbors (GTPO, PPO clipping) under identical aggressive training conditions to measure sample efficiency and stability.

## Open Questions the Paper Calls Out

**Open Question 1:** Does extending the computational model beyond last-layer approximation to deeper layers improve curvature estimates and stability guarantees, or does the last-layer model capture essential optimization dynamics? The paper suggests future research could explore multi-layer parameterizations, though deeper models may increase computational overhead.

**Open Question 2:** Can CAPO thresholds be determined adaptively or via principled heuristics rather than manual tuning? Current thresholds are manually tuned per method, and the relationship between problem characteristics and optimal thresholds remains uncharacterized.

**Open Question 3:** Do soft masking or regularization-based interventions provide better stability-performance trade-offs than hard token rejection? The paper suggests evaluating extensions like soft masking that might preserve more gradient signal while maintaining stability.

**Open Question 4:** Does CAPO maintain its advantages when scaling to larger models (≥70B parameters) and longer training schedules? Results are demonstrated only on 7B parameter models due to compute constraints, and stability dynamics may differ substantially at scale.

## Limitations
- Last-layer approximation may miss instabilities originating in deeper network layers
- Performance depends critically on manually-tuned curvature thresholds with unknown generalization across domains
- Computational overhead from additional forward/backward passes is not quantified relative to baseline methods

## Confidence
**High Confidence:** Empirical demonstration of superior sample efficiency (up to 30× improvement) on math reasoning benchmarks
**Medium Confidence:** Theoretical guarantee of monotonic improvement under realistic assumptions, though practical applicability depends on approximation accuracy
**Low Confidence:** Claim of <8% token rejection across all conditions without sensitivity analysis showing how rates vary with hyperparameters

## Next Checks
1. Cross-architecture validation testing CAPO on different LLM architectures to assess whether last-layer curvature modeling remains effective when optimization dynamics shift to deeper layers
2. Threshold robustness analysis systematically varying curvature thresholds across orders of magnitude while tracking both sample efficiency gains and token rejection rates
3. Multi-task generalization study evaluating CAPO on non-mathematical reasoning tasks to determine whether benefits extend beyond the math domain where developed