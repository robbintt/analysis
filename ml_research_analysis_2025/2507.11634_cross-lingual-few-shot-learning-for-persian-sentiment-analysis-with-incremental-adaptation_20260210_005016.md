---
ver: rpa2
title: Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental
  Adaptation
arxiv_id: '2507.11634'
source_url: https://arxiv.org/abs/2507.11634
tags:
- learning
- sentiment
- incremental
- persian
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates cross-lingual few-shot learning for Persian\
  \ sentiment analysis by leveraging multilingual pre-trained models and incremental\
  \ learning strategies. Three models\u2014XLM-RoBERTa, mDeBERTa, and DistilBERT\u2014\
  were fine-tuned on limited Persian data from five diverse domains."
---

# Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation

## Quick Facts
- arXiv ID: 2507.11634
- Source URL: https://arxiv.org/abs/2507.11634
- Authors: Farideh Majidi; Ziaeddin Beheshtifard
- Reference count: 21
- Primary result: mDeBERTa and XLM-RoBERTa achieve up to 96% accuracy on Persian sentiment analysis with knowledge distillation and rehearsal regularization in 1-20 shot settings

## Executive Summary
This study investigates cross-lingual few-shot learning for Persian sentiment analysis by leveraging multilingual pre-trained models and incremental learning strategies. Three models—XLM-RoBERTa, mDeBERTa, and DistilBERT—were fine-tuned on limited Persian data from five diverse domains. Incremental learning with regularization techniques (Elastic Weight Consolidation, knowledge distillation, and rehearsal) was employed to mitigate catastrophic forgetting. Experimental results demonstrate that mDeBERTa and XLM-RoBERTa achieved up to 96% accuracy, with knowledge distillation and rehearsal showing the strongest performance in low-shot scenarios. DistilBERT underperformed due to its smaller capacity.

## Method Summary
The research employed five Persian sentiment datasets from diverse sources: tweets (X), Instagram comments, Digikala reviews, Snappfood reviews, and Taaghche reviews. Five multilingual pre-trained models (XLM-RoBERTa, mDeBERTa, DistilBERT) were fine-tuned incrementally across these domains using three regularization techniques to prevent catastrophic forgetting. The domains were introduced sequentially in order of increasing complexity, with shot-based sampling (1-20 examples per class) to simulate few-shot scenarios. Performance was evaluated using accuracy and F1-score metrics.

## Key Results
- mDeBERTa and XLM-RoBERTa achieved up to 96% accuracy on Persian sentiment analysis
- Knowledge distillation and rehearsal regularization showed strongest performance in 1-shot scenarios (91.32% F1 for KD)
- DistilBERT underperformed across all settings due to smaller model capacity
- Rehearsal provided consistent performance across all shot counts and often rivaled no-incremental learning baselines

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Transfer via Shared Multilingual Representations
Multilingual pre-trained models transfer sentiment classification capabilities to Persian through shared semantic representations learned from high-resource languages. Models pre-trained on 100+ languages encode language-agnostic semantic patterns; when fine-tuned on minimal Persian data, these representations activate relevant sentiment features without requiring extensive target-language examples.

### Mechanism 2: Knowledge Distillation Preserves Prior Knowledge in Extreme Few-Shot
Knowledge distillation provides superior performance in 1-shot scenarios by soft-label guidance from a teacher model. The teacher outputs probability distributions over classes; these soft labels encode uncertainty and class relationships, guiding the student to retain previously learned patterns while adapting to new domains with minimal samples.

### Mechanism 3: Rehearsal Mitigates Catastrophic Forgetting Across Domain Increments
Rehearsal consistently maintains performance across shot counts by interleaving stored prior examples with new domain data. A small buffer of exemplars from previous domains is retained and mixed into training batches; this prevents weight drift away from previously learned representations while allowing new domain adaptation.

## Foundational Learning

- **Masked Language Modeling (MLM)**: Understanding how XLM-RoBERTa and mDeBERTa learned multilingual representations through predicting masked tokens. Quick check: Can you explain why masking 15% of tokens forces a model to learn contextual semantics?

- **Catastrophic Forgetting**: The core problem incremental learning and regularization techniques address when models learn domains sequentially. Quick check: What happens to task A performance when a neural network is trained exclusively on task B?

- **Elastic Weight Consolidation (EWC)**: One of three regularization strategies compared; works by penalizing changes to important weights. Quick check: How would EWC identify which parameters are "important" for a previously learned task?

- **Knowledge Distillation**: Best-performing technique at 1-shot; understanding teacher-student soft label transfer is essential. Quick check: Why might soft labels (probability distributions) contain more information than hard labels (one-hot vectors)?

- **Few-Shot Learning Paradigm**: The entire experimental framework is built around 1-20 labeled examples per class per domain. Quick check: What risks does evaluating on 1-shot vs. 20-shot pose for reliability and variance?

## Architecture Onboarding

- **Component map**: Pre-trained multilingual checkpoint -> Incremental training loop with domain sequence -> Regularization module (EWC/KD/Rehearsal) -> Classification head on [CLS] token -> Evaluation on held-out test set

- **Critical path**: Load pre-trained multilingual checkpoint with sentiment fine-tuning → Prepare datasets in specified order (X → Instagram → Snappfood → Taaghche → Digikala) → Implement chosen regularization → For each domain phase: sample N-shot per class, mix with rehearsal buffer if enabled, fine-tune with early stopping → Evaluate on held-out test set after each phase

- **Design tradeoffs**: mDeBERTa vs. XLM-RoBERTa: mDeBERTa shows more stable scaling; XLM-RoBERTa has higher ceiling at 20-shot with joint training. Incremental + regularization vs. No Incremental Learning: Joint training achieves higher peak accuracy but requires all data upfront. EWC vs. Rehearsal vs. Distillation: EWC underperforms at low shots; Rehearsal is most consistent; Distillation excels at 1-shot

- **Failure signatures**: EWC at 2-shot (XLM-R): F1 drops to ~50%—regularization may be too rigid with insufficient data. DistilBERT consistently low: <46% F1 at 1-shot across all methods—capacity insufficient for cross-lingual few-shot. No Regularization baseline: Unstable at low shots; confirms forgetting occurs without mitigation. mDeBERTa saturation: Performance plateaus at 15-20 shots; additional data yields diminishing returns

- **First 3 experiments**: 1) Baseline calibration: Load XLM-RoBERTa, train on single domain (X) with 5-shot per class, no incremental learning, report accuracy/F1 on held-out test. 2) Regularization ablation: Using mDeBERTa at 1-shot and 5-shot, compare Knowledge Distillation vs. Rehearsal vs. EWC on two sequential domains (X → Instagram). 3) Full incremental run: Train mDeBERTa with Rehearsal across all five domains sequentially at 10-shot, measuring cumulative accuracy and computing forgetting metric

## Open Questions the Paper Calls Out

Can this combined cross-lingual and incremental learning approach be generalized to other low-resource languages beyond Persian? The introduction states the approach can be extended to other low-resource languages for similar text classification tasks, but the study focuses exclusively on Persian datasets without empirical validation on other languages.

Why does mDeBERTa saturate and fail to improve with increased shot sizes in this incremental setting? The conclusion observes that mDeBERTa saturates and can't get better results with more data, but does not investigate the underlying cause such as model capacity constraints or the specific regularization techniques used.

To what extent does the specific order of domain introduction influence the mitigation of catastrophic forgetting? The methodology introduces datasets in a fixed order based on perceived complexity, but does not test alternative sequences to verify if the "simple-to-complex" heuristic is optimal.

## Limitations

Critical training hyperparameters (learning rate, batch size, optimizer, weight decay) are not specified, creating significant uncertainty in replication fidelity. No information is provided about variance across multiple random seeds, despite few-shot learning results being notoriously sensitive to initialization and sampling. The rehearsal buffer design remains unspecified, including exact buffer size, sampling strategy, and memory allocation across domains.

## Confidence

**High Confidence**: mDeBERTa and XLM-RoBERTa achieve superior performance to DistilBERT in cross-lingual few-shot settings. The architectural capacity difference is well-established and consistently demonstrated.

**Medium Confidence**: Knowledge distillation provides the strongest performance at 1-shot settings. Experimental results support this claim, but the absence of variance metrics and hyperparameter details limits confidence in the absolute magnitude of improvement.

**Low Confidence**: The sequential domain ordering (X → Instagram → Snappfood → Taaghche → Digikala) represents optimal complexity progression. The paper does not provide ablation studies on domain ordering, and the claim about complexity progression is not empirically validated.

## Next Checks

1. Run the incremental learning experiments with 5 different random seeds for each regularization method at 1-shot and 10-shot settings. Calculate 95% confidence intervals for accuracy and F1-score to determine if observed differences between methods are statistically significant.

2. Systematically vary learning rate (1e-5, 2e-5, 5e-6) and batch size (8, 16, 32) for the best-performing method (knowledge distillation) at 5-shot setting. Document performance variance to identify robust configurations and understand sensitivity to these critical but unspecified parameters.

3. Replicate the full incremental learning pipeline with reversed domain order (Digikala → Taaghche → Snappfood → Instagram → X) using mDeBERTa with rehearsal. Compare forgetting rates and final performance to test whether the claimed complexity progression meaningfully impacts learning outcomes.