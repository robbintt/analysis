---
ver: rpa2
title: Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs
arxiv_id: '2507.05810'
source_url: https://arxiv.org/abs/2507.05810
tags:
- concept
- concepts
- dataset
- block
- bagel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BAGEL, a novel framework for global mechanistic
  interpretability that analyzes how semantic concepts propagate through deep neural
  networks. Unlike existing local interpretability methods, BAGEL provides a comprehensive
  view of concept emergence and interaction across model layers by creating structured
  knowledge graphs that link dataset biases to model-internal representations.
---

# Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs

## Quick Facts
- arXiv ID: 2507.05810
- Source URL: https://arxiv.org/abs/2507.05810
- Reference count: 40
- Primary result: Introduces BAGEL framework that analyzes semantic concept propagation through DNNs using structured knowledge graphs, outperforming baseline interpretability methods in detecting dataset biases.

## Executive Summary
This paper presents BAGEL, a global mechanistic interpretability framework that provides comprehensive analysis of how semantic concepts propagate through deep neural networks. Unlike local interpretability methods that explain individual predictions, BAGEL creates structured knowledge graphs that link dataset biases to model-internal representations across all layers. The framework uses logistic regression classifiers to detect concept presence at each layer and compares these with dataset-level concept distributions using weighted F1-score and Jensen-Shannon divergence metrics. Evaluation across seven architectures and five datasets demonstrates BAGEL's effectiveness in identifying biased concept associations, with recall scores ranging from 0.35 to 1.00 depending on the method and dataset.

## Method Summary
BAGEL analyzes concept propagation through DNNs by extracting intermediate layer activations via forward hooks, applying Global Average Pooling to reduce spatial dimensions, and training layer-wise logistic regression classifiers for each semantic concept. The framework compares conditional concept probabilities between dataset statistics (obtained via CLIP zero-shot labeling or manual annotation) and model-internal representations using weighted F1-score and Jensen-Shannon divergence. Results are visualized as structured knowledge graphs where nodes represent classes and concepts, edges indicate probabilistic relationships, and colors encode whether concepts originate from dataset biases, model-specific associations, or represent mitigated biases.

## Key Results
- Recall scores for biased concept detection range from 0.35 to 1.00 across different architectures and datasets
- BAGEL outperforms baseline methods like TCAV and SAE in identifying dataset biases that influence model decisions
- Knowledge graphs reveal model-specific biases (red edges) such as "forest" and "mountain" concepts in Husky vs Wolf classification
- Layer-wise analysis shows concept emergence patterns, with early layers detecting textures/colors and deeper layers identifying object parts/structures

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Concept Detection via Linear Probing
Logistic regression classifiers trained on intermediate layer activations can detect semantic concepts as they emerge through network layers. For each layer, Global Average Pooling reduces spatial activations to scalar unit activations, and binary classifiers are trained per concept. The core assumption is that concepts are approximately linearly separable in intermediate layer feature spaces. Break condition occurs when concepts are non-linearly encoded or require spatial reasoning that GAP destroys.

### Mechanism 2: Dataset-Model Bias Alignment via Distribution Comparison
Comparing conditional concept probabilities between dataset statistics and model-internal representations reveals how biases propagate during training. The framework computes dataset bias probabilities empirically and model bias probabilities from classifier predictions, quantifying alignment using weighted F1-score and Jensen-Shannon divergence. Core assumption: dataset biases that causally influence model decisions will show aligned probability distributions. Break condition: when model learns genuinely causal features unrelated to spurious dataset correlations.

### Mechanism 3: Knowledge Graph Representation for Global Explanation
Structured knowledge graphs compactly represent concept-class-bias relationships across all layers, enabling interactive exploration of global model behavior. The framework constructs directed graphs where nodes are classes and concepts, edges connect classes to concepts, and weight vectors encode layer-wise probabilities with color-coding for bias source. Core assumption: concept-class relationships are meaningful primitives for understanding model decision-making. Break condition: when concepts are highly entangled or important features defy verbalization.

## Foundational Learning

- **Concept: Concept Activation Vectors (CAVs) and TCAV**
  - Why needed here: BAGEL builds on TCAV's approach of training classifiers on activations but extends it from single-layer sensitivity testing to full network global analysis.
  - Quick check question: Given a set of images with and without concept "striped," how would you train a CAV, and what does the resulting vector represent?

- **Concept: Global vs. Local Interpretability**
  - Why needed here: BAGEL's primary contribution is moving from per-prediction explanations to dataset-wide bias analysis—you need to understand what's being extended.
  - Quick check question: Why do saliency maps fail to identify that a husky classifier is relying on "snow" background rather than animal features?

- **Concept: Spurious Correlations and Dataset Bias**
  - Why needed here: The core problem BAGEL addresses—understanding how sampling imbalances create shortcuts that models exploit.
  - Quick check question: In a dataset where 90% of wolves have snowy backgrounds and 90% of huskies have grass backgrounds, what failure mode would you expect when deploying this model?

## Architecture Onboarding

- Component map:
  1. Feature extraction hooks → Register forward hooks on all convolutional/FC layers
  2. Global Average Pooling → Reduce spatial activations to scalar unit activations φ_ℓ(x)
  3. Concept labeling module → CLIP zero-shot classification or manual annotation for ground truth concept presence
  4. Layer-wise concept classifiers → Logistic regression per (layer, concept) pair with L2 regularization
  5. Probability aggregators → Compute p_dataset and p^ℓ_model conditional distributions
  6. Comparison engine → Weighted F1 and JS divergence computation
  7. Knowledge graph generator → D3.js visualization with color/width encodings

- Critical path:
  1. Load pretrained model and register hooks on target layers
  2. Forward pass entire dataset, caching φ_ℓ(x) for all layers
  3. Generate concept labels y_k for all images (CLIP or manual)
  4. For each (ℓ, k), train logistic regression: features = φ_ℓ(X), labels = y_k
  5. Compute p_dataset(c_k | y_i) from label statistics
  6. Compute p^ℓ_model(c_k | y_i) by averaging classifier outputs per class
  7. Apply threshold τ, generate KG edges, render interactive visualization

- Design tradeoffs:
  - GAP vs. spatial preservation: Unified layer treatment sacrifices localization (can't say *where* in image concept is detected)
  - Logistic regression vs. non-linear probes: Interpretability and speed traded for potential accuracy on complex concept encoding
  - Predefined vs. discovered concepts: User must specify concept set upfront—may miss unknown important features
  - Threshold τ selection: Higher threshold reduces false positives but may miss subtle biases

- Failure signatures:
  - All F1 scores near 0 or 1: Check label quality, classifier may be trivially succeeding/failing
  - No red edges in KG: Model may be perfectly aligned with data (unlikely) or threshold too high
  - Large JS divergence with high F1: Probability magnitudes differ but rankings align—consider if absolute calibration matters for your use case
  - Inconsistent results across random seeds: Concept classifiers unstable, increase regularization or data
  - Recall varies wildly across architectures (Table 2): Some layer types may not encode certain concepts well—check which blocks perform best

- First 3 experiments:
  1. **Sanity check on synthetic bias**: Train model on Cats/Dogs with injected color bias (white dogs, black cats). Verify BAGEL detects "color" as model-specific bias with high recall. This validates the pipeline on known ground truth.
  2. **Layer-wise concept emergence analysis**: On MonuMAI (architectural styles), plot concept probability evolution across layers. Confirm early layers detect textures/colors, deeper layers detect object parts/structures (replicate Figure 5 pattern).
  3. **Cross-architecture comparison**: Run BAGEL on ResNet18 vs. DenseNet121 for same dataset. Identify which architecture shows more model-specific biases (red edges). This reveals if skip connections or dense connectivity affect bias propagation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the BAGEL framework be extended to trace concept propagation and entanglement within multimodal architectures (e.g., vision-language models)?
- **Basis in paper:** The Conclusion states: "Future work will explore extending BAGEL to multimodal architectures..."
- **Why unresolved:** The current implementation focuses on vision-only DNNs using Global Average Pooling on convolutional and fully connected layers; it does not address the cross-attention mechanisms or heterogeneous embedding spaces found in multimodal models.
- **What evidence would resolve it:** An adaptation of BAGEL that successfully maps concept flow across distinct modalities (e.g., text and image) and identifies spurious correlations arising from the interaction between them.

### Open Question 2
- **Question:** Can automated concept discovery methods be integrated to refine concept sets without relying on manual annotation or static external knowledge graphs?
- **Basis in paper:** The Conclusion states: "Future work will explore... refining concept definitions through automated discovery."
- **Why unresolved:** The current method relies on manually annotated concepts (MonuMAI, Derm7pt) or CLIP-based zero-shot classification using predefined text queries, which may miss latent, non-human-obvious features or inherit CLIP's biases.
- **What evidence would resolve it:** A study demonstrating BAGEL's effectiveness when coupled with unsupervised dictionary learning (e.g., SAEs or NMF) to automatically generate the concept set $C$ from the model's internal activations directly.

### Open Question 3
- **Question:** Does the assumption of linear separability in the probing classifiers limit the detection of "superposed" or non-linear concept representations?
- **Basis in paper:** The Implementation section notes: "we note that it [logistic regression] assumes a linear relation between features and concepts, which may limit its ability to capture more complex patterns."
- **Why unresolved:** Deep neural networks often encode information in non-linear manifolds or through polysemantic neurons where concepts are superposed; linear probes may fail to disentangle these complex representations.
- **What evidence would resolve it:** A comparative analysis showing that non-linear probes (e.g., MLPs) utilized within the BAGEL framework yield significantly higher F1-scores or reveal concept-class biases that linear probes failed to detect.

### Open Question 4
- **Question:** To what extent does the alignment between dataset and model concept distributions imply causal reliance on those concepts for decision-making?
- **Basis in paper:** The Discussion on results states: "detection of them does not inherently confirm that the model applies it causally in its decision-making, as feature attribution methods may capture correlations rather than mechanistic dependencies."
- **Why unresolved:** BAGEL identifies associations (high probability of concept presence in a class), but this statistical correlation does not prove the model functionally uses that concept for the classification output.
- **What evidence would resolve it:** Experiments involving causal interventions (e.g., removing or altering the identified concept in the input or latent space) that result in a measurable change in the model's class prediction.

## Limitations

- Linear assumption in probing classifiers may systematically underestimate complex concept encoding in deep layers
- Reliance on predefined concept sets prevents discovery of novel important features
- Threshold τ=0.5 is somewhat arbitrary and may produce inconsistent results across datasets

## Confidence

- **High Confidence**: Technical implementation (feature extraction, GAP pooling, logistic regression training) follows established interpretability practices
- **Medium Confidence**: Distributional comparison metrics (weighted F1, JS divergence) are standard but require empirical validation for bias detection interpretation
- **Low Confidence**: Causal interpretation of KG edge colors assumes clean separation between dataset and model biases that may not hold with entangled concepts

## Next Checks

1. **Ablation on Linear vs. Non-linear Probes**: Run BAGEL with both logistic regression and a small MLP probe on the same concept sets. Compare recall scores and JS divergence patterns—significant performance gaps would indicate when linear assumptions break down.

2. **Concept Discovery Experiment**: Apply BAGEL to a dataset with known hidden biases (e.g., synthetic data with watermark patterns). Verify whether manually specified concept sets miss important discovered features, and quantify the information lost by not allowing automatic concept generation.

3. **Cross-dataset Generalization Test**: Apply the same pretrained model to two related datasets with different bias structures (e.g., Cats/Dogs with color bias vs. without). Measure how consistently BAGEL identifies the same concepts across datasets—high variance would suggest sensitivity to dataset-specific noise rather than robust concept detection.