---
ver: rpa2
title: 'Task Shift: From Classification to Regression in Overparameterized Linear
  Models'
arxiv_id: '2502.13285'
source_url: https://arxiv.org/abs/2502.13285
tags:
- regression
- lemma
- have
- task
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores task shift from classification to regression
  in overparameterized linear models, where the goal is to transfer knowledge from
  a simpler classification task to a more complex regression task on the same data
  distribution. The authors prove that zero-shot task shift is impossible for both
  sparse and random signal models under any Gaussian covariate distribution.
---

# Task Shift: From Classification to Regression in Overparameterized Linear Models

## Quick Facts
- arXiv ID: 2502.13285
- Source URL: https://arxiv.org/abs/2502.13285
- Reference count: 40
- One-line primary result: Zero-shot task shift from classification to regression is impossible, but structured attenuation of the minimum $\ell_2$-norm interpolator enables successful few-shot learning.

## Executive Summary
This paper explores task shift from binary classification to linear regression in overparameterized linear models. The authors prove that zero-shot task shift is impossible under any Gaussian covariate distribution for both sparse and random signal models. However, they show that the minimum $\ell_2$-norm interpolator exhibits structured attenuation, which enables successful few-shot task shift with limited regression data. The key insight is that while the MNI cannot recover the exact regression signal, it preserves the sign and relative magnitude structure, allowing support recovery and efficient few-shot learning.

## Method Summary
The paper studies task shift from binary classification to linear regression using overparameterized linear models where $d \gg n$. The method computes the minimum $\ell_2$-norm interpolator (MNI) $\hat{\theta} = \mathbf{X}^\top (\mathbf{X}\mathbf{X}^\top)^{-1} \hat{\mathbf{y}}$ on classification labels, then identifies the support of a sparse signal through thresholding. For few-shot tasks, the algorithm restricts the regression problem to the recovered support and performs least-squares optimization using $m$ regression samples. The theoretical analysis leverages fine-grained characterization of individual parameters arising from minimum-norm interpolation.

## Key Results
- Zero-shot task shift is impossible for both sparse and random signal models under Gaussian covariates
- Minimum $\ell_2$-norm interpolator exhibits structured attenuation that preserves signal sign but shrinks magnitude
- Support recovery from classification MNI enables $O(t/m)$ regression error with $m$ noisy regression examples
- Fundamental tradeoff exists between regression bias and task shift error for random signals

## Why This Works (Mechanism)

### Mechanism 1: Structured Attenuation of Classification Signals
The minimum $\ell_2$-norm interpolator trained on binary classification labels attenuates the true regression signal. The estimated coefficients $\hat{\theta}_j$ converge to values that preserve sign but shrink magnitude, scaling inversely with the signal strength $\|\Sigma^{1/2}\theta^*\|_2$. This structured attenuation is the key to why few-shot learning becomes possible.

### Mechanism 2: Survival vs. Contamination Separation
For sparse signals, the support can be recovered because "survival" (signal preservation) degrades slower than "contamination" (false signals). The survival $SU_j \approx \hat{\theta}_j / \theta^*_j$ and contamination $CN$ exhibit different decay rates, with true support indices decaying slower than non-support indices. This separation allows simple thresholding to distinguish the true support.

### Mechanism 3: Dimensionality Reduction via Support Estimation
Identifying the support reduces the regression problem from $d$ dimensions to $t$ dimensions (sparsity). Once the support $S$ is estimated, standard least-squares regression on these $t$ dimensions bypasses the curse of dimensionality, achieving $O(t/m)$ error with $m$ regression samples.

## Foundational Learning

- **Concept:** Minimum $\ell_2$-Norm Interpolation (MNI)
  - **Why needed here:** This is the base estimator whose properties determine task shift performance
  - **Quick check question:** Can you explain why the MNI solution tends to have smaller norms than the true signal when interpolating binary labels?

- **Concept:** Effective Rank and Spiked Covariance
  - **Why needed here:** The paper relies on specific covariance structures to define "effective rank" ($r_k(\Sigma)$), which dictates survival and contamination rates
  - **Quick check question:** How does the decay rate of covariance eigenvalues affect the "survival" of the true signal in high dimensions?

- **Concept:** One-Bit Compressive Sensing
  - **Why needed here:** This work bridges one-bit compressive sensing and high-dimensional regression
  - **Quick check question:** How does the task-shift setting differ from traditional one-bit compressive sensing regarding control over measurement design?

## Architecture Onboarding

- **Component map:** Classification Data -> MNI Computation -> Support Estimation -> Few-shot Regression
- **Critical path:** Train MNI on binary labels $\hat{y}$ -> Extract top-$t$ indices as support $S$ -> Fit linear regression on reduced space using $m$ regression samples
- **Design tradeoffs:**
  - Sparsity vs. Density: Algorithm requires $t$ to be known or estimated; fails for dense signals
  - Supervised vs. Zero-shot: Zero-shot has constant bias $O(1)$ error; few-shot requires $m$ samples for $O(t/m)$ error
  - Covariance Knowledge: Best performance when covariance spectrum $\{\lambda_j\}$ is known for threshold adjustment
- **Failure signatures:**
  - Constant Bias: Without regression samples, error plateaus at signal-dependent constant level
  - Support Leakage: High noise may cause non-support indices to masquerade as signal
- **First 3 experiments:**
  1. Train MNI on classification data, measure regression MSE to observe constant bias floor
  2. Plot magnitudes of $\hat{\theta}$ indices to visualize structured attenuation
  3. Run full pipeline (Alg 1 + Alg 2), plot regression MSE vs. number of few-shot samples to verify $O(1/m)$ decay

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can zero-shot task shift be achieved by shifting from binary classification to alternative formulations, such as multi-class classification?
- **Basis in paper:** The Discussion section explicitly asks about shifting from multiclass classification to regression
- **Why unresolved:** The authors prove zero-shot shift is impossible for binary classification but don't analyze the multi-class setting
- **What evidence would resolve it:** Theoretical proof or empirical results showing successful zero-shot transfer in a multi-class setting

### Open Question 2
- **Question:** Is it possible to design a successful few-shot procedure without assuming signal sparsity or diagonal data covariance?
- **Basis in paper:** The Discussion section lists this as a close-ended open question
- **Why unresolved:** The proposed support recovery algorithm requires signal sparsity and diagonal covariance
- **What evidence would resolve it:** An algorithm with provable regression consistency guarantees for dense signals and non-diagonal covariance matrices

### Open Question 3
- **Question:** How does task shift performance change when using minimum $\ell_p$-norm estimators for $p \neq 2$?
- **Basis in paper:** The Discussion section identifies studying task shift for minimum $\ell_p$-norm estimators as an open direction
- **Why unresolved:** The analysis relies on specific structured attenuation properties of the minimum $\ell_2$-norm interpolator
- **What evidence would resolve it:** Characterization of task shift error for minimum $\ell_1$ or $\ell_\infty$ norm interpolators

## Limitations

- The structured attenuation mechanism is proven under Gaussian covariates but robustness to non-Gaussian distributions remains unclear
- Few-shot algorithm performance critically depends on accurate support recovery, with sensitivity to misestimation not extensively explored
- The paper focuses on linear models, limiting direct applicability to modern nonlinear ML systems

## Confidence

- **High confidence**: Impossibility of zero-shot task shift for both sparse and random signal models under Gaussian covariates (Theorem 8)
- **Medium confidence**: Structured attenuation mechanism enabling few-shot learning (theoretical soundness but practical effectiveness varies)
- **Medium confidence**: Support recovery algorithm achieving exact recovery under stated conditions (theoretical guarantees strong but empirical validation needed)

## Next Checks

1. **Distribution Robustness Test**: Evaluate the few-shot algorithm on non-Gaussian covariates (t-distribution or mixture models) to assess robustness beyond Gaussian assumptions
2. **Sensitivity Analysis**: Systematically vary sparsity level $t$ and noise levels to quantify how misestimation affects support recovery and regression performance
3. **Deep Learning Extension**: Implement a simple nonlinear version (e.g., two-layer ReLU network) to test whether structured attenuation extends beyond linear models