---
ver: rpa2
title: 'ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching'
arxiv_id: '2509.16857'
source_url: https://arxiv.org/abs/2509.16857
tags:
- cache
- memory
- shadowserve
- data
- smartnic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShadowServe addresses the bottleneck of KV cache fetching in distributed
  prefix caching for LLM serving, particularly under limited bandwidth. It offloads
  the data plane to a SmartNIC, eliminating interference between decompression and
  model computation on the GPU.
---

# ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching

## Quick Facts
- arXiv ID: 2509.16857
- Source URL: https://arxiv.org/abs/2509.16857
- Reference count: 40
- Primary result: Achieves up to 2.2x lower loaded TPOT and 1.38x lower TTFT in low-bandwidth scenarios via SmartNIC offloading

## Executive Summary
ShadowServe addresses the bottleneck of KV cache fetching in distributed prefix caching for LLM serving, particularly under limited bandwidth. It offloads the data plane to a SmartNIC, eliminating interference between decompression and model computation on the GPU. To overcome SmartNIC resource constraints, ShadowServe employs a chunked pipeline for parallel processing and a minimal-copy memory management scheme. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (≤20 Gbps), and translates to up to 1.35x higher throughput. The performance is limited in high-bandwidth settings due to SmartNIC memory subsystem bottlenecks.

## Method Summary
ShadowServe introduces a novel SmartNIC-based data plane to mitigate KV cache fetching interference in distributed prefix caching, achieving notable improvements in loaded TPOT and TTFT in low-bandwidth environments. The core hypothesis—that offloading KV cache fetching to a SmartNIC reduces GPU-side interference and improves latency—is supported by controlled experiments showing up to 2.2x lower loaded TPOT and 1.38x lower TTFT under limited bandwidth (≤20 Gbps). However, the system's performance degrades in high-bandwidth scenarios, where SmartNIC memory subsystem bottlenecks become limiting factors. This suggests the approach is most beneficial in constrained network conditions, not universally across all bandwidth regimes.

## Key Results
- Up to 2.2x lower loaded TPOT compared to state-of-the-art solutions
- Up to 1.38x lower TTFT in low-bandwidth scenarios (≤20 Gbps)
- Up to 1.35x higher throughput due to reduced fetching interference

## Why This Works (Mechanism)
ShadowServe's SmartNIC offloading isolates KV cache fetching from GPU computation, eliminating resource contention. The chunked pipeline enables parallel decompression and transmission, maximizing SmartNIC throughput. Minimal-copy memory management reduces CPU overhead and memory bandwidth usage. Together, these mechanisms ensure consistent low latency in distributed prefix caching under bandwidth constraints.

## Foundational Learning
1. **Distributed Prefix Caching** - why needed: Enables efficient memory usage by sharing prefix KV caches across requests; quick check: Confirm overlapping prefixes exist in workload.
2. **KV Cache Fetching Bottleneck** - why needed: Decompression and network transfer interfere with GPU computation; quick check: Measure GPU idle time during cache fetching.
3. **SmartNIC Offloading** - why needed: Moves data plane processing off GPU to avoid interference; quick check: Compare GPU utilization with/without SmartNIC offloading.
4. **Chunked Pipeline** - why needed: Overcomes SmartNIC resource limits by parallelizing decompression; quick check: Measure throughput scaling with chunk count.
5. **Minimal-Copy Memory Management** - why needed: Reduces CPU overhead and memory bandwidth pressure; quick check: Profile CPU cycles per token with different memory schemes.
6. **Network Bandwidth Sensitivity** - why needed: Performance gains are most pronounced under limited bandwidth; quick check: Vary network speed and measure latency impact.

## Architecture Onboarding

**Component Map:** Request Scheduler -> SmartNIC Data Plane -> GPU Compute -> Memory Pool

**Critical Path:** Request arrival → SmartNIC KV fetch & decompress → GPU model inference → Response generation

**Design Tradeoffs:** SmartNIC resource constraints vs. parallelism (chunked pipeline); memory efficiency vs. copying overhead (minimal-copy scheme)

**Failure Signatures:** High GPU idle time indicates SmartNIC bottleneck; SmartNIC memory exhaustion causes pipeline stalls

**First Experiments:**
1. Measure GPU utilization and TTFT with/without SmartNIC offloading under 10 Gbps network
2. Vary chunk count in chunked pipeline and measure SmartNIC throughput and latency
3. Profile CPU cycles and memory bandwidth with minimal-copy vs. naive memory management

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades in high-bandwidth scenarios due to SmartNIC memory subsystem bottlenecks
- Evaluation limited to specific model sizes (7B and 34B parameters) and batch sizes (up to 128)
- Scalability and robustness under production-grade, heterogeneous workloads not fully characterized

## Confidence
- **Loaded TPOT improvement (2.2x)**: High - directly measured and reproducible under stated conditions
- **Throughput gain (1.35x)**: Medium - derived from latency improvements, depends on workload assumptions
- **Performance at scale/high-bandwidth**: Low - explicit limitations and bottlenecks identified

## Next Checks
1. Test ShadowServe's performance with heterogeneous SmartNICs and varying memory configurations to assess robustness beyond the specific hardware used in the study
2. Evaluate the system under diverse workload patterns, including bursty traffic and variable batch sizes, to determine real-world applicability
3. Benchmark against state-of-the-art solutions in high-bandwidth scenarios (≥100 Gbps) to quantify the SmartNIC memory bottleneck and identify potential mitigations