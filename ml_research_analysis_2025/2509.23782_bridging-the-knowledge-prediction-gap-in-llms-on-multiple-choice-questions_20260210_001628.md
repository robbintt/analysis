---
ver: rpa2
title: Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions
arxiv_id: '2509.23782'
source_url: https://arxiv.org/abs/2509.23782
tags:
- knowledge
- kappa
- knowledge-prediction
- prediction
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a widespread knowledge-prediction gap in
  LLMs across multiple-choice question benchmarks, where models often fail to utilize
  knowledge encoded in their hidden representations despite possessing it linearly
  accessibly. The authors conduct a probing analysis revealing that residual streams
  contain misaligned knowledge and prediction subspaces - the knowledge basis encodes
  ground-truth probabilities while the prediction basis encodes model-generated choices.
---

# Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions

## Quick Facts
- arXiv ID: 2509.23782
- Source URL: https://arxiv.org/abs/2509.23782
- Reference count: 40
- Primary result: Introduces KAPPA to align misaligned knowledge and prediction subspaces in LLM residual streams, substantially improving MCQ accuracy

## Executive Summary
This paper identifies a widespread knowledge-prediction gap in LLMs where models encode correct answers in their hidden representations but fail to access this knowledge during generation. Through probing analysis, the authors reveal that residual streams contain distinct knowledge and prediction subspaces that are geometrically misaligned - the knowledge basis encodes ground-truth probabilities while the prediction basis encodes model-generated choices. They introduce KAPPA, an inference-time intervention that applies minimal affine transformations to align these subspaces, substantially improving accuracy on multiple-choice reformulations of benchmarks like BBH, ARC-Challenge, and others.

## Method Summary
KAPPA addresses the knowledge-prediction gap by training two linear classifiers (knowledge and prediction probes) on LLM residual stream activations to identify misaligned subspaces. At inference, it applies instance-specific affine transformations to the residual stream that minimally adjust hidden states to align prediction coordinates with knowledge coordinates. The method operates at inference time without retraining, generalizes to free-form settings, and shows partial cross-task transfer when probing datasets requiring similar skills. The approach is evaluated across multiple-choice reformulations of several benchmarks using accuracy, agreement rate, and KL divergence metrics.

## Key Results
- Knowledge probes consistently outperform base LLM predictions on MCQ benchmarks, revealing the knowledge-prediction gap
- KAPPA substantially improves accuracy and agreement metrics compared to baselines, with gains up to 17.5% points on truthfulness tasks
- The approach generalizes to free-form settings but shows performance degradation compared to MCQ-specific applications
- Partial cross-dataset transfer works when tasks require similar skills, but subspaces are geometrically distinct for different task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode correct answer information linearly in residual stream activations, but fail to access this knowledge during generation.
- Mechanism: Linear probes trained on residual stream activations predict correct answers more accurately than the model's own generation. This indicates knowledge exists in a linearly accessible form but is not utilized in the forward pass to produce outputs.
- Core assumption: Linearly accessible knowledge represents knowledge the model "has" and could theoretically deploy; nonlinear encodings may exist but are harder to exploit.
- Evidence anchors:
  - [abstract]: "models often fail to utilize knowledge encoded in their hidden representations despite possessing it linearly accessibly"
  - [Section 3.1]: "knowledge probes consistently outperform the LLMs' generation accuracy, with positive accuracy gains (∆ACC) observed across both model families and benchmarks"
  - [corpus]: Weak support; corpus papers focus on MCQ generation quality, not internal representation analysis.
- Break condition: If knowledge is only encoded nonlinearly, linear probes would fail and this mechanism would not apply.

### Mechanism 2
- Claim: The knowledge-prediction gap arises from geometric misalignment between distinct knowledge and prediction subspaces within the residual stream.
- Mechanism: Probe weight vectors define k-dimensional subspaces. Hidden states project to different coordinates in each subspace—the knowledge subspace encodes ground-truth probabilities, while the prediction subspace encodes the model's output distribution. Systematic misalignment between these coordinates produces incorrect predictions despite correct knowledge encoding.
- Core assumption: The geometric relationship between subspaces causally determines whether predictions reflect encoded knowledge.
- Evidence anchors:
  - [abstract]: "residual streams contain misaligned knowledge and prediction subspaces"
  - [Section 3.2]: "datasets identified with a large knowledge-prediction gap... exhibit substantial off-diagonal misalignment between the two logits, whereas datasets with a smaller gap... show much stronger diagonal alignment"
  - [corpus]: No direct corpus support for subspace geometry mechanisms.
- Break condition: If misalignment is correlational rather than causal, intervening on alignment would not change predictions.

### Mechanism 3
- Claim: Instance-specific affine transformations that align prediction coordinates with knowledge coordinates causally reduce the knowledge-prediction gap.
- Mechanism: KAPPA solves a constrained optimization—minimize ||h' - h||² subject to W_pred^T h' aligning with W_know^T h. The closed-form solution modifies hidden states only within the prediction subspace, preserving orthogonal components.
- Core assumption: Causal control flows through these subspaces; modifying alignment changes output predictions.
- Evidence anchors:
  - [Section 4.1]: "This update modifies the hidden state only within the prediction subspace, leaving orthogonal components unchanged"
  - [Section 4.2]: "KAPPA consistently improves over base models... by significantly increasing agreement between the knowledge probe and the model's final predictions (up to 17.5% points)"
  - [corpus]: No comparable alignment intervention methods in corpus.
- Break condition: If the transformation disrupts other critical information, performance could degrade.

## Foundational Learning

- Concept: **Linear Probing of Hidden Representations**
  - Why needed here: KAPPA requires training and interpreting linear classifiers on LLM hidden states to identify knowledge/prediction subspaces.
  - Quick check question: Given activations h ∈ R^d and labels y ∈ {1,...,k}, what probe architecture maps h to a k-class distribution?

- Concept: **Residual Stream Structure in Transformers**
  - Why needed here: KAPPA operates on residual stream activations; understanding where information accumulates explains why mid-to-late layers are targeted.
  - Quick check question: How does the residual stream at layer l relate to the input embedding and outputs of previous layers?

- Concept: **Constrained Optimization via Lagrange Multipliers**
  - Why needed here: KAPPA's closed-form update derives from minimizing perturbation subject to alignment constraints.
  - Quick check question: How do you minimize f(x) subject to g(x) = 0 using Lagrange multipliers?

## Architecture Onboarding

- Component map:
  - **Knowledge Probe**: Linear k-class classifier (W_know, b_know) predicting ground-truth from activations
  - **Prediction Probe**: Linear k-class classifier (W_pred, b_pred) predicting model outputs from activations
  - **Layer Selector**: Identifies layers with high knowledge accuracy and >85% prediction accuracy
  - **Affine Transform Engine**: Computes h' = h + W_pred G^(-1)(p_target - W_pred^T h)
  - **Inference Hook**: Applies transformation at selected layers during forward pass

- Critical path:
  1. Extract residual stream activations from training data at all layers (last token position)
  2. Train knowledge and prediction probes independently per layer
  3. Select intervention layers (top-n consecutive high-performing layers)
  4. At inference, compute and apply transformation to residual stream
  5. Continue generation with modified activations

- Design tradeoffs:
  - **More layers** → stronger alignment but higher disruption risk
  - **Higher α, β** → stronger knowledge enforcement but may override legitimate uncertainty
  - **Task-specific probes** → better performance but require labeled data; transferred probes generalize partially

- Failure signatures:
  - **Accuracy gains without agreement gains**: Probe not capturing relevant knowledge direction
  - **Degraded fluency**: Transformation disrupting orthogonal subspaces; reduce α or intervention layers
  - **No cross-dataset transfer**: Fundamentally different subspace geometries; use task-specific probes
  - **Negative gains on high-performing tasks**: Over-correction (observed on GSM8K free-form)

- First 3 experiments:
  1. Replicate gap quantification: Train probes on TruthfulQA for a 7B model, compute AGR and KLD metrics.
  2. Single-layer KAPPA ablation: Apply at top knowledge-accuracy layer only, measure accuracy/agreement gains.
  3. Cross-dataset transfer: Train probes on BBH-Algorithmic, apply to BBH-NLP, check if similar-skill transfer yields gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can nonlinearly encoded knowledge be extracted and aligned with model predictions?
- Basis in paper: [explicit] The conclusion explicitly identifies "understanding how nonlinearly encoded knowledge can be aligned" as an important extension remaining beyond the scope of the current work.
- Why unresolved: KAPPA relies on linear probes to identify knowledge subspaces, but the authors acknowledge that models may encode richer knowledge in nonlinear forms that this methodology cannot access or utilize.
- What evidence would resolve it: An intervention method utilizing nonlinear probes (e.g., MLPs) that successfully elicits correct answers in cases where linear probing and KAPPA fail to bridge the gap.

### Open Question 2
- Question: What specific mechanisms during the training process cause the initial misalignment between the knowledge and prediction subspaces?
- Basis in paper: [inferred] The paper hypothesizes in Section 3.1 that training dynamics (e.g., "hasty answering") drive the gap, but the analysis focuses on geometric characterization and inference-time correction rather than identifying the causal training factors.
- Why unresolved: While the paper establishes the geometric existence of the gap, it does not isolate which training objectives (e.g., instruction tuning vs. pre-training) or data properties induce the subspace misalignment.
- What evidence would resolve it: A causal study correlating the magnitude of the knowledge-prediction gap with specific variations in training protocols (e.g., RLHF vs. SFT).

### Open Question 3
- Question: Can a universal knowledge subspace be identified that transfers across semantically distinct tasks without requiring task-specific probe training?
- Basis in paper: [inferred] Section 5.1 demonstrates "limited transferability" of subspaces across datasets, showing that subspaces are geometrically distinct for different tasks (e.g., reasoning vs. truthfulness).
- Why unresolved: KAPPA currently requires training probes on the specific target dataset, implying that a general "knowledge alignment" direction valid across all domains has not yet been found.
- What evidence would resolve it: Identifying a single set of probe weights or intervention vectors that improves agreement metrics across diverse benchmarks like GSM8K, MMLU, and TruthfulQA simultaneously.

## Limitations

- The knowledge-prediction gap analysis rests on the assumption that linear probes capture all relevant knowledge the model possesses, potentially missing nonlinear encodings
- The geometric alignment framework assumes causal rather than merely correlational relationships between knowledge and prediction subspaces
- The method requires labeled training data for probe training, limiting applicability to unsupervised scenarios
- The method's success on MCQs may not generalize to free-form generation where the output space is not discretely constrained

## Confidence

- **High Confidence**: The empirical observation of a knowledge-prediction gap (knowledge probes outperforming model predictions) is well-supported by multiple benchmarks and model families
- **Medium Confidence**: The causal interpretation that subspace misalignment drives incorrect predictions, and that KAPPA's alignment intervention fixes this causally, is supported by ablation studies but could involve additional confounding factors
- **Medium Confidence**: The generalization to free-form settings works but with performance degradation, suggesting the MCQ-specific assumptions (discrete output space, option selection) provide crucial inductive biases

## Next Checks

1. **Nonlinear Knowledge Probes**: Train nonlinear classifiers (MLP, attention-based) on the same residual stream activations to test whether knowledge exists in forms beyond linear accessibility. Compare their accuracy to linear probes and the base model to bound what "knowledge possession" means.

2. **Ablation of Orthogonal Components**: Modify KAPPA to preserve only the prediction subspace (zero out orthogonal components) versus the current approach that preserves them. Measure whether performance gains come from alignment versus information preservation, isolating the causal mechanism.

3. **Cross-Model Transfer**: Train probes on Llama 3.1 and apply KAPPA to Qwen2.5 (or vice versa) to test whether subspace geometries are model-specific or task-specific. This would clarify whether the approach requires model-specific probe training or can generalize across architectures with similar pretraining objectives.