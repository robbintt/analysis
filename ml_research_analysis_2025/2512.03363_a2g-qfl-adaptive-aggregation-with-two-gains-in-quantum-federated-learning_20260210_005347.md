---
ver: rpa2
title: 'A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning'
arxiv_id: '2512.03363'
source_url: https://arxiv.org/abs/2512.03363
tags:
- quantum
- geometry
- aggregation
- global
- gain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces A2G-QFL, an adaptive aggregation framework
  for federated learning in quantum-enabled and heterogeneous classical networks.
  A2G addresses performance degradation caused by non-uniform client quality, stochastic
  teleportation fidelity, device instability, and geometric mismatch between local
  and global models.
---

# A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning

## Quick Facts
- arXiv ID: 2512.03363
- Source URL: https://arxiv.org/abs/2512.03363
- Reference count: 20
- One-line primary result: A2G achieves 68.25% best accuracy, a 13.65 percentage point (25% relative) improvement over β=1.0 in quantum federated learning under heterogeneous conditions.

## Executive Summary
A2G-QFL introduces an adaptive aggregation framework for federated learning in quantum-enabled and heterogeneous classical networks. The method uses two gains—a QoS gain that modulates client importance based on teleportation fidelity, latency, and instability, and a geometry gain that adjusts global updates according to the curvature and non-Euclidean structure of the parameter space. Experiments on a quantum-classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions. For example, with geometry gain β = 0.05, A2G achieved 68.25% best accuracy, representing a 13.65 percentage point (25% relative) improvement over β = 1.0. The framework recovers FedAvg, QoS-aware averaging, and manifold-based aggregation as special cases and provides convergence guarantees under smoothness and bounded-variance assumptions.

## Method Summary
A2G-QFL implements a dual-gain aggregation rule: QoS gain computes trust weights per client based on teleportation fidelity, latency, and instability, while geometry gain β controls the blending between Euclidean and manifold-aware updates. The global model is updated via a tangent-space aggregation followed by manifold correction using Log/Exp maps. Experiments use a quantum-classical hybrid setup with binary classification tasks on breast cancer datasets, employing a SamplerQNN variational classifier with SPSA local optimization. Teleportation noise is modeled as a bit-flip channel, and non-IID data partitions are created via label and quantity skew.

## Key Results
- A2G with β = 0.05 achieves 68.25% best accuracy vs. 54.60% at β = 1.0, a 13.65 percentage point improvement.
- QoS gain enables filtering of unreliable clients, improving robustness under high teleportation noise (p ≤ 0.12).
- A2G shows larger accuracy gains under quantity-skew partitions where client quality varies more than under label-skew.
- Convergence is guaranteed under smoothness and bounded-variance assumptions with O(1/T) rate.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: QoS-sensitive trust weighting filters unreliable clients based on teleportation fidelity, latency, and instability.
- **Mechanism**: Each client's aggregation weight is computed as `qi,t = F^α_i,t / (τi,t + ε)^γ(σ²_i,t + ε)^δ`, then normalized. High-fidelity, low-latency, stable clients receive larger weights; noisy or delayed clients are downweighted. The exponents (α, γ, δ) control sensitivity to each QoS factor.
- **Core assumption**: Teleportation fidelity, latency, and instability are measurable per-round and correlate with client reliability (Assumption III.4–III.5).
- **Evidence anchors**:
  - [abstract] "modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability"
  - [Section II.C, Eq. 2–3] Formal definition of QoS factor and normalized trust weights
  - [corpus] Related work (Fedcompass, Noise-Resilient Quantum Aggregation) confirms QoS-aware aggregation improves robustness under heterogeneous conditions
- **Break condition**: If QoS indicators do not correlate with actual client contribution quality, or if measurement overhead exceeds communication budget, the weighting provides no benefit over uniform averaging.

### Mechanism 2
- **Claim**: Geometry gain (β) interpolates between Euclidean and manifold-aware aggregation, suppressing client drift on curved parameter spaces.
- **Mechanism**: The update computes a QoS-weighted tangent vector `vt = Σw_i,t·Log_θt(θi,t)` on the manifold, then applies `θt+1 = θt + β·Ψ(θt,{θi,t})` where `Ψ = Exp_θt(vt) - θt`. Small β (e.g., 0.05) provides gentle curvature correction; β=1 is fully Riemannian; β=0 recovers Euclidean FedAvg.
- **Core assumption**: Model parameters lie on a manifold (e.g., toroidal for quantum circuit angles) where Euclidean averaging misaligns with intrinsic geometry (Section I.A).
- **Evidence anchors**:
  - [abstract] "regulates geometric blending through a geometry gain"
  - [Section II.D, Eq. 5–7] Manifold correction via Log/Exp maps
  - [Section V.B, Table II] β=0.05 achieves 68.25% accuracy vs. 61.90% for β=1.0 under quantity-skew
  - [corpus] Weak direct corpus evidence for geometry gain specifically; related papers focus on noise resilience rather than manifold structure
- **Break condition**: If the parameter space is approximately Euclidean (e.g., standard neural networks without periodic parameters), or if β is set too large causing over-correction, performance degrades.

### Mechanism 3
- **Claim**: Convergence is guaranteed under smoothness and bounded-variance assumptions with explicit O(1/T) rate.
- **Mechanism**: Under L-smoothness (Assumption III.1), unbiased stochastic gradients (III.2), bounded second moments (III.3), finite QoS factors (III.4), and client independence (III.5), the A2G update satisfies `1/T ΣE[‖∇f(θt)‖²] ≤ O(1/Tκmin) + O(ηG²) + O(βρ³)`. Small β controls the curvature penalty term.
- **Core assumption**: Gradients are unbiased and bounded; client QoS and gradient noise are independent; dispersion ρ is bounded.
- **Evidence anchors**:
  - [abstract] "establish convergence guarantees under smoothness and bounded-variance assumptions"
  - [Section III.C–D, Lemma III.6, Theorem III.7] Full derivation with explicit bounds
  - [corpus] No direct corpus evidence for this specific convergence rate; related papers do not provide theoretical guarantees
- **Break condition**: If local losses are non-smooth, gradients have unbounded variance, or clients exhibit coordinated (non-independent) failures, the bound may not hold.

## Foundational Learning

- **Concept: Federated Averaging (FedAvg)**
  - Why needed here: A2G generalizes FedAvg; understanding the baseline reveals why geometry and QoS corrections help.
  - Quick check question: Can you explain why FedAvg assumes Euclidean topology and how client drift arises in non-IID settings?

- **Concept: Riemannian Manifolds and Exponential/Logarithm Maps**
  - Why needed here: The geometry gain operates via Log/Exp maps to compute geodesic-aware updates on curved spaces (e.g., tori for quantum angles).
  - Quick check question: Given a point θt on a manifold M and another point θi,t, what does Log_θt(θi,t) represent geometrically?

- **Concept: Quantum Teleportation Fidelity**
  - Why needed here: The QoS gain explicitly uses teleportation fidelity as a trust signal; lower fidelity implies noisier parameter transmission.
  - Quick check question: How does teleportation fidelity differ from classical link reliability, and why might it fluctuate stochastically?

## Architecture Onboarding

- **Component map**:
  - ClientUpdate (Alg. 2) -> QoSWeights (Alg. 3) -> GeoA2GUpdate (Alg. 4) -> Server

- **Critical path**:
  1. Server broadcasts θt to all K clients
  2. Each client runs local optimizer (e.g., SPSA), measures Fi,t, τi,t, σ²i,t
  3. Server computes QoS weights w_i,t per Eq. 2–3
  4. Server computes tangent vector vt and manifold correction Ψ
  5. Server updates global model via Eq. 8–9

- **Design tradeoffs**:
  - β too small (→0): Approaches FedAvg, losing geometry correction benefits
  - β too large (→1): Aggressive manifold correction may amplify noise, causing volatility (see Table II: β=1.0 underperforms β=0.05 by ~6% absolute)
  - η>0: Enables server-side gradient updates but increases communication and may be unstable for quantum models; experiments set η=0 for efficiency
  - QoS exponent tuning: High α heavily penalizes low-fidelity clients but may discard useful data from noisy but informative nodes

- **Failure signatures**:
  - Accuracy plateaus or oscillates: β may be too large; try reducing to 0.05–0.1
  - Sudden accuracy drop in specific rounds: Check for outlier clients with extremely low Fi,t or high τi,t corrupting QoS weights
  - Divergence on quantum models: Verify manifold maps are correctly implemented for toroidal/circular parameters; Euclidean fallback may misalign
  - No improvement over FedAvg baseline: Confirm QoS indicators are varying; uniform Fi,t ≈ 1, τi,t ≈ constant will cause A2G to degenerate to FedAvg

- **First 3 experiments**:
  1. **β sweep under fixed noise**: Run A2G with β ∈ {0.05, 0.1, 0.3, 0.5, 0.7, 1.0}, α=γ=δ=1, medium teleportation noise (p=0.06). Measure best/final/mean accuracy over 20–30 epochs. Expect optimal β ≈ 0.05–0.1.
  2. **Noise robustness test**: Fix β=0.05, vary teleportation error probability p ∈ {0.01, 0.06, 0.12}. Compare A2G vs. FedAvg baseline (no teleportation). Expect A2G to maintain advantage even at p=0.12.
  3. **Non-IID partition stress test**: Compare label-skew vs. quantity-skew partitions under A2G (β=0.05) vs. FedAvg. Expect A2G to show larger gains under quantity-skew where client reliability varies more.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does A2G performance differ when utilizing a non-zero server-side learning rate ($\eta > 0$) compared to the geometry-driven ($\eta = 0$) approach used in the experiments?
- **Basis in paper:** [explicit] The methodology section states the full formulation "supports nonzero $\eta$ when server-side gradients are desired in future work," while the experiments deliberately disable them.
- **Why unresolved:** The authors focus on a gradient-free server update to avoid the computational cost and noise sensitivity of server-side gradients in variational quantum circuits, leaving the active use of aggregated gradients untested.
- **What evidence would resolve it:** Empirical results from experiments run with $\eta > 0$, analyzing convergence speed and stability relative to the $\eta = 0$ baseline.

### Open Question 2
- **Question:** Can the geometry gain $\beta$ be dynamically adapted during training rather than fixed as a static hyperparameter?
- **Basis in paper:** [inferred] The results demonstrate high sensitivity to $\beta$ (with $\beta=0.05$ outperforming $\beta=1.0$ by 25%), suggesting that the optimal curvature correction changes as the model converges.
- **Why unresolved:** The paper establishes the existence of an optimal "gentle correction" regime but does not propose a mechanism to automate the tuning of $\beta$ as the dispersion $\rho$ or loss landscape changes over time.
- **What evidence would resolve it:** A comparative study where $\beta$ is adjusted via a feedback loop (e.g., based on client dispersion) versus the static approach.

### Open Question 3
- **Question:** Does the convergence guarantee hold when Assumption III.5 (client independence) is violated by correlated quantum noise or cross-talk in larger-scale networks?
- **Basis in paper:** [inferred] The theoretical convergence analysis relies on the assumption that client gradients and QoS factors are independent, which may not hold in physical quantum networks where environmental noise often correlates across clients.
- **Why unresolved:** The theoretical bounds depend on this independence to control variance terms, but the empirical validation uses a small number of clients ($N=5$), potentially masking correlation effects present in larger deployments.
- **What evidence would resolve it:** A convergence analysis or empirical trial in a simulated environment where noise is deliberately correlated across clients.

## Limitations
- The exact quantum model architecture (qubit count, ansatz structure) and QoS simulation methodology are not fully specified, hindering direct reproduction.
- The convergence proof relies on strong assumptions (bounded client independence, smoothness) that may not hold under extreme heterogeneity or non-IID partitions.
- The theoretical convergence guarantee does not account for correlated quantum noise or cross-talk in larger-scale networks.

## Confidence
- **High**: The mechanism of QoS gain for client importance modulation is well-supported by the equations and aligns with established federated learning literature.
- **Medium**: The geometry gain's effect on manifold-aware aggregation is plausible but has limited direct experimental validation; most evidence comes from comparison of β values rather than ablation of geometry gain alone.
- **Medium**: Convergence guarantees under smoothness and bounded-variance assumptions are formally derived, but the assumptions are strong and may not hold in practice.

## Next Checks
1. **Geometry Gain Ablation**: Run A2G with β=1.0 (fully Euclidean) and β=0.05 on a simple hybrid classical-quantum model under moderate noise. Measure if the 13.65 percentage point improvement is reproducible.
2. **QoS Weight Distribution**: Verify that QoS weights w_i,t have meaningful spread (min/max ratio >2) under simulated fidelity/latency ranges. If weights are nearly uniform, A2G should not outperform FedAvg.
3. **Non-IID Stress Test**: Compare A2G (β=0.05) vs. FedAvg under both label-skew and quantity-skew partitions. Confirm A2G shows larger gains under quantity-skew where client reliability varies more.