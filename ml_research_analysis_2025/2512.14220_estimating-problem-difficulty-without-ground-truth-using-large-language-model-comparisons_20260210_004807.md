---
ver: rpa2
title: Estimating problem difficulty without ground truth using Large Language Model
  comparisons
arxiv_id: '2512.14220'
source_url: https://arxiv.org/abs/2512.14220
tags:
- difficulty
- compare
- human
- problem
- measures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLM compare introduces a new method for estimating problem difficulty\
  \ without ground truth by using pairwise comparisons made by a large language model,\
  \ then computing Bradley-Terry scores from the comparison outcomes. The method is\
  \ validated against human annotations, human and LLM performance scores, and LLM\
  \ labels across three datasets, showing strong alignment (Pearson r \u2265 0.80)\
  \ and robustness to hallucinations with less than 6% degradation even with 10% noise\
  \ injection."
---

# Estimating problem difficulty without ground truth using Large Language Model comparisons

## Quick Facts
- **arXiv ID:** 2512.14220
- **Source URL:** https://arxiv.org/abs/2512.14220
- **Reference count:** 40
- **Primary result:** LLM compare estimates problem difficulty without ground truth by pairwise LLM comparisons, achieving Pearson r ≥ 0.80 with human annotations

## Executive Summary
This paper introduces LLM compare, a method for estimating problem difficulty without requiring ground truth answers or human performance data. The approach uses pairwise comparisons between problems made by large language models, then applies Bradley-Terry scoring to aggregate these relative judgments into continuous difficulty estimates. The method is validated across three datasets (JEE Advanced Maths 2024, CMCQRD, Omni-Math) showing strong alignment with human labels and robustness to hallucinations. LLM compare is model-agnostic, continuous, and particularly suited for scoring synthetic out-of-distribution problems that humans cannot solve.

## Method Summary
LLM compare estimates problem difficulty through pairwise LLM comparisons followed by Bradley-Terry scoring. An LLM compares two problems and selects which is more difficult, repeating this process across many pairs. The Bradley-Terry model (via Iterative Luce Spectral Ranking) converts the win/loss matrix into continuous difficulty scores, where higher scores indicate greater difficulty. The method requires no ground truth answers or human performance data, making it suitable for synthetic problems. Validation uses Pearson correlation against human labels, LLM performance scores, and LLM labels across math and reading comprehension datasets.

## Key Results
- Pearson correlation ≥ 0.80 with human labels across three datasets
- Less than 6% degradation under 10% noise injection, demonstrating hallucination robustness
- Strong agreement between OpenAI o3 and Gemini 2.5 Pro (Pearson r > 0.90), confirming model-agnosticism
- Able to identify difficulty tiers within datasets, enabling interpretable rankings

## Why This Works (Mechanism)

### Mechanism 1: Pairwise Preference Aggregation
Aggregating many relative judgments produces more reliable difficulty rankings than single-item ratings because comparative tasks are cognitively simpler and more consistent for LLMs than absolute scoring. An LLM selects which of two problems is more difficult, creating a graph of win/loss outcomes. The Bradley-Terry model converts this binary matrix into continuous difficulty scores by estimating each problem's strength parameter β, where p_ij = e^βi / (e^βi + e^βj). Iterative Luce Spectral Ranking finds maximum-likelihood estimates of these parameters, smoothing out noise from individual comparison errors.

### Mechanism 2: Robustness Through Statistical Smoothing
Converting many noisy pairwise comparisons into a single BT score makes the final difficulty estimate significantly more robust to individual LLM hallucinations than direct scoring methods. Each problem's final score is derived from multiple independent comparisons (e.g., 36-66 matches per problem). A single erroneous comparison only slightly perturbs the final score because the BT model effectively averages across all matches, unlike methods where one hallucinated rating directly corrupts the output.

### Mechanism 3: Cross-Model Convergence on Inherent Difficulty
Sufficiently capable LLMs, despite different architectures, converge on similar internal representations of problem difficulty, making the method's output largely model-agnostic. Different SOTA LLMs internalize similar patterns about linguistic complexity, domain knowledge, and reasoning requirements. When asked to compare difficulty, they access this shared latent understanding, leading to correlated judgments. This implies the method captures an inherent property of the problems, not an artifact of a single model.

## Foundational Learning

- **Bradley-Terry (BT) Model:** Converts pairwise comparison data into continuous difficulty scores. Why needed: This is the core statistical engine converting raw comparisons into final scores. Quick check: Given β_A = 2.0 and β_B = 1.0, what's P(A beats B)? (Answer: e^2/(e^2+e^1) ≈ 0.73)

- **Item Response Theory (IRT):** Traditional framework for measuring difficulty based on human response patterns. Why needed: Understanding IRT clarifies the limitation (ground truth dependence) this paper solves. Quick check: Why can't IRT estimate difficulty for synthetic problems humans haven't solved? (Answer: IRT needs response data that doesn't exist.)

- **LLM-as-a-Judge Paradigm:** Using LLMs for evaluation tasks. Why needed: The entire method is built on using LLMs for evaluation, not generation. Quick check: What's the primary risk when using LLM as judge without ground truth for calibration?

## Architecture Onboarding

- **Component map:** Problem Dataset -> LLM Comparator -> Match Scheduler -> Bradley-Terry Scorer (ISLR) -> Output (difficulty scores)

- **Critical path:** The Bradley-Terry Scorer is the linchpin. Its output is the final product, and the entire pipeline's success hinges on the quality and quantity of win/loss data it receives from the LLM Comparator.

- **Design tradeoffs:**
  - Matches per Problem vs. Cost/Time: More comparisons yield more stable BT scores but increase linearly in cost
  - Model Capability vs. Cost: Stronger models may give higher-quality judgments but are more expensive
  - All-vs-All vs. Sampled Comparisons: Complete round-robin is ideal for accuracy but O(N²) in cost

- **Failure signatures:**
  - Low inter-model correlation (Pearson r < 0.7) signals the method is not model-agnostic
  - Poor alignment with human intuition (Pearson r < 0.6) questions validity of measured "difficulty"
  - High sensitivity to noise (small corruption causing large score drops) indicates lack of robustness

- **First 3 experiments:**
  1. Convergence Analysis: For n≈500-1000, use cost-efficient LLM to run comparisons, compute BT scores after each batch, plot correlation to find diminishing returns
  2. Model-Agnosticism Validation: On dataset with ground-truth labels, run pipeline with two different SOTA LLMs, calculate correlation between BT scores (target r ≥ 0.90)
  3. Robustness Stress Test: Take converged BT scores, corrupt win/loss matrix by flipping outcomes (1%, 5%, 10%), recompute BT scores, measure correlation degradation vs. simpler methods

## Open Questions the Paper Calls Out

- **Open Question 1:** How can an automated generator be designed to effectively produce "superhuman" synthetic problems suitable for LLM compare scoring? The paper states that "Creating an appropriate generator is ongoing work" to complete the synthetic data generation pipeline.

- **Open Question 2:** Does LLM compare maintain high correlation with ground truth in non-textual or multi-modal domains (e.g., vision, code)? The authors note a "limitation is that the employed datasets span only 2 disciplines (math and reading comprehension)" and rely on text-only inputs.

- **Open Question 3:** How can the accuracy of difficulty scores be validated for "out-of-distribution" problems where no human ground truth exists? The method is designed for problems "unsolvable by humans and LLMs," yet validation is performed exclusively on datasets with human labels or performance data.

## Limitations
- Validates only on text-based math and reading comprehension datasets, not multi-modal domains
- Core assumption that LLM comparisons capture "inherent difficulty" rather than learned correlations remains largely untested
- Method's effectiveness for truly novel problem types where no human judgments exist is unproven

## Confidence
- **High Confidence:** Bradley-Terry scoring mechanism is well-established; empirical validation shows Pearson r ≥ 0.80 against human labels
- **Medium Confidence:** Robustness claims (6% degradation under 10% noise) are supported but require broader validation
- **Low Confidence:** Core assumption that LLM comparisons capture "inherent difficulty" rather than training data biases remains untested

## Next Checks
1. **Cross-Domain Transfer Test:** Apply LLM compare to novel domain (e.g., molecular biology) without human difficulty judgments, then validate against expert assessments after the fact
2. **Architecture-Specific Bias Analysis:** Compare BT scores from code-specialized models versus generalist models on same problem set to quantify model-specific difficulty representations
3. **Systematic Bias Stress Test:** Design LLM comparison prompts that systematically favor certain problem features and measure effects on BT score distributions across problem sets with varying feature distributions