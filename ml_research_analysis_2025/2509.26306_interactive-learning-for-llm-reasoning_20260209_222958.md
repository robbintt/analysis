---
ver: rpa2
title: Interactive Learning for LLM Reasoning
arxiv_id: '2509.26306'
source_url: https://arxiv.org/abs/2509.26306
tags:
- multi-agent
- learning
- llms
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ILR, a multi-agent learning framework that\
  \ enhances individual LLMs' reasoning ability through interactive training. ILR\
  \ employs dynamic interaction modes (cooperation or competition) based on problem\
  \ difficulty and uses Idea3\u2014a three-stage communication protocol (Idea Sharing,\
  \ Analysis, Fusion)\u2014to simulate human discussion."
---

# Interactive Learning for LLM Reasoning

## Quick Facts
- arXiv ID: 2509.26306
- Source URL: https://arxiv.org/abs/2509.26306
- Reference count: 40
- Multi-agent training framework improves LLM reasoning accuracy by up to 5% on math benchmarks

## Executive Summary
This paper introduces ILR, a multi-agent learning framework that enhances individual LLMs' reasoning ability through interactive training. ILR employs dynamic interaction modes (cooperation or competition) based on problem difficulty and uses Idea3—a three-stage communication protocol (Idea Sharing, Analysis, Fusion)—to simulate human discussion. Perception Calibration injects peer performance signals into reward functions to strengthen learning cohesion. Tested across two model families on five math benchmarks and one coding task, ILR consistently outperforms single-agent baselines by up to 5% and shows improved robustness in multi-agent inference. Dynamic interaction strategies and Idea3 communication prove key to these gains.

## Method Summary
ILR trains LLMs through pairwise interaction using three core mechanisms. Dynamic Interaction selects between cooperation and competition based on IRT-calculated success probability from self-estimated question difficulty. Idea3 structures communication as a three-turn protocol (Sharing → Analysis → Fusion) to enable critical peer evaluation and synthesis. Perception Calibration modifies GRPO rewards by normalizing peer performance statistics and injecting relative scores to enhance learning cohesion. The framework uses MATH dataset for training, evaluates on GSM8K, MATH-500, Olympiad Bench, AIME, and MBPP, and optimizes with batch size 256, learning rate 1e-6, and Llama-3-8b-rm-mixture as reward model.

## Key Results
- ILR improves accuracy by up to 5% over single-agent baselines on math benchmarks
- Dynamic interaction (cooperation/competition) based on IRT outperforms static strategies
- Models paired with similar-ability peers show better learning outcomes than large ability gaps
- Idea3 communication protocol demonstrates improved robustness in multi-agent inference

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Conditional Interaction
Adaptively selecting cooperation or competition based on problem difficulty and model ability improves learning efficiency over static strategies. The framework uses IRT to calculate the probability of a model solving a question independently ($P_q$). If $P_q < 0.5$, the system selects cooperation to pool capability; otherwise, it selects competition to identify efficient solutions. Core assumption: Models can accurately estimate question difficulty and their own ability via self-ranking and validation set performance.

### Mechanism 2: Critical Analysis Synthesis (Idea3)
A structured three-stage communication protocol (Idea3) improves the robustness of stronger LLMs by filtering noise from weaker peers. Instead of simple debate, agents undergo Idea Sharing → Idea Analysis (critique/reflect) → Idea Fusion. The analysis stage forces the model to explicitly identify weaknesses or complementary steps before synthesis. Core assumption: Models possess the capacity to critique peer logic accurately without hallucinating errors in correct reasoning.

### Mechanism 3: Cross-Agent Reward Calibration
Injecting peer performance distribution characteristics into an agent's reward function strengthens learning cohesion. Perception Calibration modifies the standard GRPO reward by normalizing peer reward statistics (max, min, avg) and adding a clipped relative score to the agent's own reward, making the agent "perceive" how its answer compares to the group. Core assumption: Reward models provide comparable scalar signals across different model families.

## Foundational Learning

- **Item Response Theory (IRT)**
  - Why needed: IRT is the mathematical logic used to map "model ability" vs. "question difficulty" to determine the interaction mode.
  - Quick check: If a model has ability $\gamma=0.6$ and a question has difficulty $D=0.8$, what is the approximate probability of success using the paper's logistic formula, and which mode should be triggered?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed: ILR builds upon GRPO, which uses group statistics to estimate baselines, a prerequisite for understanding how "Perception Calibration" injects peer statistics into this flow.
  - Quick check: How does GRPO estimate the baseline advantage without a separate value function (critic) model, and where does the "Perception Calibration" signal plug into this?

- **Self-Consistency / Ranking**
  - Why needed: The mechanism relies on the model ranking question difficulty itself ("Self-ranking prompt"). You must understand that LLMs can rank relative difficulty better than they can assess absolute difficulty.
  - Quick check: Why does the method average rankings over multiple random splits ($S$ times) rather than asking the model to rank the whole dataset at once?

## Architecture Onboarding

- **Component map:** Difficulty Estimator → Mode Selector → Idea3 Engine → Reward Processor → Trainer
- **Critical path:** The Perception Calibration (Eq. 5) is the most sensitive logic. If the normalization ($R_{l,max} - R_{l,min}$) approaches zero (peer answers are identical), the division creates instability. The `clip` function is the safety rail.
- **Design tradeoffs:** Pairing models of similar ability yields better results than large disparities (e.g., 8B+7B > 8B+14B). Consider grouping models by capability tiers rather than random pairing. Idea3 requires 3 inference turns per sample plus Reward Model scoring, significantly slower than single-turn SFT.
- **Failure signatures:** Communication Collapse (models copy peer answers regardless of correctness), Calibration Instability (reward variance explosion), Regression in Strong Models (over-correction due to polite biases).
- **First 3 experiments:**
  1. Sanity Check IRT: Run Difficulty Estimator on held-out validation set. Plot estimated difficulty vs. actual human-labeled difficulty or pass rates to verify ranking correlation.
  2. Ablate Interaction: Train three variants—Pure Cooperation, Pure Competition, Dynamic (ILR). Compare convergence speed and final accuracy to verify 5% gain claim.
  3. Stress Test Calibration: Pair "Toxic" agent (random output) with "Learner" agent. Check if Perception Calibration allows Learner to ignore Toxic agent's low rewards or if it destabilizes training.

## Open Questions the Paper Calls Out
- Does the effectiveness of ILR scale when increasing the number of interacting agents beyond two?
- What is the optimal strategy for pairing agents of differing abilities to maximize learning outcomes?
- Can the Idea3 communication protocol generalize to tasks without deterministic ground-truth answers, such as creative writing or open-ended dialogue?

## Limitations
- Reliance on accurate self-difficulty estimation and IRT calibration may misfire if self-ranking produces noisy or biased difficulty scores
- Perception Calibration assumes reward model outputs are comparable across different model families, which may not hold in practice
- Lack of explicit error bounds or ablation on the clip range for reward normalization creates potential for reward hacking or instability

## Confidence
- **High confidence**: Modular design (Difficulty Estimator, Idea3 Engine, Perception Calibration) is clearly specified and logically sound
- **Medium confidence**: 5% performance gain claim is based on benchmark results, but sensitivity to hyperparameter choices (e.g., IRT boundary, clip thresholds) is not explored
- **Low confidence**: Robustness of Idea3's Analysis stage against model biases (e.g., agreeable tendencies) is not empirically validated, and failure modes like communication collapse are only theorized

## Next Checks
1. Sanity Check IRT: Run the Difficulty Estimator on a held-out validation set. Plot the model's estimated difficulty (D_q) against actual human-labeled difficulty (if available) or pass rates to verify the ranking correlation.
2. Ablate Interaction: Train three small variants: (A) Pure Cooperation, (B) Pure Competition, (C) Dynamic (ILR). Compare convergence speed and final accuracy to verify the 5% gain claim.
3. Stress Test Calibration: Deliberately pair a "Toxic" agent (random/fixed output) with a "Learner" agent. Check if Perception Calibration allows the Learner to ignore the Toxic agent's low rewards, or if it destabilizes training.