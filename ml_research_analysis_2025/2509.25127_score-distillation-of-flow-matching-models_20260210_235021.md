---
ver: rpa2
title: Score Distillation of Flow Matching Models
arxiv_id: '2509.25127'
source_url: https://arxiv.org/abs/2509.25127
tags:
- sid-dit
- diffusion
- distillation
- sana
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends score distillation techniques to flow-matching
  text-to-image models by showing that diffusion and flow matching are theoretically
  equivalent under Gaussian assumptions. The authors unify these frameworks through
  Bayes' rule and conditional expectations, enabling direct application of Score identity
  Distillation (SiD) to pretrained models like SANA, SD3, SD3.5, and FLUX.1-dev without
  teacher finetuning or architectural changes.
---

# Score Distillation of Flow Matching Models

## Quick Facts
- arXiv ID: 2509.25127
- Source URL: https://arxiv.org/abs/2509.25127
- Authors: Mingyuan Zhou, Yi Gu, Huangjie Zheng, Liangchen Song, Guande He, Yizhe Zhang, Wenze Hu, Yinfei Yang
- Reference count: 40
- Extends score distillation to flow-matching text-to-image models, achieving competitive FID scores (20-30) and CLIP scores (0.33-0.34) across SANA, SD3, SD3.5, and FLUX.1-dev

## Executive Summary
This paper establishes theoretical equivalence between diffusion and flow matching models under Gaussian assumptions, enabling direct application of Score identity Distillation (SiD) to pretrained flow-matching models without architectural changes. The authors demonstrate that x₀-prediction, ε-prediction, v-prediction, and rectified-flow velocity prediction all optimize the same underlying objective—recovering E[x₀|xₜ]—differing only in loss weighting across timesteps. Experiments show SiD-DiT achieves competitive performance across all tested models, with the method working both in data-free settings and with adversarial learning. The unified theoretical perspective resolves prior concerns about stability and soundness while demonstrating robustness across different model architectures and scales.

## Method Summary
The authors extend score distillation techniques to flow-matching text-to-image models by showing that diffusion and flow matching are theoretically equivalent under Gaussian assumptions. They demonstrate that the optimal score function can be expressed equivalently across all parameterizations via linear transformations given xₜ, using Tweedie's formula and Bayes' rule. The generator loss Lθ uses the difference between teacher and "fake" score network predictions as a gradient signal, with the teacher's x₀-prediction recovered from velocity predictions via the conversion fᵩ(xₜ, t, c) = xₜ − t · vᵨᴹ(xₜ, t, c). The method works both in data-free settings and with adversarial learning, providing the first systematic evidence that score distillation applies broadly to flow-matching models.

## Key Results
- SiD-DiT achieves FID scores ranging from 20-30 across SANA, SD3, SD3.5, and FLUX.1-dev models
- CLIP scores consistently around 0.33-0.34 across all tested architectures
- Method works effectively in both data-free and adversarial learning settings
- Demonstrates first systematic application of score distillation to flow-matching models without teacher finetuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion and flow matching objectives are mathematically equivalent under Gaussian assumptions.
- Mechanism: The paper shows that x₀-prediction, ε-prediction, v-prediction, and rectified-flow velocity prediction all optimize the same underlying objective—recovering E[x₀|xₜ]—differing only in loss weighting across timesteps. Using Bayes' rule and Tweedie's formula, the optimal score function can be expressed equivalently across all parameterizations via linear transformations given xₜ.
- Core assumption: Gaussian noise corruption process (xₜ = αₜx₀ + σₜε) with monotonic SNR decrease.
- Evidence anchors:
  - [Section 2.1-2.2] Derives equivalence via Tweedie's formula: ∇xₜ log p(xₜ) = −(xₜ − αₜ E[x₀|xₜ])/σₜ²
  - [Section 2.3] Shows all formulations share optimal solutions, with practical differences arising from weight-normalized timestep distribution π(t)
  - [Corpus] Limited direct corpus evidence; related work "Alignment of Diffusion Model and Flow Matching" addresses unification but not score distillation specifically
- Break condition: Non-Gaussian corruption processes or non-standard SNR schedules may break the equivalence

### Mechanism 2
- Claim: Score distillation transfers to flow-matching models via linear conversion from velocity to x₀-prediction.
- Mechanism: For rectified flow, the teacher's x₀-prediction is recovered as fᵩ(xₜ, t, c) = xₜ − t · vᵨᴹ(xₜ, t, c). This enables direct application of SiD's Fisher divergence minimization without architectural changes. The generator loss Lθ uses the difference between teacher and "fake" score network predictions as a gradient signal.
- Core assumption: Teacher velocity predictions are sufficiently accurate and CFG scale of 4.5 transfers across architectures.
- Evidence anchors:
  - [Section 3, Eq. 15] Explicit conversion formula with CFG: fᵩ = (xₜ − t·vᵨ(xₜ,∅)) + 4.5[(xₜ − t·vᵨ(xₜ,c)) − (xₜ − t·vᵨ(xₜ,∅))]
  - [Section 3, Eq. 18] Generator loss: Lθ = wₜ(fᵩ − fψ)ᵀ(fψ − xᵍ)
  - [Corpus] "Few-Step Diffusion via Score identity Distillation" provides the original SiD formulation this extends
- Break condition: Guidance mechanism mismatch (e.g., FLUX's learned guidance vs. standard CFG) degrades performance

### Mechanism 3
- Claim: Weight-normalized timestep distribution π(t) determines which noise levels dominate distillation.
- Mechanism: The effective loss depends on π(t) = wₜ·p(t)/∫wₜ·p(t)dt, not raw p(t) or weighting wₜ alone. The paper fixes p(t) = LogitN(ln 2, 1.6²) and wₜ = 1−t, emphasizing larger t (heavier noise) for visual appeal while preserving detail coverage. Empirical ablation shows restricting to t ∈ (2/3, 1) yields appealing images lacking detail; t ∈ (1/3, 2/3) adds detail but appears hazy.
- Core assumption: Timestep coverage across [0,1] is necessary for balanced quality; teacher's SNR schedule is well-matched to chosen π(t).
- Evidence anchors:
  - [Section 4.1, Figure 3] Qualitative ablation across disjoint and overlapping t-intervals
  - [Section 4.1] Empirical finding that t ∈ (2/3,1) produces visually appealing but less detailed images
  - [Corpus] No direct corpus evidence on timestep weighting ablations for flow-matching distillation
- Break condition: Severe mismatch between π(t) and teacher's training distribution may cause instability or quality collapse

## Foundational Learning

- Concept: Tweedie's formula for Gaussian denoising
  - Why needed here: Core mathematical identity linking score functions to conditional expectations E[x₀|xₜ], enabling unification of diffusion and flow-matching objectives
  - Quick check question: Given xₜ = αₜx₀ + σₜε with ε ~ N(0,I), what does Tweedie's formula tell us about ∇xₜ log p(xₜ)?

- Concept: Fisher divergence between distributions
  - Why needed here: SiD's generator loss is derived from Fisher divergence minimization between teacher and student distributions, not trajectory matching
  - Quick check question: How does Fisher divergence differ from KL divergence, and why might it be preferable for score distillation?

- Concept: Weight-normalized distributions and importance sampling
  - Why needed here: Understanding that loss weighting wₜ and timestep sampling p(t) combine into effective distribution π(t), explaining why different parameterizations can be equivalent
  - Quick check question: If p(t) is uniform on [0,1] and wₜ = (1−t)⁻², what shape does π(t) take?

## Architecture Onboarding

- Component map:
  Teacher model (fᵩ) -> Generator (Gθ) -> Fake score network (fψ) -> Optional adversarial head

- Critical path:
  1. Sample xᵍ from Gθ at random step k ∈ {1,2,3,4}
  2. Forward-diffuse: xₜ = (1−t)xᵍ + tε
  3. Update fψ: minimize ||fψ(xₜ) − xᵍ||²
  4. Update Gθ: minimize Lθ = (1−t)(fᵩ − fψ)ᵀ(fψ − xᵍ)
  5. Alternate updates until FID plateaus

- Design tradeoffs:
  - BF16 vs. AMP: BF16 enables larger models (SD3.5-Large, FLUX) but requires aggressive Adam ε=10⁻⁴ and LR=10⁻⁵ to avoid gradient underflow
  - Data-free vs. adversarial: Data-free simpler and often sufficient; adversarial improves FID but requires quality data and may shift style
  - CFG scale: 4.5 used universally; mismatch with FLUX's learned guidance likely causes modest performance gap

- Failure signatures:
  - Gradient underflow with BF16: Symptoms include stalled training; fix by increasing Adam ε and/or learning rate
  - Memory overflow on large models: Symptoms include OOM; fix by switching from AMP+FSDP to BF16+FSDP with CPU offloading for text encoder/VAE
  - Hazy outputs: May indicate insufficient emphasis on mid-range timesteps; adjust π(t) coverage

- First 3 experiments:
  1. Validate velocity-to-x₀ conversion on a single SANA checkpoint: Generate samples using both teacher flow-matching sampling and the converted x₀-prediction with identical noise/conditioning; verify visual and metric parity
  2. Ablate timestep weighting on SANA-0.6B: Train SiD-DiT with wₜ ∈ {1, 1−t, (1−t)²} while holding p(t) fixed; measure FID/CLIP trajectories to confirm 1−t weighting preference
  3. Cross-architecture sanity check: Apply identical hyperparameters to SD3-Medium and FLUX.1-Dev; identify architecture-specific adjustments needed (e.g., guidance handling for FLUX)

## Open Questions the Paper Calls Out
None

## Limitations
- Gaussian noise assumption may break down for non-standard training regimes or alternative corruption processes
- Method shows sensitivity to timestep weighting, with suboptimal coverage leading to quality degradation
- Performance on FLUX.1-dev is notably weaker than other models, likely due to guidance mechanism mismatches
- Large model training requires careful hyperparameter tuning (Adam ε, learning rate) to avoid gradient underflow in BF16

## Confidence
- Theoretical equivalence proof: High confidence - The mathematical derivations using Tweedie's formula and Bayes' rule are rigorous and well-established
- Practical implementation effectiveness: Medium confidence - Results are competitive but not state-of-the-art, with notable performance gaps for FLUX.1-dev
- Cross-architecture generalizability: Medium confidence - Works across diverse models but with varying success rates and requires architecture-specific tuning

## Next Checks
1. **Architecture-specific optimization:** Systematically sweep CFG scales and timestep weightings for each model architecture (SANA, SD3, FLUX) to identify optimal hyperparameters rather than using a one-size-fits-all approach
2. **Non-Gaussian robustness:** Test the method with alternative noise distributions (e.g., Laplacian, t-distribution) to quantify the impact of violating Gaussian assumptions on both training stability and sample quality
3. **Guidance mechanism alignment:** For FLUX.1-dev specifically, investigate whether the learned guidance mechanism can be approximated or incorporated into the SiD-DiT framework to close the performance gap with other architectures