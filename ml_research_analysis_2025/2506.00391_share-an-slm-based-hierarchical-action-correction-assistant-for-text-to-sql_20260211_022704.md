---
ver: rpa2
title: 'SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL'
arxiv_id: '2506.00391'
source_url: https://arxiv.org/abs/2506.00391
tags:
- schema
- text-to-sql
- column
- action
- share
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARE introduces a hierarchical SLM-based correction framework
  for text-to-SQL that addresses the limitations of recursive LLM self-correction
  by converting SQL queries into stepwise action trajectories. It orchestrates three
  specialized SLMs (BAM, SAM, LOM) in a sequential pipeline to achieve precise error
  localization and correction.
---

# SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL

## Quick Facts
- arXiv ID: 2506.00391
- Source URL: https://arxiv.org/abs/2506.00391
- Reference count: 40
- Primary result: Achieves 14.80% relative improvement in execution accuracy on BIRD and 11.41% on SPIDER compared to strong LLM-based baselines.

## Executive Summary
SHARE introduces a hierarchical SLM-based correction framework for text-to-SQL that addresses the limitations of recursive LLM self-correction by converting SQL queries into stepwise action trajectories. It orchestrates three specialized SLMs (BAM, SAM, LOM) in a sequential pipeline to achieve precise error localization and correction. SHARE reduces computational overhead by one-tenth compared to strong LLM-based baselines while achieving 14.80% relative improvement in execution accuracy on BIRD and 11.41% on SPIDER. It maintains strong performance across diverse LLMs, varying query complexities, and low-resource settings, demonstrating robustness and generalizability.

## Method Summary
SHARE operates through a three-stage SLM pipeline that transforms SQL queries into actionable trajectories for correction. The Base Action Model (BAM) converts initial SQL into stepwise action trajectories using pandas-like APIs, revealing underlying reasoning steps. The Schema Augmentation Model (SAM) then refines schema-related errors through mask-and-fill techniques, while the Logic Optimization Model (LOM) addresses logic-based corrections in the trajectory. Training employs a hierarchical self-evolution strategy where BAM synthesizes data for SAM and LOM via error perturbation, reducing annotation costs. The system uses LoRA fine-tuning on Llama-3.1-8B or Phi-3-mini-3.8B backbones with specific hyperparameters (rank=8, lr=5e-5, batch=8) and inference settings (temp=0.1, top_p=0.95, max_len=1024).

## Key Results
- Achieves 14.80% relative improvement in execution accuracy on BIRD benchmark compared to strong LLM-based baselines
- Achieves 11.41% relative improvement in execution accuracy on SPIDER benchmark
- Reduces computational overhead by approximately one-tenth compared to recursive LLM self-correction methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting declarative SQL into imperative Action Trajectories (AT) exposes hidden reasoning steps, allowing for more granular error detection than direct SQL self-correction.
- **Mechanism:** The Base Action Model (BAM) translates a SQL query (e.g., `SELECT...WHERE`) into a stepwise pandas-like API sequence (e.g., `df1 = df.where(...)`). This converts the static syntax into a temporal execution trace. When the downstream Logic Optimization Model (LOM) analyzes this trace, it can identify specific faulty steps (e.g., incorrect filter parameter) rather than treating the SQL as a monolithic block.
- **Core assumption:** The intermediate action representation preserves the semantic intent of the SQL while being cognitively or syntactically easier for the SLM to debug than raw SQL.
- **Evidence anchors:**
  - [abstract]: "transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning."
  - [Section 3.2]: "BAM aims to generate the corresponding action trajectory... to present the reasoning process."
  - [corpus]: Related work "LitE-SQL" also explores execution-guided correction, supporting the hypothesis that procedural signals aid debugging.
- **Break condition:** If the BAM translation introduces semantic drift (i.e., the AT does not perfectly match the SQL logic), the subsequent SLMs will optimize the wrong logic, resulting in hallucinated corrections.

### Mechanism 2
- **Claim:** Decoupling error correction into a Schema Augmentation Model (SAM) and a Logic Optimization Model (LOM) prevents error masking and improves specialization.
- **Mechanism:** SAM focuses exclusively on linking the correct database tables and columns (schema linking) using a "mask-and-fill" technique. LOM then operates on the schema-corrected trajectory to fix reasoning (e.g., group by logic). By isolating variables, the system prevents the SLM from confusing a misidentified column with a logic error.
- **Core assumption:** Schema errors and logic errors in text-to-SQL are sufficiently distinct and can be resolved sequentially without tight feedback loops between the two.
- **Evidence anchors:**
  - [Section 3.5]: "SAM then refines... followed by... LOM for logic-based corrections."
  - [Table 5]: Ablation study showing significant performance drops when either SAM or LOM is removed.
  - [corpus]: "SQLens" and "SQLCritic" in the corpus similarly utilize modular detection, suggesting a trend toward structured error analysis over monolithic generation.
- **Break condition:** If the "Initial SQL" contains interdependent schema and logic errors (e.g., using a non-existent column inside a complex subquery), the sequential pass may fail if SAM cannot resolve the schema without understanding the logic context.

### Mechanism 3
- **Claim:** A hierarchical self-evolution strategy allows data-efficient training of specialized SLMs without expensive human annotation or recursive teacher LLM queries.
- **Mechanism:** Instead of querying a large teacher LLM (e.g., GPT-4o) to generate training data for all three models, the system uses the teacher only for BAM. The BAM then synthesizes training data for SAM and LOM via error perturbation (injecting errors into correct trajectories) and masking. This reduces the "annotation" cost.
- **Core assumption:** The synthetic errors generated by the perturbation strategy (Add, Delete, Substitute) sufficiently cover the distribution of real-world SQL errors encountered by the generator LLM.
- **Evidence anchors:**
  - [Section 3.1]: "leverages BAM to synthesize and augment task-specific training data... reducing annotation costs."
  - [Section 3.4]: "action-based perturbation strategy... to reproduce various logic errors."
  - [corpus]: Weak/No direct evidence in corpus for this specific self-evolution technique.
- **Break condition:** If the perturbation strategy is too simplistic (e.g., only deleting steps), the LOM may overfit to syntactic errors and fail to correct semantic reasoning errors.

## Foundational Learning

- **Concept:** **Schema Linking** (Text-to-SQL)
  - **Why needed here:** The SAM component is entirely dedicated to this. Understanding how LLMs map natural language words to specific database column names is critical to debugging why the paper separates this into a dedicated model.
  - **Quick check question:** Can you identify the difference between a "schema error" (wrong table join) and a "logic error" (wrong aggregation) in a user query?

- **Concept:** **Procedural vs. Declarative Representations**
  - **Why needed here:** The core innovation of SHARE is transforming Declarative SQL into Procedural Action Trajectories. You must understand the difference between saying "Get X" (Declarative) and "First filter, then sort, then select" (Procedural).
  - **Quick check question:** How would you express the SQL query `SELECT name FROM users WHERE age > 10` as a sequence of pandas-style function calls?

- **Concept:** **Knowledge Distillation & Data Synthesis**
  - **Why needed here:** The paper uses a "Hierarchical Self-Evolution" training method. You need to understand how a smaller model (student) learns from data generated by a larger model (teacher) or a sibling model (BAM).
  - **Quick check question:** Why might training an SLM on synthetic error data (generated by perturbation) be riskier than training on human-annotated errors?

## Architecture Onboarding

- **Component map:** Generator LLM -> BAM -> SAM -> LOM -> Regenerated SQL
- **Critical path:** The integrity of the **BAM translation**. If the BAM fails to convert the SQL into a valid action trajectory, the subsequent SAM and LOM steps will process garbage input. The LoRA adapters for the 3 SLMs must be loaded correctly.
- **Design tradeoffs:**
  - **Latency vs. Cost:** The system trades higher inference latency (sequential execution of 3 SLMs + LLM) for lower API cost (SLMs are cheaper/quantized) and better accuracy.
  - **Modifiability:** The separation of SAM and LOM allows you to retrain the logic module without retraining the schema module, but it adds complexity to the inference pipeline.
- **Failure signatures:**
  - **Overcorrection:** The LOM might "fix" correct logic because it is overfitted to expect errors. The paper notes a 11.20% overcorrection rate (Section C.4).
  - **Mathematical Delusion:** The system struggles to correct mathematical reasoning errors (Section 4.7), likely because the Action Trajectory syntax doesn't inherently solve arithmetic logic.
- **First 3 experiments:**
  1. **BAM Validation:** Run a set of ground-truth SQLs through BAM and attempt to convert them back to SQL using the "SQL Generation" prompt (Figure 13). If the round-trip fails, the BAM needs retraining.
  2. **Ablation Stress Test:** Run the pipeline with SAM disabled to see if the system hallucinates non-existent columns. Run with LOM disabled to see if it selects correct columns but aggregates them incorrectly.
  3. **Perturbation Analysis:** Visualize the "Erroneous Action Trajectories" generated by the training pipeline (Table 1) to ensure they mimic the types of errors your specific Generator LLM typically makes.

## Open Questions the Paper Calls Out

- **Multi-turn Interactive Self-correction:** How effective is SHARE in multi-turn interactive self-correction scenarios where the correction process undergoes multiple refinement cycles? The current study restricts evaluation to a one-turn setting where the generator model performs a single revision based on SHARE's feedback.

- **Generalization to Broader Code Generation:** Can the hierarchical action correction paradigm be effectively generalized to broader code generation tasks beyond the text-to-SQL domain? The current methodology and action trajectory definitions are specifically tailored for SQL query logic.

- **Mathematical Delusion Mitigation:** How can the framework be augmented to better mitigate "Mathematical Delusion" errors that stem from the generator model's inherent reasoning limitations? The current correction relies on the generator's internal reasoning; if the base LLM fails to understand the math, the trajectory refinement cannot fix the logic.

## Limitations
- Effectiveness depends heavily on the generator LLM's error distribution, with no validation across weaker or diverse generators
- Self-evolution training relies on synthetic error trajectories that may not fully capture real-world error patterns
- Sequential SLM pipeline may introduce latency bottlenecks in real-time applications

## Confidence
**High Confidence:**
- The hierarchical SLM architecture (BAM → SAM → LOM) is technically sound and the modular separation of schema vs. logic correction is a valid design choice
- The reported execution accuracy improvements over strong baselines (14.80% on BIRD, 11.41% on SPIDER) are likely reproducible given the detailed training and inference procedures

**Medium Confidence:**
- The computational overhead reduction claim (1/10th of LLM-based methods) is plausible given the use of quantized SLMs, but depends heavily on the specific baseline and hardware
- The generalizability claim across diverse LLMs and low-resource settings is supported by ablation studies but lacks direct empirical validation

**Low Confidence:**
- The "Hierarchical Self-Evolution" training method's ability to synthesize representative error trajectories is theoretically sound but unproven without comparison to human-annotated data
- The overcorrection rate (11.20%) and failure on mathematical reasoning errors are noted but not deeply analyzed; the root causes are speculative

## Next Checks
1. **Cross-Generator Validation:** Reproduce the SHARE pipeline using a weaker generator LLM (e.g., GPT-3.5 or open-source code models like CodeLlama) and measure execution accuracy degradation. This will test the claim of generalizability and reveal if the SAM/LOM models are overfit to GPT-4o's error patterns.

2. **Human vs. Synthetic Error Comparison:** Manually annotate a small subset of erroneous SQL queries (e.g., 100 samples) with their true error types and trajectories. Compare the error correction success rate of LOM when trained on human-annotated vs. synthetic perturbed trajectories. This will validate the effectiveness of the self-evolution training method.

3. **End-to-End Latency Benchmark:** Instrument the SHARE pipeline to measure total inference time (BAM + SAM + LOM + generator regeneration) on a standard CPU/GPU setup. Compare this to the reported "10x reduction" claim and assess if the latency is acceptable for real-time applications.