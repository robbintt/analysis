---
ver: rpa2
title: 'KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge
  Graph Contrastive Learning'
arxiv_id: '2412.04948'
source_url: https://arxiv.org/abs/2412.04948
tags:
- knowledge
- alignment
- llms
- representation
- kalm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KaLM, a method for aligning autoregressive
  language models with knowledge graphs to improve performance on knowledge-driven
  tasks. The approach combines explicit knowledge alignment through dual-view knowledge
  graph contrastive learning with implicit knowledge alignment via triple completion
  language modeling.
---

# KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning

## Quick Facts
- **arXiv ID:** 2412.04948
- **Source URL:** https://arxiv.org/abs/2412.04948
- **Reference count:** 31
- **Primary result:** Aligns LLMs with KGs via dual-view contrastive learning, reducing WN18RR MR from 15,969 to 190 and anisotropy from 0.83 to 0.21

## Executive Summary
KaLM introduces a method to align autoregressive language models with knowledge graphs by combining explicit knowledge alignment through dual-view contrastive learning and implicit knowledge alignment via triple completion language modeling. The explicit component treats tail entity descriptions and head-relation concatenations as two views of the same knowledge triple, using InfoNCE loss to align embeddings while reducing representation anisotropy. The implicit component maintains generative capabilities through standard autoregressive training on instruction-formatted triple completion tasks. Experiments show significant improvements on both embedding-based knowledge graph completion and generation-based question answering tasks.

## Method Summary
KaLM fine-tunes autoregressive LLMs (Llama-2-7B, Llama-3-8B, Mistral-7B) using LoRA adapters on feed-forward network layers. The method processes KG triples through two parallel forward passes with special tokens distinguishing views: head-relation descriptions with "["/"]" tokens and tail descriptions with "{""/"}" tokens. Embeddings are extracted from the [eos] token of the last layer. The training objective combines InfoNCE contrastive loss for explicit alignment with standard language modeling loss for implicit alignment, using a weighted combination (λ=0.1). The approach requires textual descriptions for KG entities and relations, with inverse triples added for bidirectional prediction.

## Key Results
- WN18RR: MR reduced from 15,969 to 190; Hit@10 improved from 0.020 to 0.851
- FB15k-237: MR reduced from 5,241 to 157; Hit@10 improved from 0.002 to 0.719
- KGQA: 9.6-14.1% improvement in prediction accuracy across different LLMs
- Sentence-level anisotropy reduced from 0.83 to 0.21
- LoRA fine-tuning preserves generative capability (PPL ~4.96-5.02 vs 6.42 without implicit alignment)

## Why This Works (Mechanism)

### Mechanism 1
Dual-view contrastive learning aligns knowledge representations by treating tail entity descriptions and head-relation concatenations as two views of the same knowledge. InfoNCE loss with additive margin pulls together embeddings from the same KG triple while pushing apart embeddings from different triples, learning that [head-desc][relation-desc] and [tail-desc] should map to nearby points in representation space when they describe the same triple. The model learns that KG triples represent coherent, verifiable knowledge units where head+relation semantically implies tail. This works when KG triples contain accurate, detailed textual descriptions but fails with noisy, contradictory, or sparse descriptions.

### Mechanism 2
The contrastive objective's uniformity term directly alleviates representation anisotropy by encouraging embeddings to spread across the hypersphere. Theoretical analysis proves that minimizing the second term of the contrastive loss asymptotically minimizes an upper bound on sentence-level anisotropy. By pushing negative pairs apart, the model uses more of the available representation space rather than clustering in a narrow cone. This assumes L2-normalized embeddings lie on a unit hypersphere where uniformity correlates with discriminability. The approach works when temperature τ is properly tuned and batch size is sufficient for adequate negative sampling.

### Mechanism 3
Implicit knowledge alignment via triple completion language modeling preserves generative capability while reinforcing knowledge patterns. Standard autoregressive LM objective on instruction-formatted triple completion tasks maintains next-token prediction capability and prevents catastrophic forgetting of pre-trained knowledge. The approach assumes language modeling on structured knowledge text transfers to improved knowledge reasoning without degrading fluency. This works when λ weight balances the two objectives appropriately (λ=0.1 optimal) but fails if λ is too high (degrades KGC metrics) or too low (causes PPL spike to 6.42).

## Foundational Learning

- **Contrastive Learning (InfoNCE Loss)**: Core of explicit alignment; must understand positive/negative pairs, temperature scaling, and additive margins. Quick check: Given two embedding vectors, can you compute the InfoNCE loss with margin γ and temperature τ?

- **Knowledge Graph Fundamentals**: KaLM operates on triples (h,r,t) and their textual descriptions; understanding entity/relation structure is prerequisite. Quick check: What is the difference between WN18RR and FB15k-237 in terms of relation granularity and description quality?

- **Representation Anisotropy**: The problem KaLM explicitly solves; understand why embeddings clustering in a narrow cone hurts discriminative tasks. Quick check: How would you compute sentence-level anisotropy for a corpus of N sentences given their embeddings?

## Architecture Onboarding

- **Component map**: KG triples with textual descriptions -> Dual forward passes (head-relation view, tail view) -> LoRA-finetuned LLM -> [eos] embeddings -> InfoNCE + LM losses -> Combined objective

- **Critical path**: Load KG triples with textual descriptions -> Construct paired training data: (D_hr, D_t) for each triple -> Forward pass through LLM to extract [eos] hidden states as embeddings -> Compute InfoNCE loss with in-batch negatives -> Jointly compute triple completion LM loss on instruction-formatted data -> Backprop through LoRA adapters only

- **Design tradeoffs**: LoRA on FFN only (not attention) preserves generative capability with slight KGC performance tradeoff vs full fine-tuning; separate batch sizes (24-72 for explicit, 3-8 for implicit) balance contrastive learning needs and memory constraints; λ=0.1 weight optimal from ablation studies

- **Failure signatures**: Anisotropy remains high (>0.6) indicates issues with contrastive batch size or temperature; PPL spikes above 6.0 suggest implicit alignment loss is disabled or underweighted; MR remaining in thousands indicates incorrect [bos]/[eos] token usage

- **First 3 experiments**: 1) Sanity check: Train on WN18RR subset (1000 triples) with λ=0.1; verify anisotropy drops from baseline and MR improves. 2) Ablation sweep: Compare λ∈{0, 0.01, 0.1, 1.0} on validation set; confirm PPL and Hit@10 tradeoffs match paper. 3) Module comparison: Fine-tune attention-only vs FFN-only vs both on small KG; verify FFN-only gives best balance of KGC metrics and PPL.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the KaLM methodology maintain its effectiveness and efficiency when scaled to larger autoregressive models (e.g., 13B or 70B parameters)? The authors were restricted to "limited-scale LLMs" due to computational resources and suggest evaluations on larger models to "further validate the effectiveness."

- **Open Question 2**: Can non-linear or dynamic loss weighting strategies outperform the simple linear combination of explicit and implicit alignment objectives? The paper notes the use of a "simple linear combination" and explicitly suggests that "Further investigations into various forms of loss combinations remain to be explored."

- **Open Question 3**: Can the knowledge-aligned representations learned by KaLM transfer effectively to cross-domain applications like retrieval-augmented generation (RAG)? The authors propose future work to "delve into the performance of the knowledge representations... in cross-domain applications such as retrieval-augmented generation."

## Limitations

- The method's effectiveness depends on high-quality textual descriptions for KG entities and relations, with vague descriptions hurting performance (noted in FB15k-237 results)
- Temperature τ and additive margin γ hyperparameters are described as "trainable" but neither initialization nor final values are reported, creating reproducibility gaps
- The theoretical analysis of anisotropy reduction lacks empirical validation against other anisotropy mitigation techniques like isotropic fine-tuning

## Confidence

**High Confidence:**
- Dual-view contrastive learning improves KG completion metrics (MR/Hit@10) on both WN18RR and FB15k-237
- Representation anisotropy reduction from 0.83 to 0.21 is measurable and reproducible
- LoRA fine-tuning on FFN layers preserves generative capability while improving KGC

**Medium Confidence:**
- The theoretical proof that contrastive loss bounds anisotropy reduction holds under the stated assumptions
- Implicit alignment via triple completion language modeling provides consistent improvements across different LLM sizes
- The combined approach outperforms both pure contrastive alignment and pure language modeling approaches

**Low Confidence:**
- Claims about avoiding catastrophic forgetting are supported by PPL metrics but lack direct comparison to baselines
- The generalization of improvements to truly open-domain KGQA tasks beyond the evaluated datasets
- Performance claims on FB15k-237 given the noted issues with vague relation descriptions

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Run a systematic sweep of temperature τ ∈ {0.05, 0.1, 0.2} and additive margin γ ∈ {0.1, 0.5, 1.0} on a subset of WN18RR, measuring both KGC metrics and final anisotropy values to establish optimal ranges.

2. **Cross-Dataset Generalization Test**: Apply the pre-trained KaLM model to an entirely different KG dataset (e.g., Kinship or UMLS) without further fine-tuning, measuring whether the alignment generalizes beyond the training distribution.

3. **Ablation on Representation Geometry**: Compare the angular distributions of embeddings from KaLM-trained models against baselines (isotropic fine-tuning, standard contrastive learning) using t-SNE visualization and quantitative measures of cluster separation in representation space.