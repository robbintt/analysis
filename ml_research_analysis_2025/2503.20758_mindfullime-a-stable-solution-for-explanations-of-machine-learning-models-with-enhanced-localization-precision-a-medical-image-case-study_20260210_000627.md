---
ver: rpa2
title: 'MindfulLIME: A Stable Solution for Explanations of Machine Learning Models
  with Enhanced Localization Precision -- A Medical Image Case Study'
arxiv_id: '2503.20758'
source_url: https://arxiv.org/abs/2503.20758
tags:
- lime
- samples
- data
- mindfullime
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the instability problem in Local Interpretable
  Model-agnostic Explanations (LIME), which produces inconsistent visual explanations
  for machine learning models due to random perturbation of input data. This instability
  is particularly problematic in critical domains like healthcare, where reliable
  explanations are essential.
---

# MindfulLIME: A Stable Solution for Explanations of Machine Learning Models with Enhanced Localization Precision -- A Medical Image Case Study

## Quick Facts
- arXiv ID: 2503.20758
- Source URL: https://arxiv.org/abs/2503.20758
- Authors: Shakiba Rahimiaghdam; Hande Alemdar
- Reference count: 40
- Key outcome: Addresses LIME's instability by replacing random perturbation with graph-based pruning and uncertainty sampling, achieving 100% stability and improved localization precision for chest X-ray explanations.

## Executive Summary
MindfulLIME addresses the critical instability problem in LIME (Local Interpretable Model-agnostic Explanations) by replacing random perturbation with a deterministic graph-based pruning approach combined with uncertainty sampling. The method transforms image superpixels into a graph structure and systematically deactivates vertices and their neighbors, filtering samples based on classifier confidence to ensure only in-distribution data points are used for explanation generation. Evaluated on chest X-ray datasets (CheXpert and VinDr-CXR) for thorax disease diagnosis, MindfulLIME demonstrates perfect stability compared to unstable LIME results while improving localization precision and efficiency.

## Method Summary
MindfulLIME generates purposive samples using a graph-based pruning approach where superpixels are transformed into an undirected graph. The algorithm systematically deactivates vertices (Phase 1) and their adjacent neighbors (Phase 2), evaluating each sample through a DecisionModule that filters based on classifier confidence thresholds. This deterministic approach ensures stable explanations while improving localization precision by reducing the distance between generated explanations and actual annotations. The method was evaluated using InceptionV3 classifiers trained on CheXpert and tested on VinDr-CXR datasets for chest X-ray disease diagnosis.

## Key Results
- Achieved 100% stability in explanation outputs compared to unstable LIME results across 10 repeated runs
- Demonstrated enhanced localization precision with reduced Jensen-Shannon Divergence from ground truth annotations
- Improved efficiency by generating fewer high-quality samples without requiring additional training data
- Maintained applicability across different segmentation algorithms (SLIC, Felzenszwalb, Quickshift)

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Graph-Based Pruning
Replacing random perturbation with systematic graph traversal eliminates stochastic variance in explanation outputs. The algorithm transforms image superpixels into an undirected graph and systematically generates masks by deactivating vertices and their neighbors, pruning vertices that fail confidence checks to narrow the search space.

### Mechanism 2: Confidence-Based Uncertainty Sampling
Filtering perturbed samples based on classifier confidence ensures the local surrogate model is trained only on in-distribution data, preventing distorted explanations from out-of-distribution artifacts. A DecisionModule evaluates every generated masked image and retains samples only if classifier prediction probability exceeds a specific threshold.

### Mechanism 3: Spatial Contiguity via Adjacency Constraints
Constraining sample generation to adjacent superpixels produces higher localization precision than random combinations. The algorithm iterates through edges of current vertices, generating new masks by deactivating adjacent vertices, enforcing spatial contiguity that grows the explanation region organically.

## Foundational Learning

- **Concept: Local Surrogate Models (LIME)**
  - Why needed: MindfulLIME changes only how training data for the linear explanation model is generated, not the final explanation model itself
  - Quick check: Can you explain why fitting a linear model locally around a single prediction helps interpret a non-linear neural network?

- **Concept: Superpixels and Graphs**
  - Why needed: The core innovation operates on the graph structure of superpixels
  - Quick check: If two superpixels share a boundary, are they connected by an edge in this framework?

- **Concept: Out-of-Distribution (OoD) Data**
  - Why needed: Random perturbation creates nonsensical images that the classifier has never seen, leading to unreliable explanations
  - Quick check: Why would a classifier trained on full X-rays give unreliable outputs for an image where 50% of the pixels are randomly grayed out?

## Architecture Onboarding

- **Component map:** Input (Chest X-ray) -> Segmentation (Superpixels) -> Graph Builder (G=(V,E)) -> MindfulLIME Engine (Mask generation) -> Black-Box Oracle (Classifier) -> Explanation Model (Ridge Regression)
- **Critical path:** The DecisionModule is the bottleneck, requiring a forward pass through the deep neural network for every generated mask
- **Design tradeoffs:**
  - Depth Levels vs. Runtime: Increasing levels increases sample count exponentially; 2 levels recommended for efficiency
  - Threshold Strictness: Higher thresholds yield fewer but higher-quality samples; lower thresholds increase sample count but risk noise
- **Failure signatures:**
  - Empty Graph/No Samples: If initial probability is low or threshold is too high, Phase 1 prunes all vertices
  - Runtime Explosion: Using Felzenszwalb segmentation or high-depth levels causes combinatorial explosion
- **First 3 experiments:**
  1. Calculate average probability for each class on validation set to set DecisionModule thresholds
  2. Run explanation 10 times on same image; if IOU varies >0, graph traversal is not deterministic
  3. Compare runtimes using SLIC vs. Felzenszwalb to verify O(dk^i) complexity impact

## Open Questions the Paper Calls Out

### Open Question 1
Can MindfulLIME be effectively generalized to non-medical image datasets or other data modalities like text or tabular data? The authors state future research might extend the application to diverse datasets and domains, but the current study is limited to chest X-ray datasets.

### Open Question 2
Do alternative uncertainty sampling techniques (entropy-based or margin sampling) provide better stability or precision than reverse confidence-based sampling? The conclusion suggests investigating alternative techniques for sample filtering, but the current implementation relies exclusively on reverse confidence-based sampling.

### Open Question 3
How can graph pruning policies be optimized to handle borderline classification cases where the classifier's confidence is low or ambiguous? The authors identify a need to optimize graph pruning policies and balance explanation depth with reliability, especially for borderline cases.

## Limitations

- Relies on the assumption that classifier confidence reliably indicates in-distribution data quality without empirical validation of this relationship
- Assumes spatial contiguity of disease features, which may not hold for all pathologies and could miss optimal explanations requiring non-contiguous superpixel combinations
- Computational efficiency gains depend on specific graph pruning rates that may vary with different medical imaging domains

## Confidence

- **High Confidence:** The deterministic nature achieving 100% stability is well-supported by algorithmic design; improved localization precision through reduced JS divergence is strongly supported
- **Medium Confidence:** The mechanism of graph-based pruning improving efficiency is plausible but depends on specific graph construction details; cross-segmentation algorithm applicability is asserted but not rigorously tested
- **Low Confidence:** The foundational claim that classifier confidence reliably indicates in-distribution data quality is not directly tested; classifier calibration is not discussed

## Next Checks

1. Evaluate the relationship between classifier confidence scores and true in-distribution/out-of-distribution status by generating controlled synthetic perturbations and measuring prediction reliability
2. Apply MindfulLIME to X-ray datasets with diseases that exhibit non-contiguous or scattered manifestations to test adjacency constraint limitations
3. Test the method on non-radiology medical imaging domains (e.g., dermatology or pathology) to verify broad applicability beyond chest X-rays