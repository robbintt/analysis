---
ver: rpa2
title: On the Scaling of Robustness and Effectiveness in Dense Retrieval
arxiv_id: '2505.24279'
source_url: https://arxiv.org/abs/2505.24279
tags:
- robustness
- scaling
- training
- effectiveness
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the scaling laws of robustness and effectiveness
  in dense retrieval models, revealing a trade-off between these two critical aspects.
  While scaling laws for effectiveness are well-studied, this work investigates whether
  robustness also follows similar scaling patterns.
---

# On the Scaling of Robustness and Effectiveness in Dense Retrieval

## Quick Facts
- arXiv ID: 2505.24279
- Source URL: https://arxiv.org/abs/2505.24279
- Reference count: 40
- Primary result: Pareto training achieves up to 2.5x better scaling efficiency by dynamically balancing robustness and effectiveness losses.

## Executive Summary
This paper investigates the scaling laws of robustness and effectiveness in dense retrieval models, revealing a fundamental trade-off between these two critical aspects. While effectiveness scaling is well-studied, the authors demonstrate that robustness (including out-of-distribution and adversarial robustness) also follows power-law scaling relationships with model size and data size. However, robustness and effectiveness scale differently—robustness is more sensitive to data size while effectiveness responds more to model size—creating significant resource costs when attempting to jointly improve both. To address this challenge, the authors propose Pareto training, which dynamically adjusts optimization weights to balance robustness and effectiveness losses, achieving comparable performance to multi-fold resource scaling without additional resources.

## Method Summary
The method involves training dense retrieval models using various strategies (standard, hard-negative, denoising, adversarial) while measuring both effectiveness (MS MARCO) and robustness (BEIR for OOD, WSRA attacks for adversarial). The core innovation is Pareto training, which implements a dynamic loss weighting scheme where the weight ω for robustness loss is updated at each training step based on the current distance to the estimated Pareto frontier. The training procedure uses in-batch negatives with contrastive ranking loss for 10,000 steps, and the dynamic weight update follows a prescribed formula with η=0.1. The method is evaluated by fitting power-law curves to scaling data and comparing the final joint performance to baseline strategies.

## Key Results
- Robustness follows power-law scaling with both model size and data size, similar to effectiveness.
- Robustness and effectiveness exhibit different scaling patterns, leading to significant resource costs when jointly improving both.
- Pareto training improves scaling efficiency by up to 2.5x compared to standard training, achieving comparable performance to multi-fold scaling of resources.
- Without additional resources, Pareto training can achieve performance comparable to scaling resources several times under traditional optimization strategies.

## Why This Works (Mechanism)

### Mechanism 1: Robustness Follows Power-Law Scaling
Increasing model parameters and training dataset size reduces contrastive entropy (improves robustness) according to power-law relationships. Larger models and datasets provide greater capacity and sample diversity, helping the model establish more stable decision boundaries that generalize better to unseen or perturbed data. The scaling law holds when training and evaluation benchmarks are representative of real-world conditions.

### Mechanism 2: Divergent Scaling Sensitivities Create a Trade-off
Robustness is more sensitive to data size while effectiveness is more responsive to model size. This creates a resource-intensive trade-off because improving both requires disproportionate scaling of both parameters and data. The different sensitivities mean that a strategy optimized for one objective will be suboptimal for the other.

### Mechanism 3: Pareto Training Aligns Optimization with Efficient Joint Scaling
Standard training often over-prioritizes one objective, leading to suboptimal scaling away from the Pareto frontier. Pareto training dynamically adjusts the optimization weights between robustness and effectiveness losses, estimating the distance to the frontier during training and updating the weight to balance both objectives. This alignment moves the optimization path toward the most efficient scaling trajectory.

## Foundational Learning

- **Concept: Dense Retrieval**
  - Why needed: The scaling analysis and Pareto training method are specific to dual-encoder architecture common in dense retrieval.
  - Quick check: Can you explain how a dense retriever differs from a sparse (e.g., BM25) retriever in how it computes relevance between a query and a document?

- **Concept: Out-of-Distribution (OOD) & Adversarial Robustness**
  - Why needed: The paper defines robustness primarily along these two axes and derives scaling laws for each.
  - Quick check: What is the key difference between an OOD test sample (e.g., from BEIR) and an adversarial sample (e.g., from a word substitution attack)?

- **Concept: Pareto Efficiency**
  - Why needed: The central contribution is identifying and exploiting the Pareto frontier between robustness and effectiveness.
  - Quick check: On a 2D plot of robustness vs. effectiveness, what does a point on the Pareto frontier represent compared to a point inside the frontier?

## Architecture Onboarding

- **Component map**: Base Encoder -> Training Pipeline -> Pareto Training Controller -> Evaluation Suite

- **Critical path**:
  1. Define base encoder architecture and fix model size and data size.
  2. Set up training pipelines for effectiveness and robustness data.
  3. Initialize ω₀ based on the ratio of robustness to effectiveness performance at estimated Pareto efficiency.
  4. During each training step, compute losses, update model weights via backprop on L_Pareto, and update ω using the prescribed formula.
  5. Periodically evaluate on both effectiveness and robustness benchmarks to track movement toward Pareto frontier.

- **Design tradeoffs**:
  - Complexity vs. Generality: Pareto training adds dynamic scheduling but is compatible with various base training strategies.
  - Proxy Loss vs. Final Metric: Method relies on training losses as proxies; divergence undermines optimization.
  - Robustness vs. Effectiveness: Method seeks balance; user must decide which point on frontier is desirable.

- **Failure signatures**:
  - Scaling law fails to fit (low R² values in fitting process).
  - Pareto training oscillates or fails to converge.
  - Joint performance fails to approach the frontier traced by other methods.

- **First 3 experiments**:
  1. Replicate scaling law fitting for OOD robustness on BEIR across different model sizes.
  2. Implement baseline strategies (standard, hard-negative, adversarial) and trace the empirical Pareto frontier.
  3. Integrate Pareto training and compare final joint performance to baseline points.

## Open Questions the Paper Calls Out

1. **Do the scaling laws transfer to LLM architectures?** The study focused on BERT-level architectures due to resource constraints; validation on large language models is planned for future work.

2. **How does the trade-off shift in non-dual-encoder architectures?** Other architectures like multi-vector generation or interaction-based models may offer different scaling performances and deserve further exploration.

3. **Do computational costs and attack costs follow scaling laws?** The authors list scaling laws for computational costs and attack costs as aspects left for future work.

## Limitations
- Scaling law generalization across different architectures remains untested.
- Robustness proxy alignment with final metrics is acknowledged as challenging.
- Resource cost assumptions of linear scaling may not hold with architectural innovations.

## Confidence
- **High**: Different scaling sensitivities between robustness and effectiveness creating trade-off.
- **Medium**: Pareto training achieving up to 2.5x better scaling efficiency.
- **Low**: Universal applicability of power-law scaling laws across all model architectures and robustness types.

## Next Checks
1. Test power-law scaling across different dense retrieval architectures (multi-vector, interaction-based).
2. Validate proxy loss alignment by systematically varying training losses and measuring final performance.
3. Evaluate Pareto training convergence by monitoring dynamic weight ω and analyzing stability of performance approach to frontier.