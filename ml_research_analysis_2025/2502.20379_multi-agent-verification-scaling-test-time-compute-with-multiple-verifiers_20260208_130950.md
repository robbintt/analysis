---
ver: rpa2
title: 'Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers'
arxiv_id: '2502.20379'
source_url: https://arxiv.org/abs/2502.20379
tags:
- verifiers
- verification
- arxiv
- aspect
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-Agent Verification (MAV), a new test-time
  compute paradigm that scales performance by combining multiple verifiers to evaluate
  candidate outputs. The authors propose Aspect Verifiers (AVs) - off-the-shelf LLMs
  prompted to verify specific aspects of outputs through binary approvals - as a convenient
  building block since they require no additional training and support easy combination
  via voting.
---

# Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers

## Quick Facts
- **arXiv ID:** 2502.20379
- **Source URL:** https://arxiv.org/abs/2502.20379
- **Reference count:** 40
- **Key outcome:** Introduces Multi-Agent Verification (MAV) with Aspect Verifiers that achieve stronger scaling patterns than self-consistency and reward model verification across MATH, MMLU-Pro, GPQA, and HumanEval benchmarks, with gains up to 20% for smaller models.

## Executive Summary
This paper introduces Multi-Agent Verification (MAV), a new test-time compute paradigm that scales performance by combining multiple verifiers to evaluate candidate outputs. The authors propose Aspect Verifiers (AVs) - off-the-shelf LLMs prompted to verify specific aspects of outputs through binary approvals - as a convenient building block since they require no additional training and support easy combination via voting. Their BoN-MAV algorithm combines best-of-n sampling with AVs, achieving stronger scaling patterns than self-consistency and reward model verification across MATH, MMLU-Pro, GPQA, and HumanEval benchmarks. Performance improves as the number of verifiers increases, with gains up to 20% for smaller models. The approach enables both weak-to-strong generalization (combining weaker verifiers to improve stronger generators) and self-improvement (using the same model for both generation and verification).

## Method Summary
The paper introduces BoN-MAV, which samples n=16 candidate outputs from a generator LLM, then uses m Aspect Verifiers (off-the-shelf LLMs) to produce binary True/False approvals for each candidate. These approvals are aggregated via simple majority voting, and the candidate with the highest vote count is selected. Verifiers are engineered for each domain by selecting subsets that maximize validation performance. The method leverages binary outputs for easy combination across heterogeneous models without calibration, enabling scaling of both candidate count and verifier count as orthogonal dimensions.

## Key Results
- BoN-MAV achieves up to 20% accuracy improvements over pass@1 for smaller models
- Performance scales with verifier count across all tested domains (MATH, MMLU-Pro, GPQA, HumanEval)
- Weak-to-strong generalization enables weaker verifiers to improve stronger generators (e.g., Gemini-1.5-Pro improves from 64.7% to 72.7% on MATH)
- BoN-MAV outperforms both self-consistency and reward model verification baselines
- Verifier diversity matters - using multiple diverse verifiers outperforms repeated use of a single best verifier

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining multiple verifiers through binary voting produces more robust selection signals than single-verifier approaches.
- **Mechanism:** Each Aspect Verifier produces a binary True/False approval based on its specific focus. Simple majority voting aggregates these signals, where individual verifier failures are compensated by collective agreement.
- **Core assumption:** Verifier errors are partially uncorrelated across different base models, aspects, and verification strategies.
- **Evidence anchors:** [abstract] "AVs are a convenient building block for MAV since they can be easily combined without additional training... through voting mechanisms." [Section 2.2] "This voting strategy gives equal weight to all verifiers in the final aggregated score, and it proves remarkably effective in our experiments."
- **Break condition:** If verifier errors become highly correlated (e.g., all share the same systematic blindspot), scaling verifiers yields diminishing returns.

### Mechanism 2
- **Claim:** Scaling the number of verifiers (m) provides an orthogonal scaling dimension to scaling candidate outputs (n).
- **Mechanism:** Increasing n raises the probability that at least one candidate is correct. Increasing m raises the probability that the correct candidate is properly identified. These operate independently and can be parallelized.
- **Core assumption:** The generator produces correct solutions at some non-zero rate, and verifiers can discriminate correct from incorrect solutions above chance.
- **Evidence anchors:** [abstract] "scaling the number of verifiers" is proposed as "a promising new dimension for improving language model performance at test-time." [Figure 6, right] Shows BoN-MAV initially underperforms at low compute budgets but surpasses baselines once sufficient compute allows leveraging multiple verifiers.
- **Break condition:** At extreme verifier counts without diversity (Table 4: querying same verifier repeatedly underperforms diverse sets), or when compute budget is severely constrained.

### Mechanism 3
- **Claim:** Ensembles of weaker verifiers can improve stronger generators (weak-to-strong generalization).
- **Mechanism:** Aggregated judgments from multiple small models produce verification signals robust enough to improve larger generators. Different failure modes across models cancel out through voting.
- **Core assumption:** Weak models can independently recognize specific error types even if they cannot produce correct solutions themselves.
- **Evidence anchors:** [Table 2] Gemini-1.5-Pro improves from 64.7% to 72.7% on MATH when verified by weaker models. [Section 3.2] "the diverse perspectives of multiple smaller models can collectively produce a verification signal robust enough to improve even state-of-the-art generators."
- **Break condition:** If weak verifiers systematically share a blindspot the strong generator also has, no improvement occurs.

## Foundational Learning

- **Concept: Best-of-n Sampling**
  - Why needed here: The foundational technique MAV builds upon—understanding why generating multiple candidates and selecting one is a valid test-time scaling strategy.
  - Quick check question: Given a generator with 40% pass@1 rate, what's the minimum n needed to have >80% probability of at least one correct solution?

- **Concept: Self-Consistency**
  - Why needed here: A key baseline MAV compares against—understanding majority voting over final answers vs. verifier-based selection clarifies MAV's contribution.
  - Quick check question: Why might self-consistency fail when correct answers take different forms (e.g., "0.5" vs "1/2")?

- **Concept: Reward Model Verification**
  - Why needed here: The dominant verification paradigm MAV critiques—understanding why trained reward models resist combination helps motivate Aspect Verifiers.
  - Quick check question: If Reward Model A outputs scores in [-5, 5] and Model B outputs [0, 100], why is naive averaging problematic?

## Architecture Onboarding

- **Component map:** Generator LLM → [n candidates] → Aspect Verifier Pool (m verifiers) → Binary approvals per candidate → Aggregation (sum of votes) → Select highest-scored candidate

- **Critical path:** Verifier engineering (Section 2.4)—selecting domain-appropriate verifier subsets Md from the full pool M. Table 5 shows 6 verifiers for MATH vs. 14 for HumanEval.

- **Design tradeoffs:**
  - **Binary vs. continuous scores:** Binary enables easy combination across heterogeneous models; continuous scores are richer but require calibration.
  - **Equal voting vs. weighted aggregation:** Simple voting is robust; weighting could help but requires calibration data the paper doesn't explore.
  - **Verifier count vs. compute budget:** More verifiers means fewer candidates at fixed budget (Figure 6)—optimal allocation is domain-dependent.

- **Failure signatures:**
  - Low improvement despite many verifiers → verifier errors may be correlated; check diversity across base models.
  - Performance degrades with more verifiers → potential for systematic bias in certain verifier types; ablate individual verifiers.
  - Large gap between validation and test performance → overfitting verifier selection to validation set.

- **First 3 experiments:**
  1. **Establish baseline:** Run BoN-MAV with n=16, m=6 (MATH verifier set) on 50 samples. Compare to self-consistency and pass@1 to verify implementation.
  2. **Scaling analysis:** Fix n=16, vary m from 1 to full set. Plot accuracy curve similar to Figure 5 to confirm scaling behavior on your generator model.
  3. **Ablate verifier diversity:** Compare using all 6 MATH verifiers vs. 6 copies of the single best verifier (Table 4 pattern) to validate diversity contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can more sophisticated aggregation methods (e.g., confidence-weighted voting, verifier debate, or aspect-grouped voting) substantially outperform the simple sum-of-votes aggregation used in BoN-MAV?
- **Basis in paper:** [explicit] Section 2.2 and Section 4 state: "future works could investigate more sophisticated aggregation strategies such as grouping verifiers by aspect and then voting across aspects, or having aspect verifiers debate with each other before producing an approval."
- **Why unresolved:** The paper only implements the simplest aggregation (binary vote summation), and it remains unknown whether incorporating verifier confidence or inter-verifier communication could yield significant gains.
- **What evidence would resolve it:** A systematic comparison of aggregation strategies across the same benchmarks, showing whether confidence-weighted voting or debate mechanisms improve accuracy beyond simple majority voting.

### Open Question 2
- **Question:** Can dynamically selecting verifiers per-question (rather than using a fixed domain-specific set) improve both performance and computational efficiency?
- **Basis in paper:** [explicit] Section 4 states: "our current approach uses a static engineered set of verifiers Md for all questions in a domain d, even though it may be best to use fewer or different verifiers for specific questions. Future works could investigate dynamically selecting the best set of verifiers for particular problems or adaptively choosing additional verifiers based on the results of the first few verification queries."
- **Why unresolved:** The verifier engineering approach selects a fixed subset per domain, ignoring question-specific needs.
- **What evidence would resolve it:** Experiments comparing fixed vs. adaptive verifier selection strategies, measuring both accuracy and compute efficiency (number of verifier queries used).

### Open Question 3
- **Question:** What causes diminishing returns and occasional accuracy drops when scaling the number of verifiers, and can better-engineered verifiers overcome this?
- **Basis in paper:** [inferred] Figure 5 shows that accuracy improvement varies across domains and models, with "some diminishing returns at higher verifier counts" and cases where "accuracy initially decreases before improving with additional verifiers." The authors state they expect "better-engineered verifiers to unlock even stronger scaling patterns."
- **Why unresolved:** The paper does not analyze why certain verifier combinations underperform or what properties determine scaling quality.
- **What evidence would resolve it:** An ablation study analyzing verifier correlations, failure modes, and diversity metrics to identify what verifier characteristics predict strong vs. weak scaling behavior.

### Open Question 4
- **Question:** Can combining multiple generator LLMs with multiple verifiers yield complementary gains beyond single-generator MAV?
- **Basis in paper:** [explicit] Section 4 states: "our implementation of BoN-MAV is limited to only a single generator LLM. Thus, an interesting direction would be to explore sampling from multiple generators in addition to evaluating with multiple verifiers."
- **Why unresolved:** The paper evaluates only single-generator settings, leaving the multi-generator case unexplored.
- **What evidence would resolve it:** Experiments where candidate outputs are sampled from multiple diverse generator models and then evaluated by multiple verifiers, compared against single-generator baselines.

## Limitations

- Implementation details underspecified, particularly the verifier engineering algorithm and candidate sampling hyperparameters
- Binary verification approach may miss nuanced correctness signals compared to continuous scoring methods
- Weak-to-strong generalization gains are modest and domain-dependent, particularly smaller on GPQA

## Confidence

- **Multi-verifier combination provides orthogonal scaling dimension:** High - well-supported by controlled experiments across four benchmarks
- **Aspect Verifiers are a convenient building block:** High - strong empirical support with diversity ablation studies
- **Weak-to-strong generalization through verification:** Medium - supported by MATH results but gains are smaller on other domains
- **BoN-MAV outperforms self-consistency and reward models:** High - multiple benchmarks and ablation studies provide strong evidence

## Next Checks

1. **Diversity validation:** Replicate Table 4's experiment by comparing performance when using all six MATH verifiers versus six copies of the single best verifier. Measure both absolute accuracy and inter-verifier vote correlation to confirm that diversity contributes to performance gains.

2. **Scaling threshold analysis:** Conduct a fine-grained scaling study varying both n and m to identify the compute budget threshold where BoN-MAV surpasses self-consistency (similar to Figure 6). Test multiple generator models to verify this transition point is consistent.

3. **Weak-to-strong ablation:** Test the weak-to-strong generalization claim by creating controlled scenarios where verifiers have known blind spots. Generate outputs where strong generators succeed in areas where weak verifiers fail, and vice versa, to verify that aggregation actually compensates for individual model limitations.