---
ver: rpa2
title: Testing the Limits of Machine Translation from One Book
arxiv_id: '2508.06665'
source_url: https://arxiv.org/abs/2508.06665
tags:
- translation
- grammar
- kanuri
- sentences
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the effectiveness of using grammar books
  as language resources for low-resource machine translation. The authors tested GPT-4.1
  on Kanuri translations using three different resource combinations: grammar only,
  parallel sentences only, and grammar plus parallel sentences.'
---

# Testing the Limits of Machine Translation from One Book

## Quick Facts
- arXiv ID: 2508.06665
- Source URL: https://arxiv.org/abs/2508.06665
- Reference count: 9
- Primary result: Parallel sentences outperform grammar resources for LLM-based Kanuri translation, with accuracy scores around 35% versus less than 15% for grammar-only approaches.

## Executive Summary
This paper evaluates whether grammar books can serve as effective language resources for low-resource machine translation using GPT-4.1 to translate Kanuri (Yerwa dialect) from English. The authors test three resource combinations: grammar only, parallel sentences only, and grammar plus parallel sentences, comparing against native speaker translations across dictionary-derived and domain-specific humanitarian sentences. Results demonstrate that parallel sentences remain the most effective data source, outperforming grammar resources in both human evaluations and automatic metrics. The study reveals that while LLMs can preserve semantic meaning in low-resource translation, they struggle with grammatical fluency, and grammar books alone prove insufficient for effective domain-specific translation.

## Method Summary
The study uses GPT-4.1 with up to 1M token context to translate Kanuri using four protocols: native translators, GPT-4.1 with dictionary+grammar+parallel sentences, GPT-4.1 with dictionary+parallel sentences only, and human linguist with the same resources as the second protocol. Test sets include 1,000 dictionary-derived sentences and 1,000 humanitarian domain sentences. Evaluation combines automatic metrics (ChrF, ChrF++) with human judgments on fluency and accuracy. The grammar resource (Hutchison [1981], 373 pages) was freshly OCR'd and post-edited to ensure it wasn't in model training data. Parallel sentences (2,001 pairs) were extracted from the grammar, and native speaker reference translations were collected for evaluation.

## Key Results
- Parallel sentences achieved approximately 35% accuracy versus less than 15% for grammar-only approaches
- Human evaluations showed LLMs performed better on meaning accuracy than grammatical fluency
- Grammar provided negligible value for domain generalization when parallel sentences already covered similar lexical distributions
- ChrF scores for Kanuri Dictionary/Parallel condition reached 28.85% accuracy vs 21.15% for Grammar-only

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel sentences provide stronger translation signal than declarative grammar rules for LLM-based low-resource MT.
- Mechanism: LLMs more effectively pattern-match against example input-output pairs than apply explicit grammatical rules, even when rules are semantically equivalent information.
- Core assumption: The grammar contains sufficient information for translation; the bottleneck is LLM ability to extract and apply it.
- Evidence anchors:
  - [abstract] "Results demonstrate that parallel sentences remain the most effective data source, outperforming other methods in human evaluations and automatic metrics."
  - [page 10, Table 3] Kanuri Dictionary/Parallel achieved 28.85% accuracy vs 21.15% for Grammar; ChrF 19.19 vs 17.71
  - [corpus] Aycock et al. [2024] found "LLM did not appear to use the declarative grammar for translation" though useful for grammatical tasks.

### Mechanism 2
- Claim: Meaning accuracy emerges before grammatical fluency in low-resource LLM translation.
- Mechanism: LLMs preserve semantic content via cross-lingual transfer from high-resource languages, but target-language morphosyntax requires exemplar exposure that limited parallel data cannot fully provide.
- Core assumption: Human evaluators can validly separate accuracy from fluency; back-translation neutralizes fluency variance.
- Evidence anchors:
  - [abstract] "Human evaluations reveal that LLMs achieve accuracy (meaning) more effectively than fluency (grammaticality)."
  - [page 10, Table 3] Humanitarian/Parallel: Fluency 8.82% vs Accuracy 35.29%—a ~4x gap
  - [corpus] Limited corpus evidence on this specific asymmetry; related work (Lommel et al.) establishes multidimensional evaluation frameworks but not LLM-specific patterns.

### Mechanism 3
- Claim: Grammar provides negligible value for domain generalization when parallel sentences already cover similar lexical distributions.
- Mechanism: The paper hypothesized grammar would help generalize to new domains (humanitarian) where parallel sentences were scarce; results showed grammar added no significant improvement beyond parallel-only prompting.
- Core assumption: Humanitarian domain is sufficiently distinct from grammar-embedded examples; token-overlap analysis validates this.
- Evidence anchors:
  - [page 5, Figure 1] Token-level analysis showing humanitarian dataset has lower grammar overlap than dictionary dataset
  - [page 10, Table 3] Humanitarian/Grammar (23.40 ChrF) vs Humanitarian/Parallel (24.34 ChrF)—within one standard deviation
  - [corpus] Hus & Anastasopoulos [2024] found across 16 languages, dictionary+parallel outperformed full grammar in most cases.

## Foundational Learning

- **In-Context Learning (ICL):**
  - Why needed here: The entire MTOB approach relies on LLMs learning translation patterns from context window examples without gradient updates.
  - Quick check question: Can you explain why adding more grammar text to context doesn't proportionally improve translation quality?

- **ChrF/ChrF++ Metrics:**
  - Why needed here: Paper uses these as primary automatic metrics; understanding their character-level n-gram basis explains why they poorly correlate with human judgments at low scores.
  - Quick check question: Why would ChrF penalize a semantically correct translation that uses different word forms?

- **Back-Translation Evaluation:**
  - Why needed here: Paper uses round-trip translation to isolate accuracy from fluency by having native speakers back-translate LLM outputs.
  - Quick check question: What confound remains when using back-translation to measure forward-translation accuracy?

## Architecture Onboarding

- **Component map:**
  Grammar OCR + post-editing → Parallel sentence extraction → Dictionary/glossary integration → Prompting protocols → Evaluation layer

- **Critical path:**
  1. Verify grammar is not in model training data (freshly digitized)
  2. Extract parallel sentences; validate tokenization alignment
  3. Construct test sets with controlled domain specificity (dictionary vs humanitarian)
  4. Run translation protocols with matched prompting templates
  5. Collect native speaker judgments using blind pairwise comparison

- **Design tradeoffs:**
  - Whole-grammar context (~300K tokens) vs parallel-only (~2K sentences): Paper shows parallel-only wins on efficiency and quality
  - Automatic metrics vs human evaluation: ChrF is cheap but noisy at low scores; human judgment is gold standard but expensive
  - Test set complexity: Simple sentences inflate scores; domain-specific sentences reveal real limits

- **Failure signatures:**
  - ChrF improves but human fluency doesn't → model overfitting to metric artifacts
  - Grammar+Parallel underperforms Parallel-only → grammar adding noise, not signal
  - Accuracy >> Fluency in human eval → model preserving meaning but failing morphosyntax
  - Large variance between native speaker judgments → dialect/orthography inconsistency

- **First 3 experiments:**
  1. **Baseline reproduction:** Run GPT-4.1 on Kalamang with Gemini-style prompting to validate infrastructure (target: ~56-59 ChrF as reported).
  2. **Ablation on resource type:** Compare Grammar+Dict vs Parallel+Dict on 50 sentences each; expect parallel to win by 2+ ChrF points.
  3. **Domain shift test:** Translate 20 humanitarian sentences with and without domain glossary; measure accuracy gap via back-translation to validate that glossary terms improve meaning preservation.

## Open Questions the Paper Calls Out
None

## Limitations
- Resource fidelity and model exposure uncertainty: While grammar was freshly OCR'd, uncertainty remains about whether similar grammatical descriptions existed in pretraining data
- Test set representativeness: With only 102-100 sentences per domain, results may not generalize to broader linguistic phenomena in Kanuri
- Evaluation protocol opacity: Back-translation accuracy evaluation lacks clarity on whether LLMs or humans performed the round-trip translations

## Confidence

**High Confidence:**
- Parallel sentences outperform grammar resources across both automatic metrics and human evaluation
- Meaning accuracy emerges before grammatical fluency in LLM outputs
- Grammar provides negligible value for domain generalization when parallel sentences cover similar lexical distributions

**Medium Confidence:**
- The specific superiority margin between resource types (e.g., 35% vs <15% accuracy)
- The assertion that grammar is "insufficient" rather than "suboptimal" for low-resource MT
- The generalizability of findings to other low-resource language pairs

**Low Confidence:**
- The claim that LLMs cannot effectively learn from declarative grammar rules at all
- The precise threshold at which grammar becomes useful (if reformatted differently)
- The scalability of these findings to production MT systems

## Next Checks
1. **Ablation study with reformatted grammar:** Extract key grammatical patterns from the Kanuri grammar and reformat them as example-based rules (pattern exemplars) rather than declarative text. Compare against the original grammar-only condition to test whether the issue is grammar format versus grammar content.

2. **Cross-linguistic replication:** Repeat the study with another low-resource language (e.g., from the 16 languages tested in Hus & Anastasopoulos [2024]) to determine if the parallel-vs-grammar superiority pattern holds across linguistic families and resource types.

3. **Incremental parallel data scaling:** Systematically increase the number of parallel sentences from the current 2,001 to test at what point (if any) grammar resources begin to provide complementary value rather than noise, helping distinguish between data-quantity and structural limitations.