---
ver: rpa2
title: Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank
arxiv_id: '2510.16819'
source_url: https://arxiv.org/abs/2510.16819
tags:
- documents
- query
- training
- document
- reranker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-stage retrieve-and-rerank framework
  for cross-genre authorship attribution using fine-tuned large language models. The
  approach addresses the challenge of distinguishing authorial style from topical
  content by leveraging a bi-encoder retriever followed by a cross-encoder reranker,
  both optimized with targeted negative sampling strategies.
---

# Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank

## Quick Facts
- **arXiv ID:** 2510.16819
- **Source URL:** https://arxiv.org/abs/2510.16819
- **Reference count:** 26
- **Primary result:** Achieves state-of-the-art cross-genre authorship attribution with 22.3-34.4 absolute Success@8 gains over previous models

## Executive Summary
This paper addresses the challenge of cross-genre authorship attribution (AA), where the goal is to identify the author of a document when the query and candidate documents differ in both genre and topic. The authors propose a two-stage retrieve-and-rerank framework using fine-tuned large language models (LLMs). A bi-encoder retriever efficiently narrows down candidates from tens of thousands to top-100, followed by a cross-encoder reranker that jointly scores query-candidate pairs. Experiments on HIATUS benchmarks (HRS1 and HRS2) show significant improvements over previous methods, with the reranker particularly effective when trained with negatives sampled from multiple categories (near-query, near-positive, and random).

## Method Summary
The approach uses a two-stage pipeline: first, a bi-encoder retriever based on LLM backbones (Qwen3-0.6B to 12B, or Mistral-12B) encodes documents independently and scores them via dot product, trained with hard-negative mining via k-means clustering. Second, a cross-encoder reranker (Mistral-12B) jointly encodes query-candidate pairs using a special delimiter, trained with 12 negatives per query (4 from each of random, near-query, and near-positive categories). Both stages use supervised contrastive loss and LoRA fine-tuning. The retriever outputs top-100 candidates, which the reranker then reorders to produce the final top-8 predictions. Training data is curated to ensure cross-genre pairs have low SBERT similarity (<0.2) to avoid topical confounding.

## Key Results
- **Performance scaling:** Success@8 improves consistently from 0.6B to 12B parameters (HRS1: 27.5 → 42.1; HRS2: 20.3 → 35.7).
- **Negative sampling:** Mixed negatives (n(r)+n(dq)+n(d+)) achieve 48.3 Success@8, outperforming n(dq)-only (34.8) and n(r)-only (43.8) on HRS1.
- **Retrieve-and-rerank gains:** Reranker improves Success@8 by 6.2 (HRS1) and 8.5 (HRS2) points over retriever alone, leveraging the retriever's strong Success@100.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM backbones improve cross-genre authorship attribution over smaller encoder models, with gains scaling predictably with model size.
- **Mechanism:** LLMs' larger parameter capacity captures more nuanced stylistic patterns that persist across genres, whereas smaller models like RoBERTa-large (0.3B) conflate style with topical cues more easily. The paper shows a clear scaling trend: Qwen3-0.6B (27.5 Success@8 on HRS1) → Qwen3-8B (35.7) → Mistral-12B (42.1).
- **Core assumption:** Stylistic features relevant to authorship are more disentangleable in higher-dimensional representation spaces.
- **Evidence anchors:**
  - [Table 2]: Shows consistent improvement from 0.6B to 12B parameters on both HRS1 and HRS2.
  - [Section 4]: "larger models yield consistent improvements, with performance rising from 0.6B to 8B parameters."
  - [corpus]: Related work on LLM-based code authorship (arXiv:2501.08165) reports similar scaling benefits, though in a different domain.
- **Break condition:** If model scaling plateaus beyond 12B (hardware-constrained in this work), returns may diminish. The paper explicitly notes this as an open question.

### Mechanism 2
- **Claim:** The reranker fails when trained with IR-style negatives (near query only), but succeeds when negatives span three categories: near-query [n(dq)], near-positive [n(d+)], and random [n(r)].
- **Mechanism:** Cross-genre training pairs are deliberately topically dissimilar (SBERT similarity < 0.2). When negatives are sampled only from n(dq), the model learns an "anti-topic" heuristic: score documents semantically similar to the query as negative. This backfires because the true positive is also topically distant. Including n(d+) negatives forces the model to discriminate between the positive and topically similar distractors, breaking the anti-topic shortcut.
- **Core assumption:** The three negative categories provide orthogonal learning signals that together cover the failure modes specific to cross-genre AA.
- **Evidence anchors:**
  - [Section 2.3]: Describes the three negative categories and the asymmetry between IR and AA.
  - [Table 3]: n(dq) only achieves 34.8 Success@8 (worst); n(r)+n(dq)+n(d+) achieves 48.3 (best).
  - [Section 4]: "selecting negatives only around the query... yields the weakest performance... the optimization reduces to learning an anti-topic heuristic."
  - [corpus]: Weak external validation—no directly comparable negative sampling studies for cross-genre AA found in corpus.
- **Break condition:** If query-positive pairs share topical similarity (violating cross-genre assumptions), the anti-topic heuristic may not form, and n(dq) sampling could work. Not tested in this paper.

### Mechanism 3
- **Claim:** Two-stage retrieve-and-rerank pipelines are necessary when the retriever's Success@100 substantially exceeds Success@8.
- **Mechanism:** The retriever places the correct author within top-100 but not top-8 (e.g., Mistral retriever: 42.1 Success@8 vs 84.9 Success@100 on HRS1). The reranker's joint encoding of query-candidate pairs provides higher discriminative power, recovering the correct author from the top-100 into the top-8.
- **Core assumption:** The retriever's errors are primarily ranking errors within the top-100, not retrieval failures (i.e., the needle is usually in top-100).
- **Evidence anchors:**
  - [Section 4]: "while the retriever often fails to place the correct document within the top-8, it frequently succeeds in ranking it within the top-100."
  - [Table 3]: Reranker improves Success@8 by 6.2 (HRS1) and 8.5 (HRS2) points over retriever alone.
  - [corpus]: "Beyond Sequential Reranking" (arXiv:2509.07163) discusses limitations of retrieve-and-rerank when initial retrieval quality is poor, supporting the importance of a strong retriever.
- **Break condition:** If the retriever's Success@100 is low, the reranker has insufficient signal to improve. The paper addresses this by using a strong LLM-based retriever first.

## Foundational Learning

- **Concept: Supervised Contrastive Learning**
  - **Why needed here:** Both retriever and reranker are trained using contrastive loss (Eq. 2), which learns by pulling same-author pairs together and pushing different-author pairs apart in embedding space.
  - **Quick check question:** Given a batch with N authors and 2 documents each, can you identify how many negative pairs each query document is compared against in the retriever's training?

- **Concept: Bi-encoder vs Cross-encoder Architectures**
  - **Why needed here:** The retriever uses a bi-encoder (documents encoded independently, efficiency O(|Q|+|C|)) while the reranker uses a cross-encoder (joint encoding, accuracy but O(|Q|×|C|) cost). This tradeoff motivates the two-stage design.
  - **Quick check question:** Why can't the cross-encoder scale to the full candidate pool of tens of thousands of documents?

- **Concept: Hard Negative Mining**
  - **Why needed here:** Training benefits from negatives that are "hard" (similar to query or positive in embedding space). The retriever uses clustering-based hard negative batching; the reranker explicitly samples negatives from n(dq) and n(d+) categories.
  - **Quick check question:** In the reranker, why might sampling only from n(dq) be considered "hard negative mining" but still fail for cross-genre AA?

## Architecture Onboarding

- **Component map:** Query Document (dq) → [Retriever: LLM bi-encoder + mean pooling + projection] → Top-100 candidates → [Reranker: LLM cross-encoder, trained with 3-category negatives] → Ranked candidates → Success@8 evaluation

- **Critical path:**
  1. Training data curation: Filter author pairs by SBERT similarity < 0.2 threshold (Section 3.1)
  2. Retriever training: LoRA fine-tuning with hard-negative batching (Algorithm 1), 1 epoch
  3. Reranker training: LoRA fine-tuning with m=12 negatives per query (4 from each of n(r), n(dq), n(d+)), 10% of retriever's training data
  4. Inference: Retrieve top-100 → Rerank → Return top-8

- **Design tradeoffs:**
  - **Retriever model size vs. speed:** Mistral-12B is ~6× slower than RoBERTa-large at inference (Limitations section). Consider Qwen3-4B or -8B for production.
  - **Reranker training data fraction vs. performance:** Only 10% of training data used due to compute costs. Paper does not ablate this—potential improvement opportunity.
  - **Top-k for reranking:** Paper uses k=100. Lower k saves compute; higher k may improve recall if retriever quality varies.

- **Failure signatures:**
  - **Retriever Success@8 near random:** Check if training data has insufficient cross-genre pairs (SBERT threshold too high) or if in-batch negatives aren't hard enough (clustering failed).
  - **Reranker underperforms retriever:** Likely trained with n(dq)-only negatives. Verify negative sampling distribution.
  - **Both stages fail on new dataset:** Check for PII artifacts in training data that leaked author identity; verify test data follows cross-genre constraints.

- **First 3 experiments:**
  1. **Reproduce retriever scaling curve:** Train Qwen3-0.6B, -1.7B, -4B on the public training data (Section 3.1 / Appendix A). Verify Success@8 and Success@100 match Table 2 trends on HRS1/HRS2 splits.
  2. **Ablate reranker negative sampling:** Train three rerankers with n(dq) only, n(r) only, and n(r)+n(dq)+n(d+). Confirm the failure mode and recovery shown in Table 3.
  3. **Test on in-genre setting:** Train and evaluate with SBERT threshold > 0.5 (topically similar pairs). Hypothesis: n(dq)-only sampling should improve relative to cross-genre, validating the anti-topic heuristic mechanism.

## Open Questions the Paper Calls Out

- **Does performance scaling continue beyond 12B parameters?** The authors were restricted to Mistral-Nemo-Base-2407 (12B) due to computational resource limits, and determining whether attribution system performance improves upon using even larger models "remains an open question."

- **How would alternative closeness metrics affect negative sampling?** While the paper uses SBERT for measuring "closeness" in negative sampling, the authors note that "alternative measures of closeness... are also widely used, and exploring these remains an interesting direction for future work."

- **Can reranking be extended beyond top-100 candidates?** The authors limit the reranker to top-100 candidates because of its "quadratic run-time complexity," implying a constraint on the recall scope that remains unexplored.

## Limitations

- **Dataset accessibility:** HIATUS HRS1/HRS2 test datasets require IARPA program participation, preventing independent validation of state-of-the-art claims.

- **Hardware constraints:** The observed scaling benefits may not continue beyond 12B parameters, and the paper explicitly acknowledges this limitation.

- **Mechanism validation:** The "anti-topic heuristic" explanation for why n(dq)-only sampling fails lacks external validation through ablation studies on non-cross-genre datasets.

## Confidence

- **High confidence:** Retrieve-and-rerank architecture design and general scaling trends with LLM size. Mathematical formulations and architectural descriptions are clear and reproducible.
- **Medium confidence:** Specific negative sampling strategy effectiveness and interpretation of why n(dq)-only sampling fails. Experimental evidence is strong within the paper's framework, but underlying mechanism explanation would benefit from additional ablation studies.
- **Low confidence:** Absolute performance numbers without access to HIATUS test sets. State-of-the-art claims cannot be independently verified due to dataset accessibility constraints.

## Next Checks

1. **Replicate the negative sampling ablation:** Train three reranker variants with n(dq)-only, n(r)-only, and mixed n(r)+n(dq)+n(d+) negatives on a publicly available cross-genre dataset to confirm the observed performance pattern.

2. **Test the anti-topic heuristic hypothesis:** Evaluate the reranker on an in-genre authorship attribution dataset where query-positive pairs have high topical similarity (SBERT > 0.5) to determine if n(dq)-only sampling actually improves performance when the cross-genre assumption is violated.

3. **Verify retriever quality dependency:** Systematically vary the retriever's Success@100 threshold (e.g., top-50 vs top-100 vs top-200) to quantify how much the reranker's performance depends on the initial retrieval quality, as predicted by the failure condition analysis.