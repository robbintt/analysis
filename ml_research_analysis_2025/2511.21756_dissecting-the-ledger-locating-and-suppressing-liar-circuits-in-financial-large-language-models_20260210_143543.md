---
ver: rpa2
title: 'Dissecting the Ledger: Locating and Suppressing "Liar Circuits" in Financial
  Large Language Models'
arxiv_id: '2511.21756'
source_url: https://arxiv.org/abs/2511.21756
tags:
- layer
- financial
- arithmetic
- causal
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses hallucinations in Large Language Models during
  financial arithmetic reasoning by identifying and suppressing specific internal
  mechanisms. Using Causal Tracing on GPT-2 XL, the authors discovered a dual-stage
  mechanism: a distributed computational "scratchpad" in middle layers (L12-L30) and
  a decisive "aggregation" circuit in Layer 46.'
---

# Dissecting the Ledger: Locating and Suppressing "Liar Circuits" in Financial Large Language Models

## Quick Facts
- arXiv ID: 2511.21756
- Source URL: https://arxiv.org/abs/2511.21756
- Reference count: 3
- This paper addresses hallucinations in Large Language Models during financial arithmetic reasoning by identifying and suppressing specific internal mechanisms.

## Executive Summary
This paper addresses hallucinations in Large Language Models during financial arithmetic reasoning by identifying and suppressing specific internal mechanisms. Using Causal Tracing on GPT-2 XL, the authors discovered a dual-stage mechanism: a distributed computational "scratchpad" in middle layers (L12-L30) and a decisive "aggregation" circuit in Layer 46. They validated this mechanism through ablation, showing that suppressing Layer 46 reduces hallucinatory output confidence by 81.8%. Further, a linear probe trained on Layer 46 activations achieved 98% accuracy in detecting hallucinations on unseen financial topics, demonstrating a universal geometry of arithmetic deception. This work provides a mechanistic approach to intrinsic hallucination detection, moving beyond black-box evaluations to root-cause analysis.

## Method Summary
The paper employs TransformerLens to perform Causal Tracing on GPT-2 XL, identifying functionally critical layers for arithmetic reasoning in financial contexts. The method involves corrupting inputs, patching clean activations at various (token, layer) pairs, and measuring causal impact via probability restoration. Layer 46 emerges as the decisive "aggregation" bottleneck. The authors then validate this through ablation (zeroing Layer 46's residual contribution) and demonstrate a 98% generalization accuracy of a linear probe trained on Layer 46 activations for detecting hallucinations across unseen financial domains.

## Key Results
- Layer 46 ablation reduces hallucinatory output confidence by 81.8% (0.0522 → 0.0095)
- Linear probe trained on Layer 46 activations achieves 98% generalization accuracy to unseen financial topics
- Causal Tracing identifies a distributed "scratchpad" in L12-L30 and decisive "aggregation" circuit in Layer 46

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Arithmetic reasoning in GPT-2 XL operates through a distributed computational "scratchpad" in middle layers (L12-L30), localized at operand tokens.
- Mechanism: Hidden states at operand positions in layers 12-30 carry sustained causal impact on correct outputs. Patching clean activations to these sites restores computation corrupted elsewhere.
- Core assumption: The causal tracing methodology (from Meng et al., 2022) correctly isolates functionally important activations rather than spurious correlations.
- Evidence anchors:
  - [abstract] "a distributed computational 'scratchpad' in middle layers (L12-L30)"
  - [Section 3.1, Observation 1] "sustained, distributed causal impact in Layers 12 through 30, specifically localized at the operand tokens"
  - [corpus] K-MSHC paper corroborates head-level circuit localization in transformers, though on syntactic tasks, not arithmetic.
- Break condition: If operand tokenization changes (e.g., numbers split differently), the spatial localization may shift or dissolve.

### Mechanism 2
- Claim: Layer 46 acts as a decisive aggregation circuit—a single bottleneck that consolidates upstream calculations before final decoding.
- Mechanism: The highest causal impact (0.0073) occurs at Layer 46 on the final token position. Ablating this layer reduces hallucinatory output confidence by 81.8% (0.0522 → 0.0095).
- Core assumption: Assumption: Layer 46's role as "aggregator" generalizes beyond the ConvFinQA benchmark and GPT-2 XL architecture.
- Evidence anchors:
  - [abstract] "a decisive 'aggregation' circuit in late layers (specifically Layer 46)"
  - [Section 3.2] "suppressing the activation of this layer during inference... observed an 81.8% reduction in the model's confidence for the hallucinatory output"
  - [corpus] TraceRouter paper discusses cross-layer circuits for harmful semantics, supporting the plausibility of late-layer bottlenecks, but does not directly validate Layer 46 specifically.
- Break condition: In models with different layer counts or architectures, the aggregation site will likely differ; this is not a universal layer index.

### Mechanism 3
- Claim: Hallucination states occupy a linearly separable geometry in Layer 46's residual stream, enabling detection via simple probes.
- Mechanism: A logistic regression probe trained on Layer 46 activations achieves 98% generalization accuracy to unseen financial topics (trained on Corporate Finance, tested on Stock Trading).
- Core assumption: Assumption: The "universal geometry of arithmetic deception" extends beyond the two tested financial domains.
- Evidence anchors:
  - [abstract] "a linear probe trained on this layer generalizes to unseen financial topics with 98% accuracy"
  - [Section 4] "probe achieved a generalization accuracy of 98% on the held-out Stock topic"
  - [corpus] No direct corpus corroboration for universal hallucination geometry; related work on circuit probes (CircuitProbe) addresses visual semantics, not arithmetic reasoning.
- Break condition: If hallucination types differ qualitatively (e.g., logical vs. arithmetic errors), the linear separability may not hold.

## Foundational Learning

- Concept: **Causal Tracing / Activation Patching**
  - Why needed here: This is the core diagnostic method used to locate functionally important hidden states by measuring their "restorative potential" when patched into corrupted runs.
  - Quick check question: Can you explain why patching a clean activation into a corrupted forward pass isolates causal contribution?

- Concept: **Residual Stream Architecture**
  - Why needed here: The paper hooks into `resid_pre` and sets layer contributions to zero; understanding residual connections is essential to interpret what suppression means.
  - Quick check question: What happens to downstream computation when a single layer's residual contribution is zeroed?

- Concept: **Linear Probing**
  - Why needed here: The practical application relies on training a logistic regression on activations to detect hallucinations—requires understanding what linear separability implies about representation structure.
  - Quick check question: If a linear probe achieves high accuracy, what does that suggest about the structure of the representation space?

## Architecture Onboarding

- Component map:
  - GPT-2 XL (48 layers, 1.5B parameters) -> residual stream at each layer
  - Middle layers (L12-L30): operand-level computation (scratchpad)
  - Late layer (L46): aggregation bottleneck (gatekeeper)
  - TransformerLens hooks: `resid_pre` access for patching and ablation

- Critical path:
  1. Tokenize financial query -> identify operand tokens and final token positions
  2. Run Causal Tracing: iterate over (token, layer) pairs, patch clean activations, measure impact
  3. Identify high-impact sites (L12-L30 operands, L46 final token)
  4. Validate via ablation (zero out L46 residual contribution)
  5. Train linear probe on L46 activations; test generalization

- Design tradeoffs:
  - Ablation at L46 reduces hallucination confidence but may also suppress correct outputs—paper does not report impact on non-hallucinated samples
  - Probe generalization tested only across two financial subdomains; broader domain shift unverified
  - Causal tracing is computationally expensive (O(layers × tokens) forward passes)

- Failure signatures:
  - If operand tokenization splits numbers into multiple tokens, the L12-L30 localization may fragment
  - If aggregation layer shifts in other model sizes, fixed L46 index will fail
  - Linear probe may overfit to distribution-specific error patterns, not "universal" geometry

- First 3 experiments:
  1. **Reproduce causal tracing heatmap** on a single ConvFinQA example using TransformerLens; verify L12-L30 and L46 peaks appear as reported.
  2. **Run L46 ablation study** on 20 examples (10 correct, 10 hallucinated); measure confidence reduction and check for collateral damage to correct outputs.
  3. **Train and test linear probe** with train/test split across two distinct financial topics; confirm ~98% generalization or identify where it breaks down.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "Liar Circuit" mechanism (late-layer aggregation) generalize to contemporary, larger architectures (e.g., Llama-3, GPT-4), or is it specific to GPT-2 XL's depth and size?
- Basis in paper: [inferred] The study restricts its methodology to GPT-2 XL (1.5B parameters), leaving the scalability of this specific mechanism unverified.
- Why unresolved: Different architectures may distribute arithmetic processing differently or utilize distinct layers for aggregation.
- What evidence would resolve it: Replicating the Causal Tracing and ablation protocol on diverse, larger model families to locate analogous bottlenecks.

### Open Question 2
- Question: Does suppressing the "Liar Layer" (L46) result in correct reasoning, or does it merely degrade the model's ability to output confident answers?
- Basis in paper: [inferred] The paper reports an 81.8% reduction in confidence for hallucinatory outputs but does not explicitly report if the model subsequently outputs the correct arithmetic answer.
- Why unresolved: Ablation (setting contributions to zero) could destroy the circuit's functionality entirely rather than steering it toward truth.
- What evidence would resolve it: Evaluating the exact-match accuracy of outputs after Layer 46 suppression, rather than solely measuring confidence reduction.

### Open Question 3
- Question: Is the "universal geometry of arithmetic deception" truly independent of the financial domain?
- Basis in paper: [explicit] The authors suggest a "universal geometry" after testing generalization across "unseen financial topics" (Corporate Finance vs. Stock Trading).
- Why unresolved: The probe's 98% success rate might rely on financial linguistic features common to both sub-domains, rather than a general arithmetic error manifold.
- What evidence would resolve it: Testing the linear probe trained on financial data against general mathematical reasoning benchmarks (e.g., GSM8K) entirely outside the finance domain.

## Limitations
- Causal Tracing results rely on specific tokenization patterns; changes in numeric formatting could shift the identified operand sites (L12-L30), breaking the mechanism's spatial assumptions.
- The L46 aggregation role is architecture-dependent; other model sizes or architectures will likely have different critical layers, limiting generalizability.
- Linear separability is tested only across two financial domains; broader domain generalization and robustness to different hallucination types (e.g., logical vs. arithmetic) remain unproven.

## Confidence
- **High**: Layer 46 ablation reduces hallucination confidence (81.8%) — directly measured and reported.
- **Medium**: Distributed "scratchpad" in L12-L30 — causal tracing supports this, but localization depends on tokenization specifics.
- **Medium**: Linear probe generalization (98%) — strong within tested domains, but scope limited to two subdomains.

## Next Checks
1. **Tokenization Sensitivity Test**: Run causal tracing on the same ConvFinQA examples with alternative number formatting (e.g., "1,000" vs. "1000") to verify whether L12-L30 operand localization persists.
2. **Cross-Architecture Probe Transfer**: Train the Layer 46 linear probe on GPT-2 XL, then test it on a different transformer architecture (e.g., LLaMA-7B) to assess whether the "universal geometry" claim holds beyond a single model family.
3. **Ablation Impact on Correct Outputs**: Systematically ablate Layer 46 across a balanced dataset (50% hallucinated, 50% correct) and measure not only confidence reduction but also degradation in non-hallucinated answers to quantify collateral damage.