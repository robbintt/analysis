---
ver: rpa2
title: 'Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia'
arxiv_id: '2509.23023'
source_url: https://arxiv.org/abs/2509.23023
tags:
- detective
- mafioso
- diana
- alice
- night
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Mini-Mafia, a simplified four-player social
  deduction game designed to benchmark three core interactive capabilities of large
  language models: deception, deception detection, and information disclosure. The
  game isolates these skills through role-specific win conditions, reducing complex
  interactions to a single day phase of discussion and voting.'
---

# Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia

## Quick Facts
- arXiv ID: 2509.23023
- Source URL: https://arxiv.org/abs/2509.23023
- Reference count: 26
- One-line primary result: Mini-Mafia isolates three LLM capabilities (deception, detection, disclosure) in a simplified social deduction game, revealing that smaller models can outperform larger ones in strategic reasoning.

## Executive Summary
This paper introduces Mini-Mafia, a four-player social deduction game designed to benchmark three core interactive capabilities of large language models: deception, deception detection, and information disclosure. The game isolates these skills through role-specific win conditions, reducing complex interactions to a single day phase of discussion and voting. A theoretical model shows that game outcomes depend on three intrinsic parameters—deception capability of the mafioso, disclosure capability of the detective, and detection sensitivity of the villager—through the relationship logit(p) = v×(m-d). Experiments reveal counterintuitive findings, such as smaller models like Grok 3 Mini outperforming larger ones in detection, and identify emergent phenomena like name bias and last-speaker advantages. Human baselines show that while LLMs excel at persuasive communication, they lag behind in strategic reasoning for agentic interaction. The framework provides both a benchmark and a research platform for studying multi-agent dynamics and advancing AI safety through deception detection.

## Method Summary
Mini-Mafia is a 4-player social deduction game (1 mafioso, 1 detective, 2 villagers) with a single day phase. Each night, the mafioso kills a random villager and the detective investigates to learn the mafioso's identity. During the day, players discuss for 2-8 rounds and then vote blindly. The game isolates three capabilities: deception (mafioso), detection (villager), and disclosure (detective). The authors run 14,000 games across 140 configurations using 10 different LLMs, fitting a theoretical model logit(p_ijk) = v_k × (m_i - d_j) via Bayesian inference (PyMC, NUTS sampler). They validate rankings against background-based methodology and analyze emergent biases like name effects and last-speaker advantages.

## Key Results
- Smaller models like Grok 3 Mini outperform larger ones like Claude Sonnet 4 in detection accuracy
- Detection success hinges on trusting the first player to claim investigative authority rather than sophisticated reasoning
- Name attribution creates systematic biases: Bob outperforms Diana by 2.20 percentage points in win rates
- Last speakers gain persuasive advantage: detectives achieve +7.10 percentage points when speaking last
- LLMs excel at persuasive communication but lag behind humans in strategic reasoning for agentic interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent dialogue outcomes can be compressed into three latent capability parameters.
- Mechanism: The game enforces strict role-specific win conditions that isolate deception (mafioso), detection (villager), and disclosure (detective). The theoretical model logit(p) = v × (m - d) captures how these parameters interact probabilistically, where the deception-disclosure gap is modulated by villager sensitivity.
- Core assumption: Agent behaviors in this constrained setting generalize to broader multi-agent interactions.
- Evidence anchors:
  - [abstract] "logit(p) = v × (m - d) where m, d, and v are intrinsic model parameters"
  - [section 2.4.1] "Remarkably, our model takes the functional form of the Fermi-Dirac distribution"
  - [corpus] WOLF paper (FMR=0.49) addresses LLM deception but focuses on classification; limited direct validation of parametric models.
- Break condition: If model rankings diverge significantly between theoretical model and background-based validation, the latent parameter assumption fails.

### Mechanism 2
- Claim: Detection performance depends on a simple first-claimer heuristic rather than sophisticated reasoning.
- Mechanism: Detectives possess ground-truth information and claim proactively; mafiosos lack knowledge and typically wait to counter-claim. Models that trust early investigative claims outperform those that demand additional evidence.
- Core assumption: The heuristic effectiveness holds across diverse model architectures and prompts.
- Evidence anchors:
  - [section 3.2] "success hinges on a simple strategic heuristic: trust the first player to claim investigative authority"
  - [section 3.2] Claude Sonnet 4 achieves 50.6% accuracy—"statistically consistent with random voting"
  - [corpus] Limited corpus validation; CaughtCheating (FMR=0.58) explores visual detection but not social heuristics.
- Break condition: If extended discussion rounds (8+ rounds) fundamentally shift detection strategies, the first-claimer advantage may be an artifact of short games.

### Mechanism 3
- Claim: Procedural ordering and name attribution create systematic biases in trust allocation.
- Mechanism: Last speakers gain persuasive advantage (detectives: +7.10 percentage points); names like Bob outperform Diana by 2.20 points, suggesting implicit social biases in model behavior.
- Core assumption: These biases reflect model training data rather than game-specific artifacts.
- Evidence anchors:
  - [section 4.1] "Bob 55.96±0.48%, Alice 55.55±0.48%, Charlie 54.16±0.48%, Diana 53.76±0.48%"
  - [section 4.2] "mafiosos achieve 41.45±0.72% win rate when speaking last versus 35.41±0.40% overall"
  - [corpus] BiasCause (FMR=0.65) examines social bias in LLMs but not in game-theoretic settings.
- Break condition: If ablation studies with shuffled speaker orders eliminate the advantage, the mechanism is procedural rather than model-intrinsic.

## Foundational Learning

- Concept: **Bayesian inference for binomial outcomes**
  - Why needed here: Win rates are estimated using Beta posteriors (Beta(k+1, n-k+1)) with weakly informative priors; understanding Laplace's rule of succession is essential for interpreting uncertainty estimates.
  - Quick check question: Given 15 wins out of 100 games, what is the Bayesian mean win rate (not the frequentist estimate)?

- Concept: **Logistic regression with latent parameters**
  - Why needed here: The core model logit(p) = v × (m - d) is a parameterized logistic function; shift symmetry and scale invariance require gauge-fixing constraints (E[m] = 0, E[v] = 1).
  - Quick check question: If all m and d values increase by 2, how do predicted win rates change?

- Concept: **Theory-of-mind in multi-agent systems**
  - Why needed here: The game explicitly tests asymmetric information reasoning; interpreting results requires distinguishing persuasive language from strategic reasoning.
  - Quick check question: Why might a model excel at deception (persuasion) while failing at detection (strategic inference)?

## Architecture Onboarding

- Component map:
  - Game engine -> Manages night actions, day discussion, blind voting
  - Prompt constructor -> Role-specific memory injection + game rules + response format constraints
  - LLM agents -> One per player; temperature=0.7 for most models
  - Bayesian inference pipeline -> PyMC with NUTS sampler; 30 latent parameters (3 per model × 10 models)
  - Validation module -> Background-based z-score aggregation for independent ranking verification

- Critical path:
  1. Assign roles → Execute fixed night actions → Initialize role-specific memories
  2. Run discussion rounds (randomized speaking order) → Collect votes
  3. Aggregate 14,000 games across 140 configurations → Fit theoretical model via MCMC
  4. Validate rankings against background-based methodology (5,000 games per capability)

- Design tradeoffs:
  - Single day phase limits strategic depth but enables parametric tractability
  - Fixed night actions eliminate role uncertainty but reduce ecological validity
  - 100 games per configuration balances statistical power against API costs

- Failure signatures:
  - Poor chain mixing (R̂ > 1.05): Indicates model misspecification or insufficient data
  - Negative v_k values: Model systematically responds inversely to evidence (possible prompt confusion)
  - Disclosure rates < 30%: Detective role failure; check prompt formatting

- First 3 experiments:
  1. Replicate baseline with 3 models × 3 roles × 20 games to verify pipeline correctness
  2. Ablate prompt length (full vs. 12-word minimal) to test sensitivity on deception capability
  3. Extend discussion to 8 rounds with fixed model pair to probe last-speaker advantage mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the theoretical model `logit(p) = v(m-d)` generalize to predict outcomes in complex multi-round or multi-player Mafia variants?
- Basis in paper: [explicit] Section 6.1 states that extending the framework to general games "would reveal how the intrinsic capabilities measured here combine and scale in more complex multi-agent interactions."
- Why unresolved: The current study is restricted to a simplified single-day phase to isolate three specific capabilities, leaving the scalability of the mathematical model untested.
- What evidence would resolve it: Applying the same Bayesian inference methodology to gameplay logs from standard Mafia games (e.g., 5+ players, multiple nights) to test parameter consistency.

### Open Question 2
- Question: Can deception classifiers trained on Mini-Mafia embeddings effectively generalize to detecting real-world human deception?
- Basis in paper: [explicit] Section 6.3 suggests the game "generates labeled data where we know ground truth" and proposes that one "could train classifiers on sentence embeddings to detect deception."
- Why unresolved: The paper provides the benchmark and data source but does not validate the transfer learning capabilities of a detector trained in this controlled environment.
- What evidence would resolve it: Training a classifier on game transcripts and benchmarking its accuracy against a corpus of real-world deceptive text (e.g., fauxtography or fraudulent reviews).

### Open Question 3
- Question: What specific failure modes cause advanced frontier models to underperform smaller models in strategic deception detection?
- Basis in paper: [inferred] Sections 3.2 and 5 highlight "counterintuitive results" where Claude Sonnet 4 performs near random chance while Grok 3 Mini excels, noting a disconnect between "superior linguistic capabilities" and "strategic reasoning."
- Why unresolved: The paper quantifies the gap but does not isolate whether the failure is due to over-analysis of linguistic nuances or a lack of robust "trust the first claimant" heuristics in larger models.
- What evidence would resolve it: Mechanistic interpretability studies comparing the attention patterns of large vs. small models when processing conflicting testimony from detectives and mafiosos.

## Limitations

- Single-day constraint limits ecological validity for multi-round strategic adaptation
- Fixed night actions eliminate role uncertainty but create artificial information structure
- Name bias effects (+2.20 percentage points for Bob vs Diana) suggest systematic procedural artifacts
- Limited generalizability to broader multi-agent interactions due to highly constrained experimental design

## Confidence

- High confidence: The theoretical model's mathematical consistency and MCMC implementation (R̂ < 1.02, effective sample sizes > 1000)
- Medium confidence: Role-specific capability rankings and first-claimer heuristic findings, given strong empirical support but potential overfitting to the single-day format
- Low confidence: Generalizability to multi-round games and broader multi-agent interactions, due to the highly constrained experimental design

## Next Checks

1. **Extend discussion rounds**: Run 8-round games with the same 10 models to test whether last-speaker advantages persist or disappear with extended deliberation, directly probing Mechanism 2's assumption about heuristic stability.

2. **Ablate name effects**: Shuffle speaker order independently of player names in 500-game subsets to isolate procedural ordering effects from genuine name-based biases, testing Mechanism 3's core assumption.

3. **Cross-validate rankings**: Implement the background-based methodology (5,000 games per capability) to independently verify the 10-model rankings, directly testing whether the latent parameter model's predictions match ground-truth performance.