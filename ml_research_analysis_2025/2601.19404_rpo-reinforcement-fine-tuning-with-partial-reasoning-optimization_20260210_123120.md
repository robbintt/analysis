---
ver: rpa2
title: RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization
arxiv_id: '2601.19404'
source_url: https://arxiv.org/abs/2601.19404
tags:
- training
- grpo
- reasoning
- cache
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RPO (Replay-based Policy Optimization), a
  reinforcement fine-tuning algorithm that significantly reduces computational overhead
  by generating suffixes of reasoning paths instead of full trajectories. RPO uses
  an experience cache to store and reuse previously generated reasoning paths, reducing
  token generation during training by approximately 95%.
---

# RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization

## Quick Facts
- arXiv ID: 2601.19404
- Source URL: https://arxiv.org/abs/2601.19404
- Reference count: 40
- 1.5B model: 90% training time reduction with maintained/improved accuracy
- 7B model: 72% training time reduction with maintained/improved accuracy

## Executive Summary
This paper introduces RPO (Replay-based Policy Optimization), a reinforcement fine-tuning algorithm that significantly reduces computational overhead by generating suffixes of reasoning paths instead of full trajectories. RPO uses an experience cache to store and reuse previously generated reasoning paths, reducing token generation during training by approximately 95%. When applied to 1.5B and 7B models, RPO reduces training time by 90% and 72% respectively while maintaining or improving performance compared to traditional full-path reinforcement fine-tuning algorithms like GRPO and DAPO. The method is plug-and-play, integrates with existing algorithms, and achieves better stability during multi-step training.

## Method Summary
RPO accelerates reinforcement learning by training models to generate only suffixes of reasoning paths using an experience cache. The cache stores (question, answer) pairs from initial model generations, which are then truncated to create prefix-guided rollouts. For each training iteration, RPO retrieves a cached answer, removes the last m tokens (uniformly sampled), and generates only the suffix. This approach conditions the policy on partial correct trajectories rather than requiring full exploration from scratch. The method updates the cache using ε-greedy selection and applies length-aware reward shaping to differentiate between correct solutions of varying quality.

## Key Results
- 95% reduction in token generation during training compared to full-path reinforcement fine-tuning
- 90% training time reduction for 1.5B models while maintaining or improving accuracy
- 72% training time reduction for 7B models while maintaining or improving accuracy
- Better stability during multi-step training compared to GRPO and DAPO baselines

## Why This Works (Mechanism)

### Mechanism 1
Prefix-guided generation reduces sampling variance while preserving learning signal. RPO retrieves a cached reasoning prefix for each question, truncates the last m tokens, and generates only the suffix. This conditions the policy on partial correct trajectories rather than forcing exploration from scratch. The paper proves mathematically that conditioning on augmented information (qk, ak) strictly reduces policy-gradient variance.

### Mechanism 2
Cache updates with ε-greedy selection balance exploitation of high-reward paths with exploration diversity. After each gradient update, RPO updates the cache using ε-greedy: with probability ε, replace the cached answer with the highest-reward response from the current group; otherwise, randomly select a suboptimal response. This prevents premature convergence to a single reasoning pattern.

### Mechanism 3
Length-aware reward shaping compensates for reduced intra-group diversity by differentiating correct solutions. RPO shapes rewards based on response length: R(si) = clip(r(si) / (1 + e^(-α(ℓref - len(oi)))), m, M). Shorter correct solutions receive higher rewards, creating meaningful gradients even when all responses in a group are correct.

## Foundational Learning

- **Policy Gradient with Group Relative Advantage (GRPO/DAPO)**
  - Why needed here: RPO is a plug-and-play modification to existing RL fine-tuning algorithms. Understanding how GRPO computes advantages via group normalization is essential to see where RPO's prefix conditioning fits.
  - Quick check question: Given rewards [0.8, 0.6, 0.9] for a group of 3 responses, compute the normalized advantage for the first response. (Answer: (0.8 - 0.767) / 0.153 ≈ 0.216)

- **Importance Sampling Ratio with Clipping**
  - Why needed here: RPO modifies the importance ratio by conditioning on prefixes. The clipping mechanism prevents destabilizing large policy updates.
  - Quick check question: Why does clipping the importance ratio at [1-ε, 1+ε] prevent catastrophic forgetting? (Answer: It bounds how far the new policy can deviate from the old policy in a single update)

- **Experience Replay in RL**
  - Why needed here: RPO's cache pool is a form of experience replay, storing (question, answer) pairs. Unlike traditional off-policy replay that samples from a buffer, RPO uses cached answers as prefix guides while remaining on-policy for gradient computation.
  - Quick check question: How does RPO's use of cached answers differ from standard DQN-style experience replay? (Answer: RPO uses cached answers as conditioning context for new generation, not as direct training samples; the gradient remains on-policy)

## Architecture Onboarding

- **Component map:**
  - Cache Pool C^(t) -> Rollout Module -> Reward Module -> Optimization Module -> Cache Update Module
  - Cache Pool C^(t): Stores (question, answer) pairs; initialized with model's own generations; updated via ε-greedy after each training step
  - Rollout Module: Retrieves cached answer ak, truncates last m tokens (uniform sample from [0, L]), concatenates prefix with question, generates new suffix
  - Reward Module: Computes base reward (verifiable) → applies length-aware shaping with α, m, M parameters
  - Optimization Module: Standard GRPO/DAPO gradient computation with modified importance ratio conditioned on prefix
  - Cache Update Module: ε-greedy selection between best-reward and random response

- **Critical path:**
  1. Cache initialization (one-time): Generate answers for all training questions using initial policy πθ0
  2. Per-iteration loop: Retrieve prefix → Truncate → Generate suffix → Compute shaped rewards → Compute group-relative advantages → Update policy → Update cache

- **Design tradeoffs:**
  - L (max truncation length): Higher L = faster training but less exploration; L=300 reduces tokens to ~146 vs ~2689 for full GRPO
  - ε (cache update greediness): Lower ε = more exploitation but risk of cache stagnation; ε=0.1 achieves optimal performance
  - α (length sensitivity): Controls reward shaping strength; optimal α ≈ 0.01
  - G (group size): Smaller G yields greater acceleration (7.4% of GRPO for G=6 vs 21.1% for larger G)

- **Failure signatures:**
  - Response length collapse: If L is too large relative to response lengths, model may fail to learn full reasoning chains
  - Cache contamination: If initial cache contains predominantly incorrect answers, prefix conditioning propagates errors
  - Gradient signal degradation: Without length-aware rewards, similar rewards within groups produce weak gradients

- **First 3 experiments:**
  1. Validate cache quality impact: Initialize cache with ground-truth correct answers vs. model-generated answers; measure accuracy gap
  2. Ablate truncation length: Run L ∈ {300, 500, 800, 0.5ℓ, ℓ} with fixed parameters; plot training time vs. final accuracy
  3. Test exploration capability: Compare Pass@N curves for RPO vs. GRPO across training epochs; verify RPO catches up to GRPO by epoch 4

## Open Questions the Paper Calls Out

- **Can the trade-off between training speed and response diversity be mitigated without relying solely on length-aware rewards?**
  - The paper identifies reduced variance as a necessary cost but doesn't explore architectural or sampling alternatives that might preserve diversity natively.

- **Why does initializing a smaller model's cache with a larger model's outputs fail to improve performance?**
  - The paper notes performance doesn't significantly improve when a 1.5B model uses a 7B model's cache, suggesting limitations in experience cache transferability across model capacities.

- **How does RPO perform on tasks where the initial policy is too weak to generate correct trajectories for the cache?**
  - The method assumes the model can produce at least some correct reasoning paths to serve as guides, but doesn't evaluate scenarios where cached prefixes contain incorrect reasoning paths.

## Limitations

- Cache initialization relies entirely on the base model's initial generations, which may contain systematic errors that propagate through all training iterations
- Length-aware reward shaping assumes shorter correct solutions are better, which may not hold for all reasoning tasks requiring detailed explanations
- The method sacrifices response diversity for training speed due to shared prefixes, requiring auxiliary reward shaping

## Confidence

- **High Confidence**: Training time reduction claims (90% for 1.5B, 72% for 7B) and stability improvements during multi-step training are well-supported by experimental results
- **Medium Confidence**: Performance claims maintaining or improving over GRPO/DAPO across all six datasets, as some datasets show only marginal improvements
- **Medium Confidence**: Mathematical proof of variance reduction assumes optimal cache quality, which isn't fully validated across different cache initialization strategies

## Next Checks

1. **Cache Quality Sensitivity**: Systematically evaluate RPO performance when initializing the cache with varying quality responses (ground-truth, model-generated, and mixed-quality caches) to quantify the impact of cache contamination

2. **Task-Type Generalization**: Test RPO on non-verifiable reasoning tasks (e.g., creative writing, open-ended problem solving) where the assumption that shorter solutions indicate better quality may not hold

3. **Scaling Behavior**: Evaluate RPO on larger model sizes (13B, 33B) to determine if the 95% token reduction and 72% time savings scale proportionally or if computational benefits diminish with model size