---
ver: rpa2
title: 'How ChatGPT and Gemini View the Elements of Communication Competence of Large
  Language Models: A Pilot Study'
arxiv_id: '2511.02838'
source_url: https://arxiv.org/abs/2511.02838
tags:
- chatgpt
- communication
- llms
- competence
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This pilot study investigates how advanced Large Language Models
  (LLMs) like ChatGPT and Gemini interpret elements of communication competence, using
  two theoretical frameworks: the integrated linguistic-interpersonal model (CMCC-L2)
  and the artificial systems model (CCAS). Through two case studies, the research
  demonstrates that both ChatGPT and Gemini can understand and apply these models
  to explain LLM-user interactions.'
---

# How ChatGPT and Gemini View the Elements of Communication Competence of Large Language Models: A Pilot Study

## Quick Facts
- arXiv ID: 2511.02838
- Source URL: https://arxiv.org/abs/2511.02838
- Reference count: 0
- Primary result: LLMs can understand and apply communication competence frameworks (CMCC-L2, CCAS) to explain their own user interaction behaviors

## Executive Summary
This pilot study investigates how advanced LLMs interpret elements of communication competence using two theoretical frameworks: the integrated linguistic-interpersonal model (CMCC-L2) and the artificial systems model (CCAS). Through two case studies, ChatGPT and Gemini demonstrated the ability to map abstract interpersonal skills to concrete functional behaviors when provided with explicit theoretical schemas. The research shows that LLM communication competence is a relevant area of study alongside traditional usability and user experience research, with both linguistic and interpersonal perspectives providing valuable insights into LLM-user interactions.

## Method Summary
The study employed a structured elicitation methodology using two case studies with five-prompt sequences each. Researchers uploaded PDF documents and JPEG images of theoretical models to LLMs, then systematically verified the models' understanding before eliciting detailed mappings between theoretical communication skills and LLM operational behaviors. Case 1 used the CMCC-L2 model with four competence levels (Linguistic to Supra), while Case 2 used the CCAS model with six dimensions (Intentionality, Social Relaxation, etc.). The procedure involved: (1) uploading figures and verifying image reading, (2) uploading PDFs and explaining models, (3) confirming contextualization to LLM-user interaction, and (4) eliciting skill-to-interaction mappings. Multiple LLM versions were tested with web search and deep research disabled.

## Key Results
- Both ChatGPT and Gemini successfully understood and applied theoretical communication competence models when provided with explicit schemas
- LLMs could map abstract interpersonal skills to concrete functional behaviors (e.g., "Intentionality" to "refusing when content breaches policy")
- The structured elicitation approach produced detailed, structured information about LLM communication skills that could complement traditional usability research

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing explicit theoretical schemas via context injection enables structured, domain-specific self-analysis that general pre-training cannot easily produce
- **Mechanism:** Uploading specific theoretical PDFs and images serves as a "cognitive scaffold," grounding LLM reasoning in provided definitions rather than stochastic associations
- **Core assumption:** LLMs can accurately process uploaded images and PDFs to integrate context rather than hallucinate
- **Evidence anchors:** Study investigates LLMs interpreting frameworks using uploaded materials; images were uploaded and LLM verified reading content; HEART paper emphasizes need for specific benchmarks
- **Break condition:** If LLM fails to parse uploaded materials or context window is exceeded

### Mechanism 2
- **Claim:** LLMs can map abstract interpersonal skills to concrete functional behaviors when constrained by multi-level taxonomy
- **Mechanism:** Using CMCC-L2 hierarchy, LLMs segment their own architecture, associating token generation with "Linguistic competence" and safety layers with "Social/Intercultural competence"
- **Core assumption:** LLMs possess internal representations of their safety/policy layers that can be described via prompts
- **Evidence anchors:** ChatGPT o3 describes "policy-and-planning layer" for Strategic competence; Table 4 links "Intentionality" to policy enforcement; Cultural Value Alignment paper supports distinguishable value preferences
- **Break condition:** If theoretical model requires capabilities LLM demonstrably lacks

### Mechanism 3
- **Claim:** Sequential "verification-then-elicit" prompting reduces variance of LLM self-assessments
- **Mechanism:** Multi-step prompt sequence primes context window, ensuring LLM adopts specific persona and definitions for final output
- **Core assumption:** Replicability achievable despite stochastic outputs if priming is sufficiently robust
- **Evidence anchors:** 5-step procedure explicitly designed to verify "reading" before "interpreting"; paper acknowledges procedure enables reproducibility despite stochasticity; Learning to Live with AI highlights structured vs. naturalistic approaches
- **Break condition:** If user skips verification steps, LLM defaults to generic definitions

## Foundational Learning

- **Concept: Communication Competence vs. Linguistic Competence**
  - **Why needed here:** To understand evaluation beyond grammar to include social appropriateness and goal achievement
  - **Quick check question:** Can an LLM produce grammatically correct text that is nonetheless communicatively incompetent?

- **Concept: CMCC-L2 & CCAS Models**
  - **Why needed here:** These taxonomies interrogate LLMs; understanding levels (Micro/Mezzo/Macro) and dimensions (Intentionality/Expressivity) is critical
  - **Quick check question:** According to CCAS, does "Intentionality" refer to LLM having human desires or to its "goal-driven, self-aware layer"?

- **Concept: Elicitation vs. Evaluation**
  - **Why needed here:** Paper uses LLMs to elicit information about themselves rather than external human evaluation
  - **Quick check question:** Is data derived from human ratings of chat logs or from LLMs' own generated explanations?

## Architecture Onboarding

- **Component map:** Input Layer (PDF Documents + JPEG Images + Text Prompts) -> Processing Layer (Advanced LLM with context) -> Output Layer (Structured Tables mapping Theoretical Skills to Functional Examples)

- **Critical path:**
  1. Ingestion: Upload PDF + Image
  2. Validation: Force LLM to describe uploaded image to prove processing worked
  3. Contextualization: Instruct LLM to treat L2/Humanoid context as analogous to LLM-user interaction
  4. Extraction: Request specific examples linking model skills to behaviors

- **Design tradeoffs:**
  - Theory-Driven vs. Data-Driven: Strict frameworks yield structured data but may miss emergent behaviors
  - Self-Report vs. Observation: Efficient but risks model confabulating capabilities

- **Failure signatures:**
  - Context Leakage: LLM ignores uploaded PDF and uses pre-training definitions
  - Hallucinated Capabilities: LLM claims to perform behaviors exceeding actual system capabilities

- **First 3 experiments:**
  1. Replication with "Thinking" Models: Run exact prompt sequence with o3 or DeepSeek to test reasoning token impact
  2. Blind vs. Scaffolded: Compare outputs when asking directly vs. using CMCC-L2 scaffold
  3. Cross-Model Consistency: Test if Gemini and ChatGPT interpret dimensions differently with same CCAS definitions

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Spitzberg's (2006) CMC competence model or intercultural competence frameworks be effectively applied to LLM-user interaction using same methodology?
- **Open Question 2:** How do social cognition dynamics and "uncanny valley" phenomenon manifest in human-AI interaction with highly competent LLM communicators?
- **Open Question 3:** Can user-centered XAI approaches improve interpretability and practical utility of CMCC-L2/CCAS-based LLM competence assessments for end users?

## Limitations
- Reliance on LLM self-reporting introduces potential hallucination risks
- Stochastic nature of LLM outputs makes exact replication impossible
- Theoretical frameworks were developed for human-human interaction, not LLM-user interaction
- Sample size limited to two case studies with specific model versions

## Confidence
- **High confidence:** Methodology of using structured, theory-driven prompts is sound and replicable
- **Medium confidence:** LLMs can map theoretical competencies to operational behaviors, but requires external validation
- **Low confidence:** Generalizability across different LLM architectures and versions

## Next Checks
1. Cross-Model Consistency Test: Repeat 5-prompt sequence across multiple current LLM architectures to assess consistency
2. Human-Expert Validation: Have communication competence experts review LLM-generated skill mappings
3. Blind vs. Scaffolded Comparison: Conduct parallel experiments with and without theoretical scaffolding to quantify value added