---
ver: rpa2
title: Online Pre-Training for Offline-to-Online Reinforcement Learning
arxiv_id: '2507.08387'
source_url: https://arxiv.org/abs/2507.08387
tags:
- online
- learning
- value
- pre-training
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Online Pre-Training for Offline-to-Online
  Reinforcement Learning (OPT), a method designed to address the problem of inaccurate
  value estimation in offline pre-trained agents during online fine-tuning. The core
  idea is to introduce a new value function specifically trained to handle online
  samples through a dedicated Online Pre-Training phase.
---

# Online Pre-Training for Offline-to-Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.08387
- Source URL: https://arxiv.org/abs/2507.08387
- Reference count: 34
- Primary result: Achieves 30% average improvement in offline-to-online RL by addressing inaccurate value estimation during online fine-tuning

## Executive Summary
This paper introduces Online Pre-Training for Offline-to-Online Reinforcement Learning (OPT), a method designed to address the problem of inaccurate value estimation in offline pre-trained agents during online fine-tuning. The core idea is to introduce a new value function specifically trained to handle online samples through a dedicated Online Pre-Training phase. This approach involves three learning stages: offline pre-training, online pre-training (where the new value function is trained on both offline data and online samples collected by the offline-trained policy), and online fine-tuning (where both the original and new value functions are used together). When implemented on TD3 and SPOT, OPT demonstrates an average 30% improvement in performance across various D4RL environments including MuJoCo, Antmaze, and Adroit, within a limited setting of 300k online interactions. The method is also shown to be broadly applicable, as it can be integrated with other value-based algorithms like IQL, further validating its versatility as a general enhancement module.

## Method Summary
OPT addresses the distribution shift problem in offline-to-online RL by introducing a dedicated Online Pre-Training phase. The method trains an additional value function (Qon-pt) using both offline data and online samples collected by the frozen offline policy. This Qon-pt is then used alongside the original offline-trained value function (Qoff-pt) during online fine-tuning, with a dynamically weighted combination controlled by coefficient κ. The three-stage process includes: offline pre-training (standard RL on dataset), online pre-training (training Qon-pt on offline + online samples for 50k steps), and online fine-tuning (using both value functions with scheduled κ weighting).

## Key Results
- Achieves 30% average performance improvement across D4RL benchmarks compared to vanilla TD3 fine-tuning
- Successfully handles distribution shift by using Qon-pt for online samples while leveraging Qoff-pt for in-distribution states
- Works with multiple backbone algorithms including TD3+BC, SPOT, and IQL
- Demonstrates effectiveness on challenging environments like Antmaze-large and Adroit tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a dedicated online-adapted value function mitigates inaccurate value estimation caused by distribution shift during offline-to-online transition.
- Mechanism: Offline pre-trained value functions (Qoff-pt) become biased toward the static dataset distribution and over-estimate out-of-distribution actions or remain overly conservative. OPT introduces Qon-pt, a randomly initialized value function trained during Online Pre-Training on both offline data and newly collected online samples. This dual-source training allows Qon-pt to learn value estimates that better align with the evolving policy's state-action distribution, providing more accurate gradients for policy improvement during fine-tuning.
- Core assumption: The meta-adaptation objective (training on offline data and online samples simultaneously) enables Qon-pt to generalize to online samples without inheriting Qoff-pt's biases. This relies on the gradient-based meta-learning formulation where Lon(ψ − α∇Loff(ψ)) promotes adaptation.
- Evidence anchors:
  - [abstract] "explicitly designed to address the issue of inaccurate value estimation in offline pre-trained agents"
  - [section 5.3, Figure 7] Ablation shows that removing Qon-pt and relying solely on Qoff-pt with Online Pre-Training objective results in performance degradation, especially on random datasets where Qoff-pt is heavily biased
  - [corpus] Related work "Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only" (FMR=0.65) identifies similar issues with offline pre-trained Q-functions requiring continued training for stability, suggesting the problem is well-recognized

### Mechanism 2
- Claim: Online Pre-Training phase provides Qon-pt with sufficient initialization to avoid disrupting policy learning in early fine-tuning.
- Mechanism: Randomly initializing Qon-pt and using it immediately at fine-tuning causes unstable policy updates due to uncalibrated value estimates. OPT introduces a dedicated Online Pre-Training phase where Qon-pt is trained on Nτ=25k online samples collected by the frozen offline policy πoff, combined with offline data, using the meta-adaptation objective. This "warm-starts" Qon-pt so it can provide reliable gradients from the first fine-tuning step.
- Core assumption: The meta-adaptation objective balances learning from offline data (to provide broad coverage) and online samples (to adapt to πoff's distribution). Training only on Bon would overfit to πoff's limited exploration; training only on Boff would replicate Qoff-pt's biases.
- Evidence anchors:
  - [section 3.1] "since Qon-pt is required to train from scratch, it often disrupts policy learning in the early stages"
  - [section 5.1, Figure 5, Table 8] Random initialization underperforms compared to OPT by 10-15% on average; pre-training only on Bon fails to adapt when policy evolves significantly (random datasets)
  - [corpus] Weak direct evidence. Related papers do not explicitly study this warm-start mechanism for new value functions in offline-to-online settings

### Mechanism 3
- Claim: Dynamically weighted combination of Qoff-pt and Qon-pt during fine-tuning leverages complementary strengths of each value function.
- Mechanism: Qoff-pt provides reliable estimates for state-actions similar to the offline dataset (low distribution shift). Qon-pt provides better estimates for novel online samples. OPT uses a scheduled weighting coefficient κ in the policy loss: Lπ = E[−((1−κ)Qoff-pt + κQon-pt)]. κ starts low (0.1-0.3) for medium-quality datasets where initial Qoff-pt is reliable, then increases to 0.9 to shift reliance toward Qon-pt as online fine-tuning progresses and the policy diverges from the offline data distribution. For random datasets (high initial distribution shift), κ=1 throughout.
- Core assumption: The optimal κ schedule depends on dataset quality and distribution shift magnitude. The paper assumes linear scheduling with decay period Tdecay is sufficient; more sophisticated adaptive schemes are not explored.
- Evidence anchors:
  - [section 3.2, Equation 4] Formal definition of weighted policy loss with κ coefficient
  - [section 5.2, Figure 6] t-SNE visualization shows distribution differences between Boff and policy rollouts vary by dataset quality; random datasets show immediate large shift, medium datasets show gradual divergence
  - [section 5.4, Table 4] Ablation on κ shows fixed κ=0.5 underperforms scheduled κ (0.1→0.9) on challenging environments like Antmaze-large
  - [corpus] No direct corroboration. "Behavior-Adaptive Q-Learning" (FMR=0.61) addresses distribution shift with a different mechanism (behavior-adaptive framework), suggesting the problem is active but solutions vary

## Foundational Learning

- Concept: **Distribution shift in offline-to-online RL**
  - Why needed here: The core problem OPT addresses. When an offline-trained policy interacts online, it encounters state-action pairs outside the offline dataset. The offline value function, trained only on in-distribution data, produces inaccurate estimates for these out-of-distribution samples, leading to suboptimal policy updates or conservative behavior that prevents improvement.
  - Quick check question: Can you explain why a value function trained on a static dataset of "medium-quality" expert demonstrations might underestimate the value of novel, better-performing actions discovered during online exploration?

- Concept: **Temporal Difference (TD) learning and Q-function training**
  - Why needed here: OPT extends TD3-based algorithms by adding a new Q-function. Understanding the standard TD loss LQ(θ) = E[(Qθ(s,a) − (r + γQ̄θ(s′,π(s′)))²] is essential to grasp how both Qoff-pt and Qon-pt are trained during fine-tuning.
  - Quick check question: In the TD target r + γQ̄θ(s′,π(s′)), why is a target network Q̄θ used instead of the current network Qθ, and what would happen if they were the same?

- Concept: **Meta-learning / meta-adaptation objectives**
  - Why needed here: The Online Pre-Training phase uses a meta-adaptation objective inspired by OEMA: Lpretrain = Loff + Lon(ψ − α∇Loff(ψ)). This bi-level formulation optimizes Qon-pt to adapt quickly to online data after being trained on offline data. Without understanding meta-learning, this objective appears unnecessarily complex.
  - Quick check question: In the term Lon(ψ − α∇Loff(ψ)), what does the gradient ∇Loff(ψ) represent, and why is the parameter ψ updated by α∇Loff(ψ) before computing Lon?

## Architecture Onboarding

- Component map:
  - Qoff-pt (θ): Offline pre-trained Q-function from TD3+BC or SPOT. Frozen during Online Pre-Training, updated via TD learning during fine-tuning.
  - Qon-pt (ψ): Newly initialized Q-function. Trained during Online Pre-Training using meta-adaptation objective on Boff ∪ Bon. Updated via TD learning during fine-tuning.
  - πϕ: Policy network. Frozen during Online Pre-Training. Updated during fine-tuning using weighted combination of Qoff-pt and Qon-pt.
  - Boff: Offline dataset (fixed). Used in Offline Pre-Training, Online Pre-Training, and fine-tuning (via balanced replay).
  - Bon: Online replay buffer. Filled with Nτ=25k samples during Online Pre-Training (πoff rollout), then continuously updated during fine-tuning.
  - κ scheduler: Linear schedule controlling Qoff-pt vs. Qon-pt weighting. Initialized at κinit, increases over Tdecay steps to κend.

- Critical path:
  1. Run standard offline RL (TD3+BC/SPOT/IQL) for 1M gradient steps → obtain Qoff-pt, πoff
  2. Initialize Qon-pt with random weights
  3. Collect Nτ=25k transitions by rolling out πoff in environment → store in Bon
  4. For Npretrain=50k gradient steps: sample batches from Boff and Bon, update Qon-pt using Lpretrain (Equation 3)
  5. Initialize balanced replay buffer BBR ← Boff ∪ Bon
  6. For Nfinetune=275k environment steps: interact, store in BBR, update Qoff-pt and Qon-pt via TD loss (Equation 2), update πϕ using weighted loss (Equation 4)

- Design tradeoffs:
  - **Nτ size**: Larger Nτ improves Qon-pt pre-training but delays fine-tuning. Paper finds 25k is sufficient; 100k shows no additional gain.
  - **κ schedule**: Dataset-dependent. Random datasets require κ=1 (immediate Qon-pt reliance); medium datasets use κinit=0.1-0.3. Misconfiguration causes 15-30% performance drop on Antmaze-large.
  - **Backbone choice**: TD3+BC works for MuJoCo; SPOT (VAE-based OOD penalty) performs better on Antmaze/Adroit. OPT applies to both but requires adapting policy loss formulation for IQL (stochastic policy, separate V-function).
  - **Buffer strategy**: Balanced replay (50% offline, 50% online prioritized) vs. symmetric sampling (RLPD default). OPT uses balanced replay; ablating to online-only replay drops performance by 4%.

- Failure signatures:
  - **Qon-pt random initialization without pre-training**: Early fine-tuning shows unstable policy updates, performance lags from step 0. [Section 5.1]
  - **Training Qon-pt only on Bon (no Boff)**: Overfits to πoff's limited exploration. On random datasets where policy evolves drastically, performance can drop below random initialization. [Table 9]
  - **Fixed κ=0 (Qoff-pt only)**: Cannot adapt to online distribution shift; Antmaze-large drops to ~46% vs. 78% with scheduling. [Table 7]
  - **Removing Qon-pt entirely**: Performance degrades 12-15% across MuJoCo; walker2d-random collapses to near-zero. [Figure 7, Table 11]

- First 3 experiments:
  1. **Sanity check on walker2d-random-v2**: Train TD3+BC offline (1M steps), then apply OPT. Compare to vanilla TD3 fine-tuning. Expected: OPT achieves ~88 normalized score vs. TD3's ~0 (TD3 fails completely on this task due to extreme distribution shift). This validates the core mechanism on the most challenging dataset.
  2. **Ablate Online Pre-Training**: Skip Nτ collection and Npretrain steps; initialize Qon-pt randomly and start fine-tuning immediately. Expected: 10-15% performance drop, erratic early learning curves. This isolates the contribution of the pre-training phase.
  3. **κ schedule sensitivity on Antmaze-large-diverse**: Test κinit ∈ {0.1, 0.3, 0.5} with fixed κend=0.9. Expected: κinit=0.1 performs best (~90%); κinit=0.5 drops to ~75%. This validates the dataset-dependent scheduling heuristic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the weighting coefficient κ be adapted dynamically based on real-time distribution divergence metrics rather than a fixed schedule?
- Basis: [inferred] Section 5.2 and Appendix H demonstrate that the schedule for κ must be manually tuned based on dataset characteristics (e.g., random vs. medium), creating a maintenance burden.
- Why unresolved: The current implementation relies on dataset-specific hyperparameter configurations rather than an adaptive mechanism that detects distribution shifts automatically.
- What evidence would resolve it: An adaptive κ algorithm that outperforms the fixed schedule by adjusting weights based on the divergence between Boff and current online samples.

### Open Question 2
- Question: Does the optimal number of Online Pre-Training samples (Nτ) scale linearly with state-space dimensionality?
- Basis: [inferred] Section 5.5 establishes 25k steps as optimal for D4RL tasks, but performance varies significantly with Nτ, suggesting sensitivity to environment complexity or dataset size.
- Why unresolved: The analysis is restricted to MuJoCo and Antmaze tasks; the scaling laws for high-dimensional environments (e.g., visual RL) remain unexplored.
- What evidence would resolve it: A scaling law analysis of OPT applied to high-dimensional benchmarks (e.g., DMControl) showing a predictable relationship between state dimensions and the required Nτ.

### Open Question 3
- Question: Can the computational overhead of the additional value function and meta-adaptation updates be reduced to match the wall-clock time of single-critic methods?
- Basis: [inferred] Appendix J acknowledges that OPT incurs a 50% increase in wall-clock time compared to vanilla TD3 due to the extra Online Pre-Training steps and gradient updates.
- Why unresolved: While sample efficiency improves, the practical deployment cost is higher, and the trade-off between sample efficiency and computational cost has not been optimized.
- What evidence would resolve it: An optimized variant of OPT that achieves comparable sample efficiency with significantly lower wall-clock time, potentially by sharing network backbones or reducing pre-training frequency.

## Limitations
- Dataset-dependent hyperparameters (Nτ, κ schedule) require manual tuning and may not generalize across all environments
- Missing specification of meta-adaptation step size α in Equation 3 makes exact replication challenging
- Computational overhead is 50% higher than vanilla TD3 due to additional value function and pre-training steps
- Limited validation of broad applicability claim to "any value-based offline RL algorithm" beyond three specific implementations

## Confidence
- **High confidence**: The core mechanism (adding Qon-pt with dual-source training) and its contribution to performance (30% average improvement). This is well-supported by ablation studies showing consistent gains across multiple environments and algorithm backbones.
- **Medium confidence**: The optimal hyperparameters (Nτ=25k, κ scheduling strategy). While ablation studies are provided, they test limited configurations and rely on heuristics that may not generalize to all dataset qualities or environments.
- **Low confidence**: Claims about broad applicability to "any value-based offline RL algorithm" without providing implementations beyond TD3+BC, SPOT, and IQL. The paper states this capability but doesn't validate it experimentally across diverse algorithm families.

## Next Checks
1. **Meta-adaptation sensitivity**: Systematically vary α in the Online Pre-Training loss and measure performance impact. This addresses the missing hyperparameter and tests whether the current fixed value is optimal.
2. **Balanced replay validation**: Implement and test the symmetric sampling strategy from RLPD (online-only replay) and compare against OPT's balanced replay. This validates whether the offline-online mix ratio is optimal.
3. **Dataset-agnostic κ scheduling**: Replace the heuristic κ schedule with an adaptive method that adjusts κ based on real-time distribution shift metrics (e.g., KL divergence between offline data and online samples). This tests whether the current linear scheduling is truly optimal.