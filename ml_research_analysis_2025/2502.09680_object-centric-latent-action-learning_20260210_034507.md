---
ver: rpa2
title: Object-Centric Latent Action Learning
arxiv_id: '2502.09680'
source_url: https://arxiv.org/abs/2502.09680
tags:
- slot
- action
- trajectories
- learning
- slots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of learning control policies from
  unlabeled video data in the presence of distracting visual elements. The proposed
  method uses object-centric pretraining to disentangle task-relevant object dynamics
  from background distractions.
---

# Object-Centric Latent Action Learning

## Quick Facts
- arXiv ID: 2502.09680
- Source URL: https://arxiv.org/abs/2502.09680
- Reference count: 40
- Key outcome: Object-centric pretraining improves latent action learning performance by 50% on average under distracting visual conditions compared to standard methods

## Executive Summary
This paper addresses the challenge of learning control policies from unlabeled video data when visual distractors (background videos, camera motion, color variations) obscure the agent's actions. The authors propose using object-centric pretraining to disentangle task-relevant object dynamics from background distractions, followed by latent action learning in the object-centric representation space. The method then applies behavior cloning and fine-tuning with limited ground-truth action labels. Experiments on Distracting Control Suite and Distracting MetaWorld show significant improvements over standard latent action methods, closing half the performance gap between distracted and clean-data baselines.

## Method Summary
The method follows a three-stage pipeline: (1) VideoSAUR pretraining decomposes videos into K object slots using self-supervised grouping based on spatio-temporal feature similarity, with each slot encoding properties of a coherent entity; (2) Linear action probes on slot representations identify control-relevant slots by training regressors to predict ground-truth actions from each slot's PCA-reduced embedding, selecting the top-ranked slots; (3) Latent action learning via LAPO (inverse/forward dynamics models) operates on the filtered representations, followed by behavior cloning on inferred latent actions and fine-tuning with limited ground-truth labels (4-512 trajectories, ≤2.5% of data).

## Key Results
- Object-centric pretraining improves performance by 50% on average compared to standard latent action methods
- LAPO-slots achieves 52% gap recovery on Distracting Control Suite Hard vs. 26% for LAPO-masks
- The method closes half the performance gap between distracted and clean-data baselines

## Why This Works (Mechanism)

### Mechanism 1
Object-centric decomposition isolates task-relevant entities from action-correlated visual distractors. VideoSAUR partitions each frame into K slot vectors using self-supervised grouping based on spatio-temporal feature similarity. Each slot encodes properties of a coherent entity (position, motion, appearance). The transformer-based decoder produces attention masks that localize each slot's spatial support, enabling visual inspection of what each slot captures. This works when distracting dynamics are separable from agent-object interactions by virtue of belonging to different coherent entities across time.

### Mechanism 2
Linear action probes on slot representations reliably identify control-relevant slots without manual annotation. After object-centric pretraining, a linear regressor is trained on a small set of labeled trajectories to predict ground-truth actions from each slot's PCA-reduced embedding. Lower probe MSE indicates higher action-relevance. The top-ranked slot(s) are selected for downstream latent action learning. This works when action information concentrates in a small subset of slots and linear probes are sufficient to detect this concentration.

### Mechanism 3
Operating latent action models on filtered representations reduces spurious correlations that degrade inverse dynamics learning. LAPO jointly trains an inverse dynamics model to infer latent actions from observation pairs and a forward dynamics model to predict next observations. In object-centric variants, the models operate directly on slot embeddings or masked images, with reconstruction loss forcing the latent action to encode predictive information. This works when filtered representation preserves sufficient information about the true action while removing distractor-driven variance.

## Foundational Learning

- **Latent Action Models (Inverse/Forward Dynamics)**: Understanding how IDM infers latent actions from observation pairs and how FDM uses those predictions is essential for grasping the role of object-centric filtering. Quick check: Given two observations, can you explain what the IDM predicts and how the FDM uses that prediction?

- **Slot Attention and Object-Centric Representation**: VideoSAUR produces K slot vectors per frame representing coherent entities. You must understand slot initialization (learned means + variance) and how attention masks project slots back to pixel space. Quick check: If you have K=4 slots for a scene with a robot arm, hammer, table, and moving background, what might each slot capture?

- **Behavior Cloning with Limited Labels**: The final stage fine-tunes a BC policy on ≤2.5% labeled data. Understanding the gap between latent-action BC and ground-truth-action BC is essential for interpreting normalized scores. Quick check: If a BC agent trained on latent actions achieves 0.5 normalized return, what does that mean relative to the oracle trained on all ground-truth actions?

## Architecture Onboarding

- **Component map**: VideoSAUR Encoder (DINOv2 ViT-B/14) -> Feature extraction -> Slot attention (K learned initial slots) -> K slot vectors + attention masks -> Linear Action Probe (PCA + linear regression) -> Selected top slot(s) -> LAPO-slots (MLP-based IDM/FDM) OR LAPO-masks (CNN-based IDM/FDM) -> BC Policy (Encoder + policy head)

- **Critical path**: 1) Train VideoSAUR on task videos (6-8.5h on H100, ~100K steps) 2) Run inference to extract slots and masks 3) Train linear probe on small labeled subset -> select top slots 4) Train LAPO-slots or LAPO-masks on filtered representations (1.5-7.5h) 5) Train BC policy on inferred latent actions (3h), then fine-tune with ground-truth labels

- **Design tradeoffs**: LAPO-slots vs LAPO-masks: Slots are more robust to color/camera distractors (52% vs 26% gap recovery) but may be less interpretable. Number of slots K: Too few causes collapsed representations; too many introduces redundancy. Probe budget: More labeled trajectories improve selection reliability but reduce "unsupervised" benefit.

- **Failure signatures**: Slot collapse (entities merged into one slot), incorrect slot selection (probe selects distractor slot), LAPO-masks degradation under heavy distractors, STEVE encoder failure to isolate entities.

- **First 3 experiments**: 1) Reproduce slot visualization on single DCS task to validate object-centric decomposition 2) Probe correlation study on basketball task to verify slot selection reliability 3) Ablate LAPO-slots vs LAPO vs LAPO-clean on single task to confirm 50% gap recovery

## Open Questions the Paper Calls Out

- **Unsupervised slot selection**: Can task-relevant object slots be identified in a fully unsupervised manner without relying on linear action probes that require labeled trajectories? The current method reduces supervision but still relies on a small percentage of labeled data.

- **Regularizing co-moving entities**: How can object-centric representations be regularized to prevent "collapsed" decomposition of distinct but co-moving entities (e.g., robotic arm and held object) in datasets with low trajectory diversity?

- **Real-world generalization**: To what extent does the framework degrade when applied to real-world internet video featuring heavy occlusions and multi-camera viewpoints? The study was confined to simulated benchmarks.

## Limitations

- The method assumes distractors are spatially separable entities, which may fail when the agent moves with distractors (e.g., tracked camera motion)
- Linear action probes assume action-relevant information is linearly separable in slot space, which may fail for complex multi-object interactions
- The evaluation uses curated distractors that may not represent real-world noise patterns

## Confidence

- **High confidence**: The core claim that object-centric pretraining improves latent action learning under distractors is well-supported by controlled experiments
- **Medium confidence**: The linear probe-based slot selection method works reliably when labeled data exists for probing, but effectiveness in true zero-shot settings is unverified
- **Medium confidence**: The mechanism explaining why slot-based representations outperform pixel-space masks is supported by performance numbers, though exact reasons could be explored further

## Next Checks

1. **Zero-shot slot selection**: Validate the method's effectiveness when no labeled trajectories exist for linear probing by comparing against random slot selection or unsupervised clustering approaches

2. **Generalization to novel distractors**: Test performance under distractors not seen during VideoSAUR pretraining to assess robustness to distribution shifts

3. **Multi-agent scenario validation**: Evaluate performance when multiple agents are present to test whether slot selection can distinguish between relevant and irrelevant agents in shared environments