---
ver: rpa2
title: Instruction-Following Evaluation in Function Calling for Large Language Models
arxiv_id: '2509.18420'
source_url: https://arxiv.org/abs/2509.18420
tags:
- function
- format
- arxiv
- instruction
- count
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces IFEval-FC, a benchmark designed to evaluate
  whether large language models correctly follow format instructions embedded in function
  parameter descriptions during function calling. Unlike existing benchmarks that
  only check argument correctness, IFEval-FC encodes verifiable format constraints
  (such as requiring no punctuation, specific case, or quoting rules) directly into
  JSON schema descriptions.
---

# Instruction-Following Evaluation in Function Calling for Large Language Models

## Quick Facts
- arXiv ID: 2509.18420
- Source URL: https://arxiv.org/abs/2509.18420
- Reference count: 4
- Primary result: Introduces IFEval-FC benchmark showing even top models (GPT-5, Claude 4.1) fail to follow format instructions in function calls, with no model exceeding 80% accuracy.

## Executive Summary
This paper introduces IFEval-FC, a benchmark designed to evaluate whether large language models correctly follow format instructions embedded in function parameter descriptions during function calling. Unlike existing benchmarks that only check argument correctness, IFEval-FC encodes verifiable format constraints (such as requiring no punctuation, specific case, or quoting rules) directly into JSON schema descriptions. The benchmark includes 750 test cases and uses fully algorithmic evaluation for objectivity and reproducibility. Experiments with state-of-the-art models show that none exceed 80% accuracy, indicating that even leading models frequently fail to adhere to simple formatting rulesâ€”a critical limitation for practical AI agent deployment.

## Method Summary
IFEval-FC is a benchmark that evaluates LLMs' ability to follow format instructions embedded in function parameter descriptions during function calling. The benchmark contains 750 test cases across 19 instruction types in 7 categories, using both real functions from BFCL and synthetic ones generated via GPT-5 across 80 domains. Evaluation is fully algorithmic, requiring models to produce function calls that satisfy specific format constraints (e.g., "no punctuation," "all lowercase," "quoted"). A mandatory system message forces models to always call a function, addressing high refusal rates in some models. Performance is measured as binary accuracy per instruction type and overall.

## Key Results
- No state-of-the-art model exceeds 80% accuracy on instruction-following in function calling
- Models frequently fail to adhere to simple formatting rules even when the correct data is generated
- Anthropic models show significantly higher refusal rates, requiring forced invocation via system prompts
- Accuracy varies by instruction type, with some constraints proving consistently more difficult than others

## Why This Works (Mechanism)

### Mechanism 1: Schema-Embedded Constraint Propagation
If verifiable format instructions are placed inside JSON schema parameter descriptions, models will treat them as binding constraints on the generated arguments. The model's context attention mechanism parses the function definition, coupling the instruction to the token generation probability for that specific argument. This assumes models' instruction-following training included tool use scenarios where parameter descriptions dictated format, not just semantics.

### Mechanism 2: Forced Generation via System Prompts
Highly aligned models may refuse to execute function calls if format constraints conflict with perceived ambiguity, requiring aggressive system-level overrides. The paper uses a mandatory system message ("YOU MUST CALL A FUNCTION NO MATTER WHAT") to suppress the model's inherent uncertainty-refusal alignment, forcing it to output a structured prediction that exposes its ability to satisfy the format constraint.

### Mechanism 3: Algorithmic Verifiability via Constraint Isolation
Evaluating instruction-following in tool use requires isolating the constraint to a single "free-form" parameter to ensure failures are due to format adherence, not parameter value reasoning. The dataset construction specifically targets "free-form parameters" for instruction injection, allowing deterministic script verification without an LLM judge.

## Foundational Learning

- **JSON Schema / Tool Definitions**: Essential for understanding how format instructions are embedded in parameter descriptions. Quick check: Can you locate where a "description" field fits inside a standard function-calling JSON payload?

- **IFEval (Instruction-Following Evaluation)**: The methodology IFEval-FC builds upon, using "verifiable instructions" rather than subjective quality scores. Quick check: Why is "write a summary" harder to grade automatically than "write a summary containing exactly 3 sentences"?

- **Agent Loops / Tool Use**: Understanding that IFEval-FC evaluates a single step in an agent loop (argument generation), where failure causes downstream tool execution crashes. Quick check: What happens downstream if an LLM passes a date as "July 4th" to an API expecting "2024-07-04"?

## Architecture Onboarding

- **Component map**: Dataset Generator -> Runner -> Evaluator
- **Critical path**: 
  1. Select function + instruction pair
  2. Generate user query
  3. Inject constraint into schema description
  4. Call LLM with modified schema
  5. Extract specific argument value
  6. Run specific validator
- **Design tradeoffs**: 
  - Real vs. Synthetic: Uses real functions from BFCL for validity but synthetic ones to allow "free-form" text constraints
  - Difficulty Filter: Explicitly removes trivial constraints to avoid ceiling effects, trading breadth for discriminative power
- **Failure signatures**:
  - The "Clarification" Loop: Model requests specification instead of JSON
  - Semantic Override: Model correctly formats but changes data
  - Schema Blindness: Model ignores the description field entirely
- **First 3 experiments**:
  1. Baseline Check: Run GPT-4o or Claude 3.5 on 750 test cases
  2. Ablation on System Prompt: Run Anthropic models without forced system message
  3. Constraint Complexity: Group results by instruction type to identify most brittle logic structures

## Open Questions the Paper Calls Out

### Open Question 1
How does model performance degrade when the model must select the correct function from a set of options rather than being provided with a single function? The current setup provides only one function, and a more challenging scenario would require function selection from a set of options.

### Open Question 2
Do instruction-following capabilities in function calling transfer effectively across different natural languages? The paper suggests future iterations could incorporate multilingual support to assess cross-lingual instruction-following capabilities.

### Open Question 3
Does high accuracy on IFEval-FC correlate with fewer downstream failures in complex, multi-step agentic workflows? While formatting errors are linked to downstream failures, it's unclear if correcting these errors significantly improves overall task completion rates.

## Limitations
- Synthetic function schemas may not fully capture real-world complexity
- Dataset construction relies on LLM-mediated steps (synthetic generation, query generation, difficulty filtering) with unspecified quantitative thresholds
- Benchmark focuses on single-step function calls rather than multi-turn agent interactions

## Confidence
- **High Confidence**: Benchmark design and evaluation methodology are sound; algorithmic verification addresses genuine evaluation gaps
- **Medium Confidence**: Dataset construction process introduces uncertainty about representativeness and difficulty calibration
- **Low Confidence**: Comparison between models is limited by forced system prompt intervention for Claude models

## Next Checks
1. **Dataset Representativeness Validation**: Have human annotators verify 50 test cases across different instruction categories for alignment with realistic API usage patterns
2. **Model Behavior Ablation Study**: Run controlled experiment comparing model performance with and without forced system prompt across all models
3. **Cross-Domain Generalization Test**: Evaluate IFEval-FC test cases on different function calling datasets (e.g., ToolLLM or ToolBench) to assess broader instruction-following limitations