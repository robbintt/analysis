---
ver: rpa2
title: 'EasyARC: Evaluating Vision Language Models on True Visual Reasoning'
arxiv_id: '2506.11595'
source_url: https://arxiv.org/abs/2506.11595
tags:
- green
- reasoning
- color
- grid
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EasyARC, a visual-language reasoning benchmark
  that requires multi-image, multi-step reasoning beyond simple visual extraction.
  EasyARC is procedurally generated, fully verifiable, and includes progressive difficulty
  levels, making it suitable for reinforcement learning pipelines.
---

# EasyARC: Evaluating Vision Language Models on True Visual Reasoning

## Quick Facts
- arXiv ID: 2506.11595
- Source URL: https://arxiv.org/abs/2506.11595
- Reference count: 32
- Models score below 20% accuracy despite benchmark being designed as "easy"

## Executive Summary
EasyARC introduces a visual-language reasoning benchmark requiring multi-image, multi-step reasoning beyond simple visual extraction. The benchmark is procedurally generated, fully verifiable through exact grid matching, and includes progressive difficulty levels suitable for reinforcement learning pipelines. The authors evaluate state-of-the-art vision-language models and find they score below 20% accuracy, revealing fundamental gaps in visual cognition including difficulties with precise spatial reasoning, handling noise, and integrating visual and linguistic information.

## Method Summary
EasyARC is a procedurally generated benchmark with 5 task categories (Cross/Star, Counting Cells, Double Grid, Dominant Side, Drop One Color) across three difficulty levels. Each task requires models to observe multiple input-output image pairs, extract a common transformation rule, and generalize to novel inputs. The benchmark uses a verifiable grid-based output format enabling exact cell-by-cell comparison against ground truth. Models receive stacked demonstration images with labels plus a text prompt, and must produce a delimited grid output that is programmatically verified.

## Key Results
- All models except Claude 3.7 Sonnet score below 20% accuracy on EasyARC
- Models struggle with precise spatial reasoning, noise handling, and visual-linguistic integration
- Performance drops sharply between easy and harder difficulty levels
- Current VLMs often fail at identifying connected components or handling bidirectional spatial transformations

## Why This Works (Mechanism)

### Mechanism 1: Multi-Image Transformation Rule Induction
Models must observe multiple input-output pairs, extract a common transformation rule, and generalize to novel inputs. The stacked image format forces simultaneous visual comparison rather than sequential text processing. This rules out spurious patterns and requires identifying invariant transformations across all examples.

### Mechanism 2: Verifiable Ground Truth Through Exact Grid Matching
Grid-based output format enables unambiguous, programmatic evaluation without human judgment or proxy metrics. Models must produce an exact grid matching the ground truth cell-by-cell, eliminating reward hacking through plausible-sounding but incorrect natural language responses.

### Mechanism 3: Difficulty-Graded Skill Composition
Progressive difficulty levels reveal which visual reasoning skills transfer and which are missing. Higher difficulty levels require supersets of skills from lower levels, allowing performance gaps between levels to isolate specific capability deficiencies.

## Foundational Learning

- **Abstract Reasoning Corpus (ARC) paradigm**: EasyARC is directly inspired by ARC; understanding the original challenge clarifies what constitutes "true visual reasoning" versus mere extraction. Quick check: Given input-output grid pairs demonstrating a hidden transformation, how would you verify whether a proposed rule is correct without seeing the test solution?

- **Connected component detection**: The Counting Cells task requires identifying and measuring the largest connected component—a classic CV operation that current VLMs fail at precisely. Quick check: Given a 10×10 grid with scattered colored cells, how would you algorithmically find the largest contiguous region of blue cells?

- **Spatial anchor points**: Cross/Star tasks require understanding that a highlighted cell's position determines where geometric structures are generated. Quick check: If a single yellow cell appears at position (4,3) in a green grid, what spatial relationships could that cell's position encode?

## Architecture Onboarding

- **Component map**: Generator functions → Visual encoder → Reasoning module → Output parser
- **Critical path**: Stack demonstration pairs + test input → Model receives stacked image + prompt → Extract visual features, induce rule, apply to test grid → Parse delimited grid output, compare cell-by-cell
- **Design tradeoffs**: Grid size range (3-10) balances complexity vs resolution; demonstration count affects rule ambiguity vs image clutter; difficulty sampling (50/35/15%) may need adjustment for frontier models
- **Failure signatures**: "Blurry vision" pattern (high-level features but missing fine details), unidirectional extension (generating patterns in only one direction), text-extraction fallback (converting grids to text before reasoning)
- **First 3 experiments**: 1) Reproduce Claude 3.7 baseline across categories and difficulties; 2) Ablate demonstration count to distinguish rule-discovery vs rule-application failures; 3) Isolate noise sensitivity by testing noise-free variants of medium/hard tasks

## Open Questions the Paper Calls Out

1. **Can RL pipelines utilizing EasyARC's progressive difficulty levels enable VLMs to learn "true" visual reasoning strategies rather than relying on text-based extraction?** The authors state EasyARC is "ideal for reinforcement learning (RL) pipelines" but only evaluate inference performance.

2. **Does increasing test-time compute allow models to overcome limitations in precise visual tasks, such as identifying connected components in the "Counting Cells" category?** The authors note models fail to look at the image "longer" to realize they should count connected components.

3. **How can vision-language models be trained to integrate spatial knowledge directly from visual inputs rather than relying on converting visual information into natural language for reasoning?** The failure mode analysis suggests current architectures treat vision as an extraction tool for text-based solvers.

## Limitations
- Evaluation framework assumes models can reliably parse their own grid-formatted output
- Skewed difficulty distribution (easy=50%) may not adequately stress-test frontier models
- Limited empirical validation of rule uniqueness across task categories

## Confidence
- **High confidence**: Verifiable evaluation mechanism, overall benchmark design, failure mode analysis
- **Medium confidence**: Claims about "true visual reasoning" versus pattern matching, skill composition between difficulty levels
- **Low confidence**: Claims that EasyARC is "easy" for humans (no human baseline reported)

## Next Checks
1. Conduct human evaluation study to measure how quickly humans solve EasyARC tasks across difficulty levels and validate the "easy" framing
2. Implement systematic rule ambiguity testing by generating demonstration pairs that could support multiple valid rules and measure model performance degradation
3. Run zero-shot cross-task generalization testing whether models that succeed on EasyARC can transfer to ARC or other grid-based reasoning benchmarks without fine-tuning