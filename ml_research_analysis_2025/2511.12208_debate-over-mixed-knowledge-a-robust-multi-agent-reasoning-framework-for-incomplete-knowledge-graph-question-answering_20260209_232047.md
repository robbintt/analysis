---
ver: rpa2
title: 'Debate over Mixed-knowledge: A Robust Multi-Agent Reasoning Framework for
  Incomplete Knowledge Graph Question Answering'
arxiv_id: '2511.12208'
source_url: https://arxiv.org/abs/2511.12208
tags:
- knowledge
- agent
- inference
- incompleteness
- ikgwq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel framework, Debate over Mixed-knowledge
  (DoM), to address the challenge of incomplete knowledge graph question answering
  (IKGQA). DoM leverages a multi-agent debate paradigm to dynamically integrate structured
  knowledge from KGs and unstructured knowledge from external sources.
---

# Debate over Mixed-knowledge: A Robust Multi-Agent Reasoning Framework for Incomplete Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2511.12208
- Source URL: https://arxiv.org/abs/2511.12208
- Reference count: 29
- Primary result: DoM achieves up to 13.6% relative improvement in Hits@1 on existing IKGQA datasets

## Executive Summary
This paper addresses the challenge of incomplete knowledge graph question answering (IKGQA) by proposing Debate over Mixed-knowledge (DoM), a novel multi-agent reasoning framework. DoM leverages a debate paradigm to dynamically integrate structured knowledge from knowledge graphs and unstructured knowledge from external sources like Wikipedia. The framework decomposes complex questions into sub-questions, uses specialized agents for KG-based and RAG-based inference, and employs a judge agent to synthesize final answers. The authors also construct a new benchmark, IKGWQ, to better reflect real-world KG incompleteness patterns. Extensive experiments demonstrate that DoM significantly outperforms state-of-the-art baselines across multiple IKGQA datasets.

## Method Summary
The paper proposes a novel multi-agent reasoning framework called Debate over Mixed-knowledge (DoM) to address incomplete knowledge graph question answering (IKGQA). DoM employs a debate paradigm where multiple agents collaborate to find answers by integrating both structured knowledge from knowledge graphs and unstructured knowledge from external sources like Wikipedia. The framework decomposes complex questions into sub-questions and utilizes specialized agents for KG-based and RAG-based inference. A judge agent then synthesizes the outputs from these agents to generate the final answer. The authors also construct a new benchmark dataset, IKGWQ, by injecting controlled incompleteness into the WebNLG dataset, which better reflects real-world KG incompleteness patterns.

## Key Results
- DoM achieves up to 13.6% relative improvement in Hits@1 compared to state-of-the-art baselines on existing IKGQA datasets
- The framework demonstrates strong performance on the newly constructed IKGWQ dataset, achieving 70.7% accuracy
- DoM shows robustness to varying levels of KG incompleteness and benefits from stronger LLM backbones

## Why This Works (Mechanism)
The debate paradigm enables effective integration of structured and unstructured knowledge sources by leveraging complementary strengths. KG agents provide precise, structured reasoning capabilities while RAG agents access broader contextual information from external sources. The judge agent's synthesis role ensures that the final answer benefits from both knowledge modalities. The sub-question decomposition strategy allows the framework to handle complex multi-hop reasoning tasks by breaking them into manageable components. This approach is particularly effective for IKGQA because it can compensate for missing KG facts by retrieving relevant information from external sources while maintaining the precision of structured reasoning.

## Foundational Learning
- **Multi-agent collaboration**: Why needed - Enables specialized agents to focus on different knowledge sources and reasoning strategies; Quick check - Verify each agent produces coherent intermediate outputs independently
- **Knowledge graph reasoning**: Why needed - Provides structured, precise reasoning over KG facts; Quick check - Ensure KG agents can correctly traverse entity relationships
- **Retrieval-augmented generation (RAG)**: Why needed - Accesses external knowledge to compensate for KG incompleteness; Quick check - Verify RAG agents can retrieve relevant context for given queries
- **Debate mechanisms**: Why needed - Enables synthesis of multiple perspectives to arrive at optimal answers; Quick check - Ensure judge agent can effectively combine conflicting outputs
- **Sub-question decomposition**: Why needed - Breaks complex reasoning into manageable components; Quick check - Verify decomposition produces answerable sub-questions
- **Benchmark construction with controlled incompleteness**: Why needed - Enables systematic evaluation of IKGQA performance; Quick check - Verify injected incompleteness follows real-world patterns

## Architecture Onboarding

Component Map: Question -> Sub-question Decomposition -> KG Agent & RAG Agent -> Judge Agent -> Final Answer

Critical Path: Question decomposition → Parallel KG and RAG inference → Judge synthesis → Answer generation

Design Tradeoffs: The framework trades computational overhead for improved accuracy by using multiple specialized agents rather than a single reasoning model. This approach enables better integration of heterogeneous knowledge sources but increases inference time and resource requirements.

Failure Signatures: Performance degradation when KG incompleteness exceeds the retrieval capacity of external sources, judge agent struggles to resolve conflicting outputs from specialized agents, or sub-question decomposition fails to capture the semantic relationships in complex queries.

First Experiments:
1. Verify sub-question decomposition correctly breaks down multi-hop questions into answerable components
2. Test KG and RAG agents independently on simple questions to ensure baseline functionality
3. Evaluate judge agent's ability to synthesize outputs from KG and RAG agents on straightforward queries

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The synthetic incompleteness injection method may not fully capture real-world KG incompleteness patterns
- Computational overhead of multi-agent debate remains unclear and potentially significant
- Strong dependence on LLM backbones raises scalability concerns for resource-constrained deployments

## Confidence
High: Core technical contribution of multi-agent debate for integrating structured and unstructured knowledge is well-supported
Medium: Performance improvements on existing benchmarks are convincing, but IKGWQ dataset construction needs further validation
Low: Claims about robustness to varying KG incompleteness levels need more extensive testing across diverse domains

## Next Checks
1. Test DoM on established benchmarks like WebQSP and ComplexWebQuestions with naturally occurring incompleteness
2. Conduct systematic measurements of inference time and resource consumption across different model scales
3. Evaluate the framework on KGs from different domains (biomedical, financial, scientific) to assess generalization