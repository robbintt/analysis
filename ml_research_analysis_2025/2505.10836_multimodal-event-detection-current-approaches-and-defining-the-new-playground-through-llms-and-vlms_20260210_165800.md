---
ver: rpa2
title: 'Multimodal Event Detection: Current Approaches and Defining the New Playground
  through LLMs and VLMs'
arxiv_id: '2505.10836'
source_url: https://arxiv.org/abs/2505.10836
tags:
- event
- approaches
- text
- detection
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multimodal event detection
  in social media for natural disasters, where traditional unimodal approaches struggle
  due to the rapid and varied nature of data dissemination. The authors propose a
  comprehensive framework integrating both unimodal and multimodal models, including
  ModernBERT, ConvNeXt-V2, fusion techniques, and generative models like GPT-4o and
  LLaVA.
---

# Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs

## Quick Facts
- **arXiv ID:** 2505.10836
- **Source URL:** https://arxiv.org/abs/2505.10836
- **Reference count:** 27
- **Primary result:** Multimodal fusion (ModernBERT + ConvNeXt-V2) achieves F1-score of 0.9459, outperforming both unimodal and generative approaches on disaster event detection

## Executive Summary
This paper addresses multimodal event detection in social media for natural disasters, where traditional unimodal approaches struggle due to rapid and varied data dissemination. The authors propose a comprehensive framework integrating both unimodal and multimodal models, including ModernBERT, ConvNeXt-V2, fusion techniques, and generative models like GPT-4o and LLaVA. Through systematic evaluation and error analysis, they demonstrate that multimodal approaches significantly outperform unimodal counterparts, with cross-attention fusion achieving the highest performance. The study reveals that while generative models exhibit superior robustness to social media linguistic perturbations, they lag behind supervised methods in precision due to information fabrication and instruction-following limitations.

## Method Summary
The paper systematically evaluates supervised and generative approaches for multimodal event detection. Supervised methods employ ModernBERT for text encoding, ConvNeXt-V2 for image encoding, and cross-attention fusion to combine modalities, with weighted random sampling for class imbalance. Generative models use few-shot prompting with GPT-4o and LLaVA under temperature 0.7 and top_k 0.8 settings. The framework evaluates both unimodal and multimodal settings across six disaster categories, using F1-score as the primary metric with weighted random sampling to address class imbalance (Non-damage: 2957 samples vs. Human Damage: 240 samples).

## Key Results
- Multimodal fusion (ModernBERT + ConvNeXt-V2) achieves F1-score of 0.9459, outperforming both unimodal text (0.9070) and image (0.9227) baselines
- Generative models demonstrate superior robustness to social media linguistic perturbations (leet speak, text elongation) but suffer from information fabrication and instruction-following failures
- Generative models require multimodal input to achieve reasonable performance, with significant degradation in unimodal settings (GPT-4o: 0.6378 multimodal vs. 0.3518 text-only and 0.4660 image-only)
- Error analysis reveals 38% of errors stem from incorrect handling of image-based cues, particularly in distinguishing visually similar events

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-modal fusion improves event detection by resolving ambiguities that either modality alone cannot handle.
- **Mechanism:** Visual features (ConvNeXt-V2) and textual embeddings (ModernBERT) are projected into a shared representation space where cross-attention computes relevance weights between modalities. This allows the model to leverage visual cues when text is ambiguous and textual context when images lack clear event signals.
- **Core assumption:** Disaster events manifest in both modalities with complementary information; neither modality is consistently sufficient alone.
- **Evidence anchors:**
  - [abstract]: "multimodal approaches significantly outperform unimodal counterparts, with the best-performing model (ModernBERT + ConvNeXt-V2) achieving an F1-score of 0.9459"
  - [section 6]: "ModernBERT + ConvNeXtv2 achieves an F1-score of 0.9459, outperforming both its text-only (0.9070) and image-only (0.9227) components"
  - [corpus]: Related work on multimodal misinformation detection (arxiv 2504.09154) similarly finds that "multimodal fake news detection benefits from the increased availability of information across multiple modalities"

### Mechanism 2
- **Claim:** Generative VLMs/LLMs exhibit superior robustness to social media linguistic perturbations compared to supervised models, despite lower overall precision.
- **Mechanism:** Large-scale pretraining on diverse internet text exposes generative models to informal language patterns (leet speak, elongation, typos) during pretraining, creating implicit robustness. Supervised models trained on limited annotated data lack this exposure.
- **Core assumption:** Linguistic perturbations in social media follow patterns that appear in pretraining corpora; generative models' robustness transfers from pretraining rather than task-specific learning.
- **Evidence anchors:**
  - [abstract]: "generative approaches demonstrate better handling of social media-specific linguistic perturbations such as leet speak and text elongation compared to supervised methods"
  - [section 6]: "GPT-4o was able to identify Leetspeaks like '4 people d!ed due to fire.' contains information related to a fire event"
  - [corpus]: Weak direct evidence in corpus for this specific mechanism; related stance detection survey (arxiv 2505.08464) discusses LLM contextual understanding but does not specifically address linguistic perturbations

### Mechanism 3
- **Claim:** Generative models require multimodal input to achieve reasonable performance; unimodal settings cause severe degradation.
- **Mechanism:** VLMs like GPT-4o and LLaVA are trained with image-text pairs, learning joint representations. When provided only text or only images, the model lacks the complementary modality it has learned to expect, leading to reduced confidence and increased hallucination.
- **Core assumption:** VLM pretraining creates dependency on both modalities; unimodal inference activates representations without the expected cross-modal grounding.
- **Evidence anchors:**
  - [abstract]: "generative models perform poorly in unimodal settings but show significant improvement when integrated with multimodal inputs"
  - [section 6]: "GPT-4o (multi-modal) achieves 0.6378 F1-score, a considerable improvement over both GPT-4o (text-only, 0.3518) and GPT-4o (image-only, 0.4660)"
  - [corpus]: No direct corpus evidence for this specific unimodal-to-multimodal degradation pattern in VLMs

## Foundational Learning

- **Concept:** Cross-Attention Mechanism
  - **Why needed here:** The paper's best-performing model uses cross-attention to fuse text and image features. Understanding how queries, keys, and values enable modality interaction is essential for debugging fusion failures.
  - **Quick check question:** Given text features W and image features V, can you explain what αc = softmax(QW * K_V^T) computes and how it modulates the fused representation?

- **Concept:** Instruction Following vs. Classification Heads
  - **Why needed here:** The paper reveals that generative models struggle with instruction following (outputting "There are no events" instead of class "Non-Damage"), while supervised models with classification heads avoid this by design.
  - **Quick check question:** Why does a softmax classification layer guarantee valid class outputs while prompt-based generative inference does not?

- **Concept:** Class Imbalance Handling
  - **Why needed here:** The dataset is imbalanced (Non-damage: 2957 samples vs. Human Damage: 240 samples). The paper mentions using weighted random sampling based on inverse class frequency.
  - **Quick check question:** If class A has 1000 samples and class B has 100 samples, what weight would inverse frequency sampling assign to each class?

## Architecture Onboarding

- **Component map:**
  - Text preprocessing (handle hashtags, mentions) → ModernBERT encoding → Image preprocessing → ConvNeXt-V2 encoding → Cross-attention fusion → Classification

- **Critical path:** Text preprocessing (handle hashtags, mentions) → ModernBERT encoding → Image preprocessing → ConvNeXt-V2 encoding → Cross-attention fusion → Classification. The fusion step is the critical differentiator; unimodal baselines skip cross-attention.

- **Design tradeoffs:**
  - Supervised fusion vs. Generative prompting: Supervised achieves 0.9459 F1 but requires labeled data; generative achieves 0.6378 F1 but handles linguistic noise better and requires no training data
  - Simple concatenation vs. Cross-attention: Cross-attention adds ~10-15% training time but improves F1 from 0.9250 (vanilla fusion) to 0.9459
  - ModernBERT vs. BERT: ModernBERT improves text-only F1 from 0.3314 to 0.9070 on this task

- **Failure signatures:**
  - Information fabrication: Generative models output classes not in the predefined set (e.g., "earthquake" when not in taxonomy)
  - Format mismatch: Models generate sentences instead of JSON: "The input shows a flooding event" vs. {"classification": "flood"}
  - Ambiguous visual cues: Earthquake events misclassified as floods when images show water without structural damage (38% of errors from image misinterpretation)

- **First 3 experiments:**
  1. Replicate unimodal baselines: Train BERT-only and ConvNeXt-V2-only classifiers to establish performance bounds; expect ~0.90 (text) and ~0.92 (image) F1
  2. Ablate fusion mechanisms: Compare vanilla concatenation fusion vs. cross-attention fusion; expect ~2% F1 improvement from cross-attention
  3. Test generative robustness: Create synthetic test sets with leet speak ("f1r3 d4m4g3") and elongation ("fiiiiire!!!"); compare GPT-4o vs. ModernBERT error rates

## Open Questions the Paper Calls Out

- **Question:** Can prompt optimization or instruction-tuning effectively bridge the performance gap between generative models and supervised approaches by reducing "information fabrication"?
  - **Basis in paper:** [explicit] The authors note generative models lag due to fabrication and suggest exploring "prompt design or prompt optimization" and "instruction-tuning" in future work.
  - **Why unresolved:** Current generative models (GPT-4o, LLaVA) frequently failed to follow output formats (e.g., generating sentences instead of classes), significantly hurting precision compared to supervised baselines.
  - **What evidence would resolve it:** Achieving comparable precision scores to supervised baselines (F1 > 0.94) on the disaster dataset while maintaining the robustness of generative models against linguistic noise.

- **Question:** How can multimodal event detection frameworks be adapted to generalize across diverse geographical regions and languages?
  - **Basis in paper:** [explicit] Listed under Limitations: "Future work will focus on... generalizing across different geographical regions and languages."
  - **Why unresolved:** Social media data varies significantly by region and language, and the current study relies on a specific dataset without evaluating cross-cultural or cross-linguistic transfer.
  - **What evidence would resolve it:** Successful evaluation of the model on multilingual or region-specific disaster datasets without significant performance degradation.

- **Question:** How can models be improved to handle ambiguous visual data where event-specific cues (e.g., structural damage for earthquakes) are absent?
  - **Basis in paper:** [inferred] Error analysis revealed that 38% of errors stemmed from "incorrect handling of image-based cues," specifically where visual evidence was insufficient to distinguish events like earthquakes from floods.
  - **Why unresolved:** Current cross-modal attention mechanisms still struggle to disambiguate events when the visual modality lacks distinct features, relying heavily on explicit visual markers.
  - **What evidence would resolve it:** A statistically significant reduction in false positives for visually similar classes during qualitative error analysis on ambiguous image sets.

## Limitations
- Generative models' superior robustness to linguistic perturbations lacks direct empirical validation and strong corpus evidence
- Cross-attention fusion mechanisms may introduce noise when one modality is misleading or missing, but break conditions are not explored
- The performance degradation of generative models in unimodal settings is demonstrated but not investigated through instruction-tuning or unimodal fine-tuning

## Confidence
- **High Confidence:** Multimodal approaches significantly outperform unimodal counterparts (F1 0.9459 vs. 0.9070 text-only and 0.9227 image-only); cross-attention fusion improves performance over vanilla concatenation
- **Medium Confidence:** Generative models handle social media linguistic perturbations better than supervised methods, despite lower overall precision; generative models require multimodal input to achieve reasonable performance
- **Low Confidence:** The specific mechanisms by which pretraining exposure creates robustness to linguistic perturbations in generative models; the assumption that VLMs inherently depend on both modalities due to pretraining

## Next Checks
1. **Perturbation Robustness Test:** Create synthetic test sets with controlled leet speak and text elongation variations. Compare GPT-4o vs. ModernBERT error rates on these sets to quantify the claimed robustness advantage of generative models.
2. **Unimodal Adaptation Experiment:** Fine-tune GPT-4o and LLaVA on unimodal data (text-only and image-only subsets) and re-evaluate their performance. This would test whether the unimodal degradation is inherent to VLMs or can be mitigated through adaptation.
3. **Fusion Break Condition Analysis:** Systematically evaluate the supervised fusion model on cases where one modality is misleading (stock images with irrelevant text, or text describing non-visual events). Measure whether fusion performance degrades below the best unimodal baseline in these scenarios.