---
ver: rpa2
title: The Blessing and Curse of Dimensionality in Safety Alignment
arxiv_id: '2507.20333'
source_url: https://arxiv.org/abs/2507.20333
tags:
- safety
- linear
- refusal
- fjlt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how the dimensionality of internal representations
  in large language models (LLMs) affects safety alignment. The authors hypothesize
  that as models scale up, their high-dimensional activation spaces develop exploitable
  linear structures that can be manipulated via activation engineering to bypass safety
  controls.
---

# The Blessing and Curse of Dimensionality in Safety Alignment

## Quick Facts
- **arXiv ID:** 2507.20333
- **Source URL:** https://arxiv.org/abs/2507.20333
- **Reference count:** 40
- **One-line primary result:** Reducing activation dimensionality via FJLT projections or bottleneck autoencoders significantly improves safety alignment while maintaining utility on benign tasks.

## Executive Summary
This paper explores how the dimensionality of internal representations in large language models affects safety alignment. The authors hypothesize that as models scale up, their high-dimensional activation spaces develop exploitable linear structures that can be manipulated via activation engineering to bypass safety controls. Through visualizations and linear probe experiments, they show that abstract concepts like safety become increasingly linearly separable in higher dimensions, making models more vulnerable to jailbreaking attacks. To address this, the authors propose two fine-tuning methods—one using Fast Johnson-Lindenstrauss Transform (FJLT) projections and another using a bottleneck autoencoder—to project activations into lower-dimensional subspaces. Empirical results demonstrate that both approaches significantly reduce susceptibility to linear jailbreak attacks while maintaining safety alignment. The work highlights the dual nature of model scaling: increased dimensions enhance capabilities but also create new vulnerabilities that require targeted defenses.

## Method Summary
The paper proposes two fine-tuning methods to defend LLMs against linear jailbreak attacks by reducing activation dimensionality. The FJLT method projects attention Query and Key matrices using a structured random matrix that preserves pairwise distances while embedding into a lower dimension. The Bottleneck method inserts a linear autoencoder between transformer layers that compresses activations through a learned bottleneck. Both methods are fine-tuned using refusal data and evaluated against ActAdd jailbreak attacks while measuring utility on benign tasks.

## Key Results
- Both FJLT and Bottleneck methods significantly reduce jailbreak success rates (ActAdd attacks) while maintaining reasonable utility on benign tasks
- Bottleneck method achieves better utility preservation across diverse benchmarks compared to FJLT
- Both defenses show degraded performance against non-linear GCG attacks
- Visualizations demonstrate that reduced dimensionality disrupts linear separability of safety concepts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reducing dimensionality degrades linear steering vector effectiveness by forcing safety concepts to shatter across subspace
- **Mechanism:** High-dimensional safety concepts are linearly separable along single directions. Dimension reduction increases Rademacher complexity of finding effective linear steering vectors
- **Core assumption:** Attacks rely on identifying single linear direction representing safety concept
- **Evidence anchors:** [abstract] Hypothesis about exploitable linear structures; [section 3.2/Proposition 1] Theoretical Rademacher complexity bounds; [corpus] Weak corpus support
- **Break condition:** Fails if safety is encoded non-linearly or attacks don't rely on linear directions

### Mechanism 2
- **Claim:** Bottleneck autoencoder disrupts linear separability while preserving task-relevant features
- **Mechanism:** Linear autoencoder forces activations through lower-dimensional bottleneck during fine-tuning with dual objective (refusal + utility), encouraging non-linear re-encoding of safety
- **Core assumption:** Bottleneck with specific fine-tuning objective disrupts linear safety direction while maintaining other task performance
- **Evidence anchors:** [abstract] Bottleneck proposal; [section 5.2/Figure 4] Visualizations showing concept mixing; [Table 2] Empirical results on Llama2-7B-Chat; [corpus] Weak corpus support
- **Break condition:** Too small K causes overfitting to refusal; late layer placement degrades utility

### Mechanism 3
- **Claim:** FJLT preserves attention behavior while embedding into lower dimension resistant to steering
- **Mechanism:** FJLT structured random matrix preserves pairwise Euclidean distances between queries and keys while reducing dimensionality
- **Core assumption:** Preserving query-key distances maintains utility; dimensionality reduction drives defense
- **Evidence anchors:** [abstract] FJLT proposal; [section 4.1] Johnson-Lindenstrauss lemma motivation; [Table 1] Empirical results; [corpus] Weak corpus support
- **Break condition:** Significant utility degradation on out-of-distribution tasks; poor performance against GCG

## Foundational Learning

- **Concept: Linear Representation Hypothesis**
  - **Why needed here:** Safety concepts must be linearly represented as vectors for defense strategy to work
  - **Quick check question:** If safety is linearly represented, how would harmful vs harmless prompt activations differ in 2D PCA plot?

- **Concept: Activation Engineering / Steering Vectors**
  - **Why needed here:** Understanding attacks add/ablating specific activation vectors is necessary to grasp defense rationale
  - **Quick check question:** What's the difference between ActAdd and Ablation attacks?

- **Concept: Rademacher Complexity**
  - **Why needed here:** Used to theoretically justify why reducing dimensionality makes linear classifiers less effective
  - **Quick check question:** According to Proposition 1, how does Rademacher complexity scale with input dimension D?

## Architecture Onboarding

- **Component Map:** Base Model -> FJLT Module (projects Q,K matrices) OR Base Model -> Bottleneck Module (compresses activations between layers)

- **Critical Path:**
  1. Select defense (FJLT or Bottleneck)
  2. Configure hyperparameters (K dimension, attention heads for FJLT; layer position for Bottleneck)
  3. Prepare data (refusal dataset + anchor utility for Bottleneck)
  4. Fine-tune modified model
  5. Evaluate against ActAdd jailbreak and utility benchmarks

- **Design Tradeoffs:**
  - FJLT: Theoretically grounded for distance preservation but causes larger utility drops on out-of-distribution tasks
  - Bottleneck: More flexible localized change that often better preserves utility
  - Dimensionality (K): Smaller K increases defense strength but risks utility collapse
  - Placement (Bottleneck): Early placement (layer 0) yields better results; late placement degrades performance

- **Failure Signatures:**
  - Utility Collapse: High perplexity or refusal of benign prompts (K too small)
  - Defense Failure: Low refusal/safety scores under ActAdd (linear structure not disrupted)
  - Poor Generalization: Good fine-tuning performance but failure on benchmarks like GSM8k

- **First 3 Experiments:**
  1. Replicate main result: Fine-tune Llama2-7B-Chat with Bottleneck (K=2048, layer=0) and evaluate on JailbreakBench and Alpaca
  2. Ablation on Bottleneck placement: Insert at different layers (0, 10, 20, 31) and compare defense efficacy and utility
  3. Test against non-linear attack: Evaluate FJLT and Bottleneck against GCG attack to verify limitation

## Open Questions the Paper Calls Out

- Can a principled semantics-based projection method be developed that preserves task-relevant concepts while eliminating safety exploitability?
- How can dimensionality reduction defenses be extended to protect against non-linear jailbreak attacks while maintaining effectiveness against linear attacks?
- What theoretical framework guides optimal selection of projection dimensions, layer positions, and attention heads?

## Limitations

- Core assumption that safety concepts are linearly separable may not hold universally
- Both methods involve fundamental tradeoff between defense strength and utility preservation
- Effectiveness depends heavily on quality and diversity of fine-tuning data
- FJLT method specifically targets attention mechanisms, raising questions about generalization to other attack vectors

## Confidence

- **High Confidence:** Empirical results showing dimensionality reduction methods significantly reduce linear jailbreak susceptibility
- **Medium Confidence:** Theoretical analysis linking dimensionality to Rademacher complexity and linear separability
- **Low Confidence:** Claims about dimensionality blessing/curse being fundamental property of scaling

## Next Checks

1. Test against evolved attack strategies that explicitly avoid linear directions, such as iterative optimization-based methods
2. Cross-domain utility assessment by fine-tuning on one task domain and evaluating across diverse benchmarks including mathematical reasoning
3. Layer-wise activation analysis through systematic ablation studies varying Bottleneck insertion layer to identify optimal configurations