---
ver: rpa2
title: Using physics-inspired Singular Learning Theory to understand grokking & other
  phase transitions in modern neural networks
arxiv_id: '2512.00686'
source_url: https://arxiv.org/abs/2512.00686
tags:
- learning
- neural
- experiment
- singular
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically studies Singular Learning Theory (SLT) to
  understand grokking and phase transitions in neural networks. The authors investigate
  how SLT's free energy and local learning coefficient (LLC) relate to phase transitions
  and model complexity.
---

# Using physics-inspired Singular Learning Theory to understand grokking & other phase transitions in modern neural networks

## Quick Facts
- arXiv ID: 2512.00686
- Source URL: https://arxiv.org/abs/2512.00686
- Authors: Anish Lakkapragada
- Reference count: 3
- Key outcome: Empirically tests Singular Learning Theory to understand grokking and phase transitions, finding mixed evidence for Arrhenius-style free energy barriers and confirming theoretical LLC scaling laws in specific architectures.

## Executive Summary
This paper applies Singular Learning Theory (SLT) to study grokking and phase transitions in neural networks, testing whether transition timing relates to free energy barriers (Arrhenius hypothesis) and how Local Learning Coefficient (LLC) scales with problem difficulty. The work investigates three architectures: polynomial regressors, low-rank linear networks, and autoencoders. Results show mixed evidence for the Arrhenius hypothesis across different tasks, while LLC scaling follows theoretical predictions in low-rank networks (quadratic) and autoencoders (linear), but falls below expectations for polynomial regressors due to domain-induced singularities.

## Method Summary
The study uses Stochastic Langevin Gradient Descent (SGLD) to estimate LLC at training checkpoints, then computes free energy as $F_n = nL_n + \lambda_\alpha \log n$. For grokking experiments, the Arrhenius hypothesis is tested by measuring the relationship between free energy differences and transition times. LLC scaling is examined by varying problem difficulty parameters (degree, rank) across architectures. The methodology includes transition detection through smoothed loss curves, with careful attention to hyperparameter sensitivity in SGLD estimation.

## Key Results
- Mixed evidence for Arrhenius hypothesis: modulo arithmetic showed consistent negative slope with low R²; TMS experiments yielded inconclusive vertical clusters sensitive to detection method
- Polynomial regressors exhibit LLC below theoretical $d/2$ due to singularities from constrained input domains
- Low-rank networks follow predicted quadratic LLC scaling: $\lambda_r \approx \frac{1}{2}r(2d-r)$
- Autoencoder LLC scales linearly with bottleneck dimension (R² = 0.998), a unique finding without theoretical explanation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phase transition timing may relate exponentially to free energy differences between weight-space regions.
- Mechanism: Borrowing from chemical kinetics (Arrhenius equation), the time to transition $r_{i \to j}$ between memorizing and grokking states scales as $\exp(\beta_{eff} \Delta F_{i \to j})$ where $\Delta F_{i \to j} < 0$ is the free energy reduction.
- Core assumption: The effective inverse temperature $\beta_{eff}$ remains approximately constant across runs with fixed hyperparameters.
- Evidence anchors:
  - [abstract] "Mixed evidence for the Arrhenius hypothesis on modulo arithmetic and Anthropic's Toy Models of Superposition"
  - [section 3, p.3-4] Modulo arithmetic ($p=53$) showed consistent negative slope between $\Delta F_{i \to j}$ and $\log r_{i \to j}$ albeit with low $R^2$; TMS experiments yielded vertical clusters and upward-sloping fits sensitive to transition detection method
- Break condition: Transition detection sensitivity (see TMS vertical clusters); hypothesis fails if $\Delta F_{i \to j}$ does not consistently predict $\log r_{i \to j}$ slope direction across detection methods

### Mechanism 2
- Claim: Domain constraints induce singularities in otherwise regular models, reducing LLC below theoretical predictions.
- Mechanism: Polynomial regressors are formally regular (bijection between coefficients and functions on $\mathbb{R}$), but constrained input domains $X \subset \mathbb{R}$ create situations where $f_A|_X = f_B|_X$ despite $A \neq B$.
- Core assumption: LLC reductions scale with constraint tightness (more constrained domains → more singularities → lower LLC).
- Evidence anchors:
  - [abstract] "LLC for polynomial regressors is lower than theoretical expectations due to singularities induced by constrained domains"
  - [section 3, p.5-6] Figure 5 shows LLC vs. degree curves below theoretical $\lambda_d = d/2$ across all $X \in \{[-1,1], [-0.75,0.75], [-0.5,0.5]\}$, with tighter intervals yielding lower LLCs
- Break condition: Assumption would break if domain constraints do not systematically create parameter degeneracies (i.e., if $f_A|_X = f_B|_X$ implies $A = B$ for the constrained domain)

### Mechanism 3
- Claim: LLC scales with intrinsic problem dimension in low-rank matrix factorization and autoencoder settings.
- Mechanism: For low-rank networks $f(x) = W_2 W_1 x$, the rank-$r$ parameter manifold has dimension $r(2d - r)$, yielding $\lambda_r \approx \frac{1}{2}r(2d-r)$ (quadratic in $r$). For autoencoders with bottleneck dimension $r$, LLC scales linearly despite MLP singularities.
- Core assumption: The algebraic geometry prediction for manifold dimension translates directly to LLC in these architectures.
- Evidence anchors:
  - [abstract] "LLC for low-rank networks follows the predicted quadratic scaling; LLC for autoencoders shows a linear scaling"
  - [section 3, p.6-7] Figure 6 shows quadratic fit ($-0.429r^2 + 87.342r$) matching $\frac{1}{2}r(2d-r)$ with $d=100$; Figure 7 shows linear fit with $R^2 = 0.998$
- Break condition: Different architectures or optimization procedures could alter the effective manifold geometry; linear autoencoder scaling remains unexplained theoretically

## Foundational Learning

- **Concept: Singular vs. Regular Models**
  - Why needed here: SLT's entire framework distinguishes singular models (non-identifiable: multiple parameterizations yield identical functions) from regular models. Neural networks are singular, violating PAC bound assumptions.
  - Quick check question: Can you explain why $\text{ReLU}(x) = \frac{1}{\alpha}\text{ReLU}(\alpha x)$ for all $\alpha > 0$ makes ReLU networks singular?

- **Concept: Local Learning Coefficient (LLC)**
  - Why needed here: LLC $\lambda_\alpha$ is the core complexity measure studied. It emerges from the RLCT (real log canonical threshold) and quantifies model complexity near singularities—higher LLC means greater complexity.
  - Quick check question: What does it mean physically when constraining a polynomial regressor's input domain reduces its LLC?

- **Concept: Free Energy in SLT**
  - Why needed here: The free energy $F_n \approx \min_\alpha[nL_n(w^*_\alpha) + \lambda_\alpha \log n]$ governs internal model selection, trading off loss against complexity. The Arrhenius hypothesis links $\Delta F$ to transition timing.
  - Quick check question: In the free energy formula, what happens to the loss-complexity tradeoff as $n \to \infty$?

## Architecture Onboarding

- **Component map:**
  - Training loop with checkpoint logging -> SGLD LLC estimator -> Free energy computation -> Transition detection -> Arrhenius hypothesis testing / LLC scaling analysis

- **Critical path:**
  1. Train model with checkpoint logging (100 linearly/logarithmically spaced)
  2. At each checkpoint, run SGLD to estimate LLC
  3. Compute free energy $F_n = nL_n + \lambda_\alpha \log n$
  4. For Arrhenius tests: detect transitions, compute $\Delta F_{i \to j}$ and $r_{i \to j}$, fit $\log r$ vs. $\Delta F$
  5. For scaling tests: vary difficulty parameter (degree/rank), fit LLC curve

- **Design tradeoffs:**
  - **SGLD hyperparameters**: Larger $\epsilon$ speeds estimation but risks instability; Table 1 shows $\epsilon \in [10^{-5}, 10^{-3}]$ across experiments
  - **Transition detection method**: Smoothing window and threshold highly affect TMS results (upward vs. downward slope)
  - **Checkpoint density**: More checkpoints improve resolution but increase compute (7 days CPU for 500 modulo arithmetic runs)

- **Failure signatures:**
  - **LLC estimates unstable/diverging**: SGLD hyperparameters poorly tuned for architecture
  - **TMS vertical clusters in $\log r$ vs. $\Delta F$**: Transition detection too coarse or model too small
  - **Polynomial LLC matches theoretical $d/2$**: Domain constraint not inducing singularities (check $X$ bounds)

- **First 3 experiments:**
  1. **Replicate polynomial regressor** with $d \in [100, 1000]$ on $X = [-0.5, 0.5]$, verify LLC < $d/2$; tighten $X$ further to confirm lower LLC
  2. **Low-rank matrix factorization** with $d=100$, vary $r \in [1, 100]$, confirm quadratic fit $\lambda_r \approx \frac{1}{2}r(2d-r)$
  3. **Modulo arithmetic grokking** with $p=53$, train 50 models, test Arrhenius hypothesis—check if $\log r_{i \to j}$ vs. $\Delta F_{i \to j}$ shows negative slope

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a principled method be developed to estimate free-energy barriers between weight space subsets to enable more direct tests of the Arrhenius-style reaction-rate hypothesis?
- Basis in paper: [explicit] Authors state in Future Directions: "Develop a principled method to estimate the free-energy barrier(s) between two subsets of weight space W_i and W_j. This would enable a more direct test of the Arrhenius-style reaction-rate hypothesis relating ∆F_{i→j} to r_{i→j}."
- Why unresolved: Current experiments used ∆F between observed states, not true transition barriers. Mixed evidence (modulo arithmetic showed negative slope with low R²; TMS was inconclusive due to transition detection sensitivity) suggests the hypothesis needs refinement with proper barrier estimation.
- What evidence would resolve it: A validated methodology for computing energy barriers that yields consistent exponential relationships between barrier height and transition time across multiple architectures.

### Open Question 2
- Question: Do constrained input domains (e.g., bounded pixel intensities) systematically induce singularities that lower LLC and improve generalization across standard deep learning settings?
- Basis in paper: [explicit] Authors propose: "Identify other standard deep-learning settings where natural domain constraints (e.g. pixel intensities in [0,255]) cause distinct parameterizations to yield the same function behavior... This could clarify when such constraints meaningfully lower the LLC."
- Why unresolved: Polynomial regressor experiments showed LLC lower than theoretical d/2 expectation due to constrained X⊂R domains, but this phenomenon has only been demonstrated in one toy setting.
- What evidence would resolve it: Systematic measurements showing reduced LLC in image models using constrained [0,255] inputs versus unconstrained inputs, with corresponding generalization improvements.

### Open Question 3
- Question: How does LLC differ between over-parameterized memorization architectures and compact generalization architectures as task difficulty increases?
- Basis in paper: [explicit] Authors request: "For a fixed task family (e.g. image classification), compare the LLC of a heavily over-parameterized 'memorization' network and a more compact 'generalization' network. See how this difference scales with problem difficulty."
- Why unresolved: Preliminary sinusoidal regression experiments found memorization networks had smaller LLC than generalization networks (contrary to intuition), suggesting SLT complexity measures may behave counterintuitively in highly singular models.
- What evidence would resolve it: Controlled experiments across multiple task families showing consistent LLC scaling patterns for memorization vs. generalization architectures with varying problem difficulty.

### Open Question 4
- Question: Why does the LLC of autoencoders scale linearly with bottleneck dimension despite MLPs being highly singular models?
- Basis in paper: [inferred] Authors note the linear fit (R²=0.998) is "unique" and they "have not seen other scaling law results like this in the SLT literature," yet provide no theoretical explanation for this empirical finding.
- Why unresolved: Standard SLT theory predicts more complex relationships for singular models; the clean linear scaling lacks theoretical grounding despite its empirical robustness.
- What evidence would resolve it: Theoretical analysis deriving the linear relationship from autoencoder structure, or identification of the geometric/structural properties that enable this scaling behavior.

## Limitations
- Mixed empirical support for the Arrhenius hypothesis suggests sensitivity to experimental conditions and detection methods
- LLC scaling laws are demonstrated for specific architectures but lack complete theoretical grounding, particularly for the linear scaling in autoencoders
- Domain-induced singularities in polynomial regressors represent a novel observation but may not generalize to other constrained models

## Confidence
- **High Confidence**: LLC scaling laws for low-rank networks (quadratic) and autoencoders (linear) - supported by clean empirical fits
- **Medium Confidence**: Arrhenius hypothesis mechanism - mixed evidence with strong dependence on detection methods
- **Medium Confidence**: Domain-induced singularities reducing LLC - clear empirical pattern but limited theoretical explanation

## Next Checks
1. Replicate the Arrhenius hypothesis with standardized transition detection using multiple smoothing windows and detection thresholds across both modulo arithmetic and TMS tasks to quantify sensitivity
2. Test LLC scaling in additional architectures (e.g., transformers, ResNets) to verify whether the observed quadratic/linear patterns generalize beyond the studied cases
3. Systematically vary domain constraints for polynomial regressors with precise control over singularity-inducing conditions to better understand the relationship between constraint tightness and LLC reduction