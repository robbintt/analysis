---
ver: rpa2
title: 'MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and Emotion
  Detection'
arxiv_id: '2502.08319'
source_url: https://arxiv.org/abs/2502.08319
tags:
- propaganda
- annotation
- dataset
- sentiment
- arabic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiProSE, the first Arabic multi-label
  dataset for propaganda, sentiment, and emotion detection, extending the ArPro dataset
  with manually annotated sentiment and emotion labels across 8,000 news articles.
  Three annotators with PhDs in relevant fields used majority voting to label each
  text for propaganda, sentiment (positive, negative, neutral), and emotion (happiness,
  sadness, anger, fear, none).
---

# MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and Emotion Detection

## Quick Facts
- arXiv ID: 2502.08319
- Source URL: https://arxiv.org/abs/2502.08319
- Reference count: 0
- Introduces first Arabic multi-label dataset for propaganda, sentiment, and emotion detection with 8,000 news articles

## Executive Summary
This paper introduces MultiProSE, the first Arabic multi-label dataset for propaganda, sentiment, and emotion detection. The dataset extends the ArPro corpus with manually annotated sentiment (positive, negative, neutral) and emotion (happiness, sadness, anger, fear, none) labels across 8,000 news articles. Three annotators with PhDs in relevant fields used majority voting to label each text, achieving substantial inter-annotator agreement scores. The dataset contains 63% propagandistic content, with negative sentiment and anger being most frequent in such texts. The authors established strong baselines using GPT-4o-mini and three BERT-based models, achieving Micro-F1 scores of 0.769 for propaganda, 0.842 for sentiment, and 0.750 for emotion detection.

## Method Summary
The MultiProSE dataset was created by extending ArPro (6,002 train, 672 validation, 1,326 test) with sentiment and emotion labels. Three PhD-level annotators performed majority voting on each text for propaganda (binary), sentiment (3-class), and emotion (5-class) labels. Quality control mechanisms included gold data injection (5% of annotations) and annotator trust scores below 70% resulting in discarded annotations. The authors fine-tuned GPT-4o-mini and three BERT-based models (AraBERT, XLM-RoBERTa, ArabicBERT) using AdamW optimizer (lr=2e-5), batch size 8, and 256 token max length. They reported average performance over 6 seeds for PLMs and 3 seeds for LLM across Micro-F1, Macro-F1, and Accuracy metrics.

## Key Results
- AraBERT achieved highest Micro-F1 scores: 0.769 (propaganda), 0.736 (sentiment), 0.675 (emotion)
- GPT-4o-mini outperformed other models on sentiment (0.842) and emotion (0.750) detection
- XLM-RoBERTa consistently underperformed Arabic-specific models across all tasks
- Substantial inter-annotator agreement achieved: Light's Kappa=0.7074 (emotion), 0.8128 (sentiment)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-label annotation combining propaganda, sentiment, and emotion enables cross-task signal transfer for detection models.
- Mechanism: Propaganda techniques exploit emotional salience (e.g., loaded language correlates with anger/fear). By co-annotating these dimensions, models can learn shared representations where emotional features serve as auxiliary signals for propaganda detection.
- Core assumption: Emotional and sentiment features provide predictive value for identifying propagandistic content beyond lexical cues alone.
- Evidence anchors: [abstract] "analyzing the interaction between emotions and propaganda, as well as sentiment and propaganda, could benefit overall propaganda detection"; [section 1] cites studies [5-10] showing "direct correlation between several persuasion techniques and emotional salience features"; [corpus] Related work on explainable propaganda detection (PropXplain) suggests multi-dimensional labeling supports interpretability.

### Mechanism 2
- Claim: Arabic-specific pretraining (AraBERT) outperforms multilingual models (XLM-RoBERTa) on Arabic propaganda tasks due to language-specific morphological and lexical knowledge.
- Mechanism: AraBERT is trained on 8.6B tokens of MSA text, capturing Arabic-specific patterns (morphology, orthographic conventions, dialectal variations) that multilingual models dilute across 100 languages.
- Core assumption: Propaganda techniques in Arabic rely on language-specific rhetorical patterns not well-captured by multilingual representations.
- Evidence anchors: [section 4.2] AraBERT achieves Micro-F1=0.769 vs. XLM-RoBERTa's 0.683 on propaganda; similar gaps for sentiment (0.736 vs. 0.698) and emotion (0.675 vs. 0.648); [section 1] "Arabic language has challenges in NLP because of its complex morphology, orthographic ambiguity, limited availability of linguistic resources"; [corpus] Arabic Multimodal ML paper confirms language-specific models are critical for Arabic NLP tasks.

### Mechanism 3
- Claim: Quality control mechanisms (gold data injection, annotator trust scores, multi-phase training) yield high inter-annotator agreement despite subjective task complexity.
- Mechanism: Injecting 5% gold-standard texts with known labels allows continuous monitoring of annotator reliability. Annotators below 70% accuracy have annotations discarded, preventing label noise propagation.
- Core assumption: Annotator consistency on gold samples predicts consistency on unlabeled samples; majority voting of 3 expert annotators reduces individual bias.
- Evidence anchors: [section 3.6] IAA scores: Light's Kappa=0.7074 (emotion), 0.8128 (sentiment); interpreted as "substantial" agreement; [section 3.4] "if the annotator's trust score...is below 70%, all their submitted annotations are eliminated"; [corpus] No direct corpus evidence on annotation QC transferability.

## Foundational Learning

- **Concept: Multi-label vs. Multi-class Classification**
  - Why needed here: MultiProSE assigns multiple labels per text (propaganda binary + sentiment 3-class + emotion 5-class). Understanding label independence vs. mutual exclusivity is critical for model design.
  - Quick check question: Can a text be both "positive sentiment" and "anger emotion"? (Yes—the paper explicitly handles this; see Table 2 examples.)

- **Concept: Modern Standard Arabic (MSA) vs. Dialectal Arabic**
  - Why needed here: The dataset is primarily MSA but contains dialectal quotes. Preprocessing and model selection must account for this variation.
  - Quick check question: Why might AraBERT outperform a dialect-specific model on this dataset? (Dataset is predominantly MSA; AraBERT's training data aligns better.)

- **Concept: Inter-Annotator Agreement Metrics (Cohen's/Fleiss'/Light's Kappa)**
  - Why needed here: IAA scores determine label reliability. Kappa >0.6 indicates substantial agreement; scores vary by task complexity (sentiment easier than emotion).
  - Quick check question: Why is emotion IAA (0.707) lower than sentiment IAA (0.813)? (More classes = harder annotation; emotion categories overlap more.)

## Architecture Onboarding

- **Component map**: Raw Arabic text (256 max tokens) -> Character normalization -> Diacritics removal -> Stopword removal -> Non-Arabic filtering -> AraBERT/XLM-RoBERTa/ArabicBERT backbone -> 3 task-specific classifiers (propaganda binary, sentiment 3-class, emotion 5-class) -> Probability distributions

- **Critical path**: 1) Data split: 75% train / 8.5% dev / 16.5% test (preserves ArPro splits); 2) Preprocessing normalization (critical for Arabic orthographic variants); 3) Fine-tuning with AdamW optimizer, lr=2e-5, batch=8, epochs=5 (PLMs); 4) Multi-seed averaging (6 runs for PLMs, 3 for LLM) for stable evaluation

- **Design tradeoffs**: Single-task vs. multi-task learning (paper evaluates each task independently; joint training may improve propaganda via emotion signals but increases complexity); LLM vs. PLM (GPT-4o-mini achieves best sentiment/emotion F1 but requires API access; AraBERT offers competitive propaganda detection with local deployment); Preprocessing aggressiveness (stopword removal may discard sentiment-bearing function words; paper applies it but doesn't ablate)

- **Failure signatures**: XLM-RoBERTa consistently underperforms → likely underfitting Arabic morphology; Emotion detection lowest across all models → class imbalance (fear=4.4%) and linguistic complexity; Low IAA on specific emotion pairs (anger/disgust merged; fear/sadness confusion expected)

- **First 3 experiments**: 1) Replicate baseline: Fine-tune AraBERT on propaganda task only, verify Micro-F1≈0.769 on test split; 2) Multi-task ablation: Train single model with shared backbone + 3 task heads; compare per-task F1 vs. single-task baselines; 3) Class balance analysis: Compute per-class precision/recall for emotion task; identify if fear/sadness are systematically misclassified

## Open Questions the Paper Calls Out

- **Open Question 1**: Does span-level annotation of sentiment and emotion provide deeper insights or improve detection performance compared to the current document-level approach?
  - Basis in paper: [explicit] The Conclusion states that for future work, "annotation based on span-level analysis... may provide deeper insights and boost the performance of detection models."
  - Why unresolved: The current MultiProSE dataset is annotated at the document (paragraph) level, so the granular impact of specific phrases or sentence structures remains unmeasured.
  - What evidence would resolve it: A new version of the dataset tagged at the span level and a comparative evaluation of models trained on these granular labels versus the current document-level baselines.

- **Open Question 2**: Can constructing a specific lexicon for propaganda, sentiment, and emotion significantly enhance the accuracy of detection models?
  - Basis in paper: [explicit] The Conclusion suggests that "building a lexicon may provide deeper insights and boost the performance of detection models."
  - Why unresolved: The authors established baselines using fine-tuned LLMs and PLMs without incorporating a task-specific lexicon, leaving the additive value of such a resource unknown.
  - What evidence would resolve it: Experiments demonstrating improved Micro-F1 scores when a lexicon-based feature set is integrated into the classification pipeline.

- **Open Question 3**: To what extent does the presence of dialectal Arabic within MSA texts degrade the performance of current transformer-based detection models?
  - Basis in paper: [inferred] The Limitations section notes that annotators struggled with "obscure dialectal words" and ambiguity, yet the impact of this linguistic noise on model performance (e.g., XLM-RoBERTa's lower scores) was not isolated.
  - Why unresolved: The paper reports overall performance but does not provide an error analysis specifically correlating dialectal content or lack of diacritics with misclassification rates.
  - What evidence would resolve it: A targeted evaluation of model accuracy on a subset of texts containing dialectal segments compared to a normalized, purely MSA subset.

## Limitations
- Dataset accessibility is limited due to placeholder GitHub URL (https://github.com/xxx/xxx)
- GPT-4o-mini training protocol lacks details on prompt format and API configuration
- Annotation guidelines for distinguishing nuanced emotions (e.g., anger vs. disgust merged) are unclear
- Class imbalance (63% propaganda, 4.41% fear) may skew Micro-F1 scores without per-class metrics

## Confidence
- **High Confidence**: Arabic-specific PLMs (AraBERT) superiority over multilingual models is well-supported by consistent performance gaps and established NLP literature
- **Medium Confidence**: Quality control mechanism (gold data injection + annotator trust thresholds) is plausible but relies on unverified assumptions about expert annotator reliability
- **Low Confidence**: Multi-label annotation enabling cross-task signal transfer is weakly supported without empirical demonstration of joint learning benefits

## Next Checks
1. **Replication with Available Data**: Fine-tune AraBERTv2 on ArPro (propaganda-only) using specified hyperparameters. Verify Micro-F1 ≈ 0.769 on the original test split to establish baseline performance before extending to sentiment/emotion tasks.

2. **Class Balance Analysis**: Compute per-class precision/recall for emotion detection. If fear/sadness classes show systematic misclassification, investigate whether emotion categories need consolidation or if data augmentation is required.

3. **Multi-Task Learning Ablation**: Train a single model with shared AraBERT backbone and three task-specific heads. Compare per-task F1 scores against single-task baselines to determine if joint training provides transfer benefits, particularly for propaganda detection using emotion signals.