---
ver: rpa2
title: Transformers trained on proteins can learn to attend to Euclidean distance
arxiv_id: '2502.01533'
source_url: https://arxiv.org/abs/2502.01533
tags:
- distance
- coordinates
- attention
- structure
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformers can learn to attend to Euclidean distance between
  points in 3D space by using linear embeddings of coordinates, which can approximate
  Gaussian attention filters. This ability allows standard Transformers to function
  as structure models, improving performance on protein tasks like masked token prediction
  and function prediction.
---

# Transformers trained on proteins can learn to attend to Euclidean distance

## Quick Facts
- arXiv ID: 2502.01533
- Source URL: https://arxiv.org/abs/2502.01533
- Reference count: 40
- Primary result: Transformers can learn to attend to Euclidean distance between points in 3D space using linear embeddings of coordinates

## Executive Summary
This paper demonstrates that standard Transformers can learn to attend to Euclidean distances in 3D space through linear embeddings of coordinates, without requiring specialized structural modules. The approach approximates Gaussian attention filters using linear coordinate embeddings, enabling Transformers to function as effective structure models for protein tasks. Models incorporating coordinates outperform those without, showing improved accuracy on masked token prediction and function prediction tasks, particularly for amino acids with distinct conformational preferences.

## Method Summary
The authors introduce a method for incorporating 3D spatial information into Transformers by using linear embeddings of atomic coordinates. These embeddings are designed to approximate Gaussian attention filters, allowing the model to learn distance-based attention patterns. The approach is tested on protein structure tasks, including masked token prediction and function prediction, and compared against both standard Transformers without coordinates and more complex equivariant Transformers.

## Key Results
- Models with coordinate embeddings outperform those without coordinates on protein structure tasks
- The approach achieves better accuracy and robustness, especially for amino acids with distinct conformational preferences
- Linear coordinate embeddings provide a simpler alternative to custom structural modules while maintaining performance

## Why This Works (Mechanism)
The mechanism works by using linear embeddings of 3D coordinates to approximate Gaussian attention filters. When coordinates are embedded linearly and combined with standard attention mechanisms, the resulting attention scores can capture distance-based relationships between points in space. This allows the Transformer to learn to attend more strongly to nearby points and less to distant ones, mimicking the behavior of distance-aware attention mechanisms.

## Foundational Learning
- **3D coordinate embeddings**: Why needed - to represent spatial relationships; Quick check - verify embeddings capture relative positions
- **Gaussian attention approximation**: Why needed - to enable distance-based reasoning; Quick check - test if attention scores decay with distance
- **Linear vs. learned embeddings**: Why needed - to balance efficiency and expressiveness; Quick check - compare performance with different embedding approaches
- **Distance-aware attention**: Why needed - for effective 3D structure modeling; Quick check - validate attention patterns on known structural relationships

## Architecture Onboarding
- **Component map**: Input tokens -> Linear coordinate embeddings -> Standard Transformer layers -> Output predictions
- **Critical path**: Token embedding + coordinate embedding -> Multi-head attention -> Feed-forward network -> Output
- **Design tradeoffs**: Linear embeddings (efficient, less expressive) vs. learned embeddings (more expressive, computationally expensive)
- **Failure signatures**: Poor performance on structurally diverse proteins, inability to capture long-range dependencies
- **First experiments**: 1) Test coordinate embedding performance on simple distance-based tasks; 2) Compare linear vs. learned embeddings on small protein datasets; 3) Validate attention patterns capture known structural motifs

## Open Questions the Paper Calls Out
None

## Limitations
- Results are limited to protein structures, with unclear generalizability to other 3D molecular structures
- Linear embeddings may not capture complex distance relationships as accurately as learned representations
- Computational efficiency claims lack comprehensive benchmarking across hardware configurations

## Confidence
- **High confidence**: Core claim that Transformers can learn distance-based attention through linear coordinate embeddings is well-supported by experimental results
- **Medium confidence**: Claims about computational efficiency and generalization to other 3D domains are plausible but lack comprehensive validation
- **Low confidence**: Assertions about memory efficiency without qualification are premature given limited hardware testing

## Next Checks
1. Apply the coordinate embedding approach to non-protein 3D structures (e.g., small molecules, materials science datasets) to verify cross-domain generalization
2. Conduct comprehensive runtime and memory profiling comparing the proposed method against equivariant Transformers and standard Transformers across multiple hardware platforms
3. Systematically compare linear embeddings against higher-order polynomial embeddings and learned distance functions to quantify the trade-off between parameter efficiency and distance approximation accuracy