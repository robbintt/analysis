---
ver: rpa2
title: 'Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs'
arxiv_id: '2509.25779'
source_url: https://arxiv.org/abs/2509.25779
tags:
- city
- transportation
- plan
- accommodation
- attraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Planner-R1 fine-tuned large language models for long-horizon travel\
  \ planning using agentic reinforcement learning. By leveraging dense, process-level\
  \ rewards, an 8B model achieved a 39.9% final pass rate with 3.5\xD7 higher compute\
  \ efficiency and 1.5\xD7 better memory efficiency than a 32B model, while still\
  \ matching or exceeding its planning performance."
---

# Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs

## Quick Facts
- **arXiv ID**: 2509.25779
- **Source URL**: https://arxiv.org/abs/2509.25779
- **Reference count**: 40
- **One-line result**: Dense reward shaping enables 8B LLMs to achieve 39.9% final pass rate with 3.5× higher compute efficiency than 32B models on TravelPlanner

## Executive Summary
Planner-R1 fine-tunes large language models for long-horizon travel planning using agentic reinforcement learning. By leveraging dense, process-level rewards, an 8B model achieved a 39.9% final pass rate with 3.5× higher compute efficiency and 1.5× better memory efficiency than a 32B model, while still matching or exceeding its planning performance. A 32B Planner-R1 reached 56.9% final pass rate, outperforming strong baselines. Reward shaping proved decisive: smaller models relied heavily on dense feedback to succeed, while larger models were more robust to sparse rewards but showed smaller gains. Training on 180 queries did not harm generalization—Planner-R1 models performed on par with or better than baselines on unseen tasks like MULTI-IF, NATURALPLAN, and τ-BENCH. This work establishes reward shaping as a key lever for efficient agentic RL and highlights the competitive strength of smaller models for planning tasks.

## Method Summary
Planner-R1 trains LLMs as planning agents using Group Relative Policy Optimization (GRPO) on the TravelPlanner benchmark. The method employs multi-stage reward shaping with six components (schema compliance, micro/macro commonsense constraints, micro/macro hard constraints, and final pass) weighted by λ parameters. Training uses 180 queries from TravelPlanner with 8 trajectory rollouts per query, 30 assistant turns per episode, and a 7-tool sandbox environment. The 8B Qwen3 and 32B models are fine-tuned for up to 3,000 steps with staged reward density transitions. GRPO with trajectory-level advantage normalization provides stability for long-horizon credit assignment. The framework includes a JSON-gated output structure and a multi-stage awake memory manager for efficient training.

## Key Results
- 8B Planner-R1 with dense rewards achieved 39.9% final pass rate vs. 0.0% with sparse rewards
- 8B model was 3.5× more compute-efficient and 1.5× more memory-efficient than 32B model at matched performance
- 32B Planner-R1 reached 56.9% final pass rate, outperforming strong baselines
- Generalization held: Planner-R1 models performed on par or better than baselines on MULTI-IF, NATURALPLAN, and τ-BENCH
- Larger models were robust to sparse rewards but showed smaller gains from shaping and higher variance

## Why This Works (Mechanism)

### Mechanism 1: Dense Process-Level Reward Decomposition
Breaking sparse terminal rewards into intermediate process-level signals enables smaller LLMs (8B) to learn complex multi-step planning. The six-shaped components (schema, micro/macro commonsense, micro/macro hard constraints, final pass) provide partial credit gradients early in training. Dense rewards (Stage 1) guide smaller models through long tool-use trajectories (30 steps, 30,500 tokens). This is proper reward shaping—auxiliary terms provide intermediate guidance while preserving the same optimal policy. Evidence: 8B with Stage 1 achieved 39.9% pass rate vs. 0.0% with sparse Stage 3.

### Mechanism 2: GRPO with Trajectory-Level Advantage Normalization
GRPO stabilizes learning in sparse-reward, multi-step tool-use settings compared to standard PPO. It samples G=8 trajectories per query, computes returns, and normalizes advantages as (r - mean(r)) / std(r) across trajectories. This trajectory-level normalization reduces variance when rewards are sparse and delayed. The clipped PPO objective (ρ clipped to [1-ε, 1+ε]) prevents large policy updates that could destabilize long-horizon credit assignment. Evidence: 95% confidence intervals show relatively stable performance across 5 runs for Stage 1.

### Mechanism 3: Structured JSON-Gated Output Constraining Policy Degeneracy
Schema-valid JSON output acts as a hard constraint on the policy space, reducing the effective action space and providing a strong learning signal. The r_schema reward component gates all other rewards: if output is not schema-valid, the entire episode returns 0. This creates a curriculum effect where the model must first learn format compliance before learning constraint satisfaction. Evidence: Schema compliance reaches 100% quickly while constraint satisfaction lags, confirming the difficulty ordering.

## Foundational Learning

- **Concept: Reward Shaping and Potential-Based Rewards**
  - **Why needed**: Understanding why shaped rewards preserve optimal policies is critical to avoid biased rewards that cause reward hacking.
  - **Quick check**: Given a terminal reward r_T, how would you construct a shaped reward r'_t = r_t + F(s_t) - F(s_{t-1}) that guarantees the same optimal policy? What would F represent in TravelPlanner?

- **Concept: Policy Gradient Variance Reduction in Long-Horizon MDPs**
  - **Why needed**: Understanding why trajectory-level normalization (GRPO) vs. token-level advantages matters for variance reduction is critical for debugging training instability.
  - **Quick check**: Why might normalizing advantages across trajectories (G samples per query) provide lower-variance estimates than normalizing across tokens in a single trajectory when rewards are sparse and delayed?

- **Concept: Tool-Augmented LLM Agents as MDPs**
  - **Why needed**: The paper formulates tool use as an MDP where states include full conversation history, actions are tokens (including tool calls), and transitions append tool outputs. This framing differs from standard LLM fine-tuning.
  - **Quick check**: In the TravelPlanner MDP formulation, why is the transition function defined as s_{t+1} = (s_t, a_t, o_{t+1}) rather than s_{t+1} = (s_t, a_t)? What role do tool responses (o_{t+1}) play in state construction?

## Architecture Onboarding

- **Component map**: VERL Framework -> GRPO Optimizer -> Multi-Stage Reward Function -> ReAct-Style Agent Loop -> Multi-Stage Awake Memory Manager -> Sandbox Environment

- **Critical path**: Initialize with Qwen3-8B/32B base model → Load 180 training queries → Sample G=8 trajectories per batch query using SGLang rollouts → Compute 6-component rewards via sandbox evaluator → Normalize advantages across G trajectories, apply GRPO loss → Update policy via FSDP, periodically checkpoint → Evaluate on test split (1,000 queries) via leaderboard submission

- **Design tradeoffs**:
  - 8B vs. 32B: 8B is 3.5× more compute-efficient and 1.5× more memory-efficient but requires dense rewards; 32B is robust to sparse rewards but higher variance and cost. Choose 8B + dense rewards when compute is constrained; 32B + sparse/curriculum when peak accuracy matters.
  - Stage 1 vs. Curriculum: Curriculum learning provided no significant benefit in experiments. Stage 1 (dense) alone achieved best 8B results. Skip curriculum unless you observe overfitting to intermediate rewards.
  - Thinking mode disabled: The paper disables Qwen3's explicit thinking mode because reasoning tokens inflated context length without metric gains. Enable only if your task shows clear benefit from intermediate reasoning.

- **Failure signatures**:
  - Model collapse under sparse rewards: 8B models trained with Stage 3 (sparse only) achieved 0.0% pass rate. Symptom: policy entropy drops, outputs become deterministic but wrong. Remedy: switch to Stage 1 or add dense auxiliary rewards.
  - Hallucination persistence: Model generates plausible-looking but non-existent entities. Symptom: top-5 failure for 8B even after 2,000 steps. Remedy: add negative rewards for entities not in database, or increase sandbox verification strictness.
  - OOM during weight transfer: Symptom: crash during training → inference weight copy. Remedy: enable Multi-Stage Awake to reduce peak memory by 20-23%.
  - High variance in 32B: 32B showed larger confidence intervals and performance spikes/dips during training. Symptom: run-to-run inconsistency. Remedy: average across multiple seeds, consider reducing learning rate or increasing batch size.

- **First 3 experiments**:
  1. Replicate Stage 1 vs. Stage 3 ablation: Train Planner-R1-8B with dense (λ=[1,1,1,1,1]) vs. sparse (λ=[0,0,0,0,1]) rewards for 500 steps on 180-query training set. Expect ~40% vs. ~0% final pass rate.
  2. Probe compute efficiency frontier: Train 8B for 3,000 steps and 32B for 2,000 steps, tracking FLOPs vs. final pass rate. Plot as in Figure 3 (right panel). Expect 8B to reach 90% of 32B peak at ~3.5× fewer FLOPs.
  3. Test out-of-domain transfer: Evaluate checkpoints from Experiment 2 on NaturalPlan, Multi-IF, and τ-Bench. Confirm no catastrophic forgetting (>10% regression) on majority of metrics.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the task complexity threshold beyond which 8B models cease to remain competitive with 32B models even under dense reward shaping? The study evaluated only TravelPlanner and did not test more complex or open-ended planning domains.

- **Open Question 2**: Can richer curriculum learning strategies (e.g., adaptive scheduling, task-specific curricula) improve sample efficiency over the simple staged curriculum that showed no significant benefit? The paper's curriculum was fixed and predefined, not adaptive to model performance.

- **Open Question 3**: What causes the higher variance in 32B models across runs, and can it be mitigated without sacrificing their robustness to sparse rewards? The paper documents the variance but does not isolate whether it stems from initialization, optimization dynamics, or exploration behavior.

## Limitations

- **Reward shaping design space**: The specific six-component decomposition and λ weights appear somewhat ad hoc, and the paper doesn't systematically explore the full reward shaping design space.

- **Generalization scope**: While good transfer to three held-out benchmarks is shown, these represent relatively narrow task variations, and the training set is small (180 queries).

- **Model size scalability**: The paper focuses on 8B vs. 32B comparisons but doesn't explore the full spectrum of model sizes or identify the optimal size where efficiency peaks.

## Confidence

- **High confidence**: The core finding that dense reward shaping enables smaller LLMs to achieve competitive performance with significantly better efficiency (well-supported by the 39.9% vs. 0.0% ablation).
- **Medium confidence**: The efficiency claims (3.5× compute, 1.5× memory for 8B vs. 32B) and the claim that larger models are more robust to sparse rewards (supported by experiments but depend on specific implementation choices).
- **Medium confidence**: The generalization results to held-out benchmarks (evaluation protocol and benchmark selection are limited).
- **Low confidence**: The claim that curriculum learning provides no benefit (tested only one curriculum schedule).

## Next Checks

1. **Systematic reward shaping ablation**: Design a factorial experiment varying the number of reward components (1 to 6) and their λ weights across [0,1] range. Train 8B models under each configuration for 500 steps and measure final pass rate, learning speed, and variance to identify which shaped rewards are most critical.

2. **Cross-domain generalization test**: Evaluate Planner-R1 on at least two completely different planning or reasoning tasks outside the travel domain (e.g., recipe planning, project management, or scientific experiment design) using the same 8B model and dense reward setup to test domain transfer without fine-tuning.

3. **GRPO hyperparameter sensitivity**: Run ablation studies varying G (4, 8, 16, 32 trajectories per query) and the advantage normalization method (trajectory-level vs. token-level vs. mixed). Measure training stability (variance across seeds), convergence speed, and final performance on TravelPlanner to quantify how critical the specific GRPO implementation is.