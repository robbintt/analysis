---
ver: rpa2
title: Synthetic Document Question Answering in Hungarian
arxiv_id: '2505.23008'
source_url: https://arxiv.org/abs/2505.23008
tags:
- hudocvqa
- hungarian
- zhang
- dataset
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents HuDocVQA, a synthetic dataset for document visual
  question answering (VQA) in Hungarian, addressing the lack of high-quality multilingual
  VQA data. The dataset is created by leveraging multilingual LLMs to generate question-answer
  pairs from Hungarian PDFs extracted from Common Crawl, followed by multiple rounds
  of quality filtering to match human-level annotation quality.
---

# Synthetic Document Question Answering in Hungarian

## Quick Facts
- arXiv ID: 2505.23008
- Source URL: https://arxiv.org/abs/2505.23008
- Reference count: 40
- Primary result: HuDocVQA dataset shows Llama 3.2 11B accuracy improves from 33.2% to 40.4% (+7.2%) when finetuned on synthetic Hungarian VQA data and OCR pretraining

## Executive Summary
This work addresses the lack of high-quality multilingual document visual question answering (VQA) data by introducing HuDocVQA, a synthetic dataset for Hungarian document VQA. The dataset is created using multilingual LLMs to generate question-answer pairs from Hungarian PDFs, followed by extensive quality filtering to match human annotation quality. The paper also introduces HuCCPDF, a dataset of 117k transcribed Hungarian PDF pages for OCR training. Experiments demonstrate that state-of-the-art VLMs significantly underperform on HuDocVQA compared to English DocVQA, but finetuning on the synthetic data improves accuracy by +7.2%. The authors release all datasets and code to support further research in multilingual document VQA.

## Method Summary
The methodology involves extracting text from Hungarian PDFs in Common Crawl using PyMuPDF and pytesseract, then generating synthetic QA pairs using Llama 3.3 70B with one-shot Hungarian prompts. The generated data undergoes four sequential quality filters: text length >60 chars, n-gram overlap ≥12%, language detection, and paraphrase deduplication. For training, Llama 3.2 11B Instruct is finetuned using a mixture of Cauldron, DocMatix, LAION-COCO-NLLB (Hungarian captions), HuDocVQA, and HuCCPDF datasets with model merging of the final three checkpoints. Evaluation uses LLM-as-a-Judge (Llama 3.1 405B Instruct) rather than traditional ANLS metrics.

## Key Results
- Llama 3.2 11B Instruct achieves 33.2% accuracy on HuDocVQA baseline vs 95.4% on English DocVQA
- Finetuning on HuDocVQA + HuCCPDF improves accuracy to 40.4% (+7.2% absolute)
- VQA-only finetuning degrades performance (-4.7%), but adding OCR pretraining reverses this
- Filtered synthetic data achieves 0.426 accuracy vs 0.407 for unfiltered (+1.9% relative improvement)

## Why This Works (Mechanism)

### Mechanism 1
Multilingual LLMs can generate document QA pairs that approximate human annotation quality when properly prompted. One-shot prompting with human-written Hungarian QA examples guides Llama 3.3 70B to generate syntactically correct, semantically relevant questions from extracted document text. The core assumption is that LLM's multilingual capabilities transfer to Hungarian document understanding sufficiently to produce training-grade QA pairs. Evidence shows synthetic pipeline achieves comparable quality as human annotation (0.986 Pearson correlation, p-value 0.01), though weak direct validation exists.

### Mechanism 2
Heuristic quality filters reduce synthetic data noise without discarding valid examples. Four sequential filters (text length >60 chars, n-gram overlap ≥12%, language detection, paraphrase deduplication) remove 31,911 QA pairs while preserving task-relevant diversity. The core assumption is that filters correlate with actual QA quality, not just superficial patterns. Evidence shows filtered data achieves 0.426 accuracy vs 0.407 for unfiltered (+1.9% relative improvement), though no direct external validation of these specific filter thresholds exists.

### Mechanism 3
OCR pretraining on in-language document images improves VQA accuracy more than VQA-only finetuning. Training on formatted OCR tasks (image → ground-truth text) teaches the VLM character recognition and layout understanding before applying QA reasoning on top. The core assumption is that OCR and VQA tasks share underlying visual-textual representations that transfer. Evidence shows adding 105k OCR examples improves accuracy from 0.303 to 0.404 (+7.2% over baseline), while VQA-only finetuning degraded performance (-4.7%).

## Foundational Learning

- **Document Visual Question Answering (DocVQA)**: Why needed - This is the core task answering questions about document images requiring OCR + reasoning. Quick check - Can you explain why DocVQA differs from standard VQA (natural images)?

- **Synthetic Data Generation**: Why needed - The paper's main contribution is a scalable alternative to manual annotation using LLMs. Quick check - What are two failure modes of synthetic QA generation that the paper's filters address?

- **Vision-Language Model (VLM) Finetuning**: Why needed - The evaluation methodology involves finetuning Llama 3.2 11B on mixed datasets. Quick check - Why might finetuning on a single task (HuDocVQA alone) degrade performance?

## Architecture Onboarding

- **Component map**: Common Crawl PDFs → PyMuPDF/pytesseract extraction → Llama 3.3 70B QA generation → 4-stage filtering → HuDocVQA → SFT mixture (Cauldron, DocMatix, LAION-COCO-NLLB, HuDocVQA, HuCCPDF) → model merging → evaluation with LLM-as-a-Judge

- **Critical path**: OCR data quality → synthetic QA relevance → filter thresholds → mixture composition → model merging. Errors propagate forward; OCR errors cannot be fixed downstream.

- **Design tradeoffs**: ANLS vs. LLM-as-a-Judge (ANLS fails on Hungarian morphology; LLM judge aligns better with human judgment but adds cost). Synthetic scale vs. quality (93k raw → 62k filtered; aggressive filtering reduces scale but improves per-example value). Single-task vs. mixture (mixture adds diversity but dilutes task-specific signal).

- **Failure signatures**: Accuracy drops despite ANLS increase → switch to LLM-as-a-Judge metric. Finetuning degrades baseline → insufficient data variety; add diverse SFT mixture. High filter rejection rate (>30%) → check language detection threshold or text extraction quality.

- **First 3 experiments**: 1) Replicate filtering ablation: Compare filtered vs. unfiltered HuDocVQA on a small VLM to validate +1.9% gain claim. 2) OCR scaling test: Train with 21k vs. 105k OCR examples to confirm dose-response relationship. 3) Language transfer probe: Apply the same pipeline to another medium-resource language (e.g., Czech) to test generalization.

## Open Questions the Paper Calls Out

### Open Question 1
Does the synthetic data generation pipeline generalize effectively to languages with significantly less web presence than Hungarian (~17th highest resource)? The authors only demonstrate the pipeline for Hungarian, a relatively high-resource language with substantial Common Crawl data (0.2% of filtered PDFs). Lower-resource languages may lack sufficient PDF data or LLM capability in the target language for quality QA generation.

### Open Question 2
What is the minimum effective scale and diversity of synthetic VQA data needed to improve model performance without degradation? While adding OCR data and diverse SFT mixtures improved performance, the paper does not systematically vary dataset scale or composition to identify thresholds or optimal proportions.

### Open Question 3
To what extent does the ~30% accuracy gap between English DocVQA and HuDocVQA stem from model training data imbalance versus intrinsic linguistic or document characteristics? The paper demonstrates the gap exists across multiple models but does not disentangle whether the gap arises from limited Hungarian pre-training data, differences in Hungarian document structure, OCR quality issues, or other factors.

## Limitations
- Heavy reliance on synthetic data without independent human validation of final HuDocVQA quality beyond the small 54-page manual dataset
- Language transfer assumption for Hungarian remains weakly validated; filter thresholds may not be optimal for Hungarian morphology
- Finetuning methodology shows single-task training degrades performance but optimal mixture composition and model merging strategy are not thoroughly ablated

## Confidence
- **High Confidence**: The performance gap between English and Hungarian DocVQA is real and significant (95.4% vs 33.2% accuracy). The general methodology of synthetic data generation + filtering + mixture finetuning is sound based on established literature.
- **Medium Confidence**: The specific filter thresholds and their claimed correlation with human quality. The optimal mixture composition for finetuning. The assertion that OCR pretraining is superior to other forms of domain adaptation.
- **Low Confidence**: The long-term generalizability of synthetic Hungarian QA pairs to real-world documents. The robustness of LLM-as-a-Judge across different document types and question complexities. The claim that this approach scales to other low-resource languages without modification.

## Next Checks
1. **Independent Human Validation**: Recruit 3-5 native Hungarian speakers to independently evaluate 200 randomly sampled HuDocVQA examples against the synthetic generation pipeline, measuring inter-annotator agreement and identifying systematic failure modes missed by automated filters.

2. **Cross-Lingual Transfer Experiment**: Apply the identical synthetic generation pipeline to Czech (similar linguistic family, comparable resource availability) and compare resulting VQA performance to establish whether the methodology generalizes beyond Hungarian-specific optimizations.

3. **Ablation of Filter Thresholds**: Systematically vary each filter parameter (text length, n-gram overlap, language detection confidence, deduplication threshold) and measure the resulting trade-off between dataset size and downstream VQA accuracy to identify the true Pareto frontier.