---
ver: rpa2
title: 'Q-Probe: Scaling Image Quality Assessment to High Resolution via Context-Aware
  Agentic Probing'
arxiv_id: '2601.15356'
source_url: https://arxiv.org/abs/2601.15356
tags:
- image
- quality
- assessment
- tool
- q-probe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Q-Probe is the first agentic IQA model that scales to high-resolution
  via context-aware probing, addressing the inability of existing RL-based methods
  to capture subtle local degradations. It employs a three-stage training curriculum:
  RL pre-training for global perception alignment, hybrid-resolution SFT for context-aware
  tool usage, and decoupled RL fine-tuning for precise defect localization.'
---

# Q-Probe: Scaling Image Quality Assessment to High Resolution via Context-Aware Agentic Probing

## Quick Facts
- arXiv ID: 2601.15356
- Source URL: https://arxiv.org/abs/2601.15356
- Reference count: 12
- Q-Probe achieves state-of-the-art SRCC of 0.728 on Vista-Bench and 0.892 on SPAQ

## Executive Summary
Q-Probe introduces the first agentic IQA model capable of high-resolution local degradation analysis through context-aware probing. It addresses the fundamental inability of existing RL-based methods to capture subtle local degradations by employing a three-stage training curriculum that progressively aligns the model with human preferences. The approach introduces Vista-Bench, a pioneering benchmark for high-resolution local degradation analysis, and uses context-aware cropping to eliminate spurious biases. Experimental results demonstrate superior performance across both high-resolution and low-resolution datasets, effectively balancing global aesthetic perception with fine-grained local scrutiny.

## Method Summary
Q-Probe employs a three-stage curriculum: RL pre-training for global perception alignment using pairwise ranking rewards, hybrid-resolution SFT for context-aware tool usage with mixed crop coverage, and decoupled RL fine-tuning for precise defect localization using separate accuracy and localization rewards. The model uses Qwen-2.5-VL-7B as base, with context-aware cropping (768×768) covering both degraded and pristine regions to avoid spurious "cropping-implies-degradation" bias. Vista-Bench is constructed using wavelet transform to extract high-frequency components, selectively injecting degradations into texture-rich semantic regions, with hierarchical scoring via Gemini-2.5 Pro.

## Key Results
- Achieves state-of-the-art SRCC of 0.728 on Vista-Bench for high-resolution local degradation analysis
- Achieves SRCC of 0.892 on SPAQ, demonstrating strong performance on low-resolution datasets
- Three-stage training curriculum shows 5% improvement over single-stage training on Vista-Bench
- Context-aware cropping with mixed coverage achieves 41.8% improvement over degradation-only strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-aware cropping breaks the spurious "crop-implies-degradation" causal bias
- Mechanism: Training trajectories include crops of both degraded regions AND pristine/natural-DoF regions, forcing the model to learn that cropping ≠ low quality. The model must evaluate crop content, not just associate tool usage with defects.
- Core assumption: The bias stems from training distribution imbalance (all crops contain degradations), not from architectural limitations.
- Evidence anchors:
  - [abstract]: "direct adaptation to IQA induces spurious 'cropping-implies-degradation' biases"
  - [Section 3.4]: "This strategy effectively severs the spurious causal correlation associating 'cropping' with 'low quality'"
  - [Table 4]: Degradation-only cropping achieves SRCC 0.510 vs. 0.728 with mixed coverage—direct ablation evidence
- Break condition: If models trained on mixed crops still show systematic low-score bias on pristine regions, the mechanism is insufficient.

### Mechanism 2
- Claim: Three-stage curriculum resolves the precision-diversity tradeoff in agentic tool learning
- Mechanism: Stage 1 (low-res RL) establishes global perception baseline → Stage 2 (SFT) teaches tool invocation logic without localization pressure → Stage 3 (decoupled RL) refines precision. Each stage builds on stabilized prior capability.
- Core assumption: Tool-use logic and localization precision are separable skills that interfere when trained jointly.
- Evidence anchors:
  - [abstract]: "three-stage training paradigm that progressively aligns the model with human preferences"
  - [Table 2]: Stage 2+3 alone achieves 0.679 Vista SRCC; full pipeline achieves 0.728—Stage 1 contributes ~7% gain
  - [corpus]: Refine-IQA (arXiv:2508.03763) independently validates multi-stage RFT for IQA, supporting curriculum necessity
- Break condition: If single-stage training matches or exceeds three-stage performance, curriculum decomposition is unnecessary complexity.

### Mechanism 3
- Claim: Decoupled rewards separate "looking policy" (where to crop) from "scoring policy" (quality judgment)
- Mechanism: R_loc optimizes crop bbox IoU with ground-truth defects; R_acc optimizes score prediction accuracy. Independent gradients prevent score optimization from degrading localization.
- Core assumption: Scoring and localization have conflicting gradient directions that joint optimization cannot resolve.
- Evidence anchors:
  - [Section 3.5]: "decoupled reward mechanism that disentangles the optimization of the 'Looking Policy' from the 'Scoring Policy'"
  - [Table 3]: R_acc alone: 0.698 SRCC; adding R_loc: 0.728 SRCC—4.3% improvement from localization reward
  - [corpus]: SA-IQA (arXiv:2512.05098) uses multi-dimensional rewards for IQA, providing convergent evidence for reward decomposition
- Break condition: If joint reward (R_acc + R_loc as single scalar) achieves equivalent performance, decoupling is not mechanistically necessary.

## Foundational Learning

- Concept: **Thurstone probabilistic ranking model**
  - Why needed here: Stage 1 uses pairwise comparison with uncertainty-aware probability rather than deterministic scores. Understanding Eq. (1-2) is required to modify ranking rewards.
  - Quick check question: Why does the model compute P(x_i ≻ x_j) via Gaussian CDF instead of directly comparing mean scores?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Core RL algorithm for Stages 1 and 3. Requires understanding advantage computation, clipping, and KL penalty tradeoffs.
  - Quick check question: What does the clipping threshold ε=0.2 constrain in Eq. (4)?

- Concept: **Wavelet-based artifact injection**
  - Why needed here: Vista-Bench construction decouples structure from texture before injecting degradations. Critical for extending benchmark to new distortion types.
  - Quick check question: Why inject degradations into texture-rich regions rather than uniformly?

## Architecture Onboarding

- Component map:
Qwen-2.5-VL-7B (base) -> Stage 1: GRPO (K=6 samples, pairwise ranking) -> Stage 2: SFT on Probe-CoT-3K (768×768 crops) -> Stage 3: Decoupled RL (α=1.0 accuracy, β=0.15 localization) -> R_acc: exponential score accuracy, R_loc: IoU for defect bbox, R_format: output structure compliance

- Critical path: Stage 1 → Stage 2 → Stage 3 is sequential dependency. Skipping Stage 1 drops Vista SRCC from 0.728 to 0.679 (Table 2). Stage 2 SFT data quality (crop coverage strategy) directly determines Stage 3 ceiling.

- Design tradeoffs:
  - Crop coverage: All-degradation + partial-normal (0.728) vs. partial-degradation (0.695) vs. degradation-only (0.510)—more context improves generalization but dilutes training signal
  - β (localization weight): 0.15 chosen—too high may overfit to bbox precision at expense of scoring accuracy
  - SFT then RL: SFT stabilizes logic but reduces localization precision; RL recovers precision

- Failure signatures:
  - Model systematically assigns low scores to all cropped regions → "crop-implies-degradation" bias not eliminated; check Stage 2 crop diversity
  - High Vista SRCC but poor SPAQ/KADID → global perception insufficient; verify Stage 1 coverage
  - Accurate crop locations but poor scores → R_loc dominant over R_acc; reduce β

- First 3 experiments:
  1. **Ablate crop strategy**: Train Stage 2 with degradation-only crops, measure Vista SRCC. Expect ~0.51 if mechanism is correct.
  2. **Single-stage baseline**: Skip Stage 1, train only Stages 2+3 on mixed-resolution data. Expect ~0.68 Vista SRCC per Table 2.
  3. **Reward coupling test**: Combine R_acc + R_loc into single reward (α=1.15), compare to decoupled. Expect degradation if decoupling is necessary.

## Open Questions the Paper Calls Out

- Question: Does the wavelet-based synthetic degradation pipeline used in Vista-Bench fully capture the complexity of authentic, real-world high-resolution sensor artifacts?
  - Basis in paper: [Explicit] The authors state they "employ the wavelet transform... selectively injecting degradations... to simulate realistic impairments" because existing datasets lack local high-res degradations.
  - Why unresolved: While the method simulates realistic texture, synthetic artifacts may not replicate the complex physical characteristics of genuine optical aberrations or sensor noise found in native high-resolution captures.
  - Evidence: Evaluation of Q-Probe on a benchmark comprising strictly authentic, non-synthetic high-resolution photographs with human-annotated local distortions.

- Question: To what extent does the reliance on Gemini-2.5 Pro for generating ground-truth scores limit Q-Probe's ability to surpass the teacher model's performance ceiling?
  - Basis in paper: [Explicit] The paper notes they "employ Gemini-2.5 Pro to achieve hierarchical scores, calculating weighted assessments based on local degradation severity."
  - Why unresolved: Training on labels generated by a specific LLM (Gemini) risks imparting that model's specific biases or hallucinations as ground truth, potentially capping the student model's alignment with actual human perception.
  - Evidence: A comparative ablation study where Q-Probe is trained on a subset of data labeled purely by human experts versus the Gemini-generated labels to measure the performance gap.

- Question: How does the computational latency of the multi-turn "Thinking with Images" paradigm compare to single-pass IQA methods in real-time applications?
  - Basis in paper: [Inferred] The methodology describes a sequential process ("Global Perception → Local Scrutiny → Critical Thinking") involving multiple tool invocations and crop analysis, which inherently requires more processing time than single-pass models.
  - Why unresolved: The paper optimizes for correlation accuracy (SRCC/PLCC) but does not report inference speed or token efficiency, which are critical constraints for deploying IQA in real-time video or streaming services.
  - Evidence: Benchmarking the average inference time and FLOPs per image against standard single-pass baselines like Q-Align across different hardware configurations.

## Limitations

- The three-stage curriculum's necessity and specific hyperparameters lack full specification, creating reproducibility gaps
- Reliance on synthetic Vista-Bench data may not fully capture authentic high-resolution sensor artifacts
- Computational latency of multi-turn "Thinking with Images" paradigm is not evaluated for real-time applications

## Confidence

- **High**: Q-Probe achieves state-of-the-art SRCC of 0.728 on Vista-Bench and 0.892 on SPAQ
- **Medium**: Three-stage curriculum resolves precision-diversity tradeoff in agentic tool learning
- **Medium**: Context-aware cropping eliminates spurious "crop-implies-degradation" bias
- **Medium**: Decoupled rewards separate "looking policy" from "scoring policy"

## Next Checks

1. **Reward coupling test**: Train with joint reward (α=1.15) instead of decoupled rewards to test if single-reward optimization achieves equivalent performance.

2. **Bias generalization test**: Evaluate models trained on mixed crops against degradation-only crops on pristine regions to confirm bias elimination generalizes beyond training distribution.

3. **Stage 1 necessity test**: Train a single-stage model with all tools available from the start to determine if curriculum decomposition is mechanistically necessary or merely helpful.