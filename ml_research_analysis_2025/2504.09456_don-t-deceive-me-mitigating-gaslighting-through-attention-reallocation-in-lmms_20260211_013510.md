---
ver: rpa2
title: 'Don''t Deceive Me: Mitigating Gaslighting through Attention Reallocation in
  LMMs'
arxiv_id: '2504.09456'
source_url: https://arxiv.org/abs/2504.09456
tags:
- attention
- gaslighting
- tokens
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of gaslighting in large multimodal
  models (LMMs), where misleading or contradictory user inputs can cause models to
  override their correct initial answers. The authors propose GasEraser, a training-free
  method that reallocates attention weights from misleading textual tokens to semantically
  relevant visual regions, thereby mitigating the negative impact of negation-based
  gaslighting.
---

# Don't Deceive Me: Mitigating Gaslighting through Attention Reallocation in LMMs

## Quick Facts
- **arXiv ID:** 2504.09456
- **Source URL:** https://arxiv.org/abs/2504.09456
- **Authors:** Pengkun Jiao; Bin Zhu; Jingjing Chen; Chong-Wah Ngo; Yu-Gang Jiang
- **Reference count:** 40
- **Key result:** GasEraser reduces misguidance rate by 48.2% on LLaVA-v1.5-7B and improves accuracy by up to 18.57% across evaluated LMMs

## Executive Summary
This paper addresses gaslighting vulnerability in large multimodal models (LMMs), where misleading or contradictory user inputs can cause models to override correct initial answers. The authors propose GasEraser, a training-free method that reallocates attention weights from misleading textual tokens to semantically relevant visual regions, thereby mitigating the negative impact of negation-based gaslighting. Experimental results on GaslightingBench demonstrate that GasEraser significantly improves model robustness across several leading open-source LMMs without requiring retraining or additional supervision.

## Method Summary
GasEraser is a training-free inference-time method that mitigates gaslighting by identifying attention sinks—tokens receiving disproportionate attention despite low semantic relevance—and redistributing their influence to enhance visual grounding. The method operates by detecting high-norm tokens in specific embedding dimensions, selecting vision-centric attention heads based on their image relevance and sink-likelihood scores, and reallocating attention budget from text sink tokens to visual tokens. This approach preserves proper visual grounding while reducing susceptibility to negation-based gaslighting attacks.

## Key Results
- GasEraser reduces misguidance rate by 48.2% on LLaVA-v1.5-7B
- Accuracy after negation increases by up to 18.57% across evaluated models
- Vision-centric head selection is critical, with ablation showing 18.42% accuracy drop when removed
- Text sink tokens are the primary source of gaslighting attention, contributing 18.57% gain vs. 1.16% from image sink tokens

## Why This Works (Mechanism)

### Mechanism 1: Attention Sink Identification and Suppression
Gaslighting vulnerability correlates with attention sinks—tokens receiving disproportionate attention despite low semantic relevance. The method identifies these tokens by detecting abnormally high norms in specific embedding dimensions and scales their attention weights by a factor p (typically 0.6), reducing their influence. This suppression prevents misleading tokens from distorting attention distributions that lead to gaslighting susceptibility.

### Mechanism 2: Vision-Centric Head Selection
Not all attention heads contribute equally to visual grounding. The method computes image relevance and sink-likelihood scores for each head, selecting those that strongly attend to visual tokens while avoiding sink tokens. Amplifying these vision-centric heads strengthens visual grounding and provides a foundation for robust multimodal reasoning against gaslighting attacks.

### Mechanism 3: Attention Budget Reallocation from Text to Visual Tokens
Gaslighting attention primarily originates from text tokens exploiting negation semantics. The method reallocates the suppressed attention budget from text sink tokens to visual tokens proportionally to their existing vision-centric attention ratio. This redirection from misleading textual cues to semantically relevant visual regions preserves visual grounding while neutralizing gaslighting pathways.

## Foundational Learning

- **Concept: Self-Attention and Multi-Head Attention in Transformers**
  - Why needed: GasEraser operates directly on attention maps; understanding Q, K, V projections and multi-head attention is essential for grasping head selection and reallocation
  - Quick check: Given attention weights A ∈ R^{H×S×S}, what does A_{h,t,v} represent, and how would scaling row t affect subsequent token representations?

- **Concept: Attention Sinks in LLMs and LMMs**
  - Why needed: The paper's core insight links gaslighting vulnerability to attention sink behavior; understanding why early or low-information tokens absorb disproportionate attention explains the method's rationale
  - Quick check: Why might a model attend heavily to a token with minimal semantic content, and what failure modes could this cause in multimodal reasoning?

- **Concept: Vision-Language Token Alignment in LMMs**
  - Why needed: GasEraser reallocates attention between visual and textual tokens; understanding how vision encoders project images into token sequences and interface with LLM backbones is necessary for implementation
  - Quick check: In an LMM, how are visual tokens positioned relative to text tokens in the input sequence, and how does this affect cross-modal attention patterns?

## Architecture Onboarding

- **Component map:** Image I → Vision encoder V → Visual tokens t_v (576 tokens for CLIP-L-336px) → Concatenated sequence [t_v, t_t] → LLM backbone G → Attention layers with H heads per layer → GasEraser modification

- **Critical path:**
  1. Identify sink tokens via high-norm detection (Eq. 4–6)
  2. Compute image relevance δ and sink-likelihood ξ for each head (Eq. 7–8)
  3. Select vision-centric heads H_visual (Eq. 9)
  4. Scale attention to text sink tokens by factor p (Eq. 11)
  5. Compute reallocation budget Ω (Eq. 12)
  6. Redistribute Ω to visual tokens weighted by vision-centric ratio R^V (Eq. 13–14)

- **Design tradeoffs:**
  - Layer selection: Early layers (1–16) yield greatest gains; later layers show diminishing returns. Assumption: early layers handle low-level visual processing critical for grounding.
  - Threshold sensitivity (τ, ρ, α, p): Values are model-specific; LLaVA-v1.5-7B uses τ=20, ρ=0.6, α=0.005, p=0.6, while InternVL2-8B requires α=0.1. Improper tuning may over-suppress useful attention or fail to mitigate sinks.
  - Token source: Text sink tokens are primary targets; image sink tokens contribute minimally (Table 2). Focusing solely on text tokens simplifies implementation without significant performance loss.

- **Failure signatures:**
  - No improvement: Check if sink tokens are correctly identified (visualize norms); thresholds may need adjustment for different models
  - Degraded performance on non-gaslighting tasks: Over-aggressive suppression may harm general reasoning; verify ρ and α thresholds are not too restrictive
  - Inconsistent results across layers: Ensure injection is applied to the correct layer range; late-layer injection shows minimal effect (Figure 6)

- **First 3 experiments:**
  1. Reproduce baseline vulnerability: Run LLaVA-v1.5-7B on GaslightingBench without GasEraser; confirm accuracy drop from ~63% (before negation) to ~25% (after negation)
  2. Layer ablation: Apply GasEraser to different layer ranges (1–4, 1–8, 1–16, 1–32) and plot accuracy vs. layer depth; verify early-layer dominance
  3. Token source ablation: Compare performance when reallocating from image sink tokens only vs. text sink tokens only vs. both; confirm text tokens are the primary driver (should match Table 2)

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can dialectical reasoning mechanisms be effectively integrated into general LMM dialogue tasks to improve gaslighting resistance, beyond the simplified binary discrimination strategy?
- Basis: The authors state plans to investigate incorporating dialectical thinking into LMM inference and note that LMMs demonstrate greater robustness in distinguishing gaslighting statements under the Discrimination Strategy.
- Why unresolved: The discrimination strategy requires explicit prompts to validate statements, which doesn't reflect natural user interactions where end users seek direct answers rather than statement validation.
- What evidence would resolve it: Development and evaluation of a dialectical reasoning module that maintains robustness in open-ended dialogue without requiring explicit truth-validation prompts.

### Open Question 2
**Question:** Does GasEraser's attention reallocation approach generalize to non-negation forms of gaslighting and adversarial manipulation in LMMs?
- Basis: The paper focuses exclusively on negation-based gaslighting, leaving unexplored whether attention sinks facilitate other manipulation types such as affirmative false claims, subtle misdirection, or multi-turn persuasive deception.
- Why unresolved: The method identifies gaslighting attention patterns tied specifically to negation semantics; different manipulation strategies may exploit different attention mechanisms or token patterns.
- What evidence would resolve it: Evaluation of GasEraser on benchmarks containing diverse adversarial manipulation types beyond negation, measuring whether attention reallocation consistently improves robustness.

### Open Question 3
**Question:** Can the hyperparameters for attention sink identification and reallocation (τ, ρ, α, p) be adaptively determined per-instance rather than requiring manual tuning per model?
- Basis: The paper uses different fixed hyperparameters for each model (e.g., τ=20, ρ=0.6, α varying from 0.005 to 0.1) without exploring adaptive or learned threshold selection, which may limit scalability across diverse model architectures.
- Why unresolved: Manual hyperparameter tuning per model reduces the plug-and-play nature claimed for the method and may not transfer to architectures with different attention distributions.
- What evidence would resolve it: Demonstration of an adaptive thresholding mechanism that maintains or exceeds current performance without requiring model-specific hyperparameter optimization.

## Limitations

- **Model-specific parameter tuning**: GasEraser requires careful selection of thresholds (τ, ρ, α) and scaling factors (p) for each LMM architecture, creating barriers to generalization
- **Mechanistic opacity in sink token identification**: The paper identifies specific high-norm dimensions empirically but doesn't provide a principled method for discovering these dimensions in new models
- **Evaluation scope constraints**: GaslightingBench focuses on negation-based attacks in two-round conversation settings with multiple-choice questions, leaving untested the method's effectiveness against more sophisticated gaslighting strategies

## Confidence

- **High confidence** in the core empirical findings: The accuracy improvements (18.57% gain for LLaVA-v1.5-7B, 48.2% misguidance reduction) and ablation results are well-supported by experimental data
- **Medium confidence** in the mechanism explanation: While plausible accounts of attention sink behavior and vision-centric head selection are provided, the exact causal pathways remain partially speculative
- **Medium confidence** in generalizability: The method shows effectiveness across three distinct LMM architectures, suggesting some transferability, but model-specific parameter tuning creates uncertainty about performance on other architectures

## Next Checks

1. **Cross-architecture validation**: Apply GasEraser to additional LMM architectures (e.g., Qwen-VL, LLaVA-NeXT) with different visual backbones to test generalizability and systematically evaluate whether the same parameter tuning approach works across models

2. **Mechanistic validation through intervention studies**: Design experiments that isolate specific attention pathways by selectively disabling either text-based gaslighting cues or visual grounding mechanisms; compare GasEraser performance against these ablations to confirm whether attention reallocation from text to visual tokens is the primary protective mechanism

3. **Real-world robustness testing**: Evaluate GasEraser against gaslighting scenarios beyond negation (e.g., subtle implication, contradictory evidence, false premises) and in more naturalistic conversational contexts; test whether the method maintains general reasoning capabilities while providing protection against gaslighting across diverse attack vectors