---
ver: rpa2
title: 'NLPrompt: Noise-Label Prompt Learning for Vision-Language Models'
arxiv_id: '2412.01256'
source_url: https://arxiv.org/abs/2412.01256
tags:
- learning
- prompt
- noisy
- loss
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NLPrompt, a method to improve the robustness
  of prompt learning in vision-language models against noisy labels. The core idea
  is to use mean absolute error (MAE) loss in prompt learning (PromptMAE) which, despite
  slow convergence in traditional scenarios, shows strong robustness and accuracy
  in prompt learning contexts.
---

# NLPrompt: Noise-Label Prompt Learning for Vision-Language Models

## Quick Facts
- **arXiv ID:** 2412.01256
- **Source URL:** https://arxiv.org/abs/2412.01256
- **Reference count:** 40
- **Primary result:** State-of-the-art robustness in prompt learning under label noise using hybrid MAE/CE loss with OT-based data purification.

## Executive Summary
This paper introduces NLPrompt, a method to improve the robustness of prompt learning in vision-language models against noisy labels. The core idea is to use mean absolute error (MAE) loss in prompt learning (PromptMAE) which, despite slow convergence in traditional scenarios, shows strong robustness and accuracy in prompt learning contexts. Additionally, NLPrompt incorporates PromptOT, an optimal transport-based data purification method that partitions datasets into clean and noisy subsets using text features from vision-language models as prototypes. The method applies cross-entropy loss to clean subsets and MAE to noisy subsets, leveraging the strengths of both loss functions. Extensive experiments on multiple datasets with varied noise conditions demonstrate significant performance improvements over existing methods, including CoOp, GCE, and JoAPR. NLPrompt achieves state-of-the-art results in most cases, particularly excelling in high-noise scenarios, and shows strong generalization across different prompt-tuning approaches.

## Method Summary
NLPrompt addresses noisy label challenges in vision-language model prompt learning through a two-pronged approach. First, it employs PromptMAE, using mean absolute error loss which suppresses noisy sample influence by maintaining higher signal-to-noise ratios in feature learning. Second, it introduces PromptOT, an optimal transport-based data purification method that partitions datasets into clean and noisy subsets using text features from pre-trained VLMs as prototypes. The method applies cross-entropy loss to clean subsets and MAE to noisy subsets, harmonizing the strengths of both loss functions. The architecture freezes CLIP encoders, learns context vectors (prompts), and uses a hybrid loss manager that switches between CE and MAE based on PromptOT's partition results.

## Key Results
- NLPrompt achieves state-of-the-art performance across 8 datasets under various noise conditions (synthetic and real-world)
- Outperforms existing methods like CoOp, GCE, and JoAPR, especially in high-noise scenarios (>50% noise)
- Demonstrates strong generalization across different prompt-tuning approaches beyond the base CoOp framework
- Ablation studies confirm the effectiveness of both PromptMAE and PromptOT components

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mean Absolute Error (MAE) loss suppresses the influence of noisy samples more effectively than Cross-Entropy (CE) in prompt learning contexts.
- **Mechanism:** The authors leverage feature learning theory to show that MAE loss increases the ratio of task-relevant features to task-irrelevant features compared to CE. While both losses improve this ratio for clean data, CE significantly degrades it for noisy data (increasing task-irrelevant features), whereas MAE mitigates this degradation, maintaining a higher signal-to-noise ratio.
- **Core assumption:** The latent spaces of the vision-language model (VLM) are well-aligned, and the prompt weights can be decomposed into task-relevant and task-irrelevant features (Lemma 4.1).
- **Evidence anchors:**
  - [abstract] "leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio"
  - [section 4] "Theorem 4.2... test loss for the prompt trained by MAE is lower than the prompt trained by CE"
  - [corpus] Neighbor papers like "Introducing Fractional Classification Loss" corroborate the general search for robust loss functions, though NLPrompt specifically targets the prompt learning domain.
- **Break condition:** If the dataset is purely clean, MAE may underperform or converge slower than CE due to its inherent optimization difficulties in standard settings.

### Mechanism 2
- **Claim:** Using text features as prototypes for Optimal Transport (OT) creates a robust mechanism for partitioning datasets into clean and noisy subsets.
- **Mechanism:** Unlike standard OT methods that use randomly initialized prototypes, PromptOT uses the pre-trained text features (class embeddings) as prototypes. Because VLMs like CLIP have aligned image-text spaces, these text features provide semantically meaningful anchors. The OT matrix maps image features to these prototypes, effectively identifying samples that align poorly with their labeled class (potential noise).
- **Core assumption:** The pre-trained VLM has sufficiently high-quality alignment such that text features serve as better class centroids than random initialization.
- **Evidence anchors:**
  - [section 5] "PromptOT employs text features in vision-language models as prototypes to construct an optimal transportation matrix."
  - [section 6.7] Ablation study (c) shows that replacing text feature initialization with random initialization drops performance, validating the prototype quality.
- **Break condition:** If the VLM is poorly pre-trained or the domain is significantly out-of-distribution (OOD), the text prototypes may not align well with image features, causing the OT partition to fail.

### Mechanism 3
- **Claim:** Applying distinct loss functions to the partitioned clean and noisy subsets maximizes overall performance by harmonizing the strengths of CE and MAE.
- **Mechanism:** The system uses the partition from PromptOT to treat samples differently. It applies CE loss to the "clean" subset to leverage its fast convergence and high accuracy on standard data. It applies MAE loss to the "noisy" subset to utilize its robustness against mislabeled data.
- **Core assumption:** The PromptOT purification step achieves a precision high enough that the "clean" subset is actually clean enough for CE to be beneficial.
- **Evidence anchors:**
  - [section 5] "Harmonizing MAE and CE within NLPrompt... allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset."
  - [abstract] "Cross-entropy loss generally outperforms MAE on clean datasets... [we] harmonizes the strengths of both."
- **Break condition:** In low-noise scenarios, if OT misclassifies clean samples as noisy, applying MAE to them may result in suboptimal convergence (identified in Section D: Limitation).

## Foundational Learning

- **Concept: Optimal Transport (OT) & Sinkhorn Algorithm**
  - **Why needed here:** NLPrompt relies on solving an OT problem to map image samples to class prototypes globally (rather than independently). You need to understand how the cost matrix (negative log similarity) drives this "transport" to assign pseudo-labels.
  - **Quick check question:** How does adding entropic regularization (the Sinkhorn algorithm) change the solvability of the OT problem compared to a strict linear program?

- **Concept: Feature Learning Theory (Signal-to-Noise Ratio)**
  - **Why needed here:** The theoretical justification for using MAE lies in analyzing the dynamics of "task-relevant" vs. "task-irrelevant" feature coefficients during gradient descent.
  - **Quick check question:** According to the paper, how does the gradient update for task-relevant features differ between MAE and CE when a noisy sample is encountered?

- **Concept: Prompt Learning (CoOp)**
  - **Why needed here:** NLPrompt is built on top of existing prompt learning frameworks (specifically CoOp). You need to understand that only the prompt embeddings are updated, while the image/text encoders remain frozen.
  - **Quick check question:** In the NLPrompt architecture, are the "text features" used for the OT prototypes fixed or learned?

## Architecture Onboarding

- **Component map:** CLIP Image Encoder & Text Encoder (Frozen) -> Image Features & Text Features -> PromptOT (Sinkhorn solver) -> Partitioner -> Loss Manager (CE/MAE) -> Prompt Module (Learnable context vectors)

- **Critical path:**
  1. Batch enters -> Generate Image Features (Frozen) & Text Features (Frozen + Learnable Prompt)
  2. Compute Cost Matrix $C = -\log(T \cdot I^\top)$
  3. **PromptOT:** Solve for Transport Matrix $Q^*$
  4. **Partition:** If $\hat{y}_i$ (from $Q^*$) == $\tilde{y}_i$ (label), send to CE queue; else send to MAE queue
  5. **Update:** Backpropagate combined loss to update *only* the prompt vectors

- **Design tradeoffs:**
  - **Text vs. Random Prototypes:** Using text features is computationally cheaper and semantically aligned (better performance), but introduces dependency on the quality of the pre-trained text encoder
  - **MAE vs. CE:** MAE prevents memorization of noise but converges slowly; CE is fast but overfits noise. NLPrompt trades complexity (OT step) for the benefit of using both optimally

- **Failure signatures:**
  - **Low Noise Degradation:** Performance might drop slightly compared to pure CoOp on very clean datasets if OT incorrectly flags clean samples as noisy (forcing them into the slower MAE convergence path)
  - **High Noise Collapse:** If noise rates exceed the model's tolerance (e.g., >75%), the OT matrix may become unreliable, leading to mispartitioning

- **First 3 experiments:**
  1. **Verify MAE Robustness:** Train CoOp with CE vs. MAE on a dataset with 50% symmetric noise. Confirm that MAE maintains higher accuracy (replicate Figure 1)
  2. **Validate PromptOT:** Ablate the prototype initialization. Compare "Text Features as Prototypes" vs. "Random Initialization" on the Flowers102 dataset to quantify the purification improvement
  3. **Loss Harmony Check:** Compare NLPrompt (mixed loss) vs. (a) All MAE and (b) All CE on the Food101N (real-world noise) dataset to prove the value of the split strategy

## Open Questions the Paper Calls Out
None

## Limitations
- The computational overhead of the Sinkhorn algorithm, though not explicitly discussed, could be prohibitive for larger-scale applications
- Effectiveness of PromptOT relies heavily on the quality of the pre-trained vision-language model's text embeddings, which may not generalize to domain-shifted or non-CLIP architectures
- Performance may degrade slightly on very clean datasets if OT incorrectly flags clean samples as noisy, forcing them into the slower MAE convergence path

## Confidence
- **High Confidence:** The hybrid loss strategy (CE for clean, MAE for noisy subsets) is well-supported by experimental results and theoretical justification
- **Medium Confidence:** The PromptOT purification mechanism's effectiveness is demonstrated through ablation studies, but its sensitivity to VLM quality and noise type requires further investigation
- **Medium Confidence:** The claim of state-of-the-art performance is substantiated for most tested scenarios, though comparisons with emerging methods are not comprehensive

## Next Checks
1. **Ablation on VLM Quality:** Evaluate PromptOT's purification accuracy using different pre-trained VLMs (e.g., OpenCLIP, BLIP) to quantify dependency on CLIP-specific text embeddings
2. **Scalability Analysis:** Measure runtime overhead of the Sinkhorn algorithm across varying dataset sizes and batch dimensions to assess practical applicability
3. **Domain Shift Robustness:** Test NLPrompt on out-of-distribution datasets (e.g., domain adaptation benchmarks) to verify generalization beyond standard classification tasks