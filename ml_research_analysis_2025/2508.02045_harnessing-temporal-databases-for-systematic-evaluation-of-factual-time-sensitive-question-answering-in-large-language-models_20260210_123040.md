---
ver: rpa2
title: Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive
  Question-Answering in Large Language Models
arxiv_id: '2508.02045'
source_url: https://arxiv.org/abs/2508.02045
tags:
- temporal
- time
- evaluation
- start
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TDBench, a benchmark that uses temporal databases
  and SQL techniques to systematically construct time-sensitive question-answer pairs
  for evaluating large language models (LLMs). Unlike existing benchmarks that rely
  on manual curation or limited templates, TDBench leverages temporal functional dependencies
  and Allen's 13 temporal relations to generate diverse QA pairs automatically.
---

# Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models

## Quick Facts
- arXiv ID: 2508.02045
- Source URL: https://arxiv.org/abs/2508.02045
- Authors: Soyeon Kim; Jindong Wang; Xing Xie; Steven Euijong Whang
- Reference count: 40
- Introduces TDBench, a temporal database-driven benchmark for evaluating LLMs on time-sensitive questions

## Executive Summary
This paper addresses the challenge of systematically evaluating large language models' ability to answer time-sensitive questions by introducing TDBench, a novel benchmark that leverages temporal databases and SQL techniques. Unlike existing benchmarks that rely on manual curation or limited templates, TDBench uses temporal functional dependencies and Allen's 13 temporal relations to automatically generate diverse and verifiable QA pairs. The framework also introduces "time accuracy" as a new metric that evaluates the correctness of time references in model explanations alongside answer accuracy, revealing that LLMs frequently hallucinate time references even when providing correct answers.

## Method Summary
TDBench employs a three-step pipeline: (1) SQL generation using temporal functional dependencies and Allen's relations, (2) SQL-to-text translation via LLM to create natural language questions, and (3) evaluation using a separate LLM-judge to assess both answer accuracy and time accuracy. The method leverages temporal databases as ground truth, eliminating manual curation while ensuring verifiable answers. Time accuracy specifically evaluates whether time references in model explanations match the ground truth from the database, addressing a critical gap in existing evaluation approaches.

## Key Results
- TDBench generates 8,231 time-sensitive questions across multiple domains with automatic ground truth
- LLMs show average 21.7% drop in performance when evaluated with time accuracy versus answer accuracy alone
- Models struggle most with "overlap" and "contain" temporal relations, excelling only at "equal" relations
- Performance varies significantly across reasoning hops in multi-hop questions, with errors compounding at each step

## Why This Works (Mechanism)

### Mechanism 1: SQL-Driven Question Generation
- **Claim:** Generating time-sensitive questions via temporal SQL operators yields more comprehensive and diverse TSQA benchmarks compared to manually curated or fixed-template methods.
- **Mechanism:** The Genqueries algorithm systematically generates SQL queries by combining a base query (from TFDs) with one of 13 temporal relation constraints. An LLM then translates these structured queries into natural language, ensuring linguistic diversity while the SQL guarantees temporal precision and answer verifiability against the database.
- **Core assumption:** LLMs are sufficiently capable of accurate SQL-to-text translation for the generated queries.
- **Evidence anchors:** [abstract] states the approach "reduces reliance on human labor," [section 3.1] describes the 13 temporal relations, but specific evidence for SQL generation effectiveness is weak in the provided corpus.

### Mechanism 2: Time Accuracy Metric for Holistic Evaluation
- **Claim:** Evaluating the validity of time references in a model's explanation, in addition to its final answer, is a necessary and more reliable measure of TSQA performance.
- **Mechanism:** The paper introduces "time accuracy," which assesses if the time references in the model's rationale align with the ground truth from the database. This is evaluated by an LLM-judge against a relation-specific time reference.
- **Core assumption:** The LLM-judge can accurately identify and verify time references in free-text responses.
- **Evidence anchors:** [abstract] describes time accuracy as enabling "more reliable TSQA evaluation," [section 3.2] notes LLMs generate inaccurate time references, and a related paper (arXiv:2503.17073) supports the need for such metrics.

### Mechanism 3: Database-Driven Multi-Hop Question Construction
- **Claim:** Temporal joins between structured database tables provide a scalable and verifiable method for generating complex, multi-hop reasoning questions.
- **Mechanism:** Multi-hop questions are created by joining two or more temporal database tables using temporal natural joins. The fixed schema and TFDs allow the same TDBench pipeline to generate questions requiring reasoning across multiple events.
- **Core assumption:** The temporal databases contain accurate and consistent relationships between entities and events.
- **Evidence anchors:** [section 3.3] explains multi-hop construction via table joins, [section 4.3] shows step-wise performance analysis, but direct evidence for this specific mechanism is limited in the corpus.

## Foundational Learning

- **Concept: Temporal Functional Dependencies (TFDs)**
  - **Why needed here:** TFDs are used to automatically select which attributes in a database can form valid question-answer pairs. Understanding that `X_T -> Y` means "at any point in time, attribute(s) X determine attribute Y" is essential to grasp how TDBench generates its questions.
  - **Quick check question:** In a table of political leaders, what TFD would ensure that at any given date, a specific country has only one president? (Answer: `country, start_date, end_date -> president_name`)

- **Concept: Allen's Interval Algebra (13 Temporal Relations)**
  - **Why needed here:** The benchmark's comprehensiveness hinges on generating questions based on 13 distinct temporal relations. Knowing what these relations represent (e.g., `meet`: one interval ends exactly when another starts) is crucial for interpreting the experimental results.
  - **Quick check question:** What is the difference between the "overlap" and "contain" temporal relations? (Answer: `overlap` means intervals partially intersect; `contain` means one interval fully encompasses the other.)

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The "time accuracy" metric relies on using a separate LLM to evaluate the correctness of time references in another model's output. This concept is central to the evaluation methodology.
  - **Quick check question:** What are the potential failure modes when using an LLM to judge another LLM's response, as discussed in the paper? (Answer: Instruction misinterpretation and failure to match equivalent date formats.)

## Architecture Onboarding

- **Component Map:** Temporal Database -> TFD Selection -> SQL Generation (Genqueries) -> SQL-to-Text (LLM) -> Natural Language Question -> Subject LLM Response -> LLM-Judge Evaluation
- **Critical Path:** The end-to-end data flow is: Temporal Database -> TFD Selection -> SQL Generation (Genqueries) -> SQL-to-Text (LLM) -> Natural Language Question -> Subject LLM Response -> LLM-Judge Evaluation
- **Design Tradeoffs:**
  - **SQL-to-Text vs. Direct Generation:** Using SQL as an intermediate step adds complexity but ensures the temporal logic is verifiable and grounded in the data, trading off some naturalness for precision.
  - **LLM-Judge vs. Exact Match:** Using an LLM-judge for time accuracy allows for flexible language interpretation but introduces potential evaluator bias and error, unlike deterministic exact matching.
- **Failure Signatures:**
  - **Incorrect Question:** Generated question doesn't match the SQL's temporal logic (e.g., "before" vs. "after")
  - **Judge Misinterpretation:** The LLM-judge fails to recognize a correct but differently formatted date
  - **TFD Violation:** The underlying database data violates the assumed TFD, making a definitive answer impossible
- **First 3 Experiments:**
  1. **Reproduce Single-Hop Evaluation:** Run the provided TDBench code on a single domain (e.g., Country) with a baseline LLM (e.g., GPT-3.5) to verify the pipeline and reproduce the answer vs. time accuracy gap (Table 4)
  2. **Ablate on Temporal Relations:** Generate a small test set using only simple relations (`equal`, `before`) vs. complex ones (`overlap`, `contain`) to confirm the paper's finding that models struggle more with complex temporal reasoning (Fig. 3)
  3. **Probe Multi-Hop Reasoning:** Run a 2-hop evaluation (e.g., Olympic + Leader) and manually inspect the intermediate reasoning of a model (e.g., GPT-4o) to see if it hallucinates on the first hop (identifying the host country) or the second (identifying the leader), as suggested by the H1/H2 metrics (Table 6)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can automated data cleaning techniques be integrated into the TDBench framework to ensure the reliability of generated QA pairs when input databases contain noise or inconsistencies?
- **Basis in paper:** Section 7 (Limitation) states that the correctness of QA pairs relies on underlying data quality and assumes "it is the user's responsibility to address data quality issues," suggesting automated correction as a future need.
- **Why unresolved:** The current implementation assumes clean input data, which may not hold for arbitrary temporal databases outside of curated sources like Wikipedia or Kaggle.
- **What evidence would resolve it:** A modified pipeline that includes data validation or cleaning steps and a comparison of QA pair validity scores with and without these steps.

### Open Question 2
- **Question:** How can the "time accuracy" metric be expanded to capture broader categories of factual inconsistency in model explanations beyond hallucinated start and end dates?
- **Basis in paper:** Section 7 (Limitation) notes that "time accuracy can be imperfect in capturing all types of factual errors in model explanations, as hallucinations can arise from various sources."
- **Why unresolved:** The current metric focuses strictly on timestamp validity, missing other logical or relational errors that might occur in the natural language explanation.
- **What evidence would resolve it:** A study analyzing false negatives in the current evaluation where models provide logically incorrect explanations that nonetheless pass the timestamp verification.

### Open Question 3
- **Question:** To what extent does the choice of the LLM-judge impact the precision and reliability of the "time accuracy" metric?
- **Basis in paper:** The paper uses Deepseek-R1-14B as an LLM-judge to avoid the bias of using GPT-4o to evaluate itself (Section B.5), implying that the metric's consistency depends heavily on the judge's capability.
- **Why unresolved:** The paper demonstrates high agreement with humans but highlights that using stronger LLMs increases accuracy, leaving the trade-offs between cost, bias, and accuracy unresolved.
- **What evidence would resolve it:** A comparative analysis of inter-annotator agreement between different LLM-judge architectures and human evaluators across varying difficulty levels of temporal reasoning.

## Limitations

- The benchmark relies on LLM-judges (Deepseek-R1-14B) for time accuracy assessment, introducing potential evaluator bias with an 8.9% error rate
- SQL-to-text translation using GPT-4o achieves only 91.5% accuracy, potentially introducing errors in question generation
- Temporal database construction depends on potentially noisy Wikipedia extraction, affecting data quality and consistency

## Confidence

- **High:** The core claim that SQL-driven generation provides verifiable ground truth is well-supported by the systematic construction methodology
- **Medium:** The time accuracy metric's reliability is moderately supported but depends on LLM-judge consistency
- **Medium:** The multi-hop question generation approach is sound but has limited validation across diverse domains

## Next Checks

1. **Cross-validate time accuracy** by having human annotators evaluate a random 200-sample subset from different temporal relations to assess LLM-judge reliability
2. **Test SQL-to-text robustness** by manually verifying 100 generated questions across all 13 temporal relations to quantify translation accuracy
3. **Domain generalization test** by applying TDBench to a new temporal domain (e.g., financial market data) and measuring performance consistency with existing domains