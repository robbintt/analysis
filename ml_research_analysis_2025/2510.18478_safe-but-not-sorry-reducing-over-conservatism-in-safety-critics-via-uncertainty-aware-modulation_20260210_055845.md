---
ver: rpa2
title: 'Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware
  Modulation'
arxiv_id: '2510.18478'
source_url: https://arxiv.org/abs/2510.18478
tags:
- safety
- cost
- reward
- critic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses over-conservatism in safety critics for reinforcement
  learning, which flattens cost landscapes and degrades policy learning. The proposed
  Uncertain Safety Critic (USC) integrates uncertainty-aware modulation into critic
  training, concentrating conservatism in uncertain and costly regions while preserving
  sharp gradients in safe areas.
---

# Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation

## Quick Facts
- arXiv ID: 2510.18478
- Source URL: https://arxiv.org/abs/2510.18478
- Authors: Daniel Bethell; Simos Gerasimou; Radu Calinescu; Calum Imrie
- Reference count: 40
- The paper addresses over-conservatism in safety critics for reinforcement learning, which flattens cost landscapes and degrades policy learning. The proposed Uncertain Safety Critic (USC) integrates uncertainty-aware modulation into critic training, concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas. USC achieves approximately 40% reduction in safety violations while maintaining competitive or higher rewards, and reduces error between predicted and true cost gradients by approximately 83% compared to the state-of-the-art.

## Executive Summary
The paper addresses a fundamental challenge in safe reinforcement learning: over-conservatism in safety critics that flattens cost landscapes and degrades policy learning. The proposed Uncertain Safety Critic (USC) integrates uncertainty-aware modulation into critic training, concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas. This approach achieves approximately 40% reduction in safety violations while maintaining competitive or higher rewards, and reduces error between predicted and true cost gradients by approximately 83% compared to state-of-the-art methods.

## Method Summary
The Uncertain Safety Critic (USC) introduces a novel approach to safety critic training by incorporating uncertainty estimation into the learning process. Rather than applying uniform conservatism across all state-action pairs, USC modulates the degree of conservatism based on the critic's uncertainty estimates. The method uses bootstrap ensembles or Bayesian neural networks to quantify uncertainty in cost predictions, then applies this uncertainty as a weighting factor in the loss function. This allows the critic to be more conservative in regions with high uncertainty or high predicted cost, while maintaining sharper gradients in well-understood, safe regions. The training objective combines both the standard safety critic loss with an uncertainty-aware modulation term that scales the conservatism based on the predicted cost and uncertainty levels.

## Key Results
- Achieves approximately 40% reduction in safety violations compared to baseline safety critics
- Maintains competitive or higher rewards while improving safety performance
- Reduces error between predicted and true cost gradients by approximately 83% compared to state-of-the-art methods

## Why This Works (Mechanism)
The core mechanism addresses the fundamental issue that traditional safety critics apply uniform conservatism across all state-action pairs, which flattens the cost landscape and makes policy learning difficult. By integrating uncertainty-aware modulation, USC concentrates conservatism where it's most needed - in regions with high uncertainty or high predicted cost - while preserving the ability to learn sharp gradients in safe, well-understood regions. This targeted approach maintains safety guarantees where uncertainty is high while enabling effective policy optimization in safer regions. The uncertainty estimation provides a natural way to identify where the critic's predictions are reliable and where additional conservatism is warranted, creating a more nuanced and effective safety mechanism.

## Foundational Learning

**Uncertainty Estimation in Neural Networks**
Why needed: Provides a measure of confidence in the critic's cost predictions, essential for modulating conservatism appropriately.
Quick check: Verify that uncertainty estimates correlate with actual prediction errors in validation data.

**Bootstrap Ensembles / Bayesian Neural Networks**
Why needed: These techniques provide practical methods for quantifying uncertainty in deep neural network predictions.
Quick check: Compare ensemble variance with true prediction variance on held-out data.

**Safety-Critical Reinforcement Learning**
Why needed: Understanding how safety constraints interact with policy optimization is crucial for designing effective safety critics.
Quick check: Verify that the safety critic can effectively shape the policy's behavior in dangerous regions.

**Cost Landscape Analysis**
Why needed: Understanding how conservatism affects the cost landscape is key to diagnosing over-conservatism issues.
Quick check: Compare gradient magnitudes and smoothness across different critic variants.

## Architecture Onboarding

Component map: Environment -> Policy Network -> Action -> Safety Critic -> Cost/Uncertainty -> Loss Function -> Updated Critic

Critical path: The safety critic predicts costs for state-action pairs, uncertainty is estimated for these predictions, and both are combined to modulate the conservatism applied during training. This modulated conservatism then shapes the critic's predictions, which in turn guides the policy's behavior through the optimization process.

Design tradeoffs: The main tradeoff is between computational complexity (from uncertainty estimation) and safety performance. Bootstrap ensembles provide good uncertainty estimates but increase computation, while simpler methods like Monte Carlo dropout are faster but potentially less accurate. The choice of uncertainty quantification method significantly impacts both performance and efficiency.

Failure signatures: Over-conservatism manifests as flattened cost landscapes with near-zero gradients, preventing effective policy learning. Under-conservatism shows as insufficient safety protection in uncertain regions. Poor uncertainty estimates can lead to inappropriate modulation, either being too conservative in safe regions or not conservative enough in dangerous ones.

First experiments: 1) Test uncertainty estimation accuracy on validation data, 2) Evaluate gradient magnitudes in safe vs. uncertain regions, 3) Measure safety performance on simple benchmark tasks with known safe and unsafe regions.

## Open Questions the Paper Calls Out
- How does USC perform in environments with highly dynamic or non-stationary safety constraints?
- What is the impact of different uncertainty estimation techniques on USC's performance and computational requirements?
- Can the uncertainty-aware modulation approach be extended to other safety-critical components beyond the critic?

## Limitations
- Limited evaluation on highly stochastic environments where uncertainty estimation becomes more challenging
- Potential computational overhead from uncertainty quantification that may affect real-time deployment
- Assumption that uncertainty correlates well with true safety risk, which may not hold in all scenarios

## Confidence
High confidence in the core technical contribution and experimental results. The mathematical formulation of uncertainty-aware modulation is rigorous and the empirical improvements are substantial. Medium confidence in generalizability across different RL domains, as the evaluation is primarily focused on safety-critical continuous control tasks.

## Next Checks
1. Test USC in environments with varying levels of stochasticity to evaluate robustness of uncertainty estimates
2. Benchmark computational efficiency and runtime overhead compared to standard safety critics
3. Evaluate performance when combined with different policy optimization algorithms beyond the ones tested in the paper