---
ver: rpa2
title: Integrating Arithmetic Learning Improves Mathematical Reasoning in Smaller
  Models
arxiv_id: '2502.12855'
source_url: https://arxiv.org/abs/2502.12855
tags:
- arithmetic
- gsm8k
- dataset
- reasoning
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Small language models struggle with arithmetic computations, leading
  to errors in mathematical reasoning tasks. This work investigates incorporating
  programmatically generated arithmetic datasets through two approaches: intermediate
  fine-tuning (fine-tuning on arithmetic data before reasoning data) and instruction-tuning
  mixture (including arithmetic data in post-training).'
---

# Integrating Arithmetic Learning Improves Mathematical Reasoning in Smaller Models

## Quick Facts
- arXiv ID: 2502.12855
- Source URL: https://arxiv.org/abs/2502.12855
- Reference count: 40
- Small language models struggle with arithmetic computations, leading to errors in mathematical reasoning tasks.

## Executive Summary
Small language models often struggle with arithmetic computations, which propagates errors into mathematical reasoning tasks. This work demonstrates that incorporating programmatically generated arithmetic datasets through intermediate fine-tuning (training on arithmetic before reasoning) or instruction-tuning mixture (including arithmetic in post-training) significantly improves mathematical reasoning performance. The study shows that models with arithmetic pre-training exhibit better out-of-domain generalization, while those with arithmetic in instruction tuning demonstrate improved few-shot performance and robustness to numerical variations.

## Method Summary
The authors generate a large arithmetic dataset (~1.3M examples) with operations including addition, subtraction, multiplication, division, fractions, and percentages, using log-uniform sampling for operands up to 7 digits. They evaluate two approaches: (1) Intermediate fine-tuning where models are first trained on arithmetic data for 2 epochs, then fine-tuned on GSM8k reasoning dataset, and (2) Instruction-tuning mixture where arithmetic data is merged into the TÜLU 3 SFT mixture during post-training. Models ranging from 124M to 774M parameters are evaluated across multiple benchmarks including GSM8k, MultiArith, ASDiv, SVAMP, MAWPS, AQuA, GSM-Plus, and GSM-Symbolic.

## Key Results
- Intermediate fine-tuning reduces arithmetic errors compared to direct reasoning training, with an average improvement of 11.7% in arithmetic computations
- Models with arithmetic in instruction-tuning mixture show better few-shot performance and robustness to numerical variations (25.9% vs 44.4% performance drop on GSM-Symbolic)
- Out-of-domain generalization is improved with intermediate fine-tuning, while instruction-tuning mixture provides better numerical robustness

## Why This Works (Mechanism)

### Mechanism 1: Sequential Skill Layering via Intermediate Fine-Tuning
Training arithmetic before reasoning enables more accurate computation within reasoning contexts. The model first develops computational fluency on a large, diverse arithmetic corpus, freeing capacity during reasoning training to focus on problem decomposition rather than arithmetic. This reduces simultaneous learning burden. Prolonged intermediate fine-tuning (>2 epochs) harms transfer by over-optimizing for arithmetic tasks, limiting adaptability.

### Mechanism 2: Joint Skill Acquisition via Instruction-Tuning Mixture
Co-training arithmetic with diverse instruction-following tasks improves few-shot robustness. Arithmetic examples compete with other tasks during training, forcing the model to learn context-dependent arithmetic invocation rather than task-specific overfitting. This yields better generalization to novel problem structures. Over-weighting arithmetic in the mixture may reduce performance on non-mathematical tasks.

### Mechanism 3: Distributional Coverage Through Synthetic Data Generation
Log-uniform sampling over operand magnitudes prevents digit-length bias and improves generalization. Programmatically generated arithmetic data with log-uniform sampling ensures balanced exposure across digit lengths (up to 7 digits), unlike natural corpora that skew toward small numbers. This coverage translates to robustness on numerical perturbations.

## Foundational Learning

- **Transfer Learning / Intermediate Fine-Tuning**
  - Why needed here: The core methodology depends on understanding why pre-training on a related task (arithmetic) before the target task (reasoning) improves performance.
  - Quick check question: Can you explain why fine-tuning on Task A before Task B might help, and when it might hurt?

- **Instruction Tuning vs. Task-Specific Fine-Tuning**
  - Why needed here: The paper compares two distinct training paradigms; understanding their tradeoffs is essential for architecture decisions.
  - Quick check question: What is the difference between instruction-tuning on a diverse mixture vs. fine-tuning on a single task, and what capabilities does each promote?

- **Log-Uniform Sampling**
  - Why needed here: The arithmetic dataset uses this sampling strategy to prevent digit-length bias.
  - Quick check question: Why would uniform sampling over integers 1-1,000,000 create bias toward large numbers, and how does log-uniform sampling address this?

## Architecture Onboarding

- **Component map**: Arithmetic Dataset Generator -> Intermediate Fine-Tuning Stage -> Reasoning Fine-Tuning Stage OR Arithmetic Data -> Instruction-Tuning Stage -> TÜLU 3 SFT Mixture

- **Critical path**:
  1. Generate arithmetic dataset with log-uniform sampling
  2. For intermediate fine-tuning: train 2 epochs on arithmetic → train on reasoning dataset
  3. For instruction-tuning: merge arithmetic data into TÜLU 3 SFT mixture → train 5 epochs
  4. Evaluate on GSM8k, MultiArith, ASDiv, SVAMP, SVAMP, GSM-Plus, GSM-Symbolic

- **Design tradeoffs**:
  - Intermediate fine-tuning: Better out-of-domain generalization but risks overfitting to arithmetic if >2 epochs; requires larger downstream dataset to recover adaptability
  - Instruction-tuning mixture: Better few-shot and robustness, but requires careful mixture balancing; no single-task overfitting risk

- **Failure signatures**:
  - Prolonged intermediate fine-tuning: No arithmetic accuracy gain after 2 epochs; reasoning performance plateaus or declines
  - Small downstream dataset after IFT: GPT2 struggled on GSM8k Orig. but not Dist.
  - AQuA benchmark: Near-random performance across all configurations (~20%); algebraic reasoning may require different intervention

- **First 3 experiments**:
  1. Replicate FlanT5-Large IFT experiment: Train 2 epochs on arithmetic dataset, then fine-tune on GSM8k (Orig.) for 100 epochs. Compare GSM8k test accuracy to baseline.
  2. Ablate IFT epochs: Train FlanT5-Base with 1, 2, 4, 8 epochs of intermediate fine-tuning. Plot GSM8k arithmetic accuracy vs. overall accuracy to confirm 2-epoch saturation.
  3. Mixture ratio sensitivity test: Instruction-tune GPT2-Large with arithmetic data at 0%, 10%, 25%, 50% of mixture. Evaluate on GSM-Plus numerical variation subset to find optimal robustness vs. general capability tradeoff.

## Open Questions the Paper Calls Out

- Can combining programmatic arithmetic datasets with architectural modifications, such as custom embedding schemes, further enhance numerical reasoning capabilities? (Section 8 states authors did not investigate factors like custom embedding schemes and suggest incorporating insights from works like McLeish et al. (2024) as a promising future direction.)

- Do the benefits of explicit arithmetic training via intermediate fine-tuning scale effectively to models with significantly larger parameter counts? (Section 8 notes that while the study focuses on smaller models, the findings likely apply to larger models, but explicitly leaves these explorations for future work.)

- What is the optimal ratio of synthetic arithmetic data within an instruction-tuning mixture to maximize mathematical reasoning without impairing general instruction-following abilities? (Section 8 suggests that "data mixture ablations may be necessary to optimize the instruction-tuning mixture for better overall performance.")

## Limitations
- The exact methodology for generating the arithmetic dataset is only referenced to prior work without detailed specification, including precise text format and program logic.
- AQuA benchmark results show near-random performance across all configurations (~20%), suggesting the approach may not generalize to algebraic reasoning.
- The optimal mixture ratio for arithmetic data in instruction tuning is not empirically determined, and the claim about superior few-shot performance relies on comparisons with a narrow set of baselines.

## Confidence
- High confidence: Core claim that intermediate fine-tuning with arithmetic data improves mathematical reasoning performance, supported by consistent improvements across multiple benchmarks and ablation studies.
- Medium confidence: Instruction-tuning mixture benefits, as the evidence is based on fewer comparisons and the mixture ratio is not optimized.
- Medium confidence: Distributional coverage mechanism, as the log-uniform sampling rationale is sound but the empirical validation is limited to specific benchmarks.

## Next Checks
1. Replicate the FlanT5-Large intermediate fine-tuning experiment exactly (2 epochs arithmetic, then GSM8k Orig. for 100 epochs) and verify the GSM8k accuracy improvement.
2. Conduct a systematic ablation study varying arithmetic data proportions in the instruction-tuning mixture (0%, 10%, 25%, 50%) and measure effects on both GSM8k accuracy and GSM-Plus numerical variation robustness.
3. Generate a new arithmetic dataset using the exact log-uniform sampling specification and verify that models trained on it show reduced digit-length bias compared to uniform sampling baselines.