---
ver: rpa2
title: 'DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder'
arxiv_id: '2602.00592'
source_url: https://arxiv.org/abs/2602.00592
tags:
- error
- trajectories
- agent
- environment
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DockSmith is an agentic Docker builder designed to address the
  bottleneck of reliable environment construction for software engineering agents.
  It treats Docker building as a core agentic capability involving long-horizon tool
  use, dependency reasoning, and failure recovery, yielding transferable supervision
  beyond environment setup.
---

# DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder

## Quick Facts
- **arXiv ID:** 2602.00592
- **Source URL:** https://arxiv.org/abs/2602.00592
- **Reference count:** 28
- **Primary result:** DockSmith achieves 39.72% Fail-to-Pass and 58.28% Commit Rate on Multi-Docker-Eval, setting open-source state of the art

## Executive Summary
DockSmith addresses the bottleneck of reliable environment construction for software engineering agents by treating Docker building as a core agentic capability involving long-horizon tool use, dependency reasoning, and failure recovery. The system achieves 39.72% Fail-to-Pass and 58.28% Commit Rate on Multi-Docker-Eval, setting open-source state of the art. Joint training with Docker-building and general coding trajectories further improves performance on SWE-bench Verified (+2.25 points), SWE-bench Multilingual (+2.09 points), and Terminal-Bench 2.0 (+3.37 points), demonstrating broader agentic benefits of environment construction.

## Method Summary
DockSmith is a supervised fine-tuned version of Qwen3-Coder-30B-A3B-Instruct trained on ~200K execution-grounded Docker-building trajectories generated via an augmented SWE-Factory pipeline. The training uses batch size 32, learning rate 1e-5, and 2 epochs with complexity-based curriculum sampling (1:2:2 Easy/Medium/Hard ratios) and acceptance shaping to filter redundant rollouts. The system employs a multi-agent architecture with Context Retrieval, Dockerfile, Eval Script, Test Analysis agents plus a loop-detection controller and cross-task memory pool. Joint training with Nex Agent-SFT coding trajectories at token-level ratios (optimal: 1:0.5 to 1:1 SWE:Docker) enables transfer benefits.

## Key Results
- 39.72% Fail-to-Pass and 58.28% Commit Rate on Multi-Docker-Eval
- +2.25 points on SWE-bench Verified, +2.09 points on SWE-bench Multilingual, +3.37 points on Terminal-Bench 2.0 with joint training
- Error reduction: docker_env_error (-48.7%), diag_loop_error (-59.8%), eval_patch_error (-79.1%), retrieval_miss_error (+32.6%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Docker-building supervision transfers to broader agentic tasks by strengthening execution-grounded reasoning.
- Mechanism: Training on Docker-building trajectories exercises long-horizon tool use, dependency reasoning, and failure recovery—skills that generalize to SWE tasks. Error analysis shows DockSmith increases principled system reasoning (27.1%→33.0%) and root-cause-aligned responses (34.6%→40.4%), while reducing heuristic trial-and-error.
- Core assumption: Environment construction requires transferable diagnostic skills, not just Dockerfile syntax knowledge.
- Evidence anchors:
  - [abstract] "yielding supervision that transfers beyond Docker building itself"
  - [Section 3.3] Joint training at 1:1 ratio improves SWE-bench Verified (+2.25), SWE-bench Multilingual (+2.09), Terminal-Bench 2.0 (+3.37)
  - [Section 3.5.2] P(env→env) increases +6%, P(runtime→runtime) +7%, indicating stronger within-layer persistence
  - [corpus] Multi-Docker-Eval paper confirms environment setup is primary bottleneck for open-source models (success <40%)
- Break condition: If Docker-building supervision were merely syntax memorization, it would not improve Terminal-Bench (command-line tool use) or logic-error resolution (+8.7%).

### Mechanism 2
- Claim: Curriculum sampling and acceptance shaping improve trajectory quality under fixed compute budget.
- Mechanism: Filtering excessively long/redundant rollouts removes noise, while complexity-based curriculum (1:2:2 Easy/Medium/Hard) increases exposure to challenging builds. Under identical token budgets, AS+CC improves average F2P from 32.24% to 34.93%.
- Core assumption: Trajectory quality matters more than quantity when compute is constrained.
- Evidence anchors:
  - [Section 3.4] Table 4 shows Baseline + AS + CC achieves best performance (34.93% avg vs 32.24% baseline)
  - [Section 2.3.1] Complexity score formula: Score(d) = 0.5·L(d) + 5·R(d) + 3·P(d)
  - [corpus] Weak direct corpus evidence on curriculum learning for agentic trajectories; related work focuses on static code generation
- Break condition: If curriculum had no effect, random sampling would match AS+CC performance under identical budgets.

### Mechanism 3
- Claim: Loop-detection controller and cross-task memory reduce unproductive diagnostic oscillation.
- Mechanism: Monitoring agent-invocation traces detects when identical agent combinations fail repeatedly. Case study shows DockSmith eliminates diagnostic loops (3→0) and Dockerfile rollbacks (2→0), reducing steps from 50 to 5.
- Core assumption: Multi-agent repair without intervention can enter repetitive failure modes.
- Evidence anchors:
  - [Section 2.2] "When repeated activation of an identical agent combination fails to improve outcomes for several rounds, the controller intervenes by enforcing diversification"
  - [Appendix B.1] Case study: JEG2/highline baseline shows 37 errors, 3 diagnostic loops, 2 rollback events; DockSmith: 2 errors, 0 loops
  - [Section 3.5.1] diag_loop_error reduced 59.8%, diag_vague_error reduced 71.7%
  - [corpus] SWE-Factory paper describes base pipeline but lacks loop-detection controller
- Break condition: If loops were rare, loop-detection would show no measurable impact on error rates or step counts.

## Foundational Learning

- Concept: **Docker environment construction as agentic task**
  - Why needed here: The paper reframes Dockerfile writing not as static code generation but as iterative, execution-grounded repair involving tool use and failure recovery.
  - Quick check question: Can you explain why environment setup differs from code generation in terms of feedback loops?

- Concept: **Token-level mixing in multi-task training**
  - Why needed here: Joint training balances Docker specialization with general coding capability; mixing ratios directly affect transfer performance.
  - Quick check question: Given the paper's results, why does 1:2 SWE:Docker underperform compared to 1:1?

- Concept: **Error propagation in agentic trajectories**
  - Why needed here: The error analysis framework (shell→env→runtime→logic layers) explains why DockSmith improves downstream tasks through better within-layer persistence.
  - Quick check question: What does increased P(runtime→runtime) probability indicate about diagnostic behavior?

## Architecture Onboarding

- Component map:
  - Context Retrieval Agent -> Dockerfile Agent -> Eval Script Agent -> Test Analysis Agent -> (if failure) back to appropriate agent with loop-detection check

- Critical path: Context Retrieval → Dockerfile Generation → Eval Script → Test Analysis → (if failure) back to appropriate agent with loop-detection check

- Design tradeoffs:
  - Higher retrieval recall increases context errors (+10.9%) but downstream stages improve (Dockerfile -46.7%, Eval Script -42.7%)
  - Joint training improves transfer but requires careful ratio tuning; 1:2 Docker-heavy degrades SWE performance
  - Conservative long-tail language upsampling prevents distribution shift but may limit rare-language coverage

- Failure signatures:
  - **docker_env_error**: Missing system packages or misconfigured toolchains (reduced 48.7%)
  - **diag_loop_error**: Agent alternates between incompatible strategies without progress (reduced 59.8%)
  - **eval_patch_error**: Files re-checked out after patching, silently discarding modifications (reduced 79.1%)
  - **retrieval_miss_error**: Failed to extract existing dependency information (slightly increased +32.6%)

- First 3 experiments:
  1. **Reproduce Multi-Docker-Eval baseline**: Run Qwen3-Coder-30B-A3B-Instruct on Multi-Docker-Eval subset, measure F2P and error distribution to establish comparison point.
  2. **Ablate loop detection**: Disable loop-detection controller, run pipeline on 20 repositories, compare step counts and diagnostic loop frequency against full system.
  3. **Vary mixing ratio**: Train three models with SWE:Docker ratios (1:0.25, 1:0.5, 1:1) on same token budget, evaluate on SWE-bench Verified and Terminal-Bench to reproduce transfer curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can models balance the observed trade-off between improved higher-level diagnostic reasoning and reduced precision on low-level shell/environment commands?
- Basis in paper: [explicit] Section 3.5.2 notes "mixed effects at the shell and environment layers suggest a trade-off between trajectory-level reasoning and low-level command precision, pointing to future work that strengthens fine-grained command execution."
- Why unresolved: DockSmith improves runtime and logic error resolution but shows lower shell/env resolution rates; the mechanism causing this trade-off is not investigated.
- What evidence would resolve it: Ablations isolating command-level supervision; analysis of whether separate modules or multi-objective training can improve both simultaneously.

### Open Question 2
- Question: Why does DockSmith show substantially weaker performance on systems languages (Java 19.05%, Rust 30.00%) compared to Python (51.28%) and JavaScript (51.67%)?
- Basis in paper: [inferred] Table 2 shows 20+ point gaps; the paper only attributes this to "compiler toolchains and system dependencies" without deeper causal analysis.
- Why unresolved: The complexity score used for curriculum sampling (Eq. 1) weights lines, RUN instructions, and apt packages but may not capture language-specific toolchain complexity.
- What evidence would resolve it: Fine-grained error analysis by language; evaluating whether language-specific curriculum weighting or toolchain-aware retrieval closes the gap.

### Open Question 3
- Question: What is the principled method for determining optimal Docker-to-SWE token mixing ratios across different model scales and task distributions?
- Basis in paper: [inferred] Figure 2 shows non-monotonic performance (1:0.5 optimal for Terminal, 1:1 optimal for SWE.V); the paper empirically tests ratios but offers no predictive framework.
- Why unresolved: The optimal ratio varies by benchmark, and no theory explains why certain balances favor command-heavy vs. reasoning-heavy tasks.
- What evidence would resolve it: Experiments across model scales (7B, 70B, etc.); theoretical analysis relating task characteristics (tool use frequency, error recovery depth) to optimal mixing.

## Limitations
- SWE-Factory augmented pipeline lacks full specification, particularly loop-detection controller and acceptance shaping thresholds
- Token-level mixing ratios and curriculum boundaries are empirically derived without theoretical guarantees
- Cross-task memory retrieval mechanism lacks detailed implementation specifications

## Confidence

**High Confidence:** Docker-building supervision improves SWE-bench and Terminal-Bench performance (robust statistical significance, controlled ablation studies)

**Medium Confidence:** Transfer mechanism through execution-grounded reasoning (error analysis shows improvements but could be influenced by training data artifacts)

**Medium Confidence:** Curriculum sampling and acceptance shaping improve trajectory quality (controlled experiments but under fixed compute assumptions)

**Low Confidence:** Cross-task memory implementation details (case studies provided but limited systematic evaluation)

## Next Checks

1. **Loop Detection Impact:** Disable loop-detection controller in DockSmith pipeline and measure diagnostic loop frequency and step counts on Multi-Docker-Eval subset to verify 59.8% reduction claim

2. **Curriculum Sensitivity:** Vary complexity-based curriculum ratios (Easy:Medium:Hard from 1:2:2 to 1:1:1) under identical compute budgets to test whether AS+CC consistently outperforms random sampling

3. **Mixing Ratio Transfer:** Train models with token-level SWE:Docker ratios (1:0.25, 1:1, 1:2) and evaluate transfer performance on SWE-bench Verified and Terminal-Bench to verify optimal range of 1:0.5 to 1:1