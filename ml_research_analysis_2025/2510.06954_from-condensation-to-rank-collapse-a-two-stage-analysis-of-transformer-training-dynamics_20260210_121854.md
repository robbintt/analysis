---
ver: rpa2
title: 'From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training
  Dynamics'
arxiv_id: '2510.06954'
source_url: https://arxiv.org/abs/2510.06954
tags:
- dynamics
- training
- initialization
- parameters
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes Transformer training dynamics under small initialization,
  employing gradient flow to establish a two-stage framework. The first stage involves
  condensation, where asymmetric weight perturbations enable outer parameters (WV
  , W[1], W[2]) to escape small initialization regimes and align toward task-relevant
  directions.
---

# From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics

## Quick Facts
- **arXiv ID:** 2510.06954
- **Source URL:** https://arxiv.org/abs/2510.06954
- **Reference count:** 40
- **Primary result:** Two-stage training dynamics framework revealing condensation and rank collapse in Transformer optimization

## Executive Summary
This work analyzes Transformer training dynamics under small initialization, employing gradient flow to establish a two-stage framework. The first stage involves condensation, where asymmetric weight perturbations enable outer parameters (WV, W[1], W[2]) to escape small initialization regimes and align toward task-relevant directions. The second stage sees key-query matrices (WQ, WK) become dynamically active, leading to asymptotic rank collapse. Theoretical proofs demonstrate blow-up dynamics, condensation emergence under Assumption 1, and rank collapse of normalized key-query matrices under Assumption 2. Experimental validation on synthetic and real (WikiText) datasets confirms these phenomena, showing consistent three-phase training dynamics: condensation, key-query rank collapse, and further training. The findings provide a principled foundation for understanding implicit regularization mechanisms in Transformer optimization.

## Method Summary
The authors employ gradient flow analysis to study Transformer training dynamics under small initialization conditions. They establish a theoretical framework based on two key assumptions about data spectrum and gradient behavior. The analysis tracks weight matrix evolution through time, identifying distinct phases of training behavior. Theoretical proofs are complemented by experiments on both synthetic and real datasets (WikiText), tracking weight matrix norms, singular values, and rank characteristics throughout training.

## Key Results
- Demonstrated condensation phenomenon where outer weight parameters escape small initialization and align with task-relevant directions
- Proven rank collapse of normalized key-query matrices in the second training stage
- Validated three-phase training dynamics (condensation, rank collapse, further training) across synthetic and real datasets
- Established mathematical framework showing blow-up dynamics under small initialization

## Why This Works (Mechanism)
The two-stage dynamics emerge from the interplay between initialization scale and gradient flow behavior. In the condensation stage, asymmetric weight perturbations create positive feedback loops that amplify certain weight components while suppressing others, enabling outer parameters to escape small initialization. This creates directional alignment toward task-relevant subspaces. In the rank collapse stage, the key-query matrices become dynamically active and experience gradient-driven compression, reducing their effective rank while maintaining performance. The mechanism relies on the gradient flow approximation and specific data properties characterized by the theoretical assumptions.

## Foundational Learning

**Gradient Flow Approximation**: Why needed - Enables continuous-time analysis of weight dynamics; Quick check - Verify that discrete optimization closely tracks continuous flow predictions

**Small Initialization Regime**: Why needed - Critical for observing blow-up dynamics and two-stage behavior; Quick check - Test different initialization scales to confirm threshold effects

**Rank Collapse Phenomena**: Why needed - Central to understanding key-query matrix evolution; Quick check - Monitor singular value spectra throughout training

**Data Spectrum Assumptions**: Why needed - Characterize conditions under which theoretical results hold; Quick check - Analyze empirical data spectra to verify assumption validity

## Architecture Onboarding

**Component Map**: Input -> Embedding Layer -> Multi-Head Attention (WQ, WK, WV matrices) -> Feed-Forward Network (W[1], W[2] matrices) -> Output Layer

**Critical Path**: The analysis focuses on the attention mechanism and its weight matrices as the critical path for understanding training dynamics, particularly the interaction between WQ, WK matrices and outer weight matrices WV, W[1], W[2]

**Design Tradeoffs**: Small initialization enables theoretical tractability but may require careful learning rate scheduling; rank collapse may affect representational capacity but could provide implicit regularization

**Failure Signatures**: Absence of condensation phase suggests initialization too large or data spectrum incompatible with assumptions; premature rank collapse may indicate optimization instability

**First Experiments**:
1. Track weight matrix norms and singular values throughout training on synthetic data
2. Vary initialization scale to identify threshold for condensation emergence
3. Compare rank evolution in key-query matrices across different attention head configurations

## Open Questions the Paper Calls Out
None

## Limitations
The theoretical framework relies heavily on specific assumptions about data spectrum and gradient behavior that may not generalize to all datasets. The analysis focuses primarily on key-query matrices, leaving other weight matrices less characterized. The gradient flow approximation may not fully capture discrete optimization effects present in practical training scenarios.

## Confidence

**High Confidence**: Two-stage training dynamics framework and condensation phenomena; mathematical analysis of weight matrix blow-up under small initialization

**Medium Confidence**: Characterization of rank collapse in key-query matrices under stated assumptions; exact conditions for rank collapse occurrence

**Low Confidence**: Broader implications for implicit regularization mechanisms and practical optimization strategies across diverse architectures

## Next Checks

1. Replicate analysis on larger Transformer models (BERT-base or larger) to verify persistence of two-stage dynamics at scale

2. Extend experiments to multiple diverse datasets beyond WikiText, including code, scientific text, and multilingual corpora

3. Test whether similar condensation and rank collapse phenomena occur in alternative attention mechanisms (linear attention, Performer) and model variants