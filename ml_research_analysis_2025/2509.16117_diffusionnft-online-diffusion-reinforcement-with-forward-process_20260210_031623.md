---
ver: rpa2
title: 'DiffusionNFT: Online Diffusion Reinforcement with Forward Process'
arxiv_id: '2509.16117'
source_url: https://arxiv.org/abs/2509.16117
tags:
- diffusion
- arxiv
- training
- preprint
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiffusionNFT, a novel online reinforcement
  learning paradigm for diffusion models that optimizes directly on the forward process
  via flow matching. The method contrasts positive and negative generations to define
  an implicit policy improvement direction, naturally incorporating reinforcement
  signals into the supervised learning objective.
---

# DiffusionNFT: Online Diffusion Reinforcement with Forward Process

## Quick Facts
- arXiv ID: 2509.16117
- Source URL: https://arxiv.org/abs/2509.16117
- Reference count: 40
- Primary result: Achieves up to 25× efficiency improvement over FlowGRPO while being CFG-free

## Executive Summary
DiffusionNFT introduces a novel online reinforcement learning paradigm for diffusion models that optimizes directly on the forward process via flow matching. Unlike prior work that applies RL to discretized reverse processes, DiffusionNFT operates on the continuous forward diffusion, maintaining forward consistency with the Fokker-Planck equation. The method contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective without requiring likelihood estimation or trajectory storage.

The approach achieves significant efficiency gains, improving GenEval scores from 0.24 to 0.98 within 1k steps compared to FlowGRPO's 0.95 within 5k steps with CFG. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium across diverse benchmarks including rule-based rewards (GenEval, OCR) and model-based rewards (PickScore, ClipScore, HPSv2.1, Aesthetic, ImageReward, UnifiedReward).

## Method Summary
DiffusionNFT trains diffusion models by optimizing the forward process through flow matching, contrasting positive and negative samples to define policy improvement directions. The method collects clean images with rewards using arbitrary black-box solvers, then applies forward diffusion on-the-fly during training. It uses implicit velocity parameterization to satisfy both positive and negative policy objectives simultaneously, with rewards normalizing the split between these branches. The approach employs soft EMA updates to decouple sampling from training, eliminating trajectory storage requirements. Key hyperparameters include β for guidance strength (0.1-1.0), adaptive weighting via x0-prediction normalization, and η schedules for soft updates.

## Key Results
- Achieves 25× efficiency improvement over FlowGRPO while being classifier-free guidance (CFG)-free
- Improves GenEval score from 0.24 to 0.98 within 1k steps (FlowGRPO: 0.95 within 5k steps with CFG)
- Boosts SD3.5-Medium performance across all tested benchmarks using multiple reward models
- Demonstrates ODE samplers outperform SDE samplers for noise-sensitive rewards like PickScore

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing the forward process via flow matching preserves probabilistic consistency while enabling RL signals.
- Mechanism: Applies policy optimization directly on the forward noising process $x_t = \alpha_t x_0 + \sigma_t \epsilon$, maintaining adherence to the Fokker-Planck equation rather than degenerating into cascaded Gaussians.
- Core assumption: The velocity predictor $v_\theta$ trained on the forward process generalizes to quality sampling via reverse ODE/SDE.
- Evidence anchors: [abstract] "optimizes diffusion models directly on the forward process via flow matching"; [Page 4-5] "This preserves what we term forward consistency—the adherence of the diffusion model's underlying probability density to the Fokker-Planck equation"
- Break condition: If the reward signal becomes contradictory to the forward process structure, the flow matching objective may conflict with policy improvement.

### Mechanism 2
- Claim: Contrasting positive and negative generations defines a valid policy improvement direction without requiring likelihood estimation.
- Mechanism: Collected samples are split by reward into positive ($D^+$) and negative ($D^-$) subsets. The paper proves (Theorem 3.1) that the directional difference $\Delta$ between policies satisfies $\Delta \propto v^+ - v^{old} \propto v^{old} - v^-$. An implicit parameterization trains a single model to satisfy both: $v_\theta^+ = (1-\beta)v^{old} + \beta v_\theta$ and $v_\theta^- = (1+\beta)v^{old} - \beta v_\theta$, with loss weighted by reward $r$.
- Core assumption: Reward-normalized optimality probability $r \in [0,1]$ correctly partitions samples into meaningful positive/negative distributions.
- Evidence anchors: [Page 4, Theorem 3.1-3.2] Formal derivation showing $v_{\theta^*} = v^{old} + \frac{2}{\beta}\Delta$ under unlimited data/capacity; [Page 8, Ablation] "Without the negative policy loss on $v_\theta^-$, we find rewards collapse almost instantly during online training"
- Break condition: If all samples receive similar rewards (low variance), the positive/negative split becomes uninformative, and the contrastive direction $\Delta$ approaches zero.

### Mechanism 3
- Claim: Decoupling sampling from training enables arbitrary black-box solvers and eliminates trajectory storage.
- Mechanism: Data collection uses any solver (SDE, ODE, high-order) to generate clean images $x_0$ with rewards. Training requires only $(x_0, c, r)$ tuples—no intermediate latents or trajectory storage. The forward process is applied on-the-fly during loss computation.
- Core assumption: Clean images with reward signals are sufficient for policy improvement; trajectory information is redundant.
- Evidence anchors: [Page 5] "Second, it eliminates the need to store entire sampling trajectories, requiring only clean images for policy optimization"; [Page 7, Figure 7] Ablation shows ODE samplers outperform SDE samplers for data collection
- Break condition: If the reward function depends on intermediate denoising states, the clean-image-only formulation cannot capture this signal.

## Foundational Learning

- Concept: **Flow Matching / Rectified Flow**
  - Why needed here: DiffusionNFT operates on velocity parameterization $v_\theta(x_t, t)$ under rectified flow schedule ($\alpha_t = 1-t$, $\sigma_t = t$), where target velocity $v = \epsilon - x_0$. Without understanding this, the loss formulation (Eq. 5) is opaque.
  - Quick check question: Given $x_t = 0.5 x_0 + 0.5 \epsilon$ at $t=0.5$ under rectified flow, what is the target velocity $v$?

- Concept: **Policy Gradient vs. Supervised RL Paradigms**
  - Why needed here: DiffusionNFT explicitly rejects Policy Gradient (PPO/GRPO) for a supervised learning objective augmented with negative data. Understanding this distinction clarifies why likelihood estimation is unnecessary.
  - Quick check question: Why does GRPO require discretizing the reverse process into tractable Gaussian transitions, while DiffusionNFT does not?

- Concept: **Off-Policy RL with Soft Updates**
  - Why needed here: The method decouples $\pi^{old}$ (sampling) from $\pi_\theta$ (training) and uses EMA updates. This is critical for stability—fully on-policy ($\eta=0$) causes collapse, while fully off-policy ($\eta \to 1$) is too slow.
  - Quick check question: If $\eta_i = 0.5$ at iteration $i$, what fraction of the sampling policy comes from the initial reference model after 3 iterations?

## Architecture Onboarding

- Component map:
  [Pretrained v_ref] → [Data Collection: sample K images per prompt using any solver]
                           ↓
                      [Reward Evaluation: r_raw → normalize → r ∈ [0,1]]
                           ↓
                      [Forward Process: x_t = α_t x_0 + σ_t ε, compute v target]
                           ↓
                      [Implicit Policy Branches: v_θ⁺, v_θ⁻ via β-mixing with v_old]
                           ↓
                      [Loss: r||v_θ⁺ - v||² + (1-r)||v_θ⁻ - v||²]
                           ↓
                      [Soft Update: θ_old ← η_i θ_old + (1-η_i)θ]

- Critical path: The reward normalization (Step 3.3, $r = 0.5 + 0.5 \cdot \text{clip}[\frac{r_{raw} - \mathbb{E}[r_{raw}]}{Z_c}, -1, 1]$) directly controls the positive/negative split. Errors here cascade into invalid $\Delta$ directions.

- Design tradeoffs:
  - $\beta$ (guidance strength): Higher $\beta \approx 1$ is stable but slower; lower $\beta \approx 0.1$ accelerates early progress but risks instability (Figure 10).
  - $\eta_i$ (soft update): Schedule $\eta_i = \min(0.001i, 0.5)$ balances speed/stability. Fixed $\eta=0$ collapses; $\eta=0.9$ is too slow (Figure 8).
  - Sampler choice: 2nd-order ODE slightly outperforms 1st-order ODE on GenEval; both outperform SDE on noise-sensitive rewards (Figure 7).

- Failure signatures:
  - **Reward collapse within 100-200 iterations**: Check if negative loss term is disabled or $\beta$ is too small.
  - **No improvement despite high rewards**: Likely $\eta$ too high (off-policy) or reward normalization $Z_c$ poorly calibrated.
  - **Visual artifacts/blur**: Check adaptive weighting—inverse strategies like $w(t) = 1-t$ cause collapse (Figure 9).

- First 3 experiments:
  1. **Sanity check on single reward**: Train on GenEval only with $\beta=1$, $\eta_i = \min(0.001i, 0.5)$, 10-step 1st-order ODE sampling. Target: >0.90 GenEval score within 500 iterations. If not achieved, check reward normalization.
  2. **Ablate negative loss**: Repeat experiment 1 with $(1-r)$ term removed. Expect near-instant collapse. This validates the negative-aware component.
  3. **Sampler comparison**: Compare 1st-order SDE vs. 1st-order ODE vs. 2nd-order ODE on PickScore (noise-sensitive). Expect ODE > SDE, confirming corpus-agnostic sampling benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DiffusionNFT's effectiveness generalize beyond text-to-image generation to other diffusion modalities such as video, 3D, and audio generation?
- Basis in paper: [explicit] The paper states: "We believe this paradigm offers a valid path toward a general, unified, and native off-policy RL recipe across various modalities" but only evaluates on image generation with SD3.5-Medium.
- Why unresolved: No experiments were conducted on other modalities. Video diffusion models have different temporal constraints, 3D generation involves different architectures (e.g., latent representations), and the forward process formulation may interact differently with these domains.
- What evidence would resolve it: Successful application of DiffusionNFT to video diffusion models (e.g., Sora-like architectures), 3D generation pipelines, and audio diffusion models with comparable efficiency gains over policy gradient baselines.

### Open Question 2
- Question: Why does negative data play a more critical role in diffusion RL than in LLM RL, where rejection fine-tuning (RFT) remains a strong baseline?
- Basis in paper: [explicit] "Without the negative policy loss on v⁻_θ, we find rewards collapse almost instantly during online training... This phenomenon is divergent from observations in LLMs, where RFT remains a strong baseline."
- Why unresolved: The paper empirically observes this difference but does not provide theoretical explanation for why the continuous nature of diffusion or the forward-process formulation makes negative signals essential.
- What evidence would resolve it: Theoretical analysis comparing the loss landscapes of diffusion vs. autoregressive RL, or ablation studies isolating which architectural/algorithmic factors cause this divergence.

### Open Question 3
- Question: Can the implicit policy parameterization technique be combined with CFG at inference time for additional quality gains, or does the learned guidance fully subsume CFG's benefits?
- Basis in paper: [inferred] The paper deliberately avoids CFG to demonstrate that RL can replace it, achieving superior performance without CFG. However, it does not explore whether combining the learned policy with CFG at inference could provide further improvements.
- Why unresolved: The paper positions DiffusionNFT as CFG-free and shows it outperforms CFG baselines, but does not test whether the techniques are complementary or mutually exclusive.
- What evidence would resolve it: Experiments applying CFG to DiffusionNFT-trained models at inference time across multiple guidance scales, measuring whether scores improve beyond CFG-free inference.

## Limitations
- **Unknown reward normalization constants (Z_c)**: The paper references global reward standard deviation for normalization but does not specify actual values for different rewards, creating a critical gap for exact reproduction.
- **Multi-reward stage durations**: While the paper mentions specific iteration counts per stage (800→300→200→200→100), the exact mapping of reward combinations to each stage and their relative weighting remains unclear.
- **Stability beyond 1k iterations**: The paper demonstrates strong results within 1k steps but does not report long-term stability or whether performance plateaus or degrades over extended training.

## Confidence
- **High Confidence**: The core mechanism of forward-process optimization via flow matching is theoretically sound and the efficiency improvements versus CFG-based methods are well-supported by ablation studies.
- **Medium Confidence**: The contrastive positive/negative sampling framework shows strong empirical results, but the theoretical guarantees assume unlimited data/capacity which may not hold in practice.
- **Medium Confidence**: The off-policy formulation with soft EMA updates demonstrates stability, but optimal hyperparameter schedules (β, η) may be task-specific rather than universally applicable.
- **Low Confidence**: The method's robustness to diverse reward functions and its behavior on out-of-distribution prompts are not thoroughly characterized.

## Next Checks
1. **Negative loss ablation verification**: Reproduce the claim that removing the negative policy loss causes reward collapse within 100-200 iterations by training with only the positive term.

2. **Sampler sensitivity test**: Compare SDE versus ODE sampling strategies on noise-sensitive rewards (PickScore) to validate the claim that ODE samplers are superior for data collection.

3. **Reward normalization calibration**: Systematically vary the normalization constant Z_c for GenEval and OCR rewards to determine the sensitivity of policy improvement to this hyperparameter.