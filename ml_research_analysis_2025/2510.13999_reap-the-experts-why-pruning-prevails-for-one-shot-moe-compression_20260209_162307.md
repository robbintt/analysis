---
ver: rpa2
title: 'REAP the Experts: Why Pruning Prevails for One-Shot MoE compression'
arxiv_id: '2510.13999'
source_url: https://arxiv.org/abs/2510.13999
tags:
- expert
- experts
- pruning
- reap
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of compressing Mixture-of-Experts
  (MoE) models, which have large memory overhead despite offering efficient training
  and low latency. It identifies a fundamental issue with expert merging: it causes
  "functional subspace collapse" by removing the router''s input-dependent control
  over experts, introducing irreducible error.'
---

# REAP the Experts: Why Pruning Prevails for One-Shot MoE compression

## Quick Facts
- arXiv ID: 2510.13999
- Source URL: https://arxiv.org/abs/2510.13999
- Reference count: 40
- One-line primary result: REAP consistently outperforms expert merging and other pruning methods on generative benchmarks, especially at 50% compression, by preserving the router's independent control over experts.

## Executive Summary
This paper addresses the challenge of compressing Mixture-of-Experts (MoE) models, which have large memory overhead despite offering efficient training and low latency. The authors identify a fundamental issue with expert merging: it causes "functional subspace collapse" by removing the router's input-dependent control over experts, introducing irreducible error. In contrast, expert pruning preserves the router's independent control, maintaining model quality. They propose Router-weighted Expert Activation Pruning (REAP), a novel saliency criterion that selects experts to prune based on both router gate-values and expert activation norms.

## Method Summary
The REAP method computes saliency scores for each expert by accumulating the product of router gate-values and expert activation norms over calibration data. Experts with lowest saliency scores are pruned, and router weights are redistributed among remaining experts. The approach uses domain-specific calibration datasets (1,024-12,228 samples depending on model size) and operates in a one-shot setting without fine-tuning. The method is evaluated across diverse MoE architectures ranging from 20B to 1T parameters.

## Key Results
- REAP consistently outperforms expert merging and other pruning methods on generative benchmarks, especially at 50% compression
- REAP achieves near-lossless compression on code generation and tool-calling tasks with Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts
- REAP maintains higher accuracy than baselines across coding, creative writing, mathematical reasoning, and MC benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Functional Subspace Collapse in Expert Merging
- **Claim:** Expert merging introduces irreducible error by causing "functional subspace collapse" due to loss of input-dependent router control.
- **Mechanism:** When experts are merged, their individual gates are summed and applied to a single merged expert, forcing approximation of a dynamic, input-dependent target expert with a static one. The error is proportional to router policy variability (Var[r(x)]) and expert functional difference (||Δ_ij||²).
- **Core assumption:** The router has learned meaningful input-dependent mixing policies between distinct experts.
- **Evidence anchors:** Theorem 1 proves minimal merging error = E[(g_i+g_j)²] · Var[r(x)] · ||Δ_ij||²
- **Break condition:** If the router policy has low variability (Var[r(x)] ≈ 0) or experts are functionally similar (||Δ_ij||² ≈ 0), merging error approaches zero.

### Mechanism 2: Router-Weighted Expert Activation Pruning (REAP)
- **Claim:** REAP preserves model quality by selecting experts based on their actual contribution to layer output, considering both router gates and activation norms.
- **Mechanism:** REAP computes saliency scores S_j = (1/|X_j|) Σ_{x∈X_j} g_j(x) · ||f_j(x)||_2, measuring an expert's average contribution magnitude when activated. Experts with lowest saliency are pruned, minimizing output deviation.
- **Core assumption:** Low-saliency experts contribute minimally to the layer's functional output and can be removed with minimal error.
- **Evidence anchors:** REAP consistently outperforms merging and other pruning methods on generative benchmarks, especially at 50% compression.
- **Break condition:** If calibration data is unrepresentative of the target domain, or if expert importance varies significantly across domains, REAP may select suboptimal experts.

### Mechanism 3: Independent Control Preservation in Pruning vs. Merging
- **Claim:** Pruning preserves the router's independent control over surviving experts, avoiding the functional collapse seen in merging.
- **Mechanism:** When pruning expert j, its gate-value is redistributed to remaining experts. Unlike merging, this does NOT tie gates together—the router still modulates each surviving expert independently. The error is proportional to the pruned expert's gate-value (E[g_j²]), not policy variability.
- **Core assumption:** The router can adaptively redistribute attention to remaining experts without fine-tuning.
- **Evidence anchors:** Equation 6 shows pruning error E[g_j(x)²||Δ_ij(x)||²] lacks the Var[r(x)] term present in merging error.
- **Break condition:** If pruned experts had significant unique functional roles not covered by remaining experts, quality degradation may occur.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Architecture**
  - Why needed here: Understanding SMoE architecture (router + experts, top-k gating) is essential to grasp why merging disrupts control flow while pruning doesn't.
  - Quick check question: Can you explain how top-k routing determines which experts are activated for a given input?

- **Concept: Functional Subspaces and Manifold Geometry**
  - Why needed here: The paper uses PCA to visualize functional subspaces; understanding how this relates to model capacity helps interpret the collapse phenomenon.
  - Quick check question: If two experts occupy distant regions in PCA space, what does that suggest about their functional roles?

- **Concept: Saliency Criteria for Pruning**
  - Why needed here: REAP is a specific saliency criterion; understanding the general concept helps compare it to alternatives like frequency-based or EAN pruning.
  - Quick check question: What are the trade-offs between using frequency vs. activation norms to determine expert importance?

## Architecture Onboarding

- **Component map:** Router produces gate-values (g_i(x)) for each expert based on input → Experts (f_i(x)) are specialized feed-forward networks → MoE Layer Output: h(x) = Σ_i g_i(x) f_i(x) combines router-gated expert outputs → Calibration Data provides domain-specific samples → Saliency Computation calculates S_j = (1/|X_j|) Σ_{x∈X_j} g_j(x) · ||f_j(x)||_2 → Pruning Decision removes experts with minimum S_j per layer → Router adjustment renormalizes gates for remaining experts.

- **Critical path:** 1. Calibration: Select representative domain data (e.g., evol-codealpaca for coding tasks) 2. Forward pass: Run calibration data through model, recording router gates and expert activations 3. Saliency calculation: Compute S_j for each expert per layer 4. Global or local pruning: Decide whether to prune uniformly per layer or globally based on aggregated saliency 5. Router adjustment: Renormalize gates for remaining experts.

- **Design tradeoffs:**
  - Global vs. local pruning: Global pruning may remove entire layers' experts more aggressively; local pruning preserves per-layer capacity but may keep redundant experts.
  - Calibration data size: More data improves saliency estimates but increases computation; paper uses 1,024–12,228 samples depending on model size.
  - Compression ratio: 50% compression shows REAP's advantages most clearly; 25% compression is near-lossless for most methods.

- **Failure signatures:**
  - Domain mismatch: Using c4 for code generation calibration causes accuracy collapse (Table A6).
  - High compression + weak saliency: Frequency-based pruning at 50% compression degrades severely (e.g., Qwen3-Coder-480B drops to 0% on EvalPlus).
  - Merging on generative tasks: Merged models show reduced N-gram diversity, higher cross-perplexity, and faster divergence from baseline logits (Figure 3).

- **First 3 experiments:**
  1. Calibration sensitivity: Compare REAP performance when calibrated on c4 vs. domain-specific data (evol-codealpaca) for a code generation model.
  2. Compression ratio sweep: Evaluate REAP at 25%, 50%, and 75% compression on multiple generative benchmarks to find acceptable limits.
  3. Pruning vs. merging comparison: Apply both REAP and HC-SMoE to the same model, comparing MC benchmarks (where merging is competitive) vs. generative benchmarks (where pruning excels).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can expert merging methods be redesigned to preserve input-dependent routing control and avoid functional subspace collapse?
- Basis in paper: [explicit] Authors state "the non-local nature of expert merging and high cardinality of expert clusters pose significant unresolved challenges" and prove merging introduces irreducible error due to loss of independent router modulation.
- Why unresolved: The paper demonstrates fundamental limitations of current merging approaches but does not propose solutions; restricting cluster cardinality failed to improve results.
- What evidence would resolve it: A merging variant that maintains comparable generative performance to REAP at 50% compression, or theoretical proof that no such variant exists.

### Open Question 2
- Question: Can REAP combined with fine-tuning recover or exceed baseline performance at extreme compression ratios (>50%)?
- Basis in paper: [inferred] The paper focuses on one-shot compression with no fine-tuning; fine-tuning was used in early pruning work but not evaluated with REAP.
- Why unresolved: Authors explicitly evaluate only the one-shot setting; potential gains from recovery training remain unknown.
- What evidence would resolve it: Experiments applying lightweight fine-tuning after REAP compression at 50%+ ratios, comparing against baseline and one-shot results.

### Open Question 3
- Question: How should calibration datasets be optimally constructed for domain-specific SMoE compression?
- Basis in paper: [explicit] Figure A7 shows c4 calibration causes "collapse in accuracy" for coding tasks; domain-specific data is "crucial for high-quality compressed SMoE accuracy at 50% compression."
- Why unresolved: The paper demonstrates importance empirically but provides no systematic methodology for calibration dataset selection or sizing.
- What evidence would resolve it: Ablation studies varying calibration dataset size, diversity, and domain alignment, with clear guidelines for practitioners.

## Limitations

- The theoretical analysis of functional subspace collapse relies on simplifying assumptions that may not hold for highly non-linear expert functions.
- Empirical validation focuses primarily on generative benchmarks with limited analysis of discriminative tasks or domain shift.
- Calibration data requirements (1,024-12,228 samples) represent non-trivial computational overhead not fully characterized for sensitivity.

## Confidence

**High Confidence:** The empirical superiority of REAP over expert merging at 50% compression on generative benchmarks is well-supported with multiple models and tasks. The observation that REAP preserves manifold geometry while merging causes collapse is demonstrated across different visualization methods.

**Medium Confidence:** The theoretical claim that merging introduces irreducible error due to functional subspace collapse is mathematically sound but may not fully capture real-world complexities. The assumption that router policies have meaningful input-dependent variability may not hold for all models or datasets.

**Low Confidence:** The generalizability of REAP across all MoE architectures and tasks remains incompletely tested. The paper doesn't thoroughly explore failure modes like extreme compression ratios (beyond 50%), domain adaptation challenges, or interactions with other compression techniques.

## Next Checks

1. **Domain Transfer Robustness:** Apply REAP to a model calibrated on general web data, then evaluate on specialized domains (e.g., legal or medical text) to quantify performance degradation and identify calibration requirements.

2. **Extreme Compression Analysis:** Systematically evaluate REAP at compression ratios from 10% to 90% on a diverse set of tasks to map the performance frontier and identify the point where functional collapse begins regardless of pruning method.

3. **Multi-Task Calibration Strategy:** Compare single-domain calibration (current approach) against multi-domain calibration datasets to determine if REAP can maintain quality across heterogeneous workloads without task-specific fine-tuning.