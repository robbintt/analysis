---
ver: rpa2
title: 'Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry:
  Benchmarking Deep Stereo Matching Methods'
arxiv_id: '2601.19461'
source_url: https://arxiv.org/abs/2601.19461
tags:
- stereo
- methods
- defom
- depth
- igev
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first systematic zero-shot evaluation of
  deep stereo matching methods for UAV forestry applications. The research addresses
  the critical gap in existing evaluations, which focus on urban and indoor scenarios
  while leaving vegetation-dense environments unexplored.
---

# Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods

## Quick Facts
- arXiv ID: 2601.19461
- Source URL: https://arxiv.org/abs/2601.19461
- Reference count: 32
- Primary result: DEFOM established as gold-standard baseline for vegetation depth estimation with superior cross-domain consistency

## Executive Summary
This study presents the first systematic zero-shot evaluation of deep stereo matching methods for UAV forestry applications. Eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms were evaluated using Scene Flow pretrained weights across four standard benchmarks plus a novel Canterbury Tree Branches dataset. The evaluation revealed scene-dependent performance patterns: foundation models excelled on structured scenes while iterative methods showed variable cross-benchmark performance. DEFOM was established as the gold-standard baseline for vegetation depth estimation based on superior cross-domain consistency and will serve as pseudo-ground-truth for future benchmarking.

## Method Summary
The study evaluated eight deep stereo matching methods using officially released Scene Flow pretrained weights without fine-tuning. Methods included iterative refinement (IGEV++, IGEV), foundation models (BridgeDepth, DEFOM), diffusion-based (StereoAnywhere), attention-based (ACVNet), and classical 3D CNN (PSMNet) approaches. Evaluation was conducted across four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) and a novel 5,313-pair Canterbury Tree Branches dataset (1920×1080 resolution). Metrics included End-Point Error (EPE) and D1-Error, with cross-benchmark ranking consistency used to select DEFOM as the gold-standard baseline for generating pseudo-ground-truth on the Tree Branches dataset.

## Key Results
- DEFOM achieved superior cross-domain consistency with average rank 1.75 across four benchmarks
- Foundation models excelled on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 4.65 px on Middlebury)
- Iterative methods showed variable performance (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury)
- Classical 3D CNN (PSMNet) failed catastrophically on Middlebury with 54.42% D1 failure rate

## Why This Works (Mechanism)

### Mechanism 1: Monocular Depth Priors for Cross-Domain Generalization
Foundation models achieve superior cross-domain consistency by leveraging monocular depth priors trained on diverse real-world scenes. DEFOM incorporates Depth Anything V2 into recurrent stereo matching with scale update modules, providing semantic context that resolves ambiguities in textureless foliage regions. Cross-benchmark evidence shows DEFOM achieving best ranked consistency (average rank 1.75 across 4 benchmarks) with lowest CV (0.58).

### Mechanism 2: Iterative Geometry Encoding for Fine Detail Preservation
Iterative refinement methods preserve finer branch details through multi-range geometry encoding volumes, though exhibiting higher variance across domains. IGEV++ extends geometry encoding with multi-range processing to capture large disparities while preserving fine-grained details. However, performance varies significantly across benchmarks (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury).

### Mechanism 3: Pseudo-Ground-Truth Generation via Cross-Domain Validation
DEFOM's consistent top-tier performance across diverse benchmarks validates its use for generating pseudo-ground-truth where LiDAR annotation is infeasible. The paper uses zero-shot performance on established benchmarks as a proxy for generalization capability. DEFOM's lack of catastrophic failures (unlike BridgeDepth's 20.03 px on Middlebury) makes it a safer choice for unseen domains.

## Foundational Learning

- **Concept: Stereo Matching and Disparity Estimation**
  - Why needed here: The paper assumes familiarity with disparity-to-depth conversion via triangulation and the distinction between EPE and D1 metrics
  - Quick check question: Given a stereo camera with 63mm baseline and focal length of 1000 pixels, what depth corresponds to a disparity of 50 pixels? (Answer: 1.26m)

- **Concept: Zero-Shot Cross-Domain Generalization**
  - Why needed here: The evaluation methodology relies on zero-shot inference to isolate inherent generalization capability
  - Quick check question: Why is zero-shot evaluation preferred over domain-specific fine-tuning for selecting a pseudo-ground-truth generator? (Answer: Fine-tuning requires target-domain labels, which are unavailable for vegetation scenes where LiDAR cannot penetrate canopy.)

- **Concept: Foundation Model Integration**
  - Why needed here: DEFOM's architecture combines monocular depth foundation models with stereo matching
  - Quick check question: What failure mode in vegetation scenes might monocular priors help resolve? (Answer: Textureless foliage regions create ambiguous stereo correspondences; monocular semantic understanding can disambiguate.)

## Architecture Onboarding

- **Component map:**
  - Input stereo pair -> Feature extraction -> Correlation volume construction -> Iterative refinement loop -> Disparity output -> Scale update modules (DEFOM only) -> Depth conversion

- **Critical path:**
  1. Load Scene Flow pretrained weights
  2. Input 1920×1080 rectified stereo pairs
  3. Forward pass through selected architecture
  4. Extract disparity map
  5. Convert to depth via calibration parameters

- **Design tradeoffs:**
  - DEFOM: Best cross-domain consistency (rank 1.75), smooth predictions, fewer artifacts → may blur fine branch edges
  - IGEV++: Finer detail preservation, better outlier control (D1: 7.82%) → higher variance across domains and noise in homogeneous regions
  - BridgeDepth: Lowest error on structured scenes (0.23 px ETH3D) → catastrophic failure on large disparities (20.03 px Middlebury)
  - PSMNet: Simple deployment → 54.42% D1 failure rate on Middlebury; unsuitable for forestry

- **Failure signatures:**
  - Fixed disparity range: PSMNet's 192-pixel limit causes failures on large disparities
  - Domain-specific optimization: BridgeDepth's latent alignment degrades outside moderate disparity ranges
  - Textureless regions: Iterative methods exhibit noise in homogeneous foliage areas
  - Occlusion boundaries: IGEV++ produces artifacts near thin branch structures

- **First 3 experiments:**
  1. Reproduce benchmark rankings: Run all 8 methods on ETH3D and KITTI 2015 with Scene Flow weights. Validate DEFOM inference produces EPE ≤0.40 px on ETH3D.
  2. Qualitative vegetation assessment: Apply DEFOM and IGEV++ to 10 diverse Tree Branches samples. Compare sky-region consistency, thin branch preservation, and noise in homogeneous foliage.
  3. Pseudo-ground-truth validation protocol: Select 5 scenes where physical depth measurement is possible. Compare DEFOM predictions against ground truth to quantify proxy validity gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurately do DEFOM pseudo-ground-truth depth predictions correlate with actual LiDAR measurements in vegetation-dense canopy environments?
- Basis in paper: Future work will validate against LiDAR measurements on selected scenes
- Why unresolved: Tree Branches dataset lacks ground-truth depth maps; DEFOM was selected based on cross-domain consistency and qualitative visual assessment only
- What evidence would resolve it: Correlation analysis between DEFOM predictions and LiDAR scans collected on a subset of forestry scenes with canopy penetration

### Open Question 2
- Question: What are the computational requirements and inference latency of foundation model-based stereo matching (DEFOM) for real-time embedded UAV deployment?
- Basis in paper: Computational profiling for embedded UAV deployment listed as future direction; inference speed not considered
- Why unresolved: Resource-constrained aerial platforms require balancing accuracy against power consumption and latency
- What evidence would resolve it: FPS, memory footprint, and power consumption measurements on typical UAV compute hardware across varying input resolutions

### Open Question 3
- Question: How can prediction uncertainty be quantified for stereo depth estimation in safety-critical autonomous pruning operations?
- Basis in paper: Uncertainty quantification for safety-critical applications listed as future direction
- Why unresolved: Centimeter-level depth accuracy is required for tool positioning at 1–2m operating distances, yet current methods provide point estimates without confidence bounds
- What evidence would resolve it: Uncertainty-aware stereo methods evaluated on failure cases with calibrated confidence intervals

### Open Question 4
- Question: Would multi-dataset pretraining improve zero-shot generalization to vegetation scenes beyond the Scene Flow synthetic training used in this study?
- Basis in paper: Authors note "Single-dataset training may not represent optimal generalization" as a limitation
- Why unresolved: Scene Flow is synthetic and lacks vegetation; monocular depth priors help but dedicated multi-domain stereo pretraining remains unexplored
- What evidence would resolve it: Comparative evaluation of models pretrained on Scene Flow + CREStereo Dataset + Falling Things, then tested zero-shot on Tree Branches and standard benchmarks

## Limitations
- The proxy validity of cross-benchmark consistency for vegetation-specific performance remains unvalidated until LiDAR comparisons are completed
- Tree Branches dataset evaluation is currently qualitative only, with pseudo-ground-truth generation based on relative cross-domain rankings
- Evaluation does not account for varying illumination conditions or seasonal foliage changes that affect UAV forestry applications

## Confidence
- Cross-benchmark consistency findings: High (directly measurable and reproducible)
- Foundation model superiority claims: Medium (supported by rankings but limited by lack of vegetation-specific ground truth)
- Pseudo-ground-truth methodology: Low (validity cannot be verified until LiDAR validation is completed)

## Next Checks
1. Conduct LiDAR validation on 50 Tree Branches samples to quantify the proxy validity gap between cross-benchmark consistency and absolute accuracy in vegetation scenes
2. Test all 8 methods under varying illumination conditions (backlit, overcast, direct sun) to assess robustness beyond controlled benchmark environments
3. Evaluate cross-seasonal generalization by applying DEFOM to winter and summer foliage samples to verify consistent performance across leaf-on and leaf-off conditions