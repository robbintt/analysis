---
ver: rpa2
title: In-Context Learning for Label-Efficient Cancer Image Classification in Oncology
arxiv_id: '2505.08798'
source_url: https://arxiv.org/abs/2505.08798
tags:
- paligemma
- gpt-4o
- align
- shot
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of in-context learning
  (ICL) using vision-language models (VLMs) for cancer image classification. The core
  idea is to adapt VLMs to new diagnostic tasks using only a few labeled examples
  at inference, without the need for retraining.
---

# In-Context Learning for Label-Efficient Cancer Image Classification in Oncology
## Quick Facts
- arXiv ID: 2505.08798
- Source URL: https://arxiv.org/abs/2505.08798
- Reference count: 37
- Primary result: VLMs achieve meaningful classification performance without retraining using few-shot prompting

## Executive Summary
This study explores the application of in-context learning (ICL) with vision-language models (VLMs) for cancer image classification in oncology settings. The research evaluates four VLMs—GPT-4o, Paligemma, CLIP, and ALIGN—across three oncology datasets without any parameter updates, demonstrating significant performance gains through few-shot prompting. The findings suggest that VLMs can approximate task-specific behavior using only a handful of examples, with open-source models showing competitive performance despite smaller sizes. This approach offers a promising pathway for label-efficient cancer detection in resource-constrained clinical environments.

## Method Summary
The study employs in-context learning by providing VLMs with a small number of labeled examples during inference to adapt them to new diagnostic tasks without retraining. Four VLMs were evaluated across three distinct oncology datasets: MHIST for colorectal polyps, PatchCamelyon for breast cancer metastasis, and HAM10000 for skin lesions. The models received prompts containing few-shot examples and were assessed using standard classification metrics including F1 scores. The experimental design compared binary and multi-class classification performance while maintaining the models' original parameters, focusing on the efficiency gains from ICL versus traditional fine-tuning approaches.

## Key Results
- GPT-4o achieved an F1 score of 0.81 in binary classification and 0.60 in multi-class classification using few-shot prompting
- All evaluated VLMs showed significant performance gains without any parameter updates
- Open-source models (Paligemma and CLIP) demonstrated competitive performance despite smaller model sizes

## Why This Works (Mechanism)
In-context learning leverages the pre-trained knowledge embedded in VLMs by providing task-relevant examples within the input prompt. During inference, these models use the few-shot examples to construct a contextual understanding of the target classification task, effectively adapting their behavior without modifying underlying parameters. The vision-language architecture enables simultaneous processing of image features and textual labels, allowing the model to map visual patterns to diagnostic categories through the provided examples. This approach capitalizes on the model's general visual understanding while the textual prompts guide task-specific reasoning, enabling rapid adaptation to new oncology classification tasks with minimal labeled data.

## Foundational Learning
- **In-context learning**: The ability of language models to learn new tasks from examples provided in the prompt without parameter updates; needed because it enables rapid adaptation without costly retraining
- **Vision-language models**: Neural architectures that process both visual and textual information simultaneously; needed because oncology diagnosis requires understanding both image features and diagnostic labels
- **Few-shot learning**: Training or adapting models using only a small number of labeled examples; needed because medical datasets often have limited annotations due to expert time constraints
- **Transfer learning**: Applying knowledge gained from one domain to improve performance in another; needed because pre-trained VLMs have learned general visual features applicable to medical imaging
- **Prompt engineering**: Designing input prompts to elicit desired model behaviors; needed because the quality of few-shot examples directly impacts classification performance
- **Multi-modal embeddings**: Representations that capture both visual and textual information in a shared space; needed because the model must align image features with diagnostic labels

## Architecture Onboarding
- **Component map**: Input prompt containing few-shot examples → Vision encoder → Cross-attention layers → Language decoder → Output classification
- **Critical path**: Prompt examples → Visual feature extraction → Contextual understanding → Label prediction
- **Design tradeoffs**: Model size vs. computational efficiency, few-shot examples vs. performance, open-source vs. proprietary capabilities
- **Failure signatures**: Poor example selection leading to incorrect context, domain shift between training and target datasets, insufficient few-shot examples for complex tasks
- **First experiments**: 1) Vary number of few-shot examples (1-32) to identify optimal sample size, 2) Test different prompt structures and formatting, 3) Evaluate performance across different cancer types to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Specific baseline performance metrics for fully fine-tuned systems are not provided, making performance gaps difficult to contextualize
- Evaluation is limited to three datasets covering only three cancer types, restricting generalizability to broader oncology applications
- Clinical validation aspects such as radiologist agreement and false positive/negative patterns are not addressed

## Confidence
- VLMs achieving meaningful classification performance without retraining: **Medium**
- Open-source models showing competitive few-shot performance: **Medium**
- Deployment feasibility in computing-constrained environments: **Low**

## Next Checks
1. Conduct head-to-head comparisons with state-of-the-art fine-tuned models using identical train/test splits and metrics
2. Evaluate model performance across a broader range of oncology imaging modalities and disease types, including rare cancers and early-stage lesions
3. Measure actual inference time and resource requirements for open-source models in clinical-grade hardware to validate deployment claims