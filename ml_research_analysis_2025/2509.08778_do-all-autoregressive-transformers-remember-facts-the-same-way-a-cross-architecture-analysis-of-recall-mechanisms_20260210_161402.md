---
ver: rpa2
title: Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture
  Analysis of Recall Mechanisms
arxiv_id: '2509.08778'
source_url: https://arxiv.org/abs/2509.08778
tags:
- factual
- attention
- layers
- recall
- effects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether factual recall mechanisms in autoregressive
  Transformers generalize across architectures. By extending causal tracing to GPT,
  LLaMA, Qwen, and DeepSeek models, it reveals that while GPT/LLaMA store facts in
  early MLP layers, Qwen/DeepSeek localize recall to early attention layers.
---

# Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms

## Quick Facts
- **arXiv ID**: 2509.08778
- **Source URL**: https://arxiv.org/abs/2509.08778
- **Reference count**: 24
- **Primary result**: Factual recall mechanisms differ across autoregressive Transformers; GPT/LLaMA store facts in early MLP layers while Qwen/DeepSeek localize to early attention layers

## Executive Summary
This study reveals that factual recall mechanisms in autoregressive Transformers are not universal but depend on architecture. Through causal tracing experiments across GPT, LLaMA, Qwen, and DeepSeek models, the authors demonstrate that while GPT and LLaMA store factual associations in early MLP layers, Qwen and DeepSeek localize recall to early attention layers. This divergence is quantified through restoration effects, severing effects, and knockout experiments, with semantic similarity-based evaluation revealing that standard string matching underestimates model performance. The findings indicate that factual recall localization is architecture-dependent, with significant implications for model editing, interpretability, and compression.

## Method Summary
The study employs causal tracing to identify where factual associations are stored in transformer models. The methodology involves running clean inference on factual sentences, then corrupting subject embeddings with noise and measuring the Average Indirect Effect (AIE) when restoring specific layer/module activations. For Qwen and DeepSeek models where severing effects are insufficient, knockout experiments (zeroing module outputs) verify the necessity of attention layers. Factual recall is evaluated using semantic similarity (Sentence-BERT embeddings with τ=0.7 threshold) against BM25-derived candidate objects from Wikipedia, rather than exact string matching. The analysis covers four model families at 1.5B parameters, focusing on the last subject token position.

## Key Results
- GPT and LLaMA models show highest AIE values in early MLP layers, while Qwen and DeepSeek exhibit high AIE in early attention layers
- Severing highly concentrated attention layers in Qwen/DeepSeek results in minimal AIE reduction, requiring knockout experiments for accurate assessment
- Attention-based architectures show lower Gini coefficients, indicating more distributed factual storage compared to MLP-based localization
- Semantic similarity evaluation reveals significant performance differences compared to string matching, particularly for larger models

## Why This Works (Mechanism)

### Mechanism 1: Architecture-Dependent Factual Localization
- **Claim:** Factual associations are not uniformly stored in early MLP layers across all autoregressive Transformers; localization depends on the specific model family (e.g., GPT vs. Qwen).
- **Mechanism:** The model processes subject tokens through layers. In GPT/LLaMA, the Average Indirect Effect (AIE) peaks at early MLP layers, suggesting facts are stored there. In contrast, Qwen/DeepSeek show high AIE in early Attention layers, indicating a shift in storage strategy.
- **Core assumption:** The "restoration effect" (restoring a clean hidden state in a corrupted run) accurately reveals the causal importance of that state.
- **Evidence anchors:**
  - [abstract] "Qwen/DeepSeek localize recall to early attention layers."
  - [Section 5.1] "In GPT and LLaMA, AIE values are elevated in the early MLP layers... Qwen and DeepSeek exhibit notably high AIE values in the early layers of the Attention module."
  - [corpus] "Constructing Efficient Fact-Storing MLPs for Transformers" (supports the GPT/LLaMA baseline, making the Qwen divergence notable).

### Mechanism 2: Redundant Routing in Attention vs. Isolated MLP
- **Claim:** Attention modules may have high causal importance (restoration) but resist suppression via "severing" (noise injection) due to redundant information pathways, requiring "knockout" (zeroing) for verification.
- **Mechanism:** In Qwen, restoring clean attention states improves prediction (high AIE), but severing (replacing with noise) fails to drop AIE significantly. This suggests information flows through multiple attention heads or paths. Only completely zeroing out the module (knockout) reliably drops the "objects rate," confirming the module's role.
- **Core assumption:** "Severing" introduces noise that the network can bypass or denoise, whereas "knockout" removes the signal entirely.
- **Evidence anchors:**
  - [Section 5.2] "In Qwen and DeepSeek... severing these layers results in only a minimal decrease... information can propagate through alternative paths."
  - [Section 5.3] "In Qwen... blocking the early Attention layers also results in a substantial drop [in objects rate]."

### Mechanism 3: Semantic-Similarity-Based Validation
- **Claim:** Standard string matching fails to capture factual recall in models with high lexical variance; semantic similarity thresholds are required for accurate causal measurement.
- **Mechanism:** Large models often output synonyms or valid paraphrases of the expected object "o". The paper uses Sentence-BERT embeddings and a cosine similarity threshold (τ=0.7) to identify valid predictions ("Objects Rate") rather than exact string equality.
- **Core assumption:** Semantic similarity scores correlate linearly with the factual validity of a token in this context.
- **Evidence anchors:**
  - [Section 4.3] "Tokens that exceed a predefined similarity threshold τ with any element in O are regarded as semantically valid."
  - [Appendix B] Demonstrates that pairs like "movie/cinema" (0.68) are below threshold while "city/urban" (0.86) are accepted.

## Foundational Learning

- **Concept:** **Causal Tracing / Indirect Effect**
  - **Why needed here:** This is the primary tool for identifying "where" facts live. You cannot interpret the heatmaps (AIE) without understanding that we are measuring the difference in probability (P[o]) between a corrupted state and a restored state.
  - **Quick check question:** If restoring a hidden state increases the probability of the correct object from 5% to 80%, is the Indirect Effect high or low?

- **Concept:** **Residual Stream Architecture**
  - **Why needed here:** The paper analyzes interactions between the residual stream, Attention output (a_i), and MLP output (m_i). Understanding Equation (2) (h = h + a + m) is essential to grasp why "knocking out" a or m prevents information from flowing to the final layer.
  - **Quick check question:** In a standard Transformer, does the output of layer l's MLP directly become the input to layer l+1's Attention, or does it write to a shared stream?

- **Concept:** **Gini Coefficient**
  - **Why needed here:** The paper uses Gini coefficients to quantify whether causal effects are "concentrated" in specific layers or spread out. High Gini = sparse/localized mechanism; Low Gini = distributed mechanism.
  - **Quick check question:** A model with a Gini coefficient of 0.9 for Attention likely stores facts in a few specific layers or evenly across all layers?

## Architecture Onboarding

- **Component map:**
  - Input: Subject/Relation tokens (s, r)
  - Modules: Attention (moves info between tokens) vs. MLP (processes info per token)
  - Interventions: Corruption (add noise), Restoration (patch clean activations), Severing (patch noise), Knockout (zero outputs)
  - Metrics: AIE (probability delta), Objects rate (semantic accuracy)

- **Critical path:**
  1. Run **Restoration** to find high-AIE layers (Candidates)
  2. Run **Severing** to test if those candidates are brittle (works for MLP)
  3. If Severing fails (Attention), run **Knockout** to confirm necessity
  4. Calculate **Gini** to see if storage is localized or diffuse

- **Design tradeoffs:**
  - **String Match vs. Semantic Sim:** String match is faster but underestimates capability. Semantic sim (Sentence-BERT) is robust but adds compute overhead and threshold dependency
  - **Severing vs. Knockout:** Severing preserves the activation magnitude (just shifts it), potentially allowing residual networks to adapt. Knockout destroys information entirely, giving a clearer "necessity" signal but being more destructive to the circuit

- **Failure signatures:**
  - **High Restoration, Low Severing Drop:** Do NOT conclude the module is unimportant. This is the signature of redundant pathways (common in Attention). Switch to Knockout experiments
  - **Low Objects Rate with High Probability:** The model is confident but lexically different. Ensure evaluation uses semantic similarity, not exact match

- **First 3 experiments:**
  1. **Baseline AIE Scan:** Run causal tracing (restoration) on the target model to identify if it is "MLP-heavy" (GPT-style) or "Attention-heavy" (Qwen-style)
  2. **Gini Concentration Analysis:** Compute Gini coefficients on the AIE distribution to quantify how localized the factual recall is
  3. **Knockout Verification:** Apply 5-layer rolling knockout to the identified critical region (e.g., Layers 0-4 Attention for Qwen) and measure the drop in "Objects Rate" using semantic matching

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific interaction of architectural design choices and training objectives causes factual recall to localize in early Attention layers in some architectures (Qwen/DeepSeek) but early MLP layers in others (GPT/LLaMA)?
- **Basis in paper:** [explicit] Section 6 states "the underlying causes of these architectural divergences remain unclear," and Appendix D concludes that localization arises from "complex interactions... coupled with training objectives... which warrant further investigation."
- **Why unresolved:** The authors analyzed static factors (hidden size, head count, vocabulary) in Appendix D but found no single variable explains the observed localization shift; they did not analyze training dynamics.
- **Evidence:** Ablation studies on training data distributions or optimization dynamics that isolate specific architectural variants to see if they force a shift in storage locus.

### Open Question 2
- **Question:** Why do causal tracing "severing effects" fail to accurately reflect the contribution of Attention modules in Qwen/DeepSeek models, necessitating a knockout approach?
- **Basis in paper:** [inferred] Section 5.2 notes that severing highly concentrated Attention layers results in only minimal AIE reduction, while Section 5.3 confirms the same layers are critical via knockout.
- **Why unresolved:** The paper hypothesizes that information propagates through alternative paths in Attention, avoiding the corrupted route, but does not empirically map these redundant circuits.
- **Evidence:** Circuit-level analysis of Qwen's early attention layers to identify redundant paths or distributed representations that resist corruption-based interventions.

### Open Question 3
- **Question:** How does the localization of factual recall in Attention layers (vs. MLPs) impact the efficacy and persistence of targeted model editing techniques?
- **Basis in paper:** [explicit] Section 6 suggests "knowledge editing methods... should adapt accordingly," and the Limitations section states that understanding storage locations is essential for "enabling targeted model editing."
- **Why unresolved:** While the paper identifies where facts are stored, it does not implement or validate specific editing methods (e.g., ROME adaptations) for Attention-centric architectures.
- **Evidence:** Comparative evaluation of standard MLP-focused editing techniques against new Attention-focused techniques on Qwen models, measuring edit success and generalization.

## Limitations
- The study focuses on factual recall at the last subject token position, which may not represent all knowledge retrieval scenarios
- The noise injection mechanism (σ_sub) and its exact implementation could affect causal tracing results
- The semantic similarity threshold (τ=0.7) is arbitrary and could influence objects rate measurements

## Confidence
- **High confidence**: GPT/LLaMA MLP localization pattern (supported by restoration/severing consistency)
- **High confidence**: Qwen/DeepSeek Attention localization (confirmed through knockout experiments)
- **Medium confidence**: The semantic similarity threshold captures valid factual predictions (though threshold choice is arbitrary)
- **Medium confidence**: Architecture-dependent mechanisms generalize across model sizes within families

## Next Checks
1. **Cross-dataset validation**: Apply the same causal tracing methodology to factual recall tasks from different knowledge sources (e.g., Wikidata, TriviaQA) to test if architectural patterns hold beyond COUNTERFACT

2. **Threshold sensitivity analysis**: Systematically vary the semantic similarity threshold (τ) from 0.5 to 0.9 to quantify how robust the objects rate measurements are to this critical parameter

3. **Hybrid architecture testing**: Test models with mixed architectural components (e.g., GPT with additional attention modules) to identify whether localization patterns shift gradually or exhibit discrete changes