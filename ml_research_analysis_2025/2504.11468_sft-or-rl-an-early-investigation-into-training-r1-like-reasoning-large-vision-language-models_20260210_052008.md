---
ver: rpa2
title: SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language
  Models
arxiv_id: '2504.11468'
source_url: https://arxiv.org/abs/2504.11468
tags:
- reasoning
- arxiv
- reward
- answer
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the role of SFT and RL in training reasoning-capable
  LVLMs. We introduce VLAA-Thinking, a multimodal dataset designed for both SFT and
  RL, and conduct extensive experiments comparing their effects.
---

# SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models

## Quick Facts
- arXiv ID: 2504.11468
- Source URL: https://arxiv.org/abs/2504.11468
- Reference count: 40
- Primary result: VLAA-Thinker-Qwen2.5VL-3B achieves top-1 on Open LMM Reasoning Leaderboard among 4B-scale models, surpassing prior state-of-the-art by 1.8%

## Executive Summary
This work investigates the role of SFT and RL in training reasoning-capable Large Vision-Language Models. Through extensive experiments on a novel multimodal dataset VLAA-Thinking, the authors demonstrate that SFT can induce "pseudo reasoning paths" that hinder genuine reasoning development, while RL with mixed reward fosters more adaptive reasoning behavior. Their VLAA-Thinker-Qwen2.5VL-3B achieves state-of-the-art performance on the Open LMM Reasoning Leaderboard among 4B-scale models.

## Method Summary
The authors introduce VLAA-Thinking, a multimodal dataset designed for both SFT and RL, constructed through a 6-step pipeline involving captioning, R1 distillation, rewriting, and verification. They conduct controlled experiments comparing SFT, GRPO, and SFT-then-GRPO approaches. The RL training uses a novel mixed reward module combining rule-based verification (digit matching, MCQ, math verification, IoU) with model-based scoring for open-ended cognition. Experiments are conducted on Qwen2-VL and Qwen2.5-VL models ranging from 2B to 7B parameters, with VLAA-Thinker-Qwen2.5VL-3B achieving top-1 performance on the Open LMM Reasoning Leaderboard.

## Key Results
- SFT on reasoning traces containing "aha moments" degrades performance by 10.5% on average
- SFT before GRPO training performs worse than GRPO alone, with an average drop of 8.9%
- Mixed reward design achieves best overall performance, with 6.2% average improvement over baseline
- VLAA-Thinker-Qwen2.5VL-3B achieves top-1 on Open LMM Reasoning Leaderboard among 4B-scale LVLMs

## Why This Works (Mechanism)

### Mechanism 1: SFT Induces Pseudo Reasoning Paths That Constrain Exploration
SFT forces models to imitate expert reasoning trajectories that may contain hesitant, prolonged, or incorrect intermediate steps. This locks models into rigid reasoning modes that limit exploration during subsequent RL, reducing the "upper bound" of what can be learned. The quality of distilled reasoning traces from R1 is uneven, and imitation learning captures surface patterns without underlying reasoning competence.

### Mechanism 2: Mixed Reward Module Integrates Perception and Cognition Signals
A heterogeneous reward function combining rule-based verification (for perception) and model-based scoring (for open-ended cognition) enables more effective GRPO training. Rule-based rewards provide sparse but reliable binary signals for verifiable outputs, while the open-ended reward model provides dense scores for free-form reasoning. The implicit format reward eliminates the need for separate format enforcement.

### Mechanism 3: Direct GRPO Bypasses SFT-Induced Mode Collapse
Applying GRPO directly to aligned base models without prior SFT yields superior reasoning performance. SFT constrains the policy to a narrow region around imitation targets, reducing the entropy of initial rollouts. Direct GRPO allows the model to discover its own reasoning pathways through reward-guided exploration.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed: GRPO is the core RL algorithm that estimates advantages by comparing outputs within a group, making it more sample-efficient for reasoning tasks
  - Quick check: Can you explain why GRPO samples multiple outputs per query and how the advantage is computed relative to the group mean?

- **Reward Hacking and Its Mitigation**
  - Why needed: The paper explicitly addresses reward hacking through mixed reward design and implicit format rewards
  - Quick check: What is reward hacking, and how does normalizing open-ended rewards to [0,1] alongside rule-based rewards partially mitigate it?

- **Chain-of-Thought Distillation from Teacher Models**
  - Why needed: The VLAA-Thinking dataset is constructed by distilling R1's reasoning traces, and understanding distillation limitations explains why pseudo reasoning emerges
  - Quick check: What are the failure modes when distilling chain-of-thought from a stronger model, and how might the student model's capacity affect distillation quality?

## Architecture Onboarding

- **Component map:**
  Raw image-text pairs → Captioning (GPT-4o) → Reasoning distillation (DeepSeek-R1) → Rewriting (GPT-3.5) → Verification (GPT-based) → Split curation (aha-moment filtering) → SFT path (126K, no aha moments) OR RL path (25K, with aha moments) → GRPO with Mixed Reward → VLAA-Thinker model

- **Critical path:**
  1. Start with an aligned base model (e.g., Qwen2.5-VL-Inst, NOT the raw base model)
  2. Skip SFT entirely on reasoning data—do not apply VLAA-Thinking-SFT
  3. Configure GRPO with the mixed reward module; sample 4 outputs per query at temperature 0.8
  4. Train for ~50 steps with rollout batch size 512, training batch size 256
  5. Evaluate on MathVista, MathVision, MathVerse, DynaMath, WeMath, LogicVista

- **Design tradeoffs:**
  - More RL data vs. quality: The RL split is smaller (25K) but harder (aha-moment samples). Curation quality matters more than scale
  - Rule-based vs. open-ended rewards: Rule-based rewards are reliable but sparse; open-ended rewards are dense but may introduce noise. The paper finds combining both is optimal
  - KL penalty magnitude: Smaller KL (1e-3 initial, 5e-4 target) encourages more policy evolution vs. larger KL which stabilizes but may underfit

- **Failure signatures:**
  - SFT+GRPO degradation: If your model performs worse after GRPO than GRPO-only, you likely applied SFT on reasoning traces first. Solution: remove SFT
  - Reward spike without performance gain: If reward curves rise but benchmark scores don't, the model may be exploiting the reward model. Solution: verify reward model calibration, increase rule-based reward weight
  - Loss spikes during SFT: The paper notes loss spikes on Qwen2-VL-7B with 25K SFT causing collapse. Solution: reduce learning rate or skip SFT

- **First 3 experiments:**
  1. Baseline sanity check: Run GRPO-only on VLAA-Thinking-RL with Qwen2.5-VL-3B, using only rule-based rewards. Verify you can reproduce the ~35% average score on math benchmarks
  2. Ablate mixed reward: Train with rule-based rewards only vs. mixed rewards. Confirm the paper's ~6.2% improvement from adding open-ended rewards
  3. Verify SFT incompatibility: Train SFT+GRPO vs. GRPO-only with identical hyperparameters. You should observe the ~8-13% performance drop when SFT is prepended

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SFT be redesigned or reformulated to be compatible with RL training in multimodal reasoning, or is the incompatibility fundamental?
- Basis: The paper states "SFT is NOT compatible with GRPO in multimodal reasoning" but uses "currently" suggesting potential for future resolution
- Why unresolved: Experiments only test standard SFT approaches; alternative formulations were not explored
- What evidence would resolve it: Experiments with alternative SFT methodologies that preserve model plasticity while still providing useful priors

### Open Question 2
- Question: Do the findings that SFT degrades reasoning and GRPO-only training is superior generalize to larger model scales (e.g., 70B+ parameters)?
- Basis: All experiments are conducted on 2B-7B models. The paper notes "larger models cannot immune from the degeneration brought by SFT" but only tests up to 7B
- Why unresolved: Training dynamics and the balance between imitation and exploration may differ substantially at larger scales
- What evidence would resolve it: Replication on models at 30B, 70B, or larger scales

### Open Question 3
- Question: What mechanisms underlie the emergence of authentic "aha moments" during RL training, and can they be reliably induced or amplified?
- Basis: The paper observes "authentic aha moments" emerging from GRPO without SFT, but notes "the number of aha moments is not directly correlate with overall model performance"
- Why unresolved: The paper demonstrates emergence but does not explain why some models develop more self-reflective behavior
- What evidence would resolve it: Analysis of training dynamics correlating specific reward signals, curriculum designs, or hyperparameters with aha moment frequency and quality

## Limitations

- The central claim that SFT universally undermines RL reasoning performance has important caveats and may depend on data curation quality
- The mixed reward design relies on a single open-ended reward model (InternLM-XComposer2.5-Reward), limiting generalizability
- Experiments focus on math and reasoning benchmarks, leaving unclear whether findings extend to broader multimodal reasoning tasks

## Confidence

- **High Confidence**: The empirical finding that SFT on reasoning traces with "aha moments" degrades performance (10.5% average drop), and that GRPO-only outperforms SFT-then-GRPO (8.9% average drop)
- **Medium Confidence**: The mechanism that SFT induces "pseudo reasoning paths" constraining exploration is plausible but relies on qualitative interpretations
- **Medium Confidence**: The mixed reward design's superiority (6.2% improvement) is demonstrated, but specific contribution of each component is not fully isolated

## Next Checks

1. **Ablate SFT data quality**: Train separate models with SFT on only high-quality reasoning traces (filtered for correctness and efficiency) vs. the full dataset to quantify the impact of data curation on SFT's negative effects

2. **Cross-reward generalization**: Replace InternLM-XComposer2.5-Reward with alternative open-ended reward models (e.g., GPT-4o-based) to test whether the mixed reward advantage persists across different reward model choices

3. **Broader task evaluation**: Test VLAA-Thinker-Qwen2.5VL-3B on non-math reasoning benchmarks (e.g., visual commonsense reasoning, multimodal QA) to assess whether reasoning improvements generalize beyond mathematical domains