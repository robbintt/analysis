---
ver: rpa2
title: 'CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial
  Observations'
arxiv_id: '2512.23328'
source_url: https://arxiv.org/abs/2512.23328
tags:
- state
- face
- move
- solved
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CubeBench, a diagnostic benchmark designed\
  \ to evaluate three core cognitive capabilities required for LLM agents to operate\
  \ in the physical world: spatial reasoning, long-horizon state tracking via mental\
  \ simulation, and active exploration under partial observation. Using the Rubik\u2019\
  s Cube as a controlled environment, CubeBench features a three-tiered framework\
  \ that progressively tests these abilities from symbolic to visual to partial visual\
  \ inputs."
---

# CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations

## Quick Facts
- arXiv ID: 2512.23328
- Source URL: https://arxiv.org/abs/2512.23328
- Reference count: 40
- Primary result: Universal 0.00% pass rate on long-horizon tasks reveals LLM agents' fundamental failure in long-term planning.

## Executive Summary
CubeBench is a diagnostic benchmark designed to evaluate three core cognitive capabilities required for LLM agents to operate in the physical world: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. Using the Rubik's Cube as a controlled environment, CubeBench features a three-tiered framework that progressively tests these abilities from symbolic to visual to partial visual inputs. Experiments across leading LLMs reveal critical limitations, with a universal 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. Even short-horizon symbolic tasks show poor performance, with the best model (GPT-5) achieving only 37.5% pass rate. The diagnostic framework, enhanced by external solver tools, successfully isolates these bottlenecks, identifying long-horizon planning as a primary deficit and visual-to-symbolic translation as a key challenge.

## Method Summary
The CubeBench framework evaluates LLM agents on solving Rubik's Cubes across three tiers of observational complexity: Tier 1 (Full Symbolic String), Tier 2 (Full 2D Visual), and Tier 3 (Partial Visual). The benchmark uses 160 test cases generated at specific depths (Short: 1-4, Long: 8-20) with 5 unique states per depth. Agents interact via a ReAct loop using Python tools for state manipulation and observation. The framework employs external optimal solvers (Kociemba) to isolate planning from spatial reasoning deficits, comparing Standard-Solver agents (requiring state translation) with Ideal-Solver agents (provided with pre-formatted state). Dense reward interventions test whether local feedback can compensate for long-horizon planning deficits.

## Key Results
- Universal 0.00% pass rate on all long-horizon tasks (depths 8-20) across all tested models
- Best model (GPT-5) achieves only 37.5% pass rate on short-horizon symbolic tasks
- Standard-Solver vs. Ideal-Solver performance gaps reveal spatial-to-symbolic translation as a key bottleneck
- Dense rewards improve short-horizon performance but fail to enable long-horizon planning

## Why This Works (Mechanism)

### Mechanism 1: Tiered Observability Isolation
The benchmark isolates specific cognitive deficits by progressively removing observational priors, forcing the agent to rely on internal models rather than external state dumps. The three-tiered framework tests from full symbolic state (Tier 1) through visual mapping (Tier 2) to partial observations (Tier 3). Failure in higher tiers but success in lower tiers implies specific cognitive deficits rather than general reasoning failure.

### Mechanism 2: Tool-Augmented Diagnosis (Planning vs. Grounding)
By offloading optimal planning to an external solver tool, the framework distinguishes between failures in high-level planning and failures in spatial-to-symbolic translation. The Standard-Solver Agent must format visual state into a specific solver string, whereas the Ideal-Solver Agent bypasses formatting. Comparing these isolates whether the bottleneck is finding the plan or understanding the cube state.

### Mechanism 3: Interventional Reward Analysis
Introducing dense rewards (e.g., sticker accuracy) reveals whether an agent's failure is due to a lack of guidance or a fundamental inability to perform long-horizon credit assignment. If performance improves on short-horizon tasks but remains at 0% on long-horizon ones, it indicates that the deficit is in long-term planning/mental simulation, not just local hill-climbing.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The paper models the Rubik's Cube environment as a POMDP (specifically in Tier 3) to formalize the challenge where the agent must maintain a belief state rather than accessing the true state directly.
  - Quick check question: If an agent can see the whole cube state in a string (Tier 1), is it still a POMDP? (Answer: No, it's a fully observable MDP).

- **Concept: Singmaster Notation**
  - Why needed here: This is the "vocabulary" of the action space. Understanding that "F" means Front face clockwise and "U'" means Up counter-clockwise is required to interpret the agent's action sequences and the solver's outputs.
  - Quick check question: What is the inverse of the move "R"?

- **Concept: Mental Simulation / World Model**
  - Why needed here: The paper identifies a lack of "spatial mental models" as a core deficit. This refers to the agent's ability to predict the next state $s_{t+1}$ given current state $s_t$ and action $a_t$ without actually executing the action in the environment.
  - Quick check question: Why does error accumulation in mental simulation specifically kill "Long-Horizon" performance?

## Architecture Onboarding

- **Component map:** Environment (Rubik's Cube simulator) -> Agent Interface (get_observation, make_move, apply_view_transformation) -> Diagnostic Layer (Standard-Solver, Ideal-Solver wrappers) -> Evaluation (Pass Rate, #MR metrics)

- **Critical path:** 1. State Generation: Use Optimal Solver to generate scrambled states with known depth 2. Agent Instantiation: Configure LLM with specific tools 3. Interaction Loop: LLM generates code/thoughts → Executes tools → Receives observation/reward 4. Diagnosis: Compare performance across tiers

- **Design tradeoffs:** Symbolic vs. Visual Input (symbolic is easier to debug but less realistic), Dense vs. Sparse Reward (dense rewards help short-term but add noise), Tool Reliance (solver injection measures spatial translation but masks planning deficits)

- **Failure signatures:** Brute Force Loop (high #MR with low pass rate indicates enumeration without reasoning), Format Mismatch (solver error: "Some edges are undefined" indicates failed state translation), Reward Hacking (improves sticker metric but never solves cube)

- **First 3 experiments:** 1. Tier 1 Baseline: Run Basic Agent on Tier 1 with depths 1-4 2. Solver Injection: Run Ideal-Solver on Tier 2 to test state reconstruction 3. Long-Horizon Stress Test: Run Basic Agent with Dense Rewards on depth-8 cube

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications or training paradigms are required to break the "horizon limit" that causes a uniform 0% success rate on planning tasks requiring more than 4 steps?
- Basis in paper: [explicit] The authors report a "uniform 0.00% pass rate on all long-horizon tasks" and identify "Long-Horizon State Tracking through Mental Simulation" as a fundamental failure point.
- Why unresolved: Current agents rely on computationally expensive search strategies that suffer from exponential state space growth and lack effective pruning mechanisms.
- What evidence would resolve it: A model achieving non-zero pass rates on depths 8+ in CubeBench without relying on external optimal solvers.

### Open Question 2
- Question: How can models be forced to develop genuine 3D spatial understanding rather than relying on "algorithmic parsing" shortcuts available in structured visual inputs?
- Basis in paper: [explicit] The paper notes a "significant performance gap" between Face View and Vertex View, observing that agents succeed on the former by "recasting the task as an algorithmic parsing problem" but fail when geometric inference is required.
- Why unresolved: Models tend to exploit the orderly, grid-like structure of standard inputs to bypass the need for coherent internal 3D mental models.
- What evidence would resolve it: Achieving comparable performance on CubeBench's Vertex View tasks as on structured Face View tasks.

### Open Question 3
- Question: Can dense reward signals be designed to support long-horizon planning without conflicting with capable agents' emergent internal reasoning strategies?
- Basis in paper: [explicit] The authors found that dense rewards could "conflict with their emergent internal strategies" and failed to bridge the gap to long-horizon success.
- Why unresolved: Simple heuristic rewards act only as local guides and often fail to correlate with the complex state transitions required for long-term solutions.
- What evidence would resolve it: A reward mechanism that improves an agent's long-horizon pass rate above 0% without degrading its baseline short-horizon reasoning capabilities.

## Limitations
- Generalizability uncertainty: Findings may not transfer to other physical manipulation tasks with different state spaces and action complexities
- Reproducibility gaps: Lack of published system prompts and visual rendering specifications affects cross-study comparisons
- Reward function artifacts: Dense reward design may introduce local optima that confound planning deficit diagnosis

## Confidence

- **High Confidence:** Universal 0.00% pass rate on long-horizon tasks; tiered framework's ability to isolate cognitive deficits
- **Medium Confidence:** Spatial-to-symbolic translation as primary bottleneck; visual grounding more challenging than symbolic reasoning
- **Low Confidence:** Direct applicability of diagnostic framework to other embodied AI systems

## Next Checks

1. **Cross-Task Transfer:** Test CubeBench's diagnostic framework on different physical manipulation tasks (block stacking, maze navigation) to verify same cognitive bottlenecks appear

2. **Prompt Ablation Study:** Vary system prompts and tool descriptions to quantify their impact on agent performance, especially Standard-Solver vs. Ideal-Solver differences

3. **Reward Function Sensitivity Analysis:** Systematically vary reward density and shape to determine if long-horizon failures are robust to reward signal changes or artifacts of specific reward design