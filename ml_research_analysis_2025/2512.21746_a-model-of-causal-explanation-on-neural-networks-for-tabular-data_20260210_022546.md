---
ver: rpa2
title: A Model of Causal Explanation on Neural Networks for Tabular Data
arxiv_id: '2512.21746'
source_url: https://arxiv.org/abs/2512.21746
tags:
- variables
- causal
- variable
- data
- cennet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CENNET, a causal explanation method for neural
  network predictions on tabular data that addresses the problems of pseudo-correlation
  and indirect causal factors in existing explainability methods. CENNET uses structural
  causal models (SCMs) combined with neural networks, analyzing the nearest neighbor
  latent unit (NNLU) to extract characteristic correlated variables (CCVs) that directly
  influence predictions.
---

# A Model of Causal Explanation on Neural Networks for Tabular Data

## Quick Facts
- arXiv ID: 2512.21746
- Source URL: https://arxiv.org/abs/2512.21746
- Authors: Takashi Isozaki; Masahiro Yamamoto; Atsushi Noda
- Reference count: 40
- This paper proposes CENNET, a causal explanation method for neural network predictions on tabular data that addresses the problems of pseudo-correlation and indirect causal factors in existing explainability methods.

## Executive Summary
This paper introduces CENNET, a causal explanation method for neural networks that addresses the fundamental limitations of existing interpretability approaches by eliminating pseudo-correlations and indirect causal effects. The method uses structural causal models combined with neural networks, analyzing the nearest neighbor latent unit (NNLU) to extract characteristic correlated variables (CCVs) that directly influence predictions. CENNET introduces a total explanation power (TEP) index that combines neuron weights with entropy-based measures to provide both global and local explanations. Experiments demonstrate that CENNET statistically outperforms existing methods like LIME, SHAP, and ACV in ranking important variables, particularly for non-additive and combinatorial effects, while also achieving faster computation times in most settings.

## Method Summary
CENNET uses structural causal models (SCMs) to infer direct causal relationships from tabular data, then applies these models to neural network predictions. The method analyzes each neuron in the NNLU (penultimate layer) independently to extract CCVs using a modified PC algorithm with G² tests. It computes entropy-based explanation powers (EEPs) that quantify how much a CCV configuration changes prediction probability, then combines these with neuron weights to calculate total explanation power (TEP). The approach uses discretization into three equal-frequency bins for continuous variables and provides both global rankings of important variables and local explanation configurations for individual predictions.

## Key Results
- CENNET statistically outperforms existing methods like LIME, SHAP, and ACV in ranking important variables, particularly for non-additive and combinatorial effects
- The method achieves faster computation times in most settings while providing more direct causal explanations
- Experiments on synthetic and quasi-real datasets show superior performance in identifying direct causal variables compared to baseline explainability methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CCVs identify Markovianly direct causations while eliminating pseudo-correlations and indirect effects
- Mechanism: Uses structural causal model (SCM) inference via partial correlation analysis. Variables with non-zero partial correlation across all conditioning sets, where X precedes Y in causal order, become CCVs.
- Core assumption: The data-generating process follows a DAG structure satisfying the causal Markov condition
- Evidence anchors: [abstract] "CENNET provides explanations that eliminate pseudo-correlations and indirect causal effects, presenting only direct causal reasons"; [section 3.1] "finding an SCM in a dataset means performing a partial correlation analysis and decomposing it into correlations including uncorrelated, pseudo-correlated, and indirect causality... and direct causality"
- Break condition: If the true causal structure contains cycles or hidden confounders not captured by DAG assumptions, CCV extraction may misclassify variable relationships

### Mechanism 2
- Claim: Analyzing the NNLU (Nearest Neighbor Latent Unit) rather than final output captures more contributing causal variables
- Mechanism: Extracts CCVs for each neuron in the penultimate layer separately, then aggregates via weighted combinations. This bypasses the "too few CCVs" problem observed when analyzing only the final output layer.
- Core assumption: NNLU neurons have learned non-trivial feature compositions from input variables; each neuron's output is causally downstream of inputs
- Evidence anchors: [section 3.2] "one causal model is inferred among the input variable set and one neuron in the NNLU, and other causal models are inferred for each other neuron"; [section 5] "when directly causal variables were inferred for the final output variable Y, we observed that only a few variables were selected as the CCVs for Y"
- Break condition: If NNLU neurons are too sparse or highly correlated with each other, redundant causal models may inflate computational cost without explanatory gain

### Mechanism 3
- Claim: Entropy-based Explanation Powers (EEPs) enable unified comparison of single-variable and multivariate explanations
- Mechanism: EEP(z, xe) = log{P(z|{xe})/P(z)} quantifies how much a CCV configuration changes prediction probability. TEP combines EEPs with NNLU neuron weights, separating positive (PEP) and negative (NEP) contributions.
- Core assumption: Discretization of continuous neurons/inputs preserves meaningful probability distributions; ReLU-style activations ensure non-negative neuron values
- Evidence anchors: [section 3.4] Equations 5-8 define EEP, PEP, NEP, and TEP formally; [section 3.1, Theorem 1] H(X|CCV(X)) achieves the lower bound of conditional entropy, providing information-theoretic justification
- Break condition: If discretization bins are poorly chosen (too coarse or fine), probability estimates become unreliable, degrading EEP accuracy

## Foundational Learning

- Concept: Structural Causal Models (SCMs) and DAG-based causal discovery
  - Why needed here: CENNET's core filtering of pseudo-correlations depends on understanding directed edges, partial correlation, and the causal Markov condition
  - Quick check question: Given variables A→B→C, which partial correlation should be zero?

- Concept: Shannon entropy and conditional entropy
  - Why needed here: Theorem 1's proof and EEP calculations require understanding H(X|Y) and its minimization properties
  - Quick check question: If H(X|CCV(X)) is minimized, what does this imply about information redundancy in non-CCV variables?

- Concept: Feedforward neural network layer structure and weight decomposition
  - Why needed here: Understanding how final predictions decompose into weighted sums of NNLU outputs is essential for PEP/NEP computation
  - Quick check question: In ŷ = Σw_i·n_i + b, how would you identify which neurons contribute positively vs. negatively to a specific class?

## Architecture Onboarding

- Component map:
  - Input Layer: Tabular features X (continuous variables discretized into 3 bins via equal-frequency splitting)
  - Hidden Layers: Two layers (16 neurons in first, 5 in NNLU)
  - NNLU: Penultimate layer; each neuron analyzed independently for CCV extraction
  - Causal Inference Module: Modified PC algorithm with G² tests (1% significance) per neuron
  - Explanation Engine: EEP/TEP calculator using cached conditional probabilities from training data
  - Output: Ranked list of CCVs (global) and TEP-sorted explanation configurations (local)

- Critical path:
  1. Train MLP classifier → 2. Discretize all variables → 3. For each NNLU neuron, infer causal graph → 4. Extract CCVs → 5. Cache P(n_i|CCV) probabilities → 6. For each test instance, compute EEPs and TEP for CCV combinations up to order m

- Design tradeoffs:
  - **Discretization granularity**: 3 bins used experimentally; finer bins improve precision but require more data for reliable probability estimation
  - **Maximum explanation order (m)**: Higher m captures multivariate interactions but increases O(lpt) complexity (l=NNLU neurons, p=CCV combinations, t=samples)
  - **Causal discovery algorithm choice**: PC algorithm selected; alternatives (FGES, NOTEARS) may differ in edge orientation accuracy under limited data

- Failure signatures:
  - **Empty CCV sets**: Indicates causal inference found no direct relationships; check discretization or increase training data
  - **Identical top explanations across instances**: Suggests EEP caching error or insufficient CCV diversity
  - **Computation timeout**: Likely m set too high; reduce maximum order or pre-filter CCV candidates by EMI threshold

- First 3 experiments:
  1. **Baseline validation on Non-Linear Additive synthetic data**: Verify CENNET matches SHAP/LIME performance on single-variable importance (expected ~equal average rank for X1-X4)
  2. **Non-additive stress test**: Run Non-Linear Non-Additive dataset; confirm CENNET achieves higher top-1 and top-5 correct-pair percentages than baselines
  3. **Ablation on NNLU vs. output-layer causal analysis**: Directly compare CCV extraction from final output Y vs. NNLU neurons; confirm the latter recovers more known causal variables (replicating section 5 observation)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CENNET effectively identify direct causal variables in real-world datasets where the underlying Structural Causal Model (SCM) is unknown or violates the causal Markov condition?
- Basis in paper: [inferred] The authors limit evaluation to synthetic and "quasi-real" data sampled from expert models, explicitly stating that "there are few real data with many variables for which causal relationships are known."
- Why unresolved: The method's validity relies on inferring a correct DAG, which is difficult to verify in uncontrolled, noisy real-world environments.
- Evidence: Empirical testing on domain-specific benchmarks where ground-truth causality has been established via randomized controlled trials.

### Open Question 2
- Question: How does CENNET's computational cost scale when applied to deep neural networks with high-dimensional Nearest Neighbor Latent Units (NNLUs)?
- Basis in paper: [inferred] The paper claims computational cost is "not as serious" because tabular datasets "usually do not require very large NNs," implicitly suggesting potential inefficiency for larger architectures.
- Why unresolved: The algorithm requires inferring a causal model for *each* neuron in the NNLU, which could become a bottleneck in wide or deep networks.
- Evidence: Runtime benchmarks on complex network architectures with significantly larger latent layers than those used in the provided experiments.

### Open Question 3
- Question: To what extent do errors in the initial causal discovery phase propagate to the final explanation quality?
- Basis in paper: [inferred] The method relies on a modified PC algorithm to extract Characteristic Correlated Variables (CCVs), but does not analyze sensitivity to incorrect edge orientations or missed partial correlations.
- Why unresolved: Inaccurate graph inference could lead to the selection of spurious variables or the omission of direct causes, degrading explanation fidelity.
- Evidence: Ablation studies introducing noise into the causal graph structure to measure the resulting variance in variable ranking performance.

## Limitations
- Method's reliance on discretization into exactly three bins may not preserve critical information in continuous relationships
- "Modified PC algorithm" lacks precise specification, making exact reproduction difficult
- Computational complexity O(lpt) for high-order explanations could become prohibitive with many CCVs or large datasets

## Confidence
- High confidence in the theoretical foundation of using SCMs to filter pseudo-correlations
- Medium confidence in the NNLU-based causal extraction mechanism due to limited independent validation
- Medium confidence in entropy-based TEP scoring given self-contained justification
- Low confidence in optimal discretization parameters and maximum explanation order selection

## Next Checks
1. **Edge orientation sensitivity test**: Systematically vary the significance threshold in the PC algorithm (α = 0.001, 0.01, 0.05) and measure changes in CCV selection accuracy to quantify robustness to causal discovery parameters.

2. **Discretization granularity ablation**: Compare CCV extraction and explanation accuracy using 2, 3, 5, and 10 equal-frequency bins to determine the optimal trade-off between information preservation and probability estimation reliability.

3. **NNLU vs. output-layer causal analysis replication**: Directly replicate the experiment comparing CCV counts from final output Y versus NNLU neurons, quantifying the improvement in recovered causal variables and validating the architectural choice.