---
ver: rpa2
title: 'NILC: Discovering New Intents with LLM-assisted Clustering'
arxiv_id: '2511.05913'
source_url: https://arxiv.org/abs/2511.05913
tags:
- cluster
- clustering
- intent
- utterances
- intents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NILC tackles new intent discovery by iteratively refining both
  cluster assignments and text embeddings using large language models (LLMs). The
  method introduces a dual centroid scheme, combining Euclidean centroids from embeddings
  with LLM-generated semantic centroids for richer cluster representation.
---

# NILC: Discovering New Intents with LLM-assisted Clustering

## Quick Facts
- arXiv ID: 2511.05913
- Source URL: https://arxiv.org/abs/2511.05913
- Authors: Hongtao Wang; Renchi Yang; Wenqing Lin
- Reference count: 40
- Primary result: NILC achieves up to 7.72% improvement in ARI and 11.38% in ACC in unsupervised settings

## Executive Summary
NILC addresses new intent discovery by iteratively refining both cluster assignments and text embeddings using large language models (LLMs). The method introduces a dual centroid scheme that combines Euclidean centroids from embeddings with LLM-generated semantic centroids for richer cluster representation. It also employs hard sample refinement to rewrite ambiguous or terse utterances with LLM assistance, improving embedding quality and clustering accuracy. In semi-supervised settings, NILC integrates seeding and soft must-link constraints from labeled data. Experiments across six datasets show that NILC consistently outperforms baselines and sets new state-of-the-art results in semi-supervised scenarios.

## Method Summary
NILC tackles new intent discovery through an iterative refinement process that combines K-Means clustering with LLM augmentation. The framework operates in two main phases: first, it encodes utterances using a pre-trained text encoder (typically USNID) and initializes K clusters via K-Means++. Then, it enters an iterative loop (typically 2-3 iterations) where it (1) generates semantic centroids via LLM summarization of cluster exemplars, (2) updates cluster assignments using a joint cost function that balances Euclidean distance with semantic similarity/dissimilarity, and (3) identifies and rewrites hard samples using LLM context-aware rewriting. The method also supports semi-supervised learning through seeding known intents and adding soft must-link constraints.

## Key Results
- Achieves up to 7.72% improvement in ARI and 11.38% in ACC compared to baselines in unsupervised settings
- Sets new state-of-the-art results in semi-supervised scenarios across all tested datasets
- Demonstrates consistent performance improvements across six diverse datasets (CLINC, BANKING, StackOverflow, M-CID, SNIPS, DBPedia)
- Shows framework flexibility by achieving comparable results with different encoders (USNID, SentenceBERT, Instructor)

## Why This Works (Mechanism)

### Mechanism 1: Dual Centroid Scheme
The dual centroid scheme combines geometric structure captured by Euclidean centroids with semantic nuance captured by LLM-generated summaries. By minimizing a joint cost function that balances Euclidean distance with semantic similarity/dissimilarity, NILC can make more informed assignment decisions than embedding-only approaches. This addresses the limitation that embedding averages can lose semantic nuances present in individual utterances.

### Mechanism 2: Hard Sample Refinement
Hard sample refinement identifies ambiguous utterances through Shannon entropy and rewrites them using LLM context-aware prompts. By conditionally updating embeddings only when clustering cost decreases, the method improves embedding quality while avoiding semantic drift. This addresses the challenge that ambiguous utterances can lead to poor cluster assignments and noisy embeddings.

### Mechanism 3: Semi-Supervised Constraints
The semi-supervised extensions leverage limited labeled data through seeding and soft must-link constraints. Seeding aligns initial centroids with known intents, while soft must-links pull samples toward known-intent clusters during refinement. This allows NILC to effectively combine the benefits of unsupervised discovery with the guidance of limited supervision.

## Foundational Learning

- **K-Means clustering and embedding limitations**
  - Why needed: NILC builds on K-Means; understanding why embedding averages lose semantic nuance motivates the dual centroid approach
  - Quick check question: Why might averaging embeddings for a cluster containing "cancel subscription" and "refund policy" fail to capture their distinct intents?

- **LLM in-context learning and prompting**
  - Why needed: NILC relies on carefully designed prompts for summarization and rewriting
  - Quick check question: What contextual information would you include in a prompt asking an LLM to rewrite an ambiguous technical question?

- **Shannon entropy for uncertainty quantification**
  - Why needed: HSR uses entropy to identify hard samples
  - Quick check question: If an utterance has equal probability across 5 clusters, what would its entropy indicate?

## Architecture Onboarding

- **Component map:**
  - PTE (Pre-trained Text Encoder) -> K-Means++ -> Iterative Loop -> Cluster Assignments
  - Iterative Loop: DCS -> Assignment Update -> HSR -> Repeat T times

- **Critical path:**
  1. Encode all utterances with PTE
  2. Initialize K clusters via K-Means++
  3. For T iterations: (a) generate semantic centroids via LLM, (b) update assignments via joint cost, (c) identify top-δ hard samples via entropy, (d) rewrite with LLM, (e) conditionally update embeddings

- **Design tradeoffs:**
  - LLM cost vs. performance: K + δ LLM calls per iteration (paper uses GPT-4o-Mini, δ=10)
  - Iteration count: Paper shows gains stabilize by T=3 (Fig. 13)
  - Hard sample count: Higher δ increases cost but may capture more boundary cases

- **Failure signatures:**
  - Cluster summaries become generic/repetitive → LLM not capturing domain semantics
  - Rewritten embeddings don't decrease clustering cost → HSR not helping
  - ARI plateaus or degrades with iterations → over-refinement or noise injection

- **First 3 experiments:**
  1. Ablation: Run NILC with DCS only, HSR only, and both; compare NMI/ARI/ACC on 2-3 datasets
  2. Hyperparameter sensitivity: Sweep α∈{0.1,0.3,0.5,0.7,0.9} and δ∈{5,10,15,20}; check stability
  3. Encoder swap: Test with SentenceBERT vs. Instructor; verify framework is encoder-agnostic

## Open Questions the Paper Calls Out

### Open Question 1
Can NILC be extended to automatically determine the optimal number of clusters K when the true intent count is unknown, without relying on pre-specified values? The methodology assumes K is given and uses ground-truth values from benchmark datasets, limiting practical applicability when intent counts are unknown.

### Open Question 2
How does NILC scale to industrial-scale datasets with millions of utterances, given the O(NKd) complexity and per-iteration LLM calls? The largest tested dataset is CLINC with 22,500 utterances, and scaling behavior to larger corpora is not evaluated.

### Open Question 3
Can the LLM-dependent components (semantic centroid generation, hard sample rewriting) be replaced with local models for privacy-preserving or offline deployment while maintaining comparable performance? The paper does not address scenarios where external LLM APIs are unavailable or disallowed.

### Open Question 4
What are the formal convergence guarantees for NILC's EM-style iterative process, and under what conditions might the algorithm converge to poor local minima? Section 4.5 analogizes to EM but provides no theoretical analysis of convergence properties or failure modes.

## Limitations

- LLM dependency creates scalability challenges with large datasets due to per-iteration API costs and latency
- Performance may be sensitive to hyperparameter choices (α, β, δ) without clear guidance on optimal settings
- Limited ablation studies make it difficult to quantify the individual contributions of the dual centroid scheme and hard sample refinement components

## Confidence

- **High confidence:** The dual centroid scheme's core mechanism and theoretical foundation are well-supported by formulation and experimental results
- **Medium confidence:** The hard sample refinement approach shows promise but lacks extensive ablation to quantify individual contribution
- **Medium confidence:** Semi-supervised extensions are logically sound but have limited empirical validation compared to unsupervised results

## Next Checks

1. Implement NILC variants with DCS only, HSR only, and both components disabled to quantify individual contributions on 2-3 datasets
2. Systematically vary α (semantic weight), β (separation weight), and δ (hard sample count) to identify stable operating ranges
3. Validate framework performance using alternative encoders (e.g., SentenceBERT vs. Instructor) to confirm encoder-agnostic claims