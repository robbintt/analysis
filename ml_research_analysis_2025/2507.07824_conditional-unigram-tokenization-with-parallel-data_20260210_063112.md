---
ver: rpa2
title: Conditional Unigram Tokenization with Parallel Data
arxiv_id: '2507.07824'
source_url: https://arxiv.org/abs/2507.07824
tags:
- pairedsp
- sptgt
- pairedspm
- language
- tokenization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning a target-language tokenizer
  conditioned on source-language tokens from parallel data. The approach extends unigram
  tokenization by maximizing the probability of target tokens given aligned source
  tokens.
---

# Conditional Unigram Tokenization with Parallel Data

## Quick Facts
- arXiv ID: 2507.07824
- Source URL: https://arxiv.org/abs/2507.07824
- Reference count: 40
- Primary result: Conditional tokenization improves language modeling perplexity but not machine translation quality due to quadratic scaling data efficiency bottleneck

## Executive Summary
This paper introduces a method for learning target-language tokenizers conditioned on source-language tokens from parallel data by extending the unigram tokenization framework. The approach maximizes the probability of target tokens given aligned source tokens, learning a vocabulary and segmentation that capture cross-lingual alignment. While intrinsic evaluations show comparable statistical properties to standard unigram tokenizers, the method achieves consistent perplexity reductions in language modeling tasks but fails to improve machine translation quality. The authors attribute this limitation to quadratic scaling of conditional probability estimation with vocabulary size, creating a data efficiency bottleneck that prevents the model from learning optimal tokenizations with realistic training data sizes.

## Method Summary
The method extends unigram tokenization by conditioning target token probabilities on source tokens from parallel data. It uses word alignments to build a co-occurrence table counting how often target spans co-occur with aligned source tokens, then iteratively prunes the vocabulary based on mutual information while maintaining character coverage. Tokenization is performed using a Viterbi-style algorithm that maximizes the product of conditional probabilities p(t|S) over possible segmentations. The approach assumes a fixed source tokenizer and uses Eflomal for word alignment, with preprocessing including NFKC normalization and byte-fallback support.

## Key Results
- Conditional tokenizer achieves consistent perplexity reductions in language modeling across all evaluated language pairs and vocabulary sizes
- No improvements in machine translation quality compared to standard unigram tokenization
- Intrinsic evaluation metrics (parity, fertility, one-to-one alignment, unaligned tokens) show comparable performance to baseline tokenizers
- Quadratic scaling of conditional probability estimation requires approximately 28M training examples to match unigram tokenizer quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Conditioning target token probabilities on aligned source tokens reduces language modeling perplexity
- **Mechanism**: Cross-lingual alignment embedded in tokenization creates more statistically favorable token units
- **Core assumption**: Cross-lingual semantic alignment reduces token distribution ambiguity
- **Evidence anchors**: Consistent perplexity reductions reported across all language pairs in Section 5.3
- **Break condition**: Fails with sparse parallel data relative to vocabulary size

### Mechanism 2
- **Claim**: Conditional unigram tokenizer extends standard unigram model with conditional probabilities
- **Mechanism**: Substitutes p(t) with p(t|S) using co-occurrence table from aligned parallel sentences
- **Core assumption**: Source tokens provide useful conditioning signal for optimal target segmentation
- **Evidence anchors**: Method described in Section 3 using Viterbi decoding with conditional probabilities
- **Break condition**: Crude approximation of p(t|S) or failed convergence of dynamic programming search

### Mechanism 3
- **Claim**: Quadratic scaling creates data efficiency bottleneck
- **Mechanism**: Estimating p(t|s) requires |V_tgt| × |V_src| co-occurrence counts, becoming sparse with large vocabularies
- **Core assumption**: Unreliable probability estimates lead to suboptimal tokenization
- **Evidence anchors**: Authors estimate 28M examples needed to match unigram quality in Conclusions
- **Break condition**: Explains limitation rather than success, predicts failure in data-scarce scenarios

## Foundational Learning

- **Concept**: **Unigram Tokenization**
  - **Why needed here**: Method is direct extension; understanding base model (probabilistic vocab learning, Viterbi decoding) is essential
  - **Quick check question**: How does standard Unigram tokenizer use dynamic programming to select best segmentation?

- **Concept**: **Parallel Corpora and Word Alignment**
  - **Why needed here**: Approach predicated on using aligned parallel sentences to learn conditional token probabilities
  - **Quick check question**: What output does word alignment tool like Eflomal produce from parallel text?

- **Concept**: **Probability Estimation and Smoothing**
  - **Why needed here**: Core challenge is estimating p(t|s) from sparse counts; understanding data sparsity is key
  - **Quick check question**: Why is estimating conditional probability more data-intensive than marginal distribution?

## Architecture Onboarding

- **Component map**: Pre-processor -> Co-occurrence Table -> Vocabulary Pruner -> Tokenizer (Inference)
- **Critical path**: Iterative update loop (COUNT function in Algorithm 1) re-estimates c table based on current model, refining core learning signal
- **Design tradeoffs**: Expressivity vs. Estimability (larger vocabularies allow complexity but make c table sparse); Complexity vs. Performance (introduces quadratic scaling for mixed gains)
- **Failure signatures**: No MT improvement despite alignment; Single-character token collapse in extreme cases
- **First 3 experiments**:
  1. Implement PairedSPM (marginalize over source tokens) - most practical variant for LM testing
  2. Scale ablation - systematically vary vocab/data size on single pair, plot fertility/perplexity to observe data efficiency bottleneck
  3. Probing task - synthetic task with related languages, inspect c table to verify semantic alignment capture

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What alternative parameterizations can overcome quadratic scaling bottleneck?
- **Basis in paper**: Authors hypothesize quadratic scaling creates data efficiency bottleneck requiring alternative parameterizations
- **Why unresolved**: Current formulation requires |V_tgt| × |V_src| co-occurrence counts, making it data-hungry
- **What evidence would resolve it**: Parameterization achieving comparable alignment with sub-quadratic scaling, demonstrated through MT quality or perplexity gains with reduced data

### Open Question 2
- **Question**: Why does conditional tokenization improve LM perplexity but not MT quality?
- **Basis in paper**: Reports perplexity reductions but no MT improvements without mechanistic explanation
- **Why unresolved**: Authors discuss data efficiency but not why cross-lingual alignment benefits LM while failing to simplify translation
- **What evidence would resolve it**: Ablation studies isolating perplexity gains sources; attention pattern analysis in MT models

### Open Question 3
- **Question**: What are minimum parallel data requirements for parity with standard unigram tokenizers?
- **Basis in paper**: Authors estimate 28M examples needed to match unigram fertility, noting this limits practical applicability
- **Why unresolved**: Estimates from only four language pairs; relationship between language similarity and data requirements unexplored
- **What evidence would resolve it**: Systematic experiments scaling data across diverse pairs with controlled linguistic distances

### Open Question 4
- **Question**: Can hybrid approaches preserve perplexity benefits while improving data efficiency?
- **Basis in paper**: Authors suggest combining conditional and unconditional methods as future direction
- **Why unresolved**: Unclear whether benefits can be isolated to vocabulary subsets or if staged training would work
- **What evidence would resolve it**: Experiments with interpolated objectives or staged procedures comparing pure/hybrid variants

## Limitations
- Quadratic scaling of conditional probability table requires approximately 28M training examples to achieve parity with standard unigram tokenization
- Current approximation of p(t|S) as bag-of-tokens may be too crude to capture meaningful cross-lingual dependencies
- Intrinsic evaluation metrics may not capture semantic coherence that conditional approach aims to achieve

## Confidence

**High Confidence Claims:**
- Quadratic scaling creates data efficiency bottleneck limiting practical applicability
- Method successfully learns conditional token probabilities and achieves LM perplexity reductions
- Standard unigram tokenization remains more data-efficient for most applications

**Medium Confidence Claims:**
- Comparable intrinsic metrics indicate reasonable tokenization properties
- Lack of MT improvements primarily due to data efficiency bottleneck
- Alternative parameterizations could overcome scaling limitations

**Low Confidence Claims:**
- Specific hypothesis that semantic coherence directly causes LM improvements remains speculative
- Exact threshold where scaling becomes prohibitive depends on language pair characteristics

## Next Checks

**Validation Check 1: Scale-Agnostic Evaluation**
Conduct systematic ablation study varying vocabulary size (4k, 8k, 16k, 32k) and training data size (100k, 500k, 1M, 10M) on single language pair. Plot fertility and perplexity against total co-occurrence counts to verify quadratic scaling hypothesis and identify performance plateau threshold.

**Validation Check 2: Cross-Lingual Semantic Probing**
Design synthetic probing task using closely related languages with shared vocabulary subsets. Train conditional and baseline tokenizers, inspect learned c(t,s) tables to verify whether conditional model correctly captures semantic alignments between source and target tokens.

**Validation Check 3: Alternative Parameterization Experiment**
Implement sparse or parametric approximation of conditional probability table (e.g., low-rank factorization or neural parameterization) to reduce quadratic scaling. Compare performance against full conditional model and baseline unigram tokenizer across same language pairs to assess whether scaling issue can be mitigated without sacrificing cross-lingual benefits.