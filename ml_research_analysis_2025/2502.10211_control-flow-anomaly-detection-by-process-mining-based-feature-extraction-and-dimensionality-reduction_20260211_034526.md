---
ver: rpa2
title: Control-flow anomaly detection by process mining-based feature extraction and
  dimensionality reduction
arxiv_id: '2502.10211'
source_url: https://arxiv.org/abs/2502.10211
tags:
- process
- event
- conformance
- techniques
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel process mining-based feature extraction
  approach with alignment-based conformance checking to detect control-flow anomalies
  in business processes. The method aligns deviating control flows with a reference
  Petri net and extracts additional statistics such as per-activity mismatch counts.
---

# Control-flow anomaly detection by process mining-based feature extraction and dimensionality reduction

## Quick Facts
- arXiv ID: 2502.10211
- Source URL: https://arxiv.org/abs/2502.10211
- Reference count: 40
- This paper introduces a novel process mining-based feature extraction approach with alignment-based conformance checking to detect control-flow anomalies in business processes.

## Executive Summary
This paper presents a framework for detecting control-flow anomalies in business processes by combining process mining-based feature extraction with dimensionality reduction techniques. The approach converts process traces into fixed-length feature vectors using alignment-based conformance checking, then applies dimensionality reduction methods like PCA and Autoencoders to learn normal behavior patterns. Experiments on synthetic and real-world datasets demonstrate superior performance compared to baseline conformance checking techniques while maintaining explainability through feature attribution.

## Method Summary
The framework operates by first extracting features from event logs using alignment-based conformance checking with a reference Petri net, producing feature vectors containing fitness scores and per-activity costs. These vectors are then processed by dimensionality reduction techniques (PCA, SPCA, KPCA, Autoencoder) trained only on normal data. The trained model learns to reconstruct normal feature vectors, and anomalies are detected when reconstruction error exceeds a threshold determined on validation data. The method maintains explainability by attributing reconstruction errors to specific process activities through SHAP values.

## Key Results
- Framework techniques implementing the proposed approach outperform baseline conformance checking-based techniques
- Best-performing technique achieved F1-scores of 97.3%, 88.9%, 90.5%, and 88.5% on PDC 2020/2021, ERTMS, and COV AS datasets respectively
- No single process mining-based feature extraction approach is universally optimal across all datasets
- The framework maintains explainability while improving detection performance

## Why This Works (Mechanism)

### Mechanism 1
The framework detects anomalies by learning to reconstruct a feature space derived from process diagnostics, not raw traces. The proposed alignment-based feature extraction converts a variable-length trace into a fixed-length feature vector (e.g., per-activity costs, fitness). A dimensionality reduction technique (e.g., Autoencoder) is trained only on normal feature vectors. It learns an efficient encoding and decoding for this "normal" data. When an anomalous trace is converted to a feature vector, its statistical properties differ (e.g., higher cost on specific activities). The trained model cannot reconstruct this vector accurately, resulting in a high reconstruction error that signals an anomaly. Core assumption: The feature vectors of anomalous traces are statistically distinct from normal ones in the latent space learned by the dimensionality reduction technique.

### Mechanism 2
Explainability is preserved by attributing the reconstruction error back to specific process model elements. Unlike a black-box deep learning model, the framework's input features are semantically meaningful (e.g., `uc,Ïƒ`, the cost of activity `c`). When an anomaly is flagged, additive feature-based explanation methods (like SHAP) are used to determine which input features contributed most to the high reconstruction error. Since each feature maps to an activity or transition in the reference Petri net, the explanation directly highlights the problematic steps in the business process (e.g., "activity t75 caused a mismatch"). Core assumption: The reconstruction error can be decomposed into additive contributions from input features, and the reference Petri net's elements correspond to meaningful business activities.

### Mechanism 3
A multivariate feature vector outperforms a univariate fitness score by capturing nuanced deviations. Baseline conformance checking (e.g., AB CC B) uses a single fitness value. As shown in Figure 8, the fitness distributions of normal and anomalous logs often overlap heavily, making any single threshold ineffective. The proposed framework constructs a multi-dimensional feature vector (fitness + many per-activity costs). This richer representation allows the dimensionality reduction model to learn a complex manifold of normal behavior, making it sensitive to subtle, localized deviations (e.g., a single activity failing repeatedly) that might not significantly shift the aggregate fitness score but do create an outlier in the feature space. Core assumption: Anomalies create patterns across multiple features that are more distinguishable from normal behavior than a single aggregate score.

## Foundational Learning

- **Concept: Alignment-based Conformance Checking**
  - **Why needed here:** This is the core technique used to generate the feature vectors. You must understand how an alignment is computed (optimal path through the Petri net matching the trace) and how a cost is assigned to log moves (activity in trace, not in model) and model moves (activity in model, not in trace).
  - **Quick check question:** If a trace contains an activity that is not allowed by the Petri net at that point, what is the resulting "move" in the alignment, and how does it affect the feature vector?

- **Concept: Reconstruction-based Anomaly Detection**
  - **Why needed here:** The paper uses this paradigm for its dimensionality reduction techniques. You need to grasp that the model is trained to compress and then uncompress *normal* data. A high error during the uncompression (reconstruction) step indicates the input is foreign to the learned normal distribution.
  - **Quick check question:** If an Autoencoder is trained on normal data and then fed an anomalous trace's feature vector, will its reconstruction error likely be higher or lower than the error for a normal trace? Why?

- **Concept: SHAP (SHapley Additive exPlanations) Values**
  - **Why needed here:** This is the method used to provide post-hoc explainability. You need to understand that it assigns a value to each feature indicating its contribution to the model's output (in this case, the reconstruction error).
  - **Quick check question:** If the feature `cost_of_activity_A` has a high positive SHAP value for an anomalous trace, what does that imply about activity A's role in the anomaly?

## Architecture Onboarding

- **Component map:** Event logs + Reference Petri net -> Feature Extraction (Alignment-based conformance checking) -> Dimensionality Reduction (PCA/Autoencoder) -> Threshold Determination (validation reconstruction error) -> Anomaly Detection & Explanation (test reconstruction error + SHAP values)
- **Critical path:** The feature extraction step. The quality and informativeness of the feature vectors are the bottleneck. A Petri net of poor quality will generate noisy, uninformative features, causing the entire downstream pipeline to fail.
- **Design tradeoffs:**
  - PCA vs. Autoencoder: PCA is faster and more interpretable but assumes linear relationships. Autoencoder can capture complex, non-linear patterns but is harder to train and debug.
  - TB CC vs. AB CC feature extraction: Token-based (TB CC) is computationally cheaper but provides less precise diagnostics. Alignment-based (AB CC) is more robust and feature-rich but computationally more expensive.
  - Fitness-only vs. Full Feature Vector: Using only fitness is simpler but less effective due to score overlap. Using the full vector (fitness + per-activity costs) is more powerful but increases dimensionality, potentially requiring more training data.
- **Failure signatures:**
  - Low Recall: The threshold `Eth` is likely set too high. Lower it to flag more potential anomalies, at the cost of precision.
  - Low Precision: The threshold `Eth` is too low, or the training data contains anomalous traces (label noise), causing the model to learn anomalies as normal.
  - Uninformative Explanations: The SHAP values point to many features with similar, low-impact scores. This suggests the model is struggling to differentiate the core anomaly cause, possibly due to a highly complex or poorly trained DR model.
- **First 3 experiments:**
  1. Baseline Establishment: Implement the simplest baseline technique (AB CC B) on a chosen dataset (e.g., PDC 2020). Compute fitness for all traces and observe the histogram of scores for normal vs. anomalous traces. Quantify the overlap.
  2. Feature Extraction Validation: Implement the alignment-based feature extractor. Run it on a small set of known normal and anomalous traces. Manually inspect the resulting feature vectors to confirm that anomalous ones visually differ (e.g., have non-zero entries in columns where normal ones are zero).
  3. Framework Component Test: Implement a basic framework technique, such as `(AB CC, PCA)`. Train it on the normal training set, determine the threshold on the validation set, and evaluate F1-score on the test set. Compare this single result against the baseline from experiment 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating multi-perspective attributes (e.g., time, resources) into the process mining-based feature extraction phase improve detection effectiveness for anomalies beyond control-flow deviations?
- Basis in paper: The authors state in the Conclusions that future work will extend the framework by "considering additional event log attributes including information related to other perspectives as well as the control-flow one" to enrich tabular data and improve detection.
- Why unresolved: The current framework focuses exclusively on control-flow anomalies (skipped, unknown, wrongly-ordered activities) and relies on feature extraction methods (alignment-based, token-based) optimized for this single perspective.
- What evidence would resolve it: A comparative study showing F1-scores of the framework when extended with time and resource attributes versus the control-flow-only implementation on datasets containing time-based or resource-based anomalies.

### Open Question 2
- Question: Can object-centric process mining be successfully integrated into the framework to handle feature extraction for complex processes involving multiple entities without data flattening?
- Basis in paper: The Conclusion notes that future extensions will include "object-centric process mining, which allows for focusing on the activities of the different entities involved in a process instead of flattening the event data."
- Why unresolved: The current methodology assumes a traditional event log structure (Definition 3.1), which flattens interactions, potentially losing information about complex relationships between different object types.
- What evidence would resolve it: An implementation of the framework using object-centric feature extraction techniques, demonstrating maintained or improved explainability and F1-scores on a dataset with complex, intertwined object interactions.

### Open Question 3
- Question: Can a meta-learning approach be developed to predict the optimal feature extraction technique (e.g., alignment-based vs. token-based) for a given dataset a priori?
- Basis in paper: The analysis of variance showed that the interaction between the dataset and the feature extraction approach is statistically significant and explains a large portion of the variance in F1-scores, leading to the conclusion that "no one-size-fits-all process mining-based feature extraction approach is suitable."
- Why unresolved: The paper demonstrates that performance varies by dataset (e.g., TB CC outperformed AB CC on ERTMS but not PDC 2020), but does not provide a method for selecting the best technique without testing all combinations.
- What evidence would resolve it: A model that can analyze the statistical properties of a new event log and accurately recommend the specific feature extraction method (PF) that will maximize the F1-score before the training phase begins.

## Limitations
- Exact Autoencoder architecture details (beyond "three fully-connected layers") are not specified, which could impact reproducibility and performance comparisons.
- Dataset processing specifics for the small COVAS dataset (random resampling) and ERTMS trace modifications are unclear.
- The performance superiority over baseline techniques is well-demonstrated, though the specific contribution of each component (feature extraction vs. dimensionality reduction) is not isolated.

## Confidence
- **High Confidence:** The framework's core concept of combining alignment-based feature extraction with dimensionality reduction for anomaly detection is well-supported by experimental results across multiple datasets.
- **Medium Confidence:** The explainability claims through SHAP values are theoretically sound but have limited empirical validation in the results section.
- **Medium Confidence:** The performance superiority over baseline techniques is well-demonstrated, though the specific contribution of each component (feature extraction vs. dimensionality reduction) is not isolated.

## Next Checks
1. **Architecture Sensitivity:** Test how varying the Autoencoder architecture (number of layers, neurons) affects performance on a single dataset to determine if the current configuration is optimal.
2. **Baseline Comparison Rigor:** Implement and compare against alternative baseline approaches (e.g., isolation forests on raw trace features) to isolate the value-add of the alignment-based features.
3. **Explainability Validation:** For a sample of flagged anomalies, manually verify that the top SHAP-attributed activities correspond to intuitive process errors (e.g., missing or out-of-order activities).