---
ver: rpa2
title: Video Generators are Robot Policies
arxiv_id: '2508.00795'
source_url: https://arxiv.org/abs/2508.00795
tags:
- video
- policy
- robot
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that video generation can serve as an effective
  proxy for learning robot policies, substantially improving robustness and generalization
  compared to traditional behavior cloning. The authors propose Video Policy, a modular
  framework that jointly predicts videos of robot behavior and corresponding actions
  using diffusion models.
---

# Video Generators are Robot Policies

## Quick Facts
- arXiv ID: 2508.00795
- Source URL: https://arxiv.org/abs/2508.00795
- Authors: Junbang Liang, Pavel Tokmakov, Ruoshi Liu, Sruthi Sudhakar, Paarth Shah, Rares Ambrus, Carl Vondrick
- Reference count: 40
- One-line primary result: Video Policy achieves 66% average task success rate on RoboCasa with 50 demonstrations, outperforming methods requiring 300-3000 demonstrations

## Executive Summary
This paper demonstrates that video generation can serve as an effective proxy for learning robot policies, substantially improving robustness and generalization compared to traditional behavior cloning. The authors propose Video Policy, a modular framework that jointly predicts videos of robot behavior and corresponding actions using diffusion models. By leveraging strong video priors from large-scale generative models, their method achieves state-of-the-art performance on RoboCasa and Libero10 simulation benchmarks with only 50 demonstrations per task, outperforming methods that use 300-3000 demonstrations. The method also demonstrates strong real-world generalization across variations in object locations, appearances, and backgrounds.

## Method Summary
Video Policy is a two-stage training framework that leverages diffusion models for both video and action prediction. In stage 1, the method fine-tunes Stable Video Diffusion (SVD) to generate robot execution videos conditioned on initial observations and task descriptions. In stage 2, it freezes the video U-Net and trains an action denoising head that predicts 7-DOF actions (6-DoF pose + gripper state) from the video decoder features. The framework uses a CNN adapter to extract features from specific layers of the video decoder and conditions a 1D CNN action U-Net on these features. The method employs 8 cameras (gripper, left, right views) concatenated temporally as input, and predicts a 32-step action horizon with 30 denoising steps at inference time.

## Key Results
- Achieves 66% average task success rate on RoboCasa validation set, surpassing baselines including Diffusion Policy, GR00T, and Unified Video Action Model
- Outperforms methods using 300-3000 demonstrations with only 50 demonstrations per task
- Demonstrates strong real-world generalization across variations in object locations, appearances, and backgrounds
- Ablation studies show learning to generate videos is both necessary and sufficient for learning robust manipulation policies

## Why This Works (Mechanism)
Video Policy works by leveraging the rich visual priors learned by large-scale video generation models. By first predicting what a successful robot execution looks like, the model can then extract corresponding actions from these predicted videos. This two-stage approach allows the model to benefit from the strong video generation capabilities of diffusion models while still producing precise action sequences for robot control. The method's success stems from the fact that video prediction inherently captures the temporal and spatial relationships needed for robot manipulation, while also providing a natural way to incorporate task descriptions through text conditioning.

## Foundational Learning
- **Diffusion models**: Why needed - generate high-quality videos and actions through iterative denoising. Quick check - validate that video predictions improve with more denoising steps.
- **Stable Video Diffusion (SVD)**: Why needed - provides strong video generation priors. Quick check - compare performance with and without SVD pretraining.
- **Multi-view video input**: Why needed - captures complete robot workspace from multiple perspectives. Quick check - evaluate performance with single vs. multiple camera views.
- **Two-stage training**: Why needed - prevents catastrophic forgetting and allows specialized training for video and action heads. Quick check - compare joint vs. two-stage training performance.
- **Text conditioning**: Why needed - enables task specification and generalization. Quick check - test performance with and without task descriptions.
- **Action denoising head**: Why needed - converts video predictions to actionable robot commands. Quick check - validate that action predictions correspond to predicted videos.

## Architecture Onboarding

Component map: Initial observation + Text description -> Video U-Net (SVD-based) -> CNN adapter -> Action U-Net -> 7-DOF actions

Critical path: v0,c -> video U-Net layers 9,14,17,20,23 -> CNN adapter -> h_i -> Action U-Net -> actions

Design tradeoffs:
- Joint vs. two-stage training: Two-stage training (0.63 success rate) outperforms joint training (0.57), likely due to preventing interference between video and action objectives
- Prediction horizon: Longer horizons improve generalization to distribution shifts but increase computational cost
- Camera configuration: Multiple views provide better workspace coverage but increase input complexity

Failure signatures:
- Poor video predictions lead to incorrect action predictions - diagnose by visualizing predicted vs. actual videos
- Action predictions not aligning with video content - check CNN adapter feature extraction quality
- Overfitting to training demonstrations - evaluate on held-out tasks with distribution shift

Three first experiments:
1. Validate that predicted videos improve with more denoising steps and better match ground truth demonstrations
2. Compare action predictions from video features vs. direct observation features to verify the video generation proxy is beneficial
3. Test performance on tasks with increasing amounts of object and layout variation to characterize generalization limits

## Open Questions the Paper Calls Out
None

## Limitations
- Critical CNN adapter architecture between video and action heads remains underspecified, creating potential barriers to faithful reproduction
- Real-world generalization claims are demonstrated on a relatively small set of tasks, limiting confidence in broader applicability
- The sufficiency claim for video generation is somewhat overstated given the method still requires action conditioning for deployment

## Confidence
- Performance claims on simulation benchmarks: **High**
- Data efficiency claims: **High**  
- Real-world generalization claims: **Medium** (limited task diversity)
- Claims about necessity/sufficiency of video generation: **Medium** (ablation evidence strong but theoretical claims broad)
- Claims about modular design advantages: **Medium** (design rationale clear but alternatives not extensively explored)

## Next Checks
1. **Cross-task generalization**: Evaluate Video Policy on tasks outside the RoboCasa/Libero10 distributions (e.g., novel object shapes, unseen object combinations) to test the limits of video-based generalization beyond stated benchmarks.

2. **Long-horizon planning**: Test the method's ability to chain together sequences of subtasks requiring multi-step reasoning, as the current evaluation focuses on relatively short task horizons.

3. **Resource efficiency analysis**: Quantify computational requirements during both training and inference, including memory usage and wall-clock time per rollout, to better understand practical deployment constraints.