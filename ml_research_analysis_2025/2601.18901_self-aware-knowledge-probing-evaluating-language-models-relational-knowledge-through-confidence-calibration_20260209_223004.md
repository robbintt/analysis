---
ver: rpa2
title: 'Self-Aware Knowledge Probing: Evaluating Language Models'' Relational Knowledge
  through Confidence Calibration'
arxiv_id: '2601.18901'
source_url: https://arxiv.org/abs/2601.18901
tags:
- confidence
- calibration
- accuracy
- knowledge
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel calibration probing framework to
  assess how well language models are aware of their own knowledge boundaries when
  answering factual questions. Unlike traditional knowledge probes that focus solely
  on accuracy, this work evaluates three dimensions of model confidence: intrinsic
  confidence from log-probabilities, structural consistency across semantically equivalent
  prompts, and semantic grounding via explicit uncertainty markers.'
---

# Self-Aware Knowledge Probing: Evaluating Language Models' Relational Knowledge through Confidence Calibration

## Quick Facts
- **arXiv ID**: 2601.18901
- **Source URL**: https://arxiv.org/abs/2601.18901
- **Reference count**: 27
- **Primary result**: Most language models, especially masked language models, are overconfident in their factual knowledge, with minimum-confidence aggregation across paraphrases providing the best calibration.

## Executive Summary
This paper introduces a novel calibration probing framework to assess how well language models are aware of their own knowledge boundaries when answering factual questions. Unlike traditional knowledge probes that focus solely on accuracy, this work evaluates three dimensions of model confidence: intrinsic confidence from log-probabilities, structural consistency across semantically equivalent prompts, and semantic grounding via explicit uncertainty markers. Experiments on 16 models (10 causal and 6 masked language models) reveal that most models are overconfident, with masked language models showing particularly poor calibration. The best-calibrated estimates come from minimum-confidence aggregation across multiple rephrasings, which effectively mitigates overconfidence. The study also finds that larger models begin to align their confidence with linguistic uncertainty cues, though even the largest models struggle to integrate verbalized uncertainty accurately. Overall, the results highlight a significant "reliability gap" in current models' epistemic self-awareness, suggesting the need for improved calibration strategies in factual retrieval systems.

## Method Summary
The framework evaluates language model calibration on factual knowledge using closed-set probing with BEAR benchmark data. For each (subject, relation) pair, 5 paraphrased cloze-style templates are generated. Models score candidate answers using either sum of token log-likelihoods (CLMs) or pseudo-log-likelihood with iterative masking (MLMs). Six confidence estimators are computed: C_Base (max softmax), C_Margin (top-1 minus top-2), C_Average (avg over templates), and their consistency variants. Calibration is measured using Adaptive Calibration Error (ACE) with 20 quantile bins, Brier Score, and calibration curves. The framework tests structural consistency across rephrasings and semantic grounding with uncertainty markers like "possibly" and "certainly."

## Key Results
- Most models, especially masked language models, are overconfident in their factual knowledge.
- Minimum-confidence aggregation across multiple rephrasings produces the best-calibrated confidence estimates.
- Causal language models show significantly better calibration than masked language models at comparable accuracy levels.
- Larger models begin to align their confidence with linguistic uncertainty cues, though even the largest models struggle with verbalized uncertainty.

## Why This Works (Mechanism)

### Mechanism 1: Minimum-Confidence Aggregation Mitigates Overconfidence
- Claim: Selecting the minimum confidence across multiple semantically equivalent rephrasings produces the best-calibrated confidence estimates.
- Mechanism: Aggregating across 5 templates per relation and taking the minimum CBase score exposes prompt-sensitivity artifacts; if any formulation yields low confidence, the aggregation reflects this uncertainty rather than masking it with high-confidence outliers.
- Core assumption: Models that truly "know" a fact should show consistent high confidence across paraphrases; inconsistency signals epistemic uncertainty.
- Evidence anchors:
  - [abstract]: "The best-calibrated scores come from confidence estimates that account for inconsistencies due to statement rephrasing."
  - [section 5.1]: "CMinAverage achieves an ACE of 0.024 [for opt-6.7b], which is close to perfect calibration... Minimum confidence aggregation reduces ACE and Brier Score more effectively than plurality voting."
  - [corpus]: C²GSPG paper corroborates overconfidence as a barrier to self-aware reasoning.

### Mechanism 2: Margin-Confidence Extends Calibration to Lower Confidence Ranges
- Claim: The difference between top-1 and top-2 softmax probabilities (margin) provides better-calibrated signals than raw maximum probability alone.
- Mechanism: CBase produces right-skewed distributions with few low-confidence predictions; CMargin spreads scores across [0,1] by capturing decision boundary proximity rather than just peak confidence.
- Core assumption: Models should distinguish clearly between correct answers and distractors; narrow margins indicate uncertainty even when peak probability is high.
- Evidence anchors:
  - [section 3.1]: "CMargin computes the difference between the largest and second-largest value returned by the softmax, reflecting whether the model's answer is clearly preferred."
  - [section 5.1]: "CBase does not produce enough scores in the lower confidence ranges... CMargin, however, extends the curve into these lower confidence regions."
  - [corpus]: Related work on output distribution characteristics (arXiv 2506.00637) explores calibration via distribution shape.

### Mechanism 3: Pre-training Objective Influences Calibration Capacity
- Claim: Causal language models (CLMs) exhibit significantly better calibration than masked language models (MLMs) at comparable accuracy levels.
- Mechanism: CLMs are trained to model full sequence log-likelihoods autoregressively; MLMs require pseudo-log-likelihood (PLL) approximation via iterative masking, which may not produce well-calibrated sentence-level scores.
- Core assumption: The training objective shapes how models represent uncertainty; autoregressive next-token prediction aligns better with confidence estimation than masked token prediction.
- Evidence anchors:
  - [abstract]: "Most models, especially those pre-trained with the masking objective, are overconfident."
  - [section 6]: "MLMs do not explicitly model a sentence-level log-likelihood during their pre-training phase... the absence of a well-defined sentence-level log-likelihood is the main reason behind the large calibration discrepancies."
  - [corpus]: Related work on metacognitive sensitivity (arXiv 2512.10451) notes poor calibration as a cognitive bias in models.

## Foundational Learning

- Concept: Adaptive Calibration Error (ACE)
  - Why needed here: Unlike Expected Calibration Error with fixed bins, ACE uses quantile binning to handle confidence distributions that are not uniformly distributed—critical for LMs where confidence scores cluster at extremes.
  - Quick check question: If your model outputs 90% of predictions with confidence >0.9, why would fixed-width ECE give misleading results?

- Concept: Pseudo-Log-Likelihood (PLL) for MLMs
  - Why needed here: To compare MLMs and CLMs fairly, you must understand how PLL estimates sentence likelihoods by iteratively masking each token—this approximation underlies the calibration gap between architectures.
  - Quick check question: Why can't you simply sum token probabilities for MLM sentence scoring as you do for CLMs?

- Concept: Closed-Set vs. Open-Ended Probing
  - Why needed here: This framework relies on constrained answer sets (e.g., 60 candidate countries for "place of death") to compute softmax-normalized confidence; open-ended generation lacks this structure for calibration.
  - Quick check question: What calibration metric would you need if answers were free-form text rather than multiple-choice?

## Architecture Onboarding

- Component map: Factual triplets (subject, relation, object) + K candidate objects → Template engine (5 paraphrased templates) → Scoring layer (CLM: sum log-likelihoods; MLM: PLL) → Confidence estimators (6 variants) → Calibration metrics (ACE, Brier Score)

- Critical path:
  1. For each (subject, relation) pair, generate K candidate statements using templates
  2. Compute log-likelihoods for all K candidates across all 5 templates
  3. Apply softmax normalization to get per-template confidence distributions
  4. Aggregate across templates (min-confidence or voting) to get final prediction and confidence
  5. Compute ACE by binning predictions and comparing confidence vs. accuracy

- Design tradeoffs:
  - More answer options: Improves calibration (ACE drops) but increases compute (60 options = 60× template evaluations)
  - Stricter voting (unanimity): Higher discriminative power (Brier ~0.05) but rejects >60% of samples
  - Mean vs. Sum token reduction: Mean improves calibration but crushes confidence distribution (opt-1.3b confidences never exceed 0.5)

- Failure signatures:
  - ACE >0.2 with CBase: Model is severely overconfident; try CMargin or structural consistency
  - Confidence distribution clustered at extremes with no spread: Reduction strategy inappropriate
  - Large accuracy drop when injecting "possibly"/"certainly": Model lacks semantic grounding for epistemic markers
  - MLM ACE significantly higher than CLM at same accuracy: Expected due to PLL limitations

- First 3 experiments:
  1. Baseline calibration audit: Run CBase and CMargin on BEAR's 1:1 relations (60 options) for your model; report ACE and plot calibration curves to identify overconfidence regions.
  2. Structural consistency test: Generate 5 paraphrased templates for a held-out relation; compare CMinAverage vs. CVoteAverage to quantify prompt-sensitivity costs in your domain.
  3. Answer option scaling: Sample subsets of 5, 10, 20, 40, 60 answer options from 1:1 relations; plot ACE vs. K to determine minimum options needed for stable calibration in your setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the calibration of relational knowledge behave in relations with multiple valid answers (N:M relations) compared to the 1:1 and N:1 relations currently evaluated?
- Basis in paper: [explicit] The paper states in the Limitations section that "BEAR contains no N:M relations" and that a complete assessment would require evaluating calibration over all answer options for such relations.
- Why unresolved: The current framework relies on top-label calibration (accuracy of the single best prediction), which does not capture the nuance required when multiple distinct answers are valid for a single subject.
- What evidence would resolve it: An extension of the calibration probing framework to datasets containing N:M relations, utilizing metrics like Brier Score with one-hot-encoded ground-truth labels or Static Calibration Error to account for multiple correct outputs.

### Open Question 2
- Question: To what extent do calibration error metrics derived from closed-set diagnostic probing transfer to practical, open-ended generation settings?
- Basis in paper: [explicit] The authors note in the Limitations that their calibration probing is "specific to closed-answer set scenarios" and "unlikely to transfer directly to more practical settings" which lack predefined answer options.
- Why unresolved: Practical applications often involve free-form text generation where structural consistency across rephrasings cannot be easily derived without a constrained set of candidate answers.
- What evidence would resolve it: A correlation analysis between calibration scores on the BEAR dataset and performance/hallucination rates in open-ended downstream tasks (e.g., biography generation) to see if the diagnostic metrics predict real-world reliability.

### Open Question 3
- Question: What are the fundamental architectural or training dynamics that cause Masked Language Models (MLMs) to be significantly less calibrated than Causal Language Models (CLMs) despite having comparable factual accuracy?
- Basis in paper: [inferred] The paper demonstrates a "reliability gap" where MLMs are "fundamentally less aware," hypothesizing that the absence of explicit sentence-level log-likelihood modeling (relying instead on pseudo log-likelihoods) is the cause, but this is not confirmed.
- Why unresolved: The paper establishes the empirical correlation but does not isolate whether the poor calibration is an inherent property of the masking objective or a artifact of the pseudo-log-likelihood scoring method required for MLMs.
- What evidence would resolve it: Ablation studies using unidirectional MLMs or evaluating CLMs with pseudo-log-likelihood scoring to isolate the impact of the scoring function from the model architecture.

## Limitations
- The framework is restricted to closed-answer set scenarios and may not transfer directly to open-ended generation settings.
- The study evaluates calibration over output uncertainty but does not address input uncertainty from vaguely formulated prompts.
- BEAR benchmark contains no N:M relations, limiting the framework's applicability to relations with multiple valid answers.

## Confidence

- High Confidence: Claims about CLMs outperforming MLMs in calibration are well-supported by systematic comparisons across 16 models and multiple metrics. The mechanism linking training objectives to calibration capacity is mechanistically sound.
- Medium Confidence: The superiority of minimum-confidence aggregation over voting is demonstrated empirically but may depend on specific template quality and semantic equivalence assumptions.
- Low Confidence: The semantic grounding results are suggestive but limited by the narrow set of uncertainty markers tested. Models' inability to integrate verbalized uncertainty may reflect prompt engineering limitations rather than fundamental epistemic awareness gaps.

## Next Checks

1. **Cross-lingual calibration consistency**: Apply the framework to multilingual models (e.g., mBERT, XLM-R) to test whether calibration patterns generalize beyond English factual knowledge.
2. **Adversarial template robustness**: Systematically perturb templates with controlled semantic drift to quantify how minimum-confidence aggregation responds to paraphrases that introduce factual ambiguity.
3. **Calibration transfer learning**: Fine-tune poorly calibrated MLMs on CLM-style autoregressive objectives for a small number of steps and measure calibration improvement to isolate training objective effects from architectural differences.