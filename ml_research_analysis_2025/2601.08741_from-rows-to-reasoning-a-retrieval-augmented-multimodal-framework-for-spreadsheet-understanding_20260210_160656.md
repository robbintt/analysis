---
ver: rpa2
title: 'From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet
  Understanding'
arxiv_id: '2601.08741'
source_url: https://arxiv.org/abs/2601.08741
tags:
- reasoning
- frtr
- multimodal
- spreadsheet
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FRTR, a retrieval-augmented framework for
  reasoning over large-scale enterprise spreadsheets that integrates multimodal retrieval
  with hybrid lexical-dense search. Unlike prior approaches that compress entire workbooks
  into prompts, FRTR decomposes spreadsheets into granular row, column, and block
  embeddings and retrieves only the most relevant evidence before reasoning.
---

# From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding

## Quick Facts
- arXiv ID: 2601.08741
- Source URL: https://arxiv.org/abs/2601.08741
- Authors: Anmol Gulati; Sahil Sen; Waqar Sarguroh; Kevin Paul
- Reference count: 37
- Key outcome: FRTR achieves 74% accuracy on FRTR-Bench and 87% on SpreadsheetLLM, outperforming prior methods by 50 percentage points while reducing token usage by ~50%

## Executive Summary
FRTR introduces a retrieval-augmented framework for reasoning over large-scale enterprise spreadsheets by integrating multimodal retrieval with hybrid lexical-dense search. Unlike prior approaches that compress entire workbooks into prompts, FRTR decomposes spreadsheets into granular row, column, and block embeddings and retrieves only the most relevant evidence before reasoning. The authors created FRTR-Bench, a new benchmark of 30 enterprise-grade Excel workbooks with nearly four million cells and 50+ embedded images, requiring cross-sheet and multimodal reasoning. FRTR achieves 74% answer accuracy on FRTR-Bench using Claude Sonnet 4.5, outperforming the prior state-of-the-art (24%) by 50 percentage points.

## Method Summary
FRTR processes spreadsheets through a "Retrieve, Verify, Compose" pipeline that avoids the limitations of prompt compression. The framework first chunks workbooks into smaller semantic units (rows, columns, blocks) and encodes them using a hybrid lexical-dense retriever with Amazon Titan Multimodal embeddings. For each query, it retrieves relevant evidence chunks using cosine similarity, then performs LLM-based reasoning over the retrieved content to generate JSON responses. The system handles cross-sheet and multimodal queries by retrieving from multiple sheets and processing embedded images. FRTR-Bench was constructed to evaluate this approach, containing 30 enterprise-grade workbooks with complex analytical tasks requiring both numerical and visual reasoning.

## Key Results
- FRTR achieves 74% accuracy on the new FRTR-Bench benchmark using Claude Sonnet 4.5
- Outperforms prior state-of-the-art (24%) by 50 percentage points on FRTR-Bench
- Achieves 87% accuracy on SpreadsheetLLM benchmark with GPT-5 while reducing token usage by ~50% compared to compression-based methods
- Demonstrates robustness across six different LLMs, including GPT-4, Claude Sonnet 4.5, and Llama-3.1-70B-Instruct

## Why This Works (Mechanism)
FRTR's effectiveness stems from its granular decomposition approach that avoids the context window limitations of previous methods. By breaking spreadsheets into semantically meaningful chunks and retrieving only relevant evidence, the framework maintains high fidelity while dramatically reducing computational overhead. The hybrid lexical-dense retrieval strategy combines traditional keyword matching with semantic similarity, enabling more precise targeting of relevant information. This approach is particularly effective for enterprise workbooks containing millions of cells where prompt compression would lose critical contextual information.

## Foundational Learning
- **Multimodal Embeddings**: Dense vector representations that capture both visual and textual semantics, needed to process embedded images alongside numerical data. Quick check: Verify the embedding model can handle both spreadsheet cell content and image modalities within the same vector space.
- **Hybrid Retrieval**: Combination of lexical (keyword-based) and dense (semantic) search methods, needed to balance precision and recall when retrieving relevant spreadsheet chunks. Quick check: Compare retrieval performance using only lexical vs. only dense methods on a sample dataset.
- **Chunk-based Processing**: Dividing large workbooks into smaller semantic units, needed to overcome context window limitations while preserving analytical relationships. Quick check: Test different chunk sizes to find optimal balance between context retention and retrieval efficiency.
- **Cross-sheet Reasoning**: Ability to identify and retrieve evidence from multiple worksheets, needed for complex analytical queries spanning entire workbooks. Quick check: Create queries requiring information from at least three different sheets to verify cross-referencing capability.
- **Sparse Attention Mechanisms**: Efficient processing of long sequences by focusing on relevant portions, needed to handle the computational complexity of large spreadsheets. Quick check: Measure token usage and response time as workbook size increases.
- **Agent-based Spreadsheet Processing**: Framework design that separates retrieval, verification, and composition phases, needed to maintain modularity and enable systematic debugging. Quick check: Trace a query through each pipeline stage to verify proper data flow.

## Architecture Onboarding

**Component Map**
User Query -> Hybrid Retriever -> Chunk Store -> LLM Reasoning Engine -> JSON Response

**Critical Path**
Query processing involves: 1) Hybrid retrieval to identify relevant chunks, 2) LLM reasoning over retrieved evidence, 3) JSON response generation with verification steps.

**Design Tradeoffs**
The framework prioritizes retrieval precision over comprehensiveness, accepting potential missed evidence in exchange for computational efficiency and reduced hallucination risk. This tradeoff enables processing of enterprise-scale workbooks but may miss subtle relationships between distant cells.

**Failure Signatures**
Common failure modes include: retrieval of irrelevant chunks leading to hallucinated answers, inability to capture cross-chunk relationships that span beyond retrieval windows, and limitations in handling extremely complex formulas that require execution rather than reasoning.

**First Experiments**
1. Test retrieval accuracy on a small workbook with known ground truth answers to verify chunk selection.
2. Evaluate LLM reasoning quality on retrieved evidence using simple numerical queries before scaling to complex multimodal tasks.
3. Measure token efficiency by comparing FRTR against baseline prompt compression methods on workbooks of increasing size.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can FRTR be extended from a read-only reasoning system to an agentic framework capable of directly modifying spreadsheets?
- Basis in paper: [explicit] The Conclusion states, "Future work should explore... agentic spreadsheet manipulation."
- Why unresolved: The current architecture (Algorithm 1) functions as a "Retrieve, Verify, Compose" pipeline that generates JSON outputs for human oversight but explicitly does not execute formulas or alter the workbook (Section 3.5).
- What evidence would resolve it: The implementation and evaluation of a closed-loop "Act" phase where the LLM generates executable code (e.g., OfficeScript) to modify cells, verified against a benchmark requiring state changes.

### Open Question 2
- Question: To what extent can adaptive retrieval strategies improve performance or efficiency over the static hyperparameters used in the current study?
- Basis in paper: [explicit] The Conclusion lists "adaptive retrieval" as a key area for future work.
- Why unresolved: The framework relies on fixed retrieval parameters ($K_v=20, K_s=20$) and a static windowing heuristic ($\sqrt{N/K_{target}}$) for all queries, regardless of complexity or workbook structure.
- What evidence would resolve it: An ablation study comparing fixed vs. dynamic retrieval settings (e.g., query-aware chunk sizing or dynamic top-k values) to measure accuracy and token efficiency impacts.

### Open Question 3
- Question: Would specialized fine-tuning for multimodal alignment yield significant gains over the general-purpose embeddings currently employed?
- Basis in paper: [explicit] The Conclusion suggests investigating "finer multimodal alignment."
- Why unresolved: FRTR currently uses a general off-the-shelf encoder (Amazon Titan Multimodal); it is unclear if this generic embedding space optimally captures the specific semantic relationships between numerical data and embedded financial charts or receipts.
- What evidence would resolve it: A comparative evaluation of the current model against a version utilizing domain-specific vision encoders on the image-based subset of FRTR-Bench.

### Open Question 4
- Question: How reliable is the generated formula logic without an execution engine to verify syntax and references within the live spreadsheet environment?
- Basis in paper: [inferred] Section 3.5 states that FRTR "does not execute formulas," relying instead on LLM reasoning to produce text/JSON, while Section 5.2 notes that correctness is judged by "numerical consistency" or "functional identity" rather than successful runtime execution.
- Why unresolved: Generating a formula that looks correct (textually) but fails upon execution (e.g., due to subtle syntax errors or invalid references) remains a risk that the current evaluation protocol may not fully capture.
- What evidence would resolve it: An execution-based evaluation metric where generated formulas are run directly in the spreadsheet software to check for runtime errors and valid output.

## Limitations
- The evaluation relies entirely on proprietary benchmarks constructed by the authors, lacking independent validation
- The framework's performance on established public spreadsheet reasoning benchmarks has not been tested
- The current implementation cannot modify or execute formulas within the spreadsheet environment
- The complexity and diversity of reasoning types in FRTR-Bench may not fully represent all real-world spreadsheet use cases

## Confidence
- FRTR accuracy improvements: High
- Efficiency gains (token reduction): High
- Benchmark representativeness: Medium
- Cross-LLM robustness: Medium
- Real-world applicability: Medium

## Next Checks
1. Replicate FRTR's performance on FRTR-Bench using an independent third-party team to verify the reported accuracy gains and efficiency improvements.
2. Evaluate FRTR on established public spreadsheet reasoning benchmarks (e.g., INFOTABS, TAT-QA) to assess generalizability beyond the authors' proprietary datasets.
3. Conduct ablation studies removing the multimodal retrieval component to quantify the contribution of image understanding to overall performance.