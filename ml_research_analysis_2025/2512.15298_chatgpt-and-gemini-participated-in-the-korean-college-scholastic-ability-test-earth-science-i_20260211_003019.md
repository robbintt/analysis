---
ver: rpa2
title: ChatGPT and Gemini participated in the Korean College Scholastic Ability Test
  -- Earth Science I
arxiv_id: '2512.15298'
source_url: https://arxiv.org/abs/2512.15298
tags:
- reasoning
- data
- gemini
- errors
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study analyzed how generative AI models perform on a high-stakes\
  \ Earth Science exam. It tested three LLMs\u2014GPT-4o, Gemini 2.5 Flash, and Gemini\
  \ 2.5 Pro\u2014under three input conditions: full-page, individual item, and optimized\
  \ multimodal."
---

# ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I

## Quick Facts
- arXiv ID: 2512.15298
- Source URL: https://arxiv.org/abs/2512.15298
- Authors: Seok-Hyun Ga; Chun-Yen Chang
- Reference count: 0
- Primary result: Generative AI models exhibit fundamental reasoning flaws on high-stakes Earth Science exams, with Perception-Cognition Gap and Process Hallucination being dominant error types.

## Executive Summary
This study evaluated how three leading multimodal LLMs—GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro—performed on the 2025 Korean CSAT Earth Science I exam under varying input conditions. The research revealed that while models can recognize visual data, they struggle to interpret scientific concepts and symbolic meanings, particularly when processing complex documents as single images. The findings demonstrate that current AI systems possess significant gaps between perception and cognition, and tend to rely on background knowledge rather than visual verification when answering scientific questions.

## Method Summary
The study tested three LLMs on 20 multiple-choice questions from the 2025 CSAT Earth Science I exam under three input conditions: full-page images, individually cropped items, and optimized multimodal inputs with separate text and figure components. Models were accessed via Google AI Studio and VS Code extensions, using a standardized prompt requesting problem transcription, answer identification, and solution explanation. Performance was measured by accuracy scores, while qualitative error analysis categorized incorrect responses into four main types with eight subcategories.

## Key Results
- Input structure significantly impacts performance: optimized multimodal inputs showed substantially better results than full-page images due to reduced OCR and segmentation failures.
- Perception-Cognition Gap emerged as the most common error type (43.06%), where models recognized visual data but failed to connect it to scientific concepts.
- Models exhibited "Calculation-Conceptualization Discrepancies," performing calculations correctly but drawing invalid scientific conclusions from the results.

## Why This Works (Mechanism)

### Mechanism 1: Input Structure Sensitivity
LLM performance in scientific assessment is highly conditional on the granularity and structure of the input modality. When complex documents are fed as single images, the model's vision encoder fails to segment distinct problem boundaries, leading to context mixing. Providing isolated items offloads the layout analysis task, allowing the model to allocate capacity to reasoning.

### Mechanism 2: The Perception-Cognition Gap
Multimodal LLMs often successfully execute "perception" (pixel/OCR recognition) but fail at "cognition" (mapping visual symbols to scientific rules). The model identifies visual features but lacks a grounded "symbolic interpreter" to map these features to specific scientific constraints.

### Mechanism 3: Process Hallucination via Reasoning Bypass
Models tend to skip difficult visual verification steps in favor of generating plausible, text-based reasoning paths derived from pre-training priors. Autoregressive generation favors high-probability token sequences (plausible scientific explanations) over low-probability logical steps derived strictly from specific visual inputs.

## Foundational Learning

- **Concept:** Multimodal Information Fusion
  - **Why needed here:** To understand why performance degraded in Experiment 1. You must distinguish between feature extraction and fusion.
  - **Quick check question:** Does the model fail to see the text (OCR error) or fail to bind the text to the correct question number (Segmentation error)?

- **Concept:** Symbolic vs. Sub-symbolic Reasoning
  - **Why needed here:** The paper highlights "Schematic Misinterpretation." Understanding this requires distinguishing between recognizing a shape and applying a scientific law to it.
  - **Quick check question:** If a model sees a triangle, does it output "triangle" (perception) or "warning sign" (symbolic interpretation)?

- **Concept:** Hallucination Taxonomy
  - **Why needed here:** The study distinguishes "Factual Hallucination" from "Process Hallucination." Designing fixes requires knowing which type is occurring.
  - **Quick check question:** Did the model invent a fake fact, or did it use a real fact in the wrong place because it skipped checking the data?

## Architecture Onboarding

- **Component map:** Vision Encoder -> Cross-Modal Aligner -> LLM Backbone -> Text Generator
- **Critical path:** The Projection and Reasoning Core interface is the current bottleneck. If the projection passes visual data without symbolic tags, the Reasoning Core defaults to text-based priors.
- **Design tradeoffs:**
  - Full-page Input: Lower preprocessing cost, but catastrophic failure risk due to segmentation noise.
  - Optimized Input: High manual/automated preprocessing cost, but isolates true reasoning capability from vision failures.
- **Failure signatures:**
  - Perception-Cognition Gap: The model describes the image perfectly but answers the question incorrectly.
  - Calculation-Concept Discrepancy: The model outputs correct math but draws a scientifically invalid conclusion.
- **First 3 experiments:**
  1. Establish Baseline (Repro): Replicate Experiment 1 to confirm OCR/segmentation brittleness.
  2. Isolate Reasoning: Replicate Experiment 3 using 5 "Schematic Misinterpretation" questions to test if prompting reduces the Perception-Cognition Gap.
  3. Stress Test: Input a question with deliberately contradictory visual vs. text data to see if the model relies on text priors.

## Open Questions the Paper Calls Out

### Open Question 1
Do the error patterns identified (Perception-Cognition Gap, Calculation-Conceptualization Discrepancy) generalize to other scientific domains like Physics, Chemistry, and Biology? The study was limited to Earth Science I.

### Open Question 2
Can advanced prompting techniques such as "Chain of Thought" or "Self-reflection" mitigate specific AI errors like "Process Hallucination"? The study utilized a simple, standard prompt without employing reasoning-enhancing strategies.

### Open Question 3
Do human students exhibit the same cognitive errors as LLMs, specifically the "Calculation-Conceptualization Discrepancy"? The current study focused on AI output and did not include a comparative human control group.

## Limitations
- Error taxonomy analysis was conducted manually on a relatively small sample of 20 questions from a single year's exam.
- The study does not report inter-rater reliability for the qualitative coding process.
- The "optimized multimodal input" condition required extensive manual processing, which may not scale practically for real-world applications.

## Confidence
- **High Confidence:** The quantitative finding that input structure significantly affects model performance is well-supported by the experimental design and results.
- **Medium Confidence:** The qualitative error taxonomy and specific error types are plausible but limited by the manual analysis approach and small sample size.
- **Low Confidence:** The assertion that these error types represent fundamental limitations rather than solvable technical challenges remains speculative without additional experimental evidence.

## Next Checks
1. **Reproducibility Check:** Replicate Experiment 1 and Experiment 3 with at least 50 questions from multiple years of CSAT Earth Science exams to verify the input structure effect and error type frequencies.

2. **Prompt Engineering Validation:** Test whether specific prompting strategies (e.g., Chain-of-Thought, explicit visual verification steps) can reduce "Process Hallucination" and "Perception-Cognition Gap" errors.

3. **Automated Error Classification:** Develop and validate an automated error classification system using the proposed taxonomy, then apply it to a larger dataset to assess the reliability and generalizability of the error type framework.