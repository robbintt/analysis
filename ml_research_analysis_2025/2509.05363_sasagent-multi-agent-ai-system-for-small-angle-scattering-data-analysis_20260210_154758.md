---
ver: rpa2
title: 'SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis'
arxiv_id: '2509.05363'
source_url: https://arxiv.org/abs/2509.05363
tags:
- data
- scattering
- user
- agent
- fitting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SasAgent, a multi-agent AI system powered
  by large language models (LLMs) that automates small-angle scattering (SAS) data
  analysis by leveraging tools from the SasView software. The system features a coordinator
  agent that interprets user prompts and delegates tasks to three specialized agents
  for scattering length density (SLD) calculation, synthetic data generation, and
  experimental data fitting.
---

# SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis

## Quick Facts
- arXiv ID: 2509.05363
- Source URL: https://arxiv.org/abs/2509.05363
- Reference count: 40
- Primary result: Multi-agent LLM system automates SAS data analysis via text prompts, achieving R² = 0.9479 and RMSE = 0.003169 for polymer fitting.

## Executive Summary
This paper introduces SasAgent, a multi-agent AI system that automates small-angle scattering (SAS) data analysis by leveraging large language models (LLMs) and tools from the SasView software. The system features a coordinator agent that interprets user prompts and delegates tasks to three specialized agents for scattering length density (SLD) calculation, synthetic data generation, and experimental data fitting. Through diverse examples, the authors demonstrate SasAgent's ability to interpret complex prompts, calculate SLDs, generate accurate scattering data, and fit experimental datasets with high precision. The work showcases the potential of LLM-driven AI systems to streamline scientific workflows and enhance automation in SAS research.

## Method Summary
SasAgent implements a two-layer, four-agent architecture using CrewAI. A coordinator agent interprets user intent and routes tasks to three specialized agents: SLD, generation, and fitting. Each agent is equipped with LLM-friendly tools wrapped around SasView functions, including SLD calculator, model data generator, bump fitting tool, and a RAG documentation tool built from crawled SasView docs. The system uses a Gradio frontend and accesses LLMs via OpenRouter (default gpt-4o-mini). The approach combines tool-use, multi-agent coordination, and retrieval-augmented generation to automate SAS workflows.

## Key Results
- SasAgent achieved R² = 0.9479 and RMSE = 0.003169 (14.4% relative) in fitting polymer chain scattering data.
- The system successfully calculates scattering length densities for arbitrary materials using chemical formulas and densities.
- SasAgent generates synthetic scattering data and fits experimental datasets with high precision across multiple examples.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hierarchical coordinator-agent structure enables task decomposition and specialized handling for diverse SAS analysis requests.
- Mechanism: A coordinator LLM agent interprets user prompts and delegates tasks to one of three specialized agents (SLD, generation, or fitting), each equipped with domain-specific tools. This reduces the cognitive load on any single agent and allows for the integration of purpose-built tools for each sub-task.
- Core assumption: User intent can be reliably mapped by the coordinator to one of the three pre-defined specialist agents.
- Evidence anchors:
  - [abstract] "SasAgent features a coordinator agent that interprets user prompts and delegates tasks to three specialized agents..."
  - [section] (Section II.A) "...we design a two layer structure... In total, we have 4 AI agents... The coordinator agent is responsible for direct communication with the user and distributing user requests to the appropriate expert agents."
  - [corpus] Related work on multi-agent systems (e.g., "Single-agent or Multi-agent Systems? Why Not Both?") supports task decomposition as a benefit of MAS.
- Break condition: The system will fail if a user prompt is ambiguous or falls outside the three defined tasks (SLD, generation, fitting), causing misrouting or an inability to handle the request.

### Mechanism 2
- Claim: Wrapping existing scientific software functions as LLM-friendly tools allows agents to execute precise, domain-specific computations.
- Mechanism: The system creates "tools" by wrapping functions from the SasView Python library (e.g., bump fitting, model data calculation, SLD calculation). An LLM agent can then call these tools with parameters extracted from the user's prompt to perform calculations it could not do reliably on its own.
- Core assumption: The tool's interface and documentation are sufficiently clear for the LLM to generate correct function calls with appropriate parameters.
- Evidence anchors:
  - [abstract] "...agents utilize LLM-friendly tools to execute tasks efficiently. These tools... are derived from the SasView Python library."
  - [section] (Section II.A) "...we build 4 different tools by wrapping corresponding function within the SasView software." and "In practice, our multi agent system is implemented using CrewAI..."
  - [corpus] Corpus evidence on similar tool-use frameworks is weak but the general principle of tool augmentation is a known LLM pattern.
- Break condition: The mechanism breaks if the LLM hallucinates incorrect parameters for a tool call or if the tool's output is not in a format the agent can interpret correctly.

### Mechanism 3
- Claim: Retrieval-Augmented Generation (RAG) provides necessary, up-to-date, domain-specific knowledge to the LLM, improving model selection and parameterization.
- Mechanism: A RAG documentation tool is built by crawling the SasView documentation website. When an agent needs to use a specific scattering model, it queries this tool to get accurate information about model parameters and usage, overcoming the LLM's static knowledge cutoff.
- Core assumption: The crawled documentation is accurate, complete, and the retrieval system can find the most relevant sections for a given task.
- Evidence anchors:
  - [abstract] "...tools, including... Retrieval-Augmented Generation (RAG) documentation tool... are derived from the SasView Python library."
  - [section] (Section II.A) "...we build a Retrieval-Augmented Generation (RAG) documentation tool that contains all of the documentation for all 78 models available in SasView..."
  - [corpus] Corpus evidence on this specific RAG implementation is weak.
- Break condition: The mechanism fails if the documentation is outdated, incomplete, or if the RAG system retrieves irrelevant or incorrect snippets, leading to poor model selection or parameter errors.

## Foundational Learning

- Concept: Small-Angle Scattering (SAS) and Scattering Length Density (SLD).
  - Why needed here: SAS is the core domain. Understanding that it probes nanostructure and that SLD is a critical parameter for contrast in scattering experiments is essential for all three agent tasks (calculating SLD, generating/fitting data).
  - Quick check question: If you input "calculate SLD for H2O," what key material properties must the system find or be given to perform the calculation?

- Concept: Large Language Model (LLM) Agents and Tool Use.
  - Why needed here: This system is not just an LLM but an *agent* framework. It requires understanding how an LLM can autonomously decide to call an external function (a "tool") to accomplish a task it cannot do with its internal weights alone.
  - Quick check question: What is the primary limitation of a standalone LLM that tool use is designed to overcome in this system?

- Concept: Multi-Agent Systems and Task Decomposition.
  - Why needed here: The system uses a specific 2-layer, 4-agent architecture. Understanding the role of the coordinator in breaking down problems and the specialists in executing them is key to grasping the system's design.
  - Quick check question: If a user uploads a data file and asks for it to be fitted, which two agents are primarily involved in fulfilling the request and in what order?

## Architecture Onboarding

- Component map: User Interface (Gradio) -> Coordinator Agent -> Expert Agent (SLD/Generation/Fitting) -> Tool Layer (SLD Calculator, Model Data, Bump Fitting, RAG Docs) -> Backend (SasView, LLM API)

- Critical path: A user prompt enters via Gradio -> Coordinator Agent parses intent -> Task is delegated to an Expert Agent (e.g., Fitting Agent) -> Expert Agent uses a combination of tools (e.g., RAG for model docs, SLD tool for parameters, Bump Fitting for execution) -> Result (text, plot, data) is returned to the user.

- Design tradeoffs:
  - **Generalization vs. Specialization:** The system is highly specialized for SAS tasks using SasView. It is not a general-purpose science agent. This makes it powerful in its niche but brittle outside of it.
  - **Cost/Speed vs. Capability:** The default model is `gpt-4o-mini` for low cost. More powerful but slower/expensive models like `gpt-4o` or `claude-sonnet-4` are available but may be overkill for simple SLD calculations.
  - **Automation vs. User Control:** The system automates model and parameter selection. This can speed up workflows but may obscure reasoning or make sub-optimal choices an expert would catch.

- Failure signatures:
  - **Misrouting:** Coordinator sends a fitting request to the SLD agent.
  - **Tool Call Failure:** Agent calls the Bump Fitting tool with an incorrect model name or malformed parameters.
  - **Hallucination:** Agent invents a model not in SasView or fabricates parameter values not present in the user's prompt or the RAG docs.
  - **Dead End:** Agent gets stuck in a loop asking for clarification or fails to progress due to unclear prompts.

- First 3 experiments:
  1. **Replicate an Example:** Set up the environment, run the "Calculate SLD for DMSO" example, and trace the logs from the Coordinator to the SLD Agent to the tool call. This confirms the basic request-response path.
  2. **Test Fitting Workflow:** Upload the provided polymer scattering data and run the fitting command from the paper. Verify that the Fitting Agent correctly uses the RAG tool, SLD tool, and Bump Fitting tool in sequence. This validates the multi-tool coordination.
  3. **Failure Mode Analysis:** Provide a deliberately ambiguous prompt (e.g., "analyze this stuff" with a data file). Observe how the Coordinator handles it—does it ask for clarification, make a guess, or fail? This probes the system's robustness and intent-classification limits.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can SasAgent autonomously select the optimal scattering model from the SasView library for experimental data without explicit user instruction? The current demonstrations validate the execution of user-specified models but do not test the coordinator agent's ability to blindly diagnose data and select the best fit from the 78 available models.

- **Open Question 2:** Does the integration of web search tools and literature-reading agents improve the system's ability to handle novel or complex materials? The authors state that future work should enable the web search tool so that the agent can better respond to user using internet information and suggest adding a research agent to read user-uploaded literature.

- **Open Question 3:** How does the performance and stability of the multi-agent workflow vary across different underlying LLM architectures? The interface allows users to switch between models, but the paper only presents successful results using the default model, leaving the robustness of the agent coordination across different reasoning engines unstudied.

## Limitations

- **Tool reliability:** The success of SasAgent critically depends on the LLMs' ability to correctly interpret and call the wrapped SasView tools. Errors in parameter parsing or tool output interpretation could cascade through the analysis pipeline.

- **RAG effectiveness:** The quality of model selection and parameter guidance relies on the RAG documentation tool's ability to retrieve relevant information. If the crawling or indexing misses critical details, or if retrieval relevance is low, the agents may make poor decisions.

- **Task boundary handling:** The system is designed for three specific task types (SLD, generation, fitting). It's unclear how well the coordinator handles ambiguous prompts or requests that don't cleanly map to these categories, potentially leading to misrouting or failures.

## Confidence

- **High Confidence Claims:** The fundamental feasibility of using LLM agents with scientific software tools for specialized analysis tasks is demonstrated. The two-layer architecture (coordinator + specialists) is a sound design pattern for task decomposition in multi-agent systems.

- **Medium Confidence Claims:** The specific tool implementations (SLD calculator, RAG docs, bump fitting) work as intended and integrate seamlessly with the LLM agents. The system consistently achieves high-quality fits and accurate SLD calculations across diverse user scenarios.

- **Low Confidence Claims:** The system's robustness to ambiguous or out-of-scope prompts without extensive user guidance. The long-term maintainability of the RAG documentation index as SasView evolves.

## Next Checks

1. **Tool-call robustness test:** Systematically vary input formats for tool calls (e.g., parameter naming, units, data structure) to measure how often the LLM correctly parses and executes the intended function.

2. **RAG retrieval quality audit:** For a set of common scattering models, verify that the RAG tool consistently retrieves the most relevant documentation snippets, and measure the impact of retrieval errors on subsequent model selection and fitting performance.

3. **Stress test on ambiguous prompts:** Submit a battery of deliberately vague or complex prompts that combine multiple task types (e.g., "analyze this data and tell me what's in it") to quantify the coordinator's success rate in routing and the system's ability to handle uncertainty.