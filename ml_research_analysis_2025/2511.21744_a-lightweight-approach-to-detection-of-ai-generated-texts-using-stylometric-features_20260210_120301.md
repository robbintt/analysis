---
ver: rpa2
title: A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric
  Features
arxiv_id: '2511.21744'
source_url: https://arxiv.org/abs/2511.21744
tags:
- text
- accuracy
- arxiv
- detection
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the growing need for lightweight, accurate
  detection of AI-generated text amid increasing concerns about AI-generated content
  distorting information quality. The authors propose NEULIF, a feature-driven approach
  that extracts 68 stylometric and readability features from text using spaCy and
  TextDescriptives.
---

# A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features

## Quick Facts
- arXiv ID: 2511.21744
- Source URL: https://arxiv.org/abs/2511.21744
- Reference count: 38
- Primary result: 97% accuracy, 97% F1-score, and 99.5% ROC-AUC on balanced 20,000-sample dataset using CNN with 68 stylometric features

## Executive Summary
This paper proposes NEULIF, a lightweight detection framework for AI-generated text that leverages 68 stylometric and readability features extracted using spaCy and TextDescriptives. The approach trains two compact classifiers—a 1D CNN and a Random Forest—achieving state-of-the-art performance in the lightweight detector class with 97% accuracy and 95% accuracy respectively. Both models are orders of magnitude smaller than transformer-based ensembles, enabling efficient deployment on standard CPUs. The work demonstrates that simplicity and linguistic insight can rival complexity in AI-text detection.

## Method Summary
The method extracts 68 stylometric features (readability indices, syntactic complexity, lexical diversity, cohesion metrics) from text using spaCy and TextDescriptives, then applies StandardScaler to normalize the feature vectors. Two classifiers are trained: a 1D CNN with three dense layers and a Random Forest with 100 estimators. The CNN captures local patterns among adjacent features, while the Random Forest provides interpretability through feature importance. Both models are evaluated on a balanced dataset of 20,000 samples (10,000 AI, 10,000 human) from the Kaggle "AI vs Human Text" corpus.

## Key Results
- CNN achieves 97% accuracy, 97% F1-score, and 99.5% ROC-AUC
- Random Forest achieves 95% accuracy, 94% F1-score, and 95% ROC-AUC
- Both models are significantly smaller than transformer-based approaches (CNN: 25 MB, Random Forest: 10.6 MB)
- CNN shows better-calibrated probabilities (log loss 0.09) vs Random Forest (log loss 1.60)

## Why This Works (Mechanism)

### Mechanism 1
AI-generated text exhibits systematic deviations from human writing in measurable stylometric properties that persist across samples. The pipeline extracts 68 features spanning readability indices, syntactic complexity, lexical diversity, and cohesion metrics, capturing structural "fingerprints" of how LLMs organize language differently from humans. Performance likely degrades on adversarially modified text, cross-domain texts, or when LLMs are explicitly prompted to mimic specific human styles—none of which were evaluated.

### Mechanism 2
Converting variable-length text to fixed-size feature vectors eliminates the computational burden of sequence models while retaining discriminative signal. Rather than processing raw token sequences through RNNs or transformers, spaCy + TextDescriptives produce a 68-dimensional vector per document, allowing standard feed-forward architectures to operate on uniform input shapes. The 68 features preserve sufficient information for binary classification; no critical signal is lost by discarding token-level sequential context. If discriminative patterns are primarily contextual or long-range, the fixed-vector approach may fail to capture them.

### Mechanism 3
A compact 1D CNN can learn local interaction patterns among neighboring stylometric features that improve discrimination over treating features independently. The Conv1D layer (128 filters, kernel size 3) identifies patterns among adjacent features in the 68-element vector, followed by batch normalization and three dense layers with dropout regularization. Meaningful patterns exist among adjacent features in the vector ordering, though the paper does not specify how features are ordered. If feature ordering is arbitrary, the CNN's locality assumption provides no benefit; Random Forest achieves 95% vs. CNN's 97%—the gap may reflect this or may be noise.

## Foundational Learning

- **Concept: Stylometric features**
  - Why needed here: The entire approach hinges on understanding what linguistic properties (readability, syntactic complexity, lexical diversity) differentiate AI from human text. Without this, feature selection is arbitrary.
  - Quick check question: Can you explain why type-token ratio might differ between AI and human writers?

- **Concept: Feature-based vs. end-to-end learning tradeoffs**
  - Why needed here: This paper explicitly trades off raw representation learning (transformers) for engineered features + simple classifiers. Understanding when this works—and when it fails—is critical for deployment decisions.
  - Quick check question: What information might be lost by representing a 500-word essay as 68 numbers instead of 500 token embeddings?

- **Concept: In-domain vs. out-of-distribution evaluation**
  - Why needed here: The paper reports only in-domain results (Kaggle AI vs. Human corpus). The authors explicitly note that "results are reported in-domain only" and future work will address generalization.
  - Quick check question: Why might a detector trained on GPT-3.5 essays fail on Claude-generated scientific abstracts?

## Architecture Onboarding

- **Component map:**
  Raw Text → spaCy (en_core_web_sm) + TextDescriptives → 68-dim feature vector → StandardScaler → [CNN or Random Forest] → probability score → threshold (0.5) → binary label

- **Critical path:**
  1. Ensure spaCy model is correctly loaded (`en_core_web_sm`)
  2. TextDescriptives pipeline must be attached to spaCy to compute all 68 features
  3. Feature alignment: the same 68 features must be extracted in the same order during inference as training
  4. StandardScaler must apply the fitted training mean/std—not refit on test data
  5. For CNN: reshape to (1, 68, 1) before inference

- **Design tradeoffs:**
  - CNN (25 MB, 97% accuracy, log loss 0.09): Better calibrated probabilities, higher accuracy, but less interpretable
  - Random Forest (10.6 MB, 95% accuracy, log loss 1.60): Smaller, feature importance is inspectable, but poorly calibrated probabilities
  - Assumption: Both models were evaluated only on in-domain data; cross-domain performance is unknown

- **Failure signatures:**
  - Short texts (<100 words): feature extraction may produce NaN values (paper notes these were dropped)
  - Adversarial perturbations: authors acknowledge vulnerability but did not test
  - Cross-domain texts: not evaluated—performance on scientific papers, code, or non-English text is unknown
  - Mixed authorship: the binary classification framing cannot handle human-edited AI text

- **First 3 experiments:**
  1. Validate feature extraction consistency: Run the spaCy + TextDescriptives pipeline on 10 sample texts and verify all 68 features are extracted without NaN values. Log feature names and order.
  2. Reproduce held-out test performance: Load the trained models, run inference on the test split (1,997 samples), and confirm accuracy matches reported values (±1%).
  3. Stress test with edge cases: Feed the model (a) very short texts (50 words), (b) texts from a different domain (e.g., scientific abstracts), and (c) deliberately paraphrased AI text to characterize failure modes before deployment.

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the NEULIF framework when applied to out-of-distribution domains, low-resource languages, and adversarially perturbed texts? The authors state in Section 6 that future research will "focus on the generalization of these models across domains, languages, and adversarial settings." This remains unresolved because the current study evaluates performance exclusively on a single "in-domain" dataset (Kaggle AI vs. Human), leaving cross-domain and adversarial robustness untested.

### Open Question 2
Can stylometric features successfully distinguish human authorship in historical texts spanning multiple centuries without misclassification due to linguistic drift? Section 6 identifies the need for "assessing model performance on texts spanning multiple centuries, enabling the study of diachronic language variation." This remains unresolved because the current model relies on features like readability indices which may behave differently on archaic grammar or vocabulary compared to the modern AI/Human essays used for training.

### Open Question 3
Can the lightweight, feature-driven architecture of NEULIF be effectively adapted for detecting AI-generated content in non-textual modalities? The conclusion lists a "promising direction" as the "development of similarly lightweight yet accurate approaches for other modalities such as audio, video, and images." This remains unresolved because the paper establishes efficacy only for text based on 68 specific linguistic stylometric features; equivalent compact feature sets for audio/video are not defined or tested.

## Limitations
- All results are reported in-domain only, with cross-domain generalization untested
- Binary classification framework cannot handle mixed-authorship (human-edited AI text)
- Vulnerability to adversarial attacks is acknowledged but not empirically validated
- Performance on non-English text and scientific/technical domains is unknown

## Confidence
- **High confidence** in the technical implementation details (feature extraction pipeline, model architectures, and reported in-domain metrics)
- **Medium confidence** in the practical utility claims, given the lack of cross-domain evaluation and adversarial robustness testing
- **Low confidence** in claims about feature interpretability and specific mechanism contributions (e.g., whether the CNN's locality assumption provides meaningful benefit over Random Forest)

## Next Checks
1. Cross-domain performance test: Evaluate both models on a held-out dataset from a different domain (e.g., scientific abstracts, news articles, or code) to quantify domain adaptation requirements
2. Adversarial robustness evaluation: Test models on deliberately paraphrased AI-generated text and prompt-engineered variations to measure vulnerability to common attack vectors
3. Feature importance validation: Conduct ablation studies removing specific feature categories (e.g., syntactic vs. lexical features) to empirically verify which feature groups drive the performance gains over baseline approaches