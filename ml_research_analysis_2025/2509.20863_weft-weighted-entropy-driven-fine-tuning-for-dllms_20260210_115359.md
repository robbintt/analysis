---
ver: rpa2
title: 'WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs'
arxiv_id: '2509.20863'
source_url: https://arxiv.org/abs/2509.20863
tags:
- training
- gift
- entropy
- diffusion
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of applying supervised fine-tuning
  (SFT) to diffusion language models (dLLMs), which lack precise probability estimates
  at each denoising step and produce less predictable generation. The authors propose
  WeFT (Weighted Entropy-driven Fine-Tuning), an SFT method that assigns different
  weights to tokens based on their entropy.
---

# WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs

## Quick Facts
- arXiv ID: 2509.20863
- Source URL: https://arxiv.org/abs/2509.20863
- Reference count: 40
- Method achieves 39-83% relative improvement over SFT on reasoning benchmarks

## Executive Summary
WeFT (Weighted Entropy-driven Fine-Tuning) addresses the challenge of applying supervised fine-tuning to diffusion language models (dLLMs) by introducing entropy-based token weighting. The method prioritizes high-uncertainty tokens during training by computing predictive entropy for each token position and using this to derive per-token masking rates and training weights. This theoretically grounded approach demonstrates substantial improvements over standard SFT on reasoning tasks while maintaining computational efficiency.

## Method Summary
WeFT modifies the standard SFT approach for dLLMs by introducing entropy-driven token weighting. The method computes predictive entropy for each token position by fully masking the answer and examining output logits, then derives per-token masking rates βᵢ and training weights 1/tᵢ from the square root of this entropy. The training process involves two forward passes: first to compute entropy and derive masking probabilities, then to apply token-specific masking and compute the weighted loss. This approach concentrates gradient updates on tokens where the model exhibits high uncertainty, theoretically improving reasoning capabilities.

## Key Results
- 39% relative improvement on Sudoku reasoning benchmark (LoRA, 20 epochs, s1K dataset)
- 64% relative improvement on Countdown benchmark (LoRA, 20 epochs, s1K dataset)
- 83% relative improvement on GSM8K and MATH-500 benchmarks (LoRA, 20 epochs, s1K dataset)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entropy-based token weighting prioritizes high-uncertainty tokens that carry more information for reasoning.
- **Mechanism:** The model computes predictive entropy for each token position (by fully masking the answer and examining output logits). Tokens with higher entropy receive higher masking rates βᵢ and training weights 1/tᵢ. This concentrates gradient updates on tokens where the model is uncertain.
- **Core assumption:** High-entropy tokens are more important for reasoning than low-entropy tokens.
- **Evidence anchors:** [abstract] "tokens with higher entropy correspond to positions where the model exhibits greater uncertainty in generation"; [Section 3.3, Figure 2] Visualization showing high-frequency tokens with different entropy levels; [corpus] wd1 (Tang et al.) explores related weighted optimization for reasoning in dLLMs.

### Mechanism 2
- **Claim:** The theoretically derived importance-weighted loss preserves diffusion process properties better than ad-hoc weighting schemes.
- **Mechanism:** By modifying the Q matrix to have per-token masking rates βᵢ (rather than uniform rates), and deriving the loss from the Denoising Score Entropy (DSE) objective, the method maintains theoretical consistency with continuous-time discrete diffusion dynamics.
- **Core assumption:** The fixed Q matrix assumption in the derivation is a reasonable approximation even though β varies per token in practice.
- **Evidence anchors:** [Section 3.2, Theorem 1] Full derivation of importance-weighted loss from modified Q matrix; [Table 3] GIFT (SW) and Dream (simple token-wise weighting) perform worse than SFT, while full GIFT outperforms SFT significantly.

### Mechanism 3
- **Claim:** Square root of entropy (rather than raw entropy) stabilizes training by reducing gradient norm variance.
- **Mechanism:** Raw entropy produces extremely large gradient norms (max 646.1) that cause training instability. Square root transformation reduces this (max 46.2) while preserving the relative ordering of token importance.
- **Core assumption:** The relationship between entropy and optimal masking rate follows approximately √H rather than H.
- **Evidence anchors:** [Section 4.3.3, Table 4] Gradient norm statistics showing entropy causes max 646.1 vs √entropy max 46.2; [Section 3.3] "we empirically find that using the raw entropy values may destabilize training".

## Foundational Learning

- **Concept: Diffusion Language Models (dLLMs) and masking process**
  - **Why needed here:** Understanding that dLLMs mask tokens with probability t∈[0,1] and train to reconstruct is essential. The key insight is that standard SFT treats all tokens uniformly (same t), while WeFT varies t per token.
  - **Quick check question:** Can you explain why t=0 means "no masking" and t=1 means "fully masked" in this framework?

- **Concept: Predictive entropy as uncertainty quantification**
  - **Why needed here:** The method uses H(softmax(zᵢ)) to measure model uncertainty. You need to understand why high entropy = high uncertainty = potentially more informative token.
  - **Quick check question:** If a token has entropy near 0, what does that tell you about the model's confidence and why might it be less important to train on?

- **Concept: Continuous-time discrete diffusion and Q matrices**
  - **Why needed here:** The theoretical derivation modifies the transition rate matrix Q to have per-token rates. Understanding this helps debug unexpected behavior.
  - **Quick check question:** Why does changing diagonal elements of Q from -1 to -βᵢ change the masking rate for each token?

## Architecture Onboarding

- **Component map:** First forward pass (mask answer → compute logits → calculate entropy → derive βᵢ) -> Sample t (draw t ~ U(0,1) → compute tᵢ = 1 - (1-t)^(βᵢ/βref)) -> Second forward pass (mask tokens with probability tᵢ → compute weighted loss with weights 1/tᵢ) -> Backward pass (standard gradient computation)

- **Critical path:** The entropy estimation (first forward pass) must happen on fully masked answer to get unbiased uncertainty estimates. Incorrect masking here propagates to wrong tᵢ values.

- **Design tradeoffs:**
  - Two forward passes add ~24% overhead for LoRA (46→57 min), but can be faster for full-parameter fine-tuning (28→26 min) due to fewer expected masked tokens
  - Square root entropy vs. raw entropy: stability vs. potentially stronger signal
  - Choice of βref (paper uses mean) affects numerical stability but not ranking

- **Failure signatures:**
  - Training loss oscillates → check if using raw entropy instead of √entropy
  - No improvement over SFT → verify entropy calculation is on masked answer, not partial
  - Slower than expected → check if masking logic is correct (high-entropy tokens should be masked MORE often, not less)

- **First 3 experiments:**
  1. **Sanity check:** Reproduce SFT baseline on s1K (LoRA, 20 epochs), verify your numbers approximate Table 1 (Sudoku 4.6, Countdown 23.8)
  2. **Ablation:** Compare raw entropy vs. √entropy weighting on a small subset, monitor gradient norms to confirm Table 4 findings
  3. **Full comparison:** Run WeFT vs. SFT on held-out portion of your target dataset, ensuring identical hyperparameters (lr, epochs, batch size) for fair comparison

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical derivation of the loss function be generalized to strictly account for a dynamic masking rate matrix $Q$, rather than relying on the current fixed $Q$ assumption?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that the mathematical derivation assumes a fixed matrix $Q$, while the practical implementation treats $Q$ as a heuristic, dynamic variance-control scheme.
- **Why unresolved:** The current theoretical framework relies on simplifying assumptions that do not perfectly align with the proposed algorithm's behavior, creating a gap between theory and practice.
- **What evidence would resolve it:** A revised theoretical proof or bound that establishes the validity of the loss function when $Q$ varies based on entropy during the diffusion process.

### Open Question 2
- **Question:** Do the performance gains of GIFT over standard SFT persist when scaling to significantly larger models (e.g., 70B+ parameters) or larger training datasets?
- **Basis in paper:** [explicit] The "Limitations" section notes that computational constraints restricted the experimental validation to specific scales (e.g., 8B models, 10k samples), leaving broader scaling behaviors unverified.
- **Why unresolved:** It is unclear if the 39-83% relative improvements observed are specific to the tested scales or if the method offers diminishing returns or increased overhead at larger scales.
- **What evidence would resolve it:** Empirical benchmarks applying GIFT to significantly larger diffusion language models and datasets exceeding the 10k sample limit used in the paper.

### Open Question 3
- **Question:** Is the square root of entropy the theoretically optimal transformation for weighting tokens, or does it merely serve as a practical heuristic to prevent gradient instability?
- **Basis in paper:** [inferred] Section 4.3.3 notes that raw entropy leads to large gradient norms and training oscillations, prompting the empirical choice of $\sqrt{H}$. The paper does not provide a theoretical justification for why the square root specifically is the correct functional form.
- **Why unresolved:** The choice of square root appears to be a patch for numerical stability rather than a derived requirement of the diffusion process.
- **What evidence would resolve it:** An ablation study comparing various entropy transformations (e.g., logarithmic, linear, other roots) correlated with theoretical analysis of their impact on gradient variance and convergence.

## Limitations

- The method assumes high-entropy tokens are more valuable for reasoning, but this correlation may not hold universally across all task types
- The theoretical derivation relies on the fixed Q matrix assumption, which is acknowledged as an approximation
- The two-forward-pass architecture, while only adding ~24% overhead for LoRA fine-tuning, may become prohibitive for very large models or when computational resources are constrained

## Confidence

**High Confidence (95%+):** The empirical improvements over SFT are reproducible and substantial (39-83% relative gains on tested benchmarks). The ablation studies demonstrating the necessity of entropy-based weighting and the derived loss function are well-controlled.

**Medium Confidence (70-90%):** The theoretical derivation connecting importance-weighted loss to diffusion theory is mathematically sound but relies on approximations. The claim that high-entropy tokens are more important for reasoning is supported by the data but lacks a mechanistic explanation of why this correlation exists.

**Low Confidence (below 70%):** Claims about the method's effectiveness on non-reasoning tasks are not well-supported by the experimental evidence. The assertion that the approach will generalize to other dLLM architectures beyond the tested variants is speculative.

## Next Checks

1. **Cross-domain generalization test:** Evaluate WeFT on non-reasoning benchmarks including commonsense reasoning (HellaSwag), factual QA (NaturalQuestions), and creative generation (RAFT) to determine if entropy weighting benefits extend beyond mathematical/logical tasks. Compare performance degradation relative to SFT across task types.

2. **Ablation of theoretical assumptions:** Run controlled experiments removing the square root transformation (using raw entropy) while implementing gradient clipping or learning rate scheduling to isolate whether the mathematical derivation provides benefits beyond simple stabilization techniques. Also test alternative entropy transformations (log, cubic root) to understand the sensitivity to this choice.

3. **Scalability and efficiency analysis:** Measure training dynamics and final performance when applying WeFT to full-parameter fine-tuning versus LoRA, and across different model scales (7B → 70B parameters). Profile GPU memory usage and training throughput to quantify the two-forward-pass overhead more precisely across different hardware configurations.