---
ver: rpa2
title: 'Examining Linguistic Shifts in Academic Writing Before and After the Launch
  of ChatGPT: A Study on Preprint Papers'
arxiv_id: '2505.12218'
source_url: https://arxiv.org/abs/2505.12218
tags:
- writing
- science
- academic
- computer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of Large Language Models (LLMs)
  like ChatGPT on academic writing styles using a dataset of 823,798 abstracts from
  arXiv. Through a comprehensive analysis of 30+ linguistic indicators, the research
  reveals significant shifts in academic writing following ChatGPT's release in November
  2022.
---

# Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers

## Quick Facts
- arXiv ID: 2505.12218
- Source URL: https://arxiv.org/abs/2505.12218
- Authors: Tong Bao; Yi Zhao; Jin Mao; Chengzhi Zhang
- Reference count: 21
- Primary result: ChatGPT launch caused significant linguistic shifts in arXiv abstracts, with increased complexity, decreased readability, and higher LLM-preferred word usage

## Executive Summary
This study analyzes linguistic changes in academic writing by examining 823,798 arXiv abstracts from 2014-2023, comparing pre- and post-ChatGPT periods. Using 30+ linguistic indicators across lexical complexity, syntactic complexity, cohesion, readability, sentiment, and LLM-preferred word frequency, the research reveals significant shifts following ChatGPT's November 2022 release. The analysis shows increased lexical complexity and new terminology, decreased readability and syntactic complexity, and a notable rise in LLM-preferred words. Non-native English speakers and Computer Science papers show more pronounced stylistic changes, suggesting LLMs are reshaping academic writing patterns across disciplines.

## Method Summary
The study employs a statistical analysis approach using 823,798 arXiv abstracts from Computer Science, Physics, and Mathematics. Data is divided into 40 time periods (36 pre-ChatGPT, 4 post), with first-author linguistic background determined via OpenAlex API (NES vs NNES classification). The analysis computes 30+ linguistic metrics using specialized tools (TAALES, TAALED, TAASSC, TAACO, textstat, TextBlob) and counts LLM-preferred words from Liang et al. 2024a. Linear regression is fitted on 2014-2022 metrics, predicted 2023 values are computed, and residuals are compared using two-sample Kolmogorov-Smirnov tests to identify significant shifts.

## Key Results
- Significant increase in lexical complexity and new terminology post-ChatGPT
- Decreased readability scores and syntactic complexity metrics
- Notable rise in LLM-preferred words (adjectives/adverbs like "innovative" and "comprehensive")
- Non-native English speakers show more pronounced stylistic changes
- Computer Science exhibits greater LLM influence compared to Mathematics and Physics

## Why This Works (Mechanism)
None specified in the paper.

## Foundational Learning
- **Kolmogorov-Smirnov test**: Compares distributions to detect significant differences between pre- and post-ChatGPT periods. Why needed: To statistically validate whether observed linguistic changes represent genuine shifts rather than random variation. Quick check: Verify test p-values are below conventional significance thresholds (e.g., p < 0.05).

- **TAALES/TAALED/TAASSC/TAACO tools**: Specialized software for analyzing academic writing complexity metrics. Why needed: Provide standardized, validated measures of lexical sophistication, diversity, syntactic complexity, and cohesion. Quick check: Ensure tool outputs match expected ranges from validation studies.

- **LLM-preferred word identification**: Uses curated list of 100 adjectives/adverbs from Liang et al. 2024a. Why needed: Serves as proxy for AI-generated writing patterns. Quick check: Confirm word frequency increases align with overall linguistic shift patterns.

- **OpenAlex API for author background**: Maps first-author country to NES/NNES classification. Why needed: Enables analysis of how non-native speakers are affected differently. Quick check: Validate country-to-language mapping accuracy against known linguistic demographics.

- **Residual analysis**: Compares predicted vs actual 2023 values based on pre-2022 trends. Why needed: Controls for natural language evolution while detecting abrupt changes. Quick check: Verify residuals show clear divergence patterns in 2023.

## Architecture Onboarding
- **Component map**: arXiv metadata -> OpenAlex API -> Linguistic tools (TAALES/TAALED/TAASSC/TAACO) -> Metric aggregation -> Linear regression -> Residual analysis -> KS test
- **Critical path**: Data preprocessing → Metric computation → Temporal aggregation → Statistical testing
- **Design tradeoffs**: Preprint dataset vs peer-reviewed papers (speed vs quality), automated tools vs manual annotation (scalability vs accuracy)
- **Failure signatures**: API mismatches, tool output format changes, incorrect temporal splits
- **First experiments**: 1) Validate metric computation on sample abstracts, 2) Test KS test implementation on synthetic data, 3) Verify NES/NNES classification accuracy

## Open Questions the Paper Calls Out
### Open Question 1
Do the observed linguistic shifts in abstracts persist when analyzing the full text of academic papers? The authors acknowledge that abstracts cannot fully reflect overall characteristics and explicitly state the intent to parse full-text PDFs to comprehensively measure these changes. Abstracts are concise summaries that may be more heavily polished by LLMs than methodology or results sections, skewing the representation of general writing style.

### Open Question 2
Can a direct causal link be established between LLM usage and the observed linguistic changes, distinct from natural language evolution? The "Limitations" section explicitly states that "the causal relationship between LLM use and changes in academic writing requires further validation." The current study relies on correlation between time periods and frequency of LLM-preferred words, which serves as a proxy rather than direct proof of usage.

### Open Question 3
How does the writing style of individual authors evolve over time specifically regarding LLM adoption? The "Conclusion and Future Works" section proposes examining "how writing style changes for the same authors over time" to enhance the robustness of the analysis. The current methodology aggregates data by groups, which masks individual adaptation trajectories and whether specific authors are driving the trends.

## Limitations
- arXiv dataset represents preprints rather than peer-reviewed publications, potentially capturing different writing behaviors
- NNES classification relies on author affiliation countries, which may misclassify multilingual authors
- LLM-preferred word list comes from a contemporaneous study, introducing potential circularity

## Confidence
- **High confidence**: Overall detection of linguistic shifts (increased complexity metrics, decreased readability, higher LLM-preferred word usage)
- **Medium confidence**: Attribution to ChatGPT specifically (vs general AI writing tool adoption)
- **Medium confidence**: Magnitude differences between disciplines and author backgrounds

## Next Checks
1. Replicate the analysis using peer-reviewed journal publications from the same period to verify if preprint-specific patterns hold
2. Conduct qualitative analysis of abstracts showing highest LLM-preferred word usage to assess whether changes reflect genuine writing style shifts or superficial vocabulary substitution
3. Test robustness by using alternative NNES classification methods (e.g., name-based inference) to verify non-native speaker effect magnitude