---
ver: rpa2
title: An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC
arxiv_id: '2507.13736'
source_url: https://arxiv.org/abs/2507.13736
tags:
- layer
- spinnaker2
- dram
- worker
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an end-to-end DNN inference framework for the
  SpiNNaker2 neuromorphic MPSoC, enabling edge deployment of large and complex models
  like transformers. The framework extends OctopuScheduler with multi-layer scheduling
  and includes a complete flow from PyTorch models to inference on a single SpiNNaker2
  chip, incorporating quantization, lowering, and automated parameter extraction.
---

# An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC

## Quick Facts
- arXiv ID: 2507.13736
- Source URL: https://arxiv.org/abs/2507.13736
- Reference count: 15
- Primary result: 98.34% MNIST accuracy matching quantized ONNX model with 688 µs total inference time on SpiNNaker2

## Executive Summary
This paper presents a complete end-to-end DNN inference framework for the SpiNNaker2 neuromorphic MPSoC, enabling efficient deployment of large and complex models like transformers at the edge. The framework extends OctopuScheduler with multi-layer scheduling capabilities and includes a full flow from PyTorch models to inference on a single SpiNNaker2 chip. Key innovations include automated parameter extraction, quantization using AMD Quark, lowering to application graph, and a unified DRAM-based configuration structure that enables on-chip iteration without host communication between layers.

## Method Summary
The framework converts PyTorch models to ONNX, applies INT8 post-training quantization with mean squared error optimization and cross-layer equalization, then lowers to an application graph with Linear+ReLU fusion. The model is topologically sorted and converted to S2Layer representation with partitioning and mapping decisions. A unified DRAM structure stores global configuration, time measurements, layer configurations, and data memory. During execution, a scheduler PE oversees worker PEs via interrupts, with workers fetching weights from DRAM and signaling completion without host intervention between layers.

## Key Results
- 784-512-256-16 MLP achieves 98.34% accuracy on MNIST matching quantized ONNX model
- Total execution time of 688 µs for 784-512-256-16 MLP (including DRAM weight fetching)
- Minimal scheduling overhead of 13 µs per layer, representing only 1.9% of total execution time
- Weight fetching dominates runtime (59% of FC1 layer time), leaving MLA accelerator utilized only 9% of the time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A unified DRAM-based configuration structure enables on-chip multi-layer execution without host intervention between layers.
- Mechanism: The framework statically assigns DRAM memory into four regions (Global Configuration, Time Measurements, Layer Configurations, Data Memory). Each layer's configuration block contains metadata (layer type, worker assignments, next layer address) that allows the scheduler PE to traverse layers autonomously via pointer chasing.
- Core assumption: Models can tolerate DRAM access latency for configuration reads between layers (measured at 13 µs overhead per layer).
- Evidence anchors:
  - [abstract] "uses a unified DRAM-based configuration structure and on-chip iteration without host communication between layers"
  - [Section III-B] "To avoid on-chip DRAM memory planning, the DRAM memory space is statically assigned by the host before execution"
  - [corpus] Weak direct support; neighbor papers focus on SNN deployment and mobile DNN scheduling but not this specific DRAM structure pattern.
- Break condition: If layer configurations exceed available DRAM bandwidth or if inter-layer dependencies require dynamic host decisions, the static structure fails.

### Mechanism 2
- Claim: Scheduler-Worker PE pattern with interrupt-driven synchronization enables distributed layer execution across heterogeneous workers.
- Mechanism: One scheduler PE reads layer information, triggers worker PEs via interrupts, and waits for completion signals. Workers fetch their own configuration from DRAM, execute tiled operations, and signal completion. This decouples scheduler logic from worker computation.
- Core assumption: The IRQ latency and completion signaling overhead (13 µs average) remains acceptable relative to layer computation time.
- Evidence anchors:
  - [Section II] "A single scheduler PE oversees and synchronizes all workers... workers send a signal word to the scheduler, indicating the completion"
  - [Section III-C] "scheduler loads layer information... forwards layer information and sends an interrupt to trigger all workers"
  - [corpus] Twill (arXiv:2507.00491) uses similar heterogeneous scheduling concepts for mobile edge, suggesting pattern generalizability.
- Break condition: If worker count scales beyond single-chip (152 PEs) or if layers require cross-chip synchronization, the single-scheduler pattern becomes a bottleneck.

### Mechanism 3
- Claim: Post-training quantization with INT8 power-of-two scaling preserves accuracy while enabling MLA hardware acceleration.
- Mechanism: PyTorch models are converted to ONNX, quantized using AMD Quark with mean squared error optimization and cross-layer equalization, then lowered to an application graph. Linear+ReLU fusion exploits MLA's combined operation capability.
- Core assumption: 8-bit quantization precision is sufficient for target applications (demonstrated at 98.34% MNIST accuracy vs 98.36% FP32 baseline).
- Evidence anchors:
  - [Section IV-A] "Post-training quantization from the AMD Quark library with INT8 power-of-two quantization... uses mean squared error optimization and cross-layer equalization"
  - [Table I] Shows accuracy degradation from 98.36% (FP32) to 98.34% (measured on SpiNNaker2)
  - [corpus] No direct corpus validation of this specific quantization flow on neuromorphic hardware.
- Break condition: If models exhibit high quantization sensitivity or require sub-8-bit precision, accuracy degradation may exceed acceptable thresholds.

## Foundational Learning

- Concept: **Globally Asynchronous Locally Synchronous (GALS) topology**
  - Why needed here: SpiNNaker2 uses GALS to connect PEs via NoC, enabling independent PE clocking with DMA transfers and IRQ-based synchronization.
  - Quick check question: Can you explain why GALS is preferable to fully synchronous design for a 152-PE neuromorphic chip?

- Concept: **Tiled layer partitioning for constrained SRAM**
  - Why needed here: Each PE has only 128 kB SRAM, so DNN layers must be split into tiles that fit local memory while respecting data dependencies.
  - Quick check question: Given a 784×512 weight matrix and 128 kB SRAM limit, how would you determine optimal tile dimensions?

- Concept: **Post-training quantization (PTQ) vs. quantization-aware training (QAT)**
  - Why needed here: The framework uses PTQ for deployment convenience; understanding its limitations versus QAT informs model selection.
  - Quick check question: When would QAT be necessary instead of the PTQ approach described?

## Architecture Onboarding

- Component map:
  - Host -> DRAM (2 GB) -> Scheduler PE, Worker PEs (152 total)
  - Scheduler PE -> Worker PEs (via IRQ)
  - Worker PEs -> DRAM (via DMA)
  - DRAM contains: Global Config, Time Measurements, Layer Configurations, Data Memory

- Critical path: Host config write → Scheduler reads global info → Per-layer: Scheduler triggers workers → Workers fetch weights from DRAM → MLA computation → Workers signal completion → Scheduler advances → Final output read by host

- Design tradeoffs:
  - DRAM bandwidth vs. computation: Weight fetching dominates runtime (192 µs of 323 µs for FC1 layer—59%)
  - Scheduling overhead vs. layer size: 13 µs overhead is negligible for large layers but significant for small layers (softmax: 50 µs total)
  - Parallelism vs. memory: More workers increase parallelism but require more SRAM per layer

- Failure signatures:
  - Accuracy drop >0.1% suggests quantization mismatch between ONNX and SpiNNaker2 execution
  - Layer timeout indicates worker not signaling completion (DMA stall or IRQ missed)
  - DRAM overflow if model configurations + activations exceed 2 GB
  - Scheduling overhead >20% of total time indicates too many small layers

- First 3 experiments:
  1. **Baseline validation**: Run the 784-512-256-16 MLP on MNIST, verify 688 µs latency and 98.34% accuracy match paper results
  2. **DRAM bandwidth profiling**: Measure weight fetch time vs. computation time per layer to identify bandwidth-bound layers
  3. **Scaling test**: Increase worker count from 8 to maximum available PEs, measure setup/cleanup overhead scaling (paper shows 12 µs→39 µs for setup)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can computational throughput be significantly improved by overlapping DRAM data transfers with MLA execution?
- Basis in paper: [explicit] The Discussion states that "hiding the DRAM latency behind computation should be investigated" to address the finding that weight fetching dominates runtime, leaving the accelerator utilized only 9% of the time during layers.
- Why unresolved: The current implementation loads weights from DRAM sequentially before computation begins, creating a bottleneck that limits performance.
- What evidence would resolve it: Profiling results demonstrating double-buffering or pipelined data transfers that increase MLA utilization rates above the current 9% threshold.

### Open Question 2
- Question: How does the framework's scheduling overhead and accuracy scale when deploying modern architectures like Transformers or ResNets?
- Basis in paper: [explicit] The Discussion notes that while the current MLP provides "preliminary results," "Larger models, such as ResNet architectures or small language models, will fully exploit the parallelism of SpiNNaker2."
- Why unresolved: The experimental validation is limited to a small 3-layer MLP, leaving the complex layer-fusion and partitioning capabilities required for State-of-the-Art models unverified.
- What evidence would resolve it: Successful deployment and latency benchmarks of a Transformer or ResNet model running on a single chip via this framework.

### Open Question 3
- Question: What specific modifications are required to extend the unified DRAM configuration structure for distributed inference across multiple SpiNNaker2 chips?
- Basis in paper: [explicit] The Discussion suggests that "model inference could be extended over multiple SpiNNaker2 chips" as an alternative to single-chip memory constraints.
- Why unresolved: The presented framework and memory organization are strictly designed for a single-chip edge setup with one host interface.
- What evidence would resolve it: A demonstrated extension of the data flow handling inter-chip NoC communication for layer partitioning.

## Limitations

- The framework is limited to single-chip deployment with proprietary hardware components (SpiNNaker2, OctopuScheduler, PySpiNNaker2) that are not publicly available for independent validation.
- Experimental results are limited to a small 784-512-256-16 MLP, with no validation on modern architectures like transformers or ResNets that would fully exploit the claimed parallelism.
- DRAM bandwidth limitations result in weight fetching dominating runtime (59% for FC1 layer), leaving the MLA accelerator underutilized at only 9% utilization during computation.

## Confidence

- **High**: The GALS-based PE synchronization and DRAM-based configuration structure are technically sound and align with established neuromorphic design patterns.
- **Medium**: The PTQ accuracy preservation (98.34% MNIST) is plausible given minimal degradation from baseline, but the specific Quark configuration and CLE effectiveness cannot be verified without source code.
- **Low**: The claimed scheduling overhead (13 µs per layer) and weight fetch times depend on unverified DRAM bandwidth assumptions and specific layer partitionings not disclosed.

## Next Checks

1. **Cross-platform quantization validation**: Replicate the PTQ pipeline using AMD Quark with identical parameters on standard CPU/GPU to verify the 98.34% accuracy claim independently of SpiNNaker2 hardware.
2. **DRAM bandwidth profiling**: Measure actual weight fetch times vs. computation for FC layers on similar hardware (e.g., FPGA with DRAM) to validate the 59% weight fetch observation.
3. **Scheduling overhead benchmarking**: Implement the single-scheduler PE pattern with interrupt-driven worker synchronization on any heterogeneous multicore system to measure IRQ and completion signaling latency under different layer sizes.