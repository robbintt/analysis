---
ver: rpa2
title: 'MergeIT: From Selection to Merging for Efficient Instruction Tuning'
arxiv_id: '2503.00034'
source_url: https://arxiv.org/abs/2503.00034
tags:
- instruction
- data
- quality
- merging
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MergeIT, a novel framework that shifts from
  traditional LLM-based instruction selection to instruction synthesis. The method
  operates in two stages: topic-aware filtering clusters and removes redundant instructions
  without LLM scoring, followed by LLM-based merging that synthesizes semantically
  similar instructions into richer, more informative training data.'
---

# MergeIT: From Selection to Merging for Efficient Instruction Tuning

## Quick Facts
- **arXiv ID:** 2503.00034
- **Source URL:** https://arxiv.org/abs/2503.00034
- **Reference count:** 17
- **Primary result:** Achieves 52.76% average accuracy and 4.481 MT-Bench score on Mistral-7b-v0.3, outperforming baselines by up to 0.842

## Executive Summary
MergeIT introduces a novel framework that shifts from traditional LLM-based instruction selection to instruction synthesis. The method operates in two stages: topic-aware filtering clusters and removes redundant instructions without LLM scoring, followed by LLM-based merging that synthesizes semantically similar instructions into richer, more informative training data. Experimental results demonstrate that MergeIT achieves state-of-the-art performance across multiple benchmarks, with an average accuracy of 52.76% and MT-Bench score of 4.481 on Mistral-7b-v0.3, outperforming baselines by up to 0.842. The approach effectively reduces dataset size while enhancing diversity and quality, establishing LLM-based merging as a promising alternative to conventional scoring-based selection methods.

## Method Summary
MergeIT operates in two stages: topic-aware filtering and LLM-based merging. First, K-means clustering groups instructions into semantically coherent clusters using all-MiniLM-L6-v2 embeddings, then facility location submodular selection identifies maximally representative samples within each cluster. Second, semantically similar instruction pairs (cosine similarity ≥ τ) are merged using an LLM synthesizer to create information-denser instructions. A quality preservation constraint ensures merged quality exceeds a weighted threshold of the originals before acceptance. The final dataset (~6k merged pairs) is used for LoRA fine-tuning with specified hyperparameters.

## Key Results
- MergeIT achieves 52.76% average accuracy and 4.481 MT-Bench score on Mistral-7b-v0.3
- Outperforms baselines by up to 0.842 on MT-Bench and 1.47% on average accuracy
- 6k merged samples outperform 12k selected samples, demonstrating the effectiveness of merging over selection alone
- Ablation studies show quality checking prevents regression, with -2.44% accuracy drop when removed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Topic-aware clustering with submodular selection preserves semantic diversity while eliminating redundancy without LLM scoring.
- **Mechanism:** K-means clusters instructions into semantically coherent groups using all-MiniLM-L6-v2 embeddings (384-dim). Within each cluster, the facility location function (a submodular objective) selects maximally representative samples by optimizing coverage of the cluster's semantic space. This restricts pairwise comparisons to O(|D'_j|) per cluster rather than O(n²) globally.
- **Core assumption:** Assumption: Instruction embeddings capture task semantics sufficiently for clustering to group related tasks meaningfully.
- **Evidence anchors:**
  - [abstract] "topic-aware filtering clusters and removes redundant instructions without LLM scoring, preserving diversity while eliminating redundancy"
  - [section 2.2] K-means objective (Eq. 1) partitions D into m clusters maximizing intra-topic coherence; facility location (Eq. 3-4) selects representative subsets
  - [corpus] D3 paper (arXiv:2503.11441) corroborates that small high-quality datasets can outperform large ones, supporting the efficiency premise
- **Break condition:** If clusters are too fine-grained (m too large), semantic relationships across cluster boundaries are missed; if too coarse, intra-cluster diversity is lost. Paper uses m=120 clusters empirically.

### Mechanism 2
- **Claim:** LLM-based merging synthesizes information-denser instructions that improve training signal quality per sample.
- **Mechanism:** Within each topic cluster, semantically similar pairs (cosine similarity ≥ τ) are identified. An LLM synthesizer M receives concatenated context c_ik = [x_i; y_i; x_k; y_k] and generates merged instruction-output pair (x̂, ŷ). This creates instructions that require multi-step reasoning or combined task capabilities, increasing training complexity without increasing sample count.
- **Core assumption:** Assumption: LLMs can reliably integrate task specifications while preserving all critical constraints from both source instructions.
- **Evidence anchors:**
  - [abstract] "LLM-based merging synthesizes semantically similar instructions into richer, more informative training data"
  - [section 3, Table 1] Translation tasks show 35.2% quality improvement; Language Analysis shows 141.7% improvement post-merge
  - [corpus] No direct corpus precedent for LLM-based instruction merging; related work (T-SHIRT, TACOS) focuses on scoring/filtering, validating novelty
- **Break condition:** Merging semantically incompatible pairs (e.g., creative writing + code debugging) produces incoherent instructions. Quality checking (Mechanism 3) mitigates but doesn't eliminate this risk.

### Mechanism 3
- **Claim:** Quality preservation constraint prevents regression from low-quality merges.
- **Mechanism:** Before accepting a merge, quality scores are computed for original samples (π_quality) and merged result (π'_quality). The merge proceeds only if π'_quality(M_i,j) > α(π_quality(S_i) + π_quality(S_j)) where α=0.75. This ensures merged quality exceeds a weighted threshold of originals, using Deita scorer for evaluation.
- **Core assumption:** Assumption: The Deita scorer (or similar quality metric) correlates with downstream task performance.
- **Evidence anchors:**
  - [section 2.3, Eq. 9] Quality preservation constraint formulation with α=0.75 default
  - [section 2.3] "quality assessment process incurs negligible time (0.5-1.0 seconds per sample pair)"
  - [section 4.4, Table 3] Ablation: removing quality checking causes -2.44% average accuracy drop and -0.4 MT-Bench points
  - [corpus] Weak corpus validation; no neighbor papers reference Deita scorer specifically
- **Break condition:** If quality scorer is misaligned with actual training utility, constraint may reject beneficial merges or accept harmful ones.

## Foundational Learning

- **Concept: Submodular set functions**
  - Why needed here: The facility location function is submodular, enabling greedy optimization with provable approximation guarantees for representative subset selection.
  - Quick check question: Can you explain why submodularity allows efficient optimization where general set functions would require exponential search?

- **Concept: Sentence embeddings and semantic similarity**
  - Why needed here: Both clustering (Stage 1) and pair identification (Stage 2) rely on embedding-space distances/similarities to quantify semantic relationships.
  - Quick check question: Why might cosine similarity be preferred over Euclidean distance for comparing sentence embeddings?

- **Concept: Instruction tuning data quality metrics**
  - Why needed here: The quality checking mechanism assumes existence of reliable quality scorers; understanding what these measure (clarity, complexity, correctness) is essential for debugging merge failures.
  - Quick check question: What failure modes might arise if a quality scorer rewards verbosity over precision?

## Architecture Onboarding

- **Component map:**
Raw Dataset D (52k pairs) -> [Embedding Layer] all-MiniLM-L6-v2 → 384-dim vectors -> [K-means Clustering] → m=120 topic clusters -> [Facility Location Selection] → ~10k representative pairs (D') -> [Similarity Matrix] cosine similarity within each cluster -> [Pair Identification] threshold τ to find merge candidates -> [LLM Merger] GPT-4o or Gemma2-9b synthesizes merged pairs -> [Quality Check] Deita scorer + threshold constraint -> Final Dataset D̂ (~6k merged pairs) -> [LoRA Fine-tuning] lr=2e-5, 3 epochs, batch=4

- **Critical path:** The similarity matrix computation within clusters is the efficiency bottleneck; restricting to pre-clustered groups reduces from O(n²) to O(Σ|D'_j|²) where |D'_j| << |D|.

- **Design tradeoffs:**
  - Cluster count (m): More clusters = finer topic granularity but smaller merge candidate pools
  - Similarity threshold (τ): Higher = fewer, more similar merges; lower = more aggressive merging with higher rejection risk
  - Quality threshold (α): Higher = stricter quality bar, fewer accepted merges; lower = more merges but potential quality regression
  - Merger LLM size: GPT-4o = higher merge quality but cost; Gemma2-9b = lower cost with slight performance drop (Table 6 shows -1.02 avg accuracy)

- **Failure signatures:**
  - Low merge acceptance rate: τ too high or quality scorer misaligned → reduce τ or validate scorer on held-out examples
  - Cluster imbalance: Some topics with many samples, others sparse → consider balanced K-means or per-cluster budget adjustment
  - Merged instructions incoherent: Semantic incompatibility between merged pairs → lower τ or add task-type filtering before merging
  - No performance gain vs. random selection: Diversity preservation failing → validate cluster quality with t-SNE visualization (as in Fig. 2)

- **First 3 experiments:**
  1. **Baseline replication:** Apply MergeIT to Alpaca_52k → 6k merged dataset, train Mistral-7b-v0.3 with LoRA, evaluate on MT-Bench + HuggingFace leaderboard. Target: ~4.48 MT-Bench, ~52.76% average accuracy (Table 2).
  2. **Ablation - merging vs. selection:** Compare MergeIT-6k (merged) vs. facility-location-only-12k (selected but not merged). Expected delta: ~0.18 MT-Bench, ~1.47% avg accuracy (Table 3, rows 1-2).
  3. **Scaling sweep:** Train with 1k, 6k, 8k merged samples to find optimal efficiency-quality tradeoff. Paper shows 6k is optimal; validate this holds for your target model (Fig. 4).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the merging process be refined to guarantee the preservation of critical information from the original instruction pairs?
- **Basis in paper:** [explicit] Section 7 (Limitation) states that the "Merging process occasionally loses the information given from the samples, which potentially harms the information from the original corpus."
- **Why unresolved:** While the paper introduces a quality checking mechanism, it acts as a filter rather than a corrective step, meaning information loss remains a probabilistic occurrence that is not structurally solved.
- **What evidence would resolve it:** An analysis measuring the semantic coverage or specific entity retention rate of merged instructions compared to their source pairs, ideally reaching statistical equivalence.

### Open Question 2
- **Question:** Can the topic-aware filtering mechanism maintain stability and efficiency when applied to datasets significantly larger than the 52k samples tested?
- **Basis in paper:** [explicit] Section 7 (Limitation) notes that the "inevitable clustering process to maintain the diversity... is still served as a less stable method when it comes to larger datasets."
- **Why unresolved:** The K-means clustering used in stage 1 may face computational bottlenecks or centroid drift with web-scale data, a constraint not addressed by the current implementation.
- **What evidence would resolve it:** Experiments applying MergeIT to datasets exceeding 1 million samples, comparing the stability of cluster boundaries and computational overhead against the current methodology.

### Open Question 3
- **Question:** What is the minimum capability threshold for a model to serve effectively as the "Merger" without degrading the quality of the instruction synthesis?
- **Basis in paper:** [inferred] Appendix B, Table 6 shows that while a smaller model (Gemma2-9b) can perform merging, it yields lower average accuracy (51.74%) compared to GPT-4o (52.76%), suggesting a performance dependency on the merger's intelligence.
- **Why unresolved:** The paper demonstrates that smaller models *can* perform the task but leaves open the specific trade-off between the merger model's size/capability and the final instruction tuning performance.
- **What evidence would resolve it:** A systematic ablation study using models of varying parameter counts (e.g., 3B, 7B, 13B) as the merger to identify the point of diminishing returns relative to the GPT-4o baseline.

## Limitations

- The merging process occasionally loses information from original samples, potentially harming the information from the original corpus.
- The clustering process becomes less stable and efficient when applied to datasets significantly larger than the 52k samples tested.
- The minimum capability threshold for an effective merger model remains unclear, with smaller models showing performance degradation.

## Confidence

**High confidence:** The clustering + facility location framework for initial selection is well-established in submodular optimization literature. The empirical results showing 6k merged samples outperforming 12k selected samples are internally consistent.

**Medium confidence:** The quality preservation mechanism with α=0.75 threshold is logically sound, but the actual effectiveness depends on Deita scorer reliability, which isn't independently validated.

**Low confidence:** The LLM-based merging mechanism's generalization to diverse instruction types and its ability to maintain semantic coherence across all merges is the weakest link, given the novelty of this approach.

## Next Checks

1. **Quality scorer validation:** Manually annotate 100 merged instructions with human quality ratings and compute correlation with Deita scorer predictions. Target correlation coefficient > 0.7 to validate the quality checking mechanism.

2. **Threshold sensitivity sweep:** Run experiments varying τ from 0.5 to 0.9 and α from 0.6 to 0.9 on a held-out subset of Alpaca_52k. Plot performance vs. merge acceptance rate to identify optimal operating points and assess robustness.

3. **Cross-dataset generalization:** Apply MergeIT to OpenOrca (code-focused) and UltraChat (multi-modal) datasets. Compare performance retention rate (merged vs. original) across domains to assess framework portability beyond the original dataset.