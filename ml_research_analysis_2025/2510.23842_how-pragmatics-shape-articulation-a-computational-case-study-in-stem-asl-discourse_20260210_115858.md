---
ver: rpa2
title: 'How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL
  Discourse'
arxiv_id: '2510.23842'
source_url: https://arxiv.org/abs/2510.23842
tags:
- sign
- language
- dialogue
- hand
- signs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how pragmatics shape sign articulation in American
  Sign Language (ASL) STEM discourse using motion capture and computational modeling.
  The authors collected an ASL motion capture dataset featuring dyadic instructor-student
  dialogue and isolated vocabulary productions, comparing these to monologue and interpreted
  article contexts.
---

# How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse

## Quick Facts
- **arXiv ID:** 2510.23842
- **Source URL:** https://arxiv.org/abs/2510.23842
- **Reference count:** 0
- **Primary result:** Dialogue signs in ASL STEM discourse are 24.6%-44.6% shorter in duration than isolated signs, with existing sign embedding models failing to capture these pragmatic articulatory differences.

## Executive Summary
This paper investigates how pragmatic context shapes sign articulation in American Sign Language (ASL) STEM discourse through a computational case study. The authors collected motion capture data from 10 ASL signers producing STEM vocabulary in three contexts: dyadic dialogue, monologue, and isolated vocabulary. Using continuous kinematic features, they quantified spatiotemporal changes in sign articulation and compared these across contexts. The study reveals that dialogue-specific articulatory reductions (24.6%-44.6% shorter duration) are not captured by existing sign embedding models, which were trained primarily on isolated signs or interpreter data. This finding highlights critical limitations in current sign language technologies and demonstrates how pragmatic factors fundamentally shape sign production in ways that computational models must account for.

## Method Summary
The authors collected an ASL motion capture dataset featuring STEM vocabulary produced by 10 signers across three contexts: dyadic instructor-student dialogue, monologue, and isolated vocabulary. They extracted continuous kinematic features including movement duration, path length, and velocity from the motion capture data. These features were used to quantify spatiotemporal changes in sign articulation across the different discourse contexts. The study then evaluated existing sign embedding models on their ability to generalize to the pragmatic articulatory differences observed in dialogue versus isolated production. The analysis specifically disentangled dialogue-specific entrainment effects from individual effort reduction by comparing articulation patterns across all three contexts.

## Key Results
- Dialogue signs showed 24.6%-44.6% reduction in duration compared to isolated signs
- Articulation reductions in dialogue contexts were not present in monologue productions
- Existing sign embedding models failed to capture pragmatic articulatory differences between dialogue and isolated contexts
- Kinematic analysis revealed significant spatial and temporal reductions in dialogue that are absent in monologue contexts

## Why This Works (Mechanism)
Pragmatic context fundamentally shapes sign articulation through discourse-driven entrainment mechanisms. In dialogue contexts, signers adapt their articulation based on shared understanding with their conversational partner, leading to more economical sign productions. This entrainment operates independently of individual effort reduction patterns observed in monologue contexts. The failure of existing sign embedding models stems from their training on interpreter or isolated data that lacks these pragmatic articulation patterns, making them unable to generalize to natural dialogue contexts where signs are systematically shorter and more efficient.

## Foundational Learning
- **Motion capture kinematics**: Understanding movement tracking and feature extraction from sign language data
  - *Why needed*: Provides quantitative measurements of articulatory differences across contexts
  - *Quick check*: Can you explain how path length differs from velocity in sign articulation analysis?
- **Sign embedding models**: Representation learning approaches for sign language processing
  - *Why needed*: Current models fail to capture pragmatic differences, revealing their limitations
  - *Quick check*: What training data bias might explain why existing models don't capture dialogue reductions?
- **Pragmatic entrainment**: Discourse-driven adaptation in sign articulation
  - *Why needed*: Explains why dialogue signs differ systematically from isolated productions
  - *Quick check*: How does shared context in dialogue lead to more economical sign articulation?

## Architecture Onboarding

**Component Map:**
Motion Capture Data -> Kinematic Feature Extraction -> Articulation Analysis -> Model Evaluation -> Pragmatic Context Analysis

**Critical Path:**
1. Motion capture data collection across three contexts (dialogue, monologue, isolated)
2. Kinematic feature extraction and spatiotemporal quantification
3. Comparison of articulation patterns across contexts
4. Evaluation of existing sign embedding models on pragmatic differences

**Design Tradeoffs:**
The study prioritized controlled experimental conditions over naturalistic data collection, using motion capture to obtain precise kinematic measurements but limiting ecological validity. The focus on STEM vocabulary constrains generalizability but provides domain specificity. The choice to compare three distinct contexts enables clear isolation of dialogue-specific effects but requires substantial data collection effort.

**Failure Signatures:**
- If kinematic differences between contexts are minimal, it suggests either insufficient sensitivity in feature extraction or that pragmatic effects are less pronounced than hypothesized
- If sign embedding models succeed in capturing articulation differences, it would challenge the paper's core finding about model limitations
- If individual variation overwhelms context effects, it would indicate the need for larger sample sizes or more sophisticated modeling approaches

**First 3 Experiments:**
1. Replicate the kinematic analysis on a different ASL dataset to verify the 24.6%-44.6% duration reduction finding
2. Train a dialogue-specific sign embedding model that conditions on pragmatic context and compare its performance to existing models
3. Conduct a recognition accuracy study comparing model performance on dialogue vs. isolated sign productions

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (10 ASL signers) limits generalizability and statistical power
- Constrained STEM domain focus (four topics) may not represent broader ASL discourse patterns
- Inability to fully disentangle shared context effects from articulatory economy in duration reductions
- Focus on continuous kinematic features without exploring recognition accuracy implications

## Confidence
- **High confidence**: The kinematic measurements of duration reduction (24.6%-44.6%) and the finding that dialogue-specific articulatory changes differ from monologue contexts are well-supported by the data.
- **Medium confidence**: The claim that current sign embedding models fail to generalize to pragmatic contexts is supported, but the specific reasons for this failure (e.g., training data bias vs. fundamental model limitations) require further investigation.
- **Medium confidence**: The interpretation that these articulatory changes reflect pragmatic entrainment rather than individual effort reduction is plausible but not definitively proven by the current analysis.

## Next Checks
1. Expand dataset size and domain coverage to test whether observed articulatory patterns hold across more signers, topics, and discourse types.
2. Conduct controlled recognition experiments using dialogue vs. isolated signs to quantify the practical impact of articulation differences on sign language technology performance.
3. Implement and test dialogue-specific embedding models that explicitly account for pragmatic context, comparing their performance against existing models on both isolated and conversational ASL data.