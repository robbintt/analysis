---
ver: rpa2
title: 'WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control'
arxiv_id: '2501.13592'
source_url: https://arxiv.org/abs/2501.13592
tags:
- wind
- farm
- uni00000013
- control
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WFCRL introduces the first open-source multi-agent reinforcement
  learning suite for wind farm control, providing interfaces with both static (FLORIS)
  and dynamic (FAST.Farm) simulators. The framework treats each turbine as an agent
  that can adjust yaw, pitch, or torque to maximize power production while minimizing
  structural fatigue.
---

# WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control

## Quick Facts
- **arXiv ID**: 2501.13592
- **Source URL**: https://arxiv.org/abs/2501.13592
- **Reference count**: 40
- **Primary result**: Introduces first open-source MARL suite for wind farm control with FLORIS and FAST.Farm interfaces

## Executive Summary
WFCRL presents the first open-source multi-agent reinforcement learning (MARL) benchmark suite specifically designed for wind farm control. The framework treats each turbine as an independent agent capable of adjusting yaw, pitch, or torque to optimize power production while minimizing structural fatigue. It provides interfaces with both static (FLORIS) and dynamic (FAST.Farm) wind farm simulators, enabling comprehensive testing across different simulation fidelity levels.

The benchmark includes ten wind farm layouts, five of which are based on real-world wind farms, providing researchers with realistic test environments. The suite enables systematic comparison of MARL algorithms and investigates critical challenges in wind farm control, including wake steering, turbulence modeling, and transfer learning between different simulation environments.

## Method Summary
WFCRL implements a MARL framework where each turbine operates as an autonomous agent within a shared environment. The agents receive local observations including wind speed, direction, and turbine status, and execute actions to adjust control parameters. The framework supports multiple reward structures focused on power maximization and fatigue reduction. Ten predefined wind farm layouts are provided, with five derived from actual wind farms to ensure realistic testing scenarios. The suite interfaces with both FLORIS (static wake modeling) and FAST.Farm (dynamic simulation) to enable cross-environment validation and study transfer learning capabilities.

## Key Results
- IPPO and MAPPO algorithms achieved up to 15% power increase over baseline in FLORIS simulations
- Five real-world wind farm layouts included for realistic benchmarking
- Transfer learning from FLORIS to FAST.Farm proved challenging, highlighting simulation fidelity gaps
- Framework successfully enabled systematic comparison of MARL approaches for wind farm control

## Why This Works (Mechanism)
The framework works by treating each turbine as an independent decision-making agent within a cooperative multi-agent system. Each agent observes local conditions including wind speed, direction, and its own operational status, then decides on control actions (yaw, pitch, or torque adjustments). Through MARL algorithms, agents learn optimal control policies that account for wake interactions between turbines while maximizing collective power output. The dual-simulator approach (FLORIS and FAST.Farm) allows validation across different levels of physical fidelity, from simplified wake models to full dynamic simulation including turbulence and structural dynamics.

## Foundational Learning
- **Wake steering principles**: Understanding how upstream turbine yaw angles affect downstream turbine performance - needed to design effective MARL reward structures
- **Multi-agent reinforcement learning**: Cooperative MARL algorithms like IPPO and MAPPO for decentralized control - needed to enable independent turbine decision-making
- **Wind farm simulation**: FLORIS for steady-state wake modeling and FAST.Farm for dynamic simulation - needed to create realistic training and testing environments
- **Transfer learning**: Moving policies between different simulation fidelity levels - needed to assess real-world applicability of learned controls
- **Structural fatigue modeling**: Including fatigue penalties in reward functions - needed to ensure realistic long-term operation
- **Observation space design**: Local vs. global observations for each agent - needed to balance computational efficiency with control effectiveness

## Architecture Onboarding

**Component map**: Wind Farm Layouts -> Simulation Environment (FLORIS/FAST.Farm) -> MARL Agent Interface -> Control Algorithm (IPPO/MAPPO) -> Action Space (Yaw/Pitch/Torque) -> Reward Function (Power + Fatigue)

**Critical path**: Simulation Environment -> Agent Interface -> Control Algorithm -> Action Execution -> Reward Calculation -> Policy Update

**Design tradeoffs**: Static vs. dynamic simulation fidelity, local vs. global observations, simple vs. complex reward structures, computational efficiency vs. control precision

**Failure signatures**: Policies that work in FLORIS but fail in FAST.Farm, overfitting to specific wind conditions, suboptimal wake steering due to limited observation scope, excessive structural fatigue from aggressive control policies

**First experiments**: 1) Run baseline IPPO algorithm on a simple wind farm layout in FLORIS, 2) Compare local vs. global observation performance in the same environment, 3) Test transfer learning from FLORIS to FAST.Farm on a basic layout

## Open Questions the Paper Calls Out
None

## Limitations
- Transfer learning gap between FLORIS and FAST.Farm simulations limits real-world applicability
- Limited number of training runs (100 for FLORIS, 60 for FAST.Farm) may not capture full variability
- Lack of real-world field validation for simulation-based performance claims

## Confidence
- FLORIS-based performance claims: Medium
- FAST.Farm transfer claims: Low
- Real-world applicability: Low

## Next Checks
1. Conduct real-world field tests to validate simulation-based performance claims and assess practical feasibility of learned control policies
2. Perform extensive cross-validation across additional wind farm layouts and atmospheric conditions to test policy robustness
3. Implement systematic comparison of different multi-agent RL algorithms beyond IPPO and MAPPO to establish baseline performance metrics and identify optimal approaches for wind farm control