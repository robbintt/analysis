---
ver: rpa2
title: Lower Bounds for Public-Private Learning under Distribution Shift
arxiv_id: '2507.17895'
source_url: https://arxiv.org/abs/2507.17895
tags:
- priv
- private
- public
- data
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes new lower bounds for private learning when
  both private and public data are available, particularly under distribution shift.
  The authors extend the fingerprinting method for differential privacy lower bounds
  to settings where the public and private data come from different but related distributions.
---

# Lower Bounds for Public-Private Learning under Distribution Shift

## Quick Facts
- arXiv ID: 2507.17895
- Source URL: https://arxiv.org/abs/2507.17895
- Reference count: 40
- Establishes lower bounds showing no synergy between public and private data under distribution shift in Gaussian settings

## Executive Summary
This paper establishes fundamental limits on the benefits of combining public and private data for statistical learning when the two data sources follow different distributions. The authors develop a fingerprinting method extension to analyze private learning under distribution shift, focusing on Gaussian mean estimation and linear regression problems. Their main result shows that when public and private data come from related but shifted distributions, there is typically no complementarity between them - either one source alone suffices or neither can achieve the desired accuracy. The work provides theoretical justification for why simply mixing public and private data does not automatically improve learning performance under realistic conditions.

## Method Summary
The authors extend the fingerprinting method for differential privacy lower bounds to settings with distribution shift between public and private data sources. They employ a Bayesian perspective where conjugate priors are carefully chosen to make posterior calculations tractable under distribution shift. The approach involves constructing a family of hypothesis pairs and using information-theoretic arguments to bound the error rates of any private learning algorithm. For Gaussian mean estimation, they analyze how the shift magnitude affects the learning requirements, showing that small shifts allow public data alone to suffice while large shifts require private data. The linear regression analysis extends these ideas to more complex parameter estimation problems, considering both the presence and absence of parameter shift.

## Key Results
- For Gaussian mean estimation with distribution shift, either sufficient public or private data alone is needed when shift is small; only private data matters when shift is large
- No complementarity exists between public and private data sources under distribution shift in these settings
- Similar results hold for Gaussian linear regression both with and without parameter shift
- The fingerprinting method extends to distribution shift using conjugate priors for tractable posterior calculations

## Why This Works (Mechanism)
The mechanism relies on the fingerprinting method's ability to detect when an algorithm has learned from private data by embedding statistical fingerprints in hypothesis pairs. Under distribution shift, the authors show that public data cannot effectively bridge the gap between the shifted distributions unless the shift is small enough that public data alone could solve the problem. The conjugate prior structure enables precise calculation of posterior distributions under the shifted public-private data mixture, allowing rigorous lower bound proofs. This approach demonstrates that any algorithm achieving the target accuracy must essentially rely on one data source or the other, not both synergistically.

## Foundational Learning
- Differential Privacy Fundamentals: Why needed - forms the theoretical framework for privacy guarantees; Quick check - verify understanding of Îµ-DP definition and basic composition theorems
- Information Theory: Why needed - used for proving lower bounds via mutual information and KL divergence; Quick check - compute mutual information for simple Gaussian distributions
- Conjugate Priors: Why needed - enables tractable posterior calculations under distribution shift; Quick check - derive posterior for Gaussian likelihood with Gaussian prior
- Hypothesis Testing: Why needed - fingerprinting method relies on distinguishing between neighboring hypotheses; Quick check - calculate Type I and Type II errors for simple hypothesis tests
- Gaussian Statistics: Why needed - both problems analyzed assume Gaussian distributions; Quick check - compute MLE for Gaussian mean and covariance

## Architecture Onboarding

**Component Map:**
Public Data Source -> Distribution Shift Parameter -> Private Data Source -> Learning Algorithm -> Accuracy Guarantee

**Critical Path:**
Distribution Shift Specification -> Prior Construction -> Posterior Calculation -> Error Bound Derivation -> Lower Bound Proof

**Design Tradeoffs:**
The choice between public and private data depends critically on the shift magnitude - small shifts favor public data while large shifts require private data, creating a threshold effect in the design space.

**Failure Signatures:**
When the shift exceeds a critical threshold, public data provides no benefit regardless of quantity, while insufficient private data fails to achieve target accuracy. Algorithms attempting to combine both sources perform no better than using the more informative source alone.

**First Experiments:**
1. Vary shift magnitude in Gaussian mean estimation and measure the breakpoint where public data becomes ineffective
2. Test the fingerprinting method on non-Gaussian distributions to assess generality
3. Compare empirical performance of algorithms using only public, only private, and combined data sources

## Open Questions the Paper Calls Out
None

## Limitations
- Results rely heavily on Gaussian assumptions and specific conjugate prior structures that may not generalize to arbitrary distributions
- Only provide lower bounds without corresponding upper bounds, showing impossibility but not achievability
- The claim that "no synergy is possible" is based on worst-case constructions that may not reflect practical settings
- Does not address scenarios where public and private data might actually complement each other under different conditions

## Confidence
- Gaussian mean estimation results: **High** confidence - rigorous information-theoretic arguments
- Linear regression bounds: **Medium** confidence - more complex interplay between parameter shift and distribution shift
- Broader implications about public-private learning complementarity: **Low** confidence - extrapolates beyond specific mathematical settings

## Next Checks
1. Test the fingerprinting method on non-Gaussian distributions with different prior structures to assess generality
2. Implement empirical studies comparing public-private learning performance across varying levels of distribution shift
3. Extend the analysis to non-parametric settings to understand how the bounds behave when parametric assumptions break down