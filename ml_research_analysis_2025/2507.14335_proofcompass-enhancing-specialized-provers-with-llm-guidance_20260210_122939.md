---
ver: rpa2
title: 'ProofCompass: Enhancing Specialized Provers with LLM Guidance'
arxiv_id: '2507.14335'
source_url: https://arxiv.org/abs/2507.14335
tags:
- proof
- lemmas
- theorem
- lemma
- formal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROOF COMPASS is a novel methodology that enhances specialized
  theorem provers by strategically guiding them with a large language model (LLM).
  Instead of training larger models, PROOF COMPASS uses an LLM to generate natural
  language proof strategies and analyze failed attempts to extract useful intermediate
  lemmas, enabling effective problem decomposition.
---

# ProofCompass: Enhancing Specialized Provers with LLM Guidance

## Quick Facts
- arXiv ID: 2507.14335
- Source URL: https://arxiv.org/abs/2507.14335
- Reference count: 40
- PROOF COMPASS achieves 55.3% Pass@128 on miniF2F, outperforming DeepSeek-Prover-v1.5-RL (54.9%) while using 25x fewer attempts

## Executive Summary
PROOF COMPASS introduces a novel methodology that enhances specialized theorem provers by combining LLM-generated natural language proof guidance with lemma extraction from failed attempts. Rather than training larger models, it uses an LLM to generate informal proofs and strategically select intermediate lemmas that guide a specialized Lean prover. The approach achieves state-of-the-art efficiency on the miniF2F benchmark, demonstrating that LLM-guided decomposition significantly improves computational efficiency and accuracy in formal theorem proving.

## Method Summary
PROOF COMPASS uses a three-stage pipeline: (1) an LLM generates a natural language proof summary that is injected as a block comment into the SLM's input; (2) after N=16 failed SLM attempts, 'have' statements are extracted from failed proofs and the LLM selects ≤5 globally provable lemmas; (3) lemmas are proven sequentially, then assembled with the main proof. The approach uses Gemini 2.0 Flash Thinking for proof generation and Gemini 2.0 Flash for summarization and lemma selection, while DSP-v1.5-RL in Chain-of-Thought mode serves as the specialized prover. The entire pipeline is verified through Lean 4's REPL with 20-second timeouts.

## Key Results
- Achieves 55.3% Pass@128 on miniF2F-test benchmark
- Outperforms standalone DeepSeek-Prover-v1.5-RL (54.9% Pass@3200) with 25x fewer attempts (128 vs 3200)
- With only 32 attempts, PROOF COMPASS surpasses baseline's Pass@128 performance (52.5% vs 51.6%)
- Lemmas extracted from SLM's own failed attempts require 12 attempts vs 17.25 for LLM-generated lemmas (>40% improvement)

## Why This Works (Mechanism)

### Mechanism 1: LLM-Generated Informal Proof Guidance
Providing a high-quality natural language proof summary to the specialized prover narrows the proof search space and accelerates convergence. A general-purpose LLM generates a complete NL proof, which is summarized and injected into the SLM's input as a block comment. The SLM uses this pre-generated reasoning instead of synthesizing its own informal sketch, reducing per-attempt generation time from 4.05s to 3.21s.

### Mechanism 2: Lemma Extraction from Failed SLM Attempts
Lemmas extracted from the SLM's own failed attempts are more tractable for the SLM to prove than lemmas directly generated by an LLM. Failed proof attempts contain 'have' statements—intermediate goals the SLM itself proposed. These are syntactically aligned with the SLM's training distribution, requiring 12 attempts vs 17.25 for LLM-generated lemmas.

### Mechanism 3: Budget-Aware Iterative Proving
Decomposing theorem proving into sequential lemma subproblems enables more efficient use of a fixed attempt budget. After initial N=16 attempts fail, the method iterates through lemma proving then full-proof assembly. Each lemma proof and main proof attempt consumes one unit from budget B=128, with early success on any component avoiding exhaustive search.

## Foundational Learning

- **Lean 4 'have' statements as subgoals**: The entire lemma extraction mechanism depends on understanding that 'have' creates intermediate goals with their own proofs, enabling structured decomposition. *Quick check*: Can you explain how a 'have' statement differs from a 'let' binding in Lean 4?
- **Pass@k evaluation metric**: Paper's claims of 25x efficiency depend on comparing Pass@128 to Pass@3200—you must understand that Pass@k measures probability of success within k independent attempts. *Quick check*: If a method achieves 52% Pass@32 and 55% Pass@128, what does this tell you about the marginal value of additional attempts?
- **Chain-of-Thought prompting in theorem proving**: DSP-v1.5's CoT mode generates informal proofs before formal code—ProofCompass leverages this structure to inject external guidance. *Quick check*: How would you modify the approach for a prover that lacks CoT-style output?

## Architecture Onboarding

- **Component map**: [Theorem Input] → [LLM: Generate NL Proof] → [LLM: Summarize] → [SLM: N Initial Attempts] → (if all fail) [Extract 'have' statements] → [Syntax Check] → [LLM: Select ≤5 Lemmas] → [LLM: Generate NL proofs for each lemma + main] → [For each lemma: SLM attempt] → [Build final proof with proven lemmas + Pmain] → [Lean 4 REPL: Verify]
- **Critical path**: 1) NL proof generation and summarization (one-time ~30s overhead); 2) First N=16 SLM attempts (solves majority of tractable problems); 3) Lemma selection from failed attempts (only if step 2 fails completely); 4) Sequential lemma proving → full proof assembly
- **Design tradeoffs**: N=16 vs higher balances upfront solving vs lemma pool diversity; k=5 lemmas vs more prevents budget dilution on trivial subproblems; Extract from SLM vs LLM-generate provides 40% efficiency gain but limits lemma diversity
- **Failure signatures**: Zero valid lemmas extracted → falls back to continued initial strategy; All selected lemmas proven but Pmain fails → budget exhausted without completing theorem; LLM selects incorrect lemmas (dependent on local assumptions) → SLM wastes attempts on unprovable subgoals
- **First 3 experiments**: 1) Reproduce ablations separately to validate each mechanism's contribution independently; 2) Vary N (initial attempts) to confirm N=16 balances upfront solving vs lemma pool diversity; 3) Cross-prover validation to test architectural portability claims

## Open Questions the Paper Calls Out
- **Open Question 1**: How does PROOF COMPASS's performance scale when extended to computational budgets of 2^10 to 2^17 attempts? (Authors state this as important future research avenue)
- **Open Question 2**: Does the lemma-based guidance framework maintain effectiveness across specialized provers with different architectures? (Need to validate across BFS-Prover, InternLM2.5-StepProver, Isabelle-based systems)
- **Open Question 3**: Can generalized informal proof guidance techniques be developed for provers lacking DSP-v1.5-style Chain-of-Thought interfaces? (Current mechanism depends on Lean's block comment syntax)
- **Open Question 4**: What mechanisms drive the ~40% efficiency advantage of SLM-extracted lemmas over directly LLM-generated lemmas? (Unclear whether gap stems from training data distribution, syntax familiarity, or semantic alignment)

## Limitations
- Performance scaling beyond Pass@128 remains untested, limiting understanding of efficiency gains at larger budgets
- Only validated on DeepSeek-Prover-v1.5-RL, with portability claims to other specialized provers unverified
- Informal proof injection mechanism is tailored to specific CoT-style input, requiring generalization for broader prover compatibility

## Confidence
- **High**: The core methodology and experimental results are clearly specified with detailed ablation studies and hyperparameter choices
- **Medium**: Budget allocation for lemma proving stage and DSP-v1.5 CoT activation prompt details are partially specified
- **Low**: Sampling hyperparameters for DSP-v1.5 and Gemini API calls are not fully specified

## Next Checks
1. Reproduce the ablations separately to validate each mechanism's contribution independently
2. Vary N (initial attempts) to confirm Figure 5's claim that N=16 balances upfront solving vs lemma pool diversity
3. Apply lemma extraction to a different CoT-capable prover to test architectural portability claims