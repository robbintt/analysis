---
ver: rpa2
title: 'Red Teaming with Artificial Intelligence-Driven Cyberattacks: A Scoping Review'
arxiv_id: '2503.19626'
source_url: https://arxiv.org/abs/2503.19626
tags:
- attacks
- methods
- data
- attack
- security
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review examined the use of artificial intelligence
  (AI) in cybersecurity attacks, identifying AI methods and targets for red teaming
  activities. From 470 records screened, 11 articles were included in the review.
---

# Red Teaming with Artificial Intelligence-Driven Cyberattacks: A Scoping Review

## Quick Facts
- arXiv ID: 2503.19626
- Source URL: https://arxiv.org/abs/2503.19626
- Reference count: 5
- From 470 records screened, 11 articles were included in the review.

## Executive Summary
This scoping review examined the use of artificial intelligence (AI) in cybersecurity attacks, identifying AI methods and targets for red teaming activities. Various cyberattack methods were identified, targeting sensitive data, systems, social media profiles, passwords, and URLs. AI techniques such as deep learning, machine learning, and large language models were found to enhance attack efficiency and effectiveness. The review highlights the increasing threat of AI-driven cyberattacks and emphasizes the need for comprehensive security measures.

## Method Summary
The study conducted a PRISMA scoping review using Finna library database (269 records) and Google Scholar (202 records), searching with 13 keywords related to AI cyber attacks and red teaming. After deduplication and screening against 4 inclusion criteria, 11 studies were included. The review categorized AI attack methods by functionality (classification, regression, clustering) and identified target categories. The methodology lacked specific Boolean logic details and database temporal state documentation, limiting exact reproducibility.

## Key Results
- LSTM networks were the most prevalent AI method, appearing in 5 of the 11 included studies.
- AI attacks primarily target sensitive data, URLs, social media profiles, passwords, and systems.
- Defensive tactics including AI-based anomaly detection and predictive analytics are recommended to counter evolving AI-driven threats.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LSTM networks effectively analyze sequences of data to automate and refine attack vectors like phishing URLs.
- Mechanism: LSTMs process sequential data (e.g., character strings in URLs) to recognize temporal patterns. By training on successful attack histories, the model generates new, variable outputs that evade static detection rules.
- Core assumption: Historical attack data contains learnable patterns that, when replicated, result in higher success rates than random generation.
- Evidence anchors:
  - [abstract] "LSTM was most prevalent, appearing in five studies."
  - [section 4.1] "LSTMs are popular because they are good at analyzing sequences of data, making them useful for detecting patterns over time."
- Break condition: If defenders shift to real-time, behavior-based anomaly detection that does not rely on static string matching, the advantage of generating "realistic-looking" strings via LSTM diminishes.

### Mechanism 2
- Claim: GANs facilitate attacks against authentication and data integrity by creating synthetic data that closely mimics legitimate data distributions.
- Mechanism: A generator network creates candidates and a discriminator evaluates them against real data samples. This adversarial loop continues until the generator produces outputs indistinguishable from genuine credentials.
- Core assumption: The discriminator can be trained sufficiently that the generator's output converges on realistic data that the target system cannot distinguish from real user input.
- Evidence anchors:
  - [section 5.1] "GANs... are particularly effective in creating realistic but fake data... providing cybercriminals with the predictive power needed to anticipate and exploit system weaknesses."
  - [section 4.3] Mentions PassGAN and Malware-GAN as specific implementations for password guessing and malware obfuscation.
- Break condition: If security systems implement robust input sanitization or behavioral checks that validate the source of data rather than just the format, GAN-generated inputs may be rejected.

### Mechanism 3
- Claim: Classification methods (SVM, CNN) enable "reconnaissance" and "target selection" by categorizing large datasets to identify sensitive assets or vulnerable system configurations.
- Mechanism: Offensive actors use supervised learning models to scan exfiltrated or scraped data to classify and prioritize high-value targets, accelerating the initial phase of an attack.
- Core assumption: The attacker has access to sufficient data to train a classifier that can accurately distinguish valuable targets from non-valuable ones.
- Evidence anchors:
  - [abstract] "AI attacks primarily target sensitive data, URLs, social media profiles, passwords, and systems."
  - [section 5.1] "Classification methods may help attackers to find new vulnerabilities and potential targets, since categorization of data can indicate patterns otherwise unknown."
- Break condition: If data is properly encrypted or anonymized to the point where features are indistinguishable, classification models cannot effectively categorize targets.

## Foundational Learning

- Concept: **Red Teaming vs. Blue Teaming**
  - Why needed here: The paper defines red teaming as "emulating a potential adversary," distinct from blue teaming (defense). Understanding this distinction is critical to knowing why these AI methods are used for offense (simulation) rather than direct defense.
  - Quick check question: Can you explain the difference between using AI to find a vulnerability (Red) versus using AI to patch a vulnerability (Blue)?

- Concept: **Sequential Data Processing (RNN/LSTM)**
  - Why needed here: The paper identifies LSTM as the most prevalent method. You must understand why sequential modeling is superior for tasks like URL generation or log analysis compared to static analysis.
  - Quick check question: Why would a recurrent network (LSTM) be better at generating a phishing URL than a standard feed-forward network?

- Concept: **Generative Adversarial Networks (GANs)**
  - Why needed here: GANs are highlighted as a key regression method for creating "realistic" fake data. Understanding the generator-discriminator dynamic is essential to grasping how AI bypasses detection.
  - Quick check question: In a red team context, does the GAN act as the attacker (generator) or the defender (discriminator) during the training phase?

## Architecture Onboarding

- Component map:
  Input Layer: Target Data (URLs, Passwords, Social Media, System Logs) ->
  Processing Layer (AI Core): Classification (SVM/CNN) for Recon; LSTM/Sequential for Pattern Analysis; GANs for Synthetic Generation ->
  Action Layer: Automated Phishing, Password Cracking (PassGAN), Anomaly Exploitation ->
  Target Assets: Sensitive Data, User Identities, System Vulnerabilities

- Critical path:
  1. Data Ingestion: Gather raw data (e.g., leaked passwords, URL structures).
  2. Model Training: Train LSTM (for patterns) or GAN (for generation).
  3. Payload Generation: Create phishing URLs or password candidates.
  4. Delivery/Execution: Automated interaction with target system (Access & Penetration).

- Design tradeoffs:
  - Speed vs. Stealth: The paper notes AI accelerates execution (Abstract), but faster automated scans may trigger defensive anomaly detection (Section 5.2).
  - Generalization vs. Specificity: Pre-trained models are fast to deploy but may be less effective against niche, hardened targets compared to custom-trained models.

- Failure signatures:
  - Low Success Rate in Generation: Generated URLs or passwords look random or malformed (GAN convergence failure).
  - High Detection Rate: Automated responses are flagged as "bot traffic" by defensive AI (Section 5.2 mentions defensive AI detecting deviations).
  - Overfitting: The attack model works only on the specific training data and fails on real-world variations.

- First 3 experiments:
  1. URL Pattern Analysis: Implement a basic LSTM to process a dataset of known malicious URLs to identify sequential character patterns.
  2. Target Classification: Use an SVM to classify a dummy dataset of user profiles into "high value" vs. "low value" targets based on keywords.
  3. Defensive Evasion Test: Generate synthetic password hashes using a simple GAN and test them against a standard hashcat rule set to see if they evade common cracking rules.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the AI attack methods identified in this review be systematically mapped to established cybersecurity taxonomies to provide a more granular classification?
- Basis in paper: [explicit] The conclusion states that future research should include "describing the various attack methods in more detail using existing taxonomies."
- Why unresolved: The current study categorized methods broadly by AI functionality rather than by their specific role or phase within standard cyberattack frameworks.
- What evidence would resolve it: A re-analysis of the identified methods (LSTM, GAN, etc.) mapped specifically to frameworks like MITRE ATT&CK or the Cyber Kill Chain.

### Open Question 2
- Question: What emerging attack targets are utilized by AI-driven cyberattacks that fall outside the five categories identified in this review?
- Basis in paper: [explicit] The conclusion lists "finding more attack targets used by AI attackers" as a primary direction for future research problems.
- Why unresolved: The scoping review was limited to 11 studies, which constrained the identified targets to general data, URLs, social media profiles, passwords, and system details.
- What evidence would resolve it: A broader systematic review or empirical studies identifying novel targets such as specific IoT vulnerabilities, cloud infrastructure configurations, or biometric authentication systems.

### Open Question 3
- Question: How do the capabilities of Large Language Models (LLMs) in social engineering compare to the traditional AI methods (e.g., LSTM, Markov chains) currently dominating the literature?
- Basis in paper: [inferred] The introduction highlights LLMs as a means to generate convincing phishing content, yet the results show that the reviewed literature still primarily focuses on older architectures like LSTMs and CNNs.
- Why unresolved: There is a temporal gap between the rise of generative AI and the availability of academic literature analyzing its specific application in red teaming compared to established methods.
- What evidence would resolve it: Comparative experiments measuring the click-through or success rates of phishing campaigns generated by LLMs versus those generated by the traditional AI methods identified in the review.

## Limitations
- Small final sample size (n=11 studies) constrains generalizability of findings about AI-driven cyberattacks.
- Search methodology lacks specificity regarding Boolean operators and phrase matching, making exact reproduction challenging.
- The scoping review did not assess study quality or risk of bias, which may affect reliability of conclusions about AI method prevalence and effectiveness.

## Confidence
- High Confidence: Identification of LSTM as the most prevalent AI method (appearing in 5 studies) - directly stated in results with specific counts.
- Medium Confidence: Claims about GAN effectiveness for creating synthetic data - supported by theoretical explanation and specific implementations mentioned, but limited by small sample size.
- Medium Confidence: Classification methods' role in reconnaissance - supported by abstract and discussion, but mechanism details are relatively general.

## Next Checks
1. Replicate the search methodology using the exact keywords and inclusion criteria to verify if the same 11 studies are identified, documenting any discrepancies in search results.
2. Conduct a quality assessment of the 11 included studies to evaluate their methodological rigor and potential biases in reporting AI attack effectiveness.
3. Test the proposed mechanisms by implementing a simple LSTM-based URL generator and GAN-based password generator to empirically evaluate their claimed advantages over traditional methods.