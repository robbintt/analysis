---
ver: rpa2
title: "MATA (m\u0101ta): Mindful Assessment of the Telugu Abilities of Large Language\
  \ Models"
arxiv_id: '2508.13526'
source_url: https://arxiv.org/abs/2508.13526
tags:
- questions
- telugu
- language
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATA, a carefully curated evaluation dataset
  for assessing the Telugu language capabilities of large language models. The dataset
  contains 729 questions across seven categories, including grammar, vocabulary, reading
  comprehension, factual knowledge, linguistic reasoning, idioms and proverbs, and
  other reasoning tasks.
---

# MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models

## Quick Facts
- arXiv ID: 2508.13526
- Source URL: https://arxiv.org/abs/2508.13526
- Authors: Chalamalasetti Kranti; Sowmya Vajjala
- Reference count: 36
- Key outcome: Introduces MATA, a Telugu evaluation dataset showing LLMs exploit heuristics in MCQs and struggle with open-ended questions

## Executive Summary
This paper introduces MATA, a carefully curated evaluation dataset for assessing Telugu language capabilities of large language models. The dataset contains 729 questions across seven categories, including grammar, vocabulary, reading comprehension, factual knowledge, linguistic reasoning, idioms and proverbs, and other reasoning tasks. Both multiple-choice and open-ended question formats are included, with questions sourced from Telugu school textbooks and competitive exams.

The authors evaluate 11 open-weight and proprietary LLMs using both Telugu and English prompts. Results show that models perform significantly better on multiple-choice questions than open-ended ones, with performance varying widely across categories. Some models show sensitivity to answer position and distractor patterns in multiple-choice settings, indicating reliance on heuristics rather than true understanding. LLM-as-a-judge evaluations are found to be less reliable than human judgments, particularly in the low-resource Telugu context. The study highlights the need for more nuanced, culturally grounded evaluation methods to accurately assess multilingual LLM capabilities.

## Method Summary
The study evaluates 11 LLMs (both open-weight and proprietary) on the MATA dataset using zero-shot prompting through the Inspect framework. Models are tested with both Telugu and English prompts across 729 questions spanning seven linguistic categories. Multiple-choice questions are scored using exact match against ground truth, while open-ended questions are evaluated using LLM-as-a-judge with LLaMA3.3-70B and Gemma3-27B. The authors conduct robustness analysis by manipulating answer positions in MCQs and introducing "None of the others" distractors to test for heuristic exploitation.

## Key Results
- Models achieve significantly higher accuracy on multiple-choice questions (94.2% for Gemini2.5-Pro) compared to open-ended questions (84.3% for same model)
- LLM-as-a-judge evaluations systematically underestimate performance compared to human judgments, particularly for culturally-grounded categories
- Some models show dramatic performance drops when answer positions are manipulated or when "None of the others" is introduced, indicating reliance on positional heuristics
- Gemma3-12B achieved 0% accuracy when "None of the others" was positioned at the end of answer choices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exploit positional heuristics in multiple-choice formats rather than demonstrating genuine language understanding.
- Mechanism: Models learn statistical associations between answer positions and correctness during pretraining, allowing them to achieve high accuracy on standard MCQ benchmarks without true comprehension. When answer positions are manipulated (e.g., fixing correct answers to position A), some models show dramatic performance changes—Claude-Sonnet 4 achieved 100% accuracy when correct answers were consistently in position A.
- Core assumption: Assumption: Positional biases transfer across languages, including low-resource languages like Telugu.
- Evidence anchors:
  - [abstract] "we empirically show how LLMs rely on superficial heuristics such as answer position and distractor patterns for multiple-choice questions"
  - [Section 5.2] "certain models exhibited clear preferences or aversions to specific positions... Claude-Sonnet 4 achieved 100% accuracy when the correct answer consistently appeared in position A"
  - [corpus] "Choices Speak Louder than Questions" (FMR 0.588) explores choice sensitivity in MCQA evaluation
- Break condition: When "None of the others" replaces correct answers, Gemma3-12B dropped from high performance to 0% accuracy, indicating shallow pattern matching breaks under distribution shift.

### Mechanism 2
- Claim: Open-ended generation exposes deeper linguistic capability gaps that MCQs mask.
- Mechanism: MCQs allow models to recognize correct answers from options (recognition task), while open-ended questions require free-form generation (recall + production). The performance gap—Gemini2.5-Pro achieved 94.2% on MCQs but only 84.3% on open-ended with English prompts—reveals that recognition-based evaluation overestimates true capability.
- Core assumption: The gap reflects production difficulty rather than evaluation methodology artifacts.
- Evidence anchors:
  - [abstract] "models perform significantly better on multiple-choice questions than open-ended ones"
  - [Section 5.1] "Open-ended questions proved to be significantly more challenging, with most models scoring between 0.22 and 0.45"
  - [corpus] AutoLogi paper notes MCQs "are vulnerable to random guessing, leading to overestimated performance"
- Break condition: Models providing incomplete responses (e.g., listing 1-2 items when 4 are required) suggests partial knowledge that binary correct/incorrect scoring penalizes disproportionately.

### Mechanism 3
- Claim: LLM-as-a-judge evaluation systematically underestimates model performance in low-resource languages compared to human evaluation.
- Mechanism: Judge LLMs (LLaMA3.3-70B, Gemma3-27B) lack sufficient Telugu representation to accurately assess semantic equivalence, leading to harsher grading than human native speakers.
- Core assumption: The judge models' training data has insufficient Telugu coverage for nuanced evaluation.
- Evidence anchors:
  - [abstract] "LLM-as-a-judge evaluations are found to be less reliable than human judgments, particularly in the low-resource Telugu context"
  - [Section 5.3, Table 2] Human annotators rated Qwen3-32B at 59.1% for Vocabulary while LLaMA judge scored it 64.7%, but for Linguistic Reasoning, humans gave 33.3% vs judge's 26.1%
  - [corpus] Weak corpus evidence—no direct neighbor papers address LLM-as-judge reliability specifically in low-resource settings
- Break condition: Judge disagreement >10% across categories (Tables 7-8) indicates evaluation instability.

## Foundational Learning

- Concept: **Low-resource language evaluation pitfalls**
  - Why needed here: Telugu is the 18th most spoken language globally but "typically considered low-resource in terms of NLP support." Translated benchmarks fail to capture "cultural and linguistic nuances."
  - Quick check question: Why would translating MMLU to Telugu provide an incomplete assessment compared to MATA's native-source approach?

- Concept: **Multiple-choice robustness testing**
  - Why needed here: The paper demonstrates that controlling for answer position and introducing "None of the others" options reveals whether performance reflects understanding or heuristics.
  - Quick check question: If a model achieves 90% accuracy on MCQs but drops to 20% when correct answers are always placed in position D, what does this suggest about its capabilities?

- Concept: **LLM-as-a-judge limitations**
  - Why needed here: Binary correct/incorrect grading by LLM judges showed systematic underestimation versus human evaluators, particularly for categories requiring cultural knowledge.
  - Quick check question: What validation step is essential before trusting LLM-judge scores for a new low-resource language?

## Architecture Onboarding

- Component map: MATA Dataset (729 questions) -> Categories: Grammar, Vocabulary, Reading Comprehension, Factual Knowledge, Linguistic Reasoning, Idioms/Proverbs, Other Reasoning -> Formats: MCQ (25%) + Open-ended (75%) -> Sources: Grades 1-10 textbooks + competitive exams -> Evaluation Pipeline: MCQ Scorer -> Open-ended Judge: LLaMA3.3-70B or Gemma3-27B (binary) -> Robustness Tests: Position fixing, "None" substitution

- Critical path: Start with English prompts (Section 5.1 shows slightly better performance), validate judge reliability via human comparison before scaling, then run robustness analysis on categories with high MCQ accuracy.

- Design tradeoffs:
  - Native curation vs. translation: Native sources capture cultural grounding but limit dataset scale (729 questions vs. thousands in translated benchmarks)
  - Binary vs. graded evaluation: Binary scoring enables automation but loses partial credit information
  - Judge model selection: LLaMA3.3-70B vs. Gemma3-27B showed >10% disagreement in some categories

- Failure signatures:
  - Llama3.3-70B dropped from 63.2% (EN prompt) to 21.1% (TE prompt) primarily due to format-following failures, not comprehension
  - Gemma3-12B's 0% score when "None of the others" positioned at end indicates extreme sensitivity to distribution shift
  - Sarvam-M-24B (trained on Indian languages) still struggled with Factual Knowledge (27.8% MCQ), indicating cultural knowledge gaps

- First 3 experiments:
  1. Replicate baseline: Evaluate target model on MATA with both EN and TE prompts, compare MCQ vs. open-ended gap to establish capability profile
  2. Robustness stress test: Apply position-fixing and "None of the others" modifications to identify heuristic reliance patterns specific to your model
  3. Judge validation: Sample 50+ open-ended responses across categories, compare LLM-judge scores against human evaluation to calibrate expected error bounds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the observed sensitivity to "None of the others" distractor patterns (with accuracy drops from 90% to 0% in some cases) more severe in low-resource languages like Telugu compared to high-resource languages?
- **Basis in paper:** [explicit] Section 5.2 states: "Whether this is specifically more severe in low-resource contexts is a question for further investigation in future."
- **Why unresolved:** The study only tested Telugu; direct comparison with equivalent high-resource language benchmarks was not conducted.
- **What evidence would resolve it:** Parallel robustness experiments on English, Spanish, or other high-resource benchmarks using identical MCQ manipulation protocols.

### Open Question 2
- **Question:** What explains the differential behavior between closed-weight and open-weight models when "None of the others" appears at random versus final positions?
- **Basis in paper:** [inferred] Section 5.2 observes closed models perform worse when None is at the end, while open-weight models drop more when None appears randomly—but the underlying causes remain speculative (training distribution vs. positional expectations).
- **Why unresolved:** The authors hypothesize about training associations but provide no mechanistic explanation.
- **What evidence would resolve it:** Probing studies on model attention patterns or controlled training experiments varying MCQ answer distributions.

### Open Question 3
- **Question:** Would fine-tuned judge LLMs or LLM-as-a-jury approaches significantly improve evaluation reliability for low-resource languages?
- **Basis in paper:** [explicit] Limitations section notes the authors "did not extensively conduct... other approaches such as LLMs-as-a-jury or employing fine-tuned judge LLMs instead of generic LLMs for judging."
- **Why unresolved:** Current LLM judges (LLaMA3.3-70B, Gemma3-27B) assign systematically lower scores than human annotators.
- **What evidence would resolve it:** Comparison of multiple judging paradigms against expanded human evaluation on Telugu open-ended responses.

### Open Question 4
- **Question:** How would few-shot prompting affect model performance on MATA's linguistic reasoning and culturally-grounded categories?
- **Basis in paper:** [explicit] Limitations section states "We did not extensively conduct few-shot evaluations."
- **Why unresolved:** Zero-shot performance shows wide disparities; few-shot prompting could reveal whether examples help with culturally-specific reasoning or idiomatic understanding.
- **What evidence would resolve it:** Systematic few-shot experiments with 1, 3, and 5 examples per category, comparing performance gains across linguistic and cultural categories.

## Limitations
- MATA dataset not yet publicly available, preventing independent verification of question quality
- Human evaluation sample size and annotation protocol details are not fully specified
- LLM-judge reliability in Telugu lacks comparative benchmarks from other low-resource language studies

## Confidence

- Positional heuristic exploitation: High
- MCQ vs. open-ended gap reflects production difficulty: High
- LLM-judge reliability in low-resource languages: Medium

## Next Checks

1. Replicate the positional robustness test on a new model with at least 3 different position-fixing configurations to confirm heuristic sensitivity
2. Conduct human evaluation of 100+ open-ended responses across all 7 categories to establish ground truth error bounds for LLM-judge scoring
3. Test the dataset with a Telugu-native LLM (if available) to determine if cultural grounding affects performance differently than general-purpose models