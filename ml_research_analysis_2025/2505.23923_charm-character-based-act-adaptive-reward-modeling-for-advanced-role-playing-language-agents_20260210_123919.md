---
ver: rpa2
title: 'ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing
  Language Agents'
arxiv_id: '2505.23923'
source_url: https://arxiv.org/abs/2505.23923
tags:
- reward
- arxiv
- preprint
- preference
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AAM (Act-Adaptive Margin), a novel approach
  for reward modeling in subjective tasks like role-playing dialogues. The key innovation
  is dynamically adjusting the margin between preference pairs during training based
  on the model's own generative confidence, eliminating the need for manual margin
  annotations.
---

# ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents

## Quick Facts
- **arXiv ID:** 2505.23923
- **Source URL:** https://arxiv.org/abs/2505.23923
- **Reference count:** 20
- **Primary result:** AAM improves Bradley-Terry reward models by 2.95% on general tasks and 4.85% on subjective role-playing tasks, achieving SOTA performance on CharacterEval and Charm benchmarks.

## Executive Summary
This paper introduces Act-Adaptive Margin (AAM), a novel approach for reward modeling in subjective tasks like role-playing dialogues. The key innovation is dynamically adjusting the margin between preference pairs during training based on the model's own generative confidence, eliminating the need for manual margin annotations. The method has two variants: Probability-Ratio Adaptive Margin (PR) and Loss-Difference Adaptive Margin (LD). Experimental results show AAM improves Bradley-Terry reward models by 2.95% on general tasks and 4.85% on subjective role-playing tasks. When combined with GRPO, AAM-GRPO-32B achieves state-of-the-art performance on CharacterEval and Charm benchmarks, outperforming both closed-source models like Claude-3.5-Sonnet and specialized character models. The authors also introduce RoleplayPref, a large-scale preference dataset with 1,108 characters and 16,888 bilingual dialogues, along with dedicated evaluation benchmarks.

## Method Summary
AAM modifies the standard Bradley-Terry loss by introducing an adaptive margin M that varies per preference pair based on the model's confidence. The margin is computed in two ways: PR variant uses the log probability ratio between preferred and dispreferred responses relative to a reference model, while LD variant uses the difference in SFT losses between the two responses. This dynamic adjustment allows the model to learn more effectively from ambiguous preferences by applying smaller margins when confidence is low and larger margins when confidence is high. The method is trained on both general preference datasets and specialized role-playing data, then evaluated through both reward modeling benchmarks and downstream generation quality metrics.

## Key Results
- AAM improves Bradley-Terry reward models by 2.95% on general tasks and 4.85% on subjective role-playing tasks
- AAM-GRPO-32B achieves state-of-the-art performance on CharacterEval and Charm benchmarks
- Outperforms closed-source models like Claude-3.5-Sonnet and specialized character models
- Introduces RoleplayPref dataset with 1,108 characters and 16,888 bilingual dialogues

## Why This Works (Mechanism)
The method works by recognizing that subjective tasks like role-playing have inherently ambiguous preferences where the quality difference between responses may not be clear-cut. Traditional Bradley-Terry models apply fixed margins that can be too strict for uncertain comparisons or too lenient for clear-cut cases. By adapting the margin based on the model's own confidence (measured through probability ratios or loss differences), AAM allows the learning process to be more nuanced and context-aware. When the model is uncertain about a preference, it applies a smaller margin, preventing over-correction; when confident, it applies a larger margin, reinforcing clear preferences. This dynamic adjustment leads to better alignment with human judgment in subjective domains.

## Foundational Learning
- **Bradley-Terry preference modeling**: Models pairwise preferences as a probability distribution; needed for reward modeling from preference data; quick check: verify log σ(r(x,yw) - r(x,yl)) sums to 1 over preference pairs
- **Reward modeling with language models**: Uses language model to predict reward scores; needed for aligning generated text with human preferences; quick check: ensure reward predictions are bounded and differentiable
- **Adaptive loss functions**: Modifies loss based on data characteristics; needed to handle varying confidence levels in preference data; quick check: monitor margin distribution during training
- **GRPO (Group Relative Policy Optimization)**: Reinforcement learning method for language models; needed for fine-tuning based on reward model; quick check: verify KL divergence stays within bounds
- **Self-consistency in training**: Using current model predictions for margin computation; needed for dynamic adaptation; quick check: ensure training stability when using self-predictions

## Architecture Onboarding
- **Component map**: Input Dialogue -> SFT Model -> Preference Pairs (x, yw, yl) -> AAM Margin Computation -> Bradley-Terry Loss -> Reward Model
- **Critical path**: Preference pair generation → margin computation → loss calculation → reward model update
- **Design tradeoffs**: Adaptive margins vs. fixed margins (flexibility vs. stability); self-consistency vs. reference model (current knowledge vs. stable baseline)
- **Failure signatures**: Training instability (degenerate margin distribution), poor generalization (overfitting to specific preference patterns), reward collapse (margin too large or too small)
- **3 first experiments**:
  1. Train AAM-LD variant on Skywork-Reward-Preference-80K-v0.2 and compare to vanilla BT on RewardBench
  2. Test margin M distribution evolution during training for different α values
  3. Evaluate cross-dataset generalization by testing AAM-trained RMs on non-roleplaying subjective preferences

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on self-consistency in margin computation could lead to training instability if initial predictions are noisy
- Performance gains appear highly dependent on specific reward modeling architecture
- Scalability to larger models beyond 7B and long-term stability under diverse generation conditions remain unverified

## Confidence
- **High Confidence**: General framework of adaptive margins based on model confidence is theoretically sound
- **Medium Confidence**: Reported performance improvements on CharacterEval and Charm benchmarks
- **Low Confidence**: Scalability analysis to larger models and long-term stability verification

## Next Checks
1. **Margin Distribution Analysis**: Conduct ablation studies varying α and analyze how margin M distribution evolves during training
2. **Cross-Dataset Generalization**: Test AAM-trained reward models on out-of-domain preference pairs from different subjective task domains
3. **Reward Model Inversion Risk**: Evaluate whether AAM-GRPO models exhibit reward hacking by generating adversarial inputs that maximize reward scores while producing semantically incoherent outputs