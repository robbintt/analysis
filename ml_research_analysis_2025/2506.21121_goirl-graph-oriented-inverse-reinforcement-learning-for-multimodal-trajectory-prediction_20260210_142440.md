---
ver: rpa2
title: 'GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory
  Prediction'
arxiv_id: '2506.21121'
source_url: https://arxiv.org/abs/2506.21121
tags:
- trajectory
- prediction
- learning
- features
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents GoIRL, a graph-oriented inverse reinforcement
  learning framework for multimodal trajectory prediction in autonomous driving. It
  addresses the challenge of generating diverse and accurate future trajectories for
  surrounding agents in dynamic urban environments.
---

# GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction

## Quick Facts
- arXiv ID: 2506.21121
- Source URL: https://arxiv.org/abs/2506.21121
- Authors: Muleilan Pei; Shaoshuai Shi; Lu Zhang; Peiliang Li; Shaojie Shen
- Reference count: 33
- Primary result: Graph-oriented IRL framework achieving SOTA multimodal trajectory prediction with brier-minFDE6 of 1.695 (Argoverse) and 0.86 (nuScenes)

## Executive Summary
GoIRL introduces a graph-oriented inverse reinforcement learning framework for multimodal trajectory prediction in autonomous driving. It addresses the challenge of generating diverse and accurate future trajectories for surrounding agents in dynamic urban environments. The core innovation is integrating maximum entropy IRL with vectorized context representations through a novel feature adaptor that aggregates lane-graph features into grid space, enabling IRL-based predictors to leverage detailed geometric and semantic map information. Experiments on Argoverse and nuScenes benchmarks demonstrate state-of-the-art performance, achieving a brier-minFDE6 of 1.695 and 0.86 minADE5 respectively, while exhibiting superior generalization capabilities compared to supervised models when handling drivable area changes.

## Method Summary
GoIRL is a two-stage graph-oriented IRL framework for multimodal trajectory prediction. It first encodes vectorized map context (lane graph and drivable areas) using LaneGCN and PointNet, then projects these features to a discrete 2D grid via a feature adaptor. A MaxEnt IRL module learns a reward function through approximate value iteration, enabling probabilistic policy sampling. The system employs hierarchical trajectory generation: MCMC sampling of L=600 coarse plans, clustering to K modes (6 for Argoverse, 10 for nuScenes), and refinement with local context features to predict precise Bézier trajectories. The framework is trained end-to-end with a loss combining regression, classification, and goal prediction components.

## Key Results
- Achieves SOTA brier-minFDE6 of 1.695 on Argoverse validation set and 0.86 on nuScenes validation set
- Demonstrates superior generalization to drivable area changes compared to supervised baselines
- Shows 1.8280→1.7957 improvement in brier-minFDE with refinement module ablation
- Outperforms P2T baseline by leveraging vectorized map features through grid projection

## Why This Works (Mechanism)

### Mechanism 1: Graph-to-Grid Feature Adaptation
The Feature Adaptor maps high-fidelity vectorized features (lane segments, drivable areas) onto a discrete 2D grid, allowing the downstream MaxEnt IRL to leverage geometric precision of GNNs without modifying the IRL algorithm itself. This discretization retains sufficient geometric and semantic information to outperform raw rasterization.

### Mechanism 2: Reward-Driven Generalization (IRL vs. BC)
Unlike Behavior Cloning which memorizes State→Action mappings, IRL infers a Reward Function R(s) that implicitly updates costs when environments change. This allows re-planning feasible trajectories via the optimal policy π* even for road closures never seen during training, addressing covariate shift.

### Mechanism 3: Hierarchical Proposal & Refinement
The framework separates coarse policy sampling (MCMC from IRL policy) from fine trajectory refinement. This oversamples L=600 plans, clusters them to identify K distinct modes, then refines coarse Bézier proposals with fine-grained local context features for precise offsets and improved confidence calibration.

## Foundational Learning

- **Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL)**: The mathematical engine allowing GoIRL to generate a distribution of diverse, plausible futures rather than a single average path. It recovers reward functions from expert data assuming probabilistic expert behavior.
  - Quick check: How does the "partition function Z" in the probability distribution P(τ) = (1/Z)exp(R(τ)) relate to the model's ability to produce multimodal outputs?

- **Bézier Curves**: Used to parameterize trajectory proposals, ensuring mathematically smooth and continuous paths by definition, avoiding jagged noise from coordinate-wise regression.
  - Quick check: Why are control points (p₀…pₙ) easier for a recurrent decoder to predict than raw (x,y) waypoints for ensuring temporal consistency?

- **Covariate Shift**: The core motivation for the paper—in supervised learning, errors compound when test inputs differ from training (e.g., construction zones). Understanding this failure mode explains why the authors propose the more complex IRL architecture.
  - Quick check: In Figure 1, why does the supervised model fail to predict the "turn left" trajectory even though it is geometrically valid?

## Architecture Onboarding

- **Component map**: Graph Encoder (LaneGCN + PointNet) → Feature Adaptor → IRL Core (Value Iteration) → Sampler (MCMC) → Trajectory Decoder (BiGRU + Attention) → Refinement Module
- **Critical path**: The Feature Adaptor—this is the bridge where vector information is condensed into grid structure. If the alignment logic (mapping nearest drivable nodes to grid cells) is flawed, the IRL will receive a corrupted state space.
- **Design tradeoffs**: Grid Resolution (25×25 vs 50×50 tradeoff between detail and computational cost), Oversampling (L=600 vs efficiency), State-space complexity vs IRL stability
- **Failure signatures**: "Mode Collapse" in IRL (identical paths from sampling), Static Predictions (zero-padding issues causing straight-line predictions), Refinement Module ineffectiveness
- **First 3 experiments**:
  1. Drivable Area Ablation: Manually erase a drivable area in validation set and verify probability mass shifts away from that trajectory
  2. Resolution Sensitivity: Run with feature adaptor at 50×50 instead of 25×25 to check if minADE improves or IRL training diverges
  3. Sampling Efficiency: Reduce L from 600 to 100 to determine if minFDE degrades significantly, indicating IRL policy sparsity

## Open Questions the Paper Calls Out
- **Multi-agent extension**: Future work will concentrate on extending the IRL paradigm to encompass joint multi-agent trajectory forecasting
- **Quantitative OOD validation**: The paper lacks systematic quantitative ablation across diverse distribution shifts for drivable area changes
- **Sudden policy shifts**: The model fails to anticipate maneuvers without prior indicators, lacking mechanisms to infer latent intentions

## Limitations
- Feature Adaptor implementation details are underspecified (exact alignment strategy, resolution tradeoffs)
- MaxEnt IRL training procedure relies on approximate value iteration with unspecified hyperparameters
- Sampling strategy (MCMC with L=600) is computationally expensive with unproven efficiency versus simpler diversification methods

## Confidence
- **High Confidence**: Strong quantitative results on Argoverse and nuScenes benchmarks with SOTA brier-minFDE6 scores
- **Medium Confidence**: IRL generalization claims supported by qualitative examples but lacking systematic quantitative validation
- **Low Confidence**: Feature Adaptor's exact implementation and impact on geometric precision are difficult to verify

## Next Checks
1. **Resolution Sensitivity**: Run with feature adaptor at 50×50 instead of 25×25; measure changes in minADE and minFDE
2. **Covariate Shift Robustness**: Systematically occlude drivable areas in validation set (10%, 20%, 30%) and quantify performance drop for GoIRL vs supervised baseline
3. **Sampling Efficiency**: Reduce MCMC oversampling parameter L from 600 to 100; evaluate if minFDE degrades significantly