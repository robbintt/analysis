---
ver: rpa2
title: AI Text Detectors and the Misclassification of Slightly Polished Arabic Text
arxiv_id: '2511.16690'
source_url: https://arxiv.org/abs/2511.16690
tags:
- articles
- polishing
- llms
- text
- polished
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates Arabic AI detectors\u2019 robustness when\
  \ human-authored text is slightly polished by LLMs. Two datasets were built: one\
  \ with 800 articles (half human, half AI-generated) and Ar-APT with 16,400 samples\
  \ of human text polished by 10 LLMs at 4 levels."
---

# AI Text Detectors and the Misclassification of Slightly Polished Arabic Text

## Quick Facts
- arXiv ID: 2511.16690
- Source URL: https://arxiv.org/abs/2511.16690
- Reference count: 20
- Primary result: All tested AI detectors misclassify significant portions of slightly polished human text as AI-generated, with performance dropping sharply even at 10% polishing levels

## Executive Summary
This study evaluates the robustness of Arabic AI text detectors when human-authored text undergoes slight AI polishing. The authors created two datasets: one with 800 articles (400 human, 400 AI-generated) and Ar-APT with 16,400 samples of human text polished by 10 different LLMs at four polishing levels (10%, 25%, 50%, 75%). Among 14 models tested, Claude-4 Sonnet achieved the highest accuracy at 83.51% with 16.49% FPR, while Originality.AI achieved 96% accuracy but dropped to 12% when detecting text polished by non-GPT models. The results demonstrate that all detectors are highly vulnerable to even minor polishing, misclassifying polished human text as AI-generated at alarming rates.

## Method Summary
The authors constructed two datasets: Dataset 1 contained 800 articles (400 human-authored, 400 AI-generated) evaluated across 14 detectors; Dataset 2 (Ar-APT) contained 16,400 samples of human text polished by 10 different LLMs at 10%, 25%, 50%, and 75% levels. They used cosine similarity and Jaccard similarity to verify semantic preservation during polishing. Detectors tested included 10 LLM-based approaches (via prompting) and 4 commercial tools. The evaluation measured both accuracy and false positive rate across all polishing conditions.

## Key Results
- All 14 tested detectors incorrectly classified significant portions of slightly polished human text as AI-generated
- Claude-4 Sonnet achieved 83.51% accuracy with 16.49% FPR on the primary dataset
- Originality.AI achieved 96% accuracy but dropped to 12% accuracy and 88% FPR on 10% polished text from Mistral/Gemma-3 models
- Performance degraded sharply even at 10% polishing, with Claude-4 Sonnet falling to 57.63% accuracy on LLaMA-3-polished articles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Slight AI polishing introduces linguistic features that overlap with AI-generated text signatures, causing detectors to shift their decision boundary toward "AI-generated" classifications.
- **Mechanism:** LLMs apply consistent stylistic patterns during polishing. Detectors trained to recognize these patterns as "AI signatures" cannot distinguish between "AI wrote this" and "AI edited this slightly."
- **Core assumption:** AI text detectors rely on statistical and stylistic features that are invariant between AI-generation and AI-polishing tasks.
- **Evidence anchors:**
  - [abstract] "all AI detectors incorrectly attribute a significant number of articles to AI... Claude-4 Sonnet's performance decreased to 57.63% for articles slightly polished by LLaMA-3"
  - [section 4.3.4] "Originality.AI accuracy has dropped to 12% and the false positive rate jumped to 88%" on 10% polished text
  - [corpus] Saha & Feizi (2025) "Almost AI, Almost Human" demonstrates similar vulnerability in English, confirming cross-linguistic generalization of this mechanism
- **Break condition:** If detectors incorporated provenance metadata rather than relying solely on surface features, this mechanism would weaken.

### Mechanism 2
- **Claim:** Commercial detectors overfit to specific LLM signatures (particularly GPT-family models), causing catastrophic failure when encountering polishing from less-common models.
- **Mechanism:** Commercial tools optimize for detecting outputs from widely-deployed models (GPT-4o, LLaMA). When polishing originates from Mistral or Gemma-3, which have different tokenization and generation patterns, the detector's learned signatures fail to transfer.
- **Core assumption:** Training data for commercial detectors is biased toward high-traffic LLM outputs.
- **Evidence anchors:**
  - [section 4.3.4] "Originality.AI tries to demonstrate its abilities by training on large amounts of data collected from such a model [GPT-4o]"
  - [table 6] Originality.AI maintains 90% accuracy on Claude-4-polished text but drops to 12% on Mistral/Gemma-3-polished text at 10% polishing
  - [corpus] PADBen benchmark shows detectors fail catastrophically against paraphrase attacks, suggesting similar overfitting patterns
- **Break condition:** If detector training incorporated adversarial examples from diverse LLM families with equal weighting, this mechanism would be mitigated.

### Mechanism 3
- **Claim:** Arabic's morphological complexity amplifies detector confusion because small surface changes can disproportionately affect token-level statistics that detectors rely on.
- **Core assumption:** Arabic morphological variants create denser token spaces, making statistical detection thresholds more sensitive to perturbations.
- **Evidence anchors:**
  - [section 1] "Arabic language has a richer morphology and a variety of dialects. Moreover, the use of similar words with different diacritics may lead to a different meaning"
  - [section 4.3.2] All LLMs except Qwen-3 achieved >92% cosine similarity, confirming polishing preserved meaning while altering surface features
  - [corpus] Limited direct corpus evidence on Arabic-specific morphology effects; this mechanism remains inferred from paper's language-specific claims
- **Break condition:** If detectors operated on morpheme-level rather than token-level representations, this amplification effect would likely diminish.

## Foundational Learning

- **Concept: False Positive Rate (FPR) vs. Accuracy trade-off**
  - **Why needed here:** The paper emphasizes FPR as the critical metric because falsely accusing human authors of AI plagiarism is more harmful than missing some AI-generated content.
  - **Quick check question:** If a detector achieves 95% accuracy but has 40% FPR, would you deploy it in an academic integrity system? Why or why not?

- **Concept: Cosine similarity and Jaccard similarity for semantic preservation**
  - **Why needed here:** The paper uses these metrics to verify that polishing changed surface features without altering meaning (>92% cosine similarity indicates semantic preservation).
  - **Quick check question:** Two sentences have 96% cosine similarity but 70% Jaccard similarity. What does this tell you about the relationship between word choice and semantic content?

- **Concept: Decision boundary sensitivity in binary classifiers**
  - **Why needed here:** The core finding is that slight polishing shifts text across the detector's human/AI decision boundary, even when meaning is preserved.
  - **Quick check question:** If you add a small perturbation to a data point near a classifier's decision boundary, would you expect its classification to change more or less than a point far from the boundary?

## Architecture Onboarding

- **Component map:**
  Human-authored text → [Polishing LLM + percentage setting] → Polished text → [Detector LLMs x 10 + Commercial tools x 4] → [Accuracy + FPR evaluation]

- **Critical path:** The experiment hinges on the polishing pipeline producing meaningful changes (verified via Jaccard similarity) while preserving semantics (verified via cosine similarity). If polishing either changes nothing or alters meaning, the detector evaluation becomes invalid.

- **Design tradeoffs:**
  - Using LLMs as detectors (via prompting) vs. specialized trained detectors: LLMs are accessible but not optimized for detection; commercial tools claim high performance but lack transparency
  - Testing 100 vs. 400 articles for commercial detectors: Manual API limitations forced smaller sample size, reducing statistical confidence
  - Four polishing levels (10%, 25%, 50%, 75%) vs. finer granularity: Reduces sample count per condition but improves cross-model coverage

- **Failure signatures:**
  - High FPR on unpolished human text indicates detector is too aggressive (LLaMA-3 70B: 89% FPR)
  - Large accuracy drop between 0% and 10% polishing indicates sensitivity to minor perturbations (Claude-4 Sonnet: ~18% drop)
  - Near-zero accuracy on polished text indicates complete failure mode (Originality.AI on Mistral/Gemma-3: 12%)

- **First 3 experiments:**
  1. Replicate the polishing pipeline on a small sample (10 articles), manually verify that 10% polishing produces visible but semantic-preserving changes using the paper's prompt template
  2. Test a single LLM detector (GPT-4o) on your polished samples to confirm the FPR increase pattern matches the paper's reported ~5% degradation at 10% polishing
  3. Compare detector behavior on Arabic vs. English polished text from the same source articles to test whether morphological complexity actually amplifies the effect (mechanism validation)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Arabic AI detection architectures be modified to distinguish between fully AI-generated text and human-authored text that has undergone minor AI polishing without triggering high false positive rates?
- Basis in paper: [explicit] The conclusion states there is an "urgent need for Arabic AI detectors that resist misclassification of slightly polished human-authored content."
- Why unresolved: Current models treat the linguistic features of polished text as evidence of full AI generation, lacking a mechanism to differentiate the degree of AI involvement.
- What evidence would resolve it: A new model or fine-tuning approach that maintains high accuracy on Ar-APT while keeping the False Positive Rate (FPR) below a defined threshold (e.g., <5%).

### Open Question 2
- Question: Can partial fine-tuning of models like GPT-4o successfully create a decision boundary that tolerates slight AI polishing while still identifying fully synthetic Arabic content?
- Basis in paper: [explicit] The authors suggest that "Future researchers may utilize this functionality to partially fine-tune GPT-4o and make it differentiate between AI-generated and slightly polished human-authored articles."
- Why unresolved: The paper demonstrates the failure of off-the-shelf models but does not implement the proposed fine-tuning solution to test if it resolves the misclassification issue.
- What evidence would resolve it: Comparative benchmarks showing a fine-tuned detector outperforming the baseline models on the Ar-APT dataset.

### Open Question 3
- Question: What specific linguistic or structural features in LLaMA-3 70B's polishing output cause disproportionate performance drops in detectors compared to other polishing models like GPT-4o?
- Basis in paper: [inferred] The results show polishing by LLaMA-3 70B leads to the highest accuracy decrease, but the paper does not analyze the specific stylistic or morphological changes causing this vulnerability.
- Why unresolved: The study quantifies the performance drop but does not conduct a linguistic error analysis to explain why LLaMA-3 70B is more effective at evading detection than other models.
- What evidence would resolve it: A feature importance analysis or linguistic study correlating specific Arabic morphological changes introduced by LLaMA-3 70B with detector uncertainty.

## Limitations

- The study focuses exclusively on Arabic text, limiting generalizability to other languages
- Commercial detectors' black-box nature prevents understanding whether failures stem from training data bias or architectural limitations
- Only 10 LLM models were used for polishing, potentially missing detector vulnerabilities to other model families

## Confidence

- **High confidence:** The empirical finding that all tested detectors show increased FPR on slightly polished text
- **Medium confidence:** The claim that commercial detectors are specifically vulnerable to non-GPT-family polishing
- **Low confidence:** The assertion that Arabic's morphological complexity is a primary driver of detector confusion

## Next Checks

1. Test the same polishing pipeline on English human-authored text to determine whether the observed detector vulnerabilities are language-specific or universal
2. Conduct ablation studies where polishing is applied at the morpheme level rather than token level to test whether Arabic's morphological complexity amplifies the effect
3. Evaluate detector performance on polished text from additional LLM families not included in the original study (e.g., DeepSeek, Gemini) to test the generalizability of the non-GPT-family vulnerability finding