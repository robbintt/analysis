---
ver: rpa2
title: Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness
arxiv_id: '2505.17406'
source_url: https://arxiv.org/abs/2505.17406
tags:
- reasoning
- answer
- arxiv
- joke
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATCHA, a novel evaluation framework for
  assessing the robustness of Chain-of-Thought (CoT) reasoning in large language models
  (LLMs). The key insight is that LLMs can maintain correct answers while producing
  inconsistent or nonsensical reasoning under imperceptible input perturbations.
---

# Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness

## Quick Facts
- arXiv ID: 2505.17406
- Source URL: https://arxiv.org/abs/2505.17406
- Reference count: 40
- Key outcome: MATCHA framework reveals that LLMs can maintain correct answers while producing nonsensical reasoning under imperceptible input perturbations, exposing a critical robustness gap.

## Executive Summary
This paper introduces MATCHA, a novel evaluation framework for assessing the robustness of Chain-of-Thought (CoT) reasoning in large language models (LLMs). The key insight is that LLMs can maintain correct answers while producing inconsistent or nonsensical reasoning under imperceptible input perturbations. MATCHA achieves this by conditioning rationale generation on the model's predicted answer, allowing isolation and stress-testing of the reasoning process. The framework employs both token-level and embedding-level perturbations to maximize reasoning divergence while preserving answer correctness. Experiments across three datasets (GSM8K, SingleEQ, StrategyQA) with five open-source models reveal significant vulnerabilities: MATCHA successfully induces reasoning errors in 14.7% of GSM8K examples while maintaining correct answers. Multi-step and commonsense tasks show greater susceptibility than logical tasks. The perturbations also transfer non-trivially to black-box models like GPT-4. These findings expose the decoupling between answer accuracy and reasoning robustness, highlighting the need for architectures that enforce genuine answer-reasoning consistency rather than surface-level correctness.

## Method Summary
MATCHA evaluates LLM CoT robustness by generating adversarial perturbations that corrupt reasoning while preserving answer correctness. The framework first predicts an answer, then generates the CoT conditioned on that answer, and finally applies token-level or embedding-level perturbations to disrupt the reasoning. Token-level attacks insert random tokens and use gradient-guided replacement to maximize reasoning loss while minimizing answer loss. Embedding-level attacks apply imperceptible L∞ perturbations to input embeddings. An LLM judge evaluates whether reasoning is correct or incorrect post-perturbation. The framework reports Success Rate (SR) as the percentage of correctly answered questions where reasoning becomes incorrect, and Unattackable Rate (UR) as the percentage where reasoning remains correct.

## Key Results
- MATCHA successfully induces reasoning errors in 14.7% of GSM8K examples while maintaining correct answers.
- Multi-step and commonsense tasks (GSM8K, StrategyQA) show greater susceptibility to reasoning perturbations than logical tasks (SingleEQ).
- MATCHA perturbations transfer non-trivially to black-box models like GPT-4, though transferability varies by dataset.
- Higher-performing models tend to have lower Unattackable Rates, suggesting a trade-off between accuracy and reasoning robustness.

## Why This Works (Mechanism)

### Mechanism 1: Answer-Conditioned Probing (Decoupling)
- **Claim:** If rationale generation is conditioned on the model's own predicted answer, the causal dependency between reasoning and the final output is broken, exposing "post-hoc rationalization."
- **Mechanism:** The framework fixes the answer token sequence $y_p$ before generating the Chain-of-Thought (CoT). By optimizing perturbations to disrupt the CoT ($L_c$) while preserving the answer ($L_a$), the system reveals if the model is "right for the wrong reasons."
- **Core assumption:** The model treats the provided answer as a hard constraint or premise when generating the subsequent rationale, allowing the rationale to be manipulated independently of the answer's probability.
- **Evidence anchors:**
  - [abstract] "MATCHA isolates the reasoning phase by conditioning generation on the model's predicted answer."
  - [section 3] "By anchoring the model's predicted answer first, we explicitly decouple decision-making from rationalization."
- **Break condition:** If the model's internal representation of the answer and reasoning are inextricably linked (causally bound), minimizing $L_a$ would necessarily minimize $L_c$, preventing divergence.

### Mechanism 2: Gradient-Informed Importance Ranking
- **Claim:** Calculating gradients with respect to one-hot token indicators identifies specific tokens that maximize reasoning divergence while minimizing answer shift.
- **Mechanism:** The method inserts random tokens and then uses $\nabla_{ex} (-L_{opt})$ to score the influence of vocabulary items at those positions. It selects replacements that maximize the objective $L_{opt} = L_c - \lambda L_a$.
- **Core assumption:** The loss landscape allows for local perturbations that affect the probability distribution of the reasoning tokens differently than the answer tokens.
- **Evidence anchors:**
  - [section 4.2] "We compute the gradient at each one-hot token indicator... to measure the influence of each token on the reasoning/answer."
  - [corpus] Related work (Beware of Reasoning Overconfidence) supports the fragility of reasoning paths under specific pressure, though MATCHA specifically operationalizes this via gradient optimization.
- **Break condition:** If the model is robust, the gradient directions for maximizing reasoning loss will coincide with those for maximizing answer loss, causing the answer to flip before reasoning fails.

### Mechanism 3: Embedding-Space Perturbation
- **Claim:** Imperceptible $l_\infty$ perturbations in the continuous embedding space can shift internal representations enough to derail logical steps without altering the discrete output mapping of the final answer.
- **Mechanism:** The algorithm updates input embeddings $E_c$ using $E_{tmp} \leftarrow E'_c + \alpha \cdot \epsilon \cdot \text{sign}(\nabla L_{opt})$, clamping the result to stay within an $\epsilon$-ball. This creates a "semantic drift" in the latent space.
- **Core assumption:** The decision boundary for the final answer is wider or more robust than the manifold governing the specific syntactic structure of the intermediate reasoning steps.
- **Evidence anchors:**
  - [section 4.3] "Perturb the embedding space... ensuring that the token sequence remains unchanged while disrupting the reasoning process."
  - [abstract] "MATCHA achieves this by... embedding-level perturbations to maximize reasoning divergence while preserving answer correctness."

## Foundational Learning

- **Concept: Adversarial Loss Construction**
  - **Why needed here:** To understand how MATCHA balances two competing objectives: destroying reasoning ($\max L_c$) vs. saving the answer ($\min L_a$).
  - **Quick check question:** If $\lambda$ in the loss function $L_{opt} = L_c - \lambda L_a$ is set too low, what is the likely failure mode? (Answer: The attack succeeds in changing reasoning but likely flips the answer as well, failing the "correct answer" constraint).

- **Concept: In-Context Learning (ICL) Dynamics**
  - **Why needed here:** The attack operates within an ICL setting where examples condition the model. Understanding how the model uses the "Answer: [A]" prefix as part of the context is crucial.
  - **Quick check question:** Why does the framework generate the answer $y_p$ before the reasoning $r_p$ in the prompt structure? (Answer: To simulate a scenario where the decision is fixed, and the model must rationalize it, enabling the isolation of reasoning robustness).

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** Evaluating "reasoning correctness" is subjective and cannot be done via simple string matching. The framework relies on a judge (e.g., GPT-3.5) to label reasoning as consistent or inconsistent.
  - **Quick check question:** What is the primary risk of using an LLM judge in this evaluation loop? (Answer: The judge may have its own biases or inconsistencies, potentially misclassifying valid but complex reasoning as "wrong").

## Architecture Onboarding

- **Component map:** Data Loader -> Reference Generator -> Perturbation Engine (Token-Level: Insertion Module -> Gradient Oracle -> Semantic Filter) -> Perturbation Engine (Embedding-Level) -> Evaluator
- **Critical path:** The gradient calculation for $L_{opt}$ (Eq. 3 and 4). This determines which tokens or embedding directions effectively "decouple" the reasoning from the answer.
- **Design tradeoffs:**
  - **Token vs. Embedding:** Token-level attacks are more realistic (human-readable) but harder to optimize due to discrete space. Embedding-level attacks are more potent but require white-box access to weights and produce non-readable inputs (invisible to text logs).
  - **Step Count ($j$):** More steps increase success rate but linearly increase compute cost and risk of semantic drift in the question.
- **Failure signatures:**
  - **High Wrong Rate (WR):** The perturbation is too aggressive; the answer flips from correct to incorrect. Increase $\lambda$ or decrease $\epsilon$.
  - **High Unattackable Rate (UR):** The model is robust, or the perturbation strategy is too weak (e.g., low token insertion ratio or bad gradient approximation).
  - **Semantic Drift:** Token replacements change the meaning of the math problem (e.g., changing "3 apples" to "30 apples"). The Semantic Filter (Judge) exists to prevent this, but may fail.
- **First 3 experiments:**
  1. **Baseline Verification:** Run Random vs. MATCHA on a small subset (e.g., 100 GSM8K examples) to confirm that gradient-informed replacement outperforms random insertion (verify Table 3).
  2. **Hyperparameter Sweep:** Tune the inserted token ratio $a$ (e.g., 0.1 to 0.4) to find the "sweet spot" where Success Rate (SR) is maximized before Semantic Drift invalidates the question (verify Fig 4b).
  3. **Transferability Check:** Take successful adversarial examples generated on Llama-3-8B and test them zero-shot on GPT-3.5-turbo to verify the transferability claim (verify Fig 2).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can model architectures or training objectives be redesigned to enforce genuine causal links between reasoning and answers, mitigating the accuracy-robustness trade-off?
- **Basis in paper:** [explicit] The conclusion states the need for architectures that "enforce genuine answer-reasoning consistency rather than mere surface-level accuracy," while Section 5.3 observes that high-accuracy models often exhibit lower reasoning robustness (Unattackable Rate).
- **Why unresolved:** The paper establishes the existence of the trade-off but does not propose specific mechanisms to resolve it.
- **What evidence would resolve it:** A training paradigm (e.g., process-supervised reinforcement learning) that maintains high accuracy while significantly increasing the Unattackable Rate (UR) under MATCHA perturbations.

### Open Question 2
- **Question:** What specific structural or semantic features of the generated adversarial examples drive their transferability to black-box models like GPT-4?
- **Basis in paper:** [inferred] Section 5.4 demonstrates non-trivial transfer rates but notes that transferability varies unpredictably between GPT-3.5 and GPT-4 depending on the dataset (e.g., StrategyQA transfers less to GPT-4).
- **Why unresolved:** The paper identifies that transferability exists but does not isolate the specific characteristics of the perturbations that exploit shared vulnerabilities across different model families.
- **What evidence would resolve it:** An ablation study identifying which token-level or embedding-level features correlate with high transfer success rates between distinct architectures (e.g., Llama-3 and GPT-4).

### Open Question 3
- **Question:** Does the susceptibility of multi-step reasoning tasks hold across formal logic domains outside of arithmetic and commonsense benchmarks?
- **Basis in paper:** [inferred] Section 5.3 concludes that multi-step and commonsense tasks are more susceptible than logical tasks, but the study is limited to GSM8K, SingleEQ, and StrategyQA.
- **Why unresolved:** It remains unclear if the "Decoupling Hypothesis" applies equally to formal logic (e.g., first-order logic proofs) or if it is an artifact of the specific "logical" dataset used (SingleEQ).
- **What evidence would resolve it:** Application of the MATCHA framework to formal logic datasets (e.g., LogiQA or FOLIO) to compare reasoning fragility against the multi-step math results.

## Limitations

- The framework's core claim depends heavily on the reliability of the LLM judge, which introduces subjective bias and potential misclassification of reasoning errors.
- The gradient computation method and exact ICL prompt templates are unspecified, creating potential reproducibility gaps.
- Results are primarily reported on 7B-parameter models, limiting generalizability to larger or differently architected models.
- The framework assumes reasoning consistency is binary (correct/incorrect), which may oversimplify the continuous nature of logical coherence.

## Confidence

**High Confidence:**
- The methodology for answer-conditioned probing and adversarial perturbation is technically sound and clearly described.
- The experimental results showing successful reasoning corruption while preserving answers (14.7% SR on GSM8K) are reproducible given the provided parameters.

**Medium Confidence:**
- The claim that multi-step and commonsense tasks are more vulnerable than logical tasks is supported by the data but requires more granular error analysis to confirm causality.
- The transferability of perturbations to GPT-4 is demonstrated but lacks statistical significance testing across multiple trials.

**Low Confidence:**
- The framework's ability to distinguish "genuine" reasoning errors from superficial pattern deviations depends heavily on the judge's calibration, which isn't fully characterized.
- The semantic preservation filter's effectiveness is asserted but not empirically validated with human annotations.

## Next Checks

1. **Judge Reliability Audit**: Manually validate 100+ perturbed samples across all three datasets using human annotators to establish ground-truth reasoning correctness rates and compare against LLM judge outputs. Calculate inter-annotator agreement and identify systematic judge biases.

2. **Cross-Model Stress Test**: Apply MATCHA perturbations to a broader range of model scales (1B, 13B, 34B parameters) to determine if observed reasoning fragility is size-dependent or architecture-agnostic. Report success rates stratified by model capacity.

3. **Semantic Drift Quantification**: Implement a controlled experiment measuring how perturbation strength (insertion ratio, embedding ε) correlates with semantic drift rates across datasets. Define and track drift using both LLM judges and embedding distance metrics to establish safe operational bounds.