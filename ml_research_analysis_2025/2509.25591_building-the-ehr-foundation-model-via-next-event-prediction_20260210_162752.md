---
ver: rpa2
title: Building the EHR Foundation Model via Next Event Prediction
arxiv_id: '2509.25591'
source_url: https://arxiv.org/abs/2509.25591
tags:
- clinical
- prediction
- event
- temporal
- next
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Next Event Prediction (NEP), a framework
  that enhances Large Language Models' (LLMs) temporal reasoning for Electronic Health
  Records (EHRs) by fine-tuning them to predict future clinical events in patient
  sequences. Unlike existing methods that treat EHR data as static or summarize at
  visit levels, NEP explicitly models disease progression patterns and causal relationships
  through autoregressive training on timestamped event chains.
---

# Building the EHR Foundation Model via Next Event Prediction

## Quick Facts
- arXiv ID: 2509.25591
- Source URL: https://arxiv.org/abs/2509.25591
- Authors: Zekai Chen; Arda Pekis; Kevin Brown
- Reference count: 12
- One-line primary result: NEP achieves 4.6% higher AUROC than specialized EHR models and 7.2% higher C-index than general-purpose LLMs in temporal reasoning tasks

## Executive Summary
This paper introduces Next Event Prediction (NEP), a framework that enhances Large Language Models' (LLMs) temporal reasoning for Electronic Health Records (EHRs) by fine-tuning them to predict future clinical events in patient sequences. Unlike existing methods that treat EHR data as static or summarize at visit levels, NEP explicitly models disease progression patterns and causal relationships through autoregressive training on timestamped event chains. Evaluations across oncology survival prediction and clinical diagnosis tasks demonstrate NEP's superiority, achieving 4.6% higher AUROC than specialized EHR models and 7.2% higher C-index than general-purpose LLMs in temporal reasoning tasks. The approach also produces clinically interpretable attention patterns aligned with known disease pathways.

## Method Summary
NEP fine-tunes decoder-only LLMs (Llama-3.1, Qwen2.5) using LoRA on proprietary EHR data (1.2M patients, 200M events) to predict next clinical events through autoregressive training with causal attention masking. Events are serialized with timestamps and instruction-formatted for SFT, with temperature-controlled multinomial sampling (α=0.5) to balance rare and frequent events. The model uses sliding window context (4096 tokens) and extracts mean-pooled embeddings from final hidden states for downstream classification tasks like oncology survival prediction and clinical diagnosis.

## Key Results
- NEP achieves 4.6% higher AUROC than specialized EHR models on clinical diagnosis tasks
- NEP achieves 7.2% higher C-index than general-purpose LLMs on oncology survival prediction
- Temperature-controlled sampling (α=0.5) effectively balances learning across rare and frequent clinical events

## Why This Works (Mechanism)

### Mechanism 1: Causal Attention Masking
Causal (unidirectional) attention masking forces the model to learn temporal progression by preventing access to future events during training. At each position t, the model can only attend to events ≤t, requiring it to infer what happens next based solely on historical context. This mirrors how clinicians reason prospectively. Core assumption: Temporal dependencies in EHR are learnable through sequential prediction; past events contain sufficient signal for future event anticipation.

### Mechanism 2: Event-Level Granularity
Event-level granularity preserves temporal dynamics that visit-level aggregation destroys. Each clinical event (diagnosis, procedure, medication, lab) is encoded with its timestamp as a discrete token, preserving ordering and time intervals. The model learns fine-grained transitions rather than coarse visit summaries. Core assumption: Clinical trajectories exhibit meaningful micro-temporal patterns at the event level, not just visit level.

### Mechanism 3: Temperature-Controlled Sampling
Temperature-controlled sampling prevents high-frequency events from dominating training, enabling balanced learning across event types. Using α=0.5 in multinomial sampling downsamples frequent events (routine vitals, common labs) relative to rare but informative events, ensuring the model learns diverse temporal patterns. Core assumption: Rare events carry disproportionate predictive value; uniform learning across types improves generalization.

## Foundational Learning

- **Concept: Autoregressive Language Modeling**
  - Why needed here: NEP frames EHR prediction as next-token prediction; understanding how LLMs learn sequential dependencies is prerequisite.
  - Quick check question: Can you explain why predicting "next token" differs mathematically from predicting "any future token"?

- **Concept: Causal vs Bidirectional Attention**
  - Why needed here: Section 3.2 explicitly contrasts NEP's causal masking with BERT-style bidirectional attention; this design choice determines what temporal patterns are learnable.
  - Quick check question: Given a 5-event sequence [A, B, C, D, E], which events can position 3 attend to under causal masking vs bidirectional?

- **Concept: Instruction Tuning (SFT)**
  - Why needed here: NEP uses instruction-following format rather than raw text continuation; this affects how the model learns to "reason" about predictions.
  - Quick check question: What's the difference between fine-tuning on "continue this text" vs "given this history, predict the next event"?

## Architecture Onboarding

- **Component map:** EHR Raw Data → Event Serialization → Instruction Formatting → LLM + LoRA → Autoregressive Next-Event Prediction → Frozen LLM → Mean-Pooled Embeddings → Logistic Regression Classifier → Downstream Predictions

- **Critical path:**
  1. Serialization format determines what temporal information the model sees
  2. Temperature sampling (α=0.5) controls training distribution
  3. LoRA fine-tuning enables efficient adaptation without full parameter updates
  4. Embedding extraction method (mean pooling, final layer) affects downstream quality

- **Design tradeoffs:**
  - **Causal vs Bidirectional:** Causal enables next-event prediction but loses bidirectional context for representation learning. Paper claims complementarity with existing bidirectional encoders (Section 3.2).
  - **Model scale:** Table 2 shows 8B model substantially outperforms 1B/3B on rare diseases (celiac: 0.59 vs 0.49), suggesting scale matters for pattern diversity.
  - **Training data scope:** Oncology-heavy training limits generalization to non-cancer workflows; authors acknowledge this and restrict EHRSHOT evaluation to diagnosis tasks.

- **Failure signatures:**
  - Model overfits to high-frequency events despite sampling → check loss curves per event type
  - Poor performance on short sequences → may need minimum history threshold
  - Stage IV colorectal cancer underperformance (Table 1) suggests domain gaps that temporal modeling alone cannot bridge
  - Embeddings fail on tasks outside oncology spectrum → training data distribution mismatch

- **First 3 experiments:**
  1. **Validate serialization format:** Compare tokenized event sequences against raw visit-level summaries on a held-out trajectory reconstruction task. Confirms granularity matters.
  2. **Ablate temperature parameter:** Train with α ∈ {0.0, 0.5, 1.0} and evaluate on rare-event-heavy tasks (celiac, lupus). Establishes sampling sensitivity.
  3. **Probe embedding quality:** Extract embeddings at different layers and pooling strategies; evaluate on linear probe tasks. Identifies optimal extraction configuration before downstream deployment.

## Open Questions the Paper Calls Out

### Open Question 1
Can NEP maintain its performance when applied to general hospital workflows outside of oncology?
- Basis in paper: The authors acknowledge their training data is "heavy oncology" and excluded general hospital procedures from the EHRSHOT evaluation because the model "never observed non-oncology clinical workflows during training."
- Why unresolved: The model's characterization as a "Foundation Model" is currently constrained by the domain specificity of its training corpus, limiting its validation to cancer-related trajectories.
- What evidence would resolve it: Evaluation of the NEP-finetuned model on diverse, non-oncology benchmarks (e.g., general MIMIC-IV tasks) without further domain-specific pre-training.

### Open Question 2
How can NEP models be updated to handle novel clinical concepts without losing previously learned temporal patterns?
- Basis in paper: The Ethical Considerations section states that "Evolving medical practices require models to handle new codes/treatments" and that "sustained utility will require continual learning protocols... without catastrophic forgetting."
- Why unresolved: The current study utilizes static datasets and does not evaluate the model's ability to adapt to shifting medical vocabularies or treatment paradigms over time.
- What evidence would resolve it: Experiments demonstrating stable performance on original tasks after sequential fine-tuning on data containing newly introduced medical codes or updated treatment guidelines.

### Open Question 3
What is the most effective method for integrating unstructured clinical data (e.g., notes, imaging) into the timestamped event chain?
- Basis in paper: The Conclusion identifies "multimodal integration" as a primary target for "future extensions," noting the current focus on structured event sequences.
- Why unresolved: While the framework successfully models structured codes, it has not yet demonstrated how to temporally align and reason over high-dimensional unstructured inputs within the Next Event Prediction objective.
- What evidence would resolve it: A unified NEP implementation that processes both structured codes and unstructured notes/images, showing improved AUROC on tasks heavily dependent on narrative clinical data.

## Limitations

- Proprietary 1.2M patient training corpus limits reproducibility and generalizability to other healthcare settings
- Performance may degrade on non-oncology tasks due to training data distribution mismatch
- Effectiveness of causal masking versus bidirectional context hasn't been directly validated

## Confidence

- **High confidence**: The autoregressive next-event prediction framework works for the specific oncology domain tested, and LoRA fine-tuning with causal masking is technically sound.
- **Medium confidence**: Temperature-controlled sampling (α=0.5) meaningfully improves rare event learning, though this hasn't been directly validated against alternatives.
- **Low confidence**: Generalization to non-oncology settings and the claimed superiority over all existing EHR approaches, given the proprietary data advantage.

## Next Checks

1. **Domain transfer validation**: Train NEP on general EHR data (e.g., MIMIC-IV) and evaluate on non-oncology tasks like sepsis prediction or hospital readmission. This tests whether the oncology-trained model's performance generalizes beyond its training distribution.

2. **Bidirectional ablation study**: Compare NEP's causal masking approach against a bidirectional variant on the same tasks to quantify the actual contribution of unidirectional attention versus representation quality from bidirectional learning.

3. **Rare event sensitivity analysis**: Systematically vary the temperature parameter α (0.0, 0.3, 0.5, 0.7, 1.0) and measure impact on rare disease prediction (celiac, lupus, etc.) to determine optimal sampling strategy and validate the claimed benefits of downsampling.