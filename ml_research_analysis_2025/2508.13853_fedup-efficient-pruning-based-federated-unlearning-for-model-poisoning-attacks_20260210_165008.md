---
ver: rpa2
title: 'FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks'
arxiv_id: '2508.13853'
source_url: https://arxiv.org/abs/2508.13853
tags:
- malicious
- clients
- data
- weights
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FedUP, an efficient federated unlearning algorithm
  designed to mitigate the impact of malicious clients in federated learning systems.
  The core method identifies and prunes the most important weights that diverge between
  benign and malicious client updates, relying solely on the latest model weights
  to achieve this.
---

# FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks

## Quick Facts
- arXiv ID: 2508.13853
- Source URL: https://arxiv.org/abs/2508.13853
- Reference count: 40
- Primary result: FedUP effectively removes malicious influence from poisoned federated models, lowering malicious data accuracy to match retrain-from-scratch baselines while preserving benign data performance

## Executive Summary
FedUP introduces an efficient federated unlearning algorithm that identifies and prunes the most important weights diverging between benign and malicious client updates. By operating solely on the latest model weights, FedUP achieves significant storage efficiency compared to state-of-the-art approaches while maintaining effectiveness against strong adversarial conditions, including scenarios with up to 50%-1 malicious clients. The method successfully handles both label-flipping and backdoor attacks across IID and Non-IID data distributions.

## Method Summary
FedUP detects malicious influence by computing the squared difference between averaged benign and malicious model updates, then multiplying by global model weight magnitudes to rank importance. It prunes the top P% weights per layer (where P is data-similarity calibrated) from dense and convolutional layers only, then conducts recovery rounds with remaining benign clients. The method requires only the latest round's models, achieving 20× storage efficiency compared to baselines while consistently requiring fewer recovery rounds than retraining from scratch.

## Key Results
- FedUP lowers accuracy on malicious data to match retrain-from-scratch baseline models across all attack scenarios
- The method consistently requires fewer recovery rounds than state-of-the-art federated unlearning solutions
- FedUP achieves 20× storage efficiency by requiring only latest round models (2.7 MB vs 54 MB for FedEraser)
- Performance remains effective under strong adversarial conditions with up to 50%-1 malicious clients

## Why This Works (Mechanism)

### Mechanism 1: Differential Weight Divergence Detection
- Claim: Weights with high divergence between benign and malicious client updates are more likely to encode adversarial influence
- Mechanism: FedUP computes squared difference between averaged benign and averaged malicious updates, multiplies by global model weight magnitudes to rank importance
- Core assumption: Malicious clients produce updates that systematically diverge from benign clients on weights critical to their adversarial objective
- Evidence anchors: Abstract states FedUP "carefully selecting and zeroing the highest magnitude weights that diverge the most"; Figure 3 shows normalized squared difference distributions with malicious clients exhibiting longer tail of high-divergence weights

### Mechanism 2: Selective High-Magnitude Pruning
- Claim: Zeroing highest-magnitude, most-divergent weights removes malicious influence while minimizing collateral damage
- Mechanism: FedUP creates unlearning mask selecting top P% weights per layer, applies to dense and convolutional layers only
- Core assumption: High-magnitude weights have disproportionate influence on model behavior; malicious influence concentrates in relatively small subset
- Evidence anchors: Ablation study shows random pruning and highest-magnitude-only pruning underperform compared to divergence-weighted approach; natural forgetting alone fails to fully eliminate malicious contributions

### Mechanism 3: Accelerated Recovery via Sparse Re-initialization
- Claim: Pruning strategically perturbs model away from malicious-influenced local minima, enabling faster benign re-convergence
- Mechanism: After pruning, benign clients perform standard local training; sparse model structure accelerates forgetting of unreinforced malicious patterns
- Core assumption: Neural networks can efficiently recover lost benign knowledge through additional training rounds if model retains sufficient capacity
- Evidence anchors: Recovery rounds Rrec consistently well below retraining rounds R* across all configurations; recovery rounds bound RP ≤ ⌈R* × P⌉ proposed

## Foundational Learning

- Concept: Federated Learning Aggregation (FedAvg)
  - Why needed here: FedUP operates on assumption that clients send model updates which are aggregated via weighted averaging; understanding how global models form is essential to grasp why malicious updates persist
  - Quick check question: If 3 clients with equal weight contributions have model parameters [0.5, 0.7], [0.3, 0.9], and [0.4, 0.8] for a single weight, what would the aggregated global model parameter be? (Answer: 0.4, 0.8)

- Concept: Targeted vs. Untargeted Poisoning in FL
  - Why needed here: FedUP evaluated against label-flipping (untargeted degradation) and backdoor attacks (targeted trigger-based misclassification); unlearning mechanism must handle both
  - Quick check question: In a backdoor attack where 3×3 white square trigger is embedded in 10% of malicious client's images, would attack succeed if client's data is excluded from aggregation but trigger pattern remains in model's learned weights? (Answer: Yes—unlearning requires removing learned association, not just excluding future data)

- Concept: Neural Network Pruning and Importance Scoring
  - Why needed here: FedUP inverts traditional pruning logic—removes high-importance weights for unlearning rather than low-importance weights for compression
  - Quick check question: If convolutional layer has 1000 weights and FedUP applies P=10% pruning, how many weights are zeroed? Would batch normalization layers be affected? (Answer: 100 weights; No—restricts pruning to Dense and Convolutional layers only)

## Architecture Onboarding

- Component map: Detection Module -> Model Averager -> Divergence Calculator -> Mask Generator -> Pruning Engine -> Recovery Orchestrator

- Critical path:
  1. Detection module flags malicious client IDs → FedUP receives localModels, globalModel, maliciousIDs
  2. Compute avgBenign and avgMalicious from latest round updates
  3. Calculate difference = (avgMalicious - avgBenign)², then rank = difference × globalModel
  4. Estimate pruning percentage P using data similarity metric
  5. Generate mask selecting top P% weights per convolutional/dense layer
  6. Apply mask to avgBenign model → prunedGlobalModel
  7. Broadcast pruned model to benign clients only
  8. Conduct Rrec recovery rounds with standard FedAvg aggregation
  9. Update global model and resume normal training

- Design tradeoffs:
  - Storage efficiency vs. historical information: FedUP stores only last round's models but cannot correct errors from earlier rounds
  - Pruning aggressiveness vs. recovery speed: Higher P removes more malicious influence but risks benign knowledge loss
  - IID vs. Non-IID calibration: Higher P (10-15%) for IID data where benign updates are similar; lower P (3-6%) for Non-IID where updates are heterogeneous
  - Layer selectivity vs. comprehensive removal: Pruning only dense/convolutional layers preserves stability but may miss malicious influence in other layer types

- Failure signatures:
  - Insufficient unlearning: Malicious data accuracy remains high → P too low or divergence ranking failed
  - Excessive benign degradation: Test accuracy drops significantly and does not recover → P too high or applied to wrong layers
  - Slow recovery in Non-IID: Rrec approaches or exceeds R* bound → data heterogeneity more extreme than calibration assumed
  - False positive damage: Benign client incorrectly pruned → accuracy on that client's data drops but remains higher than test accuracy
  - DoS triggering rate: Unlearning activated too frequently, disrupting convergence → rate limiter threshold T too low

- First 3 experiments:
  1. Baseline validation on benign data: Set up FL with 10 clients on CIFAR-10 (IID), train for 50 rounds to convergence
  2. Controlled attack and single-client unlearning: Introduce 1-2 malicious clients performing label-flipping, detect at round 30, apply FedUP with P=10%
  3. Non-IID stress test: Repeat experiment 2 with Dirichlet distribution (α=1) for Non-IID data partitioning, use P=5%

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimal pruning percentage P be determined dynamically without relying on empirical tuning or the proposed heuristic approximation?
- **Basis in paper:** Section V-B states "Determining the optimal pruning percentage is challenging" and proposes only heuristic approximation based on similarity
- **Why unresolved:** The paper provides a formula based on separability and similarity but acknowledges directly estimating these values is difficult
- **What evidence would resolve it:** A theoretical framework or adaptive algorithm that sets P automatically based on real-time model statistics without manual tuning

### Open Question 2
- **Question:** Is FedUP robust against adaptive adversaries specifically optimizing their updates to minimize weight divergence or magnitude?
- **Basis in paper:** Threat model assumes adversaries have full knowledge of aggregation process, but evaluation limited to standard attacks
- **Why unresolved:** Current evaluation demonstrates effectiveness against standard attacks but does not test scenarios where attack function is specifically designed to be stealthy against magnitude-divergence pruning mechanism
- **What evidence would resolve it:** Experimental results showing FedUP's performance against adversaries who explicitly optimize malicious updates to minimize L2 distance from benign updates

### Open Question 3
- **Question:** Can federated unlearning be achieved effectively when malicious clients constitute the majority of the system?
- **Basis in paper:** Assumption 1 and Section II-A state FedUP assumes benign majority and is designed to operate effectively under this assumption
- **Why unresolved:** FedUP relies on comparing averaged benign and malicious updates; if malicious updates dominate statistics, "difference" calculation would likely identify benign weights as divergent ones to be pruned
- **What evidence would resolve it:** A modified algorithm capable of identifying and removing malicious contributions even when malicious clients hold majority vote or statistical influence

## Limitations
- Exact CNN architecture for label-flipping experiments only described as "LeNet-like" without full specifications
- Pruning percentage calibration relies on empirical constants that may not generalize across different datasets or model architectures
- External detection mechanism for malicious clients is assumed but not specified, creating dependency on third-party components

## Confidence
- High confidence: FedUP's core mechanism of weight divergence detection and selective pruning is clearly specified and theoretically sound
- Medium confidence: Pruning percentage calibration formula is empirically derived and may require adjustment for different scenarios
- Medium confidence: Recovery bound RP ≤ ⌈R* × P⌉ is theoretically derived but assumes consistent convergence patterns that may not hold in highly heterogeneous settings

## Next Checks
1. Reproduce the label-flipping ablation study to verify that FedUP's divergence-weighted pruning significantly outperforms random pruning and magnitude-only pruning on both test and malicious data accuracy
2. Implement the exact pruning mask generation algorithm and verify it correctly restricts pruning to dense and convolutional layers only, preserving batch normalization and other layer types
3. Conduct a false positive pruning experiment with one benign client isolated as "malicious" to verify the expected partial but recoverable accuracy degradation pattern described in Table V