---
ver: rpa2
title: Global explainability of a deep abstaining classifier
arxiv_id: '2504.01202'
source_url: https://arxiv.org/abs/2504.01202
tags:
- reports
- classes
- cancer
- histology
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a global explainability method to identify
  sources of errors in a deep abstaining classifier (DAC) for histology classification
  of lung and breast cancers using 1.04 million pathology reports. The DAC achieves
  97% accuracy but abstains on 78% of samples due to label noise, hierarchical class
  complexity, insufficient information, and conflicting evidence.
---

# Global explainability of a deep abstaining classifier

## Quick Facts
- arXiv ID: 2504.01202
- Source URL: https://arxiv.org/abs/2504.01202
- Authors: Sayera Dhaubhadel; Jamaludin Mohd-Yusof; Benjamin H. McMahon; Trilce Estrada; Kumkum Ganguly; Adam Spannaus; John P. Gounley; Xiao-Cheng Wu; Eric B. Durbin; Heidi A. Hanson; Tanmoy Bhattacharya
- Reference count: 40
- Key outcome: Deep abstaining classifier achieves 97% accuracy on histology classification while abstaining on 78% of samples; global explainability method using GradInp + PCA identifies sources of abstention as label noise, hierarchical class complexity, insufficient information, and conflicting evidence.

## Executive Summary
This study presents a global explainability method to identify sources of errors in a deep abstaining classifier (DAC) for histology classification of lung and breast cancers using 1.04 million pathology reports. The DAC achieves 97% accuracy but abstains on 78% of samples due to label noise, hierarchical class complexity, insufficient information, and conflicting evidence. The method combines local explanations from GradInp with PCA dimensionality reduction to aggregate insights across 13,000 reports, identifying specific error patterns like ambiguous terms and class interdependencies. Results show that abstaining simplifies the decision boundary, enabling targeted improvements in training protocols such as excluding noisy reports and refining hierarchical class relationships. This approach enhances interpretability and provides actionable strategies for improving automated cancer report annotation systems.

## Method Summary
The study uses a deep abstaining classifier (DAC) with a modified cross-entropy loss function that includes an abstention class. The model is a multitask CNN architecture trained on 1.04 million pathology reports from NCI-SEER registries, with maximum sequence length of 3000 tokens processed in reverse order. Local explanations are generated using Gradient·Input (GradInp) for approximately 13,000 samples, aggregated into an Aggregated Local Explanations (ALE) matrix, and reduced via PCA to visualize global error patterns and identify sources of abstention.

## Key Results
- DAC achieves 97% accuracy on histology classification while abstaining on 78% of samples
- PCA visualization reveals distinct error patterns: label noise (reports on wrong rays), conflicting evidence (reports between rays), and insufficient information (reports near origin)
- 22% of histology reports are processed (non-abstained) to maintain 97% accuracy target
- Identified abstention sources include ambiguous terms, hierarchical class relationships, and missing clinical context

## Why This Works (Mechanism)

### Mechanism 1
The deep abstaining classifier (DAC) creates a high-confidence decision boundary by treating uncertainty as a learnable "abstain" class. The DAC modifies the standard cross-entropy loss by adding an extra output neuron (the abstention class) and a penalty term $\alpha$. During training, the model minimizes loss by routing ambiguous inputs—where class probabilities are diffuse—to this abstention class rather than forcing a low-confidence prediction.

### Mechanism 2
Gradient-based local explanations (GradInp) aggregated via dimensionality reduction reveal global error patterns that are invisible in single-sample analysis. GradInp calculates the dot product of the input embedding and the gradient of the output, assigning importance weights to words. These weights are aggregated into a matrix (ALE) where rows are reports and columns are words. Principal Component Analysis (PCA) then projects this high-dimensional matrix into 2D space, clustering reports that rely on similar keyword evidence.

### Mechanism 3
Visualizing the geometric structure of the PCA-reduced explanation space distinguishes between distinct error types: label noise vs. conflicting information. Reports containing unambiguous keywords form tight "rays" extending from the origin. Misclassified reports appearing on these rays are identified as label noise (the model sees evidence for A, label is B). Reports appearing between rays indicate conflicting evidence (model sees evidence for A and B).

## Foundational Learning

- **Concept:** Gradient·Input (GradInp) Attribution
  - Why needed here: The paper requires a computationally efficient method to explain 13,000+ reports; LIME was too slow. GradInp provides a fast proxy for feature importance.
  - Quick check question: How does GradInp differ from standard gradient saliency maps in handling the embedding space's coordinate system?

- **Concept:** Principal Component Analysis (PCA) on Non-Linear Features
  - Why needed here: To reduce the dimensionality of the aggregated local explanations (ALE matrix) so that global patterns across thousands of reports can be visualized and interpreted.
  - Quick check question: What does the first principal component (PC1) represent in the context of the ALE matrix—the most common word or the direction of maximum variance in feature importance?

- **Concept:** Abstention in Deep Learning
  - Why needed here: Understanding how a model can refuse to classify (abstain) is critical to grasping how the system maintains 97% accuracy despite noisy data.
  - Quick check question: In the loss function $L(x) = (1-p_{k+1})(-\sum...) + \alpha \log \frac{1}{1-p_{k+1}}$, does increasing $\alpha$ encourage or discourage abstention?

## Architecture Onboarding

- **Component map:** Input Pipeline -> MTCNN Base -> Task Heads -> Explainability Module
- **Critical path:** The tuning of the abstention penalty ($\alpha$) is the most critical step. It directly trades off coverage (how many reports are processed) against accuracy (risk of error).
- **Design tradeoffs:** GradInp vs. LIME trades stability for 150x speedup; Accuracy vs. Coverage explicitly sacrifices coverage to meet clinical accuracy constraints.
- **Failure signatures:** Majority Class Collapse (100% abstention on rare classes); Hierarchical Confusion (overlapping rays due to shared keywords).
- **First 3 experiments:** 1) Abstention Sweep: Vary $\alpha$ for histology task to plot Accuracy-Abstention curve; 2) ALE Stability Test: Compare ALE matrices for two random subsets of 1,000 reports; 3) Noise Filtering Intervention: Exclude "label noise" clusters from training set and retrain.

## Open Questions the Paper Calls Out

### Open Question 1
Does applying the identified global explainability interventions (e.g., excluding noisy reports, focused annotation) lead to a substantive improvement in model coverage and accuracy? The authors state they have not demonstrated that applications of their suggested workflow lead to substantive improvements in model training.

### Open Question 2
Can modifying the loss function to apply reduced penalties for errors involving hierarchically related classes effectively reduce abstention rates without sacrificing accuracy? The paper suggests this as a strategy to iteratively improve the DAC.

### Open Question 3
Does the GradInp-PCA explainability method generalize effectively to non-text data domains, such as genomic sequencing or image classification? The Conclusion claims the method could be extended to non-medical domains and genomic sequencing results.

## Limitations
- Reliance on single dataset (SEER pathology reports) without external validation
- Geometric interpretation of PCA-reduced space assumes linear separability of error types
- No statistical significance testing for identified abstention patterns

## Confidence
- Mechanism 1 (Abstention loss function): High confidence
- Mechanism 2 (GradInp + PCA aggregation): Medium confidence  
- Mechanism 3 (Geometric interpretation of error types): Low confidence

## Next Checks
1. Apply DAC explainability pipeline to pathology reports from a different cancer registry (TCGA or European database) to verify pattern consistency
2. Perform permutation tests on PCA-reduced explanation space to determine statistical significance of identified clusters
3. Replicate analysis using alternative local explanation method (SHAP or Integrated Gradients) to verify robustness of findings