---
ver: rpa2
title: 'PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic
  Culture'
arxiv_id: '2509.02550'
source_url: https://arxiv.org/abs/2509.02550
tags:
- cultural
- arabic
- data
- islamic
- palmx
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PalmX 2025 addresses the cultural bias in LLMs by introducing
  the first benchmark focused on Arabic and Islamic cultural competence. The shared
  task consists of two subtasks with multiple-choice questions in Modern Standard
  Arabic: General Arabic Culture (covering traditions, food, history, etc.'
---

# PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture

## Quick Facts
- arXiv ID: 2509.02550
- Source URL: https://arxiv.org/abs/2509.02550
- Reference count: 30
- 26 registered teams; top systems achieved 72.15% accuracy on cultural questions and 84.22% on Islamic knowledge

## Executive Summary
PalmX 2025 addresses the cultural bias in LLMs by introducing the first benchmark focused on Arabic and Islamic cultural competence. The shared task consists of two subtasks with multiple-choice questions in Modern Standard Arabic: General Arabic Culture (covering traditions, food, history, etc. from 22 Arab countries) and General Islamic Culture (covering religious practices, Quranic knowledge, etc.). Using 26 registered teams, the evaluation employed likelihood-based scoring on held-out test data, with accuracy as the primary metric.

Results show task-specific fine-tuning substantially improves performance over baseline models. The top systems achieved 72.15% accuracy on cultural questions and 84.22% on Islamic knowledge. Parameter-efficient fine-tuning (particularly LoRA) emerged as the most effective approach, while data augmentation effectiveness varied by domain. The performance gap between cultural and Islamic subtasks suggests different challenges, with Islamic knowledge benefiting from more structured canonical sources.

## Method Summary
The PalmX benchmark evaluates LLMs on Arabic and Islamic cultural competence through multiple-choice questions in Modern Standard Arabic. Two subtasks are defined: General Arabic Culture (2,000 train, 500 dev, 2,000 test questions) covering traditions, food, history, and other cultural aspects from 22 Arab countries; and General Islamic Culture (600 train, 300 dev, 1,000 test questions) covering religious practices and Quranic knowledge. Evaluation uses likelihood-based scoring where models select from four options (A-D) by computing log-likelihood of each label given the prompt. Teams were constrained to models ≤13B parameters with no RAG or live retrieval allowed.

## Key Results
- Parameter-efficient fine-tuning (LoRA) was the dominant strategy, with LoRA (r=16, α=32) emerging across teams as most effective
- Top performers achieved 72.15% accuracy on cultural questions and 84.22% on Islamic knowledge
- Fine-tuning substantially improved performance over zero-shot baselines, with top teams showing gains of 4.6% and 9.1% for cultural and Islamic subtasks respectively
- Islamic knowledge questions were slightly easier due to more structured canonical sources, while cultural knowledge showed greater variation across countries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient fine-tuning (LoRA) is the most effective approach for Arabic cultural knowledge adaptation.
- Mechanism: LoRA adds low-rank update matrices to frozen pretrained weights, enabling domain specialization without catastrophic forgetting or full retraining. This allows Arabic-centric models (NileChat-3B, ALLaM-7B) to surface latent cultural knowledge from pre-training while adapting to the MCQ format.
- Core assumption: Cultural knowledge is partially encoded during pre-training on Arabic text; the task requires format adaptation rather than knowledge injection from scratch.
- Evidence anchors:
  - [abstract] "Parameter-efficient fine-tuning (particularly LoRA) emerged as the most effective approach"
  - [section 5.2] "LoRA emerged as the dominant fine-tuning strategy across teams, demonstrating its effectiveness"
  - [corpus] Similar LoRA effectiveness reported for Arabic inheritance reasoning (QU-NLP at QIAS 2025)
- Break condition: If pre-training lacks cultural coverage, LoRA cannot synthesize new knowledge from limited MCQ data.

### Mechanism 2
- Claim: Task-specific fine-tuning on cultural MCQs provides substantial gains over zero-shot baselines.
- Mechanism: Fine-tuning recalibrates answer selection preferences, teaching the model to prioritize culturally-grounded knowledge over generic or Western-centric responses during likelihood-based scoring.
- Core assumption: The model has been exposed to relevant cultural content but needs supervised signal to learn which knowledge patterns correspond to correct answers.
- Evidence anchors:
  - [abstract] "task-specific fine-tuning substantially improves performance over baseline models"
  - [section 5.1] "top performers achiev[ed] substantial improvements of 4.6% and 9.1% for Subtasks 1 and 2"
  - [corpus] Corpus evidence on fine-tuning gains is limited; neighboring papers focus on retrieval or specialized reasoning.
- Break condition: If MCQs require knowledge entirely absent from pre-training, fine-tuning will overfit to shallow cues rather than learn transferable cultural competence.

### Mechanism 3
- Claim: Islamic knowledge questions achieve higher accuracy due to more structured canonical sources.
- Mechanism: Islamic knowledge draws from standardized, widely-replicated textual sources (Quran, Hadith, scholarly consensus), producing consistent training signal. Cultural knowledge spans heterogeneous practices across 22 countries with greater variation.
- Core assumption: LLMs more easily acquire knowledge that appears in regular, repetitive textual patterns across the pre-training corpus.
- Evidence anchors:
  - [abstract] "Islamic knowledge benefiting from more structured canonical sources"
  - [section 5.3] "Islamic knowledge questions may be slightly less challenging due to being more structured and based on canonical sources"
  - [corpus] Related work on Islamic legal reasoning shows similar structured-domain advantages (Benchmarking Legal Reasoning in Arabic Islamic Inheritance)
- Break condition: If the Islamic subtask is simply easier or has less diverse questions, structural claims may be confounded by difficulty distribution.

## Foundational Learning

- Concept: Likelihood-based MCQ evaluation
  - Why needed here: This benchmark evaluates by computing log-likelihood of answer labels (A/B/C/D) given the prompt, not generative decoding. Understanding this scoring is essential for reproducing results.
  - Quick check question: Given a question prompt and four choices, how would you compute which label the model assigns highest probability?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: LoRA dominated winning approaches; hyperparameters like rank (r=16) and alpha (α=32) directly affect performance.
  - Quick check question: Why might LoRA outperform full fine-tuning when the base model already encodes relevant but latent cultural knowledge?

- Concept: Cultural alignment vs. cultural awareness
  - Why needed here: The paper distinguishes awareness (knowing norms) from alignment (acting consistently with them); MCQ benchmarks test awareness, not behavioral alignment.
  - Quick check question: Could a model achieve high benchmark accuracy while still producing culturally inappropriate free-form responses?

## Architecture Onboarding

- Component map:
  - Base models: NileChat-3B, ALLaM-7B-Instruct, Fanar-1-9B-Instruct (Arabic-centric)
  - Adaptation: LoRA (dominant), full fine-tuning (used by winner of Subtask 1)
  - Data augmentation: LLM-based paraphrasing, external MCQs, answer shuffling (domain-dependent effectiveness)
  - Evaluation: Likelihood-based label scoring, accuracy metric, private test split

- Critical path:
  1. Select Arabic-centric base model (NileChat-3B or ALLaM-7B recommended)
  2. Apply LoRA (r=16, α=32) fine-tuning for 3 epochs on PalmX training data
  3. Evaluate on dev set using likelihood-based scoring before test submission
  4. Optionally test augmentation strategies on dev set first (may hurt in cultural domain)

- Design tradeoffs:
  - Model size: Larger models (Fanar-9B) did not guarantee wins; smaller models (NileChat-3B) placed first and second in Subtask 1
  - Full vs. PEFT fine-tuning: Full fine-tuning achieved top score (72.15%) but LoRA dominated overall due to efficiency
  - Augmentation: Helped Islamic subtask (AYA team), hurt cultural dev performance (same team)

- Failure signatures:
  - Using RAG or external retrieval (prohibited by rules; would test retrieval not knowledge)
  - Over-augmentation causing dev performance degradation (observed by AYA team)
  - Positional bias if answer options not shuffled during preprocessing

- First 3 experiments:
  1. Establish baseline: Zero-shot likelihood evaluation on dev set with NileChat-3B
  2. LoRA fine-tuning: Train LoRA (r=16, α=32, 3 epochs) on PalmX training data, compare to baseline
  3. Ablate augmentation: Add paraphrase-based augmentation to 20% of training data, measure impact on both subtasks separately

## Open Questions the Paper Calls Out

- How does the efficacy of data augmentation differ between general Arabic cultural knowledge and Islamic knowledge tasks?
  - Basis in paper: [explicit] The authors state, "The impact of data augmentation varied significantly between subtasks... suggesting that augmentation effectiveness is highly domain- and data-dependent and requires careful study."
  - Why unresolved: While the winning team found augmentation crucial for Islamic knowledge, the same team found it ineffective for general culture; the specific factors driving this divergence remain unidentified.
  - What evidence would resolve it: Ablation studies applying identical augmentation strategies (e.g., paraphrasing) to both domains while controlling for data structure and topic diversity.

- How can cultural competence evaluation be extended beyond Modern Standard Arabic (MSA) multiple-choice questions to include dialectal variation and open-ended reasoning?
  - Basis in paper: [explicit] The authors acknowledge the "Evaluation Constraints," noting the benchmark "does not capture broader aspects... such as open-ended reasoning, interactive dialogue, or sensitivity to dialectal variation."
  - Why unresolved: The current reliance on MSA-based MCQs provides reproducibility but fails to validate a model's ability to function in the diverse, real linguistic environments of the Arab world.
  - What evidence would resolve it: The development and integration of new evaluation splits utilizing dialectal Arabic and generative tasks with human-centered assessment metrics.

- To what extent does the current country-level data imbalance affect model generalization for underrepresented cultures like Iraq and Algeria?
  - Basis in paper: [explicit] The Limitations section notes, "Countries like Iraq and Algeria are underrepresented... This imbalance may bias the models toward frequently represented cultures and limit their generalization."
  - Why unresolved: The paper identifies the geographical skew in the dataset but does not quantify the resulting performance penalty for the underrepresented regions.
  - What evidence would resolve it: A fine-grained accuracy analysis comparing model performance on questions from high-representation countries versus low-representation countries.

## Limitations

- The benchmark's focus on Modern Standard Arabic limits generalizability to dialectal Arabic content, which comprises a significant portion of actual Arabic cultural discourse.
- The reliance on multiple-choice format constrains the evaluation to recognition rather than generation of culturally-appropriate responses.
- The data sources, while diverse, may not fully represent the 22 Arab countries' cultural variations, potentially biasing the evaluation toward more represented regions.

## Confidence

- High confidence: The effectiveness of LoRA for parameter-efficient fine-tuning on Arabic cultural knowledge
- Medium confidence: The claim that Islamic knowledge benefits from more structured canonical sources
- Medium confidence: The substantial performance gains from task-specific fine-tuning
- Low confidence: The relative ineffectiveness of data augmentation strategies

## Next Checks

1. **Cross-validation of augmentation effectiveness**: Replicate the data augmentation experiments across multiple teams' approaches to determine if AYA team's negative results were specific to their implementation or represent a general pattern for cultural knowledge domains.

2. **Zero-shot transfer evaluation**: Test whether models fine-tuned on the Islamic subtask show improved performance on the cultural subtask (and vice versa), validating whether the claimed structural differences between domains create domain-specific learning barriers.

3. **Free-response validation**: Conduct a small-scale human evaluation where models must generate answers to a subset of questions rather than select from options, measuring whether MCQ accuracy correlates with actual cultural competence in open-ended scenarios.