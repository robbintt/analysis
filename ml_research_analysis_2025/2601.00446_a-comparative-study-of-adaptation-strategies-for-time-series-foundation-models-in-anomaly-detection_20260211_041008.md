---
ver: rpa2
title: A Comparative Study of Adaptation Strategies for Time Series Foundation Models
  in Anomaly Detection
arxiv_id: '2601.00446'
source_url: https://arxiv.org/abs/2601.00446
tags:
- anomaly
- detection
- tsfms
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates time series foundation models (TSFMs) for
  anomaly detection, comparing zero-shot inference, full fine-tuning, and parameter-efficient
  fine-tuning (PEFT) strategies. TSFMs, pretrained on large heterogeneous datasets,
  are tested on the TSB-AD-U benchmark across 23 datasets.
---

# A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection

## Quick Facts
- arXiv ID: 2601.00446
- Source URL: https://arxiv.org/abs/2601.00446
- Reference count: 40
- Key outcome: Time series foundation models (TSFMs) combined with parameter-efficient fine-tuning (PEFT) provide a scalable, efficient paradigm for anomaly detection, with PEFT matching or exceeding full fine-tuning while updating fewer parameters.

## Executive Summary
This study evaluates time series foundation models (TSFMs) for anomaly detection, comparing zero-shot inference, full fine-tuning, and parameter-efficient fine-tuning (PEFT) strategies. TSFMs, pretrained on large heterogeneous datasets, are tested on the TSB-AD-U benchmark across 23 datasets. Zero-shot TSFMs outperform traditional baselines, but adaptation yields further gains, especially under severe class imbalance. PEFT methods like LoRA, OFT, and HRA match or exceed full fine-tuning while updating fewer parameters. Dense TSFMs excel in zero-shot settings, while Time-MoE closes the gap after adaptation. Overall, TSFMs combined with PEFT provide a scalable, efficient paradigm for time series anomaly detection.

## Method Summary
The study evaluates TSFMs (Moirai, Chronos-T5, Time-MoE) on the TSB-AD-U benchmark (23 univariate datasets) using zero-shot inference, full fine-tuning, and PEFT (LoRA, OFT, HRA, IA3). Models use 150-step context windows; evaluation is on the tuning split. Anomaly scores are computed as MSE between forecast and ground truth. PEFT adapters target attention projections (K, Q, V, O) with ranks 4-32. Training uses 10 epochs with learning rates {1e-2, 1e-3, 1e-4, 1e-5}. Losses: NLL (Moirai), cross-entropy (Chronos), Huber (Time-MoE). Metrics: AUC-PR, AUC-ROC, VUS-PR, VUS-ROC.

## Key Results
- Zero-shot TSFMs outperform traditional baselines, demonstrating strong transfer from pretraining.
- PEFT methods (LoRA, OFT, HRA) match or exceed full fine-tuning on dense models while updating 0.5-3.5% of parameters.
- Dense TSFMs excel in zero-shot settings; Time-MoE closes the gap after adaptation, especially on precision-oriented metrics under class imbalance.
- IA3 is the most parameter-efficient but underperforms adapter-based PEFT on VUS metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained forecasting models can be repurposed for anomaly detection by using prediction error as an anomaly signal, even without task-specific training.
- Mechanism: TSFMs trained on large heterogeneous corpora encode generalizable temporal patterns. At inference, the model forecasts future values conditioned on historical context; the mean squared error (MSE) between predictions and observations serves as the anomaly score. Large deviations indicate anomalies.
- Core assumption: Anomalous patterns deviate from the learned distribution of normal temporal dynamics encoded during pretraining.
- Evidence anchors:
  - [abstract] "TSFMs, pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection"
  - [section 1] "TSFMs generate multi-step predictions conditioned only on past observations, enabling their direct use in online detection settings"
  - [corpus] Neighboring paper "Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection" also uses forecasting-based error signals.
- Break condition: If the target domain exhibits temporal patterns fundamentally disjoint from pretraining data (e.g., novel physics, radically different sampling rates), zero-shot transfer degrades.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning methods (LoRA, OFT, HRA) can match or exceed full fine-tuning for TSFM adaptation while updating 0.5-3.5% of parameters.
- Mechanism: Adapter-based PEFT injects trainable low-rank (LoRA) or orthogonal (OFT, HRA) modules into attention projections (key, query, value, output), preserving pretrained knowledge while allowing targeted recalibration. This aligns the model's temporal representations with domain-specific characteristics without catastrophic forgetting.
- Core assumption: The pretrained backbone already encodes useful temporal priors; only modest adjustment is needed for task alignment.
- Evidence anchors:
  - [abstract] "PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases"
  - [section 4.3] "For Moirai-base, adapter-based approaches such as LoRA, OFT, and HRA consistently outperform full fine-tuning across most metrics"
  - [corpus] Weak comparative evidence; neighboring papers focus on novel adapter designs (STAR, CoRA) rather than systematic PEFT benchmarking.
- Break condition: For mixture-of-experts architectures, standard PEFT may insufficiently adapt expert routing/load-balancing, leaving residual gaps vs. full fine-tuning (observed for Time-MoE).

### Mechanism 3
- Claim: Dense TSFMs outperform MoE variants in zero-shot settings, but MoE closes the gap after adaptation—particularly on precision-oriented metrics under severe class imbalance.
- Mechanism: Dense models (Moirai, Chronos) provide more stable representations across heterogeneous inputs due to full parameter activation. MoE models (Time-MoE) conditionally activate expert subsets, which may under-specialize for out-of-distribution anomalies pre-adaptation. Fine-tuning calibrates expert selection, improving anomaly discrimination.
- Core assumption: Class imbalance necessitates precision-focused optimization; MoE routing benefits from task-specific calibration.
- Evidence anchors:
  - [abstract] "Dense TSFMs excel in zero-shot settings, while Time-MoE closes the gap after adaptation"
  - [section 4.3] "Time-MoE exhibits substantial gains under adaptation, indicating that mixture-of-experts architectures benefit more strongly from task-specific calibration"
  - [corpus] Neighboring work (e.g., "Moirai-MoE") discusses sparse expert activation but does not systematically compare dense vs. MoE for anomaly detection.
- Break condition: If adaptation data is insufficient or imbalanced in ways that bias expert routing, MoE may exhibit unstable or suboptimal convergence.

## Foundational Learning

- Concept: **AUC-PR vs. AUC-ROC under class imbalance**
  - Why needed here: Anomaly detection typically involves rare events; AUC-PR is more informative than AUC-ROC when positives are scarce. The paper emphasizes precision-oriented gains.
  - Quick check question: Given a dataset with 1% anomalies, which metric would you prioritize for model selection?

- Concept: **VUS (Volume Under Surface) metrics for temporal tolerance**
  - Why needed here: Point-wise AUC penalizes minor temporal misalignments; VUS-PR and VUS-ROC provide buffer zones around ground-truth anomalies, offering partial credit for near-miss detections.
  - Quick check question: If your model consistently detects anomalies 3 timesteps early, will VUS-PR likely be higher or lower than AUC-PR?

- Concept: **PEFT adapter injection points**
  - Why needed here: LoRA/OFT/HRA are applied to attention projections (K, Q, V, O), not feedforward layers (except IA3). Understanding injection architecture is critical for correct implementation.
  - Quick check question: If you apply LoRA only to query projections but not value projections, what representational capacity might you lose?

## Architecture Onboarding

- Component map:
  - **Backbone TSFMs**: Moirai (encoder-only, patch-based) -> Chronos-T5 (autoregressive, tokenized) -> Time-MoE (sparse MoE, autoregressive)
  - **PEFT modules**: LoRA (low-rank additive) -> OFT (orthogonal) -> HRA (Householder reflections) -> IA3 (multiplicative scaling)
  - **Anomaly scoring**: MSE between forecast and ground-truth per timestep
  - **Evaluation pipeline**: TSB-AD-U benchmark (23 datasets) -> compute AUC-PR, AUC-ROC, VUS-PR, VUS-ROC

- Critical path:
  1. Load pretrained TSFM backbone (e.g., Moirai-base, Chronos-t5-base, Time-MoE-base)
  2. Inject PEFT adapters into attention projections (K, Q, V, O) per specified rank
  3. Train for 10 epochs using backbone-specific loss (NLL for Moirai, cross-entropy for Chronos, Huber for Time-MoE)
  4. At evaluation, compute MSE anomaly scores and threshold-free metrics

- Design tradeoffs:
  - **Dense vs. MoE**: Dense offers more stable zero-shot; MoE requires more adaptation but can close gap post-fine-tuning.
  - **PEFT rank selection**: Rank 4-32 tested; higher rank increases capacity but reduces parameter efficiency.
  - **IA3 vs. adapter-based PEFT**: IA3 is leaner (~0.05% params) but consistently underperforms LoRA/OFT/HRA on VUS metrics.

- Failure signatures:
  - **Zero-shot underperformance on highly domain-specific data**: Indicates pretraining distribution mismatch.
  - **MoE PEFT gap vs. full fine-tuning**: Suggests expert routing not fully calibrated; consider expert-aware adapters.
  - **AUC-ROC high but AUC-PR low**: Model ranks anomalies correctly but struggles with precision—common under severe imbalance.

- First 3 experiments:
  1. **Zero-shot baseline**: Run all TSFM variants on TSB-AD-U without adaptation; compute all four metrics to establish backbone performance tiers.
  2. **PEFT rank sweep**: Apply LoRA to Moirai-base with ranks 4, 8, 16, 32; compare parameter ratio vs. VUS-PR to identify efficiency frontier.
  3. **Dense vs. MoE adaptation**: Fine-tune Moirai-base and Time-MoE-base using OFT; quantify adaptation gains on precision-oriented metrics (AUC-PR, VUS-PR) to validate architecture-specific adaptation benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can expert-aware or routing-aware parameter-efficient fine-tuning (PEFT) strategies close the performance gap between standard PEFT and full fine-tuning for Mixture-of-Experts (MoE) time series foundation models?
- Basis in paper: [explicit] The conclusion states the need for "improved expert specialization for MoE-based TSFMs," while the results section notes that standard PEFT methods only "partially recover" the gains of full fine-tuning for Time-MoE.
- Why unresolved: The study finds that while dense models benefit significantly from standard adapters (LoRA, OFT), MoE models exhibit a residual gap, likely because standard methods do not sufficiently adapt expert routing or load balancing mechanisms.
- What evidence would resolve it: Development and evaluation of novel PEFT techniques that specifically target expert gating/routing layers, demonstrating performance parity with full fine-tuning on MoE architectures.

### Open Question 2
- Question: Do the reported performance and efficiency benefits of adapted TSFMs transfer to multivariate, streaming, and online anomaly detection scenarios?
- Basis in paper: [explicit] The conclusion explicitly identifies the extension of these models to "multivariate, streaming, or online scenarios" as a direction for future work.
- Why unresolved: The current study is restricted to the TSB-AD-U benchmark, which comprises solely univariate datasets, leaving the behavior of these models in complex, high-dimensional, or real-time settings untested.
- What evidence would resolve it: Empirical benchmarking of TSFMs with PEFT on standard multivariate datasets (e.g., SMD, MSL, SMAP) and latency measurements in streaming environments.

### Open Question 3
- Question: How does the performance ranking of TSFMs versus task-specific baselines change when evaluating on the full benchmark test set rather than the smaller tuning set?
- Basis in paper: [inferred] The methodology (Section 3.1) admits to using the "tuning set" for final performance reporting rather than the larger evaluation set to enable efficient experimentation, potentially biasing the generalization estimates.
- Why unresolved: While necessary for the study's scope, evaluating on a subset designed for hyperparameter selection may not fully reflect the models' robustness across the wider variety of anomaly types found in the complete test set.
- What evidence would resolve it: A comparative analysis of model rankings using the official TSB-AD-U evaluation split to confirm if the reported gains over baselines hold statistically.

## Limitations

- The study evaluates on univariate datasets only, leaving multivariate, streaming, and online scenarios untested.
- Final hyperparameter selections (learning rate, rank) per model and PEFT method are not explicitly reported, preventing exact replication.
- MoE-specific PEFT methods exhibit a performance gap vs. full fine-tuning, suggesting current adapter designs may inadequately address expert routing dynamics.

## Confidence

- **High confidence**: Zero-shot TSFMs outperform traditional baselines; dense models show stable zero-shot performance; PEFT matches or exceeds full fine-tuning on dense models.
- **Medium confidence**: PEFT methods close gaps vs. full fine-tuning for Time-MoE; architecture-specific adaptation benefits are empirically demonstrated but lack ablation for adapter placement.
- **Low confidence**: Generalization claims to "arbitrary" datasets without task-specific training; quantitative justification for VUS metric superiority over point-wise AUC is minimal.

## Next Checks

1. Replicate the dense vs. MoE adaptation gap by fine-tuning Time-MoE with expert-aware adapters and comparing precision-oriented metrics.
2. Conduct ablation studies on adapter injection points (K/Q/V/O) to quantify representational capacity loss from partial application.
3. Evaluate PEFT robustness across learning rates and ranks on a held-out validation set to verify reported "best average performance" claims.