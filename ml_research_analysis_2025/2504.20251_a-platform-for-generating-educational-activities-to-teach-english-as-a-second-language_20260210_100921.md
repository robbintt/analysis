---
ver: rpa2
title: A Platform for Generating Educational Activities to Teach English as a Second
  Language
arxiv_id: '2504.20251'
source_url: https://arxiv.org/abs/2504.20251
tags:
- language
- platform
- activities
- generation
- exercises
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a platform for generating educational activities
  to teach English as a second language, addressing the challenge of integrating Natural
  Language Processing techniques into a real-world educational tool. The platform
  combines semi-automatically generated and manually curated resources, such as word
  lists and definitions, with text-based activity generation from teacher input.
---

# A Platform for Generating Educational Activities to Teach English as a Second Language

## Quick Facts
- arXiv ID: 2504.20251
- Source URL: https://arxiv.org/abs/2504.20251
- Authors: Aiala Rosá; Santiago Góngora; Juan Pablo Filevich; Ignacio Sastre; Laura Musto; Brian Carpenter; Luis Chiruzzo
- Reference count: 6
- Primary result: Web platform combining semi-automatically generated and manually curated resources to create vocabulary games and text-based activities for ESL teaching

## Executive Summary
This paper presents a web-based educational platform designed to generate activities for teaching English as a second language. The platform addresses the challenge of integrating NLP techniques into real-world educational tools by combining semi-automatically generated resources with manual curation. It offers both pre-curated games (crosswords, word searches) that can be used immediately and more complex text-based activities (Q&A, story games) that require teacher review before use. The system is designed for accessibility in rural schools with minimal personal data collection and aims to support English teaching through interactive, vocabulary-focused exercises.

## Method Summary
The platform employs a hybrid methodology combining semi-automated resource generation with manual curation. Vocabulary lists are expanded using word embeddings and Simple Wikipedia frequencies, then manually curated by teachers. Definition extraction uses pattern-based candidates filtered by a DistilBERT classifier (0.78 F1). Text-based activities employ a four-stage pipeline: co-reference resolution, semantic role labeling for answer candidates, transformer-based question generation, and post-processing. The system supports both pre-curated mode (immediate use) and teacher-review mode (for text-generated content). Experimental components include Stable Diffusion for images and GPT-2 for sentence generation, though these are not yet integrated due to compute constraints.

## Key Results
- Hybrid approach successfully combines semi-automated generation with manual curation for educational content
- DistilBERT classifier achieves 0.78 F1 for filtering definition extraction candidates
- Platform enables immediate use of pre-curated games while maintaining safety through teacher review for text-generated activities
- Successfully deployed in Uruguayan schools with positive user feedback
- Current CPU-only deployment results in response times exceeding one minute

## Why This Works (Mechanism)

### Mechanism 1: Semi-Automated Vocabulary Curation Pipeline
- Claim: Pre-curated word lists and definitions enable immediate, safe activity generation for children without teacher review.
- Mechanism: Initial word lists → expansion via word embeddings and Simple Wikipedia term frequencies → manual curation by English teachers → tagged vocabulary database with definitions extracted from dictionaries and Wikipedia.
- Core assumption: Manual curation can adequately filter inappropriate content and ensure definitional accuracy at the vocabulary scale required.
- Evidence anchors:
  - [abstract]: "out-of-the-box games, generated from resources created semi-automatically and then manually curated"
  - [section 3.1.1]: "We used the average word embeddings of the words in each category... to obtain related words. Then we used the term frequencies obtained from Simple English Wikipedia to filter words with a similar difficulty level"
  - [corpus]: Related work exists (REVITA, Lärka) but corpus lacks direct comparative evidence on curation effectiveness; neighbor papers focus on LLM generation rather than hybrid curation approaches.
- Break condition: If vocabulary scale grows significantly, manual curation becomes unsustainable; if embedding-based expansion drifts from target difficulty level.

### Mechanism 2: Definition Extraction with Neural Classification
- Claim: Pattern-based candidate extraction followed by neural classification produces usable crossword clues from input texts.
- Mechanism: Manual linguistic patterns extract candidate <word, clue> pairs → DistilBERT classifier (trained on ~2,200 manually annotated examples, 0.78 F1) filters for clue quality → crossword/word-search generation.
- Core assumption: Hand-crafted patterns capture sufficient definition structures in educational texts.
- Evidence anchors:
  - [section 3.2]: "a large set of manual patterns is applied to generate a large number of candidate pairs, and in a second stage a classifier is applied"
  - [section 3.2]: "The best classifier found uses a DistilBERT model and has 0.78 F1-measure"
  - [corpus]: Generating Reading Comprehension Exercises with LLMs (FMR 0.58) suggests LLM alternatives, but this paper's hybrid approach predates widespread LLM adoption.
- Break condition: If input texts use definition structures not covered by patterns; if 0.78 F1 produces too many false positives for teacher review workload.

### Mechanism 3: Teacher-in-the-Loop Content Validation
- Claim: Mandatory review stage for text-generated activities prevents inappropriate or incorrect content from reaching children.
- Mechanism: Teacher submits text → NLP generates activities (Q&A, crosswords, story games) → Teacher reviews/edits → Activity published for students.
- Core assumption: Teachers have sufficient English proficiency and time to reliably identify errors and inappropriate content.
- Evidence anchors:
  - [abstract]: "providing a stage of review and edition of the generated content before use"
  - [section 4.2]: "until the generated contents of LLMs are fully controllable and reliable, we should put the teachers in the loop in order to curate the activities"
  - [corpus]: Van den Berghe et al. (2021) cited in paper reports "AI-tools for language learning have a positive effect on learners" but does not directly validate teacher-in-the-loop mechanisms.
- Break condition: If teacher workload or English proficiency is insufficient; if generation volume exceeds review capacity.

## Foundational Learning

- **Word Embeddings (Word2Vec/FastText)**
  - Why needed here: Used to expand vocabulary categories by finding semantically related words.
  - Quick check question: Given embeddings for "cat" and "dog" are similar, how would you use this to expand an "animals" vocabulary category?

- **Transformer-based Text Classification**
  - Why needed here: DistilBERT classifier determines if extracted <word, clue> pairs are suitable crossword clues.
  - Quick check question: Why might a classifier trained on crossword clues from one text type fail on another?

- **Semantic Role Labeling (SRL)**
  - Why needed here: Identifies answer candidates in Q&A generation by detecting agents, patients, and actions in sentences.
  - Quick check question: In "The fox ate the grapes," what semantic roles would SRL assign to "fox" and "grapes"?

- **Co-reference Resolution**
  - Why needed here: Resolves pronouns in sentences before image generation (e.g., "he" → "the fox").
  - Quick check question: Why would "he saw the grapes" fail to generate a useful image without co-reference resolution?

## Architecture Onboarding

- **Component map:**
  ```
  [Teacher Input] → [Text Processing Pipeline]
                              ↓
                    [Co-reference Resolution]
                              ↓
                    [Definition Extraction] ← [Curated Vocabulary DB]
                    [SRL-based Answer Selection]    [Image Bank]
                    [Sentence Ranking]
                              ↓
                    [Activity Generators]
                    - Crossword/Word-search
                    - Story Game (sentence ordering)
                    - Q&A Exercises
                              ↓
              [Pre-curated Mode]    [Teacher Review Mode]
                    ↓                        ↓
              [Student Interface] ← [Teacher Edit Interface]
  ```

- **Critical path:** For text-based activities: input text → co-reference resolution → definition/Q&A generation → teacher review → student use. Response time currently exceeds 1 minute without GPU; migration to better server targets "a few seconds."

- **Design tradeoffs:**
  - Desktop vs. web: Chose web for easier updates; accepted connectivity requirement.
  - Neural vs. rule-based: Hybrid approach (patterns + neural classifier) balances accuracy and interpretability.
  - Pre-curated vs. on-demand: Pre-curated for immediate use (no review needed); on-demand requires teacher review.
  - LLM generation vs. controlled vocabulary: LLMs deferred to experimental stage due to hallucination risks with children.

- **Failure signatures:**
  - Response times >60 seconds on current server without GPU (section 4.1).
  - Crossword clues that are too complex for ESL learners (mitigated by classifier, but 0.78 F1 allows false positives).
  - Incoherent images when pronouns not resolved (Figure 4 demonstrates this).
  - Inappropriate content from generative models if review stage bypassed.

- **First 3 experiments:**
  1. Benchmark response times on new server for Q&A generation pipeline; establish baseline before/after migration.
  2. Evaluate definition extraction classifier on a held-out test set of teacher-provided texts from different genres (stories vs. articles).
  3. Test co-reference resolution accuracy on story texts with multiple characters; measure impact on image generation coherence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can compute-intensive generative models (specifically text-to-image and large language models) be effectively integrated into a production educational platform operating without GPU resources?
- Basis in paper: [explicit] The authors state in Sections 3.3 and 4.1 that while they are experimenting with models like Stable Diffusion and GPT-2, high computing requirements and the lack of a GPU on the server are major obstacles to integrating these experiments into the live platform.
- Why unresolved: The current and planned server infrastructure relies on CPU processing, which creates latency (over a minute for some tasks) that makes modern generative models unfeasible for real-time use without optimization or hardware upgrades.
- What evidence would resolve it: Successful deployment of generative activities on the platform with response times acceptable for classroom use (e.g., seconds rather than minutes), or the implementation of model distillation/quantization techniques that allow these models to run on the available hardware.

### Open Question 2
- Question: How can educational platforms safely implement real-time interactive activities (such as chatbots) given the necessity of manual teacher curation to prevent hallucinations and inappropriate content?
- Basis in paper: [explicit] Section 4.2 explicitly notes that the current requirement for human verification "makes harder the development of activities which require a quick answer by the system, like a chatbot or other types of interactive games."
- Why unresolved: There is a fundamental conflict between the immediate latency required for interactive games and the safety protocols (manual review) currently needed to mitigate the risks of LLM hallucinations and harmful outputs when used by children.
- What evidence would resolve it: The development of a constrained generation framework or automated safety filter reliable enough to allow interactive chatbot activities to function without manual pre-review, along with safety metrics demonstrating robustness against harmful content.

### Open Question 3
- Question: What is the quantitative impact of this NLP-based platform on English language learning outcomes compared to traditional methods or other non-AI educational tools?
- Basis in paper: [inferred] The paper details the platform's development, features, and positive reception by users (teachers and students), but it does not present a rigorous quantitative evaluation of educational efficacy or learning gains.
- Why unresolved: While the authors cite general literature stating AI tools have positive effects, the specific evaluation of this platform is limited to qualitative feedback and usage interaction, leaving the actual improvement in language proficiency unmeasured in the text.
- What evidence would resolve it: A controlled study comparing pre- and post-test scores of students using the platform against a control group using standard materials, demonstrating statistically significant improvements in vocabulary retention or grammar skills.

## Limitations

- No quantitative evaluation of learning outcomes or user satisfaction - platform effectiveness relies on qualitative feedback rather than empirical validation
- Manual curation scalability concerns - no analysis of curation effort scaling or sustainability as vocabulary size increases
- Technical constraints - current CPU-only deployment results in response times exceeding one minute, significantly impacting usability

## Confidence

**High Confidence** in the technical architecture description and hybrid methodology combining semi-automated generation with manual curation. The pipeline components (word embeddings for vocabulary expansion, DistilBERT for classification, SRL for Q&A generation) are well-specified and represent established NLP approaches.

**Medium Confidence** in the claimed educational value and appropriateness for children. While the architecture addresses safety through review stages and controlled vocabulary, no direct evidence demonstrates these measures sufficiently prevent inappropriate content or achieve learning objectives.

**Low Confidence** in scalability and real-world impact. Without usage metrics, learning outcome data, or analysis of curation effort scaling, claims about the platform's effectiveness in rural schools remain unverified.

## Next Checks

1. **Response Time Benchmarking**: Measure and document end-to-end pipeline response times on the upgraded server infrastructure. Establish baseline performance metrics for each activity type (crosswords, Q&A, story games) to verify the claimed improvement from "more than one minute" to "a few seconds."

2. **Classifier Performance Validation**: Evaluate the DistilBERT definition extraction classifier on a held-out test set of teacher-provided texts from different genres (stories, articles, dialogues). Document precision-recall tradeoffs and false positive rates to assess whether 0.78 F1 provides acceptable review workload.

3. **Co-reference Resolution Impact Study**: Test co-reference resolution accuracy on story texts with multiple characters and measure the impact on image generation coherence. Compare outputs with and without co-reference resolution to quantify the improvement in pronoun handling for image generation.