---
ver: rpa2
title: 'CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale'
arxiv_id: '2507.06959'
source_url: https://arxiv.org/abs/2507.06959
tags:
- image
- preference
- chest
- question
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CheXPO addresses hallucinations in chest X-ray vision-language
  models by introducing a preference optimization strategy that combines confidence-similarity
  joint mining with counterfactual rationale. The method identifies hard examples
  through token-level confidence analysis of supervised fine-tuning failures and uses
  similarity-based retrieval to balance long-tailed data distributions.
---

# CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale

## Quick Facts
- **arXiv ID:** 2507.06959
- **Source URL:** https://arxiv.org/abs/2507.06959
- **Reference count:** 40
- **Primary result:** Achieves 8.93% relative performance gain using only 5% of SFT samples

## Executive Summary
CheXPO addresses hallucinations in chest X-ray vision-language models through a novel preference optimization strategy that combines confidence-similarity joint mining with counterfactual rationale generation. The method identifies hard examples by analyzing token-level confidence failures from supervised fine-tuning and balances long-tailed data distributions through similarity-based retrieval. By constructing counterfactual rationales as rejection responses, CheXPO reinforces fine-grained clinical supervision without requiring expert annotations. Experimental results demonstrate state-of-the-art performance across diverse clinical tasks while using significantly fewer labeled samples than traditional approaches.

## Method Summary
CheXPO introduces a two-stage optimization framework that first identifies challenging training examples through token-level confidence analysis of SFT failures, then applies similarity-based retrieval to address data imbalance issues. The core innovation lies in generating counterfactual rationales as rejection responses, which provide negative examples for preference learning without requiring expert annotation. This approach effectively reduces hallucinations by training the model to recognize when it should decline to make predictions, while simultaneously improving its ability to provide accurate clinical reasoning when confident.

## Key Results
- Achieves 8.93% relative performance improvement over baseline methods
- Reaches state-of-the-art performance using only 5% of supervised fine-tuning samples
- Demonstrates effectiveness across diverse clinical tasks and datasets

## Why This Works (Mechanism)
CheXPO's effectiveness stems from its dual approach to addressing hallucinations: first by identifying and properly weighting hard examples through confidence-similarity joint mining, and second by training the model to recognize its own uncertainty through counterfactual rationales. The confidence analysis captures token-level failures that indicate reasoning errors, while the similarity-based retrieval ensures balanced representation across the clinical spectrum. The counterfactual rationales serve as high-quality negative examples that teach the model when to decline predictions, effectively reducing false positives and improving overall reliability.

## Foundational Learning
- **Confidence-based mining**: Analyzes token-level prediction confidence to identify challenging examples where the model struggles with clinical reasoning. Why needed: Traditional accuracy metrics miss subtle reasoning failures that lead to hallucinations.
- **Similarity-based retrieval**: Uses embedding similarity to find representative examples from underrepresented clinical conditions. Why needed: Medical datasets often exhibit severe class imbalance that biases model performance.
- **Counterfactual rationale generation**: Creates rejection responses that serve as negative examples for preference optimization. Why needed: Expert-annotated counterfactuals are expensive to obtain but essential for reducing hallucinations.
- **Preference optimization**: Trains models using pairwise comparisons between correct and counterfactual responses. Why needed: Standard supervised learning cannot capture the nuanced decision boundaries needed for clinical reasoning.

## Architecture Onboarding

**Component map:** Input X-ray -> Feature Extractor -> Confidence Analyzer -> Similarity Retriever -> Counterfactual Generator -> Preference Optimizer -> Output

**Critical path:** Confidence analysis → Hard example identification → Similarity-based balancing → Counterfactual rationale generation → Preference optimization

**Design tradeoffs:** CheXPO trades computational overhead during training (for confidence analysis and similarity retrieval) against reduced annotation costs and improved generalization. The use of rejection responses as counterfactuals avoids expert annotation but may limit the diversity of negative examples compared to human-curated alternatives.

**Failure signatures:** 
- Over-reliance on confidence scores may miss subtle reasoning errors
- Similarity-based retrieval might not capture all clinically relevant variations
- Counterfactual rationales may be too generic if the rejection template is not sufficiently diverse

**3 first experiments:**
1. Ablation study removing confidence analysis to measure its contribution to performance gains
2. Comparison with human-annotated counterfactuals to validate the quality of generated rejections
3. Cross-dataset evaluation to test generalization beyond the training domain

## Open Questions the Paper Calls Out
The paper acknowledges that the effectiveness of counterfactual rationales generated through rejection responses needs further validation, particularly regarding their ability to capture the full complexity of clinical reasoning. Additionally, the generalizability of the confidence-similarity joint mining approach to different model architectures and clinical domains remains an open question.

## Limitations
- Performance improvements may be dataset-specific and not generalize to other clinical tasks
- Counterfactual rationale generation relies on the assumption that rejection responses adequately substitute for expert-annotated examples
- The method's effectiveness depends heavily on the quality of the initial supervised fine-tuning model

## Confidence
- **High confidence**: The methodological framework and implementation details are well-described and reproducible
- **Medium confidence**: Performance improvements on benchmark datasets are valid but may not generalize
- **Medium confidence**: The effectiveness of counterfactual rationales for hallucination reduction needs further validation

## Next Checks
1. Conduct cross-dataset evaluation to verify performance gains generalize beyond the training domain
2. Perform expert clinician review of counterfactual rationales to assess clinical validity and reasoning quality
3. Test the method with different VLM architectures to evaluate robustness and transferability of the approach