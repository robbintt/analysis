---
ver: rpa2
title: 'Poster: Recognizing Hidden-in-the-Ear Private Key for Reliable Silent Speech
  Interface Using Multi-Task Learning'
arxiv_id: '2512.16518'
source_url: https://arxiv.org/abs/2512.16518
tags:
- silent
- user
- authentication
- speech
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HEar-ID uses commodity active noise-canceling earbuds to perform\
  \ silent speech interface (SSI) and user authentication simultaneously. It captures\
  \ both ultrasonic reflections (17.5\u201323 kHz) from the ear canal and low-frequency\
  \ whisper audio (0\u201311 kHz) in sliding 426-ms windows, extracts AR coefficients\
  \ for the ultrasonic channel and mel-spectrograms for the whisper channel, and processes\
  \ them through a shared encoder (TCN \u2192 Bi-GRU \u2192 MLP)."
---

# Poster: Recognizing Hidden-in-the-Ear Private Key for Reliable Silent Speech Interface Using Multi-Task Learning

## Quick Facts
- arXiv ID: 2512.16518
- Source URL: https://arxiv.org/abs/2512.16518
- Reference count: 40
- 11 participants achieved 90.25% Top-1 spelling accuracy on 50 words for 8 users with TPR of 81.76% and FPR of 3.2%

## Executive Summary
HEar-ID is a multi-task learning system that simultaneously performs silent spelling recognition and user authentication using commodity ANC earbuds. The system captures ultrasonic reflections (17.5-23 kHz) from the ear canal and low-frequency whisper audio (0-11 kHz) in sliding windows, then processes them through a shared encoder with contrastive learning to align modalities while extracting user-specific features. Evaluated on 11 participants spelling 50 words across multiple sessions, the system achieved strong authentication performance (TPR of 81.76%, FPR of 3.2%) and word recognition accuracy of 90.25% for 8 users, though performance degraded for 3 users due to inconsistent earbud placement.

## Method Summary
HEar-ID uses commodity ANC earbuds to capture dual-modal audio streams: ultrasonic reflections from 17.5-23 kHz and low-frequency whisper audio from 0-11 kHz. The system aligns these streams using OFDM-based cross-correlation, extracts AR coefficients from ultrasonic signals and mel-spectrograms from whisper audio, and processes both through a shared encoder (TCN → Bi-GRU → MLP). A contrastive learning module aligns genuine-user whisper-ultrasonic pairs while repelling impostor embeddings, and a multi-task learning architecture branches into an authentication head (angular triplet loss) and a CTC-based SSI head. The system is trained end-to-end with weighted losses prioritizing authentication, and authentication thresholds are calibrated using Youden's J statistic.

## Key Results
- Achieved 90.25% Top-1 word accuracy on 50 words for 8 out of 11 users
- Strong authentication performance with TPR of 81.76% and FPR of 3.2%
- One user achieved exceptional authentication with TPR of 99.9% and FPR of 3.4%, though spelling accuracy was below 10%
- 3 out of 11 users showed degraded performance due to inconsistent earbud placement across sessions

## Why This Works (Mechanism)

### Mechanism 1: Ear Canal Structural Uniqueness as Biometric Key
The physical structure of each individual's ear canal creates distinct acoustic propagation paths that encode both utterance content and speaker identity. When users silently articulate, facial muscle movements cause subtle ear canal deformations that modulate ultrasonic reflections in a user-specific way while simultaneously affecting low-frequency whisper audio. This creates a dual-modal signal pair carrying correlated information about both what was said and who said it. The core assumption is that ear canal anatomy varies sufficiently across individuals to produce distinguishable acoustic signatures consistent across re-wearing sessions.

### Mechanism 2: Cross-Modal Contrastive Alignment (CLWUM)
The CLWUM module uses contrastive learning to map whisper and ultrasonic signals into a shared embedding space, creating a user-specific "private key" that repels impostor samples. Paired whisper-ultrasonic segments from the genuine user and attacker samples in each batch are processed through a contrastive loss that maximizes cosine similarity between true pairs while minimizing similarity between mismatched pairs. This forces the encoder to learn features that are simultaneously cross-modally consistent and user-discriminative, assuming facial movements during silent speech induce correlated perturbations in both signal types that impostors cannot easily replicate.

### Mechanism 3: Multi-Task Learning with Angular Triplet Loss for Authentication
Jointly training spelling recognition and authentication with angular triplet loss creates embeddings where genuine-user whisper-ultrasonic pairs are pulled closer than any impostor pair, establishing a clear decision boundary. The authentication head computes embeddings for whisper and ultrasonic streams separately, then calculates angular distance. The angular triplet loss with margin m=11.45 (≈30°) ensures the angle between genuine pairs is smaller than the angle between genuine-attacker pairs by at least the margin. During inference, cosine similarity above a calibrated threshold grants access, assuming the shared encoder can produce embeddings useful for both word discrimination and user discrimination without catastrophic interference.

## Foundational Learning

- **Concept: Contrastive Learning (especially InfoNCE-style losses)**
  - Why needed here: CLWUM uses a contrastive loss formulation to align whisper-ultrasonic pairs. Understanding temperature τ affects the softness of the similarity distribution is essential for debugging convergence issues.
  - Quick check question: If increasing τ from 0.7 to 1.0, would the model become more or less sensitive to hard negatives?

- **Concept: Connectionist Temporal Classification (CTC)**
  - Why needed here: The SSI head uses CTC loss to train on letter sequences without frame-level alignment. Understanding the blank token's role and beam search decoding is essential for improving word accuracy.
  - Quick check question: Why does CTC require the blank token, and what happens if the model outputs consecutive identical letters (e.g., "hello")?

- **Concept: Angular Margin Losses (e.g., ArcFace, SphereFace)**
  - Why needed here: The authentication head uses angular triplet loss with a margin in angle space. This is geometrically different from Euclidean triplet loss and affects how the embedding space is structured.
  - Quick check question: Why might angular distance be preferred over Euclidean distance for cosine-similarity-based retrieval tasks?

## Architecture Onboarding

- **Component map:** Raw Audio (48kHz) → OFDM Alignment (coarse via cross-correlation → fine via phase minimization) → Ultrasonic Band (17.5-23kHz → 2nd-order diff → 200 AR coeffs) and Whisper Band (0-11kHz → Mel-spectrogram) → Shared Encoder (3×TCN → 2×Bi-GRU → MLP) → d-dimensional embeddings → CLWUM Head (contrastive + angular triplet) and SSI Head (concatenate → MLP → softmax)

- **Critical path:** The OFDM alignment step is fragile - if coarse alignment fails to find the correct lag, all downstream features are misaligned. The fine alignment (±25 samples phase correction) assumes the coarse alignment is within the correct symbol.

- **Design tradeoffs:** Authentication is weighted 5× more than contrastive alignment (α=0.1, β=0.5, γ=0.3), prioritizing security over spelling accuracy. The SSI head is trained only on genuine-user data, improving per-user accuracy but requiring retraining for each new user. Window size (426ms, 85ms stride) is long enough to capture word-level articulation but may introduce latency.

- **Failure signatures:** Low TPR with high FPR indicates threshold thr is too high or angular margin is insufficient for that user's ear canal consistency. Good authentication but poor spelling (e.g., S5) suggests the shared encoder has learned user-specific features but not content-discriminative features. Poor performance for specific users (S9, S10) likely indicates inconsistent earbud placement across sessions.

- **First 3 experiments:** 1) Ablation on loss weights by varying α, β, γ systematically to find the Pareto frontier between TPR and Top-1 accuracy. 2) Cross-session consistency test by training on sessions 1-3 and testing on session 4, reporting performance variance across which session is held out. 3) Imminent impostor attack simulation by adding a "close impostor" condition where impostors attempt to mimic the genuine user's articulation style or use voice conversion.

## Open Questions the Paper Calls Out

### Open Question 1
Can generative models effectively produce synthetic data to expand the recognizable lexicon without compromising user authentication accuracy? The conclusion states future work will "expand lexicons and refine continuous verification, leveraging methods including a generative model to produce realistic synthetic data." This remains untested as the current system is evaluated on a fixed set of 50 words.

### Open Question 2
How can the system mitigate performance degradation caused by inconsistent earbud placement or "idiosyncratic" articulation styles? Results show 3 of 11 users exhibited near-zero accuracy or True Positive Rates, attributed to "idiosyncratic sensor placement" and "unclear articulation." The current signal alignment and shared encoder assume a relatively stable acoustic path, failing for users who cannot maintain consistent earbud coupling.

### Open Question 3
Is the system robust to truly silent speech, or does the architecture depend on the low-frequency artifacts present in the "subtle voice" of untrained users? The authors treat silent speech as "equivalent to whispering" because participants "still make a subtle voice," and the system extracts features from a low-frequency whisper stream (0-11kHz). If a user successfully suppresses all audible vocalization, the whisper stream may contain insufficient information for the contrastive learning alignment.

## Limitations

- Small sample size of 11 participants limits generalizability, with 3 users showing degraded performance due to inconsistent earbud placement
- System requires whisper-level articulation rather than pure silent speech, creating a dependency on low-frequency signal content
- Critical architecture details missing including TCN configuration, exact layer sizes, and training hyperparameters

## Confidence

- **High Confidence**: Dual-modal sensing approach (ultrasonic + whisper) is technically sound and contrastive learning framework is well-established. Performance metrics are internally consistent with methodology.
- **Medium Confidence**: Ear canal biometrics providing reliable user discrimination is plausible but limited by small sample size and lack of cross-dataset validation. Performance degradation for specific users suggests real limitations.
- **Low Confidence**: Assertion that system works for "pure" silent speech is questionable given dependency on whisper-level signals. Claim that angular triplet loss is superior lacks comparative ablation.

## Next Checks

1. **Cross-Validation Robustness Test**: Perform 4-fold cross-validation for each participant (each session held out once) and report variance in TPR/FPR across folds to quantify how much performance depends on which sessions are used for training vs. testing.

2. **True Silent Articulation Experiment**: Repeat protocol with users instructed to articulate without any subvocalization and measure degradation in both recognition accuracy and authentication TPR to quantify whisper signal dependency.

3. **User-Independent SSI Head Evaluation**: Retrain system with SSI head using all user data (not just genuine-user data) and compare recognition accuracy and authentication performance to quantify cost of current user-dependent approach.