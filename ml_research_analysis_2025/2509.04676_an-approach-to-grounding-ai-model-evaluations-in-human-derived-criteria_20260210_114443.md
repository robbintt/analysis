---
ver: rpa2
title: An Approach to Grounding AI Model Evaluations in Human-derived Criteria
arxiv_id: '2509.04676'
source_url: https://arxiv.org/abs/2509.04676
tags:
- benchmarks
- skills
- https
- test
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study proposes augmenting traditional AI benchmarks with\
  \ human-derived evaluation criteria to improve interpretability and applicability.\
  \ Focusing on physical world modeling, the researchers conducted interviews and\
  \ surveys to identify key cognitive skills\u2014such as Prioritization, Memorizing,\
  \ Discerning, and Contextualizing\u2014critical for AI and human reasoning."
---

# An Approach to Grounding AI Model Evaluations in Human-derived Criteria

## Quick Facts
- arXiv ID: 2509.04676
- Source URL: https://arxiv.org/abs/2509.04676
- Reference count: 0
- Key outcome: Proposed approach augments traditional AI benchmarks with human-derived evaluation criteria to improve interpretability and applicability

## Executive Summary
This study proposes grounding AI model evaluations in human-derived cognitive criteria rather than relying solely on traditional benchmarks. By interviewing participants about their reasoning processes on physical world modeling tasks and surveying larger samples about skill importance, the researchers identified key cognitive skills like Prioritization, Memorizing, Discerning, and Contextualizing. The approach aims to create more interpretable evaluation frameworks that align AI capabilities with human cognitive processes, potentially reducing benchmark saturation and revealing expectation gaps between users and AI systems.

## Method Summary
The methodology uses a two-phase mixed-methods approach: first, 60-minute interviews with 8 introspective participants who explain their reasoning while solving benchmark questions and evaluating AI responses; second, a large-scale survey (n=1,515) where participants rate the importance of identified cognitive skills on a 5-point scale. The study focuses on video question-answering benchmarks (Perception Test and OpenEQA) testing physical world modeling. Skills identified through interview coding are operationalized into micro-skill components (e.g., Focus, Reprioritization, Distraction Filtering) that can augment existing benchmark evaluation.

## Key Results
- Identified four key cognitive skills critical for both AI and human reasoning in physical world modeling: Prioritization, Memorizing, Discerning, and Contextualizing
- Participants perceived AI as lacking interpretive and empathetic skills while holding high performance expectations
- Users often assumed AI responses were correct even when they disagreed, indicating over-reliance patterns
- The approach provides actionable guidelines for creating more human-aligned evaluation frameworks

## Why This Works (Mechanism)

### Mechanism 1
Decomposing benchmark tasks into human-identified cognitive micro-skills improves interpretability by revealing which reasoning capabilities contribute to performance. Interview participants introspect on their reasoning process, researchers code responses to identify skills, and surveys quantify perceived importance. Micro-skills are operationalized as evaluation questions that augment existing benchmarks. Core assumption: human introspection accurately reveals actual cognitive operations. Evidence: abstract mentions identifying skills critical for both AI and human reasoning; section 3.1 proposes questions that decompose performance into interpretable segments. Break condition: if human introspection misrepresents actual cognitive processes.

### Mechanism 2
User-centered evaluation reveals expectation gaps that traditional benchmarks miss. Participants evaluate both their own reasoning and AI responses, revealing where user expectations diverge from AI capabilities. This informs which skills benchmarks should prioritize measuring. Core assumption: user expectations reflect genuine deployment requirements. Evidence: abstract notes participants perceive AI as lacking interpretive skills yet hold high expectations; section 3 documents participants doubting their own judgment when disagreeing with AI. Break condition: if user expectations are shaped by marketing rather than functional needs.

### Mechanism 3
Micro-skill decomposition creates evaluation axes harder to game via shortcut learning. Aggregate benchmark scores can be achieved through shortcuts, but decomposing into fine-grained skills forces evaluation of specific reasoning steps. Each micro-skill has clearer success criteria, making saturation detectable. Core assumption: micro-skills are sufficiently independent to resist correlation with aggregate scores. Evidence: section 3.1 describes micro-skill components for Prioritization; section 1 references shortcut learning and saturation limitations. Break condition: if micro-skills correlate strongly with each other or aggregate scores.

## Foundational Learning

### Concept: Benchmark saturation
Why needed here: The paper's core motivation is that traditional benchmarks plateau in meaningfulness. Understanding saturation—when models achieve high scores through shortcuts rather than genuine capability—is essential to evaluating whether the proposed approach helps. Quick check question: If a model scores 95% on a video QA benchmark but fails on adversarially-selected examples from the same distribution, what does this suggest about saturation?

### Concept: Mixed-methods research (qualitative → quantitative)
Why needed here: The methodology uses interviews (n=8) for discovery and surveys (n=1,515) for validation. Without understanding why both phases are needed, one might skip the resource-intensive interview phase. Quick check question: Why can't you jump directly to a large-scale survey without conducting interviews first?

### Concept: Physical world modeling
Why needed here: The approach is demonstrated on Perception Test and OpenEQA, which test video understanding, spatial reasoning, and temporal memory. These domains require multi-step reasoning that naturally decomposes into skills. Quick check question: Would this approach work as well for a benchmark testing factual knowledge retrieval? Why or why not?

## Architecture Onboarding

### Component map:
Interview Phase (n=8, 60min each) -> code reasoning descriptions -> Skill Identification (6 skills initially) -> construct survey questions -> Survey Phase (n=1,515) -> quantify importance -> Skill Prioritization (Prioritization/Memorizing for Perception Test; Discerning/Contextualizing for OpenEQA) -> decompose into components -> Micro-skill Definition (e.g., Focus, Reprioritization, Distraction Filtering) -> write evaluation questions -> Benchmark Augmentation

### Critical path:
1. Recruiting introspective interview participants (screen for ability to articulate reasoning)
2. Coding interview transcripts to identify candidate skills
3. Survey construction that avoids leading respondents toward predetermined skills
4. Micro-skill operationalization—translating abstract skills into testable question formats

### Design tradeoffs:
- Existing benchmark grounding vs. foundational cognitive research: Paper explicitly trades theoretical completeness for practical applicability—skills are those relevant to current benchmarks, not a complete theory of physical reasoning
- Small interview sample (n=8) vs. generalizability: Authors acknowledge this limitation; sufficient for proof-of-concept but requires validation
- Perceived importance vs. actual importance: Survey measures what users believe matters, not what actually predicts performance

### Failure signatures:
- Uniform survey ratings across all skills → poor discriminant validity in question design
- High correlation between micro-skill scores and aggregate benchmark scores → no incremental value added
- Low inter-rater reliability in skill coding → unclear skill definitions
- Participants unable to articulate reasoning despite screening → recruitment criteria need refinement

### First 3 experiments:
1. Validate micro-skill questions against held-out model behavior: Do models that score higher on "Reprioritization" questions actually perform better on dynamic-scene video tasks?
2. Test cross-domain transfer: Apply the same interview→survey→micro-skill pipeline to a different benchmark category (e.g., code generation) to assess methodology generalizability.
3. Measure inter-rater reliability: Have multiple researchers independently code interview responses; report Cohen's kappa for skill identification.

## Open Questions the Paper Calls Out

### Open Question 1
Can the identified micro-skills (e.g., Focus, Distraction Filtering) be successfully operationalized into quantitative evaluation metrics that are less prone to saturation than traditional benchmarks? Basis: section 3.1 states future work will "formalize these evaluation axes into validated questions" and proposes they could be "potentially less prone to saturation." Unresolved because current study only identified and categorized skills via surveys without constructing or validating new scoring metrics. Evidence needed: follow-up study demonstrating meaningful score distributions that correlate with human judgment better than existing benchmark scores.

### Open Question 2
To what extent do the empirically derived skills (Prioritization, Memorizing, Discerning, Contextualizing) align with established constructs in cognitive science and decision theory? Basis: section 4.1 notes that a "comprehensive literature review will help map the skills identified... to existing knowledge in cognitive science." Unresolved because skills were generated inductively from participant interviews without a priori theoretical mapping. Evidence needed: theoretical mapping or experimental validation showing these specific skills correspond to recognized cognitive processes.

### Open Question 3
Does the proposed human-centered evaluation framework generalize effectively to AI domains outside of physical world modeling? Basis: section 4.1 cites "demonstrate domain adaptability" as necessary while section 1 claims methods "can be applied to various research domains." Unresolved because methodology was tested exclusively on video-based physical world modeling tasks. Evidence needed: successful application of same pipeline to derive valid evaluation criteria in a distinct domain, such as code generation or logical reasoning.

## Limitations
- Small interview sample (n=8) limits generalizability of skill identification
- No validation that micro-skills predict model capabilities beyond aggregate scores
- No evidence that human expectations reflect genuine deployment requirements versus misconceptions
- Limited transparency on AI model used and exact responses shown to participants

## Confidence
- High Confidence: The core methodology (mixed-methods interview → survey → micro-skill operationalization) is clearly specified and feasible. The observed over-reliance patterns and expectation gaps are empirically documented.
- Medium Confidence: The identified cognitive skills may not be comprehensive or may overlap significantly. The operationalization of micro-skills into specific questions remains largely unspecified.
- Low Confidence: The claim that micro-skill decomposition inherently resists shortcut learning lacks empirical validation. The translation from human-derived criteria to improved benchmark performance is hypothesized but not demonstrated.

## Next Checks
1. Validate micro-skill discriminability: Test whether micro-skill questions produce differentiated scores across models with varying capabilities.
2. Cross-domain generalizability: Apply the same interview→survey→micro-skill pipeline to a fundamentally different benchmark type.
3. Expert evaluation of skill taxonomy: Have domain experts independently review skill definitions and micro-skill operationalizations for face validity.