---
ver: rpa2
title: Using deep learning for predicting cleansing quality of colon capsule endoscopy
  images
arxiv_id: '2601.13412'
source_url: https://arxiv.org/abs/2601.13412
tags:
- pruning
- images
- https
- deep
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of deep learning for predicting
  cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500
  images labeled on the Leighton-Rex scale by 14 clinicians, a ResNet-18 model was
  trained with structured pruning techniques.
---

# Using deep learning for predicting cleansing quality of colon capsule endoscopy images

## Quick Facts
- arXiv ID: 2601.13412
- Source URL: https://arxiv.org/abs/2601.13412
- Reference count: 40
- Primary result: ResNet-18 pruning achieved 88% cross-validation accuracy with 79% sparsity, and HnLTS calibration improved external test accuracy from 61.61% to 77.10%.

## Executive Summary
This study investigates deep learning for predicting colon capsule endoscopy (CCE) cleansing quality using the Leighton-Rex scale. The authors developed an iterative structured pruning approach that reduces model size while maintaining high accuracy, and implemented explainability evaluation using the ROAD framework with multiple CAM methods. A novel hybrid temperature scaling calibration technique (HnLTS) was employed to adapt the pruned model for external datasets. The work demonstrates that pruning can improve generalization while reducing computational cost, though it identifies challenges in maintaining accuracy across all cleansing quality categories during calibration.

## Method Summary
The method uses ResNet-18 for 4-class classification (Poor, Fair, Good, Excellent) of CCE cleansing quality. The model is trained on 500 images with mode labels from 14 clinicians, using stratified 10-fold cross-validation and augmentation. Iterative structured pruning removes 20% of convolutional channels per step (13 steps total) using magnitude-based L1-norm pruning while preserving the first convolutional layer. Explainability is evaluated using ROAD with Grad-CAM, Grad-CAM++, Eigen-CAM, and Ablation-CAM. A hybrid adaptive temperature scaling (HnLTS) network calibrates the pruned model for external datasets by dynamically adjusting temperature based on prediction entropy and logits.

## Key Results
- Iterative pruning achieved 88% cross-validation accuracy with 79% sparsity (step 7), improving from the original 84% accuracy model
- ROAD evaluation showed explainability method effectiveness varies by category, with Random-CAM performing comparably to genuine methods for "Poor" cleansing images
- HnLTS calibration improved external test accuracy from 61.61% to 77.10% (using hidden units of size 64)
- Accuracy degradation began after step 7, with step 11 reaching 91% sparsity but returning to baseline 84% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative structured pruning can improve model generalization while reducing computational cost for CCE cleansing quality classification.
- Mechanism: Magnitude-based L1-norm pruning removes entire channels with lowest weights across 13 steps (20% per step), followed by fine-tuning to recover accuracy. This reduces overfitting by eliminating redundant parameters that memorize noise rather than learn transferable features.
- Core assumption: Low-magnitude channels contribute less to model performance and can be removed without degrading the learned representations critical for cleansing quality assessment.
- Evidence anchors:
  - [abstract] "achieving 88% cross-validation accuracy with 79% sparsity, demonstrating significant efficiency improvements from the original 84% accuracy model"
  - [Page 9] "mean accuracies for pruning step 2 to 7 are more than that mean accuracy at step 0... pruning can initially improve the performance of deep learning models by reducing overfitting"
  - [corpus] Weak direct corpus support for pruning-specific gains in capsule endoscopy; related papers focus on detection/segmentation tasks rather than efficiency optimization
- Break condition: Aggressive pruning beyond step 7 (≥83% sparsity) causes accuracy degradation as critical feature extractors are removed; first convolutional layer must be preserved to maintain initial feature extraction capability.

### Mechanism 2
- Claim: ROAD framework with CAM-based methods provides faithfulness evaluation but shows category-dependent reliability for medical image explainability.
- Mechanism: ROAD systematically removes important pixels (MoRF strategy) or least important pixels (LeRF strategy) identified by CAM methods and measures confidence drop. Higher S_ROAD scores indicate better attribution alignment with model decision-making.
- Core assumption: Pixel removal that causes significant confidence drops indicates the attribution method correctly identifies features the model actually uses for classification.
- Evidence anchors:
  - [Page 5-6] "ROAD removes parts of the input (e.g., pixels in an image) and replaces them using a linear combination of neighboring values with added noise to prevent information leakage"
  - [Page 11-12] "for pruning steps 5, 6, 7, 8, 10, 12, and 13, it is either comparable or more than the mean scores of other metrics" for "Poor" category, indicating Random-CAM performs similarly to genuine methods
  - [corpus] No corpus validation of ROAD effectiveness for capsule endoscopy; related papers do not evaluate explainability metrics systematically
- Break condition: When Random-CAM scores approach or exceed genuine CAM method scores (observed in "Poor" category), the faithfulness metric becomes unreliable for distinguishing meaningful from spurious attributions.

### Mechanism 3
- Claim: Hybrid adaptive temperature scaling (HnLTS) calibrates pruned models for external datasets by dynamically adjusting confidence based on prediction entropy and logit magnitudes.
- Mechanism: HnLTS combines linear temperature scaling with entropy-based scaling using a learned function that adapts temperature per sample: T_HnLTS(z) = σ_SP(w_L·z + w_H·log H̄(z)). Higher entropy predictions receive different temperature adjustments than confident predictions.
- Core assumption: Domain shift between training and external CCE datasets affects calibration non-uniformly across different prediction confidence levels, requiring sample-adaptive rather than global temperature adjustment.
- Evidence anchors:
  - [Page 6] "combines two approaches: linear temperature scaling and entropy based temperature scaling... By integrating both the logits and the uncertainty, HnLTS dynamically adapts the temperature"
  - [Page 14] "at step 7 the uncalibrated mean test accuracy is 61.61%, for hidden unit sizes, h = 16, 32, and 64 the calibrated models give test accuracies of 71.04%, 72.39%, and 77.10%, respectively"
  - [corpus] No corpus evidence for HnLTS specifically; one related paper mentions self-supervised learning for dataset challenges but not calibration
- Break condition: Calibration improves "Good" and "Excellent" categories at expense of "Poor" and "Fair" accuracy (Page 15), creating category-specific trade-offs that may not be acceptable for clinical use where under-cleansing detection is critical.

## Foundational Learning

- Concept: **Structured vs. Unstructured Pruning**
  - Why needed here: The paper uses structured pruning (removing entire channels) rather than individual weights, which is essential for actual computational speedup on hardware—a key deployment consideration for resource-constrained medical devices.
  - Quick check question: If you prune 50% of individual weights randomly vs. 50% of channels, which approach gives better inference speedup on a standard GPU?

- Concept: **Model Calibration and Temperature Scaling**
  - Why needed here: Accuracy alone doesn't reflect whether model confidence is trustworthy for clinical decisions. A model can be 80% accurate but systematically overconfident on wrong predictions, leading to dangerous false assurances in medical contexts.
  - Quick check question: If a model predicts "Excellent cleansing" with 95% confidence but is correct only 70% of the time, what is the calibration error and why does it matter for clinical workflow?

- Concept: **Faithfulness-based Explainability Evaluation**
  - Why needed here: Visual heatmaps can be misleading—they may highlight visually salient regions rather than features the model actually uses. ROAD tests whether removing highlighted regions actually affects predictions, providing objective evaluation beyond visual inspection.
  - Quick check question: If Grad-CAM highlights a polyp region but removing those pixels doesn't change the model's prediction, what does this suggest about the explanation's faithfulness?

## Architecture Onboarding

- Component map:
  Input Pipeline (circular/border masking → normalization → augmentation) → Backbone (ResNet-18 with first conv layer preserved) → Classification Head (GAP → dropout → FC) → Pruning Loop (13 steps, 20% per step) → Explainability (CAM methods → ROAD) → Calibration (HnLTS network → temperature scaling)

- Critical path:
  1. Training stability with small dataset (500 images): stratified 10-fold CV and augmentation are essential to prevent overfitting before pruning begins
  2. Pruning step selection: step 7 (79% sparsity, 88% accuracy) represents optimal trade-off; later steps degrade performance
  3. Calibration for deployment: must validate category-specific impacts since "Poor" detection accuracy may decrease

- Design tradeoffs:
  - **Sparsity vs. accuracy**: Beyond step 7, accuracy drops accelerate (91% sparsity at step 11 yields 84% accuracy—same as original with more complexity)
  - **Calibration hidden units**: Larger h (64) improves overall accuracy but requires more calibration data; smaller h (16) is more data-efficient but less effective
  - **Explainability fidelity vs. computation**: ROAD requires multiple inference passes per image (removing pixels iteratively), making it unsuitable for real-time clinical use

- Failure signatures:
  - **Layer collapse**: If pruning rate is too aggressive or first layer is pruned, entire network becomes non-functional (Page 4)
  - **Category imbalance in calibration**: "Poor" and "Fair" accuracy decreases post-calibration—critical failure for clinical safety where under-cleansing must be detected
  - **Random-CAM equivalence**: When genuine CAM methods score similarly to random baselines (observed for "Poor" category), explanations are unreliable for clinical validation

- First 3 experiments:
  1. **Baseline establishment**: Train unpruned ResNet-18 on 500-image dataset with 10-fold stratified CV, measure per-class accuracy and calibration error before any pruning to establish reference performance.
  2. **Pruning sweep**: Run iterative pruning for steps 0-13, measuring both mean accuracy and per-category accuracy at each step to identify optimal pruning level that preserves "Poor"/"Fair" detection (clinically critical categories).
  3. **Cross-dataset validation with calibration**: Test pruned model (step 7) on external dataset (396 images), apply HnLTS calibration with different hidden unit sizes, and specifically report per-category confusion matrices to verify calibration doesn't sacrifice safety-critical category performance for overall accuracy gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can sparse training techniques (introducing sparsity during training) outperform the iterative post-hoc pruning methods used in this study?
- Basis in paper: [explicit] The authors state in the Future Work section: "exploring advanced pruning techniques such as sparse training... i.e., introducing sparsity in the model's weights... during training."
- Why unresolved: The current study only evaluated a "train, prune, and fine-tune" loop, leaving the potential of dynamic sparse training untested for CCE images.
- What evidence would resolve it: A comparative study showing accuracy and sparsity trade-offs between the current iterative pruning and sparse training on the Leighton-Rex dataset.

### Open Question 2
- Question: How can model calibration be optimized to improve accuracy across all classes without sacrificing performance in the "Poor" category?
- Basis in paper: [inferred] The Discussion notes that HnLTS calibration improved "Fair," "Good," and "Excellent" categories but "comes at the expense of a decrease in classification accuracy for the categories 'Poor'."
- Why unresolved: The current calibration method shifted the decision boundary in a way that penalized the most critical failure mode (Poor cleansing), suggesting the method is not yet clinically robust.
- What evidence would resolve it: A calibration technique that yields a statistically significant increase in F1-score for the "Poor" class while maintaining the overall accuracy gains observed with HnLTS.

### Open Question 3
- Question: Do ROAD-based explainability metrics provide reliable clinical interpretability for the "Poor" cleansing category?
- Basis in paper: [inferred] The Results section highlights that for "Poor" images, Random-CAM scores were often "comparable or more than the mean scores of other metrics."
- Why unresolved: The failure of explainability metrics to consistently outperform random baselines for specific classes suggests they may not be faithfully representing the model's decision-making process.
- What evidence would resolve it: A qualitative study with clinicians confirming that the regions highlighted by Grad-CAM/Eigen-CAM for "Poor" images visually correspond to the actual artifacts (fecal matter/bubbles) causing the classification.

## Limitations

- The study lacks independent validation of ROAD framework effectiveness specifically for CCE cleansing quality assessment
- Category-specific accuracy trade-offs during calibration (improving "Good/Excellent" while degrading "Poor/Fair" detection) raise clinical safety concerns
- Several critical hyperparameters (learning rate, batch size, training epochs, augmentation parameters, HnLTS network architecture) are unspecified, preventing exact reproduction

## Confidence

- **High confidence**: ResNet-18 pruning mechanics (magnitude-based L1-norm channel pruning with first-layer preservation), iterative pruning procedure (13 steps at 20% per step), basic ROAD framework principles (MoRF/LeRF strategies)
- **Medium confidence**: Accuracy improvements from pruning (88% at 79% sparsity), HnLTS calibration effectiveness (61.61% → 77.10%), category-specific explainability findings (Random-CAM equivalence for "Poor" category)
- **Low confidence**: Generalizability of calibration to other CCE datasets, clinical acceptability of category-specific accuracy trade-offs, explainability metric reliability in real-world deployment

## Next Checks

1. **Clinical safety validation**: Test calibrated model on external dataset with explicit per-class confusion matrices, particularly measuring whether "Poor" cleansing detection accuracy remains above clinical safety thresholds (≥85%).

2. **ROAD reproducibility**: Implement ROAD evaluation with multiple runs on the same dataset to establish variance in S_ROAD scores, particularly for the "Poor" category where Random-CAM equivalence was observed.

3. **Generalization testing**: Evaluate HnLTS calibration across multiple independent CCE datasets from different clinical centers to confirm the 61.61% → 77.10% improvement is not dataset-specific.