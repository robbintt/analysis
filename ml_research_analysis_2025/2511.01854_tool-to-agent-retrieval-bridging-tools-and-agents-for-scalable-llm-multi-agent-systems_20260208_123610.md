---
ver: rpa2
title: 'Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent
  Systems'
arxiv_id: '2511.01854'
source_url: https://arxiv.org/abs/2511.01854
tags:
- retrieval
- arxiv
- tool
- tools
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tool-to-Agent Retrieval introduces a unified framework for large
  language model multi-agent systems that jointly embeds tools and their parent agents
  in a shared vector space, linked through explicit metadata relationships. This approach
  enables granular retrieval decisions by preserving fine-grained tool capabilities
  while retaining agent context, avoiding context dilution from coarse agent summaries.
---

# Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems

## Quick Facts
- arXiv ID: 2511.01854
- Source URL: https://arxiv.org/abs/2511.01854
- Reference count: 40
- Key outcome: Achieves 19.4% improvement in Recall@5 and 17.7% in nDCG@5 over state-of-the-art agent retrievers

## Executive Summary
Tool-to-Agent Retrieval introduces a unified framework for large language model multi-agent systems that jointly embeds tools and their parent agents in a shared vector space, linked through explicit metadata relationships. This approach enables granular retrieval decisions by preserving fine-grained tool capabilities while retaining agent context, avoiding context dilution from coarse agent summaries. The method supports both tool-level and agent-level retrieval through a single-pass process, allowing the system to surface relevant tools without losing their surrounding agent context. Evaluated across eight embedding models on the LiveMCPBench benchmark, Tool-to-Agent Retrieval achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over previous state-of-the-art agent retrievers.

## Method Summary
The method constructs a unified catalog C = C_T ∪ C_A where tools and agents coexist in a shared vector space. Queries are matched against both corpora in a single semantic search pass, with retrieved tools linked to their parent agents through owner(T) = A metadata. The system retrieves top-N entities, then for any tool in results, traverses to its parent agent before deduplication. This preserves execution context (authentication, policies) that pure tool-only retrieval discards. The approach supports both tool-level and agent-level retrieval without committing upfront to either strategy, allowing queries to naturally surface either specific tools or broader agent bundles depending on semantic fit.

## Key Results
- 19.4% improvement in Recall@5 over state-of-the-art agent retrievers
- 17.7% improvement in nDCG@5 on LiveMCPBench benchmark
- Consistent performance across 8 embedding models, with Amazon Titan v2 showing +28% relative gain vs MCPZero
- 39.13% of retrieved top-K items originate from agent corpus, 34.44% of matched tools trace back to agents

## Why This Works (Mechanism)

### Mechanism 1
Jointly embedding tools and agents in a unified vector space improves retrieval accuracy by enabling semantic matching at both granularities simultaneously. The system constructs a unified catalog where tools and agents coexist, allowing queries to match against both corpora in a single semantic search pass. When a tool matches, the system traverses its owner metadata to return the parent agent, preserving execution context.

### Mechanism 2
Metadata traversal from retrieved tools to parent agents preserves execution context (authentication, policies) that pure tool-only retrieval discards. Each tool entry includes an explicit owner(T) = A mapping, ensuring that when tools are retrieved, their parent agents are returned as actionable endpoints rather than orphaned tools.

### Mechanism 3
The single-pass retrieval with deduplication avoids the brittleness of two-stage (agent-first then tool-within-agent) pipelines. Rather than committing upfront to agent-level or tool-level routing, the system retrieves N entities from the joint space, then collapses tool hits into their parent agents and deduplicates, allowing the query to naturally surface either specific tools or broader agent bundles depending on semantic fit.

## Foundational Learning

- **Dense vector retrieval and semantic similarity**: The core mechanism relies on embedding models to place tools and agents in a shared vector space where cosine similarity reflects functional relevance. Quick check: Can you explain why a tool for "database query execution" might embed closer to a "SQL agent" description than to a "file system agent"?

- **Information retrieval metrics (Recall@K, nDCG@K, mAP@K)**: The paper evaluates improvements using these standard IR metrics; understanding them is necessary to interpret the 19.4% Recall@5 and 17.7% nDCG@5 claims. Quick check: What does Recall@5 = 0.83 mean in practical terms for a multi-agent routing system?

- **Bipartite graph structure (tools ↔ agents)**: The system models tool-agent relationships as a bipartite graph G = (A, T, E) where edges represent ownership; traversal operates over this structure. Quick check: In a bipartite tool-agent graph, why can't edges exist directly between two tools?

## Architecture Onboarding

- **Component map**: Tool Corpus (C_T) -> Unified Catalog (C) -> Embedding layer -> Retrieval engine -> Query processor -> Agent corpus (C_A)
- **Critical path**: Catalog construction → Query ingestion → Retrieval (Algorithm 1) → Routing
- **Design tradeoffs**: N vs K ratio affects recall vs latency; direct vs step-wise querying impacts multi-step task performance; embedding model selection varies performance significantly
- **Failure signatures**: Missing owner metadata causes tools to be skipped; context dilution in agent descriptions reduces tool-level match effectiveness; over-narrow N yields fewer than K unique agents
- **First 3 experiments**: 1) Baseline comparison with BM25-only retrieval, 2) Ablation on N/K ratio variation, 3) Metadata integrity test with 10% owner removal

## Open Questions the Paper Calls Out

- Does improved retrieval accuracy translate to higher end-to-end task completion rates when agents actually execute retrieved tools? The evaluation measures only retrieval metrics, not downstream task success.
- How does Tool-to-Agent Retrieval performance scale when expanding from hundreds of tools to thousands or tens of thousands across hundreds of agents? The evaluation uses only 70 MCP servers and 527 tools.
- In what query contexts does returning an individual tool versus returning an entire agent bundle lead to better downstream outcomes? The paper reports mixed retrieval patterns without analyzing when each pattern is optimal.
- How does the approach handle dynamic catalog updates where tools are added, removed, or modified at inference time? No mechanism for handling real-time catalog changes is described.

## Limitations
- Effectiveness depends critically on embedding quality, with gains ranging from +13% to +28% across different models
- The N ≫ K ratio is not specified, creating uncertainty about optimal parameter settings
- Relies on metadata ownership links that, if missing or inconsistent, cause tools to be skipped entirely
- Step-wise query decomposition procedure lacks implementation details

## Confidence
- **High Confidence**: The core mechanism of joint tool-agent embedding with metadata traversal is well-specified and algorithmically sound
- **Medium Confidence**: Architectural design choices are justified but optimal parameter settings remain underspecified
- **Low Confidence**: Step-wise query decomposition lacks implementation details

## Next Checks
1. Implement BM25-only retrieval on LiveMCPBench and compare Recall@5 performance to validate the reported 0.20 → 0.83 improvement gap
2. Systematically vary N from 2K to 10K while holding K=5 to identify the optimal ratio and measure recall degradation at extreme values
3. Remove owner metadata from a controlled percentage (e.g., 10%) of tools and measure the corresponding retrieval failure rate to quantify system robustness