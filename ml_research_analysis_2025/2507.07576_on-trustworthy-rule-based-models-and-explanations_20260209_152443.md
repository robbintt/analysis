---
ver: rpa2
title: On Trustworthy Rule-Based Models and Explanations
arxiv_id: '2507.07576'
source_url: https://arxiv.org/abs/2507.07576
tags:
- decision
- redundant
- overlap
- explanations
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the trustworthiness of rule-based machine
  learning models by analyzing their undesired facets, specifically negative overlap
  (where rules predicting different classes fire on the same input) and literal redundancy
  (both local and global). The authors develop algorithms to detect these facets in
  decision sets, decision trees, and Anchor explanations.
---

# On Trustworthy Rule-Based Models and Explanations

## Quick Facts
- **arXiv ID**: 2507.07576
- **Source URL**: https://arxiv.org/abs/2507.07576
- **Reference count**: 13
- **Key outcome**: This paper investigates the trustworthiness of rule-based machine learning models by analyzing their undesired facets, specifically negative overlap (where rules predicting different classes fire on the same input) and literal redundancy (both local and global). The authors develop algorithms to detect these facets in decision sets, decision trees, and Anchor explanations. They demonstrate that these negative facets exist ubiquitously in popular rule-based ML toolkits like Orange, scikit-learn, and IAI, making manual computation of abductive explanations computationally hard. The experiments show high percentages of negative overlap (up to 99% for Boomer) and significant literal redundancy in tree-based models. The paper concludes that well-known rule-based models exhibit these problematic facets, complicating the computation of rigorous explanations and challenging their trustworthiness in high-risk applications.

## Executive Summary
This paper investigates the trustworthiness of rule-based machine learning models by analyzing their undesired facets, specifically negative overlap (where rules predicting different classes fire on the same input) and literal redundancy (both local and global). The authors develop algorithms to detect these facets in decision sets, decision trees, and Anchor explanations. They demonstrate that these negative facets exist ubiquitously in popular rule-based ML toolkits like Orange, scikit-learn, and IAI, making manual computation of abductive explanations computationally hard. The experiments show high percentages of negative overlap (up to 99% for Boomer) and significant literal redundancy in tree-based models. The paper concludes that well-known rule-based models exhibit these problematic facets, complicating the computation of rigorous explanations and challenging their trustworthiness in high-risk applications.

## Method Summary
The paper develops algorithms for detecting negative facets in rule-based systems. Algorithm 1 checks for negative overlap between rules using SAT solving, where two rules overlap if their conjunction with background knowledge is satisfiable. Algorithm 2 generates background knowledge in CNF form from the dataset's feature domains. Literal redundancy is classified as local (redundant within a single rule) or global (redundant across rules with the same prediction), with Corollary 1 providing unsatisfiability tests for both. The analysis is applied to decision sets, decision trees, and Anchor explanations, using PySAT for SAT queries and evaluating on UCI ML datasets.

## Key Results
- High percentages of negative overlap found across all toolkits: 64.6% (Orange), 98.7% (sklearn), 95.6% (IAI), and 99.5% (Boomer)
- Literal redundancy also prevalent: 7.2% local (sklearn), 33.0% global (sklearn), 3.1% global (Orange), and 4.1% global (IAI)
- Boomer algorithm exploits negative overlap for accuracy, trading interpretability for performance
- When negative facets are absent, rule features directly constitute abductive explanations; otherwise, finding explanations becomes NP-hard

## Why This Works (Mechanism)

### Mechanism 1: SAT-Based Negative Overlap Detection
- Claim: Negative overlap (two rules predicting different classes firing on the same input) can be detected algorithmically using satisfiability checks.
- Mechanism: For each pair of rules (Ri, Rj) with different predictions, check if B ∧ Li ∧ Lj is satisfiable. If satisfiable, a valid input exists that triggers both rules, indicating negative overlap. Algorithm 1 iterates through all pairs with O(|M|² × f(M)) complexity where f(M) is the SAT check cost.
- Core assumption: Background knowledge B can be encoded as CNF, and the rule literals can be expressed propositionally.
- Evidence anchors:
  - [abstract] "The paper develops algorithms for the analysis of these undesired facets of rule-based systems"
  - [section] Lemma 1: "Two rules Ri and Rj overlap iff B ∧ Li ∧ Lj is satisfiable"
  - [corpus] Weak corpus support; neighbor papers focus on explainability but not specifically on SAT-based overlap detection
- Break condition: If background knowledge cannot be encoded as CNF, or if SAT solver times out (experiments used 1-hour timeout), the mechanism fails.

### Mechanism 2: Two-Stage Literal Redundancy Elimination
- Claim: Literal redundancy in rules can be classified and removed through local and global redundancy checks.
- Mechanism: Local redundancy occurs when B ∧ (Li \ {l}) |= l (the literal is implied by other literals in the same rule plus background knowledge). Global redundancy occurs when B ∧ L̄l_i |= L_i1 ∨ ... ∨ L_iz (other rules with the same prediction already cover the space). Corollary 1 provides unsatisfiability tests for both.
- Core assumption: Rules can be iteratively simplified without changing decision set equivalence (Definition 3).
- Evidence anchors:
  - [section] Theorem 1: "A literal l ∈ Li is redundant iff it is locally redundant or globally redundant"
  - [section] Example 1 demonstrates: "(age > 10) is locally redundant in R1" and "(size ≠ 140) is globally redundant in R1"
  - [corpus] No direct corpus support for this specific redundancy classification
- Break condition: If removal order matters significantly (paper notes "different removal orders might lead to different decision sets"), results may vary.

### Mechanism 3: Explanation Tractability via Facet Elimination
- Claim: When negative overlap and literal redundancy are absent, rule features directly constitute abductive explanations that humans can compute manually.
- Mechanism: Proposition 5 proves that if R_k fires on input v, has no negative overlap, and contains no redundant literals, then the features from L_k represent an AXp. This transforms an NP-hard problem into a trivial extraction.
- Core assumption: The decision set has been preprocessed to remove all negative facets.
- Evidence anchors:
  - [section] Proposition 5: "If R_k = (L_k, o_k) fires on v, there is no negative overlap, and L_k contains no (global or local) redundant literal, then the features from L_k represent an AXp"
  - [section] "Otherwise, as proved in earlier work for the concrete case of decision lists, finding an AXp is computationally hard"
  - [corpus] [Marques-Silva and Ignatiev, 2023] cited for NP-hardness of decision list explanations
- Break condition: If any negative facet exists, the tractability guarantee collapses; finding AXp becomes NP-hard.

## Foundational Learning

- Concept: **Propositional SAT Solving (CNF, DPLL/CDCL)**
  - Why needed here: All detection algorithms (Algorithms 1-2, redundancy checks) reduce to SAT queries. Understanding SAT terminology (satisfiable, unsatisfiable, entailment |=) is prerequisite.
  - Quick check question: Given F = (x ∨ y) ∧ (¬x ∨ z) ∧ (¬y), is F satisfiable? If so, provide one satisfying assignment.

- Concept: **Rule-Based Model Representations (Decision Sets vs. Trees vs. Lists)**
  - Why needed here: The paper treats decision trees as a special case of decision sets (each path = one rule). Understanding the difference between ordered lists (sequential firing) and unordered sets (potential overlap) is critical.
  - Quick check question: Why do decision trees exhibit no overlap, while random forests may?

- Concept: **Abductive Explanations (AXp)**
  - Why needed here: The paper's central thesis connects negative facets to the tractability of computing AXp. AXp is a subset-minimal set of features sufficient to guarantee the prediction.
  - Quick check question: Given a classifier and instance (v, c), what makes a feature set S a "weak" vs. "subset-minimal" abductive explanation?

## Architecture Onboarding

- Component map:
  Preprocessing layer -> Background knowledge generator -> Negative overlap detector -> Redundancy analyzer -> Explanation extractor

- Critical path:
  1. Raw model → Preprocessing → Background knowledge construction
  2. SAT-based overlap detection (must complete before redundancy analysis)
  3. Literal redundancy removal (local before global)
  4. If clean: manual AXp extraction; if not: formal explanation requires NP-hard computation

- Design tradeoffs:
  - **Boosting vs. interpretability**: Boomer exploits negative overlap for accuracy (99% overlap in experiments), trading interpretability for performance
  - **Optimal trees vs. greedy trees**: IAI (optimal) shows fewer redundancies than sklearn (greedy), but with potentially higher training cost
  - **Detection timeout vs. completeness**: 1-hour timeout in experiments; some Boomer datasets timed out

- Failure signatures:
  - High NO (negative overlap) percentage → AXp computation intractable
  - Large TR (redundancy detection time) → model too complex for practical audit
  - Logistic regression anchors show highest PO (7.81% in recidivism dataset) → explainer quality tied to model quality

- First 3 experiments:
  1. Run Algorithm 1 on a small Orange decision set (NR < 50 rules) to verify overlap detection matches manual inspection; measure TO runtime.
  2. Apply Corollary 1 to sklearn classification trees with max_depth=5; quantify local vs. global redundancy percentages and compare to paper's PG sklearn classification (33%).
  3. Generate Anchor explanations for a binary classifier on adult dataset; run Algorithm 1 to find NO pairs; verify presence of conflicting anchors (paper reports NO=3195 for xgboost adult).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learning algorithms be modified to guarantee the absence of negative overlap and redundancy by construction, rather than detecting them post-hoc?
- Basis in paper: [Inferred] The paper concludes that current toolkits ubiquitously induce rule sets with negative facets, but the proposed solution focuses on algorithms for detection and analysis rather than prevention during training.
- Why unresolved: Existing learning algorithms (like those in Orange or scikit-learn) prioritize accuracy or local optimization steps which inadvertently introduce these structural flaws.
- What evidence would resolve it: The development of a new induction algorithm that formally guarantees a rule set is free of negative overlap and literal redundancy upon creation.

### Open Question 2
- Question: How can the analysis of negative facets be extended to decision lists, which were excluded from this study?
- Basis in paper: [Explicit] The authors explicitly state, "we opt not to address decision lists due to the intrinsic difficulties with their explanation."
- Why unresolved: The sequential, "if-then-else" nature of decision lists introduces execution order dependencies that make the definitions of "overlap" and "redundancy" significantly more complex than in unordered decision sets.
- What evidence would resolve it: A theoretical framework defining negative overlap and redundancy specifically for ordered rule lists, accompanied by detection algorithms.

### Open Question 3
- Question: What is the trade-off between eliminating negative facets (specifically negative overlap) and maintaining model predictive performance?
- Basis in paper: [Inferred] Section 5.2 notes that the Boomer algorithm "exploits boosting (and as a result negative overlap) to build high-accuracy rule ensembles," implying that removing overlap might degrade accuracy.
- Why unresolved: The paper focuses on the ubiquity of these facets and the difficulty of computing explanations, but does not experimentally quantify the performance cost of enforcing a "clean" rule set.
- What evidence would resolve it: Empirical benchmarks comparing the accuracy of standard rule-based models against sanitized versions where all negative overlap and redundancies have been removed.

## Limitations

- Study focuses on specific rule-based frameworks (Orange, scikit-learn, IAI, Boomer) without examining neural-symbolic hybrids or other explainability tools
- Background knowledge generation assumes complete domain constraints, which may not hold in practice
- Analysis treats models in isolation, not accounting for ensemble effects or dynamic data distributions

## Confidence

- **High**: SAT-based overlap detection is algorithmically sound with formal proofs (Lemma 1, Theorem 1)
- **Medium**: Practical significance of findings due to limited dataset coverage and lack of comparisons with alternative simplification methods
- **Low**: Scalability to industrial-scale models given 1-hour SAT timeout threshold

## Next Checks

1. **Dataset Replication**: Reproduce overlap detection on at least three UCI datasets matching the paper's criteria (100-1M samples, 20+ per class) to verify reported PO percentages align with the 64.6-99.5% range.

2. **Scalability Test**: Apply Algorithm 1 to decision sets with >500 rules to measure how SAT timeout frequency increases and whether reported TR values remain within practical bounds.

3. **Alternative Simplification**: Implement a non-SAT-based rule simplification method (e.g., coverage-based pruning) and compare redundancy reduction against the paper's approach to assess whether SAT-based detection offers unique advantages.