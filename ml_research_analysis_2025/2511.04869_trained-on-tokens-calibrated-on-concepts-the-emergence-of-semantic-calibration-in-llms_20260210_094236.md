---
ver: rpa2
title: 'Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration
  in LLMs'
arxiv_id: '2511.04869'
source_url: https://arxiv.org/abs/2511.04869
tags:
- smece
- semantic
- idence
- calibration
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often fail to provide meaningful confidence
  estimates for their outputs. While base models are known to exhibit next-token calibration,
  it remains unclear whether they can assess confidence in the semantic meaning of
  their responses.
---

# Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs

## Quick Facts
- arXiv ID: 2511.04869
- Source URL: https://arxiv.org/abs/2511.04869
- Authors: Preetum Nakkiran; Arwen Bradley; Adam Goliński; Eugene Ndiaye; Michael Kirchhof; Sinead Williamson
- Reference count: 40
- Key outcome: Base LLMs exhibit semantic calibration in open-domain QA tasks as a byproduct of next-token prediction, but RL instruction-tuning and chain-of-thought reasoning systematically break this calibration

## Executive Summary
This paper demonstrates that large language models, despite being trained solely on next-token prediction, exhibit meaningful semantic calibration in open-domain question-answering tasks. The authors provide a theoretical framework showing that semantic calibration emerges naturally from the relationship between next-token calibration and local loss optimality. Through extensive empirical validation, they show that base LLMs are well-calibrated across various QA tasks, while post-training procedures like reinforcement learning and chain-of-thought reasoning systematically break this calibration property.

## Method Summary
The authors develop a theoretical framework connecting next-token calibration to semantic calibration through the concept of local loss optimality. They define semantic calibration in terms of the model's ability to assess confidence in the meaning of its responses rather than just token-level predictions. The framework predicts that base LLMs will be semantically calibrated when they can easily predict their own distribution over semantic answer classes. The empirical validation involves evaluating calibration metrics across multiple question-answering datasets, comparing base models with RLHF and DPO fine-tuned versions, and testing chain-of-thought reasoning models. Calibration is measured using standard metrics like expected calibration error (ECE) and Brier scores, adapted to assess semantic rather than token-level confidence.

## Key Results
- Base LLMs show surprisingly good semantic calibration across multiple open-domain QA tasks despite not being explicitly trained for calibration
- Reinforcement learning instruction-tuning (RLHF, DPO) systematically breaks semantic calibration in LLMs
- Chain-of-thought reasoning also disrupts semantic calibration, suggesting that post-training procedures that alter token distributions can harm calibration
- Semantic calibration does not depend significantly on model size for base models, indicating it's a fundamental property of the training objective

## Why This Works (Mechanism)
The mechanism behind semantic calibration emergence relies on the mathematical relationship between next-token calibration and local loss optimality. When a model is trained to minimize next-token prediction loss, it naturally learns to be calibrated at the token level. The key insight is that if the model can easily predict its own distribution over semantic answer classes before generating a response, this token-level calibration transfers to semantic calibration. This happens because the model's confidence in generating the correct semantic answer is reflected in its token-level probability distributions, creating a natural alignment between token prediction and semantic meaning assessment.

## Foundational Learning
- **Next-token calibration**: Models predict token probabilities that match observed frequencies - needed to establish baseline calibration property, quick check: verify token-level confidence matches empirical accuracy
- **Local loss optimality**: Models achieve minimal loss at predicted probabilities - needed to connect calibration to training objectives, quick check: confirm loss is minimized at predicted probabilities
- **Semantic space approximation**: Semantic concepts can be represented through token distributions - needed to bridge token-level and semantic calibration, quick check: validate semantic classes map to coherent token patterns
- **Distributional shift**: Changes in input/output distributions affect calibration - needed to explain why post-training breaks calibration, quick check: measure shifts in token distributions after RLHF
- **Expected calibration error (ECE)**: Metric measuring discrepancy between predicted confidence and actual accuracy - needed to quantify calibration quality, quick check: compute ECE for token vs semantic predictions
- **Brier score**: Proper scoring rule for probabilistic predictions - needed to evaluate calibration quality, quick check: compare Brier scores across different model versions

## Architecture Onboarding

**Component Map:**
Input question → Token prediction → Semantic interpretation → Confidence assessment → Output generation

**Critical Path:**
Question encoding → Next-token prediction → Semantic mapping → Calibration assessment → Response generation

**Design Tradeoffs:**
The paper highlights that base models trade raw performance for calibration, while post-training procedures that improve task performance (RLHF, CoT) sacrifice calibration. This tradeoff suggests that optimization for task completion may be fundamentally at odds with maintaining well-calibrated confidence estimates.

**Failure Signatures:**
Semantic calibration breaks when: (1) Post-training procedures induce distributional shifts in token space, (2) Chain-of-thought reasoning creates intermediate distributions that don't align with final semantic predictions, (3) The semantic space cannot be adequately approximated by the model's token vocabulary.

**First 3 Experiments to Run:**
1. Evaluate semantic calibration on mathematical reasoning tasks requiring multi-step inference
2. Compare calibration properties of models fine-tuned with different RLHF variants (PPO vs DPO)
3. Test whether intermediate chain-of-thought steps show better calibration than final answers

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework assumes semantic space can be adequately approximated by token distributions, which may not hold for all semantic content
- Empirical findings focus primarily on open-domain question-answering, leaving uncertainty about generalization to other NLP tasks
- Paper doesn't fully explore whether post-training procedures fundamentally alter internal representations or just shift token-level calibration

## Confidence
- Theoretical connection between next-token and semantic calibration: Medium-High
- Empirical demonstration of base model calibration: Medium-High
- Finding that RLHF breaks calibration: Medium-High
- Mechanism explaining why CoT breaks calibration: Medium

## Next Checks
1. Test semantic calibration on tasks requiring more abstract reasoning or multi-step inference beyond direct question-answering, such as mathematical reasoning or causal inference tasks
2. Conduct ablation studies on specific components of RLHF and DPO training pipelines to isolate which aspects most strongly contribute to breaking semantic calibration
3. Evaluate whether intermediate representations in chain-of-thought reasoning exhibit calibration properties, potentially identifying stages in reasoning where calibration is preserved or lost