---
ver: rpa2
title: Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization
arxiv_id: '2507.02406'
source_url: https://arxiv.org/abs/2507.02406
tags:
- preference
- trajectory
- prediction
- simpo
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies preference optimization to improve the consistency
  of multi-agent trajectory prediction in autonomous driving. It addresses the problem
  of inconsistent predictions among agents, which can lead to collisions and unsafe
  planning.
---

# Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization

## Quick Facts
- **arXiv ID:** 2507.02406
- **Source URL:** https://arxiv.org/abs/2507.02406
- **Reference count:** 35
- **Primary result:** Reduces collision rates by up to 57% (pSCR) and 64% (SCR with oversampling) in multi-agent trajectory prediction while minimally affecting prediction accuracy

## Executive Summary
This paper addresses inconsistent multi-agent trajectory predictions in autonomous driving by applying preference optimization (SimPO) to shift probability mass away from collision-containing joint modes. The method ranks predicted joint modes using a collision-penalizing preference cost and fine-tunes models to increase the likelihood of collision-free modes. Experiments on three datasets show significant reductions in collision rates while maintaining trajectory accuracy, with no additional inference-time computation required. The approach works for both marginal and joint prediction models and demonstrates effectiveness across different base architectures.

## Method Summary
The method fine-tunes pretrained trajectory prediction models using an adapted SimPO framework. It aggregates marginal predictions into joint modes via likelihood-based pairing, computes a preference cost combining average FDE and collision penalty, ranks modes, and applies a Plackett-Luce objective with position-scaled margins. A preference dataset is extracted from training scenes containing collisions or significant cost differences. The model is fine-tuned for 5 epochs on this subset, optimizing likelihoods to favor collision-free modes. For marginal models, top-6 joint modes are selected post-fine-tuning; for joint models, predictions are used directly.

## Key Results
- Achieves up to 57% reduction in probability-weighted collision rate (pSCR) and 27% reduction in scene collision rate (SCR) on Argoverse 2
- Combines with oversampling (K=15) to reach 64% SCR reduction while maintaining accuracy
- Minimal degradation in prediction accuracy (less than 10% increase in FDE) across all tested models
- Works effectively across different base architectures: QCNet, FJMP (marginal and joint variants), and BeTop

## Why This Works (Mechanism)

### Mechanism 1
Preference optimization shifts probability mass away from collision-containing joint modes by ranking predicted modes using a collision-penalizing cost. The SimPO loss increases the likelihood of collision-free modes, pushing collision modes down in probability without requiring explicit collision-avoidance terms in the base model. This works when the model has sufficient behavioral diversity to allow meaningful probability redistribution.

### Mechanism 2
Target reward margin scaling with ranking position (kγ) prevents degenerate solutions by forcing larger reward gaps between more distant rankings. This ensures stable training and prevents the optimizer from collapsing all probabilities toward uniformity or a single mode. The ranking must reflect meaningful preference differences for the model to learn to express them through likelihood outputs.

### Mechanism 3
Oversampling trajectories before preference optimization increases available joint mode combinations, enabling collision modes to be pushed out of the top-K entirely. By generating K > 6 modes and selecting top-6 after optimization, the method gains access to a richer space of agent pairings. Collision-containing combinations become less likely to remain in the final output set, but only if the model generates genuinely diverse trajectories as K increases.

## Foundational Learning

- **Concept:** Preference Optimization (DPO/SimPO paradigm)
  - **Why needed here:** This paper adapts LLM preference optimization to trajectory prediction; understanding how reward is implicit in likelihoods is essential.
  - **Quick check question:** Given two outputs y_w and y_l, can you explain why SimPO avoids needing a separate reward model?

- **Concept:** Marginal vs. Joint Trajectory Prediction
  - **Why needed here:** The method converts marginal predictions to joint predictions via likelihood-based aggregation; understanding this distinction clarifies what consistency means.
  - **Quick check question:** If Agent A has 3 trajectories with probabilities [0.5, 0.3, 0.2] and Agent B has 3 with [0.6, 0.3, 0.1], how would you form the top-3 joint modes using the paper's aggregation strategy?

- **Concept:** Mode Collapse in Multi-Modal Prediction
  - **Why needed here:** The paper explicitly identifies mode collapse as limiting SimPO's effectiveness; recognizing this failure mode is critical for debugging.
  - **Quick check question:** If all predicted trajectories for an agent pass through nearly identical endpoints despite different ground-truth behaviors, what symptom would you expect in SimPO fine-tuning?

## Architecture Onboarding

- **Component map:** Base predictor -> Aggregation module -> Preference cost calculator -> Ranking module -> SimPO loss -> Fine-tuning loop
- **Critical path:** Base model outputs → Aggregation → Preference cost computation → Ranking → SimPO loss → Gradient update. The preference dataset extraction gates training quality.
- **Design tradeoffs:**
  - Higher λ (collision penalty weight) improves consistency but may degrade accuracy
  - Higher γ (target margin) enforces stronger preference separation but risks overfitting
  - Larger K (mode count) helps if diversity exists; hurts if mode collapse present
  - Preference dataset filtering (δ threshold) trades data volume for ranking signal quality
- **Failure signatures:**
  - SCR increases after SimPO: Likely γ misconfigured or preference cost poorly designed
  - MinJointFDE degrades >10%: λ too high or γ too aggressive
  - No improvement despite fine-tuning: Check for mode collapse in base model; verify preference dataset contains sufficient collision scenes
- **First 3 experiments:**
  1. Reproduce QCNet + SimPO on Argoverse 2: Use β=2, γ=5, λ=10³, K=6. Verify ~27% SCR reduction and ~57% pSCR reduction match Table I.
  2. Ablation on γ values: Run Table III experiment (γ ∈ {0, 2, 5, 7, 10}) to confirm the catastrophic failure at γ=0 and optimal range identification.
  3. Oversampling experiment: Generate K=15 modes with QCNet, apply SimPO, select top-6. Target ~64% SCR reduction per Figure 3. If improvement <30%, investigate mode diversity in base model predictions.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the choice of alternative preference metrics or reward functions impact the balance between collision reduction and trajectory accuracy? The authors suggest exploring different ranking metrics beyond the current combination of average FDE and collision repeller cost.

- **Open Question 2:** Can architectural modifications or regularization prevent the "degenerate solutions" observed when directly optimizing the preference cost? The paper identifies trajectory collapse as a failure mode of direct cost optimization but does not investigate potential fixes with constraints.

- **Open Question 3:** Does the efficacy of preference optimization correlate strongly with the base model's ability to avoid mode collapse? The authors observe that FJMP suffered from mode collapse, limiting SimPO's effectiveness, and suggest this is essential for enabling such techniques.

## Limitations

- The method's effectiveness depends heavily on the base model's ability to generate diverse trajectories; mode collapse severely limits improvements
- Hyperparameters (γ, λ, δ) are tightly tuned to each dataset and may not transfer well to domains with different agent densities or speeds
- The "no additional inference cost" claim ignores the substantial computation needed for large-K oversampling experiments
- The preference dataset extraction may miss rare but critical multi-agent interactions if the 15% subset doesn't capture them

## Confidence

- **Collision reduction claims:** High - well-supported by ablation studies and quantitative improvements across multiple datasets
- **Zero-inference-cost claim:** Medium - valid for SimPO itself but ignores computational overhead of oversampling
- **Universal applicability:** Medium - dataset-specific hyperparameter tuning suggests limited transferability
- **Robustness against mode collapse:** Low - method fails to improve when base model suffers from mode collapse

## Next Checks

1. Apply the same SimPO pipeline to a dataset with significantly different agent densities and speeds (e.g., high-speed highway scenarios) to test hyperparameter transferability.

2. Measure actual wall-clock inference time for K=15 oversampling vs. K=6 standard inference to quantify the "zero additional cost" claim.

3. Intentionally induce mode collapse in the base model (e.g., by reducing diversity in training data) and re-run SimPO to confirm whether the method fails gracefully or exacerbates consistency issues.