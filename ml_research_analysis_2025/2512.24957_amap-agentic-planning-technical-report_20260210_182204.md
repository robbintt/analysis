---
ver: rpa2
title: AMAP Agentic Planning Technical Report
arxiv_id: '2512.24957'
source_url: https://arxiv.org/abs/2512.24957
tags:
- tool
- training
- reasoning
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STAgent is an agentic large language model designed for spatio-temporal
  understanding, enabling complex tasks like constrained point-of-interest discovery
  and itinerary planning through interaction with ten domain-specific tools. It employs
  a hierarchical data curation framework to filter high-quality queries from massive
  historical data and a cascaded training recipe combining supervised fine-tuning
  and reinforcement learning guided by query difficulty assessment.
---

# AMAP Agentic Planning Technical Report

## Quick Facts
- **arXiv ID**: 2512.24957
- **Source URL**: https://arxiv.org/abs/2512.24957
- **Reference count**: 40
- **Primary result**: STAgent achieves strong performance on TravelBench for constrained point-of-interest discovery and itinerary planning while maintaining robust generalization across general benchmarks.

## Executive Summary
STAgent is an agentic LLM designed for spatio-temporal understanding, capable of complex tasks like constrained point-of-interest discovery and itinerary planning through interaction with ten domain-specific tools. It employs a hierarchical data curation framework to filter high-quality queries from massive historical data and a cascaded training recipe combining supervised fine-tuning and reinforcement learning guided by query difficulty assessment. Initialized with Qwen3-30B-A3B-2507, STAgent demonstrates effective specialized reasoning without sacrificing broad capabilities.

## Method Summary
STAgent uses a cascaded training approach starting with Qwen3-30B-A3B-2507. The process involves hierarchical data curation to filter 30M raw queries down to ~200K high-quality samples, followed by a seed SFT stage for difficulty assessment. Training then proceeds through adaptive SFT on high-certainty samples and GRPO/GSPO RL on low-certainty samples, with a hallucination veto mechanism in the reward signal. The system integrates ten domain tools via FastMCP with LRU caching and uses ROLL infrastructure for efficient asynchronous rollout and training.

## Key Results
- Strong performance on TravelBench for spatio-temporal reasoning tasks
- Maintains robust generalization across general benchmarks (ACEBench, BFCL, AIME, LiveCodeBench, MMLU-Pro)
- Achieves high-quality query curation with a filter ratio of 1:10,000

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Capability-Aware Curriculum Learning
Selecting training samples based on the policy model's own uncertainty distribution improves learning efficiency over static difficulty metrics. The seed SFT model samples K=8 trajectories per query, computes mean reward and variance, then calculates Learnability Potential Score Si = σ²i · μ̂i. Samples with high variance and non-zero mean are retained as "learnable"—these lie on the model's decision boundary where gradients are most informative.

### Mechanism 2: SFT-Guided Reinforcement Learning with Certainty-Based Data Partition
Separating training data by model certainty—SFT on high-certainty samples, RL on low-certainty samples—enables both stable grounding and ceiling-breaking exploration. High-certainty samples undergo SFT to reinforce correct behavior patterns, while low-certainty samples are routed to RL where the model must actively explore the tool environment to discover solutions.

### Mechanism 3: Hard Hallucination Veto in Reward Signal
A strict zero-reward penalty for any hallucinated fact prevents reward hacking where the model learns to fabricate plausible-sounding but ungrounded information. The rubrics-as-reward system evaluates three dimensions with dynamic weights per task type, and a Hard Veto mechanism checks for hallucinated facts and forces R=0 regardless of other scores.

## Foundational Learning

- **Concept: Tool-Integrated Reasoning (TIR)**
  - Why needed here: STAgent interleaves thought generation with tool execution; understanding how tool calls modify the observation stream and how the model conditions on tool outputs is essential.
  - Quick check question: Can you explain why masking tool observation tokens from the loss function (Section 2.5.2, I(oi,t) indicator) prevents the model from learning to predict tool outputs rather than reason about them?

- **Concept: Group Relative Policy Optimization (GRPO) / GSPO**
  - Why needed here: The RL stage uses GRPO with a sequence-level variant (GSPO) designed for MoE architectures; understanding why token-level importance ratios destabilize MoE training is critical.
  - Quick check question: Why does GSPO use the geometric mean of likelihood ratios over the entire trajectory (Equation 5) instead of token-wise clipping?

- **Concept: Curriculum Learning via Difficulty Stratification**
  - Why needed here: The data curation pipeline explicitly scores difficulty from -1 to 5 and uses this for curriculum scheduling.
  - Quick check question: What happens to RL training if all samples in a batch are difficulty score 5 (very hard) versus a mix of scores 2-4?

## Architecture Onboarding

- **Component map**: FastMCP Tool Layer → 10 domain tools (map, travel, weather, web search) with LRU caching → ROLL Infrastructure → Async rollout + training (80% efficiency gain vs. Verl) → Data Curation Pipeline → 30M raw queries → Funnel Filtering → ~200K candidate pool → Seed SFT Model → Difficulty assessor / certainty estimator → SFT Training → High-certainty samples, trajectory loss masking → RL Training (GRPO/GSPO) → Low-certainty samples, rubric-based reward with hallucination veto

- **Critical path**: 1. Taxonomy construction (Section 2.3.1) → Annotation (Section 2.3.2) → Difficulty scoring (Section 2.3.3) 2. Seed SFT warmup on 10% tiny dataset → Full pool probing for certainty estimation 3. Partition by Learnability Potential Score → SFT on learnable/high-certainty → RL on learnable/low-certainty 4. Evaluation: TravelBench (in-domain) + general benchmarks (ACEBench, BFCL, AIME, LiveCodeBench, MMLU-Pro)

- **Design tradeoffs**: Filter ratio 1:10,000 prioritizes quality over scale; risk of overfitting to a narrow distribution if diversity filtering is too aggressive. Dynamic reward weights per task type add complexity but better align evaluation with user intent. Minimal general instruction data in SFT preserves generalization but risks under-training on edge-case instruction formats.

- **Failure signatures**: Reward collapse during RL: Check if low-certainty samples are actually impossible (μ̂≈0, σ²→0 misclassified as learnable). Hallucination spikes: Verify judge model is correctly identifying fabrications; check tool output parsing for truncation errors. General capability degradation: Monitor ArenaHard-v2.0 and IFEval during training; if scores drop >5%, reduce domain data ratio.

- **First 3 experiments**: 1. Ablate the certainty-based partition: Train with random partition vs. Learnability Potential Score partition; measure TravelBench score and training sample efficiency (trajectories needed to reach target reward). 2. Vary the hallucination veto strictness: Compare hard veto (R=0) vs. soft penalty (e.g., -0.5) vs. no veto; measure hallucination rate on held-out queries requiring numerical facts (prices, distances, times). 3. Probe generalization transfer: Evaluate tool-calling capability on out-of-domain tools (BFCL v3) after each training stage (Seed SFT → SFT → RL) to verify that spatio-temporal tool training transfers to generic function-calling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the dynamic capability-aware curriculum's definition of "Noise" ($\hat{\mu} \approx 0$) risk excluding "very hard" but solvable samples that the initial policy simply fails to solve, thereby limiting RL exploration?
- Basis in paper: [inferred] Section 2.5.3 defines the "Noise Region" as tasks where the policy yields zero reward and filters them out to prevent hallucination, but this may conflate "impossible" tasks with "currently too hard" tasks.
- Why unresolved: The paper assumes low-reward samples are noisy or outside the policy scope, but they might represent the frontier of model capability necessary for breaking performance ceilings.
- What evidence would resolve it: An ablation study comparing the current filtering strategy against a strategy that retains a percentage of low-reward/high-uncertainty samples for RL.

### Open Question 2
- Question: Does the reliance on specific Strong LLMs (DeepSeek-R1) for trajectory synthesis impose a ceiling on STAgent's reasoning capabilities?
- Basis in paper: [explicit] Section 2.5.1 states, "We utilize a Strong LLM (DeepSeek-R1) to generate TIR trajectories," implying the student model learns from a fixed teacher.
- Why unresolved: It is unclear if the agent can surpass the teacher's performance or if it merely distills the teacher's existing reasoning patterns and potential error modes.
- What evidence would resolve it: Comparative analysis of failure cases in STAgent versus the teacher model to determine if errors are inherited or novel.

### Open Question 3
- Question: What is the causal mechanism behind the performance degradation in general alignment (ArenaHard-v2.0) despite the model maintaining performance on other general benchmarks?
- Basis in paper: [explicit] Table 3 shows a -5.0 score drop on ArenaHard-v2.0, contrasting with gains or maintenance on MMLU-Pro and IFEval.
- Why unresolved: The paper claims general capabilities are "preserved," but the specific drop in a preference-based benchmark suggests the domain-specific RL may have shifted the model's tone or instruction-following style in a way not captured by knowledge benchmarks.
- What evidence would resolve it: A qualitative analysis of STAgent responses on ArenaHard to determine if the score drop is due to domain over-fitting (e.g., unnecessary tool use) or stylistic drift.

## Limitations

- **Hyperparameter opacity**: Critical values for learning rates, batch sizes, KL coefficients, and curriculum scheduling are unspecified, preventing exact replication of the reported performance.
- **Generalization boundary**: While TravelBench results are strong, the claim that STAgent maintains "robust generalization" across general benchmarks is supported only by cross-sectional evaluation on fixed datasets, not by active testing on truly out-of-distribution tasks.
- **Judge model reliability**: The hallucination veto mechanism depends entirely on Gemini-3-flash-preview's detection accuracy; no error analysis or false-positive/negative rates are reported.

## Confidence

- **High Confidence**: The cascaded training framework (SFT → RL) and the dynamic learnability-based curriculum design are well-specified and theoretically sound, with clear mechanisms for filtering trivial and noise samples.
- **Medium Confidence**: The effectiveness of the certainty-based data partition and hallucination veto in improving performance is plausible given the evaluation results, but the lack of ablation studies on these specific components limits definitive attribution.
- **Low Confidence**: Claims about maintaining broad capabilities without degradation are based on static benchmark snapshots rather than continuous monitoring or adversarial testing.

## Next Checks

1. **Ablate the certainty-based partition**: Train with random partition vs. Learnability Potential Score partition; measure TravelBench score and training sample efficiency (trajectories needed to reach target reward).
2. **Vary the hallucination veto strictness**: Compare hard veto (R=0) vs. soft penalty (e.g., -0.5) vs. no veto; measure hallucination rate on held-out queries requiring numerical facts (prices, distances, times).
3. **Probe generalization transfer**: Evaluate tool-calling capability on out-of-domain tools (BFCL v3) after each training stage (Seed SFT → SFT → RL) to verify that spatio-temporal tool training transfers to generic function-calling.