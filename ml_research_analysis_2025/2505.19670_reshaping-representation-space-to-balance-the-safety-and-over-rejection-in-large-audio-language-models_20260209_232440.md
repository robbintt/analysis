---
ver: rpa2
title: Reshaping Representation Space to Balance the Safety and Over-rejection in
  Large Audio Language Models
arxiv_id: '2505.19670'
source_url: https://arxiv.org/abs/2505.19670
tags:
- harmful
- safety
- lalms
- benign
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses safety-alignment vulnerabilities in Large
  Audio Language Models (LALMs) that arise during modality-adaptation-tuning. The
  authors propose Reshaping Representation Space (RRS), an unsupervised fine-tuning
  strategy that improves safety by relocating harmful representations to a refusal
  zone while minimizing drift of benign representations.
---

# Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models

## Quick Facts
- arXiv ID: 2505.19670
- Source URL: https://arxiv.org/abs/2505.19670
- Authors: Hao Yang; Lizhen Qu; Ehsan Shareghi; Gholamreza Haffari
- Reference count: 22
- Primary result: RRS improves safety in LALMs with only 0.88% average over-rejection increase while reducing harmful responses

## Executive Summary
This paper addresses safety-alignment vulnerabilities in Large Audio Language Models (LALMs) that arise during modality-adaptation-tuning. The authors propose Reshaping Representation Space (RRS), an unsupervised fine-tuning strategy that improves safety by relocating harmful representations to a refusal zone while minimizing drift of benign representations. Using three generations of Qwen LALMs and a newly constructed audio dataset derived from BeaverTails, they compare RRS against four Supervised Fine-tuning (SFT) strategies. RRS achieves competitive or superior safety performance across audio-text, text-only, and audio-only modalities, increasing over-rejection rate by only 0.88% on average while reducing harmful response rates. The method maintains speech chatting capabilities, particularly preserving performance on Qwen2-Audio and Qwen2.5-Omni.

## Method Summary
RRS is an unsupervised fine-tuning strategy that reshapes the representation space of LALMs to improve safety. The method identifies safety-critical features by computing the difference between representations of harmful (refusal-encouraging) and benign (standard) prompts, then selects features where this difference correlates positively with safety outcomes. A target representation is created for each sample by shifting harmful representations toward the benign cluster and benign representations away from harmful ones. LoRA adapters are fine-tuned on the LLM modules while freezing the audio encoder, minimizing the distance between predicted and target representations with a penalty term to prevent catastrophic forgetting. The approach requires only a small amount of data and can be applied without safety annotations.

## Key Results
- RRS achieves competitive safety performance compared to SFT methods while increasing over-rejection rate by only 0.88% on average
- Visualizations confirm harmful and benign representations form distinct clusters post-tuning
- RRS maintains speech chatting capabilities, particularly preserving performance on Qwen2-Audio and Qwen2.5-Omni
- The method works across audio-text, text-only, and audio-only modalities

## Why This Works (Mechanism)
The paper demonstrates that safety vulnerabilities in LALMs stem from representation space drift during modality-adaptation-tuning. RRS addresses this by identifying safety-critical features that distinguish harmful from benign representations and reshaping the space to push harmful representations into a refusal zone while preserving benign representations. The unsupervised nature allows training without safety annotations, and the penalty term prevents catastrophic forgetting. The approach leverages the observation that harmful and benign representations naturally form distinct clusters that can be manipulated through targeted representation space adjustment.

## Foundational Learning
- **Representation Space Reshaping**: Why needed: To correct safety vulnerabilities caused by modality-adaptation-tuning drift. Quick check: Visualize t-SNE plots before/after tuning to confirm cluster separation.
- **Safety Feature Selection**: Why needed: To identify which representation dimensions are critical for safety decisions. Quick check: Verify w40,p·∆vp > 0 filtering correctly identifies safety-relevant features.
- **LoRA Fine-tuning**: Why needed: To efficiently adapt LALMs without full model retraining. Quick check: Monitor training loss and ensure penalty term prevents catastrophic forgetting.
- **Mirror Dataset Construction**: Why needed: To create paired harmful/benign samples for training without safety annotations. Quick check: Validate GPT-4-Turbo rewrites maintain semantic similarity while changing safety properties.
- **Multi-Modal Evaluation**: Why needed: To ensure safety improvements generalize across audio-text, text-only, and audio-only modalities. Quick check: Test across all three modalities on AIAH benchmark.
- **Feature-based vs Classifier-based Safety**: Why needed: Feature-based methods can be more robust and interpretable than classifier-based approaches. Quick check: Compare performance against classifier-based safety methods.

## Architecture Onboarding

Component Map:
Audio Encoder -> LLM Backbone -> LoRA Adapter -> Safety Feature Extraction -> Representation Space Reshaping

Critical Path:
Audio input → Encoder representation → LLM processing → Safety feature identification → Target representation generation → LoRA fine-tuning → Improved safety outputs

Design Tradeoffs:
- **Unsupervised vs Supervised**: RRS avoids need for safety annotations but requires careful feature selection
- **Feature-based vs Classifier-based**: Feature-based allows more interpretable safety decisions but may miss complex patterns
- **LoRA vs Full Fine-tuning**: LoRA is more efficient but may have limited adaptation capacity
- **Penalty Term Weighting**: Balancing safety improvement vs catastrophic forgetting requires careful tuning

Failure Signatures:
- High over-rejection (>10%) indicates insufficient penalty term weight
- Poor safety improvement suggests incorrect safety feature selection
- Performance degradation on Air-Bench indicates excessive representation space distortion
- Mode collapse where all inputs are refused indicates over-aggressive safety tuning

First Experiments:
1. Construct Mirror dataset and verify harmful/benign pairs maintain semantic similarity
2. Extract safety features and visualize t-SNE to confirm distinct clusters emerge
3. Fine-tune LoRA with RRS and evaluate on AIAH benchmark across all three modalities

## Open Questions the Paper Calls Out
None

## Limitations
- LoRA configuration remains underspecified (rank, alpha, dropout parameters)
- Evaluation relies on synthetic Mirror dataset rather than naturally occurring harmful-benign pairs
- Method requires access to frozen original model for target representation generation
- Scalability to larger LALM architectures and real-world harmful audio inputs untested

## Confidence
**High Confidence**: The core methodological framework of feature-based representation space reshaping is sound and well-articulated. The evaluation demonstrates consistent safety improvements across multiple LALM generations and modalities.

**Medium Confidence**: The effectiveness of the specific feature selection criteria and the chosen percentage of features is supported by results but lacks sensitivity analysis. The claim that RRS minimizes over-rejection while improving safety is demonstrated but could be influenced by the synthetic nature of the evaluation dataset.

**Low Confidence**: The scalability claims to larger LALM architectures and the generalizability to real-world harmful audio inputs remain untested. The paper does not address potential adversarial attacks specifically targeting the RRS-augmented models.

## Next Checks
1. **LoRA Configuration Sensitivity**: Systematically vary LoRA rank (16, 32, 64) and alpha (16, 32, 64) parameters to determine optimal configuration and assess whether reported performance is robust to these hyperparameter choices.

2. **Real-World Dataset Validation**: Evaluate RRS on naturally occurring harmful audio data from diverse sources beyond the synthetic Mirror dataset, including real-world collected harmful speech and publicly available safety violation corpora.

3. **Adversarial Robustness Testing**: Design and execute targeted adversarial attacks against RRS-tuned models to assess whether the safety improvements hold under sophisticated attempts to bypass the refusal mechanism, particularly focusing on prompt engineering and representation-space manipulation attacks.