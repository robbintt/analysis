---
ver: rpa2
title: 'Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global
  Context'
arxiv_id: '2512.11167'
source_url: https://arxiv.org/abs/2512.11167
tags:
- arxiv
- global
- image
- visual
- tiling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors reproduced Monkey Vision-Language Model (VLM) training
  pipeline, which splits high-resolution images into tiles to preserve fine details
  while maintaining computational efficiency. They reimplemented the approach within
  the LLaVA architecture using ViT-S/16 as the vision encoder and OpenHermes-2.5-Mistral-7B
  as the language model.
---

# Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context

## Quick Facts
- arXiv ID: 2512.11167
- Source URL: https://arxiv.org/abs/2512.11167
- Authors: Anatole Jacquin de Margerie; Alexis Roger; Irina Rish
- Reference count: 6
- The study evaluated 2×2 and 3×3 tiling configurations with and without global context across six benchmarks, finding that tiling effectively recovers local details while global context restores coherence.

## Executive Summary
This paper addresses the fundamental tension between preserving fine-grained visual details and maintaining computational efficiency in Vision-Language Models. The authors reproduce and extend the Monkey Vision-Language Model training pipeline by implementing image tiling within the LLaVA architecture. Their approach splits high-resolution images into non-overlapping tiles processed at native resolution, optionally augmented with a downsampled global view. The study demonstrates that tiling effectively recovers local details lost during standard resizing while global context acts as a spatial scaffold that restores coherence. The method achieves strong performance on detail-oriented tasks (80.17% on ScienceQA-Image with 3×3+tiling) while maintaining computational efficiency through frozen base models and LoRA adapters.

## Method Summary
The method implements image tiling within the LLaVA architecture using ViT-S/16 as the vision encoder and OpenHermes-2.5-Mistral-7B as the language model. High-resolution images are split into non-overlapping tiles (2×2 or 3×3 grids) processed independently through the vision encoder at native resolution, with an optional downsampled global view. Visual embeddings from all tiles and the global image (if used) are concatenated and projected into the language model space via a shared MLP. The training follows a two-phase approach: pretraining the MLP projection on BLIP-LAION-CC-SBU-558k with frozen encoder/LLM (LR=1e-3), then finetuning on Monkey training data (1.44M samples) with LoRA adapters on the LLM (LR=2e-5), updating the encoder, MLP, and LoRA components. The approach avoids cross-tile attention to maintain computational efficiency.

## Key Results
- 3×3 tiling with global context achieved highest accuracy (80.17%) on ScienceQA-Image
- Tiling alone improved 2×2 performance on ScienceQA-Image (73.53% vs. 71.34% baseline)
- Simple concatenation approach failed on coherence-sensitive tasks: LLaVA-Bench dropped from 58.40% baseline to 16.70% with 3×3 without global context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image tiling preserves fine-grained visual details that are lost during standard resizing.
- Mechanism: High-resolution images are split into non-overlapping tiles (e.g., 2×2, 3×3 grids), with each tile processed independently through the vision encoder at native resolution. This maintains pixel-level information that would otherwise be downsampled.
- Core assumption: Fine details critical for reasoning are spatially local and can be recovered without cross-tile attention during encoding.
- Evidence anchors:
  - [abstract] "tiling effectively recovers local details"
  - [section: Results] "on ScienceQA-Image, high-resolution tiling alone provided a benefit, with the 2×2 split (73.53%) outperforming the baseline (71.34%)"
  - [corpus] Related work on UHR remote sensing (GLCANet) confirms global-local balancing is a recognized pattern for high-resolution processing
- Break condition: When tasks require integrating information scattered across multiple tiles (e.g., TextVQA dropped to 39.83% with 2×2 no-global vs. 52.99% baseline), simple concatenation fails to establish cross-tile relationships.

### Mechanism 2
- Claim: A downsampled global view acts as a spatial scaffold that restores coherence lost by fragmentation.
- Mechanism: A resized version of the full image is encoded alongside the tiles, providing holistic scene context. The model can reference this global representation to understand spatial relationships between tiles.
- Core assumption: The global view captures sufficient structural information even at reduced resolution to guide interpretation of high-res tile details.
- Evidence anchors:
  - [abstract] "investigating the effect of the inclusion of the global context, which provide practical insights"
  - [section: Results] "3×3 split with global view achieved 80.17%—significantly surpassing all other configurations" on ScienceQA-Image
  - [section: Discussion] "The Critical Role of Global Context: The consistent improvement from adding global context to the splits across all benchmarks confirms its importance"
  - [corpus] No direct corpus corroboration; related work focuses on detail enhancement rather than global-local fusion strategies
- Break condition: When extreme fragmentation (3×3 without global) occurs, coherence degrades catastrophically—LLaVA-Bench dropped from 58.40% baseline to 16.70%.

### Mechanism 3
- Claim: Simple concatenation of tile embeddings is sufficient for detail-oriented tasks but inadequate for generative coherence.
- Mechanism: All tile embeddings (and optional global embedding) are concatenated sequentially before the MLP projection layer. No cross-tile attention mechanisms are used. The language model must infer relationships from this linear sequence.
- Core assumption: The pretrained LLM can learn to interpret spatial relationships from sequentially ordered visual tokens during finetuning.
- Evidence anchors:
  - [section: Methodology] "their embeddings are concatenated before being projected into the language model space via a shared MLP. No cross-tile attention was used"
  - [section: Results] "on LLaVA-Bench... all tiling configurations performed much worse than baseline... fragmented visual representation created by concatenation hinders the model's ability to produce coherent, comprehensive narratives"
  - [corpus] Weak corpus support; related papers use attention-based fusion but don't directly evaluate concatenation vs. attention tradeoffs
- Break condition: Narrative generation tasks (LLaVA-Bench) and scattered text reasoning (TextVQA) expose this limitation—coherence-sensitive tasks require explicit relational modeling that concatenation cannot provide.

## Foundational Learning

- Concept: **Vision Transformer (ViT) patch embedding and resolution constraints**
  - Why needed here: The entire tiling approach exists because ViT encoders have fixed input resolutions. Understanding how patches are extracted and embedded explains why resizing destroys detail and why tiling is a workaround.
  - Quick check question: Given a ViT-S/16 encoder with 224px input, how many patches are processed for a single 2×2 tile configuration (4 tiles + 1 global)?

- Concept: **LoRA (Low-Rank Adaptation) for efficient LLM finetuning**
  - Why needed here: The training pipeline freezes the base LLM and only updates LoRA adapters plus the vision components. This is what makes the approach computationally tractable (~60kWh vs. thousands of GPU days for full retraining).
  - Quick check question: In the finetuning phase, which parameters are updated and which remain frozen?

- Concept: **Object hallucination in VLMs and grounding**
  - Why needed here: POPE benchmark results (2×2 configurations exceeded baseline, suggesting reduced hallucination) indicate that higher-resolution local patches provide more grounded visual evidence. Understanding hallucination mechanisms helps interpret why detail access matters.
  - Quick check question: Why might access to higher-resolution patches reduce a model's tendency to hallucinate objects?

## Architecture Onboarding

- Component map:
Input Image → Tile Splitter → [Tile₁, Tile₂, ..., Tileₙ, (Global)]
                              ↓
                    Parallel ViT-S/16 Encoders (shared weights)
                              ↓
                    Visual Token Sequences
                              ↓
                    Concatenation Layer
                              ↓
                    MLP Projection (2-layer)
                              ↓
                    OpenHermes-2.5-Mistral-7B (+ LoRA adapters)
                              ↓
                    Output Tokens

- Critical path: Pretrain MLP projection on BLIP-LAION-CC-SBU-558k (LR=1e-3, vision+LLM frozen) → Finetune on Monkey training data 1.44M samples (LR=2e-5, vision+MLP+LoRA updated). The projection layer must be pretrained first or visual tokens won't be in the LLM's embedding space.

- Design tradeoffs:
  - 2×2 vs. 3×3: 2×2 preserves coherence better (GQA: 54.20% vs. 53.82%), 3×3 captures more detail (ScienceQA: 80.17% vs. 78.19%)
  - With vs. without global: Global context consistently helps (avg. +5-15% on detail tasks), but adds 5-12% training overhead
  - Concatenation vs. attention fusion: Current design is simple and efficient but fails on narrative tasks; attention-based fusion could improve coherence at architectural cost

- Failure signatures:
  - 3×3 without global context → catastrophic coherence loss (LLaVA-Bench: 16.70%, POPE F1: 33.01)
  - TextVQA underperformance across all tiling configs → simple concatenation cannot handle scattered text requiring cross-region reasoning
  - LLaVA-Bench degradation → generative narrative tasks require holistic understanding that fragmented representations break

- First 3 experiments:
  1. **Baseline establishment**: Train 1×1 (single resized image) configuration on all benchmarks to establish reference performance before any tiling experiments.
  2. **Ablation on global context**: Compare 2×2 with-global vs. 2×2 without-global on ScienceQA-Image and GQA to quantify the coherence restoration effect (expect ~5% gap on ScienceQA, ~3% on GQA).
  3. **Failure mode characterization**: Test 3×3 without-global specifically on POPE and LLaVA-Bench to confirm the hypothesized catastrophic fragmentation failure before investing in mitigation strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic, content-aware tiling strategies outperform fixed grids in balancing local detail with global coherence?
- Basis in paper: [explicit] The authors suggest "adaptive splitting" as a future direction, where models adjust processing based on image content rather than fixed grids (p. 4).
- Why unresolved: This study only evaluated static 2×2 and 3×3 configurations, leaving the potential of variable, fovea-like processing untested.
- What evidence would resolve it: A study benchmarking a model that dynamically varies tile granularity against fixed-grid baselines on both detail-oriented (ScienceQA) and coherence-sensitive (LLaVA-Bench) tasks.

### Open Question 2
- Question: How do cross-tile attention mechanisms compare to simple concatenation for maintaining narrative coherence in generative tasks?
- Basis in paper: [inferred] The authors note that the "fragmented visual representation created by concatenation hinders the model’s ability to produce coherent... narratives" and suggest attention-based fusion as a remedy (p. 2, 3).
- Why unresolved: The reproduction was limited to the Monkey architecture's concatenation approach, which performed poorly on conversational benchmarks.
- What evidence would resolve it: Implementation of cross-tile attention showing improved LLaVA-Bench scores without sacrificing the efficiency gains noted in Table 1.

### Open Question 3
- Question: Can modified tiling architectures effectively reason about text scattered across non-contiguous image regions?
- Basis in paper: [inferred] Results showed all tiling configurations lagged behind the baseline on TextVQA; the authors suggest "simple concatenation may be suboptimal" for scattered text (p. 2).
- Why unresolved: It is unclear if the performance drop is an inherent limitation of splitting or a solvable failure of the specific fusion method used.
- What evidence would resolve it: A specialized cross-region fusion module that recovers or exceeds baseline performance on TextVQA while maintaining high-resolution inputs.

## Limitations

- The concatenation-based fusion approach fails catastrophically on coherence-sensitive tasks like LLaVA-Bench and POPE when using fine-grained tiling (3×3 without global context)
- Experimental scope is limited to a single VLM architecture (LLaVA with ViT-S/16) without investigating performance across different vision encoders or LLM scales
- Resolution specifications for tiles and global context remain unspecified, creating potential reproducibility gaps

## Confidence

- **High Confidence**: The core mechanism that tiling preserves local detail (evidenced by consistent improvements on detail-oriented benchmarks like ScienceQA-Image) and that global context improves coherence across tasks.
- **Medium Confidence**: The claim that simple concatenation is insufficient for narrative generation tasks, as this is based on single-point comparisons without exploring intermediate architectural variations.
- **Low Confidence**: The assertion that tiling reduces hallucination (POPE results), as the mechanism linking higher resolution to reduced hallucination is not explicitly tested or explained beyond correlational observation.

## Next Checks

1. **Attention Fusion Ablation**: Implement and test a cross-tile attention mechanism (replacing concatenation) on LLaVA-Bench and POPE to quantify the coherence improvement potential, specifically comparing 3×3 with-attention vs. 3×3 with-concatenation.
2. **Resolution Sensitivity Analysis**: Systematically vary the input resolution for both tiles and global context (e.g., 224px, 336px, 448px) on ScienceQA-Image to determine the optimal resolution trade-off between detail preservation and computational cost.
3. **Architecture Generalization Study**: Replicate the tiling approach with alternative vision encoders (e.g., CLIP-ViT-L/14, DINOv2) and LLM scales (7B vs. 13B) to test whether the observed benefits generalize beyond the LLaVA-ViT-S/16 configuration.