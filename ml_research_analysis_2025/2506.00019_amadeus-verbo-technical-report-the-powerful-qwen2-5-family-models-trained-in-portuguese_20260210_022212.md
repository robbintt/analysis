---
ver: rpa2
title: 'Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained
  in Portuguese'
arxiv_id: '2506.00019'
source_url: https://arxiv.org/abs/2506.00019
tags:
- qwen2
- amadeus-v
- zhang
- assin2
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report describes the development of the Amadeus-Verbo
  family of large language models for Brazilian Portuguese. The approach involves
  fine-tuning Qwen2.5 base and instruction models (0.5B to 72B parameters) on a dataset
  of approximately 600k instruction examples using full-parameter fine-tuning.
---

# Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese
## Quick Facts
- arXiv ID: 2506.00019
- Source URL: https://arxiv.org/abs/2506.00019
- Reference count: 11
- Primary result: Fine-tuned Qwen2.5 models achieve better Portuguese performance than base models

## Executive Summary
This technical report describes the development of the Amadeus-Verbo family of large language models for Brazilian Portuguese. The approach involves fine-tuning Qwen2.5 base and instruction models (0.5B to 72B parameters) on a dataset of approximately 600k instruction examples using full-parameter fine-tuning. The models were evaluated on a Portuguese language benchmark covering tasks like reading comprehension, sentiment analysis, and legal exams.

Results show that the fine-tuned Amadeus-Verbo models perform equal to or better than the original Qwen2.5 models on Brazilian Portuguese tasks, with notable improvements in areas like semantic textual similarity and natural language inference. The work demonstrates how fine-tuning foundation models can effectively create specialized LLMs for specific languages and use cases.

## Method Summary
The authors developed the Amadeus-Verbo model family by fine-tuning Qwen2.5 base and instruction models across multiple sizes (0.5B to 72B parameters) using full-parameter fine-tuning. The training data consisted of approximately 600,000 instruction examples specifically curated for Brazilian Portuguese. The fine-tuning process was conducted using standard optimization techniques with the goal of adapting the models to Portuguese language tasks while maintaining the general capabilities of the base models.

## Key Results
- Amadeus-Verbo models achieve better performance than base Qwen2.5 models on Brazilian Portuguese benchmarks
- Notable improvements in semantic textual similarity and natural language inference tasks
- Performance improvements maintained across model sizes from 0.5B to 72B parameters

## Why This Works (Mechanism)
The mechanism underlying this work relies on transfer learning through fine-tuning, where pre-trained foundation models are adapted to specific language domains. By using full-parameter fine-tuning on Portuguese instruction data, the model weights are updated across all layers to better capture language-specific patterns, idiomatic expressions, and task structures relevant to Brazilian Portuguese. The instruction-tuning approach enables the models to follow human-readable prompts and perform zero-shot or few-shot learning on new tasks within the Portuguese language domain.

## Foundational Learning
- Fine-tuning methodology: Why needed - to adapt pre-trained models to specific languages; Quick check - monitor validation loss during training
- Instruction tuning: Why needed - to improve model's ability to follow prompts; Quick check - test with few-shot examples
- Cross-lingual transfer: Why needed - to leverage knowledge from English to Portuguese; Quick check - compare with monolingual baselines
- Benchmark evaluation: Why needed - to measure performance improvements; Quick check - establish statistical significance

## Architecture Onboarding
**Component Map**: Qwen2.5 base model -> Portuguese instruction data -> Full-parameter fine-tuning -> Amadeus-Verbo model
**Critical Path**: Base model → Dataset preparation → Fine-tuning → Evaluation → Model deployment
**Design Tradeoffs**: Full-parameter fine-tuning vs. parameter-efficient methods (higher performance but more computational cost)
**Failure Signatures**: Overfitting to training data, loss of multilingual capabilities, performance degradation on non-Portuguese tasks
**First Experiments**: 1) Compare performance with and without Portuguese instruction tuning; 2) Test different fine-tuning dataset sizes; 3) Evaluate on zero-shot vs. few-shot tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of transparency in dataset composition and potential biases
- Limited evaluation to single Portuguese benchmark without comprehensive statistical analysis
- No assessment of performance on English or multilingual tasks

## Confidence
- Claim: Amadeus-Verbo models perform better than Qwen2.5 on Brazilian Portuguese tasks - Medium confidence
- Claim: Improvements in semantic textual similarity and NLI tasks - Low confidence
- Claim: Fine-tuning effectively creates specialized LLMs for specific languages - High confidence

## Next Checks
1. Conduct statistical significance testing comparing Amadeus-Verbo and Qwen2.5 model performances across all evaluated tasks, reporting p-values and confidence intervals
2. Perform ablation studies by fine-tuning on subsets of the Portuguese instruction data to determine which data characteristics drive performance improvements
3. Evaluate model performance degradation on English and multilingual tasks to quantify the trade-off from Portuguese specialization