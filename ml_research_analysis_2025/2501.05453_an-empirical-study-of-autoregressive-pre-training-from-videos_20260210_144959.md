---
ver: rpa2
title: An Empirical Study of Autoregressive Pre-training from Videos
arxiv_id: '2501.05453'
source_url: https://arxiv.org/abs/2501.05453
tags:
- tokens
- video
- arxiv
- learning
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Toto trains large transformer models to autoregressively predict
  future visual tokens in videos and images. The approach treats videos as sequences
  of visual tokens, using a causal transformer to predict the next token, and is pre-trained
  on over one trillion visual tokens from diverse datasets.
---

# An Empirical Study of Autoregressive Pre-training from Videos

## Quick Facts
- **arXiv ID**: 2501.05453
- **Source URL**: https://arxiv.org/abs/2501.05453
- **Reference count**: 18
- **Primary result**: Autoregressive pre-training on videos achieves competitive performance across image and video tasks using middle-layer representations.

## Executive Summary
This paper introduces Toto, a method for pre-training large transformer models to autoregressively predict future visual tokens in videos and images. By treating videos as sequences of visual tokens and training causal transformers on over one trillion tokens from diverse datasets, the approach achieves competitive performance across multiple downstream tasks. Despite minimal inductive biases, the models learn effective representations, and scaling analysis shows performance follows a power law similar to language models, albeit at a slower rate.

## Method Summary
Toto converts images and video frames into discrete visual tokens using a dVAE tokenizer, then flattens these into 1D sequences for autoregressive prediction. The model uses a causal transformer architecture (LLaMA-style with RMSNorm, SwiGLU, and RoPE) to predict the next token in raster-scan order. Three model sizes are explored (base, large, 1B parameters), trained on a mixture of ImageNet, Kinetics-600, Ego4D, and HowTo100M data. Downstream evaluation uses attention pooling at middle network layers, which consistently outperforms both average pooling and final-layer probing.

## Key Results
- Middle-layer probing (not final layers) yields optimal performance across all model sizes and tasks
- LLaMA architecture with RoPE and SwiGLU significantly outperforms standard GPT-2 on vision tasks
- Resolution-adaptive pre-training (low-res pre-train, high-res fine-tune) is more compute-efficient than high-res pre-training
- Scaling follows power-law behavior similar to language models but at a slower rate

## Why This Works (Mechanism)

### Mechanism 1: Unified Sequence Modeling via Visual Tokenization
If visual data is converted into discrete tokens and flattened into a 1D sequence, a standard causal transformer can learn representations by predicting future tokens, similar to language modeling. The model minimizes negative log-likelihood by predicting the next token in raster-scan order, forcing the attention mechanism to model both spatial intra-frame relationships and temporal inter-frame dynamics within a single context window.

### Mechanism 2: Middle-Layer Representation Extraction
In decoder-only autoregressive models, optimal features for discriminative downstream tasks are found in middle layers, not final output layers. The first half of the network acts as a "compressor" or encoder, transforming input tokens into semantic features, while the latter half acts as a "generator" or decoder, projecting these features back into the high-dimensional input space for reconstruction.

### Mechanism 3: Resolution-Adaptive Positional Embeddings
Pre-training at lower resolutions with RoPE and fine-tuning at higher resolutions is more compute-efficient than pre-training directly at high resolution. RoPE generalizes positional relationships better than absolute embeddings, allowing the model to extrapolate to larger sequence lengths (higher image resolution) during fine-tuning without retraining the entire backbone.

## Foundational Learning

- **Causal vs. Bidirectional Masking**: Toto uses a causal (GPT-style) mask where tokens can only attend to the past, distinct from bidirectional masks used in many vision models. Quick check: Can a token at position $t$ attend to a token at position $t+5$ in the Toto architecture? (No)

- **Vector Quantization (VQ) / Discrete Tokenizers**: The paper compares dVAE and VQGAN, which convert continuous image patches into a finite dictionary of discrete integers. Quick check: Why does the paper claim VQGAN might have "contamination" issues compared to dVAE? (VQGAN uses perceptual loss which leaks ImageNet labels)

- **Attention Pooling**: The authors argue Average Pooling fails in decoder models because the receptive field is "skewed" (first token sees nothing, last sees everything). Attention Pooling is introduced as a solution to dynamically weight these uneven tokens. Quick check: Why does a skewed receptive field make average pooling a poor choice for aggregating features across a sequence? (Early tokens have limited context, late tokens have full context)

## Architecture Onboarding

- **Component map**: Input (Video Frames & Images) -> Tokenizer (dVAE) -> Backbone (LLaMA-style Transformer) -> Probe Head (Attention Pooling at middle layers)
- **Critical path**: The quality of final representation depends heavily on selecting the correct probing layer. Using final layer will yield significantly worse performance than using middle layer.
- **Design tradeoffs**: dVAE chosen over VQGAN due to data contamination concerns; LLaMA chosen over GPT-2 for significantly better vision task performance.
- **Failure signatures**: Using Average Pooling instead of Attention Pooling causes ~7-8% drop in accuracy; video redundancy can cause the model to "cheat" by copying rather than predicting.
- **First 3 experiments**: 1) Train small Toto-base on ImageNet only, verify middle-layer probing superiority. 2) Compare Attention Pooling vs. Average Pooling to confirm performance gap. 3) Pre-train at low resolution and fine-tune at high resolution to verify efficiency claims.

## Open Questions the Paper Calls Out

1. **Universal Visual Tokenizer**: How can we develop a universal visual tokenizer that supports end-to-end learning without being constrained by current quantized vectors? The current reliance on tokenizers makes learning non-end-to-end and representation quality is bounded by tokenizer quality.

2. **Video Redundancy Mitigation**: How can video pre-training be adapted to mitigate the negative impact of temporal redundancy on representation learning? Added redundancy in video frames can hurt quality of learned representations because subsequent frames are significantly easier to predict than the first frame.

3. **Scaling Rate Differences**: Is the observed slower scaling rate for visual autoregressive models compared to LLMs an intrinsic property of visual data or a result of current modeling choices? Toto scales with a power law coefficient of -0.0378 compared to GPT-3's -0.048, suggesting visual models scale but at a slower rate.

## Limitations

- The claim of training on "over one trillion visual tokens" lacks specification of total training steps or epochs, creating uncertainty about reported performance representativeness
- The dVAE tokenizer checkpoint location and acquisition method are underspecified ("from DALL-E" lacks sufficient detail)
- Precise data mixing implementation is unclear, particularly how 16 images are batched versus 16 video frames within a single sequence
- Generalization to novel domains beyond evaluated tasks remains unproven despite competitive benchmark performance

## Confidence

**High Confidence**: Middle-layer probing strategy consistently outperforms final-layer probing across all evaluated model sizes and tasks; LLaMA architecture with RoPE and SwiGLU significantly outperforms standard GPT-2 on vision tasks (53.2% vs 48.5% top-1 accuracy); Attention Pooling outperforms Average Pooling by 7.9% on ImageNet classification.

**Medium Confidence**: Resolution-adaptive pre-training is more compute-efficient than pre-training directly at high resolution (though lacks direct comparison); scaling analysis showing power-law behavior similar to language models (occurs "albeit at a slower rate" without quantified difference).

**Low Confidence**: The claim that video frames contain sufficient signal for next-token prediction despite high redundancy (paper acknowledges redundancy issues but doesn't provide strong evidence that model learns semantic content rather than exploiting temporal redundancy).

## Next Checks

1. **Tokenization Pipeline Verification**: Implement the exact frame tokenization pipeline using the specified dVAE tokenizer (8k vocab from DALL-E). Resize to 128×128 → 16×16 tokens/frame, create 4096-token sequences from 16 frames (sample every 4 frames), and verify that the tokenization produces expected vocabulary coverage and reconstruction quality.

2. **Middle-Layer Probing Validation**: Train a small Toto-base model on ImageNet only using dVAE tokens. Verify that validation loss follows power law and that probing accuracy peaks at middle layers (e.g., layer 6 of 12). Compare against final-layer probing to confirm the 7-8% performance gap.

3. **Resolution Fine-tuning Test**: Pre-train at 16×16 tokens (128px) and fine-tune at 32×32 (256px) using RoPE. Verify if this "resolution jump" improves performance over fixed-resolution training as claimed in Table 4, testing the resolution-adaptive positional embeddings hypothesis directly.