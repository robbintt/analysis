---
ver: rpa2
title: Single-agent or Multi-agent Systems? Why Not Both?
arxiv_id: '2505.18286'
source_url: https://arxiv.org/abs/2505.18286
tags:
- theta
- code
- task
- agent
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the diminishing performance advantage of
  multi-agent systems (MAS) over single-agent systems (SAS) as large language models
  (LLMs) become more capable. Empirical results show that MAS's accuracy improvement
  over SAS drops from 10.7% to 3.0% when using stronger models, while MAS incurs 4-220x
  more prefill tokens.
---

# Single-agent or Multi-agent Systems? Why Not Both?

## Quick Facts
- arXiv ID: 2505.18286
- Source URL: https://arxiv.org/abs/2505.18286
- Reference count: 40
- Single-agent systems often match or exceed multi-agent systems in accuracy while using 4-220x fewer tokens

## Executive Summary
This study systematically compares single-agent (SAS) and multi-agent systems (MAS) across 15 datasets and 7 tasks, revealing that MAS's performance advantage diminishes as models become stronger. While MAS achieves 10.7% accuracy improvement over SAS with weaker models, this drops to only 3.0% with stronger models. Critically, MAS incurs 4-220× more prefill tokens, making it significantly more expensive. The research identifies three key defects in MAS: node-level bottlenecks from weakest agents, edge-level information overload, and path-level information loss through summarization. A confidence-guided tracing method and hybrid agent routing strategies demonstrate that selectively combining SAS and MAS can achieve better accuracy-cost tradeoffs than using either paradigm alone.

## Method Summary
The study evaluates SAS and MAS across 15 datasets (HumanEval, GSM8K, MBPP, etc.) using 9 frameworks including MetaGPT, ChatDev, and Debate. Experiments use various models (GPT-4, Gemini-2.0-Flash) and measure accuracy (pass@1, exact match) and cost (prefill + 2×decode tokens). Three optimization strategies are implemented: (1) Confidence-guided critical agent identification using self-reported confidence scores to target agent upgrades, (2) Agent routing with an LLM rater assessing task difficulty to selectively route between SAS and MAS, and (3) Agent cascade attempting SAS first and escalating to MAS only on verification failure.

## Key Results
- MAS accuracy improvement over SAS drops from 10.7% to 3.0% when using stronger models
- MAS consumes 4-220× more prefill tokens than SAS across all tasks
- Confidence-guided tracing improves accuracy by 1.1-12% by targeting critical agent upgrades
- Hybrid routing achieves up to 88.1% cost reduction while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Routing Based on Complexity Estimation
A lightweight LLM rater assesses request difficulty (1-10) and routes above-threshold requests to MAS, others to SAS. The rater prompt is cacheable with minimal overhead. This works because the rater's difficulty estimates correlate with actual task complexity and MAS's relative advantage. Break condition: poor rater accuracy leads to unnecessary MAS invocation on simple tasks or incorrect routing to SAS for complex tasks.

### Mechanism 2: Confidence-Guided Critical Agent Identification
Agents report confidence scores (1-10), and importance scores are updated using I_i = Σ(Q_r·c_i + (s-c_i)(1-Q_r)) where Q_r is final output quality. The lowest-scoring agent is identified as critical and upgraded. This works because agent confidence meaningfully correlates with output quality, making targeted upgrades more cost-effective than uniform upgrades. Break condition: uncalibrated confidence scores misidentify critical agents, leading to suboptimal augmentation.

### Mechanism 3: Cascade Verification with SAS-First Attempt
Requests first route to SAS, with outputs evaluated by a verifier. If satisfactory, return result; otherwise escalate to MAS. Total cost: (1-p)·C_MAS(r) + C_SAS(r) where p is proportion passing SAS. This works because SAS processing cost is significantly lower than MAS and verification is cheap/accurate. Break condition: expensive or unreliable verification diminishes cascade benefits.

## Foundational Learning

- **Concept: Directed Acyclic Graph (DAG) Representation of Agent Workflows**
  - Why needed here: Formalizes agentic execution as G=(V,E) where nodes are agents/tools and edges are message dependencies, underpinning defect analysis and optimization strategies
  - Quick check question: Given a 3-agent debate system where agents pass outputs to a summarizer that returns to all agents for another round, draw the graph—does it contain cycles?

- **Concept: Token Cost Decomposition (Prefill vs. Decode)**
  - Why needed here: Cost model C(r) = Σ|w_i,j|×(c_out(i)+c_in(j)) distinguishes input (prefill) and output (decode) costs, critical for understanding why MAS is 4-220× more expensive
  - Quick check question: If decode tokens cost 2× prefill tokens, and MAS generates 5× more prefill tokens but 3× more decode tokens, what's the relative cost ratio?

- **Concept: Confidence Calibration in LLM Outputs**
  - Why needed here: Confidence-guided tracing assumes self-reported confidence correlates with correctness; poor calibration (overconfident on wrong answers) breaks importance scoring
  - Quick check question: If an agent is 90% confident on 80% of correct answers and 60% confident on 20% of incorrect answers, is it well-calibrated?

## Architecture Onboarding

- **Component map:** Rater Agent → Router → (SAS Pipeline → Verifier → Return) OR (MAS Pipeline → Return)
- **Critical path:** 1. Request → Rater (difficulty score) 2. If score < threshold → SAS → Verifier → Return (or escalate if fail) 3. If score ≥ threshold → MAS → Return 4. Periodically: Confidence Tracker updates importance scores → identify critical agent → upgrade
- **Design tradeoffs:** Higher routing threshold lowers cost but risks accuracy drop on edge cases; lower threshold increases cost and over-processes simple tasks; cascade adds SAS latency for all requests vs direct routing adding rater latency
- **Failure signatures:** Node-level defect (accuracy plateaus despite upgrades); Edge-level defect (agent overwhelmed by upstream inputs); Path-level defect (correct intermediate results lost); Routing failure (high discrepancy between predicted and actual difficulty)
- **First 3 experiments:** 1. Baseline calibration: run identical task set through both SAS and MAS with same model; 2. Routing threshold sweep: implement rater with thresholds 3-8 on validation set; 3. Critical agent ablation: upgrade one agent at a time vs random agent upgrade

## Open Questions the Paper Calls Out
- How do MAS defects manifest in non-task-solving applications like social simulation or creative writing?
- Can the "Path-Level Defect" (irreversible information loss via summarization) be mitigated without resorting to hybrid SAS-MAS routing?
- Do the diminishing MAS advantages persist when using fine-tuned, domain-specific models?

## Limitations
- Experiments focused exclusively on general-purpose models, leaving fine-tuned models as future work
- Conclusions based on task-solving benchmarks; generalizability to other agentic paradigms (social simulation, creative writing) is unknown
- Assumes verifier costs are negligible, which may not hold for complex validation tasks

## Confidence
- **High Confidence:** MAS's diminishing advantage with stronger models (10.7% → 3.0% accuracy gain) and disproportionate token cost (4-220× prefill tokens)
- **Medium Confidence:** Confidence-guided critical agent identification mechanism and hybrid routing paradigms, as results depend on rater accuracy and confidence calibration

## Next Checks
1. Test agent routing and cascade paradigms on frameworks beyond Debate (e.g., MetaGPT, ChatDev) to verify generalizability across different multi-agent architectures
2. Systematically measure confidence score calibration across multiple agent types and tasks by comparing self-reported confidence with actual accuracy rates
3. Evaluate how different LLM pricing models (varying prefill vs decode costs) and caching strategies affect the accuracy-cost tradeoffs claimed for hybrid approaches