---
ver: rpa2
title: 'SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional
  Representations'
arxiv_id: '2508.02901'
source_url: https://arxiv.org/abs/2508.02901
tags:
- sensorial
- liwc
- style
- language
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between sensorial language
  and traditional stylistic features using a novel Reduced-Rank Ridge Regression approach.
  The authors demonstrate that low-dimensional latent representations of LIWC features
  (r=24) effectively capture stylistic information for sensorial language prediction
  compared to the full feature set (r=74).
---

# SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations

## Quick Facts
- arXiv ID: 2508.02901
- Source URL: https://arxiv.org/abs/2508.02901
- Authors: Osama Khalid; Sanvesh Srivastava; Padmini Srinivasan
- Reference count: 24
- Key outcome: SLIM-LLMs with low-rank LIWC features match full-scale models while reducing parameters by up to 80%, achieving up to 0.545 accuracy in sensorial word prediction

## Executive Summary
This paper introduces SLIM-LLMs (Stylometrically Lean Interpretable Models) to model the relationship between sensorial language and traditional stylistic features through low-dimensional representations. The authors propose a Reduced-Rank Ridge Regression approach that extracts 24-dimensional latent representations from 74-dimensional LIWC features, demonstrating these capture stylistic information effectively for sensorial language prediction. Across five genres, SLIM-LLMs achieve performance comparable to full-scale language models while reducing parameters by up to 80%. The method combines SVD-projected BERT embeddings with latent LIWC features, trained via an MLP classifier for masked sensorial word prediction.

## Method Summary
The study investigates sensorial language prediction by mapping LIWC stylistic features to sensorial words using Reduced-Rank Ridge Regression (R4) with rank 24. For SLIM-LLMs, BERT-base embeddings for masked sensorial tokens are projected to rank 240 via SVD, then concatenated with 24-dimensional latent LIWC features (extracted via the R4 transformation matrix) and fed into an MLP classifier. The model is trained on 300k sentences from five genres containing words from the Lancaster Sensorimotor Lexicon. Evaluation uses 5-fold cross-validation on a single T4 GPU for 10 epochs, measuring multi-class classification accuracy over 18,749 sensorial words.

## Key Results
- SLIM-LLMs achieve up to 0.545 accuracy in predicting sensorial words across five genres
- Low-dimensional latent representations (r=24) match performance of full 74-dimensional LIWC features
- Parameter reduction of up to 80% compared to full-scale language models while maintaining comparable performance
- Rank-24 LIWC projections effectively capture stylistic information for sensorial language prediction

## Why This Works (Mechanism)
The approach works by identifying low-dimensional manifolds in both stylistic feature space and semantic embedding space that preserve the essential relationships for sensorial language prediction. By projecting high-dimensional LIWC vectors to 24 dimensions via Reduced-Rank Ridge Regression, the model captures the most informative stylistic dimensions while eliminating redundancy. The combination of SVD-reduced BERT embeddings with these latent LIWC features creates a compact representation that preserves both semantic and stylistic information necessary for predicting sensorial words, achieving comparable performance to much larger models through targeted dimensionality reduction.

## Foundational Learning
- **Reduced-Rank Ridge Regression**: A regression technique that constrains the solution to a lower-dimensional subspace; needed to extract informative latent features from high-dimensional LIWC vectors while preventing overfitting.
- **SVD-based Embedding Projection**: Singular Value Decomposition applied to embedding matrices to reduce dimensionality while preserving variance; needed to create compact semantic representations that can be effectively combined with stylistic features.
- **Multi-Layer Perceptron Classification**: Feedforward neural network architecture for mapping concatenated feature representations to target classes; needed to learn non-linear relationships between combined stylistic and semantic features.
- **Lancaster Sensorimotor Lexicon**: A vocabulary of words associated with sensory and motor experiences; needed to define the target vocabulary for sensorial language prediction tasks.
- **LIWC Feature Extraction**: Linguistic Inquiry and Word Count tool for extracting psychological and stylistic features from text; needed to quantify traditional stylistic dimensions for modeling their relationship with sensorial language.
- **Masked Language Modeling**: Prediction task where a word is masked and the model must predict it from context; needed as the evaluation framework for assessing sensorial word prediction capabilities.

## Architecture Onboarding

**Component Map:**
Raw Text -> LIWC Vector Extraction -> R4 (Reduced-Rank Ridge Regression) -> Latent LIWC Features -> BERT Embedding + SVD Projection -> MLP Classifier -> Sensorial Word Prediction

**Critical Path:**
Text preprocessing → LIWC feature calculation (excluding target word) → R4 latent feature extraction → BERT embedding and SVD projection → MLP training → Prediction

**Design Tradeoffs:**
- **Dimensionality vs. Performance**: Using rank 24 for LIWC features balances information preservation with computational efficiency, though higher ranks might capture more nuance at increased computational cost.
- **Semantic vs. Stylistic Integration**: Combining BERT embeddings with LIWC features through simple concatenation versus more complex fusion mechanisms; chosen for simplicity and interpretability.
- **Model Complexity vs. Interpretability**: SLIM-LLMs prioritize interpretability through explicit stylistic feature integration while maintaining competitive performance.

**Failure Signatures:**
- **Overfitting**: High training accuracy but poor validation performance indicates the MLP is too complex relative to the data size.
- **Feature Leakage**: Including the target sensorial word in LIWC calculation will artificially inflate performance metrics.
- **Rank Insufficiency**: If r=24 is too low, the latent LIWC features won't capture necessary stylistic information, resulting in degraded prediction accuracy.

**First Experiments to Run:**
1. **Ablation Study**: Train models with varying LIWC ranks (12, 24, 36, 48) to determine optimal dimensionality for capturing stylistic information.
2. **Feature Importance Analysis**: Use SHAP values or similar methods to quantify the contribution of latent LIWC features versus SVD-projected BERT embeddings to prediction accuracy.
3. **Cross-Genre Transfer**: Evaluate a model trained on one genre (e.g., Yelp) on test data from another genre (e.g., Wikipedia) to assess generalizability of learned style-sensory relationships.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the SLIM-LLM dimensionality reduction approach be effectively applied to decoder-only architectures like GPT? The current implementation was restricted to encoder-only transformer architectures, and successful adaptation would demonstrate the methodology's broader applicability.
- **Open Question 2**: Do the relationships between stylometric features and sensorial language persist across different languages and cultures? The study used exclusively English content, and multilingual validation would determine if the 24-dimension latent structure is universal or language-specific.
- **Open Question 3**: How does the interaction between traditional style and sensorial style evolve temporally within an author's writing history? The current methodology treats linguistic style as static without accounting for temporal evolution across extended publication periods.
- **Open Question 4**: Do native speaker judgments align with the low-dimensional model's predictions of sensorial language use? The evaluation relies primarily on quantitative metrics, and human qualitative validation would provide additional evidence for the model's effectiveness.

## Limitations
- Key implementation details such as MLP architecture specifications and regularization hyperparameters are not provided, limiting reproducibility.
- The study focuses on a specific task (masked sensorial word prediction) which may limit generalizability to other stylistic prediction tasks.
- The methodology treats linguistic style as static without accounting for temporal evolution within an author's writing history.

## Confidence
- **High Confidence**: The core conceptual framework using low-dimensional latent representations for stylistic feature reduction and combining with semantic embeddings is well-articulated and methodologically sound.
- **Medium Confidence**: The claim that SLIM-LLMs achieve performance comparable to full-scale language models while reducing parameters by up to 80% is supported by the methodology, though exact replication would require the unspecified hyperparameters.
- **Medium Confidence**: The finding that rank-24 representations capture stylistic information effectively compared to full 74-dim features is theoretically justified, but empirical validation would benefit from additional sensitivity analyses.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the regularization parameter λ in the Reduced-Rank Ridge Regression and test different MLP architectures to establish the robustness of the reported 0.545 accuracy across the five genres.
2. **Ablation Study on Feature Dimensions**: Conduct experiments comparing SLIM-LLM performance across multiple latent dimensions (e.g., r=12, 24, 36, 48) to validate that r=24 is indeed optimal for this task.
3. **Cross-Dataset Generalization Test**: Evaluate the trained SLIM-LLM models on an independent sensorial language dataset not included in the original five genres to assess the generalizability of the learned style-sensory relationships.