---
ver: rpa2
title: 'Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic
  Tasks'
arxiv_id: '2512.07697'
source_url: https://arxiv.org/abs/2512.07697
tags:
- delay
- da-dp
- inference
- policy
- delays
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of inference delay in dynamic
  robotic tasks, where the gap between observation and execution causes actions to
  lag behind fast-moving objects. The authors propose Delay-Aware Diffusion Policy
  (DA-DP), which explicitly incorporates inference delays into policy learning by
  correcting training data to account for delayed execution states and conditioning
  the policy on measured delay.
---

# Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks

## Quick Facts
- arXiv ID: 2512.07697
- Source URL: https://arxiv.org/abs/2512.07697
- Reference count: 25
- Primary result: DA-DP maintains significantly higher success rates (0.96-0.72) compared to standard diffusion policy (0.20-0.01) across varying delay conditions in dynamic robotic tasks

## Executive Summary
This paper addresses the critical problem of inference delay in dynamic robotic tasks, where the gap between observation and execution causes actions to lag behind fast-moving objects. The authors propose Delay-Aware Diffusion Policy (DA-DP), which explicitly incorporates inference delays into policy learning by correcting training data to account for delayed execution states and conditioning the policy on measured delay. The method is evaluated across three dynamic tasks with different robot morphologies and demonstrates robust performance across various delay conditions.

## Method Summary
DA-DP processes zero-delay trajectories to create delay-compensated versions by skipping states and compressing trajectories, then trains diffusion policies on these corrected datasets while conditioning on delay values. The approach consists of two key components: (1) delay-aware data processing that compensates for inference latency by compressing trajectories so the robot reaches target states at the correct time despite delays, and (2) delay conditioning that explicitly incorporates measured delay into the policy input through state augmentation. The method is trained on multiple delay values to create a policy that generalizes within and slightly beyond the training delay distribution.

## Key Results
- In pick up rolling ball task: DA-DP achieves success rates of 0.96 and 0.72 at 0.05s and 0.10s delays versus 0.20 and 0.01 for baseline diffusion policy
- Across all tasks: DA-DP maintains significantly higher success rates compared to standard diffusion policy across various delay conditions
- Demonstrates robustness to delays both within and outside the training distribution, maintaining 0.62-0.48 success on out-of-distribution delays

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Compression for Temporal Alignment
Delay-aware data processing compensates for inference latency by compressing trajectories so the robot reaches target states at the correct time despite delays. The algorithm computes adjusted length n' = (n∆t + δ)/(∆t + δ/H_act), skips m = δ/∆t states after each action chunk, then reconstructs actions transitioning between compressed states: a'_i = s'_{i+1} - s'_i.

### Mechanism 2: Delay Conditioning as Policy Input
The policy is explicitly conditioned on measured delay by concatenating the delay δ with the state observation to form an augmented state ŝ = concat(s, δ). This enables the network to learn a family of policies parameterized by delay, producing different action patterns for different δ values.

### Mechanism 3: Multi-Delay Training for Distributional Robustness
Training on multiple delay values creates a policy that generalizes within and slightly beyond the training delay distribution. By exposing the policy to a range of temporal misalignments through multiple delay-aware datasets {τ'_δ} with varying δ, the method enables interpolation between learned delay-response patterns.

## Foundational Learning

- Concept: Diffusion Policy (DDPM for sequential decision-making)
  - Why needed here: DA-DP extends diffusion policy; understanding denoising process, training objectives, and action horizon H_act is prerequisite.
  - Quick check question: Can you explain how diffusion policy generates action sequences conditioned on observations, and what happens during the forward vs. reverse diffusion process?

- Concept: Inference delay composition and measurement
  - Why needed here: The method requires understanding what contributes to δ (sensing, computation, actuation) and how to measure it; incorrect measurements produce misaligned data.
  - Quick check question: Given a robotic system, can you identify all latency sources and design a procedure to measure total inference delay from observation capture to actuation?

- Concept: Trajectory time alignment and discrete control
  - Why needed here: Data processing relies on control timesteps ∆t, action chunk size H_act, and computing adjusted lengths accounting for accumulated delays.
  - Quick check question: For a trajectory of 30 timesteps with ∆t=0.02s, H_act=8, and δ=0.06s, what is the compressed trajectory length n'?

## Architecture Onboarding

- Component map:
  Delay-aware data processor -> Diffusion policy backbone -> Delay conditioning module -> Training pipeline

- Critical path:
  1. Measure system inference delay δ (or target range)
  2. Collect zero-delay expert demonstrations
  3. Apply Algorithm 1 to generate delay-compensated trajectories for each δ in training set
  4. Train diffusion policy on combined datasets with delay-augmented conditioning
  5. At deployment, measure actual δ and provide as policy input

- Design tradeoffs:
  - **Discrete n' rounding**: Must round adjusted length to integer; choice of floor/ceil affects temporal precision
  - **Smoothing**: Optional trajectory smoothing after compression trades motion naturalness against exact timing
  - **Delay range breadth**: Wider training ranges improve robustness but may dilute performance at specific delays
  - **Action representation**: a'_i = s'_{i+1} - s'_i assumes position deltas; other action spaces require adaptation

- Failure signatures:
  - Policy executes at wrong time despite conditioning → check delay measurement accuracy and data processing
  - Performance collapses only at OOD delays → expand training delay distribution
  - Smooth but chronically late motions → debug trajectory compression (verify n', skip pattern)
  - High variance across delay conditions → insufficient delay space coverage in training

- First 3 experiments:
  1. **Delay measurement validation**: Run multiple control cycles, timestamp observation capture and action execution, compute δ statistics (mean, variance). Verify alignment with values used for data processing.
  2. **Single-delay sanity check**: Train DA-DP on one delay value, evaluate at that exact delay. Success should significantly exceed baseline DP. If not, debug Algorithm 1 implementation (verify n' calculation, state indexing, action reconstruction).
  3. **Interpolation test**: Train on {δ₁, δ₃}, evaluate at interpolated δ₂ where δ₁ < δ₂ < δ₃. Tests whether policy learns continuous delay-response mapping vs. memorizing discrete modes.

## Open Questions the Paper Calls Out
None

## Limitations
- Trajectory compression approach assumes predictable dynamics and may fail in highly chaotic environments where state evolution during the delay window cannot be anticipated
- Reliance on accurate delay measurement creates vulnerability to latency estimation errors, particularly in systems with high variability
- Performance degrades for out-of-distribution delays, suggesting limits to interpolation capabilities despite claims of robustness

## Confidence
- **High Confidence**: Core mechanism of trajectory compression for temporal alignment and delay conditioning producing measurable performance improvements
- **Medium Confidence**: Claims of robustness to out-of-distribution delays (success rates 0.62-0.48 at OOD delays vs near-zero for baseline) require independent validation across more diverse delay regimes
- **Low Confidence**: Generalization to highly chaotic dynamics or systems with unmeasurable delays remains speculative based on current experimental scope

## Next Checks
1. **Delay Measurement Robustness**: Systematically vary delay measurement accuracy (±20% error) and quantify performance degradation to establish sensitivity to latency estimation errors
2. **Cross-Domain Transfer**: Apply DA-DP to a non-ManiSkill3 environment (e.g., real robot or different simulator) with different dynamics characteristics to test generalizability beyond the original task suite
3. **Chaotic Dynamics Stress Test**: Evaluate on tasks with increasing degrees of chaotic behavior (Lorenz attractor-like dynamics) to identify the boundary where trajectory compression fails