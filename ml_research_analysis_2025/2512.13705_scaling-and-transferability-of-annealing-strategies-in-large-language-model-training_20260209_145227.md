---
ver: rpa2
title: Scaling and Transferability of Annealing Strategies in Large Language Model
  Training
arxiv_id: '2512.13705'
source_url: https://arxiv.org/abs/2512.13705
tags:
- training
- learning
- annealing
- loss
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the transferability of annealing strategies
  in large language model training, focusing on the Warmup-Steady-Decay scheduler.
  A predictive framework incorporating training steps, maximum learning rate, and
  annealing behavior is developed to model training dynamics.
---

# Scaling and Transferability of Annealing Strategies in Large Language Model Training

## Quick Facts
- **arXiv ID:** 2512.13705
- **Source URL:** https://arxiv.org/abs/2512.13705
- **Reference count:** 35
- **Primary result:** Optimal annealing ratios follow consistent power-law patterns across model sizes, learning rates, and training steps, enabling reliable transfer from small to large models

## Executive Summary
This work investigates the transferability of annealing strategies in large language model training, focusing on the Warmup-Steady-Decay scheduler. A predictive framework incorporating training steps, maximum learning rate, and annealing behavior is developed to model training dynamics. Experiments demonstrate that smaller models can reliably proxy larger ones for optimizing annealing ratios, with optimal annealing ratios following consistent power-law patterns across model sizes, learning rates, and training steps. The Adam-style momentum formulation shows superior stability and generalizability compared to multiplicative accumulation methods, with mean absolute percentage errors below 2% for loss curve fitting across diverse configurations.

## Method Summary
The paper develops a predictive framework for modeling training dynamics by tracking loss against training steps rather than total tokens, incorporating an Adam-style momentum term for learning rate changes, and deriving power-law relationships for optimal annealing ratios. The method uses the Warmup-Steady-Decay scheduler with L-BFGS-B optimization and Huber loss for coefficient fitting. Validation is performed across Dense and MoE architectures (50M-1.5B parameters) using The Pile dataset, with transferability tested between model sizes while maintaining architectural consistency.

## Key Results
- Optimal annealing ratios ($R_{opt}$) follow power-law relationships with maximum learning rate and total training steps
- Adam-style momentum (ASMT) formulation achieves MAPE < 2% for loss curve fitting, outperforming multiplicative accumulation methods
- Training steps serve as reliable proxy for optimization progress when batch sizes exceed critical threshold ($B_{opt}$)
- Annealing ratio transferability is validated within same architecture (Dense→Dense or MoE→MoE), but not across architectures

## Why This Works (Mechanism)

### Mechanism 1: Step-Based Optimization Dynamics
- Claim: Tracking loss against training steps (updates) rather than total tokens provides a more reliable proxy for optimization progress, provided the batch size exceeds a critical threshold ($B_{opt}$).
- Mechanism: Gradient updates represent discrete units of model movement. When the batch size is sufficiently large to approximate the true gradient (reducing noise), the number of parameter updates (steps) becomes the dominant independent variable for convergence, normalizing trajectories that would otherwise diverge if measured by token consumption.
- Core assumption: The batch size is within a "practical optimal range" where training efficiency is stable and the gradient estimate is reliable.
- Evidence anchors:
  - [abstract] "incorporating training steps... enabling more efficient optimization"
  - [page 3] "training steps can serve as a reliable metric for tracking loss curves... allowing us to focus on other influential factors."
  - [page 5] Observation 2 regarding Batch Size Dependence.
  - [corpus] The paper "Learning Dynamics in Continual Pre-Training" supports the focus on learning dynamics, though it focuses on CPT rather than initial pre-training scaling.
- Break condition: If batch sizes are too small ($< B_{opt}$) or excessively large (causing instability or inefficiency), the step-based convergence assumption degrades.

### Mechanism 2: Annealing Momentum as a Kinetic Proxy
- Claim: The "annealing momentum" term, calculated via an Adam-style accumulation of learning rate (LR) changes, accurately predicts the rapid loss reduction during the decay phase.
- Mechanism: The Adam-style momentum term ($M$) captures the "kinetic effect" of the LR drop. By using exponential moving averages (similar to Adam optimizer updates) on the *change* in learning rate ($\Delta \eta_t$), the model smooths out the decay dynamics and fits the final loss drop better than multiplicative accumulation (CMMT), which suffers from instability.
- Core assumption: The loss reduction during annealing follows a predictable dynamic related to the rate and magnitude of LR decay, which can be integrated over time.
- Evidence anchors:
  - [page 2] "We replace the multiplicative accumulation in the momentum term with an Adam-style momentum, mitigating the risk of instability."
  - [page 3] Equations 6-9 defining the Adam-style momentum update.
  - [page 12] Comparative Analysis showing ASMT generally outperforms CMMT in transfer scenarios.
  - [corpus] "AdaLRS" explores LR transferability but does not specifically validate this Adam-style momentum mechanism; corpus evidence for this specific math is weak.
- Break condition: If the learning rate schedule is irregular or non-monotonic in ways the momentum term cannot integrate smoothly, the predictive power may drop.

### Mechanism 3: Power-Law Scaling of Annealing Ratios
- Claim: The optimal annealing ratio ($R_{opt}$) follows a power-law relationship with the maximum learning rate ($\eta_{max}$) and total training steps ($T$), enabling transfer from small to large models.
- Mechanism: There is a consistent trade-off between the "forward effect" (training at high LR) and the "annealing effect" (decaying LR). This trade-off scales predictably with model size and steps. Because the relationship is stable (power-law), one can find the optimal ratio on a small proxy model and scale it to a target large model without exhaustive search.
- Core assumption: The underlying architecture (Dense or MoE) remains consistent, and the training dynamics are governed by the proposed scaling laws.
- Evidence anchors:
  - [page 6] Equation 13: $R_{opt} = \lambda_\eta \cdot \eta_{max}^{\alpha_\eta}$
  - [page 7] Equation 14: $R_{opt} = \lambda_T \cdot T^{\alpha_T}$
  - [page 7] Figure 8 showing the decay of $R_{opt}$ with total steps.
  - [corpus] "Using Scaling Laws for Data Source Utility Estimation" reinforces the general validity of scaling laws in pre-training, though not this specific ratio.
- Break condition: Transfer claims are limited to within the same architecture class (Dense $\to$ Dense or MoE $\to$ MoE); direct transfer between Dense and MoE is not supported by the evidence.

## Foundational Learning

- Concept: **Warmup-Steady-Annealing (WSD) Schedulers**
  - Why needed here: The paper specifically optimizes for WSD, which decouples the steady-state training from the decay phase, unlike cosine schedulers which tie decay to the total period.
  - Quick check question: Why is the WSD scheduler preferred over cosine scheduling for continued training or flexible training durations?
- Concept: **Adam Optimizer Moments ($\beta_1, \beta_2$)**
  - Why needed here: The paper repurposes the mathematical structure of Adam (exponential moving averages) to model the *learning rate's* momentum, rather than the gradients'.
  - Quick check question: How does bias correction in Adam prevent skewed estimates in early training steps?
- Concept: **Optimal Batch Size ($B_{opt}$)**
  - Why needed here: The step-based scaling laws only hold when the batch size is large enough to approximate the true gradient but not so large as to cause instability.
  - Quick check question: If your batch size is significantly below $B_{opt}$, would you expect the step-based loss curves to converge? (Answer: No).

## Architecture Onboarding

- Component map: Model Size $N$, Max LR $\eta_{max}$, Steps $T$ -> Forward Term ($S$) + Annealing Momentum ($M$) -> Predictive Model ($L = \lambda_S S^{-\alpha} + \lambda_N N^{-\alpha_N} + \lambda_M M + L_0$) -> Predicted loss curve and optimal annealing ratio ($R_{opt}$)
- Critical path: Fitting the coefficients ($\lambda, \alpha$) on a small proxy model $\to$ Calculating $R_{opt}$ for the target configuration $\to$ Applying to large model training
- Design tradeoffs: The paper argues for **Adam-Style Momentum (ASMT)** over **Cumulative Multiplicative Momentum (CMMT)**. ASMT is slightly more complex to implement but offers significantly lower Mean Absolute Percentage Error (MAPE) and robustness to hyperparameter $\lambda$ selection.
- Failure signatures:
  - **High Fitting Error (MAPE > 2%):** Likely caused by using a batch size outside the optimal range or using the unstable CMMT formulation.
  - **Transfer Failure:** Attempting to transfer $R_{opt}$ across different architectures (e.g., from Dense to MoE) violates the paper's stated scope.
- First 3 experiments:
  1.  **Replicate Step vs. Token Tracking:** Train a small model (e.g., 50M params) with fixed tokens but varying batch sizes (above $B_{opt}$) and plot loss vs. steps to verify curve convergence (Figure 1).
  2.  **Validate Momentum Formulations:** Implement both ASMT and CMMT on a 500M Dense model; compare the MAPE to confirm ASMT stability (Table 5-7).
  3.  **Proxy Annealing Transfer:** Find $R_{opt}$ for a small model (e.g., 100M), apply the derived power-law scaling to predict $R_{opt}$ for a 500M model, and run a short training to validate the predicted loss minimization (Figure 6).

## Open Questions the Paper Calls Out

- **Question:** Do the power-law scaling relationships for optimal annealing ratios remain valid for models exceeding 10 billion parameters?
  - **Basis in paper:** [explicit] The "Scope and Limitation" section states experiments were limited to 7B parameters due to compute constraints and explicitly leaves "extending to 10B+" for future work.
  - **Why unresolved:** Scaling laws sometimes exhibit inflection points or "breaks" at extreme scales (e.g., Chinchilla optimality shifts), so it is unconfirmed if the observed power laws ($\alpha_\eta \approx 0.7$) persist.
  - **What evidence would resolve it:** Replicating the transferability experiments (fitting $R_{opt}$ vs. $\eta_{max}$) on a 13B or 70B parameter model to verify if the MAPE remains below 2%.

- **Question:** Can the predictive framework and optimal annealing ratios be successfully transferred between fundamentally different architectures, specifically from Dense to Mixture-of-Experts (MoE) models?
  - **Basis in paper:** [explicit] The authors explicitly state in the limitations: "Our transferability claims hold within the same architecture... We do not claim direct transfer across Dense and MoE."
  - **Why unresolved:** MoE models exhibit different training dynamics due to routing mechanisms and sparse activation, potentially altering the relationship between the momentum integral ($M$) and loss reduction.
  - **What evidence would resolve it:** Fitting the Dense model's scaling coefficients to MoE training data and measuring the prediction error; successful transfer would require finding universal coefficients ($\lambda, \alpha$) valid for both architectures.

- **Question:** Does the proposed Adam-style momentum formulation maintain high predictive accuracy when applied to non-linear decay schedules (e.g., $\beta \neq 1$)?
  - **Basis in paper:** [inferred] The paper defines the WSD scheduler with a linear decay ($\beta=1$) in Equation 12 and utilizes linear decay in the main experiments (Page 5), leaving the robustness to curved annealing profiles unexplored.
  - **Why unresolved:** The cumulative momentum term $M$ relies on the integral of $\Delta \eta_t$; non-linear decays change the distribution of $\Delta \eta_t$ over time, which might introduce bias not present in the linear case.
  - **What evidence would resolve it:** Experiments fitting the loss curve of models trained with polynomial or exponential decay schedulers to verify if the MAPE stays within the 2% threshold achieved by linear decay.

## Limitations

- The transferability framework is constrained by architectural consistency, requiring the same model type (Dense or MoE) for reliable scaling
- The step-based loss curve assumption breaks down when batch sizes fall below the optimal threshold (B_opt), which is not universally fixed but depends on model size and configuration
- The paper's evidence base is primarily synthetic validation rather than full-scale production model training, with transfer claims validated only within architectural classes rather than across fundamentally different model designs

## Confidence

- **High Confidence:** The superiority of Adam-style momentum (ASMT) over multiplicative accumulation (CMMT) for loss curve fitting, supported by comparative MAPE metrics across multiple experiments
- **Medium Confidence:** The power-law scaling relationships for optimal annealing ratios, based on controlled experiments across model sizes but limited to the same architectural family
- **Medium Confidence:** The batch size threshold hypothesis for step-based convergence, with empirical observations but no systematic exploration of the full B_opt parameter space

## Next Checks

1. **Batch Size Sensitivity Test:** Systematically vary batch sizes around the estimated B_opt threshold for a fixed model size, measuring MAPE degradation as batch size moves away from optimal values to establish the precise boundary conditions
2. **Cross-Architecture Transfer Test:** Attempt annealing ratio transfer between Dense and MoE architectures of similar parameter counts to quantify the architectural transfer gap and identify failure modes
3. **Full-Scale Production Validation:** Apply the optimal annealing ratio derived from small models to a production-scale (1B+ parameter) model training run, measuring actual convergence behavior against predictions to validate real-world applicability