---
ver: rpa2
title: Data Wrangling Task Automation Using Code-Generating Language Models
arxiv_id: '2502.15732'
source_url: https://arxiv.org/abs/2502.15732
tags:
- data
- code
- tasks
- system
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an automated system for data wrangling tasks
  using code-generating language models (LLMs). The system addresses data quality
  challenges in large tabular datasets by automatically generating executable code
  for tasks like missing value imputation, error detection, and correction.
---

# Data Wrangling Task Automation Using Code-Generating Language Models

## Quick Facts
- arXiv ID: 2502.15732
- Source URL: https://arxiv.org/abs/2502.15732
- Reference count: 5
- Primary result: Achieves 0.99 accuracy on Airline dataset using 20 LLM calls vs 0.97 accuracy with 1376 row-wise calls

## Executive Summary
This paper presents an automated system for data wrangling tasks using code-generating language models (LLMs) that addresses data quality challenges in large tabular datasets. The system automatically generates executable code for tasks like missing value imputation, error detection, and correction while significantly reducing LLM calls through pattern identification and code generation. By leveraging LLMs to identify data patterns and incorporating external knowledge when needed, the approach achieves comparable accuracy to row-wise LLM invocation while dramatically improving efficiency. Results on Airline and BigBasket datasets demonstrate the system's effectiveness in balancing accuracy and efficiency by reducing manual effort and minimizing repeated LLM calls through automated code generation.

## Method Summary
The system uses granite-34b-code-instruct-8k LLM to generate Python functions for data wrangling tasks. It begins by filtering semantically relevant columns using Histogram-based Gradient Boosting Classification Trees, then determines whether tasks are Memory-Dependent (requiring external Knowledge Base retrieval via RAG) or Memory-Independent (using Row-alone or Few-shot methods). The system employs k-fold cross-validation on ground truth data to generate multiple code snippets, which are validated and combined through majority consensus. This approach shifts computation from per-row inference to one-time pattern compilation, achieving high accuracy while dramatically reducing LLM calls from thousands to tens.

## Key Results
- Achieves 0.99 accuracy on Airline dataset imputation using only 20 LLM calls
- Reduces LLM calls from 1376 (row-wise) to 15-20 for comparable accuracy
- Successfully handles both Memory-Dependent tasks (using external KB) and Memory-Independent tasks through iterative code generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating reusable executable code once and applying it across all rows preserves accuracy while dramatically reducing LLM invocations.
- Mechanism: The LLM identifies data patterns from a small sample of ground-truth rows, codifies them into a Python function, and the generated code is executed locally without further LLM calls. This shifts computation from per-row inference to one-time pattern compilation.
- Core assumption: Data patterns are consistent enough across rows to be captured in a single code snippet; edge cases can be handled via "Unknown" returns and multi-snippet consensus.
- Evidence anchors:
  - [abstract] "Results on Airline and BigBasket datasets show the system achieves comparable accuracy to row-wise LLM invocation while significantly reducing the number of LLM calls (e.g., from 1376 to 15-20 calls)"
  - [section] Table 1: Imputation Airline accuracy 0.99 (#20 calls) vs row-wise 0.97 (#1376 calls)
  - [corpus] Related work (Dango, Buckaroo) focuses on mixed-initiative or visual wrangling; this approach uniquely emphasizes call-reduction via code generation.
- Break condition: If data patterns are highly heterogeneous or row-specific context is essential (e.g., free-text fields requiring semantic interpretation per row), single code snippets may underfit, degrading accuracy.

### Mechanism 2
- Claim: Selecting only semantically relevant columns improves code generation quality and reduces prompt/context bloat.
- Mechanism: A Histogram-based Gradient Boosting Classification Tree ranks columns by predictive relevance to the target column. The filtered dataset reduces noise and focuses LLM attention on meaningful relationships (e.g., city→state, opening/closing time→24-hour service).
- Core assumption: Statistical feature importance correlates with semantic relevance for the wrangling task; irrelevant columns introduce noise rather than signal.
- Evidence anchors:
  - [abstract] "selecting only semantically relevant columns to improve scalability"
  - [section] "our system employs a Histogram-based Gradient Boosting Classification Tree (Guryanov 2019) method to identify columns relevant to the target column, resulting in the filtered dataset eD"
  - [corpus] Weak direct corpus evidence on column selection for LLM-based wrangling; related papers focus on profiling (StructVizor) rather than targeted column filtering.
- Break condition: If the target column depends on weakly-correlated but semantically important features (e.g., rare but critical signals), gradient boosting may filter them out, causing code to miss key patterns.

### Mechanism 3
- Claim: Iterative code generation with k-fold cross-validation and majority consensus improves output robustness by capturing diverse patterns and filtering outliers.
- Mechanism: Ground-truth data G is split into k folds; each fold-out generates a code snippet trained on k-1 folds and validated on the held-out fold. Multiple snippets vote on final outputs, reducing single-snippet failure impact.
- Core assumption: Valid patterns are reproducible across different data subsets; spurious correlations appear in fewer folds and lose majority votes.
- Evidence anchors:
  - [abstract] "uses iterative refinement and cross-validation to generate multiple code snippets, with final outputs determined through majority consensus"
  - [section] "Our system employs k-fold cross-validation on G, generating code with k-1 folds and validating with the kth fold... final output is determined through a majority consensus among the outputs of all code snippets"
  - [corpus] Weak corpus signal; consensus mechanisms are not explicitly discussed in neighbor papers.
- Break condition: If the dataset has highly imbalanced patterns or sparse ground truth, k-fold splits may not contain sufficient examples per pattern, leading to unstable code generation.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The system uses RAG-inspired retrieval to incorporate external Knowledge Base mappings (e.g., city→state) for Memory-Dependent tasks.
  - Quick check question: Can you explain how RAG differs from standard LLM prompting, and why external retrieval is necessary for certain imputation tasks?

- Concept: **K-Fold Cross-Validation**
  - Why needed here: The system uses k-fold splits to generate multiple diverse code snippets, each validated on held-out data.
  - Quick check question: What problem does k-fold cross-validation solve compared to a single train/test split, and how does it apply to code generation rather than model training?

- Concept: **Code-Generating LLMs**
  - Why needed here: The system relies on LLMs trained on code corpora (granite-34b-code-instruct-8k) to produce executable Python functions from natural-language prompts and data examples.
  - Quick check question: What distinguishes a code-generating LLM from a general-purpose LLM, and what failure modes are specific to code output (e.g., syntax errors, runtime exceptions)?

## Architecture Onboarding

- Component map:
  Input -> Column Selection -> Knowledge Retrieval -> Task Router -> Code Generator -> Validation -> Consensus Engine

- Critical path:
  1. Column selection filters D → D̃
  2. KB lookup determines task type
  3. Iterative prompting generates code with examples from G
  4. K-fold cross-validation produces multiple snippets
  5. Snippets execute on full dataset; majority consensus yields final output

- Design tradeoffs:
  - Accuracy vs. Efficiency: Fewer LLM calls (15-30) vs row-wise (1300+); may sacrifice nuanced row-specific handling.
  - Memory-Dependent vs Memory-Independent: External KB improves recall for domain mappings but requires maintaining and querying KB; adds latency and dependency.
  - Row-alone vs Few-shot: Row-alone is simpler but may fail on complex patterns; Few-shot adds prompt complexity and token cost.

- Failure signatures:
  - High "Unknown" return rate: Code fails to capture patterns; check column selection and prompt example diversity.
  - Majority consensus ties or instability: K-fold splits may be too small; increase k or ground-truth size.
  - Runtime errors in generated code: LLM output may include invalid syntax or missing imports; add post-generation validation/sandboxing.
  - KB retrieval returns irrelevant data: Similarity threshold too low or KB schema mismatch; tune retrieval embedding and threshold.

- First 3 experiments:
  1. **Baseline replication**: Run the system on Airline dataset with default k-fold settings; compare accuracy and LLM call count against reported values (0.99 accuracy, 20 calls). Verify environment parity with granite-34b-code-instruct-8k.
  2. **Ablation on column selection**: Disable the Histogram-based Gradient Boosting filter; feed all columns to the LLM. Measure accuracy change and prompt token increase to quantify the selection module's contribution.
  3. **Task type stress test**: Create a synthetic dataset with mixed Memory-Dependent and Memory-Independent columns; verify correct routing via KB retrieval and measure performance delta when KB is withheld (simulating Memory-Independent fallback for Memory-Dependent tasks).

## Open Questions the Paper Calls Out

- Question: How does the system's performance scale with datasets containing significantly higher column dimensionality or more complex inter-dependencies than the Airline and BigBasket datasets?
- Basis in paper: [inferred] The evaluation is limited to two specific datasets; the paper does not analyze performance degradation as the schema complexity or the number of semantic relationships increases.
- Why unresolved: The Histogram-based Gradient Boosting method for column selection may lose effectiveness if many columns are partially relevant, potentially confusing

## Limitations

- The iterative prompt template structure and specific wording are not provided, only component descriptions
- The k-fold cross-validation configuration (k value, ground truth size) is unstated
- The external Knowledge Base structure and retrieval quality remain unspecified
- No quantitative analysis of pattern heterogeneity or edge-case frequency in datasets

## Confidence

- **High confidence**: The mechanism of reducing LLM calls through code generation is empirically validated (Table 1 shows 0.99 accuracy with 20 calls vs 0.97 with 1376 calls). The column selection approach is well-specified and directly supported by the text.
- **Medium confidence**: The k-fold cross-validation and majority consensus mechanism is described but lacks quantitative validation of its contribution versus simpler approaches. The RAG integration is mentioned but not evaluated for retrieval quality or impact on task accuracy.
- **Low confidence**: The iterative prompt generation process is underspecified - no prompt examples, iteration counts, or handling of generation failures are provided. The Memory-Dependent vs Memory-Independent task distinction is conceptually clear but lacks empirical comparison of when each succeeds or fails.

## Next Checks

1. **Pattern heterogeneity analysis**: Measure the variance in data patterns across rows in the Airline dataset. If significant heterogeneity exists, test whether single code snippets consistently capture all patterns or if accuracy degrades with increasing pattern diversity.

2. **Knowledge Base retrieval evaluation**: Implement a synthetic KB with controlled similarity scores and measure how varying the retrieval threshold affects Memory-Dependent task accuracy. Compare performance against Memory-Independent approaches on the same tasks to quantify KB value.

3. **Consensus stability testing**: Run the k-fold cross-validation with different k values (3, 5, 10) on the same ground truth data. Measure how consensus agreement rates and final accuracy change with k, identifying the minimum ground truth size needed for stable code generation.