---
ver: rpa2
title: 'Assessing the Real-World Utility of Explainable AI for Arousal Diagnostics:
  An Application-Grounded User Study'
arxiv_id: '2510.21389'
source_url: https://arxiv.org/abs/2510.21389
tags:
- arousal
- performance
- section
- human
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates explainable AI (XAI) in arousal scoring using
  polysomnography (PSG) data. Eight professional sleep scorers used a web-based decision
  support system (DSS) with black-box (BB) and white-box (WB) AI assistance, either
  from the start or as post-hoc quality control (QC).
---

# Assessing the Real-World Utility of Explainable AI for Arousal Diagnostics: An Application-Grounded User Study

## Quick Facts
- arXiv ID: 2510.21389
- Source URL: https://arxiv.org/abs/2510.21389
- Reference count: 40
- Primary result: White-box AI assistance during quality control improved arousal scoring accuracy by ~30% compared to black-box assistance.

## Executive Summary
This study evaluates explainable AI (XAI) in clinical arousal scoring using polysomnography (PSG) data. Eight professional sleep scorers used a web-based decision support system with black-box (BB) and white-box (WB) AI assistance, either from the start or as post-hoc quality control (QC). The white-box version provided explanations for AI predictions. Results show AI assistance significantly improved performance over unaided experts, with WB-QC yielding approximately 30% higher event-level accuracy than BB assistance. Participants strongly favored transparency, with seven of eight willing to adopt the system. While WB and QC approaches increased scoring time, start-time assistance was faster and preferred by most participants.

## Method Summary
The study used a modified DeepSleep U-Net architecture trained on the CPS dataset (113 PSG studies, ~3 hours each) with F2-score optimization. Post-processing applied the ALPEC framework for event merging and thresholding. Explanations were generated using DeepLIFT with a Gaussian noise baseline. Eight sleep experts scored events using a web-based DSS with BB and WB assistance in two timing conditions: start-time and QC. Performance was measured against both consensus and CPS ground truths using F1-score, precision, recall, and count-based accuracy metrics.

## Key Results
- AI assistance significantly improved event-level and count-based performance compared to unaided experts
- White-box QC assistance yielded ~30% higher event-level performance than black-box assistance
- Seven of eight participants strongly favored transparent AI assistance and would adopt the system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted transparency improves error recovery and alignment in Quality Control (QC) workflows.
- Mechanism: Post-hoc feature attributions (DeepLIFT) allow experts to verify why AI flagged regions, preventing automation bias and reducing false positives more effectively than opaque suggestions.
- Core assumption: Experts can validate explanation features; explanations are faithful to model logic.
- Evidence anchors: Abstract shows WB-QC outperforms BB by 30%; Section 4.3.2 confirms this difference is statistically significant only in QC timing.
- Break condition: If experts are inexperienced or explanations highlight noise as signal, trust may degrade or experts may be misled.

### Mechanism 2
- Claim: QC workflow timing enforces a "second pass" verification that reduces systematic count bias.
- Mechanism: Start assistance encourages verification-only strategy (scanning only AI hits), while QC forces full manual pass first. This combines human precision with AI recall, shifting total count closer to ground truth.
- Core assumption: Time cost of second pass is acceptable; initial manual pass is of reasonable quality.
- Evidence anchors: Abstract confirms QC timing enhances count-based outcomes; Section 4.4 shows QC regimes improve AI-baseline Improvement Ratio by factor of four.
- Break condition: If expert is fatigued or rushed, initial manual pass may be too poor to serve as effective baseline.

### Mechanism 3
- Claim: AI acts as a vector for standard alignment rather than objective truth generation.
- Mechanism: AI trained on CPS ground truth propagates that labeling style to human users, improving performance specifically against CPS target but not necessarily consensus reference.
- Core assumption: CPS ground truth represents valid clinical standard despite differing from consensus.
- Evidence anchors: Abstract shows AI assistance advantageous when objective is alignment with CPS standard; Section 5.1 discusses training standard governance.
- Break condition: If CPS ground truth contains systematic errors, AI reinforces those errors in human-AI team.

## Foundational Learning

- **Concept: U-Net Architectures for Time-Series Segmentation**
  - Why needed here: System uses modified "DeepSleep" U-Net that downsamples for context and upsamples for localization.
  - Quick check question: How does the "bottleneck" layer in a U-Net help determine the difference between background noise and an arousal event?

- **Concept: The Precision-Recall Trade-off (F2 vs F1 Score)**
  - Why needed here: System optimized for F2 (weighted towards Recall), tolerating false alarms to avoid missing real events.
  - Quick check question: Why would a sleep clinician prefer an F2-optimized model (more false positives) over an F1-optimized model (balanced) for initial screening?

- **Concept: Post-hoc Explainability (DeepLIFT)**
  - Why needed here: "White Box" uses DeepLIFT to explain predictions by comparing neuron activations to reference state.
  - Quick check question: In DeepLIFT, if reference state is "silence" or "random noise," how might that distort explanation for rhythmic breathing signal?

## Architecture Onboarding

- **Component map:** PSG Data -> DeepSleep U-Net -> Confidence Score Channel -> ALPEC Thresholding -> Event Markers -> DeepLIFT Attribution -> Visual Overlay
- **Critical path:** Raw PSG -> DeepSleep (Inference) -> Confidence Score Channel -> ALPEC Thresholding -> Event Markers -> DeepLIFT Attribution (Triggered on marker) -> Visual Overlay
- **Design tradeoffs:**
  - Recall vs. Precision: Low decision threshold (optimized for F2) increases workload but minimizes risk of missing events
  - Speed vs. Transparency: WB modes take ~2x longer than BB but reduce variability and error
  - Consensus vs. Authority: Training on CPS standardizes output but ignores consensus variability
- **Failure signatures:**
  - Over-reliance (Automation Bias): Expert only reviews AI-flagged regions, missing events AI skipped
  - Threshold Mismatch: Users complain of "Too many false arousals" if mental model doesn't match F2-optimization
  - Explanation Noise: DeepLIFT attributions showing relevance on artifacts rather than physiological features
- **First 3 experiments:**
  1. Threshold Sensitivity Analysis: Re-run ALPEC framework varying threshold (F1 vs F2 vs F0.5) to measure trade-off between review time and False Negative rate
  2. Explanation Faithfulness Check: Run DeepLIFT vs. GradientSHAP on 10 random events to quantify if "relevant" channels align with AASM guidelines
  3. Workflow Timing A/B Test: Implement timer on UI to test "Start" vs. "QC" modes on synthetic dataset with known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative explanation modalities (e.g., counterfactual, text) compare to post-hoc feature attributions in terms of clinical adoption and error detection?
- Basis in paper: Section 5.6 states study utilized a "single explanation type," limiting understanding of "how different explanation modalities might affect collaboration."
- Why unresolved: Only DeepLIFT visualizations were tested; other methods might yield different cognitive loads or validation speeds.
- What evidence would resolve it: Comparative A/B user study testing DeepLIFT against other XAI methods within same clinical decision support system.

### Open Question 2
- Question: How does optimizing AI for precision rather than high recall (F2 score) alter human-AI team dynamics and reliance strategies?
- Basis in paper: Section 5.6 suggests need for "exploring alternative optimization strategies," noting current F2 optimization may have influenced "human adoption patterns."
- Why unresolved: Model's bias toward high recall may have shaped specific user behaviors not present with precision-optimized model.
- What evidence would resolve it: User study comparing AI models trained with different loss functions (e.g., F1 vs F2) to measure changes in team performance and correction rates.

### Open Question 3
- Question: Does time cost associated with white-box assistance and QC workflows diminish with long-term user training?
- Basis in paper: Section 5.6 highlights "limited exposure duration" and need to explore "long-term effects on user trust, fatigue, and adaptation."
- Why unresolved: Study measured first-time usage; unknown if efficiency gap between WB and BB assistance closes as users gain fluency.
- What evidence would resolve it: Longitudinal study tracking user efficiency and accuracy over weeks or months of continuous deployment.

### Open Question 4
- Question: How does effectiveness of transparent AI assistance generalize to larger, more diverse cohorts of sleep scorers and clinical institutions?
- Basis in paper: Section 5.6 notes relying on eight participants "limits ability to identify more subtle interactions and may introduce individual biases."
- Why unresolved: Small sample size sufficient for large effects but lacked statistical power for finer nuances and broader generalizability.
- What evidence would resolve it: Multi-center user study with significantly larger number of expert scorers demonstrating consistent performance metrics.

## Limitations
- Small sample size (eight sleep scorers) limits generalizability and statistical power for subtle effects
- Time cost of white-box assistance (~2x longer than black-box) raises scalability concerns in clinical settings
- Findings specific to arousal detection in polysomnography may not generalize to other medical imaging tasks

## Confidence

- **High Confidence:** Claims about AI assistance improving performance over unaided experts
- **Medium Confidence:** Claims about white-box superiority over black-box assistance
- **Medium Confidence:** Claims about QC workflow superiority
- **Low Confidence:** Claims about long-term adoption and sustained benefits

## Next Checks

1. **Replication with Larger Sample:** Conduct study with 20-30 sleep scorers across multiple institutions to verify 30% performance improvement with white-box QC assistance and assess generalizability.

2. **Real-Time Clinical Integration:** Implement system in clinical sleep lab for 3-6 months, measuring actual workflow impact, adoption barriers, and sustained performance changes compared to control groups.

3. **Cross-Task Transferability:** Test same XAI framework (DeepLIFT explanations with QC workflow) on different medical imaging task (e.g., apnea detection or seizure identification) to evaluate generalizability of "targeted transparency" mechanism.