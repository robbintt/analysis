---
ver: rpa2
title: 'Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast
  Asia'
arxiv_id: '2509.09121'
source_url: https://arxiv.org/abs/2509.09121
tags:
- emcommerce
- multilin
- data
- southeast
- domainmspeci
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Compass-v3, a 245B-parameter Mixture-of-Experts
  model with 71B active per token, designed for Southeast Asian e-commerce. The model
  addresses challenges in noisy, heterogeneous, multilingual, and dynamic e-commerce
  data through a combination of architectural innovations (fewer but larger experts),
  training optimizations (intra-node expert parallelism, customized memcpy operators),
  and alignment enhancements (Optimal-Transport Direct Preference Optimization).
---

# Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia

## Quick Facts
- **arXiv ID**: 2509.09121
- **Source URL**: https://arxiv.org/abs/2509.09121
- **Reference count**: 2
- **Primary result**: 245B-parameter MoE model with 71B active per token, state-of-the-art on Southeast Asian e-commerce tasks, surpassing GPT-4 series and Qwen3-235B

## Executive Summary
Compass-v3 is a 245B-parameter Mixture-of-Experts model designed specifically for Southeast Asian e-commerce applications. The model addresses challenges in noisy, heterogeneous, multilingual data through architectural innovations (fewer but larger experts), training optimizations (intra-node expert parallelism with customized memcpy operators), and alignment enhancements (Optimal-Transport Direct Preference Optimization). Trained on 12T tokens of curated multilingual corpora and large-scale synthetic e-commerce instructions, Compass-v3 demonstrates superior performance on e-commerce benchmarks while maintaining strong multilingual capability across low-resource Southeast Asian languages.

## Method Summary
Compass-v3 employs a 64-expert MoE architecture with top-8 routing and 71B active parameters per token. The training pipeline spans five stages: general pretraining → e-commerce/SEA enhancement → reasoning with 8K context → long-context scaling from 32K to 128K → multi-token prediction interleaving. A unified single-stage SFT with mixed-loss handles both e-commerce (full-sequence) and general (answer-only) tasks. The model uses intra-node expert parallelism (EP=8) over NVLink with customized memcpy operators to optimize the combination stage bottleneck. OTPO alignment incorporates both on-policy rejected and off-policy chosen responses with token-level weighting.

## Key Results
- Surpasses DeepSeek-V3.1, GPT-4 series, and Qwen3-235B on e-commerce benchmarks
- Achieves state-of-the-art performance on multilingual Southeast Asian languages (Indonesian, Thai, Filipino, Vietnamese, Malay, Tagalog)
- Demonstrates strong Portuguese language capability alongside SEA languages
- Accounts for over 70% of total LLM usage in Shopee's industrial-scale e-commerce platform

## Why This Works (Mechanism)

### Mechanism 1
Fewer-but-larger experts improve computational efficiency without sacrificing representational capacity. Consolidating capacity into 16 large experts (activating 2 per token) reduces Group GEMM fragmentation. Small-expert designs fragment matrix multiplications across many experts, underutilizing GPU tensor cores. Larger experts yield more contiguous, hardware-friendly operations. This works when Group GEMM efficiency is the bottleneck rather than expert communication overhead.

### Mechanism 2
The combination stage (not all-to-all communication) is the primary MoE training bottleneck. Profiling revealed that repeated memcpy operations during token permutation/unpermutation dominate runtime. A customized memcpy operator eliminates redundant copies and reorders data layouts for contiguous access. This works when intra-node EP over NVLink sufficiently reduces inter-node all-to-all overhead.

### Mechanism 3
Optimal Transport Preference Optimization (OTPO) improves instruction adherence by emphasizing preference-critical tokens. Standard DPO treats all tokens equally, but in e-commerce scenarios, only certain tokens (product attributes, prices, compliance terms) distinguish chosen from rejected responses. OTPO computes token-level weights via optimal transport alignment between chosen/rejected token distributions. This works when token-level granularity captures meaningful preference distinctions.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: Understanding sparse routing is prerequisite to debugging load imbalance and expert collapse. Quick check: Can you explain why auxiliary load-balancing loss prevents router collapse, and what happens if the coefficient α decays too fast?

- **Direct Preference Optimization (DPO) and Variants**: OTPO extends DPO with token-level weighting; baseline DPO understanding is needed to see why token-level distinctions matter for commerce tasks. Quick check: How does DPO avoid training a separate reward model, and what role does the reference policy (frozen) play in the loss?

- **FP8 Quantization and Hopper Tensor Cores**: Inference optimization uses expert-aware FP8 (W8A8); activation smoothing and per-channel scaling understanding is critical to reproducing the 2× speedup. Quick check: Why does naive per-tensor FP8 quantization fail for MoE experts with heterogeneous activation patterns, and how does oversampling rare experts help?

## Architecture Onboarding

- **Component map**: Backbone Transformer with MoE layers (16 experts, top-2 routing, 71B active params) -> MTP Heads (auxiliary lightweight transformer blocks) -> Router (top-2 gating with auxiliary load-balancing loss + z-loss) -> Distributed Training (DP + PP + TP + intra-node EP) -> Inference (expert-aware FP8 quantization)

- **Critical path**: 1. Pretraining (12T tokens): General → E-commerce/SEA enhancement → Reasoning → Long-context (128K) → MTP interleaved 2. SFT: Unified single-stage with mixed-loss (full-sequence for e-commerce, answer-only for general) 3. RL Alignment: Mixed-policy (on-policy rejected + off-policy chosen) → OTPO with precomputed reference log-probs

- **Design tradeoffs**: Larger experts = better Group GEMM efficiency but less routing flexibility and harder per-expert calibration for quantization; Intra-node EP = fast synchronization but limits expert count to GPU-per-node; Mixed-loss SFT = better e-commerce comprehension but risks overfitting long prompts

- **Failure signatures**: Router collapse (one expert >80% of tokens); OOM in early pipeline stages (VPP increases micro-batches but raises memory); FP8 accuracy degradation on low-resource languages (rare experts under-calibrated)

- **First 3 experiments**: 1. Expert load balancing ablation: Train 2B proxy with/without z-loss and decaying auxiliary loss; measure expert activation entropy and downstream benchmark variance 2. Memcpy kernel microbenchmark: Profile MoE layer with naive vs. optimized memcpy on identical batch sizes; isolate combination-stage speedup 3. OTPO vs. DPO on token-level distinctions: Create synthetic preference pairs where only 1-2 tokens differ; compare accuracy recovery

## Open Questions the Paper Calls Out

- **Open Question 1**: Does transitioning from mixed-policy RL to large-scale online RL yield significant performance gains for low-resource SEA languages compared to computational overhead? The current mixed-policy approach was selected for efficiency; marginal utility of full online RL is unquantified.

- **Open Question 2**: How does the introduction of multimodal product data (images/videos) impact the stability of expert-aware FP8 quantization and compute-bound inference bottlenecks? Current optimizations are tailored for text-only MoE architectures.

- **Open Question 3**: To what extent does the synthetic instruction evolution pipeline fail to capture cultural nuances required for robust human-in-the-loop preference optimization? Synthetic evolution improves linguistic complexity but lacks metric for cultural fidelity.

## Limitations

- Architecture scaling limits: Optimal expert count (16) and activation policy (top-2) were chosen based on Shopee's specific GPU configuration, potentially limiting generalizability
- Data composition opacity: Exact proportions and quality control mechanisms for the 12T-token training corpus remain unspecified
- Alignment methodology gaps: OTPO's effectiveness relies on optimal transport computations, but solver configuration and regularization parameters are unspecified

## Confidence

- **High confidence**: General architectural approach (large experts with top-2 routing, combined with hardware optimizations) achieves measurable performance gains; 70% adoption rate at Shopee provides real-world validation
- **Medium confidence**: Specific claims about OTPO improving instruction adherence and exact performance superiority over GPT-4 series on e-commerce benchmarks
- **Low confidence**: Claims about multilingual performance on low-resource SEA languages are difficult to verify without access to specific evaluation datasets

## Next Checks

- **Validation Check 1**: Conduct controlled ablation study varying expert count (4, 8, 16, 32) and activation policy (top-1, top-2, top-4) on 10B proxy model to validate "fewer-but-larger" expert hypothesis
- **Validation Check 2**: Implement and compare OTPO vs. standard DPO on synthetic preference dataset where only 1-2 tokens distinguish chosen from rejected responses
- **Validation Check 3**: Replicate memcpy optimization claim through microbenchmarking; profile MoE layer combination stages with naive vs. optimized memcpy implementations