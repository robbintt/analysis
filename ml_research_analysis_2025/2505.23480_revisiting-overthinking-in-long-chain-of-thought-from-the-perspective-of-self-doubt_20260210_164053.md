---
ver: rpa2
title: Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt
arxiv_id: '2505.23480'
source_url: https://arxiv.org/abs/2505.23480
tags:
- reasoning
- overthinking
- answer
- self-doubt
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the overthinking problem in Long Chain-of-Thought
  reasoning, where large language models engage in excessive verification even after
  arriving at correct answers. The authors quantify overthinking through the lens
  of self-doubt, categorizing reasoning paths into three types: overthinking with
  self-doubt, overthinking without self-doubt, and non-overthinking.'
---

# Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt

## Quick Facts
- arXiv ID: 2505.23480
- Source URL: https://arxiv.org/abs/2505.23480
- Authors: Keqin Peng; Liang Ding; Yuanxin Ouyang; Meng Fang; Dacheng Tao
- Reference count: 7
- Key outcome: Simple prompting strategy reduces overthinking by up to 37.1% token usage while maintaining/improving accuracy across math and missing-premise tasks

## Executive Summary
This paper investigates overthinking in Long Chain-of-Thought reasoning, where large language models engage in excessive verification even after arriving at correct answers. The authors quantify overthinking through the lens of self-doubt, categorizing reasoning paths into three types: overthinking with self-doubt, overthinking without self-doubt, and non-overthinking. Experiments reveal that overthinking is widespread, with self-doubt being a major contributor, especially in complex tasks. To address this, the authors propose a simple prompting strategy that first prompts the model to validate the input question's completeness and then generates a concise response based on that assessment. This approach significantly reduces answer length by up to 37.1% while maintaining or improving accuracy across three mathematical reasoning tasks and four missing-premise datasets. It also effectively minimizes reasoning steps and self-doubt, particularly in complex problems.

## Method Summary
The proposed method employs a two-stage prompting strategy that wraps user queries with instructions to first validate input completeness and then respond concisely. The prompt template instructs models to "check whether all necessary information is available" before reasoning, explicitly stating that if key data is missing or ambiguous, they should flag this first; otherwise, they should answer with "the minimum number of tokens required." This approach is applied to various reasoning tasks using models like DeepSeek-R1-Distill-Qwen-32B with temperature=0 for deterministic outputs. The method is evaluated across mathematical reasoning benchmarks (GSM8K, GSM8K-Zero, MATH-500) and missing-premise datasets (MiP-Formula, MiP-SV AMP, MiP-GSM8K, MiP-MATH), measuring response length, accuracy, abstain rate, self-doubt ratio, and reasoning steps.

## Key Results
- Response length reduced by up to 37.1% while maintaining/improving accuracy
- Self-doubt ratio decreased by 23.8% on MATH-500 (complex tasks)
- Abstain rate improved by ~40% on missing-premise datasets (MiP-Formula)
- Average token reduction of 80%+ on MiP-Formula with maintained accuracy

## Why This Works (Mechanism)

### Mechanism 1: Self-Doubt as Primary Overthinking Driver
- Claim: Excessive verification of already-correct answers contributes significantly to unnecessary reasoning steps, especially in complex tasks.
- Mechanism: Models engage in redundant re-verification loops even after reaching correct conclusions, inflating token usage without accuracy gains.
- Core assumption: Self-doubt stems from models' learned tendency toward excessive deference to user inputs, akin to social comparison theory where reliance on external validation creates uncertainty.
- Evidence anchors: [abstract] "self-doubt, characterized by excessive token usage devoted to re-verifying already-correct answer"; [section 2] Figure 1 shows self-doubt accounts for 59.5% of cases in MATH-500 (complex), vs. 8.8% in GSM8K (simpler); [corpus] Related work (Fu et al., 2025) identifies self-doubt as "one major source of overthinking"
- Break condition: If tasks are simple enough that verification loops are minimal, self-doubt contribution drops substantially (see GSM8K vs. MATH-500 disparity).

### Mechanism 2: Input Validation Elicits Critique Capacity
- Claim: Prompting models to first assess input validity activates latent critique abilities, reducing subsequent over-verification.
- Mechanism: Two-stage prompting—(1) validate question completeness, (2) respond concisely based on evaluation—shifts model from reactive verification to proactive assessment.
- Core assumption: Models possess underutilized critique capabilities that, when explicitly invoked, reduce the need for post-hoc self-doubt cycles.
- Evidence anchors: [section 3] "we first prompt the model to assess the validity of the input query... thereby engaging its critique abilities and boosting its confidence"; [section 6, Table 4] Self-doubt ratio reduced by 23.8% on MATH-500 with the proposed method; [corpus] Limited direct corpus support; related work focuses on length-based rewards rather than critique elicitation
- Break condition: If questions are genuinely ill-posed with missing premises, the validation stage correctly identifies this without triggering long reasoning chains (evidenced by abstain rate improvements).

### Mechanism 3: Concise Response Constraint
- Claim: Explicit instruction to use "minimum tokens required" provides length regularization without sacrificing accuracy.
- Mechanism: The combined prompt ("check completeness... otherwise answer with minimum tokens") creates a dual signal—validate first, then compress—preventing open-ended verification spirals.
- Core assumption: Models can accurately assess information sufficiency before committing to full reasoning chains.
- Evidence anchors: [section 5, Table 1] Average 37.1% token reduction while maintaining/improving accuracy; [section 5, Table 3] 80%+ token reduction on MiP-Formula; abstain rate improved by ~40%; [corpus] Related work (DRQA, Dynamic Early Exit) explores quota-based approaches but through allocation mechanisms rather than prompting
- Break condition: If validation assessment is incorrect (false positive/negative on completeness), subsequent response quality degrades.

## Foundational Learning

- Concept: **Chain-of-Thought Reasoning**
  - Why needed here: Long CoT is the core architecture being analyzed; understanding how step-by-step reasoning works is prerequisite to diagnosing overthinking.
  - Quick check question: Can you explain why intermediate reasoning steps improve accuracy on multi-step math problems?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: The paper uses Qwen2.5-72B-Instruct to classify reasoning paths into SD/OT/NOT categories; understanding evaluator limitations is critical.
  - Quick check question: What biases might an LLM evaluator introduce when judging "redundant" vs. "necessary" verification?

- Concept: **Missing Premise Detection**
  - Why needed here: A key application domain; distinguishing between ill-posed questions (requiring abstention) and well-posed ones is central to the method.
  - Quick check question: How should a model respond when asked "What's 2 + x?" without a value for x?

## Architecture Onboarding

- Component map: User question -> Prompt wrapper (validation + conciseness) -> Two-stage generation (validation check -> concise response) -> LLM-as-judge evaluation -> Accuracy/length metrics
- Critical path: 1. User question arrives 2. Wrap with validation prompt (Table 2 template) 3. Model generates response (with early exit if missing premise detected) 4. Evaluate: accuracy (well-defined) / abstain rate (MiP) / response length
- Design tradeoffs: Prompt simplicity vs. task coverage (current prompt optimized for math/MiP; may not transfer to commonsense or planning tasks); Automated evaluation speed vs. reliability (LLM-judge introduces potential misclassification; manual validation would be more accurate but infeasible at scale)
- Failure signatures: GSM8K showed increased self-doubt (+3.5%) despite overall improvements—early validation may trigger verification on short reasoning paths; Complex tasks with ambiguous "completeness" may yield inconsistent abstention decisions
- First 3 experiments: 1. **Baseline replication**: Run DeepSeek-R1-Distill-Qwen-32B on GSM8K and MATH-500 with default prompts; measure token length, accuracy, and self-doubt ratio using the Table 5 evaluation template 2. **Prompt ablation**: Test each component separately (validation-only, conciseness-only, both) to isolate contribution of each instruction 3. **Domain transfer probe**: Apply the same prompt template to a non-math reasoning task (e.g., commonsense QA or code generation) to test generalization limits acknowledged in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the self-doubt reduction effect generalize beyond mathematical reasoning to other domains such as commonsense QA, multi-step planning, or multimodal tasks?
- Basis in paper: [explicit] The Limitations section states: "It remains unclear whether the same reduction in overthinking and self-doubt would hold for other reasoning domains (e.g., commonsense QA, multi-step planning, or multimodal tasks)."
- Why unresolved: The method was only evaluated on three math benchmarks and four missing-premise datasets, leaving domain transfer untested.
- What evidence would resolve it: Experiments applying the prompting strategy to benchmarks like StrategyQA, Planning tasks, or multimodal reasoning datasets.

### Open Question 2
- Question: Why does the method increase self-doubt in simpler datasets (GSM8K: +3.5%) while reducing it in complex ones (MATH-500: -23.8%)?
- Basis in paper: [inferred] Table 4 shows mixed results, and the authors only hypothesize: "we suspect it stems from the early verification of the question's correctness and the short reasoning path."
- Why unresolved: The mechanism causing this dataset-dependent behavior is not empirically validated.
- What evidence would resolve it: Controlled experiments varying problem complexity and reasoning path length, with analysis of when early verification helps vs. harms.

### Open Question 3
- Question: How reliable is the LLM-as-judge approach for classifying overthinking patterns, and what biases does it introduce?
- Basis in paper: [explicit] Limitations section notes: "Our quantitative analysis... depends on an LLM-based judge... This introduces potential biases or misclassifications, since the evaluator itself may misunderstand nuanced reasoning patterns."
- Why unresolved: No human annotation baseline or inter-annotator agreement is provided to validate the automated classification.
- What evidence would resolve it: Comparison between Qwen2.5-72B-Instruct judgments and human expert annotations on a shared sample of reasoning paths.

## Limitations
- Generalizability constraint: Method only validated on mathematical reasoning and missing-premise detection tasks; effectiveness on other domains remains untested
- Evaluation reliability: Heavy reliance on LLM-as-judge for self-doubt classification introduces potential evaluator bias without human validation baseline
- Dataset specification: Incomplete details for GSM8K-Zero and MiP datasets require reference to external work (Fan et al., 2025) that may not be precisely reproducible

## Confidence

**High Confidence**: The observation that overthinking is widespread in Long CoT reasoning and that self-doubt contributes significantly to excessive token usage (59.5% in MATH-500 vs. 8.8% in GSM8K). These empirical findings are well-supported by the data presented in Figure 1 and the corpus evidence from related work.

**Medium Confidence**: The effectiveness of the two-stage prompting strategy in reducing token usage by up to 37.1% while maintaining or improving accuracy. While the results are promising, they depend heavily on the reliability of the self-doubt classification and may not generalize beyond the tested domains.

**Low Confidence**: The mechanism explanation that prompting for input validation "activates latent critique abilities" and "boosts confidence." This psychological framing of LLM behavior lacks empirical validation and may be an anthropomorphic interpretation of pattern matching rather than genuine cognitive processes.

## Next Checks
1. **Evaluator Reliability Test**: Implement human validation for a random 10% sample of reasoning paths to measure inter-annotator agreement with the LLM-as-judge self-doubt classification. This will quantify the reliability of the core measurement mechanism.

2. **Cross-Domain Transfer Study**: Apply the exact same prompting strategy to three non-mathematical reasoning tasks (e.g., commonsense QA, code generation, and text-based games) to empirically test the generalizability limits acknowledged by the authors.

3. **Component Ablation Experiment**: Systematically test each component of the two-stage prompt separately (validation-only, conciseness-only, both) across all datasets to quantify the individual contribution of each instruction and identify potential interference effects.