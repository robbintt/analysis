---
ver: rpa2
title: 'Flat Minima and Generalization: Insights from Stochastic Convex Optimization'
arxiv_id: '2511.03548'
source_url: https://arxiv.org/abs/2511.03548
tags:
- flat
- generalization
- convex
- smooth
- minima
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the relationship between flat minima and generalization\
  \ in stochastic convex optimization. The authors introduce a strong flatness condition\
  \ where a loss function admits a perfectly flat minimum within a radius \u03C1,\
  \ and analyze how well algorithms designed to find flat minima generalize."
---

# Flat Minima and Generalization: Insights from Stochastic Convex Optimization

## Quick Facts
- **arXiv ID:** 2511.03548
- **Source URL:** https://arxiv.org/abs/2511.03548
- **Reference count:** 40
- **Primary result:** Even under strong flatness assumptions, flat empirical risk minimizers can generalize poorly in stochastic convex optimization, exhibiting trivial Ω(1) population risk while sharp minimizers generalize optimally.

## Executive Summary
This paper challenges the conventional wisdom that flat minima lead to better generalization by studying stochastic convex optimization. The authors introduce a strong flatness condition where a loss function admits a perfectly flat minimum within a radius ρ, and analyze how algorithms designed to find flat minima generalize. They construct specific SCO instances where flat empirical minima incur trivial Ω(1) population risk while sharp minima generalize optimally. The analysis extends to two sharpness-aware algorithms: SA-GD and SAM. The results demonstrate that even in the basic convex and smooth regime, algorithms explicitly designed to locate flat minima may converge to solutions that generalize poorly, while standard gradient methods achieve optimal generalization.

## Method Summary
The paper analyzes stochastic convex optimization problems where the loss function is convex, non-negative, and β-smooth. The authors study three algorithms: SA-ERM (oracle minimizer of sharpness-aware empirical risk), SA-GD (gradient descent on SAER with exact inner maximization), and SAM (gradient descent on SAER approximation using normalized gradients). They construct synthetic SCO instances where flat minima exist but generalize poorly, proving lower bounds for population risk. The analysis uses algorithmic stability techniques to establish upper bounds for SA-GD and SAM. Key parameters include perturbation radius r, flatness radius ρ, step size η, and sample size n.

## Key Results
- Flat empirical risk minimizers can incur trivial Ω(1) population risk while sharp minimizers generalize optimally, even under strong ρ-flatness assumptions.
- SA-GD converges to flat minima at O(1/T) rate but may still generalize poorly with population risk Ω(1).
- SAM minimizes empirical risk but may converge to sharp minima and exhibit poor generalization due to gradient normalization effects.
- Algorithmic stability upper bounds for SA-GD and SAM are O(β/n + βr²n), establishing a tradeoff between robustness and generalization.

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Subspace Decoupling
- **Claim:** Flatness of empirical minima does not causally guarantee low population risk, even under strong ρ-flatness and convexity.
- **Mechanism:** The authors construct a specific SCO instance where the empirical risk landscape contains "spurious" flat regions in orthogonal subspaces that align with sample noise but not the population distribution. A compositional loss function creates flat regions dependent on the specific sample set S but has high probability of non-zero loss in the population.
- **Core assumption:** The loss function is non-negative, convex, β-smooth, and admits a ρ-flat minimum.
- **Evidence anchors:**
  - [abstract]: "flat empirical minima may incur trivial Ω(1) population risk while sharp minima generalizes optimally."
  - [section 3.1]: "The main technical challenge is that in extending those prior constructions the ERM exhibiting poor generalization is not a flat minimizer... we use the simple observation that the function h(x) = ½max(x-ρ,0)² is 1-smooth and ρ-flat."
  - [corpus]: Neighbors like *Understanding Flatness in Generative Models* suggest flatness correlates with generalization in generative contexts, highlighting that this paper's counter-mechanism is specific to the SCO regime.
- **Break condition:** The mechanism fails if the empirical risk minimizer is constrained to lie within the "true" flat basin shared by the population distribution (e.g., r ≤ ρ and the initialization is already in the correct basin), or if uniform convergence holds.

### Mechanism 2: SA-GD Ascent-Step Drift
- **Claim:** Sharpness-Aware Gradient Descent (SA-GD) can provably converge to flat minima that generalize poorly due to the directional bias of the ascent step.
- **Mechanism:** SA-GD calculates v_t = argmax_{||v||≤r} F_S(w_t+v). In the constructed "bad" instance, this maximization identifies the direction of the spurious flat ERM. The subsequent gradient step moves η(r-ρ) toward this bad ERM. The construction applies this dynamic across T orthogonal subspaces, causing the algorithm to drift away from the population optimum toward a sample-specific flat minimum.
- **Core assumption:** The perturbation radius r ≳ ρ + 1/√T and step size η ≃ 1/β.
- **Evidence anchors:**
  - [section 3.2]: "In the first iteration we have v_1 = r e_i, where e_i corresponds to the spurious ERM... As a result, SA-GD makes a single step of size η(r-ρ) toward this bad ERM."
  - [theorem 3]: Establishes the lower bound Ω(η²(r-ρ)²T) on population risk.
  - [corpus]: Corpus evidence (e.g., *Efficiently Seeking Flat Minima*) generally supports SA-GD/SAM efficacy, contrasting with this paper's finding that the causal mechanism of the ascent step can be hijacked in specific SCO landscapes.
- **Break condition:** If the perturbation radius r is small (r ≤ ρ) or the flatness radius ρ is sufficiently large relative to the noise dimension, the drift term max(r-ρ, 0)² vanishes, preventing the drift to the spurious minimum.

### Mechanism 3: SAM Gradient Normalization Amplification
- **Claim:** SAM (Sharpness-Aware Minimization) may converge to sharp minima and generalize poorly due to the interaction of gradient normalization with the loss geometry.
- **Mechanism:** Unlike SA-GD, SAM approximates the ascent step using normalized gradients (r·∇F_S(w_t)/||∇F_S(w_t)||). The authors construct a case where small gradients at initialization are amplified by this normalization, forcing a step of magnitude ηr toward a bad ERM. Furthermore, the "approximation" nature of SAM means it minimizes empirical risk but fails to minimize the Sharpness-Aware Empirical Risk (SAER), potentially landing on sharp minima.
- **Core assumption:** Step size η ≃ 1/β and perturbation r ≳ 1/√T or η ≃ 1/√T.
- **Evidence anchors:**
  - [theorem 5]: "SAM might incur an additional term of Ω(r²) in the convergence rate for the SAER... demonstrating that SAM can converge to a non-flat minimum."
  - [section 3.3]: "The main difficulty... is that... normalization step amplifies [small gradients], producing a progress of ηr toward the bad ERM."
  - [corpus]: *VASSO: Variance Suppression* and other corpus papers argue SAM benefits generalization, suggesting the failure mechanism here relies on the specific constructed orthogonal subspace dynamics not present in standard deep learning benchmarks.
- **Break condition:** The mechanism relies on small gradients being present in the "trap" dimensions. If gradients are large or uniform, or if r is very small, the normalization bias is controlled, and SAM behaves closer to standard GD (which generalizes well in this setting).

## Foundational Learning

- **Concept:** Stochastic Convex Optimization (SCO) with β-smoothness
  - **Why needed here:** The entire analysis is restricted to convex, non-negative, β-smooth objectives. Understanding this setting is critical because it is a regime where uniform convergence is known to fail (ERM can overfit), yet standard algorithms like GD/SGD typically generalize optimally. The paper exploits the gap between "algorithmic generalization" and "ERM generalization" in this specific regime.
  - **Quick check question:** Does the claim that "flat minima generalize poorly" apply to non-convex deep networks based on this paper alone? (Answer: No, strictly SCO).

- **Concept:** ρ-flatness (Strong Flatness)
  - **Why needed here:** The paper introduces a specific, strong definition of flatness: a loss f is ρ-flat if f(w) = 0 for all w within a ball of radius ρ of the minimum. This is stronger than spectral measures (e.g., Hessian trace). One must grasp this to understand why the counter-examples are considered "flat" yet still "bad."
  - **Quick check question:** If a function has Hessian eigenvalues of zero (spectral flatness) but is not perfectly zero-valued in a neighborhood, does it satisfy this paper's definition of ρ-flatness? (Answer: Not necessarily).

- **Concept:** Algorithmic Stability
  - **Why needed here:** The authors use algorithmic stability arguments (specifically on-average-leave-one-out stability) to derive the upper bounds for SA-GD and SAM. This is the theoretical tool used to prove that if parameters are tuned correctly (e.g., r ≲ ρ + 1/√T), these methods can generalize.
  - **Quick check question:** What trade-off does the stability bound reveal regarding the perturbation radius r? (Answer: Larger r improves empirical robustness/convergence to flatness, but degrades stability/generalization bounds via the O(βr²n) term).

## Architecture Onboarding

- **Component map:** Base Framework (Stochastic Convex Optimization) -> Target Objectives (Empirical Risk, SAER) -> Algorithms (SA-ERM, SA-GD, SAM)
- **Critical path:** The theoretical separation relies on the interplay between the perturbation radius (r), the flatness radius (ρ), and the step size (η). The failure occurs when r is tuned to seek flatness (r ≈ ρ) but the algorithm dynamics push the iterates into spurious sample-specific subspaces.
- **Design tradeoffs:**
  - **Convergence vs. Generalization:** SA-GD guarantees convergence to a flat minimum (O(1/T) empirical risk), but Theorem 3 shows this minimum may have Ω(1) population risk. Standard GD converges and generalizes optimally (O(1/n) risk), suggesting the "sharpness-aware" modification adds theoretical risk in this regime.
  - **Approximation Quality:** SAM is computationally efficient but Theorem 5 proves it fails to minimize SAER (converges to sharp minima) in specific convex cases, unlike SA-GD which is faithful to the SAER objective.
- **Failure signatures:**
  - **The "Spurious ERM" Trap:** Algorithms converging to minimizers that exploit dimensions inactive in the training set (e.g., z(I)=0 for all z ∈ S) but active in the population (z'(I)=1).
  - **Normalization Artifact:** In SAM, observing divergence or convergence to sharp minima when ||∇F_S(w)|| is small but non-zero, causing the normalization r·∇/||∇|| to take large, noisy steps in orthogonal directions.
- **First 3 experiments:**
  1. **Replicate the Construction (Verification):** Implement the convex function defined in Eq. (B.1) and Theorem 1 proofs. Run SA-GD and SAM with varying n (sample size) and r (perturbation). Verify if population risk stays at Ω(1) while empirical risk drops to 0.
  2. **Sensitivity to r vs ρ:** Systematically vary the perturbation radius r relative to the true flatness ρ (which you set in the synthetic loss). Confirm the "safe zone" r ≲ ρ + 1/√T where generalization is preserved vs. the "danger zone" where Theorem 3 predicts failure.
  3. **SAM vs. SA-GD Trajectory Analysis:** Visualize the trajectory of weights in the orthogonal subspaces described in the paper. Check if SAM's normalization causes it to "bounce" between subspaces or drift away from the population optimum, contrasting it with the more stable but computationally expensive SA-GD path.

## Open Questions the Paper Calls Out

- **Does SAM exhibit poor generalization in the ρ-flat case (ρ ≫ 0), or can flatness guarantees improve its generalization performance?**
  - **Basis in paper:** [explicit] The authors state they show SAM exhibits poor generalization "under the realizable setting (ρ = 0), leaving the ρ-flat case (ρ ≫ 0) for future work."
  - **Why unresolved:** The proof techniques for the lower bound rely on constructions specific to the realizable case, and extending to ρ-flat objectives requires new analytical approaches.
  - **What evidence would resolve it:** Either a construction showing Ω(1) population risk for SAM under ρ-flatness with ρ > 0, or improved upper bounds demonstrating that flatness mitigates the generalization gap.

## Limitations

- The theoretical separation relies on very specific synthetic constructions with orthogonal subspace structure that may not transfer to practical deep learning scenarios where flatness is typically measured spectrally (e.g., Hessian eigenvalues) rather than through perfect zero-valued neighborhoods.
- The lower bounds hold under exact knowledge of the construction, but practical algorithms face numerical precision issues when gradients become extremely small or near-zero.
- The results are confined to convex settings; the relationship between flatness and generalization in non-convex deep networks remains an open question.

## Confidence

- **High confidence:** Theorems 1, 3, and 6 regarding SA-GD and SA-ERM behavior in the constructed SCO instances, as these follow directly from the mathematical proofs provided.
- **Medium confidence:** Theorem 5 about SAM's convergence properties, given the approximation error analysis relies on specific gradient normalization dynamics that may vary with implementation details.
- **Low confidence:** Extrapolation of these results to practical deep learning settings where loss landscapes are non-convex and flatness is measured differently.

## Next Checks

1. Replicate the synthetic convex constructions and verify that SA-GD and SAM indeed converge to flat minima with Ω(1) population risk while standard GD generalizes optimally.
2. Test whether spectral flatness measures (e.g., Hessian trace) correlate with generalization in these SCO instances, and whether they would detect the "spurious" flat regions.
3. Analyze the effect of numerical precision on SAM's behavior in the near-zero gradient regime to determine if the theoretical divergence is observable in practice.