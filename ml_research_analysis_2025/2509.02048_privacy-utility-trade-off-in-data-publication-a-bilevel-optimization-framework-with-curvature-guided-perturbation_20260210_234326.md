---
ver: rpa2
title: 'Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework
  with Curvature-Guided Perturbation'
arxiv_id: '2509.02048'
source_url: https://arxiv.org/abs/2509.02048
tags:
- data
- privacy
- curvature
- samples
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of protecting private datasets
  while maintaining their utility for downstream tasks, specifically defending against
  membership inference attacks (MIA). The proposed solution is a novel bilevel optimization
  framework that balances data utility and privacy.
---

# Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation

## Quick Facts
- **arXiv ID:** 2509.02048
- **Source URL:** https://arxiv.org/abs/2509.02048
- **Reference count:** 40
- **One-line primary result:** A novel bilevel optimization framework achieves superior privacy protection (53.11% MIA success rate) while maintaining high data utility on multiple datasets.

## Executive Summary
This paper addresses the challenge of protecting private datasets while maintaining their utility for downstream tasks, specifically defending against membership inference attacks (MIA). The proposed solution is a novel bilevel optimization framework that balances data utility and privacy. The upper-level task focuses on maximizing data utility through a discriminator-guided generation process, while the lower-level task enhances privacy by perturbing vulnerable data points identified using local extrinsic curvature on the data manifold. The framework employs a Riemannian Variational Autoencoder (RVAE) as its backbone, enabling geodesic perturbations toward low-curvature regions to suppress distinctive features vulnerable to MIA. Experiments on multiple datasets demonstrate that this method achieves superior performance in both privacy protection and data utility compared to existing privacy-preserving techniques.

## Method Summary
The framework uses a Riemannian VAE (RVAE) as its backbone, where the decoder's Jacobian defines a pullback metric that captures the local extrinsic curvature of the data manifold. A bilevel optimization approach alternates between two objectives: the upper-level maximizes data utility by updating the RVAE-GAN to generate realistic samples, while the lower-level maximizes privacy by identifying and perturbing high-curvature points. Vulnerability is estimated through local extrinsic curvature calculated via finite differences of the pullback metric eigenvalues. Perturbations follow geodesic paths toward low-curvature regions (often cluster centers) to suppress distinctive features while preserving utility. The system is trained through alternating updates: RVAE reconstruction, discriminator optimization, decoder adversarial improvement, and curvature estimator adaptation.

## Key Results
- Achieves 53.11% average MIA success rate across multiple datasets (MNIST variants, OCTMNIST), significantly outperforming baseline methods
- Maintains high downstream classification accuracy while preserving data diversity (low FID and high IS scores)
- Successfully defends against black-box loss-based membership inference attacks while maintaining utility for downstream tasks
- Demonstrates effectiveness across multiple grayscale image datasets with varying characteristics

## Why This Works (Mechanism)

### Mechanism 1
High extrinsic curvature on the data manifold serves as a geometric proxy for vulnerability to Membership Inference Attacks (MIA). The framework approximates local extrinsic curvature using the rate of change of the Riemannian metric eigenvalues (pullback metric). High curvature regions correspond to "sharp" areas on the manifold where samples likely contain unique feature combinations or lie near decision boundaries. Theoretically, these regions exhibit higher loss sensitivity to perturbations ($\Delta L \propto \epsilon^4 \cdot \|H_{decoder}\|^2$), making them easier for attackers to distinguish.

### Mechanism 2
Geodesic interpolation toward low-curvature regions suppresses distinctive features while preserving manifold topology better than linear noise injection. Instead of adding isotropic noise, the model moves a latent variable $z$ along a geodesic path toward a "safe" RBF center, sampling points along this path and halting at the point with the lowest curvature before hitting a high-curvature "ridge" (often a class boundary).

### Mechanism 3
Bilevel optimization resolves the conflicting objectives of generation quality (Utility) and feature suppression (Privacy) by treating them as coupled tasks. The Upper-Level task (RVAE-GAN) updates the decoder to maximize fidelity using a discriminator, while the Lower-Level task updates the curvature estimator. Crucially, the decoder updates modify the Jacobian, which alters the pullback metric, creating a coupled optimization problem.

## Foundational Learning

- **Riemannian Geometry & Pullback Metrics**
  - Why needed here: Standard VAEs treat latent space as Euclidean. This paper assumes data lies on a curved manifold. You must understand how the decoder's Jacobian ($J$) creates a metric $G(z) = J^T J$ to interpret how the model "sees" curvature.
  - Quick check question: How does the pullback metric change if the decoder weights are updated to flatten a specific region of the latent space?

- **Membership Inference Attacks (MIA)**
  - Why needed here: This is the threat model. You need to understand that MIA exploits the difference in loss/confidence between training (member) and non-training (non-member) data.
  - Quick check question: Why would a sample with a unique feature combination (high curvature) result in a different loss landscape than a generic sample?

- **Bilevel Optimization**
  - Why needed here: The architecture is not a single feed-forward pass; it is a nested loop. Understanding that the Lower-Level problem (privacy) is a constraint for the Upper-Level (utility) is essential for debugging convergence.
  - Quick check question: In this framework, does the Upper-Level optimizer directly minimize the MIA success rate, or does it optimize a proxy (Curvature)?

## Architecture Onboarding

- **Component map:** RVAE (Encoder -> Decoder -> Pullback Metric) -> Discriminator -> Curvature Estimator -> Geodesic Obfuscator -> Perturbed Samples
- **Critical path:** Pre-training: RVAE-GAN ($\mu$ then $\sigma$ stages) → Pre-train Curvature Estimator → Bilevel Loop: Perturb latents via geodesics → Update Discriminator → Update Decoder → Update Curvature Estimator
- **Design tradeoffs:** Curvature Approximation vs. Hessian Calculation (finite differences are faster but noisier); Geodesic Step Size (too large drops accuracy, too small fails to hide from MIA)
- **Failure signatures:** Mode Collapse/Blurry Images (weak discriminator or aggressive privacy); High MIA Rate (>60%) (curvature estimation failing or decoder overfitting); Label Flipping (geodesic path crosses decision boundary)
- **First 3 experiments:** 1) Visualizing the Manifold: PCA/ISOMAP on latent space, color-code by curvature, verify unique samples in high-curvature zones; 2) Ablation on Interpolation: Linear vs. Geodesic, check FID and validity; 3) MIA Correlation Test: Plot Curvature vs. MIA Vulnerability Score, confirm positive correlation

## Open Questions the Paper Calls Out
- **Future Work on Alternative Metrics:** The authors state that "future work will explore alternative metrics to replace the pullback metric" to address the computational burden of the Jacobian matrix.
- **High-Resolution Color Images:** The paper notes that "current research on RVAEs is largely confined to grayscale dataset generation" due to computational load.
- **White-Box Attack Robustness:** The evaluation relies primarily on black-box attacks; white-box adversaries who know the geodesic obfuscation strategy are not tested.

## Limitations
- The geometric proxy assumption (high curvature = high MIA vulnerability) shows only moderate correlation (r=0.1636) in experiments
- Exact architectural specifications for RVAE, discriminator, and curvature estimator are missing, affecting reproducibility
- Computational burden of Jacobian calculation limits scalability to high-dimensional, full-color datasets

## Confidence

- **High Confidence:** The geometric interpretation of high-curvature regions as containing unique features is consistent with differential geometry principles and the experimental correlation observed.
- **Medium Confidence:** The bilevel optimization framework effectively balances utility and privacy objectives through alternating updates, though the "chasing tail" convergence problem is acknowledged but not empirically validated.
- **Low Confidence:** The claim that geodesic interpolation toward low-curvature regions is universally superior to linear noise injection across all dataset types lacks comprehensive ablation studies beyond the presented experiments.

## Next Checks

1. **Manifold Visualization:** Run PCA/ISOMAP on the latent space and color-code samples by estimated curvature to verify that visually unique samples (e.g., unusual handwriting) consistently land in high-curvature regions.
2. **Ablation Study:** Compare geodesic interpolation against linear interpolation for perturbation, measuring FID scores and MIA success rates to confirm geodesic paths maintain manifold structure better.
3. **Cross-Dataset Generalization:** Test the curvature-based vulnerability detection on a held-out dataset with different characteristics (e.g., from tabular data to medical images) to validate the geometric proxy assumption beyond the current image datasets.