---
ver: rpa2
title: 'MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for
  Health and Behavioral Understanding'
arxiv_id: '2512.21506'
source_url: https://arxiv.org/abs/2512.21506
tags:
- activity
- motionteller
- behavioral
- actigraphy
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MotionTeller, a generative framework that\
  \ combines raw minute-level actigraphy data with large language models (LLMs) to\
  \ produce natural language behavioral summaries. The method uses a frozen pretrained\
  \ actigraphy encoder (PAT) and a lightweight projection module to map sensor embeddings\
  \ into the LLM\u2019s token space, enabling autoregressive generation of daily activity\
  \ descriptions."
---

# MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding

## Quick Facts
- arXiv ID: 2512.21506
- Source URL: https://arxiv.org/abs/2512.21506
- Reference count: 40
- Primary result: MotionTeller achieves BERTScore-F1 of 0.924 and ROUGE-1 of 0.722 in generating daily activity summaries from actigraphy data

## Executive Summary
MotionTeller introduces a generative framework that bridges wearable sensor data and large language models by translating raw minute-level actigraphy into natural language behavioral summaries. The system employs a frozen pretrained actigraphy encoder (PAT) combined with a lightweight projection module to map sensor embeddings into LLM token space for autoregressive generation. Using a novel dataset of 54,383 synthetic ⟨actigraphy, text⟩ pairs created via GPT-4o from NHANES data, MotionTeller demonstrates strong performance in capturing circadian patterns and organizing participants into semantically coherent clusters. The approach offers a scalable solution for transforming wearable data into human-centered behavioral narratives with potential applications in health monitoring and behavioral understanding.

## Method Summary
The framework processes minute-level actigraphy data through a frozen PAT encoder to generate embeddings, which are then projected into the LLM's token space via a lightweight module. This enables autoregressive generation of daily activity descriptions using GPT-4o-synthesized summaries from NHANES data. The model is trained on 54,383 paired actigraphy-text samples, achieving convergence at training loss of 0.38 by epoch 15. Evaluation uses BERTScore-F1 (0.924) and ROUGE-1 (0.722) metrics, with qualitative and PCA-based analyses confirming the system's ability to capture circadian patterns and create semantically meaningful participant clusters.

## Key Results
- Achieves BERTScore-F1 of 0.924 and ROUGE-1 of 0.722, outperforming prompting-based baselines by 7%
- Training loss converges to 0.38 by epoch 15 with stable performance across runs
- Qualitative and PCA analyses confirm capture of circadian patterns and semantically coherent participant clustering

## Why This Works (Mechanism)
MotionTeller works by leveraging the complementary strengths of specialized sensor encoders and general-purpose LLMs. The frozen PAT encoder preserves learned actigraphy representations while the lightweight projection module efficiently bridges the modality gap to LLM token space. The synthetic dataset creation via GPT-4o provides abundant paired training examples, while the autoregressive generation approach allows for coherent, context-aware behavioral summaries. The system's success stems from effectively mapping temporal sensor patterns into linguistic structures that capture daily routines and health-related behaviors.

## Foundational Learning
1. **Actigraphy encoding** - Converts raw accelerometer signals into meaningful behavioral representations; needed to extract temporal patterns from sensor data; quick check: verify encoder preserves circadian rhythms
2. **Cross-modal projection** - Maps sensor embeddings to language model token space; needed to bridge sensor and text modalities; quick check: ensure projection maintains semantic relationships
3. **Autoregressive generation** - Sequential text prediction from encoded inputs; needed for coherent behavioral narrative creation; quick check: validate generated summaries maintain temporal consistency
4. **Synthetic data generation** - Uses GPT-4o to create training pairs from NHANES; needed to scale dataset creation; quick check: assess synthetic summary quality against human-written baselines
5. **Multimodal integration** - Combines time-series sensor data with language models; needed for comprehensive behavioral understanding; quick check: verify multimodal embeddings capture complementary information
6. **Temporal pattern analysis** - Identifies circadian rhythms and daily routines; needed for health-relevant behavioral insights; quick check: confirm pattern detection aligns with known sleep-wake cycles

## Architecture Onboarding

**Component Map:**
Raw actigraphy data → PAT encoder → Projection module → LLM token space → Autoregressive generation → Natural language summary

**Critical Path:**
Actigraphy preprocessing → PAT embedding extraction → Cross-modal projection → LLM inference → Summary generation

**Design Tradeoffs:**
- Frozen PAT encoder: reduces training complexity but limits adaptation to new sensor modalities
- Synthetic training data: enables large-scale training but may introduce distribution mismatches
- Lightweight projection: improves efficiency but may constrain representational capacity

**Failure Signatures:**
- Degraded performance on populations outside NHANES demographic
- Loss of temporal coherence in generated summaries
- Inability to capture novel activity patterns not present in training distribution

**First Experiments:**
1. Validate encoder preservation of circadian patterns on held-out actigraphy samples
2. Test projection module stability across varying activity intensity levels
3. Assess summary quality consistency across different time-of-day inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on synthetic text summaries from GPT-4o rather than human-written descriptions, potentially creating distribution mismatches
- Dataset represents specific NHANES demographics that may not generalize to broader populations
- Frozen PAT encoder architecture limits adaptation to domain-specific or emerging sensor modalities
- Lacks human evaluation metrics to assess clinical relevance and practical utility of generated summaries

## Confidence
- **High confidence** in technical implementation details (architecture, training, evaluation metrics)
- **Medium confidence** in performance claims due to synthetic training data nature
- **Low confidence** in real-world applicability without external validation on human-written summaries

## Next Checks
1. Conduct human evaluation studies comparing MotionTeller summaries against clinician-written behavioral descriptions using standardized accuracy and clinical relevance rubrics
2. Perform cross-population validation on actigraphy data from diverse demographic groups not represented in NHANES
3. Evaluate temporal generalization by testing model consistency on sequential data spanning multiple months for longitudinal behavioral pattern detection