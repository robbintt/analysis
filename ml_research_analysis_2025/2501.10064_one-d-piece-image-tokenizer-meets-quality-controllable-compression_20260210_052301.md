---
ver: rpa2
title: 'One-D-Piece: Image Tokenizer Meets Quality-Controllable Compression'
arxiv_id: '2501.10064'
source_url: https://arxiv.org/abs/2501.10064
tags:
- image
- token
- tokens
- quality
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: One-D-Piece is a variable-length discrete image tokenizer that
  addresses the inefficiency of fixed-length tokenization by enabling adaptive token
  counts ranging from 1 to 256. It introduces a regularization technique called "Tail
  Token Drop" that concentrates critical information at the beginning of the token
  sequence, allowing for high reconstruction quality even with fewer tokens.
---

# One-D-Piece: Image Tokenizer Meets Quality-Controllable Compression

## Quick Facts
- arXiv ID: 2501.10064
- Source URL: https://arxiv.org/abs/2501.10064
- Reference count: 40
- Primary result: Achieves rFID of 1.08 with 256 tokens, outperforming JPEG/WebP at small byte sizes

## Executive Summary
One-D-Piece introduces a variable-length discrete image tokenizer that addresses the inefficiency of fixed-length tokenization by enabling adaptive token counts ranging from 1 to 256. The method uses a "Tail Token Drop" regularization technique that forces the model to encode critical information at the beginning of the token sequence, enabling high reconstruction quality even with fewer tokens. It achieves state-of-the-art perceptual quality metrics (rFID) while maintaining strong performance across diverse downstream vision tasks.

## Method Summary
One-D-Piece builds on the TiTok architecture with a two-stage training approach. Stage 1 pretrains the model to predict logits from a pretrained MaskGIT tokenizer using cross-entropy loss. Stage 2 fine-tunes with reconstruction objectives (L2 + perceptual + GAN loss) while applying Tail Token Drop regularization. During training, a random number k is sampled uniformly from U(0, N-1), and tokens at positions [N-k, N] are dropped before decoding. This forces the model to prioritize information ordering, with critical content concentrated at the sequence head. The architecture uses a ViT encoder with 256 learnable latent tokens, vector quantization with 4096-codebook size, and a decoder with transformer layers followed by CNN upsampling.

## Key Results
- Achieves rFID of 1.08 with 256 tokens, outperforming JPEG and WebP in perceptual quality at smaller byte sizes
- Maintains strong performance across downstream tasks including classification, detection, segmentation, and depth estimation
- Demonstrates variable-length capability with quality scaling from 1 to 256 tokens

## Why This Works (Mechanism)

### Mechanism 1
Random tail truncation during training forces the model to encode information in a priority-ordered sequence. By sampling k ~ U(0, N-1) and dropping tokens at positions [N-k, N], the model learns to place survival-critical information earlier in the sequence. This creates pressure since any token at position i has probability (N-i)/N of being dropped.

### Mechanism 2
The first token captures global/image-wide semantic information, enabling meaningful reconstruction even at extreme compression (1-8 tokens). The Tail Token Drop pressure creates hierarchical encoding where early tokens carry broad semantic content sufficient for coarse reconstruction, while later tokens refine local details.

### Mechanism 3
Perceptual quality (FID) improves more efficiently with token count than pixel-level metrics (PSNR) due to the training objective composition. The Stage 2 loss combines L2 + perceptual loss + GAN loss, prioritizing perceptual fidelity over pixel-exact reconstruction, creating a rate-distortion-perception tradeoff.

## Foundational Learning

- **Vector Quantization (VQ-VAE)**: Essential for understanding how continuous latent representations are discretized into codebook indices. Quick check: Can you explain why straight-through estimation is needed for gradient flow through the quantization operation?

- **Rate-Distortion Theory**: Critical for understanding the variable-length design's explicit address of the tradeoff between compression rate and reconstruction quality. Quick check: For a given distortion budget D, what does rate-distortion theory say about the minimum achievable bitrate R(D)?

- **Vision Transformer (ViT) Patch Embeddings**: Necessary for understanding how the encoder processes images as patch sequences with learned positional embeddings plus N learnable latent tokens. Quick check: Why does the architecture use learnable latent tokens in addition to patch embeddings, rather than only patch tokens?

## Architecture Onboarding

- **Component map**: Input image -> patch embedding (16Ã—16 patches) -> concatenate with learnable latent tokens -> ViT encoder attention over [patches, latent_tokens] -> extract latent token outputs z -> quantize z -> discrete indices q -> apply Tail Token Drop (sample k ~ U(0, 255), retain q[:256-k]) -> pad dropped positions with mask tokens -> decode -> reconstruct

- **Critical path**: The encoder processes images as patch sequences with learned positional embeddings plus 256 learnable latent tokens, outputting latent representations z for the N=256 latent positions. These are quantized to discrete indices q, then truncated via Tail Token Drop before being padded with mask tokens and decoded through transformer layers and CNN upsampler.

- **Design tradeoffs**: Maximum 256 tokens vs. 512+ in 2D tokenizers limits maximum fidelity but enables variable-length flexibility; uniform vs. learned drop probability is simpler but may not match optimal information-theoretic allocation; L2 + perceptual + GAN loss provides better visual quality but worse PSNR than pure reconstruction objectives.

- **Failure signatures**: Token contribution heatmap shows flat/uniform distribution indicates Tail Token Drop not applied correctly during training; reconstruction quality doesn't improve with more tokens suggests decoder learned to ignore late-position tokens; codebook collapse (many unused codes) may need commitment loss weight adjustment.

- **First 3 experiments**: 1) Token contribution analysis: systematically replace each token position with random codebook entry, measure L1 reconstruction error to verify head-concentration hypothesis; 2) Rate-quality curve: evaluate rFID, LPIPS, and PSNR at token counts [1, 2, 4, 8, 16, 32, 64, 128, 256] to validate variable-length behavior; 3) Downstream task sanity check: reconstruct ImageNet validation images at 16/32/64/128 tokens, run through pretrained ConvNeXT classifier, compare Acc@1 to baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Can the latent space be utilized more efficiently by packing additional information into the currently low-contribution tail tokens? The analysis section states that tail tokens show almost no contribution and suggests there is potential for creating a more efficient tokenizer by packing more information into them, leaving this exploration for future research.

### Open Question 2
How does increasing the maximum token count beyond 256 affect reconstruction quality and pixel-level performance? The Limitations section notes that evaluation was restricted to a maximum of 256 tokens, and the impact of extending this limit remains unexplored.

### Open Question 3
How can the variable-length tokenizer be optimized to match the reconstruction quality of fixed-length baselines at very low token counts? The authors note that their models do not match TiTok at lower token counts (32, 64, 128) due to the Tail Token Drop trade-off, explicitly stating they leave improvements in this area for future exploration.

## Limitations

- Architecture is validated only up to 256 tokens, with performance beyond this limit unexplored
- Claims about Tail Token Drop being optimal for adaptive tokenization compared to alternative information-theoretic methods remain unverified
- Computational efficiency gains relative to fixed-length approaches may be less significant when accounting for mask token handling and variable sequence lengths during training

## Confidence

**High Confidence**: The Tail Token Drop mechanism successfully creates ordered information distribution in token sequences (verified by token contribution analysis); One-D-Piece achieves superior rFID scores compared to JPEG and WebP at comparable byte sizes; The method demonstrates versatility across multiple downstream vision tasks.

**Medium Confidence**: First token captures global semantic information (based on clustering analysis, but not validated across diverse datasets); Perceptual quality scales efficiently with token count (supported by rate-quality curves, but PSNR degradation suggests perceptual vs. pixel-level tradeoff); Two-stage training approach is optimal for this architecture.

**Low Confidence**: Claims about Tail Token Drop being the most effective approach for adaptive tokenization compared to alternative information-theoretic methods like InfoTok; Generalizability to datasets beyond ImageNet-1K; Long-term stability and performance consistency across extended training runs.

## Next Checks

1. **Token Contribution Analysis Replication**: Systematically replace each token position in the sequence with random codebook entries and measure the resulting L1 reconstruction error. Verify that token contribution follows the head-concentration pattern with the first 32 tokens carrying the majority of information.

2. **Cross-Dataset Performance Evaluation**: Train and evaluate One-D-Piece on diverse datasets (e.g., COCO, ADE20K, and a medical imaging dataset) to assess generalization beyond ImageNet. Measure rFID, downstream task performance, and token contribution patterns across these domains.

3. **Ablation of Training Mechanisms**: Create controlled experiments isolating the effects of Tail Token Drop by training variants with (a) deterministic truncation, (b) learned truncation probabilities, and (c) no truncation. Compare token contribution patterns, reconstruction quality, and downstream performance to quantify the specific contribution of the uniform random truncation approach.