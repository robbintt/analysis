---
ver: rpa2
title: 'APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus
  photographs'
arxiv_id: '2506.07542'
source_url: https://arxiv.org/abs/2506.07542
tags:
- images
- fundus
- data
- image
- challenge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The APTOS-2024 challenge addressed the problem of generating high-quality
  3D OCT images from 2D fundus photographs, a critical need in ophthalmology for accessible
  retinal imaging. The challenge established a benchmark dataset of 1,247 fundus-OCT
  pairs and employed two fidelity metrics: image-based distance (PSNR/SSIM for pixel-level
  similarity) and video-based distance (FVD for volumetric consistency).'
---

# APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs

## Quick Facts
- arXiv ID: 2506.07542
- Source URL: https://arxiv.org/abs/2506.07542
- Reference count: 0
- Primary result: Leading approach achieved FVD score of 624.59 for 3D OCT generation from fundus images

## Executive Summary
The APTOS-2024 challenge addressed the critical need for accessible retinal imaging by establishing a benchmark for generating high-quality 3D OCT images from 2D fundus photographs. The challenge dataset comprised 1,247 fundus-OCT pairs from 693 patients, with evaluation based on both image-level (PSNR/SSIM) and video-level (FVD) fidelity metrics. The top-performing method employed a two-stage latent diffusion model with pretrained foundation models and intra-batch self-attention mechanisms to achieve superior volumetric consistency.

## Method Summary
The best-performing approach (Algorithm 1) used a two-stage latent diffusion framework. Stage 1 trained an unconditional LDM on external OCT datasets (NEHUT2021, OLIVES, OCTA-500) to learn the OCT distribution. Stage 2 added conditioning modules including fundus encoders (RETFound, MM-Retinal, FLAIR), spatial map encoder, cross-attention, and intra-batch self-attention, then fine-tuned on the paired APTOS dataset. Data preprocessing included ROI cropping, resizing fundus to 224×224, removing OCT markers, and cross-modality augmentation through horizontal flipping with frame reordering.

## Key Results
- Algorithm 1 achieved FVD score of 624.59, significantly outperforming baseline methods
- PSNR and SSIM showed inverse relationships with FVD, indicating different sensitivity to transformation types
- Cross-modality augmentation and external pretraining contributed to performance improvements
- Intra-batch self-attention effectively preserved anatomical continuity across 3D volumes

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Semantic Conditioning via Foundation Models
Pretrained vision foundation models (RETFound, MM-Retinal, FLAIR) extract disease-relevant semantic features from fundus images that generalize to OCT generation. When used as condition encoders, they provide semantically rich features rather than raw pixel statistics, enabling the diffusion model to infer plausible depth structures from 2D surface projections. Break condition: Foundation models trained on different disease distributions may not transfer effectively.

### Mechanism 2: Intra-Batch Self-Attention for Volumetric Consistency
Treating sequential OCT B-scans as spatiotemporal sequences with intra-batch self-attention preserves anatomical continuity across 3D volumes. Standard frame-wise generation causes discontinuities, while self-attention allows each generated slice to attend to sibling slices, learning implicit correlations that enforce structural coherence. Break condition: Spatially sparse B-scans or disparate retinal regions may lead to spurious correlations.

### Mechanism 3: Latent Diffusion with External Dataset Pretraining
Two-stage training compensates for limited paired data. Stage 1 learns the OCT distribution from abundant unpaired external data, while Stage 2 adds conditioning and fine-tunes on paired data, requiring less paired data for convergence. Break condition: External datasets with different acquisition protocols or disease profiles may introduce harmful domain shift.

## Foundational Learning

- **Latent Diffusion Models (LDMs)**: Understanding VAE compression and iterative denoising is essential since Algorithm 1's architecture is built on LDM. Quick check: Can you explain why LDMs are more computationally efficient than pixel-space diffusion?

- **Cross-Attention Conditioning**: The fundus encoder conditions generation via cross-attention layers; understanding query/key/value dynamics helps debug conditioning failures. Quick check: How does cross-attention differ from concatenation-based conditioning in terms of spatial control?

- **Fréchet Video Distance (FVD)**: FVD was the primary ranking metric; understanding that it measures feature distribution similarity (not pixel accuracy) explains why SSIM/PSNR showed counterintuitive results. Quick check: Why might FVD remain stable while SSIM drops significantly under random cropping?

## Architecture Onboarding

- **Component map**: VAE Encoder/Decoder -> Fundus Encoder -> Spatial Map Encoder -> Denoising U-Net (with cross-attention and intra-batch self-attention)

- **Critical path**:
  1. Preprocess fundus (crop ROI → 224×224) and OCT (remove markers → crop to retina)
  2. Encode OCT B-scans individually through VAE encoder to latent space
  3. Stage 1: Train unconditional LDM on all available OCT data (challenge + external)
  4. Stage 2: Add fundus encoder, spatial map encoder, cross-attention, intra-batch self-attention; fine-tune on paired data
  5. Inference: Denoise latents conditioned on fundus, decode via VAE decoder

- **Design tradeoffs**:
  - LDM vs. GAN: LDM offers better mode coverage and training stability; GAN is faster at inference but risks mode collapse
  - Frame-wise vs. 3D learning: Frame-wise is simpler but loses coherence; 3D preserves consistency but increases complexity
  - Pixel metrics vs. FVD: PSNR/SSIM are sensitive to linear transformations; FVD captures semantic similarity but requires more data for stable estimation

- **Failure signatures**:
  - Anatomically implausible layers: Insufficient cross-attention conditioning or poor foundation model features
  - Slice-to-slice discontinuity: Intra-batch self-attention not learning or batch size too small
  - Blurry outputs: VAE decoder bottleneck or insufficient diffusion steps
  - OCT artifacts mismatch: External pretraining datasets have incompatible noise patterns

- **First 3 experiments**:
  1. Baseline reproduction: Implement Stage 1 only (unconditional OCT generation) on provided dataset to validate VAE and diffusion training pipeline
  2. Ablation on foundation models: Train three variants—using each foundation model (RETFound, MM-Retinal, FLAIR) individually—to measure which provides most transferable features
  3. Attention mechanism validation: Disable intra-batch self-attention and measure FVD degradation; this quantifies the mechanism's contribution to volumetric consistency

## Open Questions the Paper Calls Out

### Open Question 1
Does the inclusion of retinal layer boundary annotations in training data significantly improve the clinical accuracy and diagnostic utility of generated 3D OCT images? The current benchmark dataset lacks contour information for retinal layers, so this hypothesis remains untested. Evidence would require a comparative study training models on datasets with and without layer annotations, then evaluating both pixel-level metrics and clinical diagnostic accuracy.

### Open Question 2
Can cross-modal generation models maintain performance across rare, long-tail ocular conditions given the current dataset distribution? The authors note that increasing dataset heterogeneity would help mitigate biases and enable more equitable model performance across both common and rare conditions. Evidence would require stratified evaluation of model performance across disease frequency quartiles, with particular attention to FVD scores and structural fidelity in rare conditions.

### Open Question 3
What is the optimal evaluation metric or combination of metrics that correlates with clinical diagnostic utility in generated OCT images? The challenge established technical benchmarks but did not include clinician-based assessment of diagnostic value. Evidence would require correlation analysis between FVD/PSNR/SSIM scores and clinician ratings of diagnostic utility on a held-out test set.

### Open Question 4
Can the fundus-to-OCT generation approach be extended to produce complete volumetric OCT reconstructions rather than the current six-slice representation? The current dataset focuses primarily on partial OCT representations rather than complete volumetric reconstructions. Evidence would require development and evaluation of models trained on dense OCT volumes, with volumetric fidelity metrics and assessment of inter-slice consistency.

## Limitations

- Limited generalization to rare disease subtypes not well-represented in the APTOS dataset
- Unspecified implementation details for spatial map encoder and foundation model integration strategy
- Inverse relationship between pixel-level metrics (PSNR/SSIM) and FVD raises questions about clinical relevance

## Confidence

- **High confidence**: Two-stage training approach and intra-batch self-attention mechanism are well-specified and demonstrably effective based on FVD results
- **Medium confidence**: Foundation model transfer mechanism is plausible but not fully validated in the corpus
- **Low confidence**: Spatial map encoder implementation and optimal foundation model combination strategy are not specified

## Next Checks

1. Cross-dataset generalization test: Evaluate Algorithm 1 on external OCT datasets (NEHUT2021, OLIVES, OCTA-500) not used in training to assess domain transfer and identify potential overfitting to the APTOS distribution

2. Foundation model ablation study: Systematically test each foundation model (RETFound, MM-Retinal, FLAIR) individually and in combination to quantify their individual contributions to OCT generation quality and identify the most effective feature extractor

3. Metric correlation analysis: Investigate the relationship between FVD, PSNR, and SSIM scores by computing pairwise correlations across all submissions, and conduct a radiologist evaluation to determine which metrics best predict clinical utility of generated OCT volumes