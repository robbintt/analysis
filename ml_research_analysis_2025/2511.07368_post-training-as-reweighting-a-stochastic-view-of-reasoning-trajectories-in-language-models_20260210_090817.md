---
ver: rpa2
title: 'Post-Training as Reweighting: A Stochastic View of Reasoning Trajectories
  in Language Models'
arxiv_id: '2511.07368'
source_url: https://arxiv.org/abs/2511.07368
tags:
- cots
- reasoning
- easy
- arxiv
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical framework to analyze post-training
  methods for large language models, particularly focusing on the tension between
  exploration and overfitting. The authors model reasoning processes as tree-structured
  Markov chains, where pretraining discovers the reasoning structure and post-training
  reweights existing reasoning chains.
---

# Post-Training as Reweighting: A Stochastic View of Reasoning Trajectories in Language Models

## Quick Facts
- **arXiv ID**: 2511.07368
- **Source URL**: https://arxiv.org/abs/2511.07368
- **Reference count**: 40
- **Key outcome**: Post-training methods like RLVR and inference-time reward aggregation exhibit "squeezing" bias, suppressing rare but essential reasoning paths needed for hard problems while favoring easy-to-reason paths.

## Executive Summary
This paper provides a theoretical framework analyzing post-training methods for large language models through the lens of tree-structured Markov chains. The authors demonstrate that both reinforcement learning with verifiable rewards (RLVR) and inference-time reward aggregation methods systematically suppress rare but crucial reasoning paths, creating a "simplicity bias" that favors easy-to-reason chains. They prove that exploration-oriented mechanisms like rejecting easy instances and applying KL regularization help preserve these rare reasoning paths. Empirical simulations on abstract TMC models support the theoretical findings, showing that diversity-promoting methods maintain better multi-task capability while standard RL fine-tuning methods tend to forget reasoning patterns for secondary tasks.

## Method Summary
The authors model reasoning processes as tree-structured Markov chains (TMC), where pretraining discovers the reasoning structure and post-training reweights existing reasoning chains. They analyze both training-time RLVR methods (REINFORCE, RAFT, PPO, GRPO) and inference-time reward aggregation (ORM, PRM) through the lens of Markov decision processes. The framework establishes how different optimization objectives affect the distribution over reasoning trajectories, proving that standard methods create a self-reinforcing feedback loop that suppresses rare but essential reasoning paths. Diversity-preserving methods like RL-rejection and KL-regularized RL are shown to maintain multi-task capability by preserving these rare paths.

## Key Results
- RLVR methods (REINFORCE, RAFT, PPO, GRPO without KL) systematically suppress rare but essential reasoning paths by over-favoring easy-to-reason CoTs
- Both ORM and likelihood-based PRM prefer easy-to-reason CoTs even when hard-to-reason CoTs are correct for specific instances
- KL-regularized RL (GRPO with β > 0) and Doob's h-transform-based PRM (DPRM) preserve rare CoTs via temperature-controlled trade-offs
- Empirical TMC simulations confirm that standard RL methods generate >90% easy CoTs while diversity-preserving methods maintain balanced distributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: RLVR methods systematically suppress rare but essential reasoning paths by over-favoring easy-to-reason CoTs
- **Mechanism**: Advantage-based RL creates a self-reinforcing feedback loop where easy transitions receive positive advantage updates while hard transitions receive negative updates, compounding until rare paths are suppressed
- **Core assumption**: The multi-task TMC model correctly links transition probability to expected instance correctness
- **Evidence anchors**: Theoretical proof of advantage gap in Lemma 3, empirical simulation showing REINFORCE generates 94.33% easy vs 3.43% hard valid CoTs for TASK1
- **Break condition**: If advantage gap is small or reward noise is too high for consistent advantages, the squeezing effect weakens

### Mechanism 2
- **Claim**: Both ORM and likelihood-based PRM prefer easy-to-reason CoTs even when hard-to-reason CoTs are correct for specific instances
- **Mechanism**: Population rewards compute expected correctness over instances; easy-to-reason CoTs have higher base probabilities and thus appear correct more often in expectation
- **Core assumption**: Instance correctness is probabilistic per Def. 2, condition (iii)—correctness probability proportional to transition likelihood
- **Evidence anchors**: Proof in Prop. 2 showing strict preference ordering, empirical simulation showing ORM-BoN generates 21.00% easy vs 7.30% hard
- **Break condition**: If instance correctness is deterministic, or if hard CoTs have systematically higher correctness despite lower probability

### Mechanism 3
- **Claim**: KL-regularized RL and Doob's h-transform-based PRM preserve rare CoTs via temperature-controlled trade-offs
- **Mechanism**: KL-regularized optimal sampler is Gibbs distribution; DPRM uses h-transform to create adjusted distribution that ensures non-zero sampling for all base-model transitions when β, λ are finite
- **Core assumption**: Base model assigns positive probability to all valid CoTs—no true zeros in support
- **Evidence anchors**: Theoretical proof in Cor. 2 and Cor. 5, empirical simulation showing GRPO-KL generates 46.47% easy vs 16.27% hard
- **Break condition**: If base model assigns zero probability to certain valid CoTs, or if β → 0 (KL negligible)

## Foundational Learning

- **Concept: Tree-structured Markov Chains (TMC)**
  - Why needed here: The framework models reasoning as discrete state transitions in a tree-structured graph; states are logical assertions, transitions encode step difficulty
  - Quick check question: Why does the C_{o_l} (high-prob) vs. feeble transition distinction capture easy vs. hard CoT separation?

- **Concept: Advantage Function Decomposition**
  - Why needed here: Squeezing arises because A(o_l, o_{l+1}^{easy}) > 0 > A(o_l, o_{l+1}^{hard}); understanding this gap is essential
  - Quick check question: Given Q(o_l, o_{l+1}) = E[R^{out}|o_l, o_{l+1}] and V(o_l) = E[Q(o_l, o_{l+1})], why does the advantage gap emerge for easy vs. hard transitions?

- **Concept: Doob's h-Transform and Gibbs Distribution**
  - Why needed here: DPRM is derived via h-transform, linking KL-regularized optimization to inference-time adjusted sampling
  - Quick check question: How does h_k(o_l) = E[exp(λ R^{out})|o_l] ensure P^{DPRM}(o) = P^{Gibbs}(o)?

## Architecture Onboarding

- **Component map**: Base model → RLVR module → Population rewards → Diversity mechanisms → Inference scaling
- **Critical path**:
  1. Pretrain base model to convergence
  2. If RLVR: use RL-rej from start OR KL-regularized objective
  3. If inference scaling: replace ORM/PRM-BoN with DPRM-AS
  4. Monitor pass@K on target AND held-out tasks
  5. Tune β (training) or λ (inference)

- **Design tradeoffs**:
  - β/λ: Higher → more diversity, slower convergence; lower → faster target gain, more forgetting
  - BoN budget N: Need N ≥ Ω(log(ε)/log((M-1)/M)) for PRM-BoN
  - RL-rej threshold: Keep G = O(1) parallel samples; reject when all succeed
  - Multi-task tradeoff: RLVR on k can suppress k' via shared states; KL mitigates

- **Failure signatures**:
  - Entropy collapse: training converges but pass@K plateaus
  - Consistency bias: high ORM scores but low correctness on hard instances
  - Task interference: degradation on held-out tasks
  - Temperature misconfig: β too low → squeezing; too high → stalled convergence

- **First 3 experiments**:
  1. Reproduce TMC simulation with 4+ layers, 3+ tasks. Measure pass@K across tasks under REINFORCE, RL-rej, GRPO-KL
  2. Ablate β ∈ {0.1, 1.0, 10.0}: track convergence speed, target vs. held-out pass@K, CoT type distribution
  3. Compare ORM-BoN vs. DPRM-AS on hard-instance subset; vary N and measure pass@K

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the TMC framework be extended to model non-Markovian reasoning trajectories in actual LLMs?
  - Basis: The paper acknowledges MC models cannot perfectly capture actual LLM inference due to non-Markovianity
  - Why unresolved: Current TMC assumes Markovian transitions, but real LLM reasoning exhibits longer-range dependencies
  - What evidence would resolve it: Extending framework to incorporate k-order Markov properties or hidden-state models

- **Open Question 2**: Can the TMC framework analyze reflective reasoning behaviors and "aha moments"?
  - Basis: The paper identifies applying TMC analysis to reflective behavior and aha moments as a promising direction
  - Why unresolved: Current framework models forward reasoning without explicit mechanisms for reflection or revision
  - What evidence would resolve it: Modeling reflection as transitions revisiting earlier states

- **Open Question 3**: How does squeezing effect manifest under non-linear transformer parameterization?
  - Basis: The paper notes parameter isolation conditions typically fail in practice for non-linear transformers
  - Why unresolved: Main theoretical results rely on linear softmax models with decoupled representations
  - What evidence would resolve it: Theoretical bounds under empirical Neural Tangent Kernel approximations

## Limitations
- TMC abstraction assumes correctness probability is proportional to transition likelihood, which may not hold in real reasoning tasks
- Empirical validation is limited to synthetic TMC simulations rather than real LLMs
- Theory assumes perfect advantage estimation and infinite exploration, while real implementations face estimation noise

## Confidence
- **High**: Mathematical proofs of squeezing dynamics in TMCs (Theorems 2, 10) are rigorous and well-established
- **Medium**: Mapping from TMC dynamics to real LLM post-training behavior is plausible but not directly validated
- **Low**: Claims about specific hyperparameter ranges (β, λ) for practical diversity preservation are based on theoretical scaling

## Next Checks
1. **Empirical scaling test**: Implement TMC simulation with varying L (2-6) and M (2-10) to verify squeezing severity scales as predicted
2. **Real-model ablation**: Apply RL-rej and KL-regularized PPO to a small reasoning model on a verifiable math dataset
3. **Multi-task forgetting probe**: Train on two reasoning tasks with shared states, then measure degradation on Task 2 when Task 1 is emphasized