---
ver: rpa2
title: 'Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration
  in Search Clarifications'
arxiv_id: '2507.00543'
source_url: https://arxiv.org/abs/2507.00543
tags:
- human
- annotation
- tasks
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of using large language models
  (LLMs) for complex, subjective annotation tasks in search clarification. The authors
  evaluate four LLMs (GPT-4o, Claude 3, Cohere Command R, Mistral 7B) across three
  multi-dimensional annotation tasks and propose a human-in-the-loop (HITL) workflow
  using confidence thresholds and inter-model disagreement to selectively involve
  human review.
---

# Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications

## Quick Facts
- arXiv ID: 2507.00543
- Source URL: https://arxiv.org/abs/2507.00543
- Reference count: 40
- Primary result: Human-in-the-loop workflow using LLM ensemble disagreement reduces annotation effort by 24-45% while maintaining weighted Kappa > 0.7

## Executive Summary
This study addresses the challenge of using large language models (LLMs) for complex, subjective annotation tasks in search clarification. The authors evaluate four LLMs (GPT-4o, Claude 3, Cohere Command R, Mistral 7B) across three multi-dimensional annotation tasks and propose a human-in-the-loop (HITL) workflow using confidence thresholds and inter-model disagreement to selectively involve human review. Their results show that LLMs struggle with fine-grained distinctions, but the HITL framework improves annotation reliability while reducing human effort by 24–45% across tasks, with weighted Kappa scores remaining above 0.7. The approach offers a practical, scalable solution for deploying LLMs in high-quality annotation workflows with minimal human involvement.

## Method Summary
The authors evaluate four LLMs on three multi-dimensional search clarification annotation tasks using few-shot and zero-shot prompting strategies. They implement a human-in-the-loop workflow where instances with high inter-model disagreement or low confidence are routed to human reviewers, while high-confidence consensus instances are auto-accepted. Thresholds are calibrated using a 10% human-annotated subset through Pareto optimization, balancing effort reduction against quality metrics. The system uses verbalized confidence scores from LLMs as proxy measures for uncertainty.

## Key Results
- LLMs show significant performance variation across tasks, with Mistral 7B consistently underperforming proprietary models
- The HITL framework reduces human effort by 24-45% across tasks while maintaining weighted Kappa scores above 0.7
- Inter-model disagreement serves as an effective proxy for annotation difficulty, correctly flagging 76-89% of instances requiring human review
- Few-shot prompting improves Claude 3 performance but harms Mistral 7B, indicating model-specific prompting strategies are needed

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Based Filtering via Ensemble Disagreement
- **Claim:** Routing instances with high inter-model disagreement to humans while auto-accepting high-confidence consensus preserves quality while minimizing manual effort.
- **Mechanism:** The system aggregates predictions from diverse LLMs. If models agree (low standard deviation in confidence) and individual confidence is high, the label is accepted. High standard deviation acts as a proxy for "hard" or "ambiguous" instances, triggering human review.
- **Core assumption:** Inter-model variance correlates with annotation difficulty and likelihood of error; high-confidence consensus correlates with correctness.
- **Evidence anchors:** [abstract] "inter-model disagreement to selectively involve human review." [section 4.4] "The standard deviation acts as a proxy for uncertainty: high variance indicates low consensus... and flags the instance for human review."

### Mechanism 2: Threshold Calibration via Pareto Optimization
- **Claim:** Optimal confidence thresholds are task-dependent and can be identified by balancing human effort reduction against quality metrics (Weighted Kappa).
- **Mechanism:** A small human-annotated subset (10%) is used to simulate the HITL workflow across various threshold combinations. The "best" solution is selected from the Pareto front to maximize efficiency while keeping Kappa > 0.7.
- **Core assumption:** The statistical distribution of confidence and error in the 10% sample is representative of the full dataset.
- **Evidence anchors:** [section 4.4] "Identifying optimal thresholds... using Pareto front optimisation... A constraint is applied... Kw... minimum acceptable threshold of 0.7." [section 5] "The close alignment between the 10% and 15% subsets suggests that 10% is sufficient."

### Mechanism 3: Model Diversity for Error Mitigation
- **Claim:** Aggregating outputs from structurally diverse models (proprietary vs. open-source, different sizes) creates a more robust "crowd" than relying on a single high-performance model.
- **Mechanism:** Individual models exhibit different failure modes (e.g., Mistral's confusion vs. GPT-4o's adjacent class errors). Averaging predictions ( ensemble labeling) cancels out individual idiosyncrasies, similar to human crowd-voting.
- **Core assumption:** Model errors are not perfectly correlated; biases in one model are not universally present in others.
- **Evidence anchors:** [section 5] "GPT-4o and Claude 3... performed comparably... while Mistral 7B consistently underperformed." [section 4.4] "Leveraging multiple diverse LLMs... capturing this diversity... to inform a robust... strategy."

## Foundational Learning

- **Concept: Weighted Kappa (Kw)**
  - **Why needed here:** Standard accuracy is misleading for imbalanced, ordinal data (e.g., a rating of 4 vs 5 is closer than 1 vs 5). The paper uses Kw to measure inter-rater reliability while penalizing large discrepancies.
  - **Quick check question:** Why is Macro F1 insufficient for evaluating the "preference rating" task in this paper? (Answer: It doesn't account for the ordinal nature/adjacency of errors).

- **Concept: Verbalized Confidence Estimation**
  - **Why needed here:** Unlike classifiers, LLMs output text. The system requires a confidence score to function. This technique prompts the LLM to output its own certainty (e.g., "Confidence: 90%"), which serves as the gating signal for human review.
  - **Quick check question:** How does the paper validate that verbalized confidence is a reliable signal compared to raw probabilities?

- **Concept: Pareto Front Analysis**
  - **Why needed here:** There is no single "correct" setting for the HITL workflow—it is a tradeoff between cost (human effort) and quality (Kappa). Pareto analysis identifies the set of "non-dominated" solutions.
  - **Quick check question:** If you lower the confidence threshold to 70%, what likely happens to Human Effort Reduction (HER) and Weighted Kappa?

## Architecture Onboarding

- **Component map:** Inference Layer (4 LLMs) -> Aggregation Engine (Majority Vote, Mean Confidence, SD) -> Routing Logic (Threshold checks) -> Output (Auto-accepted or Human Review Queue)

- **Critical path:** The initial calibration phase (Step 1-6). You must annotate the 10% seed dataset and run the grid search over thresholds to find the optimal settings before deploying the pipeline to the remaining 90%.

- **Design tradeoffs:**
  - **Ensemble Size vs. Latency/Cost:** Using 4 models increases inference cost and latency 4x compared to a single model, but significantly boosts reliability.
  - **Few-Shot vs. Zero-Shot:** The paper shows FSS helps Claude but hurts Mistral. You must decide on a per-model prompting strategy rather than a global one.

- **Failure signatures:**
  - **High SD, High Confidence:** Models are confident but disagree. This often occurs in "subjective" or "ambiguous" domains (Section 5.4). System correctly flags for human review.
  - **Low SD, Low Confidence:** Models agree they are uncertain. System flags for human review.
  - **Calibration Drift:** Auto-accepted labels (High Conf, Low SD) start showing low Kappa. Indicates thresholds are too loose or data distribution has shifted.

- **First 3 experiments:**
  1. **Threshold Sensitivity Test:** Run the pipeline on a held-out set with thresholds set to force 10%, 30%, and 50% human review. Plot the resulting Kappa scores to verify the Pareto curve.
  2. **Ablation on Model Count:** Run the aggregation using only the top 2 models (GPT-4o, Claude 3) vs all 4. Measure the drop in reliability (Kw) vs. the cost savings.
  3. **Domain Stress Test:** Feed the system queries from the "Technical Topics" or "Other" categories identified in Section 5.4 to see if the "flagged for review" rate increases appropriately for these hard domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed HITL workflow generalize to high-stakes domains (e.g., legal or medical annotation) or other IR tasks beyond search clarification?
- Basis in paper: [explicit] The authors state in the "Conclusions and Future Work" that "future work should study the generalisability of these findings across tasks and datasets," acknowledging the study is limited to a single dataset.
- Why unresolved: The specific confidence thresholds and effort reduction rates (24–45%) were derived from search clarification tasks; it is uncertain if these parameters hold for domains with higher cognitive loads or different ambiguity profiles.
- What evidence would resolve it: Applying the framework to diverse datasets (e.g., medical triage or legal review) and measuring if the Human Effort Reduction (HER) remains substantial while maintaining a weighted Kappa above 0.7.

### Open Question 2
- Question: Can alternative ensemble strategies, such as performance-weighted voting, outperform the simple majority voting approach used in the current framework?
- Basis in paper: [explicit] The paper lists "evaluating alternative ensemble strategies in HITL" as a primary direction for future work.
- Why unresolved: The current implementation aggregates model predictions via simple majority voting, treating all models equally despite their varying architectures and calibration levels (e.g., Mistral 7B vs. GPT-4o).
- What evidence would resolve it: Comparing the annotation quality (Kappa scores) and human effort required for weighted ensemble methods against the majority voting baseline on the same dataset.

### Open Question 3
- Question: Does task-specific fine-tuning of LLMs reduce the reliance on manual review more effectively than the proposed few-shot HITL approach?
- Basis in paper: [explicit] The Conclusion suggests that "Fine-tuning LLMs for specific tasks may also further improve alignment with human judgement and reduce reliance on manual review."
- Why unresolved: While fine-tuning is proposed as a cost-saving measure, it is unclear if the reduction in human effort justifies the computational cost of training compared to the lightweight few-shot HITL method.
- What evidence would resolve it: A comparative analysis of Human Effort Reduction (HER) factors and annotation costs between fine-tuned models and the few-shot HITL pipeline.

## Limitations

- Evaluation focuses on three specific annotation tasks within search clarification, limiting generalizability to other domains
- Optimal threshold calibration relies on a 10% human-annotated seed set, but sensitivity to sample size is not thoroughly investigated
- Reliance on verbalized confidence scores introduces potential calibration issues across different model families

## Confidence

**High Confidence Claims:**
- LLMs struggle with fine-grained distinctions in multi-dimensional annotation tasks (supported by quantitative performance differences)
- The HITL framework improves annotation reliability while reducing human effort (demonstrated through weighted Kappa scores > 0.7 and measured HER reductions)
- Inter-model disagreement serves as a reliable proxy for annotation difficulty (validated through standard deviation metrics)

**Medium Confidence Claims:**
- The 10% seed set provides sufficient calibration data (supported by comparison with 15% subset but not extensively validated)
- Pareto optimization effectively identifies optimal thresholds (methodologically sound but dependent on representative sampling)
- Model diversity meaningfully improves aggregation quality (demonstrated through comparative analysis)

**Low Confidence Claims:**
- The framework generalizes to annotation tasks beyond search clarification (not empirically tested)
- The specific threshold ranges are universally optimal (likely task-dependent but not extensively explored)
- Verbalized confidence scores provide reliable uncertainty estimates across all model families (mixed results observed)

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the seed set size (5%, 10%, 20%, 30%) and measure how threshold calibration accuracy and resulting HER/Kappa performance change.

2. **Cross-Domain Transferability Test**: Apply the HITL framework to a fundamentally different annotation task (e.g., sentiment analysis, medical coding) while maintaining the same model ensemble and routing logic.

3. **Longitudinal Stability Evaluation**: Run the same annotation pipeline on the same dataset at different time points to measure how stable the auto-accepted labels remain over time, particularly focusing on calibration drift.