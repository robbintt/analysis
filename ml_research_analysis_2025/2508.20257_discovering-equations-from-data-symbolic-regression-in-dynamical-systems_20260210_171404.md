---
ver: rpa2
title: 'Discovering equations from data: symbolic regression in dynamical systems'
arxiv_id: '2508.20257'
source_url: https://arxiv.org/abs/2508.20257
tags:
- systems
- data
- equations
- symbolic
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares six symbolic regression methods\u2014GPlearn,\
  \ AI-Feynman, PySINDy, PySR, PyKAN, and ODEFormer\u2014on nine dynamical systems\
  \ including chaotic models, biological interactions, and epidemiological compartments.\
  \ All but ODEFormer and AI-Feynman successfully recovered governing equations, with\
  \ PySR achieving the highest accuracy, correctly identifying all systems and producing\
  \ expressions statistically indistinguishable from the originals."
---

# Discovering equations from data: symbolic regression in dynamical systems

## Quick Facts
- arXiv ID: 2508.20257
- Source URL: https://arxiv.org/abs/2508.20257
- Reference count: 0
- Primary result: Six symbolic regression methods tested on nine dynamical systems; PySR achieved highest accuracy, correctly identifying all systems with statistically indistinguishable equations.

## Executive Summary
This paper benchmarks six symbolic regression methods—GPlearn, AI-Feynman, PySINDy, PySR, PyKAN, and ODEFormer—on nine dynamical systems including chaotic models, biological interactions, and epidemiological compartments. All but ODEFormer and AI-Feynman successfully recovered governing equations, with PySR achieving the highest accuracy, correctly identifying all systems and producing expressions statistically indistinguishable from the originals. PySINDy and PyKAN also performed well but were more sensitive to noise. The study demonstrates strong potential for equation discovery across diverse domains, though challenges remain in handling noise, high-dimensional systems, and real-world data.

## Method Summary
The study compares six symbolic regression algorithms on synthetic time-series data generated from nine dynamical systems. Methods include evolutionary approaches (GPlearn, PySR), sparse regression (PySINDy), neural-based methods (PyKAN, ODEFormer), and a brute-force algorithm (AI-Feynman). Each method attempts to recover the governing differential equations from numerical data. Performance is evaluated using structural recovery accuracy, statistical indistinguishability via Wilcoxon tests, and trajectory prediction via R² scores. The authors provide explicit parameter settings and operator libraries for reproducibility.

## Key Results
- PySR achieved perfect structural recovery across all nine systems and produced equations statistically indistinguishable from originals
- PySINDy and PyKAN successfully identified all systems but showed significant sensitivity to noise
- ODEFormer and AI-Feynman underperformed due to algorithmic limitations and outdated code
- Most methods required 5-15 minutes for equation discovery on standard hardware

## Why This Works (Mechanism)

### Mechanism 1: Multi-Objective Evolutionary Search (PySR)
The algorithm evolves populations of expression trees using genetic programming, optimizing for both accuracy and parsimony through Pareto efficiency. This combinatorial search explores operator-variable spaces to find equations that balance prediction error against complexity. The method assumes governing dynamics can be represented by finite combinations of provided operators and variables.

### Mechanism 2: Sparse Identification of Nonlinear Dynamics (PySINDy)
Governing equations are extracted by enforcing sparsity on a library of candidate functions. The method constructs a matrix of candidate terms and applies sparse regression to identify coefficient vectors where most entries are zero. This assumes the time derivative of state variables is sparse in the space of library functions.

### Mechanism 3: Kolmogorov-Arnold Representation (PyKAN)
Symbolic expressions are learned by replacing fixed activation functions in neural networks with learnable, spline-based univariate functions on edges. Instead of learning weights on nodes, KANs learn activation functions (splines) on edges, theoretically avoiding catastrophic forgetting while capturing complex relationships.

## Foundational Learning

- **Ordinary Differential Equations (ODEs)**: Core task involves recovering equations of form $\dot{x} = f(x, t)$. Quick check: Can you explain the difference between a state variable (e.g., susceptible population $S$) and a parameter (e.g., infection rate $\beta$)?

- **Regularization & Sparsity (L1/L2)**: Essential for preventing overfitting and selecting simplest equations. Quick check: Why would increasing the sparsity threshold ($\lambda$) result in a simpler equation but potentially lower accuracy?

- **Genetic Programming (GP) Fundamentals**: Required to understand PySR and GPlearn, which evolve "populations" of equations using "crossover" and "mutation". Quick check: In the context of GP, what represents the "chromosome" or "genetic code"?

## Architecture Onboarding

- **Component map**: Synthetic data generator → Numerical differentiator → State matrix $X$ & derivative matrix $\dot{X}$ → Method layer (expression trees, sparse regression, or neural networks) → Evaluation layer (complexity metric, R², Wilcoxon test)

- **Critical path**: Define operator library (missing critical operators prevents recovery), configure differentiation or topology, tune regularization to balance fit vs. noise robustness

- **Design tradeoffs**: PySR offers robustness and exact recovery but requires more compute time; PySINDy is instant but brittle to noise; PyKAN allows interactivity but suffers from instability

- **Failure signatures**: Bloat (rapid complexity increase without accuracy gains), noise amplification ($R^2$ drops to ~0), structural hallucination (outputting dimensionally incorrect equations)

- **First 3 experiments**:
  1. Baseline Recovery (Pendulum): Run PySR with `binary_operators=["+", "*"]` and `unary_operators=["sin"]` to recover $\ddot{\theta} = -\frac{g}{l}\sin(\theta)$
  2. Noise Stress Test (Lotka-Volterra): Inject 1% Gaussian noise and compare PySINDy baseline vs. with adjusted threshold or denoising
  3. Library Mismatch (Lorenz): Attempt recovery with strictly polynomial library (no multiplication) to observe failure on $xy$ or $xz$ terms

## Open Questions the Paper Calls Out

- Can symbolic regression methods accurately recover governing equations from real-world epidemiological data given multicollinearity and unknown system behaviors? This requires moving beyond synthetic data with known ground truth to empirical datasets where inferred equations must align with biological mechanisms.

- How do specific noise-mitigation strategies improve the structural recovery of noise-sensitive algorithms like PyKAN and PySINDy? The paper identified noise sensitivity as critical but didn't test available denoising features that could restore accuracy.

- What frameworks are required to guarantee the unique identifiability of symbolic regression models when applied to partial differential equations (PDEs)? While ODE identifiability is better understood, theoretical guarantees for PDEs remain largely unresolved.

- To what extent can symbolic regression algorithms autonomously discover necessary coordinate transformations without requiring prior domain knowledge? Current methods need hints or predefined transformation libraries for systems requiring non-linear coordinate transforms.

## Limitations

- The benchmark relies entirely on synthetic data with known ground truth, limiting generalizability to real-world systems with unmodeled dynamics
- AI-Feynman's poor performance partly reflects unmaintained dependencies rather than pure algorithmic limitations
- PyKAN's dramatic noise sensitivity (100% $R^2$ drop) raises questions about reliability in practical applications

## Confidence

- **High confidence**: PySR's superior performance on structural recovery and statistical indistinguishability is well-supported by quantitative results
- **Medium confidence**: PySINDy's sensitivity to noise is documented, but systematic regularization parameter sweeps were not explored
- **Low confidence**: PyKAN's claimed interpretability advantage is undermined by its instability; the trade-off between interactivity and reliability remains unclear

## Next Checks

1. **Noise floor determination**: Systematically vary noise levels (0.1% to 10%) for all methods to identify the noise threshold where each algorithm fails completely

2. **Operator library ablation**: Test each method's recovery accuracy when systematically removing critical operators to quantify library completeness requirements

3. **Real-world transferability**: Apply top performers (PySR, PySINDy) to noisy, subsampled epidemiological data from public health databases, comparing recovered parameters to literature values