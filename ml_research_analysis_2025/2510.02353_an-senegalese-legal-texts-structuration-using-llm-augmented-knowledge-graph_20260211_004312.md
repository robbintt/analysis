---
ver: rpa2
title: An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph
arxiv_id: '2510.02353'
source_url: https://arxiv.org/abs/2510.02353
tags:
- legal
- articles
- arxiv
- article
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study applies AI and LLMs to improve access to legal texts
  in Senegal's judicial system. Legal documents were processed using a rule-based
  algorithm to extract 7,967 articles, which were stored in a relational database.
---

# An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph

## Quick Facts
- arXiv ID: 2510.02353
- Source URL: https://arxiv.org/abs/2510.02353
- Reference count: 31
- Processed 7,967 legal articles from 20 Senegalese documents using rule-based extraction and LLM triple extraction

## Executive Summary
This study presents a framework for structuring Senegalese legal texts using AI and LLMs to improve judicial system accessibility. The approach combines rule-based document parsing with graph database construction and LLM-powered knowledge extraction. Legal documents were processed to extract 7,967 articles, which were stored in a relational database before being manually structured into a Neo4j graph database with 2,872 nodes and 10,774 relationships. Knowledge triples were extracted using Few-Shot Chain-of-Thought prompting with LLMs including GPT-4o, GPT-4, and Mistral-Large, achieving ROUGE scores between 58% and 86%. The framework aims to enhance legal comprehension for Senegalese citizens and legal professionals.

## Method Summary
The methodology employs a three-stage approach: (1) Rule-based extraction algorithm parses DOCX legal documents to extract articles with metadata including domain, law number, article number, signature date, and hierarchical subdivisions; (2) Manual graph construction creates a Neo4j database with 2,872 nodes and 10,774 relationships representing legal entities and their semantic connections; (3) LLM-based triple extraction uses Few-Shot Chain-of-Thought prompting to identify knowledge triples from articles, evaluated against ground truth using ROUGE metrics. The system processes 7,967 articles from 20 Senegalese legal documents, with GPT-4o achieving the highest extraction accuracy (86% R-1) while Mistral-Large provides optimal inference efficiency.

## Key Results
- Extracted 7,967 legal articles from 20 Senegalese documents using O(n) rule-based algorithm
- Built Neo4j graph database containing 2,872 nodes and 10,774 relationships
- GPT-4o achieved highest ROUGE-1 score of 86% for triple extraction, Mistral-Large fastest at 2m23s
- Framework demonstrates effective structuring of legal texts for enhanced accessibility and comprehension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical rule-based parsing enables reliable extraction of legal articles from structured DOCX files with O(n) time complexity.
- Mechanism: The algorithm iterates through document elements, identifies subdivision markers and article boundaries using Senegal's legislative drafting guidelines, and captures metadata (domain, law number, signature date) from file paths and content structure. Conditional flags track parsing state (in_art, in_subdiv, in_r_subs) to correctly attribute content to articles.
- Core assumption: Legal documents follow the prescribed hierarchical structure (Parts → Books → Titles → Paragraphs) with consistent article numbering patterns including multiplicative adverbs (bis, ter, quarter).
- Evidence anchors:
  - [abstract] "Legal documents were processed using a rule-based algorithm to extract 7,967 articles"
  - [section III.A] Algorithm 1 details extraction logic with flags and metadata capture; time complexity O(n) stated
  - [corpus] Weak direct corpus support; related work [Jain et al. 2022, Vuong et al. 2023] confirms knowledge graph construction from legal corpora is viable but uses different extraction methods
- Break condition: Scanned PDFs or non-DOCX formats cannot be processed; OCR preprocessing required (noted as limitation in Section V.A).

### Mechanism 2
- Claim: Manually constructed graph databases provide verifiable ground truth for evaluating LLM-extracted relationships.
- Mechanism: Domain experts identify node labels (Law, Decree, Article, Person, etc.) and relationship types (publish, possess, modify, repeal, frame, execute, based_on) from table of contents analysis. Regular expressions extract entities, creating 2,872 nodes and 10,774 relationships in Neo4j.
- Core assumption: The Land and Public Domain Code's table of contents accurately reflects the full entity structure and relationship semantics.
- Evidence anchors:
  - [abstract] "A graph database was then developed using Neo4j, containing 2,872 nodes and 10,774 relationships"
  - [section III.B] "primary node labels were identified from the legal document's table of contents"; Fig. 4 shows node/relationship types
  - [corpus] No corpus papers directly validate this ground-truth methodology; [Dong et al. 2021] (ref [10]) uses similar Neo4j approach
- Break condition: Labor-intensive manual validation becomes infeasible as legal corpus scales; accuracy depends on expert completeness.

### Mechanism 3
- Claim: Few-Shot Chain-of-Thought prompting enables LLMs to extract knowledge triples with 82-86% ROUGE scores on legal reference extraction.
- Mechanism: Prompts include 2 example articles with content, metadata, extracted references, and correct triples. The "Let's think step by step" directive activates reasoning. LLMs output triples like (current_article, refers_to, article_X_of_law_Y). ROUGE metrics compare generated triples against graph database ground truth.
- Core assumption: Few-shot examples generalize across diverse article structures; ROUGE scores adequately capture triple quality.
- Evidence anchors:
  - [abstract] "Knowledge triples were extracted using LLM models including GPT-4o, GPT-4, and Mistral-Large, with ROUGE scores ranging from 58% to 86%"
  - [section IV.C, Table II] GPT-4o: R-1=86%, GPT-4: R-1=83.49%, Mistral-Large: R-1=82.90%; Mistral-Large fastest (2m23s)
  - [corpus] No direct corpus validation; [LegalOne] and related LLM-legal papers focus on reasoning, not triple extraction specifically
- Break condition: Smaller models (GPT-3.5-Turbo, GPT-4o-Mini, Mistral-Nemo) fail on complex number ranges and format adherence (Figs. 7-8); insufficient examples cause prefix errors (adding "R.") and missed references.

## Foundational Learning

- Concept: **Knowledge Graph Construction (Nodes, Relationships, Triples)**
  - Why needed here: The system stores legal entities as nodes (Law, Article, Person) and their semantic connections as relationships (modify, repeal, refer_to). Triples (subject, predicate, object) represent atomic facts for LLM extraction.
  - Quick check question: Given "Article 5 of Law 2020-10 modifies Article 3 of Decree 2019-05," identify two triples this represents.

- Concept: **Few-Shot Chain-of-Thought (CoT) Prompting**
  - Why needed here: The paper uses Few-Shot CoT to guide LLMs through complex legal reasoning (identifying referenced articles, handling number ranges with ellipses). Examples + "think step by step" improve triple extraction.
  - Quick check question: What is the difference between zero-shot, few-shot, and chain-of-thought prompting? Why combine them?

- Concept: **ROUGE Metrics (R-1, R-2, R-L, R-Lsum)**
  - Why needed here: Paper evaluates triple extraction quality using ROUGE against ground-truth triples. R-1 captures unigram overlap, R-2 captures bigram coherence, R-L captures longest common subsequence.
  - Quick check question: Why might ROUGE scores overestimate quality for legal triples? What errors might ROUGE miss?

## Architecture Onboarding

- Component map:
  DOCX Legal Files → [Rule-based Extractor] → Articles (7,967) + Metadata → [Relational Database] → [Manual Graph Construction] → Neo4j (2,872 nodes, 10,774 relationships) → [Sample Articles] → [Few-Shot CoT Prompt] → [LLM APIs: GPT-4o/GPT-4/Mistral-Large] → [Output Parser] → Knowledge Triples → [ROUGE Evaluator] ← Ground Truth from Neo4j

- Critical path: DOCX parsing → Article extraction accuracy → Graph database quality → LLM prompt design → Triple evaluation. Errors propagate; faulty extraction corrupts both ground truth and LLM training examples.

- Design tradeoffs:
  - **Accuracy vs. Speed**: GPT-4o highest accuracy (86% R-1) but 3m56s; Mistral-Large near-optimal accuracy (82.9%) with best speed (2m23s)
  - **Manual vs. Automated Graph Construction**: Manual ensures precision but doesn't scale; fully automated risks relationship errors
  - **Model Size**: Larger models (GPT-4o ~1.8T params) outperform smaller ones, but Pixtral-Large (124B) underperforms due to multimodal training diluting text focus

- Failure signatures:
  - **Prefix hallucination**: Models add "R." to article numbers not marked regulatory (GPT-4o-Mini, Mistral-Nemo)
  - **Range formatting failure**: Smaller models omit ellipses or split ranges incorrectly (GPT-3.5-Turbo)
  - **Context bleed**: Models extract references from metadata rather than content (Pixtral-Large extracted "law 2009-23" from signature date)
  - **Incomplete triples**: Reasoning correct but final triple missing predicate (GPT-3.5-Turbo identified "R.38" but didn't complete triple)

- First 3 experiments:
  1. **Validate extraction algorithm** on 5 diverse legal codes beyond Land/Domain; manually verify article counts and metadata accuracy against source documents
  2. **A/B test prompt variations**: Compare 2-shot vs. 5-shot prompts; measure ROUGE delta and error types on held-out articles
  3. **Efficiency benchmark**: Run Mistral-Large on full 7,967 articles; measure total time, cost per 1000 articles, and spot-check triple quality at scale

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Retrieval-Augmented Generation (RAG) and Reasoning and Acting (ReAct) techniques be effectively integrated with the LLM-based knowledge triple extraction approach to create an accurate legal assistant?
- Basis in paper: [explicit] The conclusion states this methodology "will lay a strong foundation for future projects aimed at developing an advanced legal assistant" using "Retrieval-Augmented Generation (RAG) and Reasoning and Acting (ReAct) techniques."
- Why unresolved: The current work focuses only on extraction and structuring; the interactive assistant component with RAG/ReAct integration is proposed but not yet developed or tested.
- What evidence would resolve it: A functional prototype assistant with RAG/ReAct integration, evaluated on user query accuracy, response relevance, and legal correctness metrics.

### Open Question 2
- Question: Can advanced OCR systems using LLMs or TrOCR be optimized to accurately extract legal content from scanned documents that currently impede access to jurisprudence?
- Basis in paper: [explicit] The discussion notes "many recent documents are only available in scanned formats, complicating data extraction" and states "there is a necessity to optimize advanced OCR systems using LLMs or TrOCR through extensive text datasets in forthcoming research."
- Why unresolved: The current extraction algorithm works only on DOCX files; scanned PDFs remain inaccessible and require different processing approaches.
- What evidence would resolve it: Comparative OCR accuracy results on scanned Senegalese legal documents, with extraction quality measured against manual transcription baselines.

### Open Question 3
- Question: Can the labor-intensive graph database construction process be automated using LLMs while maintaining the precision achieved through manual regular expression identification?
- Basis in paper: [inferred] The methodology section states that entity identification "is labor-intensive and necessitates meticulous recognition of various components to accurately retrieve distinct entities."
- Why unresolved: Manual construction ensures accuracy but doesn't scale; automated LLM-based graph construction is mentioned as future evaluation but not yet implemented or validated.
- What evidence would resolve it: An automated LLM-based graph construction pipeline evaluated against the manually constructed Neo4j database (2,872 nodes, 10,774 relationships) using precision, recall, and F1 metrics.

### Open Question 4
- Question: What evaluation metrics beyond ROUGE scores can better capture the semantic accuracy and legal validity of extracted knowledge triples?
- Basis in paper: [inferred] ROUGE metrics evaluate surface-level text similarity but may miss semantic errors the paper documents, such as models adding incorrect prefixes ("R.") or extracting extraneous metadata not present in the source article.
- Why unresolved: The paper demonstrates that models achieving high ROUGE scores can still produce legally invalid triples (formatting errors, hallucinated references), suggesting ROUGE alone is insufficient.
- What evidence would resolve it: A new evaluation framework incorporating semantic similarity, legal expert validation, and structured format compliance that correlates better with actual triple correctness.

## Limitations

- Manual graph construction ensures accuracy but creates scalability bottleneck that limits application to larger legal corpora
- ROUGE metrics may not fully capture semantic quality of legal triples or detect subtle errors in legal reasoning
- Rule-based extraction algorithm is tightly coupled to Senegalese legislative formatting conventions, raising portability concerns

## Confidence

- **High Confidence**: Article extraction algorithm performance (7,967 articles extracted with O(n) complexity), Neo4j graph construction methodology, and LLM inference efficiency measurements
- **Medium Confidence**: Knowledge triple extraction quality as measured by ROUGE scores
- **Low Confidence**: Generalizability of the framework to other legal domains, document formats beyond DOCX, and jurisdictions with different legislative drafting conventions

## Next Checks

1. **Cross-jurisdictional validation**: Apply the rule-based extraction algorithm to legal documents from at least two other Francophone African countries (e.g., Mali, Ivory Coast) and compare extraction accuracy, metadata completeness, and processing time

2. **Alternative evaluation metrics**: Implement semantic evaluation methods (e.g., question-answering accuracy based on extracted triples, legal expert review of triple validity) alongside ROUGE to assess whether the high ROUGE scores translate to meaningful legal knowledge extraction

3. **Scalability stress test**: Process the full set of 7,967 extracted articles through the complete pipeline (Neo4j construction + LLM triple extraction) using Mistral-Large to measure total processing time, cost, and spot-check accuracy at scale