---
ver: rpa2
title: Bulk-boundary decomposition of neural networks
arxiv_id: '2511.02003'
source_url: https://arxiv.org/abs/2511.02003
tags:
- neural
- networks
- deep
- network
- boundary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a bulk-boundary decomposition (BBD) framework
  for analyzing the training dynamics of deep neural networks. The key insight is
  to reorganize the Lagrangian of stochastic gradient descent into two parts: a data-independent
  bulk term, governed by network architecture and activation functions, and a data-dependent
  boundary term, encoding interactions from training samples at input and output layers.'
---

# Bulk-boundary decomposition of neural networks

## Quick Facts
- arXiv ID: 2511.02003
- Source URL: https://arxiv.org/abs/2511.02003
- Reference count: 0
- Primary result: Introduces a bulk-boundary decomposition framework that reorganizes SGD Lagrangian into data-independent bulk terms and data-dependent boundary terms, enabling field-theoretic analysis of neural network training dynamics

## Executive Summary
This paper presents a novel theoretical framework for analyzing deep neural network training dynamics through bulk-boundary decomposition (BBD). By reformulating the Lagrangian of stochastic gradient descent and promoting pre-activations to fundamental degrees of freedom, the authors expose the local and homogeneous structure underlying deep networks. The decomposition naturally separates architectural contributions from statistical ones, revealing translational symmetry along depth for architectures with repeating layer structures. The framework further enables the construction of field-theoretic actions that capture both bulk and boundary contributions, bridging deep learning dynamics with statistical physics and field theory.

## Method Summary
The authors reformulate neural network training as a dissipative Lagrangian system by changing variables from weights and biases to weights and pre-activations (neurons). This substitution allows the original Lagrangian to be decomposed into data-independent bulk terms (governed by network architecture and activation functions) and data-dependent boundary terms (encoding interactions from training samples). For architectures with local connectivity, they derive field-theoretic actions by taking continuum limits, treating network depth as an effective spatial coordinate. The framework applies to standard loss functions including MSE, cross-entropy, KL divergence, and hinge loss.

## Key Results
- The Lagrangian of SGD can be decomposed into bulk (data-independent) and boundary (data-dependent) terms through a change of variables from (W, b) to (W, z)
- For repeating layer structures, the bulk Lagrangian exhibits discrete translational symmetry along the depth direction
- Field-theoretic continuum limits can be constructed for locally-connected architectures, yielding actions that capture signal propagation properties
- The framework naturally separates architectural contributions from statistical ones, providing a new theoretical foundation for understanding optimization and generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Promoting pre-activations to fundamental degrees of freedom enables locality decomposition.
- Mechanism: The paper performs a change of variables from weights/biases to weights/neurons (pre-activations $z^{(m)}_i$), replacing biases via $b^{(m)}_i = z^{(m+1)}_i - \sum_j W^{(m)}_{ij} \sigma(z^{(m)}_j)$. This reformulation allows the Lagrangian to be split into bulk and boundary terms, making locality along depth explicit.
- Core assumption: Neuron dynamics inherited from parameter dynamics are sufficient to capture the system's degrees of freedom; the recursive structure can be inverted consistently.
- Evidence anchors: [abstract]: "By promoting neurons to fundamental degrees of freedom through a change of variables, the authors expose the local and homogeneous structure underlying deep networks..."; [section]: Page 2, Eq. (6)–(7) — explicit substitution and Lagrangian decomposition.

### Mechanism 2
- Claim: The bulk Lagrangian exhibits discrete translational symmetry along depth for repeated architectures.
- Mechanism: For architectures with repeating layer structures, $L_{bulk}$ is invariant under $m \to m+1$ (layer index shift), analogous to lattice translational symmetry. This arises because bulk terms couple only adjacent layers after decomposition.
- Core assumption: Repeating layer structure; no layer-specific architectural variations breaking translation invariance.
- Evidence anchors: [abstract]: "...reveals translational symmetry along the depth direction for architectures with repeating layer structures..."; [section]: Page 3 — "the bulk Lagrangian is invariant under $m \to m+1$."

### Mechanism 3
- Claim: Field-theoretic continuum limits can be constructed from the discrete BBD Lagrangian under locality assumptions.
- Mechanism: Treating discrete indices as continuous coordinates (depth $x$, width $y$) and expanding in lattice spacing ($a_x$, $a_y$), the authors derive continuum bulk and boundary field Lagrangians (Eqs. 9–10) with derivative terms capturing signal propagation.
- Core assumption: Locality in width requires architectural restrictions (e.g., local connectivity as in CNNs); continuum expansion converges for small lattice spacing.
- Evidence anchors: [abstract]: "...constructing field theories for both bulk and boundary sectors that naturally emerge from the discrete Lagrangian."; [section]: Page 3–4, "Field Description" — Eqs. (8)–(10).

## Foundational Learning

- Concept: Lagrangian mechanics and action principles
  - Why needed here: The paper reformulates SGD dynamics as a dissipative Lagrangian system to decompose into bulk and boundary terms.
  - Quick check question: Can you explain why a loss function acts as potential energy in this Lagrangian, and what the kinetic terms represent?

- Concept: Translational symmetry in discrete systems
  - Why needed here: Understanding how layer-index shifts correspond to symmetry operations helps interpret the bulk symmetry claim.
  - Quick check question: For a 1D chain of coupled oscillators, what does invariance under $n \to n+1$ imply about the coupling structure?

- Concept: Continuum limits and field theory
  - Why needed here: The paper derives field theories by taking discrete layer/width spacing to zero; understanding lattice-to-continuum transitions is essential.
  - Quick check question: When expanding a discrete Laplacian in lattice spacing $a$, what leading-order continuum operator does it approximate?

## Architecture Onboarding

- Component map:
  - Pre-activation neurons $z^{(m)}_i$: New fundamental degrees of freedom (replacing biases)
  - Weights $W^{(m)}_{ij}$: Retained dynamical variables
  - Bulk Lagrangian $L_{bulk}$: Data-independent kinetic and coupling terms across adjacent layers
  - Boundary Lagrangian $L_{boundary}$: Data-dependent terms at input/output layers plus loss
  - Continuum fields $\hat{z}(x, y, t)$, $\hat{w}(x, y, t)$: Coarse-grained versions for field theory

- Critical path:
  1. Start from standard SGD Lagrangian in $(W, b)$ variables (Eq. 5)
  2. Perform substitution $b \to z$ using recursive relation (Eq. 6)
  3. Expand Lagrangian to identify bulk and boundary terms (Eq. 7)
  4. For field theory: impose locality in width, take continuum limit (Eqs. 9–10)

- Design tradeoffs:
  - Locality vs. generality: Local width coupling enables clean field theory but restricts architecture class
  - Expressiveness vs. symmetry: Repeating layers yield translational symmetry but exclude heterogeneous architectures
  - Discrete clarity vs. continuum insight: Discrete BBD is exact; continuum requires approximations

- Failure signatures:
  - Nonlocal bulk terms → check for layer-specific architectural variations
  - Continuum limit divergence → verify width-direction connectivity is truly local
  - Broken symmetry → inspect layer-index-dependent hyperparameters

- First 3 experiments:
  1. Implement change of variables (Eq. 6) numerically for a small MLP and verify $L = L_{bulk} + L_{boundary}$ holds to machine precision.
  2. Measure discrete translational invariance of $L_{bulk}$ for a repeating-layer MLP by computing layer-shifted Lagrangian values; quantify symmetry breaking.
  3. For a local-architecture CNN, coarse-grain discrete variables and compare simulation trajectories to predictions from continuum field equations (Eqs. 9–10).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does a statistical-mechanical description of boundary stochasticity explain generalization, and what effective temperature emerges from data sampling?
- Basis in paper: [explicit] "The statistical-mechanical description of boundary stochasticity may clarify how generalization arises from effective thermal ensembles"
- Why unresolved: The boundary Lagrangian is derived, but the statistical mechanics framework connecting sampling noise to generalization is not developed.
- What evidence would resolve it: Derivation of an effective temperature parameter from data statistics that quantitatively predicts generalization gaps across architectures.

### Open Question 2
- Question: Can the bulk field theory predict signal propagation properties such as critical initialization or gradient flow behavior?
- Basis in paper: [explicit] "This Lagrangian contains all the information about how the signal from the input boundary is transmitted to the output boundary. Exploring its implications will be an interesting direction for future work."
- Why unresolved: The bulk Lagrangian (Eq. 10) is constructed but not analyzed to extract concrete predictions about information propagation.
- What evidence would resolve it: Analytical predictions from the bulk Lagrangian matching known phenomena like signal propagation at criticality.

### Open Question 3
- Question: Does the bulk field theory exhibit dynamical phase transitions that correlate with qualitative changes in training behavior?
- Basis in paper: [explicit] "symmetry-breaking analyses could connect network anisotropy to dynamical phase transitions"
- Why unresolved: Translational symmetry along depth is identified, but no order parameters or critical points are analyzed.
- What evidence would resolve it: Identification of order parameters and critical points corresponding to observable transitions in training dynamics.

## Limitations

- The change of variables from (W, b) to (W, z) requires careful verification of invertibility and consistency across training dynamics
- Translational symmetry claims hold only for strictly repeating layer architectures and break down with heterogeneous designs
- Field-theoretic continuum limits require strong locality assumptions that exclude fully connected networks and depend on careful coarse-graining procedures not fully specified

## Confidence

- Medium confidence: The bulk-boundary decomposition mechanism (Mechanism 1) has explicit derivation but limited independent validation
- Low confidence: The translational symmetry claim (Mechanism 2) lacks empirical verification and depends on restrictive architectural assumptions
- Low confidence: The field-theoretic continuum formulation (Mechanism 3) is sketched but incomplete, with key terms omitted and no numerical validation

## Next Checks

1. Verify the change of variables numerically by implementing the decomposition for a small MLP and checking that the Lagrangian splits correctly to machine precision during training
2. Test the translational symmetry claim by measuring invariance of L_bulk under layer shifts for architectures with varying degrees of repetition
3. Validate the continuum field theory by comparing training trajectories from discrete simulations to continuum predictions for a local CNN architecture