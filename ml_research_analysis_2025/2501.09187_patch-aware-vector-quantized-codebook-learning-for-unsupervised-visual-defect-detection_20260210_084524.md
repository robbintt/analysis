---
ver: rpa2
title: Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual Defect
  Detection
arxiv_id: '2501.09187'
source_url: https://arxiv.org/abs/2501.09187
tags:
- detection
- defect
- learning
- normal
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unsupervised visual defect detection, a critical
  task in industrial quality control where defect samples are rare or absent during
  training. The challenge is to design a representation space that is both expressive
  enough to capture normal data patterns and compact enough to avoid mode collapse,
  which blurs the distinction between normal and defective samples.
---

# Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual Defect Detection

## Quick Facts
- arXiv ID: 2501.09187
- Source URL: https://arxiv.org/abs/2501.09187
- Authors: Qisen Cheng; Shuhui Qu; Janghwan Lee
- Reference count: 40
- Primary result: State-of-the-art performance on MVTecAD, BTAD, and MTSD datasets for unsupervised visual defect detection

## Executive Summary
This paper addresses unsupervised visual defect detection in industrial quality control, where defect samples are rare or absent during training. The authors propose Patch-aware Vector Quantized Autoencoder (PVQAE), which extends the VQ-VAE framework with a patch-aware dynamic code assignment scheme. This allows the model to allocate codes of varying resolutions based on the context richness of different image regions, optimizing spatial representation efficiency. Additionally, the model learns normal budget priors to enhance defect detection by constraining reconstruction quality for unexpected patterns. Experiments demonstrate that PVQAE achieves state-of-the-art performance, outperforming existing methods in both image-level and pixel-level defect detection tasks.

## Method Summary
PVQAE combines a VQ-VAE backbone with dynamic resolution allocation and budget-constrained learning. The encoder produces multi-resolution feature embeddings, which are routed through a Dynamic Routing Module that selects optimal code resolution per patch using an MLP with Gumbel-Softmax sampling. A progressive budget learning strategy penalizes excessive fine-resolution code usage, with costs weighted by DWT-based entropy. The model learns normal budget priors via a transformer that predicts per-class budget sequences, which are used during inference to constrain reconstruction capacity for anomaly detection.

## Key Results
- Achieves state-of-the-art AUROC on MVTecAD, BTAD, and MTSD datasets
- Outperforms existing methods in both image-level and pixel-level defect detection
- Ablation shows per-class budget priors significantly improve performance (97.3% vs 91.5% image AUROC on MVTecAD)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic, content-aware code resolution assignment improves normal pattern coverage while preventing defect reconstruction.
- Mechanism: A Dynamic Routing Module evaluates multi-resolution feature embeddings via average pooling and an MLP gating mechanism. Gumbel-Softmax provides differentiable selection among resolution levels R = {r₁ < r₂ < ... < rₖ}, allocating finer codes to high-context regions (e.g., intricate textures) and coarser codes to homogeneous areas (e.g., smooth surfaces).
- Core assumption: Context richness correlates with DWT coefficient entropy, and regions with simpler patterns require less representational capacity.
- Evidence anchors:
  - [abstract] "patch-aware dynamic code assignment scheme, enabling context-sensitive code allocation"
  - [Section III-B.1] "we use a Dynamic Routing Module to determine the optimal resolution... Gumbel-Softmax technique... providing a soft, differentiable approximation"
  - [corpus] Weak direct support; neighbor papers address VQ in other domains (audio, graphs, time-series) but not spatially-adaptive resolution for anomaly detection.
- Break condition: If defects exhibit similar context entropy to normal regions (e.g., subtle uniform discoloration), resolution assignment may not distinguish them.

### Mechanism 2
- Claim: Budget-constrained learning prevents mode collapse by limiting the model's ability to over-express any input.
- Mechanism: A budget loss L_budget penalizes fine-resolution code usage with exponentially higher cost (c > 1 factor per level). DWT-normalized entropy inversely weights base cost, making detailed regions cheaper. The loss weight λ schedules linearly from 0→max, forcing initial expressive learning before capacity optimization.
- Core assumption: Normal patterns can be reconstructed within a learnable budget; defects require excessive capacity to reconstruct well.
- Evidence anchors:
  - [abstract] "avoiding mode collapse—blurring the distinction between normal and defect data embeddings"
  - [Section III-B.2] "λ to linearly increase from 0 to the maximum value... encourages the model to learn expressive latent representations... then progressively pivot towards refining"
  - [corpus] No direct corroboration for progressive budget scheduling in VQ-based anomaly detection.
- Break condition: If budget constraints are too aggressive (high λ), normal sample reconstruction degrades, increasing false positives.

### Mechanism 3
- Claim: Normal budget priors amplify defect signals by restricting reconstruction capacity during inference.
- Mechanism: During training, budget allocation patterns B_norm are recorded per object class and learned by a Budget Prior Transformer via masked token prediction. At inference, the predicted normal budget B̂_norm replaces dynamically-allocated budgets, preventing the model from allocating extra capacity to defect regions. Defect score S = S_prior × S_recon combines budget deviation and reconstruction error.
- Core assumption: Defects are localized anomalies; most regions remain normal and their budget can be inferred from spatial context.
- Evidence anchors:
  - [abstract] "learned strategy for code allocation in inference, to enlarge the discrepancy between normal and defective samples"
  - [Section III-C.2] "we use B̂_norm for code allocation to prevent the model from utilizing excessive representation power, thereby limiting reconstruction quality for unseen defects"
  - [Table III] Ablation shows per-class priors achieve 97.3% / 95.9% (image/pixel AUROC) vs. 91.5% / 90.8% without priors.
- Break condition: If defects span large image regions or alter global context significantly, the prior prediction may adapt and mask anomaly signals.

## Foundational Learning

- Concept: **Vector Quantization (VQ-VAE)**
  - Why needed here: The discrete codebook structure constrains latent space, providing natural regularization against mode collapse.
  - Quick check question: Can you explain why discrete codes help prevent an autoencoder from reconstructing arbitrary inputs?

- Concept: **Gumbel-Softmax Relaxation**
  - Why needed here: Enables differentiable sampling from categorical resolution choices during end-to-end training.
  - Quick check question: What happens to gradient variance as temperature τ → 0?

- Concept: **Discrete Wavelet Transform (DWT) for Entropy Estimation**
  - Why needed here: Provides a deterministic proxy for "context richness" without requiring labeled region importance.
  - Quick check question: Why might DWT entropy fail for periodic textures vs. stochastic textures?

## Architecture Onboarding

- Component map:
  Encoder E -> Dynamic Routing Module -> Codebook Q -> Decoder G -> Output
  Budget Prior Transformer (separate training)

- Critical path:
  1. Input → Encoder → multi-resolution embeddings Z
  2. Dynamic Routing → resolution scores b → codebook lookup per patch
  3. Quantized codes → Decoder → reconstruction
  4. Budget sequences → Budget Prior Transformer → B̂_norm
  5. Inference: S_prior (CE vs. predicted budget) × S_recon (L2 error) → defect score

- Design tradeoffs:
  - **Resolution hierarchy depth |k|**: More levels increase flexibility but complicate routing and budget learning.
  - **Budget weight λ_max**: Higher values enforce stricter budgets but risk under-representing normal samples (peaks at ~1.25 per Figure 5).
  - **Per-class vs. universal priors**: Per-class improves performance but requires class labels and sufficient samples.

- Failure signatures:
  - **High false positives**: Budget too tight (λ too high) or DWT entropy misestimates context for certain textures.
  - **Low defect localization**: Defects blend into normal budget patterns; prior transformer over-generalizes.
  - **Mode collapse**: Budget learning disabled (λ=0) or codebook size K insufficient for normal pattern diversity.

- First 3 experiments:
  1. **Ablate λ schedule**: Compare constant vs. cosine vs. linear scheduling on MVTecAD to validate progressive budget learning hypothesis.
  2. **Visualize budget maps**: Overlay learned resolution assignments on normal vs. defective samples to confirm defects require higher budgets when unconstrained.
  3. **Vary codebook size K**: Test sensitivity of AUROC to K to characterize the expressiveness-compactness tradeoff explicitly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the single codebook with class-token architecture scale effectively to datasets with significantly higher object diversity than MVTecAD?
- Basis in paper: [inferred] The authors demonstrate multi-object training using a class token (Section III.C.1), but only validate on MVTecAD (15 classes) and a combined small dataset.
- Why unresolved: A single discrete codebook might suffer from capacity collapse or competition when tasked with encoding hundreds of diverse industrial products simultaneously.
- What evidence would resolve it: Evaluation of codebook utilization rates and detection performance on large-scale industrial datasets containing >50 distinct object categories.

### Open Question 2
- Question: Is Discrete Wavelet Transform (DWT) entropy a robust proxy for "context richness" across all texture types?
- Basis in paper: [inferred] The progressive budget learning relies on DWT entropy to determine resolution costs (Section III.B.2).
- Why unresolved: DWT is a hand-crafted signal processing metric that may correlate poorly with semantic complexity in certain stochastic textures or high-frequency noise patterns.
- What evidence would resolve it: An ablation study comparing DWT-based budget assignment against a fully learned or attention-based importance metric.

### Open Question 3
- Question: What is the computational latency overhead of the Dynamic Routing Module during inference?
- Basis in paper: [inferred] The paper claims to address "wasted computational resources" (Abstract), yet introduces additional inference steps: the MLP-based router and the Budget Prior Transformer.
- Why unresolved: While storage efficiency is improved via compact codes, the runtime cost of the routing and transformer mechanisms is not quantified relative to standard VQ-VAE.
- What evidence would resolve it: Frames-per-second (FPS) or latency (ms/image) benchmarks comparing the full PVQAE pipeline against baselines on equivalent hardware.

## Limitations
- The reliance on DWT-based entropy as a proxy for context richness may not generalize well to all texture types, particularly stochastic patterns
- The single transformer block for budget prior learning may be insufficient for complex, multi-modal budget distributions across diverse defect types
- Progressive budget scheduling strategy lacks direct experimental validation against alternative scheduling methods

## Confidence

- **High**: The core VQ-VAE + budget-constrained learning framework is well-established in the literature, and the paper's empirical improvements on MVTecAD are robust across multiple object classes.
- **Medium**: The patch-aware dynamic resolution assignment mechanism is plausible but depends critically on the DWT entropy proxy being discriminative; limited ablation on entropy alternatives reduces confidence.
- **Low**: The budget prior transformer's effectiveness relies on assumptions about defect locality and normal budget predictability that are not thoroughly validated against global or large-region defects.

## Next Checks

1. **Ablate DWT entropy**: Replace DWT entropy with alternative context measures (e.g., local variance, Laplacian energy) and compare resolution allocation quality and final AUROC.
2. **Test global defect scenarios**: Introduce synthetic defects spanning >50% of image area and evaluate whether the budget prior correctly identifies them versus false negatives.
3. **Hyperparameter sensitivity**: Systematically vary codebook size K, resolution hierarchy depth, and λ_max to map the robustness frontier of PVQAE's performance.