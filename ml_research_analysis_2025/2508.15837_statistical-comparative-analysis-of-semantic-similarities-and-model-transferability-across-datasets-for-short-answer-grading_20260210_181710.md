---
ver: rpa2
title: Statistical Comparative Analysis of Semantic Similarities and Model Transferability
  Across Datasets for Short Answer Grading
arxiv_id: '2508.15837'
source_url: https://arxiv.org/abs/2508.15837
tags:
- similarity
- datasets
- dataset
- mohler
- sprag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the transferability of state-of-the-art models
  across different datasets in short answer grading. It compares two established datasets
  (STSB and Mohler) with a novel dataset (SPRAG) using both contextual and non-contextual
  similarity metrics.
---

# Statistical Comparative Analysis of Semantic Similarities and Model Transferability Across Datasets for Short Answer Grading

## Quick Facts
- arXiv ID: 2508.15837
- Source URL: https://arxiv.org/abs/2508.15837
- Authors: Sridevi Bonthu; S. Rama Sree; M. H. M. Krishna Prasad
- Reference count: 27
- Key outcome: SPRAG dataset shows higher semantic similarity to Mohler than STSB, suggesting Mohler-trained models can potentially transfer to SPRAG without extensive retraining.

## Executive Summary
This study evaluates the transferability of state-of-the-art short answer grading models across different datasets by comparing semantic similarities using both contextual and non-contextual metrics. The analysis compares two established datasets (STSB and Mohler) with a novel dataset (SPRAG) containing Python programming answers. Statistical analyses reveal that SPRAG is more similar to Mohler than to STSB, with contextual similarity metrics performing consistently across all datasets while non-contextual metrics favor the Mohler dataset. The findings suggest that models developed for the Mohler dataset could potentially be adapted for use with SPRAG, reducing the need for resource-intensive, dataset-specific training in NLP applications.

## Method Summary
The methodology involves preprocessing three datasets (STSB, Mohler, SPRAG) to extract sentence pairs and similarity scores, then computing similarity using 8 metrics: Jaccard, TF-IDF Cosine, Word Mover's Distance (WMD), and four contextual embeddings (USE, SBERT cross-encoder, SBERT bi-encoder, SimCSE supervised/unsupervised). Paired t-tests and Cohen's d effect sizes are calculated on similarity scores across dataset pairs to assess statistical significance and practical differences. The approach assumes normalized similarity scores (0-5 scale) and standard tokenization, with analysis focusing on whether small effect sizes between datasets indicate potential transferability.

## Key Results
- SPRAG dataset shows higher semantic and statistical similarity to the Mohler dataset than to STSB across multiple metrics
- Non-contextual metrics (Jaccard, TF-IDF, WMD) favor the Mohler dataset due to shared lexical domain
- Contextual similarity metrics (USE, SBERT variants, SimCSE) perform consistently across all datasets despite SPRAG's unique programming language features
- Cohen's d effect sizes between Mohler-SPRAG are consistently smaller than STSB-SPRAG, indicating more similar distributions and higher transfer potential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared lexical domain between source and target datasets enables non-contextual similarity metrics to detect transfer potential.
- Mechanism: Word-level overlap in domain-specific vocabulary (20% intersection between Mohler and SPRAG top-20 words) allows Jaccard, TF-IDF cosine, and Word Mover's Distance to produce higher similarity scores for transferable dataset pairs.
- Core assumption: Lexical overlap correlates with semantic task similarity for short answer grading.
- Evidence anchors:
  - [abstract] "Non-contextual metrics favor the Mohler dataset"
  - [section] Table 1 shows 20% word overlap between Mohler and SPRAG (e.g., "function," "list," "variable"); STSB has zero intersection with either.
  - [corpus] Weak corpus evidence—no direct neighbor papers examine lexical overlap as a transfer predictor.
- Break condition: If target dataset uses domain-specific jargon absent from source dataset (as SPRAG does with Python keywords like "def," "elif"), non-contextual metrics will underestimate transferability.

### Mechanism 2
- Claim: Contextual embeddings provide domain-invariant similarity signals that remain stable across datasets with differing lexical distributions.
- Mechanism: Pre-trained transformers (USE, SBERT, SimCSE) encode sentences into high-dimensional vectors where semantic relationships persist despite surface-form differences, enabling consistent similarity computation across datasets.
- Core assumption: Pre-training corpora contain sufficient exposure to both technical programming language and general natural language.
- Evidence anchors:
  - [abstract] "Contextual similarity metrics perform consistently across all datasets"
  - [section] Figure 7 shows contextual metrics (USE, SBERT CE, SBERT BiE, SimCSE) produce comparable similarity distributions across Mohler, STSB, and SPRAG, unlike non-contextual metrics.
  - [corpus] Neighbor paper "GenZ" notes foundational models often fail to capture dataset-specific patterns, suggesting contextual metrics provide generalization at the cost of fine-grained domain adaptation.
- Break condition: If target domain contains novel syntactic structures or symbols not seen during pre-training (e.g., code syntax in SPRAG), contextual embeddings may produce noisy similarity estimates.

### Mechanism 3
- Claim: Cohen's d effect size between dataset similarity score distributions predicts transfer feasibility more reliably than p-values alone.
- Mechanism: Small effect sizes (d < 0.2) between SPRAG-Mohler on contextual metrics (USE: 0.012, NegWMD: 0.021) indicate overlapping distributions, suggesting models trained on one should generalize to the other without extensive retraining.
- Core assumption: Similarity score distribution alignment implies grading model transferability.
- Evidence anchors:
  - [abstract] "SPRAG dataset shows higher semantic and statistical similarity to the Mohler dataset than to STSB"
  - [section] Table 3 shows Cohen's d for Mohler-SPRAG is consistently smaller (closer to zero) than for STSB-SPRAG across contextual metrics, indicating more similar distributions.
  - [corpus] Neighbor paper "Relation Extraction or Pattern Matching?" finds RE models struggle with unseen data despite higher intra-dataset performance—suggesting distribution alignment is necessary but not sufficient for transfer.
- Break condition: If similarity metrics do not correlate with downstream task performance (grading accuracy), effect size alignment becomes a proxy without predictive validity.

## Foundational Learning

- **Concept: Semantic Textual Similarity (STS)**
  - Why needed here: The entire methodology rests on computing similarity between sentence pairs as a proxy for grading model transferability.
  - Quick check question: Can you explain why cosine similarity between TF-IDF vectors differs from cosine similarity between SBERT embeddings?

- **Concept: Effect Size vs. Statistical Significance**
  - Why needed here: The paper uses Cohen's d to quantify practical differences between datasets, complementing t-test p-values that only indicate whether differences exist.
  - Quick check question: If two datasets have a statistically significant difference (p < 0.05) but Cohen's d = 0.1, what does this mean for transfer potential?

- **Concept: Transfer Learning in NLP**
  - Why needed here: The core research question is whether models trained on established datasets (Mohler, STSB) can be applied to a novel dataset (SPRAG) without retraining.
  - Quick check question: What factors determine whether a pre-trained language model will transfer effectively to a new domain?

## Architecture Onboarding

- **Component map:**
  Dataset Preprocessing -> Non-contextual Similarity Layer -> Contextual Similarity Layer -> Statistical Analysis Layer -> Interpretation Module

- **Critical path:**
  1. Preprocess all three datasets to extract sentence pairs and similarity labels
  2. Compute similarity scores using all 8 metrics for each dataset independently
  3. Run paired t-tests and Cohen's d on metric outputs across dataset pairs
  4. Identify which dataset pair shows smallest effect sizes → highest transfer potential

- **Design tradeoffs:**
  - Non-contextual metrics are faster but fail on datasets without lexical overlap (STSB-SPRAG: Jaccard ≈ 10⁻²²)
  - Contextual metrics capture semantic similarity but may conflate code syntax with natural language (SPRAG challenge noted in Section 5)
  - Effect size threshold (d < 0.2) is heuristic; paper does not validate against actual transfer performance

- **Failure signatures:**
  - Jaccard/TF-IDF scores near zero with high Cohen's d → datasets are lexically disjoint, transfer unlikely
  - Contextual metrics show high variance across datasets → pre-training domain mismatch
  - p-values significant but effect sizes large → distributions differ practically, transfer risky

- **First 3 experiments:**
  1. Replicate similarity computations on Mohler-SPRAG subset; verify Cohen's d < 0.05 for USE and NegWMD
  2. Fine-tune an SBERT model on Mohler, evaluate zero-shot performance on SPRAG to validate whether small effect size predicts transfer success
  3. Ablate contextual vs. non-contextual metrics: train grading models using each metric type as features, compare accuracy drops when transferring to SPRAG

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed statistical and semantic similarity between the Mohler and SPRAG datasets directly translate to high performance when applying Mohler-trained models to the SPRAG dataset without resource-intensive fine-tuning?
- Basis in paper: [inferred] The conclusion states that knowledge from SOTA models "could potentially be transposed" based on similarity metrics, but the study does not actually perform the transfer learning or training experiments to validate this hypothesis.
- Why unresolved: The research establishes dataset similarity as a proxy for transferability but does not empirically verify the downstream grading performance of a transferred model.
- What evidence would resolve it: Empirical results comparing the grading accuracy of a model trained on Mohler when evaluated directly on the SPRAG test set versus a baseline model trained specifically on SPRAG.

### Open Question 2
- Question: To what extent do the unique "intricate sentence structures" and code-related keywords in the SPRAG dataset cause the observed performance degradation in contextual similarity metrics?
- Basis in paper: [inferred] The interpretation of results notes that contextual metrics "encountered challenges with the SPRAG dataset due to its intricate sentence structures," yet the paper does not isolate these features for analysis.
- Why unresolved: While the paper identifies a discrepancy in performance, it does not investigate the specific linguistic or syntactic features (e.g., Python keywords like `def`, `del`) that drive this challenge.
- What evidence would resolve it: An ablation study or error analysis that correlates low similarity scores with the frequency and positioning of code-specific tokens within the SPRAG sentences.

### Open Question 3
- Question: Is the combination of paired t-tests and Cohen's d effect sizes on semantic similarity scores a reliable predictor of model transferability for datasets outside of short answer grading?
- Basis in paper: [inferred] The authors utilize these statistical tools to "yield comprehensive insights into the potential applicability" of models, assuming that statistical similarity is a valid proxy for functional adaptability.
- Why unresolved: The paper validates the statistical similarity but leaves the causal link between these specific statistical indicators and successful neural model adaptation unproven across broader NLP domains.
- What evidence would resolve it: A correlation analysis across diverse NLP tasks demonstrating that high statistical similarity (low p-values, specific Cohen's d ranges) consistently predicts minimal performance loss during domain adaptation.

## Limitations
- The correlation between similarity metrics and actual grading performance remains unvalidated through empirical transfer experiments
- The analysis assumes normal distribution of similarity scores, which may not hold for non-contextual metrics like Jaccard
- Effect size thresholds (d < 0.2) are heuristic without theoretical grounding in transfer learning literature

## Confidence
- **High Confidence (8/10):** The comparative methodology using paired t-tests and Cohen's d is sound and the statistical results are reproducible
- **Medium Confidence (6/10):** The claim that Mohler-trained models will transfer to SPRAG is plausible but unverified through actual model training and evaluation
- **Low Confidence (4/10):** The assumption that contextual embeddings provide domain-invariant representations for programming language evaluation is questionable

## Next Checks
1. **Transfer Validation:** Fine-tune a grading model on Mohler dataset and evaluate zero-shot performance on SPRAG. Compare accuracy against a model trained directly on SPRAG to quantify actual transfer loss versus predicted similarity-based transfer potential.

2. **Metric Correlation Analysis:** Compute correlation coefficients between each similarity metric and downstream grading accuracy. This would validate whether the proposed similarity metrics are predictive of actual model performance rather than just statistical artifacts.

3. **Cross-Dataset Lexical Analysis:** Perform detailed analysis of vocabulary overlap, focusing on domain-specific terms. Calculate precision/recall of shared terminology between Mohler and SPRAG to quantify the extent of lexical transfer and identify potential failure points for non-contextual metrics.