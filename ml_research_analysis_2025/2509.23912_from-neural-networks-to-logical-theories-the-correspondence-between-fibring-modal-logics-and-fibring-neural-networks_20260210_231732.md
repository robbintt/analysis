---
ver: rpa2
title: 'From Neural Networks to Logical Theories: The Correspondence between Fibring
  Modal Logics and Fibring Neural Networks'
arxiv_id: '2509.23912'
source_url: https://arxiv.org/abs/2509.23912
tags:
- neural
- fibred
- fibring
- each
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a formal correspondence between fibring
  of neural networks and fibring of modal logics, two frameworks originally developed
  in different contexts. The authors redefine fibring of neural networks to generalize
  the original concept, allowing for any number and combination of neural networks.
---

# From Neural Networks to Logical Theories: The Correspondence between Fibring Modal Logics and Fibring Neural Networks

## Quick Facts
- arXiv ID: 2509.23912
- Source URL: https://arxiv.org/abs/2509.23912
- Reference count: 12
- Establishes formal correspondence between fibring of neural networks and fibring of modal logics

## Executive Summary
This paper establishes a formal correspondence between fibring of neural networks and fibring of modal logics, two frameworks originally developed in different contexts. The authors redefine fibring of neural networks to generalize the original concept, allowing for any number and combination of neural networks. They then introduce a fibred language based on a fibring architecture and define compatible fibred models that align with fibred neural networks and their inputs. By proving that the class of compatible fibred models is non-empty and closed under Kripke-model isomorphism, the authors demonstrate that their proposed fibred logic is a valid formalism. They further show that the formulas from this fibred logic exactly capture the outputs of fibred neural networks.

Leveraging this correspondence, the authors derive non-uniform logical expressiveness results for Graph Neural Networks (GNNs), Graph Attention Networks (GATs), and Transformer encoders. Specifically, they prove that for any instance of these architectures, there exists a fibring architecture and a family of fibring functions such that the corresponding fibred neural network reproduces the original network's behavior on all inputs. Consequently, the formulas in the associated fibred logic characterize the logical expressiveness of these architectures. This work opens the door to using fibring as a unifying formalism for studying the logical expressiveness of various neural architectures and provides a foundation for future research on uniform expressiveness, interpretability, and formal verification.

## Method Summary
The authors establish a correspondence between fibring of neural networks and fibring of modal logics by redefining the concept of fibring for neural networks to generalize the original framework. They introduce a fibred language based on a fibring architecture and define compatible fibred models that align with fibred neural networks and their inputs. By proving that the class of compatible fibred models is non-empty and closed under Kripke-model isomorphism, they demonstrate that their proposed fibred logic is a valid formalism. They then show that the formulas from this fibred logic exactly capture the outputs of fibred neural networks.

## Key Results
- Establishes a formal correspondence between fibring of neural networks and fibring of modal logics
- Proves that the class of compatible fibred models is non-empty and closed under Kripke-model isomorphism
- Demonstrates that fibred formulas capture the logical expressiveness of GNNs, GATs, and Transformer encoders

## Why This Works (Mechanism)
The correspondence works by creating a parallel between the compositional structure of fibred neural networks and the modular construction of fibred modal logics. By redefining fibring for neural networks, the authors enable any number and combination of neural networks to be composed in a structured way. This compositional structure is then mirrored in the fibred language, where the formulas can express the same relationships and dependencies as the neural network's computations. The compatible fibred models provide a semantic interpretation that aligns with the neural network's inputs and outputs, ensuring that the logical formalism accurately captures the network's behavior.

## Foundational Learning
- **Fibring of modal logics**: A framework for combining multiple modal logics into a single, unified logic. Why needed: Provides the theoretical foundation for the logical expressiveness results. Quick check: Can you explain how fibring allows for the combination of different modal logics?
- **Fibring of neural networks**: The redefined concept that generalizes the original framework to allow for any number and combination of neural networks. Why needed: Enables the compositional structure necessary for the correspondence with fibred modal logics. Quick check: Can you describe how the redefined fibring differs from the original concept?
- **Compatible fibred models**: Models that align with fibred neural networks and their inputs, providing a semantic interpretation for the fibred language. Why needed: Ensures that the logical formalism accurately captures the network's behavior. Quick check: Can you explain how compatible fibred models are constructed and why they are necessary?

## Architecture Onboarding
- **Component map**: Fibred neural networks -> Fibred modal logic -> Compatible fibred models -> Logical expressiveness results
- **Critical path**: Redefine fibring for neural networks -> Introduce fibred language -> Define compatible fibred models -> Prove correspondence -> Derive expressiveness results
- **Design tradeoffs**: The redefinition of fibring for neural networks extends beyond the original framework, which may affect generalizability to all neural architectures. The logical expressiveness results depend on finding appropriate fibring architectures and functions, which may not be straightforward in practice.
- **Failure signatures**: If the class of compatible fibred models is empty or not closed under Kripke-model isomorphism, the proposed fibred logic would not be a valid formalism. If the fibred formulas do not accurately capture the outputs of fibred neural networks, the correspondence would break down.
- **3 first experiments**:
  1. Verify the non-emptiness construction for compatible fibred models with increasingly complex neural architectures beyond the examples provided
  2. Test the practical feasibility of finding appropriate fibring architectures and functions for diverse neural network configurations
  3. Implement the fibred logic formalism to verify its expressiveness matches the target neural architectures on real-world datasets

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the countable family of fibred formulas characterizing GATs and Transformer encoders be collapsed into a single uniform logical formula?
- Basis in paper: [explicit] Section 6 states that while GNNs are known to collapse to Presburger formulas, "The problem remains open for GATs and Transformer encoders."
- Why unresolved: The paper establishes non-uniform expressiveness (families of formulas) but leaves the technical details of mapping these families to a single, input-independent formula (uniform expressiveness) for these architectures undefined.
- What evidence would resolve it: A constructive proof deriving a uniform logic, such as a specific fragment of temporal logic, that captures the full expressiveness of GATs or Transformer encoders.

### Open Question 2
- Question: Does the established correspondence between fibred neural networks and modal logics hold for GATs and Transformers utilizing soft (continuous) attention mechanisms?
- Basis in paper: [inferred] Section 5 notes that the technical results "consider GATs and Transformers with hard attention only," omitting the standard soft attention used in practice.
- Why unresolved: The current logical mapping relies on discrete state transitions (hardmax), whereas soft attention introduces continuous probability distributions, potentially breaking the direct correspondence to standard Kripke models.
- What evidence would resolve it: An extension of Theorem 5.1 and 5.2 that successfully incorporates continuous attention functions, or a formal proof of the limitations of the current fibred logic in this context.

### Open Question 3
- Question: Can the commonalities between fibred formulas for typical inputs be reverse-engineered to extract interpretable logical rules that capture a network's computations?
- Basis in paper: [explicit] Section 6 suggests that "scrutinizing the commonalities between fibred formulas... constitutes an exciting new take on the future possible extraction of interpretable logical rules."
- Why unresolved: The paper focuses on the theoretical correspondence and expressiveness; it does not implement or validate a method for analyzing these formulas to produce human-readable logical theories.
- What evidence would resolve it: An algorithm that analyzes the fibred formulas of a trained network to output a simplified logical theory that approximates the network's behavior on a dataset.

## Limitations
- The redefinition of fibring for neural networks extends beyond the original framework, which may affect generalizability to all neural architectures
- The non-emptiness proof for compatible fibred models relies on specific construction methods that may not scale to more complex architectures
- The logical expressiveness results depend on finding appropriate fibring architectures and functions, which may not be straightforward in practice

## Confidence
- High confidence: The formal correspondence between fibring of modal logics and redefined fibring of neural networks
- Medium confidence: The existence of compatible fibred models and their closure under Kripke-model isomorphism
- Medium confidence: The logical expressiveness characterization for specific architectures (GNNs, GATs, Transformers)

## Next Checks
1. Verify the non-emptiness construction for compatible fibred models with increasingly complex neural architectures beyond the examples provided
2. Test the practical feasibility of finding appropriate fibring architectures and functions for diverse neural network configurations
3. Conduct empirical validation by implementing the fibred logic formalism to verify its expressiveness matches the target neural architectures on real-world datasets