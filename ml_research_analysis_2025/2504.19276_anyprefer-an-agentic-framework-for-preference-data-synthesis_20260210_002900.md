---
ver: rpa2
title: 'Anyprefer: An Agentic Framework for Preference Data Synthesis'
arxiv_id: '2504.19276'
source_url: https://arxiv.org/abs/2504.19276
tags:
- data
- preference
- anyprefer
- arxiv
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Anyprefer, a framework that synthesizes high-quality
  preference data for aligning foundation models across diverse applications. It addresses
  the limitation of existing self-rewarding approaches that suffer from bias by introducing
  a cooperative two-player Markov game between a target model and a judge model, augmented
  with external tools for accurate evaluation.
---

# Anyprefer: An Agentic Framework for Preference Data Synthesis

## Quick Facts
- arXiv ID: 2504.19276
- Source URL: https://arxiv.org/abs/2504.19276
- Reference count: 36
- Introduces Anyprefer, a framework that synthesizes high-quality preference data for aligning foundation models across diverse applications.

## Executive Summary
Anyprefer introduces an agentic framework that synthesizes high-quality preference data for aligning foundation models across diverse applications. It addresses the limitation of existing self-rewarding approaches that suffer from bias by introducing a cooperative two-player Markov game between a target model and a judge model, augmented with external tools for accurate evaluation. A feedback mechanism optimizes both models' prompts to improve data quality. Experiments across 21 datasets covering natural language generation, vision-language understanding, medical image analysis, and visuo-motor control show significant performance gains, with average improvements of 18.55%, 3.66%, 30.05%, and 16.00% respectively.

## Method Summary
Anyprefer is an agentic framework that synthesizes preference data by treating the process as a cooperative two-player Markov game between a target model (that generates candidate responses) and a judge model (that evaluates them). The framework introduces external tools to assist the judge model in accurately rewarding responses, mitigating biases in the rewarding process. A feedback mechanism optimizes both models' prompts to improve data quality, and the process can be iterated to improve model performance over time.

## Key Results
- Significant performance gains across 21 datasets covering natural language generation, vision-language understanding, medical image analysis, and visuo-motor control
- Average improvements of 18.55%, 3.66%, 30.05%, and 16.00% respectively across the four application areas
- Strong self-improvement capabilities through iterative updates, with consistent score increases over 3 iterations across all applications

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Judging for Bias Mitigation
- **Claim:** Offloading fact-checking and grounding to external tools reduces the "self-rewarding" bias where a model reinforces its own hallucinations.
- **Mechanism:** The framework treats preference synthesis as a two-player Markov game. Instead of the Target Model ($\pi_t$) judging itself, a Judge Model ($\pi_j$) queries a set of external tools $\{M_i\}$ (e.g., Web Search, Grounded SAM). The Judge aggregates this external knowledge $q_i$ to score candidate responses, separating the generation of the answer from the verification of the answer.
- **Core assumption:** The external tools provide higher factual accuracy or grounding than the internal parametric knowledge of the Target Model.
- **Evidence anchors:** [abstract]: "...introducing external tools... to assist the judge model in accurately rewarding... mitigating biases in the rewarding process." [section 3.5]: "...tools and feedback mechanisms can greatly enhance the judge model’s evaluation accuracy... absolute improvement of approximately 22.4%."

### Mechanism 2: Prompt-Based Policy Optimization via Surrogate Rewards
- **Claim:** Optimizing the *prompts* of the Target and Judge models via gradient descent enables a self-correcting loop without retraining model weights.
- **Mechanism:** A Reward Model $R$ acts as a "surrogate" for expensive human evaluation. If a generated preference pair receives a low reward $R(y+, y-) < \tau$, the system uses the feedback to optimize the text prompts $p_t$ and $p_j$. This treats the prompts as policy parameters, steering the models toward higher-quality data generation.
- **Core assumption:** The textual gradient (feedback) effectively translates into prompt updates that improve the statistical quality of the output distribution.
- **Evidence anchors:** [section 2.4]: "...feedback R(y+, y−) can be viewed as the gradients passing through the models to update their parameters pt and pj." [section 3.6]: "...strong correlation between the surrogate reward scores and the direct evaluation results..."

### Mechanism 3: Iterative Capability Drift
- **Claim:** Repeated cycles of synthesis and fine-tuning allow the Target Model to improve on specialized domains (like medicine/robotics) where initial performance is low.
- **Mechanism:** The process is iterative: Synthesize Data $\rightarrow$ DPO Fine-tuning $\rightarrow$ Stronger Target Model. As the Target Model improves, it generates better candidates $y_c$, and the Judge Model receives better inputs, creating a positive feedback loop.
- **Core assumption:** The preference data quality is sufficient to induce a positive drift rather than model collapse.
- **Evidence anchors:** [section 3.4]: "...Anyprefer exhibits significant performance improvements in all tasks... through multiple iterative updates." [figure 3]: Shows consistent score increases over 3 iterations across 4 applications.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** This is the loss function used to actually align the Target Model once the synthetic data is generated. Understanding it is necessary to grasp *why* high-quality preference pairs $(y+, y-)$ are the primary output of the Anyprefer system.
  - **Quick check question:** How does DPO differ from standard Supervised Fine-Tuning (SFT) in terms of input data requirements?

- **Concept: Markov Games (Multi-Agent Reinforcement Learning)**
  - **Why needed here:** The paper frames the synthesis process as a cooperative game between two agents (Target and Judge). Understanding the state/observation space and shared goals helps in debugging why the agents might "collude" or fail to cooperate.
  - **Quick check question:** In a cooperative Markov game, what happens if one agent optimizes a local reward that conflicts with the global reward?

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The reliability of the entire Anyprefer framework hinges on the ability of the Reward Model $R$ and Judge $\pi_j$ to accurately assess quality. Biases in this evaluation (e.g., length bias, verbosity) directly corrupt the dataset.
  - **Quick check question:** What are the two most common failure modes when using an LLM to evaluate another LLM's output?

## Architecture Onboarding

- **Component map:** Input Prompt $x$ $\rightarrow$ Target Model (Generate $C$ candidates) $\rightarrow$ Toolbox (Retrieve knowledge $q_i$) $\rightarrow$ Judge Model (Rank candidates $\rightarrow$ select $y+, y-$) $\rightarrow$ Reward Model (Score pair) $\rightarrow$ Prompt Optimizer (Update prompts if Score < Threshold)

- **Critical path:** Input Prompt $x$ $\rightarrow$ **Target Model** (Generate $C$ candidates) $\rightarrow$ **Toolbox** (Retrieve knowledge $q_i$) $\rightarrow$ **Judge Model** (Rank candidates $\rightarrow$ select $y+, y-$) $\rightarrow$ **Reward Model** (Score pair). *If Score < Threshold:* Feedback $\rightarrow$ Prompt Optimizer $\rightarrow$ Retry. *If Score > Threshold:* Save to Anyprefer-V1.

- **Design tradeoffs:**
  - **Cost vs. Quality:** Using powerful models (GPT-4o) as judges/tools ensures high data quality but increases synthesis cost compared to self-rewarding.
  - **Surrogate vs. Real Evaluation:** Optimizing for the Reward Model's score is a proxy; it is faster than running full benchmarks every step but risks overfitting to the Reward Model's biases.

- **Failure signatures:**
  - **Stagnant Iterations:** Performance plateaus after Iteration 1 (suggests the Judge Model is not providing distinct enough ranking signals).
  - **Tool Latency/Timeouts:** The framework relies on sequential tool calls; failure here breaks the Markov game loop.
  - **Reward Hacking:** The Reward Model score goes to 10/10, but human eval shows the text is repetitive or verbose.

- **First 3 experiments:**
  1. **Validation of "Surrogate" Correlation:** Verify that the Reward Model $R$ used in your specific domain actually correlates with ground-truth task performance (replicate Section 3.6).
  2. **Ablation on Candidate Count ($C$):** The paper sets $C=5$. Test $C=3$ vs $C=10$ to see if diversity is constrained or if "noise" increases unnecessary compute load.
  3. **Tool Necessity Test:** Run the synthesis pipeline *without* tools (relying only on $\pi_j$'s internal weights) and compare the distinctness of the generated preference pairs (replicate Section 3.3).

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's reliance on external tools introduces a critical dependency on tool quality and availability, where inaccurate or outdated tool information could propagate errors into the synthetic preference data.
- The iterative capability drift mechanism assumes positive feedback loops, but the paper doesn't sufficiently address risks of model collapse or overfitting to synthetic data patterns.
- The prompt-based policy optimization through surrogate rewards shows promise, but the framework's resistance to reward hacking requires further validation.

## Confidence
- **High Confidence:** The basic architecture (Target Model → Toolbox → Judge Model → Reward Model → Prompt Optimizer) is clearly described and technically sound. The reported performance improvements across 21 datasets are statistically significant.
- **Medium Confidence:** The tool-augmented judging mechanism's effectiveness depends heavily on tool quality. While the paper demonstrates improvements, the sensitivity analysis for tool failures is limited.
- **Medium Confidence:** The prompt-based policy optimization through surrogate rewards shows promise, but the framework's resistance to reward hacking requires further validation.

## Next Checks
1. **Tool Failure Robustness Test:** Systematically degrade tool accuracy (e.g., inject controlled misinformation) and measure the resulting quality of synthetic preference data and downstream model performance. This validates whether the framework degrades gracefully or catastrophically when tools fail.

2. **Reward Model Generalization Stress Test:** Create edge case prompts designed to exploit known Reward Model biases (e.g., length preference, verbosity) and verify whether the framework generates preference pairs that optimize for the Reward Model score rather than actual quality.

3. **Cross-Domain Transfer Validation:** Take preference data synthesized for one domain (e.g., natural language generation) and apply it to fine-tune models in a structurally different domain (e.g., medical image analysis). This tests whether the synthetic data captures generalizable alignment signals or overfits to domain-specific patterns.