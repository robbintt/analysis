---
ver: rpa2
title: Incremental Learning with Repetition via Pseudo-Feature Projection
arxiv_id: '2502.19922'
source_url: https://arxiv.org/abs/2502.19922
tags:
- learning
- class
- repetition
- task
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new class-incremental learning scenario with
  class repetition, addressing limitations of traditional scenarios that assume strict
  no-repetition. The authors introduce a novel method called Horde that dynamically
  adjusts an ensemble of independent feature extractors and aligns them through pseudo-feature
  projection.
---

# Incremental Learning with Repetition via Pseudo-Feature Projection

## Quick Facts
- arXiv ID: 2502.19922
- Source URL: https://arxiv.org/abs/2502.19922
- Reference count: 40
- Primary result: Horde achieves 54.4% average accuracy in EFCIR-U (repetition) scenario vs 49.3% for next best method

## Executive Summary
This paper introduces Horde, a novel method for exemplar-free class-incremental learning (EFCIL) that addresses the emerging class-incremental learning with repetition (EFCIR) scenario. Horde dynamically manages an ensemble of frozen feature extractors that are aligned through pseudo-feature projection, enabling learning from class distributions without storing exemplars. The method achieves state-of-the-art performance in EFCIR scenarios while maintaining competitive results in traditional CIL settings.

## Method Summary
Horde employs a dynamic ensemble of frozen feature extractors (FEs) that are trained independently and never updated after freezing. When new classes cannot be adequately represented by the existing ensemble, a new FE is added using one of two heuristics: class set maximization or error rate thresholds. A unified classification head is trained using both current-task data and pseudo-features generated through projection, which simulates absent classes by translating features using class prototypes. This enables training on all encountered classes without storing exemplars, while the frozen ensemble provides zero-forgetting stability.

## Key Results
- Achieves 54.4% average accuracy in EFCIR-U scenario vs 49.3% for PASS
- Maintains competitive performance in traditional CIL (62.9% vs 61.2% for competitors)
- "Original features" estimation significantly outperforms zeros/random baselines (55.2% vs 30-48%)
- Weight regularization methods improve under repetition, but prototype-based methods degrade
- Horde achieves state-of-the-art performance in both traditional and repetition scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Ensembles of frozen feature extractors provide zero-forgetting stability while enabling representation expansion. Each FE is trained on a specific task and permanently frozen, eliminating catastrophic forgetting within that FE. New tasks trigger FE addition rather than modification, allowing the system to adapt without degrading previously learned representations.
- Core assumption: Frozen representations remain discriminative for their target classes throughout the incremental sequence, and new classes can be distinguished using combinations of existing and newly added feature spaces.
- Evidence: Frozen FEs avoid catastrophic drift in embedding space during task sequences; weak corpus support for frozen ensemble mechanisms specifically.

### Mechanism 2
- Pseudo-feature projection enables training a unified classifier on all encountered classes without storing exemplars. The method projects available samples through statistical transformation: ˆFc = μc + (f(xi;θ) - μyi)/σyi · σc, preserving relative position within class distribution while shifting to target class centroid.
- Core assumption: Class embeddings approximate multivariate Gaussian distributions, and translating features by adjusting mean and standard deviation produces valid pseudo-samples for classifier training.
- Evidence: Empirical validation shows original features estimation achieves 55.2% average accuracy vs 30.0% for zero-baseline; no direct corpus validation of specific pseudo-feature projection formula.

### Mechanism 3
- Class repetition in training data aligns ensemble feature extractors and improves pseudo-feature projection accuracy. When classes reappear, their prototype statistics can be computed directly rather than estimated. Multiple FEs learning representations for repeated classes creates overlapping coverage.
- Core assumption: Repeated classes appear frequently enough with sufficient distributional coverage to enable accurate prototype computation before catastrophic forgetting would occur in non-frozen systems.
- Evidence: Repetition aligns representations, enabling unified classification head to learn discrimination across diverse representation space; one related paper explicitly addresses class-incremental with repetition.

## Foundational Learning

- **Catastrophic forgetting and stability-plasticity dilemma**: Neural networks rapidly lose previously learned information when trained sequentially. This is essential to grasp why zero-forgetting FEs are necessary. Quick check: Can you explain why standard fine-tuning fails in incremental learning and what trade-off the stability-plasticity dilemma describes?

- **Class prototypes and exemplar-free incremental learning (EFCIL)**: Horde stores class statistics rather than raw samples to generate pseudo-features. Understanding how prototype-based rehearsal works without storing exemplars is critical for understanding the privacy-preserving aspect and pseudo-feature projection mechanism. Quick check: What statistical properties does a class prototype typically capture, and why does this avoid data privacy concerns compared to exemplar replay?

- **Knowledge distillation and feature translation**: Pseudo-feature projection builds on FeTrIL's feature translation concept. Understanding how features can be transformed between class distributions provides the foundation for understanding the extended formula incorporating standard deviation scaling. Quick check: Given feature translation formula ˆFc = f(xi;θ) + μc - μyi, what limitation does this have that pseudo-feature projection attempts to address?

## Architecture Onboarding

- **Component map**: Feature Extractor Ensemble -> Growth Heuristic Module -> Prototype Store -> Pseudo-Feature Projector -> Unified Classification Head
- **Critical path**: 1) Receive incremental task data 2) Compute growth heuristic; if triggered, train new FE, freeze, add/replace in ensemble 3) Extract/update prototypes for all classes present 4) For each training batch: compute CE loss on current classes, generate pseudo-features for absent classes, compute CE loss on pseudo-features 5) Backpropagate combined loss through unified head only (FEs remain frozen)
- **Design tradeoffs**: Larger ensemble budget allows more specialized FEs but increases memory and inference cost (~2x parameters of single-model distillation approaches). Hordem vs Hordec heuristics show similar performance. "Original features" estimation provides 10%+ accuracy gain over random baselines. Freezing classifier weights for absent classes prevents severe class-recency bias.
- **Failure signatures**: Rapid accuracy collapse indicates ensemble growth heuristic not triggering when needed. High forgetting despite frozen FEs suggests unified head overfitting to recent classes. Degraded performance in repetition scenarios for prototype-based methods is expected for PASS/PRAKA but not Horde. Class-recency bias diagonal in confusion matrix indicates incorrect gradient application.
- **First 3 experiments**: 1) Ablation on prototype estimation (zeros, random-k, original features) to verify original features provides 10%+ gain. 2) Ensemble budget sweep (B ∈ {1, 3, 5, 10, 15}) to identify optimal memory-accuracy trade-off. 3) Repetition frequency analysis with Beta distribution parameters (5%-50%) to confirm Horde maintains advantage across range.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does an imbalanced training data distribution combined with biased repetition frequency affect class-incremental learning performance?
- Basis: The authors state further investigation is needed to assess whether imbalanced training data distribution in conjunction with biased repetition frequency would increase difficulty.
- Why unresolved: EFCIR-B tested only biased repetition frequency while keeping intra-task sample distributions balanced. The interaction between class imbalance within tasks and uneven repetition across sequence remains unexplored.
- What evidence would resolve it: Experiments comparing accuracy and forgetting metrics under scenarios with (1) balanced repetition + balanced intra-task distribution, (2) biased repetition + balanced intra-task, (3) balanced repetition + imbalanced intra-task, and (4) both biases combined.

### Open Question 2
- Question: Why do prototype-based EFCIL methods (PASS, PRAKA, IL2A, SSRE) degrade under repetition while weight-regularization methods improve?
- Basis: The authors hypothesize that estimation of class prototypes with incomplete class data distribution in former methods leads to suboptimal feature embedding space, which is then propagated through incremental task sequence via knowledge distillation.
- Why unresolved: The paper observes performance gap but does not isolate whether degradation stems from prototype estimation error, knowledge distillation propagation, or interaction between the two mechanisms.
- What evidence would resolve it: Ablation studies that (a) measure prototype quality under partial class data, (b) compare prototype-based methods with and without distillation under repetition, and (c) track embedding space drift across tasks.

### Open Question 3
- Question: Is the assumption that complete training data for each class is available within a single task realistic for practical continual learning scenarios?
- Basis: The authors ask whether the assumption that complete training data distribution of an individual class is a realistic assumption for continual learning scenarios.
- Why unresolved: Traditional CIL benchmarks assume disjoint classes with full data per task, but real-world streams have partial exposures. The paper introduces repetition scenarios but does not systematically vary completeness of class distributions per encounter.
- What evidence would resolve it: Benchmarks that vary fraction of class data available per task (e.g., 10%, 25%, 50%, 100%) and measure how methods adapt when classes are learned incrementally through partial exposures.

### Open Question 4
- Question: What is the optimal heuristic for controlling ensemble growth in dynamic feature extractor methods like Horde?
- Basis: The paper proposes two heuristics (Hordem: Class Set Maximisation and Hordec: Task Error Rate) that achieve similar average accuracy but provides limited analysis of when each is preferable or whether alternatives exist.
- Why unresolved: Both heuristics use fixed thresholds without adaptive tuning. Trade-offs between ensemble diversity, computational cost, and task complexity remain unexplored.
- What evidence would resolve it: Systematic comparison across varying ensemble budgets, analysis of which heuristic triggers expansion more frequently on different task sequences, and evaluation of adaptive threshold strategies based on task properties.

## Limitations

- Pseudo-feature projection assumes Gaussian class distributions, which may fail for multimodal or heavy-tailed distributions
- Memory efficiency comes at cost of approximately 2x parameter count compared to single-model approaches
- Ensemble growth heuristics require specific thresholds that weren't fully specified, potentially affecting reproducibility

## Confidence

- **High confidence**: Horde's performance advantage in repetition scenarios (54.4% vs 49.3% for PASS) and the ablation showing original-features estimation outperforms random/zeros baselines (55.2% vs 30-48%)
- **Medium confidence**: The generalization of pseudo-feature projection formula to arbitrary class distributions, as corpus support for this specific mechanism is limited
- **Medium confidence**: The effectiveness of frozen ensemble approach across diverse domain shifts, as experiments were primarily on CIFAR-100

## Next Checks

1. **Prototype distribution validation**: Analyze class embedding distributions in the ensemble feature space to verify Gaussian assumptions and identify failure cases for pseudo-feature projection
2. **Budget sensitivity analysis**: Systematically vary ensemble size B from 1 to 15 on CIL 50/10 to identify optimal memory-accuracy trade-off and confirm diminishing returns around B=5-10
3. **Repetition frequency threshold**: Test Horde across synthetic repetition scenarios with Beta distribution parameters ranging from 5% to 50% class reappearance rates to identify where ensemble approaches begin outperforming weight regularization methods