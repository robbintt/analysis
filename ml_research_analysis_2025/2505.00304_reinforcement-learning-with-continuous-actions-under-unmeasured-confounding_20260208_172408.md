---
ver: rpa2
title: Reinforcement Learning with Continuous Actions Under Unmeasured Confounding
arxiv_id: '2505.00304'
source_url: https://arxiv.org/abs/2505.00304
tags:
- policy
- learning
- function
- optimal
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles offline policy optimization in reinforcement
  learning with continuous action spaces under unmeasured confounding, where standard
  approaches fail due to unobserved state variables affecting both actions and rewards.
  The authors extend proximal causal inference to infinite-horizon settings by leveraging
  reward-inducing proxy variables to identify policy value via V-bridge functions,
  overcoming the unobserved confounder issue.
---

# Reinforcement Learning with Continuous Actions Under Unmeasured Confounding

## Quick Facts
- arXiv ID: 2505.00304
- Source URL: https://arxiv.org/abs/2505.00304
- Reference count: 13
- This paper extends proximal causal inference to offline RL with continuous actions under unmeasured confounding using proxy variables and V-bridge functions.

## Executive Summary
This paper addresses offline policy optimization in reinforcement learning with continuous action spaces when unobserved state variables confound both actions and rewards. The authors develop a framework that uses reward-inducing proxy variables to identify policy value through V-bridge functions, overcoming the limitations of standard RL approaches that fail under unmeasured confounding. They propose a minimax estimator with kernel-based critic function approximation and a policy-gradient algorithm to find the optimal policy within a parameterized class. The method demonstrates superior performance compared to MDP-based and state-of-the-art RL baselines in both synthetic and real-world Pairfam datasets.

## Method Summary
The method combines proximal causal inference with RL to handle unmeasured confounding in offline settings. It uses reward-inducing proxy variables (W_t) that correlate with unobserved states (S_t) but are conditionally independent of actions (A_t). The core is a minimax estimator that identifies policy value via Q-bridge functions in RKHS, with the critic function enabling efficient optimization. A policy-gradient algorithm then searches for the optimal policy by accounting for the dependency between Q-bridge parameters and policy parameters. The framework supports both linear and neural network policy parameterizations, with the linear version yielding interpretable results for relationship satisfaction optimization.

## Key Results
- Outperforms MDP-based and state-of-the-art RL baselines in both synthetic and real-world Pairfam datasets
- Theoretical guarantees include consistency of the estimator and finite-sample error bounds of O(n^{-1/2})
- Linear policy class yields interpretable results for relationship satisfaction optimization while maintaining competitive performance
- Method shows robust performance across different policy parameterizations (linear and neural networks)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unmeasured confounders can be addressed using proxy variables to identify policy values nonparametrically.
- **Mechanism:** Reward-inducing proxy variables (W_t) correlate with unobserved state (S_t) but are conditionally independent of action (A_t). By leveraging these proxies and treating previous state-action pairs as action-inducing proxies, the method satisfies completeness conditions allowing policy value identification without observing full state.
- **Core assumption:** Assumption 4 (Completeness) requires proxies contain sufficient information about the unobserved confounder to render confounding effect identifiable.
- **Evidence anchors:** Abstract states "leveraging reward-inducing proxy variables to identify policy value via V-bridge functions"; Section 3.1 discusses completeness assumption.
- **Break condition:** Mechanism fails if chosen proxies lack sufficient correlation with unmeasured confounder or directly affect the action.

### Mechanism 2
- **Claim:** Minimax estimator with kernel-based critics can reliably estimate Q-bridge function for continuous actions.
- **Mechanism:** Estimation framed as minimax optimization where Q-function class is minimized against critic function class. Restricting critic to RKHS provides closed-form solution, converting problem to tractable single-stage minimization for continuous action spaces.
- **Core assumption:** Assumption 7 (Well-posedness) assumes critic function class is sufficiently rich to approximate Bellman error.
- **Evidence anchors:** Abstract mentions "develop a minimax estimator with kernel-based critic function approximation"; Section 3.2 explains Theorem 3.3 transformation.
- **Break condition:** Fails if kernel bandwidth or regularization parameters are misspecified, leading to overfitting or underfitting Bellman error.

### Mechanism 3
- **Claim:** Policy-gradient algorithm can find optimal in-class policy by maximizing estimated policy value derived from Q-bridge.
- **Mechanism:** Algorithm iteratively updates policy parameters by calculating gradient of estimated policy value. Crucially accounts for dependency of Q-bridge parameters on policy parameters, avoiding bias from treating Q-bridge as fixed.
- **Core assumption:** Assumption 5 requires policy class to be compact and Lipschitz continuous for stable optimization.
- **Evidence anchors:** Abstract states "introduce a policy-gradient-based algorithm to identify the in-class optimal policy"; Section 3.3 discusses gradient ascent with policy parameter dependency.
- **Break condition:** Fails if policy class is too restrictive or gradient updates are unstable due to high variance in Q-bridge estimation.

## Foundational Learning

- **Concept: Proximal Causal Inference**
  - **Why needed here:** Standard RL assumes no unmeasured confounders. Understanding how "negative control" or "proxy" variables allow causal identification when "no unmeasured confounding" assumption is violated.
  - **Quick check question:** Can you explain why standard inverse propensity scoring fails if behavior policy depends on unobserved variable?

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - **Why needed here:** Paper uses "kernel trick" to solve minimax optimization efficiently. Understanding RKHS is necessary to implement critic function and understand closed-form solution in Theorem 3.3.
  - **Quick check question:** How does representing function in RKHS allow computing inner products implicitly in high-dimensional space?

- **Concept: Minimax Estimation**
  - **Why needed here:** Core estimator involves "game" between Q-function estimator and discriminator (critic). Need to grasp why minimizing maximum loss helps satisfy conditional moment restrictions.
  - **Quick check question:** In objective min_q max_f E[L(q,f)], what is role of function class f?

## Architecture Onboarding

- **Component map:** Input dataset -> Minimax Optimizer (kernel-based) -> Policy Evaluator -> Policy Optimizer (gradient ascent)
- **Critical path:** Definition of valid proxy variables and implementation of RKHS critic are bottlenecks. If proxies are not valid, entire causal chain breaks.
- **Design tradeoffs:**
  - Linear vs. Neural Policy: Linear policies are interpretable but may capture less complex dynamics
  - Kernel vs. Neural Critic: Theory relies on RKHS for tractability, whereas practical deep RL often uses neural networks
- **Failure signatures:**
  - High Bias: Policy value estimates significantly lower than true value; check if proxy weakly correlated with unobserved state
  - Divergence: Loss explodes; check regularization parameters
  - Corpus signals: Related work warns of failure in high-risk tasks if action distributions are unimodal
- **First 3 experiments:**
  1. Synthetic Validation: Replicate "Env I" setup where ground truth S_t is known; verify estimated value tracks true value as sample size increases
  2. Ablation on Proxies: Run algorithm on Pairfam dataset removing proxy variables to confirm performance degrades to "MDP" baseline levels
  3. Policy Class Comparison: Compare Linear-Beta vs. NN-Gaussian policies on real dataset to quantify trade-off between interpretability and performance

## Open Questions the Paper Calls Out

- **Open Question 1:** How can offline policy evaluation and learning be adapted to handle partial data coverage within confounded POMDP framework?
  - **Basis:** Authors state "we assumed no coverage issue for batch dataset... properly addressing data coverage issue would be interesting topic"
  - **Why unresolved:** Current theoretical guarantees rely on dataset sufficiently covering state-action space, often violated in data-scarce fields
  - **What evidence would resolve it:** New error bounds or estimators that remain consistent under weak coverage assumptions

- **Open Question 2:** How can variable selection be systematically performed in offline RL setting to improve policy generalization?
  - **Basis:** Discussion notes "conducting variable selection in offline RL setting remains challenging challenge"
  - **Why unresolved:** No ground truth available for performance comparison in offline settings, making it difficult to validate which observed state variables are essential
  - **What evidence would resolve it:** Theoretical framework or algorithm capable of identifying relevant state variables without requiring online environment interaction

- **Open Question 3:** Can computational efficiency be improved by directly identifying policy gradient in continuous-action POMDPs?
  - **Basis:** Authors mention algorithm requires "relatively large computation and memory resources" and suggest identifying policy gradient directly
  - **Why unresolved:** Current minimax estimator requires repeated evaluation of Q-bridge function, and identification of policy gradient for continuous actions in this specific POMDP setting remains open challenge
  - **What evidence would resolve it:** Development of direct policy gradient identification theorem and corresponding algorithm avoiding computational overhead

## Limitations
- The completeness assumption (Assumption 4) is unverifiable from observational data and its violation would break the causal identification chain
- Theoretical guarantees rely on RKHS representations that may not hold for neural network critics
- Pairfam dataset's proxy validity is assumed but not experimentally verified through sensitivity analysis

## Confidence
- **High confidence:** Minimax estimator with kernel critics correctly implements theoretical framework
- **Medium confidence:** Policy gradient algorithm converges to in-class optimal policy given theoretical regret bound
- **Low confidence:** Linear policy interpretability extends beyond specific Pairfam setting to general confounded RL problems

## Next Checks
1. **Sensitivity analysis on proxy strength:** Systematically weaken correlation between W_t and S_t in synthetic environments to quantify performance degradation and identify threshold where completeness assumption breaks
2. **Ground truth confounding test:** Implement modified synthetic environment where S_t is observable; compare learned policies against confounded and unconfounded baselines to isolate confounding effect
3. **Finite-sample validation:** Track convergence rate of estimated policy value to true value across different sample sizes to empirically verify O(n^{-1/2}) error bound