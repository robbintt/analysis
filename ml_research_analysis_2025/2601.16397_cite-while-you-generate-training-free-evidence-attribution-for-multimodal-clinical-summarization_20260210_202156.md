---
ver: rpa2
title: 'Cite-While-You-Generate: Training-Free Evidence Attribution for Multimodal
  Clinical Summarization'
arxiv_id: '2601.16397'
source_url: https://arxiv.org/abs/2601.16397
tags:
- attribution
- image
- source
- text
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free, attention-guided framework
  for generating fine-grained citations in multimodal clinical summarization. The
  method leverages decoder attention distributions to attribute each generated sentence
  to supporting text spans and/or images during inference, avoiding post-hoc or retraining-based
  approaches.
---

# Cite-While-You-Generate: Training-Free Evidence Attribution for Multimodal Clinical Summarization

## Quick Facts
- **arXiv ID:** 2601.16397
- **Source URL:** https://arxiv.org/abs/2601.16397
- **Reference count:** 40
- **Primary result:** Training-free decoder attention-based framework achieves up to +15% F1 gain in fine-grained clinical summarization citations, with caption-based mode offering lightweight multimodal attribution.

## Executive Summary
This paper introduces a training-free, attention-guided framework for generating fine-grained citations in multimodal clinical summarization. The method leverages decoder attention distributions to attribute each generated sentence to supporting text spans and/or images during inference, avoiding post-hoc or retraining-based approaches. Two modes are proposed: raw image mode, which directly uses image patch attentions, and caption-as-span mode, which substitutes images with generated captions for purely text-based alignment. Evaluations on clinician-patient dialogues and radiology reports show consistent improvements over embedding-based and self-attribution baselines, with up to +15% F1 gain in text-level attribution and strong multimodal citation accuracy. Caption-based attribution achieves competitive performance with raw-image attention while being more lightweight and practical. These results highlight attention-guided attribution as a promising step toward transparent and trustworthy clinical summarization systems.

## Method Summary
The method extracts decoder cross-attention weights during summary generation to attribute output tokens to source text spans or images. Source text is chunked into sentences; images are handled either as raw patch attentions (IMG_RAW) or replaced by generated captions (IMG_CAP). For each generated token, the top-k most-attended source tokens are selected, mapped to sentence IDs, and assigned via majority vote. Sentence-level attributions are aggregated and filtered by threshold τ. The approach is training-free and model-agnostic, evaluated on CliConSummation (dialogues) and MIMIC-CXR (radiology reports).

## Key Results
- Attention-guided attribution achieves +15% F1 gain in text-level citations vs. embedding-based baselines.
- IMG_CAP mode reaches 34.87 joint exact match (vs. 26.46 for IMG_RAW) while reducing memory usage.
- Macro-F1 peaks at τ=0.15–0.16 (76.4 F1) and Exact Match at τ=0.16 (58.70), confirming optimal threshold range.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Majority voting over top-k attended tokens produces stable sentence-level citations from noisy token-level attention distributions.
- **Mechanism:** For each generated token, collect top-k most-attended source tokens, map them to sentence IDs, and assign the token to the majority-vote sentence. Aggregate across all tokens in a summary sentence, then apply threshold τ to retain only sources with consistent support.
- **Core assumption:** Decoder attention weights encode meaningful source-to-output alignments that survive averaging across layers and heads, rather than being pure noise.
- **Evidence anchors:**
  - [abstract]: "leverages decoder attentions to directly cite supporting text spans or images"
  - [section 5.2.1]: "max collapses to ∼12 Macro-F1... majority consistently yields 60+ point gains in F1"
  - [corpus]: Weak direct validation; neighbor papers focus on image watermarking and retrieval, not attention-based text attribution.
- **Break condition:** If attention patterns become decorrelated from semantic grounding (e.g., in layers with strong positional bias or shallow fusion), majority voting will amplify noise rather than signal.

### Mechanism 2
- **Claim:** Caption-based image attribution (IMG_CAP) achieves competitive joint grounding accuracy while avoiding direct patch-level attention computation.
- **Mechanism:** Replace each image with a model-generated one-sentence caption during preprocessing. The caption is treated as a text span in the source sequence. Standard text-to-text attention alignment then attributes summary sentences to the caption span, which is interpreted as an image citation.
- **Core assumption:** Generated captions capture sufficient visual semantics to serve as proxies for the original image evidence.
- **Evidence anchors:**
  - [abstract]: "caption-as-span mode, which substitutes images with generated captions to enable purely text-based alignment"
  - [section 5.1.2]: IMG_CAP achieves 34.87 joint exact match vs. 26.46 for IMG_RAW (Qwen2.5-VL), with 77.85% image accuracy.
  - [corpus]: No direct validation; Visual-RAG and multimodal retrieval papers focus on retrieval augmentation, not citation generation.
- **Break condition:** If captions hallucinate or omit critical visual details (e.g., subtle lesion features), text-based alignment will cite incorrect or incomplete evidence.

### Mechanism 3
- **Claim:** Threshold τ controls precision-recall tradeoff in citation filtering; optimal range (τ ≈ 0.15–0.18) balances spurious matches against missing sparse evidence.
- **Mechanism:** After aggregating token-level attributions into sentence-level counts, require that a source sentence be supported by at least τ × |T_j| tokens in the generated sentence. This filters noisy alignments while preserving consistent evidence.
- **Core assumption:** True evidence spans receive distributed attention across multiple output tokens; spurious alignments are token-isolated.
- **Evidence anchors:**
  - [section 3.2]: "A(j) = {i: (1/|T_j|) Σ_{t∈T_j} I[ŝ(t)=i] ≥ τ}"
  - [section 5.2.1]: "Macro-F1 reaches its maximum at τ=0.15–0.16 (76.4 F1), while Exact Match peaks at τ=0.16 (58.70)"
  - [corpus]: No external validation of threshold sensitivity in citation tasks.
- **Break condition:** If a summary sentence paraphrases a single source sentence using few tokens, high τ will incorrectly discard the citation.

## Foundational Learning

- **Concept:** Transformer cross-attention mechanisms
  - **Why needed here:** The entire framework depends on extracting and interpreting decoder-to-encoder attention weights. Without understanding how cross-attention aligns output tokens to input tokens, the aggregation logic is opaque.
  - **Quick check question:** Given a decoder layer with 32 heads, what does averaging attention weights across heads assume about head specialization?

- **Concept:** Token-to-sentence mapping in structured documents
  - **Why needed here:** Citations operate at sentence granularity, but attention is token-level. The chunking stage must correctly map tokens to their containing sentences, accounting for image token blocks that shift indices.
  - **Quick check question:** If an image is inserted at token position 50 in a 200-token source, how do you adjust sentence boundary indices that originally spanned tokens 40–60?

- **Concept:** Evaluation metrics for attribution (F1, exact match, joint accuracy)
  - **Why needed here:** The paper reports +15% F1 gains, but interpreting these requires understanding what macro-F1 measures vs. exact match, and why joint EM is the hardest metric.
  - **Quick check question:** If a model correctly cites 3 of 4 source sentences for a summary sentence, what is the impact on F1 vs. exact match?

## Architecture Onboarding

- **Component map:** Input processor -> Attention extractor -> Attention pooler -> Token attributor -> Sentence aggregator -> Citation formatter
- **Critical path:** Generation -> attention extraction -> pooling -> majority vote -> threshold aggregation -> citation output. Any failure in attention extraction (e.g., memory limits on long sequences) breaks the pipeline.
- **Design tradeoffs:**
  - **IMG_RAW vs. IMG_CAP:** Raw patch attention provides higher text F1 (65.52 vs. 58.37) but lower joint EM (26.46 vs. 34.87). Caption mode is memory-efficient and privacy-preserving but depends on caption quality.
  - **Top-k size:** k=3 optimal for Qwen2.5-VL; larger k dilutes F1. Different models may require retuning.
  - **Threshold τ:** Lower τ (0.10) increases recall but admits spurious citations; higher τ (0.25+) improves precision but misses valid sparse evidence.
- **Failure signatures:**
  - **Near-random F1 (~12):** Indicates "max" mode instead of majority voting (confirmed in ablation).
  - **High text F1 but near-zero image accuracy:** Suggests image token block not correctly registered in attention pooling.
  - **Citations always empty:** Threshold τ too high or attention weights not being captured (check `attn="eager"` is enabled).
- **First 3 experiments:**
  1. **Reproduce text-only attribution:** Run pipeline on MIMIC-CXR subset with k=3, τ=0.16. Verify macro-F1 ≈ 76 against embedding baseline (target: +4 F1).
  2. **Ablate aggregation mode:** Compare "max" vs. "majority" on a held-out set. Confirm majority recovers 60+ F1 points.
  3. **Test IMG_CAP fallback:** Replace raw images with captions on multimodal CliConSummation samples. Measure joint EM gap vs. IMG_RAW (target: maintain within 5–10 points while reducing memory by ~40%).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can attention-guided attribution generalize to clinical domains beyond dialogues and radiology reports (e.g., pathology reports, discharge summaries, or multi-document patient histories)?
- **Basis in paper:** [explicit] "Future work will extend to larger clinical corpora, explore domain-tuned models, and investigate real-world deployment in clinical workflows."
- **Why unresolved:** Evaluation is limited to two datasets (CliConSummation and MIMIC-CXR), both with relatively structured formats; performance on longer or less structured clinical documents remains unknown.
- **What evidence would resolve it:** Evaluation on additional clinical benchmarks such as MIMIC-III discharge summaries, pathology reports, or longitudinal patient records, with consistent attribution quality metrics.

### Open Question 2
- **Question:** Does training-free attribution accuracy improve when applied to domain-tuned clinical models versus general-purpose MLLMs?
- **Basis in paper:** [inferred] The paper evaluates Qwen2.5-VL and LLaVA-NeXT, both general-purpose models; the discussion mentions "explore domain-tuned models" as future work, implying this is untested.
- **Why unresolved:** Domain-specific models may have different attention distributions due to fine-tuning, potentially affecting whether majority-voting aggregation remains optimal.
- **What evidence would resolve it:** Comparison of attribution performance before and after domain adaptation (e.g., clinical instruction tuning) on the same datasets, with analysis of attention distribution shifts.

### Open Question 3
- **Question:** How reliable are LLM-generated silver reference attributions compared to human expert annotations?
- **Basis in paper:** [inferred] "Since gold-standard citations are unavailable, we construct a reference set using an LLM judge following Xie et al. (2024)"—the paper acknowledges this limitation without quantifying annotation noise.
- **Why unresolved:** Attribution metrics (e.g., +15% F1 over baselines) are computed against silver references; if LLM judges have systematic biases, reported gains may be inflated or misestimated.
- **What evidence would resolve it:** Human annotation study on a subset of generated summaries with expert clinicians providing ground-truth citations, followed by correlation analysis between silver and gold labels.

### Open Question 4
- **Question:** Under what conditions does caption-based attribution (IMG_CAP) outperform raw-image attention (IMG_RAW) in clinical settings with privacy or bandwidth constraints?
- **Basis in paper:** [explicit] "Caption-based attribution emerges as a lightweight yet effective alternative...particularly useful in scenarios where images cannot be shared or processed directly."
- **Why unresolved:** The trade-off between fidelity (IMG_RAW achieves higher text F1) and practicality (IMG_CAP achieves higher joint EM) is documented, but optimal selection criteria for deployment contexts remain undefined.
- **What evidence would resolve it:** Systematic analysis across varying image resolutions, caption quality levels, and simulated network constraints, measuring both attribution quality and computational/privacy costs.

## Limitations
- **Attention quality dependence:** Attribution quality relies on decoder cross-attention reflecting semantic alignment; in complex models, attention may be dominated by positional patterns or residual connections, making token-level alignments unreliable.
- **Caption proxy fidelity:** IMG_CAP substitutes images with generated captions, but caption quality and completeness are not directly validated; omitted visual details (e.g., subtle lesions) lead to systematically incomplete citations.
- **Threshold tuning brittleness:** Optimal τ range (0.15–0.18) is empirically derived per model and dataset with no theoretical justification or adaptive tuning mechanism; fixed thresholds may over-filter or admit spurious citations in different domains.

## Confidence
- **High confidence:** The core attention aggregation and majority voting mechanism works as described and improves over random baselines. The ablation showing max assignment collapsing to ~12 F1 is internally consistent and well-documented.
- **Medium confidence:** Text-only attribution (CS mode) is reliable and improves over embedding baselines by ~4 F1 points. The claim is supported by ablation and threshold sweeps, but limited by lack of external replication.
- **Low confidence:** Multimodal attribution (IMG_RAW and IMG_CAP) depends on fragile assumptions about caption quality and attention interpretability. The strong performance of IMG_CAP is promising but not independently verified against ground-truth image citations.

## Next Checks
1. **Cross-model threshold sensitivity:** Sweep τ on MIMIC-CXR using both Qwen2.5-VL and LLaVA-NeXT. Plot F1/EM curves to confirm peak stability and identify if fixed thresholds generalize across models.
2. **Caption quality audit:** For a stratified sample of CliConSummation multimodal examples, generate IMG_CAP citations and compare against IMG_RAW. Conduct a blinded human evaluation (2 clinicians) to rate whether captions retain sufficient visual detail for accurate citation.
3. **Attention layer ablation:** Modify the pooling stage to retain only specific decoder layers (e.g., last 2 vs. all). Measure F1 degradation to quantify how much attribution quality depends on lower vs. higher layers, testing the assumption that averaged attention is meaningful.