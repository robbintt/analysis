---
ver: rpa2
title: Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language
  Representations of Text-to-SQL System Outputs
arxiv_id: '2510.23854'
source_url: https://arxiv.org/abs/2510.23854
tags:
- nlrs
- evaluation
- uqdb
- llms
- combo-eval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the evaluation of natural language representations
  (NLRs) generated from database query results in Text-to-SQL systems. The proposed
  Combo-Eval method combines traditional metrics with LLM-as-a-judge evaluation, reducing
  LLM calls by 25-61% while maintaining high fidelity.
---

# Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs

## Quick Facts
- arXiv ID: 2510.23854
- Source URL: https://arxiv.org/abs/2510.23854
- Reference count: 40
- Primary result: Combo-Eval reduces LLM calls by 25-61% while maintaining high fidelity in evaluating natural language representations of SQL result sets

## Executive Summary
This paper addresses the critical challenge of evaluating natural language representations (NLRs) generated from database query results in Text-to-SQL systems. The proposed Combo-Eval method combines traditional metrics with LLM-as-a-judge evaluation, achieving significant cost reductions while maintaining high alignment with human judgments. The framework introduces NLR-BIRD, a benchmark dataset spanning 11 domains, and demonstrates that a threshold-based metric pre-filtering approach can effectively route easy cases away from expensive LLM evaluation, with the remaining ambiguous samples evaluated by LLMs for semantic reasoning.

## Method Summary
The Combo-Eval framework evaluates NLRs by first computing ROUGE-1 recall against reference outputs, then applying calibrated threshold bands to classify samples into direct categories (class_0 for incorrect, class_1 for correct) or route them to LLM-as-judge evaluation. The method establishes extreme threshold bands where metric scores provide clear separation between correct and incorrect NLRs, reducing LLM calls by 25-61%. When samples fall within ambiguous metric ranges, an LLM judge evaluates them using comparison prompts against either ground-truth NLRs or user-question-derived references (UQDB). The approach is particularly effective with smaller judge models, offering cost-efficient evaluation without compromising accuracy.

## Key Results
- Combo-Eval reduces LLM calls by 25-61% compared to pure LLM evaluation while maintaining superior alignment with human judgments
- NLR generation quality degrades significantly with larger result sets, with incomplete information being the primary error source (accuracy drops from 80-91% for size 3-9 to 30-59% for size 50-499)
- Smaller judge models exhibit more prominent improvement with Combo-Eval compared to larger models, though overall accuracy remains lower

## Why This Works (Mechanism)

### Mechanism 1: Threshold-Based Metric Pre-Filtering
- Claim: ROUGE-1 recall at extreme threshold values provides statistically meaningful separation between correct and incorrect NLRs, enabling classification without LLM inference for many samples.
- Mechanism: The method establishes upper and lower threshold bands for both classes. Samples falling within confident ROUGE score ranges (e.g., ROUGE-1 recall ≥0.9 for class 1, ≤0.1 for class 0) receive direct classification; only ambiguous samples proceed to LLM evaluation.
- Core assumption: Tabular data values (numbers, proper nouns) are typically rendered literally in correct NLRs, making lexical overlap a stronger signal than in general text generation.

### Mechanism 2: Cascade Decision with Metric-to-LLM Handoff
- Claim: A two-stage cascade where metrics filter "easy" cases before LLM evaluation improves cost-accuracy tradeoffs, particularly for smaller judge models.
- Mechanism: Compute ROUGE-1 recall → check against threshold bands → classify directly if within band, else invoke LLM-judge with comparison prompt. The LLM only evaluates samples in the "pending" zone where metrics provide insufficient signal.
- Core assumption: The ambiguous middle-range of metric scores contains cases requiring semantic reasoning (faithfulness, completeness) that n-gram metrics cannot capture.

### Mechanism 3: Size-Dependent Completeness Degradation
- Claim: NLR generation and evaluation accuracy both degrade with increasing result-set size, with "incomplete information" as the dominant error mode.
- Mechanism: Larger tables require tracking more data points across longer generation sequences. LLMs omit rows, skip null values, or truncate outputs, and judges similarly struggle to verify completeness at scale.
- Core assumption: Inherent attention/context limitations manifest as completeness failures rather than hallucination when tasks require faithful reproduction of tabular content.

## Foundational Learning

- **Concept: Natural Language Representation (NLR)**
  - Why needed here: NLR is the target artifact being evaluated—prose generated from SQL result tables. Unlike Table QA (extracting answers from tables), NLR requires narrating the complete result-set.
  - Quick check question: Given a SQL result with 10 rows of (name, salary), can you distinguish between a correct NLR listing all 10 vs. one listing only 3?

- **Concept: LLM-as-a-Judge Paradigm**
  - Why needed here: Combo-Eval builds on the premise that LLMs can evaluate other LLMs' outputs, but adds metric-based pre-filtering for efficiency.
  - Quick check question: What are two failure modes of LLM judges on large tables according to this paper?

- **Concept: Evaluation Scenarios (GT vs UQDB)**
  - Why needed here: Real-world systems often lack ground-truth NLRs. UQDB (User Question + DB result) provides an alternative reference when GT is unavailable.
  - Quick check question: Which scenario achieves higher alignment with human judgment, and by approximately what margin?

## Architecture Onboarding

- **Component map**:
  - NLR Generator: LLM that takes (question, table) → natural language response
  - Metrics Layer: ROUGE-1 recall computation against reference (GT or UQDB)
  - Threshold Router: Classifies samples into {class_0, class_1, pending} based on calibrated bands
  - LLM Judge: Evaluates pending samples via comparison prompt
  - NLR-BIRD Dataset: 1,468 samples across 11 domains with human-annotated ground truth

- **Critical path**:
  1. Execute SQL query → produce result-set table
  2. Generate NLR using target LLM
  3. Compute ROUGE-1 recall against reference
  4. Apply threshold bands (GT: 0-0.1→class_0, 0.9-1.0→class_1; UQDB: 0.05-0.1→class_0, 0.87-1.0→class_1)
  5. Escalate pending samples to LLM-judge

- **Design tradeoffs**:
  - Higher threshold bands → fewer LLM calls but more samples in ambiguous zone
  - Smaller judge models → greater Combo-Eval improvement but lower absolute accuracy
  - UQDB vs GT: UQDB eliminates annotation cost but reduces accuracy ~7% (LLM-judge) to ~6.7% (Combo-Eval)

- **Failure signatures**:
  - High false-negative rate on large result-sets (rc+cc≥50): consider chunking or summarization
  - Formatting inconsistencies in generated NLRs: check for spacing/newline issues, especially with GPT-4o
  - Low class-0 recall: threshold calibration may be too conservative; adjust lower bounds

- **First 3 experiments**:
  1. **Baseline calibration**: Run metrics-only evaluation on dev set, plot ROUGE-1 recall distributions for class 0 vs class 1 to verify separation at extremes.
  2. **Threshold sensitivity**: Test multiple threshold band configurations (tight vs loose) measuring macro F1 and LLM call reduction rate on held-out test set.
  3. **Judge model comparison**: Compare GPT-4o vs GPT-4o-mini vs Llama-3.1-70B as judge under Combo-Eval, measuring both final accuracy and cost savings.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can hybrid summarization or rule-based strategies effectively mitigate the performance degradation of NLR generation and evaluation observed in large database result sets (rc+cc >= 500)?
- **Open Question 2**: How does the Combo-Eval framework perform on reasoning-aware table-to-text tasks compared to the standard query-result narration tasks presented in NLR-BIRD?
- **Open Question 3**: To what extent can specialized LLM training strategies or data augmentation reduce the "incomplete information" error rate, which is currently the primary source of incorrect NLRs?
- **Open Question 4**: Does the alignment of Combo-Eval with human judgment remain robust across highly specialized industrial domains not fully represented in the current 11-domain dataset?

## Limitations

- NLR-BIRD dataset contains only 1,468 samples from 11 domains, limiting generalizability across diverse industrial applications
- Performance degrades significantly for larger result sets (rc+cc≥50), with incomplete information as the dominant error mode
- The framework excludes result-sets with rc+cc≥500 entirely, leaving a significant gap in evaluation coverage for real-world database queries

## Confidence

**High confidence**: The cost reduction claims (25-61% fewer LLM calls) are well-supported by the threshold-based filtering mechanism and clearly demonstrated through experimental results. The degradation pattern with increasing result-set size is consistently observed across multiple metrics and human evaluations.

**Medium confidence**: The superiority of Combo-Eval over standalone metrics or LLM evaluation holds within the tested domains and sample sizes, but the ~7% accuracy gap between GT and UQDB scenarios suggests sensitivity to reference quality that may affect generalization.

**Low confidence**: The universal applicability of calibrated threshold bands across different generation LLMs and domains requires further validation, as the current calibration was performed on a specific subset of the BIRD dataset.

## Next Checks

1. **Threshold robustness test**: Validate threshold band calibration across diverse generation LLMs beyond the four tested (GPT-4o, GPT-3.5-turbo, Llama-3.1-70B, Qwen2.5-72B-Chat) using datasets from different domains to assess generalizability.

2. **Large result-set handling evaluation**: Systematically test NLR generation and evaluation for rc+cc in the 500-1000 range using chunking or summarization strategies to determine if the complete exclusion of large result-sets is necessary or if hybrid approaches could extend coverage.

3. **Cross-lingual and non-English domain validation**: Apply Combo-Eval to NLR generation in non-English domains (particularly relevant given the AraTable reference) to assess whether the ROUGE-1 recall threshold mechanism generalizes across languages with different morphological and syntactic structures.