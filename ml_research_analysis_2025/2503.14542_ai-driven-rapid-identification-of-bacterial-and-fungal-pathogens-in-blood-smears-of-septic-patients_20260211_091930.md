---
ver: rpa2
title: AI-Driven Rapid Identification of Bacterial and Fungal Pathogens in Blood Smears
  of Septic Patients
arxiv_id: '2503.14542'
source_url: https://arxiv.org/abs/2503.14542
tags:
- species
- bacteria
- were
- images
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed deep learning models to identify 14 bacterial
  and 3 fungal species from Gram-stained blood smear images in sepsis patients. Using
  16,637 microscopic images, the pipeline combined Cellpose 3 for segmentation and
  Attention-based Deep Multiple Instance Learning for classification.
---

# AI-Driven Rapid Identification of Bacterial and Fungal Pathogens in Blood Smears of Septic Patients

## Quick Facts
- arXiv ID: 2503.14542
- Source URL: https://arxiv.org/abs/2503.14542
- Reference count: 30
- This study developed deep learning models to identify 14 bacterial and 3 fungal species from Gram-stained blood smear images in sepsis patients, achieving 77.15% accuracy for bacteria with active learning.

## Executive Summary
This study presents a deep learning pipeline for rapid identification of bacterial and fungal pathogens in Gram-stained blood smears from septic patients. The system combines Cellpose 3 for instance segmentation with Attention-based Deep Multiple Instance Learning for classification, processing 16,637 clinical microscopic images. The approach successfully identifies 17 core pathogen species with accuracy up to 77.15% for bacteria and 71.39% for fungi when incorporating active learning. The pipeline shows particular promise for identifying Cutibacterium acnes, Enterococcus faecium, and Stenotrophomonas maltophilia among bacteria, and Nakaseomyces glabratus among fungi.

## Method Summary
The method uses a two-stage pipeline: first, Cellpose 3 is fine-tuned separately for bacteria and fungi to segment individual cells from Gram-stained images, producing instance masks. Second, an Attention-based Deep Multiple Instance Learning (AbMILP) classifier aggregates features from 96x96 px patches (bacteria) or 224x224 px patches (fungi) into a single image-level prediction. For bacteria, a ResNet-50 encoder is fine-tuned on the large dataset; for fungi, a frozen DINO Vision Transformer encoder is used due to the smaller dataset size. Active learning iteratively refines segmentation masks through expert-in-the-loop corrections, improving accuracy from 75.27% to 77.15% for bacteria and from 55.70% to 71.39% for fungi.

## Key Results
- Achieved 77.15% accuracy and 0.97 ROC AUC for bacterial identification with active learning
- Achieved 71.39% accuracy and 0.88 ROC AUC for fungal identification with active learning
- Best performance for Cutibacterium acnes (94.57% accuracy), Enterococcus faecium (91.70%), and Stenotrophomonas maltophilia (90.00%) among bacteria
- Most challenging classifications were for closely related species like Staphylococcus hominis vs Staphylococcus haemolyticus

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multiple Instance Learning (MIL) with attention pooling enables image-level classification based on heterogeneous sets of segmented cells, mitigating the need for cell-level annotations.
- **Mechanism:** The pipeline extracts patches for every detected cell. The Attention-based Deep Multiple Instance Learning Pooling (AbMILP) block assigns weights to these patches (instances) and aggregates them into a single feature vector representing the entire image, which is then classified.
- **Core assumption:** The model assumes that the image-level label (pathogen species) can be recovered from the collective features of individual cells, even if some segmented instances are noise or background.
- **Evidence anchors:**
  - [abstract] Mentions using "Attention-based Deep Multiple Instance Learning for classification."
  - [section 2.4] Describes aggregating patch representations using AbMILP to produce a single feature vector per image.
  - [corpus] Related work on "Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification" confirms the utility of MIL in hematological image analysis where single-cell annotation is costly.
- **Break condition:** If the segmentation model captures significant non-cell debris or misses the majority of pathogen cells, the attention mechanism may aggregate noise or miss the signal entirely, leading to misclassification.

### Mechanism 2
- **Claim:** Active learning iteratively refines the segmentation ground truth, correcting model drift and handling edge cases in clinical samples.
- **Mechanism:** A user (expert) reviews segmentation masks predicted by Cellpose. Corrections (accepting, clearing, or skipping masks) are fed back into the training loop, expanding the ground truth dataset from 120 to 1597 samples for bacteria.
- **Core assumption:** The assumption is that segmentation errors are a primary bottleneck for performance, and that expert-in-the-loop corrections provide higher value than simply increasing the quantity of raw images.
- **Evidence anchors:**
  - [abstract] Notes that with active learning, accuracy improved to 77.15% (from 75.27%) for bacteria.
  - [section 2.3] Describes the semi-automated interactive process where experts validate or correct masks to extend the fine-tuning dataset.
  - [corpus] Specific corpus evidence for active learning in this context is weak; however, general principles align with data-centric AI approaches in "DinoBloom" regarding the value of curated data.
- **Break condition:** If the visual features of the pathogen are ambiguous even to experts (e.g., overlapping cells), or if the active learning sampling strategy fails to select informative samples, performance will plateau.

### Mechanism 3
- **Claim:** Freezing pre-trained encoders for smaller datasets (fungi) prevents overfitting, while fine-tuning encoders for larger datasets (bacteria) maximizes domain adaptation.
- **Mechanism:** For fungi, the authors use a DINO (Vision Transformer) encoder pre-trained on ImageNet but **do not** train it, relying on the generalizability of self-supervised features. For bacteria, the ResNet-50 encoder is fine-tuned jointly with the classifier.
- **Core assumption:** The frozen DINO model possesses sufficient universal feature extraction capabilities to distinguish fungal morphology without seeing domain-specific gradients, whereas the bacterial dataset is large enough to support fine-tuning without catastrophic overfitting.
- **Evidence anchors:**
  - [section 2.5] States: "we did not train our encoder and relied on the DINO's ability... given the network will observe only around 1250 images... resulting in overfitting."
  - [section 3.4] Highlights that bacterial classification benefits from a trained encoder on the larger dataset.
  - [corpus] "DinoBloom" reinforces that foundation models (like DINO) can provide generalizable cell embeddings without full fine-tuning.
- **Break condition:** If the domain shift between ImageNet and Gram-stained microscopy is too large, the frozen encoder will fail to capture the specific texture/color features required for discrimination.

## Foundational Learning

- **Concept: Multiple Instance Learning (MIL)**
  - **Why needed here:** Standard deep learning requires a label for every input image. In microscopy, an image contains hundreds of cells (instances), but we only have one label (the species) for the whole image. MIL allows the model to learn which instances matter without needing a label for every single cell.
  - **Quick check question:** If you had labels for every individual cell, would you still need MIL, or could you use a standard classifier? (Answer: You could use a standard classifier, but MIL handles the aggregation logic naturally).

- **Concept: Instance Segmentation vs. Object Detection**
  - **Why needed here:** To analyze specific bacteria, the system must not only find where they are (detection) but delineate their exact shape at the pixel level (segmentation). This is handled by Cellpose.
  - **Quick check question:** Why is simple bounding box detection insufficient for identifying *S. pneumoniae* vs *E. faecalis*? (Answer: Morphology and specific arrangement of cells are key discriminators; pixel-level shape context helps).

- **Concept: Transfer Learning & Domain Adaptation**
  - **Why needed here:** Medical datasets are small compared to general computer vision datasets. The model leverages weights learned from millions of natural images (ImageNet) and adapts them to the specific distribution of Gram-stain colors and textures.
  - **Quick check question:** Why did the authors choose *not* to fine-tune the encoder for the fungi model? (Answer: The dataset was too small, risking overfitting where the model memorizes training images rather than learning general features).

## Architecture Onboarding

- **Component map:** Input Images -> Cellpose Segmentation -> Instance Masks -> Cell-Centered Patches -> Encoder (ResNet-50/DINO) -> Feature Vectors -> AbMILP Attention Pooling -> Single Image Vector -> Classifier -> Species Prediction

- **Critical path:** The **Segmentation-to-Patch** link is the highest risk. If Cellpose fails to distinguish touching cells or rejects debris, the Encoder receives garbage input, and the AbMILP attention mechanism cannot recover the signal.

- **Design tradeoffs:**
  - **ResNet vs. DINO:** The team traded the potentially higher accuracy of a fine-tuned ResNet for the stability of a frozen DINO model to accommodate the smaller fungal dataset size (~1.5k images vs ~14k bacterial images).
  - **Speed vs. Accuracy:** The pipeline requires processing potentially hundreds of patches per image, which is computationally heavier than a single-image pass but necessary for cell-level resolution.

- **Failure signatures:**
  - **High "Other" class confusion:** The model struggles to reject species not in the core 17 classes (59% accuracy for "Other" bacteria).
  - **Intra-genus confusion:** *Staphylococcus hominis* vs *Staphylococcus haemolyticus* fail frequently due to morphological similarity (t-SNE plots show overlapping clusters).
  - **Morphological diversity:** *Candida albicans* performs poorly (42.3%) likely due to high variance in cell shape (pseudohyphae vs blastospores) confusing the segmentation and aggregation steps.

- **First 3 experiments:**
  1. **Segmentation Baseline:** Run Cellpose on a held-out validation set and manually inspect the Intersection over Union (IoU) for touching vs. isolated cells to quantify segmentation noise.
  2. **Ablation on Encoder Freezing:** Attempt to fine-tune the fungal encoder with a lower learning rate to see if overfitting can be managed better than using the frozen DINO model.
  3. **Attention Map Visualization:** Visualize the attention weights from the AbMILP layer on a failed classification (e.g., *S. hominis* misidentified as *S. haemolyticus*) to see if the model is "looking" at the wrong cells or just failing to distinguish the correct ones.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a fungal-specific segmentation module improve classification accuracy for polymorphic yeast-like fungi?
- Basis in paper: [explicit] The authors state, "We hypothesize that the classification of fungal species can be further improved by introducing a fungal-specific segmentation module. Unlike Cellpose, it should be able to detect shapes other than spheroids."
- Why unresolved: The current Cellpose model struggles with the varied morphology of *Candida albicans* (blastospores vs. pseudohyphae), contributing to its low classification accuracy (42.3%).
- What evidence would resolve it: Comparative performance metrics of a custom segmentation model trained on non-spheroid fungal shapes versus the current Cellpose implementation.

### Open Question 2
- Question: How can prediction reliability be quantified to support clinical decision-making?
- Basis in paper: [explicit] The limitations section notes, "Finally, the method does not have any mechanism to quantify the uncertainty and reliability of the prediction, which we plan to address in future research."
- Why unresolved: Deep learning models can yield incorrect results with high confidence; without uncertainty quantification, clinicians cannot assess the trustworthiness of a specific diagnosis.
- What evidence would resolve it: Integration of uncertainty metrics (e.g., Monte Carlo Dropout) that correlate strongly with prediction error rates.

### Open Question 3
- Question: How does the model perform on data from different clinical facilities and imaging equipment?
- Basis in paper: [explicit] The authors acknowledge, "Additionally, we have not investigated how the model reacts to samples from different facility and how batch effects are affecting the model performance."
- Why unresolved: The study utilized images from a single hospital using specific equipment, raising concerns about generalizability to different staining protocols or microscope setups.
- What evidence would resolve it: Validation results from external datasets collected from multiple hospitals using different microscope brands and staining protocols.

## Limitations
- The dataset is limited to a single hospital's retrospective samples, raising concerns about generalizability to different staining protocols and equipment
- The model struggles with morphologically similar species within the same genus, particularly Staphylococcus species
- No mechanism exists to quantify prediction uncertainty, limiting clinical decision support capabilities

## Confidence
- **High Confidence:** The general pipeline architecture combining Cellpose segmentation with Attention-based Deep Multiple Instance Learning is technically sound and appropriately designed for the problem
- **Medium Confidence:** The reported performance metrics (75.27% accuracy for bacteria, 55.70% for fungi without active learning) are likely accurate for the described dataset
- **Low Confidence:** The generalizability to unseen pathogen species beyond the 17 trained classes, particularly the 59% accuracy for "Other" bacteria

## Next Checks
1. **Segmentation Quality Analysis:** Run Cellpose on a held-out validation set and manually quantify Intersection over Union (IoU) metrics for touching versus isolated cells, particularly focusing on morphological similarity cases like Staphylococcus species that showed poor performance.
2. **Cross-Domain Testing:** Test the trained models on Gram-stained images from different hospitals or imaging systems to assess robustness to variations in staining protocols, camera equipment, and sample preparation techniques.
3. **Prospective Clinical Validation:** Implement the pipeline in a real-time diagnostic workflow where the model's predictions are compared against standard microbiological identification methods (MALDI-TOF) on newly collected blood culture samples from septic patients, measuring both accuracy and time-to-diagnosis compared to conventional methods.