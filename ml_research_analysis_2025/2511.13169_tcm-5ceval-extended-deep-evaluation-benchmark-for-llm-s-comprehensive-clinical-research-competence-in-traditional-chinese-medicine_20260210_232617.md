---
ver: rpa2
title: 'TCM-5CEval: Extended Deep Evaluation Benchmark for LLM''s Comprehensive Clinical
  Research Competence in Traditional Chinese Medicine'
arxiv_id: '2511.13169'
source_url: https://arxiv.org/abs/2511.13169
tags:
- chinese
- medicine
- clinical
- knowledge
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TCM-5CEval is a comprehensive benchmark for evaluating LLMs in
  Traditional Chinese Medicine across five dimensions: Core Knowledge, Classical Literacy,
  Clinical Decision-Making, Chinese Materia Medica, and Clinical Non-pharmacological
  Therapy. Built from authoritative TCM textbooks, it contains 1,366 questions across
  single-choice, multiple-choice, and open-ended formats.'
---

# TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2511.13169
- Source URL: https://arxiv.org/abs/2511.13169
- Reference count: 36
- Models struggle with classical text interpretation and show severe positional bias sensitivity across all tested models

## Executive Summary
TCM-5CEval is a comprehensive benchmark evaluating large language models on Traditional Chinese Medicine competence across five dimensions: Core Knowledge, Classical Literacy, Clinical Decision-Making, Chinese Materia Medica, and Clinical Non-pharmacological Therapy. Built from authoritative TCM textbooks, it contains 1,366 questions across single-choice, multiple-choice, and open-ended formats. The benchmark reveals that while models excel at recalling foundational knowledge, they struggle significantly with classical text interpretation and clinical reasoning, with all models showing severe sensitivity to option ordering in consistency testing.

## Method Summary
The benchmark evaluates 15 leading LLMs on 1,366 questions across five TCM dimensions, using accuracy metrics for objective questions and BertScore plus macro recall for open-ended responses. A key methodological innovation is permutation-based consistency testing, where single-choice questions are presented with five cyclic rotations of options to reveal positional bias. Models must answer correctly on all permutations to pass consistency testing, exposing reliance on memorized patterns rather than semantic understanding.

## Key Results
- deepseek_r1 and Kimi_K2_Instruct_0905 excelled in foundational knowledge but struggled with classical text interpretation
- All models showed substantial performance degradation when faced with varied question option ordering, indicating positional bias
- Error hotspots clustered in clinical four-diagnosis pattern differentiation and classical exegesis interpretation
- Models achieved high accuracy on single-choice questions but failed consistency testing, revealing shallow reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permutation-based consistency testing reveals shallow reasoning via positional bias sensitivity
- Mechanism: When models are presented with cyclically rotated option orderings (5 permutations per question), performance degradation indicates reliance on positional heuristics rather than semantic understanding
- Core assumption: Robust knowledge should be invariant to option ordering; sensitivity indicates surface-level pattern matching
- Evidence anchors: All evaluated models displayed substantial performance degradation with varied option ordering

### Mechanism 2
- Claim: Multi-format question architecture captures distinct cognitive depth levels
- Mechanism: Single-choice tests recall, multiple-choice tests comparative discrimination, open-ended tests generative synthesis
- Core assumption: Different question formats exercise qualitatively different cognitive processes
- Evidence anchors: Models excel at recall but struggle with interpretation and generation

### Mechanism 3
- Claim: Training corpus composition determines sub-dimensional specialization
- Mechanism: Different models excel in different sub-dimensions based on training data exposure (classical texts vs. pharmaceutical databases vs. clinical procedural texts)
- Core assumption: Domain specialization emerges from corpus-specific exposure
- Evidence anchors: deepseek_r1 leads classical literacy; Kimi leads materia medica; grok leads non-pharmacological therapy

## Foundational Learning

- Concept: **Syndrome Differentiation (辨证论治)**
  - Why needed here: Core TCM diagnostic reasoning—identified as top error category across all question types
  - Quick check question: Given symptoms of "lower back pain worse with cold, better with heat," can you differentiate between kidney yang deficiency vs. cold-damp stagnation?

- Concept: **Positional Bias in LLM Evaluation**
  - Why needed here: The benchmark's key methodological innovation
  - Quick check question: If a model answers "B" when correct answer is in position B, but "C" when the same answer is moved to position C, what does this reveal about its reasoning?

- Concept: **TCM Classical Literature Interpretation**
  - Why needed here: Universal weakness across models
  - Quick check question: Translate and interpret: "见肝之病，知肝传脾，当先实脾" (Seeing liver disease, know it transmits to spleen, first strengthen the spleen)—what clinical principle does this express?

## Architecture Onboarding

- Component map:
  Data Layer: 30 TCM course exercise sets → 5 sub-datasets (Exam, LitQA, MRCD, CMM, ClinNPT) → 1,366 questions
  Evaluation Layer: Accuracy (objective questions) + BertScore + macro recall (open-ended) + permutation consistency testing
  Analysis Layer: Sub-dimensional performance comparison + error hotspot aggregation + consistency degradation measurement

- Critical path:
  1. Understand the 5C dimension taxonomy
  2. Implement permutation testing first—this is the diagnostic that separates genuine understanding from surface patterns
  3. Map error hotspots to identify systematic gaps

- Design tradeoffs:
  - Breadth vs. depth: 5 dimensions provide coverage but limited questions per dimension
  - Automated vs. expert evaluation: BertScore/macro recall enable scalability but may miss clinical nuance
  - Textbook vs. real-world data: Authoritative but artificial; real clinical cases would test practical applicability

- Failure signatures:
  - High single-choice accuracy but low permutation consistency → positional bias
  - Large gap between TCM-Exam and TCM-LitQA → memorization without interpretation
  - Error clustering in "clinical four-diagnosis pattern differentiation" → failure to integrate holistic reasoning

- First 3 experiments:
  1. Run permutation test: Calculate consistency rate vs. baseline accuracy
  2. Analyze sub-dimensional error gaps; correlate with training corpus composition
  3. Compare performance across single-choice vs. multiple-choice vs. open-ended within same dimension

## Open Questions the Paper Calls Out

None

## Limitations

- Positional bias across all models undermines single-choice accuracy as a measure of TCM competence
- Automated metrics (BertScore, macro recall) may miss clinical nuance in open-ended questions
- Focus on textbook exercises rather than real clinical cases limits practical applicability assessment

## Confidence

**High Confidence**: Positional bias findings and consistency testing methodology are robust
**Medium Confidence**: Training corpus composition determining specialization is plausible but correlational
**Low Confidence**: Benchmark's ability to predict real-world clinical performance remains speculative

## Next Checks

1. Implement the exact permutation testing protocol on a new set of LLMs to verify positional bias patterns
2. Analyze training corpus composition for models showing strong sub-dimensional performance
3. Design follow-up study comparing textbook-based performance with real-world clinical case outcomes