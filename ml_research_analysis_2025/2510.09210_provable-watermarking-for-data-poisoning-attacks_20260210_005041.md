---
ver: rpa2
title: Provable Watermarking for Data Poisoning Attacks
arxiv_id: '2510.09210'
source_url: https://arxiv.org/abs/2510.09210
tags:
- watermarking
- data
- poisoning
- attacks
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of ensuring transparency in data
  poisoning attacks by introducing provable watermarking schemes. It proposes two
  approaches: post-poisoning watermarking, where a third party adds watermarks to
  poisoned data, and poisoning-concurrent watermarking, where the poisoner embeds
  watermarks during poisoning.'
---

# Provable Watermarking for Data Poisoning Attacks

## Quick Facts
- **arXiv ID:** 2510.09210
- **Source URL:** https://arxiv.org/abs/2510.09210
- **Reference count:** 40
- **Primary result:** Introduces provable watermarking schemes for data poisoning attacks, with theoretical bounds requiring Θ(√d/εw) dimensions for post-poisoning and Θ(1/εw²) for concurrent watermarking, validated across multiple datasets with AUROC up to 1.0.

## Executive Summary
This paper addresses the problem of ensuring transparency in data poisoning attacks by introducing provable watermarking schemes. It proposes two approaches: post-poisoning watermarking, where a third party adds watermarks to poisoned data, and poisoning-concurrent watermarking, where the poisoner embeds watermarks during poisoning. Theoretical analysis shows that effective watermarking requires lengths of Θ(√d/εw) for post-poisoning and Θ(1/εw²) to O(√d/εp) for poisoning-concurrent, ensuring both watermark detectability and poisoning utility. Experiments on backdoor and availability attacks across multiple datasets and models validate these findings, demonstrating strong detection performance (AUROC up to 1.0) with minimal impact on poisoning effectiveness when watermarking length is appropriately bounded.

## Method Summary
The method embeds detectable watermarks into poisoned data using random key vectors. For post-poisoning, a third party adds perturbations after poisoning using a random key ζ ∈ {-1,+1}ᵈ to create statistical separability. For poisoning-concurrent, the poisoner controls both signals, separating watermark and poison dimensions to preserve utility. The watermark perturbation δ_w = ε_w · sign(ζ)|_W is applied to selected dimensions, with detection based on the inner product ζᵀx. The approach is evaluated on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet-18 models trained with standard SGD hyperparameters.

## Key Results
- Detection AUROC reaches 1.0 for appropriately sized watermarks (q ≥ Θ(√d/εw) for post-poisoning)
- Poisoning utility (ASR for backdoors, accuracy degradation for availability) is preserved when watermark length stays within theoretical bounds
- Concurrent watermarking requires shorter lengths (Θ(1/εw²)) than post-poisoning (Θ(√d/εw)) for equivalent detectability
- Watermark detectability degrades rapidly when length falls below theoretical thresholds

## Why This Works (Mechanism)

### Mechanism 1: Statistical Separability via Random Key
The method generates a random key vector ζ (entries in {-1, +1}ᵈ). For clean data x, the inner product ζᵀx behaves like a random walk, remaining near zero. The watermark δ_w is crafted as ε_w · sign(ζ) on q dimensions. This forces the inner product ζᵀ(x+δ_w) to increase linearly with qε_w, creating a statistical gap that allows detection. If q is too small (specifically q < Ω(√d/εw)), the signal qε_w is indistinguishable from the variance of ζᵀx, causing detection to fail.

### Mechanism 2: Dimensional Isolation (Concurrent Watermarking)
In poisoning-concurrent watermarking, dimensions W (watermark) and P (poison) are disjoint. The watermark occupies indices d₁, ..., d_q while the poison perturbs the remaining indices. Because the watermark signal does not override the poison signal in the P dimensions, the model can still learn the poison features effectively. If the poison relies on specific features that happen to fall in the watermark dimensions W, or if q becomes so large (q ≈ O(√d/εp)) that insufficient dimensions remain for the poison to be effective, utility degrades.

### Mechanism 3: Theoretical Trade-off Bounds
The theoretical lower bound on watermark length q required for detection differs significantly between the two proposed architectures. Post-poisoning requires q ≈ Θ(√d/εw) because the watermark acts as noise added after the poison. Concurrent watermarking requires only q ≈ Θ(1/εw²) since the generator controls the space and restricts noise to watermark dimensions. If the watermarking budget ε_w is extremely small, the required q may exceed the physical dimension d, rendering the provable bounds impossible to satisfy physically.

## Foundational Learning

- **Concept: Concentration of Measure (Random Projections)**
  - Why needed here: The core detection logic assumes that for a random vector ζ, the inner product ζᵀx for "clean" data is small compared to the induced watermark signal. Understanding that high-dimensional vectors tend to be orthogonal is key to seeing why the thresholding works.
  - Quick check question: If you project a 1000-dimensional clean image onto a random binary vector ζ ∈ {-1,1}¹⁰⁰⁰, do you expect the sum to be close to 0 or close to 500?

- **Concept: L∞ Perturbation Budget (εp, εw)**
  - Why needed here: The entire framework operates within strict bounds on how much an image can change to remain "imperceptible." The trade-off is governed by the ratio of the allowable budget to the necessary signal strength (length q).
  - Quick check question: If εw is cut in half (smaller watermark allowed), how must the watermark length q change to maintain the same detectability?

- **Concept: Poisoning Objectives (Availability vs. Backdoor)**
  - Why needed here: The paper validates against two distinct goals. Availability attacks aim to tank accuracy (UE, AP), while Backdoor attacks (Narcissus, AdvSc) aim for targeted misclassification. The watermark must preserve both specific failure modes.
  - Quick check question: If a watermark inadvertently acts as a "trigger" for a backdoor, is it preserving the poison's utility?

## Architecture Onboarding

- **Component map:**
  Key Generator -> Dimension Selector -> Perturbation Engine -> Detector
  (Key Generator: Generates ζ ∈ {-1, +1}ᵈ)
  (Dimension Selector: Chooses indices W={d₁, ..., d_q})
  (Perturbation Engine: Applies δ_w = ε_w · sign(ζ)|_W to data)
  (Detector: Computes v = ζᵀx̃. If v > threshold, returns "Watermarked")

- **Critical path:**
  1. Determine budget ε_w and calculate required length q using bounds (e.g., q ≈ Θ(√d/ε_w) for post-poisoning)
  2. Sample random key ζ
  3. Select q dimensions for watermark injection
  4. Inject watermark (and poison, if concurrent)
  5. In deployment, use ζ to check if data is "owned/poisoned" before training or to verify "safe" data removal

- **Design tradeoffs:**
  - Post-Poisoning vs. Concurrent: Post-poisoning is easier for third-party auditors but requires a larger q (more distortion). Concurrent minimizes q (less distortion for same detectability) but requires the poisoner to coordinate both signals.
  - Length q vs. Poison Utility: Increasing q improves AUROC (detectability) but eventually degrades poisoning utility (ASR drops / Accuracy recovers), especially in the Concurrent setting where q eats into poison dimensions.

- **Failure signatures:**
  - Random Guessing (AUROC ~ 0.5): q is too small for the given dimension d and budget ε_w. The watermark signal is drowned out by the data's variance.
  - Poison Neutralization: For concurrent watermarking, if q exceeds O(√d/ε_p), the clean accuracy recovers (for availability attacks) or ASR drops (for backdoors), meaning the watermark has overwritten the poison.
  - False Positives: Threshold τ is set too low relative to the data distribution's variance.

- **First 3 experiments:**
  1. Detection ROC Sweep: Fix a poison type (e.g., Narcissus) and dimension d. Vary watermark length q (e.g., 100 to 3000). Plot AUROC vs. q to verify the theoretical threshold where detection becomes reliable (>0.9).
  2. Utility Preservation Check: Using the "Concurrent" method, vary q and plot the Attack Success Rate (ASR). Verify that ASR remains high until q approaches the upper bound, then drops off.
  3. Budget Scaling: Fix q and vary the watermark budget ε_w (e.g., 4/255 to 32/255). Confirm that lower ε_w requires higher q to maintain the same AUROC, as predicted by the bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the necessary conditions for watermarking length and budget to ensure both detectability and poisoning utility?
- Basis in paper: [explicit] The authors state in Section 7 that "the necessary conditions for these properties remain an open area for future research," noting that the current work only establishes sufficient conditions.
- Why unresolved: The paper provides sufficient bounds for watermark length q (e.g., Θ(√d/εw)) but does not derive the lower bounds required to prove these conditions are tight.
- What evidence would resolve it: A theoretical proof establishing a strict lower bound for q below which the watermarking or poisoning is guaranteed to fail.

### Open Question 2
- Question: Can more sophisticated watermarking designs achieve better robustness against adaptive removal without degrading poisoning utility?
- Basis in paper: [explicit] Section 7 identifies exploring "more sophisticated watermarking designs" for better performance and robustness as a promising direction.
- Why unresolved: The paper evaluates removal methods in Appendix E but finds that robust defenses often destroy the poisoning utility, leaving the design of a watermark that survives without destroying the poison unresolved.
- What evidence would resolve it: A watermarking scheme that survives adversarial perturbations or image transformations while maintaining the Attack Success Rate (ASR) within the theoretical bounds.

### Open Question 3
- Question: Do the theoretical guarantees for watermarking generalize to architectures beyond standard feed-forward networks?
- Basis in paper: [inferred] The theoretical analysis in Section 5 relies on assumptions specific to L-layer feed-forward networks with Xavier normalization (Assumption 5.4), which may not strictly hold for modern architectures like Vision Transformers.
- Why unresolved: While empirical results show transferability, the theoretical proofs do not cover the dynamics of attention mechanisms or residual connections directly.
- What evidence would resolve it: An extension of Theorems 5.2 and 5.6 to non-FFN architectures, or theoretical bounds based on Neural Tangent Kernels (NTK) for more general models.

## Limitations

- The theoretical bounds assume i.i.d. data dimensions and rely on concentration inequalities that may not hold for natural image datasets with strong spatial correlations.
- The poisoning baselines are referenced from external papers without complete hyperparameter specifications, creating reproduction uncertainty.
- The required watermark length may approach or exceed available dimensionality for high-dimensional data, limiting practical applicability.

## Confidence

- **High Confidence:** The statistical mechanism of using random keys to create detectable signals in high-dimensional space is theoretically sound and experimentally validated (AUROC up to 1.0).
- **Medium Confidence:** The poisoning utility preservation claims hold within tested parameter ranges but may degrade for attacks requiring specific feature distributions that conflict with watermark dimensions.
- **Low Confidence:** The theoretical bounds' tightness and generalizability to non-image data distributions remain unverified.

## Next Checks

1. **Parameter Sensitivity Test:** Systematically vary watermark budget εw (e.g., 4/255 to 32/255) while holding q constant to verify the predicted inverse relationship between budget and required length for detection.

2. **Cross-Dataset Generalization:** Apply the watermarking scheme to datasets with different dimensional structures (e.g., speech spectrograms, tabular data) to test whether the √d/εw scaling holds beyond natural images.

3. **Threshold Robustness Analysis:** Evaluate detection performance across varying threshold settings and data distributions to determine sensitivity to the assumption of data concentration around zero in random projections.