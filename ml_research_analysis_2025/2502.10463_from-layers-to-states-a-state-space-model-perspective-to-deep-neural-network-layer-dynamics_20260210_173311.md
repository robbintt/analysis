---
ver: rpa2
title: 'From Layers to States: A State Space Model Perspective to Deep Neural Network
  Layer Dynamics'
arxiv_id: '2502.10463'
source_url: https://arxiv.org/abs/2502.10463
tags:
- layer
- state
- s6la
- layers
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes treating deep neural network layer outputs
  as continuous states of a state space model (SSM), rather than discrete steps, to
  better aggregate information in very deep networks. Inspired by selective state
  space models (S6), it introduces Selective State Space Model Layer Aggregation (S6LA),
  which models layer dynamics as a continuous process and uses the selective mechanism
  to improve information flow across layers.
---

# From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics

## Quick Facts
- arXiv ID: 2502.10463
- Source URL: https://arxiv.org/abs/2502.10463
- Reference count: 40
- Primary result: S6LA improves ResNet-50 top-1 accuracy from 76.1% to 78.0% on ImageNet

## Executive Summary
This paper proposes treating deep neural network layer outputs as continuous states of a state space model (SSM), rather than discrete steps, to better aggregate information in very deep networks. Inspired by selective state space models (S6), it introduces Selective State Space Model Layer Aggregation (S6LA), which models layer dynamics as a continuous process and uses the selective mechanism to improve information flow across layers. The approach is applied to both CNNs and vision transformers, achieving significant performance gains with only minor increases in parameters and FLOPs.

## Method Summary
S6LA treats each layer output as a state in a continuous state space model, where the latent state h evolves according to h'(t) = Ah(t) + Bx(t). This continuous formulation is discretized using zero-order hold to obtain h_t = exp(∆A)h_{t-1} + ∆B·x_t, where ∆ and B are input-dependent parameters computed via selective projections. For CNNs, the latent state is concatenated with the current features before convolution, while for ViTs, it modulates patch tokens through multiplicative interactions. The method is applied to ResNet and Transformer backbones, with the latent state dimension set to N=32 for optimal performance.

## Key Results
- ResNet-50 with S6LA: 76.1% → 78.0% top-1 accuracy on ImageNet
- ResNet-101 with S6LA: 77.4% → 79.1% top-1 accuracy on ImageNet
- S6LA consistently improves object detection and segmentation metrics on MS COCO 2017 across Faster R-CNN and Mask R-CNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating layer outputs as continuous states improves information aggregation in very deep networks compared to discrete-step perspectives.
- **Mechanism:** The paper models layer dynamics as a continuous process where h'(t) = Ah(t) + Bx(t), then discretizes via zero-order hold to obtain h_t = Āh_{t-1} + B̄x_t. This allows the hidden state h to accumulate information across layers with structured memory decay controlled by matrix A, rather than treating each layer as an independent discrete transformation.
- **Core assumption:** Very deep networks (e.g., ResNet-152) behave like high-frequency sampled continuous systems, making continuous modeling more appropriate than discrete formulations.
- **Evidence anchors:**
  - [abstract] "This paper novelly treats the outputs from layers as states of a continuous process and considers leveraging the state space model (SSM) to design the aggregation of layers in very deep neural networks."
  - [section 3.4] "for a very deep neural network, it is more like the scenario of high-frequency data, and hence a continuous process is more suitable"
  - [corpus] Weak direct evidence; neighbor papers focus on other SSM applications (graph networks) without validating the continuous-discrete claim.
- **Break condition:** Shallow networks where T is small may not benefit; the continuous approximation could over-parameterize simple discrete transitions.

### Mechanism 2
- **Claim:** The selective mechanism (input-dependent ∆ and B parameters) enables context-aware filtering and retention of layer information.
- **Mechanism:** Rather than fixed transition dynamics, S6LA computes ∆ = Linear(x_t) and B = Linear(x_t) at each layer. This allows the model to adaptively compress or expand the effective temporal resolution—remembering critical features indefinitely (when ∆ is small) or filtering noise (when ∆ is large), directly conditioned on the current layer's output.
- **Core assumption:** Different layers contribute unequally to the final representation; some should be heavily weighted while others filtered.
- **Evidence anchors:**
  - [abstract] "the Selective State Space Models (S6) is employed to design a new module... the selective mechanism to better aggregate information"
  - [section 5] "the selective mechanism is crucial for our model... w/o selective: 77.3 vs S6LA: 78.0"
  - [corpus] "Mamba-Based Graph Convolutional Networks" paper applies selective SSM to address over-smoothing, suggesting selection helps with depth-related issues, though in a different domain.
- **Break condition:** If all layers truly contribute equally, the overhead of computing input-dependent parameters provides no benefit over fixed dynamics.

### Mechanism 3
- **Claim:** Propagating a latent state h_t alongside the main feature stream creates a parallel information pathway that augments layer representations without disrupting backbone gradients.
- **Mechanism:** For CNNs: h_{t-1} is concatenated with X_t before convolution, then updated via h_t = exp(∆A)h_{t-1} + ∆B·O_t. For ViTs: h_t modulates patch tokens via X_p^t = X_p^{t-1} + W·X_p^{t-1}·h_t. The latent state carries compressed historical context while X follows its normal forward pass plus residual influence from h.
- **Core assumption:** Information valuable for deep layer aggregation differs from information needed for local feature extraction.
- **Evidence anchors:**
  - [section 3.5] "h influence X: Concatenate the t-1-th output X^{t-1} and the t-1-th latent state h_{t-1}"
  - [section 3.6] "h influence X, multiplication not simple concatenation... X_p^t = X_p^{t-1} + W·X_p^{t-1}·h_t"
  - [corpus] No direct corpus support; related work on layer interactions (RLA, MRLA) uses different parallel pathways.
- **Break condition:** If h_t dimension N is too small, information bottleneck occurs; if too large, computational overhead outweighs gains (ablation shows N=32 optimal, N=64 degrades).

## Foundational Learning

- **Concept:** State Space Models (continuous-to-discrete)
  - **Why needed here:** S6LA's core formulation derives from continuous differential equations discretized via zero-order hold. Understanding h'(t) = Ah(t) + Bx(t) and its discretization is essential to grasp why layer aggregation can be "continuous."
  - **Quick check question:** Can you explain why exp(∆A) approximates the discrete transition matrix and what role ∆ plays in controlling memory?

- **Concept:** Selective Scan / Mamba Architecture
  - **Why needed here:** The S6 mechanism (input-dependent ∆ and B) is borrowed from Mamba. Without this background, the distinction between S4 (fixed dynamics) and S6 (selective dynamics) is opaque.
  - **Quick check question:** Why does making ∆ input-dependent allow the model to "forget" irrelevant information while "remembering" relevant information indefinitely?

- **Concept:** Layer Aggregation / Skip Connections
  - **Why needed here:** S6LA positions itself against DenseNet, RLA, and MRLA. Understanding how these methods aggregate layers (concatenation vs. additive vs. recurrent) clarifies what S6LA does differently.
  - **Quick check question:** How does DenseNet's concatenation strategy differ from S6LA's latent state propagation in terms of parameter growth?

## Architecture Onboarding

- **Component map:** Backbone (ResNet/ViT) -> S6LA module (latent state update + selective projections) -> Augmented features -> Next backbone layer
- **Critical path:**
  1. Initialize h_0 via Kaiming normal (critical for gradient flow)
  2. At each layer t: compute ∆_t, B_t from projected features
  3. Update h_t = exp(∆_t·A)·h_{t-1} + ∆_t·B_t·input
  4. Fuse h_t into X_t (concatenation for CNN, multiplication for ViT)
  5. Pass augmented X_t to next backbone layer

- **Design tradeoffs:**
  - N (latent dimension): Higher N = more capacity but more FLOPs. Ablation shows N=32 sweet spot.
  - Selective vs. fixed: Selection adds ~0.2M params but critical for performance (77.3→78.0).
  - Trainable h vs. random: Trainable h consistently outperforms (Table 5).

- **Failure signatures:**
  - **Overfitting on small datasets:** Enhanced layer interactions can overfit when training data is limited (authors note ResNet's outdated augmentation contributes here).
  - **Initialization collapse:** Poor h_0 initialization leads to gradient issues; Kaiming normal is required.
  - **N too large:** N=64 degrades performance vs. N=32 (Table 6).

- **First 3 experiments:**
  1. **Validate selective mechanism:** Train ResNet-50+S6LA with and without input-dependent ∆, B (use fixed values). Expect ~0.7% gap per Table 5.
  2. **Latent dimension sweep:** Test N∈{16, 32, 64} on a small validation split to confirm N=32 optimal before full training.
  3. **Backbone integration test:** Apply S6LA to a minimal 4-layer CNN on CIFAR-10 to verify h propagation and gradient flow before scaling to ImageNet.

## Open Questions the Paper Calls Out
- The authors explicitly state their aim to "optimize our approach by reducing parameters and FLOPs while enhancing accuracy" as a future direction, acknowledging the non-zero overhead introduced by the aggregation module.

## Limitations
- The theoretical justification for why continuous modeling outperforms discrete layer aggregation in very deep networks remains underdeveloped.
- The ablation studies are limited to N=32 and the selective mechanism, with other design choices (structured matrix A, latent state initialization) less explored.
- The paper only validates on standard depths (50–152 layers), not on extremely deep architectures where the continuous approximation might be more beneficial.

## Confidence
- **High** confidence in performance claims due to consistent improvements across ResNet, ViT, and detection tasks.
- **Medium** confidence in ablation results (N=32 optimal, selective mechanism crucial) as they are well-controlled but limited in scope.
- **Medium** confidence in efficiency claims, as a direct comparison of params/FLOPs vs. accuracy against all baselines is not fully quantified.

## Next Checks
1. **Ablate the structured matrix A initialization** by testing random vs. Toeplitz vs. other structured forms to isolate its contribution.
2. **Test S6LA on shallow networks (4-8 layers)** to validate the claim that continuous modeling is specifically beneficial for very deep architectures.
3. **Benchmark against DenseNet and RLA with matched parameter budgets** to directly quantify the efficiency gains claimed in the paper.