---
ver: rpa2
title: 'Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks'
arxiv_id: '2508.14536'
source_url: https://arxiv.org/abs/2508.14536
tags:
- ch-dqn
- function
- chebyshev
- learning
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Chebyshev-DQN (Ch-DQN), a novel architecture
  that integrates a Chebyshev polynomial basis into the DQN framework to improve function
  approximation for deep reinforcement learning. By projecting input states into a
  feature space defined by Chebyshev polynomials, Ch-DQN provides a more powerful
  and efficient representation of complex value functions compared to standard MLPs.
---

# Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks

## Quick Facts
- arXiv ID: 2508.14536
- Source URL: https://arxiv.org/abs/2508.14536
- Authors: Saman Yazdannik; Morteza Tayefi; Shamim Sanisales
- Reference count: 17
- One-line primary result: Ch-DQN achieves up to 39% better asymptotic performance and 3x better sample efficiency by using Chebyshev polynomials for value function approximation

## Executive Summary
This paper introduces the Chebyshev-DQN (Ch-DQN), a novel architecture that integrates a Chebyshev polynomial basis into the DQN framework to improve function approximation for deep reinforcement learning. By projecting input states into a feature space defined by Chebyshev polynomials, Ch-DQN provides a more powerful and efficient representation of complex value functions compared to standard MLPs. The approach leverages the orthogonality and minimax properties of Chebyshev polynomials to reduce inherent approximation error and improve training stability. Experiments on CartPole-v1, MountainCar-v0, and Acrobot-v1 show that Ch-DQN significantly outperforms standard DQN, achieving up to a 39% improvement in asymptotic performance and nearly 3x better sample efficiency on sparse-reward tasks. The results highlight the trade-off in polynomial degree selection, with moderate degrees (N=4) excelling on simpler tasks and higher degrees (N=8) needed for more complex value landscapes. This work validates the use of orthogonal polynomial bases in deep reinforcement learning.

## Method Summary
Ch-DQN integrates a Chebyshev polynomial feature layer into the standard DQN architecture. The method projects normalized states (scaled to [-1, 1]) into a Chebyshev basis using the recurrence relation T₀=1, T₁=x, Tₙ₊₁=2xTₙ−Tₙ₋₁. These orthogonal features are then passed through a standard MLP to produce Q-values. The architecture is trained using standard DQN procedures with MSE loss, Adam optimizer, and target network updates. The key innovation is replacing the first linear layer with a fixed Chebyshev transformation, providing better-conditioned features for the subsequent MLP to learn from.

## Key Results
- 39% improvement in asymptotic performance on CartPole-v1 compared to standard DQN
- 3x better sample efficiency on MountainCar-v0 (fewer episodes to reach threshold)
- Higher polynomial degrees (N=8) required for complex tasks like Acrobot, while moderate degrees (N=4) suffice for simpler tasks
- Lower training variance and improved stability across all tested environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing standard ReLU-based feature extraction with an orthogonal polynomial basis improves the conditioning of the learning problem and mitigates training instability.
- **Mechanism:** The Chebyshev layer projects input states into a feature space where basis functions are orthogonal, reducing destructive interference during semi-gradient updates common in the "deadly triad."
- **Core assumption:** Correlated, non-orthogonal features in standard MLPs contribute to instability in DQN training.
- **Evidence anchors:** Abstract states orthogonality "improves training stability"; section 6.2 discusses orthogonality mitigating conditioning problems; corpus provides weak direct evidence.
- **Break condition:** If inputs are not properly normalized to [-1, 1], the orthogonality property is mathematically invalidated.

### Mechanism 2
- **Claim:** A Chebyshev basis minimizes the maximum approximation error (minimax property) compared to ad-hoc feature learning.
- **Mechanism:** Chebyshev polynomials possess a "minimax" property, guaranteeing the best polynomial approximation to a continuous function under the L∞ norm, reducing inherent approximation error.
- **Core assumption:** True Q-functions in target environments are smooth enough to be well-approximated by polynomial bases.
- **Evidence anchors:** Abstract mentions "minimax properties... to reduce inherent approximation error"; section 6.1 discusses truncated Chebyshev series as near-optimal approximants.
- **Break condition:** If environment dynamics or reward functions are highly discontinuous, polynomial bias will fail to fit efficiently.

### Mechanism 3
- **Claim:** The polynomial degree (N) acts as a tuner for "spectral bias," requiring calibration to match task complexity.
- **Mechanism:** Degree N controls frequency content of feature space; low N limits high-frequency features (preventing overfitting in simple tasks), while high N includes necessary features for complex value landscapes.
- **Core assumption:** Correlation exists between environment complexity and optimal spectral bandwidth of the approximator.
- **Evidence anchors:** Abstract states "moderate degrees (N=4) excelling on simpler tasks and higher degrees (N=8) needed for more complex value landscapes"; section 6.3 discusses degree N explicitly engineering frequency content.
- **Break condition:** If N is too high for simple tasks, agent overfits noise; if too low for complex tasks, agent lacks representation capacity.

## Foundational Learning

- **Concept:** Orthogonal Polynomials & Chebyshev Recurrence
  - **Why needed here:** You must implement the feature layer Tₙ(x) manually rather than using a library activation function.
  - **Quick check question:** Can you implement a function that takes a vector x and returns the Chebyshev features up to degree N using only the recurrence relation?

- **Concept:** Input Normalization for Basis Functions
  - **Why needed here:** Chebyshev polynomials are strictly defined on the domain [-1, 1]. Passing unnormalized inputs will cause polynomial values to explode.
  - **Quick check question:** Do you know the state-space bounds of your target environment, and have you implemented a scaling layer to clamp inputs to [-1, 1] before the Chebyshev layer?

- **Concept:** The "Deadly Triad" in RL
  - **Why needed here:** The paper frames Ch-DQN as a solution to instability caused by function approximation + bootstrapping + off-policy learning.
  - **Quick check question:** Can you explain why removing correlation from input features might help prevent divergence in a semi-gradient update system like DQN?

## Architecture Onboarding

- **Component map:** Input -> Normalizer -> Chebyshev Layer -> Approximator -> Output
- **Critical path:** The Input Normalization is the single point of failure. If the environment bounds are unknown or the scaling is incorrect, the Chebyshev layer outputs will diverge, causing gradient explosion.
- **Design tradeoffs:**
  - Degree (N) vs. Overfitting: Increasing N increases feature dimensionality and representational power but introduces high-frequency noise susceptibility
  - Parameter Efficiency: Ch-DQN adds parameters via feature expansion (D×(N+1)) before the MLP
- **Failure signatures:**
  - Catastrophic divergence early in training: Likely due to missing input normalization
  - High variance/instability in simple tasks: Likely due to N being too high (e.g., 8)
  - Stagnation at low rewards in complex tasks: Likely due to N being too low (e.g., 4)
- **First 3 experiments:**
  1. Sanity Check (CartPole-v1): Train Ch-DQN (N=4) vs. Standard DQN to replicate 39% performance improvement
  2. Sample Efficiency Test (MountainCar-v0): Compare episodes-to-solve to validate "3x better sample efficiency" claim
  3. Ablation on Degree (Acrobot-v1): Run N=4, 6, 8 to confirm higher N required for complex dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive mechanisms be developed to automatically tune the Chebyshev polynomial degree N during training?
- **Basis in paper:** [explicit] The conclusion states, "Future work could explore adaptive methods for tuning the polynomial degree."
- **Why unresolved:** The paper establishes that N is a critical, sensitive hyperparameter yet relies on manual selection.
- **What evidence would resolve it:** An algorithm that dynamically adjusts N based on validation loss or value-function complexity, matching or exceeding hand-tuned performance.

### Open Question 2
- **Question:** Does Ch-DQN effectively solve high-dimensional, pure sparse-reward tasks when combined with advanced exploration strategies?
- **Basis in paper:** [explicit] The authors conclude, "Future work could... combine the Ch-DQN architecture with more advanced exploration strategies to tackle pure sparse-reward problems."
- **Why unresolved:** The current study used reward shaping for the complex Acrobot task to ensure tractability, masking the agent's raw exploration capability.
- **What evidence would resolve it:** Empirical results on sparse-reward benchmarks (e.g., Montezuma's Revenge) without shaping, showing superior sample efficiency over standard DQN.

### Open Question 3
- **Question:** Can the Chebyshev feature layer be effectively integrated into convolutional architectures for visual reinforcement learning tasks?
- **Basis in paper:** [inferred] The architecture relies on normalizing low-dimensional state vectors and has not been tested on raw pixel inputs.
- **Why unresolved:** Standard DQN famously solved visual control (Atari), but it is unclear how the Chebyshev projection handles the high-dimensional, unstructured feature space of CNNs.
- **What evidence would resolve it:** An implementation of Ch-DQN processing CNN feature maps, demonstrating improved stability or data efficiency in visual domains.

## Limitations

- Experimental validation limited to three classic control environments with deterministic dynamics, leaving uncertainty about generalization to more complex domains
- Theoretical claims about orthogonal polynomial benefits in RL lack direct empirical or analytical validation specific to the reinforcement learning setting
- Assumes known state-space bounds for normalization, which may not hold in practical applications with unbounded or dynamic observation spaces

## Confidence

- **High Confidence:** The claim that Ch-DQN improves performance on CartPole-v1 (39% improvement) and sample efficiency on MountainCar-v0 (3x faster) is well-supported by experimental results
- **Medium Confidence:** The mechanism claims about orthogonality improving training stability and minimax properties reducing approximation error are plausible but lack direct RL-specific validation
- **Medium Confidence:** The degree selection trade-off (N=4 for simple tasks, N=8 for complex tasks) is validated on tested environments but may not generalize without further testing

## Next Checks

1. **Validation Check 1:** Test Ch-DQN on a stochastic environment (e.g., CartPole-v1 with randomized initial states) to verify if stability benefits persist under uncertainty
2. **Validation Check 2:** Implement a "no-normalization" ablation where states are passed directly to the Chebyshev layer without scaling to [-1, 1], which should cause catastrophic divergence
3. **Validation Check 3:** Create a "random activation" baseline replacing the Chebyshev layer with a random orthogonal matrix of the same dimension to determine if benefits come specifically from Chebyshev polynomials or general orthogonal feature decorrelation