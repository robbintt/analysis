---
ver: rpa2
title: Domain-Specific Machine Translation to Translate Medicine Brochures in English
  to Sorani Kurdish
arxiv_id: '2501.13609'
source_url: https://arxiv.org/abs/2501.13609
tags:
- translation
- brochures
- kurdish
- machine
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of Kurdish language medical resources
  by developing a specialized machine translation model for translating English medicine
  brochures into Sorani Kurdish. A parallel corpus of 22,940 aligned sentence pairs
  from 319 brochures was created and used to train a Statistical Machine Translation
  (SMT) model using the Moses toolkit.
---

# Domain-Specific Machine Translation to Translate Medicine Brochures in English to Sorani Kurdish

## Quick Facts
- arXiv ID: 2501.13609
- Source URL: https://arxiv.org/abs/2501.13609
- Reference count: 23
- This paper develops a specialized machine translation model for translating English medicine brochures into Sorani Kurdish using a parallel corpus of 22,940 aligned sentence pairs.

## Executive Summary
This paper addresses the critical shortage of Kurdish language medical resources by developing a specialized machine translation system for translating English medicine brochures into Sorani Kurdish. The researchers created a parallel corpus from 319 bilingual brochures (168 Awamedica, 151 Pioneer) containing 22,940 aligned sentence pairs, then trained a Statistical Machine Translation model using the Moses toolkit. The model achieved BLEU scores ranging from 22.65 to 48.93 across seven experiments, with post-processing using a medical dictionary further improving scores to 56.87, 31.05, and 40.01 for three new brochures. Human evaluation by native Kurdish-speaking pharmacists, physicians, and medicine users showed that 83.3% found the translations accurate, while 66.7% of users felt confident using the medications based on the translations.

## Method Summary
The researchers developed a domain-specific SMT system using the Moses toolkit with Giza++ alignment and KenLM language models. They created a parallel corpus through OCR extraction from PDF/AI files, manual correction, and InterText alignment. Seven corpus variants were tested, including original, shuffled, XML-tagged, undersampled, and oversampled versions. Post-processing addressed unknown words through a custom medical dictionary with expert consultation and Google Cloud Translation API fallback. The system was evaluated using BLEU scores and human assessment via 4-point Likert scales by nine native Kurdish-speaking medical professionals and users.

## Key Results
- BLEU scores ranged from 22.65 to 48.93 across seven experiments, with oversampling achieving the highest score of 48.93
- Post-processing with medical dictionary improved BLEU scores to 56.87, 31.05, and 40.01 for three new brochures
- Human evaluation showed 83.3% accuracy rating, 66.7% user confidence, and 50% consistency rating from medical professionals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Oversampling shorter brochures to balance category lengths improves SMT performance for low-resource domain translation.
- Mechanism: Experiment 7 (oversampling) achieved the highest BLEU score of 48.93 by duplicating sentences from shorter brochures to match the longest brochure within each category, providing more balanced exposure across medication categories.
- Core assumption: Category-level balance matters more than avoiding duplicate training examples for phrase-table quality in SMT.
- Evidence anchors:
  - [section 5.3.1] "the oversampling technique in the seventh experiment achieved the highest score of 48.93"
  - [section 5.2.4] "we used the oversampling technique to enlarge the shorter brochures by duplicating sentences, ensuring consistent length across all brochures within each category"
- Break condition: If test data contains entirely new medication categories not represented in training, oversampling benefits may not generalize.

### Mechanism 2
- Claim: Domain-specific post-processing with medical dictionaries significantly reduces unknown-word errors in translated output.
- Mechanism: Raw SMT output for three new brochures produced low BLEU scores (25.79, 6.46, 12.25) due to out-of-vocabulary medical terms. After replacing unknown words using a custom medical dictionary, scores improved to 56.87, 31.05, and 40.01 respectively.
- Core assumption: Unknown words are primarily domain-specific terminology rather than general vocabulary gaps or OCR errors.
- Evidence anchors:
  - [abstract] "We addressed unknown words through post-processing with a medical dictionary, resulting in BLEU scores of 56.87, 31.05, and 40.01"
  - [section 5.6] "The initial translations revealed issues such as unknown words... To improve the translations, we performed post-processing steps, including replacing unknown words with equivalents from a medical dictionary"
- Break condition: If unknown words arise from morphological variations rather than missing vocabulary entries, simple dictionary substitution will fail.

### Mechanism 3
- Claim: Professional human evaluation correlates moderately with BLEU improvements but captures distinct quality dimensions.
- Mechanism: Despite BLEU scores post-editing ranging from 31.05 to 56.87, human evaluators rated 83.3% of translations as accurate, but only 50% rated them as consistent. Users (66.7%) found translations understandable and expressed confidence in medication use.
- Core assumption: Native-speaking medical professionals can reliably assess translation adequacy for patient safety information.
- Evidence anchors:
  - [abstract] "Human evaluation by native Kurdish-speaking pharmacists, physicians, and medicine users showed that 50% of professionals found the translations consistent, while 83.3% rated them accurate"
  - [section 5.6] "nine native Kurdish-speaking assessors, including pharmacists, physicians, and medicine users" used "a 4-point Likert scale"
- Break condition: If evaluators lack domain expertise or if Likert scales lack calibration anchors, subjective ratings may not reflect actual usability.

## Foundational Learning

- Concept: **Phrase-Based Statistical Machine Translation (PB-SMT)**
  - Why needed here: Moses engine uses phrase tables mapping source phrases to target equivalents via Giza++ alignment. Understanding this is prerequisite to interpreting why oversampling affects phrase-table coverage.
  - Quick check question: Can you explain why SMT phrase tables might benefit from duplicate training sentences, unlike neural approaches?

- Concept: **BLEU Score Limitations**
  - Why needed here: The paper relies on BLEU for automatic evaluation, but Kurdish morphology (agglutinative, rigid word-matching) makes BLEU conservative. Scores of 48.93 may underrepresent actual usability.
  - Quick check question: Why might BLEU penalize morphologically correct Kurdish translations that differ in surface form from reference?

- Concept: **OCR Error Propagation**
  - Why needed here: The corpus was built using OCR tools (PDF2Go, i2OCR) with known Kurdish text extraction issues—extra symbols, Arabic character confusion, incorrect line breaks. These errors propagate into training data noise.
  - Quick check question: What preprocessing step would you add to detect systematic OCR substitution errors (e.g., Arabic ك vs Kurdish ک)?

## Architecture Onboarding

- Component map:
  Data Pipeline: PDF/AI files → OCR extraction → Manual correction → InterText alignment → Corpus versions (7 variants)
  Training Pipeline: Tokenization → Truecasing → Giza++ word alignment → KenLM language model → Moses phrase table → Decoder
  Post-Processing: Raw translation → Unknown word detection → Medical dictionary lookup → Expert/Google API fallback → Final output
  Evaluation: BLEU automatic scoring + Human Likert evaluation (professionals + users)

- Critical path:
  1. Corpus quality (manual OCR correction: 2-3 hours per brochure)
  2. Sentence alignment accuracy (Hunalign + manual review)
  3. Oversampling strategy (Experiment 7 configuration)
  4. Dictionary coverage for post-processing unknown terms

- Design tradeoffs:
  - **SMT vs. NMT**: Paper uses Moses SMT; neighboring research (Ahmadi & Masoud, Amini et al.) shows NMT achieving 16.81-22.72 BLEU for Kurdish-English. SMT chosen likely for interpretability and lower data requirements.
  - **Oversampling vs. undersampling**: Oversampling (32,784 lines) outperformed undersampling (16,767 lines), suggesting data quantity outweighs duplication concerns.
  - **Manual vs. automatic post-editing**: Current approach requires manual dictionary consultation; paper notes future work to automate with probability-based models.

- Failure signatures:
  - **Very low BLEU on new brochures** (<15): Indicates unknown vocabulary—check dictionary coverage.
  - **Broken sentence structure**: SMT reordering failures—consider adding rule-based post-processing.
  - **Arabic character contamination**: OCR introduced Arabic variants of shared letters; requires Unicode normalization.
  - **Untranslated sections**: 90% of Awamedica brochures missing "drug interactions"— corpus gaps, not model failure.

- First 3 experiments:
  1. Reproduce Experiment 7 (oversampling) on 90/10 split with Moses baseline; confirm ~48 BLEU on held-out test set.
  2. Add three new brochures from unseen medication categories; measure BLEU drop and characterize unknown-word types (terminology vs. morphology vs. OCR artifacts).
  3. Build automated dictionary lookup pipeline using the paper's medical dictionary format; compare manual vs. automatic post-editing BLEU deltas to quantify human-in-the-loop value.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would neural machine translation (NMT) approaches outperform the statistical machine translation (SMT) model for English-to-Sorani Kurdish medical brochure translation?
- Basis in paper: [explicit] The authors state that "leveraging state-of-the-art neural network approaches could further boost performance, especially with larger datasets."
- Why unresolved: The study only implemented SMT using Moses; no NMT comparison was conducted despite NMT being the dominant paradigm in recent MT research.
- What evidence would resolve it: Train NMT models (e.g., Transformer-based) on the same corpus and compare BLEU scores and human evaluation results against the SMT baseline.

### Open Question 2
- Question: Can the post-editing process for unknown words be automated using probability-based models rather than manual dictionary lookups?
- Basis in paper: [explicit] The authors identify "automating the post-editing process with probability-based models" as a future direction to handle unknown words more contextually.
- Why unresolved: The current study relied on manual post-processing with a self-created medical dictionary and Google Cloud Translation API for unresolved cases.
- What evidence would resolve it: Develop and evaluate an automated post-editing module that predicts appropriate translations for unknown words based on context and probability distributions.

### Open Question 3
- Question: How does the domain-specific SMT system compare to general-purpose translation platforms like Google Translate or Claude AI for medical brochure translation?
- Basis in paper: [explicit] The authors state that "evaluating and comparing the current system with existing translation platforms, such as Google Translator and Claude AI, could provide valuable insights into its effectiveness."
- Why unresolved: No comparative evaluation with existing commercial or open-source translation systems was conducted in this study.
- What evidence would resolve it: Translate the same test brochures using Google Translate and Claude AI, then compare BLEU scores and human evaluation ratings across all systems.

### Open Question 4
- Question: Would increasing the corpus size beyond 22,940 sentence pairs lead to diminishing returns or continued improvements in translation quality?
- Basis in paper: [inferred] The corpus is described as limited, and BLEU scores showed substantial variance (22.65–48.93) across experiments, suggesting data sparsity affects model stability.
- Why unresolved: The study does not analyze the relationship between corpus size and translation quality thresholds or saturation points.
- What evidence would resolve it: Conduct experiments with incrementally larger corpus sizes (e.g., 50K, 100K sentence pairs) and measure BLEU score trajectories to identify performance plateaus.

## Limitations
- The 22,940-sentence parallel corpus is not publicly available, preventing independent replication.
- OCR quality varies by tool, introducing inconsistent noise into training data with Arabic character contamination and line break errors.
- BLEU evaluation may understate translation adequacy for Kurdish's agglutinative morphology.
- Human evaluation uses small samples (n=9) and lacks standardized calibration for Likert scales.

## Confidence
- **High Confidence**: Mechanism 1 (oversampling improves BLEU) - directly supported by experiment results showing 48.93 vs 26.73 BLEU.
- **Medium Confidence**: Mechanism 2 (dictionary post-processing reduces unknown words) - supported by BLEU improvements (25.79→56.87), but dictionary size and coverage unknown.
- **Low Confidence**: Mechanism 3 (human evaluation correlates with BLEU) - human ratings show moderate correlation (83.3% accuracy vs 50% consistency), but small n and lack of BLEU-human correlation thresholds for Kurdish medical text limit inference.

## Next Checks
1. **Corpus Replication Test**: Train Moses SMT on substitute Kurdish-English corpus (e.g., Awta 229K pairs), apply Sorani tokenization and truecasing, and measure baseline BLEU. Compare to reported 48.93 to assess domain adaptation impact.

2. **Unknown-Word Classification**: For three new brochures, categorize unknown words into (a) domain terminology, (b) morphological variants, (c) OCR artifacts. Apply targeted fixes (dictionary, morphological rules, character normalization) and measure individual BLEU contributions.

3. **Human-BLEU Correlation Study**: Recruit 10+ native Kurdish medical professionals to evaluate translations using calibrated Likert scales. Compute correlation between BLEU scores and human ratings to establish validation thresholds for medical Kurdish MT.