---
ver: rpa2
title: 'Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based
  Queries'
arxiv_id: '2508.00679'
source_url: https://arxiv.org/abs/2508.00679
tags:
- legal
- retrieval
- documents
- case
- rhetorical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of realistic legal case retrieval
  under limited information by using rhetorical role-based queries instead of full
  case documents. The proposed TraceRetriever pipeline integrates hierarchical BiLSTM-CRF
  for rhetorical role segmentation, semantic vector search, BM25 retrieval on candidate
  subsets, Reciprocal Rank Fusion, and cross-encoder re-ranking.
---

# Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries

## Quick Facts
- arXiv ID: 2508.00679
- Source URL: https://arxiv.org/abs/2508.00679
- Reference count: 5
- Top MAP of 0.3783 and MRR of 0.3924 when using Facts+Issue+Reasoning queries

## Executive Summary
This paper addresses the challenge of realistic legal case retrieval under limited information by using rhetorical role-based queries instead of full case documents. The proposed TraceRetriever pipeline integrates hierarchical BiLSTM-CRF for rhetorical role segmentation, semantic vector search, BM25 retrieval on candidate subsets, Reciprocal Rank Fusion, and cross-encoder re-ranking. Evaluated on IL-PCR and COLIEE 2025 datasets, the approach improves retrieval effectiveness by filtering noise through targeted query construction and demonstrates that semantic vector methods consistently outperform BM25 in precision while cross-encoders achieve higher recall at higher computational cost.

## Method Summary
The TraceRetriever pipeline begins with hierarchical BiLSTM-CRF classification to segment legal documents into rhetorical roles (Facts, Issue, Arguments, Reasoning, Decision). Queries are constructed using specific role combinations (e.g., Facts+Issue+Reasoning) and embedded using Snowflake Arctic Embed v2.0 for dense vector retrieval in Milvus. BM25 retrieval is applied to top candidates from the vector search, and results are fused using Reciprocal Rank Fusion. A cross-encoder re-ranker (bge-reranker-v2-m3) then re-ranks the fused candidates with chunking for long documents. The approach targets realistic partial-information scenarios common in legal practice.

## Key Results
- Targeted rhetorical role queries (Facts+Issue+Reasoning) outperform full-document inputs with MAP improving from 0.3484 to 0.3783
- Semantic vector retrieval (Snowflake Arctic Embed v2.0) achieves MAP of 0.3783 and MRR of 0.3924, significantly outperforming BM25 (MAP 0.2081)
- Cross-encoders achieve higher recall (0.2815 at k=11) but require larger candidate sets than bi-encoder vector search

## Why This Works (Mechanism)

### Mechanism 1
Targeted rhetorical role queries outperform full-document inputs by reducing noise while preserving legally discriminative signals. The HierBiLSTM-CRF classifier segments documents into rhetorical roles, and querying with only Facts+Issue+Reasoning filters out procedural/boilerplate text that dilutes semantic embeddings, improving signal-to-noise ratio in dense retrieval. Assumption: Rhetorical roles identified by the classifier correlate with retrieval-relevance; the classifier trained on Indian judgments generalizes within the same legal system. Evidence: Vector DB MAP improved from 0.3484 (Full Query) to 0.3783 (Facts+Issue+Reasoning).

### Mechanism 2
Semantic vector retrieval consistently outperforms BM25 in precision for legal case retrieval. Dense embeddings (Snowflake Arctic Embed v2.0, 768-dim) capture semantic similarity beyond lexical overlap, ranking relevant cases higher at lower k. BM25 relies on term frequency and fails on nuanced legal semantics where key concepts are paraphrased. Assumption: Embedding model pretraining includes sufficient legal domain exposure; legal relevance aligns with semantic similarity. Evidence: Vector DB achieves MAP 0.3783, MRR 0.3924 vs BM25 MAP 0.2081 for Facts+Issue+Reasoning.

### Mechanism 3
Cross-encoders achieve higher recall but require larger candidate sets than bi-encoder vector search. Cross-encoders jointly encode query-document pairs, capturing fine-grained interactions at O(n) inference cost per pair. Used only for re-ranking a fused candidate set, they recover relevant cases missed by initial retrieval at the cost of lower precision at top ranks. Assumption: Cross-encoder model (bge-reranker-v2-m3) generalizes to legal query-document pairs without domain-specific fine-tuning. Evidence: Cross-encoder recall 0.2815 at k=11 vs Vector DB recall 0.2088 at k=5 for Facts+Issue+Reasoning.

## Foundational Learning

- Concept: Rhetorical Role Labeling
  - Why needed here: Legal judgments have functional segments; knowing which sentences are Facts vs Reasoning enables query construction from partial information.
  - Quick check question: Can you list 4 rhetorical roles and explain why Facts alone may be insufficient for retrieval?

- Concept: Dense vs Sparse Retrieval (Bi-encoder vs BM25)
  - Why needed here: The pipeline fuses both; understanding their failure modes guides parameter tuning.
  - Quick check question: Why might BM25 fail when a query uses "deduction under Section 80IA" while the precedent discusses "tax exemption eligibility"?

- Concept: Reciprocal Rank Fusion (RRF)
  - Why needed here: Combines ranked lists from Vector DB and BM25 into a single ordering before cross-encoder re-ranking.
  - Quick check question: Given ranks [1, 5] and [3, 2] from two methods for document A and B respectively, compute RRF scores with k=60 and determine the final order.

## Architecture Onboarding

- Component map: HierBiLSTM-CRF -> Rhetorical Role Labels -> Milvus Vector DB (Snowflake Arctic Embed v2.0) -> BM25 on top-1000 -> RRF Fusion -> Cross-encoder (bge-reranker-v2-m3) -> Final Ranked List

- Critical path: Query → Rhetorical segmentation → Role-filtered query embedding → Vector DB retrieval (top-1000) → BM25 on candidates → RRF fusion → Cross-encoder re-ranking → Final ranked list

- Design tradeoffs:
  - Precision vs recall: Vector DB favors precision (lower k), cross-encoder favors recall (higher k)
  - Computational cost: Cross-encoder re-ranking is expensive; limiting candidate pool reduces latency but risks missing relevant cases
  - Generalization: Rhetorical classifier trained on Indian law; performance on COLIEE (Canadian) degraded (MAP 0.1695 vs 0.3783 on IL-PCR)

- Failure signatures:
  - Low MAP with Facts-only: semantic sparsity; add Issue or Reasoning
  - BM25 dramatically underperforms: check term overlap, consider hybrid weight adjustment
  - Cross-encoder recall gains disappear: verify candidate pool size and chunking strategy for long documents

- First 3 experiments:
  1. Ablation on rhetorical role combinations: Compare Facts, Facts+Issue, Facts+Issue+Reasoning to confirm optimal query configuration on validation split.
  2. RRF hyperparameter sweep: Vary k (40, 60, 80) and compare MAP/MRR to identify fusion stability.
  3. Candidate pool size impact: Test vector retrieval top-k at [100, 500, 1000] before BM25 to measure recall-latency tradeoff.

## Open Questions the Paper Calls Out

- How can rhetorical role-based retrieval frameworks be adapted to generalize across different legal jurisdictions and languages? The authors state in the Conclusion, "We also aim to explore cross-lingual and multi-domain retrieval," prompted by the observation that their classifier trained on Indian judgments failed to generalize to the Canadian COLIEE dataset.

- Can optimization techniques like pruning or quantization maintain retrieval performance while reducing the computational overhead of dense retrievers and cross-encoders? The Limitations section notes that "computational complexity... can hinder scalability" and explicitly suggests "optimization techniques such as pruning or quantization" as necessary future work.

- How can retrieval robustness be improved when query inputs are limited to sparse segments like Facts or Issue? The Conclusion lists "improving retrieval robustness under sparse queries" as a primary goal for future work.

## Limitations

- Rhetorical role generalization: The HierBiLSTM-CRF classifier is trained only on Indian judgments and may not generalize to other jurisdictions (e.g., COLIEE 2025 Canadian data shows MAP drops from 0.3783 to 0.1695).

- Hybrid retrieval hyperparameter opacity: The paper specifies using RRF but never reports the exact k constant used, making it impossible to precisely replicate the fusion behavior.

- Domain dependency of embedding models: The Snowflake Arctic Embed v2.0 model's performance on legal text is assumed to be sufficient, but no ablation or comparison with other embeddings is provided to confirm this assumption.

## Confidence

- High confidence: The claim that targeted rhetorical role queries (Facts+Issue+Reasoning) outperform full-document inputs is supported by strong empirical evidence (MAP 0.3783 vs 0.3484).

- Medium confidence: The superiority of dense retrieval over BM25 in precision is demonstrated, but the evidence is limited to a single embedding model and dataset.

- Low confidence: The claim that cross-encoders achieve higher recall is based on observed results, but the mechanism is under-specified (chunking strategy, context truncation risks) and lacks comparative analysis with alternative re-rankers.

## Next Checks

1. Ablation on rhetorical role combinations: Systematically test Facts, Facts+Issue, and Facts+Issue+Reasoning configurations on a held-out validation set to confirm the optimal query configuration and sensitivity to role selection.

2. RRF hyperparameter sweep: Vary the RRF constant k (e.g., 40, 60, 80) and evaluate MAP/MRR to identify the optimal fusion parameter and assess stability across retrieval methods.

3. Cross-encoder chunking strategy validation: Experiment with different chunk sizes and overlap settings for long legal documents, measuring recall at various candidate pool sizes to quantify the tradeoff between computational cost and retrieval effectiveness.