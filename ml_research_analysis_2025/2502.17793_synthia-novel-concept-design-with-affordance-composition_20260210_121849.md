---
ver: rpa2
title: 'SYNTHIA: Novel Concept Design with Affordance Composition'
arxiv_id: '2502.17793'
source_url: https://arxiv.org/abs/2502.17793
tags:
- concept
- concepts
- novel
- affordances
- affordance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SYNTHIA, a framework for generating novel,
  functionally coherent designs by integrating desired affordances. The key idea is
  to leverage a hierarchical concept ontology that decomposes concepts into parts
  and affordances, and use a curriculum learning scheme to progressively fine-tune
  T2I models on increasingly complex affordance compositions.
---

# SYNTHIA: Novel Concept Design with Affordance Composition

## Quick Facts
- **arXiv ID:** 2502.17793
- **Source URL:** https://arxiv.org/abs/2502.17793
- **Reference count:** 40
- **Primary result:** Novel framework integrating affordances into text-to-image generation achieves 25.1% novelty and 14.7% functional coherence gains over state-of-the-art models

## Executive Summary
SYNTHIA introduces a framework for generating novel, functionally coherent designs by leveraging a hierarchical concept ontology and curriculum learning. The key insight is that structured decomposition of concepts into parts and affordances enables models to learn functional relationships rather than superficial visual correlations. By progressively fine-tuning T2I models on increasingly complex affordance compositions, SYNTHIA achieves significant improvements in both novelty and functional coherence, outperforming state-of-the-art models in human evaluation.

## Method Summary
SYNTHIA operates by first constructing a hierarchical concept ontology that decomposes concepts into superordinate categories, concepts, parts, and affordances. It then generates training data by sampling affordance pairs with varying distances, using GPT-4o for caption generation and DALL-E for image synthesis. The framework employs a three-stage curriculum learning scheme that progressively exposes the model to increasingly distant affordance pairs, using contrastive fine-tuning on Kandinsky 3.0 with triplet loss to balance novelty and functional coherence. The approach maintains visual novelty while ensuring the generated designs align with desired affordances.

## Key Results
- 25.1% gain in novelty over state-of-the-art T2I models in human evaluation
- 14.7% gain in functional coherence compared to baseline models
- Effective curriculum learning that progressively improves both novelty and coherence scores

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Ontology Grounding
The four-level ontology (Superordinate → Concept → Parts → Affordances) explicitly links affordances to specific parts, providing "functional grounding" that allows the model to retrieve relevant parts based on affordances rather than randomly combining visual features. This enables generation to be well-grounded on functionality rather than superficial visual feature combinations.

### Mechanism 2: Affordance Distance Curriculum
The curriculum samples affordance pairs with increasing distance (computed via Jaccard similarity on affordance sets + BERT semantic similarity). Stage 1 uses close pairs to reinforce fundamentals; Stage 3 uses distant pairs to force novel integration. This prevents the model from defaulting to existing concepts when faced with unfamiliar combinations.

### Mechanism 3: Contrastive Push-Away from Existing Concepts
The triplet loss with positive (affordance) and negative (existing concept) constraints forces novel outputs while preserving functional coherence. The model learns to generate designs that satisfy affordances while avoiding retrieval of familiar objects, balancing the tradeoff between novelty and coherence.

## Foundational Learning

- **Affordance Theory (Gibson)**: Why needed here: Understanding affordances as "functionality offered by an object or its parts" is essential for grasping why structured affordance composition differs from visual style transfer. Quick check: Can you explain why "brew" and "deliver" are affordances rather than visual attributes?

- **Curriculum Learning (Bengio et al.)**: Why needed here: The three-stage training scheme assumes you understand why easy-to-hard progression helps models generalize to complex compositions they've never seen. Quick check: Why might random training on distant affordance pairs fail where curriculum succeeds?

- **Diffusion Model Fine-Tuning (LoRA/DreamBooth)**: Why needed here: The method fine-tunes Kandinsky's UNet while freezing other components; understanding what gets updated vs. frozen is critical for implementation. Quick check: What catastrophic forgetting risks exist when fine-tuning T2I models on small datasets?

## Architecture Onboarding

- **Component map:** Hierarchical Ontology (O = S,C,P,A) -> Affordance Sampling (distance metric D_A) -> Curriculum Construction (3 stages by distance) -> Training Data Generation (GPT-4o captions → DALL-E images → CLIP filtering) -> Contrastive Fine-Tuning (Kandinsky 3.0 UNet, triplet loss) -> Inference (affordance-only prompt)

- **Critical path:** Ontology construction is the foundation—without accurate concept-part-affordance links, downstream components fail. Distance metric calibration determines curriculum quality. Pseudo-novel image quality directly affects fine-tuning effectiveness.

- **Design tradeoffs:** Training data size: 600 pairs optimal; more data didn't improve results. γ parameter: too high sacrifices coherence, too low sacrifices novelty. Ontology coverage: 590 concepts provides reasonable coverage but limits domain applicability.

- **Failure signatures:** Multiple disjoint objects instead of fused concept → model failed to learn affordance composition. Outputs resembling existing concepts → negative constraint insufficient or curriculum skipped easy stages. Missing affordance functionality → pseudo-novel training images didn't clearly encode that affordance.

- **First 3 experiments:** 1) Ontology validation: Sample 20 affordance pairs, manually verify concept-part-affordance links are accurate and non-redundant. 2) Distance metric sanity check: Visualize affordance pair distances; confirm close pairs cluster below 0.4 and distant pairs exceed 0.7. 3) Ablation without curriculum: Replicate Table 7 comparison—train with randomly shuffled data vs. curriculum on same 600 pairs, expect ~10-15% coherence drop.

## Open Questions the Paper Calls Out

### Open Question 1
How can the hierarchical concept ontology be extended to cover a broader range of real-world categories without introducing noise that disrupts the affordance composition curriculum? The authors state the current ontology "does not cover every plausible concept category" and suggest follow-up works should explore "constructing a more diverse, richer concept ontology." Success would be demonstrated by applying SYNTHIA to an expanded ontology with new superordinate categories while maintaining or improving the 14.7% functional coherence gain.

### Open Question 2
To what extent can automated evaluators (e.g., LLM-as-a-Judge) reliably assess the subjective "novelty" of a design compared to human intuition? The paper acknowledges that evaluation "inherently relies on the human intuition to evaluate the novelty" and while LLMs are used to alleviate bias, the question persists. Resolution would come from a correlation study between human expert rankings and LLM scores on adversarially generated concepts.

### Open Question 3
How does SYNTHIA's performance scale when required to integrate significantly more than four affordances into a single object? Ablation studies show a "slight performance drop" when moving from 2 to 3 or 4 affordances, but performance on complex compositions (5+ affordances) is untested. Evidence would come from evaluating generated concepts utilizing 5 or more disparate affordances and measuring the retention rate of each individual function.

## Limitations
- Ontology coverage limited to 590 concepts, explicitly not encompassing every plausible category
- Data generation quality depends on external systems (GPT-4o, DALL-E) that may not accurately represent affordance combinations
- Implementation complexity with multiple hyperparameters requiring careful calibration (γ, distance metric weights, curriculum stages)

## Confidence
- **High confidence:** Novelty and coherence improvements (human evaluation, controlled ablations)
- **Medium confidence:** Curriculum learning effectiveness (novel methodology, no direct corpus comparison)
- **Medium confidence:** Ontology-grounded generation mechanism (well-reasoned but requires ontology construction)

## Next Checks
1. **Ontology coverage test:** Measure concept retrieval accuracy on a held-out set of common objects to quantify ontology limitations
2. **Cross-domain generalization:** Apply SYNTHIA to a different domain (e.g., furniture instead of household appliances) to assess scalability
3. **Ablation on distance metric:** Compare performance when using only semantic similarity vs. full Jaccard + BERT distance to isolate distance metric contribution