---
ver: rpa2
title: 'SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot
  Framework for Medical Diagnosis Using Contrastive Representations'
arxiv_id: '2509.20567'
source_url: https://arxiv.org/abs/2509.20567
tags:
- medical
- learning
- languages
- swasthllm
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SwasthLLM, a unified cross-lingual, multi-task,
  and meta-learning framework for medical diagnosis that operates effectively across
  English, Hindi, and Bengali without requiring language-specific fine-tuning. At
  its core, SwasthLLM leverages the multilingual XLM-RoBERTa encoder augmented with
  a language-aware attention mechanism and a disease classification head, enabling
  the model to extract medically relevant information regardless of the language structure.
---

# SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations

## Quick Facts
- arXiv ID: 2509.20567
- Source URL: https://arxiv.org/abs/2509.20567
- Reference count: 24
- Primary result: 97.22% accuracy and 97.17% F1 in supervised settings; 92.78% Hindi and 73.33% Bengali zero-shot accuracy

## Executive Summary
SwasthLLM is a unified cross-lingual medical diagnosis framework that achieves strong zero-shot performance across English, Hindi, and Bengali without language-specific fine-tuning. The model combines XLM-RoBERTa with language-aware attention, Siamese contrastive learning for cross-lingual alignment, and Model-Agnostic Meta-Learning (MAML) for rapid adaptation to new languages. Extensive experiments show the model achieves 97.22% accuracy in supervised settings and maintains strong performance in zero-shot scenarios, with 92.78% accuracy on Hindi and 73.33% on Bengali medical text. The framework's phased training approach emphasizes robust representation alignment before task-specific fine-tuning.

## Method Summary
SwasthLLM leverages a multilingual XLM-RoBERTa encoder augmented with language-aware attention to extract medically relevant information across languages. The core innovation is a Siamese contrastive learning module that aligns semantically equivalent medical texts across languages into a shared embedding space, ensuring language-invariant representations. A translation consistency module and contrastive projection head reinforce this alignment. The model is trained using a four-phase pipeline: (1) contrastive pretraining for cross-lingual alignment, (2) supervised classification, (3) joint multi-task learning optimizing disease classification, translation alignment, and contrastive objectives, and (4) MAML for rapid adaptation to unseen languages. The framework uses balanced loss weighting (α=1.0 classification, β=0.5 translation, γ=0.8 contrastive) and achieves strong performance across all evaluation metrics.

## Key Results
- Supervised setting: 97.22% accuracy and 97.17% F1-score on medical diagnosis
- Zero-shot Hindi: 92.78% accuracy, demonstrating strong cross-lingual transfer
- Zero-shot Bengali: 73.33% accuracy, showing effective generalization to low-resource languages
- Contrastive learning provides largest single improvement (+4.6% F1) in Bengali zero-shot
- MAML adds 2.2% F1 gain for zero-shot Bengali at ~25% training overhead
- Language-aware attention reduces false negatives by 30% compared to mBERT

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Cross-Lingual Alignment
A Siamese contrastive learning module projects XLM-RoBERTa embeddings through an MLP into a 128-dimensional shared space, using NT-Xent loss to pull parallel translations together while pushing non-aligned samples apart. Translation consistency loss reinforces this alignment, forcing language-invariant representations where disease semantics transcend script differences. This requires parallel medical corpora during training and assumes semantic equivalence is preserved across translations. The ablation shows contrastive learning delivers +4.6% F1 gain—the largest single-stage improvement in Bengali zero-shot.

### Mechanism 2: Language-Aware Attention for Medical Term Salience
A learned attention head computes scalar importance scores for each token embedding via tanh-activated MLP, producing weighted sentence representations that allow the model to focus on symptom descriptors regardless of word order differences between languages. This assumes medical diagnostic information is concentrated in specific tokens rather than distributed across the full sequence. The model achieves 30% fewer false negatives compared to mBERT, indicating superior reliability in clinical scenarios, though attention may dilute across competing symptoms in multi-disease narratives.

### Mechanism 3: Meta-Learning for Rapid Language Adaptation
MAML preconditions model parameters for fast adaptation to unseen languages or rare diseases with minimal gradient steps through two-level optimization—inner loop adapts parameters on support sets per language/task, outer loop updates parameters based on query set performance. This simulates few-shot scenarios during training, teaching the model to learn new language patterns quickly. MAML adds ~25% training time overhead but provides +2.2% F1 gain in zero-shot Bengali, with optimal performance when inner-loop learning rate is properly tuned.

## Foundational Learning

- **Transformer Attention and Multilingual Encoders (XLM-RoBERTa)**: Understanding self-attention, tokenization (SentencePiece), and cross-lingual transfer is prerequisite as the entire architecture builds on XLM-RoBERTa's pretrained multilingual representations. Quick check: Can you explain why [CLS] token embeddings are used as sentence representations, and how XLM-R handles 100+ languages in a shared vocabulary?

- **Contrastive Learning (Siamese Networks, NT-Xent Loss)**: The core alignment mechanism relies on contrastive objectives, requiring understanding of positive/negative pair construction, temperature scaling, and embedding space geometry. Quick check: Given two parallel sentences in English and Hindi, how would NT-Xent loss treat them versus non-parallel sentences in the same batch?

- **Meta-Learning (MAML Inner/Outer Loops)**: Phase 4 training uses MAML for rapid adaptation, requiring understanding of bi-level optimization, support/query splits, and meta-gradient computation for debugging convergence. Quick check: In MAML, what happens to gradients from the inner loop during the outer loop update, and why does this enable fast adaptation?

## Architecture Onboarding

- **Component map:**
  Input (x^(l)) → XLM-RoBERTa Encoder (E_θ) → [CLS] embedding (h^(l)) → Language-Aware Attention (z = Σα_i h_i) → Classification Head + Contrastive Projection + Translation Consistency → Multi-task Losses

- **Critical path:**
  1. Contrastive pretraining (Phase 1) must converge before classification (Phase 2)—misaligned embeddings harm downstream disease prediction
  2. Multi-task joint training (Phase 3) requires careful loss balancing (α=1.0, β=0.5, γ=0.8); misweighting causes task dominance
  3. MAML Phase 4 depends on well-initialized parameters from Phases 1-3; starting meta-learning early wastes compute

- **Design tradeoffs:**
  - Projection dimension (128) vs. encoder dimension (768): Lower projection compresses information but speeds contrastive training
  - Temperature τ=0.07: Low temperature sharpens distributions but may cause gradient instability
  - MAML adds 25% training overhead for +2.2% F1 gain—worthwhile for zero-shot but may be overkill for supervised-only deployments

- **Failure signatures:**
  - Bengali zero-shot accuracy drops >15% below Hindi: Contrastive alignment failed; check parallel corpus quality for Bengali
  - High accuracy but 30%+ false negatives on specific diseases: Language-aware attention may be ignoring key symptom tokens; inspect attention weights
  - MAML query loss increases while support loss decreases: Inner-loop overfitting; reduce η or increase support set diversity

- **First 3 experiments:**
  1. **Ablation reproduction**: Train V1 (base XLM-R) through V4 (full SwasthLLM) on English-only training, evaluate zero-shot Hindi/Bengali. Verify Table IV gains (+4.6%, +2.8%, +2.2%) to confirm each component works
  2. **Attention visualization**: Extract α_i weights for 10 Hindi/Bengali test samples. Check if high-attention tokens correspond to medical terms; if not, attention MLP may need deeper layers or ReLU→GELU switch
  3. **Temperature sweep**: Retrain contrastive module with τ ∈ {0.03, 0.05, 0.07, 0.1, 0.2}. Plot Bengali F1 vs. τ to find optimal temperature for this specific dataset scale (1200 samples)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating explicit entity linking or symptom-disease graph priors effectively mitigate attention dilution in long-form medical narratives containing multiple comorbidities?
- Basis in paper: The Discussion section notes that while the language-aware attention mechanism works well for single-disease cases, attention scores are sometimes diluted in multi-disease scenarios, and the authors suggest graph priors as a potential improvement
- Why unresolved: The current architecture and evaluation focus primarily on concise symptom descriptions mapped to single disease labels, leaving multi-label diagnostic capabilities unverified
- What evidence would resolve it: A comparative performance analysis (F1-score) on a dataset specifically composed of complex, multi-comorbidity patient records

### Open Question 2
- Question: To what extent does domain-specific monolingual pretraining improve the alignment of rare idiomatic expressions and culturally specific symptom descriptions in low-resource languages?
- Basis in paper: The authors identify the translation module's struggle with idiomatic expressions in Bengali as a source of error and propose incorporating monolingual pretraining on domain-specific corpora as a solution
- Why unresolved: The current framework relies on general multilingual embeddings (XLM-RoBERTa) which may not capture rare, culturally specific medical nuances without further domain alignment
- What evidence would resolve it: An ablation study showing zero-shot accuracy improvements on a benchmark of idiomatic medical texts with and without the proposed monolingual pretraining phase

### Open Question 3
- Question: How effectively does the meta-learning framework generalize to low-resource languages that are structurally distinct from the Indo-European languages tested?
- Basis in paper: While the Conclusion proposes expanding linguistic coverage, the study is restricted to English, Hindi, and Bengali (all Indo-European); the model's ability to handle morphologically divergent languages (e.g., Dravidian or Sino-Tibetan) remains unproven
- Why unresolved: The zero-shot success reported may be inflated by the structural similarities between the training (English/Hindi) and target (Bengali) languages
- What evidence would resolve it: Evaluation of zero-shot transfer performance on a non-Indo-European low-resource language without retraining the core architecture

## Limitations

- Limited dataset scale (1200 samples) raises questions about generalization to more diverse clinical scenarios and additional languages
- Contrastive learning fundamentally depends on parallel medical text across languages, but translation quality and semantic preservation are not quantified
- MAML adds significant computational overhead (~25%) for modest performance gains, raising cost-effectiveness concerns for practical deployment

## Confidence

**High Confidence:**
- Overall framework architecture and training methodology are clearly specified and internally consistent
- Four-phase training pipeline (contrastive pretraining → classification → joint multi-task → MAML) is well-defined
- Reported supervised performance (97.22% accuracy, 97.17% F1) on the evaluation dataset

**Medium Confidence:**
- Cross-lingual zero-shot performance on Hindi (92.78% F1) is supported by ablation studies showing progressive improvements
- Contrastive learning provides the largest single improvement (+4.6% F1), consistent with the mechanism's design
- Language-aware attention reduces false negatives by 30% compared to mBERT

**Low Confidence:**
- Generalization to languages beyond Hindi and Bengali without additional validation
- Performance on highly imbalanced disease distributions or more diverse clinical presentations
- Long-term stability and maintenance requirements for production deployment

## Next Checks

1. **Translation quality validation**: Manually audit 50 random parallel sentence pairs across all three languages to assess translation accuracy and semantic preservation. Measure correlation between translation quality scores and Bengali zero-shot performance to quantify the impact of parallel corpus quality on contrastive alignment.

2. **Attention mechanism interpretability**: Visualize attention weights for 20 diverse medical text samples (10 Hindi, 10 Bengali). Annotate which tokens receive highest attention and verify they correspond to clinically relevant symptoms/descriptors. Compare attention patterns between correctly classified and misclassified samples to identify failure modes.

3. **Dataset scale sensitivity analysis**: Retrain the model using progressively smaller subsets of the training data (25%, 50%, 75%, 100%) and evaluate zero-shot performance on Bengali. Plot F1-score vs. training data size to determine the data efficiency threshold and identify whether the contrastive learning component provides disproportionate benefits at smaller scales.