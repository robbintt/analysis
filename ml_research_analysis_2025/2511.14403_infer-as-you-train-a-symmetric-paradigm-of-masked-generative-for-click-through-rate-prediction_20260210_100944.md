---
ver: rpa2
title: 'Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through
  Rate Prediction'
arxiv_id: '2511.14403'
source_url: https://arxiv.org/abs/2511.14403
tags:
- generative
- prediction
- feature
- features
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the asymmetry between training and inference
  in generative CTR models, which prevents them from fully utilizing their generative
  capabilities to improve prediction accuracy. The authors propose SGCTR, a novel
  framework that applies generative capabilities during online inference by iteratively
  refining input features based on model confidence scores.
---

# Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through Rate Prediction

## Quick Facts
- arXiv ID: 2511.14403
- Source URL: https://arxiv.org/abs/2511.14403
- Reference count: 15
- Primary result: AUC improvements of up to 0.8186 on Criteo dataset

## Executive Summary
This paper addresses the asymmetry between training and inference in generative CTR models, which prevents them from fully utilizing their generative capabilities to improve prediction accuracy. The authors propose SGCTR, a novel framework that applies generative capabilities during online inference by iteratively refining input features based on model confidence scores. During training, SGCTR uses discrete diffusion to learn feature dependencies, while the inference phase employs an iterative refinement strategy to denoise input features. Experiments across four datasets (Criteo, Avazu, Malware, and an industrial dataset) demonstrate SGCTR's superiority, achieving AUC improvements of up to 0.8186 (Criteo) compared to state-of-the-art baselines. An online A/B test further validates the model's effectiveness with a 2.1% CTR improvement.

## Method Summary
SGCTR implements a symmetric generative paradigm for CTR prediction where the same generative model is used for both training and inference. During training, discrete diffusion with masked feature prediction forces the model to learn joint distributions over feature fields. At inference, the model iteratively refines inputs rather than performing a single discriminative pass. The inference loop uses confidence-gated feature redefinition, where cosine similarity between generated and original embeddings determines which features to refine at each step. The framework employs an optimal mask schedule (cosine or square) that aligns training noise levels with inference refinement steps, with T=5 iterations found optimal for balancing accuracy and latency.

## Key Results
- SGCTR achieves AUC of 0.8186 on Criteo dataset, outperforming state-of-the-art baselines
- Online A/B test shows 2.1% CTR improvement in industrial deployment
- Confidence-gated refinement outperforms direct feature replacement by >0.5% AUC across all datasets
- Cosine and square mask schedules perform best, with T=5 iterations optimal for balancing accuracy and latency

## Why This Works (Mechanism)

### Mechanism 1: Symmetric Training-Inference Paradigm
The alignment of generative objectives across training and inference phases enables the model to apply learned feature dependencies at prediction time. Training uses discrete diffusion with masked feature prediction, while inference employs iterative refinement. The core assumption is that feature dependencies learned via masking during training transfer directly to the denoising task at inference. Break condition: if training mask schedules don't span noise levels encountered during iterative inference, the model will miscalibrate confidence scores.

### Mechanism 2: Confidence-Gated Feature Redefinition
Model-predicted feature similarity serves as a confidence signal to enable selective refinement of uncertain features while preserving reliable ones. At each iteration, cosine similarity between generated feature representation and original embedding yields a confidence score. Features below the k-th percentile confidence are re-masked. Core assumption: cosine similarity correlates with feature correctness. Break condition: if confidence scores are systematically miscalibrated, the refinement loop will propagate errors rather than reduce them.

### Mechanism 3: Progressive Mask Schedule Alignment
Matching mask ratio schedules between training and inference ensures the generative model operates within its learned distribution at each refinement step. Training uses noise schedule λ(t); inference uses mask schedule γ(t/T). Both monotonically decrease masking. Cosine and square schedules perform best because they maintain high mask ratios early, forcing core feature refinement first. Core assumption: the model's generative capability is strongest when inference mask ratios fall within training noise distribution. Break condition: if inference mask ratios deviate significantly from training noise levels, the generative scorer operates out-of-distribution and produces unreliable confidence estimates.

## Foundational Learning

- **Discrete diffusion for tabular features**: SGCTR's training phase uses discrete diffusion to corrupt and predict masked categorical features. Quick check: Can you explain why sampled softmax is used instead of full softmax for high-cardinality ID features during training?
- **Iterative vs. single-shot generation**: The inference-phase contribution hinges on multi-step refinement. Quick check: What would happen to prediction accuracy if you set T=1 during inference while keeping the training diffusion process unchanged?
- **Confidence calibration in generative models**: The entire refinement loop depends on confidence scores being meaningful. Quick check: If cosine similarity between generated and original embeddings systematically favors certain feature types, what failure mode would you expect during inference?

## Architecture Onboarding

- **Component map**: Embedding layer → shared across all feature fields → Generative scoring network G_k(·) → predicts masked features per field → Confidence calculator → cosine similarity between generated and original embeddings → Mask scheduler γ(·) → controls unmasking rate per iteration → Final discriminator → binary CTR prediction on refined features

- **Critical path**: 
  1. Pre-train using DGenCTR-style discrete diffusion with sampled softmax loss (Eq. 2-3)
  2. Freeze generative weights; implement inference loop with T iterations
  3. At each iteration: generate masked features → compute confidence → re-mask low-confidence positions → scale retained features
  4. After T steps, pass refined X^T to discriminator for final CTR score (Eq. 6)

- **Design tradeoffs**: 
  - T (iteration steps): Higher T improves refinement but increases latency. Paper finds T=5 optimal for industrial deployment
  - Mask schedule: Cosine/square outperform exponential; aggressive early masking focuses model on core features
  - Feature replacement vs. scaling: Direct replacement (SGCTR(GenFea)) fails; confidence-weighted scaling is necessary for high-cardinality ID features

- **Failure signatures**: 
  - AUC drops below discriminative baseline → likely training-inference schedule mismatch
  - Inference latency exceeds budget → reduce T; cache key-value tensors after first iteration
  - Confidence scores all near 1.0 or all near 0 → embedding collapse or cosine similarity saturation

- **First 3 experiments**: 
  1. Reproduce DGenCTR training on Criteo subset; validate sampled softmax loss convergence
  2. Implement T=5 inference loop with cosine mask schedule; compare AUC against T=1 and T=10
  3. Run ablation on confidence scaling: compare SGCTR vs. SGCTR(GenFea) on held-out validation set

## Open Questions the Paper Calls Out
None

## Limitations
- The exact form of the noise schedule λ(t) and mask schedule γ(t) are not specified, only that "cosine and square" performed best
- The DGenCTR backbone architecture is referenced but not fully specified in the paper
- Confidence calibration across feature types is assumed to be uniform, but not validated

## Confidence
- **High confidence**: The core mechanism of symmetric training-inference paradigm works as described, supported by consistent AUC improvements across all four datasets and the online A/B test
- **Medium confidence**: The confidence-gating mechanism's effectiveness relies on cosine similarity being a reliable quality signal, which is plausible but not directly validated
- **Medium confidence**: The mask schedule alignment claim is supported by ablation studies, but the paper doesn't explore whether alternative alignment strategies could achieve similar results

## Next Checks
1. Implement DGenCTR training with sampled softmax loss on Criteo subset; verify that generative scoring network achieves reasonable masked feature accuracy before proceeding to inference experiments
2. Compare SGCTR's performance across different T values (1, 3, 5, 10) on a validation set to empirically verify the optimal T=5 finding and identify the degradation point
3. Run ablation study comparing SGCTR with and without confidence weighting (direct feature replacement) on high-cardinality ID features to quantify the performance gap and validate the paper's claim that confidence scaling is essential for sparse features