---
ver: rpa2
title: 'HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language
  Models'
arxiv_id: '2511.12693'
source_url: https://arxiv.org/abs/2511.12693
tags:
- arxiv
- hallucination
- clustering
- semantic
- radflag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HEDGE, a hallucination detection framework
  for vision-language models that reframes the problem as one of geometric stability
  under controlled visual perturbations. It combines answer sampling, semantic clustering
  (via entailment-based and embedding-based methods), and uncertainty metrics (SE,
  RadFlag, VASE) to estimate hallucination likelihood without requiring task-specific
  labels.
---

# HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models

## Quick Facts
- arXiv ID: 2511.12693
- Source URL: https://arxiv.org/abs/2511.12693
- Reference count: 40
- Authors: Sushant Gautam; Michael A. Riegler; Pål Halvorsen
- Key outcome: HEDGE reframes hallucination detection as geometric stability under visual perturbations, achieving superior detection performance with VASE metric, especially when paired with embedding clustering and moderate sampling scales (~10-15)

## Executive Summary
This paper introduces HEDGE, a hallucination detection framework for vision-language models that reframes the problem as one of geometric stability under controlled visual perturbations. It combines answer sampling, semantic clustering (via entailment-based and embedding-based methods), and uncertainty metrics (SE, RadFlag, VASE) to estimate hallucination likelihood without requiring task-specific labels. Evaluations on medical VQA datasets with three VLMs show that VASE consistently outperforms SE and RadFlag, especially when paired with embedding clustering and moderate sampling scales (~10-15). Architectural factors and prompt design strongly influence detectability, with denser fusion and concise answers yielding clearer signals.

## Method Summary
HEDGE applies controlled visual perturbations (affine transforms, color jitter, Gaussian/Poisson noise) to generate noisy responses alongside clean baseline answers. The framework clusters responses using either NLI-based entailment prediction or embedding similarity, then computes semantic uncertainty metrics. VASE, the proposed metric, amplifies geometric instability by contrasting clean and noisy semantic distributions. The method requires no task-specific labels, instead using a few-shot calibrated adjudicator for ground-truth hallucination labeling.

## Key Results
- VASE metric provides the most robust hallucination signal, achieving 0.71-0.75 AUC at n=1 and stabilizing around n=10-15
- Embedding clustering excels for concise, label-style answers while NLI-based clustering is stronger for longer, sentence-level responses
- Architectural factors strongly influence detectability: denser fusion models show clearer geometric instability signals than cross-attention or flat early fusion architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled visual perturbations expose geometric instability in VLM representations that correlates with hallucination risk.
- Mechanism: HEDGE applies visual distortions to generate noisy responses. It contrasts the semantic distribution of clean versus noisy responses via VASE = SE(softmax(s_clean + α(s_clean - s_noisy))). When hallucinations exist, representations fragment under perturbation, causing high semantic entropy divergence between conditions.
- Core assumption: Hallucination manifests as geometric instability in the visual-linguistic manifold, where embeddings disperse when inputs are perturbed.
- Evidence anchors:
  - [abstract] "HEDGE... measures geometric stability in multimodal representations under controlled visual perturbations"
  - [Page 2, Section 1.2] "We posit that hallucination is not merely a probabilistic irregularity but a geometric instability within the visual-linguistic manifold"
  - [corpus] VideoHEDGE extends entropy-based detection to video but does not validate the geometric instability hypothesis explicitly
- Break condition: If perturbations destroy task-relevant visual information, semantic divergence may reflect input degradation rather than hallucination instability.

### Mechanism 2
- Claim: Embedding-based clustering provides comparable or superior hallucination detection to NLI-based clustering for concise answers while remaining computationally tractable at scale.
- Mechanism: The framework groups responses using cosine similarity in embedding space (O(nT_enc) + O(n²d) complexity) versus NLI's O(n²T_NLI) pairwise entailment checks. For short, label-style answers, embeddings capture semantic proximity effectively; for verbose responses, NLI's logical entailment handles paraphrastic variation better.
- Core assumption: Semantic equivalence can be approximated by geometric proximity in embedding space for structured, concise outputs.
- Evidence anchors:
  - [abstract] "Embedding-based clustering excels for concise, label-style answers, whereas NLI-based clustering is stronger for longer, sentence-level responses"
  - [Page 6, Table 1] Complexity comparison shows embedding clustering scales better for large n
  - [corpus] Semantic Reformulation Entropy paper notes "unstable clustering of variable-length answers" but does not directly compare embedding vs. NLI approaches
- Break condition: For domains with high paraphrastic variation, embedding similarity may conflate semantically distinct responses with similar surface forms.

### Mechanism 3
- Claim: A moderate sampling budget (n ≈ 10-15) with VASE metric provides the most robust hallucination signal across model architectures.
- Mechanism: VASE amplifies the stability gap between clean and noisy semantic distributions. With n=1, VASE achieves strong performance (AUC 0.59-0.75); with n≈10-15, all metrics stabilize with ≤0.01-0.02 additional AUC gains. Beyond this point, computational cost increases without proportional signal improvement.
- Core assumption: Hallucination detection follows diminishing returns with sampling scale—initial samples capture the primary uncertainty structure.
- Evidence anchors:
  - [abstract] "VASE metric provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ≈ 10-15)"
  - [Page 10, Table 3] Shows VASE achieving 0.71-0.75 AUC at n=1 and plateauing around n=10-15
  - [corpus] FaithSCAN mentions single-pass detection but focuses on a different mechanism (model-driven verification)
- Break condition: For rare hallucination types with low baseline frequency, more samples may be needed to reliably estimate semantic distributions.

## Foundational Learning

- Concept: Semantic Entropy (SE)
  - Why needed here: Core metric measuring uncertainty over semantically grouped responses using normalized mean log-likelihoods. Understanding how SE differs from token-level entropy is essential for interpreting VASE and RadFlag results.
  - Quick check question: If five sampled answers cluster into two semantic groups with probabilities 0.7 and 0.3, what would SE approximate? (Answer: ≈0.61, indicating moderate uncertainty)

- Concept: Natural Language Inference (NLI) for Clustering
  - Why needed here: One of two clustering strategies compared in HEDGE. NLI models predict entailment/contradiction/neutral relationships between response pairs, building a directed graph for cluster extraction via mutual entailment closure.
  - Quick check question: Why does NLI clustering scale as O(n²T_NLI) rather than O(nT_NLI)? (Answer: Each response must be compared pairwise with all others)

- Concept: ROC-AUC as Evaluation Metric
  - Why needed here: Primary quantitative measure for comparing hallucination detection methods. Understanding what AUC values mean (0.5 = random, 1.0 = perfect) is critical for interpreting Table 2 results where Qwen2.5-VL achieves 0.89 AUC while Med-Gemma drops to 0.52-0.60.
  - Quick check question: If a hallucination detector achieves 0.75 ROC-AUC, what proportion of randomly chosen hallucinated/non-hallucinated pairs would it rank correctly? (Answer: 75%)

## Architecture Onboarding

- Component map: Visual perturbation -> Answer generation -> Semantic clustering -> Entropy computation -> Hallucination score
- Critical path: Visual perturbation → Answer generation → Semantic clustering → Entropy computation → Hallucination score. The clustering choice (NLI vs. embedding) directly determines both computational cost and detection performance for different answer styles.
- Design tradeoffs:
  - NLI clustering: Higher interpretability, better for verbose answers, O(n²) cost limits scalability
  - Embedding clustering: Lower cost, better for concise answers, requires threshold tuning (τ ∈ [0.8, 0.99])
  - Sampling scale: n=1-2 provides minimal signal for SE; n=10-15 captures most information with diminishing returns beyond
  - Prompt style: Default/clinical-phrase most robust; minimal-label effective for strong models only; one-sentence hardest regime
- Failure signatures:
  - SE ≈ 0.50 at n=1: Insufficient samples to estimate semantic distribution (switch to VASE or RadFlag)
  - NLI clustering OOM at n>20: Quadratic memory growth; switch to embedding clustering
  - Embedding clustering fragments similar responses: Semantic similarity threshold τ too high; retune on validation data
  - Med-Gemma AUC ≈ 0.55: Architectural limitation (restricted visual tokenization bottleneck); detection method cannot compensate
- First 3 experiments:
  1. Baseline sanity check: Run HEDGE on VQA-RAD with Qwen2.5-VL, n=10, default prompt, embedding clustering. Verify ROC-AUC in 0.73-0.76 range per Table 2. If significantly lower, check visual preprocessing and log-probability extraction.
  2. Clustering comparison: On same configuration, compare NLI vs. embedding clustering with one-sentence prompt. Expect NLI to outperform embedding (≈0.73 vs. ≈0.68 VASE per Table 3) due to better paraphrase handling for verbose outputs.
  3. Sampling scale ablation: Sweep n ∈ {1, 5, 10, 15, 20} for VASE metric with embedding clustering on minimal-label prompt. Plot AUC vs. n; expect plateau at n≈10-15 with ≤0.02 additional gains beyond.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do sampling scale and visual perturbation strength independently influence hallucination detection performance?
  - Basis: Section 6.2 notes current design couples sample count with perturbation budget, calling for future work to decouple these factors.
  - Why unresolved: Current protocol modulates number of samples and distortions simultaneously, making it impossible to attribute performance gains solely to statistical power versus visual robustness testing.
  - What evidence would resolve it: Ablation studies varying sample count with fixed perturbations and vice versa.

- **Open Question 2**: Can a hybrid clustering strategy fusing NLI-based entailment and embedding similarity outperform individual methods?
  - Basis: Section 6.5 lists "Explore hybrid clustering that fuses entailment and embeddings in a single graph" as a specific future extension.
  - Why unresolved: Paper treats NLI and embedding clustering as competing alternatives, observing they excel in different regimes (verbose vs. concise).
  - What evidence would resolve it: Implementation of unified graph approach or ensemble method and comparison of its ROC-AUC against standalone baselines.

- **Open Question 3**: Does the HEDGE framework maintain efficacy in non-medical domains, multilingual settings, or longer-form text generation?
  - Basis: Section 6.5 explicitly calls for extending HEDGE to "longer clinical reports, non-medical domains, and multilingual settings."
  - Why unresolved: Experiments restricted to medical VQA datasets (VQA-RAD, KvasirVQA-x1) with short, low-ambiguity answers and high-resolution structured images.
  - What evidence would resolve it: Evaluation of HEDGE metrics on general-purpose VQA benchmarks or long-form report generation tasks.

## Limitations
- The optimal SentenceTransformer checkpoint for embedding clustering remains unspecified, potentially affecting detection accuracy.
- Ground-truth hallucination labeling depends on Qwen3-30B-A3B adjudicator calibration with few-shot examples, which are not publicly available.
- The validation split used for Optuna-based threshold optimization is not specified, creating ambiguity in the exact optimization procedure.

## Confidence
- **High confidence**: Geometric stability hypothesis linking visual perturbations to hallucination detection is well-supported by controlled experiments across three VLMs, with VASE consistently outperforming baseline metrics.
- **Medium confidence**: Architectural sensitivity findings are demonstrated but limited to specific models tested; generalizability across diverse VLM architectures requires further validation.
- **Low confidence**: Claim that embedding clustering universally excels for concise answers while NLI clustering excels for verbose responses needs broader domain testing, as paraphrastic variation patterns may differ significantly across contexts.

## Next Checks
1. **SentenceTransformer model ablation**: Test multiple SentenceTransformer checkpoints (e.g., all-MiniLM-L6-v2, paraphrase-MiniLM-L6-v2, nli-MiniLM-L12-v2) on VQA-RAD with Qwen2.5-VL to quantify performance variance from model choice.

2. **Cross-domain generalization**: Apply HEDGE to non-medical VQA datasets (e.g., OK-VQA, GQA) with the same three VLMs to verify whether embedding clustering maintains superiority for concise answers and NLI for verbose responses across domains.

3. **Extreme perturbation analysis**: Systematically vary distortion intensity to identify the boundary where geometric instability shifts from detecting hallucinations to merely reflecting input degradation, clarifying the mechanism's limits.