---
ver: rpa2
title: Elastic Spectral State Space Models for Budgeted Inference
arxiv_id: '2601.22488'
source_url: https://arxiv.org/abs/2601.22488
tags:
- spectral
- budget
- inference
- es-ssm
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Elastic Spectral State Space Models (ES-SSM) enable a single trained
  model to be directly truncated for inference at arbitrary runtime budgets without
  retraining. The method combines an input-adaptive spectral gate with budget dropout
  during training to concentrate predictive information in low-index spectral channels,
  ensuring stable performance under truncation.
---

# Elastic Spectral State Space Models for Budgeted Inference

## Quick Facts
- **arXiv ID**: 2601.22488
- **Source URL**: https://arxiv.org/abs/2601.22488
- **Reference count**: 15
- **One-line result**: Single trained model achieves competitive accuracy and can be directly truncated at arbitrary inference budgets without retraining.

## Executive Summary
Elastic Spectral State Space Models (ES-SSM) enable a single trained model to be truncated at arbitrary runtime budgets without retraining. The method combines input-adaptive spectral gating with budget dropout during training to concentrate predictive information in low-index spectral channels. Tested across long-sequence benchmarks, a single ES-SSM trained at full capacity achieves competitive accuracy while maintaining near-full-capacity performance at small budgets.

## Method Summary
ES-SSM approximates SSM convolution kernels using a fixed Hankel spectral basis, allowing controlled truncation. During training, budget dropout randomly samples spectral budgets per minibatch, masking higher channels and forcing information concentration in low-index channels. A masked normalization rule rescales logits to maintain stable softmax distributions across different budgets. The model consists of 8 layers with model dimension 256, trained with AdamW and cosine schedule.

## Key Results
- Single ES-SSM trained at K=32 achieves competitive accuracy with modern baselines across 6 LRA tasks
- Maintains 98% accuracy with only 9-13% of channels active at small budgets
- Budget dropout and spectral gating are both necessary for reliable truncation performance
- Demonstrates flat accuracy curves across different inference budgets compared to static training

## Why This Works (Mechanism)

### Mechanism 1: Spectral Truncation via Hankel Basis
The method projects the SSM convolution kernel onto a fixed Hankel spectral basis where eigenvalues decay rapidly. This allows truncation of higher-index channels with predictable error since the first K channels capture dominant temporal features. The ordered channel assumption relies on the spectral decay property of Hankel matrices.

### Mechanism 2: Information Concentration via Budget Dropout
Randomly sampling spectral budgets during training forces the model to store primary predictive features in earliest spectral channels. By masking channels k > K_train and providing zero gradient, the optimizer learns to make low-index channels sufficient for prediction, turning higher channels into optional refinements.

### Mechanism 3: Stability via Masked Normalization
Rescaling active logits during masked softmax prevents distribution shift when switching between different runtime budgets. The method applies a rescaling factor to keep the effective softmax temperature consistent whether using 32 or 4 channels, avoiding over- or under-emphasis of the subset of parameters currently being trained.

## Foundational Learning

- **Hankel Operators & Spectral Filtering**
  - Why needed: The method relies on Hankel matrix eigenvalues decaying rapidly to enable ordered channel truncation
  - Quick check: Can you explain why eigenvalues of the Hankel matrix decay, and how this relates to the "memory" of a linear dynamical system?

- **State Space Models (SSM) as Convolutions**
  - Why needed: The method replaces recurrent view with convolution view to apply spectral filters
  - Quick check: How does the spectral basis φk modify the convolution kernel G in Eq. (5)?

- **Softmax Temperature & Scaling**
  - Why needed: Masked normalization acts as dynamic temperature adjuster to maintain stable gating
  - Quick check: If you mask 50% of channels without rescaling logits, does the probability mass on remaining channels increase, and how does the paper's rescaling correct for this?

## Architecture Onboarding

- **Component map**: Input -> Spectral Branch (FFT convolution with Hankel filters) + Gate Branch (2-layer MLP) -> Elastic Logic (rescaled masked softmax) -> Weighted sum of spectral features
- **Critical path**: Budget Dropout sampling logic and Rescaling factor (Eq 8b). Errors here will cause train-test mismatch
- **Design tradeoffs**: Training overhead slightly higher due to sampling and dynamic masking, but avoids training multiple model variants; allows fine-grained control vs. fixed-size distillation
- **Failure signatures**: Cliff collapse (accuracy drops at specific K thresholds); Oscillating Loss (gradients fighting between different sampled budgets)
- **First 3 experiments**:
  1. Sanity Check (Static vs Elastic): Train two models on ListOps, compare accuracy vs. K curves
  2. Ablation (Rescaling): Remove rescaling factor in Eq 8b, verify if low K' performance degrades
  3. Efficiency Profiling: Measure wall-clock latency for K ∈ {2, 4, 32}, verify actual reduction

## Open Questions the Paper Calls Out

- **Scaling to Foundation Models**: Whether ES-SSM retains truncation properties and accuracy when scaled to multi-billion parameter models remains untested, as current experiments use only ~20M parameter models
- **Hybrid Architecture Integration**: How elastic spectral gating interacts with attention blocks in hybrid models is not understood, since the study focuses on pure SSM architectures
- **Online Budget Modulation**: Whether runtime budget K can be effectively modulated online in closed-loop embodied settings like robotics remains an open question

## Limitations
- Performance depends on assumption that task-relevant information concentrates in low-index spectral channels, which may fail for high-frequency tasks
- Hankel basis computed once per sequence length, but generalization to different test lengths is not validated
- Budget dropout sampling strategy (uniform) may not be optimal for all tasks

## Confidence
- **High**: Single model truncation without retraining is well-supported by empirical results
- **Medium**: Masked normalization stability relies on assumptions about logit distributions
- **Low**: General applicability across domains with different signal characteristics is not rigorously established

## Next Checks
1. **High-Frequency Task Validation**: Test on synthetic task requiring high-frequency temporal features to validate method's limits
2. **Sequence Length Generalization**: Train on short sequences and test on long sequences to measure basis generalization
3. **Budget Dropout Strategy Comparison**: Implement alternative sampling strategies to determine if uniform sampling is optimal