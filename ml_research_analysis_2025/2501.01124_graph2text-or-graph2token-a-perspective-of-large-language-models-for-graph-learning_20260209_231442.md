---
ver: rpa2
title: 'Graph2text or Graph2token: A Perspective of Large Language Models for Graph
  Learning'
arxiv_id: '2501.01124'
source_url: https://arxiv.org/abs/2501.01124
tags:
- graph
- graphs
- information
- llms
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews how Large Language Models (LLMs) can be applied
  to graph learning tasks. It proposes a new taxonomy dividing methods into Graph2text
  (transforming graphs into human-readable text) and Graph2token (converting graphs
  into token representations).
---

# Graph2text or Graph2token: A Perspective of Large Language Models for Graph Learning

## Quick Facts
- arXiv ID: 2501.01124
- Source URL: https://arxiv.org/abs/2501.01124
- Reference count: 40
- This paper proposes a taxonomy dividing LLM-based graph learning into Graph2text (text transformation) and Graph2token (token representation) paradigms.

## Executive Summary
This survey paper proposes a comprehensive taxonomy for applying Large Language Models (LLMs) to graph learning tasks, dividing methods into Graph2text and Graph2token paradigms. The authors identify four fundamental challenges in this integration: alignment of graph and text structures, positional information preservation, multi-level semantics, and context handling. Through extensive literature review, they provide practical guidelines for selecting appropriate approaches based on graph types and identify five future research directions. The work serves as both a theoretical framework and practical guide for researchers integrating LLMs with graph data processing.

## Method Summary
The paper conducts a systematic survey of LLM4graph methods, categorizing them into two paradigms: Graph2text (converting graphs to natural language descriptions or Graph Description Languages) and Graph2token (projecting graph embeddings into LLM token space via encoders). For Graph2text, the approach uses prompt engineering or GDL serialization to transform graph structures into sequential text. For Graph2token, GNN or graph transformer encoders project node features into the LLM's embedding space, followed by alignment layers. The survey identifies four key challenges and provides implementation guidelines, though it does not present original experimental results.

## Key Results
- Proposes a clear taxonomy dividing LLM4graph methods into Graph2text and Graph2token paradigms
- Identifies four critical challenges: alignment, position, hierarchy, and context
- Provides practical guidelines for selecting approaches based on graph types (textual, attributed, heterogeneous)
- Identifies five future research directions including domain-specific instructions, theoretical understanding, fairness, efficiency, and dynamic graphs

## Why This Works (Mechanism)

### Mechanism 1: Topological Serialization via Transformation Paradigms
- **Claim:** LLMs can process graph data by serializing irregular topological structures into sequential inputs (text or tokens) that fit the model's context window.
- **Mechanism:** The paper proposes two paradigms: Graph2Text, which flattens graph structures into human-readable natural language or code (e.g., GraphML, Cypher), and Graph2Token, which uses graph encoders (e.g., GNNs) to project node/edge features into the LLM's embedding space. This transformation bridges the gap between non-Euclidean graph data and the sequential attention mechanisms of Transformers.
- **Core assumption:** The LLM's attention mechanism can approximate structural relationships (e.g., connectivity, distance) when the graph is presented as a sequence, provided the serialization preserves this logic.
- **Evidence anchors:** [abstract] "The core of LLM4graph lies in transforming graphs into texts for LLMs to understand and analyze... methods can be divided into two paradigms: Graph2text and Graph2token." [section 5] "Tokenization is the pivotal step... converting each component of the graph into tokens or sequences of tokens... allowing LLMs to understand and process this information."
- **Break condition:** The mechanism fails if the serialization method (e.g., random node ordering in text) destroys the positional or relational information required for the task, leading the LLM to hallucinate connections.

### Mechanism 2: Cross-Modal Alignment (Structure to Semantics)
- **Claim:** Projecting graph embeddings directly into the LLM's token space (Graph2Token) allows the model to utilize its pre-trained semantic knowledge to interpret abstract structural patterns.
- **Mechanism:** Encoders (like GNNs or Graph Transformers) capture structural features (alignment). These are then mapped to the LLM's dimension via projection layers (e.g., linear layers or "Translators"). This fuses the graph's "skeleton" (structure) with the LLM's "flesh" (semantic knowledge).
- **Core assumption:** There exists a translatable latent space where graph structural features align meaningfully with linguistic semantic features.
- **Evidence anchors:** [section 3] "Alignment problem... supplementing/fusing characteristics of graphs and texts... If the alignment problem is overlooked, LLMs may fail to fully capture the interplay between structural dependencies... and semantic richness." [section 5.1.1] "AlignLLM function is used to align the node's structural token with the language model's pre-existing text token space."
- **Break condition:** The mechanism degrades if the projection layer is insufficiently trained or if the graph features are too abstract (lacking attributes), causing a misalignment where the LLM cannot map structural vectors to meaningful concepts.

### Mechanism 3: Context Augmentation via Instruction Tuning
- **Claim:** Enhancing the input with specific structural instructions or fine-tuning on graph-text pairs mitigates the "context problem," where raw graphs lack the dense information density of natural language corpora.
- **Mechanism:** Prompting (Hard/Soft) explicitly guides the model's attention to specific graph elements (e.g., "Build-a-Graph"), while Fine-tuning (e.g., Prefix/Corpus) adjusts weights to recognize graph-specific patterns (like homophily).
- **Core assumption:** LLMs possess latent reasoning capabilities that can be unlocked or directed by explicit context, rather than requiring the model to learn graph theory from scratch.
- **Evidence anchors:** [section 6.2.1] "Hard prompts are artificially designed... 'Build-a-Graph Prompting'... helps LLMs better understand dynamic graph structures." [corpus] "Semantic Refinement with LLMs... structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize well." (Suggests the need for adaptive instructions).
- **Break condition:** The mechanism fails if the prompt exceeds the context window (for large graphs) or if fine-tuning data is too sparse, leading to catastrophic forgetting of the model's original linguistic capabilities.

## Foundational Learning

- **Concept: Message Passing (GNNs)**
  - **Why needed here:** The paper relies on Graph Neural Networks (GCN, GAT) as the primary encoders for the Graph2Token paradigm. You cannot understand how the graph is converted to tokens without understanding how nodes aggregate neighbor information.
  - **Quick check question:** Can you explain how a node updates its representation based on its neighbors in a GCN layer?

- **Concept: Transformer Attention vs. Graph Structure**
  - **Why needed here:** The core challenge identified is the mismatch between sequential text (Transformer) and irregular graphs. Understanding the *Attention* mechanism (Section 2.3) is vital to grasping why "Alignment" and "Position" are challenges.
  - **Quick check question:** How does the self-attention mechanism handle the relationship between two words in a sentence, and how does that differ from how a graph edge connects two nodes?

- **Concept: Tokenization (Text vs. Soft Prompts)**
  - **Why needed here:** The paper distinguishes between human-readable text tokens (Graph2Text) and vector-based "soft" tokens (Graph2Token). Distinguishing discrete from continuous representations is crucial for implementation.
  - **Quick check question:** What is the difference between feeding an LLM the text string "Node A is connected to Node B" versus feeding it the vector embeddings of Node A and Node B?

## Architecture Onboarding

- **Component map:** Graph Input $G = \langle V, E \rangle$ -> Encoder (Optional for Graph2Token) -> Transformation Layer -> LLM Backbone -> Output
- **Critical path:** The choice between Graph2Text and Graph2Token
  - Select Graph2Text if interpretability is paramount and the graph is small/tree-like (to avoid context window limits)
  - Select Graph2Token if the graph is large, dense, or requires preserving precise structural nuances (e.g., molecular properties)
- **Design tradeoffs:**
  - Interpretability vs. Performance: Graph2Text is highly interpretable (you see the prompt) but loses structural nuance. Graph2Token performs better on complex structural tasks but acts as a "black box"
  - Context Window: Graph2Text consumes tokens rapidly (O(N^2) potentially for dense graph descriptions). Graph2Token compresses graphs into fixed-length "soft" tokens, saving context space
- **Failure signatures:**
  - Structure Hallucination: The LLM invents edges or nodes not present in the input (common in Graph2Text if the prompt is ambiguous)
  - Modality Mismatch: The LLM ignores the graph tokens and answers based only on text attributes (Failure in Alignment)
  - Over-smoothing: If using a GNN encoder for Graph2Token, node representations become indistinguishable if the graph is too deep
- **First 3 experiments:**
  1. Zero-Shot Graph2Text: Implement a "Build-a-Graph" prompt (Section 6.2.1) for a simple node classification task to establish a baseline without training
  2. Tokenizer Ablation: Compare inputting a graph as a raw adjacency list (Text) vs. inputting it as a Python program (GDL) to verify the paper's claim that GDL improves structural reasoning (Section 4.1)
  3. Graph2Token Alignment: Implement a simple GNN encoder and a linear projection layer (Section 5.1.1). Fine-tune only the projection layer (freezing the LLM) to see if structural information transfers to the LLM's output space

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a unified modular framework successfully combine domain-specific Graph2text instructions with the general structural learning of Graph2token methods?
- **Basis in paper:** [Explicit] Section 7.1 notes that Graph2text methods are often domain-specific while Graph2token methods lack domain customization. The authors suggest developing a modular framework where Graph2text serves as a domain-specific module and Graph2token as a general learning module.
- **Why unresolved:** Current methods generally excel in one area (either specificity or generality) but fail to synthesize domain knowledge with general structural encoding capabilities effectively.
- **What evidence would resolve it:** The creation and validation of a hybrid model that outperforms standalone baselines across diverse domains (e.g., social networks vs. molecules) by effectively integrating structural tokens with domain-specific textual prompts.

### Open Question 2
- **Question:** Can LLMs strictly satisfy permutation invariance and geometric equivariance when processing graph structures converted into text or tokens?
- **Basis in paper:** [Explicit] Section 7.2 highlights the "theoretical power" of LLMs as a critical area of exploration, specifically questioning if they can maintain consistent outputs despite changes in node order (permutation invariance) or spatial orientation (geometric equivariance).
- **Why unresolved:** LLMs are inherently sequence-based and sensitive to token order, whereas graph structures theoretically lack a canonical order; standard text conversion methods may disrupt these geometric properties.
- **What evidence would resolve it:** Theoretical proofs or empirical benchmarks demonstrating that LLM outputs remain stable under node shuffling and geometric transformations (e.g., rotations, translations) without explicit structural encoding steps that force these properties.

### Open Question 3
- **Question:** How can LLMs effectively capture temporal evolution and real-time updates in dynamic graphs where structures and attributes change continuously?
- **Basis in paper:** [Explicit] Section 7.5 identifies "LLMs for Dynamic Graphs" as a future direction, noting it is "largely unexplored" and citing the lack of benchmarks and challenges in designing prompts for temporal evolutions.
- **Why unresolved:** Most existing LLM4graph methods focus on static snapshots. Representing time-dependent structural changes and high-frequency updates in a prompt or token sequence without exceeding context limits or losing temporal logic is an unsolved challenge.
- **What evidence would resolve it:** The establishment of dynamic graph benchmarks and the development of models that perform on-par with specialized dynamic GNNs on tasks involving time-evolving data, such as traffic flow prediction or temporal recommendation.

## Limitations
- The survey is theoretical and lacks quantitative benchmarks comparing Graph2Text and Graph2Token paradigms across standardized datasets
- The paper does not provide empirical validation of the four identified challenges or systematic measurements of their impact on downstream performance
- Computational efficiency comparisons between paradigms are not addressed, and detailed implementation guidelines for practitioners are limited

## Confidence
- **High confidence:** The taxonomy dividing LLM4graph methods into Graph2text and Graph2token paradigms
- **Medium confidence:** The identification of four key challenges (alignment, position, hierarchy, context) as fundamental issues in LLM-graph integration
- **Medium confidence:** The practical guidelines for selecting appropriate models based on graph types
- **Low confidence:** The proposed future research directions

## Next Checks
1. Implement a controlled experiment comparing Graph2Text and Graph2Token approaches on a standardized graph classification task (e.g., molecule property prediction) to empirically validate the proposed taxonomy's practical implications
2. Design a benchmark suite that quantifies the four identified challenges (alignment quality, positional preservation, hierarchical representation, and context adaptation) across multiple graph types and sizes
3. Conduct ablation studies on the alignment mechanism (projection layer effectiveness) by varying graph encoder architectures and measuring their impact on LLM performance in the Graph2Token paradigm