---
ver: rpa2
title: 'Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks'
arxiv_id: '2505.07473'
source_url: https://arxiv.org/abs/2505.07473
tags:
- code
- benchmark
- project
- frameworks
- build
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Web-Bench is a new benchmark for evaluating large language models
  on web development tasks. It contains 50 projects with 20 sequential tasks each,
  covering core web standards (HTML, CSS, JavaScript, TypeScript, SVG, WebGL) and
  frameworks (React, Vue, Angular, Next.js, etc.).
---

# Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks

## Quick Facts
- arXiv ID: 2505.07473
- Source URL: https://arxiv.org/abs/2505.07473
- Authors: Kai Xu; YiWei Mao; XinYi Guan; ZiLong Feng
- Reference count: 40
- Best model (Claude 3.7 Sonnet) achieves only 25.1% Pass@1 on 50 projects with 20 sequential tasks each

## Executive Summary
Web-Bench is a new benchmark for evaluating large language models on web development tasks. It contains 50 projects with 20 sequential tasks each, covering core web standards (HTML, CSS, JavaScript, TypeScript, SVG, WebGL) and frameworks (React, Vue, Angular, Next.js, etc.). The tasks simulate real-world development workflows with end-to-end tests. On Web-Bench, the best model (Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower than SWE-Bench's 65.4% and indicating the benchmark's higher difficulty. The benchmark aims to address saturation in existing coding benchmarks by focusing on practical web development skills.

## Method Summary
Web-Bench evaluates LLMs on 50 web development projects, each containing 20 sequentially dependent tasks. The evaluation uses Docker-based orchestrator that iterates through tasks 1-20 per project. For each task, the "Web-Agent" prompts the LLM with the task description, current files, and optional error context, then parses the code response to update files. The system uses Playwright E2E tests to verify functional correctness. Metrics include Pass@1 (tasks passed without prior failure) and Pass@2 (tasks passed allowing one retry with error context). The benchmark covers both web standards (DOM, CSS, WebGL) and frameworks (React, Redux, Next.js).

## Key Results
- Best model (Claude 3.7 Sonnet) achieves 25.1% Pass@1 across all projects
- SWE-Bench SOTA (65.4%) is significantly higher than Web-Bench SOTA, suggesting greater difficulty
- LLMs show partial recovery with error-context retries (Pass@2 is ~10% higher than Pass@1)
- Sequential dependency creates cascading failures where early errors propagate through remaining tasks

## Why This Works (Mechanism)

### Mechanism 1: Sequential Task Dependency Creates Compounding Difficulty
- Claim: Tasks with sequential dependencies amplify evaluation difficulty because errors propagate forward through the project
- Mechanism: Each project contains 20 tasks where Task-N depends on outputs from Task-(N-1). If an LLM produces incorrect code in Task-3, all subsequent tasks (4-20) will likely fail regardless of generation quality
- Core assumption: Real-world development workflows involve cumulative state, so benchmarking should penalize early failures more severely than independent task benchmarks
- Evidence anchors: [abstract] "50 projects, each consisting of 20 tasks with sequential dependencies"; [Section 2.1] "Task-2 depends on the execution result ('main' element) of Task-1"

### Mechanism 2: Standards + Framework Dual Coverage Increases Knowledge Breadth Requirements
- Claim: Requiring proficiency in both foundational standards (DOM, CSS, WebGL) AND high-level frameworks (React, Redux, Next.js) exposes gaps in LLM training coverage
- Mechanism: Web standards represent stable, specification-driven knowledge. Frameworks evolve rapidly and have version-specific APIs. LLMs trained on general web code may know React patterns but struggle with CSS Grid edge cases, or vice versa
- Core assumption: LLMs have uneven coverage of the web development knowledge graph because pretraining data emphasizes popular frameworks over foundational specifications
- Evidence anchors: [Section 2.2] Explicit separation into "Web Standards" and "Web Frameworks"; [Table 3] Web-Bench SOTA (25.1%) is lower than SWE-Bench Verified (65.4%)

### Mechanism 3: End-to-End Testing Enforces Functional Correctness Over Syntactic Validity
- Claim: Using Playwright E2E tests rather than unit tests or syntax checks forces LLMs to produce actually runnable applications, not just plausible-looking code
- Mechanism: Each task includes ~3.6 E2E test cases that verify browser-rendered behavior (DOM structure, user interactions, visual output). This catches errors invisible to static analysis: incorrect event handlers, layout bugs, broken state transitions
- Core assumption: E2E tests better approximate real-world correctness than function-level unit tests, especially for UI code
- Evidence anchors: [Section 3.2] "Test: end-to-end (E2E) test with Playwright"; [Table 1] "Testcases Per Task: 3.6 (average)"

## Foundational Learning

- **Pass@k Metric Interpretation**
  - Why needed here: The benchmark reports Pass@1 and Pass@2 with specific definitions—Pass@1 counts tasks before any failure; Pass@2 counts total passed including one retry
  - Quick check question: If a model passes tasks 1-5, fails task-6, then retries and passes tasks 6-15, what are Pass@1 and Pass@2?

- **Sequential Dependency Graphs**
  - Why needed here: Understanding that Task-N depends on Task-(N-1)'s output means debugging requires tracing backwards through the dependency chain
  - Quick check question: Why might fixing Task-6's code not resolve a test failure if Task-4 was implemented incorrectly?

- **Web Standards vs. Frameworks Abstraction Levels**
  - Why needed here: The benchmark categorizes projects by knowledge type. Standards (DOM, CSS Grid) are specification-driven; frameworks (React, Redux) are API-driven
  - Quick check question: Would you expect an LLM trained primarily on GitHub code to perform better on React tasks or CSS Selector tasks? Why?

## Architecture Onboarding

- **Component map:**
  - Docker container -> Web-Agent (prompt builder, LLM API caller, response parser) -> Evaluator (orchestrator, file rewriter, test runner)

- **Critical path:**
  1. Evaluator loads project, gets initial code from `src-init/`
  2. For each task (1-20): Evaluator calls Web-Agent with task description + current files
  3. Web-Agent builds prompt, calls LLM API, extracts generated files
  4. Evaluator writes files, runs build (optional), runs Playwright tests
  5. On failure: one retry with error context; on second failure, evaluation ends

- **Design tradeoffs:**
  - **Retry limit (2 attempts)**: Balances evaluation cost against giving models fair chance to self-correct. Could mask systematic errors if retries succeed for wrong reasons
  - **Context truncation**: Long projects may exceed context length; truncation loses earlier context
  - **Single-file vs. multi-file projects**: Most projects are single files; scaling to larger repos may require different agent architecture

- **Failure signatures:**
  - **Early termination**: Pass@1 = 5/20 suggests first failure at task-6; model never attempts tasks 7-20
  - **Framework version confusion**: Appendix notes React Router v7 breaking changes cause failures—LLMs generate v6 code
  - **Standards edge cases**: CSS specificity, SVG transform-origin failures indicate incomplete specification knowledge

- **First 3 experiments:**
  1. **Reproduce baseline**: Run Web-Bench with Claude 3.5 Sonnet (not 3.7) on 5 projects to verify Pass@1 ~23% before attempting improvements
  2. **Ablate retry**: Run evaluation with Pass@1 only (no retry) to measure how much error-context helps; expect ~10% drop
  3. **Analyze failure distribution**: For 3 projects, categorize task failures into (a) syntax errors, (b) logic errors caught by tests, (c) dependency errors from earlier tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Sequential dependency mechanism may overstate model limitations if early failures cascade artificially
- Paper does not quantify how often retries succeed on different task types
- Environment setup commands and Best-of-5 sampling implementation remain underspecified

## Confidence
- **High Confidence**: Sequential task dependency creates compounding difficulty (directly verified in task definitions and metric calculations)
- **Medium Confidence**: Standards + framework dual coverage increases knowledge breadth requirements (supported by task categorization but no comparative analysis of framework vs standards-only performance)
- **Medium Confidence**: End-to-end testing enforces functional correctness (methodology clear, but test strictness not analyzed)

## Next Checks
1. **Ablate retry mechanism**: Run Web-Bench evaluation with Pass@1 only (no error-context retry) across 5 projects to measure baseline performance without recovery capability
2. **Analyze failure distribution**: For 3 projects, categorize task failures into syntax errors, logic errors, and dependency errors from earlier tasks to identify primary failure modes
3. **Compare test types**: Evaluate same projects using only unit tests vs E2E tests to quantify strictness difference and determine if functional correctness is the primary bottleneck