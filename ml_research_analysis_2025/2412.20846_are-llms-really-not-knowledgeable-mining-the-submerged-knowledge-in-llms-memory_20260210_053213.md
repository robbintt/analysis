---
ver: rpa2
title: Are LLMs Really Not Knowledgeable? Mining the Submerged Knowledge in LLMs'
  Memory
arxiv_id: '2412.20846'
source_url: https://arxiv.org/abs/2412.20846
tags:
- knowledge
- hits
- correct
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a systematic gap between knowledge storage
  and expression in large language models (LLMs), revealing that models often retain
  correct information in their internal probability distributions even when generating
  incorrect answers. To quantify this phenomenon, the authors propose Hits@k, a novel
  metric that measures knowledge retention by checking if the correct answer appears
  within the top-k tokens of the model's output distribution.
---

# Are LLMs Really Not Knowledgeable? Mining the Submerged Knowledge in LLMs' Memory

## Quick Facts
- arXiv ID: 2412.20846
- Source URL: https://arxiv.org/abs/2412.20846
- Authors: Xingjian Tao; Yiwei Wang; Yujun Cai; Zhicheng Yang; Jing Tang
- Reference count: 10
- One-line primary result: LLMs retain significantly more factual knowledge than standard QA accuracy suggests, with correct answers often appearing in top-k token probabilities even when not selected.

## Executive Summary
This paper identifies a systematic gap between knowledge storage and expression in large language models (LLMs), revealing that models often retain correct information in their internal probability distributions even when generating incorrect answers. To quantify this phenomenon, the authors propose Hits@k, a novel metric that measures knowledge retention by checking if the correct answer appears within the top-k tokens of the model's output distribution. Extensive experiments across multiple datasets and models demonstrate that LLMs possess significantly more factual knowledge than standard QA accuracy metrics suggest.

## Method Summary
The paper introduces Hits@k as a metric to measure knowledge retention in LLMs by checking if the correct answer appears within the top-k tokens of the model's output distribution. The method involves running QA tasks with greedy decoding (temperature=0.0), extracting logits for top-k tokens at answer positions, and applying string matching with ≥3 consecutive characters. An "Unsure" Filtering Decoding strategy removes uninformative tokens (empty strings, "unsure," short tokens, stop words) and resamples to recover masked knowledge. The approach is validated across DBPedia, IMDB, and GoodReads datasets using LLaMA2/3, Qwen2, and Mistral models.

## Key Results
- LLaMA3-8B achieves only 17.2% Hits@1 on DBPedia but reaches 57.9% for Hits@5, indicating substantially more stored knowledge
- Filtering "unsure" tokens during decoding recovers 2–12% additional correct answers across models
- A larger model size does not mean a higher Hits@k score, suggesting capacity is not the bottleneck
- Hits@k reveals significant knowledge retention in domain-specific datasets (IMDB, GoodReads) where standard accuracy is low

## Why This Works (Mechanism)

### Mechanism 1: Latent Knowledge Retention in Token Distributions
LLMs encode factual knowledge in their output probability distributions even when this knowledge is not expressed in final outputs. Knowledge is stored in model parameters during pre-training and surfaces in logits (token probability distributions), but decoding dynamics—specifically greedy selection of only the top-1 token—mask correct answers that appear at lower ranks.

### Mechanism 2: Confidence Threshold Suppression via "Unsure" Responses
Prompting strategies that permit "unsure" responses can inadvertently suppress correct answers by discouraging low-confidence generation. When models are prompted to output "unsure" for uncertain cases, the "unsure" token or related uninformative tokens achieve higher logits than correct answers despite the model possessing the knowledge—creating a "memory-masking effect."

### Mechanism 3: Decoding Dynamics as Primary Expression Bottleneck
The storage-expression gap stems primarily from decoding strategy rather than knowledge absence or model capacity. Greedy decoding selects only the top-1 token, discarding information from the full distribution; uninformative tokens (empty strings, "unsure," stop words) can outrank correct answers due to output layer biases.

## Foundational Learning

### Concept: Logits and Token Probability Distributions
- Why needed here: The Hits@k framework and "unsure" filtering mechanism both depend on understanding how LLMs assign probabilities to vocabulary tokens before output selection.
- Quick check question: Why does examining top-k logits reveal more about model knowledge than examining only the greedy decoding output?

### Concept: Knowledge Storage vs. Knowledge Expression
- Why needed here: The paper's central thesis is that storage ≠ expression; conflating these leads to misdiagnosing expression failures as knowledge gaps.
- Quick check question: If a model outputs an incorrect answer but the correct answer appears in its top-5 logits, would you attribute this to a knowledge gap or an expression problem?

### Concept: Confidence Calibration in LLMs
- Why needed here: The "unsure" suppression mechanism operates through miscalibrated confidence; understanding why models over- or under-estimate certainty is essential for designing prompts and decoding strategies.
- Quick check question: Why might enabling an "unsure" option cause fewer correct answers to be expressed even when the model possesses the knowledge?

## Architecture Onboarding

### Component map:
Question input -> Model forward pass -> Logits extraction -> Top-k token ranking -> (Standard: Greedy decode -> Output) OR (Filtered: Remove uninformative -> Select highest informative -> Continue decoding -> Output)

### Critical path:
Question input → Model forward pass → Logits extraction → Top-k token ranking → (Standard: Greedy decode → Output) OR (Filtered: Remove uninformative → Select highest informative → Continue decoding → Output)

### Design tradeoffs:
1. **k value selection**: Small k (5–10) is efficient and captures most latent knowledge; larger k (50–100) reveals more with diminishing returns and increased noise
2. **"unsure" option in prompts**: Reduces hallucinations but may suppress correct low-confidence answers; paper shows 2–12% recovery when filtering
3. **String matching threshold**: 3 consecutive characters balances subword handling vs. false positive risk

### Failure signatures:
1. **High uninformative response rate (>50%)**: Indicates over-cautious prompting or miscalibrated uncertainty (see Figure 7)
2. **Large Hits@1-to-Hits@10 gap**: Significant storage-expression gap; correct answers present but not selected
3. **Low Hits@k even at k=100**: Suggests genuine knowledge absence rather than expression failure
4. **Domain-specific Hits@k << open-domain Hits@k**: May indicate training data coverage issues

### First 3 experiments:
1. **Baseline storage-expression gap**: Run Hits@k (k=1, 5, 10, 50, 100) on target model and dataset; compare Hits@1 vs. Hits@5/100 to determine if failures are storage- or expression-related.
2. **Quantify "unsure" suppression**: Compare prompts with vs. without "unsure" option; measure what percentage of "unsure" responses have correct answers in top-k; apply filtered decoding to measure recovery.
3. **Cross-domain and popularity analysis**: Evaluate Hits@k across open-domain (DBPedia) and domain-specific (IMDB, GoodReads) datasets; partition by entity popularity (head/torso/tail) to analyze correlation between storage-expression gap and domain/popularity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific mechanistic factors cause "unsure" or low-confidence tokens to suppress correct factual tokens during the final decoding step?
- **Basis in paper:** The authors observe that allowing "unsure" outputs "can inadvertently suppress correct answers" and attribute this to "confidence calibration or decoding dynamics," but they do not isolate the specific attention head or layer behaviors responsible for this suppression.
- **Why unresolved:** The study quantifies the phenomenon using Hits@k but stops short of performing a causal mechanistic analysis (e.g., probing internal layers) to explain *why* the model prioritizes an uninformative token over a high-probability correct answer.
- **What evidence would resolve it:** Causal tracing or layer-wise probing that identifies specific circuits where the "unsure" token probability overtakes the correct entity probability.

### Open Question 2
- **Question:** Can decoding strategies be developed that dynamically leverage the Hits@k distribution to reduce hallucinations without sacrificing recall?
- **Basis in paper:** The authors propose a static "Unsure" Filtering Decoding strategy as an "analytical probe" rather than a deployment-ready method, noting that fully automating the identification of correct answers within the top-k tokens remains challenging.
- **Why unresolved:** While the paper proves that knowledge exists in the top-k tokens, it does not provide a robust, generalizable algorithm for determining *which* of the top-k tokens is correct during inference without ground-truth access.
- **What evidence would resolve it:** A new decoding algorithm that utilizes the entropy or relative probability gaps within the top-k tokens to select correct answers more accurately than greedy decoding.

### Open Question 3
- **Question:** Why does increasing model scale improve standard accuracy but fail to yield consistent improvements in latent knowledge retention (Hits@k)?
- **Basis in paper:** The authors explicitly note that "a larger model size does not mean a higher Hits@k score" and that "rankings of LLMs based on Accuracy and Hits@k differ significantly" (e.g., LLaMA2-70B vs LLaMA3-8B).
- **Why unresolved:** The paper suggests newer models benefit from updated training data, but it does not explain the lack of scaling laws for knowledge *retention* versus *expression*, leaving the architectural or training efficiency causes unclear.
- **What evidence would resolve it:** A controlled study comparing models of identical architectures trained on the same data but differing only in parameter count to isolate the effect of scale on Hits@k.

## Limitations
- The Hits@k metric assumes that correct answers appearing in top-k logits represents genuine knowledge retention without controlling for spurious correlations
- The "unsure" suppression mechanism relies on observational evidence rather than causal mechanistic analysis
- Results may not generalize to models with different architectures or training objectives (e.g., RLHF-tuned systems)

## Confidence
- **High confidence**: LLMs show systematic storage-expression gaps measured by Hits@k differences; filtering uninformative tokens recovers 2-12% additional correct answers; the storage-expression gap correlates with domain specificity and entity popularity
- **Medium confidence**: "Unsure" prompting suppresses correct low-confidence answers through miscalibration; larger models don't necessarily show better knowledge retention; the 3-character substring matching threshold effectively handles subword tokenization
- **Low confidence**: The "memory-masking effect" specifically causes correct answers to be suppressed rather than alternative prompt-response dynamics; Hits@k scores represent true latent knowledge rather than statistical artifacts; the storage-expression gap is primarily decoding-strategy bottleneck rather than attention or training distribution issues

## Next Checks
1. **Ablation study on spurious correlations**: Compare Hits@k performance on factual questions versus similarly structured questions where correct answers are semantically plausible but factually wrong to determine whether high-probability correct answers reflect genuine knowledge versus pattern matching.

2. **Cross-architecture calibration analysis**: Test the "unsure" suppression mechanism across models with different confidence calibration properties - specifically comparing base models versus RLHF-tuned models, or models with explicit uncertainty modeling to validate whether the suppression effect is tied to calibration quality.

3. **Alternative decoding strategy comparison**: Implement and compare multiple decoding strategies (top-k sampling, nucleus sampling, beam search) to determine whether the storage-expression gap persists across all decoding approaches or is specific to greedy decoding.