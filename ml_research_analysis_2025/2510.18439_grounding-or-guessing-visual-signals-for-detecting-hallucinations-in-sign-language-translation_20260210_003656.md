---
ver: rpa2
title: Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign
  Language Translation
arxiv_id: '2510.18439'
source_url: https://arxiv.org/abs/2510.18439
tags:
- visual
- language
- reliability
- video
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses hallucination detection in sign language translation
  (SLT), where models may generate fluent text unsupported by visual evidence. Unlike
  other multimodal tasks, in SLT the video itself is the source language, making hallucinations
  equivalent to translation errors.
---

# Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation

## Quick Facts
- arXiv ID: 2510.18439
- Source URL: https://arxiv.org/abs/2510.18439
- Reference count: 40
- Primary result: Token-level reliability measure detects hallucinations in SLT with up to 97% accuracy and Spearman correlation up to 0.72

## Executive Summary
This work addresses hallucination detection in sign language translation (SLT), where models may generate fluent text unsupported by visual evidence. Unlike other multimodal tasks, in SLT the video itself is the source language, making hallucinations equivalent to translation errors. To detect these, the authors propose a token-level reliability measure that quantifies how much the decoder relies on visual input versus language priors. The method combines feature-based sensitivity (internal state changes when video is masked) with counterfactual signals (probability differences between clean and altered video inputs), aggregated into a sentence-level reliability score. Evaluated on PHOENIX-2014T and CSL-Daily datasets with both gloss-based and gloss-free models, reliability predicts hallucination rates with high accuracy (up to 97%) and strong correlation (Spearman ρ up to 0.72). It generalizes across datasets and architectures, decreases under visual degradations, and distinguishes grounded from guessed tokens. When combined with text-only signals, it further improves detection. Qualitative analysis reveals gloss-free models hallucinate more due to underuse of visual input, defaulting to language priors. Reliability thus provides a practical, reusable tool for diagnosing hallucinations in SLT and lays groundwork for multimodal hallucination detection.

## Method Summary
The method quantifies token-level reliability by measuring how much the decoder relies on visual input versus language priors. It runs three parallel decoder passes: clean video, no-video (cross-attention disabled), and mismatched video (batch-shuffled). From these, it extracts feature-based signals (hidden state angular shifts, cross-attention mass differences) and counterfactual signals (log/logit/prob margins, deltas). These signals are fused via a learned linear layer with sigmoid to produce token-level reliability scores. Reliability scores are aggregated to sentence level using tail-10 pooling (average of lowest 10% tokens) and calibrated via logistic regression (detection) or isotonic regression (CHAIR prediction). The method is evaluated on PHOENIX-2014T and CSL-Daily with SpaMo (gloss-free) and TwoStream-SLT (gloss-based) models.

## Key Results
- Reliability predicts hallucination rates with up to 97% accuracy (AUROC) and 0.72 Spearman correlation on PHOENIX-2014T
- Cross-dataset transfer works well: R_tail-10 trained on PHOENIX transfers to CSL with minimal accuracy drop
- Reliability decreases under visual degradation (noise, frame dropping) and increases when visual information is restored
- Gloss-free models show lower reliability than gloss-based models, indicating more reliance on language priors
- Combining visual reliability with text-only uncertainty signals improves detection over either alone

## Why This Works (Mechanism)

### Mechanism 1: Feature-Based Sensitivity (Internal Reliance)
- Claim: When video input genuinely influences decoder hidden states, the model is visually grounded; when states remain stable under video masking, the model is guessing from language priors.
- Mechanism: Compare decoder hidden states with and without video input using normalized angular distance (cosine similarity). Measure cross-attention mass directed to video encoder positions versus masked positions, with per-sequence quantile scaling.
- Core assumption: Large angular shifts in hidden states and increased attention to video positions indicate visual grounding rather than language prior usage. This assumes attention is meaningful and not just spurious correlation.
- Evidence anchors:
  - [abstract] "feature-based sensitivity, which measures internal changes when video is masked"
  - [Page 4, Eq. 1-2] Hidden state sensitivity: s_hid_t = π^(-1) arccos(⟨h_vid, h^0⟩ / ||h_vid|| ||h^0||); cross-attention usage with quantile scaling
  - [corpus] Limited direct corpus evidence for this specific mechanism in SLT; related work in LVLM grounding exists but with different modalities
- Break condition: If hidden state angular changes correlate poorly with hallucination rates, or if attention patterns are noisy/uninformative across different video conditions.

### Mechanism 2: Counterfactual Probability Signals (External Evidence)
- Claim: Tokens genuinely grounded in video should have higher probability under clean video than under no-video or mismatched-video conditions; probability margins quantify grounding strength.
- Mechanism: Run three parallel decoder passes with shared prefix: (1) clean video, (2) no video (cross-attention disabled), (3) mismatched video. Compute log-probability margins, logit margins, and probability advantages between clean and counterfactual conditions.
- Core assumption: If p_vid(y_t) ≈ max(p_0(y_t), p_mis(y_t)), the token is likely guessed from language priors rather than grounded in visual content. Assumes mismatched video provides a strong enough counterfactual.
- Evidence anchors:
  - [abstract] "counterfactual signals, which capture probability differences between clean and altered video inputs"
  - [Page 4-5, Eq. 3] Counterfactual distribution: p_cf(y_t) = max{p_0(y_t), p_mis(y_t)} — maximum captures stronger alternative
  - [Page 8, Table 1] Grounding-based reliability achieves up to 97% accuracy in hallucination detection
  - [corpus] OmniDPO paper addresses omni-modal hallucination with similar counterfactual reasoning; ResNetVLLM-2 addresses multi-modal hallucinations in video-language models
- Break condition: If probability margins fail to distinguish hallucinated tokens across model architectures, or if mismatched video condition is too easy (always rejected) or too hard (confuses grounding).

### Mechanism 3: Tail Pooling for Sentence-Level Aggregation
- Claim: The least reliable tokens in a sequence are disproportionately informative for detecting overall hallucination risk; averaging dilutes the signal.
- Mechanism: Instead of mean pooling token reliabilities, extract only the lowest q% (tail-10%) of token reliability scores and average them into a sentence-level measure.
- Core assumption: Hallucinations manifest as localized failures of visual grounding, so the worst tokens are predictive of overall quality. May not hold if hallucinations are distributed uniformly.
- Evidence anchors:
  - [Page 5, Section 3.3] R_tail-q = (1/⌈qT⌉) Σ over lowest q% tokens
  - [Page 20, Table 5] Tail-10 pooling improves Spearman correlation from -0.672 to -0.682 on CSL GF; -0.523 to -0.590 on PHOENIX GF
  - [corpus] No direct corpus evidence for this pooling strategy in SLT
- Break condition: If hallucinations are distributed evenly across tokens, tail pooling may be too sensitive to outliers and increase variance.

## Foundational Learning

- **Counterfactual inference in encoder-decoder models**
  - Why needed here: The core mechanism requires running multiple forward passes with perturbed inputs and comparing outputs. Without understanding how to construct valid counterfactuals (masked video, mismatched video), the reliability scores will be uninformative.
  - Quick check question: Given a decoder pass with video x producing probability p_vid(y), what would you expect p_0(y) to look like if the token is purely guessed from language priors?

- **Cross-attention in transformer decoders**
  - Why needed here: The attention-based signal s_attn_t requires extracting attention weights from decoder layers to encoder positions. Implementation requires understanding where to hook into the model and how to aggregate across layers/heads.
  - Quick check question: How would you extract cross-attention matrices A^(ℓ,h) from a HuggingFace encoder-decoder model during generation?

- **CHAIR metric and entity-level hallucination detection**
  - Why needed here: The paper adapts CHAIR (originally for image captioning) to SLT by defining content tokens and measuring hallucinated entities. Understanding this metric is essential for the regression target.
  - Quick check question: If a generated sentence contains "rain in the Alps" but the reference says "snow in the north," what tokens would CHAIR flag as hallucinated?

## Architecture Onboarding

- **Component map:**
  Feature extractor -> Counterfactual signal computer -> Token-level fusion layer -> Sentence-level pooling -> Calibration head

- **Critical path:**
  1. Extract features → 2. Compute counterfactual margins → 3. Fuse at token level → 4. Pool to sentence level → 5. Calibrate with held-out labels

- **Design tradeoffs:**
  - Mean vs. tail pooling: Mean is more stable; tail-10 captures worst-case hallucinations but higher variance
  - No-video vs. mismatched counterfactual: No-video tests pure language prior; mismatched tests visual specificity. Maximum of both is used
  - Feature-based vs. counterfactual signals: Feature-based is faster (no extra forward pass for mismatched); counterfactual is more interpretable (probability margins)

- **Failure signatures:**
  - Reliability does not decrease under visual degradation (noise, frame dropping) → signal not capturing visual usage
  - High correlation with text-only confidence but low correlation with CHAIR → grounding signal is redundant with uncertainty
  - Cross-model transfer fails (GB→GF drops significantly) → weights overfit to tokenization granularity

- **First 3 experiments:**
  1. **Sanity check**: Run reliability extraction on PHOENIX test set, plot R_tail-10 vs. CHAIR. Confirm negative correlation (ρ ≈ -0.6 to -0.7).
  2. **Ablation**: Train detection head with (a) feature-based only, (b) counterfactual only, (c) combined. Compare AUC and Spearman to isolate contribution of each signal class.
  3. **Degradation stress test**: Add Gaussian noise to video features (p ∈ {0.1, 0.2, 0.3, 0.4}), measure reliability drop. Confirm monotonic decrease (see Page 19, Tables 3-4 for expected values).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational cost of reliability estimation be reduced while maintaining detection accuracy, given that it currently requires three forward passes (clean, no-video, mismatched)?
- Basis in paper: [inferred] The method requires three parallel decoder passes per input (Algorithm 1), which is computationally expensive. The paper does not address efficiency or real-time applicability.
- Why unresolved: No ablation explores whether fewer passes (e.g., only clean vs. no-video) yield comparable performance, nor whether approximation techniques could reduce cost.
- What evidence would resolve it: A systematic ablation varying the number and type of counterfactual passes, measuring detection accuracy degradation versus computational savings.

### Open Question 2
- Question: Can reliability scores be used actively to mitigate hallucinations (e.g., through re-prompting, filtering, or decoder intervention) rather than solely for post-hoc detection?
- Basis in paper: [explicit] The authors state that "a crucial first step toward mitigating and reducing hallucinations is the ability to characterize and detect them," but the paper only addresses detection, not mitigation.
- Why unresolved: The paper establishes reliability as a diagnostic tool but does not explore intervention strategies that leverage token-level reliability during or after generation.
- What evidence would resolve it: Experiments using low-reliability tokens or sentences to trigger re-sampling, constrained decoding, or human-in-the-loop verification, measuring downstream translation quality improvements.

### Open Question 3
- Question: Does the reliability measure generalize to other sign language translation architectures beyond mBART-based and T5-based models, particularly newer end-to-end visual-LLM systems?
- Basis in paper: [inferred] The paper evaluates only two architectures (TwoStream-SLT with mBART, SpaMo with T5) and notes that "transfer of the calibration to newer gloss-free architectures is more feasible," but does not test this claim on diverse model families.
- Why unresolved: The cross-architecture transfer experiments are limited to the two tested systems, leaving open whether reliability signals are architecture-agnostic or architecture-specific.
- What evidence would resolve it: Evaluation on additional SLT architectures (e.g., SIGNUM) to assess whether reliability weights and calibration transfer effectively.

### Open Question 4
- Question: How does the choice of mismatched video for counterfactual comparison affect reliability estimation, and is batch-shuffling optimal?
- Basis in paper: [inferred] The method generates mismatches "by in-batch shuffling" (Section F.1) without exploring whether different mismatch selection strategies (random from other datasets, synthetic perturbations, semantically similar but incorrect videos) yield different reliability signals.
- Why unresolved: The counterfactual signal depends on the mismatched video, but the paper provides no analysis of how mismatch quality or characteristics influence detection performance.
- What evidence would resolve it: Systematic comparison of mismatched video selection strategies, correlating mismatch characteristics (visual similarity, semantic overlap) with reliability signal quality and hallucination detection accuracy.

## Limitations
- The method assumes cross-attention patterns and hidden-state angular distances reliably indicate grounding, but these features may be noisy or architecture-dependent
- Counterfactual design assumes video inputs are sufficiently distinct within a batch—this may fail for repetitive content or small datasets
- While the method generalizes across datasets and models, the regression heads are trained per-dataset, limiting true zero-shot transfer

## Confidence
- **High**: Detection accuracy (97% on PHOENIX, strong AUCs) - empirical performance is robust and well-validated
- **Medium**: Interpretability of internal mechanisms - attention and hidden states are plausible indicators but not directly validated as causal
- **Low**: Absolute reliability calibration across all architectures - limited testing beyond two studied models raises uncertainty about generalizability

## Next Checks
1. **Architecture stress test**: Evaluate reliability on a third SLT model (e.g., SIGNUM) to confirm generalizability beyond SpaMo and TwoStream-SLT, particularly for cross-architecture transfer
2. **Computational efficiency ablation**: Test whether detection accuracy is maintained with fewer counterfactual passes (e.g., clean vs. no-video only) to reduce computational cost
3. **Mismatch quality analysis**: Systematically vary mismatched video selection strategies (random, semantically similar, synthetic) and measure impact on reliability signal quality and detection performance