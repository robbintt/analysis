---
ver: rpa2
title: Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving
arxiv_id: '2509.02754'
source_url: https://arxiv.org/abs/2509.02754
tags:
- motion
- agents
- generation
- driving
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates how well LLM components transfer to autonomous\
  \ driving motion generation. The authors systematically study five key modules\u2014\
  tokenizer design, positional embedding, pre-training, post-training, and test-time\
  \ computing\u2014on the Waymo Sim Agents benchmark."
---

# Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving

## Quick Facts
- arXiv ID: 2509.02754
- Source URL: https://arxiv.org/abs/2509.02754
- Reference count: 40
- Primary result: LLM-inspired modules (tokenizer, positional embedding, post-training, test-time computing) improve autonomous driving motion generation when appropriately adapted to the driving domain

## Executive Summary
This paper systematically evaluates how well LLM components transfer to autonomous driving motion generation tasks using the Waymo Sim Agents benchmark. The authors investigate five key modules - tokenizer design, positional embedding, pre-training, post-training, and test-time computing - and find that most LLM-inspired techniques improve performance when adapted to the driving domain. Their optimized model achieves competitive results on the leaderboard, demonstrating that LLM modules can generalize effectively to autonomous driving when appropriately tailored. The study reveals both the potential and limitations of transferring NLP techniques to embodied AI tasks.

## Method Summary
The method adapts LLM modules for autonomous driving motion generation by treating trajectory prediction as next-token prediction. The authors implement agent-centric Verlet-Agent tokenization (169 tokens) to ensure consistent action-token mappings, Global-DRoPE positional embedding to preserve map feature semantics while capturing relative positions, and GRPO post-training to balance safety and realism. The model uses a ShapeNet encoder and 4-layer GPT decoder (5.3M parameters) trained on Waymo Open Motion Dataset with next-token prediction, followed by 1 epoch of GRPO fine-tuning. Test-time inference uses 1024 parallel rollouts with safety filtering and K-Medoids clustering to select 32 diverse outputs.

## Key Results
- Agent-centric Verlet-Agent tokenizer improves accuracy to 0.460 vs 0.454 for agent-agnostic approaches
- Global-DRoPE positional embedding reduces collision rate from 0.150 to 0.139 and off-road rate from 0.186 to 0.175
- GRPO post-training achieves highest realism score (0.728) while reducing collision rate to 0.093 vs 0.124 baseline
- Combined optimized model achieves competitive performance on Waymo Sim Agents leaderboard

## Why This Works (Mechanism)

### Mechanism 1: Agent-centric tokenization
- Claim: Verlet-Agent tokenizer improves motion generation by creating consistent token-action mappings
- Mechanism: Normalizes trajectory information to each agent's local coordinate frame before discretizing acceleration space, ensuring identical physical actions map to identical tokens regardless of agent position
- Core assumption: Consistency in token-action correspondence enables more efficient learning of motion priors
- Evidence: Verlet-Agent achieves 0.460 accuracy vs 0.454 for Verlet-Scene with lower ADE/FDE; MotionLM/Trajeglish use similar discretization but without systematic comparison

### Mechanism 2: Global-DRoPE positional embedding
- Claim: Global-DRoPE preserves semantic richness of map features while maintaining relative spatial reasoning
- Mechanism: Maintains agent/map encodings in global SDC coordinate system while applying rotary positional embeddings during cross-attention to capture relative poses
- Core assumption: Map tokens require absolute positional semantics to distinguish functionally different lane segments; relative position suffices for attention-based spatial reasoning
- Evidence: Global-DRoPE reduces collision rate (0.139 vs 0.150 for DRoPE) and off-road rate (0.175 vs 0.186); DRoPE-Traj paper cited as foundation

### Mechanism 3: GRPO post-training
- Claim: GRPO achieves better safety-realism trade-offs than REINFORCE/A2C by using within-group advantage normalization and KL regularization
- Mechanism: Normalizes returns across parallel rollouts from same initial state while KL divergence constraints prevent deviation from human-like pre-trained behavior
- Core assumption: Safety improvements should not come at the cost of deviating significantly from human driving distribution
- Evidence: GRPO achieves highest realism score (0.728) while reducing collision rate (0.093 vs 0.124 baseline); REINFORCE/A2C have lower realism despite better safety

## Foundational Learning

- **Autoregressive Next-Token Prediction**
  - Why needed here: Motion generation framed as predicting discrete motion tokens sequentially, analogous to language modeling
  - Quick check question: Can you explain why teacher-forcing training may cause exposure bias during closed-loop inference?

- **Continuous-to-Discrete Tokenization via Verlet Integration**
  - Why needed here: Trajectories are continuous but Transformers require discrete tokens; Verlet wrapper discretizes position changes via implicit acceleration encoding
  - Quick check question: Given a token representing "accelerate," how does initial velocity affect final position after N timesteps?

- **Rotary Position Embeddings (RoPE)**
  - Why needed here: Enables attention to capture relative positional relationships without explicit coordinate transformations
  - Quick check question: Why might relative position encoding be insufficient for distinguishing semantically different map elements?

## Architecture Onboarding

- **Component map:**
  Scene Encoder (ShapeNet) -> 4-layer GPT decoder -> Motion Generator -> Verlet-Agent tokenizer -> Verlet wrapper decoder

- **Critical path:**
  1. Encode scene in SDC-global coordinates via ShapeNet
  2. Initialize motion tokens from 1s history
  3. Autoregressively sample 8s future tokens (2Hz)
  4. Decode tokens to trajectories via Verlet integration
  5. Post-process with test-time search + clustering

- **Design tradeoffs:**
  - M2M attention removed: -2% accuracy, +30% speed
  - Vocabulary size (169 vs 2048): Higher accuracy, lower coverage risk
  - GRPO vs REINFORCE: Better realism-safety balance, more hyperparameter sensitivity

- **Failure signatures:**
  - Token repetition loops (token <84>): Model stuck repeating "maintain previous action," leads to collision (~0.5% of cases)
  - Initial collision states: Out-of-distribution inputs cause uncontrolled rollouts
  - Boundary-proximity scenarios: Tight turns near road edges frequently collide

- **First 3 experiments:**
  1. Tokenizer ablation: Compare SMART vs Verlet-Scene vs Verlet-Agent on held-out validation set; measure token prediction accuracy and closed-loop ADE
  2. Position embedding sanity check: Visualize lane features under DRoPE vs Global-DRoPE using PCA projection; verify Global-DRoPE produces higher variance in feature space
  3. GRPO reward shaping: Run post-training with collision-only vs collision+offroad rewards; confirm offroad penalty is necessary

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Mixture-of-Experts (MoE) architectures be successfully transferred to autonomous driving motion generation tasks?
- Basis: Authors state investigation of MoE as an unexplored direction
- Why unresolved: Study focused on dense transformers; MoE requires domain-specific adaptation for motion tokens
- What evidence would resolve it: Comparative experiments training MoE-based motion generators on Waymo data

### Open Question 2
- Question: What triggers the rare token repetition phenomenon and how can it be prevented?
- Basis: Authors report token <84> loops in ~0.5% of cases, hypothesizing progressive OOD drift
- Why unresolved: Phenomenon is rare and mechanism connecting OOD drift to repetition loops is not understood
- What evidence would resolve it: Systematic analysis of initial conditions and interventions like repetition-aware decoding

### Open Question 3
- Question: Do scaling laws continue to hold for motion generation models beyond 11.6M parameters?
- Basis: Authors note no scaling behavior study due to limited data/computation; overfitting observed even at largest scale
- Why unresolved: Largest model showed overfitting on augmented data; unclear if larger models would benefit with sufficient data diversity
- What evidence would resolve it: Training experiments with 50M-500M parameter models on larger, more diverse driving datasets

### Open Question 4
- Question: How should evaluation benchmarks balance likelihood-based realism metrics against safety when ground-truth contains noise?
- Basis: Authors document fundamental limitation where safer RL policies receive lower scores when ground-truth contains false-positive collisions (~6% of validation scenarios)
- Why unresolved: Current benchmarks penalize models deviating from flawed ground-truth distributions
- What evidence would resolve it: Development of benchmark variants decoupling likelihood matching from safety evaluation

## Limitations

- Focus on single-agent prediction within Waymo Sim Agents benchmark may not capture multi-agent interaction complexities
- Agent-centric tokenization could face challenges in highly dynamic environments with uncertain/noisy agent states
- GRPO post-training relies on simulation-based rewards that may not perfectly align with real-world safety objectives
- Global-DRoPE performance depends on assumption that global coordinate systems remain consistent across environments
- Test-time rollouts introduce computational overhead that may limit real-time deployment

## Confidence

**High Confidence** - Claims about agent-centric tokenization and Global-DRoPE effectiveness supported by systematic ablations showing consistent improvements across multiple metrics (accuracy, collision rate, off-road rate)

**Medium Confidence** - GRPO superiority over REINFORCE/A2C demonstrated but relies on specific reward shaping and hyperparameter choices; KL regularization theoretically sound but may not generalize to different driving cultures

**Low Confidence** - Claims about real-world generalizability primarily based on simulation performance; paper does not address potential domain gaps between simulated and real driving environments

## Next Checks

1. **Cross-Environment Transfer Test**: Evaluate trained model on CARLA or LGSVL with distinct map layouts and traffic patterns to assess generalization beyond Waymo benchmark

2. **Robustness to Perception Noise**: Introduce controlled sensor noise and localization errors into input trajectories and map features to test whether agent-centric tokenization and Global-DRoPE maintain performance under realistic uncertainty

3. **Multi-Agent Interaction Stress Test**: Design scenarios with complex multi-agent interactions (merging, intersections with multiple vulnerable road users) to evaluate whether LLM-inspired modules handle emergent behaviors beyond single-agent prediction