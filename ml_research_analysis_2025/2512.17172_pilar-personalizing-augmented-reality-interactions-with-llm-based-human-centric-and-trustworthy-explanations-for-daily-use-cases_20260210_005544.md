---
ver: rpa2
title: 'PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric
  and Trustworthy Explanations for Daily Use Cases'
arxiv_id: '2512.17172'
source_url: https://arxiv.org/abs/2512.17172
tags:
- user
- recipe
- explanation
- explanations
- pilar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PILAR is a framework that leverages a pre-trained LLM to generate
  personalized, human-centric explanations for real-time AI-powered AR systems, addressing
  the limitations of traditional, fragmented XAI methods. It provides context-aware,
  adaptive reasoning tailored to user needs, covering key explainability dimensions
  (when, what, how, who, where) to foster trust and engagement.
---

# PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases

## Quick Facts
- **arXiv ID:** 2512.17172
- **Source URL:** https://arxiv.org/abs/2512.17172
- **Reference count:** 36
- **One-line primary result:** PILAR's LLM-based AR interface significantly outperforms template-based methods, reducing task completion time by 40% and increasing user satisfaction and perceived transparency.

## Executive Summary
PILAR is a framework that leverages a pre-trained large language model (LLM) to generate personalized, human-centric explanations for real-time AI-powered augmented reality (AR) systems. It addresses the limitations of traditional, fragmented explainability methods by providing context-aware, adaptive reasoning tailored to individual user needs across five key dimensions (when, what, how, who, where). The system was implemented in a smartphone-based AR application for personalized recipe recommendations, integrating real-time object detection, recipe suggestions, and LLM-driven explanations. Evaluated through a user study with 16 participants, PILAR's LLM-based interface significantly outperformed a traditional template-based interface, with participants completing tasks 40% faster and reporting higher satisfaction, ease of use, and perceived transparency.

## Method Summary
PILAR employs a unified LLM-based approach to generate personalized explanations in AR applications, replacing fragmented XAI techniques with a single LLM (GPT-4o mini) using In-Context Learning. The system detects kitchen ingredients in real-time using YOLOv8-small, retrieves recipe recommendations from the Edamam API based on user dietary profiles, and generates explanations by injecting context (ingredients, user constraints, query intent) into LLM prompts. A dual-interface design allows comparison between the LLM-based system and a traditional template-based approach using SHAP, PDP, DICE, and CEM. The prototype was implemented in Unity 2022.3 for Android on a Samsung Galaxy S10, with evaluations focusing on task completion time and user trust/satisfaction metrics.

## Key Results
- LLM-based interface achieved 40% faster task completion times (163s vs 310.2s) compared to template-based interface.
- Users rated LLM explanations significantly higher for understandability, transparency, and satisfaction across all evaluation dimensions.
- System maintained real-time performance with YOLOv8-small achieving 15-20 FPS and total latency under 3 seconds for end-to-end explanation generation.

## Why This Works (Mechanism)

### Mechanism 1: Unified LLM-based Explanation Generation
The framework replaces fragmented XAI techniques (SHAP, DICE) with a single LLM that synthesizes human-centric reasoning in natural language, reducing cognitive load by eliminating the need to mentally map different visualization types to questions. Users process natural language narratives more efficiently than distinct visual widgets or template-based text blocks.

### Mechanism 2: Context-Driven Prompt Personalization
By injecting specific user constraints (dietary goals) and environmental states (detected objects) into the LLM prompt, the system enables personalized explanations that reference individual user needs. The LLM learns its "role" through In-Context Learning from the prompt examples and context objects provided at inference time.

### Mechanism 3: Adaptive Natural Language Query Mapping
The system bypasses hard-coded logic trees by passing natural language queries directly to the LLM, which infers intent (contrastive, actionable, etc.) and generates responses that mimic traditional XAI methods. This eliminates the need for explicit routing labels while maintaining the functionality of feature importance and counterfactual explanations.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed: PILAR adapts the frozen GPT-4o mini model to the "recipe explainer" domain through prompt conditioning rather than retraining.
  - Quick check: If you changed the LLM to a smaller model with a tiny context window, would PILAR's personalization mechanism still function? (Likely no, as the context object would be truncated).

- **Concept: Post-Hoc Explainability**
  - Why needed: The PILAR architecture separates the *decision* (Edamam API recipe ranking) from the *explanation* (LLM generation), introducing risks of hallucinated reasoning.
  - Quick check: Does the LLM in PILAR rank the recipes, or does it explain the ranking provided by the Edamam API?

- **Concept: YOLO (You Only Look Once) Object Detection**
  - Why needed: YOLO provides the "Where" dimension by detecting ingredients in real-time, with its output serialized into the text prompt for the LLM.
  - Quick check: What happens to the LLM's explanation if YOLO misclassifies a bell pepper as an apple?

## Architecture Onboarding

- **Component map:** Smartphone Camera (Unity/AR Foundation) -> On-device YOLOv8-small (detects ingredients) -> Edamam Recipe API (Filters/Ranks recipes) -> GPT-4o mini API (Generates explanations) -> Unity Canvas (Displays floating recipe card + explanation text).

- **Critical path:** The Perception-to-Explanation Pipeline must capture a frame, run YOLO (~50-67ms), call Edamam (1-2s), and then call the LLM (~2.5s). Real-time feel depends heavily on API latencies.

- **Design tradeoffs:** YOLO runs locally for privacy/speed, but the LLM is cloud-based, creating dependency on internet connectivity for explanations. Template methods are faster to compute but slower for users to process, while LLMs are slower to compute but result in faster user task completion.

- **Failure signatures:** Hallucinated Constraints (LLM assumes general properties), Stale Context (recipe lags behind visual reality), P3 Lag (LLM times breaking immersion).

- **First 3 experiments:**
  1. **Latency Profiling:** Measure end-to-end time from "Camera Frame Capture" to "Explanation Text Render" to identify primary bottlenecks.
  2. **Context Ablation:** Run LLM explainer *without* User Profile to verify personalization changes output quality.
  3. **Error Injection:** Feed LLM a "wrong" recommendation to see if it hallucinates justification or identifies mismatch.

## Open Questions the Paper Calls Out

- **Open Question 1:** How effectively does PILAR generalize to AR domains beyond recipe recommendation (e.g., furniture assembly, fitness coaching, navigation)?
  - Basis: Authors state "qualitative feedback suggests strong potential for PILAR's broader applicability" but only evaluated recipe context.
  - What evidence would resolve: Comparative user studies across at least two additional AR application domains.

- **Open Question 2:** How does user trust in LLM-based AR explanations evolve during extended, repeated use over days or weeks?
  - Basis: Authors note "study did not explicitly examine how user trust evolves over extended usage."
  - What evidence would resolve: Longitudinal study tracking same users' trust ratings across multiple sessions over 2â€“4 weeks.

- **Open Question 3:** How does PILAR perform on head-mounted AR displays compared to smartphone-based AR?
  - Basis: Prototype evaluated exclusively on Samsung Galaxy S10; authors acknowledge "performance on other AR devices remains untested."
  - What evidence would resolve: Cross-device user study comparing smartphone and HMD conditions.

## Limitations

- **Sample Size:** User study involved only 16 participants, limiting generalizability of quantitative results.
- **Simulation vs. Real-world:** Evaluation tasks were simulated interactions, not real-time cooking or shopping scenarios, potentially overstating practical usability.
- **AI Dependency:** System performance heavily depends on cloud API quality and latency (Edamam, GPT-4o mini), with any degradation directly impairing real-time experience.

## Confidence

- **High Confidence:** Architectural description and implementation details are clearly specified; dual-interface design allows valid A/B comparison with measurable outcomes.
- **Medium Confidence:** Claim that unified LLM is superior to stitched-together XAI methods is plausible but not definitively proven; no empirical comparison against hybrid systems.
- **Low Confidence:** Paper lacks evidence that LLM-generated explanations are faithful to actual recommendation logic; risk of generating convincing but incorrect justifications.

## Next Checks

1. **Faithfulness Audit:** Conduct blind study where human evaluators rate plausibility and accuracy of LLM explanations against ground-truth Edamam API logic to measure hallucination extent.
2. **Stress Test Latency:** Measure end-to-end system latency under degraded network conditions (3G, 50% packet loss) to identify minimum viable bandwidth for "real-time" experience.
3. **User Diversity Study:** Replicate user study with larger, more diverse sample (n>50, varied ages, technical literacy, dietary restrictions) to test robustness of reported benefits.