---
ver: rpa2
title: Imperceptible Jailbreaking against Large Language Models
arxiv_id: '2510.05025'
source_url: https://arxiv.org/abs/2510.05025
tags:
- imperceptible
- variation
- jailbreaks
- malicious
- selectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces imperceptible jailbreaks that exploit invisible
  Unicode variation selectors to bypass LLM safety alignment without visible modifications.
  The method appends invisible variation selectors to malicious prompts, altering
  their tokenization while maintaining visual indistinguishability.
---

# Imperceptible Jailbreaking against Large Language Models

## Quick Facts
- arXiv ID: 2510.05025
- Source URL: https://arxiv.org/abs/2510.05025
- Reference count: 33
- Key outcome: Attack achieves up to 100% ASR against four aligned LLMs by appending invisible Unicode variation selectors to malicious prompts

## Executive Summary
This paper introduces a novel jailbreaking technique that exploits invisible Unicode variation selectors to bypass LLM safety alignment without visible text modifications. The method appends these invisible characters to malicious prompts, altering their tokenization while maintaining visual indistinguishability. A chain-of-search pipeline optimizes these invisible suffixes to maximize the likelihood of harmful responses. Experiments demonstrate high attack success rates against Vicuna, Llama-2-Chat, Llama-3.1, and Mistral, outperforming traditional visible jailbreak methods.

## Method Summary
The attack uses invisible Unicode variation selectors (U+FE00–U+FE0F and U+E0100–U+E01EF) that are encoded as distinct multi-token blocks by LLM tokenizers. The chain-of-search optimization pipeline initializes a suffix of L variation selectors, then iteratively mutates M contiguous selectors to maximize the log-likelihood of target-start tokens (e.g., "Sure"). After each round, successful suffix/target-token pairs bootstrap the search for unsolved queries. The method was tested on 50 malicious questions from AdvBench, achieving up to 100% attack success rate against four aligned LLMs.

## Key Results
- Achieves up to 100% attack success rate against Vicuna, Llama-2-Chat, Llama-3.1, and Mistral
- Outperforms traditional visible jailbreak methods on all tested models
- Successfully generalizes to prompt injection attacks using 50 samples from Open Prompt Injection dataset
- Optimization is query-dependent but shows transferability patterns across rounds (2-5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Appending invisible Unicode variation selectors alters the discrete token sequence input to the model without changing the prompt's visual rendering.
- Mechanism: The method exploits 256 Unicode variation selectors that LLM tokenizers encode as specific multi-token blocks (typically 3-4 tokens per selector), injecting a "hidden" adversarial perturbation vector.
- Core assumption: The target LLM's tokenizer retains and uniquely encodes variation selectors rather than stripping them.
- Evidence anchors: Section 3.1 defines Unicode ranges; Table 1 shows specific selectors map to distinct token IDs in Llama-2.
- Break condition: If the input pipeline normalizes text by removing variation selectors, the attack surface collapses.

### Mechanism 2
- Claim: Optimized invisible suffixes shift model attention away from harmful semantic content toward the suffix itself, diluting refusal triggers.
- Mechanism: The chain-of-search optimizes suffixes to maximize the log-likelihood of affirmative target-start tokens, redistributing attention weights and reducing focus on trigger words.
- Core assumption: Safety alignment relies significantly on attention mechanisms that flag specific harmful keywords.
- Evidence anchors: Section 4.4 notes the model's attention shifts toward appended invisible suffixes; Figure 8 visualizes this attention shift.
- Break condition: If the safety mechanism operates on a global semantic classifier independent of attention distribution, the distraction effect will fail.

### Mechanism 3
- Claim: A bootstrapped random search (Chain-of-Search) is required to overcome the constrained search space of invisible characters.
- Mechanism: The method uses random search (modifying M=10 contiguous selectors) and reuses successful suffix/target-token pairs from solved queries to initialize the search for unsolved queries.
- Core assumption: Adversarial suffixes learned for one malicious query contain transferable sub-patterns effective for other queries.
- Evidence anchors: Section 3.2 describes the pipeline where successful suffixes are retained as new initialization; Figure 5 shows success rates improve over rounds.
- Break condition: If the solution space is highly query-specific with near-zero transferability, the chain-of-search will fail to bootstrap.

## Foundational Learning

- **Unicode Tokenization & Normalization**: Why needed: The attack relies on the tokenizer treating invisible codepoints as valid, distinct tokens. Quick check: Does the target model's tokenizer increase sequence length when invisible variation selectors are appended, or are they stripped during pre-processing?

- **Log-Likelihood of Target Tokens**: Why needed: The search algorithm optimizes for the probability of the model emitting a "Start Token" rather than the full harmful output. Quick check: If the model assigns high probability to "Sure" but follows it with "I cannot help," has the attack succeeded?

- **Attention Mechanisms in Transformers**: Why needed: The paper explains the jailbreak via attention redirection. Understanding how attention layers weigh input tokens is necessary to interpret why an invisible suffix bypasses safety training. Quick check: If an attention head assigns low weight to "bomb" in a prompt, is it more or less likely to trigger a safety refusal?

## Architecture Onboarding

- **Component map**: Input Preprocessing (Q + S) -> Tokenizer (converts S to multi-token blocks) -> Search Loop (random search over M selectors) -> Scorer (checks log-probability of Target-Start Tokens) -> Buffer (stores successful pairs)

- **Critical path**:
  1. Target Token Selection: Incorrectly selecting "Sure" for a model that prefers "Title" will stall optimization.
  2. Suffix Length (L): Setting L too low (e.g., 400) prevents building sufficient "distraction" capacity for robust models.

- **Design tradeoffs**:
  - Suffix Length (L) vs. Search Cost: Longer suffixes (1200) provide more capacity but require more memory to process extended token sequences.
  - Step Size (M): Modifying too many selectors (M > 10) per iteration may destabilize the search.

- **Failure signatures**:
  - Stagnant Log-Prob: If log-probability of target token doesn't increase over 10,000 iterations, search space is too constrained.
  - High Perplexity Defenses: If the model rejects the prompt due to "unnatural" token distribution, the attack is detected.
  - Zero Transfer: If Round 1 succeeds but Rounds 2-5 show no improvement, transferability assumption is failing.

- **First 3 experiments**:
  1. Tokenizer Stress Test: Verify if the target model's tokenizer preserves invisible variation selectors by checking sequence length changes.
  2. Target Token Mapping: Identify which affirmative tokens (e.g., "Sure", "Here", "To") are most probable at the start of successful completions.
  3. Length Ablation: Run the attack with suffix lengths L=200, 800, 1200 to find the minimal effective length.

## Open Questions the Paper Calls Out

- **Can adaptive attacks leveraging invisible characters be developed to circumvent perplexity-based input filtering or output monitoring defenses?** The authors suggest developing adaptive attacks capable of circumventing these defenses, specifically referring to perplexity filters and harmfulness probes.

- **Does the imperceptible jailbreak method maintain high attack success rates against proprietary, closed-source models such as GPT-4?** While GPT tokenizers process variation selectors similarly to open-source models, the optimization pipeline was only validated on open-source models where log-likelihoods are accessible.

- **How effective are input normalization or sanitization techniques at mitigating this specific class of Unicode-based attacks?** The study assumes the prompt enters the tokenizer intact and does not test defensive scenarios where a firewall normalizes Unicode before tokenization.

## Limitations
- Effectiveness depends on specific tokenizer behaviors that may vary across models and deployment environments
- Generalizability to production-grade models (GPT-4, Claude) remains untested
- Assumes target model processes the full prompt including invisible characters without normalization or filtering

## Confidence

- **High Confidence**: The fundamental mechanism of using invisible Unicode variation selectors is well-established through systematic testing; experimental results showing successful jailbreaks on four tested models are reproducible and directly verifiable.
- **Medium Confidence**: The claim that optimized invisible suffixes redirect model attention away from harmful content relies heavily on attention visualization, though the causal link requires further validation across different attention heads.
- **Low Confidence**: The transferability claim across different malicious queries through chain-of-search bootstrapping is supported by round-by-round success rate improvements, but the underlying assumption lacks rigorous ablation studies.

## Next Checks

1. **Tokenizer Normalization Test**: Test the attack against a production API that implements Unicode normalization (NFC/NFKC) to verify whether invisible variation selectors are preserved or stripped.

2. **Attention Head Ablation**: Run the attack with specific attention heads disabled or masked to determine whether jailbreak success correlates with attention redistribution or other mechanisms.

3. **Cross-Query Transferability Quantification**: Measure success rate when bootstrapping is disabled versus enabled across multiple rounds to quantify the actual contribution of transferability to the attack's efficiency.