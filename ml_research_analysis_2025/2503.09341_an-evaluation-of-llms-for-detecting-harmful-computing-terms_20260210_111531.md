---
ver: rpa2
title: An Evaluation of LLMs for Detecting Harmful Computing Terms
arxiv_id: '2503.09341'
source_url: https://arxiv.org/abs/2503.09341
tags:
- harmful
- terms
- language
- detection
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for detecting
  harmful and non-inclusive terminology in technical contexts. Researchers tested
  encoder, decoder, and encoder-decoder models using a standardized prompt to classify
  64 technical terms as harmful or non-harmful.
---

# An Evaluation of LLMs for Detecting Harmful Computing Terms
## Quick Facts
- arXiv ID: 2503.09341
- Source URL: https://arxiv.org/abs/2503.09341
- Reference count: 19
- Primary result: Decoder models outperform encoders in harmful computing term detection with Gemini Flash 2.0 achieving 44/64 accuracy

## Executive Summary
This study systematically evaluates large language models for detecting harmful and non-inclusive terminology in technical contexts. Researchers tested encoder, decoder, and encoder-decoder models using zero-shot classification on a standardized prompt to classify 64 technical terms as harmful or non-harmful. The results demonstrate that decoder models, particularly Gemini Flash 2.0 and Claude AI, excel in contextual analysis and nuanced classification through descriptive reasoning rather than binary outputs. Encoder models like BERT-base-uncased show strong pattern recognition but struggle with classification certainty, producing probability scores near 0.50 that indicate uncertainty.

## Method Summary
The evaluation employed zero-shot classification across multiple model architectures using a standardized prompt to identify harmful or non-inclusive language. Researchers tested BERT-base-uncased, RoBERTa large-mnli, T5-large, BART-large-mnli, Gemini Flash 1.5/2.0, Claude AI Sonnet 3.5, and GPT-4 on a curated dataset of 64 technical terms with contextual use cases. Encoder and encoder-decoder models were evaluated via Hugging Face's zero-shot pipeline with binary labels, while decoder models were accessed through web interfaces. Each model was prompted three times consecutively to assess consistency, with accuracy measured as the proportion of correctly identified harmful terms.

## Key Results
- Decoder models achieved highest accuracy: Gemini Flash 2.0 (44/64 correct) and Claude AI (37/64)
- Encoder models showed uncertainty with BERT-base-uncased producing probability scores near 0.50
- GPT-4 demonstrated inconsistent performance, declining from 35 to 24 correct identifications across three iterations
- T5-large produced repetitive, non-informative outputs in zero-shot classification mode

## Why This Works (Mechanism)
### Mechanism 1
Decoder models outperform other architectures because their generative approach enables descriptive reasoning rather than forced binary classification. By generating explanatory outputs that contextualize why a term is harmful, decoder models reduce false positives/negatives compared to rigid probability thresholds used by encoder models. This mechanism assumes that nuanced contextual explanations improve detection accuracy over binary label outputs.

### Mechanism 2
Zero-shot classification with task-specific prompts enables cross-architecture comparison but reveals architecture-specific weaknesses. When models are prompted identically without fine-tuning, inherent architectural biases become apparent. T5-large produced "repetitive, non-informative outputs" while BERT showed uncertainty near 0.50 probability scores, demonstrating that standardized prompts fairly expose each architecture's capabilities.

### Mechanism 3
Detection consistency varies across model iterations, with some decoder models showing degradation over repeated prompts. GPT-4's accuracy declined from 35→28→24 across three iterations, suggesting prompt sensitivity or session-level drift. This indicates that consistent performance across iterations is crucial for production deployment and highlights the importance of testing for iteration-level stability.

## Foundational Learning
- **Concept: Encoder vs. Decoder Architectures** - Why needed: The paper explicitly compares these architectures as the core research question. Encoders produce fixed representations while decoders generate sequential text. Quick check: Can you explain why a generative decoder might produce more nuanced harmful term explanations than a classification-focused encoder?
- **Concept: Zero-Shot Classification** - Why needed: The evaluation methodology relies on zero-shot prompting without task-specific training data. Quick check: What are the tradeoffs of using zero-shot classification versus fine-tuning for a specialized domain like harmful computing terminology?
- **Concept: Binary vs. Descriptive Output Formats** - Why needed: A key finding is that descriptive outputs outperform simple harmful/non-harmful labels. Quick check: How would you modify a BERT-based classifier to provide explanatory outputs instead of just binary predictions?

## Architecture Onboarding
- **Component map:** Input prompt + 64-term dataset → Encoder models (BERT, RoBERTa) → probability scores; Encoder-decoder models (T5, BART) → text classification; Decoder models (Gemini, Claude, GPT-4) → descriptive explanations → Accuracy evaluation
- **Critical path:** 1) Curate harmful terms database with contextual use cases 2) Design standardized prompt for cross-model comparison 3) Configure each model via appropriate interface 4) Run 3 iterations per model 5) Evaluate accuracy and analyze misclassification patterns
- **Design tradeoffs:** Accuracy vs. interpretability (decoders provide better explanations but require more compute); Consistency vs. flexibility (GPT-4 shows high variability; Gemini/Claude more stable); Binary vs. descriptive (binary classification enables easier aggregation but loses nuance)
- **Failure signatures:** BERT scores clustering near 0.50 → uncertainty; T5 repetitive outputs → model not suited for zero-shot classification; GPT-4 accuracy declining → session drift; False negatives on subtle terms → contextual understanding gaps
- **First 3 experiments:** 1) Reproduce baseline BERT-base-uncased and Gemini Flash 2.0 to validate reported accuracy and observe score distribution 2) Test prompt variations with explicit definitions of "harmful" to probe architecture vs. prompt dependency 3) Fine-tune T5-large on harmful terms dataset to measure improvement from zero-shot performance

## Open Questions the Paper Calls Out
- Can LLMs reliably suggest contextually appropriate inclusive alternatives when prompted to replace harmful terms in technical sentences?
- Can decoder models accurately explain the specific rationale for why a term is considered harmful in a technical context?
- Does the superior performance of decoder models persist when analyzing full technical documents or code comments rather than isolated terms?
- To what extent does prompt composition impact the stability and consistency of harmful term detection across different model architectures?

## Limitations
- The exact composition of the 64 harmful terms dataset is not provided, limiting independent verification
- Ground truth labels and classification methodology for what constitutes "harmful" vs "non-harmful" are not fully specified
- Handling of T5-large's "inconclusive" outputs in accuracy calculation is unclear
- The comparison assumes zero-shot prompting without fine-tuning, potentially disadvantaging architectures that require task-specific training

## Confidence
- **High confidence:** Decoder models achieve higher accuracy than encoder models on the specific 64-term classification task
- **Medium confidence:** Descriptive generation approach reduces false classifications compared to binary probability outputs
- **Low confidence:** Iteration-level performance degradation in GPT-4 is representative of general decoder model instability

## Next Checks
1. Reconstruct and publicly release the complete 64-term dataset with ground truth labels to enable reproducibility
2. Test whether the decoder advantage persists when encoder models are configured to generate explanatory outputs rather than binary probabilities
3. Fine-tune T5-large on the harmful terms dataset and re-run the evaluation to determine if poor zero-shot performance reflects architectural limitations or training requirements