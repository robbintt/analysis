---
ver: rpa2
title: Optimising the attribute order in Fuzzy Rough Rule Induction
arxiv_id: '2506.02805'
source_url: https://arxiv.org/abs/2506.02805
tags:
- fuzzy
- rule
- attributes
- frri
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether optimising the order of attributes
  in the Fuzzy Rough Rule Induction (FRRI) algorithm improves its performance. FRRI
  is a rule induction algorithm that uses fuzzy rough set theory to generate interpretable
  decision rules.
---

# Optimising the attribute order in Fuzzy Rough Rule Induction

## Quick Facts
- arXiv ID: 2506.02805
- Source URL: https://arxiv.org/abs/2506.02805
- Reference count: 23
- Simply reordering attributes in FRRI does not improve performance, but removing up to 10% of attributes using FRFS before reordering does improve balanced accuracy and reduces rule length.

## Executive Summary
This study investigates whether optimising attribute order in the Fuzzy Rough Rule Induction (FRRI) algorithm improves its performance. FRRI is a rule induction algorithm that uses fuzzy rough set theory to generate interpretable decision rules. The authors hypothesised that reordering attributes during the rule shortening step might lead to better results. Through experiments on 18 benchmark datasets comparing balanced accuracy, number of rules, and average rule length, the primary finding is that simply reordering attributes does not significantly impact FRRI's performance. However, removing a small number (up to 10%) of attributes using fuzzy rough feature selection, combined with reordering, does lead to improvements.

## Method Summary
The study evaluates three feature selection methods (fuzzy rough feature selection, mutual information-based, and correlation-based) to reorder and/or reduce attributes before applying FRRI. The FRRI algorithm generates one rule per training object with all attributes initially set to "similar," then shortens each rule via greedy attribute removal, and finally selects a minimal covering ruleset via integer programming. Experiments were conducted on 18 numerical benchmark datasets from the KEEL repository using 10-fold cross-validation. The authors compared balanced accuracy, number of rules, and average rule length across different preprocessing approaches: control (no reordering), reorder-only, and reorder combined with attribute retention at various percentages.

## Key Results
- Simply reordering attributes does not significantly improve FRRI performance across multiple metrics
- Removing up to 10% of attributes using fuzzy rough feature selection combined with reordering improves balanced accuracy and reduces average rule length
- Removing more than 10% of attributes (especially with full reduct) degrades performance significantly, with accuracy dropping and ruleset size exploding
- FRFS outperforms mutual information and correlation-based methods for combined removal+reordering approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attribute reordering alone does not improve FRRI performance because the greedy rule shortening process evaluates attributes independently per object.
- Mechanism: FRRI's rule shortening phase iterates through attributes for each object, testing whether each condition can be set to unused, dominant, dominated, or similar while maintaining discriminative coverage. This per-object greedy selection effectively identifies relevant attributes regardless of their initial position in the input sequence.
- Core assumption: The greedy search adequately explores the condition-type space for each attribute-objection pair.
- Evidence anchors:
  - [abstract] "optimising only the order of attributes using known methods from fuzzy rough set theory and classical machine learning does not improve the performance of FRRI on multiple metrics"
  - [Section 3.2] Describes the greedy rule_prune procedure applied independently per object
  - [Section 5.2] "the greedy algorithm used in the rule reduction of FRRI effectively identifies the most relevant attributes to represent an object, regardless of their initial order"
- Break condition: If the greedy search were replaced with an exact optimization (mentioned as future work in Section 6), attribute ordering might become relevant.

### Mechanism 2
- Claim: Removing a small percentage (up to 10%) of attributes via FRFS before FRRI improves balanced accuracy and reduces rule length because FRFS targets globally redundant features that provide minimal contribution to the fuzzy positive region.
- Mechanism: QuickReduct (FRFS) iteratively adds attributes that maximize the degree of dependency γ_B (Eq. 6), producing an ordering from most to least discriminative. Attributes not selected early are those with low marginal contribution to the positive region across all objects. Removing the bottom 10% eliminates features that are unlikely to be useful for any object, reducing search complexity without sacrificing discriminative power.
- Core assumption: Attributes with low global dependency contribution are also low-value for individual object discrimination.
- Evidence anchors:
  - [abstract] "removing a small number of attributes using fuzzy rough feature selection during this step positively affects balanced accuracy and the average rule length"
  - [Section 4.1] Describes QuickReduct and its use of γ_B for attribute selection
  - [Section 5.2] "ofrfs with 90% of the attributes performs the best... a one-sided Wilcoxon test to determine whether ofrfs-.9 is more accurate than the control is weakly significant"
  - [corpus] Related work on fuzzy rough feature selection (GBFRS) supports robustness of fuzzy rough methods for attribute reduction, though no direct citation comparison is available.
- Break condition: If datasets contain many locally-relevant-but-globally-irrelevant features, this mechanism fails (see Mechanism 3).

### Mechanism 3
- Claim: Removing more than 10% of attributes degrades performance because global feature selection eliminates attributes that, while redundant overall, may be critical for distinguishing specific objects.
- Mechanism: FRRI performs object-specific attribute removal—different objects may require different attribute subsets to distinguish them from neighbors. Global feature selection (FRFS, MI, correlation) removes attributes uniformly across all objects, potentially eliminating features that are locally essential for certain objects but globally redundant.
- Core assumption: The tested datasets contain object-specific attribute dependencies that are not captured by global feature importance measures.
- Evidence anchors:
  - [Section 5.2] "different objects may require distinct subsets of attributes to be accurately distinguished... the feature selection methods might remove attributes that are irrelevant to the dataset as a whole but which are crucial for discerning a particular object from its neighbours"
  - [Figure 1] Shows accuracy declining when less than 80-90% of attributes are retained
  - [Section 5.2] "ofrfs-0 is significantly worse than all other algorithms" (ofrfs-0 = stop when γ_B = 1)
  - [corpus] No direct corpus evidence on this object-specific vs. global tension; this remains an author hypothesis.
- Break condition: If an exact optimization approach were used for rule shortening (per future work), the sensitivity to global feature removal might decrease.

## Foundational Learning

- Concept: **Fuzzy Rough Sets**
  - Why needed here: FRRI combines fuzzy set membership (gradual similarity) with rough set approximations (lower/upper bounds) to handle uncertainty and vagueness in attribute values. The fuzzy lower approximation (Eq. 1) and fuzzy upper approximation (Eq. 2) define how objects belong to decision classes based on fuzzy indiscernibility.
  - Quick check question: Given a fuzzy relation R and fuzzy set A, can you compute the membership of an object u in the fuzzy lower approximation A^I_R?

- Concept: **Fuzzy Indiscernibility and Dominance Relations**
  - Why needed here: FRRI uses R_i (Eq. 3) for similarity-based comparison and R_d (Eq. 4) for ordered comparison. These relations define how objects are compared and aggregated via t-norms (Eq. 5) to form fuzzy B-indiscernibility relations.
  - Quick check question: For two objects u, v with normalized attribute values a(u)=0.3 and a(v)=0.7, what is the fuzzy indiscernibility R_i(u, v)?

- Concept: **Rule Induction and Covering Sets**
  - Why needed here: FRRI generates rules from training objects, shortens them via greedy attribute removal, and selects a minimal covering set via integer programming (Section 3.3). Understanding the rule format (similar/dominant/dominated conditions) and matching degree M_r(v) is essential for interpreting results.
  - Quick check question: How does the covering set S_r(v) combine the antecedent matching degree with the consequent?

## Architecture Onboarding

- Component map:
  1. **Preprocessing**: Min-max normalization of numerical attributes (Section 2.3)
  2. **Feature Selection (optional)**: QuickReduct (FRFS), mutual information, or correlation-based ordering and/or reduction (Section 4)
  3. **Rule Shortening**: Greedy per-object attribute removal with condition type selection (Section 3.2)
  4. **Rule Selection**: Integer programming to minimize ruleset size while maintaining full coverage (Section 3.3, uses gurobipy)
  5. **Inference**: Class assignment via maximum covering degree across rules (Section 3.4)

- Critical path:
  1. Normalize all numerical attributes to [0,1]
  2. (Recommended) Apply FRFS with 10% attribute removal
  3. Generate total rules (one per training object, all attributes "similar")
  4. Shorten each rule via greedy attribute removal
  5. Solve integer program to select minimal covering ruleset
  6. For inference, compute covering degree S_r(v) for each rule, assign class with highest coverage

- Design tradeoffs:
  - **Reordering only vs. removal + reordering**: Reordering alone has no effect; removal of 10% with FRFS improves accuracy/rule length but increases ruleset size slightly
  - **FRFS vs. MI/Correlation**: FRFS outperforms MI and correlation-based methods for combined removal+reordering; MI/Correlation show no improvement over baseline
  - **Removal percentage**: 10% removal is optimal; 20%+ removal degrades accuracy significantly; full reduct (ofrfs-0) performs worst
  - **Per-object vs. global selection**: FRRI's per-object attribute removal is more flexible than global feature selection, but the two can complement each other when global removal is limited to 10%

- Failure signatures:
  - Accuracy drops sharply when >20% of attributes are removed (Figure 1 shows inflection point)
  - Ruleset size explodes when too many attributes are removed pre-FRRI (Table 3: mean rules increase from 106 to 243 when using full reduct)
  - ofrfs-0 (full reduct) produces worst accuracy (mean 0.525 vs. 0.703 baseline)
  - Correlation-based methods (pcc-0.8, pcc-0.9) underperform FRFS counterparts

- First 3 experiments:
  1. **Baseline validation**: Run FRRI on a KEEL dataset without feature selection, record balanced accuracy, ruleset size, and average rule length. Compare to Table 2-4 control columns.
  2. **FRFS 10% removal test**: Apply QuickReduct to get ordered attributes, remove bottom 10%, run FRRI. Verify that rule length decreases and accuracy matches or exceeds baseline (one-sided Wilcoxon p≈0.071).
  3. **Sensitivity analysis**: Test FRFS with 20%, 30%, 50% removal on a high-dimensional dataset (e.g., sonar with 60 features). Confirm accuracy degradation and ruleset size explosion as predicted by Figure 1 and Table 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Fuzzy Rough Rule Induction (FRRI) algorithm be effectively adapted for regression and ordinal classification problems?
- Basis in paper: [explicit] The authors state in Section 6 (Future Work) their intent to adapt the algorithm for these specific problem types.
- Why unresolved: The current implementation assumes a categorical decision attribute (Section 2.3) and relies on fuzzy lower approximations defined for distinct decision classes.
- What evidence would resolve it: A modified FRRI framework capable of handling continuous decision values or ordered classes, validated on regression and ordinal benchmark datasets.

### Open Question 2
- Question: Does reformulating the rule shortening step as an exact optimisation problem improve performance compared to the current greedy heuristic?
- Basis in paper: [explicit] Section 6 lists reformulating the rule shortening step as an optimisation problem as a goal, to circumvent the need for attribute ordering.
- Why unresolved: The study concluded that attribute ordering does not significantly impact the current greedy approach (Section 5.2), suggesting the heuristic itself may be the limiting factor.
- What evidence would resolve it: Comparative experiments between the greedy FRRI and an exact optimisation version, measuring rule minimality and classification accuracy.

### Open Question 3
- Question: Can evolutionary fuzzy algorithms be applied to the FRRI ruleset to increase test accuracy and reduce overfitting?
- Basis in paper: [explicit] The authors propose in Section 6 applying evolutionary fuzzy algorithms to the generated ruleset to improve generalization.
- Why unresolved: While FRRI generates interpretable rules, the paper does not evaluate the potential overfitting of the "minimal covering set" on unseen data, nor does it tune the rules post-induction.
- What evidence would resolve it: Experimental results comparing the balanced accuracy of the standard FRRI ruleset against an evolutionarily-tuned ruleset on test folds.

### Open Question 4
- Question: Does constructing hierarchical rulesets by combining similar rules improve the explainability and compactness of the model?
- Basis in paper: [explicit] Section 6 identifies the construction of hierarchical rulesets as a future objective to reduce the number of rules and improve explainability.
- Why unresolved: The current algorithm produces a flat list of rules; the potential for creating more general, higher-level rules from the existing output remains unexplored.
- What evidence would resolve it: A prototype implementation that aggregates low-level rules into a hierarchy, evaluated using complexity metrics and user interpretability studies.

## Limitations
- The study excludes categorical attributes, limiting generalizability to mixed-attribute datasets
- The optimal 10% removal threshold was not systematically tuned across all datasets
- The mechanism explaining why global feature selection fails for object-specific attribute dependencies remains a hypothesis without direct empirical validation
- The study does not evaluate whether the FRRI ruleset overfits to training data

## Confidence
- **High**: FRRI's rule shortening is order-independent; removing up to 10% of attributes with FRFS improves balanced accuracy and rule length
- **Medium**: The optimal 10% removal threshold is robust across datasets; FRFS outperforms MI and correlation-based methods
- **Low**: The object-specific vs. global attribute dependency tension is the primary reason for accuracy degradation beyond 10% removal

## Next Checks
1. **Exact Optimization Test**: Replace the greedy rule shortening with an exact optimization method and re-evaluate whether attribute ordering becomes relevant
2. **Cross-Dataset Threshold Tuning**: Systematically vary the attribute removal percentage (5%, 10%, 15%, 20%) across all datasets to identify if 10% is truly optimal or dataset-specific
3. **Categorical Attribute Extension**: Modify FRRI to handle categorical attributes and test whether the 10% FRFS + reordering recommendation holds for mixed-attribute datasets