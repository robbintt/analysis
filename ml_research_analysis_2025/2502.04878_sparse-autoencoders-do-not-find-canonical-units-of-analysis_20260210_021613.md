---
ver: rpa2
title: Sparse Autoencoders Do Not Find Canonical Units of Analysis
arxiv_id: '2502.04878'
source_url: https://arxiv.org/abs/2502.04878
tags:
- latents
- saes
- latent
- features
- smaller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the idea that sparse autoencoders (SAEs)
  can find a unique, complete, and irreducible ("canonical") set of interpretable
  features in neural networks. The authors introduce two novel methods: SAE stitching,
  which shows that smaller SAEs are incomplete by revealing novel features in larger
  ones, and meta-SAEs, which demonstrate that larger SAE latents are often compositions
  of smaller ones rather than atomic units.'
---

# Sparse Autoencoders Do Not Find Canonical Units of Analysis

## Quick Facts
- **arXiv ID**: 2502.04878
- **Source URL**: https://arxiv.org/abs/2502.04878
- **Reference count**: 28
- **Primary result**: Sparse autoencoders do not identify unique, complete, and irreducible "canonical" features in neural networks; larger SAEs learn composed features rather than atomic units.

## Executive Summary
This paper challenges the prevailing assumption that sparse autoencoders can discover a unique set of interpretable, atomic features ("canonical units") in neural networks. Through two novel methods—SAE stitching and meta-SAEs—the authors demonstrate that SAEs of different sizes capture overlapping but distinct feature sets. Larger SAEs both refine existing features and learn entirely new ones, while also forming composed latents that decompose into simpler concepts. Using GPT-2 and Gemma models, they show that feature representation is inherently hierarchical rather than atomic, suggesting practitioners should pragmatically select SAE sizes based on specific interpretability tasks rather than seeking universal solutions.

## Method Summary
The authors introduce SAE stitching, which swaps latents between differently-sized SAEs to identify "novel" features in larger SAEs that improve reconstruction when added to smaller ones. They also develop meta-SAEs, which are trained on the decoder directions of standard SAEs to decompose latents into interpretable meta-features. Both methods are applied to SAEs of varying widths (e.g., 768 vs 1536 latents) trained on GPT-2 Small and Gemma 2 models, using reconstruction error (MSE) and cosine similarity thresholds to evaluate completeness and atomicity of features.

## Key Results
- Larger SAEs contain latents that significantly improve reconstruction when stitched into smaller SAEs, proving smaller SAEs are incomplete
- Meta-SAEs successfully decompose "Einstein" latents into interpretable components like "scientist," "Germany," and "famous person"
- No consistent feature set emerges across SAE sizes, challenging the notion of canonical units
- The composition of features in larger SAEs appears driven by sparsity constraints favoring fewer active latents

## Why This Works (Mechanism)

### Mechanism 1: Sparsity-Induced Composition
The SAE loss function penalizes the number of active latents ($L_0$). As dictionary width grows, the optimizer can satisfy sparsity constraints more effectively by allocating a single latent to frequently co-occurring concept combinations (1 active unit) rather than activating multiple atomic latents (2+ active units). This creates composed features like "blue square" instead of separate "blue" and "square" latents.

### Mechanism 2: Capacity-Limited Incompleteness (Stitching)
Smaller SAEs lack capacity to model low-frequency or highly specific features found in larger SAEs. SAE stitching identifies "novel latents" in larger SAEs (low cosine similarity with smaller SAE latents) that reduce reconstruction error when added, proving they capture information previously missing due to capacity constraints.

### Mechanism 3: Linear Subspace Decomposition (Meta-SAEs)
SAE decoder directions are not atomic unit vectors but linear combinations of more fundamental "meta-features." A Meta-SAE trained on decoder matrix columns can reconstruct SAE latents through sparse activation of interpretable meta-latents, demonstrating that seemingly atomic features are actually compositions.

## Foundational Learning

- **Concept: Sparse Dictionary Learning**
  - Why needed here: This is the fundamental operation of an SAE—decomposing dense activations into a sparse combination of "dictionary" vectors
  - Quick check question: If you increase the sparsity penalty ($\lambda$), does an SAE tend to create more specific "composed" features or broader "atomic" features? (Answer: It forces the model to be more efficient, favoring composed features that activate singly)

- **Concept: Cosine Similarity in Activation Space**
  - Why needed here: The paper relies on cosine similarity to match latents across different SAE sizes (stitching) and to validate Meta-SAEs against smaller SAEs
  - Quick check question: Two latents have a cosine similarity of 0.9. Does this mean they activate on the exact same inputs? (Answer: Not necessarily, but they point in similar directions in the residual stream, implying similar causal effects on the output)

- **Concept: Polysemanticity vs. Monosemanticity**
  - Why needed here: The paper challenges the idea that SAEs resolve polysemantic neurons into monosemantic "canonical" units
  - Quick check question: A neuron activates for "apples" and "cars". Is it polysemantic? (Answer: Yes. An SAE tries to split this into two latents, but this paper suggests the SAE might just create a latent for "round objects" depending on size)

## Architecture Onboarding

- **Component map**: Base SAE -> Stitching Layer (cosine similarity matching) -> Meta-SAE (trained on decoder matrix)
- **Critical path**:
  1. Train/Load SAEs of varying widths on same layer
  2. Compute decoder cosine similarity between all pairs of latents
  3. **Stitching**: Insert high-similarity latents (reconstruction) and low-similarity latents (novel) from large to small SAE to measure MSE impact
  4. **Meta-Analysis**: Train Meta-SAE on the large SAE decoder; verify Meta-SAE latents align with small SAE latents

- **Design tradeoffs**:
  - Stitching Threshold: High threshold (0.9) misses valid correspondences; low threshold (0.5) falsely equates unrelated features (Paper uses 0.7)
  - Meta-SAE Width: Must be smaller than Base SAE to force compression/decomposition

- **Failure signatures**:
  - "Dead" Latents: Latents that never activate, often recycled via auxiliary loss (TopK)
  - Feature Absorption: A specific concept (e.g., "and") is absorbed into a larger composed latent (e.g., "technology and") rather than remaining independent
  - Non-atomicity: Inability to interpret a latent as a single human concept (e.g., "Einstein" appearing atomic but actually being composed)

- **First 3 experiments**:
  1. **Stitching Interpolation**: Take a small SAE (768) and a large SAE (1536). Sequentially add "novel" latents from the large one to the small one. Plot the MSE decrease to confirm the small SAE was incomplete
  2. **Meta-SAE Decomposition**: Train a Meta-SAE on a GPT-2 Small SAE (layer 8). Check if the "Einstein" latent decomposes into "Scientist" and "Germany" meta-latents
  3. **Seed Variance Check**: Train two SAEs of the same size on the same data with different seeds. Check if they find the same "canonical" units (Hypothesis: They will not, consistent with the non-canonical view)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do canonical units of analysis (unique, complete, atomic features) exist in neural networks, or is feature representation inherently hierarchical?
- Basis in paper: The authors state they "are uncertain whether canonical units of analysis exist" despite testing SAEs of varying sizes
- Why unresolved: The paper shows SAEs do not converge on these units; larger SAEs learn compositions while smaller ones miss information, suggesting the "true" feature set may be undefined
- What evidence would resolve it: The discovery of a method that converges on a consistent feature set regardless of hyperparameters, or a theoretical proof that features must be multi-scale

### Open Question 2
- Question: How can practitioners rigorously determine the optimal SAE dictionary size for specific interpretability tasks?
- Basis in paper: The authors note their methods "neither identify... the size of dictionary to use for a given task" and advise a "pragmatic approach"
- Why unresolved: There is a trade-off where larger SAEs capture novel information but also form non-atomic composed features, complicating the choice of size
- What evidence would resolve it: A set of benchmarks correlating specific dictionary sizes with success rates in downstream tasks like steering or unlearning

### Open Question 3
- Question: Can SAE training objectives be modified to prevent the learning of composed features (e.g., "Einstein") in favor of atomic features?
- Basis in paper: The authors demonstrate that the standard sparsity penalty incentivizes the model to learn composed latents to reduce L0, compromising atomicity
- Why unresolved: The fundamental L1/L0 sparsity constraint creates a pressure to merge distinct concepts into single latents if they frequently co-occur
- What evidence would resolve it: An SAE variant that enforces atomicity (e.g., via information bottlenecks or orthogonality constraints) without degrading reconstruction fidelity

## Limitations

- The analysis relies heavily on reconstruction error as a proxy for feature completeness, which may not fully represent semantic coverage of the feature space
- The Meta-SAE methodology does not establish whether meta-features represent truly fundamental units or if they too could be further decomposed
- The non-canonical hypothesis cannot be proven definitively—the paper provides evidence against uniqueness but cannot prove impossibility

## Confidence

**High Confidence**: The observation that larger SAEs contain latents not present in smaller SAEs (confirmed via stitching experiments across multiple models with clear MSE improvements)

**Medium Confidence**: The claim that larger SAEs preferentially learn composed features over atomic ones (supported by "blue square" example and reconstruction patterns, but depends on specific sparsity regularization)

**Low Confidence**: The assertion that no SAE can ever find canonical units (more of a philosophical claim about the nature of neural network features rather than an empirically testable hypothesis)

## Next Checks

1. **Feature Independence Test**: Train SAEs on datasets with known independent features (e.g., synthetic data with clearly separable concepts) and measure whether stitching still reveals novel features to determine if incompleteness is fundamental or an artifact of natural language data correlations

2. **Cross-Model Canonical Units**: Apply the stitching methodology across different LLMs (e.g., compare SAEs from GPT-2, LLaMA, and Claude) to test whether any latents appear "canonical" across architectures—finding shared latents would challenge the non-canonical hypothesis

3. **Semantic Verification of Novel Latents**: For latents identified as "novel" by stitching, conduct human evaluations to verify they represent semantically distinct concepts rather than just statistical artifacts to strengthen the claim that these are genuine missing features rather than noise