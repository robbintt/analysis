---
ver: rpa2
title: 'CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning
  for BabyLMs'
arxiv_id: '2510.25364'
source_url: https://arxiv.org/abs/2510.25364
tags:
- instruction
- tuning
- data
- both
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether BabyLM-scale models can benefit
  from instruction tuning. It compares conversational and question-answering instruction
  tuning datasets, applied either merged or sequentially, using decoder-only models
  with 100M and 140M parameters.
---

# CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs

## Quick Facts
- **arXiv ID**: 2510.25364
- **Source URL**: https://arxiv.org/abs/2510.25364
- **Reference count**: 11
- **Primary result**: Sequential instruction tuning curricula outperform merged data mixing in BabyLM-scale models, yielding consistent fine-tuning gains but unclear zero-shot generalization benefits.

## Executive Summary
This paper investigates whether BabyLM-scale models (100M/140M parameters) can benefit from instruction tuning. It compares conversational and question-answering instruction tuning datasets, applied either merged or sequentially, using decoder-only models. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization.

## Method Summary
The study uses LLaMA-style decoder-only transformers (140M and 100M parameters) pre-trained on ~91M words from the BabyLM corpus. Instruction tuning is performed on processed Switchboard conversational pairs (~1.3M words) and LLaMA-3.2-3B-Instruct-augmented SimpleWiki Q&A pairs (~8.7M words subset). Three instruction tuning strategies are compared: merged datasets, sequential (Switchboard→Wiki), and sequential (Wiki→Switch). Training uses loss computed only on target tokens. Models are evaluated via SuperGLUE fine-tuning and zero-shot benchmarks including BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic R² correlation, with global comparison via z-scores.

## Key Results
- Sequential curriculum-based instruction tuning yields more consistent performance than merged data mixing across both model sizes
- Instruction tuning creates a trade-off between downstream task performance and zero-shot linguistic generalization in small models
- Smaller models (100M) show better correlation with human psycholinguistic data than larger variants

## Why This Works (Mechanism)

### Mechanism 1
Sequential curriculum-based instruction tuning yields more consistent performance than merged data mixing by allowing the model to adapt to each interaction type distinctly before integrating, reducing interference between task-specific learning signals. This assumes task separation reduces gradient conflict during optimization. Evidence shows sequential models have similar median scores but smaller IQR, with all z-scores above zero. The effect may disappear if datasets share substantial representational overlap.

### Mechanism 2
Instruction tuning at small scale creates a trade-off between downstream task performance and zero-shot linguistic generalization by shifting model internal distributions toward task-specific response patterns, potentially overwriting broadly useful linguistic representations when model capacity is limited. This assumes 100-140M parameter models lack sufficient capacity to maintain both task-specific and general linguistic representations. Evidence shows instruction tuned models may be biased to learn to solve specific tasks, losing generalization abilities on just language. Larger models may not exhibit this trade-off.

### Mechanism 3
Smaller models (100M) better correlate with human psycholinguistic data than larger variants because reduced model capacity constrains representations to simpler statistical patterns that align more closely with human processing signatures. This assumes human reading times and EEG responses reflect constrained cognitive resources, better matched by smaller models. Evidence shows 100M model variants vastly superior to both 140M models and baselines on R²-based tasks, consistent with prior literature. The effect may be evaluation-specific and not generalize to other psycholinguistic measures.

## Foundational Learning

- **Concept: Instruction Tuning**
  - Why needed here: Core intervention; distinguishes from standard pre-training by training on prompt-response pairs with loss computed only on target tokens
  - Quick check question: Can you explain why instruction tuning uses loss only on response tokens rather than the full sequence?

- **Concept: Curriculum Learning**
  - Why needed here: Sequential ordering of datasets is a key experimental manipulation; understanding data ordering effects is essential
  - Quick check question: What is the hypothesized benefit of presenting datasets sequentially versus shuffled together?

- **Concept: Zero-shot vs Fine-tuning Evaluation**
  - Why needed here: Results diverge across these paradigms; understanding the evaluation methodology is critical for interpreting findings
  - Quick check question: Why might instruction tuning help fine-tuning performance but not zero-shot linguistic judgments?

## Architecture Onboarding

- **Component map**: Pre-training corpus (CHILDES, Gutenberg, BNC, OpenSubtitles, Switchboard, SimpleWiki) → Tokenizer training → LLaMA-style decoder-only pre-training (140M or 100M) → Instruction tuning (merged or sequential) → Evaluation (SuperGLUE fine-tuning or zero-shot benchmarks)

- **Critical path**: 
  1. Tokenizer training on full corpus before pre-training
  2. Pre-training for 8 epochs with cross-entropy loss on all tokens
  3. Instruction tuning for 10 epochs with loss computed only on target/answer tokens
  4. Evaluation via SuperGLUE fine-tuning or zero-shot log-likelihood comparisons

- **Design tradeoffs**: 
  - Larger vocab (32K) vs. deeper model (20 layers): 140M prioritizes representation capacity; 100M prioritizes depth
  - Sequential vs. merged instruction tuning: Sequential provides consistency; merged is simpler but noisier
  - Switchboard-first vs. Wiki-first ordering: Order effects are small; Switchboard→Wiki slightly better but may reflect dataset size imbalance

- **Failure signatures**: 
  - Zero-shot BLiMP/EWoK scores flat despite instruction tuning gains on SuperGLUE → indicates capacity-limited trade-off
  - Entity tracking near floor for all models → task may require architectural modifications beyond scale
  - High variance across tasks with no clear winner → suggests generalization constraints under data limits

- **First 3 experiments**:
  1. **Ablate instruction tuning loss masking**: Compare full-sequence loss vs. target-only loss to verify that masking is necessary for gains
  2. **Scale the instruction tuning dataset**: Test whether increasing the proportion of instruction data amplifies or reverses the fine-tuning vs. zero-shot trade-off
  3. **Add intermediate evaluation checkpoints**: Evaluate after each sequential stage to isolate whether Switchboard or Wiki provides the primary benefit, and whether catastrophic forgetting occurs between stages

## Open Questions the Paper Calls Out

- **Open Question 1**: Does reallocating the 100-million-word budget to provide more instruction tuning data (and less pre-training data) improve low-resource model performance? The authors state in the Limitations section that the instruction tuning datasets were relatively small compared to the pre-training corpus, and "A different allocation... might yield stronger effects." The current study utilized a fixed allocation (approx. 90M pre-train / 10M instruction), leaving the optimal balance unexplored.

- **Open Question 2**: Do interactive, communicative benchmarks better capture the specific skills gained from conversational instruction tuning compared to standard zero-shot evaluations? The Discussion notes that evaluating via log-likelihoods (as done in BLiMP/EWoK) may "undermine" conversational models, and suggests future work "extend the evaluation to interactive and communicative benchmarks." The current zero-shot evaluation relies on grammatical minimal pairs and psycholinguistic correlations, which do not directly measure the interactive competence targeted by the training.

- **Open Question 3**: Can specific architectures or hybrid approaches mitigate the trade-off between interaction-focused adaptation and broad linguistic generalization in small models? The Conclusion explicitly calls for future work to "investigate architectures better suited for low-resource generalization." The study observed a trade-off where instruction tuning helped fine-tuning but hurt zero-shot generalization in the decoder-only models tested.

## Limitations

- **Dataset scale imbalance**: The instruction tuning corpus combines ~1.3M words of conversational data with ~8.7M words of QA data in a 7:1 ratio, potentially confounding curriculum order effects and making it difficult to isolate whether sequential ordering or dataset size drives observed performance differences.

- **Capacity-limited trade-off**: The study demonstrates that instruction tuning improves fine-tuning performance while potentially degrading zero-shot linguistic generalization in 100-140M parameter models, but the mechanisms underlying this trade-off remain speculative, and it is unclear whether this represents a fundamental limitation of small models or an artifact of the specific training configuration.

- **Zero-shot evaluation sensitivity**: Zero-shot tasks show mixed and sometimes contradictory results across model variants, with the entity tracking task yielding near-floor performance for all models, suggesting either task-inappropriate evaluation or architectural limitations beyond parameter count.

## Confidence

- **High confidence**: Sequential curriculum instruction tuning yields more consistent performance than merged data mixing, supported by multiple evaluation metrics and clear statistical patterns in results section.
- **Medium confidence**: Trade-off between fine-tuning gains and zero-shot generalization is supported by data but requires careful interpretation; mechanisms remain speculative and effect may be specific to exact model scales and datasets used.
- **Low confidence**: Superiority of 100M models for psycholinguistic correlation is cited to prior literature but lacks direct support within study; phenomenon may be evaluation-specific and not generalize to other psycholinguistic measures.

## Next Checks

1. **Ablate instruction tuning loss masking**: Compare full-sequence loss versus target-only loss during instruction tuning to verify that the masking strategy is necessary for the observed gains, isolating whether benefits stem from instruction tuning format itself or merely from additional training on instruction datasets.

2. **Scale the instruction tuning dataset**: Test whether increasing the proportion of instruction data (currently ~16M of 100M pre-training words) amplifies or reverses the fine-tuning versus zero-shot trade-off, determining whether observed trade-off is a function of dataset size relative to model capacity.

3. **Add intermediate evaluation checkpoints**: Evaluate model performance after each sequential instruction tuning stage (Switchboard only, then SimpleWiki only) to isolate whether one dataset provides the primary benefit and whether catastrophic forgetting occurs between stages, clarifying contribution of each curriculum component to final performance.