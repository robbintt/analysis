---
ver: rpa2
title: 'RELexED: Retrieval-Enhanced Legal Summarization with Exemplar Diversity'
arxiv_id: '2501.14113'
source_url: https://arxiv.org/abs/2501.14113
tags:
- legal
- summarization
- exemplars
- arxiv
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RELexED, a retrieval-augmented framework
  for legal document summarization that addresses two key challenges: content theme
  deviation and inconsistent writing styles. The core method employs a two-stage exemplar
  selection strategy using determinantal point processes (DPP) to balance relevance
  and diversity among retrieved exemplars, with quality and similarity scores computed
  via influence functions.'
---

# RELexED: Retrieval-Enhanced Legal Summarization with Exemplar Diversity

## Quick Facts
- arXiv ID: 2501.14113
- Source URL: https://arxiv.org/abs/2501.14113
- Reference count: 15
- Primary result: RELexED achieves ROUGE-1 scores of 54.04 (SuperSCOTUS) and 41.68 (CivilSum), outperforming baselines by 2.87 and 1.94 points respectively.

## Executive Summary
RELexED introduces a retrieval-augmented framework for legal document summarization that addresses content theme deviation and inconsistent writing styles through diverse exemplar selection. The method employs a two-stage exemplar selection strategy using determinantal point processes (DPP) to balance relevance and diversity among retrieved exemplars, with quality and similarity scores computed via influence functions. Experiments on SuperSCOTUS and CivilSum datasets demonstrate significant performance improvements over models without exemplars and those using similarity-only selection, with notable gains in coherence and fluency metrics. The approach highlights the effectiveness of incorporating diverse, relevant exemplars for generating more accurate and stylistically consistent legal summaries.

## Method Summary
RELexED uses a Longformer encoder-decoder model trained with retrieved exemplar summaries. The process involves two-stage exemplar selection: first, BM25 retrieves top-40 candidates from the training corpus; second, a determinantal point process with influence function-based quality and similarity scores selects the final diverse exemplars. Quality scores measure a candidate's influence on the query document, while similarity scores measure pairwise influence between candidates. The selected exemplars are concatenated with the source document as input to the summarizer, which is fine-tuned end-to-end. The framework balances relevance and diversity to improve both content accuracy and stylistic consistency.

## Key Results
- ROUGE-1 scores: 54.04 (SuperSCOTUS) vs. 51.17 baseline, 41.68 (CivilSum) vs. 39.74 baseline
- Coherence improvement: 74.86 (RELexED) vs. 69.12 baseline on SuperSCOTUS
- Fluency improvement: 72.26 (RELexED) vs. 66.20 baseline on SuperSCOTUS
- Inter-exemplar similarity reduced from 0.81 (BM25) to 0.74 (RELexED) on SuperSCOTUS

## Why This Works (Mechanism)

### Mechanism 1: DPP-Based Diversity-Quality Trade-off
The determinantal point process models subset probability as det(L_k), where the kernel L encodes quality scores (relevance to query) on the diagonal and pairwise similarity off-diagonal. High-quality, dissimilar items maximize the determinant, producing an "implicitly repulsive" selection. This balances redundant guidance from similar exemplars with broader coverage from diverse exemplars.

### Mechanism 2: Gradient-Based Influence Scores Capture Deeper Similarity
Influence functions (TracIn) compute quality and similarity scores that better reflect model-relevant relationships than lexical overlap. TracIn approximates influence as the dot product of gradients: g(θ, z) · g(θ, z'). Examples that induce similar gradient updates are deemed related, capturing functional relevance beyond surface-level lexical similarity.

### Mechanism 3: Exemplar Summaries Provide Implicit Templates
Concatenating exemplar summaries with the source document guides the model toward appropriate structure, terminology, and formality. Rather than learning templates implicitly, the model receives explicit reference summaries as in-context guides through supervised fine-tuning, helping maintain legal domain conventions.

## Foundational Learning

- **Determinantal Point Process (DPP):** Mathematical framework for diversity-aware selection; understanding how det(L_Y) encodes volume spanned by selected items clarifies the quality-diversity trade-off. Quick check: Can you explain why det(L_Y) for two items equals q_i²·q_j²·(1−s_ij²), and what happens when s_ij → 1?
- **Influence Functions / TracIn:** Method for computing gradient-based influence scores; essential for implementing the scoring mechanism. Quick check: Why does TracIn use only the first encoder layer, and what trade-off does this introduce?
- **Retrieval-Augmented Summarization (Fine-tuning variant):** Distinguishes this approach from in-context learning; the model parameters are updated using retrieved exemplars. Quick check: How does the input sequence construction differ between this supervised approach and few-shot in-context learning?

## Architecture Onboarding

- **Component map:** Query document → BM25 filtering → Influence scorer (auxiliary model) → DPP selector → Concatenation → Main summarizer
- **Critical path:** Query document → BM25 filtering (top-40 candidates) → Influence score computation (TracIn) → DPP matrix construction → Greedy selection (k=4 or 8 exemplars) → Concatenation with source document → Longformer-ED forward pass
- **Design tradeoffs:** Number of exemplars vs. context length (more exemplars = better guidance but shorter encoder budget for source); influence computation cost vs. quality (first-layer approximation speeds up but may miss higher-level patterns); greedy DPP vs. exact MAP (greedy is tractable, exact is NP-hard)
- **Failure signatures:** Exemplars all from same narrow topic (redundant guidance, low IE diversity score but no performance gain); influence scores near zero (auxiliary model may be undertrained or gradients vanishing); generated summary mimics exemplar content rather than query (over-reliance on exemplars, insufficient query grounding)
- **First 3 experiments:** 1) Ablate diversity: Run DPP with BM25 scores vs. influence scores; expect smaller diversity gains, per Table 1's BM25+DPP(BM25) intermediate results; 2) Vary k: Test k=2,4,8,16 exemplars on validation set; plot ROUGE-1 vs. k to find saturation point where context crowding hurts; 3) Cross-dataset transfer: Train influence scorer on one dataset (SuperSCOTUS), apply to another (CivilSum); assess whether influence patterns generalize across jurisdictions

## Open Questions the Paper Calls Out

- **Generalization to other legal systems:** The extent to which observations hold for broader legal systems beyond the United States and India remains open, as experiments were confined to these two datasets.
- **Incorporating temporal information:** Exploring additional features such as the temporal nature of legal documents and multi-aspect considerations could enhance summarization quality.
- **Expert validation:** The absence of direct participation by legal experts means standard metrics may not fully capture nuanced legal content essential for professionals.
- **Domain-specific evaluation metrics:** Further research is needed into developing legal domain-specific evaluation metrics that can more accurately reflect the complexities inherent in legal documents.

## Limitations
- The framework's generalizability to legal systems beyond the United States and India is unknown and requires evaluation on diverse jurisdictions.
- The reliance on semantic similarity and diversity may benefit from incorporating additional features such as temporal information and multi-aspect considerations.
- The absence of direct validation by legal experts means automated metrics may not fully capture professional utility and nuanced legal content.

## Confidence

- **High**: Performance improvements over baseline (ROUGE, coherence, fluency gains are robust)
- **Medium**: DPP diversity mechanism effectiveness (evidence is present but indirect; correlation vs. causation)
- **Low**: Influence function superiority over lexical methods (limited legal-domain validation)

## Next Checks
1. Ablate diversity mechanism: Run DPP with BM25-based quality/similarity scores vs. influence-based scores; compare inter-exemplar similarity and performance to isolate diversity contribution.
2. Cross-dataset influence transfer: Train influence scorer on SuperSCOTUS, apply to CivilSum; assess whether influence patterns generalize across jurisdictions.
3. Scale exemplar count: Vary k=2,4,8,16 exemplars; plot ROUGE-1 vs. k to identify saturation point where context crowding degrades performance.