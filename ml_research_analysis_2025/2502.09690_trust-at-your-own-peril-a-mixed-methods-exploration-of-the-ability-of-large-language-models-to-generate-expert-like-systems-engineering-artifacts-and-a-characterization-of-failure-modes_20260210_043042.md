---
ver: rpa2
title: 'Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large
  Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization
  of Failure Modes'
arxiv_id: '2502.09690'
source_url: https://arxiv.org/abs/2502.09690
tags:
- llms
- system
- generated
- engineering
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the ability of multi-purpose Large Language
  Models (LLMs) to generate expert-like Systems Engineering (SE) artifacts without
  fine-tuning. A human-expert generated artifact served as a benchmark, and various
  LLMs were prompted to generate SE artifact segments.
---

# Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes

## Quick Facts
- arXiv ID: 2502.09690
- Source URL: https://arxiv.org/abs/2502.09690
- Reference count: 40
- Multi-purpose LLMs can generate SE artifacts with high semantic similarity to expert benchmarks, but these artifacts often contain serious deficiencies invisible to automated evaluation.

## Executive Summary
This study investigates whether multi-purpose Large Language Models can generate expert-like Systems Engineering (SE) artifacts without fine-tuning. Using a human-expert benchmark from the Bulldog UGV case study, the research employed both quantitative (MAUVE algorithm for semantic similarity) and qualitative (human expert analysis) methods to evaluate LLM-generated artifacts. While LLMs achieved MAUVE scores up to 0.9932 when prompted carefully, qualitative analysis revealed three critical failure modes: premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify. These findings demonstrate that LLMs can produce text statistically indistinguishable from human experts while containing substantive domain errors, highlighting the need for caution when using AI-assisted SE tools.

## Method Summary
The study used the Bulldog UGV dataset containing 7-8 multi-page SE documents, which were chunked into ~52 prompt-response pairs of 1-2 paragraphs each. Three closed-source LLMs (GPT-3.5 Turbo, GPT-4, Claude) were prompted using three configurations of increasing specificity: generic prompts, content-specific prompts, and length-bounded prompts. MAUVE algorithm measured semantic similarity between LLM outputs and the human expert benchmark, while two independent coders performed qualitative analysis to identify failure modes. The evaluation focused on Capability Description Document (CDD) segments including Key Performance Parameters (KPPs), Key System Attributes (KSAs), and Operational Scenarios.

## Key Results
- LLMs achieved MAUVE scores up to 0.9932 when prompted with specific length constraints, but these scores masked substantive deficiencies
- Three failure modes were identified: premature requirements definition (introducing "shall" statements in problem-bounding documents), unsubstantiated numerical estimates (providing confident but baseless numbers), and propensity to overspecify (introducing unrequested variables)
- The Bulldog UGV unit cost estimate of $383M generated by LLM vastly exceeded the benchmark $0.6M, demonstrating the severity of unsubstantiated estimates

## Why This Works (Mechanism)

### Mechanism 1
LLMs can generate SE artifacts that are statistically indistinguishable from human expert text using semantic similarity metrics, yet contain substantive deficiencies invisible to automated evaluation. The MAUVE algorithm measures distributional similarity between text corpora via Kullback-Leibler divergence. When prompted with specific length constraints and domain context, LLMs produce text that falls within the same probability distribution as human-generated artifacts—achieving MAUVE scores up to 0.9932 for Claude. However, this metric captures linguistic patterns, not domain-correct reasoning. Semantic similarity is a necessary but insufficient proxy for expert-quality SE artifacts.

### Mechanism 2
Multi-purpose LLMs exhibit failure modes characteristic of novice engineers because they retrieve statistically plausible patterns from broad training data rather than applying contextualized engineering reasoning. The LLM processes the user-prompt and "seeks some anchoring segments of text in the system prompt" to search within its pre-training corpus for similar artifacts, from which "plausibly relevant terms are scraped, converted, and provided as a response." This retrieval-based approach lacks the engineering analysis capability to validate numerical estimates or distinguish between problem-bounding statements (needs) and solution specifications (requirements).

### Mechanism 3
Prompt engineering effectiveness requires a priori expert knowledge of the target artifact structure, creating a paradox where only those who can evaluate outputs can elicit quality outputs. The study iterated through three prompt configurations. Configuration 1 (generic prompts) produced MAUVE scores of 0.0000. Configuration 3 (specific length constraints and domain framing) achieved 0.9932. LLMs only generate similar answers for those who can evaluate the similarity of the response. The expert must know what the artifact should and should not contain before prompting.

## Foundational Learning

- **Concept**: MAUVE (Measurement of Agreement Using Vector Embeddings)
  - **Why needed here**: This metric enabled the paper's central paradox—showing that "expert-like" statistical similarity coexists with substantive failure. Understanding K-L divergence helps explain why LLM outputs can look right while being wrong.
  - **Quick check question**: If two text distributions have MAUVE score 0.99, does this guarantee the texts are semantically equivalent? (Answer: No—it measures distributional overlap, not factual correctness or domain validity.)

- **Concept**: Capability Description Document (CDD) vs. Requirements Specification
  - **Why needed here**: Failure Mode 1 (premature requirements definition) stems from conflating problem-bounding documents (CDD identifies capabilities/needs) with solution-specifying documents (requirements define "shall" statements). The LLM cannot distinguish these lifecycle phases.
  - **Quick check question**: A CDD should identify what the system must achieve operationally, not how it should be built. If an artifact includes "shall operate at -20 to +60 degrees Celsius," is this appropriate for a CDD? (Answer: Per the paper, this is premature—CDDs bound problems; requirements specify solutions.)

- **Concept**: Threshold and Objective Values in DoD Acquisition
  - **Why needed here**: The LLM consistently generated threshold/objective pairs in correct format (demonstrating pattern learning) but with absurd values (demonstrating lack of engineering analysis). Understanding this distinction is critical for interpreting Failure Mode 2.
  - **Quick check question**: If an LLM generates a threshold of $383M and objective of $351M for a UGV unit cost, what's wrong beyond the absolute values? (Answer: The threshold should be less stringent than the objective; lower cost is better, so the ordering is inverted.)

## Architecture Onboarding

- **Component map**: Bulldog UGV case study -> 52 chunked prompt-response pairs -> LLM Layer (GPT-3.5 Turbo, GPT-4, Claude) -> Evaluation Layer (MAUVE + human expert qualitative coding) -> CDD segments

- **Critical path**:
  1. Chunk human-expert benchmark into ~52 instances of 1-2 paragraphs each (required for MAUVE minimum)
  2. Construct system prompt with domain context (DoD SE, ACAT program, Bulldog mission)
  3. Iterate user prompts through three configurations: generic -> content-specific -> length-bounded
  4. Run MAUVE comparison; select highest-scoring output for qualitative analysis
  5. Two independent coders identify failure patterns; third coder synthesizes themes

- **Design tradeoffs**:
  - Siloed prompts vs. conversation history: Study structured each prompt as independent, avoiding "cascading hallucinations" but preventing cross-artifact consistency validation
  - No fine-tuning: Baseline assessment of off-the-shelf LLMs; trades potential quality improvements for research generality
  - Single benchmark dataset: Controls for artifact quality but limits generalizability (sample size = 1 case study)

- **Failure signatures**:
  - Premature requirements: Look for "shall" statements in CDD-level artifacts where capability bounds should appear
  - Unsubstantiated estimates: Numerical values with no traceability to standards, analysis, or ConOps; internal contradictions across related prompts
  - Overspecification: Introduction of stakeholder categories, performance metrics, or system boundaries not present in source prompts

- **First 3 experiments**:
  1. Run identical SE artifact generation task with 10 different phrasings of the same prompt to measure output variance
  2. Fine-tune a smaller model (e.g., Llama) on 50-100 DoD SE artifacts and compare failure mode frequency against multi-purpose LLMs on held-out Bulldog chunks
  3. Apply the same methodology to a different SE domain (e.g., civil aviation or spacecraft) to test whether failure modes generalize beyond defense acquisition artifacts

## Open Questions the Paper Calls Out

### Open Question 1
Do the failure modes of premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify generalize to systems engineering artifacts generated for domains outside of Defense acquisition? The empirical validation was limited to a single case study (Bulldog UGV) within the DoD acquisition framework. Replication of the mixed-methods evaluation using benchmarks from diverse SE domains, such as commercial aerospace, civil infrastructure, or consumer electronics would resolve this.

### Open Question 2
How does the stochastic nature of large language models impact the consistency and variance of generated systems engineering artifacts? The paper notes, "Research is needed to test and monitor consistency of these tools," acknowledging that this study "assumed LLMs would behave in a rather deterministic manner." A statistical analysis of qualitative and quantitative differences across multiple outputs generated from identical prompts and seeds would resolve this.

### Open Question 3
Can domain-specific fine-tuning mitigate the identified failure modes without introducing the risk of "over-engineering" outputs based on historical precedents? The authors discuss fine-tuning as a potential remedy but warn of the "risk that model responses will produce content exactly like previous acquisitions were designed, which may be undesirable." Comparative experiments measuring the frequency of failure modes in general-purpose versus fine-tuned models, alongside metrics for design novelty would resolve this.

## Limitations
- The study's findings are constrained by a single case study (Bulldog UGV), limiting generalizability to other SE domains
- The MAUVE metric measures distributional similarity rather than factual correctness or domain validity
- The qualitative coding relied on two independent coders without inter-rater reliability metrics reported

## Confidence

- **High confidence**: The observation that LLMs can generate text with high MAUVE scores (0.99+) yet contain substantive domain errors. The three failure modes are clearly identified and well-documented in the Bulldog case.
- **Medium confidence**: That these failure modes generalize beyond defense acquisition artifacts to other SE contexts. The mechanism explanation is plausible but requires validation in different domains.
- **Low confidence**: That prompt engineering alone can consistently elicit expert-quality SE artifacts without fine-tuning. The dramatic MAUVE score improvement from prompt configuration changes suggests sensitivity that may not be reliable in practice.

## Next Checks

1. **Cross-domain replication**: Apply the same methodology to SE artifacts from civil aviation or spacecraft domains to test whether failure modes generalize beyond defense acquisition contexts.
2. **Domain-specific fine-tuning**: Fine-tune a smaller model (e.g., Llama) on 50-100 DoD SE artifacts and compare failure mode frequency against multi-purpose LLMs on held-out Bulldog chunks.
3. **Human expert blind evaluation**: Have practicing SE experts evaluate LLM-generated artifacts without knowing their source, measuring whether experts can distinguish AI-generated from human-generated artifacts beyond statistical metrics.