---
ver: rpa2
title: 'TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs'
arxiv_id: '2506.16990'
source_url: https://arxiv.org/abs/2506.16990
tags:
- latex
- llms
- error
- code
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TeXpert introduces a multi-level benchmark for evaluating LLMs'
  ability to generate LaTeX code from natural language prompts for scientific documents.
  The dataset contains 440 high-quality samples across three difficulty levels (Simple,
  Average, Hard), focusing on atomic LaTeX commands commonly used in scientific writing.
---

# TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs

## Quick Facts
- arXiv ID: 2506.16990
- Source URL: https://arxiv.org/abs/2506.16990
- Reference count: 20
- Key outcome: Introduces multi-level benchmark showing LLMs struggle with LaTeX generation, with accuracy dropping from 78.8% to 15% as complexity increases

## Executive Summary
TeXpert introduces a multi-level benchmark for evaluating LLMs' ability to generate LaTeX code from natural language prompts for scientific documents. The dataset contains 440 high-quality samples across three difficulty levels (Simple, Average, Hard), focusing on atomic LaTeX commands commonly used in scientific writing. Experiments with 10 open and closed-source LLMs reveal that even top-performing models struggle with LaTeX generation, with accuracy dropping significantly as task complexity increases. Open-source models like DeepSeek v3 and DeepSeek Coder rival closed-source counterparts, but all models show frequent logical and formatting errors, highlighting a gap in LaTeX-specific training data. The benchmark provides insights into LLM limitations and common error types, offering a foundation for future improvements in LaTeX code generation.

## Method Summary
The authors constructed a benchmark dataset (TeXpert) with 440 natural language prompts for LaTeX code generation, categorized into three difficulty levels based on command count and package complexity. They evaluated 10 LLMs (including GPT-4o, DeepSeek v3, Claude) by generating LaTeX code from these prompts using deterministic settings. An LLM-as-a-judge (GPT-4o, with DeepSeek v3 for OpenAI models) classified outputs into success/failure and five error categories: Capability, Syntax, Logical, Package, and Formatting errors. The atomic command taxonomy (319 commands across 5 functional groups) provided a structured framework for evaluating performance at increasing complexity levels.

## Key Results
- GPT-4o accuracy drops from 78.8% (Simple) to 15% (Hard), demonstrating a dramatic performance cliff
- Open-source models (DeepSeek v3, DeepSeek Coder) match or exceed closed-source models on Simple and Average tasks
- Logical errors (missed instructions) are most common (50-60% of all errors), followed by package errors (15-30%)
- All models show significant accuracy degradation on Hard tasks (80+ commands, 5+ packages)

## Why This Works (Mechanism)

### Mechanism 1: Atomic Command Taxonomy for Controlled Evaluation
- **Claim:** Decomposing LaTeX generation into atomic commands allows for systematic evaluation of LLM capabilities at increasing levels of complexity.
- **Mechanism:** The authors collected 319 atomic LaTeX commands from Overleaf documentation and scientific templates, categorizing them into five functional groups. These atoms are combined into tasks of varying complexity (Simple: 10-20 commands; Average: 12-80 commands; Hard: 80+ commands, 5+ packages).
- **Core assumption:** LLM performance degrades predictably with increasing complexity (number of commands, packages, and constraints).
- **Evidence anchors:** [abstract] Mentions "focusing on atomic LaTeX commands" and "three difficulty levels." [section: 3 Dataset Construction] Details the 319 atomic commands and specific constraints.

### Mechanism 2: LLM-as-a-Judge for Deterministic Error Classification
- **Claim:** Using a powerful LLM with a specific evaluation prompt can reliably classify and quantify error types in generated LaTeX code.
- **Mechanism:** Generated LaTeX code and original natural language prompt are fed to GPT-4o (or DeepSeek v3 for OpenAI models) with a system prompt asking it to determine success/failure and classify errors into five categories.
- **Core assumption:** The judge LLM can accurately understand natural language requirements and interpret LaTeX code's intent.
- **Evidence anchors:** [section: 4 Experimental Setup] Describes using GPT-4o as a judge and the predefined evaluation prompt. [section: Table 7] Defines the five error types.

### Mechanism 3: Performance Cliff as a Diagnostic for Training Data Gaps
- **Claim:** The dramatic drop in LLM accuracy as task complexity increases serves as a diagnostic signal for insufficient LaTeX examples in pre-training data.
- **Mechanism:** Testing 10 LLMs across three difficulty levels revealed universal performance patterns: decent performance on Simple tasks but a sharp "cliff" on Hard tasks, with high frequency of package and formatting errors.
- **Core assumption:** Failure on "Hard" tasks and specific error types is directly linked to training data composition.
- **Evidence anchors:** [abstract] Highlights "significant accuracy drop-off as the complexity of tasks increases" and suggests "lack of diverse LaTeX examples in the training datasets." [section: 5 Result Discussion] Shows the accuracy drop and high percentage of logical and package errors.

## Foundational Learning

- **Concept: LaTeX Package System & Dependencies**
  - **Why needed here:** Most frequent error source after logical errors. Understanding that commands like `\includegraphics` require `\usepackage{graphicx}` is essential for diagnosing Package Errors.
  - **Quick check question:** If a LaTeX document uses `\cellcolor` to color a table cell, which package must be imported in the preamble?

- **Concept: LLM-as-a-Judge Evaluation Paradigm**
  - **Why needed here:** The entire evaluation framework rests on this. The study uses GPT-4o to grade other models. Understanding potential for bias and need for prompt refinement is critical.
  - **Quick check question:** Why did the authors use DeepSeek v3 instead of GPT-4o to evaluate the outputs of GPT-4o and GPT-4o-mini?

- **Concept: Instruction-Following Complexity**
  - **Why needed here:** The paper's core finding is that LLMs struggle with multi-constraint instructions. Difficulty levels are defined by the number of formatting instructions and components that must be simultaneously satisfied.
  - **Quick check question:** In the TeXpert dataset, what are the three primary factors that differentiate a "Simple" task from a "Hard" task?

## Architecture Onboarding

- **Component map:** Natural language prompts -> Target LLM generation -> LaTeX code extraction -> Judge LLM evaluation -> Error classification -> Accuracy aggregation
- **Critical path:**
  1. Define a natural language task from the TeXpert dataset
  2. Generate LaTeX code using the target LLM with deterministic settings
  3. Evaluate the generated code using the Judge LLM against original prompt's requirements
  4. Classify any failure into one of the five predefined error categories
  5. Aggregate results to compute accuracy and error distributions

- **Design tradeoffs:**
  - Manual vs. Automated Evaluation: LLM-as-a-judge for scalability vs. potential evaluator bias
  - Dataset Size vs. Quality: Small (440 samples) but manually verified for high quality
  - Judge Bias Mitigation: Using DeepSeek v3 for OpenAI models adds complexity but ensures fairness

- **Failure signatures:**
  - Logical Error (LE): Code is syntactically correct but misses a stated requirement (most common)
  - Package Error (PE): Code uses a command but is missing the required package import
  - Performance Cliff: Accuracy remains stable from Simple to Average but collapses on Hard tasks

- **First 3 experiments:**
  1. Baseline Evaluation: Run the TeXpert benchmark on a target LLM to establish baseline accuracy across difficulty levels
  2. Error Profiling: Use the judge model to produce a detailed error distribution, pinpointing specific LaTeX weaknesses
  3. Failure Analysis on Hard Set: Manually analyze 5-10 zero-score samples from Hard set to determine root causes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-tuning LLMs on the TeXpert benchmark significantly reduce logical and package errors in LaTeX code generation?
- **Basis in paper:** [explicit] The limitations section states: "Using our dataset to fine-tune models and reduce logical and package errors in LaTeX-based tasks is another straightforward extension to our work."
- **Why unresolved:** The authors created the dataset with columns for future fine-tuning but did not conduct fine-tuning experiments themselves.
- **What evidence would resolve it:** Fine-tune open-source models on TeXpert and measure error rate reductions, particularly for logical errors (50-60% of all errors) and package errors (15-30% across models).

### Open Question 2
- **Question:** What is the precise instruction complexity threshold at which LLMs experience dramatic performance degradation in LaTeX generation?
- **Basis in paper:** [explicit] The authors state: "This consistent degradation pattern clearly shows a threshold on the number and complexity of instructions for LaTeX generation using LLMs."
- **Why unresolved:** The gap between Average and Hard is broad; the exact inflection point remains unidentified.
- **What evidence would resolve it:** Create intermediate difficulty samples between Average and Hard constraints, then plot accuracy curves to identify where performance drops precipitously.

### Open Question 3
- **Question:** To what extent do LLMs hallucinate non-existent LaTeX packages versus misapply real but inappropriate ones?
- **Basis in paper:** [explicit] The error analysis notes: "the use of non-standard or incompatible packages, especially in DeepSeek and Mistral models, is concerning and may point to LLMs hallucinating or making up packages."
- **Why unresolved:** The authors categorize package errors broadly but do not distinguish between hallucinated vs. misapplied packages.
- **What evidence would resolve it:** Manual analysis of package errors to classify them as hallucinated vs. misapplied, potentially using CTAN package database verification.

## Limitations
- **Evaluation reliability**: LLM-as-a-judge methodology introduces potential subjectivity in error classification, with iterative prompt refinement not fully documented
- **Dataset representativeness**: Only 440 samples may not capture full diversity of LaTeX use cases in scientific writing
- **Generalizability concerns**: Observed performance patterns may be specific to particular prompt templates and evaluation methodology

## Confidence
- **High confidence**: The fundamental observation that LLM performance degrades predictably with task complexity is well-supported by experimental results
- **Medium confidence**: Specific accuracy percentages and error distributions are reliable within experimental setup but may vary with different approaches
- **Low confidence**: The causal link between training data composition and observed failure patterns remains speculative, lacking direct analysis of model training corpora

## Next Checks
1. **Cross-validation with alternative evaluators**: Repeat evaluation using human experts for a subset of samples to validate reliability of LLM-as-a-judge classifications
2. **Prompt engineering ablation study**: Systematically vary prompt templates, formatting instructions, and temperature settings to determine if performance gaps are robust to different prompting strategies
3. **Model capability decomposition**: Isolate whether failures stem from inability to parse complex natural language instructions versus fundamental gaps in LaTeX knowledge by testing models on progressively simplified versions of Hard tasks