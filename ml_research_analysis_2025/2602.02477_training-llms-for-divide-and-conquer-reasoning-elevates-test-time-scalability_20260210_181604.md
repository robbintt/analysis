---
ver: rpa2
title: Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability
arxiv_id: '2602.02477'
source_url: https://arxiv.org/abs/2602.02477
tags:
- reasoning
- frac
- subproblem
- training
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of standard chain-of-thought
  (CoT) reasoning in large language models (LLMs) when tackling the most challenging
  reasoning tasks. The authors identify a misalignment between general-purpose post-training
  and divide-and-conquer (DAC) reasoning, where DAC-style inference underperforms
  CoT due to lack of dedicated training.
---

# Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability

## Quick Facts
- arXiv ID: 2602.02477
- Source URL: https://arxiv.org/abs/2602.02477
- Reference count: 40
- Standard chain-of-thought reasoning saturates on hard reasoning tasks; dedicated divide-and-conquer training closes the performance gap.

## Executive Summary
This paper addresses the fundamental misalignment between general-purpose post-training and divide-and-conquer (DAC) reasoning in large language models. The authors propose an end-to-end reinforcement learning framework (DAC-RL) that trains models to decompose problems into subproblems, solve them sequentially, and condition the original problem's solution on subproblem results. Experiments show DAC-trained models outperform CoT-trained ones on competition-level benchmarks (AIME, Beyond-AIME, HMMT) with absolute improvements of 8.6% in Pass@1 and 6.3% in Pass@32, while also demonstrating higher reasoning ceilings and better test-time scalability.

## Method Summary
The method uses Group Relative Policy Optimization (GRPO) to train models in a divide-and-conquer paradigm. During division, the model generates subproblems (minimum 3 per problem) grouped into 4 sets. During conquering, each group of subproblems is solved sequentially, and the original problem is solved based on these results. Division rewards combine format validity, quantity, and helpfulness (measured by final answer correctness), while conquering rewards depend solely on final answer correctness. The training uses a batch size of 256, runs for 400 steps, and evaluates using Pass@k metrics on integer-answer mathematical benchmarks.

## Key Results
- DAC-RL achieves 8.6% absolute improvement in Pass@1 and 6.3% in Pass@32 over CoT-RL on competition benchmarks
- DAC-trained models demonstrate higher reasoning ceilings and maintain higher policy entropy during training
- DAC-style training improves both DAC and CoT reasoning performance
- Test-time compute allocation to more division groups (larger n) outperforms allocating all compute to conquering samples (larger m)

## Why This Works (Mechanism)

### Mechanism 1: DAC Post-Training Alignment
General-purpose post-training misaligns with DAC inference; dedicated RL for DAC closes this gap and raises the performance ceiling beyond CoT saturation. CoT-centric training optimizes sequential step generation but doesn't teach decomposition. DAC-RL introduces explicit training signals for both division and conquering, creating a new optimization path that CoT-RL cannot reach.

### Mechanism 2: Surrogate Reward via Final-Answer Correctness
Final-answer correctness provides a valid surrogate signal for subproblem correctness under assumed causal structure. Lemma 2.1 proves that if subproblem correctness causally influences final-answer correctness (monotonicity), then rewarding correct final answers preferentially upweights trajectories with more correct subproblems—no subproblem ground truth required.

### Mechanism 3: Test-Time Compute Allocation via Division Diversity
Allocating inference compute to more division groups (n) rather than more conquering samples per group (m) improves Pass@k by expanding exploration diversity. More subproblem groups yield diverse decompositions, increasing the chance that at least one leads to a correct solution path. This outperforms allocating all compute to repeated CoT-style sampling.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**
  - Why needed here: DAC-RL uses GRPO to compute advantages via group-normalized rewards without a separate critic, enabling stable updates for both division and conquering.
  - Quick check question: Given rewards [0.2, 0.5, 0.8] for three responses, can you compute the group-relative advantage for each?

- **Causal Reward Structures & Credit Assignment**
  - Why needed here: Lemma 2.1 relies on understanding when a surrogate reward (final answer) validly reinforces upstream behaviors (subproblem solving).
  - Quick check question: If subproblems are independent of the final answer, does rewarding the final answer still provide a useful learning signal? Why or why not?

- **Test-Time Scaling (Pass@k, n/m Budget Allocation)**
  - Why needed here: The paper shows that how you allocate inference compute—across division groups vs conquering samples—directly impacts performance.
  - Quick check question: For fixed budget k=64, describe a scenario where increasing n (more groups) helps more than increasing m (more samples per group).

## Architecture Onboarding

- **Component map**: Input problem → Division prompt → Subproblem generation (Ns minimum) → Conquering prompt → Sequential subproblem solving → Original problem solving → Final answer extraction → Reward computation → GRPO policy update

- **Critical path**: 
  1. Division must produce parsable, ≥Ns subproblems; otherwise reward=0 and policy cannot learn decomposition.
  2. Conquering must correctly solve the original problem for positive reward; this signal must propagate back through causal structure to reinforce good subproblems.

- **Design tradeoffs**:
  - Strict format constraint: Enforcing explicit subproblem answers improves instruction-following but incurs alignment tax—performance drops.
  - Division reward strictness: Using exact conquering accuracy as division reward causes premature solving during division; relaxed lower-bound reward prevents collapse.
  - Cold-start distillation: Distillation from stronger models helps smaller models but requires 3K+ high-quality samples; RL-only is cheaper but slower to converge.

- **Failure signatures**:
  1. Division collapses to CoT: Model generates solution steps instead of subproblems. Fix: Enforce quantity constraint and use relaxed helpfulness reward.
  2. Entropy collapse: Subproblems become repetitive across training. Fix: Maintain Gd≥4 groups, use Clip-Higher, monitor entropy.
  3. Alignment tax from format constraints: Strict subproblem-answer formats degrade performance. Fix: Use implicit format rewards, not explicit format enforcement.

- **First 3 experiments**:
  1. Baseline comparison: Train Qwen3-4B-Instruct with DAC-RL vs CoT-RL on DAPO-Math-17k; report Pass@1 and Pass@32 on AIME 2024/2025. Expect DAC to surpass CoT by 6–9%.
  2. n/m sweep at fixed budget: Fix k=1024, sweep n∈{1,2,4,8,16,32,64,128} with m=k/n; plot Pass@1024. Expect peak at moderate-to-high n.
  3. Transfer test: After DAC-RL training, evaluate CoT-style inference (no division). Expect improved CoT performance vs CoT-RL baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the "alignment tax" observed when enforcing strict subproblem-solving formats be mitigated?
- **Basis:** Section 4.5 and Table 3 demonstrate that requiring the model to explicitly solve subproblems in a rigid format degrades performance compared to unconstrained generation.
- **Why unresolved:** The authors establish the existence of the trade-off but do not propose a method to maintain structural discipline without sacrificing reasoning capability.
- **What evidence would resolve it:** A modified training objective or reward structure that enforces formatting while retaining the accuracy of the unconstrained baseline.

### Open Question 2
- **Question:** Does DAC-RL generalizes to non-mathematical domains such as code generation or logical deduction?
- **Basis:** Section 3.1 explicitly restricts evaluation to competition-level mathematical benchmarks with integer answers to ensure precise assessment.
- **Why unresolved:** While the decomposition strategy is theoretically domain-agnostic, its reliance on mathematical structure for "helpfulness" rewards may not transfer directly to open-ended tasks.
- **What evidence would resolve it:** Experimental results applying DAC-RL to coding benchmarks or logical reasoning datasets.

### Open Question 3
- **Question:** Can the allocation of compute between division and conquering be dynamically optimized during inference?
- **Basis:** Section 4.2 investigates optimal static configurations for a fixed budget, noting that allocating more groups generally improves performance.
- **Why unresolved:** The current analysis relies on grid search over static configurations; it does not determine if the optimal split varies by problem difficulty.
- **What evidence would resolve it:** An adaptive compute allocation strategy that outperforms fixed configurations by adjusting the subproblem/solution ratio based on input complexity.

## Limitations

- DAC training requires careful reward design to prevent premature solving during division and maintain subproblem diversity
- The approach relies on monotonicity assumptions for credit assignment that may not hold for all problem distributions
- Performance gains depend on problem structure; for some problems, DAC may introduce unnecessary overhead

## Confidence

- **High**: DAC-RL training improves DAC-style reasoning performance (empirical results are clear)
- **Medium**: DAC-style training also improves CoT reasoning (transfer effect observed but mechanism not fully characterized)
- **Medium**: DAC achieves higher reasoning ceilings than CoT (based on saturation observations but not exhaustive)
- **Low**: DAC is universally more efficient than CoT for test-time scaling (efficiency depends on problem characteristics)

## Next Checks

1. Compare DAC-RL against CoT-RL + verification training on the same benchmarks to isolate the decomposition benefit
2. Test DAC-RL on non-integer-answer benchmarks to validate generalization beyond the current evaluation suite
3. Analyze DAC subproblem quality across training epochs to measure whether decomposition diversity or conquering accuracy drives the primary performance gains