---
ver: rpa2
title: 'Probabilistic causal graphs as categorical data synthesizers: Do they do better
  than Gaussian Copulas and Conditional Tabular GANs?'
arxiv_id: '2504.11547'
source_url: https://arxiv.org/abs/2504.11547
tags:
- data
- synthetic
- such
- disability
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates synthetic categorical data generation for
  accessibility research using causal graph models. It compares Structural Equation
  Modeling (SEM)-based Bayesian Networks (BN) against Gaussian Copula and CTGAN methods
  for generating privacy-preserving synthetic survey data on disability and accessibility
  barriers.
---

# Probabilistic causal graphs as categorical data synthesizers: Do they do better than Gaussian Copulas and Conditional Tabular GANs?

## Quick Facts
- arXiv ID: 2504.11547
- Source URL: https://arxiv.org/abs/2504.11547
- Reference count: 32
- Bayesian Networks achieved TVD of 0.9979, outperforming Gaussian Copulas (0.976) and CTGAN (0.90-0.92)

## Executive Summary
This study evaluates synthetic categorical data generation methods for accessibility research, comparing Structural Equation Modeling-based Bayesian Networks against Gaussian Copula and CTGAN approaches. Using Canadian Survey on Disability data focused on accessibility barriers, the research demonstrates that the SEM-based BN approach achieves superior statistical fidelity while preserving causal relationships and maintaining privacy. The synthesized data enables researchers to conduct inferential analyses on sensitive disability-related barriers without compromising individual confidentiality.

## Method Summary
The research employs a three-stage methodology: (1) causal graph construction using Structural Equation Modeling to identify relationships between demographic variables and accessibility barriers, (2) Bayesian Network implementation to capture both statistical and causal properties of the original survey data, and (3) comparative analysis against Gaussian Copula and CTGAN synthetic data generators. The BN approach uses a probabilistic framework that models conditional dependencies between categorical variables, generating synthetic samples that maintain the joint probability distribution of the original dataset while ensuring privacy through data generalization.

## Key Results
- BN achieved highest Total Variation Distance (0.9979) among all methods tested
- Statistical validation confirmed BN's superior ability to preserve original data distributions
- Synthetic data maintained confidentiality while enabling accurate causal inference on accessibility barriers
- BN effectively identified vulnerable demographic groups and predicted barrier frequencies for specific disability types

## Why This Works (Mechanism)
The Bayesian Network approach succeeds because it explicitly models causal relationships between variables rather than just statistical correlations. By constructing a probabilistic graph that captures conditional dependencies, the BN method preserves the joint probability distribution of the original categorical data. This allows the synthetic data to maintain both the statistical properties and the underlying causal structure of the real survey responses. The SEM-based construction ensures that the network topology reflects empirically validated relationships between demographic factors and accessibility barriers, enabling more meaningful synthetic data generation than purely statistical methods like Gaussian Copula or GAN-based approaches.

## Foundational Learning
- **Causal Graph Construction**: Why needed - establishes theoretical foundation for synthetic data relationships; Quick check - verify DAG structure captures known domain relationships
- **Structural Equation Modeling**: Why needed - provides statistical rigor for identifying variable relationships; Quick check - confirm model fit indices meet acceptance criteria
- **Total Variation Distance**: Why needed - measures distributional similarity between synthetic and real data; Quick check - compare TVD values across different sample sizes
- **KL Divergence**: Why needed - quantifies information loss between distributions; Quick check - ensure symmetric KL values indicate good fit
- **Chi-square Test**: Why needed - validates statistical similarity of marginal distributions; Quick check - confirm p-values exceed significance threshold

## Architecture Onboarding

**Component Map**: Data Preprocessing -> SEM Causal Graph Construction -> Bayesian Network Training -> Synthetic Data Generation -> Statistical Validation

**Critical Path**: The SEM-based causal graph construction is the critical path as it determines the network topology that directly impacts synthetic data quality. Errors in causal identification propagate through the entire pipeline.

**Design Tradeoffs**: 
- BN: Higher computational complexity but better causal preservation
- Gaussian Copula: Faster generation but weaker causal structure capture
- CTGAN: Good for complex patterns but may miss explicit causal relationships

**Failure Signatures**:
- High KL divergence indicates poor distribution matching
- Low Chi-square p-values suggest statistical dissimilarity
- Poor TVD scores reveal inadequate synthetic data quality
- Unexpected edge directions in causal graph indicate model misspecification

**First Experiments**:
1. Compare marginal distributions of single variables between real and synthetic data
2. Test conditional independence relationships in the synthetic dataset
3. Evaluate privacy risk through membership inference attack simulations

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the SEM-based Bayesian Network performance scale when modeling the full range of accessibility barriers (communications, attitude, ICT, physical) rather than just interaction barriers?
- Basis in paper: [explicit] Section 4.2.3 states, "The current study focuses only on interaction barriers to simplify our network."
- Why unresolved: It is unclear if the proposed method maintains its superiority (TVD 0.9979) when the network complexity increases to include the four broader barrier categories mentioned in the Introduction.
- What evidence would resolve it: A comparative analysis of TVD and KL divergence using the SEM-based BN on the complete dataset including all barrier types.

### Open Question 2
- Question: Does the SEM-based BN synthetic data improve the utility of downstream machine learning classifiers compared to Gaussian Copulas and CTGANs?
- Basis in paper: [inferred] The paper evaluates statistical fidelity (TVD, KL) and causal inference, but does not assess "Train on Synthetic, Test on Real" (TSTR) performance, despite listing "training the models" as a primary goal in the Abstract.
- Why unresolved: High statistical similarity does not always translate to better performance on specific predictive tasks, such as identifying vulnerable demographic groups.
- What evidence would resolve it: Benchmarking the accuracy of standard classifiers (e.g., Random Forest, Logistic Regression) trained on the synthetic data when evaluated on held-out real data.

### Open Question 3
- Question: What is the quantifiable privacy risk of the SEM-based BN synthetic data against membership inference attacks?
- Basis in paper: [inferred] The paper claims the method "maintains confidentiality" and reduces re-identification risk, but relies on the generalization properties of the BN without providing formal privacy guarantees (like differential privacy) or attack simulations.
- Why unresolved: The authors test Differential Privacy (epsilon values) for the "independent attribute mode," but the winning BN model is evaluated solely on statistical similarity, leaving its specific vulnerability to privacy attacks unknown.
- What evidence would resolve it: Metrics from privacy attacks, such as success rates of singling out individuals or attribute inference, applied to the generated BN dataset.

## Limitations
- Dataset focus on specific disability and accessibility context limits generalizability
- Comparison against only two alternative methods provides incomplete landscape view
- Lack of computational efficiency and scalability analysis for real-world deployment
- No formal privacy guarantees beyond statistical indistinguishability claims

## Confidence
- High: Statistical validation methodology (TVD, KL divergence, Chi-square tests)
- Medium: Causal relationship preservation claims
- Medium: Privacy preservation through generalization (no formal DP guarantees)

## Next Checks
1. Test BN approach on diverse categorical datasets from different domains to assess generalizability
2. Implement differential privacy mechanisms to quantify actual privacy preservation levels
3. Conduct downstream task performance evaluation where synthetic data is used for predictive modeling to verify practical utility beyond statistical similarity