---
ver: rpa2
title: 'Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View'
arxiv_id: '2506.15156'
source_url: https://arxiv.org/abs/2506.15156
tags:
- memory
- input
- mamba
- recall
- recency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates memory mechanisms in state-space models,
  particularly Mamba, by analyzing primacy and recency effects through structured
  recall tasks. The authors identify three key findings: (1) long-term memory is supported
  by a sparse subset of internal channels in the selective state-space block that
  persistently encode early input tokens, (2) short-term memory is governed by delta-modulated
  recurrence where recent inputs receive more weight due to exponential decay, but
  this recency advantage collapses when distractor items are introduced, and (3) memory
  allocation is dynamically modulated by semantic regularity where repeated relations
  in the input sequence shift the delta gating behavior, increasing the tendency to
  forget intermediate items.'
---

# Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View

## Quick Facts
- **arXiv ID**: 2506.15156
- **Source URL**: https://arxiv.org/abs/2506.15156
- **Reference count**: 22
- **Key outcome**: Long-term memory in Mamba is supported by sparse high-M(i) channels encoding early tokens, while short-term memory follows delta-modulated exponential decay that collapses with distractors; semantic regularity modulates forgetting via delta gating

## Executive Summary
This paper investigates memory mechanisms in Mamba models through structured recall tasks, identifying three distinct memory dynamics. The authors demonstrate that primacy effects emerge from sparse long-memory channels with high memory coefficients that persistently encode early inputs, while recency effects result from architectural bias where recent inputs receive exponentially higher weighting due to fewer accumulated decay steps. They further show that memory allocation is dynamically modulated by semantic regularity, where repeated relations shift delta gating behavior to increase forgetting of intermediate items. These findings are validated through targeted ablations and input perturbations on two large-scale Mamba-based models.

## Method Summary
The authors use structured recall tasks with subject-relation-object triplets to probe memory mechanisms in Mamba models. They evaluate two pretrained models (Falcon Mamba 7B and Mamba 1.4B) across sequence lengths from 8 to 128 triplets, using both repeated relation (same predicate throughout) and random relation (unique predicates) settings. Memory channel identification employs Algorithm 1 to extract channels with memory coefficient M(i) exceeding threshold τ=0.7 and proportion p=0.7. Intervention experiments ablate recurrence matrices A_t at the first triplet timestep for identified channels. Additional experiments test distraction effects (inserting 192-384 random tokens) and semantic regularity effects (varying repetition periods in input sequences).

## Key Results
- Long-term memory emerges from sparse channels in selective state-space block with high memory coefficients that persistently encode early tokens and are causally linked to primacy effects
- Short-term memory follows delta-modulated recurrence where recent inputs receive exponentially higher weighting due to fewer decay multiplications, but this recency advantage collapses when distractor items are introduced
- Memory allocation is dynamically modulated by semantic regularity: repeated relations in input sequence shift delta gating behavior, increasing tendency to forget intermediate items

## Why This Works (Mechanism)

### Mechanism 1: Long-Term Memory via Sparse Channel Allocation
A sparse subset of channels in Mamba's selective state-space block function as dedicated long-term memory units, persistently encoding early tokens. Channels with memory coefficient M(i) approaching 1.0 maintain cumulative recurrence from early timesteps through small Δt values that result in slower decay within the state transition matrix At. Targeted ablation of these identified channels causes significant drop in first-position recall while random ablation leaves performance intact.

### Mechanism 2: Recency via Delta-Modulated Exponential Decay
Recency effects emerge from Mamba's architectural bias where recent inputs receive exponentially higher weighting due to fewer accumulated decay steps. Following the recurrence equation, the j-th input contribution is proportional to cumulative product of At from j+1 to t. Recent inputs have undergone fewer decay multiplications, creating inherent recency advantage that collapses when distractor items are introduced between context and query.

### Mechanism 3: Semantic Regularity Modulates Delta Gating
Repeated patterns in input (e.g., identical relation tokens) shift Δt behavior, causing faster forgetting of intermediate items through a Ranschburg-like effect. High-frequency repetitive input suppresses Δt (slow forgetting, weak input injection) while low-frequency/varied input elevates Δt (fast forgetting, strong input injection). This creates periodic structure that biases the model toward relying on history rather than attending to intermediate inputs.

## Foundational Learning

- **State-Space Model (SSM) Recurrence** - Why needed here: Mamba's memory emerges from recursive hidden state updates (ht = At·ht-1 + Bt·xt), not attention. Understanding this is prerequisite to interpreting primacy/recency as decay dynamics. Quick check: Given ht = At·ht-1 + Bt·xt, what happens to early input x1's contribution at timestep t=100 if At approaches zero?

- **Discretization Parameter Δt** - Why needed here: Δt controls the forgetting rate and input integration strength. Small Δt → long memory; large Δt → fast forgetting. This is the key lever for both primacy and recency. Quick check: If Δt=0.1 for channel A and Δt=1.0 for channel B, which channel retains information from 50 timesteps ago better?

- **Serial Position Effect (Primacy/Recency)** - Why needed here: The paper uses cognitive psychology frameworks as behavioral probes. U-shaped recall accuracy is the diagnostic signature being explained mechanistically. Quick check: In a U-shaped recall curve, what does the "dip in the middle" indicate about memory allocation?

## Architecture Onboarding

- **Component map**: Input token → embedding xt → Δt computed from xt → At = exp(Δt · A_base) → Bt scaled by Δt → ht updated via recurrence → yt = Ct · ht + D · xt

- **Critical path**: 1) Input token → embedding xt 2) Δt computed from xt via projection + softplus 3) At = exp(Δt · A_base) determines decay rate 4) Bt scaled by Δt determines input integration strength 5) ht updated via recurrence; early tokens persist if At≈1 6) yt = Ct · ht + D · xt produces output

- **Design tradeoffs**: Coupled gating uses same Δt for both At (forgetting) and Bt (input injection), which the paper suggests could be decoupled to improve flexibility. Zero initialization favors early positions while uniform initialization flattens U-curve but may reduce edge benefits. Sparse long-memory channels concentrate in specific layers (Layer 17 in 7B model), creating functional localization but single points of failure.

- **Failure signatures**: Lost-in-the-middle shows strong U-shape with near-zero middle accuracy from repetitive input structure triggering delta suppression. Recency collapse shows sharp accuracy drop at all positions when distractors added, indicating short-term memory saturation. Primacy loss after intervention shows first-position recall drops ~40-60% when long-memory channels ablated while middle/recency preserved.

- **First 3 experiments**: 1) Replicate U-shape baseline by running structured recall (L=32, repeated relation) on your Mamba variant and plot accuracy vs. relative position 2) Identify long-memory channels using Algorithm 1 with τ=0.7, p=0.7 and visualize layer-wise distribution 3) Feed synthetic sequences with controlled repetition periods (k∈{2,4,8,16,32,64}) and log Δt values per layer to verify inverse relationship between input frequency and delta magnitude

## Open Questions the Paper Calls Out

- Does the U-shaped positional bias observed in synthetic structured recall tasks translate to performance degradation in real-world downstream applications such as long-context question answering, summarization, or reasoning? The authors state it is crucial to assess whether this positional bias meaningfully affects downstream performance.

- Would decoupling the discretization parameter Δt from the input projection matrix Bt improve memory dynamics and mitigate the rapid forgetting observed in repetitive inputs? The authors note that relaxing this coupling could improve memory dynamics.

- Can non-zero initialization of recurrent states effectively mitigate the "lost-in-the-middle" effect without compromising the model's strong primacy and recency performance? The authors show uniform state initialization flattens the U-shaped accuracy curve but leave trade-offs unexplored.

## Limitations

- Findings are validated only on two Mamba-based models using synthetic structured recall tasks, leaving generalization to other architectures and naturalistic tasks unverified
- Ablation experiments provide correlational rather than fully causal evidence for high-M(i) channels' role in primacy, lacking tests of artificial enhancement or varying initialization schemes
- The interaction between the three identified mechanisms is not fully characterized, particularly how semantic regularity modulates both delta gating and long-memory channel activation

## Confidence

**High Confidence** (⭐⭐⭐)
- Mathematical framework connecting SSM recurrence to primacy/recency effects is internally consistent and well-grounded
- Baseline U-shaped recall patterns and distraction effects are clearly observable and reproducible characteristics

**Medium Confidence** (⭐⭐)
- Identification of sparse long-memory channels via Algorithm 1 appears sound but threshold values are somewhat arbitrary
- Semantic regularity mechanism linking input periodicity to delta gating is plausible but causal pathway needs further validation

**Low Confidence** (⭐)
- Broader implications for model design and proposed architectural improvements are speculative without empirical validation

## Next Checks

1. **Decoupling Validation** - Implement modified Mamba variant with independent gating for state retention versus input integration, then compare primacy/recency patterns and semantic regularity effects against baseline coupled architecture

2. **Layer-wise Ablation Granularity** - Extend ablation experiments to test contribution of individual layers containing high-M(i) channels rather than entire channel sets to clarify functional criticality of Layer 17 concentration

3. **Natural Language Generalization** - Replace synthetic structured recall task with naturalistic sentence completion or question-answering tasks that preserve core memory demands while using realistic linguistic input to measure persistence of identified patterns