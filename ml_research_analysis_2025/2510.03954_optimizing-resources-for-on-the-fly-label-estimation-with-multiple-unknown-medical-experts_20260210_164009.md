---
ver: rpa2
title: Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown
  Medical Experts
arxiv_id: '2510.03954'
source_url: https://arxiv.org/abs/2510.03954
tags:
- experts
- expert
- data
- algorithm
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an adaptive algorithm for real-time ground
  truth inference from multiple unknown medical experts, addressing the challenge
  of efficiently allocating limited expert resources to a continuous stream of medical
  data. The core idea is to dynamically adjust the number of experts queried for each
  data point based on its latent difficulty, stopping when a confidence threshold
  is met.
---

# Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts

## Quick Facts
- **arXiv ID**: 2510.03954
- **Source URL**: https://arxiv.org/abs/2510.03954
- **Reference count**: 27
- **Primary result**: Adaptive querying strategy reduces expert queries by up to 50% while achieving accuracy comparable to non-adaptive baseline.

## Executive Summary
This paper presents an adaptive algorithm for real-time ground truth inference from multiple unknown medical experts, addressing the challenge of efficiently allocating limited expert resources to a continuous stream of medical data. The core idea is to dynamically adjust the number of experts queried for each data point based on its latent difficulty, stopping when a confidence threshold is met. Unlike prior methods, this approach works without prior knowledge of expert reliability, pre-labeled data, or ground truth feedback, and supports on-the-fly annotation. Experiments on three multi-annotator datasets (glioma classification, weather sentiment, music genre) show that the adaptive querying strategy reduces expert queries by up to 50% while achieving accuracy comparable to a non-adaptive baseline.

## Method Summary
The method implements a modular framework that processes a stream of data points sequentially. For each data point, it ranks available experts using one of three sampling strategies (AUER, Greedy, or Random), queries the top experts incrementally, computes confidence in the inferred label using weighted likelihood aggregation, and stops querying when confidence exceeds threshold τ or all experts are exhausted. Expert trust is estimated online using Bayesian posterior updates with Beta-Bernoulli conjugacy, and parameters are retrospectively updated using EM on the annotation history. The system requires no pre-labeled data or ground truth feedback, making it suitable for real-time medical workflows.

## Key Results
- Adaptive querying reduces expert queries by up to 50% compared to non-adaptive baseline while maintaining comparable accuracy
- Greedy expert sampling achieves the best accuracy-cost tradeoff due to passive exploration from multiple queries per round
- The method successfully operates without prior knowledge of expert reliability or pre-labeled data

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Adaptive Query Stopping
Dynamically stopping expert queries when confidence threshold τ is met reduces annotation cost while maintaining accuracy. For each data point, the algorithm queries experts sequentially, computing confidence after each query using weighted likelihood aggregation. Querying stops when confidence meets τ or all experts are exhausted. Core assumption: expert errors are independent across data points and across experts for a given data point.

### Mechanism 2: Bayesian Trust Estimation with Cold Start
Expert trust scores can be estimated online without prior knowledge using Bayesian posterior updates. Each expert's trust is computed as the mean of Beta posterior, initialized with uniform prior to enable cold start. EM is applied retrospectively to update parameters from annotation history. Core assumption: expert reliability remains relatively static during annotation session.

### Mechanism 3: Multi-Armed Bandit Expert Ranking
MAB-based ranking balances exploration (discovering expert reliability) and exploitation (prioritizing accurate experts). AUER ranks by upper confidence bound, Greedy by trust with unqueried experts first, Random samples uniformly. Core assumption: multiple queries per round provide sufficient passive exploration to compensate for greedy exploitation.

## Foundational Learning

- **Bayesian Inference with Beta-Bernoulli Conjugacy**
  - Why needed: Core to trust estimation without priors; understanding how Beta posterior updates with Bernoulli trials
  - Quick check: Given 3 successes in 5 queries, what is the mean of Beta(4, 3)?

- **Expectation-Maximization (EM) for Latent Variable Estimation**
  - Why needed: Required for function C to retrospectively estimate expert parameters from annotation history without ground truth
  - Quick check: In EM for mixture models, what happens in the E-step vs. M-step?

- **Multi-Armed Bandit Tradeoffs (UCB/Greedy/ε-greedy)**
  - Why needed: Understanding exploration-exploitation in expert selection and why Greedy works with multiple queries per round
  - Quick check: Why does UCB provide regret guarantees that Greedy does not in single-query settings?

## Architecture Onboarding

- **Component map**: Expert Ranking (A) -> Label Inference (B) -> Parameter Update (C) -> Main Loop
- **Critical path**: Initialize θ̂ with uniform Beta priors, empty history ζ. For each xn: rank experts → query top 2 → compute confidence → while confidence < τ and experts remain, query next expert → append to ζ → update θ̂. Return coalitional labels.
- **Design tradeoffs**:
  - τ (confidence threshold): Low τ = lower cost, lower accuracy; high τ = higher cost, higher accuracy
  - Expert sampling strategy: AUER = balanced workload, robust to uncertainty; Greedy = cost-efficient but unequal load; Random = fair distribution but inefficient
  - Expert model complexity: Paper uses scalar trust for sample efficiency vs. confusion matrices requiring O(|Z|²) samples
- **Failure signatures**:
  - Confidence never reaches τ: Check if experts are too unreliable or τ > 1 is effectively unreachable
  - Accuracy drops at high Qn: Less reliable experts adding noise
  - One expert dominates queries: Greedy sampling without exploration; switch to AUER
  - Queries remain high through time: Coalition may have high inherent disagreement; check if EM is converging
- **First 3 experiments**:
  1. Baseline vs. Adaptive on WS dataset: Fix Q ∈ {2, 4, 6, 8, 10, 12} for baseline; vary τ ∈ {0.4, 0.6, 0.8} for adaptive. Plot accuracy vs. average queries.
  2. Sampling strategy comparison on GC dataset: Run AUER, Greedy, Random with τ = 0.6. Measure Gini coefficient on query distribution.
  3. Convergence analysis over time: Track average Qn per 50-sample window for τ = 0.8 on MG dataset.

## Open Questions the Paper Calls Out
- Can the adaptive querying framework be extended to structured prediction tasks like medical image segmentation while maintaining efficiency gains?
- How does the algorithm perform with real clinical experts in live screening workflows compared to benchmark datasets?
- Does incorporating time-varying expertise modeling improve performance when expert skills evolve during annotation?

## Limitations
- Assumes expert errors are independent across data points and across experts for a given data point, which may not hold when experts share systematic biases
- EM implementation details are underspecified, potentially affecting practical performance and convergence behavior
- Evaluation relies on crowdsourced and simulated experts rather than real medical professionals, limiting clinical generalizability

## Confidence
- **High confidence**: Confidence-adaptive stopping mechanism and cost-accuracy tradeoff are well-supported by experimental results
- **Medium confidence**: Bayesian trust estimation is theoretically sound but depends heavily on EM implementation details
- **Low confidence**: MAB expert ranking claims lack direct corpus support and theoretical justification for greedy performance

## Next Checks
1. **Correlation sensitivity test**: Introduce controlled correlation between expert errors in synthetic datasets and measure how confidence estimates degrade
2. **EM convergence analysis**: Track parameter estimation stability across multiple random initializations and varying history lengths
3. **Expert availability simulation**: Test the method under realistic medical workflows where experts have limited availability windows