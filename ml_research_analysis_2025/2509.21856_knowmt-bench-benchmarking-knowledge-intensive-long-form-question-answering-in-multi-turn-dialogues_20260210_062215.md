---
ver: rpa2
title: 'KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering
  in Multi-Turn Dialogues'
arxiv_id: '2509.21856'
source_url: https://arxiv.org/abs/2509.21856
tags:
- answer
- question
- multi-turn
- qwen-2
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KnowMT-Bench, the first benchmark for Multi-Turn
  Long-Form Question Answering (MT-LFQA), addressing the gap in evaluating conversational
  factual capabilities in knowledge-intensive domains. The benchmark employs a dynamic
  evaluation setting where models generate their own multi-turn dialogue histories,
  and uses a human-validated automated pipeline based on Natural Language Inference
  to assess factual capability and information delivery efficiency.
---

# KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues

## Quick Facts
- **arXiv ID:** 2509.21856
- **Source URL:** https://arxiv.org/abs/2509.21856
- **Reference count:** 40
- **Primary result:** Introduces KnowMT-Bench, the first benchmark for Multi-Turn Long-Form Question Answering (MT-LFQA), revealing that multi-turn contexts degrade model performance due to contextual noise from self-generated histories, and showing that RAG can effectively mitigate this degradation.

## Executive Summary
This paper introduces KnowMT-Bench, the first benchmark for Multi-Turn Long-Form Question Answering (MT-LFQA), addressing the gap in evaluating conversational factual capabilities in knowledge-intensive domains. The benchmark employs a dynamic evaluation setting where models generate their own multi-turn dialogue histories, and uses a human-validated automated pipeline based on Natural Language Inference to assess factual capability and information delivery efficiency. Experiments reveal that multi-turn contexts degrade model performance, primarily due to contextual noise from self-generated histories, leading to reduced factuality and increased verbosity. Retrieval-augmented generation (RAG) is shown to effectively mitigate this degradation, even reversing factual decline in some cases. The findings highlight the inadequacy of single-turn evaluations and underscore the need for robust frameworks like KnowMT-Bench to guide future research in enhancing conversational factual capabilities of LLMs.

## Method Summary
KnowMT-Bench uses a dynamic evaluation setting where models self-generate multi-turn dialogue histories for knowledge-intensive questions. The pipeline employs Qwen2.5-32B-Instruct to decompose answers into atomic statements and Qwen2.5-14B-Instruct with NLI-based prompts to judge factuality. Evaluation metrics include Factuality F1 (Sf), Hallucination F1 (Sh), and efficiency measures (Df, Dh). RAG strategies (Base, Last, Rounds, All) are tested to mitigate multi-turn degradation. The benchmark covers 801 instances across medicine, finance, and law domains.

## Key Results
- Multi-turn contexts universally degrade model performance compared to single-turn baselines, with factuality declining due to contextual noise from self-generated histories.
- RAG effectively mitigates this degradation, with the "Rounds" strategy (retrieval at each turn) performing best and even reversing factual decline in some cases.
- Domain-specific fine-tuning improves multi-turn robustness in medicine (HuatuoGPT) but shows inconsistent benefits in finance (Fin-R1), highlighting the need for robust multi-turn strategies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn performance degradation is driven by contextual noise from self-generated history, not dialogue length.
- Mechanism: Models accumulate factual errors and irrelevant content in their own prior turns; this "noise" propagates forward, impairing subsequent fact retrieval and synthesis. The degradation is independent of turn count—single vs. multi-turn matters, not 2 vs. 5 turns.
- Core assumption: The model's own outputs contain sufficient factual imperfections to measurably impact later reasoning.
- Evidence anchors:
  - [abstract]: "factual capability declines due to the contextual noise from self-generated histories"
  - [Section 4.3]: "factual capability (Sf ) and (Sh) do not exhibit a clear monotonic trend with the number of rounds... a universal performance drop occurs in all multi-turns compared to the single-turn baseline"
  - [corpus]: Related work (D-SMART) similarly identifies "factual inconsistencies and logical decay in extended, multi-turn dialogues" from static knowledge reliance, supporting the noise hypothesis.
- Break condition: If dialogue history is replaced with higher-quality outputs (e.g., from a stronger model), degradation should diminish (validated in the "Replace" experiment, Figure 6).

### Mechanism 2
- Claim: RAG reverses factual degradation by grounding each turn with external evidence, preventing noise accumulation.
- Mechanism: Retrieval provides fresh, authoritative context at each turn, overriding the model's noisy self-generated history. The "Rounds" strategy (retrieval at each turn using current query) performs best because it interrupts noise propagation early.
- Core assumption: Retrieved evidence is sufficiently accurate and relevant to outweigh historical noise.
- Evidence anchors:
  - [abstract]: "Retrieval-augmented generation (RAG) is shown to effectively mitigate this degradation, even reversing factual decline in some cases"
  - [Section 5.2]: "Rounds is the best strategy in the multi-turn setting... it enables the model to achieve a higher factuality score in the multi-turn setting than even the RAG-enhanced single-turn baseline"
  - [corpus]: QA-Dragon demonstrates query-aware dynamic RAG for knowledge-intensive VQA, corroborating the value of retrieval grounding in multi-turn settings.
- Break condition: If retrieval corpus is low-quality or queries are ambiguous, RAG may introduce new noise and underperform.

### Mechanism 3
- Claim: Dynamic evaluation (model self-generates history) reveals real-world fragility that static evaluation masks.
- Mechanism: Static benchmarks use curated or model-generated but fixed histories, which may be cleaner than actual deployment outputs. Dynamic evaluation forces the model to confront its own error propagation, exposing brittleness in factuality and efficiency.
- Core assumption: Real-world consultations involve progressive, multi-turn interactions where prior turns influence the final answer.
- Evidence anchors:
  - [abstract]: "To faithfully assess the model's real-world performance, KnowMT-Bench employs a dynamic evaluation setting where models generate their own multi-turn dialogue histories"
  - [Section 3.2]: "To simulate a realistic human-LLMs interaction, the benchmark requires models to generate their own dialogue history following logically progressive human-authored question sequences"
  - [corpus]: TriMediQ similarly notes reasoning deterioration "in multi-turn clinical dialogues where patient information is scattered across turns," reinforcing the need for dynamic, multi-turn benchmarks.
- Break condition: If models are deployed in single-turn or heavily supervised settings, dynamic degradation may not manifest.

## Foundational Learning

- Concept: Natural Language Inference (NLI) for factuality evaluation
  - Why needed here: The pipeline decomposes answers into atomic statements and uses NLI (Entailment/Contradiction/Neutral) to assess factual consistency against ground truth. Without NLI, surface-level metrics like ROUGE fail to capture factual correctness.
  - Quick check question: Given statement A "The drug reduces fever" and statement B "The medication has antipyretic effects," what is the NLI relationship?

- Concept: Long-Form Question Answering (LFQA) vs. short-form QA
  - Why needed here: LFQA requires synthesizing multiple facts into paragraph-level answers, not extracting short spans. Multi-turn LFQA adds the complexity of dialogue history context.
  - Quick check question: How does evaluation differ for "What is the capital of France?" vs. "Explain the causes and consequences of the 2008 financial crisis?"

- Concept: Information delivery efficiency (tokens per fact)
  - Why needed here: The paper introduces efficiency metrics (Df, Dh, DR) to measure verbosity and fact density. Long answers with few facts degrade user utility.
  - Quick check question: Model A generates 100 tokens containing 5 correct facts. Model B generates 200 tokens containing 7 correct facts. Which has better Df?

## Architecture Onboarding

- Component map: Single-turn question -> Multi-turn sequence generation -> Dynamic history construction -> Final-turn answer generation -> Atomic decomposition -> NLI judgment -> Metric aggregation

- Critical path: Single-turn question → multi-turn sequence generation → dynamic history construction → final-turn answer generation → atomic decomposition → NLI judgment → metric aggregation.

- Design tradeoffs:
  - Decomposer model size: Larger decomposer (32B) improves segmentation but increases compute cost.
  - Evaluator model selection: Qwen2.5-14B balances accuracy and efficiency; larger models showed instability in constrained judgment tasks.
  - RAG strategy: "Rounds" is most effective but requires retrieval at every turn; "Base" is cheaper but less robust to noise.

- Failure signatures:
  - Under-segmentation in decomposition (SMAPE 18.1%, omission 5.9%) leads to bundled claims, reducing metric precision.
  - Self-preference bias in evaluation is minimal (Table 7), but evaluator calibration drift should be monitored.
  - CoT reasoning models may be undervalued by n-gram metrics due to stylistic artifacts (Appendix G.5).

- First 3 experiments:
  1. Replicate single-turn vs. multi-turn degradation on a subset (e.g., 50 instances) using your target model; verify that Sf drops and Sh rises in multi-turn.
  2. Implement the "Rounds" RAG strategy and compare factuality (Sf) against the non-RAG baseline; expect improvement if retrieval corpus is high-quality.
  3. Run ablation on dialogue length (2, 3, 4, 5 turns) to confirm that degradation is not length-dependent, isolating noise as the driver.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can model architectures be designed to break the strong positive correlation ($R^2=0.82$) between token cost for correct facts ($D_f$) and token cost for hallucinated facts ($D_h$) in multi-turn dialogues?
- **Basis in paper:** [explicit] Section 4.2 states that "a key direction for future research is to design models or strategies that can break this trade-off by achieving a low $D_f$ while simultaneously increasing $D_h$ within the multi-turn dialogue."
- **Why unresolved:** Current models are observed to be uniformly concise or verbose; none currently achieve high efficiency for correct information while remaining verbose on errors to make them detectable.
- **What evidence would resolve it:** A model architecture or loss function that successfully minimizes $D_f$ (cost per correct fact) while maximizing $D_h$ (cost per error), pushing performance beyond the established regression baseline.

### Open Question 2
- **Question:** What sophisticated intervention strategies can dynamically discern between useful context and compounding noise in self-generated dialogue history?
- **Basis in paper:** [explicit] Section 5.3 notes that simple prompting interventions create a trade-off (improving factuality but increasing hallucination) and concludes that "achieving simultaneous gains... will likely require more sophisticated intervention strategies that can dynamically discern between useful context and compounding noise."
- **Why unresolved:** Explicitly filtering noise via simple system prompts causes models to disregard useful context, increasing hallucination rates.
- **What evidence would resolve it:** An intervention method that yields simultaneous improvements in both Factual F1 ($S_f$) and Hallucination F1 ($S_h$) compared to the non-prompted baseline.

### Open Question 3
- **Question:** Why does domain-specific fine-tuning improve multi-turn robustness in medicine (HuatuoGPT) but fail to do so consistently in finance (Fin-R1)?
- **Basis in paper:** [explicit] Section 5.1 highlights that while HuatuoGPT suppressed noise effectively, "the case of Fin-R1 highlights that such benefits are not guaranteed," as it showed no consistent superiority over the baseline.
- **Why unresolved:** The paper establishes the inconsistency but does not analyze the specific properties of medical vs. financial training data or model architectures that cause this divergence.
- **What evidence would resolve it:** A comparative analysis of training data noise distributions or knowledge density that explains why intrinsic knowledge injection suppresses noise in one domain but not the other.

## Limitations
- The NLI-based evaluation pipeline has potential noise in fact extraction (SMAPE 18.1%, omission 5.9%) that could propagate to metric calculations.
- Dynamic evaluation may amplify model-specific artifacts like CoT reasoning patterns that are penalized by n-gram metrics.
- The evaluation is constrained to three knowledge domains (medicine, finance, law), limiting generalizability to other fact-intensive areas.

## Confidence
- **High Confidence:** The core finding that multi-turn contexts degrade model performance due to contextual noise from self-generated histories is well-supported by controlled experiments (Section 4.3) and corroborated by ablation studies (Section 5.1).
- **Medium Confidence:** The claim that RAG effectively mitigates degradation is supported, but the "reversal" effect is context-dependent and not universal across all models or domains.
- **Low Confidence:** The assertion that single-turn evaluations are "inadequate" is plausible but overstated. Static benchmarks may still capture certain aspects of model capability.

## Next Checks
1. Replicate the "Replace" experiment (Section 5.1) with a different strong model (e.g., GPT-4) to confirm that replacing self-generated history reduces degradation, isolating noise as the primary driver.
2. Test RAG failure modes by intentionally degrading the retrieval corpus (e.g., introducing irrelevant or incorrect documents) to assess whether RAG can introduce new noise and underperform the baseline.
3. Expand evaluation to additional domains (e.g., history, science) to verify the generalizability of the degradation and mitigation effects observed in medicine, finance, and law.