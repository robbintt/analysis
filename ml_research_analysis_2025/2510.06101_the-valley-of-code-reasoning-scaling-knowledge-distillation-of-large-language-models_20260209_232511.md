---
ver: rpa2
title: 'The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language
  Models'
arxiv_id: '2510.06101'
source_url: https://arxiv.org/abs/2510.06101
tags:
- reasoning
- data
- arxiv
- distillation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a "valley of code reasoning" in distillation:
  small models first degrade by half in competitive coding performance when trained
  on small datasets (1K examples), then improve sharply (50-100% gains) in a log-linear
  trend up to 30K examples. The effect holds across two non-reasoning LLMs (Qwen2.5-7B
  and Llama3.1-8B).'
---

# The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models

## Quick Facts
- arXiv ID: 2510.06101
- Source URL: https://arxiv.org/abs/2510.06101
- Reference count: 3
- Key outcome: Small models first degrade by half in competitive coding performance when trained on small datasets (1K examples), then improve sharply (50-100% gains) in a log-linear trend up to 30K examples.

## Executive Summary
This paper identifies a "valley of code reasoning" in knowledge distillation: small models initially drop in competitive coding performance when trained on small datasets, then improve sharply as data increases. The effect holds across two non-reasoning LLMs (Qwen2.5-7B and Llama3.1-8B). Surprisingly, correctness of teacher responses has no impact on distillation outcomes, while easier coding questions consistently yield better performance than harder ones. These findings suggest small models in low-data regimes benefit more from simpler examples and output structure rather than answer accuracy, informing data curation strategies for reasoning distillation.

## Method Summary
The study distills chain-of-thought reasoning from large reasoning models (DeepSeek-R1-0528, KAT-V1-40B) into small non-reasoning LLMs (Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct) using the OCR2 dataset with 34,125 problems. Training uses torchtune on 8×H100 GPUs with batch size 128, learning rate 8e-5, and 5 epochs. The primary evaluation metric is LIVECODEBENCH (LCB) Pass@1, supplemented by completion rate and think tag occurrence rate. Experiments vary training data size (1K→10K→30K), correctness (via TACO labels), and difficulty (stratified subsets).

## Key Results
- Performance drops by more than half from baseline when training on 1K examples, then improves by ~50% at 10K examples
- Output structure matters more than content correctness during distillation (both subsets lift LCB score by ~50%)
- Small models benefit significantly more from easier coding questions (Qwen2.5 improves by 41% on easy vs 7% on hard)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In low-data regimes (1K-30K samples), reasoning distillation exhibits non-monotonic scaling where performance first drops then rises sharply.
- Mechanism: Small models initially allocate capacity toward learning output structure (CoT tags, response format) at the cost of task performance. Once structural patterns stabilize (~10K samples), additional data enables rapid content learning with sharper-than-log-linear gains.
- Core assumption: The "valley" reflects a phase transition where structural imitation competes with reasoning acquisition before converging.
- Evidence anchors:
  - [abstract] "performance initially drops when training data is increased from 1K to 10K samples, then steadily improves with further data increases in a sharper-than-log-linear fashion"
  - [section 4] "The model's LCB score first decreases by more than half from the baseline when trained on 1K examples, then improves and surpasses the baseline by around 50% when the data size reaches 10K"
  - [corpus] Related work (OpenCodeReasoning, s1) focuses on large-scale or minimal data regimes; none characterize the intermediate 1K-30K dynamics explicitly.
- Break condition: May not hold above ~100K samples where saturation effects emerge, or for models already fine-tuned on reasoning data.

### Mechanism 2
- Claim: Output structure—not code correctness—drives distillation effectiveness for small models.
- Mechanism: Student models extract reasoning patterns (planning, decomposition, verification) from teacher traces regardless of whether the final code executes correctly. The format teaches "how to think," not "what answer is right."
- Core assumption: Structural features (CoT tags, step-by-step breakdown) transfer independently of factual accuracy.
- Evidence anchors:
  - [abstract] "output structure matters more than content correctness during distillation"
  - [section 4, Table 2] "Compared to each baseline, the two subsets both lift the LCB score by around 50%, which suggests that the correctness of responses has no effect on distillation outcomes"
  - [corpus] Li et al. (2025) and related works found similar results for larger (32B+) models; this paper extends findings to 7-8B models across training stages.
- Break condition: May fail when incorrect outputs have structurally degraded reasoning (incoherent traces), or in domains where correctness and structure are confounded.

### Mechanism 3
- Claim: Small models gain more from easier problems than hard ones during distillation.
- Mechanism: Easy problems provide clearer signal-to-noise ratio for learning reasoning patterns. Harder problems may introduce complexity that overwhelms limited model capacity before basic patterns are established.
- Core assumption: Difficulty affects learnability of reasoning patterns, not just solution complexity.
- Evidence anchors:
  - [abstract] "small models benefit significantly more from easier coding questions than harder ones"
  - [section 4, Table 2] "Qwen2.5 improves by 41% [on easy data], whereas Qwen2.5 improves by 7% [on hard data]"
  - [corpus] Weak corpus signal on difficulty-stratified distillation; most work focuses on overall data quality or scale.
- Break condition: May reverse in high-data regimes (>100K) where models have capacity to extract signal from hard examples.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Distillation**
  - Why needed here: The entire method relies on transferring reasoning traces from teacher to student via supervised fine-tuning.
  - Quick check question: Can you explain why training on reasoning *traces* differs from training on final answers only?

- Concept: **Supervised Fine-Tuning (SFT) Dynamics**
  - Why needed here: Understanding how loss landscapes shift with data scale is essential to interpret the "valley" effect.
  - Quick check question: What happens to model behavior when SFT data is insufficient to cover output distribution variance?

- Concept: **Benchmark Contamination**
  - Why needed here: The paper uses LiveCodeBench specifically because it's contamination-controlled; understanding this ensures valid evaluation.
  - Quick check question: Why would standard coding benchmarks give misleading signals about reasoning distillation progress?

## Architecture Onboarding

- Component map:
  Teacher models (DeepSeek-R1-0528, KAT-V1-40B) -> Data pipeline (OCR2 dataset) -> Student models (Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct) -> Training (torchtune, 8×H100) -> Evaluation (LIVECODEBENCH, completion rate, think tag rate)

- Critical path:
  1. Start with 1K random subset → expect performance drop (valley entry)
  2. Scale to 10K → expect recovery to baseline
  3. Scale to 30K → expect sharper-than-log-linear improvement
  4. Optional: Stratify by difficulty (easy preferred) or correctness (no expected difference)

- Design tradeoffs:
  - Random sampling preserves distribution but may include signal-diluting hard examples
  - 32K token limit truncates long reasoning traces (completion rate becomes proxy metric)
  - 5 epochs may underfit at 30K scale (paper hints at non-saturation)

- Failure signatures:
  - No  Matthtags in outputs → student failed to learn structure (check tokenization, data format)
  - High completion rate but low LCB score → structural learning without reasoning transfer
  - Performance plateau at 10K → possible data quality issue or learning rate too low

- First 3 experiments:
  1. Replicate valley effect: Train Qwen2.5-7B on 1K, 10K, 30K OCR2 subsets; plot LCB scores to verify non-monotonic curve.
  2. Ablate correctness: Train on 6K correct vs 6K incorrect responses (TACO-labeled); confirm ~equal LCB gains.
  3. Ablate difficulty: Train on 4K easy vs 4K hard; verify easy subset yields 3-4x larger improvement.

## Open Questions the Paper Calls Out

- Does the "valley of code reasoning" phenomenon persist in medium-high and high data regimes (above 100K training samples), and does the sharper-than-log-linear scaling trend continue?
- Do the findings about correctness irrelevance and difficulty importance generalize to high-data regimes (100K+ samples)?
- Does the valley phenomenon occur in reasoning domains beyond competitive coding, such as mathematics or general logical reasoning?
- What mechanistic explanation accounts for the initial performance drop (the "valley") when scaling from 1K to 10K samples?

## Limitations

- The scaling effect was only tested across two non-reasoning base models and two teacher models, leaving generalization to other model families uncertain.
- Correctness labels from TACO test cases may not fully capture all dimensions of answer quality, potentially confounding the no-impact finding.
- The dataset (OCR2) is competition-focused, so findings may not extend to production or enterprise coding tasks with different complexity distributions.

## Confidence

- **High**: The non-monotonic scaling curve (valley effect) is well-supported by direct experimental evidence and consistent across both student models tested.
- **Medium**: The claim that output structure matters more than correctness is supported by controlled ablations, but relies on correctness labels that may not capture all dimensions of answer quality.
- **Medium**: The finding that easier problems yield better distillation outcomes is well-documented within the tested dataset, but may reverse in different domains or at larger scales.

## Next Checks

1. Test the valley effect with a third student model (e.g., Mistral-7B-Instruct) to assess model-family independence.
2. Evaluate correctness-stratified distillation on a second dataset (e.g., HumanEval or MBPP) to confirm the no-impact finding is not dataset-specific.
3. Scale training to 50K-100K samples to determine whether the sharper-than-log-linear gains plateau or continue, and whether the valley effect persists at higher data regimes.