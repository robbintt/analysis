---
ver: rpa2
title: 'On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic
  Review and Taxonomy'
arxiv_id: '2510.12201'
source_url: https://arxiv.org/abs/2510.12201
tags:
- evaluation
- system
- user
- design
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review of 65 user studies identifies key challenges
  in human-centered explainable AI (XAI) evaluation. Current practices lack validation,
  standardization, and holistic assessment of system properties beyond trust.
---

# On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy

## Quick Facts
- arXiv ID: 2510.12201
- Source URL: https://arxiv.org/abs/2510.12201
- Reference count: 40
- Key outcome: Identifies lack of validation and standardization in XAI evaluation, proposing taxonomy categorizing metrics into core system, explanation, and user characteristics

## Executive Summary
This systematic review of 65 user studies reveals critical gaps in human-centered XAI evaluation, including insufficient validation, lack of standardization, and overemphasis on trust metrics. The authors propose a comprehensive taxonomy separating evaluation into core system properties (affection, cognition, usability, interpretability) and explanation-specific metrics (usefulness, satisfaction). They introduce distinct design goals for AI novices (responsible use, acceptance, usability) and data experts (human-AI collaboration, task performance), along with guidelines emphasizing validated questionnaires, comprehensive metric selection, and behavioral intention measurement.

## Method Summary
The study employed a Composite Literature Review Method, searching IEEE Xplore and ACM Digital Library for papers published 2016-2023. Two-step filtering yielded 65 user studies, which were analyzed through inductive coding by two raters with consensus checks. Concepts appearing fewer than 5 times were excluded. The methodology focused on identifying human-centered evaluation metrics and design goals, distinguishing between AI novices and data experts, and generating binary concept matrices for system properties and evaluation metrics.

## Key Results
- Current XAI evaluation practices lack validation and standardization
- Most studies focus on single metrics (75% measure only one) rather than holistic assessment
- Proposed taxonomy categorizes metrics into core system, explanation, and user characteristics
- Different design goals needed for AI novices versus data experts
- Only 4 out of 28 trust questionnaires were validated in reviewed studies

## Why This Works (Mechanism)

### Mechanism 1: The Holistic Evaluation Taxonomy
- **Claim:** Evaluating XAI systems using a holistic taxonomy (Core System + Explanation + User) rather than isolated metrics may provide a more accurate prediction of real-world adoption and safety.
- **Mechanism:** By separating evaluation into Core System metrics (affection, cognition, usability, interpretability) and Explanation metrics (usefulness, satisfaction), designers can identify specific failure points versus global failures.
- **Core assumption:** Users perceive the "Core System" and the "XAI Explanation" as distinct but interacting components; a failure in one can be compensated by the other, or drag down the whole.
- **Evidence anchors:** [Abstract] "...distinction between the core system and the XAI explanation, which together form the whole system." [Section 5.1.1] "Most of the evaluated XAI systems were analysis systems... An important finding is the distinction between the core system and the XAI explanation..."

### Mechanism 2: User-Group Specific Design Goals
- **Claim:** Aligning XAI design objectives with specific user expertise levels (AI Novices vs. Data Experts) increases the relevance of the system for that demographic.
- **Mechanism:** AI Novices prioritize "responsible use" and "acceptance" (emotional/safety goals), whereas Data Experts prioritize "human-AI collaboration" and "task performance" (functional goals).
- **Core assumption:** The needs of AI novices and data experts are mutually exclusive or at least distinct enough to require different metric weightings.
- **Evidence anchors:** [Abstract] "For AI novices, the relevant extended design goals include responsible use, acceptance, and usability. For data experts, the focus is performance-oriented..." [Section 5.2.1] "The first design goal we identified for AI novices was to promote responsible use..."

### Mechanism 3: Validated Questionnaires vs. Behavioral Intention
- **Claim:** Using validated questionnaires (e.g., for Trust or Satisfaction) alongside behavioral intention metrics reduces the risk of measuring "subjective bias" without actual usage follow-through.
- **Mechanism:** Subjective metrics (like perceived usefulness) are predictors of Intention to Use, but Intention to Use is the direct predictor of actual Usage Behavior.
- **Core assumption:** Self-reported intention is a necessary but insufficient proxy for actual system adoption.
- **Evidence anchors:** [Section 5.1.2] "Even though other metrics such as perceived usefulness are also related to system adoption, they are rather predictors of intention to use than direct predictors of usage behavior." [Section 6.1.5] "It is therefore recommended to measure intention to use..."

## Foundational Learning

- **Concept:** **Construct Validity**
  - **Why needed here:** The paper explicitly critiques the field for using unvalidated questionnaires (only 4 out of 28 trust questionnaires were validated). Understanding validity is required to select the right metrics from the taxonomy.
  - **Quick check question:** Can you distinguish between a "face-valid" question (looks relevant) and a "construct-valid" scale (proven to measure the specific psychological trait)?

- **Concept:** **Mental Models**
  - **Why needed here:** This is a primary metric in the "Cognition" category. The mechanism of XAI relies on aligning the user's mental model with the system's actual logic.
  - **Quick check question:** How do you measure if a user's internal representation of the system logic matches the algorithm's actual behavior?

- **Concept:** **Technology Acceptance Model (TAM)**
  - **Why needed here:** The paper uses TAM to contextualize "Perceived Usefulness" and "Intention to Use". This model underpins the "Acceptance" design goal for AI novices.
  - **Quick check question:** In the context of TAM, does "Perceived Ease of Use" directly cause "Intention to Use", or does it mediate through "Perceived Usefulness"?

## Architecture Onboarding

- **Component map:** Define Target User (Novice/Expert) → Select Design Goal (e.g., Responsible Use) → Map Goal to Metrics (e.g., Trust + Transparency) → Select Validated Instrument (e.g., TXAI scale)
- **Critical path:** Target User → Design Goal → Metric Mapping → Validated Instrument
- **Design tradeoffs:**
  - **Validation vs. Speed:** Using validated scales (e.g., Hoffman's scales) takes time to administer but ensures data reliability; custom scales are faster but lack comparability.
  - **Local vs. Global Explanations:** Local (specific output) aids immediate task performance (Data Experts); Global (general model behavior) aids overall trust/transparency (Novices).
- **Failure signatures:**
  - **The "Trust Trap":** High subjective trust scores but low task performance (users are "comforted" but not "helped").
  - **Explanation Disconnect:** High explanation satisfaction but low user task performance (explanations are pleasing but not informative).
- **First 3 experiments:**
  1. **Metric Audit:** Review your current user study protocol. Count how many metrics are "Core System" vs. "Explanation". If 0 in either, the evaluation is incomplete based on the paper's taxonomy.
  2. **Scale Validation:** Replace any ad-hoc "Trust" questions with the validated TXAI scale or TPA scale mentioned in Section 5.1.2.
  3. **Behavioral Check:** Add a behavioral metric (e.g., time on task or decision accuracy) alongside subjective "Intention to Use" questionnaires to verify if stated intent matches actual capability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can frameworks of "desirable" explanation characteristics be synthesized with this taxonomy to bridge the gap between user preferences and actual evaluation practices?
- **Basis in paper:** [explicit] Section 6.3 states that a synthesis between frameworks describing what users want (e.g., Kim's meaningfulness characteristics) and this paper's taxonomy of what is measured would be beneficial.
- **Why unresolved:** This review focused exclusively on metrics currently employed in the literature, which may not fully align with the implicit needs or desires of end-users.
- **What evidence would resolve it:** An integrated model mapping user-defined desirable characteristics to specific, measurable evaluation metrics, validated through empirical user studies.

### Open Question 2
- **Question:** Can standardized, validated questionnaires be developed for XAI-specific metrics (e.g., explanation usefulness) to replace the current reliance on ad-hoc measures?
- **Basis in paper:** [explicit] Section 6.3 and 6.1.1 identify a critical lack of validated instruments, noting that only 1 out of 17 questionnaires for explanation usefulness was validated.
- **Why unresolved:** The authors note that the absence of validation leaves open the possibility that intended constructs are not being accurately measured, hindering comparability.
- **What evidence would resolve it:** The development and psychometric validation of new scales (e.g., a domain-independent "Domain Expertise" scale) that demonstrate high reliability and validity across multiple XAI application contexts.

### Open Question 3
- **Question:** What are the distinct human-centered evaluation metrics and design goals for "AI Experts," who were absent from the reviewed user studies?
- **Basis in paper:** [explicit] Section 2.4 and 3.1.2 note that AI experts (who design ML algorithms) were excluded because they were not found as a target group in the reviewed literature, leaving a gap in the framework.
- **Why unresolved:** The current design goals are limited to AI novices and data experts; it is unclear if AI experts require different metrics beyond technical performance for their debugging/inspection tasks.
- **What evidence would resolve it:** A systematic series of user studies targeting AI experts to identify if unique affective, cognitive, or usability metrics are required for their specific workflow.

## Limitations

- The taxonomy's effectiveness depends on accurate user classification, yet the paper doesn't address hybrid users (e.g., domain experts who are AI novices)
- The inductive coding process lacks reported inter-rater reliability statistics, making it difficult to assess consistency in concept categorization
- Only 4 out of 28 trust questionnaires in the reviewed studies were validated, suggesting potential gaps in the current state of XAI evaluation practices

## Confidence

- **High Confidence:** The core finding that current XAI evaluation lacks standardization and holistic assessment (75% of studies measure only single metrics) is well-supported by the systematic review methodology.
- **Medium Confidence:** The proposed taxonomy's practical utility depends on user classification accuracy and whether teams will adopt comprehensive evaluation despite increased costs.
- **Medium Confidence:** The distinction between Core System and Explanation metrics provides a useful framework, though the boundaries between these categories can be subjective in practice.

## Next Checks

1. **Classification Audit:** Review your current user study protocol and classify each metric as either "Core System" or "Explanation". If 0 in either category, the evaluation is incomplete per the paper's taxonomy.
2. **Validation Check:** Replace any ad-hoc "Trust" questions with the validated TXAI scale or TPA scale mentioned in Section 5.1.2 to improve construct validity.
3. **Behavioral Correlation:** Add a behavioral metric (e.g., time on task or decision accuracy) alongside subjective "Intention to Use" questionnaires to verify if stated intent matches actual capability, addressing the common failure mode of high subjective scores but low task performance.