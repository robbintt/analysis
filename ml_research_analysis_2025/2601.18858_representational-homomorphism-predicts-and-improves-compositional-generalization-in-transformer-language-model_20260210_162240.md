---
ver: rpa2
title: Representational Homomorphism Predicts and Improves Compositional Generalization
  In Transformer Language Model
arxiv_id: '2601.18858'
source_url: https://arxiv.org/abs/2601.18858
tags:
- compositional
- training
- modifier
- generalization
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces homomorphism error (HE), a metric that quantifies
  deviations from approximate homomorphisms between the expression algebra and a model''s
  hidden-state space. Two HE instantiations are studied: modifier HE for unary composition
  and sequence HE for binary composition.'
---

# Representational Homomorphism Predicts and Improves Compositional Generalization In Transformer Language Model

## Quick Facts
- arXiv ID: 2601.18858
- Source URL: https://arxiv.org/abs/2601.18858
- Reference count: 8
- HE-regularized training improves OOD accuracy with p=0.023 significance

## Executive Summary
This paper introduces homomorphism error (HE), a metric that quantifies deviations from approximate homomorphisms between the expression algebra and a model's hidden-state space. Two HE instantiations are studied: modifier HE for unary composition and sequence HE for binary composition. In controlled experiments with small decoder-only transformers on SCAN-style tasks, HE predicts out-of-distribution (OOD) compositional generalization under noise injection with R² = 0.73 correlation. The paper also tests HE-regularized training, finding that explicitly enforcing low modifier HE during training significantly reduces HE and improves OOD accuracy (p = 0.023).

## Method Summary
The method trains decoder-only transformers (4-layer, d=128, h=4, FFN=256) on SCAN-style compositional tasks and computes homomorphism error by learning auxiliary operators (linear, bilinear, MLP) to predict composed representations from component parts. HE-regularized training adds an auxiliary loss based on modifier HE at intermediate layers. OOD evaluation tests generalization to expressions with 5-12 primitives using 200 samples per length. The approach measures both modifier HE (unary composition) and sequence HE (binary composition) to diagnose representational structure.

## Key Results
- HE predicts OOD compositional generalization under noise injection with R² = 0.73 correlation
- HE-regularized training significantly reduces modifier HE (p = 1.1×10⁻⁴) and sequence HE (p = 0.001)
- OOD accuracy improves with HE-regularization (p = 0.023) beyond correlation
- Noise injection systematically increases modifier HE while sequence HE remains relatively stable

## Why This Works (Mechanism)

### Mechanism 1: Approximate Homomorphism as Representational Structure
- **Claim:** When hidden states preserve compositional operations approximately (low HE), models generalize compositionally; when representations become entangled (high HE), OOD performance degrades.
- **Mechanism:** HE quantifies whether Φ(e₁∘e₂) ≈ Φ(e₁) ⋆ Φ(e₂) by learning operators ⋆ that predict composed representations from components. Low HE indicates representations factor compositionally rather than memorizing wholes.
- **Core assumption:** Compositional generalization requires structure-preserving mappings between expression algebras and representation spaces.
- **Evidence anchors:** [abstract] "HE predicts out-of-distribution (OOD) compositional generalization under noise injection, achieving R² = 0.73 correlation between modifier HE and OOD accuracy"; [Section 5.4] Modifier HE shows monotonic increase (0.002 to 0.012 MSE) as noise increases, tracking OOD degradation (47% to 42%).

### Mechanism 2: HE-Regularized Training as Causal Intervention
- **Claim:** Explicitly enforcing low HE during training causally improves compositional generalization, beyond correlation.
- **Mechanism:** An auxiliary loss L = L_CE + λ·HE^mod_ℓ trains representations to satisfy Φ(m(e)) ≈ ⋆_m(Φ(e)). The paper regularizes intermediate layers (2 and 4), biasing the model toward structure-preserving representations.
- **Core assumption:** Enforcing local compositional consistency (unary modifier operations) propagates to improved global compositional behavior.
- **Evidence anchors:** [abstract] "Explicitly enforcing low modifier HE during training significantly reduces modifier HE (p = 1.1×10⁻⁴) and sequence HE (p = 0.001) and yields improvement in OOD accuracy (p = 0.023)"; [Section 5.4, Figure 7] Arrows connect baseline to HE-regularized models showing consistent leftward shift (lower HE) and upward shift (higher OOD accuracy).

### Mechanism 3: Noise Disrupts Unary More Than Binary Composition
- **Claim:** Spurious tokens in training systematically increase modifier HE while leaving sequence HE relatively stable, revealing differential sensitivity.
- **Mechanism:** Noise tokens break the mapping between modifier representations and their compositional transformations, forcing models toward memorization for unary operations. Binary connectors, which operate over longer spans, may average out noise effects.
- **Core assumption:** Unary modifiers require precise local transformations that noise disrupts; binary composition can leverage distributed representations across spans.
- **Evidence anchors:** [Section 5.4, Figure 4] "Noise injection systematically increases modifier HE and degrades OOD accuracy" while "sequence HE remains relatively stable"; [Section 5.4] "This dissociation suggests that spurious tokens primarily disrupt unary (modifier) composition while leaving binary (sequence) composition comparatively intact."

## Foundational Learning

- **Concept: Homomorphism**
  - **Why needed here:** HE formalizes compositionality as approximate structure preservation between expression algebras and representation spaces.
  - **Quick check question:** Given sets A, B with operations ∘ and ⋆, what property must f: A → B satisfy to be a homomorphism? (Answer: f(a₁ ∘ a₂) = f(a₁) ⋆ f(a₂))

- **Concept: Probing classifiers**
  - **Why needed here:** HE is computed by training auxiliary operators (linear, bilinear, MLP) to predict composed representations from components—this is a probing methodology.
  - **Quick check question:** Why average HE across multiple operator families rather than using a single architecture? (Answer: To avoid biasing analysis toward a particular functional form)

- **Concept: Decoder-only transformer**
  - **Why needed here:** The experiments use small causal language models; understanding representation extraction requires knowing how hidden states flow.
  - **Quick check question:** In a decoder-only transformer, which tokens' hidden states are used to compute HE for a modifier like "twice" applied to "jump"? (Answer: The representations of the primitive and modifier tokens, mean-pooled)

## Architecture Onboarding

- **Component map:** Base model -> HE probe operators (linear/bilinear/MLP) -> HE computation -> Auxiliary loss for regularization
- **Critical path:** 1. Extract representation triples (part1, part2, combined) from training data; 2. Train probe operators to minimize MSE between predicted and actual combined representations; 3. Report HE as the minimum MSE across operator families; 4. For regularization: pre-mine modifier pairs, compute auxiliary loss each step, add to L_CE
- **Design tradeoffs:** Operator family selection (linear interpretable but may underestimate structure; MLP expressive but may overfit); Regularization layers (intermediate layers 2,4 balance structure enforcement vs output flexibility); λ strength (too high constrains model; too low provides weak signal)
- **Failure signatures:** Low HE but low OOD accuracy (representations structured but misaligned with task semantics); High HE with high ID accuracy (model memorized training distribution); Regularization instability (if loss diverges, reduce λ or regularize fewer layers)
- **First 3 experiments:** 1. Baseline HE measurement: Train 4-layer transformer on SCAN-style data with 2 primitives, 0 noise. Compute layer-wise modifier and sequence HE. Verify HE stabilizes by layer 2-3. 2. Noise ablation: Add 5 random noise tokens to inputs. Measure HE increase and OOD accuracy drop. Confirm modifier HE rises while sequence HE stays stable. 3. HE-regularized training: Re-run noise experiment with λ=0.1 HE regularization on layers 2 and 4. Compare post-training HE and OOD accuracy to baseline; expect p<0.05 improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does homomorphism error analysis generalize to established compositional benchmarks using natural language (SCAN, COGS, CFQ)?
- **Basis in paper:** [explicit] The authors state: "A natural next step is applying homomorphism error analysis to established compositional benchmarks using natural language, including SCAN, COGS, and CFQ. This would validate the metric's utility beyond synthetic settings."
- **Why unresolved:** All experiments use synthetic SCAN-style datasets with controlled compositional structure; real language has ambiguity, context dependence, and irregular constructions.
- **What evidence would resolve it:** Demonstrating that HE predicts OOD generalization on SCAN, COGS, and CFQ benchmarks with comparable R² correlations.

### Open Question 2
- **Question:** How does homomorphism error scale with model size and pretraining data in large language models?
- **Basis in paper:** [explicit] The authors ask: "Investigating how homomorphism error scales with model size and pretraining data could provide insights into whether the compositional capabilities of large language models emerge from improved representational structure or alternative mechanisms."
- **Why unresolved:** Experiments only use small decoder-only transformers (1-10 layers, 128 hidden dim); large pretrained LMs remain untested.
- **What evidence would resolve it:** Measuring HE across model scales (e.g., 125M to 70B parameters) and correlating with compositional benchmark performance.

### Open Question 3
- **Question:** Can homomorphism error serve as a primary optimization objective rather than an auxiliary regularizer?
- **Basis in paper:** [explicit] The authors propose: "Future work could explore using homomorphism error as an optimization objective or regularization term, directly encouraging models to learn structured representations during training."
- **Why unresolved:** HE-regularized training adds HE as auxiliary loss (λ·HE), but HE as the primary objective is unexplored.
- **What evidence would resolve it:** Training models with HE as the main loss and evaluating whether this yields better compositional generalization than cross-entropy with HE regularization.

### Open Question 4
- **Question:** Does directly regularizing sequence HE provide additional improvements beyond modifier HE regularization?
- **Basis in paper:** [inferred] The paper regularizes only modifier HE (unary), finding it also reduces sequence HE (p=0.001), but does not test direct sequence HE regularization.
- **Why unresolved:** Unknown whether binary composition regularization offers complementary benefits or whether modifier HE suffices.
- **What evidence would resolve it:** Comparing OOD accuracy between models regularized on modifier HE only, sequence HE only, and both jointly.

## Limitations
- Experimental scope limited to small 4-layer decoder-only transformers on synthetic SCAN-style data
- OOD evaluation focuses only on primitive count variation, not exploring other generalization splits
- Method assumes compositional generalization requires approximately homomorphic representations, which may not capture all compositional mechanisms

## Confidence
- **High confidence:** The correlation between modifier HE and OOD accuracy (R²=0.73) is empirically well-supported by controlled noise-injection experiments
- **Medium confidence:** The causal claim that HE-regularized training improves compositional generalization (p=0.023) is supported but requires careful interpretation
- **Low confidence:** Claims about HE's diagnostic value for real-world compositional generalization extend beyond the experimental evidence

## Next Checks
1. **Scale and Architecture Generalization:** Reproduce the HE-regularized training on larger transformers (8-12 layers, d=256-512) and encoder-decoder models to test whether the effect scales and transfers across architectures.

2. **Probe Family Sensitivity Analysis:** Systematically vary the operator families used to compute HE (different MLP architectures, attention-based probes) to assess whether the correlation with OOD accuracy is robust to probe design choices.

3. **Alternative OOD Splits:** Test HE's predictive power on other OOD splits beyond primitive count (e.g., length-based splits, template-based splits) to validate that HE captures compositional structure rather than just memorization capacity.