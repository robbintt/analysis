---
ver: rpa2
title: Efficient Model Selection for Time Series Forecasting via LLMs
arxiv_id: '2504.02119'
source_url: https://arxiv.org/abs/2504.02119
tags:
- forecasting
- time
- selection
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to model selection in time
  series forecasting by leveraging Large Language Models (LLMs) instead of traditional
  meta-learning methods. The proposed method eliminates the need for an explicit performance
  matrix, which is computationally expensive to construct, by utilizing the reasoning
  capabilities of LLMs through zero-shot prompting.
---

# Efficient Model Selection for Time Series Forecasting via LLMs

## Quick Facts
- arXiv ID: 2504.02119
- Source URL: https://arxiv.org/abs/2504.02119
- Reference count: 16
- Key outcome: LLM-based approach achieves up to 89× faster inference time while maintaining or improving forecasting accuracy

## Executive Summary
This paper introduces a novel approach to model selection in time series forecasting that leverages Large Language Models (LLMs) instead of traditional meta-learning methods. The proposed method eliminates the computationally expensive explicit performance matrix by utilizing LLM reasoning capabilities through zero-shot prompting. The approach demonstrates significant speed improvements (89× faster) while achieving superior or competitive forecasting performance compared to random selection, popular methods, and meta-learning baselines across 320 datasets.

## Method Summary
The method uses zero-shot prompting with LLMs (Llama 3.2, GPT-4o, and Gemini 2.0) to select appropriate forecasting models without constructing an explicit performance matrix. By providing dataset characteristics and task descriptions as prompts, the LLM reasons about which model would perform best. The approach can optionally incorporate meta-features for improved selection accuracy, though this increases computational overhead. The system evaluates multiple forecasting models and uses LLM reasoning to predict the optimal choice, bypassing the traditional meta-learning pipeline that requires extensive performance data collection.

## Key Results
- 89× faster inference time compared to exhaustive model evaluation
- Superior hit@k accuracy compared to random selection, popular methods, and meta-learning baselines
- Zero-shot prompting approach maintains competitive or superior forecasting accuracy across 320 datasets

## Why This Works (Mechanism)
The approach leverages LLMs' natural language reasoning capabilities to perform model selection without requiring explicit performance matrices. LLMs can understand dataset characteristics and model descriptions, then reason about which model would be most appropriate for a given forecasting task. This eliminates the need for computationally expensive meta-learning pipelines that require collecting performance data across many datasets.

## Foundational Learning

1. **Zero-shot prompting**
   - Why needed: Enables model selection without training data or fine-tuning
   - Quick check: Verify LLM can correctly identify model characteristics from text descriptions

2. **Meta-learning vs. LLM reasoning**
   - Why needed: Understanding the fundamental difference between traditional meta-learning and LLM-based approaches
   - Quick check: Compare computational complexity and data requirements between approaches

3. **Time series forecasting models**
   - Why needed: LLM needs to understand different model capabilities to make informed selections
   - Quick check: Ensure LLM can distinguish between model types (ARIMA, LSTM, Transformer, etc.)

4. **Hit@k accuracy metric**
   - Why needed: Primary evaluation metric for model selection effectiveness
   - Quick check: Verify calculation methodology for hit@k across different k values

5. **Dataset characteristics**
   - Why needed: Features used in prompts to inform LLM reasoning
   - Quick check: Validate that dataset features are sufficient for model selection decisions

## Architecture Onboarding

**Component Map:** Dataset features -> LLM prompt -> Model selection prediction -> Forecast evaluation

**Critical Path:** Dataset → Prompt Construction → LLM Inference → Model Selection → Forecast Generation

**Design Tradeoffs:** 
- Speed vs. accuracy: Zero-shot vs. fine-tuned approaches
- Cost vs. performance: Meta-features inclusion increases overhead but improves accuracy
- LLM choice: Different models (Llama, GPT-4o, Gemini) offer varying performance/cost tradeoffs

**Failure Signatures:** 
- Poor selection when dataset characteristics are ambiguous
- Degradation when using Chain-of-Thought prompting
- Performance drops with inadequate dataset feature representation

**First Experiments:**
1. Test LLM's ability to distinguish between simple dataset characteristics and recommend appropriate models
2. Evaluate hit@k performance on a small subset of datasets to validate approach
3. Compare inference time against traditional meta-learning approach on a single dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM reasoning capabilities may vary across different model versions and domains
- Zero-shot prompting may not capture complex temporal dependencies as effectively as domain-specific approaches
- Performance could degrade with different or newer LLM versions not evaluated in the study

## Confidence

**High Confidence:** The 89× speedup claim and the basic premise that LLMs can perform model selection without explicit performance matrices

**Medium Confidence:** The superiority over meta-learning baselines and the effectiveness of the zero-shot approach across diverse datasets

**Medium Confidence:** The findings regarding meta-features and Chain-of-Thought prompting, as these could be sensitive to implementation details and dataset characteristics

## Next Checks
1. **Cross-Domain Generalization Test:** Validate the approach on non-typical time series datasets (e.g., high-frequency trading data, irregularly sampled data) to assess robustness beyond the 320 datasets used

2. **LLM Version Sensitivity Analysis:** Test whether performance degrades or improves significantly with newer/older versions of the evaluated LLMs or alternative LLMs

3. **Cost-Benefit Analysis Under Real-World Constraints:** Evaluate the approach when model selection must be performed under strict computational budgets or real-time constraints, measuring both selection accuracy and actual resource usage