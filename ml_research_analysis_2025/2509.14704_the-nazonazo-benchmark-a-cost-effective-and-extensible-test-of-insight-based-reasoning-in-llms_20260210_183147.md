---
ver: rpa2
title: 'The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based
  Reasoning in LLMs'
arxiv_id: '2509.14704'
source_url: https://arxiv.org/abs/2509.14704
tags:
- reasoning
- japanese
- answer
- such
- nazonazo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the NazoNazo benchmark, a low-cost and easily
  extensible test of insight-based reasoning in LLMs using Japanese children's riddles.
  Evaluating 38 frontier models and 126 adults on 120 riddles, the study finds that
  no model except GPT-5 matches human performance (52.9% mean accuracy), with non-reasoning
  models averaging 7.6% and reasoning models 17.6% accuracy.
---

# The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs

## Quick Facts
- arXiv ID: 2509.14704
- Source URL: https://arxiv.org/abs/2509.14704
- Reference count: 5
- Primary result: 38 frontier models scored 7.6% (non-reasoning) and 17.6% (reasoning) accuracy on Japanese insight riddles, far below human baseline of 52.9%

## Executive Summary
This paper introduces the NazoNazo benchmark, a low-cost and easily extensible test of insight-based reasoning in LLMs using Japanese children's riddles. Evaluating 38 frontier models and 126 adults on 120 riddles, the study finds that no model except GPT-5 matches human performance (52.9% mean accuracy), with non-reasoning models averaging 7.6% and reasoning models 17.6% accuracy. Analysis of thought logs reveals that models often generate correct solutions among intermediate candidates but fail to select them as final answersâ€”a verification failure suggesting weak metacognitive control. The benchmark offers a renewable, cross-linguistic test of AI insight reasoning and highlights the need for improved metacognitive calibration in future models.

## Method Summary
The NazoNazo benchmark uses 120 Japanese nazonazo riddles requiring insight-based reasoning through constraint relaxation and chunk decomposition. Models are evaluated using zero-shot prompting at temperature=0.0, with thought logs captured to analyze reasoning processes. Gold answer variants are established through pilot testing. The benchmark is designed to be low-cost, extensible, and cross-linguistic, leveraging Japanese linguistic features unlikely to be prioritized in English-centric training.

## Key Results
- Human baseline accuracy: 52.9% (n=126)
- Non-reasoning models: 7.6% accuracy
- Reasoning models: 17.6% accuracy
- GPT-5 is the only model approaching human performance
- Models frequently generate correct intermediate candidates but fail to select them as final answers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The benchmark detects reasoning gaps by requiring representational change rather than knowledge retrieval.
- **Mechanism:** Items are designed to induce an "impasse" where standard linguistic associations fail. Solving requires constraint relaxation (loosening initial assumptions) and chunk decomposition (breaking perceptual units like kanji or words into sub-components), forcing a non-linear structural shift.
- **Core assumption:** LLMs primarily operate on linear associative retrieval; insight requires a distinct cognitive restructuring process.
- **Evidence anchors:** [Page 3] Defines insight via "representational change account" (Ohlsson, 1992), citing "constraint relaxation" and "chunk decomposition." [Page 4] Explains that nazonazo probes "representational shift from a fixed initial parse." [Corpus] *BanglaRiddle* and *Indian Riddles* papers support the efficacy of culturally grounded riddles for testing reasoning beyond simple retrieval.
- **Break condition:** If models solve items via simple keyword association or memorized patterns without restructuring, the diagnostic validity for "insight" fails.

### Mechanism 2
- **Claim:** The benchmark isolates a "verification failure" bottleneck distinct from generation inability.
- **Mechanism:** By analyzing thought logs, the test distinguishes between "cannot generate" and "cannot endorse." Models often traverse the correct solution state in their reasoning chain but lack the metacognitive signal (Feeling of Rightness/FoR) to terminate the search and output the answer.
- **Core assumption:** Thought logs are sufficiently faithful proxies for internal processing states to diagnose selection vs. generation errors.
- **Evidence anchors:** [Abstract] Notes models "often generate correct solutions among intermediate candidates but fail to select them." [Page 16] Defines "Verification Failure" where models explicitly consider the right answer (e.g., "Tabasco") but discard it. [Corpus] Corpus neighbors do not specifically replicate this "verification failure" mechanism, limiting cross-validation.
- **Break condition:** If models never generate the correct candidate in the thought log, the mechanism defaults to a standard knowledge/retrieval failure.

### Mechanism 3
- **Claim:** Cross-linguistic design prevents contamination and isolates reasoning from English-centric training biases.
- **Mechanism:** Using Japanese riddles leverages unique linguistic features (kanji radicals, homophony) unlikely to be prioritized in English-centric pre-training. This lowers contamination risk. Furthermore, models thinking in English often outperform those thinking in Japanese, suggesting reasoning mechanisms are partially decoupled from the specific language of the prompt.
- **Core assumption:** The reasoning core of frontier models is language-agnostic enough to operate on Japanese inputs even when processing English thought traces.
- **Evidence anchors:** [Page 2] Claims "low contamination risk" due to language specificity. [Page 12] Reports "thinking in Japanese did not necessarily raise accuracy," supporting decoupling. [Page 18] Argues primary failure is control processes, not tokenization.
- **Break condition:** If performance is purely capped by Japanese tokenization limitations rather than reasoning, the cross-linguistic mechanism is invalid.

## Foundational Learning

- **Concept: Insight Problem Solving (Representational Change)**
  - **Why needed here:** To distinguish why standard CoT (linear step-by-step) often fails on this benchmark. Success requires changing the problem representation, not just adding steps.
  - **Quick check question:** Does the model fail because it lacks a fact, or because it is "stuck" in the wrong interpretation?

- **Concept: Metacognitive Feelings (FoR/FoE)**
  - **Why needed here:** To diagnose "verification failure." One must understand that a model needs a functional "Feeling of Rightness" to stop searching, distinct from just finding a logical path.
  - **Quick check question:** If a model generates "Aha!" in the log but the answer is wrong, which metacognitive signal is malfunctioning?

- **Concept: Candidate Selection vs. Generation**
  - **Why needed here:** To properly evaluate the thought logs. Evaluation must check if the correct answer appears anywhere in the reasoning trace, not just in the final output.
  - **Quick check question:** A model lists 5 candidates, including the correct one, but outputs a 6th. Is this a reasoning error or a selection error?

## Architecture Onboarding

- **Component map:** Input (Japanese riddle) -> Processing (Thought Log + Final Answer) -> Evaluator (Exact match + Verification failure detection)
- **Critical path:**
  1. Prompt engineering: Zero-shot instruction to output reasoning followed by a specific format answer
  2. Log Capture: Ensure the API returns the hidden/internal thought log tokens, not just the final completion
  3. Candidate Scan: Regex/semantic search the entire log for the gold answer to differentiate "Generation" vs. "Selection" accuracy

- **Design tradeoffs:**
  - Zero-shot vs. Few-shot: Paper uses zero-shot to measure "out-of-the-box" insight, trading off potential accuracy gains from examples for purity of reasoning measurement
  - Language of Thought: Allowing the model to choose its "thinking language" may improve reasoning but complicates error analysis (translation errors vs. logic errors)

- **Failure signatures:**
  - False Aha: Log contains high-confidence markers ("That's it!") followed by an incorrect answer
  - Infinite Regress: The model cycles through candidates without triggering a "stop" signal because no candidate exceeds the FoR threshold
  - Hallucinated Verification: The log simulates checking an external source or "giving up" (Page 19)

- **First 3 experiments:**
  1. Quantify Verification Gap: Run the set on a reasoning model (e.g., o3-mini or Grok-3). Calculate the gap between % of items where the answer appears in the log vs. % of correct final outputs
  2. Language Ablation: Force specific models to "Think in English" vs. "Think in Japanese" explicitly to verify the language-independence claim
  3. Forced Selection: Modify the prompt to require the model to explicitly score its top 3 candidates for "Feeling of Rightness" before selecting one, to test if explicit metacognitive scaffolding reduces verification failure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does constraining the internal chain-of-thought language to Japanese reduce verification failure rates compared to reasoning in English?
- **Basis in paper:** [explicit] The authors state, "In new prompts we will specify the language from the outset, comparing the performance of English-only thinking with Japanese-only thinking."
- **Why unresolved:** Current models switch languages unpredictably, and it is unclear if verification failures are artifacts of cross-lingual reasoning or deeper cognitive deficits.
- **What evidence would resolve it:** A controlled experiment comparing accuracy and verification failure rates when models are forced to generate thought logs in a specific language.

### Open Question 2
- **Question:** Can specialized metacognitive prompting methods significantly improve accuracy on insight-based riddles compared to standard zero-shot baselines?
- **Basis in paper:** [explicit] The authors note they "did not employ a specialized 'metacognitive prompting method'" and left "a systematic comparison with such prompting methodologies... for future work."
- **Why unresolved:** It is unknown if the poor performance is due to a lack of intrinsic capability or the lack of specific prompting strategies that induce lateral thinking.
- **What evidence would resolve it:** Evaluating models using specific prompting techniques (e.g., Bai et al., 2025) and measuring the performance delta against the zero-shot baseline.

### Open Question 3
- **Question:** Can models be trained to strengthen metacognitive "feelings" (e.g., Feeling of Rightness) to better recognize and endorse correct intermediate candidates?
- **Basis in paper:** [explicit] The authors propose they "will explore training methods that strengthen such 'feelings'" to address the identified bottleneck of weak metacognitive control.
- **Why unresolved:** Current models often generate the correct candidate but fail to select it; it is unclear if this "verification failure" can be fixed via targeted training on self-evaluation signals.
- **What evidence would resolve it:** Fine-tuning models on datasets designed to calibrate confidence/feelings of rightness, then observing if the selection of correct candidates improves.

## Limitations

- The benchmark's diagnostic power rests on the assumption that thought logs faithfully reflect internal reasoning states, yet this cannot be independently verified without access to model internals
- The verification-failure mechanism, while theoretically compelling, lacks external validation beyond the single corpus example provided
- The language-decoupling claim requires more systematic testing across diverse model families to rule out subtle translation artifacts

## Confidence

- **High Confidence:** The basic benchmark construction (120 riddles, human baseline of 52.9%, zero-shot evaluation protocol) is methodologically sound and reproducible
- **Medium Confidence:** The identification of verification failure as a distinct failure mode is well-supported by the thought log analysis, though the underlying metacognitive mechanism remains theoretical
- **Low Confidence:** The claim that cross-linguistic design inherently prevents contamination requires more rigorous testing, as English-centric training could still influence reasoning patterns through multilingual embeddings

## Next Checks

1. **Verification Gap Quantification:** Systematically measure the gap between candidates appearing in thought logs versus final answers across 5+ diverse reasoning models to establish verification failure as a consistent pattern rather than anecdotal observation

2. **Language Force-Failure:** Design an experiment where models are explicitly forced to "think in Japanese" versus "think in English" with identical prompts, measuring accuracy differences to test the language-decoupling hypothesis

3. **Control Process Isolation:** Implement an enhanced prompt requiring explicit candidate scoring ("Rate each candidate 1-10 for confidence") to determine whether metacognitive scaffolding reduces verification failures, thereby validating the metacognitive control mechanism