---
ver: rpa2
title: 'Idea2Plan: Exploring AI-Powered Research Planning'
arxiv_id: '2510.24891'
source_url: https://arxiv.org/abs/2510.24891
tags:
- research
- plan
- rubric
- does
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how large language models can transform
  research ideas into well-structured research plans, a critical capability for scientific
  discovery and autonomous research agents. The authors introduce the Idea2Plan task
  and benchmark, constructed from 200 post-LLM-cutoff ICML 2025 papers, featuring
  paired research ideas and reference plans with automatically generated grading rubrics.
---

# Idea2Plan: Exploring AI-Powered Research Planning

## Quick Facts
- arXiv ID: 2510.24891
- Source URL: https://arxiv.org/abs/2510.24891
- Authors: Jin Huang; Silviu Cucerzan; Sujay Kumar Jauhar; Ryen W. White
- Reference count: 40
- Primary result: GPT-5 and GPT-5-mini outperform other models on Idea2Plan benchmark, but substantial improvement headroom remains

## Executive Summary
This paper introduces Idea2Plan, a benchmark for evaluating AI systems' ability to transform research ideas into structured research plans. Built from 200 ICML 2025 papers published after major LLM training cutoffs, the benchmark features paired research ideas and reference plans with automatically generated grading rubrics. Experiments with various prompting strategies and frontier LLMs reveal that GPT-5 consistently outperforms other models, though simpler prompting approaches (Naïve, 0-shot, 1-shot) surprisingly beat more complex ReAct agentic methods. The authors also develop LLM-based judges validated against human expert annotations, achieving 0.91 F1 score.

## Method Summary
The Idea2Plan benchmark extracts research ideas from paper abstracts and reference plans from full papers, then generates yes/no grading rubrics targeting conceptual requirements rather than specific implementations. The framework evaluates generated plans using LLM-as-judge (o4-mini with reasoning=high) against these rubrics. Four baselines are tested: Naïve (minimal prompt), 0-shot (detailed instructions), 1-shot (with example), and ReAct agent (with search/read tools). The evaluation uses macro-averaged binary accuracy across rubric questions, with plans required to follow a 5-section template covering Introduction, Key Literature, Methods, Initial Experimental Design, and Resources/Compliance/Ethics.

## Key Results
- GPT-5 and GPT-5-mini consistently outperform other models on the benchmark
- Simpler prompting strategies (Naïve, 0-shot, 1-shot) outperform ReAct agentic approaches
- LLM-based judges validated against human experts achieve 0.91 F1 score
- Substantial improvement headroom remains despite GPT-5's strong performance
- SFT on idea-plan pairs actually decreased performance and increased hallucination

## Why This Works (Mechanism)

### Mechanism 1: Rubric-Based Evaluation for Multi-Valid Plan Assessment
- Claim: Multiple valid plans can exist for the same research idea; rubrics capture essential elements that any reasonable plan should address.
- Mechanism: Extract reference plans from published papers, then generate yes/no rubric questions targeting conceptual requirements rather than specific implementations. Grade plans on whether they satisfy these criteria.
- Core assumption: The key design decisions in published papers represent necessary components for any valid approach to that idea.
- Evidence anchors:
  - [abstract] "Each benchmark instance includes a research idea and a grading rubric capturing the key components of valid plans."
  - [section 3.2] "This approach allows us to evaluate whether generated plans cover essential conceptual elements while accommodating legitimate variations."
  - [corpus] PLANET benchmark (2504.14773) similarly evaluates planning capabilities but focuses on general planning rather than scientific research planning specifically.
- Break condition: If rubrics over-specify particular methodological choices as "required," they would penalize valid alternative approaches.

### Mechanism 2: LLM-as-Judge with Expert-Calibrated Validation
- Claim: LLM-based judges can provide expert-aligned grading at scale when validated against human annotations.
- Mechanism: Use o4-mini (reasoning=high) to grade plans against rubrics; validate on JudgeEval dataset (10 plans with expert ground truth) achieving 0.91 F1.
- Core assumption: Judge performance on the development set generalizes to the full benchmark.
- Evidence anchors:
  - [abstract] "We develop LLM-based judges to evaluate generated plans against these rubrics and assess judge reliability through human expert annotations."
  - [section 3.6] "o4-mini achieves the highest F1 score (0.91), while GPT-4.1-mini offers the most cost-effective option."
  - [corpus] LLM4SR survey (2501.04306) documents LLMs' expanding role across research stages, providing context for judge reliability expectations.
- Break condition: If judges exhibit systematic biases not captured in the small validation set (5 papers, 10 plans), scores may not reflect true plan quality.

### Mechanism 3: Post-Cutoff Data Contamination Prevention
- Claim: Using papers published after LLM training cutoffs prevents memorization from masquerading as genuine planning ability.
- Mechanism: Select ICML 2025 Spotlight/Oral papers with no pre-cutoff arXiv versions; blocklist target papers and their citations from agent retrieval tools.
- Core assumption: Papers published after cutoffs were not in training data in any form.
- Evidence anchors:
  - [abstract] "benchmark built from 200 ICML 2025 Spotlight and Oral papers released after major LLM training cutoffs"
  - [section 3.3] "We filter out any papers with arXiv submissions predating the most recent data cutoff of the LLMs we test (GPT-5, October 2024)."
  - [corpus] No directly comparable contamination-prevention mechanisms found in neighbor papers; most rely on held-out test sets without temporal filtering.
- Break condition: If preprints, blog posts, or related content about these papers appeared before cutoffs, contamination may still occur.

## Foundational Learning

- Concept: **Research Plan Structure (AI context)**
  - Why needed here: The framework evaluates plans against a 5-section template: Introduction, Key Literature, Methods, Initial Experimental Design, and Resources/Compliance/Ethics.
  - Quick check question: Can you list the five required sections and explain what each should contain?

- Concept: **LLM-as-Judge Evaluation**
  - Why needed here: Grading uses an LLM to assess plans against rubrics; understanding judge limitations is critical for interpreting results.
  - Quick check question: What are two failure modes of LLM judges when evaluating structured outputs?

- Concept: **ReAct Agent Architecture**
  - Why needed here: The paper tests ReAct-style agents with search/read tools; understanding why they underperform informs agent design.
  - Quick check question: In a ReAct loop, what happens when retrieved information conflicts with parametric knowledge?

## Architecture Onboarding

- Component map:
  - Paper abstract -> o4-mini -> first-person research idea
  - Full paper -> o4-mini -> structured research plan
  - Reference plan + idea -> o4-mini -> JSON rubric
  - Research idea + (optional tools) -> test LLM -> structured research plan
  - Plan + rubric section -> o4-mini (reasoning=high) -> yes/no judgments
  - Binary accuracy per section -> macro-average across sections -> macro-average across papers

- Critical path:
  1. Data prep: Extract 200 ideas, plans, and rubrics from ICML 2025 papers (filter by cutoff)
  2. Human validation: Expert rating of extraction quality (target: >4.0/5.0 on Likert scale)
  3. Judge validation: Expert-annotate 10 plans; confirm judge F1 > 0.85
  4. Generation experiments: Run all baselines (Naïve, 0-shot, 1-shot, ReAct) with 3 trials each
  5. Evaluation: Judge all generated plans; compute Planning Scores

- Design tradeoffs:
  - **Naïve vs. ReAct**: Simpler prompting outperforms agentic retrieval; retrieval can introduce noise when irrelevant papers are retrieved (knowledge conflict)
  - **Cost vs. accuracy**: o4-mini (0.91 F1, $0.16/plan) vs. GPT-4.1-mini (0.89 F1, $0.005/plan)
  - **Section-wise vs. whole-plan grading**: Section-wise grading avoids context length issues but requires 5 API calls per plan

- Failure signatures:
  - **ReAct retrieval noise**: Agent cites tangentially related papers instead of foundational work; see Table 22 (case study on parametric vs. retrieved knowledge conflict)
  - **SFT-induced hallucination**: Fine-tuning on idea-plan pairs increased hallucination in literature sections; SFT models underperformed base models by 9-12 percentage points
  - **Over-specified rubrics**: Requiring exact paper titles or specific datasets penalizes valid alternative approaches

- First 3 experiments:
  1. **Baseline replication**: Run Naïve and 1-shot prompting on 20 held-out ideas; verify GPT-5 > GPT-4.1 > open-source models ordering holds
  2. **Ablate curated vs. retrieved literature**: Compare ReAct with blocklisted search vs. providing 3 hand-curated papers; measure delta in Literature section scores
  3. **Judge stress test**: Create 5 adversarial plans (verbose but vacuous, correct structure but wrong content, etc.); verify judge assigns low scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do ReAct agents with external search tools fail to outperform simpler prompting baselines for research planning?
- Basis in paper: [explicit] The authors state, "Surprisingly, agentic approaches using ReAct scaffolding do not surpass simpler prompting methods," and suggest models struggle to filter retrieved information.
- Why unresolved: While the paper identifies knowledge conflicts as a potential cause, it does not determine if the issue lies in retrieval noise, the agent's integration logic, or the specific planning task structure.
- What evidence would resolve it: Ablation studies comparing different retrieval filtering mechanisms or an agent architecture specifically designed to resolve conflicts between parametric and retrieved knowledge.

### Open Question 2
- Question: Can supervised fine-tuning (SFT) strategies be modified to improve research planning without inducing hallucination?
- Basis in paper: [explicit] The authors note in Appendix J that "SFT models exhibit a decrease in overall performance" and "tend to hallucinate more," suggesting standard training on idea-plan pairs is insufficient.
- Why unresolved: It is unclear if the failure is due to the data quality, the specific factual density of research plans, or the SFT training objective itself.
- What evidence would resolve it: Experiments using reinforcement learning (e.g., PPO) with rubric-based feedback or training on augmented datasets that include synthetic ethical and resource details.

### Open Question 3
- Question: Do LLM research planning capabilities generalize to scientific domains outside of AI?
- Basis in paper: [explicit] The limitations section states the focus on ICML 2025 papers "may limit generalizability to other scientific domains or earlier-stage research ideas."
- Why unresolved: The current benchmark is confined to Machine Learning, leaving the performance of models on domains with different experimental norms (e.g., wet lab biology) unknown.
- What evidence would resolve it: Evaluating frontier models on a new benchmark constructed from papers in distinct fields like physics or medical science.

## Limitations

- **GPT-5 model availability**: The paper's evaluation relies on GPT-5 and GPT-5-mini, which may not be publicly accessible at the time of review, creating a fundamental barrier to reproducing claimed performance differences.
- **Rubric over-specification risk**: While designed to accommodate multiple valid plans, the rubric generation process could inadvertently encode implicit assumptions from the reference paper, potentially penalizing legitimate alternative approaches.
- **Judge generalizability**: The o4-mini judge shows 0.91 F1 on 10 human-annotated plans, but this validation set represents only 5 papers. Systematic judge biases may emerge with larger or more diverse test cases.

## Confidence

- **High Confidence**: The benchmark construction methodology (post-cutoff papers, rubric-based evaluation) is well-specified and technically sound. The finding that simpler prompting strategies outperform agentic approaches is supported by clear experimental evidence.
- **Medium Confidence**: Claims about GPT-5 outperforming other models depend on model availability and exact implementation details. The rubric's ability to accommodate multiple valid plans requires further validation with domain experts.
- **Low Confidence**: Extrapolation of current results to fully autonomous research agents requires significantly more capability than demonstrated; the paper itself notes substantial improvement headroom remains.

## Next Checks

1. **Judge validation expansion**: Annotate an additional 20 plans with human experts and compute judge F1 scores by section to identify potential systematic biases in specific rubric areas.
2. **Rubric flexibility stress test**: Have domain experts evaluate 10 plans that deliberately use different methodologies than the reference paper to verify rubrics don't over-specify particular approaches.
3. **Alternative model replication**: Re-run key experiments with GPT-4.1 and Claude 3.5 Sonnet (publicly available models) to verify that the relative performance ordering holds without GPT-5 access.