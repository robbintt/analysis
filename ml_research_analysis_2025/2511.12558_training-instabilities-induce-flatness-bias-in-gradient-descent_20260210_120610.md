---
ver: rpa2
title: Training Instabilities Induce Flatness Bias in Gradient Descent
arxiv_id: '2511.12558'
source_url: https://arxiv.org/abs/2511.12558
tags:
- learning
- curvature
- which
- training
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical and empirical analysis of training
  instabilities in deep learning. The authors show that instabilities, rather than
  being harmful, actually induce an implicit flatness bias in gradient descent that
  improves generalization.
---

# Training Instabilities Induce Flatness Bias in Gradient Descent

## Quick Facts
- arXiv ID: 2511.12558
- Source URL: https://arxiv.org/abs/2511.12558
- Authors: Lawrence Wang; Stephen J. Roberts
- Reference count: 40
- Primary result: Training instabilities induce implicit flatness bias in gradient descent that improves generalization

## Executive Summary
This paper presents a theoretical and empirical analysis of training instabilities in deep learning optimization. The authors demonstrate that instabilities, rather than being harmful, actually induce an implicit flatness bias in gradient descent that improves generalization. They introduce a new mechanism called Rotational Polarity of Eigenvectors (RPE) which explains how eigenvector rotations during instabilities promote exploration and lead to flatter minima. The key theoretical result is a dynamical system that captures how instability-driven fluctuations in higher-order curvature moments systematically reduce curvature over time, formalizing the flatness bias.

## Method Summary
The authors develop a theoretical framework using dynamical systems analysis to model how training instabilities affect the optimization landscape. They introduce the concept of Rotational Polarity of Eigenvectors (RPE) as a mechanism explaining how eigenvector rotations during instabilities promote exploration toward flatter minima. The theoretical model tracks fluctuations in higher-order curvature moments that systematically reduce curvature over time. Empirically, they validate their findings through experiments on CIFAR-10 and Fashion-MNIST using fully connected networks, comparing training with large learning rates that induce instability against stable training regimes. They also propose a new optimizer called Clipped-Ada that leverages instabilities in adaptive methods.

## Key Results
- Instabilities during training induce an implicit flatness bias that improves generalization
- Large learning rates that cause instability lead to better generalization than stable training
- The flatness benefit persists under stochastic gradient descent conditions
- RPE mechanism explains how eigenvector rotations during instabilities promote exploration toward flatter minima

## Why This Works (Mechanism)
Training instabilities create fluctuations in the optimization trajectory that, counterintuitively, drive the model toward flatter minima rather than sharp ones. The Rotational Polarity of Eigenvectors (RPE) mechanism explains this through eigenvector rotations during unstable phases, which cause the optimizer to explore the loss landscape more broadly. These fluctuations systematically reduce higher-order curvature moments over time, creating a dynamical system where instability acts as a beneficial exploration mechanism that biases the optimization toward regions of lower curvature and better generalization properties.

## Foundational Learning
- Dynamical systems analysis: Why needed - To model how instabilities affect optimization trajectories over time; Quick check - Verify that the system captures both stable and unstable regimes
- Hessian curvature analysis: Why needed - To quantify flatness of minima and connect to generalization; Quick check - Confirm that curvature measures correlate with generalization performance
- Eigenvector rotation dynamics: Why needed - To explain the RPE mechanism and its role in exploration; Quick check - Validate that RPE changes direction during instability periods

## Architecture Onboarding
Component map: Data -> Loss function -> Gradient computation -> Optimization update -> Parameter space exploration
Critical path: Training data flows through forward pass, loss computation, backward pass for gradients, then update step where instabilities may occur
Design tradeoffs: Stable training (smaller learning rates) vs. unstable training (larger learning rates that induce flatness bias
Failure signatures: Excessive instability leading to divergence, or insufficient instability failing to reach flat minima
First experiments:
1. Compare training with small vs. large learning rates on simple convex problems
2. Measure Hessian spectrum evolution during stable vs. unstable training
3. Test RPE mechanism on synthetic loss landscapes with known curvature properties

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on simplified settings and specific assumptions about loss landscape structure
- Extension to highly non-convex deep neural network landscapes requires further validation
- RPE mechanism needs more rigorous mathematical formalization
- Claim of universal generalization improvement may not hold across all learning scenarios

## Confidence
- Theoretical framework linking instabilities to flatness bias: Medium
- RPE mechanism as the primary driver: Low-Medium
- Empirical validation on benchmark datasets: Medium-High
- Proposed Clipped-Ada optimizer effectiveness: Low-Medium

## Next Checks
1. Test instability-induced flatness bias on larger-scale architectures (ResNets, Transformers) and more diverse datasets
2. Conduct ablation studies isolating RPE contribution from other flatness-inducing mechanisms
3. Evaluate generalization performance of instability-driven training across different task types