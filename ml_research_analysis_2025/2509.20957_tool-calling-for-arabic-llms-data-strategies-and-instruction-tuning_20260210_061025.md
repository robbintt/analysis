---
ver: rpa2
title: 'Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning'
arxiv_id: '2509.20957'
source_url: https://arxiv.org/abs/2509.20957
tags:
- tool
- arabic
- laminitial
- alefisolated
- aleffinal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether Arabic Large Language Models (LLMs)
  benefit from in-language tool-calling training data versus relying on cross-lingual
  transfer, and the value of general-purpose instruction tuning and tool-specific
  fine-tuning. Experiments with Fanar Arabic LLM across five configurations show that
  bilingual tool-calling data and tool-specific fine-tuning significantly improve
  performance, especially for previously unseen tools.
---

# Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning

## Quick Facts
- **arXiv ID:** 2509.20957
- **Source URL:** https://arxiv.org/abs/2509.20957
- **Reference count:** 7
- **Primary result:** Bilingual (Arabic/English) tool-calling data and tool-specific fine-tuning significantly improve Arabic LLM tool-calling performance over cross-lingual transfer alone.

## Executive Summary
This study investigates whether Arabic Large Language Models (LLMs) benefit from in-language tool-calling training data versus relying on cross-lingual transfer, and the value of general-purpose instruction tuning and tool-specific fine-tuning. Experiments with Fanar Arabic LLM across five configurations show that bilingual tool-calling data and tool-specific fine-tuning significantly improve performance, especially for previously unseen tools. For instance, instruction-tuned models with bilingual data and custom tool examples achieved near-perfect recall and argument population accuracy (ArgA > 0.99) for Arabic and English tasks. Cross-lingual transfer alone yielded lower ArgA (e.g., 0.58 for CustomTools Arabic), while fine-tuning on general-purpose data improved function calling but reduced precision for non-function-calling cases. Tool-specific fine-tuning is most effective for high-priority tools.

## Method Summary
The study fine-tuned Fanar-1-9B and Fanar-1-9B-Instruct using Supervised Fine-Tuning (SFT) via LLaMA-Factory. Training data included translated versions of Glaive and xLAM datasets (using Gemini-2.5-Flash), custom synthesized tools (CustomTools), and real-world logs (IslamicRAGTool). Models were evaluated on weighted precision/recall for tool selection and Argument Population Accuracy (ArgA) for exact match of function names and arguments. Training used a cosine learning rate schedule (peak $5.0 \times 10^{-7}$, min $5.0 \times 10^{-8}$) with batch size 640. Five model configurations were tested: base, base with general SFT, base with tool SFT, base with bilingual tool SFT, and instruction-tuned with bilingual tool SFT plus tool-specific fine-tuning.

## Key Results
- Bilingual (Arabic/English) tool-calling data significantly outperformed English-only cross-lingual transfer for Arabic tasks (ArgA > 0.99 vs. 0.58 for CustomTools Arabic).
- General-purpose instruction tuning improved tool-calling recall but reduced precision on non-function-calling queries (non-FC recall dropped from 0.97 to 0.74).
- Tool-specific fine-tuning on custom tools dramatically improved performance for those tools, with near-perfect ArgA (0.99+) for high-priority, fine-tuned tools.
- The best-performing configuration combined instruction tuning, bilingual data, and tool-specific fine-tuning, achieving ArgA > 0.99 for both Arabic and English tasks.

## Why This Works (Mechanism)
The study demonstrates that Arabic LLMs require in-language tool-calling data to achieve high Argument Population Accuracy (ArgA), as cross-lingual transfer from English yields significant performance gaps, particularly for argument extraction. General-purpose instruction tuning biases models toward tool-calling behavior, improving recall but harming precision on non-tool queries. Bilingual data mitigates this by exposing models to both Arabic and English tool-calling patterns, while tool-specific fine-tuning provides the granular examples needed for high-performance on custom tools. The iterative tool-router architecture allows the model to decide between calling tools or outputting `<no_tool_call>`, with fine-tuning shaping this decision boundary.

## Foundational Learning
- **Concept:** Argument Population Accuracy (ArgA)
  - Why needed here: This is the key end-to-end metric used to evaluate the practical effectiveness of tool calling. Unlike simple precision/recall on tool name detection, ArgA measures the harder task of correctly extracting and formatting *all* arguments, which was a primary bottleneck.
  - Quick check question: Can you explain why a model might have high tool-calling recall but low ArgA?

- **Concept:** Supervised Fine-Tuning (SFT)
  - Why needed here: The paper investigates two distinct types of SFT: general-purpose (on chat, summarization) and task-specific (on tool-calling data). Understanding their conflicting effects is central to the findings.
  - Quick check question: According to the paper, what is the primary tradeoff introduced by general-purpose SFT on tool-calling performance?

- **Concept:** Cross-Lingual Transfer
  - Why needed here: A core research question is whether a model trained on English tool-calling data can generalize to Arabic prompts. The results show transfer is possible but imperfect, especially for argument values.
  - Quick check question: What specific type of error was most exacerbated by training on English-only data and testing on Arabic?

## Architecture Onboarding
- **Component map:** Base LLM (Fanar) → General SFT (Instruction Tuning) → Task-Specific SFT (Tool-Calling Fine-tuning) → Inference with a Tool Router (model decides <no_tool_call> or generates structured call). An iterative loop exists where a tool's output is fed back to the LLM.
- **Critical path:** The most impactful path for a production system is starting with a strong instruction-tuned model, then performing continued fine-tuning on a bilingual (Arabic/English) dataset that includes examples for all high-priority, custom tools.
- **Design tradeoffs:**
  - **Generalization vs. Precision:** Relying on a general tool-calling model alone leads to poor performance on niche tools (low recall/ArgA). The tradeoff is to invest in curating and fine-tuning on specific tool examples, which dramatically improves performance but adds a maintenance cost for each new tool.
  - **General SFT vs. Tool SFT:** General-purpose SFT improves the model's reasoning and recall for tool calls but makes it over-eager, increasing false positives on non-tool queries. This must be balanced with tool-specific training that includes negative examples (no tool needed).
- **Failure signatures:**
  - **Low Recall, High Precision:** The model is too conservative and fails to call a tool even when appropriate. Common when fine-tuning on insufficient data or for highly specialized, unseen tools.
  - **Low Precision, High Recall:** The model hallucinates tool calls for queries that don't require them. A signature of over-application of general-purpose instruction tuning without balanced negative examples.
  - **Low ArgA with Good Recall:** The model correctly decides to call the tool but fails to extract or format arguments. Indicates a need for in-language training data and/or examples that clarify expected argument syntax.
- **First 3 experiments:**
  1.  **Baseline & Cross-Lingual Test:** Take a base model and fine-tune it on English-only tool-calling data. Evaluate on both English and Arabic test sets. Observe the drop in ArgA on Arabic, especially for unseen tools. This establishes the limits of cross-lingual transfer.
  2.  **Ablate General SFT:** Take an instruction-tuned model and fine-tune it on tool-calling data. Compare its performance to the base model fine-tuned in the same way. Quantify the improvement in recall on unseen tools and the degradation in precision on non-tool-calling cases.
  3.  **Targeted Fine-Tuning:** Take the best performing model from step 2 and perform continued fine-tuning by adding training examples for a set of high-priority, custom tools. Measure the improvement on the held-out test set for those specific tools to justify the investment in tool-specific data curation.

## Open Questions the Paper Calls Out
- **Question:** To what extent do improvements in static tool-calling metrics (like ArgA) correlate with success in multi-turn, real-world agentic workflows?
  - **Basis in paper:** [explicit] The authors explicitly state their evaluation "does not account for downstream utility or correctness of tool execution in real-world agentic systems."
  - **Why unresolved:** The study relies on decomposed, single-turn test splits rather than measuring the success rate of completed tasks in a live environment where errors compound.
  - **What evidence would resolve it:** Evaluation of these models within a live agent loop, measuring end-task completion rates rather than isolated function call accuracy.

- **Question:** How can the negative impact of general-purpose SFT on non-function-calling precision be effectively mitigated without sacrificing recall?
  - **Basis in paper:** [inferred] Section 5.3 notes that general SFT causes a "concerning decline" in non-FC recall (e.g., dropping from 0.97 to 0.74), likely due to a bias toward generative behavior over classification precision.
  - **Why unresolved:** While bilingual data helped recover some performance, the structural tension between general instruction-following and precise refusal behavior remains unresolved.
  - **What evidence would resolve it:** A training curriculum that dynamically balances generative and classification tasks, or preference optimization specifically targeting refusal behavior.

- **Question:** Can highly granular function and argument descriptions in prompts eliminate the need for tool-specific fine-tuning?
  - **Basis in paper:** [explicit] The analysis suggests that crafting explicit format definitions "could guide the model's behavior... potentially reducing the need for extensive fine-tuning data."
  - **Why unresolved:** The study focused on fine-tuning strategies; the efficacy of in-context learning via improved prompting was not tested against the fine-tuning baselines.
  - **What evidence would resolve it:** Experiments comparing fine-tuned models against base models provided with maximally detailed tool schemas and few-shot examples.

## Limitations
- The reported ArgA scores are heavily dependent on the normalization pipeline for Arabic text (lowercasing, whitespace normalization, date format standardization), whose robustness to dialectal or morphological variation is not demonstrated.
- The ablation design assumes a clean separation between general-purpose and tool-specific fine-tuning effects, but the study does not control for confounding factors like total training steps or dataset size per phase.
- The synthetic tool generation (CustomTools) and IslamicRAGTool are not publicly available, limiting reproducibility of the exact experimental setup.

## Confidence
- **High confidence:** The comparative advantage of bilingual (Arabic/English) training data over English-only cross-lingual transfer for ArgA on Arabic tasks.
- **Medium confidence:** The specific performance rankings among the five model configurations, due to potential sensitivity to unreported hyperparameters (e.g., training epochs, exact data mixing ratios).
- **Low confidence:** The generalizability of the "tool-specific fine-tuning" recommendation beyond the narrow set of tools tested (CustomTools, IslamicRAGTool).

## Next Checks
1. **Re-run ablation with controlled training steps:** Ensure each fine-tuning phase (general SFT, tool SFT, tool-specific SFT) uses the same number of optimization steps to isolate the effect of data type from training duration.
2. **Stress-test normalization pipeline:** Evaluate ArgA scores on a held-out subset where Arabic arguments are manually annotated in multiple formats (dialectal variants, morphological inflections) to quantify normalization-induced errors.
3. **External tool generalization test:** Apply the best-performing model to a new, unseen tool (e.g., a financial calculator or weather API) without additional fine-tuning, measuring ArgA drop-off to validate the scope of tool-specific gains.