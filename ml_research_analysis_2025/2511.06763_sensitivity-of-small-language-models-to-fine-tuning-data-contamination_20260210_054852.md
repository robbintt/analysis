---
ver: rpa2
title: Sensitivity of Small Language Models to Fine-tuning Data Contamination
arxiv_id: '2511.06763'
source_url: https://arxiv.org/abs/2511.06763
tags:
- contamination
- semantic
- data
- transformations
- reversal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically investigates the vulnerability of 23 Small
  Language Models (270M-4B parameters) to data contamination during instruction tuning.
  Four contamination types (character reversal, word reversal, irrelevant, counterfactual)
  were applied at 25%, 50%, 75%, and 100% levels to evaluate model robustness.
---

# Sensitivity of Small Language Models to Fine-tuning Data Contamination

## Quick Facts
- arXiv ID: 2511.06763
- Source URL: https://arxiv.org/abs/2511.06763
- Reference count: 40
- Small Language Models (270M-4B parameters) show extreme sensitivity to syntactic data contamination during instruction tuning, with character reversal causing near-complete failure across all models.

## Executive Summary
This study systematically investigates how Small Language Models respond to various types of data contamination during instruction tuning. Through controlled experiments applying four contamination types (character reversal, word reversal, irrelevant, counterfactual) at multiple levels, the researchers demonstrate that SLMs exhibit dramatically different sensitivities to syntactic versus semantic transformations. The findings reveal that larger, more capable models paradoxically become more vulnerable to learning harmful patterns from counterfactual contamination, challenging assumptions about robustness scaling in language models.

## Method Summary
The researchers evaluated 23 Small Language Models ranging from 270M to 4B parameters using instruction tuning on a 500k-instance dataset. Four contamination types were systematically applied at 25%, 50%, 75%, and 100% levels: character reversal, word reversal, irrelevant (semantically disconnected), and counterfactual (factually contradictory). Models were assessed on accuracy, grammatical correctness, and semantic similarity metrics. The controlled experimental design allowed for precise measurement of contamination effects across different model sizes and transformation types, providing insights into how different forms of data corruption impact model performance and behavior.

## Key Results
- Character reversal contamination caused near-complete failure across all models, with accuracy dropping below 5% even at 25% contamination levels
- Word reversal allowed larger models to maintain moderate performance, suggesting some resilience to less severe syntactic transformations
- Semantic transformations (irrelevant and counterfactual) showed greater resilience, with models preserving grammatical correctness (95-100%) and semantic similarity (75-85%) even at high contamination levels
- Counterfactual contamination revealed a "capability curse" where larger, more capable models became more susceptible to learning harmful patterns

## Why This Works (Mechanism)
The study demonstrates that language models process syntactic and semantic information differently during training, with syntactic transformations causing more severe performance degradation than semantic ones. This differential sensitivity suggests that models have distinct mechanisms for handling structural versus meaning-based information. The "capability curse" finding indicates that models with greater learning capacity may also have greater capacity to internalize harmful patterns from contaminated data, particularly when those patterns contradict established knowledge. The preservation of grammatical correctness and semantic similarity under semantic contamination suggests that models can maintain core linguistic competencies even when exposed to contradictory or irrelevant information.

## Foundational Learning
- **Data Contamination Types**: Understanding different forms of data corruption (syntactic vs semantic) and their distinct impacts on model learning - needed to design robust training protocols and evaluate model resilience
- **Instruction Tuning**: The process of fine-tuning models on instruction-following datasets to improve task performance and alignment - needed to understand how contamination affects the alignment process
- **Model Size Scaling**: How model capacity affects learning behavior and contamination sensitivity - needed to interpret why larger models show different contamination responses
- **Performance Metrics**: Accuracy, grammatical correctness, and semantic similarity as evaluation measures - needed to quantify contamination effects across different dimensions
- **Robustness vs Capability Trade-offs**: The relationship between model capability and vulnerability to harmful patterns - needed to understand the "capability curse" phenomenon

## Architecture Onboarding

**Component Map**: Dataset (clean/contaminated) -> Instruction Tuning Process -> Fine-tuned Model -> Evaluation Metrics

**Critical Path**: Data preparation and contamination application -> Model training with contaminated data -> Performance evaluation across contamination levels -> Analysis of size-robustness relationships

**Design Tradeoffs**: Controlled contamination types provide methodological rigor but may not reflect real-world complexity; focus on accuracy metrics captures performance but may miss safety implications; single dataset limits generalizability but enables precise comparisons

**Failure Signatures**: Character reversal contamination causes near-complete accuracy collapse; word reversal shows size-dependent resilience; semantic contamination preserves linguistic correctness but may internalize harmful patterns; counterfactual contamination exhibits capability-dependent vulnerability

**First 3 Experiments**:
1. Test mixed contamination scenarios combining syntactic and semantic transformations
2. Evaluate downstream task performance (not just accuracy) on contaminated models
3. Replicate counterfactual contamination experiments with alternative model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Controlled contamination types may not capture real-world data contamination complexity
- Evaluation metrics focus on accuracy, grammatical correctness, and semantic similarity but don't assess downstream task performance or safety implications
- Single instruction tuning dataset (500k instances) may limit generalizability across different training paradigms or dataset scales

## Confidence

**Major Claim Clusters Confidence:**
- **High confidence**: Syntactic transformation effects (character/word reversal causing performance degradation) are well-established through multiple contamination levels and model sizes
- **Medium confidence**: Semantic transformation resilience (grammatical correctness and semantic similarity preservation) is demonstrated but may vary with different evaluation metrics
- **Medium confidence**: "Capability curse" for counterfactual contamination is supported but requires replication with different model architectures and training approaches

## Next Checks
1. Test contamination effects using mixed contamination types (combining syntactic and semantic transformations) to evaluate real-world scenario resilience
2. Evaluate downstream task performance (not just accuracy) on contaminated models to assess practical implications
3. Replicate the counterfactual contamination experiments with different model architectures (e.g., LLaMA, Mistral) and alternative training datasets to validate the "capability curse" phenomenon