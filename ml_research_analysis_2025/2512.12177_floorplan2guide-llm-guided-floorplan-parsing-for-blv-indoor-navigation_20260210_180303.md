---
ver: rpa2
title: 'Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation'
arxiv_id: '2512.12177'
source_url: https://arxiv.org/abs/2512.12177
tags:
- navigation
- graph
- floorplan
- indoor
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach for indoor navigation of visually
  impaired users by integrating large language models with floorplan analysis. It
  introduces a method that parses floorplan images into structured knowledge graphs
  using multimodal reasoning, enabling context-aware navigation instruction generation.
---

# Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation

## Quick Facts
- arXiv ID: 2512.12177
- Source URL: https://arxiv.org/abs/2512.12177
- Reference count: 40
- Primary result: 92.31% success rate on short routes using graph-based spatial encoding, outperforming direct visual reasoning by 15.4%

## Executive Summary
Floorplan2Guide is a novel approach for indoor navigation assistance for visually impaired users that leverages large language models (LLMs) to parse floorplan images into structured knowledge graphs. The system converts floorplan images into spatial knowledge graphs using multimodal reasoning, then generates context-aware navigation instructions. By constraining LLM reasoning to valid spatial relationships through graph structures, the system achieves higher accuracy than direct visual reasoning approaches. Real-world testing on the UMBC Math & Psychology building demonstrates practical effectiveness for infrastructure-free indoor navigation.

## Method Summary
The method uses a three-stage pipeline: (1) OCR preprocessing with multiple engines (PaddleOCR, EasyOCR, Tesseract) to extract text labels from floorplans, (2) LLM-based knowledge graph generation that converts floorplan images and OCR output into structured spatial representations (nodes, edges, adjacency matrices), and (3) navigation instruction generation from the graph using few-shot prompting. The system was tested on the CVC-FP dataset and a real-world floorplan (UMBC Math & Psychology Building MP-1) using three MLLMs (GPT-4o, Claude 3.7 Sonnet, LLaMA 3.2 Vision-Instruct 11B) with ArUco markers for localization checkpoints.

## Key Results
- 92.31% success rate on short routes using graph-based spatial structure
- 15.4% higher performance than direct visual reasoning across all models
- 6-9% improvement from few-shot prompting compared to zero-shot learning
- Success rates decrease with route length: 92.31% (short) → 61.54% (medium) → 38.46% (long)

## Why This Works (Mechanism)

### Mechanism 1: Graph-Based Spatial Encoding Reduces Hallucination
- Converting floorplans into explicit knowledge graph structures improves navigation accuracy by constraining LLM reasoning to valid spatial relationships. The adjacency matrix and node-edge representation enforce topological consistency—paths can only traverse edges that exist in the graph, eliminating impossible routes that direct visual reasoning might generate. Evidence shows Claude 3.7 improves from 76.92%→84.62% (short), 61.54%→69.23% (medium), 38.46%→53.85% (long) when using graph representation.

### Mechanism 2: Few-Shot Prompting Stabilizes Spatial Reasoning
- Providing exemplar floorplan-to-navigation pairs in the prompt improves route generation consistency without fine-tuning. In-context examples establish a reasoning pattern—how to count steps, describe turns, and reference landmarks—that the model replicates for new queries. All three models show 6-9% improvement with few-shot prompting on CVC-FP dataset.

### Mechanism 3: OCR-Augmented Entity Grounding
- Ensemble OCR preprocessing improves room label extraction accuracy compared to relying on LLM vision capabilities alone. Multiple OCR engines run in parallel with confidence filtering, providing reliable text labels that the LLM matches to visual regions. The paper claims OCR achieves higher accuracy in recognizing small or distorted characters on floorplans.

## Foundational Learning

- **Knowledge Graphs (nodes, edges, adjacency matrices)**: The entire pipeline hinges on representing floorplans as G = (V, E, A, Mη, W, S). Without understanding how nodes encode rooms and edges encode connectivity, the graph construction and path reasoning steps are opaque. Quick check: Given a 4-room floorplan where rooms A-B, B-C, and C-D share doors, write the adjacency matrix.

- **Few-Shot / In-Context Learning**: The performance gains depend on understanding why providing k=5 examples in the prompt improves accuracy versus k=0, without updating model weights. Quick check: Explain why the same model with identical weights performs differently on the same query when preceded by different example demonstrations.

- **Fiducial Markers (ArUco)**: Localization in Floorplan2Guide uses ArUco markers as ground-truth position anchors linked to graph node IDs. Understanding marker detection and ID mapping is essential for the localization module. Quick check: What information does an ArUco marker provide, and how would you associate each marker ID with a knowledge graph node?

## Architecture Onboarding

- **Component map**: OCR text extraction → Knowledge graph generation → Navigation instruction generation → ArUco marker detection → User interface (audio)
- **Critical path**: Floorplan image → OCR text → Graph construction → Route query → Instruction generation. Errors in graph construction cascade to all navigation tasks.
- **Design tradeoffs**: Graph-based vs. direct visual reasoning: +15.4% accuracy but adds preprocessing step and potential graph extraction errors. Few-shot (5-shot) vs. zero-shot: +6-9% accuracy but requires curated examples per building type. ArUco markers vs. infrastructure-free: Reliable localization but requires physical deployment.
- **Failure signatures**: Graph extraction returns disconnected components (missing edges), navigation instructions reference non-existent rooms (hallucination), ArUco marker not detected (lighting/occlusion), OCR misreads room numbers.
- **First 3 experiments**: 1) Validate graph extraction on CVC-FP sample floorplans by manually verifying nodes match ground-truth rooms and edges match doors/passages. 2) Ablate OCR by disabling preprocessing and passing floorplan directly to MLLM, comparing accuracy against OCR-augmented baseline. 3) Test few-shot scaling with 0-shot, 1-shot, 3-shot, 5-shot prompting on a held-out floorplan to determine if returns diminish beyond k=3.

## Open Questions the Paper Calls Out

- **User study with BLV participants**: The current validation used only sighted users, so accessibility, usability, and instruction effectiveness for the target population remain unknown. What modifications to instruction generation are needed based on BLV user feedback?
- **Mobile deployment performance**: Current experiments used a MacBook Pro with M1 Pro processor; real-world deployment requires smartphone-compatible solutions. Can the system maintain navigation accuracy on mobile devices with limited computational resources?
- **Dynamic obstacle avoidance**: The infrastructure-free design goal conflicts with real-time obstacle detection needs; existing vision-based solutions require retraining per environment. How should dynamic obstacle avoidance be integrated without requiring environment-specific retraining?
- **Generalization to diverse architectures**: Only one building floor and one benchmark dataset were tested; stairwells, elevators, and multi-level navigation remain unvalidated. Does performance generalize to multi-floor buildings and architecturally diverse floorplans?

## Limitations

- Graph extraction reliability depends on accurate knowledge graph construction, with no ablation studies on extraction accuracy itself
- Few-shot generalization remains unvalidated for architectural variations beyond the single building used for examples
- OCR ensemble effectiveness lacks direct comparison with vision-only graph extraction on the same floorplans
- Route classification criteria (short/medium/long) are not quantified, making it difficult to assess improvement scaling

## Confidence

- **High confidence**: Graph-based spatial encoding reduces hallucination compared to direct visual reasoning (supported by direct comparison in Table I across multiple models and route lengths)
- **Medium confidence**: Few-shot prompting stabilizes spatial reasoning (supported by CVC-FP results, but limited to one building's examples)
- **Low confidence**: OCR-augmented entity grounding improves accuracy (no direct comparison provided; only qualitative justification)

## Next Checks

1. Validate graph extraction accuracy by running the pipeline on CVC-FP sample floorplans and manually verifying that extracted nodes match ground-truth rooms and edges match doors/passages, checking for missing or spurious connections
2. Ablate OCR preprocessing by disabling OCR and passing floorplan directly to MLLM, then compare graph extraction accuracy and downstream navigation performance against OCR-augmented baseline to quantify the claimed improvement
3. Test few-shot scaling by evaluating 0-shot, 1-shot, 3-shot, 5-shot prompting on a held-out floorplan (not used in prompt examples) to determine if returns diminish beyond k=3 and whether performance generalizes across architectural styles