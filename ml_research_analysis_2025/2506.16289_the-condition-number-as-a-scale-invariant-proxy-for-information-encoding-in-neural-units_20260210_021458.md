---
ver: rpa2
title: The Condition Number as a Scale-Invariant Proxy for Information Encoding in
  Neural Units
arxiv_id: '2506.16289'
source_url: https://arxiv.org/abs/2506.16289
tags:
- information
- condition
- number
- forgetting
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using the condition number of neural network
  weight tensors as a scale-invariant proxy for information encoding. The condition
  number, defined as the ratio of largest to smallest singular values, indicates how
  much a unit has learned to selectively amplify or compress information.
---

# The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units

## Quick Facts
- arXiv ID: 2506.16289
- Source URL: https://arxiv.org/abs/2506.16289
- Reference count: 19
- Key outcome: KappaTune reduces catastrophic forgetting by 6x on sarcasm classification while maintaining accuracy, outperforming LoRA baselines by selectively fine-tuning low-condition-number tensors

## Executive Summary
This paper introduces KappaTune, a selective fine-tuning method that uses the condition number (ratio of largest to smallest singular values) of neural network weight tensors as a proxy for information encoding. The method assumes that high-condition-number tensors encode specialized, pre-trained knowledge that should be preserved, while low-condition-number tensors have broader transformation capacity suitable for adaptation. Theoretical analysis shows that for fixed weight norm, high condition number reduces information transfer capacity, reflecting specialized encoding. Empirically, KappaTune achieves superior catastrophic forgetting mitigation compared to LoRA baselines on three tasks: sarcasm classification (OLMoE), sentiment analysis (DeepSeek-V2-Lite), and multimodal ASR adaptation (Llama-3.2-8B).

## Method Summary
KappaTune computes the condition number κ(W) = σ_max/σ_min for each weight tensor in a pre-trained model, then selectively fine-tunes only the K tensors with lowest κ values while freezing the rest. The method can be applied directly or with LoRA adapters placed only on low-κ tensors. The theoretical justification connects condition number to information encoding through entropy bounds under Gaussian assumptions, showing that high κ indicates reduced information transfer capacity for fixed weight norm. The approach is evaluated on three tasks: sarcasm classification (K=75, 18 epochs), sentiment analysis (K=300, LoRA r=190), and multimodal ASR (K=100, unfreezing lowest-κ Llama tensors).

## Key Results
- On OLMoE for sarcasm classification, KappaTune achieves 6× lower forgetting (∆PPL = 0.2363 vs 1.4783) with comparable accuracy to LoRA
- For DeepSeek-V2-Lite on IMDB sentiment, KappaTune improves generalization and reduces forgetting by 2.8× compared to uniform LoRA placement
- In multimodal ASR experiment, unfreezing 100 lowest-κ tensors yields monotonic WER reduction to 5.61%; unfreezing highest-κ tensors causes degradation after initial improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The condition number κ(W) = σ_max/σ_min serves as a scale-invariant proxy for how much discriminative information a weight tensor has encoded during pre-training.
- Mechanism: High κ indicates anisotropic transformation—the unit selectively amplifies certain input directions (large singular values) while compressing others (small singular values). This concentration reflects learned feature hierarchies that prioritize task-relevant signals.
- Core assumption: The model is well-trained; high κ arises from meaningful specialization, not numerical ill-conditioning from poor training.
- Evidence anchors:
  - [abstract] "Theoretical analysis shows that for a fixed weight norm, a high condition number corresponds to reduced information transfer, reflecting specialized encoding."
  - [Section 3.2, Proposition 1] "This selective processing leads to a restructuring of the information distribution in the output."
  - [corpus] "Lipschitz Constant Meets Condition Number" links condition number to robustness and compactness, supporting the broader relevance of κ to network quality, though not directly validating the information-encoding proxy claim.

### Mechanism 2
- Claim: Under a fixed Frobenius norm constraint, differential entropy of the output is maximized when κ = 1; deviation toward high κ reduces information transfer capacity.
- Mechanism: The log-volume scaling factor Σ log(σ_i) determines entropy. For fixed Σ σ²_i = C, this sum is maximized when all σ_i are equal (κ = 1). Concentrated singular values shrink this factor, reducing representational breadth.
- Core assumption: Gaussian input distribution; linear unit Y = WX.
- Evidence anchors:
  - [Section 3, Theorem 1] Full proof via Lagrange multipliers showing κ = 1 maximizes entropy under norm constraint.
  - [Section 3] Equation (6) directly links output entropy to product of singular values.
  - [corpus] No direct corpus validation for this specific theorem; mechanism is theoretically derived, not empirically cross-validated.

### Mechanism 3
- Claim: Selectively fine-tuning low-κ tensors mitigates catastrophic forgetting by preserving specialized high-κ tensors that encode pre-trained knowledge.
- Mechanism: Low-κ tensors have broader, less specialized transformations with higher information transfer capacity. Adapting them allows new task learning while high-κ tensors—containing compressed discriminative features—remain frozen, protecting prior knowledge.
- Core assumption: High-κ tensors encode critical pre-trained information that should not be perturbed.
- Evidence anchors:
  - [Section 4.4, Figure 1] Unfreezing 100 lowest-κ tensors yields monotonic WER reduction to 5.61%; unfreezing highest-κ tensors causes degradation after initial improvement.
  - [Section 4.2, Table 1] KappaTune achieves 6× lower forgetting (∆PPL = 0.2363 vs 1.4783) with comparable accuracy to LoRA.
  - [corpus] Corpus lacks direct replication; method remains paper-specific.

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD) and condition number**
  - Why needed here: The entire method hinges on computing κ = σ_max/σ_min for each weight tensor to rank tensors by specialization.
  - Quick check question: Given a 512×512 weight matrix with singular values [10, 5, 0.1], what is its condition number?

- Concept: **Differential entropy of multivariate Gaussians**
  - Why needed here: Theoretical justification connects entropy to singular values via h(Y) ∝ Σ log(σ_i), establishing the κ-entropy relationship.
  - Quick check question: Why does entropy increase with the product of singular values?

- Concept: **Catastrophic forgetting and parameter isolation methods**
  - Why needed here: KappaTune is positioned as a forgetting mitigation strategy; understanding prior methods (EWC, LoRA, PackNet) contextualizes the contribution.
  - Quick check question: What is the key limitation of EWC and experience replay that KappaTune aims to bypass?

## Architecture Onboarding

- Component map:
  - Load pre-trained model -> Compute κ(W) for all tensors -> Sort tensors by κ ascending -> Select K lowest-κ tensors -> Freeze all parameters except selected tensors -> Fine-tune with Adam on task loss

- Critical path:
  1. Load pre-trained model
  2. Iterate through all weight tensors, compute κ
  3. Select K tensors with lowest κ
  4. Freeze all parameters; unfreeze selected tensors
  5. Train with Adam on task-specific loss

- Design tradeoffs:
  - **Budget K**: Lower K → less forgetting but potentially underfitting; higher K → more capacity but higher forgetting risk. Paper uses 75–300 tensors depending on model size.
  - **LoRA vs direct unfreezing**: Direct unfreezing (Algorithm 1) is faster (no adapter overhead) but permanently modifies weights; LoRA allows detachment but adds latency.
  - **Rank allocation (KappaTune-LoRA)**: Concentrating full parameter budget on fewer low-κ tensors with higher rank outperforms spreading lower-rank adapters uniformly.

- Failure signatures:
  - **Degradation curve inversion**: If WER/loss improves then degrades (as in INVERSE condition), likely unfreezing high-κ tensors causing knowledge destruction.
  - **Underfitting on new task**: If K is too low, model cannot learn new task; increase budget or verify selection includes relevant layers.
  - **High ∆PPL on held-out corpus**: Forgetting still occurring; check that selection correctly prioritizes lowest κ tensors.

- First 3 experiments:
  1. **Baseline κ distribution analysis**: Load pre-trained model, compute κ for all tensors, visualize distribution. Verify that κ varies meaningfully across layers/types (attention vs MLP, early vs late layers).
  2. **Ablation: K sensitivity**: On a small classification task, compare K ∈ {25, 50, 75, 150, 300} measuring both task accuracy and ∆PPL on pre-training corpus. Identify sweet spot where forgetting is minimized without sacrificing accuracy.
  3. **Inverse selection sanity check**: Compare unfreezing K lowest-κ vs K highest-κ tensors. Replicate paper's finding that high-κ selection causes degradation, validating the mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the relationship between the condition number and information encoding change under non-Gaussian, realistic data distributions?
- Basis: [explicit] The conclusion states, "Future work should extend this analysis beyond Gaussian assumptions to realistic data distributions, rigorously quantifying how the full singular value spectrum... governs information encoding."
- Why unresolved: The theoretical derivations in Sections 3 and 3.2 rely on the assumption of Gaussian inputs ($X \sim N(\mu_X, \Sigma_X)$) to link entropy to singular values. Real-world neural activations often violate this assumption.
- What evidence would resolve it: Empirical validation of the entropy-condition number correlation on layers with measured non-Gaussian activation statistics, or theoretical extensions of the entropy bounds for non-Gaussian distributions.

### Open Question 2
- Question: Does the full singular value spectrum provide a more accurate proxy for information encoding than the condition number ratio alone?
- Basis: [explicit] The paper suggests future work should focus on "rigorously quantifying how the full singular value spectrum, not only the condition number, governs information encoding in deep networks."
- Why unresolved: The current method simplifies the spectral properties into a single ratio ($\kappa = \sigma_{max}/\sigma_{min}$). It is unclear if this scalar proxy discards nuanced information about the "log-volume scaling factor" that would be captured by the full spectrum.
- What evidence would resolve it: A comparative analysis where fine-tuning selection is guided by spectral entropy or the full product of singular values versus the simple condition number ratio.

### Open Question 3
- Question: Is a high condition number a strictly necessary condition for a weight tensor to have encoded significant discriminative pre-training information?
- Basis: [explicit] Conjecture 1 posits that a high condition number is a necessary condition for robust encoding, but the author notes this "warrants further empirical and theoretical investigation."
- Why unresolved: While high anisotropy is argued to reflect specialized encoding, it is currently a conjecture. It is possible that "flat" tensors (low $\kappa$) might also store critical generalizable features not captured by this specific geometric proxy.
- What evidence would resolve it: Ablation studies on well-trained models where high-kappa tensors are selectively corrupted versus low-kappa tensors, measuring the relative drop in downstream task performance to confirm necessity.

### Open Question 4
- Question: Can adaptive threshold selection policies for tensor unfreezing outperform the current fixed-budget approach used in KappaTune?
- Basis: [explicit] The conclusion suggests that "adaptive threshold selection policies, would enhance the method’s generality and scalability for large neural systems."
- Why unresolved: The current KappaTune algorithm (Algorithm 1) utilizes a fixed budget $K$ (e.g., 75 or 100 tensors). This static approach may not generalize optimally across different architectures or task complexities without manual tuning.
- What evidence would resolve it: Implementation and testing of a dynamic selection algorithm that adjusts $K$ based on local layer-wise statistics or training dynamics, comparing its forgetting mitigation performance against the fixed budget.

## Limitations

- Theoretical Generalization: The entropy bound relies on Gaussian input assumptions that may not hold for real-world data distributions and nonlinear activations.
- Proxy Validity: High condition number assumes well-behaved pre-training; training pathologies could create false positives for specialized encoding.
- Domain Specificity: Method effectiveness across diverse domains and model architectures remains untested beyond transformer-based LLMs.

## Confidence

- **High Confidence**: The empirical results demonstrating KappaTune's effectiveness in reducing catastrophic forgetting compared to LoRA baselines.
- **Medium Confidence**: The theoretical foundation linking condition number to information encoding under stated assumptions.
- **Low Confidence**: The universal applicability of condition number as a proxy for information encoding across all neural network architectures and tasks.

## Next Checks

1. **Ablation Study on K Values**: Systematically vary K (number of low-κ tensors unfrozen) across a wider range (e.g., 25-500) on the IMDB sentiment task to identify optimal trade-offs between forgetting mitigation and task performance.

2. **Distribution Analysis Validation**: Compare the condition number distributions of tensors that are empirically important (measured via ablation) versus those selected by KappaTune to test whether high κ genuinely indicates specialized, task-relevant encoding.

3. **Cross-Architecture Testing**: Apply KappaTune to a convolutional neural network on a vision task (e.g., CIFAR-10 fine-tuning) to determine whether the condition number proxy generalizes beyond transformer architectures.