---
ver: rpa2
title: Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs
arxiv_id: '2505.11008'
source_url: https://arxiv.org/abs/2505.11008
tags:
- syllable
- syllables
- sequences
- abugida
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied Transformer-based models to predict syllable
  sequences in six Abugida languages (Bengali, Hindi, Khmer, Lao, Myanmar, Thai) using
  the Asian Language Treebank dataset. The core method involved reconstructing complete
  syllable sequences from incomplete inputs including consonant-only, vowel-only,
  randomly deleted character sequences, and masked syllable sequences.
---

# Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs

## Quick Facts
- arXiv ID: 2505.11008
- Source URL: https://arxiv.org/abs/2505.11008
- Reference count: 6
- Task: Reconstructing complete syllable sequences from incomplete inputs in 6 Abugida languages using Transformer models.

## Executive Summary
This study applied Transformer-based models to predict syllable sequences in six Abugida languages (Bengali, Hindi, Khmer, Lao, Myanmar, Thai) using the Asian Language Treebank dataset. The core method involved reconstructing complete syllable sequences from incomplete inputs including consonant-only, vowel-only, randomly deleted character sequences, and masked syllable sequences. Results showed consonant sequences were critical for accurate prediction, achieving BLEU scores of 62.65-99.87, while vowel sequences yielded much lower scores of 13.62-60.28. The model demonstrated robust performance on partial and masked syllable reconstruction, with BLEU scores ranging from 65.19-99.10 for 1-2 character deletions and maintaining high scores even with 3-10 masked syllables. Khmer and Thai achieved the highest performance, while Myanmar showed the greatest challenges due to its complex diacritic system.

## Method Summary
The study used the Asian Language Treebank (ALT) dataset with approximately 20,104 sentences per language, split into 18,104 training, 1,000 development, and 1,000 test sentences. Data was cleaned by removing punctuation, numbers, and non-language Unicode characters, then syllable-segmented. Input variants included consonant-only sequences, vowel-only sequences, random character deletions (1-2 per syllable), and masked syllables (3, 5, 8, 10 syllables with no consecutive masks). Marian NMT Transformer models were trained with 2 encoder/decoder layers, 8 attention heads, dropout 0.3, label smoothing 0.1, learning rate 0.0003 with inverse square root decay, beam size 6, and early stopping at 10 epochs. BLEU scores were the primary evaluation metric.

## Key Results
- Consonant-only input sequences achieved high BLEU scores of 62.65-99.87 across languages
- Vowel-only input sequences performed poorly with BLEU scores of 13.62-60.28
- Random deletion of 1-2 characters per syllable maintained high BLEU scores of 65.19-99.10
- Masking 3-10 syllables while avoiding consecutive masks preserved strong performance

## Why This Works (Mechanism)
Assumption: The high performance on consonant-only inputs (BLEU 62.65-99.87) demonstrates that Abugida scripts encode syllable identity primarily through consonant characters, with vowels serving as secondary modifiers. This suggests the Transformer model successfully learned to leverage consonant anchors while predicting missing vowel components.

## Foundational Learning
- **Syllable segmentation in Abugida scripts**: Critical for aligning source-target pairs; quick check: verify segmented output matches expected syllable count distributions per language.
- **Unicode range filtering for South Asian scripts**: Ensures data quality; quick check: confirm retained characters fall within language-specific Unicode blocks.
- **Transformer encoder-decoder architecture**: Enables sequence-to-sequence prediction; quick check: monitor training loss and BLEU on dev set.
- **Marian NMT framework**: Provides efficient multilingual training; quick check: ensure tied embeddings and label smoothing are configured.
- **BLEU score calculation for non-Latin scripts**: Measures translation quality; quick check: validate with known reference translations.

## Architecture Onboarding

**Component map:**
Marian NMT Transformer (2-layer encoder → 8-head attention → 2-layer decoder → tied embeddings) → BLEU evaluation

**Critical path:**
Data cleaning → Syllable segmentation → Input variant generation → Marian Transformer training → BLEU evaluation

**Design tradeoffs:**
- Limited to 2 layers for efficiency vs deeper models for potential accuracy gains
- Fixed beam size of 6 balances speed and quality
- Early stopping at 10 epochs prevents overfitting but may miss optimal checkpoints

**Failure signatures:**
- Mismatched source-target lengths indicate segmentation errors
- Vanishing gradients suggest learning rate or architecture issues
- Low BLEU on dev set indicates model capacity or data quality problems

**3 first experiments:**
1. Train on clean syllable sequences only (no deletions/masking) to establish baseline BLEU
2. Test consonant-only vs vowel-only reconstruction to verify reported performance gap
3. Apply random deletion of 1 character per syllable to assess model robustness

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out open questions in the provided text. Potential areas for future research include investigating the impact of different preprocessing techniques on performance and exploring deeper Transformer architectures for potentially improved accuracy.

## Limitations
- Exact syllable segmentation regex rules not provided, requiring language-specific assumptions
- Vowel/consonant extraction logic unspecified, especially for complex diacritic systems like Myanmar
- Random deletion and masking algorithms lack precise implementation details
- No analysis of how preprocessing variations affect absolute BLEU scores

## Confidence
- **High confidence**: BLEU score trends showing consonant sequences vastly outperform vowel-only inputs, and model robustness across partial input types
- **Medium confidence**: Relative performance differences between languages (e.g., Khmer/Thai high, Myanmar low), given that exact preprocessing may vary
- **Low confidence**: Absolute BLEU scores without exact preprocessing replication

## Next Checks
1. Verify syllable segmentation outputs match expected character count distributions in Table 2 for each language
2. Confirm that vowel-only and consonant-only input generation correctly separates base characters from diacritics for all six languages
3. Test that masking never produces consecutive masked syllables and that deletion counts per syllable are consistently applied across training/validation/test sets