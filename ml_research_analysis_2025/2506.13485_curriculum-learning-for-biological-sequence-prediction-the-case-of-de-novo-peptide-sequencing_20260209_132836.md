---
ver: rpa2
title: 'Curriculum Learning for Biological Sequence Prediction: The Case of De Novo
  Peptide Sequencing'
arxiv_id: '2506.13485'
source_url: https://arxiv.org/abs/2506.13485
tags:
- peptide
- learning
- sequence
- training
- sequencing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training non-autoregressive
  transformer models for de novo peptide sequencing from mass spectrometry data. The
  authors propose a structured curriculum learning strategy combined with iterative
  refinement to improve training stability and sequence prediction accuracy.
---

# Curriculum Learning for Biological Sequence Prediction: The Case of De Novo Peptide Sequencing

## Quick Facts
- arXiv ID: 2506.13485
- Source URL: https://arxiv.org/abs/2506.13485
- Reference count: 40
- Primary result: Reduces training failures by over 90% and achieves state-of-the-art peptide recall and amino acid precision in de novo peptide sequencing

## Executive Summary
This paper addresses the challenge of training non-autoregressive transformer models for de novo peptide sequencing from mass spectrometry data. The authors propose a structured curriculum learning strategy combined with iterative refinement to improve training stability and sequence prediction accuracy. Their method dynamically adjusts learning difficulty by masking parts of the target sequence and iteratively refines predictions using learned embeddings. Evaluated on nine benchmark species, the approach significantly outperforms existing methods while reducing training failures by over 90%.

## Method Summary
The method employs a 9-layer transformer architecture with curriculum learning for training stability and iterative refinement for inference improvement. During training, the model samples CTC alignment paths and progressively masks tokens based on current accuracy to create conditioned learning tasks. The curriculum difficulty is controlled by a peek factor that adjusts masking ratio. At inference, the model performs multiple forward passes where each iteration uses the previous prediction as input, followed by precise mass control to filter candidates within mass tolerance. The approach is evaluated on the MassIVE-KB dataset across nine species, demonstrating superior performance in both amino acid precision and peptide recall.

## Key Results
- Reduces training failures from 18 cases to 1 case compared to baseline PrimeNovo
- Achieves state-of-the-art performance in amino acid precision and peptide recall
- Iterative refinement improves peptide recall from 0.728 to 0.736 with 3 iterations
- Training stability dramatically improved through curriculum learning strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum learning with adaptive masking stabilizes NAT training by progressively reducing the CTC search space
- Mechanism: During training, the model samples the most probable CTC alignment path from its current predictions, then masks a subset of tokens to create conditional inputs. The masking ratio ρ = α(1 - accuracy) starts high (easier task, more ground truth visible) and decreases as accuracy improves. This transforms the objective from independent sequence prediction P(A|I) to conditioned estimation P(A|ρ(A,y),I), where ρ(A,y) represents unmasked tokens from the sampled CTC path.
- Core assumption: The model can generalize from partial CTC path exposure to unobserved valid paths; the sampled path is representative of correct alignment structure
- Evidence anchors:
  - [abstract] "This approach adjusts protein's learning difficulty based on the model's estimated protein generational capabilities through a sampling process, progressively learning peptide generation from simple to complex sequences"
  - [Section 3.3] "By exposing the model to simpler targets first, the curriculum learning approach enables a more gradual and smooth learning process"
  - [Section 4.2] "PrimeNovo fails in 18 cases due to loss explosion or extreme overfitting, while our model fails only once"
  - [corpus] No direct corpus validation; curriculum learning in biological sequences (arXiv:2510.03811) explores similar strategies for mRNA but with different objectives

### Mechanism 2
- Claim: Iterative refinement leverages curriculum-trained embeddings to improve predictions without additional training
- Mechanism: The curriculum training adds an embedding layer capable of encoding conditional inputs (partial CTC paths). At inference, the model performs N forward passes where each pass y^(i) = argmax P(·|I, EmbeddingLayer(y^(i-1))) uses the previous argmax prediction as decoder input. This allows error correction through bidirectional context across iterations.
- Core assumption: The embedding layer learns meaningful representations that transfer from partial ground truth (training) to partial predictions (inference)
- Evidence anchors:
  - [abstract] "we introduce a self-refining inference-time module that iteratively enhances predictions using learned NAT token embeddings"
  - [Section 3.4] "at the i-th iteration, the previously decoded pseudo path using argmax, y^(i-1), is used as input for the decoder again"
  - [Table 5] Peptide recall improves from 0.728 (1 iteration) to 0.736 (3 iterations), plateauing thereafter
  - [corpus] arXiv:2507.10955 (Diffusion Decoding) proposes alternative iterative approach via diffusion models; different mechanism, similar goal

### Mechanism 3
- Claim: Precise Mass Control enforces physical constraints during decoding, filtering invalid candidates
- Mechanism: Given precursor mass m and tolerance σ, PMC formulates decoding as a knapsack problem. Each amino acid token has a mass weight and log-probability value. A 2D dynamic programming table tracks the most probable sequences at each mass bin, retaining only top-B candidates per bin. Final selection maximizes probability within [m-σ, m+σ].
- Core assumption: The precursor mass constraint is reliable and the DP beam B is sufficient to retain valid candidates
- Evidence anchors:
  - [Section 3.2] "Since the precursor mass m provided as input is a strict constraint for the generated sequence y, we use a dynamic programming solver to find the path that satisfies this constraint"
  - [Appendix D] Detailed DP formulation with mass-constrained updates
  - [corpus] No corpus comparison; PMC is inherited from prior work (Zhang et al., 2025)

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC) Loss
  - Why needed here: Core training objective for NAT models; requires understanding alignment path enumeration, blank tokens, and reduction rules
  - Quick check question: Given target sequence "ATC" and max length 5, can you identify which paths are valid under CTC reduction?

- Concept: Autoregressive vs Non-Autoregressive Generation
  - Why needed here: NAT enables parallel decoding and bidirectional context but loses token dependency modeling; explains why curriculum learning is needed
  - Quick check question: Why does NAT have a larger search space than autoregressive models for the same sequence length?

- Concept: Curriculum Learning
  - Why needed here: Motivates the difficulty annealing strategy; understanding easy-to-hard training progression
  - Quick check question: How should task difficulty be measured and adjusted during training for sequence generation?

## Architecture Onboarding

- Component map: Input (spectrum I, precursor mass m, charge z) -> Spectrum Encoder (9-layer Transformer, sinusoidal encoding for m/z and intensity) -> NAT Decoder (9-layer, 8 heads, 1024 FFN, takes curriculum embeddings as input) -> [Training] CTC Path Sampling → Difficulty Annealing → Masked Oracle → CTC Loss / [Inference] Argmax CTC → Iterative Refinement (3 passes) → PMC Unit → Output

- Critical path:
  1. Spectrum encoding to latent representation E^(m)
  2. NAT decoder forward pass producing probability matrix P(·|I) over T positions
  3. Training: Sample best CTC alignment → compute accuracy → determine masking ratio → create masked oracle → backprop
  4. Inference: Argmax decode → refine 3 iterations → PMC mass-constrained selection

- Design tradeoffs:
  - Peek factor α controls how aggressive curriculum masking is; too high may cause information leakage, too low may not stabilize training
  - Iteration count N=3 balances accuracy gains vs inference cost (Table 5 shows plateau)
  - PMC beam size B trades memory/computation vs candidate retention

- Failure signatures:
  - Loss explosion early in training: masking ratio too aggressive, reduce α or warm up curriculum gradually
  - Oscillating validation accuracy without improvement: model memorizing rather than generalizing; check curriculum progression
  - Poor mass constraint satisfaction: PMC tolerance σ may be too tight or beam B too small

- First 3 experiments:
  1. Baseline ablation: Train without curriculum learning (fixed mask ratio=0.7) to confirm training instability replicates; compare loss curves
  2. Hyperparameter sweep: Vary peek factor α ∈ {0.5, 1.0, 1.5} on validation split; monitor both training stability and final accuracy
  3. Inference iteration study: Measure peptide recall and latency for N ∈ {1, 2, 3, 5, 10} iterations to confirm plateau and select production setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CTC-path-centric curriculum learning strategy be effectively generalized to non-autoregressive models utilizing different objective functions (e.g., Knowledge Distillation or Cross-Entropy) or to autoregressive architectures?
- Basis in paper: [inferred] Appendix F.1 explicitly states the method is "specific to NAT models utilizing CTC" and relies on "CTC-path-centric refinement," distinguishing it from other techniques but leaving its applicability to other architectures untested.
- Why unresolved: The masking strategy relies on sampling valid paths based on CTC reduction rules, which may not have direct equivalents in standard cross-entropy or autoregressive decoding schemes.
- What evidence would resolve it: Applying the curriculum masking strategy to a standard Transformer or a non-CTC NAT baseline and comparing convergence stability and final performance.

### Open Question 2
- Question: To what extent does the proposed method improve robustness to domain shifts caused by variations in mass spectrometry instrument precision?
- Basis in paper: [inferred] Appendix E notes that performance drops on the 7-species dataset due to "distribution mismatch" from different MS equipment precision, despite the curriculum learning strategy improving general training stability.
- Why unresolved: While the method stabilizes optimization on the training distribution, it does not explicitly address the domain adaptation challenges posed by hardware variations, leading to lower relative performance on out-of-distribution instruments.
- What evidence would resolve it: Evaluating the model's performance degradation curves when trained on high-precision data and tested on low-precision data compared to baseline models.

### Open Question 3
- Question: Is the performance plateau observed during iterative refinement (approx. 3 steps) an inherent limitation of the model's capacity or a result of error propagation in the pseudo-labels?
- Basis in paper: [explicit] Appendix B.2 notes that metrics "plateau at 0.737 for recall... beyond the third iteration," leading to the selection of 3 steps, but does not investigate the underlying cause of this ceiling.
- Why unresolved: It is unclear if the model fails to extract new information from the refined embeddings or if the accumulated errors in the argmax pseudo-labels prevent further self-correction.
- What evidence would resolve it: Analyzing the token-level prediction entropy and error accumulation at each iteration step (4 through 10) to determine if confidence increases while accuracy remains static.

## Limitations
- Data dependence and domain specificity may limit generalization to different spectral quality distributions
- Computational overhead from iterative refinement increases inference latency compared to single-pass NAT models
- Hyperparameter sensitivity to peek factor α and other curriculum parameters is not fully characterized

## Confidence
- Mechanism 1 (Curriculum Learning): Medium confidence - training stability improvements are well-supported but theoretical justification is limited
- Mechanism 2 (Iterative Refinement): Medium-High confidence - 1.1% improvement in peptide recall is statistically significant with clear plateau
- Mechanism 3 (Precise Mass Control): High confidence - well-established constraint with standard knapsack formulation

## Next Checks
1. Curriculum Ablation Study: Systematically vary the peek factor α (e.g., {0.5, 1.0, 1.5}) and compare both training stability curves and final accuracy metrics
2. Iterative Refinement Cost-Benefit Analysis: Measure inference latency and computational cost for N ∈ {1, 2, 3, 5, 10} iterations while tracking accuracy gains
3. Cross-Dataset Generalization: Evaluate the curriculum-trained model on spectra from different mass spectrometry platforms not represented in the training data