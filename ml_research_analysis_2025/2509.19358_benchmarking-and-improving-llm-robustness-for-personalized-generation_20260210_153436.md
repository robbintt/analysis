---
ver: rpa2
title: Benchmarking and Improving LLM Robustness for Personalized Generation
arxiv_id: '2509.19358'
source_url: https://arxiv.org/abs/2509.19358
tags:
- preference
- response
- uni00000057
- uni00000048
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PERG, a framework for evaluating LLM robustness
  in personalized generation, focusing on both factuality and preference alignment.
  Current LLMs struggle to maintain factual accuracy when conditioned on user preferences,
  with even the strongest models failing 5% of the time and smaller models exceeding
  20% failure rates.
---

# Benchmarking and Improving LLM Robustness for Personalized Generation

## Quick Facts
- **arXiv ID:** 2509.19358
- **Source URL:** https://arxiv.org/abs/2509.19358
- **Reference count:** 40
- **Primary result:** LLMs fail 5-20% of the time when personalizing generation; a two-stage Pref-Aligner framework improves robustness by 25%.

## Executive Summary
This paper introduces PERG, a framework for evaluating LLM robustness in personalized generation, focusing on both factuality and preference alignment. Current LLMs struggle to maintain factual accuracy when conditioned on user preferences, with even the strongest models failing 5% of the time and smaller models exceeding 20% failure rates. The study finds that irrelevant preferences amplify misalignment, and that different preference types (e.g., clarity vs. context) impact robustness differently. To address these issues, the authors propose Pref-Aligner, a two-stage approach that decouples generation from personalization, improving robustness by an average of 25% across models. The work highlights the need for more comprehensive evaluation methods and scalable training strategies to ensure reliable, user-aligned AI systems.

## Method Summary
The authors create PERGData by curating 7,208 examples from MMLU, TruthfulQA, and CommonsenseQA, then generate 35 relevant and irrelevant preferences for each. They evaluate LLMs using a two-stage Pref-Aligner framework: a Generator Agent produces factual responses without preferences, and an Aligner Agent edits these responses to incorporate relevant preferences. Robustness is measured via Breakage Rate (loss of factuality), Alignment Failure (ignoring preferences), and their union (Robustness Error). An answer extractor (Mistral-7B fine-tuned with LoRA) and GPT-4o-mini evaluator assess correctness and preference alignment respectively.

## Key Results
- Zero-shot LLMs show significant robustness errors (5-20% failure rates) when personalizing generation
- Irrelevant preferences amplify misalignment, causing higher robustness errors than relevant ones
- Different preference types impact robustness differently: "Context" preferences cause the most breakage, while "Clarity" preferences improve factuality
- Pref-Aligner improves robustness by an average of 25% across all models and datasets
- Smaller models (e.g., Llama3-8B) show higher failure rates than larger ones (e.g., Llama3-70B)

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Generation from Alignment
Separating the task of answering a query from the task of styling that answer improves factual robustness by preventing preference signals from interfering with reasoning. The Pref-Aligner framework utilizes a Generator Agent to produce a base response solely conditioned on the query (ensuring high accuracy), followed by an Aligner Agent that edits the style based on preferences. Core assumption: factual recall and preference adherence are conflicting objectives when processed simultaneously in a single forward pass. Evidence: Section 7 states "In Stage 1, a generation agent responds to user queries without considering their defined preferences... ensuring that the base content remains factual."

### Mechanism 2: Semantic Distance Reduction
Editing an existing response to fit a preference is computationally and semantically easier than generating a response from scratch that satisfies both constraints. The Aligner operates on a constrained rewrite task. Core assumption: the error surface for "stylistic editing" is distinct from and safer than the error surface for "reasoning under constraint." Evidence: Cites Ji et al. (2024) noting "the semantic distance between an unaligned response $r'$ and an aligned response $r$ is smaller." Table 2 shows Robustness Error dropping significantly (e.g., Llama3-70B from 9.0 to 6.5).

### Mechanism 3: Irrelevance Filtering via Explicit Instruction
Instructing a model to explicitly filter irrelevant preferences prevents "over-alignment" errors where models hallucinate connections. The Aligner Agent is explicitly prompted to "decide which preferences are relevant, if any... If it finds no relevant preference, the aligner simply returns $r'$ as $r$." Core assumption: Models are better at binary classification (relevant vs. irrelevant) in a post-hoc editing step than during initial generative inference. Evidence: Section 7 describes the Aligner's decision logic to only modify if relevant.

## Foundational Learning

- **Concept: Robustness vs. Alignment Trade-off**
  - **Why needed here:** The paper redefines "robustness" not as just error-free, but as the intersection of *factuality* and *preference alignment*. You must understand that optimizing for one can break the other (Breakage vs. Alignment Failure).
  - **Quick check question:** If a model answers a math problem correctly but uses verbose language when the user asked for brevity, is it "robust" by this paper's definition?

- **Concept: Chain-of-Thought (CoT) Interference**
  - **Why needed here:** The paper identifies that certain preferences (e.g., "provide context") inadvertently trigger CoT reasoning, which can lead to hallucination. Understanding this triggers the need for the Pref-Aligner.
  - **Quick check question:** Why might asking a model to "explain its reasoning" actually reduce its factual accuracy on TruthfulQA data?

- **Concept: Agentic Decoupling**
  - **Why needed here:** This is the architectural shift from "One Model, One Prompt" to "Specialized Agents in Sequence" (Generator -> Aligner).
  - **Quick check question:** Why does passing the output of the Generator Agent to the Aligner Agent yield better results than prompting a single model with both the question and preference at once?

## Architecture Onboarding

- **Component map:** PERG Dataset -> Generator Agent -> Aligner Agent -> Evaluator
- **Critical path:**
  1. **Data Ingestion:** Query ($q$) + Preference ($P$).
  2. **Stage 1 (Generator):** Model processes $q$ → generates $r'$ (Base Response).
  3. **Stage 2 (Aligner):** Model processes ($r'$, $P$) → checks relevance → generates $r$ (Final Response).
  4. **Evaluation:** Check $r$ against ground truth (Factuality) and $P$ (Alignment).
- **Design tradeoffs:**
  - **Latency:** The system doubles inference latency (two forward passes).
  - **Complexity:** Requires managing state between agents.
  - **Context Window:** The Aligner must read the full generated response, potentially hitting token limits for very long outputs.
- **Failure signatures:**
  - **Breakage:** Model answers correctly without preferences, but incorrectly with them.
  - **Alignment Failure:** Model answers correctly but ignores the style preference.
  - **Instruction Following Drop:** Models often fail to output the specific multiple-choice letter (e.g., "A") when distracted by preferences.
- **First 3 experiments:**
  1. **Baseline Breakage Test:** Run a target model (e.g., Mistral-7B) on PERG with zero-shot prompting including preferences. Measure the "Breakage Rate" (how often it gets a question wrong that it previously got right).
  2. **Ablation on Relevance:** Run the model with *irrelevant* preferences vs. *relevant* preferences to confirm the hypothesis that irrelevant data increases alignment errors.
  3. **Pref-Aligner Integration:** Implement the two-stage Pref-Aligner pipeline using the same base model for both agents and compare the "Robustness Error" against the zero-shot baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can data-efficient fine-tuning methods using curated "robust" examples improve base model stability more effectively than naive preference fine-tuning?
  - **Basis in paper:** The authors explicitly propose "the pursuit of data-efficient methods" and training on "carefully curated examples that inherently emphasize robustness" as a necessary step to improve base models without architecture changes (Section 8).
  - **Why unresolved:** The paper demonstrates that naive fine-tuning (as seen in the Janus model) degrades robustness, but it evaluates system-level interventions (Pref-Aligner) rather than specific training cures for the base model.

- **Open Question 2:** What are the underlying mechanisms causing preference alignment to impair instruction-following capabilities, and how can this trade-off be mitigated?
  - **Basis in paper:** The authors identify in Section 8 that "Preference alignment impairs instruction following," noting that models conditioned on preferences often fail to output valid multiple-choice options (Delivery Failure), but they do not provide a solution for this specific degradation.

- **Open Question 3:** Does the "breakage" phenomenon—where reasoning preferences disrupt factual accuracy—scale similarly to open-ended, free-form generation tasks?
  - **Basis in paper:** The authors acknowledge in Section 10 (Limitations) that they "limit our focus to evaluating within a multiple-choice question setting" and suggest future work should look into "free-form format" queries.

## Limitations
- Reliance on internal evaluator (GPT-4o-mini) for preference alignment scoring introduces potential bias
- Pref-Aligner effectiveness depends on Generator Agent producing factually correct responses
- System doubles inference latency, limiting practical deployment in real-time applications
- Semantic distance reduction mechanism is supported by external work rather than direct evidence from this paper's experiments

## Confidence

- **High Confidence:** The empirical finding that LLMs exhibit significant robustness errors (5-20% failure rates) when preferences are added, and that irrelevant preferences amplify these errors. The two-stage Pref-Aligner architecture is clearly specified and implemented.
- **Medium Confidence:** The claim that decoupling generation from personalization improves robustness by 25% on average, as this depends on the quality of the Aligner Agent and the specific preference types tested.
- **Low Confidence:** The theoretical mechanism of "semantic distance reduction" as the primary driver of Pref-Aligner's success, since this is largely inferred from related work rather than directly measured in the experiments.

## Next Checks

1. **Cross-Evaluator Validation:** Run the same PERG evaluation using multiple independent alignment judges (e.g., Claude, GPT-4) to verify the consistency of GPT-4o-mini's scoring and assess potential evaluator bias.

2. **Failure Mode Analysis:** Systematically analyze cases where Pref-Aligner fails, specifically examining whether the Generator Agent produces incorrect responses or whether the Aligner Agent fails to properly edit, to identify which stage is the bottleneck.

3. **Latency-Performance Trade-off:** Measure the exact inference latency increase of Pref-Aligner compared to zero-shot approaches and determine the point at which the 25% robustness improvement is no longer worth the computational cost.