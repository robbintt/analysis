---
ver: rpa2
title: Is your LLM trapped in a Mental Set? Investigative study on how mental sets
  affect the reasoning capabilities of LLMs
arxiv_id: '2501.11833'
source_url: https://arxiv.org/abs/2501.11833
tags:
- problems
- complex
- mental
- reasoning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first study to apply the cognitive psychology
  concept of mental sets to evaluate large language models' (LLMs) reasoning capabilities.
  The authors compare three LLM models (Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct,
  and GPT-4o) on mathematical equivalence problems designed to test adaptability versus
  reliance on familiar strategies.
---

# Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs

## Quick Facts
- arXiv ID: 2501.11833
- Source URL: https://arxiv.org/abs/2501.11833
- Reference count: 8
- Key outcome: This paper presents the first study to apply the cognitive psychology concept of mental sets to evaluate large language models' (LLMs) reasoning capabilities. The authors compare three LLM models (Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, and GPT-4o) on mathematical equivalence problems designed to test adaptability versus reliance on familiar strategies. The results show that while few-shot prompting with chain-of-thought reasoning improves accuracy (Exact Match scores ranging from 0.33 to 0.83 depending on model and condition), it increases the number of reasoning steps compared to direct few-shot prompting. The study demonstrates that LLMs exhibit mental set behaviors similar to humans, requiring more complex reasoning paths even for simpler problems, and highlights the need for evaluation metrics that capture adaptability beyond traditional accuracy measures.

## Executive Summary
This paper introduces the concept of mental sets from cognitive psychology to evaluate how large language models approach problem-solving. The authors investigate whether LLMs, like humans, can become "trapped" in familiar problem-solving strategies even when simpler solutions exist. By designing mathematical equivalence problems that require either complex or simple solution paths, they demonstrate that LLMs exhibit mental set behaviors, showing improved accuracy with chain-of-thought prompting but at the cost of increased reasoning complexity. The study provides empirical evidence that LLMs may not naturally exhibit the adaptability humans show when encountering new problem-solving shortcuts.

## Method Summary
The researchers created a dataset of mathematical equivalence problems categorized into two conditions: complex-first (where problems initially require complex solutions) and shortcut-first (where simpler solutions are available from the start). They tested three LLM models (Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, and GPT-4o) under three prompting conditions: direct prompting (DP), few-shot prompting (FS), and few-shot prompting with chain-of-thought reasoning (FS+CoT). The study measured Exact Match (EM) scores to evaluate accuracy and analyzed the number of reasoning steps taken. The experimental design allowed comparison of how different prompting strategies affect both accuracy and reasoning efficiency when LLMs encounter problems requiring strategy adaptation.

## Key Results
- Few-shot prompting with chain-of-thought reasoning improves accuracy across all models (EM scores ranging from 0.33 to 0.83 depending on model and condition)
- FS+CoT significantly increases the number of reasoning steps compared to direct few-shot prompting
- LLMs exhibit mental set behaviors similar to humans, requiring more complex reasoning paths even for simpler problems
- The study demonstrates that LLMs show reduced adaptability when presented with shortcut-first problems compared to complex-first problems

## Why This Works (Mechanism)
The mental set phenomenon occurs when problem solvers become fixated on familiar strategies, preventing them from recognizing simpler solutions. In LLMs, this manifests as a tendency to follow established reasoning patterns learned during training, even when alternative approaches would be more efficient. The chain-of-thought prompting mechanism, while improving accuracy by encouraging step-by-step reasoning, may inadvertently reinforce existing mental sets by providing more opportunities for the model to follow familiar solution paths rather than exploring alternative strategies.

## Foundational Learning
- **Mental Set Theory**: Understanding how cognitive fixation affects problem-solving flexibility - needed to frame LLM behavior in cognitive psychology terms; quick check: can identify examples of functional fixedness in everyday problem-solving
- **Chain-of-Thought Reasoning**: Knowledge of step-by-step reasoning prompting techniques - needed to understand the experimental prompting conditions; quick check: can explain how CoT differs from direct prompting
- **Mathematical Equivalence Problems**: Familiarity with cognitive psychology assessment tools - needed to understand the experimental paradigm; quick check: can solve simple equivalence problems (e.g., 8+4=12, 9+3=?)
- **Evaluation Metrics**: Understanding Exact Match and reasoning efficiency measures - needed to interpret experimental results; quick check: can calculate EM scores from model outputs
- **Prompt Engineering**: Knowledge of how different prompt structures affect LLM behavior - needed to understand the experimental design; quick check: can describe differences between DP, FS, and FS+CoT
- **Cognitive Flexibility**: Understanding adaptability in problem-solving - needed to grasp the core research question; quick check: can explain why flexibility matters in AI reasoning

## Architecture Onboarding

**Component Map:** Mathematical Problems -> LLM Models (Llama-8B, Llama-70B, GPT-4o) -> Prompting Conditions (DP, FS, FS+CoT) -> Evaluation Metrics (EM, Step Count) -> Analysis

**Critical Path:** Problem Design → Model Selection → Prompt Engineering → Performance Evaluation → Behavioral Analysis

**Design Tradeoffs:** The study trades breadth (testing many problem types) for depth (thorough analysis of mental set effects), focusing on mathematical equivalence problems to establish a clear baseline for mental set behavior before expanding to other domains.

**Failure Signatures:** Mental set effects may be masked if problems are too simple, if prompts are too directive, or if evaluation metrics only capture final accuracy without analyzing reasoning paths.

**First Experiments:**
1. Test mental set effects with simpler mathematical problems to establish baseline behavior
2. Compare mental set effects across different reasoning domains (e.g., logical puzzles vs. mathematical problems)
3. Experiment with anti-rigidity prompts designed to explicitly encourage simpler solutions

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can mental set effects be reliably replicated in Vision-Language Models (VLMs) using multimodal versions of the mathematical equivalence problems?
- Basis in paper: [explicit] The authors state: "Future work includes expanding this dataset to test problem solving in VLMs."
- Why unresolved: The current study only evaluates text-only LLMs; VLMs may process and represent problems differently due to visual input channels.
- What evidence would resolve it: Running the same experimental paradigm with VLMs (e.g., GPT-4V, InternVL, Pixtral) and comparing mental set effects to text-only baselines.

### Open Question 2
- Question: Do mental set effects in LLMs generalize to reasoning domains beyond mathematical equivalence problems?
- Basis in paper: [inferred] The study uses only one problem type (mathematical equivalence) with only 6 problems per condition, limiting generalizability claims.
- Why unresolved: Mental set may manifest differently across domains (logical, spatial, commonsense reasoning); the narrow dataset cannot establish cross-domain validity.
- What evidence would resolve it: Replicating the complex-first/shortcut-first paradigm across diverse reasoning benchmarks (e.g., Big-Bench, ARC) with analogous strategy manipulations.

### Open Question 3
- Question: Can prompting strategies be designed to explicitly reduce mental set effects while maintaining accuracy gains from chain-of-thought reasoning?
- Basis in paper: [inferred] The paper shows FS+CoT improves accuracy but increases reasoning steps, suggesting CoT may exacerbate mental set; no mitigation strategy is tested.
- Why unresolved: The trade-off between accuracy and cognitive flexibility remains unaddressed; it is unclear whether the two objectives can be simultaneously optimized.
- What evidence would resolve it: Developing and testing anti-rigidity prompts (e.g., "look for simpler solutions first") and measuring both EM scores and step efficiency.

## Limitations
- The mental set framework may not fully translate to how LLMs process problems differently from human cognition
- The study's reliance on mathematical equivalence problems as the sole evaluation domain raises questions about generalizability to other reasoning tasks
- The sample size and specific problem set construction could influence the observed effects

## Confidence
**High confidence**: The core finding that few-shot prompting with chain-of-thought reasoning improves accuracy while increasing reasoning steps is well-supported by the experimental data.
**Medium confidence**: The assertion that LLMs exhibit mental set behaviors analogous to humans is theoretically plausible but requires additional validation across diverse problem domains.
**Low confidence**: The recommendation for new evaluation metrics beyond accuracy, while reasonable, lacks concrete implementation details or validation.

## Next Checks
1. Replicate the study across diverse reasoning domains (e.g., logical puzzles, commonsense reasoning, multi-step planning) to assess generalizability of mental set effects.
2. Conduct ablation studies varying problem complexity and prompt structure to isolate which aspects of mental set theory most strongly influence LLM behavior.
3. Implement and test alternative evaluation metrics (e.g., reasoning efficiency, solution path diversity) to validate the paper's call for more nuanced assessment approaches.