---
ver: rpa2
title: Distilling Realizable Students from Unrealizable Teachers
arxiv_id: '2505.09546'
source_url: https://arxiv.org/abs/2505.09546
tags:
- student
- teacher
- policy
- state
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses policy distillation under privileged information,
  where a student policy with partial observations learns from a teacher with full-state
  access. The core challenge is information asymmetry, where state aliasing prevents
  the student from resolving conflicting expert actions, leading to policy degradation.
---

# Distilling Realizable Students from Unrealizable Teachers

## Quick Facts
- arXiv ID: 2505.09546
- Source URL: https://arxiv.org/abs/2505.09546
- Reference count: 27
- Key outcome: CritiQ and ReTRy achieve near-perfect success rates on robotic tasks under privileged information, significantly outperforming standard DAgger and RL baselines.

## Executive Summary
This paper addresses policy distillation under privileged information, where a student policy with partial observations learns from a teacher with full-state access. The core challenge is information asymmetry, where state aliasing prevents the student from resolving conflicting expert actions, leading to policy degradation. The authors propose two novel methods: CritiQ, an imitation learning approach that queries the teacher only at critical states to avoid excessive aliasing, and ReTRy, a reinforcement learning method that iteratively refines the reset distribution by rolling out the teacher from student-visited states. Evaluated on robotic tasks in simulation and reality, both methods significantly outperform standard baselines, with ReTRy achieving near-perfect success rates and CritiQ showing strong performance with faster training.

## Method Summary
The paper introduces two complementary approaches to address policy distillation under privileged information. CritiQ is an imitation learning method that selectively queries the teacher only at critical states—those where the student risks entering unrecoverable trajectories—using a discriminator to detect such states and limit aliasing. ReTRy is a reinforcement learning approach that iteratively refines the reset distribution by alternating between rolling out the student from teacher-visited states and rolling out the teacher from student-visited states, enabling efficient exploration under partial observability. Both methods are evaluated on three robotic tasks (Drawer Search, Push Block Search, Navigation Search) in simulation and reality, demonstrating significant improvements over standard DAgger and RL baselines.

## Key Results
- ReTRy achieves near-perfect success rates across all tasks, including 100% on real robot drawer task.
- CritiQ shows strong performance with faster training compared to ReTRy, particularly in early exploration phases.
- Both methods significantly outperform standard baselines (BC, DAgger, SAC) which fail due to state aliasing and exploration challenges.
- Sim-to-real transfer shows CritiQ drops from 78% (sim) to 66% (real) on drawer task, while ReTRy maintains 100%.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: State aliasing causes conflicting supervision that standard imitation learning cannot resolve, degrading policy quality.
- Mechanism: A surjective mapping from privileged teacher state to partial student observation collapses multiple distinct teacher states (with different optimal actions) into identical student observations. When DAgger queries the teacher at all visited states, it aggregates conflicting actions for the same observation: `D(sn) = (sn, c0, a0), (sn, c1, a1), ..., (sn, ck, ak)` [Page 3, III-A.3]. The student cannot observe context `c`, creating inconsistent supervision that makes the imitation loss irreducible beyond the realizability error `δ`.
- Core assumption: The teacher's optimal action depends on privileged context that is fundamentally inaccessible to the student's observation function.
- Evidence anchors:
  - [abstract]: "A key challenge is information asymmetry: the student cannot directly access the teacher's state space, leading to distributional shifts and policy degradation."
  - [section]: "DAgger queries the expert at random states, leading to a dataset that becomes increasingly unrealizable, causing policy divergence" [Page 2, Figure 1].
  - [corpus]: "Driving Beyond Privilege" confirms aliasing challenges when distilling dense-reward simulator knowledge (privileged state) into sparse-reward visual policies (partial observations).
- Break condition: If the student observation function contains sufficient information to disambiguate contexts, aliasing does not occur and standard IL applies.

### Mechanism 2
- Claim: Selective querying at critical states limits aliasing growth while providing recovery data.
- Mechanism: CritiQ uses a discriminator G_φ trained to distinguish teacher vs student trajectories. When `G_φ(s) < κ` (low discriminator score indicates student-like/unvisited states), the teacher is queried. Dataset expansion is limited: `|D_t(s)| ≈ |D_{t-1}(s)|, ∀s ∉ S_critical` [Page 4, III-A.3], preventing the monotonic growth of conflicting labels that plagues DAgger.
- Core assumption: Critical states where the student risks entering unrecoverable trajectories can be reliably detected via a learned discriminator distinguishing teacher from student visitation patterns.
- Evidence anchors:
  - [abstract]: "querying only when necessary and resetting from recovery states—to stay on a recoverable path within its own observation space"
  - [section]: "CritiQ selectively queries the teacher only at critical states—states that the student visits that the expert has not visited" [Page 3, III-A.2].
  - [corpus]: Limited direct relevance; corpus papers focus on LLM distillation data selection rather than state-criticality detection.
- Break condition: If the discriminator misclassifies critical states (overly aggressive queries increase aliasing; overly conservative queries miss necessary corrections), the benefit collapses.

### Mechanism 3
- Claim: Iteratively refining the reset distribution via teacher rollouts from student-visited states enables sample-efficient RL under partial observability.
- Mechanism: ReTRy alternates between (1) rolling out the student from teacher-visited states and (2) rolling out the teacher from student-visited states to expand the reset distribution. This ensures "the density ratio remains bounded" [Page 5, III-B.3]. The Performance Difference Lemma bounds the optimality gap by the expectation over `d_{π_opt}(s)`, so matching the reset distribution to the optimal realizable policy's visitation accelerates learning.
- Core assumption: The teacher's policy, when rolled out from student-visited states, produces recoverable trajectories that expand the support of feasible reset states.
- Evidence anchors:
  - [abstract]: "an RL approach that selects where to initialize training for efficient exploration"
  - [section]: "ReTRy mitigates this issue by iteratively refining the reset distribution" [Page 5, III-B.3].
  - [corpus]: "Driving Beyond Privilege" addresses privileged-to-partial distillation via dense-to-sparse reward shaping; no direct corpus evidence on reset-based refinement.
- Break condition: If teacher rollouts from student states produce unrealizable actions (teacher cannot recover from certain student-visited states), the reset distribution may not converge to the optimal realizable visitation.

## Foundational Learning

- **Contextual Markov Decision Processes (CMDPs):**
  - Why needed here: The problem is formalized as a CMDP where context `c` (privileged information) affects transitions and rewards but is hidden from the student. Understanding this formulation is prerequisite to grasping why standard MDP imitation fails.
  - Quick check question: Can you explain why a CMDP with hidden context reduces to a POMDP from the student's perspective?

- **DAgger (Dataset Aggregation):**
  - Why needed here: DAgger is the baseline that CritiQ improves upon. The paper analyzes how DAgger's performance bound degrades under aliasing: `J(π) ≤ J(π*) + uTε^N + O(1)` becomes unachievable when realizability error δ grows monotonically.
  - Quick check question: In standard DAgger, why does querying the expert at all student-visited states improve performance, and why does this break under state aliasing?

- **Performance Difference Lemma:**
  - Why needed here: ReTRy's theoretical justification relies on this lemma: `J(π^{opt}) - J(π) = (1/(1-γ)) E_{s~d_{π^{opt}}}[A^{π^{opt}}(s,a)]`. Understanding how reset distributions affect this bound is essential.
  - Quick check question: How does resetting the student to states visited by a good policy affect the density ratio and thus the performance bound?

## Architecture Onboarding

- **Component map:**
  - Teacher policy π* -> Trained with full state access (privileged context + environment state)
  - Student policy π -> Operates on partial observations; trained via CritiQ (IL) or ReTRy (RL)
  - Discriminator G_φ (CritiQ only) -> Binary classifier distinguishing teacher vs student trajectories
  - Reset distribution D_π* (ReTRy only) -> Dynamic buffer of states for initializing student rollouts

- **Critical path:**
  1. Train privileged teacher to convergence on full-state MDP
  2. Collect initial teacher demonstrations; initialize student via behavior cloning
  3. **CritiQ path:** Train discriminator → roll out student → query teacher at low-discriminator-score states → update student and discriminator iteratively
  4. **ReTRy path:** Initialize student from teacher-visited states → roll out student → roll out teacher from student-visited states → update reset buffer → train student via policy gradient
  5. Validate and select best checkpoint

- **Design tradeoffs:**
  - CritiQ vs ReTRy: CritiQ trains faster (Figure 5 shows rapid early exploration gains) but is vulnerable to compounding BC errors and discriminator inaccuracies. ReTRy achieves near-perfect success (100% on real robot drawer task) but requires longer training
  - Discriminator threshold κ: Lower threshold → fewer queries → less aliasing but potential under-coverage; higher threshold → more recovery data but increased conflicting labels
  - Teacher optimality: CritiQ assumes near-optimal teacher; ReTRy tolerates suboptimal teachers via RL's direct reward optimization

- **Failure signatures:**
  - DAgger baseline: Rapid performance collapse as dataset accumulates conflicting labels (0% success on drawer/navigation tasks)
  - CritiQ: Performance saturation or regression if discriminator becomes too strong (over-classifying states as teacher-like, missing critical states) or too weak (excessive aliasing)
  - ReTRy: Slow convergence if reset distribution fails to expand sufficiently; may struggle in very large exploration spaces
  - Sim-to-real transfer: CritiQ dropped from 78% (sim) to 66% (real) on drawer task due to dynamics shift; ReTRy maintained 100%

- **First 3 experiments:**
  1. Baseline comparison: Reproduce BC, DAgger, and SAC baselines on Drawer Search (sim) to confirm DAgger's failure mode (conflicting labels) and RL's exploration challenge. Verify reported success rates: BC ~random, DAgger ~0%, SAC ~0% except block-push at 65%
  2. Ablation on discriminator threshold κ: On a single task (e.g., Push Block Search), sweep κ values and plot success rate vs number of teacher queries. Identify the threshold that balances query efficiency and coverage
  3. Reset distribution visualization: On Navigation Search, visualize the reset distribution density over iterations for ReTRy. Confirm that high-probability reset states (dark blue regions in Figure 2) expand from initial teacher states toward student-visited frontier

## Open Questions the Paper Calls Out

- Can principled critical state detection methods be developed that reduce or eliminate reliance on learned discriminators?
- How do CritiQ and ReTRy scale to environments with significantly larger state spaces and longer horizons?
- Can CritiQ and ReTRy be combined into a unified algorithm that leverages both selective querying and adaptive resets?

## Limitations
- The paper assumes privileged information creates surjective mapping to partial observations, limiting generalizability to scenarios where this assumption doesn't hold.
- Performance bounds rely on near-optimal teacher policies, with limited exploration of how methods perform with suboptimal teachers.
- No evaluation on generalization to unseen environments or tasks with shifted privileged context distributions.

## Confidence
- **High confidence:** The core mechanism of state aliasing causing conflicting supervision in standard DAgger (Mechanism 1). The empirical demonstration that DAgger fails while CritiQ and ReTRy succeed is well-supported by the experimental results.
- **Medium confidence:** The theoretical guarantees for CritiQ's bounded aliasing growth and ReTRy's optimal reset distribution refinement. While the arguments are sound, the proofs assume ideal conditions that may not hold in practice.
- **Medium confidence:** The claim that ReTRy's iterative refinement of reset distribution is "sample-efficient" compared to standard RL. The comparison is qualitative rather than quantitative.

## Next Checks
1. **Ablation study on teacher optimality:** Systematically vary the teacher policy quality (from optimal to 50% success rate) and measure CritiQ vs ReTRy performance to quantify their respective robustness to teacher suboptimality.

2. **Generalization stress test:** Evaluate policies on novel environments with shifted privileged context distributions (e.g., new goal configurations in Navigation Search) to measure true generalization capability beyond the training distribution.

3. **Scaling analysis:** Test the methods on higher-dimensional state spaces and longer horizon tasks to determine where the information asymmetry becomes prohibitive and whether the proposed solutions maintain their relative advantages.