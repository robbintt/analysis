---
ver: rpa2
title: 'TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical
  Time Series Classification'
arxiv_id: '2509.17802'
source_url: https://arxiv.org/abs/2509.17802
tags:
- learning
- contrastive
- time
- series
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-subject generalization
  in medical time series classification, which is hindered by significant individual
  variability in physiological signals. The authors propose TS-P2CL, a plug-and-play
  framework that leverages pre-trained vision models by transforming 1D time series
  into 2D pseudo-images.
---

# TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification

## Quick Facts
- **arXiv ID**: 2509.17802
- **Source URL**: https://arxiv.org/abs/2509.17802
- **Reference count**: 31
- **Primary result**: Achieves 91.74% accuracy on sEEG dataset, surpassing semi-supervised and vision-based approaches

## Executive Summary
This paper addresses the challenge of cross-subject generalization in medical time series classification by proposing TS-P$^2$CL, a plug-and-play framework that leverages pre-trained vision models. The method transforms 1D time series into 2D pseudo-images and employs dual contrastive learning: intra-modal consistency ensures temporal coherence across augmented views, while cross-modal alignment connects time series embeddings with visual features from frozen pre-trained vision models. Experiments on six datasets (EEG, ECG, sEEG) demonstrate superior performance compared to fourteen baselines in both subject-dependent and subject-independent settings.

## Method Summary
TS-P$^2$CL converts 1D medical time series into 2D pseudo-images using learnable transformation modules (Conv2D for CNN encoders, Conv1D+Transpose for ViT). The framework employs two contrastive objectives: intra-modal consistency between augmented views of the same series, and cross-modal alignment between time series embeddings and frozen visual features. A progressive joint learning schedule gradually shifts emphasis from contrastive objectives to classification loss. The method is model-agnostic, using pre-trained vision encoders as frozen feature extractors, and achieves strong cross-subject generalization across six medical time series datasets.

## Key Results
- Achieves 91.74% accuracy on sEEG dataset, surpassing semi-supervised and vision-based approaches
- Outperforms fourteen baselines in both subject-dependent and subject-independent settings
- Demonstrates superior cross-subject generalization compared to existing methods
- Shows effectiveness across multiple modalities: EEG, ECG, and sEEG

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Intra-modal contrastive learning enforces temporal coherence across augmented views, yielding stable representations that resist subject-specific noise.
- **Mechanism**: Multi-view positive pairs are constructed via spatial-temporal masking. The NT-Xent loss maximizes similarity between views of the same series while separating them from other samples in the batch, reducing intra-series discrepancy under varying transforms.
- **Core assumption**: Subject-specific variations manifest as noise that can be smoothed out through consistency enforcement across augmented views.
- **Evidence anchors**: [abstract] "intra-modal consistency enforces temporal coherence across augmented views"; [section 3.2] Eq. 3 shows Lintra formulation; [corpus] Limited direct corpus support; related work TFEC (arXiv:2601.07550) uses contrastive learning for time-series but focuses on clustering rather than cross-subject generalization.
- **Break condition**: If augmentation strategy corrupts diagnostically-relevant temporal patterns, consistency enforcement may wash out discriminative features.

### Mechanism 2
- **Claim**: Cross-modal alignment with frozen vision encoders transfers universal visual priors to time-series representations, improving generalization.
- **Mechanism**: 1D signals are transformed into 2D pseudo-images (Conv2D for CNN encoders, overlapping patches for ViT). Time-series prototypes (averaged augmented embeddings) are aligned with frozen visual features via NT-Xent loss, treating matched pairs as positive.
- **Core assumption**: Time series and natural images share structural commonalities—trends as edges, periodicities as textures, discontinuities as abrupt changes—enabling transfer of visual knowledge.
- **Evidence anchors**: [abstract] "cross-modal alignment connects time series embeddings with visual features from frozen pre-trained vision models"; [section 3.3] Eq. 8-9 define cross-modal contrastive loss; [corpus] Robust Multimodal Representation Learning (arXiv:2601.21941) discusses multimodal integration for healthcare but doesn't specifically address time-series-to-vision transfer.
- **Break condition**: If learnable image conversion module fails to preserve temporal structure, alignment will transfer irrelevant visual priors.

### Mechanism 3
- **Claim**: Progressive joint learning with adaptive scheduling harmonizes representation learning with task-specific discrimination.
- **Mechanism**: Total loss L = λ·Ldual + (1−λ)·Lcls where λ(T) = 1 - (T/TMax)². Early epochs prioritize contrastive objectives for robust structure discovery; later epochs shift to classification for fine-grained decision boundaries.
- **Core assumption**: Representations benefit from early structural learning before task-specific refinement, rather than simultaneous optimization.
- **Evidence anchors**: [section 3.4] Eq. 13 defines adaptive schedule; "prioritizes contrastive learning in early stages to build robust representations, gradually shifting focus to classification"; [Figure 4] T-SNE shows Ldual yields clearer class separation than either loss alone; [corpus] No direct corpus evidence for this specific scheduling approach in medical time series.
- **Break condition**: If TMax is poorly calibrated, the schedule may transition too early (undertrained representations) or too late (insufficient classification refinement).

## Foundational Learning

- **Concept: Contrastive Learning (NT-Xent Loss)**
  - **Why needed here**: Core objective for both intra- and cross-modal alignment; understanding positive/negative pair construction is essential.
  - **Quick check question**: Given a batch of N samples with two augmented views each, how many negative pairs does one positive pair compete against?

- **Concept: Frozen Pre-trained Encoders**
  - **Why needed here**: The vision encoder remains frozen; gradients only flow through the time-series encoder and projection heads.
  - **Quick check question**: Which parameters receive gradients during cross-modal alignment—fθI, gϕI, fθX, or gϕX?

- **Concept: Domain Shift in Physiological Signals**
  - **Why needed here**: Cross-subject variability is the central problem; understanding why models trained on one subject fail on another contextualizes the approach.
  - **Quick check question**: Why does subject-independent evaluation split by subject rather than randomly sampling trials?

## Architecture Onboarding

- **Component map**: Input → augmentation (two views) → TS encoder → projection → [Lintra between views] + [prototype averaging] → pseudo-image conversion → frozen vision encoder → cross-modal projection → [Linter alignment] → progressive loss combination

- **Critical path**: Input → augmentation (two views) → TS encoder → projection → [Lintra between views] + [prototype averaging] → pseudo-image conversion → frozen vision encoder → cross-modal projection → [Linter alignment] → progressive loss combination

- **Design tradeoffs**:
  - Vision encoder choice: ViT offers stronger semantic priors but higher memory; VanillaNet is lightweight but may transfer less knowledge.
  - Conversion strategy: Learnable preserves temporal structure; rule-based (spectrogram, GAF) risks distortion but requires no training.
  - Frozen vs. fine-tuned vision encoder: Frozen enables plug-and-play and efficiency; fine-tuning could improve alignment at cost of overfitting risk.

- **Failure signatures**:
  - High Lintra but low Linter: Augmentation working but vision alignment failing—check pseudo-image quality.
  - Subject-independent much worse than subject-dependent: Contrastive objectives may be overfitting to seen subjects; increase augmentation diversity.
  - Early-epoch classification collapse: λ schedule may be transitioning too quickly; verify TMax setting.

- **First 3 experiments**:
  1. **Sanity check**: Train with Lintra only on single subject; verify positive pairs converge faster than negatives diverge. Check T-SNE for intra-subject clustering.
  2. **Conversion ablation**: Compare learnable Conv2D conversion vs. rule-based spectrogram on PTB dataset (smaller, faster iteration). Expect >2% gap per Fig. 3.
  3. **Cross-subject probe**: Train on P1–P4, test on P5 (sEEG). If accuracy <60%, inspect which subjects P5 is most similar to (Table 6 shows P5→P1 is 78.30%) and verify data preprocessing matches training distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the framework be adapted to maintain high performance when transferring between subjects with extreme distributional divergence (e.g., the P4 subject pairings)?
- **Basis in paper**: [explicit] Page 9 notes the method "faces limitations under extreme cross-subject variability" and Table 6 shows accuracy drops to ~26-50% for specific dissimilar pairs.
- **Why unresolved**: The current alignment strategy fails to bridge the gap when physiological signals differ significantly in morphology or noise profiles.
- **What evidence would resolve it**: A modified alignment mechanism that stabilizes pairwise transfer accuracy above 80% for outlier subjects.

### Open Question 2
- **Question**: Does the semantic assumption that time series trends manifest as visual "edges" and "textures" hold for non-cyclical or chaotic medical signals?
- **Basis in paper**: [inferred] Page 2 posits that "trends manifest as edges... periodicities resemble textures," but experiments are limited to EEG, ECG, and sEEG.
- **Why unresolved**: The "vision-guided paradigm" relies on structural similarity to natural images, which may not exist for non-waveform physiological data.
- **What evidence would resolve it**: Successful application to non-rhythmic medical time series (e.g., irregular spasms) without architectural modification.

### Open Question 3
- **Question**: To what degree does freezing the vision encoder limit the model's ability to learn domain-specific pathological features compared to full fine-tuning?
- **Basis in paper**: [inferred] Page 6 emphasizes the vision encoder is "kept frozen" for efficiency, but does not benchmark this against an unfrozen baseline.
- **Why unresolved**: It is unclear if the "plug-and-play" efficiency comes at the cost of maximum achievable accuracy.
- **What evidence would resolve it**: An ablation study comparing the performance of frozen versus end-to-end fine-tuned vision encoders.

## Limitations
- Architecture specification gap: Unclear TS encoder implementation details for raw 1D inputs
- Limited ablation completeness: Missing comparisons to other frozen-encoder approaches and semi-supervised methods without contrastive learning
- Cross-subject generalizability constraints: Subject-independent evaluation may not capture all forms of individual variability

## Confidence
- **High Confidence**: The dual contrastive learning mechanism and progressive joint learning schedule are well-specified and theoretically sound.
- **Medium Confidence**: Reported performance improvements are credible but reproducibility is hampered by unspecified implementation details.
- **Low Confidence**: Claims about computational efficiency and plug-and-play nature are difficult to verify without exact architecture details.

## Next Checks
1. **Architecture Clarification Check**: Implement the TS encoder using the exact vision backbone architectures specified (VanillaNet, ViT-Tiny, EfficientNet-B0) and verify that the 1D→2D conversion maintains temporal structure. Compare learned Conv2D conversion outputs against rule-based methods (spectrogram, GAF) to quantify information preservation.

2. **Subject-Independent Stress Test**: Systematically evaluate cross-subject generalization by training on extreme subsets (P1 only, P2 only) and testing on all other subjects. Measure the correlation between training subject similarity and test performance to identify which types of individual variability remain problematic.

3. **Contrastive Loss Ablation**: Remove either the intra-modal or cross-modal contrastive objective while keeping all else constant. Measure the degradation in subject-independent accuracy and analyze which loss component contributes more to cross-subject generalization.