---
ver: rpa2
title: 'Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via
  Pose-Free Hierarchical Memory'
arxiv_id: '2602.02393'
source_url: https://arxiv.org/abs/2602.02393
tags:
- memory
- world
- arxiv
- interactive
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Infinite-World introduces a pose-free hierarchical memory compressor
  that enables stable simulation over 1000+ frames by recursively distilling historical
  latents into a fixed memory budget. The method jointly optimizes the compressor
  with a DiT backbone, eliminating the need for explicit camera poses.
---

# Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory

## Quick Facts
- **arXiv ID**: 2602.02393
- **Source URL**: https://arxiv.org/abs/2602.02393
- **Reference count**: 14
- **Key outcome**: Achieves ELO rating 1719 on 100-scenario benchmark, with memory consistency 1.92 and visual fidelity 1.67, enabling stable simulation over 1000+ frames via pose-free hierarchical memory

## Executive Summary
Infinite-World introduces a novel pose-free hierarchical memory compressor that enables stable simulation over 1000+ frames by recursively distilling historical latents into a fixed memory budget. The method eliminates the need for explicit camera poses through joint optimization with a DiT backbone, while an uncertainty-aware action labeling module discretizes continuous motion into a tri-state logic to shield action-response learning from noisy pose estimations. A revisit-dense finetuning strategy using a compact dataset activates long-range loop-closure capabilities.

The system demonstrates state-of-the-art performance on a benchmark of 100 diverse scenarios with handcrafted 16-chunk trajectories, achieving dominant ELO rating of 1719. It maintains global landmark preservation and accurate loop-closure where baselines fail, showing robust long-range memory and action controllability in real-world environments. The hierarchical memory architecture and joint optimization framework are technically sound and address known limitations of existing world models.

## Method Summary
Infinite-World employs a pose-free hierarchical memory compressor that recursively distills historical latents into a fixed memory budget, enabling stable simulation over 1000+ frames without requiring explicit camera poses. The system jointly optimizes the compressor with a DiT backbone, eliminating pose estimation dependencies. An uncertainty-aware action labeling module discretizes continuous motion into a tri-state logic (forward, backward, neutral) to protect action-response learning from noisy pose data. The approach includes a revisit-dense finetuning strategy using a compact dataset to activate long-range loop-closure capabilities, with evaluation on a benchmark of 100 diverse scenarios using handcrafted 16-chunk trajectories.

## Key Results
- Achieves ELO rating of 1719 on 100-scenario benchmark, outperforming existing world models
- Demonstrates memory consistency score of 1.92 and visual fidelity score of 1.67
- Maintains global landmark preservation and accurate loop-closure in real-world environments where baselines fail

## Why This Works (Mechanism)
The hierarchical memory architecture enables efficient compression and retrieval of long-term historical information through recursive latent distillation, maintaining a fixed memory budget while preserving essential spatial and temporal relationships. The joint optimization of the compressor with the DiT backbone eliminates explicit pose estimation requirements, removing a major source of error propagation in long-horizon simulations. The uncertainty-aware action labeling module protects the action-response learning from noisy pose estimations by discretizing continuous motion into robust tri-state categories, while the revisit-dense finetuning strategy specifically activates long-range loop-closure capabilities through targeted training on compact datasets.

## Foundational Learning
- **Hierarchical Memory Compression**: Recursive latent distillation into fixed memory budget - needed for efficient long-term information retention; quick check: memory usage remains constant while storing 1000+ frames
- **Pose-Free World Modeling**: Joint optimization without explicit camera poses - eliminates error propagation from pose estimation; quick check: stable simulation without pose inputs
- **Uncertainty-Aware Action Labeling**: Tri-state discretization of continuous motion - shields learning from noisy pose data; quick check: consistent action-response mapping under varying pose quality
- **Loop-Closure Activation**: Revisit-dense finetuning strategy - enables long-range spatial consistency; quick check: accurate landmark preservation across trajectories
- **Fixed Memory Budget Management**: Constant memory usage regardless of sequence length - ensures scalability; quick check: memory consumption measured at 100, 500, and 1000 frames

## Architecture Onboarding
**Component Map**: Pose-Free DiT Backbone -> Hierarchical Memory Compressor -> Uncertainty-Aware Action Labeling -> Revisit-Dense Finetuning

**Critical Path**: Input frames → DiT backbone → Hierarchical memory compression → Action response generation → Output frames, with memory compression and retrieval occurring at each timestep

**Design Tradeoffs**: Pose-free operation eliminates explicit geometric constraints for robustness but removes potential geometric accuracy benefits; hierarchical compression balances memory efficiency with information retention; uncertainty-aware discretization trades motion granularity for learning stability

**Failure Signatures**: Memory corruption leading to inconsistent landmark preservation, action-response mismatches from ambiguous motion states, loop-closure failures in revisit-dense scenarios, degradation under extreme pose noise despite uncertainty shielding

**First Experiments**: 1) Validate hierarchical memory stability across 100-1000 frame sequences with varying content complexity, 2) Test uncertainty-aware action labeling robustness under systematic pose noise degradation, 3) Evaluate loop-closure accuracy in synthetic environments with controlled revisit patterns

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Claims of long-horizon stability are based on 16-chunk sequences, with limited empirical validation beyond this benchmark horizon
- Performance generalizability to open-world scenarios with arbitrary user interactions remains untested
- Scalability assertions to 1000+ frames require validation beyond current benchmark scope and controlled evaluation conditions

## Confidence
**High**: Hierarchical memory architecture and joint optimization framework are technically sound and address known limitations of existing world models. Reported benchmark performance and comparative advantages over baselines are well-supported by presented metrics.

**Medium**: Claims regarding pose-free operation maintaining accuracy and effectiveness of uncertainty-aware action labeling in shielding from noisy pose estimations are supported by internal metrics but would benefit from additional ablation studies across varied noise conditions.

**Low**: Scalability assertions to 1000+ frames and real-world deployment readiness require validation beyond current benchmark scope and controlled evaluation conditions.

## Next Checks
1. Test memory stability and loop-closure accuracy on trajectories exceeding 16 chunks with varying lengths up to 1000+ frames in both synthetic and real-world environments
2. Evaluate performance under systematic degradation of input pose quality to quantify robustness of uncertainty-aware action labeling module
3. Deploy system in open-world interactive scenario with arbitrary user-initiated trajectories to assess generalization beyond handcrafted paths