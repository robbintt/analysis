---
ver: rpa2
title: 'Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic
  Health Records'
arxiv_id: '2504.20547'
source_url: https://arxiv.org/abs/2504.20547
tags:
- data
- patient
- text
- arxiv
- mimic-iv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study revisited the MIMIC-IV benchmark for electronic health
  records (EHRs) to improve its accessibility and applicability for text-based natural
  language models. The authors integrated MIMIC-IV data into the Hugging Face datasets
  library and proposed a template-based approach to convert tabular EHR data into
  textual format.
---

# Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records

## Quick Facts
- arXiv ID: 2504.20547
- Source URL: https://arxiv.org/abs/2504.20547
- Reference count: 0
- Fine-tuned text-based models achieved AU-ROC 0.87-0.88 for mortality classification, competitive with tabular classifiers

## Executive Summary
This study revisited the MIMIC-IV benchmark for electronic health records (EHRs) to improve its accessibility and applicability for text-based natural language models. The authors integrated MIMIC-IV data into the Hugging Face datasets library and proposed a template-based approach to convert tabular EHR data into textual format. Experiments on patient mortality classification showed that fine-tuned text-based models achieved competitive performance compared to robust tabular classifiers, with AU-ROC scores between 0.87 and 0.88. However, zero-shot large language models struggled to effectively leverage EHR representations, particularly when prompts were modified. The results demonstrate the potential of text-based approaches in the medical domain while highlighting the need for better methods to encode structured EHR data for large language models.

## Method Summary
The authors converted MIMIC-IV tabular EHR data to text using template-based verbalization, then evaluated fine-tuned transformer models and zero-shot LLMs on mortality classification. They tested multiple feature subsets (demographics, diagnoses, chart events, medications, procedures, outputs) and model architectures (BERT, RoBERTa, DistilBERT, BioClinicalBERT, BioBERT, BiomedNLP for fine-tuning; Llama 2 13B, Meditron 7B for zero-shot). Models were evaluated using AU-ROC and AU-PRC metrics with 5-fold cross-validation for classical ML baselines and oversampling for class imbalance.

## Key Results
- Fine-tuned text-based models achieved AU-ROC 0.87-0.88, comparable to tabular Gradient Boosting (0.86) and XGBoost (0.85-0.86)
- Zero-shot LLMs showed poor performance with AU-ROC 0.50-0.61, highly sensitive to prompt formatting
- Diagnosis codes (COND feature) provided the strongest predictive signal, with additional features showing diminishing returns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Template-based conversion of tabular EHR data to text enables standard transformer models to achieve competitive performance with specialized tabular classifiers.
- Mechanism: Structured clinical features are serialized into natural language using fixed templates. This allows pre-trained language models to leverage their textual representations without architectural modifications or domain-specific pre-training.
- Core assumption: The semantic information in structured EHR fields can be adequately preserved through textual verbalization, and transformer attention mechanisms can learn relevant patterns from this format.
- Evidence anchors:
  - [abstract] "we investigate the application of templates to convert EHR tabular data to text... fine-tuned text-based models are competitive against robust tabular classifiers"
  - [section 4.3, Table 4] Fine-tuned models achieved AU-ROC 0.87-0.88, comparable to tabular Gradient Boosting (0.86) and XGBoost (0.85-0.86)
  - [corpus] Related work on foundation models for EHRs confirms ongoing interest in representation learning from structured data, but corpus evidence for template-specific approaches is limited
- Break condition: Template-based verbalization may fail when temporal patterns across multiple time windows are critical, as aggregation may lose discriminative signal.

### Mechanism 2
- Claim: Diagnosis codes (COND feature) provide the strongest predictive signal for mortality classification, with additional features showing diminishing or negative returns.
- Mechanism: ICD-coded diagnoses capture expert-validated clinical assessments that implicitly encode severity and prognosis. When serialized to text via ICD descriptions, these become high-semantic-density tokens that transformer models can effectively weight.
- Core assumption: ICD code descriptions carry sufficient clinical meaning in text form, and models can generalize from diagnosis patterns seen during pre-training.
- Evidence anchors:
  - [section 4.3, Table 6] Ablation study shows COND alone achieves AU-ROC 0.75 (BERT), while adding MEDS, PROC, and OUTE did not improve best combinations
  - [section 4.2] "1,034 values among 1,110 from the vector representation are sparse as they are dedicated to the diagnosis representation"
  - [corpus] Weak direct evidence; neighbor papers focus on multimodal fusion rather than feature-level ablation
- Break condition: This mechanism may not generalize to tasks where diagnoses are less predictive or when ICD coding quality is inconsistent.

### Mechanism 3
- Claim: Zero-shot LLMs fail to reliably encode EHR text representations for downstream tasks, with performance highly sensitive to prompt formatting.
- Mechanism: Without task-specific fine-tuning, LLMs lack calibrated priors for clinical prediction from structured-to-text inputs. Minor prompt modifications cause large shifts in model attention and output validity.
- Core assumption: The failure stems from representation misalignment rather than model capacity—LLMs may encode relevant information but cannot reliably map it to binary outcomes without supervision.
- Evidence anchors:
  - [section 4.3, Table 4] Zero-shot Llama 2 achieved AU-ROC 0.50 (random baseline) with prompt P2; Meditron 0.61 with P1
  - [section 4.3, Table 5] Llama 2 left 69.37% of samples unanswered with prompt P2 vs 3.30% with P1
  - [corpus] Neighbor paper "Foundation models for electronic health records" discusses transferability challenges, supporting representation alignment issues
- Break condition: Performance may improve with in-context learning, chain-of-thought prompting, or instruction tuning on clinical tasks.

## Foundational Learning

- Concept: **Template-based Data-to-Text Transformation**
  - Why needed here: Converting structured records to text is the core enabling technique; understanding template design tradeoffs is prerequisite to improving the approach.
  - Quick check question: Given a patient with HR=85, Temp=37.2°C, and diagnosis "pneumonia," write a minimal template that preserves clinical semantics.

- Concept: **AU-ROC vs AU-PRC for Imbalanced Clinical Data**
  - Why needed here: Mortality prediction is inherently imbalanced; AU-ROC showed convergence (0.87-0.88) while AU-PRC revealed model differences (0.42-0.47).
  - Quick check question: If mortality rate is 10%, why might AU-ROC show 0.88 while AU-PRC shows only 0.45?

- Concept: **Fine-tuning vs Zero-Shot Transfer**
  - Why needed here: The paper's central finding is that fine-tuning enables competitive performance while zero-shot fails; understanding this gap informs practical deployment.
  - Quick check question: What minimal dataset size would you estimate is needed for effective fine-tuning based on the reported results?

## Architecture Onboarding

- Component map: MIMIC-IV raw data -> preprocessing (Gupta et al. pipeline) -> tabular representation -> template-based text conversion -> Hugging Face datasets object -> fine-tuned transformers or zero-shot LLMs -> mortality prediction

- Critical path:
  1. Access MIMIC-IV via PhysioNet credentialing
  2. Load Hugging Face dataset (`thbndi/Mimic4Dataset`) with user-provided raw data path
  3. Select representation (tabular vs text) and feature subset
  4. For fine-tuning: set learning rate=5e-5, epochs=3, max tokens=512, with oversampling
  5. For zero-shot: test multiple prompts, expect 0-70% valid response rates

- Design tradeoffs:
  - **Representation 1 vs Representation 2**: 2,766 features (concatenated windows) vs 1,110 features (aggregated)—similar performance, but Representation 2 is more efficient
  - **Token limit**: 512 tokens truncates MEDS/PROC/OUTE for some patients; 1024 tokens for zero-shot models
  - **Medical vs general models**: Medical domain models show better AU-PRC and more stable zero-shot behavior, but general models (RoBERTa) achieve comparable AU-ROC

- Failure signatures:
  - Zero-shot LLMs returning invalid responses (not "yes"/"no")—especially Llama 2 with complex prompts
  - Performance degradation when adding MEDS, PROC, OUTE features (indicates template inadequacy)
  - Large AU-ROC / AU-PRC gap suggests poor calibration on minority class (mortality)

- First 3 experiments:
  1. **Baseline replication**: Load tabular representation, train Gradient Boosting with default parameters, verify AU-ROC ≈ 0.86 on mortality task
  2. **Template ablation**: Fine-tune BERT with COND-only text vs full-template text, compare AU-PRC to quantify marginal feature contributions
  3. **Prompt robustness test**: Run zero-shot Meditron with 5 prompt variations, measure response validity rate and AU-ROC variance to assess prompt sensitivity

## Open Questions the Paper Calls Out

- **Template Enhancement for Clinical Features**: Can more sophisticated template designs or structured prompting strategies improve the integration of medication, procedure, and output features that showed limited contribution in the current approach? The ablation study showed that "MEDS, PROC, and OUTE... do not improve previous combinations. This indicates that more elaborated templates are worth investigating to integrate these features."

- **Zero-Shot LLM Optimization**: Would in-context learning or prompt-tuning techniques significantly improve zero-shot LLM performance on EHR-based mortality prediction compared to the standard prompting approach tested? The authors state that findings "motivate further research and experimentation by applying alternative techniques such as in-context learning or prompt-tuning."

- **Token Truncation Impact**: How does the 512-token truncation threshold affect model performance, particularly given that it removes information from MEDS, PROC, and OUTE features for some patients? The paper notes truncation "at times removed relevant information related to MEDS, PROC, and OUTE features" but does not quantify this impact.

## Limitations
- **Data Splitting and Reproducibility**: Train/validation/test split ratios and random seeds are not specified, creating uncertainty about result reproducibility.
- **Oversampling Technique**: The specific oversampling method used to address class imbalance is not specified, limiting generalizability.
- **Prompt Engineering Instability**: Zero-shot LLM performance showed extreme sensitivity to prompt formatting, suggesting reported results may not generalize beyond tested templates.

## Confidence
- **High Confidence**: The core finding that fine-tuned text-based models achieve competitive performance with tabular classifiers (AU-ROC 0.87-0.88) is well-supported by ablation studies and multiple model comparisons.
- **Medium Confidence**: The claim that diagnosis codes provide the strongest predictive signal is supported by ablation results, but the mechanism may not generalize to other clinical tasks.
- **Low Confidence**: The zero-shot LLM results are highly uncertain due to prompt sensitivity and the lack of in-context learning or instruction tuning exploration.

## Next Checks
1. **Reproduce baseline results**: Train Gradient Boosting on the tabular representation with the same train/validation/test split and oversampling parameters to verify AU-ROC ≈ 0.86 on mortality task, establishing a solid baseline for comparison.

2. **Test prompt robustness systematically**: Evaluate zero-shot LLMs with a structured prompt ablation study (5+ variations) while measuring both performance metrics and response validity rates to quantify the relationship between prompt engineering and reliability.

3. **Validate temporal representation**: Implement the proposed Representation 2 (aggregated windows) and compare its performance against the original concatenated windows approach across multiple clinical tasks to assess whether efficiency gains compromise predictive capability.