---
ver: rpa2
title: 'Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency
  in LLMs'
arxiv_id: '2507.14649'
source_url: https://arxiv.org/abs/2507.14649
tags:
- cleanse
- similarity
- uncertainty
- arxiv
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in large language
  models (LLMs), where models generate inaccurate or factually incorrect responses.
  The authors propose Cleanse, a novel uncertainty estimation approach that quantifies
  semantic consistency by leveraging clustering of LLM hidden embeddings.
---

# Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs

## Quick Facts
- arXiv ID: 2507.14649
- Source URL: https://arxiv.org/abs/2507.14649
- Authors: Minsuh Joo; Hyunsoo Cho
- Reference count: 10
- Primary result: Clustering-based semantic consistency outperforms lexical similarity for hallucination detection

## Executive Summary
This paper introduces Cleanse, a novel uncertainty estimation approach for detecting hallucinations in large language models (LLMs). The method leverages clustering of hidden embeddings to quantify semantic consistency across multiple LLM outputs, treating inter-cluster similarity as a penalty term. By computing the proportion of intra-cluster similarity in total similarity, Cleanse provides interpretable uncertainty scores that consistently outperform existing methods like perplexity and semantic entropy on QA benchmarks. Experiments demonstrate significant AUROC improvements, particularly under stricter correctness thresholds, making it effective for deployment across different model architectures.

## Method Summary
Cleanse generates K outputs per question, extracts last-token hidden embeddings from the middle layer of the LLM, and clusters them using bi-directional NLI entailment. The method computes a consistency score by dividing the sum of intra-cluster cosine similarities by the total cosine similarity across all pairs. This score quantifies uncertainty, with lower scores indicating higher uncertainty and potential hallucinations. The approach requires access to internal hidden states (white-box method) and uses a lightweight NLI model for clustering, making it computationally more expensive than black-box alternatives but providing superior detection performance.

## Key Results
- Cleanse achieves 0.878-0.926 AUROC on LLaMA models, significantly outperforming perplexity baselines
- The method shows 15-20% relative improvement over lexical similarity baselines, especially on Mistral-7B
- Higher Rouge-L thresholds (0.9 vs 0.7) improve Cleanse's relative performance by 1-5% AUROC
- DeBERTa-v3-base often outperforms larger NLI models for clustering, suggesting efficiency benefits

## Why This Works (Mechanism)

### Mechanism 1: Semantic Density as a Consistency Proxy
The relative density of hidden-state embeddings within semantic clusters serves as a more reliable signal for uncertainty than aggregate lexical similarity. High uncertainty manifests as high inter-cluster similarity (semantic drift), which acts as a penalty against intra-cluster similarity (consistency).

### Mechanism 2: Bi-directional Entailment for Cluster Purity
Enforcing bi-directional entailment (A→B and B→A) creates "purer" semantic clusters than unidirectional NLI or lexical overlap. This strict criterion ensures cosine similarity calculations occur only between vectors that are semantically nearly identical, reducing noise in the consistency score.

### Mechanism 3: Mid-Layer Hidden State Geometry
Extracting embeddings from the middle layer of the LLM captures the "semantic information" necessary for consistency checks better than the final output layer. The middle layer represents a "semantic sweet spot" before probabilities collapse into specific tokens.

## Foundational Learning

- **Bi-directional Entailment**: The gatekeeper for the clustering mechanism. Quick check: If Answer A entails B, but B does not entail A, would Algorithm 1 place them in the same cluster? (Answer: No)
- **White-box vs. Black-box Uncertainty**: Cleanse is explicitly a "white-box" method requiring hidden states. Quick check: Can you deploy Cleanse to monitor a model served via a commercial API that does not expose embeddings? (Answer: No)
- **AUROC (Area Under the ROC Curve)**: The paper relies heavily on AUROC to prove Cleanse detects hallucinations better than baselines. Quick check: If a hallucination detector randomly guesses, what is the expected AUROC? (Answer: 0.5)

## Architecture Onboarding

- **Component map**: Sampler -> Clusterer -> Scorer
- **Critical path**: The Clusterer is the computational bottleneck outside the LLM. The choice of NLI model directly impacts the "purity" of clusters.
- **Design tradeoffs**: Using stricter Rouge-L threshold (0.9) improves relative performance but may reduce training data. Smaller NLI models may generalize better than larger, potentially overfit ones.
- **Failure signatures**: High Intra-cluster Score on Wrong Answers indicates "confidently wrong" consistent hallucination. Singleton Clusters result in maximum uncertainty.
- **First 3 experiments**:
  1. Baseline Comparison: Replicate Table 1 on 100 SQuAD samples comparing Cleanse vs. Perplexity
  2. Layer Sensitivity Ablation: Extract embeddings from early, middle, and late layers to verify middle layer optimality
  3. Cluster Model Ablation: Swap NLI clusterer to observe impact on "Cluster Difference" metric

## Open Questions the Paper Calls Out

### Open Question 1
Can Cleanse maintain its effectiveness when applied to black-box LLMs by substituting internal hidden embeddings with external sentence embeddings? The current methodology relies on extracting specific hidden states from the middle layer, and it's unverified whether external embeddings capture sufficient semantic nuance.

### Open Question 2
How does Cleanse perform on open-ended generation tasks (e.g., abstractive summarization or dialogue) compared to the strict QA tasks evaluated? The evaluation relies on Rouge-L as a correctness measure, which is ill-suited for open-ended tasks.

### Open Question 3
Is the heuristic of selecting the "middle layer" for hidden embeddings optimal for semantic consistency across different LLM architectures? Different model architectures may store semantic information in different layers, and the middle layer assumption may introduce suboptimal results.

## Limitations

- Method is limited to white-box models requiring access to internal hidden states
- Computational overhead is higher than black-box methods due to multiple generations and embedding extraction
- Performance on non-QA generation tasks and open-ended domains remains unverified
- Assumes mid-layer embeddings consistently capture semantic information across different architectures

## Confidence

**High Confidence:**
- Cleanse outperforms baseline uncertainty methods on AUROC metrics for tested models and datasets
- Bi-directional NLI clustering approach is effective at grouping semantically equivalent responses
- Higher Rouge-L thresholds improve relative performance

**Medium Confidence:**
- Middle layer consistently captures optimal semantic information across different model architectures
- Method generalizes to model families beyond those tested
- Computational overhead is acceptable for practical deployment

**Low Confidence:**
- Cleanse performs equally well on non-QA generation tasks
- Method works effectively on models with fundamentally different architectures
- Clustering approach remains effective with very large K values (>20)

## Next Checks

1. Run an ablation study across different layers (early, middle, late) for each model to verify middle layer optimality and compare AUROC scores.
2. Test Cleanse on a GPT-family model and a smaller model to assess performance across the architectural spectrum and measure effectiveness scaling.
3. Compare performance using different NLI models (e.g., RoBERTa, DeBERTa variants) to quantify sensitivity to the clustering component.