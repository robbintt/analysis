---
ver: rpa2
title: 'CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization'
arxiv_id: '2507.06181'
source_url: https://arxiv.org/abs/2507.06181
tags:
- mathematical
- lean
- arxiv
- problem
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CriticLean, a novel critic-guided reinforcement
  learning framework for mathematical formalization that elevates the critic model
  from passive validator to active learning component. The framework introduces CriticLeanGPT,
  trained via supervised fine-tuning and reinforcement learning to rigorously assess
  semantic fidelity of Lean 4 formalizations, and CriticLeanBench, a benchmark measuring
  models' ability to distinguish correct from incorrect formalizations.
---

# CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization

## Quick Facts
- arXiv ID: 2507.06181
- Source URL: https://arxiv.org/abs/2507.06181
- Authors: Zhongyuan Peng, Yifan Yao, Kaijing Ma, Shuyue Guo, Yichi Zhang, Chenchen Zhang, Yifan Zhang, Zhouliang Yu, Luming Li, Minghao Liu, Yihang Xia, Jiawei Shen, Yuchen Wu, Yixin Cao, Zhaoxiang Zhang, Wenhao Huang, Jiaheng Liu, Ge Zhang
- Reference count: 40
- Key outcome: 84% accuracy in mathematical formalization pipeline using critic-guided RL

## Executive Summary
CriticLean introduces a novel framework for mathematical formalization that elevates the critic model from passive validator to active learning component. The system trains CriticLeanGPT to rigorously assess semantic fidelity of Lean 4 formalizations, distinguishing correct from incorrect translations of natural language problems. By combining supervised fine-tuning with reinforcement learning, the framework achieves significant improvements over traditional compiler-only feedback approaches. The resulting FineLeanCorpus dataset contains over 285K verified Lean 4 statements across diverse mathematical domains.

## Method Summary
The method involves training CriticLeanGPT through supervised fine-tuning on 48K samples from CriticLeanInstruct, followed by reinforcement learning using GRPO on 4K seed data with accuracy and format rewards. The framework builds CriticLeanBench (500 NL-Lean pairs, 250 correct/250 incorrect) for evaluation. The autoformalization pipeline uses iterative generation with compiler feedback and critic validation, attempting up to 200 times per problem. The critic is trained to identify semantic errors beyond syntax validation, enabling regeneration of flawed formalizations rather than simple rejection.

## Key Results
- CriticLeanGPT models significantly outperform strong baselines on CriticLeanBench, with Qwen3-32B-RL achieving 87.0% accuracy
- The autoformalization pipeline achieves 84% accuracy through compiler feedback and critic validation
- Multi-attempt regeneration recovers substantial failed formalizations, increasing success from 12.6% (attempt 1) to 52.8% (attempt 200)
- Human evaluation shows accuracy progression: 38% (single pass) → 54% (compiler feedback) → 84% (full CriticLean pipeline)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a semantic critic to the formalization loop improves correctness beyond compiler-only feedback
- Mechanism: The Lean compiler validates syntax but passes incorrect formalizations that compile yet misrepresent meaning. CriticLeanGPT evaluates semantic fidelity and rejects mismatches, triggering regeneration
- Core assumption: The base formalizer can generate correct candidates within retry budget given better rejection signals
- Evidence anchors: 38% → 54% → 84% accuracy progression across evaluation stages; compiler-only misses semantic errors
- Break condition: If base formalizer cannot generate correct candidates within retry limit, critic only filters without improving yield

### Mechanism 2
- Claim: Reinforcement learning improves critic's discriminative ability over supervised fine-tuning alone
- Mechanism: SFT teaches critique patterns, then RL (via GRPO) optimizes directly for prediction accuracy using rule-based rewards for correctness and format
- Core assumption: Ground-truth labels are accurate and reward signal is sufficiently dense to guide policy improvement
- Evidence anchors: Qwen3-32B-RL achieves 87.0% accuracy vs. Qwen2.5-32B-Instruct-SFT at 76.2%; RL variants consistently outperform SFT-only counterparts
- Break condition: If reward hacking occurs or label noise is high, RL gains degrade

### Mechanism 3
- Claim: Multi-attempt regeneration with critic gating recovers substantial portion of initially failed formalizations
- Mechanism: Single-pass system accepts only first-attempt successes, while CriticLean pipeline retries up to 200 times, accumulating correct formalizations that succeed on later attempts
- Core assumption: Errors are non-deterministic; sampling variability produces correct candidates across attempts
- Evidence anchors: 12.6% success on attempt 1 → 52.8% cumulative success by attempt 200; iterative loop diagram shows generate → compile → critic → accept or retry
- Break condition: If formalizer's error modes are systematic rather than stochastic, retries yield diminishing returns

## Foundational Learning

- Concept: **Lean 4 type system and syntax**
  - Why needed here: CriticLeanGPT must distinguish syntactic errors (caught by compiler) from semantic errors (its responsibility). Understanding Lean's type constraints, quantifier scoping, and Mathlib conventions is prerequisite to meaningful critique
  - Quick check question: Given a Lean statement using `Fin n` for bounded naturals, can you identify whether a quantifier range is off-by-one?

- Concept: **Semantic fidelity vs. syntactic validity**
  - Why needed here: The core innovation is evaluating whether formalization means the same as natural language problem, not just whether it compiles. This requires mapping informal mathematical intent to formal structures
  - Quick check question: A Lean theorem compiles but proves existence while original problem asks for specific value—is this a syntactic or semantic error?

- Concept: **GRPO / rule-based RL for LLMs**
  - Why needed here: The paper uses Group Relative Policy Optimization with discrete rewards. Understanding advantage estimation, clipping, and KL regularization is necessary to reproduce or extend training pipeline
  - Quick check question: Why does reward function use `min(r_accuracy, r_format)` rather than weighted sum?

## Architecture Onboarding

- Component map:
  Autoformalizer (Kimina-Autoformalizer-7B or similar) → Lean 4 Compiler → CriticLeanGPT → Iteration Controller (manages up to 200 attempts)

- Critical path:
  1. Natural language statement → Autoformalizer → candidate Lean code
  2. Candidate → Lean compiler → if fails, regenerate with error feedback
  3. If compiles → CriticLeanGPT → if rejected, regenerate with critique feedback
  4. If accepted → add to FineLeanCorpus

- Design tradeoffs:
  - Retry budget (200) vs. latency: Higher budget improves yield but increases inference cost; Table 8 shows diminishing returns after ~100 attempts
  - Critic model size: Larger critics (32B) achieve higher accuracy but are slower; smaller critics (7B-14B) may be sufficient for high-throughput filtering with higher false-positive rates
  - SFT data composition: 1:3 critic-to-general math/code ratio improves generalization but dilutes task-specific signal

- Failure signatures:
  - High false-negative rate: Critic rejects correct formalizations; check if critic was trained on data with different distribution than deployment
  - Stuck in retry loop: Neither compiler nor critic accepts; indicates base formalizer lacks capability for problem class
  - Reward hacking in RL: Model outputs correct format without genuine reasoning; inspect chain-of-thought for shallow justifications

- First 3 experiments:
  1. Baseline comparison: Run single-pass formalization vs. compiler-only loop vs. full CriticLean pipeline on held-out 100-problem set; measure accuracy via human evaluation
  2. Critic ablation: Train critics with SFT-only vs. SFT+RL; evaluate on CriticLeanBench to isolate RL contribution
  3. Retry budget sweep: Vary max attempts (1, 5, 20, 50, 100, 200) and plot cumulative success rate to identify optimal latency-accuracy tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on high-capacity base formalizers (Kimina-Autoformalizer-7B) and large critic models (Qwen2.5-32B/3-32B) that may not be accessible to all researchers
- Human-labeled correctness judgments for semantic fidelity introduce potential subjectivity and scalability constraints
- Semantic errors remain dominant failure mode at 24.9%, suggesting fundamental limitations in current autoformalization capabilities
- 200-attempt retry budget may be computationally prohibitive for many applications

## Confidence
- **High confidence**: Optimizing critic phase is essential for reliable formalizations (38% → 54% → 84% accuracy progression)
- **Medium confidence**: RL improvement over SFT alone (87.0% vs 76.2% on CriticLeanBench) but lacks ablation studies on semantic understanding vs format compliance
- **Medium confidence**: Multi-attempt regeneration recovers substantial failures (12.6% → 52.8%) but doesn't analyze whether later attempts represent genuine capability improvements or stochastic variation

## Next Checks
1. **Human validation scalability**: Test whether CriticLeanBench methodology (250 correct/250 incorrect pairs) can be scaled to 2,500 pairs while maintaining labeling consistency and inter-annotator agreement above 85% for semantic judgments
2. **Cross-model transferability**: Evaluate whether critics trained on Qwen2.5/Qwen3 generalize to different base formalizers (e.g., GPT-4, Claude) by measuring performance drop when applied to different autoformalizer outputs
3. **Error type decomposition analysis**: Perform systematic analysis of 47.2% failure rate at 200 attempts, categorizing failures by semantic error type and identifying whether these represent fundamental limitations or training data gaps