---
ver: rpa2
title: An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved
  LLM Agents
arxiv_id: '2505.15117'
source_url: https://arxiv.org/abs/2505.15117
tags:
- search
- training
- reward
- arxiv
- engine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper empirically studies reinforcement learning for training
  large language models to reason and interact with search engines. The authors examine
  three key design factors: reward formulation, the underlying LLM backbone, and the
  choice of search engine.'
---

# An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents

## Quick Facts
- arXiv ID: 2505.15117
- Source URL: https://arxiv.org/abs/2505.15117
- Reference count: 40
- Key outcome: Format rewards significantly improve base LLM performance in reasoning-search tasks, with general-purpose models outperforming reasoning-specialized ones and stronger search engines producing more stable training

## Executive Summary
This paper empirically studies reinforcement learning for training large language models to reason and interact with search engines. The authors examine three key design factors: reward formulation, the underlying LLM backbone, and the choice of search engine. They find that format rewards improve performance, particularly when training from a base LLM, while intermediate retrieval rewards have limited impact. General-purpose LLMs outperform reasoning-specialized ones in RL settings, likely due to better instruction-following early in training, and larger models show consistent but diminishing performance gains. The quality of the search engine used during training strongly affects RL dynamics—stronger engines lead to more stable training and efficient search behavior, while weaker engines result in suboptimal outcomes. At inference time, agents trained with one search engine are generally robust to others, with stronger retrieval systems yielding better performance. These insights offer practical guidance for developing effective and reliable LLM-based search agents.

## Method Summary
The authors conduct empirical studies on reinforcement learning for reasoning-search interleaved LLM agents using Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO). They train agents on Natural Questions and HotpotQA datasets, examining the effects of format rewards, intermediate retrieval rewards, model backbone types (base vs. instruct, general-purpose vs. reasoning-specialized), model scale (3B, 7B, 14B parameters), and search engine quality (BM25, E5-HNSW, E5-exact, random). The agents use an XML-based action format with <search></search> and <information></information> tags. Performance is measured using Bamboogle scores (exact match on answer passages and final answers) and average scores across multiple metrics. The study includes comprehensive ablation studies and cross-inference experiments with different search engines.

## Key Results
- Format rewards (λf = 0.2-0.4) significantly improve base LLM performance from 0.303 to 0.389 average score, while having minimal impact on instruct models
- General-purpose LLMs (e.g., Qwen2.5-Base) outperform reasoning-specialized LLMs (e.g., DeepSeek-R1-Distill) in RL settings due to better instruction-following at early training stages
- Stronger search engines during training (E5-exact) produce agents with more efficient search behavior (1.5-2 calls vs. 3+ for BM25), leading to more stable training dynamics
- Agents trained with one search engine are generally robust to others at inference time, with stronger retrieval systems yielding better performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Format rewards significantly improve final performance for base LLMs by enabling proper search engine invocation.
- **Mechanism:** The format reward (λf = 0.2–0.4) provides dense auxiliary signals that teach base LLMs the correct XML-style action format (`<search></search>`, `<information></information>`). This scaffolding is critical because base models lack instruction-following capabilities to learn format purely from sparse outcome rewards.
- **Core assumption:** The model can credit-assign format compliance to successful rollouts despite the temporal gap between format adherence and final outcome.
- **Evidence anchors:**
  - [abstract] "format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact"
  - [Section 4.1, Table 1] Base models with format reward improve from 0.303 to 0.389 avg score (3B PPO); instruct models show smaller gains
  - [corpus] R-Search (arXiv:2506.04185) confirms multi-reward RL improves reasoning-search but doesn't isolate format effects; limited corpus evidence directly validates format reward necessity
- **Break condition:** If λf > 0.6, models overfit to format at the expense of reasoning quality; if λf < 0.1, format reward provides insufficient signal for base models.

### Mechanism 2
- **Claim:** General-purpose LLMs outperform reasoning-specialized LLMs as search agent backbones due to superior instruction-following at early training stages.
- **Mechanism:** Reasoning-specialized models (e.g., DeepSeek-R1-Distill) have been optimized for internal chain-of-thought, which creates a distributional shift that impedes learning the new `<search>` token schema. General-purpose models (e.g., Qwen2.5-Base) have more flexible policy distributions that adapt faster to the agentic action space.
- **Core assumption:** The reasoning-specialized models' prior training doesn't transfer to interleaved reasoning-search patterns; the bottleneck is format learning, not reasoning capacity.
- **Evidence anchors:**
  - [abstract] "general-purpose LLMs outperform reasoning-specialized LLMs due to better instruction-following"
  - [Section 5.1, Table 3] Qwen2.5-7B-Base (PPO: 0.434 avg) vs. DeepSeek-R1-Distill-Qwen-7B (PPO: 0.344, GRPO collapsed to 0.100)
  - [corpus] No corpus papers directly compare general vs. reasoning-specialized backbones for search agents; this finding appears novel to this work
- **Break condition:** If the task required no format learning (pure reasoning), reasoning-specialized models would likely excel; if training iterations were extended 5-10×, reasoning models might eventually converge.

### Mechanism 3
- **Claim:** Stronger search engines during training produce agents with more efficient search behavior and better final performance.
- **Mechanism:** High-quality retrievers (E5 exact) return relevant passages early, creating positive reward feedback loops that reinforce strategic search. Weak retrievers (BM25, random) either provide irrelevant results (causing agents to abandon search) or require many compensatory queries (increasing training variance).
- **Core assumption:** The search engine quality is stable during training; the agent doesn't overfit to specific retrieval patterns that won't generalize.
- **Evidence anchors:**
  - [Section 6.1, Figure 3c] Agents trained with random noise make ~0 search calls; BM25 agents make 3+ calls; E5 agents make 1.5-2 strategic calls
  - [Section 6.2, Table 5] Agents generalize across retrievers at inference; Google Search consistently improves performance regardless of training retriever
  - [corpus] R-Search confirms retrieval quality matters but focuses on multi-reward design rather than retriever quality effects
- **Break condition:** If the training search engine is too strong (oracle-like), the agent may not learn robust query formulation skills needed for weaker inference engines.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) vs. Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper uses both algorithms; PPO is more stable for reasoning-specialized models, GRPO is more compute-efficient but can collapse on difficult initialization.
  - Quick check question: Can you explain why GRPO removes the need for a learned value function and when this causes instability?

- **Concept: Credit assignment in interleaved reasoning-search**
  - Why needed here: The reward is only given at episode end; the model must attribute success to specific search queries made 5-10 turns earlier.
  - Quick check question: If a model makes 3 searches and gets the right answer, how does it learn which search was most responsible?

- **Concept: Dense vs. sparse reward signals**
  - Why needed here: Format rewards are dense (every turn), intermediate retrieval rewards are turn-level but noisy, outcome rewards are sparse (episode-end only).
  - Quick check question: Why might dense intermediate retrieval rewards (λr) hurt performance even though they seem helpful?

## Architecture Onboarding

- **Component map:**
  User Query → LLM Policy (πθ) → <reasoning> → <search>query</search>
                                        ↓
  Search Engine R → Top-k passages → <information>context</information>
                                        ↓
  LLM Policy (iterates) → <answer>final</answer> → Reward Computation
                                        ↓
  PPO/GRPO Update → New Policy

- **Critical path:**
  1. Format validation (`is_valid_sequence`) must pass before search is triggered
  2. Retrieval must return at least one relevant passage for positive reward signal
  3. Outcome reward (EM) must be non-zero for learning to propagate

- **Design tradeoffs:**
  - **Base vs. Instruct initialization:** Base requires format reward (λf = 0.2-0.4); Instruct converges faster but may have lower peak performance
  - **Retriever strength during training:** Strong (E5 exact) → stable training, efficient search; Weak (BM25) → noisier training, more calls; Random → agent learns to avoid search entirely
  - **λf tuning:** Must balance format learning vs. outcome optimization; too high causes overfitting to structure

- **Failure signatures:**
  - Training collapse (reward drops sharply): GRPO with reasoning-specialized models, or λf > 0.6
  - Zero search calls: Format reward too low with base model, or retriever returns noise
  - Excessive search calls (>5 per query): Weak retriever with high λr

- **First 3 experiments:**
  1. **Ablate format reward:** Train Qwen2.5-7B-Base with λf ∈ {0.0, 0.2, 0.4, 0.6} on NQ+HotpotQA; measure convergence speed and final Bamboogle score
  2. **Compare retriever quality:** Train with Random/BM25/E5-HNSW/E5-Exact; log search call frequency and training stability (reward variance)
  3. **Test backbone type:** Train Qwen2.5-7B-Base vs. DeepSeek-R1-Distill-Qwen-7B with identical hyperparameters (PPO, λf=0.2); track early-stage format compliance rate (first 50 steps)

## Open Questions the Paper Calls Out
None

## Limitations
- The format reward findings may be specific to NQ+HotpotQA datasets and XML-based action schema
- The comparison between general-purpose and reasoning-specialized LLMs uses only DeepSeek-R1-Distill-Qwen-7B as the reasoning model
- Search engine analysis focuses on single retrievers rather than diverse search paradigms
- Cross-retriever generalization tests are limited to three retrievers

## Confidence
- **High Confidence:** Format rewards improve base LLM performance (clear quantitative improvement and ablation study across model sizes)
- **Medium Confidence:** General-purpose LLMs outperform reasoning-specialized ones (single comparison, but mechanism explanation is plausible)
- **Medium Confidence:** Search engine quality affects training dynamics (observed behavioral differences, but limited to three retrievers)
- **Medium Confidence:** Cross-retriever generalization at inference (table results, but test set is small)

## Next Checks
1. **Dataset Generalization:** Replicate format reward experiments on multiple datasets (MultiHopQA, WebQuestions, TriviaQA) with different answer formats to verify mechanism extends beyond current data
2. **Broader Reasoning Model Comparison:** Compare Qwen2.5-Base against at least three reasoning-specialized models (DeepSeek-R1, o1, and Gemini-Thinking) using identical training protocols
3. **Search Engine Diversity:** Train agents with hybrid search (BM25 + semantic), reranker-based search, and multi-stage search systems to determine if quality-stability relationship holds across different retrieval architectures