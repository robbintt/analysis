---
ver: rpa2
title: 'Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine Approach'
arxiv_id: '2506.13328'
source_url: https://arxiv.org/abs/2506.13328
tags:
- numerical
- table
- mentions
- pairs
- cofitcheck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of automated numerical cross-checking
  across tables in disclosure documents, where numerical consistency is critical for
  accuracy and credibility. The proposed CoFiTCheck framework tackles two main challenges:
  managing the combinatorial explosion of candidate instances at the document level
  (C1) and comprehending multi-faceted numerical semantics (C2).'
---

# Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine Approach

## Quick Facts
- arXiv ID: 2506.13328
- Source URL: https://arxiv.org/abs/2506.13328
- Authors: Chaoxu Pang; Yixuan Cao; Ganbin Zhou; Hongwei Li; Ping Luo
- Reference count: 40
- Key result: ~90% F1 score for document-level numerical cross-checking, ~10 points above prior methods

## Executive Summary
This paper introduces CoFiTCheck, a coarse-to-fine approach for automated numerical cross-checking across tables in disclosure documents. The method addresses the combinatorial explosion of candidate numerical mention pairs at the document level and the challenge of comprehending multi-faceted numerical semantics. By employing a two-stage process—embedding-based filtering followed by discriminative classification—CoFiTCheck achieves high accuracy while maintaining practical efficiency. The framework leverages contextualized instructional parallel encoding and a decoupled InfoNCE objective for efficient representation, combined with a specialized LLM enhanced by cross-table numerical alignment pretraining.

## Method Summary
CoFiTCheck tackles document-level numerical semantic matching through a two-stage coarse-to-fine methodology. The first stage uses an Embedding LLM (EmbLLM) with contextualized instructional parallel encoding (CIPE) and a decoupled InfoNCE objective to efficiently represent and filter numerical mentions. This reduces the candidate pairs from a potential combinatorial explosion to a manageable set. The second stage employs a Classification LLM (ClsLLM) with cross-table numerical alignment pretraining (CNAP) to perform fine-grained discriminative classification on the filtered pairs. The method is evaluated on three Chinese financial document datasets (270/762/200 documents) using set-level precision, recall, and F1 as metrics, achieving approximately 90% F1 score and processing documents in just 40.8 seconds using four NVIDIA GeForce RTX 4090 GPUs.

## Key Results
- Achieves ~90% F1 score for document-level numerical cross-checking, surpassing prior methods by ~10 points.
- Reduces computational cost by filtering out 90% of candidate pairs before fine-grained classification.
- Processes documents in 40.8 seconds using four NVIDIA GeForce RTX 4090 GPUs, demonstrating practical efficiency.

## Why This Works (Mechanism)
The coarse-to-fine approach effectively addresses the dual challenges of combinatorial explosion and multi-faceted numerical semantics. By first filtering candidates using efficient embedding-based methods, the framework avoids the computational burden of pairwise comparison at the document level. The decoupled InfoNCE loss improves embedding quality by separately optimizing for positive and negative pair relationships, leading to better recall. The subsequent discriminative classification stage, enhanced by CNAP pretraining, captures the nuanced numerical semantics required for accurate matching. This two-stage architecture balances efficiency and accuracy, making large-scale cross-checking feasible.

## Foundational Learning
- **Contextualized Instructional Parallel Encoding (CIPE)**: A technique for encoding numerical mentions with attention masking and positional reset to preserve table context while enabling efficient parallel processing. Needed to capture both numerical values and their semantic context within tables. Quick check: Verify that attention masks are correctly applied per equation and that positional embeddings are reset as specified.
- **Decoupled InfoNCE Objective**: A contrastive learning loss that separately optimizes for positive and negative pairs, improving embedding quality and recall. Needed to handle the imbalanced nature of numerical mention pairs (many non-matches, few matches). Quick check: Confirm that both Li (positive) and L2 (negative) components are included and weighted correctly.
- **Cross-Table Numerical Alignment Pretraining (CNAP)**: A pretraining strategy that orders tables by numerical alignment to enhance the LLM's ability to capture numerical semantics across tables. Needed to provide the classifier with strong prior knowledge of numerical relationships. Quick check: Ensure the table ordering algorithm (Algorithm 1) is correctly implemented and that pretraining uses the specified hyperparameters.
- **Set-level Evaluation Metrics**: Precision, recall, and F1 calculated at the document level to assess the model's ability to capture all relevant numerical pairs. Needed because exact matching is less important than capturing the full set of semantically equivalent pairs. Quick check: Verify that the evaluation script computes metrics over the full set of predicted pairs per document.

## Architecture Onboarding

**Component Map:**
EmbLLM (Qwen2.5-0.5B-Instruct) -> Filter Candidates -> ClsLLM (Qwen2.5-0.5B–7B-Instruct) -> Final Predictions

**Critical Path:**
1. Extract numerical mentions and contexts from tables in disclosure documents.
2. Encode mentions using EmbLLM with CIPE and decoupled InfoNCE loss.
3. Filter candidates using embedding similarity (threshold 0.5).
4. Classify filtered pairs using ClsLLM with CNAP pretraining.
5. Evaluate set-level precision, recall, and F1.

**Design Tradeoffs:**
- Coarse-to-fine vs. end-to-end: The two-stage approach trades some potential end-to-end accuracy for significant gains in efficiency and scalability.
- Embedding-based filtering vs. direct classification: Filtering reduces computational cost but risks losing some true positives; threshold tuning is critical.
- Decoupled InfoNCE vs. standard contrastive loss: Separately optimizing positive and negative pairs improves recall but adds complexity.

**Failure Signatures:**
- Low recall: May indicate overly aggressive filtering in Stage 1 (threshold too high) or insufficient pretraining in Stage 2.
- High computational cost: Could result from insufficient filtering (threshold too low) or inefficient embedding encoding.
- Poor generalization: May stem from overfitting to Chinese financial documents or insufficient diversity in pretraining data.

**First Experiments:**
1. Verify that the decoupled InfoNCE loss is correctly implemented with both Li and L2 components and that the weights (α1=0.75, α2=0.25) are applied as specified.
2. Test the embedding filtering stage with varying thresholds (0.4, 0.5, 0.6) to find the optimal balance between recall and efficiency.
3. Evaluate the impact of CNAP pretraining by comparing ClsLLM performance with and without pretraining on a held-out validation set.

## Open Questions the Paper Calls Out
None

## Limitations
- The method is specialized for Chinese financial documents (IPO prospectuses, auditor's reports, annual reports) and may not generalize well to other languages or domains without further adaptation.
- The performance depends heavily on the quality and quantity of annotated data; the lack of public datasets limits reproducibility and community validation.
- The computational efficiency, while impressive, still requires multiple high-end GPUs (four RTX 4090s), which may not be accessible to all practitioners.

## Confidence
- **High confidence** in the overall methodology design and the reported performance gains relative to baselines, given the detailed ablation studies.
- **Medium confidence** in the absolute performance numbers due to the absence of public datasets and exact prompts.
- **Low confidence** in generalizability beyond Chinese financial documents without further validation.

## Next Checks
1. Request and publicly release the annotated datasets (or at least detailed annotation guidelines) to enable community verification and extension.
2. Publish the exact prompt templates and CIPE instructions to ensure faithful reproduction of the embedding and classification stages.
3. Conduct experiments on at least one non-Chinese or non-financial dataset to assess domain and language generalizability.