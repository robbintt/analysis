---
ver: rpa2
title: 'MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis
  and Risk Prediction'
arxiv_id: '2510.26151'
source_url: https://arxiv.org/abs/2510.26151
tags:
- breast
- data
- cancer
- learning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MV-MLM, a Multi-View Vision-Language Contrastive
  Learning model designed for breast cancer detection and risk prediction using mammography
  images. The model addresses the challenge of limited paired mammogram-report datasets
  by aligning high-resolution mammograms with synthetic text reports generated from
  structured tabular annotations.
---

# MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction

## Quick Facts
- arXiv ID: 2510.26151
- Source URL: https://arxiv.org/abs/2510.26151
- Reference count: 40
- Multi-view vision-language model achieves SOTA AUC scores (0.7649 mass, 0.9393 calcification, 0.7406 malignancy) on mammography datasets

## Executive Summary
This paper introduces MV-MLM, a Multi-View Vision-Language Contrastive Learning model designed for breast cancer detection and risk prediction using mammography images. The model addresses the challenge of limited paired mammogram-report datasets by aligning high-resolution mammograms with synthetic text reports generated from structured tabular annotations. Using contrastive learning, MV-MLM learns robust representations from multi-view mammography data and synthetic reports, enabling effective training without actual clinical text reports.

The model was evaluated on multiple downstream tasks: malignancy classification, mass and calcification classification, and breast cancer risk prediction. Results show state-of-the-art performance across public datasets (VinDr-Mammo and RSNA-Mammo) and internal data. For example, MV-MLM achieved AUC scores of 0.7649 for mass classification, 0.9393 for calcification classification, and 0.7406 for malignancy classification, outperforming existing fully supervised and CLIP-based baselines. The model also demonstrated strong data efficiency, achieving high performance even with limited training data. Additionally, MV-MLM improved breast cancer risk prediction, with a C-index of 0.73 and 2-year and 5-year AUC scores of 0.76 and 0.69, respectively. The approach highlights the potential of synthetic report generation and multi-view contrastive learning for medical imaging applications with limited annotated data.

## Method Summary
MV-MLM generates synthetic radiology reports from structured tabular metadata (BI-RADS, mass size, calcification type, etc.) using LLaMa-3-7B-instruct, then encodes these reports with frozen BioClinicalBERT. The model aligns high-resolution mammogram images (CC/MLO views) with their synthetic reports through dual contrastive losses: L_I,I for multi-view alignment and L_I,T for image-text alignment. Both image and text modalities are tokenized to 256 tokens, processed by 4-layer transformers (8 heads), and globally pooled. The architecture uses ResNet-34 or EfficientNet-B5 as image encoders, with training on 540,000 mammography images from 134,500 exams, achieving SOTA performance on downstream classification and risk prediction tasks.

## Key Results
- MV-MLM achieves AUC scores of 0.7649 (mass), 0.9393 (calcification), and 0.7406 (malignancy) classification on public datasets
- The model demonstrates strong data efficiency, maintaining high performance even with limited training data
- Breast cancer risk prediction shows C-index of 0.73 with 2-year/5-year AUC scores of 0.76/0.69
- Outperforms CLIP-based and fully supervised baselines across all evaluation metrics and data regimes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic reports generated from tabular metadata via LLMs provide sufficient semantic supervision for vision-language pre-training, eliminating the need for paired clinical reports.
- **Mechanism:** The model extracts a subset C of structured annotations (BI-RADS, mass size, calcification type, etc.), prompts LLaMa-3-7B-instruct with prefix/suffix templates, and post-processes the output to create pseudo-reports. These reports are then encoded by a frozen BioClinicalBERT and aligned with image embeddings via contrastive loss L_I,T, forcing the vision encoder to learn features that correspond to clinical semantics.
- **Core assumption:** The tabular metadata contains the clinically relevant information that would appear in real reports, and LLM-generated prose preserves sufficient semantic structure for contrastive alignment.
- **Evidence anchors:**
  - [abstract]: "MV-MLM...trained on a dataset of paired mammogram images and synthetic radiology reports"
  - [section 3.1]: "We definex T i,lat as the post-processed text generated by a large language model (LLM)f LLM (·)"
  - [corpus]: Related work GLAM confirms "development of a foundation visual language model (VLM) is hindered by limited data and domain differences"—MV-MLM's synthetic approach directly addresses this bottleneck.
- **Break condition:** If tabular annotations are sparse, noisy, or missing critical clinical descriptors (e.g., subtle texture features radiologists note but don't codify), the synthetic reports will fail to guide the vision encoder toward diagnostically relevant features.

### Mechanism 2
- **Claim:** Multi-view contrastive learning between CC and MLO views enforces view-invariant feature learning, improving robustness and generalization.
- **Mechanism:** The loss L_I,I pulls embeddings from different views of the same breast laterality together while pushing apart views from different patients. Since both views capture the same underlying pathology from different angles, the network learns features visible across perspectives while suppressing view-specific noise and artifacts.
- **Core assumption:** Clinically relevant findings (masses, calcifications, malignancy indicators) manifest consistently across CC and MLO views, while artifacts and noise do not.
- **Evidence anchors:**
  - [section 3.3]: "L I,I is capable of learning crucial visual attributes that are visible from both views. This objective forces the network to focus on fine-grained constant information between different views."
  - [Table 1]: MV-CLIP consistently outperforms (Custom-)CLIP in linear probing across all data regimes (e.g., 0.6941 vs 0.6797 for mass LP@100% with EN.B5), showing improved generalization.
  - [corpus]: Mammo-Mamba paper notes "Accurate and efficient interpretation of multi-view mammograms is essential for early detection"—corroborating multi-view's clinical importance.
- **Break condition:** If pathologies are only visible in one view (e.g., asymmetries noted in section 3.1 footnote), L_I,I may incorrectly penalize the model for representing legitimate single-view findings.

### Mechanism 3
- **Claim:** Feature tokenization with transformer modules on CNN feature maps improves representation quality for high-resolution mammograms while maintaining computational tractability.
- **Mechanism:** CNN feature maps (C×H×W) are reshaped to pseudo-tokens, projected to 256 tokens via linear layer g_I, processed through 4 transformer blocks with 8 attention heads, and globally pooled. This preserves spatial relationships better than direct pooling while reducing the token count from H·W to N_intermediate.
- **Core assumption:** The transformer's self-attention can capture clinically relevant spatial relationships between image regions that direct pooling would lose.
- **Evidence anchors:**
  - [Table 1]: MV-CLIP+Tr outperforms MV-CLIP without transformer (0.7083 vs 0.6941 for mass LP@10%, 0.8614 vs 0.8514 for FT)
  - [section 3.2]: "Both encoders are also parameterized byθ I andθ T...leveraging a global max pooling layer gets the embedding representations"
  - [corpus]: Weak/missing—no direct comparison in neighboring papers.
- **Break condition:** If the projection to 256 tokens discards too much spatial resolution for subtle findings (microcalcifications at pixel level), the transformer cannot recover the lost information.

## Foundational Learning

- **Concept: Contrastive Learning (CLIP-style)**
  - Why needed here: The entire training framework is built on L_I,T and L_I,I losses that require understanding how softmax-scaled cosine similarity with temperature creates embedding spaces where similar pairs cluster and dissimilar pairs separate.
  - Quick check question: Given a batch of 4 image-text pairs, can you explain why the loss pushes the paired image embedding toward its text while pushing it away from the other 3 texts?

- **Concept: Vision Transformers and Tokenization**
  - Why needed here: The "+Tr" variant uses transformer blocks on tokenized feature maps; understanding how self-attention operates on sequences of tokens (not raw pixels) is essential for debugging the feature aggregation module.
  - Quick check question: If a feature map has shape 64×48×64 (C×H×W), how many pseudo-tokens result before linear projection, and why does the paper project to only 256?

- **Concept: Multi-view Medical Imaging**
  - Why needed here: The paper assumes CC and MLO views share pathology but differ in perspective; understanding why both views are acquired clinically (CC for medial-lateral coverage, MLO for superior coverage including axilla) clarifies what the model should learn.
  - Quick check question: Why would a mass visible in MLO potentially be missed in CC, and how would L_I,I handle this?

## Architecture Onboarding

- **Component map:**
Mammogram (CC/MLO) → Image Encoder (ResNet-34/EN.B5) → Feature Map (C×H×W)
                                                              ↓
                              Linear Projection → 256 Visual Tokens
                                                              ↓
                              Transformer (4 blocks, 8 heads) → z^I
                                                              ↓
Tabular Metadata → LLM → Pseudo Report → BioClinicalBERT (frozen) → Text Tokens
                                                              ↓
                              Linear Projection → 256 Text Tokens
                                                              ↓
                              Transformer (4 blocks, 8 heads) → z^T
                                                              ↓
                    L_I,T(z^I, z^T) + L_I,I(z^I_MLO, z^I_CC) → Training

- **Critical path:** The vision encoder output → tokenization quality is the highest-leverage component. If the projection to 256 tokens loses spatial precision, neither the transformer nor contrastive losses can recover it.

- **Design tradeoffs:**
  - Resolution (1520×912) vs batch size: High resolution requires smaller batches (18 per device for EN.B5), limiting contrastive pair diversity.
  - Frozen vs trainable text encoder: Freezing BioClinicalBERT saves computation but may limit domain adaptation to mammography-specific language.
  - Synthetic vs real reports: Synthetic enables scaling but may miss nuanced clinical descriptors radiologists document.

- **Failure signatures:**
  - L_I,T converging but downstream task performance near random: Text supervision is too weak/noisy; inspect pseudo-report quality.
  - L_I,I significantly lower than L_I,T early in training: Multi-view alignment is easier than image-text; may indicate views are too similar (data leakage) or text is poorly structured.
  - Linear probing performs well but fine-tuning degrades: Over-regularized representations; consider reducing weight decay or increasing learning rate for fine-tuning.

- **First 3 experiments:**
  1. **Ablate synthetic report quality:** Train with tabular-only (no LLM prose) vs LLM-generated reports to isolate the contribution of linguistic structure vs raw metadata.
  2. **Vary token count:** Test 128, 256, 512 visual tokens to find the resolution-efficiency tradeoff, especially for calcification detection which requires fine detail.
  3. **Cross-dataset generalization:** Evaluate linear probing on VinDr after training only on proprietary data (no VinDr pre-training) to test true transferability claims.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MV-MLM framework be effectively adapted to 3D imaging modalities like Digital Breast Tomosynthesis (DBT) or MRI, where slice-alignment and volumetric data differ significantly from 2D mammography?
- **Basis in paper:** [explicit] The conclusion states, "Future work includes extending our approach to other imaging modalities..."
- **Why unresolved:** The current architecture is optimized for 2D multi-view alignment (CC/MLO) using specific tokenization and contrastive losses; extending this to volumetric data introduces computational and spatial alignment challenges not addressed by the current method.
- **What evidence would resolve it:** Successful implementation and evaluation of the MV-MLM pre-training strategy on DBT or MRI datasets, demonstrating comparable data efficiency and classification performance to the 2D mammography results.

### Open Question 2
- **Question:** Does the multi-view contrastive objective, which assumes clinical findings are constant across views, limit the detection of abnormalities visible in only a single view (e.g., asymmetries)?
- **Basis in paper:** [inferred] The Method section states the model assumes "clinical findings and annotations are constant across views," explicitly excluding view-level annotations like asymmetries (see Footnote 1) to simplify the learning objective.
- **Why unresolved:** Forcing the embeddings of CC and MLO views to align via contrastive loss might suppress unique features present in only one view, potentially reducing sensitivity to view-specific pathologies.
- **What evidence would resolve it:** A targeted evaluation on a dataset with annotated single-view pathologies to compare the false negative rate of MV-MLM against single-view baselines.

### Open Question 3
- **Question:** Does the stochastic variability of LLM-generated synthetic reports provide a distinct regularization advantage over deterministic, template-based synthetic reports?
- **Basis in paper:** [inferred] The authors state, "We hypothesized that simple, noisy text supervision without an exact structured form would work well," but they do not compare the LLM-based generation against a non-stochastic baseline.
- **Why unresolved:** It is unclear if the natural language variation generated by the LLM acts as beneficial text augmentation or if the inherent noise dilutes the semantic signal required for precise contrastive learning.
- **What evidence would resolve it:** An ablation study comparing the vision encoder's downstream performance when trained with LLM-generated reports versus fixed, rule-based textual templates derived from the same metadata.

### Open Question 4
- **Question:** How can the interpretability of the learned representations be enhanced for modalities with limited annotated data without relying on costly ground-truth spatial annotations?
- **Basis in paper:** [explicit] The conclusion lists "enhancing interpretability for modalities with limited annotated data" as a specific direction for future work.
- **Why unresolved:** While the model aligns text and image features, current interpretability methods (like those in Mammo-CLIP) often rely on sentence-level alignment or bounding boxes, which are unavailable in the low-data regimes MV-MLM targets.
- **What evidence would resolve it:** The development of a zero-shot localization method that generates heatmaps from synthetic text descriptions, validated via qualitative review by radiologists or correlation with existing atlases.

## Limitations

- The model's performance relies on synthetic reports from tabular metadata, which may not capture nuanced clinical descriptors radiologists document in real reports
- Multi-view contrastive loss assumes pathologies are consistently visible across CC and MLO views, potentially missing single-view findings like asymmetries
- The proprietary pre-training data lacks detailed clinical validation metrics for synthetic report generation quality and patient demographic information

## Confidence

**High confidence**: The architectural framework combining multi-view contrastive learning with synthetic vision-language supervision is technically sound and the empirical results on public datasets (VinDr-Mammo, RSNA-Mammo) are reproducible given the specified hyperparameters and evaluation protocols.

**Medium confidence**: The claim that synthetic reports from tabular metadata provide sufficient supervision for clinical-grade feature learning. While ablation studies support this, direct comparison with real clinical reports is absent, and the quality of synthetic reports depends heavily on LLM prompt design and post-processing.

**Low confidence**: The model's generalizability to diverse clinical settings without domain-specific fine-tuning. The pre-training dataset composition and patient demographics are not disclosed, and performance on rare pathologies or atypical presentations remains unevaluated.

## Next Checks

1. **Direct comparison with real clinical reports**: Train an identical MV-MLM architecture using a subset of the proprietary dataset that has paired real clinical reports (if available) versus synthetic reports, measuring the gap in downstream task performance to quantify the cost of synthetic supervision.

2. **Token count sensitivity analysis**: Systematically evaluate MV-CLIP performance with varying visual token counts (128, 256, 512, 1024) on calcification classification, where pixel-level detail is critical, to determine if the 256-token bottleneck limits detection of microcalcifications.

3. **Cross-institutional generalization test**: Evaluate the publicly released MV-CLIP model on an independent mammography dataset from a different healthcare system or country (e.g., Chinese or European datasets) without additional fine-tuning to assess true domain transferability beyond the VinDr and RSNA benchmarks.