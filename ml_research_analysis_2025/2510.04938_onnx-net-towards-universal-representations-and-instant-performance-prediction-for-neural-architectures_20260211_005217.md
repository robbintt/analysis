---
ver: rpa2
title: 'ONNX-Net: Towards Universal Representations and Instant Performance Prediction
  for Neural Architectures'
arxiv_id: '2510.04938'
source_url: https://arxiv.org/abs/2510.04938
tags:
- search
- neural
- spaces
- architecture
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of neural architecture search
  (NAS) performance prediction across diverse search spaces. The authors propose ONNX-Net,
  a universal text-based encoding method for neural architectures using ONNX computational
  graphs, enabling performance prediction through large language models (LLMs).
---

# ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures

## Quick Facts
- **arXiv ID:** 2510.04938
- **Source URL:** https://arxiv.org/abs/2510.04938
- **Reference count:** 16
- **Primary result:** Zero-shot transfer from NAS-Bench-101 to NAS-Bench-201 achieving Spearman's ρ=0.747

## Executive Summary
This paper addresses the challenge of neural architecture search (NAS) performance prediction across diverse search spaces. The authors propose ONNX-Net, a universal text-based encoding method for neural architectures using ONNX computational graphs, enabling performance prediction through large language models (LLMs). They also introduce ONNX-Bench, a benchmark dataset comprising over 600,000 architecture-accuracy pairs from multiple search spaces unified in ONNX format. Experiments demonstrate that ONNX-Net achieves strong zero-shot transferability across disparate search spaces with minimal pretraining samples, outperforming previous methods while requiring fewer training samples.

## Method Summary
ONNX-Net converts neural architectures into ONNX computational graphs, which are then serialized into text strings representing operations, inputs, and parameters. This text encoding allows a single surrogate model (ModernBERT) to process architectures from different search spaces that would otherwise be incompatible under traditional graph-based adjacency matrix encodings. The approach is trained on ONNX-Bench, a unified benchmark containing over 600,000 architecture-accuracy pairs from multiple NAS benchmarks. Performance prediction is formulated as a pairwise ranking task using hinge loss, enabling the model to learn generalizable principles of architecture performance rather than overfitting to specific search space distributions.

## Key Results
- Zero-shot transfer from NAS-Bench-101 to NAS-Bench-201 achieves Spearman's ρ=0.747, outperforming FLAN (ρ=0.697)
- ONNX-Net reaches peak zero-shot correlation with only 5k training samples, fewer than previous methods
- The approach generalizes well to 8 Unseen NAS tasks, demonstrating effectiveness across diverse architectures
- Input connectivity encoding provides the largest single-step gain in prediction accuracy over base variants

## Why This Works (Mechanism)

### Mechanism 1: Search-Space Agnostic Textual Representation
The text-based encoding allows a single surrogate model to process architectures from disparate search spaces by converting ONNX computational graphs into sequential text strings. This flexible format can represent arbitrary operator types, heterogeneous topologies, and fine-grained hyperparameters in a unified format ingestible by LLMs, unlike fixed-size adjacency matrices.

### Mechanism 2: Zero-Shot Transfer via LLM Priors
Fine-tuning an LLM on ONNX-Bench using pairwise hinge loss enables zero-shot performance prediction on unseen search spaces. The LLM leverages pre-trained semantic understanding to learn generalizable principles of architecture performance rather than overfitting to single search space distributions.

### Mechanism 3: Input Connectivity as Primary Performance Signal
Explicitly encoding input information (weight shapes and input names) provides the most significant gain in prediction accuracy. The model relies heavily on these connectivity cues and weight shape information to effectively rank architectures, as they define the actual computational flow and capacity.

## Foundational Learning

- **ONNX (Open Neural Network Exchange)**
  - Why needed: Serves as the "universal intermediate language" translating diverse architectures into a common graph format before text serialization
  - Quick check: Can you explain how an ONNX graph represents a simple Convolution -> BatchNorm -> ReLU block in terms of nodes and edges?

- **Neural Architecture Search (NAS) Surrogates**
  - Why needed: This paper accelerates NAS by using cheap proxy models instead of expensive training
  - Quick check: Why is Kendall's Tau or Spearman's Rho a better metric for a surrogate model than Mean Squared Error (MSE) in NAS?

- **Encoder-only LLMs (e.g., BERT/ModernBERT)**
  - Why needed: Identified as superior to decoder-based models for this task due to bidirectional attention
  - Quick check: Why might a bidirectional attention mechanism be more suitable for encoding a static architecture graph than a unidirectional decoder?

## Architecture Onboarding

- **Component map:** Raw NAS Benchmarks -> ONNX Converter -> Graph Optimizer -> Text String Encoder -> ModernBERT -> Regression Head
- **Critical path:** The ONNX-to-Text Encoding is the most fragile step, requiring careful optimization to avoid stripping critical performance-determining features while fitting the sequence into the LLM's context window
- **Design tradeoffs:**
  - Context Window vs. Fidelity: Merging chains of operations and removing nodes risks dropping subtle signals
  - Generality vs. Specificity: Training on all search spaces can hurt performance on specific spaces compared to targeted training
- **Failure signatures:**
  - Negative Transfer: Training on mixed search spaces can degrade performance on specific targets
  - Overfitting: Performance dips at high sample counts suggest overfitting to source domain
- **First 3 experiments:**
  1. Validate pipeline by converting NB101 architecture to ONNX, applying text encoder, and verifying string format
  2. Reproduce baseline by training ModernBERT on 5k NB101 samples and evaluating zero-shot on NB201 (target ρ≈0.76)
  3. Conduct encoding ablation by retraining with only "Base" information and comparing accuracy drop

## Open Questions the Paper Calls Out

- What is the optimal data mixture strategy for training a universal surrogate model to prevent negative transfer between disparate search spaces?
- Can the ONNX-based text representation be effectively used for guided generation of novel architectures rather than just performance prediction?
- How can the surrogate model be adapted to take dataset context into account for cross-dataset performance prediction?
- Does the ONNX-Net text encoding maintain its efficacy when applied to attention-based architectures like Vision Transformers?

## Limitations

- The ONNX-to-text encoding involves lossy compression that may discard subtle architectural features
- Performance may degrade significantly on truly novel architectures with operations absent from ONNX-Bench training distribution
- Computational overhead of text serialization and LLM inference may offset efficiency gains compared to lighter-weight surrogates

## Confidence

- **High Confidence:** Text-based encoding methodology and ONNX-Bench dataset construction
- **Medium Confidence:** Zero-shot transferability results, sensitive to specific search space pairings
- **Low Confidence:** Claim of "universal" applicability across arbitrary neural architectures, primarily validated on cell-based NAS spaces

## Next Checks

1. Test ONNX-Net on architectures containing operations (e.g., attention mechanisms, GNNs) minimally represented in ONNX-Bench to assess true zero-shot generalization boundaries
2. Systematically vary node removal and subgraph merging in text encoding to quantify trade-off between context constraints and prediction accuracy
3. Measure end-to-end inference time (including ONNX conversion and text processing) and compare against traditional graph-based surrogates and actual architecture training times