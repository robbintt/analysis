---
ver: rpa2
title: Knowledge-Aware Diverse Reranking for Cross-Source Question Answering
arxiv_id: '2506.20476'
source_url: https://arxiv.org/abs/2506.20476
tags:
- question
- knowledge
- documents
- reranking
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents Team Marikarp\u2019s solution for the SIGIR\
  \ 2025 LiveRAG competition, which achieved first place. The proposed knowledge-aware\
  \ diverse reranking RAG pipeline addresses the challenge of retrieving and synthesizing\
  \ relevant information from a large corpus of 15M web documents."
---

# Knowledge-Aware Diverse Reranking for Cross-Source Question Answering

## Quick Facts
- **arXiv ID**: 2506.20476
- **Source URL**: https://arxiv.org/abs/2506.20476
- **Reference count**: 40
- **Primary result**: Team Marikarp achieved first place in SIGIR 2025 LiveRAG competition using a knowledge-aware diverse reranking RAG pipeline

## Executive Summary
This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG competition, which achieved first place. The proposed knowledge-aware diverse reranking RAG pipeline addresses the challenge of retrieving and synthesizing relevant information from a large corpus of 15M web documents. The method combines hybrid sparse and dense retrieval, followed by a novel two-stage reranking approach that decomposes retrieved documents into key knowledge elements and generates diverse queries to ensure comprehensive coverage.

## Method Summary
The approach uses hybrid sparse and dense retrieval to search 15M web documents, followed by a two-stage reranking pipeline. First, an initial reranking model filters documents, then the LLM (Falcon3-10B-Instruct) identifies key knowledge elements and summarizes them into two distinct queries. Reranking is performed separately for each summarized query, and results are merged to ensure diverse, balanced coverage. The final answer is generated using the top 10 reranked documents with jina-reranker-m0 as the reranking model.

## Key Results
- Team Marikarp achieved first place in SIGIR 2025 LiveRAG competition
- The pipeline showed highest scores for both correctness and faithfulness metrics
- Method successfully handles retrieval and synthesis from 15M web documents

## Why This Works (Mechanism)
The method works by decomposing retrieved documents into key knowledge elements and generating diverse queries to ensure comprehensive coverage. By performing separate reranking for each summarized query and merging results, the approach ensures that both diverse and relevant information is captured, addressing the challenge of information synthesis from large document collections.

## Foundational Learning

- **Hybrid retrieval**: Combining sparse (BM25) and dense (vector) retrieval methods to capture both keyword and semantic similarity. Needed to ensure comprehensive document coverage across different types of queries.
- **Reranking**: Using learned models to reorder initial retrieval results based on relevance. Needed to improve precision after initial retrieval from large corpus.
- **Knowledge decomposition**: Breaking down retrieved documents into key elements using LLM. Needed to identify core information that addresses different aspects of the query.
- **Query summarization**: Creating multiple queries from decomposed knowledge. Needed to capture different facets of information needs and ensure diversity.
- **Diverse merging**: Combining results from multiple reranking runs. Needed to prevent information silos and ensure balanced coverage.

## Architecture Onboarding

- **Component map**: Document Retrieval -> Initial Reranking -> Knowledge Decomposition -> Query Summarization -> Separate Reranking -> Result Merging -> Answer Generation
- **Critical path**: The knowledge-aware decomposition and query summarization stage is critical as it directly impacts the diversity and comprehensiveness of final results
- **Design tradeoffs**: The choice of two summarized queries represents a balance between diversity and computational efficiency; more queries could increase diversity but also complexity
- **Failure signatures**: Poor decomposition or summarization could lead to redundant or incomplete information coverage, reducing answer quality
- **First experiments**: 1) Test pipeline performance with 1, 2, and 3 summarized queries to find optimal diversity-accuracy tradeoff; 2) Evaluate impact of different initial reranking thresholds on final answer quality; 3) Compare performance using different LLMs for knowledge decomposition

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of transparency regarding specific evaluation metrics used in the competition
- No detailed information about competition dataset characteristics
- No evidence of effectiveness on standard QA benchmarks or different retrieval scenarios

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Two-stage knowledge-aware reranking approach is effective | Medium |
| Knowledge decomposition and summarization strategy improves diversity and coverage | Medium |
| Pipeline achieves state-of-the-art performance | Low |

## Next Checks

1. Conduct ablation studies to quantify the contribution of each component (knowledge decomposition, query summarization, diverse reranking) by comparing performance against variants that remove these elements
2. Test the pipeline on established QA datasets like Natural Questions, SQuAD, or HotpotQA to evaluate generalizability beyond the competition context
3. Implement and evaluate alternative diversity mechanisms (e.g., maximal marginal relevance, topic modeling-based diversity) to determine if the proposed knowledge-aware approach provides measurable advantages over established methods