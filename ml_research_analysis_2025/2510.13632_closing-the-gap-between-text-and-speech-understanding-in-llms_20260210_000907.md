---
ver: rpa2
title: Closing the Gap Between Text and Speech Understanding in LLMs
arxiv_id: '2510.13632'
source_url: https://arxiv.org/abs/2510.13632
tags:
- speech
- text
- training
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the text-speech understanding gap in speech-adapted
  large language models, identifying forgetting of text capabilities and cross-modal
  misalignment as the primary causes. The authors propose SALAD (Sample-efficient
  Alignment with Learning through Active selection and cross-modal Distillation),
  which combines cross-modal distillation with active data selection to improve alignment
  while mitigating forgetting.
---

# Closing the Gap Between Text and Speech Understanding in LLMs

## Quick Facts
- arXiv ID: 2510.13632
- Source URL: https://arxiv.org/abs/2510.13632
- Reference count: 30
- Primary result: SALAD achieves competitive performance with strong open-weight baselines across knowledge, language understanding, and reasoning benchmarks while training on over an order of magnitude less speech data from public corpora

## Executive Summary
This paper addresses the text-speech understanding gap in speech-adapted large language models by identifying forgetting of text capabilities and cross-modal misalignment as the primary causes. The authors propose SALAD (Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation), which combines cross-modal distillation with active data selection to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with strong open-weight baselines across knowledge, language understanding, and reasoning benchmarks while training on over an order of magnitude less speech data from public corpora. The method outperforms most baselines and maintains the closest performance to original text capabilities compared to other speech-adapted models.

## Method Summary
SALAD uses a two-stage training framework with cross-modal knowledge distillation and active data selection. Stage I trains on 141K hours of natural speech (Emilia + LibriHeavy) with pure distillation (α=1) for 24B tokens. Stage II computes cluster-level misalignment on a probe set, then samples synthetic speech data from high-misalignment clusters (γ=5 importance weighting) to target domain gaps. The architecture uses a frozen Mimi speech encoder, 122M-param adapter (12 Llama-style layers), and Qwen2.5-3B/7B backbone. The model processes interleaved speech and text inputs with a 2048-token context window.

## Key Results
- SALAD achieves 2.5-4.8 point improvements over uniform sampling on MMSU, OBQA, and ARC-C benchmarks
- Maintains closest performance to original text capabilities compared to other speech-adapted models
- Achieves competitive performance with >10× less speech data than baseline approaches
- Shows strong correlation (R²=0.74) between forgetting and text performance degradation

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Knowledge Distillation Preserves Text Capabilities
- Claim: Distillation from the text LLM teacher mitigates forgetting and improves alignment compared to standard NLL training.
- Mechanism: The objective interpolates between NLL and distillation via α ∈ [0,1]. At α=1, the model minimizes L_DIST, which forces the speech-adapted model's predictions conditioned on speech to match the text LLM's predictions conditioned on equivalent text. This constrains the model to remain faithful to the teacher while learning cross-modal consistency.
- Core assumption: The text LLM Q_φ better approximates the general language distribution Q than narrow-domain speech datasets.
- Evidence anchors:
  - [abstract] "cross-modal distillation... improves alignment while mitigating forgetting"
  - [Section 3.5] "Higher values of α yield better alignment, and for α>0, training on narrow-domain data generalizes to reduced broad-domain misalignment with scale"
  - [corpus] Related work "Cross-Modal Knowledge Distillation for Speech Large Language Models" confirms systematic evaluation of forgetting and modality inequivalence in speech LLMs (FMR=0.59)
- Break condition: If the teacher model has poor text capabilities or the speech encoder produces irreparably misaligned representations, distillation cannot transfer knowledge effectively.

### Mechanism 2: Model-Guided Active Selection Targets Domain Gaps
- Claim: Using the model's own misalignment signals to guide synthetic data selection outperforms random sampling.
- Mechanism: Text corpus is partitioned into K clusters via k-means on sentence embeddings. Per-cluster importance weights are computed as w(c) ∝ M(c)^γ, where M(c) is the distillation loss measured on a small pre-synthesized probe set. Clusters with higher misalignment are upweighted. Synthetic speech is generated only for samples from high-importance clusters.
- Core assumption: Misalignment on a probe set correlates with downstream benchmark performance gaps.
- Evidence anchors:
  - [Section 4.1] "we let the model itself define P_target... treating the divergence between P_θ and Q_φ within each cluster as a proxy for how much that cluster belongs to the 'missing' domain"
  - [Table 4] Active selection shows 2.5-4.8 point improvements over uniform sampling on MMSU, OBQA, and ARC-C
  - [corpus] Limited direct corpus evidence on active selection for speech LLMs; related work focuses on distillation rather than data selection
- Break condition: If probe set is unrepresentative, or γ is too high (over-focusing on gaps without exploration), performance degrades. Figure 6 shows γ=5 optimal; γ=30 underperforms at larger budgets.

### Mechanism 3: Two-Stage Training Achieves Sample Efficiency
- Claim: Sequential distillation-on-natural-speech followed by targeted synthetic data achieves competitive performance with >10× less data than baselines.
- Mechanism: Stage I trains on 141K hours of natural speech (Emilia + LibriHeavy) until alignment plateaus at irreducible misalignment E. Stage II adds only 1% additional synthetic data (~1.4K hours equivalent) targeting domain gaps. The approach avoids costly full-scale synthesis while expanding domain coverage.
- Core assumption: Residual misalignment after Stage I is primarily due to domain mismatch, not architectural limitations.
- Evidence anchors:
  - [Figure 1] Shows SALAD achieving comparable gap reduction with 10^4-10^5 hours vs. competitors at 10^6+ hours
  - [Table 2] For α=1 on narrow-domain data, E≈0.13 reached at ~48B tokens; domain-matched distillation achieves E≈0.04
  - [corpus] Related work on speech-text alignment mechanisms confirms representational drift and modality gaps (FMR=0.61)
- Break condition: If natural speech data is too narrow or synthetic speech quality is poor, Stage II gains will be limited.

## Foundational Learning

- Concept: **Knowledge Distillation**
  - Why needed here: The core training objective requires understanding how KL divergence between teacher and student distributions preserves capabilities while enabling cross-modal transfer.
  - Quick check question: Given teacher distribution P and student Q, does minimizing D_KL(P||Q) make Q concentrate mass where P has high probability?

- Concept: **Catastrophic Forgetting in Transfer Learning**
  - Why needed here: The paper explicitly quantifies forgetting (Equation 3) and shows it correlates with text performance degradation (R²=0.74).
  - Quick check question: When fine-tuning a pretrained model on a new domain, why might performance on the original domain decrease?

- Concept: **Importance Sampling for Data Selection**
  - Why needed here: Stage II uses clustered importance sampling derived from CRISP (Grangier et al., 2025) to prioritize synthetic data generation.
  - Quick check question: If you have clusters with weights w(c), how do you sample proportionally to those weights rather than uniformly?

## Architecture Onboarding

- Component map: Waveform -> Mimi encoder (frozen) -> Z -> Adapter (122M params, trainable) -> Z' -> Language model (Qwen2.5 3B/7B, trainable) -> Next token distribution

- Critical path:
  1. Initialize LLM from pretrained Qwen2.5 text checkpoint
  2. Train adapter + LLM with α=1 (pure distillation) on Emilia + LibriHeavy for 24B tokens
  3. Compute cluster misalignment M(c) on probe set
  4. Sample from D_web proportional to M(c)^γ, synthesize speech, train additional 1.9B tokens

- Design tradeoffs:
  - Lightweight encoder (Mimi) vs. aligned representations: Authors choose streaming-friendly architecture despite worse input alignment, expecting findings to generalize
  - Distillation strength (α): Higher α improves alignment but may limit learning from speech data
  - Selectivity (γ): γ=5 optimal; higher values overfit to gaps, lower values waste budget on already-aligned domains

- Failure signatures:
  - Speech performance plateaus despite continued training -> check if misalignment has reached E (Table 2)
  - Text performance degrades significantly -> forgetting exceeded; verify distillation objective is active
  - Stage II shows no improvement over Stage I -> probe set may be unrepresentative or γ misconfigured

- First 3 experiments:
  1. **Ablate distillation strength**: Train with α ∈ {0, 0.5, 1.0} on same data, measure both forgetting (Equation 3) and misalignment (Equation 2) to validate R² correlations.
  2. **Verify active selection contribution**: Compare Stage II with γ=5 vs. γ=0 (uniform) on held-out benchmarks, confirming Table 4 improvements.
  3. **Test domain mismatch hypothesis**: Train Stage I only on broad-domain synthetic speech (FineWeb-Edu) vs. narrow natural speech, measuring whether domain matching alone reduces irreducible misalignment E.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The two-stage training framework may not generalize to other speech encoder architectures beyond the streaming-oriented Mimi encoder used here
- The active selection mechanism depends heavily on the quality of the probe set and the assumption that cluster-level misalignment correlates with benchmark performance, but this relationship is not rigorously validated across different probe set sizes
- The synthetic speech generation pipeline (Kokoro-TTS) introduces potential quality variability that could affect the alignment improvements, though this is not directly evaluated

## Confidence
- **High Confidence**: The core finding that cross-modal distillation with α=1 effectively reduces the text-speech understanding gap and mitigates forgetting is well-supported by systematic ablation studies (Table 3) and strong correlation analysis (R²=0.74 for forgetting, R²=0.80 for misalignment)
- **Medium Confidence**: The sample efficiency claims (>10× less data than baselines) are supported by Figure 1 comparisons, but the baselines used are not all directly comparable in architecture or training procedures
- **Medium Confidence**: The active selection mechanism shows consistent improvements (2.5-4.8 points) over uniform sampling in Table 4, but the optimal γ=5 parameter is tuned on a single dataset and may not generalize

## Next Checks
1. Test SALAD's performance when using a different speech encoder architecture (e.g., wav2vec or HuBERT) to verify the findings aren't specific to the streaming-oriented Mimi encoder
2. Conduct a sensitivity analysis on the probe set size and composition to determine how representative the probe must be for the active selection mechanism to work effectively
3. Evaluate the synthetic speech quality directly by measuring intelligibility and naturalness scores, and correlate these with downstream performance to quantify the impact of TTS quality on alignment