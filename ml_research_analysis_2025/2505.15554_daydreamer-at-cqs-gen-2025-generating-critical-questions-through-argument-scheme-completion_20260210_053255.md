---
ver: rpa2
title: 'DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument
  Scheme Completion'
arxiv_id: '2505.15554'
source_url: https://arxiv.org/abs/2505.15554
tags:
- critical
- questions
- scheme
- argument
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a critical question generation pipeline leveraging
  large language models and Walton's argumentation schemes. Their approach involves
  extracting arguments using scheme templates, generating critical questions based
  on the extracted arguments, and ranking the generated questions.
---

# DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion

## Quick Facts
- arXiv ID: 2505.15554
- Source URL: https://arxiv.org/abs/2505.15554
- Reference count: 3
- Authors: Wendi Zhou, Ameer Saadat-Yazdi, Nadin Kökciyan
- Placed 4th out of 13 teams in CQs-Gen shared task with 60 helpful, 25 unhelpful, and 17 invalid questions

## Executive Summary
The authors present DayDreamer, a pipeline for generating critical questions (CQs) using large language models and Walton's argumentation schemes. Their approach involves extracting arguments from intervention texts using scheme templates, generating scheme-specific critical questions, and ranking the best questions for each intervention. The system achieves competitive performance on the CQs-Gen shared task, demonstrating that structured argumentation theory can effectively guide LLM-based question generation. They identify key optimizations including conversational prompting and scheme sorting that improve output quality.

## Method Summary
DayDreamer implements a three-stage conversational pipeline: (1) Argument Extraction where LLMs instantiate scheme templates from Walton et al. (2008) to extract structured arguments, (2) CQ Generation where extracted arguments are used to generate critical questions based on scheme-specific CQ templates, and (3) Ranking where the top 3 most helpful questions are selected. The system uses either GPT-4o-mini or LLaMa-3.1-8B-Instruct, with conversational prompting to maintain context across stages. Scheme sorting is applied to extract multiple argument instances per scheme, and ER-prefixed schemes (lacking definitions) are handled by skipping templates.

## Key Results
- Achieved 4th place out of 13 teams in CQs-Gen shared task
- GPT-4o-mini produced more helpful questions while generating fewer invalid and unhelpful ones compared to LLaMa-3.1-8B baseline
- Conversational prompting improved useful CQ rate by ~6 percentage points over direct prompting
- Scheme sorting increased useful question diversity but also slightly increased invalid outputs
- Removing ER-schemes reduced invalid outputs from the pipeline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scheme templates constrain LLM reasoning and reduce hallucination during argument extraction.
- Mechanism: By providing explicit argumentation scheme definitions from Walton et al. (2008), the system forces the LLM to map raw intervention text onto structured premise-conclusion forms before generating questions.
- Core assumption: LLMs can reliably instantiate abstract scheme templates given domain definitions.
- Evidence anchors: [abstract], [Section 4, Argument Extraction]
- Break condition: Scheme templates without clear definitions (e.g., ER-prefixed schemes) cause confusion, leading to invalid outputs.

### Mechanism 2
- Claim: Conversational prompting preserves contextual grounding across pipeline stages.
- Mechanism: Instead of isolated prompts per stage, the system maintains chat history so each stage's output remains accessible.
- Core assumption: Context window utilization improves coherence without degrading output quality.
- Evidence anchors: [abstract], [Section 4], [Section 5.2]
- Break condition: Excessive context accumulation may introduce noise; not tested in ablation.

### Mechanism 3
- Claim: Scheme-sorting enables extraction of multiple argument instances per scheme, increasing output diversity.
- Mechanism: By grouping identical scheme names before prompting, LLMs can assess how many argument instances exist and extract them together.
- Core assumption: LLMs can count and batch-extract repeated patterns when given consolidated scheme lists.
- Evidence anchors: [Section 5.2]
- Break condition: Tradeoff—diversity increases but invalid outputs also rise slightly.

## Foundational Learning

- **Walton's Argumentation Schemes**: The theoretical framework mapping argument patterns to structured templates; needed because the pipeline depends on mapping text to scheme templates to generate scheme-specific critical questions. Quick check: Given "Dr. Smith says climate change is accelerating," which scheme applies and what are two associated critical questions?

- **Chain-of-Thought Prompting**: Sequential decomposition of complex reasoning into multiple stages; needed because CQ generation is broken into sequential stages (extraction → generation → ranking), each building on prior outputs. Quick check: How does multi-stage prompting differ from single-prompt generation for complex reasoning tasks?

- **Critical Questions in Argumentation Theory**: Questions that challenge premises, evidence, or biases in arguments; needed because CQs are the target output and understanding their role is essential for evaluating helpfulness. Quick check: For "Argument from Expert Opinion," what aspects should critical questions target?

## Architecture Onboarding

- **Component map**: Argument Extraction -> CQ Generation -> Ranking
- **Critical path**: Scheme definition quality → Argument extraction accuracy → CQ relevance → Ranking quality. ER-scheme ambiguity breaks step 1.
- **Design tradeoffs**: GPT-4o-mini vs. LLaMa-3.1-8B (GPT yields more helpful CQs; LLaMa produces more errors); Conversational vs. Direct prompting (Conversational improves useful CQ rate by ~6 percentage points); Including vs. excluding undefined ER-schemes (Removing ER templates reduces invalid outputs).
- **Failure signatures**: High invalid CQ rate → Check for ER-prefixed schemes in input; Low CQ count per intervention → Verify fallback prompt triggers when CQs < 6; Repetitive CQs → Ensure scheme-sorting is applied.
- **First 3 experiments**: 1) Reproduce baseline vs. conversational pipeline on validation set to validate Table 3 trends. 2) Ablate scheme-sorting: compare shuffled vs. sorted scheme lists to quantify diversity gain. 3) Test ER-scheme handling: compare (a) using Walton definitions, (b) omitting templates, (c) custom definitions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a comprehensive compendium of argument scheme definitions with associated critical questions for underspecified schemes (e.g., "ER" variants) significantly improve CQ generation quality?
- Basis in paper: [explicit] Authors state: "Constructing a compendium of argument scheme definitions used in the dataset, alongside generating critical questions, would also likely improve results in follow-up work, as it would avoid the issues we found with 'ER' schemes."
- Why unresolved: The "ER" schemes lacked definitions in Walton et al. (2008), causing LLM confusion and invalid outputs; no such compendium currently exists.
- What evidence would resolve it: Ablation study comparing pipeline performance with vs. without complete scheme definitions across all scheme types.

### Open Question 2
- Question: How can the pipeline instability observed across repeated runs be reduced to ensure consistent critical question generation?
- Basis in paper: [explicit] Authors note: "GPT-4o-mini-run2 shows a similar but slightly worse profile, suggesting some instability in our pipeline" and "our pipeline fails to achieve consistent performance in unlocking their full potential."
- Why unresolved: LLM non-determinism and conversational prompting variability cause output differences even with identical inputs.
- What evidence would resolve it: Multiple-run experiments with controlled seeds, temperature settings, and statistical significance testing across runs.

### Open Question 3
- Question: What mechanisms can enable critical question generation for argument schemes that lack predefined critical question templates in existing argumentation theory?
- Basis in paper: [explicit] Authors state in Limitations: "certain schemes used in the dataset were not provided with critical questions in Walton et al. (2008), preventing us from generating critical questions once the scheme has been extracted."
- Why unresolved: The theoretical framework underlying the pipeline depends on pre-existing CQ templates for each scheme.
- What evidence would resolve it: Development and evaluation of template-free or automatically-derived CQ generation methods for novel schemes.

### Open Question 4
- Question: Does maintaining full conversational context across all pipeline stages (including ranking) improve critical question selection compared to resetting context?
- Basis in paper: [inferred] The ranking stage explicitly starts "a new chat history as we are only interested in the original intervention and the generated critical questions," discarding argument extraction context that could inform helpfulness assessment.
- Why unresolved: The trade-off between focused ranking input and potentially valuable contextual information from earlier stages is unexplored.
- What evidence would resolve it: Comparative experiment measuring ranking quality with full vs. reset context while controlling for token limits.

## Limitations
- Reliance on Walton's 2008 scheme definitions, which are incomplete for certain scheme variants (e.g., ER-prefixed schemes), causing 17 invalid questions
- Uncertainty whether conversational prompting advantage is due to context preservation itself or better prompt engineering
- Lack of exploration of generation parameters (temperature, max_tokens) that could significantly impact CQ quality and diversity

## Confidence

- **High confidence**: GPT-4o-mini outperforms LLaMa-3.1-8B-Instruct in producing helpful questions; scheme-sorting increases useful question diversity
- **Medium confidence**: Conversational prompting improves useful question rate by ~6 percentage points; removing ER-schemes reduces invalid outputs
- **Low confidence**: The exact mechanism by which scheme templates constrain LLM reasoning; whether context window utilization is the primary driver of conversational prompting benefits

## Next Checks

1. **Scheme template ablation**: Systematically compare outputs when using Walton's definitions vs. custom definitions vs. no templates for ER-prefixed schemes to quantify the impact on invalid question rate.

2. **Conversational vs. direct prompting isolation**: Create a hybrid prompting approach that preserves context without using conversational history to determine whether context preservation or prompt structure drives performance gains.

3. **Generation parameter sweep**: Test different temperature and max_tokens settings across both LLMs to identify optimal parameters for maximizing helpful questions while minimizing invalid ones.