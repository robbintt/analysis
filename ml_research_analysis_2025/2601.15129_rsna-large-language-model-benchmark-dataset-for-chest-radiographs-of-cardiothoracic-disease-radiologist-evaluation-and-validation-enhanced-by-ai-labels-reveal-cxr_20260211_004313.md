---
ver: rpa2
title: 'RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic
  Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)'
arxiv_id: '2601.15129'
source_url: https://arxiv.org/abs/2601.15129
tags:
- labels
- radiologists
- chest
- findings
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A benchmark dataset of 200 chest radiographs with 12 cardiothoracic
  abnormality labels was created and released, with each study verified by three radiologists.
  An AI-assisted labeling workflow using LLMs extracted findings from radiology reports
  and mapped them to benchmark labels, enabling radiologists to review images more
  efficiently.
---

# RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)

## Quick Facts
- arXiv ID: 2601.15129
- Source URL: https://arxiv.org/abs/2601.15129
- Reference count: 40
- Primary result: 200 chest radiographs with 12 cardiothoracic labels, verified by three radiologists each, κ=0.622 overall

## Executive Summary
REVEAL-CXR introduces a benchmark dataset of 200 chest radiographs with 12 cardiothoracic abnormality labels verified by three fellowship-trained radiologists. The dataset was created using an AI-assisted labeling workflow where GPT-4o extracts findings from radiology reports, which are then mapped to benchmark labels by a locally hosted Phi-4-Reasoning model with guided decoding. Radiologists review and modify suggested labels through a web-based interface, with studies requiring majority consensus (≥2 "Agree All" votes) for inclusion. The benchmark aims to provide high-quality, clinically relevant data for evaluating multimodal large language models on chest radiograph interpretation.

## Method Summary
The dataset creation involved a two-stage LLM pipeline where GPT-4o first extracts abnormal findings from radiology reports with ICD-10 codes and laterality, then Phi-4-Reasoning maps these findings to 12 predefined benchmark labels using guided decoding via vLLM. A stratified sampling algorithm selected 1,000 studies balancing label distribution and difficulty, with final curation prioritizing rare and multi-label cases. Seventeen radiologists from ten institutions reviewed each study independently, and majority voting resolved discrepancies. The final benchmark consists of 200 studies (100 released, 100 holdout), with inter-rater agreement measured at κ=0.622.

## Key Results
- Substantial inter-rater agreement among radiologists (κ=0.622, 95% CI [0.590, 0.651])
- Most individual findings showed high agreement (κ > 0.7)
- Airspace opacity showed notably lower agreement (κ=0.484)
- AI-suggested labels were modified by radiologists in 61.9% of cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-assisted labeling reduces radiologist workload while maintaining ground-truth quality
- Mechanism: A two-stage LLM pipeline first extracts clinical findings from radiology reports using GPT-4o, then maps those findings to 12 predefined benchmark labels using a locally hosted model (Phi-4-Reasoning) with guided decoding that constrains outputs to a structured JSON schema. Radiologists verify suggested labels rather than generate labels de novo.
- Core assumption: LLM-extracted labels from reports provide a sufficiently accurate starting point that radiologist verification is faster than independent annotation.
- Evidence anchors:
  - [abstract] "An AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment."
  - [section 2.1] "GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning)."
- Break condition: If LLM-extracted labels are frequently wrong or misleading, radiologist verification time could exceed de novo annotation time.

### Mechanism 2
- Claim: Multi-radiologist consensus with majority voting produces reliable benchmark labels
- Mechanism: Each chest radiograph is independently reviewed by three fellowship-trained cardiothoracic radiologists from a pool of 17 across 10 institutions. Radiologists select "Agree All," "Agree Mostly," or "Disagree" for LLM-suggested labels. Studies require at least 2 "Agree All" votes for inclusion; majority voting resolves discrepancies when radiologists modify labels.
- Core assumption: Three independent reviews with majority consensus sufficiently reduces individual radiologist bias and labeling errors.
- Evidence anchors:
  - [abstract] "inter-rater agreement among radiologists was substantial (κ=0.622, 95% CI [0.590, 0.651])"
  - [section 4.2] "Cohen's κ between individual radiologists and the majority vote... was 0.622... reflecting substantial agreement according to the Landis & Koch scale."
- Break condition: If radiologists systematically disagree on specific findings (e.g., airspace opacity κ=0.484), consensus labels for those conditions may not represent ground truth.

### Mechanism 3
- Claim: Stratified sampling prioritizing rare and multi-label cases creates a more informative benchmark for model evaluation
- Mechanism: Initial sampling ensures representation across all 12 labels and varying difficulty (1-6 findings per study). Final 200-study selection prioritizes less common findings (COPD, pneumothorax, lymphadenopathy, etc.) and studies with multiple labels, yielding harder examples that better reveal model limitations.
- Core assumption: Hard examples with rare or multiple conditions are more discriminative for evaluating multimodal LLMs than easy, common cases.
- Evidence anchors:
  - [section 2.1] "This algorithm ensured that each study selected contained at least one of the 12 benchmark labels as specified by the LLM-generated annotations."
  - [section 1] "Although the sample size is not large, it focuses on rare conditions and complex cases involving multiple diseases, which are the most informative data points for model evaluation."
- Break condition: If prioritized rare/multi-label cases are unrepresentative of clinical practice, benchmark performance may not generalize to real-world deployment.

## Foundational Learning

- Concept: **Cohen's Kappa (κ)** - Inter-rater reliability metric comparing observed agreement to chance-expected agreement.
  - Why needed here: The paper reports κ=0.622 overall and κ=0.484 for airspace opacity; understanding these values is essential to interpret label quality.
  - Quick check question: If two raters agree 85% of the time but would agree 75% by chance, what is κ? (Answer: 0.40)

- Concept: **Guided/Constrained Decoding** - LLM inference technique that restricts outputs to valid tokens per a schema.
  - Why needed here: Phi-4-Reasoning uses guided decoding via vLLM to enforce JSON output matching the 12-label enum.
  - Quick check question: Why use guided decoding instead of post-hoc parsing for structured medical labels? (Answer: Guarantees valid output, reduces hallucination to invalid categories)

- Concept: **DICOM vs Downsampled Formats** - DICOM preserves full image metadata and bit depth; JPEG/PNG lose information.
  - Why needed here: Unlike prior datasets (MIMIC-CXR-JPG, CheXpert), REVEAL-CXR releases original DICOM files to avoid information loss.
  - Quick check question: Name one type of information lost when converting DICOM to JPEG for chest radiographs. (Answer: Bit depth reduction, lossy compression artifacts, metadata stripping)

## Architecture Onboarding

- Component map: GPT-4o extraction → Phi-4-Reasoning mapping → stratified sampling → radiologist review → majority vote → final benchmark
- Critical path: Report → GPT-4o extraction → Phi-4 mapping → stratified sampling → radiologist review → majority vote → final benchmark
- Design tradeoffs:
  - Small model (Phi-4) for mapping is faster but may underperform GPT-4o on complex cases (acknowledged limitation)
  - 200-study benchmark is small but focuses on hard examples; larger scale would require more radiologist time
  - "Agree Mostly" and "Disagree" collapsed to single category due to inconsistent usage
- Failure signatures:
  - Airspace opacity shows κ=0.484 (moderate agreement); labels for this finding are less reliable
  - In 61.9% of cases, radiologists' final labels diverged from LLM suggestions—initial LLM labels are not reliable standalone
  - Radiologists did not consistently distinguish "Disagree" vs "Agree Mostly" despite different intended meanings
- First 3 experiments:
  1. Replicate the GPT-4o → Phi-4 pipeline on a held-out set of MIDRC reports; measure label distribution vs radiologist majority vote
  2. Train a multimodal classifier on the released 100 studies; evaluate on holdout to establish baseline performance for future model comparisons
  3. Measure radiologist annotation time with and without LLM-suggested labels to validate efficiency claims (not done in paper)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would using larger LLMs (e.g., GPT-4o) instead of Phi-4-Reasoning for the label mapping step improve agreement between AI-suggested labels and radiologist adjudication?
- Basis in paper: [explicit] The authors state: "we used a relatively small model (Phi-4-Reasoning) for the mapping step; while this approach is faster, its performance may not match that of larger models such as GPT-4o. Therefore, the mapping performance may be suboptimal."
- Why unresolved: No comparative evaluation was conducted between different LLM sizes for the mapping task.
- What evidence would resolve it: A controlled comparison of mapping accuracy using different LLM sizes on the same report set, with radiologist adjudication as ground truth.

### Open Question 2
- Question: What specific factors contribute to the low inter-rater agreement for airspace opacity (κ=0.484), and can clearer operational definitions or additional imaging context improve reliability?
- Basis in paper: [explicit] The authors report: "The notable exception was airspace opacity (κ = 0.484, 95% CI [0.440, 0.524]), which is consistent with previous reports of low interrater agreement for this finding."
- Why unresolved: While acknowledged as a known problem, no specific intervention was tested to address it in this benchmark.
- What evidence would resolve it: A follow-up study using refined definitions, training materials, or clinical context for airspace opacity with measurement of resulting agreement improvement.

### Open Question 3
- Question: How does the inclusion of patient clinical context and demographic information affect the accuracy of both AI-assisted labeling and radiologist adjudication?
- Basis in paper: [explicit] The authors state: "we lacked access to comprehensive patient clinical conditions and demographic characteristics, which are often crucial for achieving a more accurate diagnosis."
- Why unresolved: The benchmark was created without clinical context, so its impact remains unknown.
- What evidence would resolve it: A comparative study where the same cases are labeled with and without clinical context, measuring differences in accuracy and agreement.

### Open Question 4
- Question: Can the AI-assisted labeling workflow scale efficiently to create larger benchmarks (thousands of studies) while maintaining the quality achieved with three-radiologist verification?
- Basis in paper: [inferred] The paper created a benchmark of 200 studies with 17 radiologists and notes the approach "is particularly valuable for creating larger-scale benchmarks in the future," but scaling feasibility is not tested.
- Why unresolved: The current workflow required substantial expert time; whether this process is feasible at 10-100x scale is unknown.
- What evidence would resolve it: A pilot study applying the workflow to a 2,000+ case dataset with metrics on time, cost, and quality retention.

## Limitations
- The benchmark size (200 studies) limits statistical power for evaluating rare conditions
- LLM-suggested labels were modified by radiologists in 61.9% of cases, indicating limited standalone reliability
- Efficiency gains from AI assistance were not quantitatively measured
- Lack of patient clinical context and demographic information may limit diagnostic accuracy

## Confidence
- High confidence: The overall inter-rater agreement statistic and its substantial interpretation
- Medium confidence: The representativeness of the 200-study benchmark for multimodal LLM evaluation
- Medium confidence: The AI-assisted workflow's contribution to labeling efficiency (lacks quantitative validation)

## Next Checks
1. Measure radiologist annotation time with and without LLM-suggested labels to quantify efficiency gains
2. Evaluate multimodal LLM performance on the released 100 studies and compare to radiologist performance on the holdout 100 studies
3. Replicate the GPT-4o → Phi-4 mapping pipeline on a held-out MIDRC subset to validate label consistency and identify systematic failure modes