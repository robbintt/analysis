---
ver: rpa2
title: If Concept Bottlenecks are the Question, are Foundation Models the Answer?
arxiv_id: '2504.19774'
source_url: https://arxiv.org/abs/2504.19774
tags:
- concept
- concepts
- annotations
- bottleneck
- cbms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Concept Bottleneck Models (CBMs) are designed to be both accurate
  and interpretable by using a two-step process: first extracting high-level concepts
  from inputs (e.g., images) and then using these concepts to make predictions. However,
  their interpretability depends heavily on the quality of the learned concepts.'
---

# If Concept Bottlenecks are the Question, are Foundation Models the Answer?

## Quick Facts
- arXiv ID: 2504.19774
- Source URL: https://arxiv.org/abs/2504.19774
- Reference count: 40
- Primary result: VLM-CBMs achieve high label accuracy despite learning low-quality, leaky, and entangled concepts compared to expert-annotated CBMs

## Executive Summary
Concept Bottleneck Models (CBMs) are designed to be both accurate and interpretable by extracting high-level concepts from inputs before making predictions. This study evaluates whether recent "VLM-CBMs" that use weak supervision from Vision-Language Models (VLMs) instead of expert annotations can maintain this interpretability. The research reveals that while VLM-CBMs achieve good label accuracy, they often learn inaccurate, entangled, or leaky concepts that compromise interpretability. The findings highlight that VLM-CBMs extend applicability but at the cost of interpretability, emphasizing the need for more robust methods to ensure concept quality.

## Method Summary
The study compares standard CBMs (trained on expert-annotated concepts) with VLM-CBMs (trained on VLM-generated pseudo-labels) across three datasets: Shapes3d, CelebA, and CUB. Models evaluated include Label-free CBM, LaBo, and VLG-CBM using CLIP and LLaVa embeddings. The pipeline involves: 1) generating concept annotations via VLMs, 2) training concept extractors on these annotations, and 3) training a linear inference layer on ground-truth labels. Performance is measured using five metrics: label accuracy (F1), concept accuracy (AUC), concept leakage (LEAK), disentanglement (DCI), and oracle impurity score (OIS).

## Key Results
- VLM-CBMs achieve F1(Y) scores comparable to CBM baselines but with substantially lower concept accuracy (AUC(C))
- On Shapes3d, VLM-CBMs reached label F1 scores up to 0.96 but concept AUC scores as low as 0.24
- VLM-CBMs showed higher leakage (up to 0.99 on CelebA) and lower disentanglement compared to expert-annotated CBMs
- VLM annotation quality varies significantly by dataset, with synthetic data (Shapes3d) producing particularly poor annotations

## Why This Works (Mechanism)

### Mechanism 1: Information Leakage Enables Good Labels with Poor Concepts
VLM-CBMs achieve high label accuracy even when learned concepts poorly match ground-truth semantics because the concept bottleneck unintentionally encodes task-relevant information (e.g., background cues, stylistic features) unrelated to the concept's intended meaning. The linear inference layer exploits this leaked information for predictions, creating a disconnect between label and concept accuracy.

### Mechanism 2: VLM Annotation Quality Varies by Distribution Alignment
VLM-provided concept annotations are more accurate when queried concepts and images align with the VLM's pre-training distribution. VLMs like CLIP and G-DINO produce embeddings by matching queries against learned visual-linguistic patterns, degrading annotation quality for synthetic or out-of-distribution inputs.

### Mechanism 3: Weak Supervision Leads to Concept Entanglement
VLM-supervised concepts exhibit higher entanglement and impurity than expert-supervised ones because weak supervision doesn't enforce semantic isolation. Without precise per-concept ground truth, the concept extractor optimizes for label prediction by allowing concepts to encode information about correlated concepts, increasing DCI entanglement and OIS impurity.

## Foundational Learning

- **Concept Bottleneck Architecture (CBM)**: Why needed here: The entire paper evaluates variants of the two-stage CBM pipeline; understanding how concept extractors feed interpretable classifiers is prerequisite. Quick check: Can you trace how an image prediction flows through concept logits to a class score in a standard CBM?

- **Information Leakage in Bottlenecks**: Why needed here: The central finding is that high label accuracy does not guarantee high concept quality due to leakage. Quick check: If a concept "young" encodes basketball team logos to improve gender classification, is the resulting explanation faithful?

- **Vision-Language Model Embeddings**: Why needed here: VLM-CBMs use CLIP/G-DINO/LLaVa to generate concept supervision; understanding their embedding spaces explains annotation quality variance. Quick check: How does CLIP compute a similarity score between an image and a text concept description?

## Architecture Onboarding

- **Component map:**
  Input Image -> Backbone (ResNet/CLIP ViT) -> Linear Projection → Concept Logits -> Sigmoid → Concept Probabilities -> Sparse Linear Layer → Class Logits

- **Critical path:**
  1. Define concept vocabulary T (gold-standard in this study; LLM-generated in production)
  2. Query VLM to annotate training set with concept pseudo-labels
  3. Train concept extractor f by regressing/classifying on VLM pseudo-labels
  4. Freeze f; train sparse linear inference layer g on ground-truth labels

- **Design tradeoffs:**
  - Larger vocabularies → higher F1(Y) but risk of meaningless concepts (Table 10: LaBo uses ≥2000 concepts vs. 112 gold-standard)
  - Task-specific backbone pretraining → better CUB performance but reduced generality
  - Sparsity regularization (GLM-SAGA) → interpretable weights but potential accuracy drop

- **Failure signatures:**
  - High F1(Y) + Low AUC(C) → suspect leakage
  - Low DIS + High OIS → concepts entangled/impure
  - VLM precision/recall < 0.5 on validation → annotation quality insufficient for reliable supervision

- **First 3 experiments:**
  1. **Annotation quality audit**: Before training, measure VLM annotation precision/recall against gold-standard to diagnose supervision quality.
  2. **Leakage probe**: Train linear SVM on k least label-correlated concepts; F1 significantly above chance indicates leakage.
  3. **Architecture ablation**: Compare LF-CBM (similarity scores) vs. VLG-CBM (binary labels + bounding boxes) on same vocabulary to isolate annotation modality effects.

## Open Questions the Paper Calls Out

### Open Question 1
Does using larger, state-of-the-art foundation models (e.g., Llama-vision 3.2) for annotation significantly mitigate the issues of leakage and entanglement observed in current VLM-CBMs? The authors note that while bigger VLMs might yield better annotations, they are not typically used due to costs, and they explicitly "leave a more detailed study thereof to future work."

### Open Question 2
To what extent does increasing the concept vocabulary size allow VLM-CBMs to retain task-relevant information through "leaky" or meaningless concepts rather than improving genuine interpretability? Section 4.2 hypothesizes that "a more extensive concept vocabulary allows the bottleneck to retain more information about label predictions even if concepts do not correspond to anything meaningful."

### Open Question 3
Can incorporating sound confidence estimates or uncertainty quantification during the VLM distillation process effectively reduce concept leakage and entanglement? The Conclusion suggests that designing architectures robust to annotation mistakes "could be achieved... by leveraging sound confidence estimates of the VLM's annotations during training."

## Limitations

- The study uses fixed gold-standard concept vocabularies rather than LLM-generated ones, potentially underestimating VLM-CBM capabilities in practical settings
- The Shapes3d dataset's synthetic nature creates high variance in VLM supervision quality, limiting generalizability to natural image domains
- The paper doesn't explore architectural modifications (e.g., disentanglement regularizers) that could mitigate concept leakage and entanglement issues

## Confidence

**High confidence**: The core finding that VLM-CBMs can achieve high label accuracy despite low concept quality is well-supported by consistent metrics across all three datasets. The mechanism of information leakage enabling this disconnect is clearly demonstrated through AUC(C) vs F1(Y) comparisons.

**Medium confidence**: Claims about VLM annotation quality varying by distribution alignment are supported by empirical observations but lack direct corpus evidence on out-of-distribution effects. The mechanism of weak supervision leading to concept entanglement is plausible but supported by relatively weak direct evidence.

**Low confidence**: The paper doesn't explore whether the observed phenomenon is specific to the chosen architectures or represents a fundamental limitation of VLM-based supervision. The absence of architectural modifications to address identified issues limits confidence in the conclusions about VLM-CBMs' interpretability limitations.

## Next Checks

1. **OOD Robustness Audit**: Test VLM annotation quality on a diverse set of synthetic vs. natural images to quantify the distribution alignment effect and establish clear thresholds for reliable supervision.

2. **Leakage Attribution Analysis**: Use SHAP or integrated gradients to identify which input features the concept extractor uses, distinguishing between intended concept features and leaked information.

3. **Architectural Intervention Study**: Implement and compare CBM variants with explicit disentanglement constraints (orthogonality, sparsity) to determine whether the observed concept quality issues are architecture-dependent.