---
ver: rpa2
title: Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving
  Maps
arxiv_id: '2501.06999'
source_url: https://arxiv.org/abs/2501.06999
tags:
- likelihood
- diffusion
- arxiv
- hierarchical
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of intractable likelihood evaluation
  in cascaded diffusion models, which limits their use for likelihood-based training
  and inference. The authors propose a solution using hierarchical volume-preserving
  maps, a class of transformations where the likelihood function remains invariant,
  allowing the joint likelihood over scales to directly serve as the desired likelihood.
---

# Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps

## Quick Facts
- arXiv ID: 2501.06999
- Source URL: https://arxiv.org/abs/2501.06999
- Authors: Henry Li; Ronen Basri; Yuval Kluger
- Reference count: 40
- Primary result: Exact likelihood computation in cascaded diffusion models using hierarchical volume-preserving maps achieves 2.35-2.36 BPD on CIFAR-10

## Executive Summary
This paper addresses the fundamental challenge of intractable likelihood evaluation in cascaded diffusion models, which has limited their use for likelihood-based training and inference. The authors propose a solution using hierarchical volume-preserving maps - a class of transformations where the likelihood function remains invariant - enabling the joint likelihood over scales to directly serve as the desired likelihood without marginalization. By modeling diffusion processes on latent spaces induced by Laplacian pyramids and wavelet transforms, they construct probabilistic cascaded diffusion models (PCDMs) that retain exact likelihood computation while achieving significant improvements in density estimation, lossless compression, and out-of-distribution detection.

## Method Summary
The approach leverages hierarchical volume-preserving maps (det(Jacobian) = 1) to decompose data into multiple scales where diffusion models can be trained independently. Two specific transforms are used: Laplacian pyramids (tight frames) and Haar wavelets (orthogonal). Each scale has its own U-Net diffusion model, with higher scales conditioned on lower ones through channel-wise concatenation. The training objective combines the base scale likelihood with conditional likelihoods across scales. The method achieves tractable exact likelihood computation by exploiting the volume-preservation property, eliminating the need for marginalization.

## Key Results
- Density estimation: 2.35-2.36 BPD on CIFAR-10 vs 2.49 for standard VDM
- Lossless compression: 2.37-2.40 BPD on CIFAR-10 (3-4% improvement over VDM)
- OOD detection: 0.74-0.71 AUROC on CIFAR-10 vs SVHN (significant improvement over VDM)
- Theoretical contribution: Connection between likelihood training and score matching with Earth Mover's Distance

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Invariance via Volume-Preserving Maps
- Claim: Joint likelihood over hierarchical scales equals the data likelihood without marginalization.
- Mechanism: Hierarchical volume-preserving maps ensure log pθ(x) = log pθ[h(x)] by the generalized change-of-variables formula.
- Core assumption: The map h is a smooth homeomorphism with full-rank Jacobian.
- Evidence anchors: [abstract] "decompose spatially structured data in a hierarchical fashion without introducing local distortions"; [section 4.1] Formal proof via matrix volume condition.
- Break condition: Using standard downsampling cascades violates volume-preservation, causing joint likelihood to diverge from pθ(x).

### Mechanism 2: Cascaded Diffusion with Scale-Conditional Factorization
- Claim: Training decomposes into independent diffusion objectives per scale with tractable likelihood bounds.
- Mechanism: Each scale z(s) has its own SDE; conditional models use previous scales as conditioning via channel-wise concatenation.
- Core assumption: Conditioning information can be compressed into same spatial resolution via reconstruction and downsampling.
- Evidence anchors: [section 5.1, Eq. 15-17] Loss decomposition; [appendix B] Conditioning via z_cond = (d ∘ ... ∘ d)(h⁻¹(z(<s), 0, ...)).
- Break condition: If conditioning scales lack sufficient information, samples may exhibit coherence artifacts across scales.

### Mechanism 3: EMD Approximation via Multi-Scale Basis
- Claim: Likelihood training implicitly minimizes an upper bound on Earth Mover's Distance between true and modeled scores.
- Mechanism: Wavelet/Laplacian decomposition permits linear-time approximation of W_p(∇log q, s_θ) via weighted L2 norms in coefficient space.
- Core assumption: p ≤ 1; wavelet basis provides sufficient approximation of optimal transport cost.
- Evidence anchors: [section 5.2, Theorem 5.1] Formal statement; [appendix A, Lemma A.1] Relies on Shirdhonkar & Jacobs 2008 wavelet-EMD approximation.
- Break condition: If score distributions violate unnormalized histogram interpretation, or wavelet basis poorly captures image structure.

## Foundational Learning

- Concept: Change of variables in probability densities (Jacobian determinant)
  - Why needed here: Volume-preservation is defined via det(A^T A) = 1; understanding why standard downsampling violates this is prerequisite.
  - Quick check question: Why does nearest-neighbor upsampling (repeating columns) change the density under variable transformation?

- Concept: Variational lower bounds for diffusion (ELBO decomposition)
  - Why needed here: Training loss derives from bounding log p(x_0) with KL terms between forward/reverse processes.
  - Quick check question: In Eq. 5, what does each KL term penalize?

- Concept: Wavelet/Laplacian pyramid decomposition
  - Why needed here: These are the concrete h maps; understanding tight frames vs. orthogonality determines channel counts.
  - Quick check question: Why does Haar wavelet produce 9 channels at 16×16 while Laplacian pyramid produces 3?

## Architecture Onboarding

- Component map:
  - **Base model (z(1))**: Unconditional U-Net, 128 channels (CIFAR), standard VDM architecture
  - **Conditional models (z(2)...z(S))**: U-Net with 64-128 channels, receives concatenated conditioning
  - **h-transform**: Forward (decompose x → {z(s)}) and inverse (reconstruct from coefficients)
  - **Conditioning pathway**: h⁻¹(z(<s), zeros) → downsampling → channel concat

- Critical path:
  1. Input x → h(x) produces {z(s)}
  2. Each z(s) + conditioning → separate noise prediction network
  3. Loss: sum of L(z(1)) + Σ L(z(s)|z(<s)) computed via antithetic time sampling
  4. Likelihood evaluation: use trained model to compute bound C(x)

- Design tradeoffs:
  - **Wavelet vs. Laplacian**: Wavelet is orthogonal (exact volume-preservation, 9 channels); Laplacian is overcomplete tight frame (2x latent dim, 3 channels). Wavelet achieves better BPD (2.35 vs 2.36 on CIFAR) but Laplacian has simpler conditioning.
  - **Scale count**: More scales improve density estimation but increase parameters and memory. CIFAR uses S=2; ImageNet128 uses S=4.
  - **Channel budget**: Base model gets 2x channels vs. conditional models (128 vs. 64 for CIFAR).

- Failure signatures:
  - **BPD regression vs. VDM**: Check if h is truly volume-preserving; ablation in Table 4 shows C-VDM (non-volume-preserving) degrades to 3.20 BPD.
  - **Incoherent multi-scale samples**: Conditioning pathway may be misconfigured; verify z_cond spatial alignment with z(s).
  - **Training instability**: Antithetic time sampling and learnable noise schedule are critical; do not remove them.

- First 3 experiments:
  1. **Validate volume-preservation**: Implement h and h⁻¹, verify ||h⁻¹(h(x)) - x|| < 1e-5 and det(Jacobian) ≈ 1 numerically on random images.
  2. **Single-scale baseline**: Train VDM on CIFAR, confirm ~2.49 BPD before adding hierarchy.
  3. **Two-scale PCDM**: Implement LP-PCDM with S=2 on CIFAR, target 2.36 BPD; ablate by replacing h with standard downsampling to observe degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does score matching under an Earth Mover's Distance (EMD) norm provide the same statistical guarantees (e.g., consistency, efficiency, asymptotic normality) as standard score matching?
- Basis in paper: [explicit] The conclusion states, "An open question is whether score matching under an EMD norm enjoys the same statistical guarantees as the standard score matching, e.g., consistency, efficiency, and asymptotic normality."
- Why unresolved: While the paper establishes a theoretical connection between its loss function and the EMD metric, it does not derive the statistical properties of the resulting estimator.
- What evidence would resolve it: Formal proofs demonstrating the consistency and asymptotic normality of the estimator derived from the EMD-based upper bound.

### Open Question 2
- Question: Can a direct compressor implementation effectively remove the bit overhead associated with the Bits-Back ANS algorithm used in this framework?
- Basis in paper: [explicit] In Section 6.3, the authors note that the current implementation has "significant overhead" and state, "This limitation can be removed by implementing a direct compressor... We leave this avenue of research for further work."
- Why unresolved: The current lossless compression results rely on Bits-Back ANS, which requires initialization bits and sequential encoding, making single-image compression inefficient.
- What evidence would resolve it: The development and benchmarking of a direct compressor variant of PCDM that eliminates initialization overhead on single images.

### Open Question 3
- Question: Are there effective non-linear hierarchical volume-preserving maps that outperform the linear Laplacian pyramid and wavelet transforms demonstrated?
- Basis in paper: [inferred] The paper defines a general class of maps H satisfying Eq. 8, but restricts experiments to two specific linear instances (Laplacian and Wavelet) because they are "well-known" and "simple."
- Why unresolved: The theoretical framework allows for complex homeomorphisms, but it is untested whether learning or designing non-linear volume-preserving maps offers better hierarchical representations for diffusion.
- What evidence would resolve it: An ablation study comparing the density estimation performance of PCDMs using learned non-linear volume-preserving maps versus the standard linear transforms.

## Limitations

- The Earth Mover's Distance connection remains theoretical without empirical validation of bound tightness
- Numerical stability of inverse transforms (particularly for wavelets) needs verification across diverse image content
- Generalization beyond natural images to other domains like medical imaging or spectrograms is untested

## Confidence

- **High confidence**: The mechanism of volume-preservation enabling tractable likelihood computation - supported by formal proofs and ablation experiments
- **Medium confidence**: The effectiveness of the cascaded diffusion architecture with scale-conditional factorization - BPD improvements demonstrated but conditioning pathway robustness needs validation
- **Low confidence**: The practical significance of the EMD-score matching connection - theoretical derivation exists but lacks empirical validation of the bound's tightness

## Next Checks

1. **EMD Bound Tightness**: Implement the Earth Mover's Distance between true and predicted score distributions using the wavelet-based linear-time approximation. Compute the actual gap between the training loss C(x) and the EMD-based bound to assess practical significance.

2. **Numerical Stability of Inverse Transforms**: For Haar wavelet implementation, verify the exact reconstruction property across diverse image content. Test with adversarial inputs (e.g., checkerboard patterns) to identify potential numerical instabilities in h⁻¹.

3. **Generalization Across Domains**: Train PCDMs on non-natural image datasets (e.g., spectrograms, medical imaging) to test whether the volume-preservation property and conditioning mechanisms generalize beyond CIFAR/ImageNet.