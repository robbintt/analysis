---
ver: rpa2
title: 'Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation
  Towards Safer Answers'
arxiv_id: '2510.12672'
source_url: https://arxiv.org/abs/2510.12672
tags:
- calm
- harmful
- concept
- concepts
- profs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CALM, a method that reduces harmful content
  generation in large language models by suppressing harmful concept directions in
  the latent space. CALM combines concept whitening with orthogonal projection to
  align and remove harmful concepts at inference time, without retraining.
---

# Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers

## Quick Facts
- arXiv ID: 2510.12672
- Source URL: https://arxiv.org/abs/2510.12672
- Authors: Ruben Belo; Marta Guimaraes; Claudia Soares
- Reference count: 40
- Primary result: CALM reduces harmful content generation by suppressing harmful concept directions in latent space via whitening and orthogonal projection

## Executive Summary
This paper introduces CALM, a method that reduces harmful content generation in large language models by suppressing harmful concept directions in the latent space. CALM combines concept whitening with orthogonal projection to align and remove harmful concepts at inference time, without retraining. Experiments show CALM improves safety metrics across multiple model families and datasets, often outperforming baseline methods, while also providing interpretable concept activations.

## Method Summary
CALM identifies harmful concept directions by whitening and aligning answer embeddings, then removes these directions via orthogonal projection during inference. The method extracts embeddings from harmful and harmless answers, applies ZCA whitening to decorrelate the latent space, uses SVD to find principal concept directions, learns an orthogonal rotation to align these with canonical axes, and projects out harmful dimensions before inverse transformation. This is done entirely at inference time without model retraining.

## Key Results
- CALM outperforms base models in harmfulness reduction in 23 out of 32 cases across diverse models and datasets
- Improves safety metrics while maintaining reasonable perplexity on safe answers
- Provides interpretable concept activations through aligned concept directions
- Effective across multiple model families (LLaMA, Mistral) but mixed results on Gemma

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whitening decorrelates the latent space, improving separability of harmful vs. harmless concepts.
- Mechanism: ZCA whitening transforms embeddings to zero mean and identity covariance, converting anisotropic feature representations into isotropic ones. This makes concept directions more orthogonal and easier to isolate.
- Core assumption: Harmful and harmless concepts occupy distinct but potentially correlated directions that whitening can disentangle.
- Evidence anchors: [abstract] "Leveraging concept whitening technique from Computer Vision combined with orthogonal projection, CALM removes unwanted latent directions"; [section 2] "The whitening transformation ensures zero mean and identity covariance"

### Mechanism 2
- Claim: Orthogonal rotation aligns identified concept directions with canonical axes for interpretable manipulation.
- Mechanism: After whitening, SVD extracts principal directions from harmful and harmless embedding sets. An orthogonal matrix Q is learned to align these top-K directions with the first 2K canonical axes.
- Core assumption: The top-K singular vectors capture the most salient conceptual dimensions, and aligning them to axes preserves their semantic meaning.
- Evidence anchors: [section 2] "We apply Singular Value Decomposition (SVD) to the projected, whitened embeddings"; [section 2] "The orthogonal matrix Q is learned such that each selected concept direction Cj is aligned with one of the first 2K basis axes"

### Mechanism 3
- Claim: Zeroing aligned harmful dimensions via projection reduces harmful output generation at inference time.
- Mechanism: A diagonal projection matrix P sets harmful concept indices to zero while preserving others. The modified embedding is inverse-transformed back to the original space before softmax.
- Core assumption: The model relies on these harmful dimensions to generate harmful outputs; removing them reduces harm without destroying language modeling capability.
- Evidence anchors: [abstract] "CALM reduces harmful outputs and outperforms baseline methods in most metrics"; [section 4.3] Table 4 shows CALM surpasses base models in harmfulness reduction in 23 out of 32 cases

## Foundational Learning

- Concept: ZCA Whitening (vs PCA Whitening)
  - Why needed here: The paper uses ZCA whitening specifically to preserve spatial semantics while decorrelating. Understanding the difference from PCA whitening is critical for debugging why certain features survive or vanish.
  - Quick check question: Given a covariance matrix Σ = UΛU^T, what is the ZCA whitening matrix and how does it differ from PCA whitening?

- Concept: Orthogonal Projection onto Subspace Complements
  - Why needed here: The core intervention is projecting embeddings onto the orthogonal complement of harmful directions. You must understand why (I - P) removes the subspace and why this is reversible only before projection.
  - Quick check question: If P_toxic = Σv_i v_i^T for harmful directions {v_i}, what does (I - P_toxic)x compute?

- Concept: SVD for Concept Direction Extraction
  - Why needed here: The method extracts "principal directions" via right singular vectors. Understanding why V (not U) contains directions in the original feature space is essential for correct implementation.
  - Quick check question: For X = UΣV^T where X is d×N embeddings, which matrix contains the concept directions in R^d?

## Architecture Onboarding

- Component map:
  1. **Embedding Extraction**: Mean-pool token embeddings from last decoder layer (d-dimensional, e.g., 4096 for LLaMA-7B)
  2. **Whitening Module (W)**: Precomputed ZCA whitening matrix from normal corpus embeddings
  3. **Alignment Module (Q)**: Learned orthogonal rotation aligning harmful/harmless directions to canonical axes
  4. **Projection Module (P)**: Diagonal mask zeroing K harmful concept indices
  5. **Inverse Transform**: W^(-1) Q^(-1) to recover modified embedding for softmax

- Critical path:
  - Offline: Gather X_norm (Alpaca), X_neg (harmful answers), X_pos (harmless answers) → compute W via ZCA → project & center → SVD for concept directions → learn Q
  - Inference: For each token embedding x → apply (W, Q, P) → inverse transform → softmax

- Design tradeoffs:
  - **Number of concepts (K)**: Higher K removes more harmful directions but risks over-pruning (see Table 8 ablation). Paper uses K=5-20.
  - **With vs without alignment**: CALM-noAlign (Appendix B) skips Q, losing interpretability but simplifying pipeline
  - **Embedding source**: Using last decoder layer captures final representations; earlier layers might capture different concept encodings (not explored)

- Failure signatures:
  - **High perplexity on safe answers**: Over-aggressive projection (K too large or wrong directions identified)
  - **Unstable SVD**: Paper notes some models showed unstable decompositions—likely due to insufficient or low-quality embedding samples
  - **Gemma family underperformance**: Table 4 shows mixed results, suggesting architecture-specific concept encoding differences

- First 3 experiments:
  1. **Sanity check on toy data**: Create synthetic embeddings with known harmful/benign directions. Verify CALM identifies and removes only harmful directions.
  2. **K-sensitivity sweep**: On a single model (e.g., Llama 3 8B Instruct), sweep K ∈ {1, 2, 5, 10, 15, 20} and plot PPL_safe vs PPL_unsafe tradeoff curve.
  3. **Ablation: alignment vs no alignment**: Compare CALM vs CALM-noAlign on the Harmful Q&A dataset to quantify interpretability vs performance tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can methods be developed to disentangle overlapping or entangled harmful concepts in the latent space?
- Basis in paper: [explicit] "Handling overlapping or entangled concepts in the latent space is left for future work" (Limitations, page 8).
- Why unresolved: Concept axes may encode multiple behaviors (e.g., Concept 5 captures both "Identity Theft" and "Bomb Making"), reducing selectivity.
- What evidence would resolve it: Demonstration of improved concept separability metrics or targeted removal without affecting related benign concepts.

### Open Question 2
- Question: Does increasing the volume of embeddings used during whitening improve the quality and stability of CALM across diverse model families?
- Basis in paper: [inferred] Authors note "preliminary tests on additional models showed unstable SVD decompositions" and suggest "Increasing this volume may improve both quality and stability" (Limitations, page 8).
- Why unresolved: Current experiments use ~10,000 phrase embeddings; scaling impact on decomposition stability is unknown.
- What evidence would resolve it: Experiments with orders of magnitude more embeddings showing consistent SVD stability and perplexity improvements across varied architectures.

### Open Question 3
- Question: How can fine-grained harmful concepts be identified and manipulated to understand their specific impact on model behavior?
- Basis in paper: [explicit] "We plan to explore more fine-grained harmful concepts in future work to better understand their specific impact" (Limitations, page 8).
- Why unresolved: Current approach groups harmfulness broadly; nuance between concept types (e.g., violence types) is unexplored.
- What evidence would resolve it: Granular concept taxonomies with correlated behavioral changes when individually suppressed.

### Open Question 4
- Question: Does CALM generalize effectively to non-English languages, and does concept alignment transfer across linguistic latent spaces?
- Basis in paper: [explicit] "Our evaluation was conducted solely on English text, and future research could benefit from expanding this approach to other languages" (Limitations, page 8).
- Why unresolved: Whitening and projection rely on language-specific embeddings; cross-linguistic transfer is untested.
- What evidence would resolve it: Multilingual evaluation showing comparable safety improvements without language-specific retraining.

## Limitations

- Concept separability assumption may not hold for entangled harmful concepts, limiting effectiveness
- Mixed performance across model families suggests architecture-specific encoding differences
- Requires curated harmful/normal answer datasets for concept direction extraction, limiting generalization

## Confidence

**High confidence** in: The core mechanism of whitening to decorrelate latent space and the mathematical validity of orthogonal projection for concept removal.

**Medium confidence** in: The claim that CALM outperforms baseline methods in most metrics.

**Low confidence** in: The interpretability claims regarding what harmful concept directions actually represent.

## Next Checks

1. **Concept direction stability analysis**: For a single model family (e.g., LLaMA 3 8B), vary the training dataset composition and size for X_neg/X_pos, then measure how much the extracted concept directions change. Compute cosine similarity between direction sets to quantify stability.

2. **Linear separability quantification**: Before applying CALM, compute the maximum mean discrepancy or Fisher discriminant ratio between harmful and harmless answer embeddings. This would empirically test whether the linear separability assumption holds for each model family.

3. **Safety domain generalization test**: Apply CALM trained on HarmfulQA to a distinct safety-critical domain (e.g., medical advice generation or financial guidance) and evaluate whether harmful concept suppression transfers or causes domain-specific failures.