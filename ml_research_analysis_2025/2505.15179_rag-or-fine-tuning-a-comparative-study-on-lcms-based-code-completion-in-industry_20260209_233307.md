---
ver: rpa2
title: RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry
arxiv_id: '2505.15179'
source_url: https://arxiv.org/abs/2505.15179
tags:
- code
- fine-tuning
- completion
- retrieval
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the effectiveness of Retrieval-Augmented
  Generation (RAG) versus fine-tuning for adapting Large Code Models (LCMs) to industrial
  code completion tasks. Using a proprietary C++ codebase of over 160,000 files from
  Tencent''s WXG department, the researchers compare six LCMs across three dimensions:
  effectiveness, efficiency, and parameter sensitivity.'
---

# RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry
## Quick Facts
- arXiv ID: 2505.15179
- Source URL: https://arxiv.org/abs/2505.15179
- Reference count: 40
- Primary result: RAG with BM25 retrieval outperforms fine-tuning and other RAG methods for industrial code completion, with combined approaches showing synergistic gains

## Executive Summary
This study compares Retrieval-Augmented Generation (RAG) versus fine-tuning for adapting Large Code Models (LCMs) to industrial code completion tasks. Using a proprietary C++ codebase of over 160,000 files from Tencent's WXG department, researchers evaluate six LCMs across effectiveness, efficiency, and parameter sensitivity dimensions. The findings demonstrate that RAG with BM25 retrieval achieves the highest performance, with the combination of RAG and fine-tuning producing synergistic effects that yield further improvements across all evaluation metrics.

## Method Summary
The researchers conducted a comparative study using a proprietary C++ codebase from Tencent's WXG department containing over 160,000 files. They evaluated six different LCMs across three dimensions: effectiveness (measured through exact match metrics), efficiency (computational resource usage and throughput), and parameter sensitivity (how performance scales with model size and configuration). The study specifically compared pure fine-tuning approaches against various RAG configurations, including BM25-based retrieval, and examined the potential synergies when combining both methods.

## Key Results
- RAG with BM25 retrieval achieved 116.9% improvement in exact match over base models, outperforming fine-tuning and other RAG methods
- Combining RAG and fine-tuning produced synergistic effects with additional performance gains across all evaluation metrics
- RAG demonstrated better scalability, maintaining performance gains as codebase size increased, while fine-tuning showed diminishing returns beyond 90,000 files

## Why This Works (Mechanism)
RAG works effectively for code completion by leveraging retrieval to access relevant code context from large codebases, allowing models to incorporate up-to-date and domain-specific information without expensive retraining. The BM25 retrieval method particularly excels at finding semantically relevant code snippets based on term frequency and inverse document frequency, which aligns well with the structured nature of code. Fine-tuning adapts model parameters to the specific distribution of the target codebase, but this adaptation becomes less effective as the codebase grows larger and more diverse. The synergistic effect occurs because RAG provides contextual relevance while fine-tuning optimizes the model's ability to process that context, with each method compensating for the other's limitations.

## Foundational Learning
- Large Code Models (LCMs): Why needed - Foundation models trained on vast code repositories that serve as base models for adaptation; Quick check - Verify the model was pre-trained on diverse code datasets
- Retrieval-Augmented Generation (RAG): Why needed - Framework that combines retrieval of external knowledge with generative models; Quick check - Confirm retrieval mechanism is properly integrated with the base model
- BM25 retrieval: Why needed - Probabilistic retrieval function that ranks documents based on term frequency and document length; Quick check - Validate retrieval quality through precision-recall metrics
- Fine-tuning: Why needed - Process of adapting pre-trained models to specific downstream tasks or domains; Quick check - Monitor training loss and validation performance during adaptation
- Code completion metrics: Why needed - Standardized evaluation measures for assessing code generation quality; Quick check - Ensure exact match and other metrics are computed correctly

## Architecture Onboarding
Component map: Codebase -> Retrieval Engine (BM25) -> RAG Module -> Base LCM -> Output Generator
Critical path: User input → Retrieval Engine → Context augmentation → LCM inference → Code completion output
Design tradeoffs: RAG prioritizes runtime flexibility and scalability at the cost of retrieval overhead; Fine-tuning offers faster inference but requires expensive retraining and shows diminishing returns with larger codebases
Failure signatures: RAG failures manifest as irrelevant retrieval results degrading output quality; Fine-tuning failures appear as overfitting to training data or poor generalization to unseen code patterns
First experiments: (1) Benchmark BM25 retrieval quality on a sample of the codebase, (2) Test base LCM performance on code completion without any adaptation, (3) Compare throughput and latency between RAG and fine-tuned models on identical queries

## Open Questions the Paper Calls Out
None

## Limitations
- Limited scalability testing with only two codebase sizes evaluated, constraining confidence in scalability conclusions
- Single programming language focus (C++) and single industrial context restricts generalizability claims
- Efficiency measurements constrained by testing only one retrieval method (BM25) and one fine-tuning variant

## Confidence
- Comparative results between RAG and fine-tuning approaches: High
- Scalability conclusions: Medium
- Generalization claims across languages and contexts: Low
- Efficiency measurements across different methods: Medium

## Next Checks
- Replicate experiments across multiple programming languages and diverse codebase types to assess generalizability
- Test additional retrieval methods (semantic, hybrid) and fine-tuning variants to validate efficiency claims
- Conduct long-term deployment studies to measure real-world performance under varying workloads and maintenance cycles