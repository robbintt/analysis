---
ver: rpa2
title: 'Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants'' Question-Answering
  in Asynchronous Learning Environments'
arxiv_id: '2509.17961'
source_url: https://arxiv.org/abs/2509.17961
tags:
- pedagogical
- post
- learning
- level
- discussion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first pedagogical evaluation framework
  for Virtual Teaching Assistants (VTAs) in asynchronous online forums, grounded in
  learning science theory. The framework defines five pedagogical levels of increasing
  cognitive complexity and operationalizes them into observable instructional behaviors.
---

# Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments

## Quick Facts
- arXiv ID: 2509.17961
- Source URL: https://arxiv.org/abs/2509.17961
- Authors: Li Siyan; Zhen Xu; Vethavikashini Chithrra Raghuram; Xuanming Zhang; Renzhe Yu; Zhou Yu
- Reference count: 40
- Primary result: Introduces first pedagogical evaluation framework for VTAs in asynchronous forums, showing context dilution effects and complexity gradients in automated classification

## Executive Summary
This paper introduces the first pedagogical evaluation framework for Virtual Teaching Assistants (VTAs) in asynchronous online forums, grounded in learning science theory. The framework defines five pedagogical levels of increasing cognitive complexity and operationalizes them into observable instructional behaviors. Expert annotations of simulated VTA responses reveal that including forum-level context can reduce pedagogical effectiveness in lower-level tasks. Automatic classifiers built using these annotations perform reliably at lower pedagogical levels but struggle with higher-level evaluations and cross-model generalization.

## Method Summary
The study developed a five-level pedagogical rubric aligned with learning science theories, then collected 300 academic forum posts from 116 students across 85 courses. VTA responses were generated using Llama-3-70B-Instruct in both context-free and forum-level context conditions. Expert annotators rated responses using the rubric, achieving ICC scores of 0.78-0.81. Automatic classifiers were built using two approaches: prompt-based classifiers with GPT-4o-mini/4o and DSPy's SIMBA optimizer, and fine-tuned open-weight models (Qwen3-4B, Qwen3-8B, ModernBERT-base) trained on synthetic data. The framework was validated through in-distribution and out-of-distribution tests across multiple LLM response sources.

## Key Results
- Expert annotations achieved ICC = 0.78-0.81, demonstrating good inter-rater reliability for the pedagogical rubric
- Context inclusion caused 59.7% of Level 2 responses to decrease by 1-2 points, suggesting "context dilution" effects
- Automatic classifiers achieved 83-86% F1 on Level 1 but only 30-60% on Levels 4-5, showing complexity gradients
- Prompt-based classifiers with SIMBA optimization outperformed baseline configurations by 5-15 percentage points
- Fine-tuned models matched or exceeded prompt-based performance on in-distribution data but showed limited cross-model generalization

## Why This Works (Mechanism)

### Mechanism 1: Theory-to-Behavior Operationalization
Abstract pedagogical goals can be systematically translated into observable, ratable instructional behaviors through hierarchical mapping from cognitive mechanisms → pedagogical goals → concrete behavioral rubrics with 4-point scales. This creates shared standards between human experts and automated classifiers, assuming observable response behaviors validly reflect underlying pedagogical capability.

### Mechanism 2: Context Dilution Effect
Adding forum-level peer context reduces pedagogical effectiveness at Levels 1–4 by shifting model attention from deep engagement with the original question toward surface-level connections across posts. The degradation stems from attention allocation issues in long-context processing, where models prioritize the final instruction (Level 5 goal).

### Mechanism 3: Complexity Gradient in Classification
Automated pedagogical classification accuracy decreases as pedagogical level complexity increases because lower levels have more concrete, observable features while higher levels require inferring latent cognitive states or social dynamics from text alone.

## Foundational Learning

- **Constructivist & Sociocultural Learning Theory**: The framework grounds its pedagogical goals in the view that learning is "an active, socially mediated process of meaning-making," explaining why collaborative knowledge construction (Level 5) is the highest level. Quick check: Can you explain why "clarifying confusion" and "fostering higher-order thinking" are treated as distinct levels rather than simultaneous goals?

- **ICAP Framework (Interactive > Constructive > Active > Passive)**: The framework organizes pedagogical goals "hierarchically by cognitive engagement and instructional complexity, in accordance with well-established theories (ICAP)," explaining why Level 3 (higher-order thinking) is more complex than Level 1 (clarification). Quick check: How would you classify a VTA response that provides a correct definition but doesn't prompt further reasoning?

- **Inter-Rater Reliability (ICC)**: The paper uses ICC to validate rubric usability, with 0.78 indicating "good reliability." Without this, the annotation framework couldn't support automated classifier training. Quick check: Why is ICC more appropriate than simple percent agreement for multi-level ordinal ratings?

## Architecture Onboarding

- **Component map**: Student forum post + course context + (optional) similar peer posts → VTA response generator (Llama-3-70B-Instruct or other LLM) → Rubric definitions + post-response pairs → expert annotators OR automated classifier (GPT-4o-mini/4o with DSPy, or fine-tuned Qwen/ModernBERT) → Training annotations → prompt optimization (SIMBA) or synthetic data generation → fine-tuning → Validation layer: In-distribution test (Llama3-Test) + out-of-distribution tests (New-LLM-Test, Llama3-MOOC)

- **Critical path**: Define pedagogical rubric levels aligned to your learning context → Collect representative post-response pairs and have domain experts annotate using rubric → Measure inter-rater reliability; iterate on rubric if ICC < 0.7 → Train classifiers using prompt optimization or fine-tuning on synthetic+real data → Evaluate on held-out data; expect lower accuracy at higher pedagogical levels

- **Design tradeoffs**: Context-free vs. forum-level context (context-free scores higher on Levels 1–4 but cannot address Level 5); Prompt-based vs. fine-tuned classifiers (prompt-based generalizes better across response sources; fine-tuned matches or exceeds on in-distribution data); API vs. local models (API models perform better at higher levels; local models address privacy concerns but need synthetic data scaling)

- **Failure signatures**: Classifiers systematically rate responses higher than experts, especially on out-of-distribution VTA outputs; Presence/absence of Markdown formatting affects classifier predictions differently across test sets; Classifiers conflate adjacent levels more often at Levels 3–5

- **First 3 experiments**: Implement the 5-level rubric and collect/generate forum post-VTA response pairs; Have 2 trained annotators rate responses independently; resolve discrepancies >1 via discussion and discrepancies =1 via third reviewer; target ICC >0.75; Generate VTA responses with and without peer context for the same 30 posts; compare score distributions across all 5 levels; Expect Level 2 decline with context; Train a GPT-4o-mini classifier with level-specific prompt optimization on 100 annotations; evaluate on 50 held-out pairs; Establish Level 1 baseline (target: >80% F1) before optimizing for higher levels

## Open Questions the Paper Calls Out
None

## Limitations
- The framework was developed using forum posts from a single MOOC provider (Coursera), limiting generalizability to other educational contexts
- Expert annotations represent a small pool of three annotators, despite showing good inter-rater reliability
- Automatic classifiers were trained primarily on responses from one LLM (Llama-3-70B-Instruct), raising questions about cross-model robustness

## Confidence
- **High confidence**: The framework's ability to operationalize pedagogical goals into observable behaviors (supported by ICC analysis and expert agreement)
- **Medium confidence**: Context dilution effects and their impact on specific pedagogical levels (based on controlled experiments with limited model diversity)
- **Medium confidence**: The difficulty gradient in automated classification (validated across multiple classifier configurations but limited to specific model families)

## Next Checks
1. **External validity testing**: Apply the framework to forum posts from diverse educational contexts (K-12, higher education, professional training) to assess generalizability beyond MOOCs
2. **Learning outcome correlation**: Conduct a longitudinal study measuring whether VTA responses with higher rubric scores predict improved student performance on subsequent assessments
3. **Multi-model robustness**: Generate responses from diverse LLM architectures (including smaller models and different training paradigms) to test classifier generalization across the full spectrum of potential VTA implementations