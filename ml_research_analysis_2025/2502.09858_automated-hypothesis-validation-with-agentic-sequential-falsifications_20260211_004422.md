---
ver: rpa2
title: Automated Hypothesis Validation with Agentic Sequential Falsifications
arxiv_id: '2502.09858'
source_url: https://arxiv.org/abs/2502.09858
tags:
- test
- hypothesis
- grap2
- il-2
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "POPPER is a novel LLM-based framework for rigorous automated validation\
  \ of free-form hypotheses. Inspired by Karl Popper\u2019s falsification principle,\
  \ it employs specialized agents to design and execute experiments targeting measurable\
  \ implications of a hypothesis."
---

# Automated Hypothesis Validation with Agentic Sequential Falsifications

## Quick Facts
- **arXiv ID:** 2502.09858
- **Source URL:** https://arxiv.org/abs/2502.09858
- **Reference count:** 40
- **Key outcome:** LLM-based framework POPPER achieves rigorous automated hypothesis validation with strict Type-I error control and expert-level performance

## Executive Summary
POPPER is a novel LLM-based framework for rigorous automated validation of free-form hypotheses. Inspired by Karl Popper's falsification principle, it employs specialized agents to design and execute experiments targeting measurable implications of a hypothesis. A sequential testing framework ensures strict Type-I error control while adaptively gathering evidence. Tested across six domains, POPPER achieved robust error control, high statistical power, and scalability. In expert user studies, it matched human scientists' performance while reducing validation time by an order of magnitude.

## Method Summary
POPPER uses a three-agent system: a Design Agent proposes falsification tests, a Relevance Checker ensures logical implication between sub-hypotheses and the main hypothesis, and an Execution Agent runs code to generate p-values. The framework converts p-values to e-values and multiplies them sequentially, rejecting the null hypothesis when cumulative e-value exceeds 1/α. This approach enables optional stopping while maintaining strict Type-I error control.

## Key Results
- Achieved strict Type-I error control (≤ α) across six scientific domains
- Matched human expert performance in user studies while reducing validation time by 10x
- ReAct-based iterative execution outperformed one-shot code generation in statistical power

## Why This Works (Mechanism)

### Mechanism 1
The system maintains statistical rigor while adaptively gathering evidence by converting p-values into e-values, which allows for flexible aggregation and optional stopping. The "p-to-e calibrator" (e_i = κ p_i^(κ-1)) transforms each test result, and these e-values are multiplied sequentially (E = ∏ e_i). If the cumulative e-value exceeds 1/α, the null hypothesis is rejected with strict error control. The system continues testing if E < 1/α, otherwise stops and returns validation status.

### Mechanism 2
Free-form hypotheses are validated by decomposing them into specific, falsifiable sub-hypotheses via a self-refining LLM agent. An "Experiment Design Agent" proposes a falsification test, and a "Relevance Checker" (LLM-as-judge) verifies that the sub-hypothesis is logically implied by the main hypothesis. This prevents the agent from running irrelevant tests that could invalidate the error control. If the Relevance Checker score is below threshold τ, the experiment is discarded and a new one is proposed.

### Mechanism 3
The framework grounds abstract reasoning in concrete data by generating and executing code, reducing the risk of statistical hallucination. The "Experiment Execution Agent" uses a ReAct (Reason + Act) loop, writing Python code to query datasets, executing it to obtain observations, and iteratively fixing errors. This process yields a concrete p-value rather than a textual guess. If code execution fails after maximum retries, the experiment is logged as failed and skipped in aggregation.

## Foundational Learning

- **Concept: E-values (vs. P-values)**
  - **Why needed here:** Traditional p-value aggregation requires independence and does not allow optional stopping. E-values allow POPPER to stop early if evidence is strong or continue if weak, while maintaining validity.
  - **Quick check question:** Why does POPPER multiply e-values rather than averaging them to aggregate evidence?

- **Concept: Falsification Principle (Popperian)**
  - **Why needed here:** The system is designed to refute hypotheses, not prove them. Understanding this clarifies why the agent looks for "implications" that, if false, would invalidate the main hypothesis.
  - **Quick check question:** If a sub-hypothesis is falsified (p < 0.05), does that automatically mean the main hypothesis is false?

- **Concept: ReAct (Reason + Act) Loop**
  - **Why needed here:** The execution agent isn't just a code generator; it observes outputs (like error messages or data summaries) and adjusts its strategy, which is critical for robust data analysis.
  - **Quick check question:** In the POPPER architecture, does the ReAct loop happen in the Design Agent or the Execution Agent?

## Architecture Onboarding

- **Component map:** Design Agent -> Relevance Checker -> Execution Agent (ReAct) -> P-value -> E-value Conversion -> Aggregation
- **Critical path:** Hypothesis Input → Design Agent → Relevance Check → Execution Agent (ReAct) → P-value → E-value Conversion → Aggregation
- **Design tradeoffs:** ReAct (Iterative execution) outperforms CodeGen (One-shot generation) in power because it can debug its own code. Removing the Relevance Checker significantly inflates Type-I error because the system aggregates irrelevant evidence.
- **Failure signatures:** Misinterpreted P-Value (35.9%): Agent misunderstands directionality or context of the p-value. Falsification Breaks Implication (17.2%): The designed test does not actually logically test the main hypothesis.
- **First 3 experiments:**
  1. Run POPPER-NoReleCheck: Run the system without the relevance checker to observe the inflation of Type-I error on a synthetic null dataset (verifying Assumption 1 necessity).
  2. Trace ReAct Loop: Inspect the execution logs of the "Experiment Execution Agent" to see how it recovers from a "FileNotFound" or "KeyError" error during data loading.
  3. Sensitivity Analysis: Vary the alpha α (e.g., 0.05, 0.1, 0.2) on a known positive hypothesis (like "GRAP2 regulates IL-2" in the paper) to confirm the system scales the required evidence threshold (1/α) correctly.

## Open Questions the Paper Calls Out

### Open Question 1
Can POPPER be extended to control the False Discovery Rate (FDR) when validating a large batch of hypotheses? The paper explicitly states that the current Type-I error control is insufficient for scientific discovery at scale and suggests extending the framework to FDR control using the e-BH procedure.

### Open Question 2
How does POPPER perform in active, real-world environments like automated wet-lab experiments versus the static data analysis tested in the paper? The paper describes POPPER as a "general framework" capable of "newly conducted procedures" and "laboratory experiments," but limits the actual instantiation and evaluation to a static corpus of datasets.

### Open Question 3
How can the LLM agents be refined to reduce the 28.1% failure rate caused by ineffective falsification experiment design? The Error Analysis identifies "ineffective falsification experiment design" and "misinterpreted p-values" as the primary failure modes, explicitly stating "further improvement" is needed.

## Limitations
- Performance consistency across different LLM models and prompts remains uncertain
- The framework's generalization to extremely complex or abstract hypotheses is untested
- The six domains evaluated may not represent the full breadth of scientific hypothesis validation scenarios

## Confidence
- **High confidence:** Statistical error control mechanism (p-to-e conversion and sequential testing), code execution pipeline (ReAct loop), and replication results in expert user studies
- **Medium confidence:** Generalization across scientific domains, relevance checking reliability, and scalability claims
- **Low confidence:** Performance consistency across different LLM models and prompts, handling of extremely complex or abstract hypotheses

## Next Checks
1. **Reproduce Type-I error inflation:** Run POPPER without the Relevance Checker on a synthetic null hypothesis to empirically verify the theoretical error control depends on Assumption 1.
2. **Validate e-value calibration:** Test different values of the calibrator parameter κ across various alpha levels to confirm the p-to-e transformation maintains the claimed error bounds.
3. **Cross-domain robustness:** Apply POPPER to a new scientific domain (e.g., climate science or particle physics) to test whether the framework generalizes beyond the six domains evaluated.