---
ver: rpa2
title: 'Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination
  Detection'
arxiv_id: '2510.22395'
source_url: https://arxiv.org/abs/2510.22395
tags:
- hallucination
- language
- languages
- question
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAP, a multilingual dataset for detecting
  hallucinations in large language models (LLMs) when generating scientific text.
  The dataset covers nine languages, including five high-resource and four low-resource
  languages, with 900 curated questions and over 7,000 LLM-generated answers from
  16 publicly available models.
---

# Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection

## Quick Facts
- **arXiv ID:** 2510.22395
- **Source URL:** https://arxiv.org/abs/2510.22395
- **Reference count:** 0
- **Key outcome:** CAP dataset introduced for multilingual hallucination detection in scientific text, revealing existing tools perform near-random on scientific domains

## Executive Summary
This paper introduces CAP, a multilingual dataset for detecting hallucinations in large language models (LLMs) when generating scientific text. The dataset covers nine languages, including five high-resource and four low-resource languages, with 900 curated questions and over 7,000 LLM-generated answers from 16 publicly available models. Each instance is annotated for factual correctness and fluency, capturing both the linguistic quality and factual accuracy of the outputs. The study highlights that existing hallucination detection tools perform poorly on CAP, indicating the dataset's complexity and the need for improved evaluation methods. The research also reveals that question types and linguistic cues are more predictive of hallucinations than citation counts, emphasizing the importance of considering both fluency and factuality in hallucination detection.

## Method Summary
The CAP dataset is constructed from 293 award-winning ACL Anthology papers (1995-2024), with 100 questions created per language across nine languages. Native annotators create questions independently per language rather than through translation. LLM responses are generated using 16 models with varied hyperparameters (top-p=0.9/0.95, top-k=50, temp=0.1/0.2), producing 6-18 outputs per question. Each response is annotated for factuality (binary) and fluency (yes/minor/no). The dataset includes metadata such as paper information, generation parameters, and annotation labels. High-resource languages have train/val/test splits while low-resource languages only have test splits. The study evaluates six baseline hallucination detection models using zero-shot inference on CAP.

## Key Results
- Existing hallucination detectors achieve Macro-F1 scores at or below random (0.29-0.46) on CAP, with most performing below 0.46 overall
- Question types show significant association with hallucination rates (χ²(14, N=877) = 61.21, p < .001, Cramér's V = 0.26), with Verification and Goal orientation questions showing lower hallucination rates
- Low-resource languages (ml, te, bn, gu) primarily struggle with fluency issues, while English struggles more with factuality
- Citation counts do not significantly predict hallucination rates after Bonferroni correction
- FAVA's recall decreases while HDM2's improves when narrowing context from full documents to relevant sections

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hallucination detection requires jointly assessing fluency and factuality as separate but interacting dimensions.
- **Mechanism:** The paper operationalizes hallucination as outputs that are "fluent but not factual"—linguistically coherent yet factually inconsistent with source publications. This dual-facet framing captures that non-fluent outputs represent a different failure mode than fluent falsehoods.
- **Core assumption:** Fluency and factuality are independent enough to warrant separate annotation but interact in determining what constitutes a "hallucination proper."
- **Evidence anchors:**
  - Each instance is annotated with binary factuality label and fluency label (yes/minor/no)
  - Figure 4 shows different languages exhibit different fluency-factuality distributions
  - HalluVerse25 and BHRAM-IL similarly operationalize fine-grained hallucination categories

### Mechanism 2
- **Claim:** Question type (illocutionary function) predicts hallucination rates more reliably than topic popularity proxies like citation counts.
- **Mechanism:** Using Graesser and Person (1994)'s taxonomy, Verification and Goal orientation questions exhibit significantly lower hallucination rates, while Quantification and Concept completion questions show higher rates.
- **Core assumption:** Question type influences LLM behavior through differences in required reasoning complexity.
- **Evidence anchors:**
  - χ²(14, N=877) = 61.21, p < .001, Cramér's V = 0.26
  - Mann-Whitney U-tests show no significant difference in citation counts between hallucinations and non-hallucinations after Bonferroni correction

### Mechanism 3
- **Claim:** Existing hallucination detectors fail on scientific text because they are not calibrated for domain-specific terminology and long-context reasoning.
- **Mechanism:** Six baseline detectors achieve Macro-F1 scores at or below random (0.29–0.46) on CAP. The paper suggests long-context documents require chunking, which biases toward false-positive hallucination classifications.
- **Core assumption:** Poor performance reflects genuine domain mismatch rather than implementation issues.
- **Evidence anchors:**
  - Off-the-shelf models perform remarkably poorly, always below 0.46
  - FAVA's recall decreases with narrowed context while HDM2's improves

## Foundational Learning

- **Concept: Natural Language Inference (NLI) as hallucination detection proxy**
  - Why needed here: Most benchmarked detectors frame hallucination detection as an entailment problem—does the generated answer entail, contradict, or remain neutral to the source?
  - Quick check question: Given a scientific claim and an LLM-generated summary, would you classify the relationship as entailment, contradiction, or neutral?

- **Concept: Reference-based vs. reference-free hallucination detection**
  - Why needed here: CAP evaluates both paradigms—SelfCheckGemma requires no source document while FAVA and HDM2 require grounding in source text.
  - Quick check question: If an LLM generates a fluent, plausible scientific claim that contradicts its training data but not the provided source document, would a reference-based detector catch it?

- **Concept: Low-resource language degradation patterns**
  - Why needed here: The paper shows LLMs exhibit different failure modes across languages—some produce fluent falsehoods, others produce disfluent outputs regardless of factuality.
  - Quick check question: Why might a model generate fluent Hindi but disfluent Gujarati, given both are Indic languages?

## Architecture Onboarding

- **Component map:** Paper metadata → Question creation → LLM response generation → Human annotation → Post-processing sanity check → Dataset with splits
- **Critical path:** Question creation from ACL papers → LLM response generation with varied hyperparameters → Human annotation → Post-processing sanity check
- **Design tradeoffs:**
  - Questions are independently created per language → enables language-specific perspectives but prevents direct cross-lingual comparison
  - Using paper abstract as optional context vs. full paper → trades off context richness against chunking artifacts
  - Single annotator per question → faster collection but no inter-annotator agreement measurement
- **Failure signatures:**
  - Chunked long-context inputs inflate hallucination predictions
  - Fluency filtering improves detection for low-resource languages but not consistently for high-resource
  - Models may conflate disfluency with hallucination
- **First 3 experiments:**
  1. Run HHEM and SelfCheckGemma on English test split to confirm reported Macro-F1 (~0.48–0.50). Vary chunk size to measure sensitivity.
  2. Subset evaluation on Verification vs. Quantification questions to verify hallucination rate differential.
  3. Evaluate detectors on fluent-only subset across Spanish and Telugu to quantify the fluency confound.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can hallucination detection methods be adapted to achieve reliable performance on multilingual scientific text?
- **Basis:** Existing tools perform near or below random on CAP, indicating fundamental limitations in out-of-domain scientific settings.
- **Why unresolved:** All six baseline models achieved macro-F1 scores below 0.46 overall.
- **Evidence needed:** Development of a detection method achieving consistently high F1 scores (>0.7) across all nine languages in CAP.

### Open Question 2
- **Question:** What are the causal mechanisms by which question type influences hallucination rates in scientific QA?
- **Basis:** The paper finds significant association between question type and hallucination occurrence (χ² = 61.21, p < .001).
- **Why unresolved:** While statistical association is established, the paper does not explain why certain question types are more susceptible.
- **Evidence needed:** Controlled experiments manipulating question type while holding other factors constant.

### Open Question 3
- **Question:** Why do different hallucination detection models show opposite responses to context narrowing?
- **Basis:** FAVA's recall decreases while HDM2's improves when narrowing context from full documents to relevant sections.
- **Why unresolved:** The paper observes this contingency but does not investigate the underlying causes.
- **Evidence needed:** Systematic analysis of model attention weights and intermediate representations with full vs. narrowed contexts.

## Limitations
- Single annotator per question prevents inter-annotator agreement measurement
- Questions are independently created per language rather than translated, preventing direct cross-linguistic comparison
- Poor baseline detector performance may reflect zero-shot inference rather than fundamental architectural limitations
- Reliance on LLM-based question type classification without human validation

## Confidence
- **High confidence:** Dataset construction methodology and basic evaluation metrics are well-documented and reproducible
- **Medium confidence:** Finding that existing detectors perform poorly on CAP is supported but may reflect implementation choices
- **Medium confidence:** Relationship between question types and hallucination rates is statistically significant but relies on LLM classification

## Next Checks
1. Replicate baseline detector performance on the English test split to confirm reported Macro-F1 scores (~0.48-0.50) and test sensitivity to chunking parameters.
2. Conduct human validation of question type classifications for a subset of instances to verify the association between question types and hallucination rates.
3. Evaluate baseline detectors on a fluency-filtered subset across high-resource (Spanish) and low-resource (Telugu) languages to quantify the fluency confound and test the claim that question types are more predictive than citation counts.