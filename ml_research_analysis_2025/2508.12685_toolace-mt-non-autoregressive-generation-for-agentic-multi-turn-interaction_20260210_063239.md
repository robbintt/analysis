---
ver: rpa2
title: 'ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction'
arxiv_id: '2508.12685'
source_url: https://arxiv.org/abs/2508.12685
tags:
- tool
- user
- turn
- data
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes ToolACE-MT, a non-autoregressive iterative
  generation framework for creating high-quality multi-turn agentic dialogues. Unlike
  costly autoregressive multi-agent simulations, ToolACE-MT generates full conversational
  trajectories through three stages: coarse-grained initialization, iterative refinement,
  and offline verification.'
---

# ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction

## Quick Facts
- arXiv ID: 2508.12685
- Source URL: https://arxiv.org/abs/2508.12685
- Reference count: 40
- Primary result: Achieves 40.25% multi-turn accuracy on BFCL-v3, outperforming autoregressive baselines

## Executive Summary
ToolACE-MT introduces a non-autoregressive iterative generation framework for creating high-quality multi-turn agentic dialogues. Unlike costly autoregressive multi-agent simulations, ToolACE-MT generates full conversational trajectories through three stages: coarse-grained initialization, iterative refinement, and offline verification. The approach builds structurally complete dialogue skeletons before filling semantic details, enabling global context awareness during planning. Experiments demonstrate that ToolACE-MT enables efficient, effective, and generalizable agentic data generation across different backbones and model sizes.

## Method Summary
ToolACE-MT is a three-stage pipeline for generating multi-turn agentic dialogues. First, it initializes a structurally complete yet semantically coarse dialogue skeleton with all subtasks and tool requirements specified upfront. Second, it iteratively refines this skeleton through complexity injection (adding realistic challenges like clarification requests and error recovery) and reasonability refinement (regenerating turns via mask-and-fill operations with LLM judge acceptance). Third, it applies hybrid offline verification using both rule-based checks for format violations and model-based evaluation with specialized LLM experts for semantic coherence. The approach uses LoRA fine-tuning on LLaMA3.1-8B-Instruct with specific hyperparameters and evaluation on BFCL-v3 multi-turn accuracy.

## Key Results
- Achieves 40.25% multi-turn accuracy on BFCL-v3, outperforming Llama3.1-8B-Inst (9.25%) and MAS baseline (31.38%)
- Shows strong generalization across different backbones and model sizes
- Hybrid offline verification reduces parameter hallucination errors by 51% in rule-based failures
- Iterative refinement partially substitutes for verification, with performance gap narrowing to <2% after 15+ iterations

## Why This Works (Mechanism)

### Mechanism 1
Non-autoregressive trajectory generation produces more coherent multi-turn dialogues than autoregressive multi-agent simulation by enabling global context awareness during planning. Rather than generating turns one-by-one without visibility into the full task trajectory, ToolACE-MT initializes a complete structural skeleton with all subtasks and tool requirements specified upfront. This allows the model to optimize the overall output structure before filling in semantic details. The core assumption is that multi-turn tool-calling coherence benefits more from holistic task planning than from turn-by-turn reactive generation.

### Mechanism 2
Iterative mask-and-fill refinement improves both dialogue complexity and semantic coherence through controlled perturbation-and-repair cycles. Two distinct operations alternate: complexity injection via mask-and-extend adds realistic challenges (clarification requests, error recovery, tool-awareness gaps, non-tool chitchat), and reasonability refinement via mask-and-fill regenerates randomly selected turns with an LLM judge deciding whether to accept changes. Selection probability decreases for previously chosen turns to encourage diverse refinement. The core assumption is that masked regeneration with acceptance criteria can fix local inconsistencies without destroying global structure established in initialization.

### Mechanism 3
Hybrid offline verification (rule-based + model-based) filters hallucinations more effectively than either approach alone, with complementary coverage. Rule-based checks catch format violations, executability failures, and detectable hallucinations (e.g., references to IDs not in history). Model-based evaluation decomposes verification into sub-questions handled by specialized LLM experts, catching semantic coherence issues and complex hallucinations rules cannot express. Final decision aggregates both signals. The core assumption is that hallucination types in multi-turn tool-calling are partially separable into syntactic/formatting errors (rule-catchable) and semantic inconsistencies (model-catchable).

## Foundational Learning

- **Partially Observable Markov Decision Process (POMDP)** for dialogue modeling: The paper formulates multi-turn agentic interaction as a POMDP where environment states remain latent. Understanding this framing clarifies why global planning helps—the assistant must act without full state visibility. Quick check: Can you explain why tool outputs are observations rather than state reveals in this formulation?

- **Non-autoregressive generation**: Core innovation is extending NAT from token-level to turn-level. Must understand that NAT sacrifices token-dependency modeling for parallelism and global optimization ability. Quick check: What tradeoff does non-autoregressive generation make compared to autoregressive generation, and why might it benefit structured dialogue planning?

- **Mask-and-fill / mask-predict decoding**: Iterative refinement is built on this technique. Understanding that masked positions are iteratively predicted and replaced is essential for implementing the refinement loop. Quick check: How does reducing selection probability for previously refined turns encourage diversity in the mask-and-fill process?

## Architecture Onboarding

- Component map: Tool Pool → Task Initializer → Trajectory Initializer → Initial Skeleton → Iterative Refinement Loop → Offline Verification → Valid Training Data

- Critical path: Task initialization → trajectory skeleton → complexity injection (at least 1 type) → reasonability refinement (1-5 passes) → verification. Skip verification only if refinement iterations are very high (>15).

- Design tradeoffs: More complexity injection types → more realistic dialogues but risk redundant patterns if same types repeat; more refinement iterations → higher quality but increased generation cost; stronger generator model → much higher pass rate but higher cost per sample.

- Failure signatures: Low pass rate in verification usually indicates parameter hallucination (51% of rule-based failures) or parameter extraction errors (23% of model-based failures); over-refinement if turns are repeatedly selected despite probability reduction; format drift with smaller generator models.

- First 3 experiments: 1) Baseline comparison: Generate 1000 samples each with ToolACE-MT vs MAS using same tool pool and generator, comparing API calls required, verification pass rate, and BFCL multi-turn accuracy after fine-tuning. 2) Ablation by stage: Generate data with initialization only, init + complexity injection, init + reasonability refinement, and full pipeline to measure incremental quality gains. 3) Refinement scaling: Vary reasonability refinement iterations (1, 5, 10, 15, 20) with and without verification, plotting accuracy vs cost to find optimal operating point.

## Open Questions the Paper Calls Out

- **Theoretical limit of iterative refinement**: What is the theoretical limit of iterative refinement in closing the quality gap with offline verification, and can convergence be mathematically characterized? The authors note the gap never fully disappears even after 30 refinement steps, but don't establish whether there exists a refinement count or strategy that could fully substitute for verification.

- **Smaller LLM viability**: Can smaller LLMs be enabled to perform high-quality ToolACE-MT generation through architectural changes or decomposition strategies? The paper reports that GPT-4o-mini and LLaMA3.1-8B-Inst failed to produce valid instances, leaving open whether smaller models could be made viable through techniques like context windowing or hierarchical decomposition.

- **Parameter hallucination reduction**: How can parameter hallucination be systematically reduced in agentic dialogue generation? Failure analysis reveals parameter hallucination (51% of rule-based failures) and parameter extraction (23% of model-based failures) are the top error types, identified as the primary bottleneck but without proposed targeted interventions.

- **Clarification vs execution trade-off**: What is the optimal balance between cautious multi-turn planning (asking clarifications) and aggressive single-turn execution for ambiguous user queries? The paper observes models trained with richer multi-turn supervision favor asking clarification questions, but no solution or optimal operating point is proposed.

## Limitations

- Tool pool composition is not specified or released, making it difficult to reproduce exact results or assess tool-dependency
- Verification decomposition details (specific sub-questions and aggregation logic) are not fully detailed
- Parameter schedules for mask-and-fill probability reduction and complexity injection log implementation specifics are unclear

## Confidence

**High confidence**: The core architectural framework (3-stage pipeline with initialization, iterative refinement, and verification) is well-specified and internally consistent. The BFCL-v3 multi-turn accuracy improvement from 9.25% to 40.25% is clearly demonstrated with appropriate baseline comparisons.

**Medium confidence**: The ablation studies showing individual component contributions are convincing, though some specific implementation details remain unclear. The generalization across model sizes and backbones is supported but could benefit from more diverse model comparisons.

**Low confidence**: The exact composition of the tool pool and its impact on results is uncertain. The paper doesn't release this critical component, making it difficult to assess whether results are tool-dependent or generalizable to different tool sets.

## Next Checks

1. **Tool pool dependency analysis**: Generate ToolACE-MT dialogues using different tool pools (your own curated set vs. existing datasets) and measure how BFCL-v3 accuracy varies to validate whether results depend critically on specific tool composition.

2. **Verification ablation with alternative judges**: Replace the model-based verification component with simpler alternatives (e.g., GPT-3.5, direct semantic similarity) while keeping rule-based checks constant. Compare pass rates and downstream performance to quantify the contribution of sophisticated expert decomposition.

3. **Refinement iteration scaling**: Systematically vary reasonability refinement iterations (1, 3, 5, 10, 15, 20) with complete ablation of verification. Plot accuracy vs. generation cost to identify the optimal operating point and test whether iterative refinement alone can substitute for verification as claimed.