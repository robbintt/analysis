---
ver: rpa2
title: Advancing Medical Representation Learning Through High-Quality Data
arxiv_id: '2503.14377'
source_url: https://arxiv.org/abs/2503.14377
tags:
- medical
- images
- dataset
- image
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how dataset quality impacts representation
  learning in medical vision-language models. The authors introduce Open-PMC, a high-quality
  dataset of 2.2 million medical image-text pairs extracted from PubMed Central articles,
  with enriched captions, in-text references, subfigure decomposition, and modality
  annotations.
---

# Advancing Medical Representation Learning Through High-Quality Data

## Quick Facts
- arXiv ID: 2503.14377
- Source URL: https://arxiv.org/abs/2503.14377
- Reference count: 34
- Primary result: High-quality curated dataset (Open-PMC) outperforms larger noisy datasets in medical vision-language tasks

## Executive Summary
This paper investigates how dataset quality impacts representation learning in medical vision-language models. The authors introduce Open-PMC, a high-quality dataset of 2.2 million medical image-text pairs extracted from PubMed Central articles, with enriched captions, in-text references, subfigure decomposition, and modality annotations. Through extensive experiments, they demonstrate that Open-PMC achieves superior or comparable performance to larger datasets like Biomedica and PMC-15M across zero-shot classification and retrieval tasks, highlighting that careful curation yields better representations than merely increasing data volume. Ablation studies confirm that subfigure decomposition and in-text reference summaries significantly enhance performance, especially in radiology. The findings underscore the critical role of data quality in advancing multimodal medical AI.

## Method Summary
The authors develop Open-PMC by extracting figures, captions, and in-text references from PubMed Central XML files, then applying multi-stage filtering to ensure medical relevance and quality. They decompose compound figures into subfigures using DETR-based detection, align subcaptions with subfigures using YOLOv3 and GPT-4o segmentation, and enrich captions with summarized in-text references using GPT-4o-mini. The final dataset contains 2.2 million pairs across radiology, microscopy, and visible light photography modalities. Training uses contrastive learning with ViT-B/16 and PubMedBERT encoders for 64 epochs, followed by zero-shot evaluation on classification and retrieval tasks.

## Key Results
- Open-PMC achieves best Average Recall (0.170) and MRR (0.653) vs. PMC-15M (0.160, 0.608) and Biomedica (0.154, 0.591)
- Subfigure decomposition improves pneumonia classification from 63.55 to 73.58 (+10.03 points)
- In-text reference summaries boost DeepEyeNet T2I retrieval from 0.157 to 0.183 (+0.026)
- Open-PMC with subfigures outperforms baseline on 11 of 19 classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Subfigure Decomposition for Cleaner Alignment
- Decomposing compound medical images into individual subfigures paired with specific subcaptions improves representation learning by creating precise visual-textual mappings
- Core assumption: The decomposition model accurately detects subfigure boundaries, and the label-to-subcaption alignment correctly matches text to image regions
- Evidence anchors: Table 3 shows PneumoniaMNIST+ improves from 63.55 (compound) to 73.58 (subfigures), a 10.03 point increase; ablation studies confirm subfigure decomposition enhances performance especially in radiology
- Break condition: The decomposition pipeline was "primarily optimized for radiology images." If accuracy degrades for microscopy or visible light photography, gains may not transfer

### Mechanism 2: Contextual Enrichment via In-Text Reference Summarization
- Augmenting captions with summarized in-text references from article bodies provides richer semantic signal for contrastive learning
- Core assumption: GPT-4o-mini summarization extracts relevant information without hallucination, and the cross-reference mapping correctly links text to figures
- Evidence anchors: Table 4 shows Open-PMC with in-text summaries achieves higher Average Recall (0.162 vs 0.160) and notable gains on DeepEyeNet T2I retrieval (0.183 vs 0.157)
- Break condition: For compound figures, determining which in-text references describe which subfigure is non-trivial. If summarization conflates information across subfigures, this introduces alignment noise

### Mechanism 3: Quality-First Curation Over Raw Scale
- Aggressive quality filtering produces better representations than increasing dataset size alone by reducing noise-to-signal ratio
- Core assumption: The filtering heuristics correctly identify high-quality pairs without excessive false negatives that would remove valuable diversity
- Evidence anchors: Open-PMC (2.2M pairs) achieves best Average Recall (0.170) and MRR (0.653) vs. PMC-15M (15M) and Biomedica (24M); ChiMed 2.0 abstract supports quality-over-quantity thesis
- Break condition: If downstream tasks require rare pathologies excluded by filtering, the smaller curated dataset may underperform regardless of average quality

## Foundational Learning

- **Contrastive Language-Image Pretraining (CLIP-style)**: The entire training approach uses contrastive loss to align vision and text embeddings. Understanding how positive/negative pairs shape the representation space is essential to interpret why cleaner alignment and richer text would improve learning.
  - *Quick check*: If you doubled the batch size during contrastive training, how would this affect the negative pair distribution, and what might this imply for the relative importance of data quality vs. quantity?

- **Zero-Shot Transfer Evaluation**: All performance claims rest on zero-shot classification and retrieval—no task-specific fine-tuning. Understanding how class prompts are constructed and how Recall@K metrics work is necessary to evaluate whether improvements are meaningful.
  - *Quick check*: For zero-shot classification, why might prompt engineering (e.g., "an X-ray showing pneumonia" vs. "a photo of pneumonia") disproportionately affect models trained on different datasets?

- **Vision and Language Encoder Architectures**: The paper uses ViT-B/16 (vision) and PubMedBERT (text). Knowing their inductive biases helps predict how data changes affect representations.
  - *Quick check*: PubMedBERT is pretrained on biomedical text. Would you expect a general-domain text encoder (e.g., standard BERT) to benefit more or less from in-text reference enrichment? Why?

## Architecture Onboarding

- **Component map**: PMC XML parsing -> figure/caption extraction -> in-text reference extraction -> compound image decomposition (DETR trained on MedICaT) -> subcaption segmentation (GPT-4o) -> label-to-subfigure alignment (YOLOv3 + ResNet-152 via Exsclaim) -> modality classification (GPT-4o-mini) -> confidence-based filtering -> 2.2M final pairs
- **Critical path**: Subfigure decomposition accuracy—if this fails, caption alignment cascades into noise; In-text summarization quality—GPT-4o-mini must extract relevant context without hallucination; Modality filtering balance—preserve diversity while removing non-medical noise
- **Design tradeoffs**: Scale reduction: 16.7M → 2.2M (87% filtered). Favors precision over coverage; may exclude rare conditions; LLM reliance: GPT-4o/4o-mini for segmentation, summarization, and classification. Faster than human annotation but errors are model-inherent and opaque; Radiology optimization: Decomposition pipeline trained primarily on radiology; microscopy/VLP gains less guaranteed
- **Failure signatures**: Per-modality performance divergence: If microscopy retrieval (Quilt) significantly underperforms radiology (MIMIC-CXR), decomposition may not transfer; MMD analysis null results: If representation distributions don't significantly differ from baseline datasets via permutation testing, curation isn't creating distinct representations; Ablation contradictions: If removing in-text summaries improves performance on certain tasks, enrichment may be adding noise for those domains
- **First 3 experiments**: Cross-modality decomposition validation: Reproduce the subfigure vs. compound ablation (Table 3) on microscopy tasks to test generalization beyond radiology; Summary length sensitivity: Vary the GPT-4o-mini summary constraint (currently <250 words) across [100, 250, 500] words and measure retrieval performance to identify optimal context density; Rare class coverage probe: Evaluate on a downstream dataset containing rare pathologies not well-represented in Open-PMC to test whether aggressive filtering sacrifices long-tail coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can image decomposition techniques be effectively generalized to microscopy and visible light photography to match the performance gains observed in radiology?
- Basis: [explicit] The authors acknowledge their decomposition pipeline "was primarily optimized for radiology images, limiting its effectiveness for microscopy and other medical modalities."
- Why unresolved: The current methodology relies on a DETR-based model trained on a radiology-heavy subset (MedICaT), creating a domain gap for other modalities
- What evidence would resolve it: Development and evaluation of modality-agnostic decomposition models, or specific fine-tuning for microscopy, showing improved zero-shot classification metrics in those domains

### Open Question 2
- Question: How does the integration of uncertainty estimation and human-in-the-loop validation quantitatively impact dataset reliability and downstream model performance?
- Basis: [explicit] The conclusion suggests future work should focus on "uncertainty estimation, outlier detection, and human-in-the-loop validation" to enhance data quality
- Why unresolved: The current pipeline relies on automated filtering (e.g., keyword matching, classifier thresholds) without quantifying the noise remaining in the final 2.2M pairs
- What evidence would resolve it: An ablation study comparing the current automated pipeline against a dataset refined by uncertainty sampling and expert review

### Open Question 3
- Question: Does the "quality over quantity" trade-off persist when scaling model architectures beyond ViT-B/16 to larger foundation models?
- Basis: [inferred] The experiments fix the architecture to ViT-B/16; it is unclear if larger models might utilize the noisy data in larger datasets (like Biomedica) more effectively than the curated Open-PMC data
- Why unresolved: Scaling laws in medical VL learning may interact differently with data noise compared to the fixed-capacity models tested
- What evidence would resolve it: Benchmarking Open-PMC against PMC-15M/Biomedica using larger backbones (e.g., ViT-L/14) to observe if the performance gap narrows or widens

## Limitations

- Reliance on GPT-4o/4o-mini for critical data curation steps introduces model-dependent variability that cannot be directly controlled or reproduced
- Performance gains from in-text reference summarization are modest (0.002 Recall@200 gain) and the mechanism for matching in-text references to specific subfigures remains unvalidated
- The decomposition pipeline was optimized for radiology and may not generalize effectively to microscopy and visible light photography modalities

## Confidence

- High: Open-PMC dataset construction pipeline and basic performance comparisons to Biomedica/PMC-15M
- Medium: Causal attribution of performance gains to subfigure decomposition and in-text reference enrichment
- Low: Generalization of decomposition pipeline to non-radiology modalities and long-term retention of rare medical conditions

## Next Checks

1. Conduct ablation study varying GPT-4o-mini summary length constraints (100-500 words) to quantify optimal semantic enrichment vs. noise tradeoff
2. Implement modality-specific decomposition accuracy validation by human annotation of subfigure boundaries in radiology vs. microscopy vs. VLP images
3. Test Open-PMC representations on rare disease classification tasks to assess whether aggressive quality filtering excludes clinically important long-tail cases