---
ver: rpa2
title: Predicting partially observable dynamical systems via diffusion models with
  a multiscale inference scheme
arxiv_id: '2511.19390'
source_url: https://arxiv.org/abs/2511.19390
tags:
- solar
- inference
- multiscale
- scheme
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of predicting partially observable
  dynamical systems, where only limited measurements are available at each time step.
  The authors focus on solar dynamics, where direct observation of the Sun's interior
  is impossible, yet long-range dependencies influence surface phenomena.
---

# Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme

## Quick Facts
- arXiv ID: 2511.19390
- Source URL: https://arxiv.org/abs/2511.19390
- Reference count: 40
- Outperforms autoregressive diffusion rollouts on solar active region prediction with lower Wasserstein distances (3.0 vs 7.3-12.0) and better physical parameter preservation

## Executive Summary
This paper addresses the challenge of predicting partially observable dynamical systems where long-range temporal dependencies exist but are not fully captured by standard autoregressive diffusion models. The authors propose a multiscale inference scheme that uses exponentially spaced temporal templates to condition predictions on both fine-grained recent data and coarser past information, enabling the model to capture long-range dependencies efficiently. Applied to solar dynamics prediction from NASA's Solar Dynamics Observatory, their method achieves significant improvements over standard approaches in predicting solar active region evolution across multiple time horizons.

## Method Summary
The method extends conditional diffusion models with a multiscale inference scheme using "multiscale templates" that condition predictions on temporally structured context windows. Instead of generating future steps sequentially, the approach uses exponentially spaced time indices (e.g., {-9, -3, -1, 0, 1, 3, 9}) to create templates that span both fine and coarse temporal resolutions. The denoiser backbone is a Vision Transformer with 16 attention layers, 512 hidden dimensions, and 4 heads, processing 3D patches. During inference, templates are selected iteratively to maintain 4 conditioning frames while generating new predictions, allowing the model to reach exponentially farther into the past compared to uniform templates while maintaining fixed computational cost.

## Key Results
- Achieves lower Wasserstein distances (3.0 vs 7.3-12.0) compared to standard autoregressive diffusion rollouts
- Better preserves power spectrum characteristics with lower MAE values
- Improves physical parameter predictions (SHARP parameters: MeanGBT, MeanGBZ) across multiple time horizons (1-32 hours)
- Generates up to 9 steps ahead in a single diffusion call versus 3 calls required for autoregressive methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exponentially spaced temporal templates capture long-range dependencies with fixed computational budget.
- Mechanism: The multiscale template $T^K_\alpha = \{t^K_{-K}, ..., t^K_0, ..., t^K_K\}$ uses time increments as powers of $\alpha \geq 1$, enabling the model to reach exponentially farther into the past (horizon $\sim \alpha^K$) compared to uniform templates (horizon $\sim K$).
- Core assumption: The target dynamical system exhibits smoothly decaying temporal autocorrelation (long-memory property).
- Evidence anchors: Synthetic experiment shows Wasserstein distance 0.021 (multiscale) vs 0.23 (autoregressive) when noise obscures local trend.
- Break condition: Systems dominated by short-term patterns with rapid autocorrelation decay may not benefit.

### Mechanism 2
- Claim: Generating distant future steps in a single diffusion call mitigates error accumulation from autoregressive rollouts.
- Mechanism: Standard autoregressive schemes generate frames sequentially, conditioning only on recent predictions after the first iteration. The multiscale scheme generates time step $t=9$ directly from observed past data in the first call, then fills intermediate gaps using smaller templates while maintaining conditioning on grounded frames.
- Core assumption: Conditional diffusion models can learn to map from noisy inputs and sparse temporal context to plausible trajectories when trained on appropriate template-conditioned data.
- Evidence anchors: Achieves lower Wasserstein distances (3.0 vs 7.3-12.0) across multiple time horizons.
- Break condition: Beyond the largest template's timescale, errors can still propagate.

### Mechanism 3
- Claim: Frequent re-conditioning on distant past observations throughout inference preserves long-memory context.
- Mechanism: Autoregressive schemes lose explicit connection to observed data after the first iteration. The multiscale scheme re-conditions on original observations at multiple iterations by design, as different shifted templates overlap with the observed past region.
- Core assumption: The underlying system state is not fully recoverable from recent observations alone; distant past contains irrecoverable information about latent drivers.
- Evidence anchors: Synthetic experiment with sinusoidal trend + Gaussian noise shows multiscale advantage when local trend is obscured.
- Break condition: When observations are corrupted or the signal-to-noise ratio is extremely low, conditioning on distant noisy past may introduce spurious correlations.

## Foundational Learning

- Concept: Score-based diffusion models and the denoising objective
  - Why needed here: The paper builds on conditional diffusion where a denoiser learns to recover clean data from noisy inputs, with the score function derived from this denoising task.
  - Quick check question: Can you explain why minimizing $\mathbb{E}[\|D(x_s, s) - x\|^2]$ yields the score function, and how conditioning is incorporated via masking?

- Concept: Partial observability vs. Markovian assumptions
  - Why needed here: The paper targets systems where observations are insufficient to determine state (hidden Markov), unlike fully observable fluid simulations where recent frames suffice.
  - Quick check question: Why does partial observability motivate longer temporal context windows, even if the underlying physics is Markovian?

- Concept: Long-memory processes and autocorrelation decay
  - Why needed here: The multiscale template design is motivated by power-law autocorrelation decay; understanding this helps set appropriate template horizons.
  - Quick check question: How would you diagnose whether your target system exhibits long-memory behavior from data?

## Architecture Onboarding

- Component map:
  - Data input -> Masking layer -> ViT backbone (16 layers, 512 dim, 4 heads) -> Conditional denoising output -> Template selector

- Critical path:
  1. Define template set $T = \{\tau^{(1)}, ..., \tau^{(N)}\}$ ordered by horizon
  2. At each inference step, select template satisfying $|C_n| = K+1$ (4 conditioning frames for $K=3$)
  3. Generate $K=3$ new frames, mark completed, repeat until horizon $H$ is filled
  4. Shift present forward and repeat for longer trajectories

- Design tradeoffs:
  - Larger $\alpha_{max}$ → longer reach but sparser conditioning; may miss fine-grained dynamics
  - Larger $K$ → more frames generated per call but increased model capacity requirements
  - Template horizon vs. decorrelation time: Should match system's memory; overspanning wastes compute, underspanning loses long-range context

- Failure signatures:
  - Predictions drift toward mean or blur over time → insufficient long-range conditioning or $\alpha_{max}$ too small
  - High-frequency content lost → deterministic baseline behavior; check diffusion sampling quality
  - Wasserstein distance improves but physical parameters degrade → distribution matches pixels but violates physics constraints

- First 3 experiments:
  1. Replicate the sinusoidal trend + Gaussian noise experiment to verify multiscale advantage over autoregressive baseline
  2. Compare $\alpha_{max} \in \{1.5, 2.0, 2.5, 3.0\}$ on held-out validation data, measuring Wasserstein distance and power spectrum MAE
  3. Restrict multiscale scheme to same past horizon as autoregressive to isolate contribution of template structure vs. conditioning reach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive or learned conditioning strategies outperform the fixed multiscale templates for systems where observations are dominated by short-term rather than long-term patterns?
- Basis in paper: [explicit] "While our method is well suited for long-memory systems with smoothly decaying temporal dependencies, it may not remain competitive when observations are dominated by short-term patterns. Future work could explore adaptive or learned conditioning strategies."
- Why unresolved: The fixed templates assume smoothly decaying autocorrelation; no experiments tested systems where short-term dynamics dominate.
- What evidence would resolve it: Comparing learned vs. fixed templates on diverse dynamical systems with varying memory characteristics.

### Open Question 2
- Question: What is the principled way to select optimal template parameters (K, α_max, horizon) for a given dynamical system?
- Basis in paper: [inferred] The paper uses K=3, α_max=2.5, horizon=9 in experiments but provides no theoretical guidance for these choices.
- Why unresolved: Parameter selection appears heuristic; the relationship between system properties and optimal template design is not formalized.
- What evidence would resolve it: Ablation studies across template configurations with theoretical analysis linking system memory properties to optimal template parameters.

### Open Question 3
- Question: Does the multiscale inference scheme generalize to other partially observable domains such as oceanography, climatology, or seismology where interior states are unobservable?
- Basis in paper: [explicit] "The method shows promise for any partially observable system with long-range dependencies, such as oceanography or seismology"; [inferred] Only validated on solar dynamics and one synthetic Navier-Stokes example.
- Why unresolved: No experiments on real oceanographic, climate, or seismic data were conducted.
- What evidence would resolve it: Systematic benchmarks on datasets from these domains showing comparable improvements over autoregressive baselines.

## Limitations
- The method's advantage diminishes when observations are dominated by short-term patterns with rapid autocorrelation decay
- The approach relies on the assumption that distant past observations contain recoverable information about latent dynamics
- Full-resolution, full-modality training was not attempted due to computational constraints

## Confidence

- **High confidence**: The core mechanism of using exponentially spaced temporal templates to capture long-range dependencies is well-supported by both synthetic experiments and real-world results
- **Medium confidence**: The claim that this approach generalizes to other partially observable systems with long-range dependencies is reasonable but requires additional validation
- **Medium confidence**: The assertion that frequent re-conditioning on distant past observations preserves long-memory context is supported by ablation experiments

## Next Checks

1. Test the method on a synthetic partially observable system with known long-memory properties (e.g., fractional Brownian motion) to verify the multiscale advantage generalizes beyond solar physics
2. Conduct an ablation study varying the maximum template horizon $\alpha_{max}$ relative to the system's autocorrelation decay time to identify the optimal tradeoff between long-range context and computational efficiency
3. Evaluate the method on a non-solar domain with partial observability (e.g., ocean surface temperature prediction from subsurface measurements) to assess cross-domain applicability