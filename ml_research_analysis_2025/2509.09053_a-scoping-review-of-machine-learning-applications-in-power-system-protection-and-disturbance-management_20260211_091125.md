---
ver: rpa2
title: A Scoping Review of Machine Learning Applications in Power System Protection
  and Disturbance Management
arxiv_id: '2509.09053'
source_url: https://arxiv.org/abs/2509.09053
tags:
- protection
- data
- fault
- power
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review systematically analyzes over 100 publications on machine
  learning applications in power system protection and disturbance management, addressing
  the challenges posed by increasing renewable and distributed energy resources. It
  identifies trends in fault detection, classification, and localization using various
  ML methods, including deep learning and reinforcement learning.
---

# A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management

## Quick Facts
- **arXiv ID**: 2509.09053
- **Source URL**: https://arxiv.org/abs/2509.09053
- **Reference count**: 40
- **Key outcome**: Systematic analysis of over 100 publications on ML applications in power system protection; identifies high accuracy on simulated data but insufficient real-world validation; calls for standardized evaluation practices and benchmark datasets.

## Executive Summary
This scoping review systematically analyzes ML applications in power system protection and disturbance management, addressing challenges from increasing renewable and distributed energy resources. The review identifies trends in fault detection, classification, and localization using various ML methods including deep learning and reinforcement learning. While high accuracy is often reported on simulated data, real-world validation remains scarce, and evaluation practices lack standardization. Key limitations include inconsistent terminology, inadequate dataset documentation, and insufficient robustness testing under realistic grid conditions.

## Method Summary
The review employs PRISMA-ScR framework, searching Scopus (349 records) and IEEE Xplore (254 records) for publications from Jan 2020–Dec 2024, yielding 603 initial records. After deduplication and screening using Rayyan, 119 studies were included. The methodology involves narrative synthesis and comparative tables (Supplementary Tables S.1–S.4) to analyze trends in Fault Detection (FD), Fault Classification (FC), and Fault Localization (FL) tasks.

## Key Results
- ML models demonstrate high accuracy (>95%) on simulated datasets but lack real-world validation
- Deep learning architectures (CNNs, LSTMs, hybrid models) dominate recent research
- Inconsistent terminology and evaluation metrics across studies prevent meaningful comparison
- No standardized benchmark datasets exist for protection tasks
- Real-time feasibility and hardware deployment requirements remain underexplored

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Supervised ML models can map input signals to protection task outputs when trained on labeled datasets derived from grid simulations.
- **Mechanism**: Feature extraction (e.g., via wavelet transforms) converts raw voltage/current signals into structured inputs; classifiers learn decision boundaries from these feature–label pairs.
- **Core assumption**: Simulated fault scenarios adequately represent real-world grid conditions.
- **Evidence anchors**: Extensive use of Simulink/PowerFactory simulations for FD/FC/FL with CNNs, SVMs, and ANNs, often reporting >95% accuracy.
- **Break condition**: Poor generalization to real grids due to unmodeled noise, topology changes, or inverter dynamics.

### Mechanism 2
- **Claim**: Time–frequency feature extraction enhances ML model discrimination of transient fault signatures in IBR-dominated grids.
- **Mechanism**: Transforms raw waveforms into multi-resolution representations that separate fault-induced transients from steady-state signals.
- **Core assumption**: Chosen transform basis captures most discriminative patterns for target fault types.
- **Evidence anchors**: DWT, CWT, and Stockwell transforms frequently used for preprocessing in FC/FL tasks, particularly for HVDC and hybrid grids.
- **Break condition**: Fault transients in real grids differ significantly from simulated ones.

### Mechanism 3
- **Claim**: Hybrid ML architectures can jointly exploit spatial and temporal dependencies in synchronized phasor or waveform data.
- **Mechanism**: CNNs extract local spatial patterns; RNNs/LSTMs model sequential dependencies; outputs are fused for multi-task protection.
- **Core assumption**: Grid fault dynamics exhibit learnable spatiotemporal structure preserved in measurement windows.
- **Evidence anchors**: Hybrid CNN-LSTM models mentioned for FL in distribution networks with DG penetration.
- **Break condition**: Insufficient temporal resolution or missing synchronization disrupts spatiotemporal pattern learning.

## Foundational Learning

- **Concept**: Supervised Learning Taxonomy (Binary vs. Multi-class Classification vs. Regression)
  - **Why needed here**: Most protection tasks are framed as classification; FL is regression. Understanding this distinction is critical for model selection and evaluation metric choice.
  - **Quick check question**: For a task estimating fault location as a percentage of line length, should you use a classifier or a regressor?

- **Concept**: Signal Processing Basics (Time-Domain, Frequency-Domain, Time-Frequency Analysis)
  - **Why needed here**: Review emphasizes DWT, FFT, and phasor-domain features as inputs to ML models.
  - **Quick check question**: Why might a discrete wavelet transform be preferred over a Fourier transform for analyzing non-stationary fault transients?

- **Concept**: Power System Protection Fundamentals (Relay Zones, Fault Types, DER/IBR Impacts)
  - **Why needed here**: Paper builds on conventional protection concepts and identifies how IBRs challenge them.
  - **Quick check question**: How does reduced fault current contribution from inverter-based resources affect the reliability of traditional overcurrent protection?

## Architecture Onboarding

- **Component map**: Data Source -> Preprocessing/Feature Engineering -> ML Core -> Decision Logic -> Output
- **Critical path**: High-quality simulated/real data → Appropriate feature extraction for grid type → Model training with class-aware evaluation → Robustness testing → Hardware-aware deployment validation
- **Design tradeoffs**:
  - Model complexity vs. latency: Deep models may offer higher accuracy but risk violating sub-cycle tripping requirements
  - Simulation fidelity vs. data availability: High-fidelity simulations are scarce; real data is limited
  - Communication dependency vs. reliability: Centralized schemes need robust comms; decentralized schemes are more resilient
- **Failure signatures**:
  - Overfitting to simulation artifacts: Near-perfect accuracy on simulated test sets but poor performance on real events
  - Class imbalance blindness: High overall accuracy but near-zero recall for rare fault types
  - Topology obsolescence: Model fails when DER penetration or line configurations change
- **First 3 experiments**:
  1. Implement simple ANN or SVM for FD on IEEE test bus system simulation; evaluate accuracy and F1-score across fault types
  2. Compare performance using raw waveforms vs. DWT-extracted features vs. phasor-only inputs for FC task
  3. Introduce additive noise (10–30 dB SNR) to trained model; measure degradation in detection latency and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the research community establish standardized, public benchmark datasets that include realistic grid disturbances, noise profiles, and varying DER penetration levels?
- **Basis in paper**: Explicitly calls for "public benchmark datasets" and "comprehensive dataset documentation"
- **Why unresolved**: Legal/regulatory barriers restrict access to real-world fault recordings; no unified format for reporting simulation metadata
- **What evidence would resolve it**: Publication of widely adopted, open-access dataset with ground-truth labels and complete metadata

### Open Question 2
- **Question**: Can advanced architectures such as Physics-Informed Neural Networks (PINNs) and Graph Neural Networks (GNNs) improve generalizability and interpretability over standard CNNs or SVMs?
- **Basis in paper**: States "Transformer architectures, GNNs, and PINNs show promise... but remain largely unexplored"
- **Why unresolved**: Current literature predominantly relies on simpler supervised models trained on simulated data
- **What evidence would resolve it**: Comparative studies demonstrating PINNs or GNNs maintain higher accuracy under domain shifts

### Open Question 3
- **Question**: What are the deployment-specific performance trade-offs regarding latency, memory usage, and throughput when deploying ML protection schemes on embedded hardware?
- **Basis in paper**: Notes that "Real-time feasibility is also underexplored" and "runtime and hardware requirements are seldom reported"
- **Why unresolved**: Most studies evaluate models purely on offline classification accuracy using high-performance computing
- **What evidence would resolve it**: HIL test results showing model inference times and memory footprints fitting within strict latency budgets

## Limitations
- Simulation-to-reality gap: Most models report high accuracy on simulated data but lack validation on real-world fault events
- Standardization deficit: Inconsistent terminology and evaluation metrics across studies prevent meaningful cross-comparison
- Dataset opacity: Many papers omit critical details about data generation, making it difficult to assess model generalizability

## Confidence
- **High confidence**: Systematic identification of simulation dominance in current ML protection research and call for standardized reporting practices
- **Medium confidence**: Proposed taxonomy for protection tasks and observed trend toward deep learning architectures, given limited real-world validation data
- **Low confidence**: Specific performance claims without standardized benchmarks or real-world testing data to anchor them

## Next Checks
1. **Benchmark Dataset Creation**: Compile public repository of real and simulated fault data with standardized labeling and preprocessing protocols
2. **Robustness Stress Testing**: Systematically evaluate trained ML models under noise injection, missing data, and topology changes
3. **Real-World Event Validation**: Test high-performing simulation models on actual fault recordings (e.g., COMTRADE archives) to measure performance degradation