---
ver: rpa2
title: 'LoCoT2V-Bench: Benchmarking Long-Form and Complex Text-to-Video Generation'
arxiv_id: '2510.26412'
source_url: https://arxiv.org/abs/2510.26412
tags:
- video
- generation
- prompt
- character
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LoCoT2V-Bench, a benchmark for evaluating
  long-form text-to-video generation under complex, multi-scene prompts with hierarchical
  metadata derived from real-world videos. The authors construct a comprehensive evaluation
  framework (LoCoT2V-Eval) covering five dimensions: perceptual quality, text-video
  alignment (coarse-to-fine), temporal quality, dynamic quality, and Human Expectation
  Realization Degree (HERD).'
---

# LoCoT2V-Bench: Benchmarking Long-Form and Complex Text-to-Video Generation

## Quick Facts
- arXiv ID: 2510.26412
- Source URL: https://arxiv.org/abs/2510.26412
- Reference count: 40
- Primary result: Introduces LoCoT2V-Bench, a benchmark for evaluating long-form text-to-video generation under complex, multi-scene prompts with hierarchical metadata.

## Executive Summary
This paper introduces LoCoT2V-Bench, a benchmark for evaluating long-form text-to-video generation under complex, multi-scene prompts with hierarchical metadata derived from real-world videos. The authors construct a comprehensive evaluation framework (LoCoT2V-Eval) covering five dimensions: perceptual quality, text-video alignment (coarse-to-fine), temporal quality, dynamic quality, and Human Expectation Realization Degree (HERD). Experiments on 13 representative long video generation models reveal strong performance in perceptual quality and background consistency, but significant weaknesses in fine-grained text-video alignment and character consistency.

## Method Summary
The benchmark constructs 248.85 average word prompts from 234 real-world videos (30-60s) through MLLM description, LLM expansion with hierarchical metadata, and human verification. The evaluation framework implements five parallel pipelines using streaming implementations to handle long sequences: perceptual quality via DeQA-Score, text-video alignment via hierarchical MLLM-based VQA with gating, temporal quality via SAM3 character tracking and CLIP similarity, dynamic quality via pre-fitted linear combination of six sub-metrics, and HERD via dual-agent MLLM scoring.

## Key Results
- Models excel at perceptual quality and background consistency but struggle with fine-grained text-video alignment and character consistency
- Character Consistency scores remain below 40% while Background Consistency exceeds 90%, revealing sharp performance contrast
- Fine-grained alignment shows notable drop from overall alignment, indicating models fail to handle prompt intricacies
- Virtual prompt category shows lower overall scores despite higher complexity, suggesting semantic difficulty or training data bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical metadata decomposition enables fine-grained evaluation that traditional single-metric approaches miss.
- Mechanism: Prompts are decomposed into scene sequences with structured metadata along three axes—Character (attributes, actions), Background (type, visual anchors), and Camera Movement—allowing independent verification of each semantic component rather than holistic scoring.
- Core assumption: Complex prompts can be meaningfully segmented without losing inter-scene dependencies.
- Evidence anchors:
  - [abstract] "featuring multi-scene prompts with hierarchical metadata (e.g., character settings and camera behaviors), constructed from collected real-world videos"
  - [section 4.2.2] "Hierarchical Multi-turn Verification... A scene-level existence question is first issued, and its response defines a binary gate g(s)... If g(s) = 0, the entire evaluation subtree of scene s is pruned"
  - [corpus] Related work CoAgent and StoryAnchors similarly decompose video generation into scene-level planning for consistency.

### Mechanism 2
- Claim: Dual-agent auditor-evaluator architecture reduces hallucination bias in high-level quality assessment.
- Mechanism: The Auditor agent produces a factual report without access to expectations; the Evaluator agent receives both the report and HERD expectations to score, preventing the evaluator from hallucinating visual content to match expected themes.
- Core assumption: Decomposing observation from judgment yields more reliable alignment than direct prompting.
- Evidence anchors:
  - [section 4.5] "The Auditor analyzes the generated video without access to the expectations and produces a factual report... The Evaluator is then provided with both the report and the video to mitigate potential hallucinations"
  - [section 5.2, Fig. 4] Shows dual-agent method achieves higher human alignment (PLCC) than direct scoring across all HERD sub-dimensions except Visual Style
  - [corpus] VideoScore2 employs similar reasoning-before-scoring approaches for video evaluation.

### Mechanism 3
- Claim: Character identity tracking via attribute-augmented segmentation plus MLLM verification yields more reliable consistency scores than embedding similarity alone.
- Mechanism: SAM3 extracts character trajectories guided by attribute-augmented prompts (e.g., "woman in white"); an MLLM verifier filters tracking errors by confirming instances match character descriptions; only verified instances contribute to FG-CLIP2 embedding similarity computation.
- Core assumption: Semantic verification of tracked instances corrects segmentation drift better than pure visual similarity.
- Evidence anchors:
  - [section 4.3] "we employ SAM3 to extract character trajectories... To further mitigate tracking errors, a strong MLLM acts as a verifier, retaining only those instances that align with the character's description"
  - [section 5.2, Table 3] Character Consistency achieves 47.17-49.90% SRCC with human judgment, notably lower than Background Consistency (51.11%), reflecting genuine difficulty
  - [corpus] ContextAnyone and Lights Camera Consistency papers identify character identity preservation as a central unsolved challenge.

## Foundational Learning

- Concept: **Diffusion vs. Autoregressive Video Generation Paradigms**
  - Why needed here: The paper evaluates both paradigms (e.g., SkyReels-V2 uses diffusion-forcing; CausVid uses autoregressive). Understanding their temporal coherence mechanisms is essential for interpreting character consistency failures differently across model types.
  - Quick check question: Can you explain why autoregressive models may exhibit different failure modes in character consistency compared to bidirectional diffusion models?

- Concept: **VQA-Based Evaluation with Gating**
  - Why needed here: The fine-grained alignment metric uses conditional binary questions where scene non-existence gates all downstream queries. This hierarchical structure prevents false positives from "hallucinated" scene verification.
  - Quick check question: Given a video missing scene 3 of 5, how should the FGA scoring formula penalize this versus a video with all scenes but missing a character attribute?

- Concept: **Streaming Evaluation for Long-Form Content**
  - Why needed here: Several metrics (Background Consistency, Warping Error, Dynamic Degree) are explicitly adapted to streaming implementations to avoid memory exhaustion on long videos. Understanding frame-level vs. segment-level vs. video-level aggregation is critical for extending metrics.
  - Quick check question: Why does computing background consistency on full frames outperform character-masked frames in human alignment (Table 3 shows 51.11% vs 30.98% SRCC)?

## Architecture Onboarding

- Component map: Real-world videos (234) -> MLLM raw description (Seed1.5-VL) -> LLM story expansion (GPT-5) -> Human verification -> Post-processing (name deduplication, safety filtering) -> 248.85 avg word prompts with hierarchical metadata -> LoCoT2V-Eval framework (5 parallel pipelines)

- Critical path: Fine-grained Alignment computation is the most complex—scene segmentation must precede VQA, and character tracking for consistency requires the same metadata. These should be computed together to avoid redundant prompt parsing.

- Design tradeoffs:
  - **MLLM vs. CLIP for alignment**: MLLM achieves finer semantic discrimination but at higher compute cost; CLIP remains useful for temporal similarity
  - **Streaming vs. full-video metrics**: Streaming enables long-video evaluation but may miss long-range dependencies (e.g., character reappearance after 50+ frames)
  - **HERD dual-agent vs. direct scoring**: Higher human alignment but requires two MLLM calls per video

- Failure signatures:
  - **Character Consistency < 30%** with Background Consistency > 90%: Indicates model generates coherent environments but fails identity preservation—likely a conditioning or attention issue
  - **FGA << OA** (e.g., 10.38 vs 18.12 for FreeNoise): Model captures broad narrative but misses specific attributes—prompt grounding weakness
  - **HERD scores inconsistent with TVA**: Narrative coherence may be achieved visually without textual faithfulness, suggesting the model learned filmic conventions but not precise instruction following

- First 3 experiments:
  1. **Validate prompt complexity impact**: Correlate each complexity dimension (semantic, structural, control) against performance drops. The paper finds weak negative correlations (~-0.03 to -0.18); determine if this is due to limited variance in prompts (Table 7 shows 2.84-4.40% variance) or genuine robustness.
  2. **Ablate HERD auditor**: Run HERD evaluation with direct scoring vs. dual-agent on a held-out validation set to quantify hallucination reduction. The paper's Fig. 4 suggests this is significant but the Visual Style dimension shows regression—investigate why.
  3. **Cross-category performance analysis**: The paper groups prompts into Daily Life, Nature, and Virtual categories (Table 4). Investigate why Virtual prompts show lower overall scores despite higher average length (265.7 words)—is this semantic complexity or training data distribution bias?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can video generation architectures be modified to close the performance gap between high background consistency and low character consistency in long-form generation?
- Basis in paper: [Explicit] Section 5.1 highlights a "sharp contrast" where models achieve Character Consistency (CC) scores below 40% while maintaining Background Consistency (BC) scores above 90%.
- Why unresolved: Current methods prioritize static environment stability over dynamic identity tracking across multi-scene prompts.
- What evidence would resolve it: A model achieving parity (>80%) between CC and BC scores on the LoCoT2V-Bench.

### Open Question 2
- Question: What training strategies are required to improve Fine-Grained Alignment (FGA) without sacrificing the strong perceptual quality current models exhibit?
- Basis in paper: [Explicit] The abstract and Section 5.1 identify the "notable drop" from Overall Alignment (OA) to Fine-Grained Alignment (FGA) as a key limitation, noting models fail to handle prompt intricacies.
- Why unresolved: Models tend to focus on global scene semantics rather than adhering to specific attributes or constraints within complex prompts.
- What evidence would resolve it: A model that maintains high Perceptual Quality (>80%) while significantly raising FGA scores closer to OA levels.

### Open Question 3
- Question: Does the reliance on a specific proprietary LLM (GPT-5) for the HERD metric limit the framework's reproducibility or introduce specific biases in evaluating "Human Expectations"?
- Basis in paper: [Inferred] Section 4.5 details the HERD metric's reliance on GPT-5 for both generating expectations and the dual-agent evaluation process.
- Why unresolved: The alignment results in Table 3 are high, but it is unstated if this holds when using different, potentially less capable or open-source LLMs.
- What evidence would resolve it: A study comparing HERD evaluation scores using various LLM backbones to ensure the metric is robust and not model-dependent.

## Limitations
- Reliance on proprietary MLLM models (GPT-5, Seed1.5-VL) and SAM3 creates reproducibility barriers despite implementation details provided
- Streaming implementations for long-form evaluation lack specific technical details (window sizes, buffering logic), potentially affecting metric consistency
- Character consistency metrics show fundamental weakness in current models, with scores remaining below 40% despite strong background consistency

## Confidence
- **High Confidence:** Perceptual quality results (PQ), background consistency metrics, and overall TVA findings using established methods (DeQA-Score, CLIP similarity)
- **Medium Confidence:** Fine-grained alignment and character consistency results using complex hierarchical VQA and MLLM verification
- **Low Confidence:** Dynamic quality weights and complexity dimension impacts due to unspecified "pre-fitted" weights and minimal variance in correlations

## Next Checks
1. **Ablate the HERD auditor** on a held-out validation set to quantify hallucination reduction benefits versus direct scoring, particularly investigating why Visual Style regressed
2. **Cross-category performance analysis** to understand why Virtual prompts score lower despite higher complexity, testing whether this reflects semantic difficulty or training data distribution bias
3. **Validate prompt complexity impact** by correlating each complexity dimension against performance drops across the full prompt set, determining whether weak correlations reflect genuine robustness or limited variance in the benchmark