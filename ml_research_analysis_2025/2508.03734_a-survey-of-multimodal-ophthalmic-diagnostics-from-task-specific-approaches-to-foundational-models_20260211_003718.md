---
ver: rpa2
title: 'A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches
  to Foundational Models'
arxiv_id: '2508.03734'
source_url: https://arxiv.org/abs/2508.03734
tags:
- multimodal
- image
- learning
- retinal
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey provides the first comprehensive review of multimodal\
  \ deep learning in ophthalmology, covering both task-specific approaches and large-scale\
  \ foundation models. It highlights the clinical need for integrating multiple imaging\
  \ modalities\u2014such as color fundus photography, optical coherence tomography,\
  \ and angiography\u2014to improve diagnostic accuracy for diseases like diabetic\
  \ retinopathy, glaucoma, and age-related macular degeneration."
---

# A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models

## Quick Facts
- **arXiv ID:** 2508.03734
- **Source URL:** https://arxiv.org/abs/2508.03734
- **Reference count:** 40
- **Primary result:** First comprehensive review of multimodal deep learning in ophthalmology, covering task-specific and foundation model approaches.

## Executive Summary
This survey systematically reviews multimodal deep learning methods for ophthalmic diagnosis, highlighting how integrating multiple imaging modalities (e.g., CFP, OCT, FFA) can improve diagnostic accuracy for diseases like diabetic retinopathy, glaucoma, and age-related macular degeneration. It categorizes methodologies into three core tasks—disease diagnosis, lesion detection/segmentation, and data generation/augmentation—while addressing challenges including data heterogeneity, annotation scarcity, and limited population diversity. The review emphasizes both task-specific fusion architectures and large-scale foundation models trained via self-supervision or contrastive learning, identifying key future directions such as ultra-widefield imaging and reinforcement learning-driven reasoning.

## Method Summary
The survey synthesizes findings from 77 studies and major public datasets (IDRiD, DDR, Messidor for DR; REFUGE, ORIGA, GAMMA for glaucoma; AREDS, AREDS2 for AMD). It reviews three methodological categories: (1) task-specific multimodal fusion using CNN/Transformer architectures with early/late/hierarchical fusion strategies; (2) foundation models pre-trained via masked image modeling (RETFound) or contrastive alignment (EyeCLIP); and (3) multimodal large language models (VisionUnite, EyecareGPT) for report generation. Performance metrics include AUC, accuracy, F1-score for classification; Dice, IoU for segmentation; and BLEU/ROUGE for text generation. The review highlights the need for standardized evaluation protocols and cross-institutional validation.

## Key Results
- Multimodal imaging (CFP + OCT/FFA) provides complementary pathological information essential for accurate diagnosis across DR, glaucoma, and AMD.
- Foundation models like RETFound, pre-trained on 1.6M unlabeled retinal images, demonstrate strong transfer learning performance across diverse downstream tasks.
- Attention-based fusion and cross-modal alignment improve lesion detection and segmentation accuracy compared to unimodal baselines.
- Clinical context injection (e.g., keywords, knowledge graphs) enhances the logical consistency and validity of AI-generated ophthalmic reports.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Complementarity and Fusion
- **Claim:** Integrating multiple imaging modalities (e.g., CFP, OCT, FFA) may improve diagnostic accuracy and robustness by capturing complementary pathological features that are invisible or ambiguous in single modalities.
- **Mechanism:** Distinct modalities capture different physical properties; for instance, CFP visualizes surface vasculature while OCT resolves cross-sectional retinal layers. Fusion strategies (early, mid, or late) combine these feature maps, allowing the model to correlate surface lesions with structural deformities.
- **Core assumption:** The information gain from the secondary modality exceeds the noise introduced by registration errors or data heterogeneity.
- **Evidence anchors:** [Abstract] "...multimodal imaging providing complementary information that is essential for accurate ophthalmic diagnosis."; [Page 3] "In real-world clinical practice, ophthalmologists seldom rely on a single imaging modality... combine CFP, OCT, and OCT angiography (OCTA) to jointly evaluate..."; [Corpus] Neighbor paper *FusionFM* reinforces this, suggesting fusing eye-specific foundational models optimizes diagnosis.
- **Break condition:** If modalities are misaligned or if one modality is consistently low-quality/noisy, fusion may degrade performance compared to a high-quality single-modality baseline.

### Mechanism 2: Semantic Alignment via Foundation Models
- **Claim:** Pre-training on large-scale heterogeneous data using Masked Image Modeling (MIM) or Contrastive Learning (CLIP) likely produces representations that generalize better to downstream tasks (e.g., zero-shot detection) than task-specific supervised training.
- **Mechanism:** Methods like RETFound (MIM) or EyeCLIP (Contrastive) force the encoder to reconstruct missing parts of images or align images with textual reports. This compels the model to learn robust, generalized anatomical and pathological features rather than overfitting to the sparse labels of a specific dataset.
- **Core assumption:** The structure of ophthalmic images shares universal latent features (anatomy, artifacts) that are consistent across devices and populations, which can be captured via self-supervision.
- **Evidence anchors:** [Page 2] "Foundation models offer a unified framework that can integrate a wide range of clinical data... enabling richer contextual understanding."; [Page 13] "RETFound... pretrained on 1.6 million unlabeled retinal images... demonstrating strong performance across various downstream tasks."; [Corpus] Related work on *OphthBench* and *EH-Benchmark* suggests that while general LLMs struggle, specialized alignment improves reasoning.
- **Break condition:** High domain shift (e.g., pediatric vs. geriatric retinas) where the pre-trained "universal" features fail to transfer, or when fine-tuning data is too small to adapt the foundation model effectively.

### Mechanism 3: Context-Aware Attention for Report Generation
- **Claim:** Injecting clinical context (keywords or knowledge graphs) into the decoding process via attention mechanisms likely improves the logical consistency and clinical validity of generated reports compared to image-only captioning.
- **Mechanism:** Architectures like GCA-Net or the M3 Transformer use attention to weight visual features based on textual priors (keywords like "hemorrhage" or "drusen"). This guides the decoder to focus on specific regions of the image when generating corresponding sentences, aligning visual evidence with medical terminology.
- **Core assumption:** There is a deterministic or high-probability mapping between visual patterns and clinical descriptors that can be learned via attention weights.
- **Evidence anchors:** [Page 11] "GCA-Net... generates context-aware keyword embedding representations... enhancing the generation of multimodal medical image descriptions."; [Page 12] "CGT proposes a method that incorporates clinical relation triples into visual features as prior knowledge to guide the decoding process."; [Corpus] *Constructing Ophthalmic MLLM* supports this, noting that clinical cognitive chain reasoning enhances positioning-diagnosis collaboration.
- **Break condition:** If the visual features are weak (poor image quality) or the knowledge graph is incomplete, the attention mechanism may hallucinate findings to satisfy the textual prior (keyword) that aren't visually present.

## Foundational Learning

- **Concept: Multimodal Fusion Strategies (Early vs. Late vs. Hybrid)**
  - **Why needed here:** The survey categorizes models by how they combine data (e.g., concatenating raw images vs. combining feature vectors vs. decision-level voting). Understanding this is required to interpret the "Task-Specific" section.
  - **Quick check question:** Does the model combine pixels before processing (early), abstract feature vectors (mid/hybrid), or final probabilities (late)?

- **Concept: Self-Supervised Learning (Masked Image Modeling & Contrastive)**
  - **Why needed here:** The paper heavily focuses on "Foundation Models" which rely on MIM (masking parts of an image and asking the model to reconstruct them) and Contrastive Learning (pulling image/text pairs closer in vector space).
  - **Quick check question:** Is the model learning from explicit human labels (supervised) or from the structure of the data itself (reconstruction/alignment)?

- **Concept: Ophthalmic Modalities (CFP, OCT, FFA)**
  - **Why needed here:** Knowing what each modality captures is essential to understanding *why* fusion helps (e.g., OCT = 3D structure, FFA = blood flow/leakage).
  - **Quick check question:** Which modality would you use to see fluid leakage behind the retina (FFA/OCTA) vs. surface drusen (CFP)?

## Architecture Onboarding

- **Component map:** Data Ingestion -> Registration/Alignment -> Feature Extraction (Encoders) -> Feature Fusion -> Task Output
- **Critical path:** Data Ingestion $\rightarrow$ Registration/Alignment (critical for multimodal pairs) $\rightarrow$ Feature Extraction (Encoders) $\rightarrow$ Feature Fusion $\rightarrow$ Task Output
- **Design tradeoffs:**
  - **Task-Specific vs. Foundation:** Task-specific models are lightweight and high-accuracy for narrow problems but brittle. Foundation models are heavy, expensive to train, but generalizable.
  - **Unimodal vs. Multimodal:** Multimodal adds diagnostic value but introduces massive data-engineering overhead (aligning datasets, handling missing modalities).
- **Failure signatures:**
  - **Modality Dropout:** Model ignores the secondary modality (e.g., OCT) because the primary (CFP) is easier to process.
  - **Hallucination:** In report generation, the text describes findings not present in the image due to strong textual priors.
  - **Registration Artifacts:** Fusion layers blend misaligned images, creating "ghost" lesions.
- **First 3 experiments:**
  1. **Sanity Check:** Train a unimodal ResNet on CFP for DR grading to establish a baseline.
  2. **Simple Fusion:** Concatenate feature maps from a frozen CFP encoder and a frozen OCT encoder and train a simple classifier head to measure the "complementarity" gain.
  3. **Foundation Adaptation:** Fine-tune a pre-trained foundation model (like RETFound or EyeCLIP mentioned in the survey) on a small, specific downstream dataset to validate transfer learning efficiency vs. training from scratch.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reinforcement learning (RL) be effectively adapted to enable multimodal foundation models to perform explicit, step-by-step clinical reasoning similar to ophthalmologists?
- **Basis in paper:** [Explicit] Section 6.2 explicitly highlights the need for "large model frameworks based on reinforcement learning (RL)" to emulate clinician decision-making and generate structured diagnostic paths, contrasting with current purely supervised approaches.
- **Why unresolved:** Current foundation models often lack the ability to generate long-chain reasoning or explain the "why" behind diagnostic steps in a clinically grounded manner.
- **What evidence would resolve it:** A model trained via RL (e.g., adapting DeepSeek-R1 style approaches) that outperforms supervised baselines in diagnostic accuracy while providing human-interpretable reasoning chains on benchmarks like FFA-IR or MMFundus.

### Open Question 2
- **Question:** How can multimodal fusion architectures dynamically weight the relevance of specific imaging modalities based on patient context rather than treating all modalities as equally informative?
- **Basis in paper:** [Explicit] Section 6.1 states that current fusion methods generally consider each modality equally informative, "overlooking the context-dependent relevance of specific modalities," which restricts diagnostic robustness.
- **Why unresolved:** Existing methods (early/late fusion) rely on static weights or simple concatenation, failing to capture the nuanced clinical reality where one modality (e.g., OCT) might be critical for one condition but less so for another compared to CFP.
- **What evidence would resolve it:** Development of an attention-based fusion mechanism that dynamically adjusts modality weights per sample and demonstrates superior performance on heterogeneous datasets compared to static fusion baselines.

### Open Question 3
- **Question:** What specific data curation or domain adaptation strategies are required to mitigate performance drops when deploying multimodal ophthalmic AI on demographically and geographically diverse populations?
- **Basis in paper:** [Explicit] Section 6.1 identifies "limited generalization to demographically and geographically diverse populations" as a major threat, noting that datasets like REFUGE and GAMMA show decreased performance in different ethnic or low-resource settings.
- **Why unresolved:** Most datasets are single-region and biased toward specific ethnic groups, and benchmarks rarely stratify performance by demographic attributes.
- **What evidence would resolve it:** A study showing consistent performance (AUC variance < 5%) across multi-ethnic validation sets using a new domain adaptation technique, with specific stratification reports by age, sex, and comorbidities.

## Limitations
- Many methods lack published source code or publicly available pretrained weights, making exact reproduction difficult.
- Substantial heterogeneity in multimodal datasets—particularly missing paired samples across imaging modalities—introduces potential data leakage and selection bias in reported results.
- Performance comparisons between fusion strategies are confounded by inconsistent evaluation protocols and unreported hyperparameter tuning, limiting direct method-level conclusions.

## Confidence
- **High Confidence:** The categorization of core tasks (diagnosis, segmentation, generation) and the role of cross-modal complementarity are well-supported by the literature and clinical consensus.
- **Medium Confidence:** Claims about foundation model generalization (e.g., RETFound's transfer learning efficacy) are promising but not yet validated across diverse real-world populations.
- **Low Confidence:** Performance comparisons between fusion strategies are confounded by inconsistent evaluation protocols and unreported hyperparameter tuning, limiting direct method-level conclusions.

## Next Checks
1. **Reproduce a minimal multimodal fusion baseline** (CFP + OCT) on a single well-documented dataset (e.g., REFUGE) to confirm that modality integration improves over unimodal baselines within a fixed experimental protocol.
2. **Perform a systematic ablation study** comparing early, mid, and late fusion strategies using a consistent backbone and dataset to isolate the contribution of fusion design from other factors.
3. **Test foundation model generalization** by fine-tuning RETFound or EyeCLIP on a small, clinically distinct dataset (e.g., pediatric or underrepresented population) to assess transfer robustness versus task-specific models.