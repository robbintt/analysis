---
ver: rpa2
title: Active Task Disambiguation with LLMs
arxiv_id: '2502.04485'
source_url: https://arxiv.org/abs/2502.04485
tags:
- questions
- solutions
- question
- task
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal framework for task ambiguity and
  proposes an active disambiguation method using Bayesian Experimental Design. The
  key idea is to select clarifying questions that maximize expected information gain
  by sampling from the space of possible solutions rather than relying on implicit
  reasoning.
---

# Active Task Disambiguation with LLMs

## Quick Facts
- arXiv ID: 2502.04485
- Source URL: https://arxiv.org/abs/2502.04485
- Reference count: 40
- One-line primary result: EIG-uniform strategy outperforms zero-shot and implicit reasoning baselines by explicitly reasoning about solution space partitions.

## Executive Summary
This paper addresses the challenge of task ambiguity in natural language instructions by introducing a formal framework for active disambiguation. The authors propose a method that selects clarifying questions to maximize expected information gain (EIG) using Bayesian Experimental Design, sampling from the space of possible solutions rather than relying on implicit LLM reasoning. Experiments on 20 Questions and code generation tasks demonstrate that this approach significantly improves alignment with user intent, particularly when using less capable models, by explicitly reasoning about solution space partitions.

## Method Summary
The method treats clarifying questions as "experiments" to reduce uncertainty about user intent, using Bayesian Experimental Design to select questions that maximize expected information gain. It works by sampling N candidate solutions and M candidate questions, then using a pseudo-oracle to evaluate how each question partitions the solution space. The EIG is calculated based on entropy reduction, and the question with highest EIG is selected. The approach uses uniform weighting over solutions to avoid LLM bias, and applies self-critic filtering to ensure solutions meet requirements. Experiments use N=20 solutions, M=5 questions, and T=10 iterations for 20 Questions or T=4 for code generation.

## Key Results
- EIG-uniform strategy significantly outperforms zero-shot and implicit reasoning approaches on both 20 Questions and code generation tasks
- EIG-logprobs significantly underperforms EIG-uniform, validating the choice to use uniform distribution over solutions
- Performance gains are particularly pronounced with less capable models like GPT-4o-mini and Llama3-8B
- The method effectively handles requirement incompatibility through self-critic filtering

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Framing question selection as Bayesian Experimental Design allows agents to maximize information gain objectively rather than relying on heuristics.
- **Mechanism:** The agent treats a clarifying question as an "experiment" where the user's answer reduces the entropy (uncertainty) of the solution space. By calculating the Expected Information Gain (EIG), the agent selects the question expected to most effectively partition the space of possible solutions.
- **Core assumption:** The utility of a question can be approximated by its ability to distinguish between currently viable hypotheses (solutions).
- **Evidence anchors:** [abstract] Mentions framing the problem "through the lens of Bayesian Experimental Design." [section 2.1] Defines EIG formally (Eq. 3) as the reduction in entropy $H[p^*(h|S)] - E_{p^*}[H[p^*(h|S \cup (q, a))]]$.
- **Break condition:** If the user's answer does not actually constrain the solution space (i.e., the answer is uninformative or the question is irrelevant), the EIG calculation fails to predict real utility.

### Mechanism 2
- **Claim:** Explicitly sampling candidate solutions before generating questions improves performance because LLMs are better at solution generation than meta-cognitive reasoning about questions.
- **Mechanism:** Instead of asking the LLM to "think of a good question" (implicit reasoning), the system samples N solutions, generates M questions, and uses the samples to estimate which question best splits the solutions. This shifts the reasoning load from the question space to the solution space.
- **Core assumption:** LLMs have stronger solution-generation capabilities than meta-cognitive questioning capabilities.
- **Evidence anchors:** [section 3] States the method "shifts the load from implicit reasoning about the best question to explicit reasoning via sampling from the solution space." [page 7] Experimental results show "EIG-uniform strategy significantly outperforms baseline zero-shot and implicit reasoning approaches."
- **Break condition:** If the LLM's solution sampling is low quality (e.g., hallucinated or non-compliant with requirements), the question selection will be biased or ineffective.

### Mechanism 3
- **Claim:** Assuming a uniform distribution over sampled solutions (EIG-uniform) creates a more robust estimate of question utility than using the LLM's internal token probabilities.
- **Mechanism:** The authors approximate the unknown user preference distribution $p^*$ with a uniform distribution over the set of requirements-compatible solutions. This avoids contamination from the LLM's potential biases or misaligned priors.
- **Core assumption:** Treating all valid sampled solutions as equally likely is a safer prior than trusting the LLM's implicit generative probabilities (log-probs) in ambiguous settings.
- **Evidence anchors:** [section 2.1] Discusses "Uniformity of the unknown" and the derivation of the partition logic. [page 7] Figure 4 and text show "EIG-logprobs significantly underperforms in comparison to EIG-uniform."
- **Break condition:** If the solution space is vast but the sample size N is small, the uniformity approximation over the sample may fail to represent the true structure of the solution space.

## Foundational Learning

- **Concept: Bayesian Experimental Design (BED)**
  - **Why needed here:** This is the theoretical engine of the paper. BED provides the math for valuing a "query" based on how much it reduces uncertainty about a hidden state (the user's intent).
  - **Quick check question:** How does BED quantify the value of an experiment before its outcome is known? (Answer: via Expected Information Gain, averaging over possible outcomes).

- **Concept: Shannon Entropy**
  - **Why needed here:** Entropy is the measure of uncertainty. The paper's goal is to reduce the entropy of the solution distribution $p(h|S)$ with every question asked.
  - **Quick check question:** If a question splits a set of 100 solutions into subsets of 50/50, how does that compare to a split of 90/10 in terms of information gain? (Answer: The 50/50 split maximizes entropy reduction/information gain).

- **Concept: Distributional Bias in LLMs**
  - **Why needed here:** The paper argues that LLMs have an internal bias ($p_\phi$) that may not match the user's intent ($p^*$). Understanding this misalignment is crucial for grasping why the "Uniform" strategy works better than "Logprobs."
  - **Quick check question:** Why might relying on an LLM's token log-probabilities for EIG estimation be risky in ambiguous tasks? (Answer: The model's high-probability solutions might be "average" or hallucinated, failing to match the specific user's needs).

## Architecture Onboarding

- **Component map:** Problem Statement ($S_t$) -> Solution Sampler -> Question Generator -> Pseudo-Oracle/Evaluator -> EIG Estimator -> Selector
- **Critical path:** The **Solution Sampler** and **Pseudo-Oracle**. If the sampled solutions are not diverse (mechanism failure) or the self-evaluation of requirements is noisy (hallucination), the EIG calculation is garbage-in-garbage-out.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The method requires N solution generations and N × M answer evaluations. This is computationally heavy compared to a single zero-shot query.
  - **Uniform vs. Weighted:** The paper chooses uniformity to ignore model bias, but this discards potentially useful confidence signals from the model.
- **Failure signatures:**
  - **Homogeneous Sampling:** The LLM generates identical or highly similar solutions, causing EIG to flatline across all questions.
  - **Requirement Drift:** The LLM fails to enforce constraints during solution sampling, including invalid solutions in the EIG calculation.
- **First 3 experiments:**
  1. **Verify Sampling Diversity:** Run the solution generator on a known ambiguous prompt. Check if the output list contains semantically distinct options (e.g., "return index" vs "return bool" for a search function).
  2. **Ablate the Evaluator:** Compare performance when the Pseudo-Oracle is the LLM itself (self-critique) vs. a deterministic tool (code interpreter) to measure evaluation noise.
  3. **Visualize Partitions:** For a binary question, print the set of solutions mapped to "Yes" vs "No". Ensure the split is close to 50/50 (maximizing EIG) rather than 99/1.

## Open Questions the Paper Calls Out

- **Question:** Can supervised fine-tuning on synthetic datasets of EIG-optimal questions successfully distill this explicit reasoning ability into LLMs for implicit, zero-shot disambiguation?
  - **Basis in paper:** [explicit] The Discussion section proposes that the framework could generate synthetic datasets "serving as a resource for supervised fine-tuning" to enhance LLMs' innate meta-cognitive skills.
  - **Why unresolved:** It is unclear if the sampling-based reasoning process can be internalized into model weights to improve performance without the expensive inference-time overhead.
  - **What evidence would resolve it:** A comparison of zero-shot performance on disambiguation tasks between a base model and a model fine-tuned on EIG-optimal question-answer pairs.

- **Question:** What effective strategies exist for autonomously detecting task ambiguity to determine when active disambiguation is necessary?
  - **Basis in paper:** [explicit] Section 5 states that "handling ambiguously specified tasks involves two equally important aspects determining a) that the given problem is ambiguous," which remains unaddressed by the current elicitation method.
  - **Why unresolved:** The proposed method assumes a task is ambiguous; applying it to unambiguous tasks incurs unnecessary interaction costs, yet distinguishing the two remains an open challenge.
  - **What evidence would resolve it:** Development and validation of a metric or classifier that correlates with the formal definition of ambiguity ($H \supset H^*$) and can predict the need for elicitation.

- **Question:** How can an agent autonomously determine the optimal stopping point for querying to balance information gain against the cost of user interaction?
  - **Basis in paper:** [explicit] Section 5 identifies determining "when a sufficient number of requirements have been collected to stop querying the user" as a critical, unresolved aspect of the problem.
  - **Why unresolved:** The experiments use a fixed number of iterations ($T$), but real-world deployment requires a dynamic stopping criterion based on the value of information versus the cost of asking.
  - **What evidence would resolve it:** A utility function incorporating query cost, validated by showing the agent stops querying when the marginal information gain drops below a threshold.

## Limitations

- The method's effectiveness depends heavily on the quality and diversity of solution sampling, which may not capture true user intent in large solution spaces with limited samples (N=20).
- Computational overhead is significant, requiring N solution generations and N×M answer evaluations per question, making it impractical for real-time applications.
- The approach assumes a truthful oracle and doesn't address how to handle uncertain or incorrect answers, which are common in human interactions.

## Confidence

- **High:** The theoretical framework connecting Bayesian Experimental Design to task disambiguation is sound and well-articulated. The observation that EIG-logprobs underperforms EIG-uniform due to model bias misalignment is empirically supported.
- **Medium:** The experimental results showing EIG-uniform's superiority over baselines are robust within the tested domains (20 Questions, HumanEval, APPS), but generalization to other ambiguous task types remains unproven.
- **Low:** The scalability of the approach to real-world ambiguous tasks with extremely large or complex solution spaces is questionable given the computational overhead and potential sampling limitations.

## Next Checks

1. **Solution Space Coverage Test:** Systematically vary N from 10 to 100 and measure how solution diversity and question quality change. This would reveal whether the current N=20 is sufficient or if performance plateaus/declines due to sampling inadequacy.
2. **External Oracle Ablation:** Replace the self-critic evaluator with a deterministic oracle (e.g., Python interpreter for code, fact database for 20 Questions) to isolate whether evaluation noise from the LLM significantly impacts EIG accuracy.
3. **Cross-Domain Transfer:** Apply the framework to a new ambiguous task domain (e.g., creative writing prompts, legal contract drafting) with different solution space characteristics to test robustness beyond the current experimental scope.