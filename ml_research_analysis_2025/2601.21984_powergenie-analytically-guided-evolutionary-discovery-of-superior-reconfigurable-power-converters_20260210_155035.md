---
ver: rpa2
title: 'PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable
  Power Converters'
arxiv_id: '2601.21984'
source_url: https://arxiv.org/abs/2601.21984
tags:
- circuit
- vout
- power
- device
- topology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PowerGenie is an automated framework for discovering high-performance
  reconfigurable power converters that exceed the capabilities of existing designs.
  It introduces an analytical method to determine circuit functionality and theoretical
  performance limits without SPICE simulation, combined with an evolutionary finetuning
  approach that co-evolves the generative model with its training distribution.
---

# PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters

## Quick Facts
- arXiv ID: 2601.21984
- Source URL: https://arxiv.org/abs/2601.21984
- Reference count: 40
- PowerGenie discovers an 8-mode converter with 23% higher FoM than training set, validated by SPICE showing 10% average absolute efficiency gains

## Executive Summary
PowerGenie introduces an automated framework for discovering high-performance reconfigurable power converters that exceed the capabilities of existing designs. The method combines an analytical framework that determines circuit functionality and theoretical performance limits without SPICE simulation, with an evolutionary finetuning approach that co-evolves the generative model with its training distribution. PowerGenie discovers a novel 8-mode converter achieving 23% higher figure-of-merit than the best training topology, with SPICE validation showing 10% average absolute efficiency gains across all modes and up to 17% at single modes.

## Method Summary
PowerGenie pretrains a 6-layer GPT on 11,837 circuit topologies represented as Eulerian circuit sequences, then applies evolutionary finetuning to discover superior reconfigurable power converters. The framework uses an analytical method to rapidly evaluate candidate topologies by computing SSL and FSL metrics from circuit structure without component sizing or SPICE simulation. Over 500 generations, it co-evolves the generative model with its training distribution through fitness selection and uniqueness verification, maintaining population diversity to prevent mode collapse. The discovered 8-mode converter topology achieves 23% higher figure-of-merit than training baselines.

## Key Results
- Discovered 8-mode converter achieves 23% higher FoM than best training topology
- SPICE validation shows 10% average absolute efficiency gain across all modes
- Analytical evaluation provides 90.8% syntax validity, 85.3% functional validity, 32.1% novelty rate

## Why This Works (Mechanism)

### Mechanism 1
The analytical framework enables rapid topology evaluation by computing theoretical performance limits directly from circuit structure without component sizing or SPICE simulation. It converts circuit netlists into directed graphs, constructs fundamental loop matrices via spanning tree decomposition, then applies Tellegen's Theorem to extract voltage conversion ratios and charge multipliers. These yield SSL and FSL metrics through matrix inversion operations (Theorems 3.6–3.9).

### Mechanism 2
Evolutionary finetuning prevents mode collapse by maintaining population diversity through tournament selection combined with global uniqueness filtering. Unlike standard RL/DPO which reinforce high-reward patterns until the model collapses onto similar designs, tournament selection gives lower-fitness circuits nonzero survival probability (k=3 tournament preserves ~33% chance for lower-fitness candidates), while graph isomorphism testing ensures each new circuit is topologically distinct from all previously validated designs.

### Mechanism 3
The five-orders-of-magnitude speedup from analytical evaluation (0.07s vs 2.4h per topology) makes evolutionary search computationally tractable. Each candidate topology requires matrix operations taking ~0.07 seconds rather than gradient-based sizing optimization requiring ~500 SPICE simulations. Over 128,000 evaluations (500 generations × 256 candidates), this reduces total evaluation time from ~35 years to ~2.5 hours.

## Foundational Learning

- **Concept**: Switched-capacitor converter fundamentals (voltage conversion ratios, SSL/FSL impedances, charge multipliers)
  - **Why needed here**: The entire analytical framework assumes understanding of how SC converters transfer energy through charge redistribution, and why SSL (capacitor-dominated) and FSL (switch-dominated) regimes have different loss mechanisms.
  - **Quick check question**: Given a two-phase SC converter, can you explain why charge multipliers must sum to zero over a complete switching cycle?

- **Concept**: Fundamental loop and cutset matrices in graph theory
  - **Why needed here**: The framework uses spanning tree decomposition to construct B and Q matrices that encode KVL and KCL constraints—properly-posed topologies are those where both submatrices are square and invertible.
  - **Quick check question**: For a circuit graph with 5 nodes and 7 edges, how many fundamental loops would a spanning tree decomposition produce?

- **Concept**: Co-evolutionary algorithms (population-based optimization where fitness landscape and search distribution change together)
  - **Why needed here**: PowerGenie simultaneously evolves the population of circuits and the generative model's parameters, unlike standard finetuning on fixed datasets.
  - **Quick check question**: How does co-evolution differ from iterated local search where only the current solution evolves?

## Architecture Onboarding

- **Component map**: Pretrained GPT-6 -> Analytical verifier -> Selection module -> Finetuning loop -> Uniqueness filter
- **Critical path**: Pretrain on 11,837 circuit topologies -> Initialize population with 432 labeled 8-mode converters -> Loop 500 generations: select -> finetune -> generate -> validate -> filter -> augment -> Extract best topology, validate with SPICE sizing optimization
- **Design tradeoffs**: Higher tournament k -> more exploitation pressure but less diversity (default k=3); Higher temperature -> more exploration but lower syntax validity (default T=0.7); More augmentations per circuit -> richer training signal but slower generations (default 50); Assumption: The 8-mode complexity (47 switches, 65,536 control configurations) represents the target design regime
- **Failure signatures**: Syntax validity <80% -> model not sufficiently pretrained on circuit grammar; Functional validity <50% -> selection pressure too weak or validation too permissive; Novelty <5% -> uniqueness filtering not enforced, or tournament size too large; FoM plateauing at training baseline -> generation component not contributing (see ablation Table 3)
- **First 3 experiments**: Validate analytical framework accuracy on 10 known topologies; Ablate selection strategy with elite-only vs tournament-only; Scale test on 4-mode converters for 100 generations

## Open Questions the Paper Calls Out

Can PowerGenie's analytical framework generalize to other switched-capacitor circuit classes beyond power converters, such as data converters and discrete-time filters? The conclusion states this is a promising future extension, but the current framework is specialized for SC power converter topologies with specific VCR and efficiency metrics.

Does the discovered 8-mode converter topology maintain its efficiency advantage under post-layout effects and non-ideal operating conditions not captured by the analytical model? The analytical framework relies on Assumptions 3.1-3.2 (no-load steady state, asymptotic operating limits) and SPICE validation used ideal gate drives; parasitic resistance, thermal effects, and layout-dependent capacitance remain unexplored.

Can the evolutionary finetuning approach discover superior topologies for converter architectures beyond switched-capacitor (e.g., inductor-based or hybrid topologies)? The paper states the framework focuses on SC converters, while the introduction notes that "diverse converter architectures" represent a future extension; the analytical framework's derivation depends fundamentally on charge conservation principles specific to capacitor-based energy transfer.

## Limitations

The analytical framework's accuracy depends critically on the SSL/FSL metrics' correlation with real-world performance, validated only on a single discovered topology rather than systematically across diverse architectures. The evolutionary finetuning approach may struggle with scalability to higher-mode converters where the search space grows exponentially (65,536 control configurations for 8-mode). The reliance on a small 432-circuit training set for 8-mode converters may limit the model's ability to discover truly novel architectures beyond incremental variations.

## Confidence

- Analytical framework for rapid topology evaluation: High confidence - the mathematical framework is well-specified and the 0.07s vs 2.4h speedup is directly measured and reproducible
- Evolutionary finetuning prevents mode collapse: Medium confidence - the mechanism is sound but diversity maintenance effectiveness depends heavily on hyperparameters and circuit complexity
- Novel topology discovery capability: Medium confidence - SPICE validation confirms 10% efficiency gains, but this is demonstrated on a single topology; broader validation across multiple discovered designs is needed

## Next Checks

1. **Systematic analytical accuracy validation**: Test the SSL/FSL framework's predictive power across 50 diverse known converter topologies by comparing analytical rankings against SPICE-optimized efficiency results, measuring correlation coefficient and mean absolute prediction error.

2. **Scalability validation**: Run PowerGenie on 4-mode and 6-mode converters (with 16 and 4,096 control configurations respectively) to verify the evolutionary search maintains effectiveness as complexity increases, tracking novelty rate and FoM improvement trends.

3. **Multiple topology validation**: Discover and SPICE-validate at least 5 additional high-performance topologies beyond the single published result, measuring average efficiency gains and assessing whether improvements generalize across different architectural approaches.