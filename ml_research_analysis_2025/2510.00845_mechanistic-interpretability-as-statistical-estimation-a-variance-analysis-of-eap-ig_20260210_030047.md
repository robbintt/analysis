---
ver: rpa2
title: 'Mechanistic Interpretability as Statistical Estimation: A Variance Analysis
  of EAP-IG'
arxiv_id: '2510.00845'
source_url: https://arxiv.org/abs/2510.00845
tags:
- circuit
- variance
- circuits
- https
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the stability and reliability of circuit
  discovery methods in mechanistic interpretability. The authors argue that circuit
  discovery should be viewed as a statistical estimation problem and conduct a comprehensive
  analysis of variance and robustness across multiple perturbation strategies, including
  input resampling, prompt paraphrasing, hyperparameter variation, and noise injection
  in causal analysis.
---

# Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG

## Quick Facts
- **arXiv ID:** 2510.00845
- **Source URL:** https://arxiv.org/abs/2510.00845
- **Reference count:** 40
- **Primary result:** Circuit discovery methods exhibit high structural variance and sensitivity to hyperparameters, questioning their stability and reliability

## Executive Summary
This paper investigates the stability and reliability of circuit discovery methods in mechanistic interpretability, treating the problem as statistical estimation. The authors conduct a comprehensive analysis of variance across multiple perturbation strategies, including input resampling, prompt paraphrasing, hyperparameter variation, and noise injection in causal analysis. Their results show that EAP-IG exhibits high structural variance and sensitivity to hyperparameters across models and tasks, questioning the stability of its findings. The study demonstrates that causal mediation analysis already has high intrinsic variance, which is amplified by approximation methods, leading to unstable circuits that are sensitive to methodological choices. Based on these findings, the authors advocate for routine reporting of stability metrics through bootstrap resampling and hyperparameter sensitivity analysis to promote more rigorous and statistically grounded interpretability research.

## Method Summary
The authors analyze variance and stability of EAP-IG circuit discovery across multiple perturbation strategies. They compare exact CMA vs EAP family estimators on three tasks (IOI, SVA, Greater-Than) using gpt2-small, Llama-3.2-1B, and Llama-3.2-1B-Instruct models. Methods include bootstrap resampling (n=100), noise amplitude sweeps, and hyperparameter variations. They compute structural stability via pairwise Jaccard index, faithfulness via circuit error/KL divergence, and coefficient of variation (CV = σ/|μ|). Exact CMA is restricted to gpt2-small/IOI due to computational costs.

## Key Results
- Causal importance scores exhibit high intrinsic variance (CV ≈ 0.5) even with exact computation
- EAP-IG amplifies this variance through gradient approximation noise, pushing CV above 1.0
- Circuit discovery yields multimodal structural solutions with low Jaccard index overlap under perturbation
- Stability metrics are sensitive to hyperparameter choices, particularly noise amplitude and patching methods

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Variance of Causal Effects
The causal importance of a neural network edge is a volatile random variable dependent on the specific input rather than a fixed property. The score $S(e, x, x_{corr})$ fluctuates significantly as clean input $x$ and counterfactual $x_{corr}$ vary. Even with exact computation, the coefficient of variation (CV) for edge scores is high ($\approx 0.5$), suggesting the behavior of interest is supported by distributed mechanisms where specific contributions vary by context.

### Mechanism 2: Variance Amplification via Gradient Approximation
Fast approximation methods (EAP-IG) function as noisy estimators that degrade the signal-to-noise ratio of the underlying causal effect. EAP-IG estimates causal effects using gradients (local linear approximations or path integrals), introducing approximation error that compounds the intrinsic variance of the causal effect, pushing the CV above 1.0. This occurs because the relationship between mediator and output is sufficiently non-linear that local gradient information fails to capture the global intervention effect accurately.

### Mechanism 3: Fragile Structural Selection from Noisy Aggregates
Circuit extraction via thresholding on aggregated noisy scores yields unstable, multimodal structural solutions. Because underlying edge score distributions have high variance and overlap, aggregating via mean/median and applying a discrete threshold causes the final subgraph to fluctuate drastically under data resampling or hyperparameter shifts. The "true" circuit is sparse and distinguishable, but estimator variance masks this separation, creating a non-identifiability problem where multiple distinct circuits satisfy the selection criteria.

## Foundational Learning

**Causal Mediation Analysis (CMA)**
- Why needed: This is the theoretical engine of the paper; understanding how "importance" is defined as a causal effect through a mediator is essential to grasp why it varies with input
- Quick check: Can you explain why the Natural Indirect Effect (NIE) changes when the input $x$ changes, even if the model weights stay fixed?

**Bootstrap Resampling**
- Why needed: The authors use this to isolate sampling variance; you need to distinguish between the variance of a single score and the variance of the aggregate circuit estimate
- Quick check: If bootstrapping yields a bimodal distribution of circuits, what does that imply about the "ground truth" circuit?

**Coefficient of Variation (CV)**
- Why needed: The paper relies on CV ($\sigma / |\mu|$) rather than raw variance to compare noise levels across different edge magnitudes
- Quick check: Why is a high mean score with high variance (low CV) preferable to a low mean score with low variance (high CV) for circuit discovery?

## Architecture Onboarding

**Component map:** Estimand Layer (true distribution of causal scores $\mu_e$) -> Estimator Layer (EAP/EAP-IG implementations) -> Aggregation Layer (mean/median of scores over dataset $D$) -> Selection Layer (threshold/pruning heuristics → Discrete Circuit)

**Critical path:** Input Sampler → Perturbation Strategy → EAP-IG Score → Aggregation

**Design tradeoffs:**
- Exact CMA vs. EAP-IG: Exact CMA is stable but computationally intractable; EAP-IG is fast but amplifies noise (Signal-to-Noise ratio < 1)
- Mean vs. Median Aggregation: Mean is sensitive to outliers; median is robust but may discard weak signals

**Failure signatures:**
- Multimodal Circuits: Running the same pipeline with different random seeds yields disjoint subgraphs (low Jaccard index)
- Hyperparameter Sensitivity: Changing noise amplitude or patching method drastically changes the circuit
- High CV: The standard deviation of an edge's importance score exceeds its mean across the dataset

**First 3 experiments:**
1. **Variance Baseline:** Compute exact CMA scores for a subset of edges (if compute permits) or EAP-IG scores, and plot the histogram of CVs. Confirm the "intrinsic variance" hypothesis.
2. **Bootstrap Stability:** Run the full circuit discovery pipeline on 50+ bootstrap resamples of your dataset. Calculate the pairwise Jaccard index. If $J < 0.5$, the circuit is unstable.
3. **Intervention Sweep:** Vary the noise amplitude used to generate $x_{corr}$. Plot the resulting circuit error and Jaccard index to identify the "critical regime" where stability collapses.

## Open Questions the Paper Calls Out

**Open Question 1:** Do newer circuit discovery methods (e.g., HAP, RelP) exhibit improved stability over EAP-IG, or do they inherit the same fundamental volatility from CMA? Authors note newer methods remain downstream of CMA and likely inherit its volatility, though their specific rules may act as stabilizing regularizers. This remains untested.

**Open Question 2:** Does the high intrinsic variance observed in exact CMA for gpt2-small on IOI generalize to larger models and other tasks? Exact CMA was computationally restricted to gpt2-small/IOI; generalizing this instability to other models and tasks relies on approximation-based evidence.

**Open Question 3:** Can weighted stability metrics reveal a stable "functional core" of circuits despite fluctuating peripheral edges? Current stability metrics treat all edges equally, whereas weighted metrics might reveal a stable core despite fluctuating periphery.

**Open Question 4:** Can circuit discovery methods be reformulated to output posterior distributions over graphs rather than single discrete subgraphs? Since underlying CMA scores are distributions, MI methods could output posterior distributions over graphs, formalized using Bayesian structure learning approaches.

## Limitations

- Computational cost of exact CMA limits validation to small models/tasks, making it difficult to definitively prove that EAP-IG variance is solely due to approximation error
- Non-identifiability problem could stem from inherent distributed nature of learned mechanisms rather than estimator variance
- Dataset sizes and prompt generators for the three tasks are not fully specified, affecting reproducibility

## Confidence

**High Confidence:** The observation that causal effects vary significantly across inputs (CV ≈ 0.5) is well-supported by direct computation and fundamental to causal analysis.

**Medium Confidence:** The claim that EAP-IG amplifies this variance is strongly supported for tested models/tasks but may not generalize to all architectures or tasks.

**Medium Confidence:** Structural instability findings are reproducible, but interpretation that this represents a fundamental methodological flaw versus inherent model properties requires further validation.

## Next Checks

1. **Cross-Model Validation:** Test the variance patterns on a larger language model (e.g., LLaMA-3-8B) to determine if EAP-IG variance scales with model size or remains consistent across scales.

2. **Alternative Aggregation Methods:** Compare mean/median aggregation against more robust statistics (trimmed mean, quantile regression) to quantify whether current aggregation methods are suboptimal rather than fundamentally flawed.

3. **Ground Truth Circuit Test:** For a synthetic task with known ground truth circuit, measure how often EAP-IG discovers the correct structure versus alternative methods to establish practical reliability bounds.