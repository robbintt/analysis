---
ver: rpa2
title: Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency
  Parsing
arxiv_id: '2505.20976'
source_url: https://arxiv.org/abs/2505.20976
tags:
- constituency
- treebank
- generation
- back
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for cross-domain constituency
  parsing by generating a target-domain treebank using large language models (LLMs).
  The key idea is to use LLM back generation, where an incomplete constituency tree
  with only domain keywords is given as input, and the LLM fills in the missing words
  to produce a complete parse tree.
---

# Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing

## Quick Facts
- **arXiv ID**: 2505.20976
- **Source URL**: https://arxiv.org/abs/2505.20976
- **Reference count**: 22
- **Primary result**: Proposes LLM back generation with contrastive learning to generate target-domain treebanks for cross-domain constituency parsing, achieving state-of-the-art F1 scores on MCTB benchmark.

## Executive Summary
This paper addresses cross-domain constituency parsing by generating target-domain treebanks using large language models (LLMs) through a novel back generation approach. Unlike direct treebank annotation which often produces invalid parse trees due to LLM hallucinations, the method provides LLMs with incomplete constituency trees containing only domain keywords and has them fill in the missing words. To further enhance parsing performance, the authors introduce span-level contrastive learning pre-training that uses positive and negative examples based on constituent spans. Experiments on the MCTB benchmark demonstrate superior performance compared to natural corpus treebanks, conventional parsers, and previous cross-domain methods, validating the effectiveness of combining LLM back generation with contrastive learning for cross-domain parsing tasks.

## Method Summary
The proposed method generates cross-domain treebanks through LLM back generation, where an incomplete constituency tree with only domain keywords is provided as input to the LLM, which then fills in the missing words to produce complete parse trees. This approach mitigates the hallucination problems common in direct treebank annotation. To further improve parsing quality, the authors implement span-level contrastive learning pre-training. This involves creating positive examples by keeping valid constituent spans and negative examples by corrupting these spans with invalid structures. The parser is pre-trained on this contrastive task to better distinguish between valid and invalid constituent spans before fine-tuning on the generated treebank. The combined approach addresses both the generation quality issue and the parser's ability to recognize valid syntactic structures across domains.

## Key Results
- Achieves state-of-the-art average F1 score on MCTB benchmark compared to various baselines
- LLM back generation produces higher quality treebanks than direct annotation approaches
- Span-level contrastive learning pre-training significantly improves parser performance across domains
- Outperforms both natural corpus treebanks and conventional parsers in cross-domain settings

## Why This Works (Mechanism)
The method works by leveraging LLMs' strong language understanding capabilities while constraining their output through incomplete tree structures, preventing hallucinations from generating invalid parses. The back generation approach guides LLMs to produce syntactically valid trees by providing partial structural context. The contrastive learning component then teaches the parser to recognize valid versus invalid constituent spans through exposure to both positive (correct) and negative (corrupted) examples. This dual approach addresses both the generation quality problem and the parser's generalization ability across domains. The span-level focus ensures the parser learns fine-grained syntactic distinctions rather than just sentence-level patterns, making it more robust to domain-specific variations in phrase structure.

## Foundational Learning
- **Constituency parsing fundamentals**: Understanding how parse trees represent hierarchical syntactic structures is essential for grasping the problem domain and evaluation metrics. Quick check: Can identify constituents and their relationships in a sample parse tree.
- **Large language model capabilities and limitations**: Knowledge of LLM strengths in language understanding and weaknesses in structured prediction helps explain why direct annotation fails but guided generation succeeds. Quick check: Can explain why LLMs hallucinate in structured tasks.
- **Contrastive learning principles**: Understanding how models learn from positive/negative pairs is crucial for comprehending the pre-training strategy and its effectiveness. Quick check: Can describe how contrastive loss functions work.
- **Cross-domain generalization**: Familiarity with domain adaptation challenges in NLP provides context for why domain-specific treebanks are needed and how they improve performance. Quick check: Can explain what makes domains different for parsing tasks.
- **Treebank construction and annotation**: Understanding traditional treebank creation methods highlights the innovation of using LLMs for automatic generation. Quick check: Can compare manual vs. automatic treebank creation approaches.

## Architecture Onboarding

**Component Map**: Domain text corpus -> Keyword extraction -> Incomplete tree generation -> GPT-4 back generation -> Generated treebank -> Span extraction -> Positive/negative span pairs -> Contrastive pre-training -> Parser fine-tuning -> Cross-domain parsing

**Critical Path**: The most critical components are the LLM back generation (ensuring treebank quality) and the contrastive learning pre-training (ensuring parser robustness). The pipeline flows from domain corpus through GPT-4 to generate high-quality treebanks, then uses contrastive learning to train a parser that can handle domain variations effectively.

**Design Tradeoffs**: The method trades computational cost (using GPT-4 for generation) for annotation quality and scalability. Alternative approaches like using weaker models or direct annotation would be cheaper but produce lower quality outputs. The contrastive learning adds pre-training overhead but significantly improves generalization.

**Failure Signatures**: Poor treebank quality manifests as low parsing F1 scores and syntax errors in generated trees. Contrastive learning failure shows as inability to distinguish valid from invalid spans, leading to poor cross-domain performance. GPT-4 refusal or generation of ungrammatical structures indicates the approach may not work with weaker models.

**First 3 Experiments**:
1. Generate treebanks using GPT-4 back generation for each MCTB domain and evaluate parsing F1 scores
2. Apply span-level contrastive learning pre-training and measure improvement over baseline parsers
3. Compare performance against direct treebank annotation methods and conventional parsers across all domains

## Open Questions the Paper Calls Out
- Can the LLM back generation method be effectively generalized to morphologically rich or low-resource languages where GPT-4 may have weaker syntactic control?
- Would adopting a multi-parser strategy (training separate models per domain) resolve the performance gap in the Law and Literature domains?
- To what extent can the span-level contrastive learning pre-training compensate for the lower quality or higher error rates of treebanks generated by weaker, open-source LLMs?

## Limitations
- The approach is verified only on the English dataset, with unclear effectiveness for other languages due to lack of cross-domain treebanks
- Performance in Law and Literature domains lags behind baselines, potentially due to the single unified parser needing to balance domain differences
- Reliance on GPT-4 for back generation creates model dependency and computational costs that may not scale

## Confidence
- **High confidence**: Core claim that LLM back generation with contrastive learning improves cross-domain parsing F1 scores on MCTB benchmark
- **Medium confidence**: Generalizability of the approach to other domains or treebanks beyond the evaluated scope
- **Low confidence**: Scalability and robustness claims, as these aspects were not empirically tested in the paper

## Next Checks
1. Test the proposed method on additional treebanks (e.g., Penn Treebank, Universal Dependencies) to assess cross-dataset generalization
2. Compare the computational efficiency and resource requirements of the method against conventional parsers and other LLM-based approaches
3. Evaluate the impact of using different LLMs (e.g., LLaMA, Claude) for back generation to determine model dependency