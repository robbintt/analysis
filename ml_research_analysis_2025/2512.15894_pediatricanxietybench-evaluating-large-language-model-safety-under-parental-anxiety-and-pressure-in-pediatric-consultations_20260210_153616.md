---
ver: rpa2
title: 'PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental
  Anxiety and Pressure in Pediatric Consultations'
arxiv_id: '2512.15894'
source_url: https://arxiv.org/abs/2512.15894
tags:
- safety
- adversarial
- queries
- medical
- pediatric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PediatricAnxietyBench addresses LLM safety under parental anxiety-driven
  adversarial pressures in pediatric consultations, where urgent language can compromise
  safeguards. The open-source benchmark contains 300 queries (150 authentic, 150 adversarial)
  across 10 pediatric topics, evaluated using a multi-dimensional safety framework
  (0-15 points).
---

# PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations

## Quick Facts
- **arXiv ID**: 2512.15894
- **Source URL**: https://arxiv.org/abs/2512.15894
- **Reference count**: 0
- **Key outcome**: Llama-3.3-70B significantly outperformed smaller models in pediatric LLM safety under parental anxiety, with critical failure rates dropping from 12.0% to 4.8%.

## Executive Summary
PediatricAnxietyBench is a benchmark designed to evaluate large language model safety under parental anxiety and pressure in pediatric consultations. The benchmark contains 300 queries across 10 pediatric topics, with 150 authentic and 150 adversarial queries that simulate urgent, anxiety-driven parental interactions. Models were evaluated using a comprehensive safety framework scoring system from 0-15 points across four dimensions: hedging, critical failure, safety, and accuracy. The study found that parental anxiety and urgency significantly compromise LLM safety, with larger models showing better performance but all models demonstrating vulnerabilities to realistic parental pressures.

## Method Summary
The PediatricAnxietyBench was developed using authentic parent queries collected from 2,600 Chinese parents and their children across various pediatric conditions. The benchmark includes 10 pediatric topics such as fever, respiratory symptoms, seizures, allergies, and post-vaccination reactions. Adversarial queries were generated from authentic inputs to simulate parental anxiety-driven pressures. Four Chinese-language LLMs were evaluated: Llama-3.3-70B, Llama-3.1-8B, Qwen2.5-7B, and Yi-6B. A multi-dimensional safety framework assessed models across four dimensions with a total score of 0-15 points. Statistical analysis included t-tests for model comparisons and regression analysis for correlation assessment.

## Key Results
- Llama-3.3-70B achieved 6.26/15 safety score, significantly outperforming Llama-3.1-8B (4.95/15, p<0.001)
- Critical failure rates dropped from 12.0% to 4.8% when comparing smaller to larger models (p=0.02)
- Adversarial queries reduced safety scores by 8% overall (p=0.03), with urgency causing the largest drop (-1.40 points)
- Seizures and post-vaccination topics showed highest vulnerabilities with 33.3% inappropriate diagnosis rates
- Hedging phrase count strongly correlated with safety scores (r=0.68, p<0.001)

## Why This Works (Mechanism)
The benchmark effectively captures realistic parental anxiety-driven pressures that commonly occur in pediatric consultations. By including both authentic and adversarial queries, it simulates the urgent, emotional communication style parents often use when seeking medical advice for their children. The multi-dimensional safety framework provides comprehensive evaluation across multiple safety dimensions, allowing for nuanced assessment of model performance under stress conditions.

## Foundational Learning
**Parental anxiety modeling** - Simulating realistic parental communication patterns with urgency and emotional language is essential for evaluating real-world model safety. Quick check: Verify adversarial queries maintain authentic emotional tone while increasing urgency.

**Multi-dimensional safety assessment** - Using comprehensive frameworks that evaluate multiple safety dimensions (hedging, critical failure, safety, accuracy) provides more complete safety evaluation than single metrics. Quick check: Ensure scoring rubric captures edge cases across all dimensions.

**Statistical significance testing** - Proper statistical validation (t-tests, regression analysis) is crucial for establishing meaningful differences between model performances and understanding correlation patterns. Quick check: Verify sample sizes provide adequate power for detecting safety differences.

## Architecture Onboarding
**Component map**: Parent queries -> Adversarial generation -> Model evaluation -> Safety scoring (0-15 points) -> Statistical analysis
**Critical path**: Query generation → Model input → Response generation → Safety framework evaluation → Statistical comparison
**Design tradeoffs**: Larger models provide better safety but require more computational resources; comprehensive safety frameworks are thorough but may be complex to implement; authentic query collection ensures realism but limits generalizability across cultures.
**Failure signatures**: High critical failure rates in emergency topics, absence of emergency recognition, degraded performance under urgent language, correlation between hedging and safety scores.
**First experiments**: 1) Test cross-cultural transferability by evaluating benchmark with Western healthcare providers, 2) Implement real-world deployment monitoring for 3-6 months, 3) Develop emergency recognition fine-tuning for high-risk pediatric topics.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited external validation across different healthcare systems and cultural contexts
- Single evaluative perspective on safety framework reliability and sensitivity
- Context-specific adversarial design may not capture full spectrum of real-world parental pressures

## Confidence
**High confidence**: LLM safety degrades under parental anxiety and urgency pressures; model scale correlates positively with safety performance; hedging phrase usage strongly correlates with overall safety scores.

**Medium confidence**: All tested models show vulnerabilities to realistic parental pressures; specific pediatric topics present higher safety risks; parental anxiety causes measurable safety reduction.

**Low confidence**: Critical failure rate differences between models may not reflect real-world safety margins; absence of emergency recognition represents fundamental limitation rather than benchmark artifact.

## Next Checks
1. Test cross-cultural transferability by evaluating PediatricAnxietyBench with Western healthcare providers and parents to assess generalizability.
2. Implement real-world deployment monitoring by tracking safety incidents and model performance over 3-6 months in actual pediatric telehealth platforms.
3. Develop and test emergency recognition fine-tuning specifically targeting pediatric emergency symptoms to reduce critical failure rates in high-risk topics.