---
ver: rpa2
title: 'VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning'
arxiv_id: '2511.17731'
source_url: https://arxiv.org/abs/2511.17731
tags:
- reasoning
- answer
- arxiv
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VisReason introduces a large-scale dataset for training multimodal
  models in visual Chain-of-Thought reasoning, addressing the lack of spatially grounded,
  multi-round supervision. The dataset comprises 489K examples across four domains,
  each featuring multi-round human-like rationales that guide models through interpretable
  visual reasoning steps, supplemented by a 165K expert-curated subset with richer
  reasoning traces and 3D spatial grounding.
---

# VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning

## Quick Facts
- **arXiv ID:** 2511.17731
- **Source URL:** https://arxiv.org/abs/2511.17731
- **Reference count:** 40
- **Primary result:** 489K example dataset enabling state-of-the-art visual CoT reasoning with 0.807 average score on Visual-CoT benchmark

## Executive Summary
VisReason introduces a large-scale dataset for training multimodal models in visual Chain-of-Thought reasoning, addressing the lack of spatially grounded, multi-round supervision. The dataset comprises 489K examples across four domains, each featuring multi-round human-like rationales that guide models through interpretable visual reasoning steps, supplemented by a 165K expert-curated subset with richer reasoning traces and 3D spatial grounding. Fine-tuning Qwen2.5-VL on VisReason and VisReason-Pro yields state-of-the-art performance across multiple metrics, demonstrating improved fidelity, interpretability, and cross-benchmark generalization.

## Method Summary
VisReason fine-tunes Qwen2.5-VL-7B using LoRA (rank 32) with frozen vision tower and trainable projector+LLM. The dataset provides multi-round, spatially grounded supervision through generated rationales, RoI predictions, and optional depth-augmented annotations. Training proceeds in two stages: 2 epochs on VisReason, then 1 epoch on combined VisReason+VisReason-Pro. Inference uses a multi-round dialog interface with optional zoom tool calls based on bbox predictions. The approach teaches models to perform global-to-local "zoom-and-verify" reasoning with depth awareness.

## Key Results
- VISREASON-PRO-7B achieves 0.807 average score on Visual-CoT benchmark vs. 0.770 baseline
- Fine-grained recognition: 0.831 IoU@0.5 (vs 0.681 baseline)
- Spatial relation reasoning: 0.805 accuracy (vs 0.498 baseline)
- Depth grounding: 0.276 grounded ratio, 0.266 depth error (vs 0.011 grounded ratio baseline)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-round, spatially grounded supervision improves visual reasoning accuracy compared to single-step input-to-answer approaches
- Mechanism: The dataset provides explicit intermediate reasoning steps (scene description → RoI selection → rationale → zoom iteration) that models learn to reproduce, creating a "zoom-and-verify" loop that progressively refines attention on relevant regions
- Core assumption: Models can learn to decompose complex visual queries into localized sub-problems through supervised fine-tuning, and this decomposition transfers to unseen queries
- Evidence anchors:
  - [abstract] "VisReason comprises 489K annotated examples... each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps"
  - [section 5.1] Table 2 shows VISREASON-PRO-7B achieves 0.807 average score vs. 0.770 for Qwen-VL-2.5-7B baseline, with gains in fine-grained recognition (0.831 vs 0.681) and spatial relation reasoning (0.805 vs 0.498)
  - [corpus] Related work GeoChain and SceneCOT similarly use multi-step CoT for spatial reasoning, suggesting the approach generalizes, but no direct validation of VisReason's specific mechanism exists in corpus
- Break condition: If task does not require spatial decomposition (e.g., global scene classification), multi-round supervision may add computational overhead without accuracy gains; [section 5.1] notes text/doc OCR tasks show smaller gains

### Mechanism 2
- Claim: Pseudo-3D annotations (depth maps + segmentation) enable depth-aware spatial reasoning without requiring true 3D inputs
- Mechanism: Monocular depth estimation (Depth-Anything V2) and semantic segmentation provide ordinal depth cues that the model learns to reference in reasoning chains, grounding spatial relations like "in front of/behind" alongside 2D relations
- Core assumption: Models can infer approximate 3D structure from single-view depth estimates and use this information consistently during reasoning
- Evidence anchors:
  - [abstract] "VisReason-Pro... enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations"
  - [section 4] Table 4 shows VISREASON-PRO-7B achieves lower depth error (0.266) compared to InternVL-2.5 (0.290) and higher grounded ratio (0.276 vs 0.011)
  - [corpus] SpatialThinker paper similarly uses spatial rewards for 3D reasoning, but corpus lacks direct comparison to VisReason's pseudo-3D approach
- Break condition: If depth estimation is unreliable (e.g., occluded scenes, reflective surfaces), ordinal depth cues may mislead reasoning; this limitation is not quantified in the paper

### Mechanism 3
- Claim: Adaptive zoom-in strategy (zooming only when target is small or cluttered) preserves efficiency while improving localization
- Mechanism: The training data includes single-round examples for large objects (>30% image area) and multi-round chains for smaller targets, teaching models to conditionally invoke zoom operations
- Core assumption: Models can learn a decision boundary for when zooming is beneficial versus wasteful
- Evidence anchors:
  - [section 3] "zoom-in operations are triggered only when the target object or text is small or visually subtle"
  - [section 5.2] Table 6 ablation shows adding adaptive zoom-in (AZ) improves fine-grained (0.831) and relation reasoning (0.729) compared to training on VisReason-Pro alone
  - [corpus] No corpus papers directly validate adaptive zoom strategies; evidence is internal to this work
- Break condition: If threshold for "small" is misestimated, models may over-zoom (wasting computation) or under-zoom (missing fine details)

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) reasoning in LLMs**
  - Why needed here: VisReason extends text-only CoT to multimodal settings; understanding how CoT elicits intermediate rationales in LLMs is prerequisite to grasping why spatial CoT might work similarly
  - Quick check question: Can you explain why step-by-step rationales improve arithmetic reasoning in LLMs, and how this might analogize to visual reasoning?

- Concept: **Visual grounding and region-of-interest (RoI) prediction**
  - Why needed here: The core mechanism requires models to predict bounding boxes [x1, y1, x2, y2] in normalized coordinates; without understanding visual grounding, the RoI prediction task is opaque
  - Quick check question: Given an image and query "What color is the car behind the tree?", what information must a model localize before answering?

- Concept: **Multimodal alignment (vision encoder + LLM projection)**
  - Why needed here: VisReason fine-tunes Qwen2.5-VL, which uses a ViT encoder + projector + LLM; understanding this architecture is necessary to interpret how visual features flow into reasoning steps
  - Quick check question: In a typical MLLM architecture, where does the visual encoder output interface with the language model, and what role does the projector play?

## Architecture Onboarding

- Component map: User -> Qwen2.5-VL-7B (ViT encoder + MLP projector + Qwen LLM) -> Output (answer or zoom tool call)
- Critical path:
  1. User provides image + query
  2. Model generates scene description, predicts RoI, and provides rationale in  I think...  block
  3. If zoom needed, model calls  <tool call>  with bbox coordinates (normalized [0,1])
  4. System crops image to bbox, feeds back as new visual input
  5. Repeat steps 2-4 for up to 5 rounds (evaluation) or until model outputs  <answer> 
  6. Final RoI = last zoom bbox; answer extracted from  <answer>  block

- Design tradeoffs:
  - Multi-round vs. single-round: Multi-round improves localization (IoU@0.75: 0.23 vs. ~0.13 for single-step baselines) but increases inference latency
  - Depth-augmented (VisReason-Pro) vs. 2D-only (VisReason): Pro improves spatial relation reasoning (0.729 vs. 0.705) but requires additional annotation pipeline (Depth-Anything V2 + SAM2 + segmentation)
  - LoRA vs. full fine-tuning: LoRA reduces memory (enables 96 global batch size on 4x H200) but may limit capacity to learn new grounding patterns

- Failure signatures:
  - Over-zooming: Model calls zoom repeatedly without converging; may indicate poor decision boundary for when zoom is needed
  - Bbox out-of-bounds: Model predicts coordinates outside [0,1] range; handled by discarding invalid calls and continuing on previous view
  - Depth hallucination: Model assigns incorrect ordinal depths; Table 4 shows ~0.27 absolute depth error, indicating this remains challenging

- First 3 experiments:
  1. Reproduce VisReason-7B training on a small subset (10K samples) with LoRA; verify model learns to output valid bbox coordinates in  I think...  blocks
  2. Ablate multi-round vs. single-round: Train two models (one with full multi-round chains, one with only single-round rationales) and compare IoU@0.5 on VisReason-Pro benchmark
  3. Test adaptive zoom threshold: Vary the area ratio threshold N (default N=2 in Algorithm 1) and measure impact on inference rounds and accuracy; expect tradeoff between efficiency and localization precision

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does training on synthetic rationales generated by teacher LLMs (GPT-4.1-Nano/Mini) constrain the student model to mimicking specific linguistic patterns rather than learning generalizable visual logic?
- **Basis in paper:** [Inferred] from Section 3.1, which describes the dataset construction relying entirely on model-generated "human-like rationales" and "detailed reasoning traces" without human verification of the reasoning logic itself.
- **Why unresolved:** While the paper demonstrates improved answer accuracy, it does not analyze if the *intermediate* reasoning steps are logically distinct or diverse, or if the model is simply learning the teacher's "style" of reasoning.
- **What evidence would resolve it:** An analysis of the semantic diversity of the generated rationales compared to human baselines, or ablation studies measuring performance drops when the linguistic style of the rationale is perturbed.

### Open Question 2
- **Question:** How does the geometric error inherent in monocular pseudo-depth estimation propagate through the training pipeline, limiting the model's ability to resolve fine-grained occlusion or metric depth relationships?
- **Basis in paper:** [Inferred] from Section 8.2 (Pseudo-3D annotation), which acknowledges the use of Depth-Anything V2 for depth maps, and Section 5.1, which notes that "grounded ratios and depth predictions remain challenging for all models."
- **Why unresolved:** It is unclear if the reported depth errors (Tab 4) are a result of the model's limited capacity or noise in the pseudo-ground truth supervision used for VisReason-Pro.
- **What evidence would resolve it:** Evaluating the model on a benchmark with LiDAR or stereo-depth ground truth to determine if the pseudo-depth supervision creates a "glass ceiling" for 3D reasoning accuracy.

### Open Question 3
- **Question:** Can the "zoom-and-verify" behavior be induced effectively in model architectures that lack explicit region-of-interest (RoI) extraction mechanisms, or is the performance gain dependent on the base model's architecture?
- **Basis in paper:** [Inferred] from Section 5, which exclusively utilizes Qwen2.5-VL for fine-tuning and evaluation, leaving the architectural dependency of the CoT training unexplored.
- **Why unresolved:** It remains untested whether the spatially grounded CoT data is universally beneficial or if it requires a specific visual encoder (like Qwen's) to be utilized effectively.
- **What evidence would resolve it:** Fine-tuning diverse MLLM architectures (e.g., InternVL, LLaVA) on VisReason and comparing the relative performance gains on the Visual-CoT benchmark.

## Limitations
- Dataset construction relies entirely on GPT-4.1-Nano/Mini without human verification of reasoning quality
- Pseudo-3D annotations introduce noise that may limit model's 3D spatial reasoning accuracy
- Performance gains may be architecture-dependent (tested only on Qwen2.5-VL)

## Confidence
- **High confidence**: Multi-round supervision improves visual reasoning accuracy
- **Medium confidence**: Pseudo-3D annotations improve depth-aware reasoning
- **Medium confidence**: Adaptive zoom strategy preserves efficiency

## Next Checks
1. **Dataset quality audit**: Generate 100 samples from VisReason-Pro and have 3 independent human annotators rate the faithfulness of depth-grounded reasoning chains (0-1 scale); compare against GPT-4.1-Nano annotations to estimate annotation reliability
2. **Depth noise robustness test**: Retrain VISREASON-PRO-7B on VisReason without depth annotations; compare performance on spatial reasoning tasks to quantify contribution of depth cues versus other factors
3. **Generalization stress test**: Evaluate trained models on out-of-distribution scenes (e.g., medical imaging, satellite imagery) with novel spatial relations to verify that depth-aware reasoning transfers beyond training domains