---
ver: rpa2
title: Large Language Models for Explainable Threat Intelligence
arxiv_id: '2511.05406'
source_url: https://arxiv.org/abs/2511.05406
tags:
- context
- system
- user
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAGRecon addresses the challenge of processing and understanding
  complex cybersecurity threat intelligence (CTI) by combining large language models
  (LLMs) with retrieval-augmented generation (RAG) and knowledge graph (KG) visualization.
  The system retrieves relevant context from domain-specific documents, uses an LLM
  to generate answers and structured relationship data, and visualizes this information
  as an interactive knowledge graph to enhance explainability and transparency.
---

# Large Language Models for Explainable Threat Intelligence

## Quick Facts
- arXiv ID: 2511.05406
- Source URL: https://arxiv.org/abs/2511.05406
- Reference count: 26
- RAGRecon achieves faithfulness >0.8 and context relevance ~8% on cybersecurity CTI datasets

## Executive Summary
RAGRecon addresses the challenge of processing and understanding complex cybersecurity threat intelligence (CTI) by combining large language models (LLMs) with retrieval-augmented generation (RAG) and knowledge graph (KG) visualization. The system retrieves relevant context from domain-specific documents, uses an LLM to generate answers and structured relationship data, and visualizes this information as an interactive knowledge graph to enhance explainability and transparency. Experimental evaluation on two datasets—conventional CTI (24 reports) and blockchain CTI (28 reports)—with seven different LLMs demonstrated that RAGRecon achieved faithfulness scores exceeding 0.8 out of 1.0 and context relevance around 8%, with over 91% of responses matching reference answers in the best configurations. Manual verification of 2,050 automated decisions confirmed high accuracy, ranging from 90% to 97%, indicating that RAGRecon effectively reduces hallucinations and provides reliable, explainable threat intelligence insights.

## Method Summary
RAGRecon implements a RAG-Sequence approach where documents are chunked (1000 characters, 100-character overlap) and embedded using sentence-transformers/all-MiniLM-L6-v2, stored in ChromaDB. For each query, the system retrieves top-6 semantically similar chunks, generates an answer using an LLM, and extracts structured knowledge graph data as JSON. The system uses dual prompts—one for answer generation and another for KG extraction—and visualizes the results interactively. The evaluation used two datasets (24 conventional CTI and 28 blockchain CTI reports) with 50 question-reference pairs each, testing seven different LLMs with LLM self-evaluation validated by manual verification on 2,050 decisions.

## Key Results
- Faithfulness scores exceeded 0.8 out of 1.0, indicating low hallucination rates
- Context relevance averaged approximately 8%, meaning 92% of retrieved content was unused
- Over 91% of responses matched reference answers in best configurations
- Manual verification confirmed 90-97% accuracy across 2,050 automated decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation reduces hallucinations by grounding LLM responses in domain-specific context
- Mechanism: Query embedding → Dense retrieval from vector store → Top-K context concatenation → Context-injected prompt → Grounded generation
- Core assumption: The embedding model (all-MiniLM-L6-v2) captures semantic similarity sufficient for CTI domain relevance
- Evidence anchors:
  - [abstract]: "RAGRecon achieved faithfulness scores exceeding 0.8 out of 1.0... indicating that RAGRecon effectively reduces hallucinations"
  - [section V.D.2]: "The faithfulness of the generated answers was consistently high for both datasets, indicating a low rate of model hallucination"
  - [corpus]: "Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation" supports RAG integration for domain-specific cybersecurity knowledge (FMR: 0.59)
- Break condition: If retrieval returns irrelevant chunks (low context relevance), the LLM may still hallucinate despite RAG; if source documents lack answer, system cannot generate accurate response

### Mechanism 2
- Claim: Knowledge Graph visualization makes LLM reasoning traceable by exposing extracted entities and relationships
- Mechanism: Retrieved context → Specialized prompt → LLM extracts entities/relationships as structured JSON → NetworkX graph construction → Pyvis interactive HTML
- Core assumption: The LLM can reliably output valid JSON with meaningful entity-relationship triples
- Evidence anchors:
  - [abstract]: "generating and visually presenting to the user a knowledge graph for every reply... increases transparency and interpretability"
  - [section VI]: "A key limitation we observed, particularly in models with up to 20 billion parameters, was their inconsistent reliability in processing data for the KG. Formatting errors were the primary cause"
  - [corpus]: Limited corpus support for this specific KG visualization mechanism; related papers focus on detection rather than explainability
- Break condition: JSON parsing failures from LLM formatting errors; smaller models exhibit inconsistent structured output reliability

### Mechanism 3
- Claim: LLM self-evaluation with manual spot-checking provides scalable yet trustworthy quality assessment
- Mechanism: Generated answer → Decompose into statements → Separate LLM verifies each against context → Calculate faithfulness ratio; parallel process for context relevance
- Core assumption: The evaluating LLM has sufficient semantic understanding to judge equivalence and support
- Evidence anchors:
  - [section V.C]: "Faithfulness = Number of Statements Supported by Context / Total Number of Statements"
  - [section V.D.3]: "Manual verification of 2,050 automated decisions was performed... correct decision rates generally ranging from 90% to 97% with 7 LLMs"
  - [corpus]: Related papers don't directly validate this specific self-evaluation methodology for CTI
- Break condition: If evaluator LLM has systematic biases (e.g., over-generous equivalence judgments), manual verification reveals discrepancies >10%

## Foundational Learning

- Concept: **Dense Retrieval with Vector Embeddings**
  - Why needed here: RAGRecon converts documents and queries into high-dimensional vectors; similarity search finds semantically relevant chunks regardless of keyword overlap
  - Quick check question: Why might dense retrieval outperform keyword matching for a query like "What TTPs does APT29 use?" when the report says "Cozy Bear employs living-off-the-land techniques"?

- Concept: **RAG-Sequence vs. RAG-Token**
  - Why needed here: RAGRecon uses RAG-Sequence (retrieve once, generate entire response); understanding this choice clarifies latency vs. precision tradeoffs
  - Quick check question: What response characteristic might suffer if you switched to RAG-Token for real-time threat queries?

- Concept: **Context Window and Chunk Overlap**
  - Why needed here: Documents split at 1000 characters with 100-character overlap; this affects what context the LLM receives for each query
  - Quick check question: If a critical entity-relationship spans a chunk boundary, what happens to that information in retrieval?

## Architecture Onboarding

- Component map:
  PyPDFDirectoryLoader -> RecursiveCharacterTextSplitter (1000 chars, 100 overlap, hierarchical separators) -> HuggingFaceEmbeddings (all-MiniLM-L6-v2) -> ChromaDB (disk-persisted at ./chroma_db) -> User query -> Embedding -> ChromaDB similarity search (k=6) -> Context concatenation -> Dual prompts (answer + graph extraction) -> LLM via aisuite -> JSON parser -> NetworkX + Pyvis visualization -> Response delivery

- Critical path:
  Query submission → Embedding generation → Top-6 chunk retrieval → Context assembly → LLM call 1 (graph JSON) → JSON parse → LLM call 2 (answer) → KG HTML generation → Response delivery

- Design tradeoffs:
  - k=6 chunks: Paper reports ~8% context relevance, suggesting 92% retrieved content unused; higher k increases context noise
  - Single embedding model for query and documents: Consistency but domain-specific models (e.g., cybersecurity-tuned) might improve retrieval
  - Model-agnostic LLM interface: Flexibility but requires robust prompt engineering across diverse model behaviors
  - Stateful conversation history in Flask global variables: Simple but not production-safe for concurrent users

- Failure signatures:
  - **JSON parsing errors in KG generation**: Check LLM output format; smaller models (<20B params) show inconsistent reliability per Conclusion
  - **Low faithfulness (<0.8)**: Inspect retrieved chunks for relevance; may indicate embedding model mismatch or sparse source coverage
  - **Empty or sparse KG**: Prompt may need refinement; verify context actually contains extractable relationships
  - **Context relevance near 0%**: Retrieval failure; check embedding quality, chunk strategy, or query-document semantic gap

- First 3 experiments:
  1. **Baseline retrieval quality**: Run 10 sample queries, manually inspect top-6 chunks for each; calculate precision@k to validate 8% context relevance finding
  2. **KG extraction reliability by model**: Test same 5 contexts across 3 different LLMs; measure JSON parse success rate and entity-relationship quality
  3. **Faithfulness vs. k sweep**: Run evaluation dataset with k=3, k=6, k=12; plot faithfulness and context relevance curves to identify optimal retrieval size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the observed performance advantage for blockchain CTI over conventional CTI statistically significant, or simply attributable to limited dataset size?
- Basis in paper: [explicit] The authors state "More testing with larger datasets is required to determine whether this performance difference is statistically significant" after noting the blockchain dataset showed slightly higher correct decision rates (94.57–94.86% vs 92.57–93.43%).
- Why unresolved: Current datasets are small (50 questions each from 24 and 28 reports), preventing statistical validation of the trend.
- What evidence would resolve it: Large-scale evaluation with significantly larger datasets (e.g., 500+ questions per domain) with statistical significance testing.

### Open Question 2
- Question: How can knowledge graph extraction reliability be improved for models with fewer than 20 billion parameters?
- Basis in paper: [explicit] The authors note "A key limitation we observed, particularly in models with up to 20 billion parameters, was their inconsistent reliability in processing data for the KG. Formatting errors were the primary cause of this problem."
- Why unresolved: The paper does not propose or evaluate solutions to the formatting reliability issue in smaller models.
- What evidence would resolve it: Comparative study of prompt engineering techniques, fine-tuning, or structured output methods across model sizes measuring JSON formatting success rates.

### Open Question 3
- Question: Can retrieval efficiency be improved beyond the observed ~8% context relevance while maintaining answer quality?
- Basis in paper: [inferred] The system uses "approximately 8% of the retrieved context on average to generate answers," meaning 92% of retrieved content was not directly useful for answering, yet this impacts computational cost.
- Why unresolved: The paper does not explore whether different chunk sizes, retrieval methods, or top-K values could improve relevance without degrading faithfulness or answer accuracy.
- What evidence would resolve it: Systematic ablation study varying chunk size, retrieval strategies, and top-K values, measuring context relevance, faithfulness, and answer match rates.

## Limitations
- Knowledge graph generation shows inconsistent reliability, particularly with models up to 20 billion parameters, where formatting errors are common
- Approximately 92% of retrieved context is unused (8% relevance), suggesting significant retrieval inefficiency
- Limited dataset size (24 conventional CTI and 28 blockchain CTI reports) prevents statistical validation of observed performance trends

## Confidence

- **High confidence**: The retrieval-augmented generation mechanism effectively reduces hallucinations (faithfulness >0.8) when domain-specific context is available in the source documents. This finding is supported by consistent experimental results across both datasets and multiple LLMs.

- **Medium confidence**: The knowledge graph visualization meaningfully increases transparency and interpretability. While the paper demonstrates the capability exists, the noted formatting errors and inconsistent reliability suggest this feature may not work robustly across all use cases or model configurations.

- **Low confidence**: The claimed context relevance of approximately 8% represents an area of concern. This suggests that 92% of retrieved content may be irrelevant, indicating potential inefficiencies in the retrieval pipeline that could impact both performance and user experience.

## Next Checks

1. **Retrieval precision validation**: Run the 50 evaluation queries on the conventional CTI dataset with manual inspection of the top-6 retrieved chunks for each query. Calculate precision@k to verify the 8% context relevance claim and identify whether the low relevance stems from embedding model limitations or document-query semantic gaps.

2. **KG extraction reliability benchmark**: Test the knowledge graph generation across at least three different LLMs (small, medium, large parameter counts) using identical context. Measure JSON parsing success rates, entity-relationship extraction quality, and time-to-first-error to quantify the inconsistent reliability problem noted in the conclusion.

3. **Faithfulness sensitivity analysis**: Systematically vary the number of retrieved chunks (k=3, 6, 9, 12) on a subset of 20 evaluation queries. Plot faithfulness scores and answer accuracy against k to determine whether the 8% context relevance represents an optimal tradeoff or indicates retrieval inefficiency.