---
ver: rpa2
title: 'DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation'
arxiv_id: '2504.05122'
source_url: https://arxiv.org/abs/2504.05122
tags:
- translation
- context
- docia
- document-level
- refinement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DoCIA introduces an online framework for speech translation that
  integrates document-level context into the ASR refinement, MT, and MT refinement
  stages using LLM-based modules. The framework employs a multi-level context integration
  strategy and a determination mechanism to prevent hallucinations during refinement.
---

# DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation

## Quick Facts
- arXiv ID: 2504.05122
- Source URL: https://arxiv.org/abs/2504.05122
- Reference count: 35
- Primary result: DoCIA integrates document-level context into cascaded ASR/MT refinement stages, achieving +1.42 s-Comet and +0.319 d-Comet improvements over baselines

## Executive Summary
DoCIA introduces an online framework for speech translation that integrates document-level context into the ASR refinement, MT, and MT refinement stages using LLM-based modules. The framework employs a multi-level context integration strategy and a determination mechanism to prevent hallucinations during refinement. Experimental results across five language pairs and four LLMs show DoCIA significantly outperforms traditional baselines in both sentence and discourse-level metrics.

## Method Summary
DoCIA decomposes speech translation into four sequential stages: ASR → ASR Refinement → MT → MT Refinement. Document-level context is incorporated at stages 2-4 through auxiliary LLM-based modules. The framework uses a multi-level context integration strategy combining short-memory (m adjacent segments) and long-memory (BM25-retrieved earlier segments) contexts. An online updating mechanism ensures each segment benefits from refined predecessors, while a determination mechanism prevents hallucinations by filtering refinement outputs based on similarity thresholds.

## Key Results
- Average improvements of +1.42 in s-Comet and +0.319 in d-Comet scores across five language pairs
- Multi-level context integration shows complementary effects: reducing short-memory impacts sentence-level metrics more, while reducing long-memory affects discourse-level metrics more
- Determination mechanism is critical: removing it causes -0.82 to -0.98 s-Comet drop across models
- Online context updating provides +1.09 s-Comet improvement over offline context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating document-level context at multiple cascaded stages improves speech translation quality more effectively than single-stage integration.
- Mechanism: DoCIA decomposes ST into four sequential stages (ASR → ASR Refinement → MT → MT Refinement). Document-level context is injected at stages 2-4 via LLM prompts, allowing error correction to propagate. Online context updating ensures each segment benefits from refined predecessors.
- Core assumption: Cascaded refinement with context provides compounding benefits; ASR errors can be partially corrected before translation.
- Evidence anchors:
  - [abstract] "DoCIA decomposes the ST pipeline into four stages. Document-level context is integrated into the ASR refinement, MT, and MT refinement stages through auxiliary LLM-based modules."
  - [section 3.2] "Document-level context boosts performance more when combined with other stages than using alone... DoCIAa-m brings +1.12 s-Comet and +0.164 d-Comet gains over DoCIAa."
  - [corpus] Related work "Speech Translation Refinement using Large Language Models" (FMR=0.63) supports LLM-based ST refinement, though DoCIA's multi-stage cascade is novel.

### Mechanism 2
- Claim: Combining short-term adjacent context with retrieved long-term context balances local coherence and global discourse dependencies better than using all preceding context.
- Mechanism: Multi-level context = Short-memory (m immediately preceding segments) + Long-memory (top-n segments retrieved via BM25 from earlier document). Fixed window L=m+n limits computational overhead while preserving discourse signals.
- Core assumption: BM25 retrieval surfaces relevant long-range dependencies; not all context is equally useful.
- Evidence anchors:
  - [section 2.2] "Short-Memory Context... adjacent sentences are effective in addressing inter-sentence issues... Long-Memory Context... clues for alleviating inter-segment issues may lie in a longer memory window."
  - [section 4.5, Figure 4] "Reducing short-memory context has more significant impact on s-Comet, while decreasing long-memory context affects d-Comet score more... complementary nature."
  - [corpus] "Discourse Graph Guided Document Translation" (FMR=0.58) explores alternative discourse modeling; DoCIA's retrieval-based approach is simpler but less structured.

### Mechanism 3
- Claim: A similarity-based determination threshold prevents LLM refinement from introducing hallucinations while preserving beneficial corrections.
- Mechanism: Compute normalized indel similarity between draft (I) and refined (O) outputs. If similarity < threshold λ, discard refinement and retain draft. This limits excessive modification that could distort semantics.
- Core assumption: Small edits are safer; large edits correlate with hallucinations or semantic drift.
- Evidence anchors:
  - [section 2.3] "A simple yet effective determination mechanism is introduced to prevent hallucinations from excessive refinement, ensuring the reliability of the final results."
  - [Table 2, section 4.1] Removing refinement determination (w/o R.D.) causes -0.82 to -0.98 s-Comet drop across models, confirming its role in quality preservation.
  - [corpus] "Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?" (FMR=0.55) examines context in APE but doesn't address hallucination thresholds directly; DoCIA's mechanism is a practical contribution.

## Foundational Learning

- Concept: Cascaded Speech Translation (ASR → MT pipeline)
  - Why needed here: DoCIA extends traditional 2-stage cascade to 4 stages with refinement. Understanding error propagation across stages is essential.
  - Quick check question: Can you explain why ASR errors compound in cascaded ST and where DoCIA intervenes?

- Concept: Document-Level Context in MT
  - Why needed here: Core premise of DoCIA. You must understand discourse phenomena (coreference, lexical cohesion) that require inter-segment context.
  - Quick check question: What discourse phenomena does document-level context address that sentence-level MT cannot?

- Concept: LLM Prompting for Structured Refinement
  - Why needed here: DoCIA uses instruction-following LLMs with JSON output formatting for each stage. Prompt design (Appendix A) directly affects quality.
  - Quick check question: How would you design a prompt to refine ASR output while preventing over-correction?

## Architecture Onboarding

- Component map:
  - Audio segment → ASR → draft transcription → Context Store → BM25 Retriever → LLM Agent → Determination Module → refined output

- Critical path:
  1. Audio segment → ASR → draft transcription (¯si)
  2. Retrieve context (Cs_asr + Cl_asr via BM25) → LLM ASR refinement → si (apply determination)
  3. Update ASR context store with si
  4. Retrieve translation context (Ctr) → LLM translation → ¯ti
  5. LLM translation refinement → ti (apply determination)
  6. Update translation context store with ti
  7. Repeat for next segment (online update)

- Design tradeoffs:
  - Window size L=6 (m=n=3): Paper shows diminishing returns beyond L=6; larger windows increase latency and noise.
  - BM25 vs. dense retrieval: BM25 is faster; dense embeddings may improve relevance but add complexity.
  - Shared λ for ASR/translation determination: Simplifies tuning; separate thresholds might be marginally better.
  - Online vs. offline context: Online updating yields +1.09 s-Comet improvement; requires sequential processing (no parallelization across segments).

- Failure signatures:
  - **Hallucination in refinement**: Output diverges semantically from input; determination module should catch this (low similarity). If λ is misconfigured, hallucinations pass through.
  - **Context noise degradation**: If BM25 retrieves irrelevant segments, translation quality drops. Monitor d-Comet for discourse-level regression.
  - **ASR error propagation**: Severe ASR errors (e.g., misrecognized entities) may not be correctable if context lacks the entity. ERA metric (Table 6) tracks this.
  - **Latency bottleneck**: Four-stage LLM pipeline per segment; production deployment requires latency budgeting.

- First 3 experiments:
  1. **Ablate multi-level context**: Run DoCIA with only short-memory (m=6, n=0) vs. only long-memory (m=0, n=6) vs. balanced (m=n=3). Measure s-Comet/d-Comet to validate complementary roles (replicate Figure 4).
  2. **Sensitivity analysis on λ**: Sweep λ ∈ {0.5, 0.6, 0.7, 0.8, 0.9} on a held-out validation set. Identify optimal threshold and confirm paper's λ=0.7 finding (replicate Table 9).
  3. **Online vs. offline context comparison**: Run full DoCIA pipeline with online context updates vs. fixed offline context (no updates). Quantify the contribution of real-time context quality (replicate Table 4).

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead from four-stage LLM pipeline per segment
- BM25 retrieval assumes lexical overlap captures discourse relevance, which may fail for abstract content
- Experimental scope covers five language pairs but excludes low-resource languages and domain-specific speech

## Confidence
- **High confidence**: Multi-stage context integration improves quality (supported by ablation studies and consistent s-Comet/d-Comet gains across all LLMs and language pairs)
- **Medium confidence**: Multi-level context (short + long memory) is optimal (evidence shows complementary effects, but retrieval quality not independently validated)
- **Medium confidence**: Determination mechanism prevents hallucinations (quantitative drop when disabled, but qualitative analysis of false positives/negatives missing)

## Next Checks
1. **Domain robustness test**: Evaluate DoCIA on specialized speech corpora (medical, technical) to assess retrieval relevance and refinement accuracy outside general conversational domains.

2. **Latent retrieval comparison**: Replace BM25 with dense passage retrieval (e.g., DPR) and measure impact on discourse coherence (d-Comet) and computational overhead.

3. **Hallucination analysis**: Conduct human evaluation on refinement outputs with λ thresholds swept from 0.5-0.9 to quantify false rejection/acceptance rates and semantic drift cases.