---
ver: rpa2
title: Tractable Asymmetric Verification for Large Language Models via Deterministic
  Replicability
arxiv_id: '2509.11068'
source_url: https://arxiv.org/abs/2509.11068
tags:
- verification
- output
- framework
- agent
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of verifying the authenticity
  of LLM outputs in multi-agent systems, where an agent needs to confirm that another
  agent's output was genuinely generated by a claimed model and not falsified. The
  authors propose a verification framework based on deterministic replicability, leveraging
  the autoregressive nature of LLMs to enable targeted validation of specific segments
  of an output sequence.
---

# Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability

## Quick Facts
- arXiv ID: 2509.11068
- Source URL: https://arxiv.org/abs/2509.11068
- Authors: Zan-Kai Chong; Hiroyuki Ohsaki; Bryan Ng
- Reference count: 33
- One-line primary result: Verification of LLM outputs can be 12× faster than full regeneration while maintaining security guarantees through distributed probabilistic verification

## Executive Summary
This paper addresses the challenge of verifying LLM output authenticity in multi-agent systems, where one agent must confirm that another agent's output was genuinely generated by a claimed model. The authors propose a verification framework based on deterministic replicability, leveraging the autoregressive nature of LLMs to enable targeted validation of specific segments. They introduce distributed probabilistic verification where multiple validators randomly sample segments to achieve high collective security with minimal computational overhead. The framework requires a homogeneous computational environment with identical hardware and software stacks for all agents.

## Method Summary
The verification framework exploits deterministic token generation in autoregressive LLMs. When given a prompt and model weights, an LLM deterministically generates the same output sequence. The authors leverage this by having validators regenerate only specific segments of the output using the original prompt concatenated with the prefix tokens. This targeted validation is substantially faster than full regeneration. For security, they distribute verification across multiple validators who randomly sample different segments, creating an exponentially increasing probability of detecting tampering as more validators participate. The approach requires strict hardware and software homogeneity to ensure exact token matching across generator and validator agents.

## Key Results
- Targeted verification can be over 12× faster than full regeneration for validating the last 50 tokens of a 792-token output
- Distributed probabilistic verification achieves tunable detection probability with minimal computational overhead
- Cross-GPU verification fails due to floating-point non-determinism, necessitating homogeneous hardware environments
- The framework successfully detects tampered segments when validators sample randomly from output sequences

## Why This Works (Mechanism)

### Mechanism 1: Targeted Validation via Deterministic Replicability
- **Claim:** Verifying an LLM output segment requires substantially less computation than regenerating the entire sequence.
- **Mechanism:** Autoregressive LLMs sample tokens sequentially based on a probability distribution conditioned on all prior tokens ($y_i \sim P(\cdot | x_1, ..., y_{i-1})$). If a validator constructs a prompt containing the original input $X$ and the first $j$ tokens of the output $Y$, an identical model instance will deterministically generate the exact subsequent tokens.
- **Core assumption:** The generation process is computationally deterministic; identical input context, model weights, and sampling parameters yield identical outputs.
- **Evidence anchors:**
  - [abstract] "targeted verification can be over 12 times faster than full regeneration"
  - [section III-B] "If we construct a new prompt X' by concatenating the original prompt with the first j tokens of its output... L will subsequently generate the exact remaining sequence."
  - [corpus] Corpus signals discuss multi-agent verification but primarily focus on process variance or safety constraints (e.g., MAS-ProVe) rather than the low-level computational determinism exploited here.
- **Break condition:** Hardware heterogeneity introduces floating-point discrepancies, breaking the exact replicability required for token matching.

### Mechanism 2: Distributed Probabilistic Verification
- **Claim:** Increasing the number of independent validators creates an exponentially higher probability of detecting tampering, even if each validator checks only a small fraction of the output.
- **Mechanism:** The output sequence is divided into $k$ segments, with $f$ tampered segments. If $q$ validators independently sample $r$ segments each, the probability that all validators miss the tampered segments is $P(Fail)^q$. This allows system-wide security to scale efficiently without increasing the load on any single validator.
- **Core assumption:** Adversaries cannot predict which segments specific validators will sample (random selection).
- **Evidence anchors:**
  - [abstract] "distributes the verification workload effectively... tunable detection probability"
  - [section III-C] "The system’s security now grows not only exponentially with the number of validators, q..."
  - [corpus] Adjacent work in MAS-ProVe suggests process verification helps, but this specific mathematical formulation for distributed segment sampling is unique to this paper.
- **Break condition:** If the adversary can compromise the random sampling mechanism or predict validator assignments, they can evade detection.

### Mechanism 3: Hardware-Enforced Determinism
- **Claim:** Verification success is strictly bound by a homogeneous hardware and software stack across generator and validator agents.
- **Mechanism:** Verification relies on exact token matching. The authors experimentally show that different GPU architectures (e.g., RTX 4000 Ada vs. NVIDIA A40) produce minute floating-point variances during inference, resulting in different output tokens for the same context. Therefore, strict environmental standardization acts as a prerequisite control rather than a tunable parameter.
- **Core assumption:** Non-determinism in floating-point operations across different hardware is significant enough to alter token selection.
- **Evidence anchors:**
  - [section IV-C] "generating an output sequence on one GPU model... and attempting to validate it on a different GPU model... led to verification failures."
  - [abstract] "strictly necessitates a computationally homogeneous environment"
  - [corpus] Limited direct evidence in the provided corpus neighbors regarding hardware-level floating point variance in LLM verification.
- **Break condition:** Deployment in decentralized networks with consumer-grade heterogeneous hardware.

## Foundational Learning

- **Concept:** Autoregressive Token Dependency
  - **Why needed here:** The entire verification scheme depends on understanding that tokens are not generated in isolation but are strict mathematical functions of all preceding tokens.
  - **Quick check question:** If you change token $y_5$ in a sequence, can you guarantee that token $y_6$ remains valid without regenerating it?

- **Concept:** Asymmetric Computational Effort
  - **Why needed here:** The paper aims to solve the "verification cost" problem. Understanding this concept explains why "Targeted Validation" (checking $j$ tokens) is superior to "Full Regeneration" (checking all tokens).
  - **Quick check question:** Does the verification cost scale linearly, sub-linearly, or constantly with the length of the original output in this framework?

- **Concept:** Floating-Point Non-Determinism
  - **Why needed here:** A common misconception is that software determinism (seed + code) ensures identical outputs. The paper highlights that hardware parallelism (GPU kernels) breaks this assumption.
  - **Quick check question:** Why would $1.0 + 1.0$ potentially yield different results on two different pieces of hardware?

## Architecture Onboarding

- **Component map:**
  - Generator Agent -> Context Constructor -> Validator Agent -> Comparator

- **Critical path:** The verification flow (Context Construction → Partial Inference → Comparison) must be lower latency than the generation flow (Full Inference). The requirement for **Identical Stack** (CUDA version, GPU architecture) is a hard dependency for deployment.

- **Design tradeoffs:**
  - **Segment Size ($r$) vs. Validator Count ($q$):** You can achieve high detection rates by either having many validators check 1 segment each or fewer validators check multiple segments. The paper suggests increasing $q$ is often more robust.
  - **Speed vs. Coverage:** Validating the "Last 50 tokens" is 12× faster than full generation, but provides no verification for the first 90% of the content.

- **Failure signatures:**
  - **Intermittent Mismatches:** If verification fails randomly across identical hardware, suspect non-determinism in software libraries (e.g., nondeterministic algorithms in attention layers) rather than malicious intent.
  - **Total Mismatch:** If *all* tokens fail validation, suspect a model weight mismatch or prompt formatting difference.

- **First 3 experiments:**
  1. **Hardware Sensitivity Test:** Generate a text on GPU A and verify on GPU B. If $GPU_A \neq GPU_B$, measure the discrepancy rate to validate Section IV-C.
  2. **Latency Profiling:** Measure the wall-clock time of generating 500 tokens vs. verifying the last 50 tokens to reproduce the "12× faster" claim in Table I.
  3. **Detection Probability Simulation:** Simulate a "malicious" output with 10% tampered segments. Run 10,000 trials with 5 validators checking 2 segments each to verify Eq. 3 predictions.

## Open Questions the Paper Calls Out

None

## Limitations
- Requires homogeneous hardware and software environments across all agents, limiting practical deployment in heterogeneous systems
- Security guarantees depend on random segment selection, with no discussion of adversary prediction or sampling mechanism compromise
- Experimental validation limited to single model (Meta-Llama-3-8B-Instruct) and specific prompt type, reducing generalizability

## Confidence

**High Confidence:**
- Targeted validation is computationally more efficient than full regeneration (12x speedup claim)
- Hardware heterogeneity introduces floating-point discrepancies that break exact replicability
- The mathematical formulation for distributed probabilistic verification is internally consistent

**Medium Confidence:**
- The framework's security guarantees against sophisticated adversaries
- The detection probability model accurately reflects real-world adversarial behavior
- Performance benefits translate to other LLM models and prompt types

**Low Confidence:**
- Practical deployability in heterogeneous hardware environments
- Security against targeted attacks that compromise the random sampling mechanism
- Scalability to very large output sequences or models beyond 8B parameters

## Next Checks
1. **Cross-Platform Determinism Test:** Implement the verification framework across three different GPU architectures (e.g., RTX 4000 Ada, A40, and consumer-grade RTX 3060) using identical software stacks. Measure the exact token mismatch rate and determine if the determinism requirement is an absolute barrier or if there's a tolerance threshold that maintains security while allowing some hardware diversity.

2. **Adversarial Sampling Attack Simulation:** Design an attack where the adversary attempts to predict which segments validators will check. Implement a basic predictor that learns from validator behavior patterns and measure the success rate of bypassing detection. This would validate whether the "random" sampling assumption is sufficient or if additional obfuscation mechanisms are needed.

3. **Multi-Model Generalization Study:** Test the verification framework with diverse model families (including non-Transformer architectures, different parameter counts from 1B to 70B, and models with different tokenization schemes). Document whether the computational efficiency gains and security guarantees hold across model types, or if the framework is specific to the tested Meta-Llama-3-8B-Instruct configuration.