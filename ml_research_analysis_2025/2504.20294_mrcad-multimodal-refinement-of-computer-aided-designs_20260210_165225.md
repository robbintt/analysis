---
ver: rpa2
title: 'mrCAD: Multimodal Refinement of Computer-aided Designs'
arxiv_id: '2504.20294'
source_url: https://arxiv.org/abs/2504.20294
tags:
- refinement
- instructions
- rounds
- text
- drawing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mrCAD, a dataset and benchmark for studying
  multimodal refinement in collaborative design. The authors collected 15,163 instruction-execution
  rounds from 1,092 human dyads playing a communication game where one player (Designer)
  guides another (Maker) to recreate CAD designs using text and drawing.
---

# mrCAD: Multimodal Refinement of Computer-aided Designs

## Quick Facts
- arXiv ID: 2504.20294
- Source URL: https://arxiv.org/abs/2504.20294
- Reference count: 16
- Primary result: State-of-the-art VLMs perform worse at refinement instructions than generation instructions, with models like GPT-4o achieving 0.547 proportional improvement on generation versus -0.119 on refinement.

## Executive Summary
This paper introduces mrCAD, a dataset and benchmark for studying multimodal refinement in collaborative design. The authors collected 15,163 instruction-execution rounds from 1,092 human dyads playing a communication game where one player (Designer) guides another (Maker) to recreate CAD designs using text and drawing. Analysis shows that human designers use more drawing for initial generation and more text for refinement, with multimodal instructions being more effective than single-modality ones. When evaluated, state-of-the-art vision-language models perform worse at following refinement instructions than generation instructions.

## Method Summary
The mrCAD dataset was collected through a two-player communication game where Designers instruct Makers to recreate target CAD designs over multiple rounds using text and drawing tools. The dataset contains 6,082 games with 15,163 instruction-execution rounds from 1,092 human dyads. CAD designs were sampled from SketchGraphs and normalized to a 20×20 grid. The evaluation framework uses symmetric chamfer distance to measure reconstruction accuracy, with proportional improvement (PI) as the primary metric comparing design distances before and after instruction execution. The authors fine-tuned Qwen2.5-VL-7B-Instruct with LoRA and evaluated GPT-4o, Claude-3.7-Sonnet, and the fine-tuned model using tool-calling prompts.

## Key Results
- Human designers use more drawing for initial generation and more text for refinement, with multimodal instructions achieving higher success rates
- State-of-the-art VLMs show 0.547 proportional improvement on generation rounds versus -0.119 on refinement rounds compared to human performance of 0.854 and 0.119 respectively
- Fine-tuning with supervised learning improves overall performance but fails to transfer gains to refinement turns

## Why This Works (Mechanism)
Multimodal instructions combining text and drawing achieve better reconstruction accuracy than single-modality instructions because drawings efficiently convey spatial relationships and geometric primitives that are cumbersome to describe textually, while text provides precise semantic information and procedural guidance that drawings cannot easily express.

## Foundational Learning
- **Symmetric Chamfer Distance**: Measures point-to-curve distance between designs by sampling points along curves; needed to quantify reconstruction accuracy objectively; quick check: validate against provided ground truth distances
- **Proportional Improvement (PI)**: Normalizes performance gain relative to initial design quality; needed to fairly compare across different starting designs; quick check: ensure PI values fall between -1 and 1
- **Tool-Calling Framework**: Enables VLMs to execute CAD operations through structured API calls; needed to bridge natural language instructions with CAD manipulation; quick check: verify tool outputs match expected schema
- **Multi-round Communication**: Models the iterative refinement process in collaborative design; needed to study how instructions evolve from generation to modification; quick check: ensure context history includes all prior rounds
- **Multimodal Instruction Classification**: Categorizes instructions by modality (text/drawing) and intent (generate/modify); needed to analyze strategy effectiveness; quick check: validate classification accuracy on human instructions

## Architecture Onboarding
**Component Map**: CAD Designs -> VLM with Tool-Calling -> Tool Execution -> New Design -> Distance Computation -> PI Score
**Critical Path**: VLM processes multimodal context (images + text) → generates tool calls → tools execute CAD operations → compute chamfer distance → calculate PI
**Design Tradeoffs**: The framework trades computational complexity (sampling points on curves) for accuracy in measuring geometric similarity; uses simple symmetric chamfer distance rather than more sophisticated shape matching algorithms
**Failure Signatures**: Models outputting invalid JSON tool calls, negative PI scores indicating performance degradation, or failure to handle complex curve combinations
**First Experiments**:
1. Validate chamfer distance implementation on simple test cases (parallel lines, concentric circles)
2. Test tool execution pipeline with hand-crafted tool calls before VLM integration
3. Evaluate VLM on single-round generation before testing multi-round refinement

## Open Questions the Paper Calls Out
**Open Question 1**: What specific training methodologies beyond supervised fine-tuning are required to close the performance gap between generation and refinement in vision-language models? The authors conclude that "more sophisticated training approaches might be needed to tackle refinement" because supervised fine-tuning gains failed to transfer to refinement turns.

**Open Question 2**: How can the "very-dense" subset of the dataset be utilized to quantify the variance in human multimodal strategies for the same target? The paper states that the dataset contains a "very-dense set... which we leave for future work."

**Open Question 3**: Does a more granular classification of instruction types within rounds improve the accuracy of benchmark evaluations? Section 6.1 notes that the binary grouping of rounds as "generation" vs. "refinement" is "imprecise and can be improved in future works."

## Limitations
- The chamfer distance metric may not fully capture semantic or functional aspects of CAD designs
- Human baseline was established under specific game conditions that may not generalize to real-world design scenarios
- Dataset focuses on simple geometric primitives and may not represent professional CAD workflow complexity

## Confidence
**High Confidence**: Multimodal instructions are more effective than single-modality instructions, supported by dataset analysis showing higher success rates and shorter completion times.
**Medium Confidence**: VLMs perform worse at refinement than generation, though the magnitude difference (0.547 vs -0.119 PI) should be interpreted cautiously given the relatively small dense evaluation set.
**Low Confidence**: Current VLMs are fundamentally limited in refinement capabilities versus generation capabilities, as the performance gap could stem from evaluation methodology rather than inherent architectural limitations.

## Next Checks
1. **Distance Metric Validation**: Implement and validate symmetric chamfer distance independently using provided ground truth distances; test edge cases including overlapping curves and complex geometries.
2. **Human Baseline Replication**: Replicate human performance measurements with different participant pools to establish confidence intervals for human PI scores across CAD complexity levels.
3. **VLM Performance Robustness**: Evaluate VLMs across multiple random seeds and prompting strategies to determine if PI differences are consistent or implementation-sensitive; test additional VLMs beyond those originally evaluated.