---
ver: rpa2
title: 'Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs'
arxiv_id: '2511.18931'
source_url: https://arxiv.org/abs/2511.18931
tags:
- search
- split
- static
- llms
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a benchmark to evaluate how well modern large
  language models (LLMs) decide when to invoke internal web search tools. It uses
  a static split of 783 pre-knowledge-cutoff questions and a dynamic split of 288
  post-cutoff queries to test both the necessity and effectiveness of web search.
---

# Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs
## Quick Facts
- arXiv ID: 2511.18931
- Source URL: https://arxiv.org/abs/2511.18931
- Reference count: 40
- Primary result: Web search significantly boosts accuracy on pre-knowledge-cutoff questions but degrades confidence calibration

## Executive Summary
This study introduces a benchmark to evaluate how well modern large language models decide when to invoke internal web search tools. The research uses both static and dynamic question splits to test search necessity and effectiveness across multiple models. Results demonstrate that while web search substantially improves accuracy on pre-knowledge-cutoff questions, it introduces calibration issues and shows limited benefit on post-cutoff queries due to weak query formulation.

The findings suggest web search functions best as a low-latency verification layer rather than a comprehensive analytical tool. Models generally detect when search is needed but still under-invoke in some cases. The research highlights opportunities for improvement in calibration and query formulation, with selective invocation based on model confidence outperforming static policies despite remaining imperfect.

## Method Summary
The study employs a curated dataset of 1,071 questions split into 783 pre-knowledge-cutoff questions and 288 post-cutoff queries. Researchers evaluate multiple modern LLMs including GPT-5-mini and Claude Haiku 4.5, testing both with and without web search capabilities enabled. The evaluation measures accuracy improvements, cost per accuracy-improving call, confidence calibration, and selective invocation strategies. Static policies are compared against confidence-based selective invocation approaches to determine optimal search invocation patterns.

## Key Results
- Enabling web search improves accuracy on static questions significantly (GPT-5-mini: 52.3% to 84.6%, Claude Haiku 4.5: 41.4% to 74.7%)
- Search degrades confidence calibration while improving accuracy
- On dynamic queries, models remain below 70% accuracy despite calling search
- Selective invocation based on confidence outperforms static policies but under-invokes in some cases
- Cost per accuracy-improving search call is low, but gains diminish after initial retrieval

## Why This Works (Mechanism)
Models leverage web search as an external knowledge verification mechanism, accessing up-to-date information beyond their training cutoff. The selective invocation strategy uses confidence scores to determine when additional verification is likely needed, balancing accuracy gains against computational costs. The calibration degradation occurs because search results introduce uncertainty that models struggle to properly integrate into their probability estimates.

## Foundational Learning
1. **Knowledge Cutoff Management** - Why needed: Models need to distinguish between pre-training knowledge and post-training information. Quick check: Test model accuracy on mixed pre/post-cutoff questions with and without search.
2. **Confidence Calibration** - Why needed: Models must accurately estimate their certainty to make effective search decisions. Quick check: Compare predicted confidence vs actual accuracy across search/no-search conditions.
3. **Query Formulation** - Why needed: Effective search requires translating natural language questions into search-optimized queries. Quick check: Analyze search query quality vs final answer accuracy correlation.
4. **Cost-Benefit Analysis** - Why needed: Search invocation has computational costs that must be justified by accuracy gains. Quick check: Calculate accuracy improvement per search call across different models.
5. **Verification Layer Architecture** - Why needed: Search serves as external validation rather than primary reasoning tool. Quick check: Test whether search primarily corrects errors vs generates new insights.
6. **Selective vs Static Policies** - Why needed: Adaptive search invocation outperforms blanket search strategies. Quick check: Compare accuracy and efficiency of confidence-based vs fixed search policies.

## Architecture Onboarding
**Component Map:** User Question -> Confidence Assessment -> Search Decision Module -> Query Formulation -> Web Search API -> Result Processing -> Answer Generation -> Confidence Recalibration

**Critical Path:** Question Reception → Confidence Scoring → Search Invocation Decision → Query Generation → Search Execution → Result Integration → Final Answer

**Design Tradeoffs:** The system balances computational cost against accuracy improvement, prioritizing low-latency verification over comprehensive analysis. Selective invocation reduces unnecessary searches but risks missing some needed lookups. The architecture favors speed and efficiency over exhaustive information gathering.

**Failure Signatures:** Under-invocation when confidence scores are miscalibrated, over-reliance on search for dynamic queries where formulation is weak, degraded confidence calibration after search integration, diminishing returns after initial retrieval.

**First Experiments:** 1) Test confidence calibration across different question types with/without search. 2) Measure search query quality correlation with final answer accuracy. 3) Compare selective vs static search policies on diverse question sets.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset focuses on fact-based queries with clear answers, limiting generalizability to open-ended or subjective questions
- Evaluation metrics prioritize accuracy over comprehensiveness or timeliness of information
- Findings may not translate to external web-browsing LLMs with different retrieval patterns
- Under-invocation persists despite selective invocation strategies

## Confidence
- High confidence: Web search significantly improves accuracy on pre-knowledge-cutoff questions (supported by consistent empirical gains across models)
- Medium confidence: Selective invocation outperforms static policies but remains imperfect due to calibration issues
- Medium confidence: Models detect when search is needed, though under-invocation indicates inconsistent capability

## Next Checks
1. Test selective invocation strategy on diverse datasets including subjective, analytical, and multi-hop reasoning questions
2. Evaluate whether search query formulation improves with additional fine-tuning or prompting strategies
3. Compare internal search tool performance against external web-browsing capabilities