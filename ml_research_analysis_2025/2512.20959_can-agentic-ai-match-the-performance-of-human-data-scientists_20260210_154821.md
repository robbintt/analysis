---
ver: rpa2
title: Can Agentic AI Match the Performance of Human Data Scientists?
arxiv_id: '2512.20959'
source_url: https://arxiv.org/abs/2512.20959
tags:
- data
- agentic
- science
- performance
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address whether agentic AI can match human data scientists in
  predictive tasks requiring domain knowledge. To isolate this question, we design
  a synthetic property insurance dataset where a key latent variable (RoofHealth)
  is embedded in roof images rather than tabular features.
---

# Can Agentic AI Match the Performance of Human Data Scientists?

## Quick Facts
- arXiv ID: 2512.20959
- Source URL: https://arxiv.org/abs/2512.20959
- Reference count: 28
- A standard agentic AI approach using only tabular features achieves a normalized Gini of 0.3823, significantly underperforming methods that incorporate image-based domain knowledge.

## Executive Summary
This study investigates whether agentic AI can match human data scientists in predictive tasks requiring domain knowledge by designing a synthetic property insurance dataset where a critical latent variable (RoofHealth) is embedded in roof images rather than tabular features. The experiments demonstrate that generic agentic AI pipelines relying solely on tabular data achieve significantly lower performance (normalized Gini of 0.3823) compared to approaches incorporating image-derived domain knowledge (0.7271-0.8310 normalized Gini). The results show that current agentic AI systems fall short when domain-specific insights from multimodal data are required, highlighting the need for AI systems that can autonomously recognize and incorporate such knowledge.

## Method Summary
The researchers created a synthetic property insurance dataset with 2,000 policies where RoofHealth (a three-level ordinal variable indicating roof condition) is intentionally hidden in roof images rather than tabular features. The dataset includes six tabular features (HouseValue, HouseAge, WallType, AreaRisk, CreditScore, PolicyID) and one 1024×1024 roof image per policy. They compare Random Forest models trained on tabular-only features versus those augmented with image-derived features using CLIP embeddings, VLM-extracted labels, and true RoofHealth labels. Performance is measured using normalized Gini coefficient, with the oracle benchmark using exact generative formulas with true RoofHealth values.

## Key Results
- Standard agentic AI approach using only tabular features achieves normalized Gini of 0.3823
- Methods incorporating image-based domain knowledge improve to 0.7271-0.8310 normalized Gini
- Human domain experts (or models with true RoofHealth) achieve performance approaching the oracle benchmark of 0.8379
- Dense CLIP embeddings outperform explicit VLM labeling (0.7719 vs 0.7271 normalized Gini)

## Why This Works (Mechanism)

### Mechanism 1: Latent Variable Encapsulation in Non-Tabular Modalities
When predictive information is intentionally encoded in modalities outside standard tabular features, generic agentic AI pipelines that generate code for tabular-only analysis fail to recover critical signals. The synthetic dataset design places RoofHealth exclusively in overhead roof images, where it directly influences both claim frequency and severity through the data generating process. Tabular features are statistically associated with RoofHealth but do not deterministically reveal it, creating a partial information bottleneck.

### Mechanism 2: Vision-Language Model Domain Extraction
VLMs can partially recover domain-specific latent variables from images when prompted with appropriate domain context, but imperfectly. Using gpt-4o-mini to extract RoofHealth labels from roof images achieves 0.8062 correlation with true labels, improving normalized Gini from 0.3823 to 0.7271—a 90% relative improvement. However, the 0.8062 correlation indicates residual extraction error limits performance.

### Mechanism 3: Dense Visual Embedding Information Capacity
Pre-trained vision encoders (CLIP) capture latent variable information in dense embeddings that predictive models can leverage, sometimes outperforming explicit discrete labeling. Using full CLIP features (768-dimensional embeddings) directly in a Random Forest achieves Gini 0.7719, surpassing the explicit VLM-extracted labels (0.7271). Clustering CLIP embeddings into 3 categories achieves only 0.5042, indicating information loss from discretization.

## Foundational Learning

- **Normalized Gini Coefficient**: Measures rank-ordering quality for heavy-tailed loss distributions. Why needed: This is the primary evaluation metric. Gnorm = 1 indicates perfect ranking; 0 indicates random predictions. Quick check: If a model predicts all losses as equal, what would the normalized Gini be?

- **Compound Frequency-Severity Models**: The synthetic data generation follows actuarial practice—claim count (Negative Binomial) times claim severity (Gamma) equals total loss. Why needed: Understanding this clarifies why RoofHealth affects predictions through two distinct pathways. Quick check: Why might a latent variable affect claim frequency differently than claim severity?

- **Latent Variable Inference from Observable Proxies**: RoofHealth is never directly observed but correlates with HouseAge, AreaRisk, and CreditScore. Why needed: Understanding proxy-variable relationships is essential for diagnosing why tabular-only models achieve partial rather than random performance. Quick check: If CreditScore were perfectly negatively correlated with RoofHealth, would tabular models perform better or worse?

## Architecture Onboarding

- Component map:
  ```
  [Tabular Features] ───────────────┐
                                     ├──> [Random Forest] ──> [Predictions]
  [Images] ─> [Feature Extractor] ───┘
                 │
                 ├── CLIP (dense embeddings)
                 ├── VLM (explicit labels)  
                 └── Human annotation (oracle labels)
  ```

- Critical path:
  1. Identify multimodal data availability—determine if auxiliary modalities might contain latent information relevant to the prediction task
  2. Extract features from non-tabular sources—apply pre-trained encoders or VLMs to create numeric representations
  3. Integrate with tabular pipeline—concatenate embeddings or extracted labels with existing features before model training
  4. Evaluate against domain-informed baseline—if human experts can extract latent variables, use their labels as an upper-bound reference

- Design tradeoffs:
  - Dense embeddings preserve more information but increase dimensionality and require models that handle high-dimensional input
  - Explicit VLM labels are interpretable and low-dimensional but introduce extraction errors
  - Human annotation is most accurate but does not scale

- Failure signatures:
  - Large performance gap between tabular-only and domain-expert models indicates missing latent variables
  - VLM extraction correlation < 0.9 with true labels suggests prompting or model limitations
  - Clustered embeddings underperforming vs. dense embeddings indicates discretization loss

- First 3 experiments:
  1. Reproduce the tabular-only baseline (Random Forest on the 6 policy features) and verify Gini ≈ 0.38
  2. Extract CLIP embeddings for all images, concatenate to tabular features, retrain Random Forest; expect Gini ≈ 0.77
  3. Use a VLM to explicitly label RoofHealth from images, add as categorical feature; expect Gini ≈ 0.73 and extraction correlation ≈ 0.81

## Open Questions the Paper Calls Out

### Open Question 1
Can agentic AI systems be designed to autonomously recognize when domain-specific knowledge from non-tabular modalities is needed and incorporate it appropriately? Current agentic AI systems generate generic algorithms without mechanisms to detect latent variables hidden in images or other modalities. Development of agentic systems that automatically identify multimodal data relevance and extract domain-specific features would resolve this question.

### Open Question 2
Would the performance gap between agentic AI and human data scientists persist with real-world datasets rather than synthetic data? The synthetic dataset allows controlled experiments but may not capture the full complexity of real-world data science problems. Replication using authentic industry datasets with known but unmeasured latent variables would provide evidence.

### Open Question 3
Can multimodal agentic AI systems with native image understanding capabilities close the performance gap observed with tabular-only approaches? The study isolates tabular-only AI pipelines but does not evaluate whether more capable multimodal agents exist or could be built. Evaluation of agentic systems that autonomously process both tabular and image data would address this question.

## Limitations
- The synthetic nature of the dataset may overstate real-world challenges where domain knowledge integration is more incremental
- CLIP model's pre-training distribution may not generalize to all insurance domains
- gpt-4o-mini extraction quality could vary significantly across different roof types or lighting conditions

## Confidence
- **High Confidence**: The fundamental mechanism that tabular-only agentic AI fails to capture multimodal latent variables
- **Medium Confidence**: The specific performance numbers (0.3823, 0.7271, 0.8310, 0.8379) are reproducible within the synthetic framework
- **Medium Confidence**: The superiority of dense CLIP embeddings over explicit VLM labeling (0.7719 vs 0.7271)

## Next Checks
1. Apply the same methodology to a real-world dataset where domain knowledge is embedded in non-tabular modalities to assess whether the performance gap persists outside synthetic conditions
2. Systematically vary roof image quality (lighting, angle, weather) and re-extract RoofHealth labels to quantify how extraction errors propagate to downstream prediction performance
3. Replace CLIP with domain-specific insurance image encoders or contrastive learning models trained on building/roof imagery to evaluate whether the dense embedding advantage persists with specialized visual representations