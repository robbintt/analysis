---
ver: rpa2
title: Revisiting Feedback Models for HyDE
arxiv_id: '2511.19349'
source_url: https://arxiv.org/abs/2511.19349
tags:
- feedback
- hyde
- query
- terms
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper revisits the application of traditional feedback models\
  \ (Rocchio, RM3) to HyDE, a pseudo-relevance feedback method that uses LLM-generated\
  \ hypothetical answer documents to enrich query representations for sparse retrievers\
  \ like BM25. While recent LLM-based methods often use simple string concatenation,\
  \ the authors systematically evaluate whether incorporating established feedback\
  \ models can improve HyDE\u2019s performance."
---

# Revisiting Feedback Models for HyDE

## Quick Facts
- **arXiv ID**: 2511.19349
- **Source URL**: https://arxiv.org/abs/2511.19349
- **Reference count**: 20
- **Primary result**: Traditional feedback models (Rocchio, RM3) substantially improve HyDE effectiveness by 1.4 points (4.2%) over string concatenation, with even larger gains on low-resource tasks.

## Executive Summary
This paper demonstrates that applying traditional pseudo-relevance feedback models (Rocchio, RM3) to HyDE-generated hypothetical documents significantly outperforms recent string concatenation approaches. The authors systematically evaluate 14 retrieval datasets and show that feedback models provide better term weighting and filtering than naive concatenation, improving recall by up to 1.4 points. The key insight is that established IR techniques remain highly effective even when applied to LLM-generated pseudo-relevance feedback, offering a simple way to boost retrieval accuracy without complex modifications.

## Method Summary
The method applies Rocchio and RM3 feedback models to HyDE-generated hypothetical documents. For each query, an LLM generates N=8 hypothetical answer documents (≤512 tokens each). Term selection filters out common corpus terms appearing in >10% of documents and ranks remaining terms by normalized frequency sum across all generated docs, keeping top-k=128. Rocchio computes weighted query vectors using α=1.0 and β=0.75, while RM3 uses λ=0.5. The resulting weighted term vectors are converted to Lucene queries with term boosts for BM25 retrieval. The approach is evaluated on 14 datasets including MS MARCO DL19-23 and BEIR tasks using Qwen2.5-7B-Instruct, Qwen3-14B, and gpt-oss-20b LLMs.

## Key Results
- Rocchio improves recall@20 by 1.4 points (4.2%) over the best string concatenation baseline
- Term selection filtering removes noisy terms and provides substantial gains over naive concatenation
- Feedback models show stronger robustness on low-resource BEIR tasks compared to adaptive string concatenation methods
- Improvements stem from more stable, linear weighting of query terms versus implicit weighting in concatenation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Term selection filters noisy tokens from LLM-generated expansions before query update.
- Mechanism: After HyDE generates hypothetical documents, a preprocessing step removes terms appearing in >10% of corpus documents and ranks remaining terms by normalized frequency sum across all generated docs, keeping only top-k. This removes common/irrelevant terms while preserving discriminative expansion content.
- Core assumption: LLM generations contain both useful expansion terms and noisy/generic language that degrades retrieval when included wholesale.
- Evidence anchors:
  - [section 2.1] "filters out common corpus terms from each feedback vector, keeping only the terms which occur in less than 10% of the corpus documents"
  - [page 3, bullet 2] "Comparing HyDE (w/ Avg Vector) to HyDE (w/ Naive Concat)... the average vector approach is more effective with a 3, 2.3, and 3.1 point improvement... main difference lies in the term selection step"
  - [corpus] "Pseudo Relevance Feedback is Enough to Close the Gap" confirms PRF vulnerability to "topic drift when early results include noisy or tangential content"

### Mechanism 2
- Claim: Rocchio's explicit α/β weighting corrects HyDE's under-emphasis of original query terms.
- Mechanism: Rocchio computes w_t,qnew = α·f(q)[t] + β·(1/N)Σf(d_i)[t], where α=1.0 and β=0.75. This gives original query terms higher weight than the simple averaging in HyDE's naive approach, preventing expansion terms from overwhelming the core information need.
- Core assumption: String concatenation implicitly under-weights original query relative to lengthy generated text, causing topic drift.
- Evidence anchors:
  - [page 3, bullet 3] "Rocchio algorithm gives more weight to the query terms. This suggests that the HyDE feedback under emphasizes query terms"
  - [page 4, conclusion] "improvements come from... a more stable and linear weighting of query terms and terms in feedback documents via simple parameters like α and λ rather than via a string repeat"
  - [corpus] Insufficient direct evidence in corpus for α/β parameter analysis

### Mechanism 3
- Claim: Traditional feedback models generalize better to diverse query types than concatenation heuristics.
- Mechanism: Fixed parameterization (α, β, λ) provides consistent behavior across query variations, whereas adaptive methods like MuGI's query repeat depend on document length ratios (γ), introducing instability when generated doc lengths vary unpredictably.
- Core assumption: Query-document length ratios are noisy signals for determining appropriate query emphasis.
- Evidence anchors:
  - [page 3, bullet 4] "String concatenation methods are competitive on MS MARCO, but are less effective on low-resource BEIR tasks... RM3 beats Query2Doc on all 10 datasets, and MuGI Concat on 8 of the 10"
  - [page 4, conclusion] "average vector, RM3, and Rocchio feedback models demonstrated stronger robustness on the diverse range of queries present in low-resource BEIR tasks"
  - [corpus] "Generalized Pseudo-Relevance Feedback" supports that traditional PRF addresses vocabulary mismatch systematically

## Foundational Learning

- Concept: Pseudo-relevance feedback (PRF)
  - Why needed here: HyDE is fundamentally a PRF variant—understanding that PRF extracts expansion terms from assumed-relevant documents (whether retrieved or generated) is prerequisite to understanding why term selection and weighting matter.
  - Quick check question: What two phases does traditional PRF perform after obtaining candidate feedback documents?

- Concept: Rocchio algorithm
  - Why needed here: The paper's core intervention applies Rocchio to LLM-generated text; without understanding the α/β parameterization, you cannot reason about why it outperforms concatenation.
  - Quick check question: In Rocchio's formula w = α·q + β·(1/N)Σd_i, what does increasing β relative to α accomplish?

- Concept: BM25 query representation
  - Why needed here: The paper modifies how queries are represented as weighted term vectors for BM25; understanding that BM25 computes query term weights from term frequencies (not just presence/absence) explains why term weighting matters.
  - Quick check question: Why does string concatenation (q + generated_text) differ from explicit term weighting in BM25's scoring?

## Architecture Onboarding

- Component map: HyDE Generator -> Term Selection Module -> Feedback Model -> Query Builder -> BM25 Retriever

- Critical path:
  1. Prompt LLM with original query → generate hypothetical docs
  2. Extract term-frequency vectors from generated docs
  3. Apply corpus-frequency filtering via IndexReader API
  4. Compute Rocchio-weighted query vector
  5. Build Lucene BooleanQuery with term boosts
  6. Execute BM25 retrieval

- Design tradeoffs:
  - k (expansion terms): Larger k = more recall potential but more noise; paper uses 128 to match Query2Doc budget
  - N (generated docs): More samples = better term coverage but higher latency/cost; paper uses 8
  - α vs β: Higher β emphasizes expansion (better for vague queries), higher α prioritizes original query (better for specific queries)

- Failure signatures:
  - Recall drops vs naive concatenation → check term selection is not over-aggressive (increase k or relax 10% threshold)
  - Large variance across BEIR datasets → query length distribution may require adaptive α/β
  - Slow retrieval → N=8 generations may be bottleneck; profile LLM inference vs retrieval time

- First 3 experiments:
  1. Ablate term selection: Compare Rocchio with/without corpus-frequency filtering on 2-3 BEIR datasets to quantify noise-filtering contribution.
  2. Parameter sweep α/β: Grid search α∈[0.5,1.5], β∈[0.5,1.0] on DL19/20 to identify optimal balance for web queries vs BEIR's diverse tasks.
  3. Robustness check: Compare Rocchio-HyDE vs MuGI Concat on a single high-variance BEIR subset (e.g., FiQA + ArguAna) to confirm stability claims with statistical testing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive parameter reweighting schemes (similar to MuGI's adaptive query repeat) improve Rocchio's effectiveness over fixed α and β values?
- Basis in paper: [explicit] The authors state: "An interesting area for future work would be to investigate how to adaptively reweight the α and β parameter for Rocchio using a scheme like that of MuGI."
- Why unresolved: The paper uses fixed hyperparameters (α=1, β=0.75) from literature without exploring adaptive tuning, while MuGI's adaptive approach showed competitive results with string concatenation.
- What evidence would resolve it: Experiments comparing fixed vs. adaptive α/β parameter schemes across the same 14 datasets, potentially using feedback document length or query characteristics to dynamically adjust weights.

### Open Question 2
- Question: Why does RM3 underperform relative to Rocchio when applied to HyDE-generated documents, but not when applied to BM25-retrieved documents?
- Basis in paper: [inferred] The authors observe this discrepancy in results but do not investigate the cause: "while RM3 is less effective than Rocchio when applied to HyDE documents, this is not the case when applied to top-ranked BM25 documents."
- Why unresolved: The paper reports the phenomenon but offers no analysis of whether this stems from differences in term distribution, document structure, or probability estimation between hypothetical and retrieved documents.
- What evidence would resolve it: Analysis comparing term distributions, relevance model probability estimates, and expansion term quality between HyDE-generated and BM25-retrieved documents when processed by RM3 vs. Rocchio.

### Open Question 3
- Question: Do traditional feedback models provide similar improvements when applied to dense retrieval methods rather than BM25?
- Basis in paper: [inferred] The paper exclusively evaluates sparse retrieval with BM25, noting that HyDE was "originally designed for dense vectors" but their reformulation adapts it to "bag-of-words vector space."
- Why unresolved: While HyDE's original formulation targeted dense retrieval, this study only demonstrates feedback model benefits for sparse retrieval, leaving open whether similar gains apply to dense embeddings.
- What evidence would resolve it: Experiments applying Rocchio-style weighting mechanisms to dense vector representations of HyDE documents and queries, evaluated using dense retrievers like Contriever or ANCE.

## Limitations
- Exact HyDE prompt template is unspecified, creating potential variability in generated document quality across implementations
- The paper provides minimal detail on how RM3 probabilities are computed for LLM-generated text (only references Anserini)
- Performance gains are measured on synthetic "hypothetical" relevance rather than actual relevance judgments from LLM generations

## Confidence
- **High Confidence**: The observation that Rocchio improves over concatenation is well-supported by controlled ablation experiments and clear α/β parameter effects
- **Medium Confidence**: The claim that term selection removes "noisy" tokens is plausible but not directly validated—the paper shows improved performance but doesn't empirically demonstrate noise reduction
- **Low Confidence**: The robustness claim across diverse BEIR tasks is based on aggregate results without per-dataset statistical significance testing or analysis of failure cases

## Next Checks
1. Replicate the term selection filtering step on a held-out corpus to measure actual reduction in term entropy and confirm it removes generic vs. discriminative language
2. Conduct per-dataset significance testing (paired t-tests) between Rocchio-HyDE and concatenation baselines to verify robustness claims aren't driven by a few outlier datasets
3. Perform a controlled ablation: generate hypothetical documents with and without term selection, then apply Rocchio to both to isolate the filtering contribution from the weighting contribution