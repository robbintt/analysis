---
ver: rpa2
title: Human-AI collaboration or obedient and often clueless AI in instruct, serve,
  repeat dynamics?
arxiv_id: '2508.10919'
source_url: https://arxiv.org/abs/2508.10919
tags:
- interactions
- students
- prompts
- network
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how students interact with AI when solving
  complex technical problems. It uses qualitative coding of student-AI conversations
  combined with transition network analysis, sequence analysis, partial correlation
  networks, chi-square tests, and mosaic plots.
---

# Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?

## Quick Facts
- arXiv ID: 2508.10919
- Source URL: https://arxiv.org/abs/2508.10919
- Reference count: 40
- Primary result: Student-AI interactions are dominated by iterative instruction-following loops rather than collaborative negotiation, with no correlation between task complexity and performance.

## Executive Summary
This study investigates how students interact with AI when solving complex technical problems, revealing that interactions are dominated by iterative instruction-following rather than genuine collaboration. Using qualitative coding of student-AI conversations combined with transition network analysis, sequence analysis, and statistical comparisons, the research finds that long conversation threads indicate misalignment and frustration rather than deeper engagement. The study concludes that current LLMs, optimized for instruction-following rather than cognitive partnership, limit their capability as cognitively stimulating collaborators and raise serious challenges for educational assessment.

## Method Summary
The study analyzed 122 conversation logs from 49 students solving network science problems, with prompts manually coded into seven categories (Instruct, Context, Specify, Disagree, Agree, Request, Conclude). Sequence analysis using TraMineR generated index plots, while Transition Network Analysis (TNA) with the tna package modeled interaction patterns using Markov assumptions. Psychological networks were estimated using bootnet for regularized partial correlations. Permutation tests (n=1000) compared groups (high/low performers, early/late assignments, short/long sequences), and chi-square tests with Pearson-residual mosaic plots identified significant differences in transition patterns.

## Key Results
- Interactions characterized by iterative ordering rather than collaborative negotiation, with long threads revealing misalignment between prompts and AI output
- No significant correlation found between assignment complexity, prompt length, and student grades, indicating lack of cognitive depth or problem difficulty effects
- Long conversation threads signal misalignment and frustration rather than deeper engagement, with permutation tests showing more Disagree transitions and fewer Conclude transitions in extended sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-following loops dominate human-AI interaction patterns on complex tasks, creating iterative refinement cycles rather than collaborative negotiation.
- Mechanism: Students issue instructions → AI generates compliant output → students identify misalignment → students issue refined instructions. This creates a "guess-and-correct" dynamic where both parties lack shared mental models. The TNA showed Instruct as the primary central event (84.43% initial probability), with transitions from Disagree feeding back into Specify (52.5%) and Instruct (33.9%), indicating iterative correction rather than dialogue.
- Core assumption: LLMs are optimized for user satisfaction and compliance, not cognitive partnership; system prompts prioritize obedience over argumentation.
- Evidence anchors:
  - [abstract] "interactions characterized by iterative ordering rather than collaborative negotiation... long threads that showed misalignment"
  - [section 4.2] "The overarching dynamics paints a picture of trial, re-trial and gauging or guessing in an iterative process of refinement"
  - [corpus] Weak direct evidence; neighbor papers focus on collaboration frameworks but not the specific instruction-following loop mechanism.
- Break condition: If LLMs were retrained with system prompts prioritizing cognitive engagement, proactively challenging user assumptions, or maintaining persistent task models across turns.

### Mechanism 2
- Claim: Task complexity does not correlate with interaction depth or performance when using instruction-tuned LLMs.
- Mechanism: LLMs pattern-match against training data rather than reason about problem structure. This flattens difficulty—simple and complex problems both reduce to prompt-response cycles. The study found no significant correlation between assignment complexity and grades (r = .081, p = .40), nor between prompt length and grades (r = .036, p = .70).
- Core assumption: LLM responses are stochastic and unpredictable, making task outcomes weakly coupled to problem difficulty or user skill.
- Evidence anchors:
  - [abstract] "no significant correlations between assignment complexity, prompt length, and student grades"
  - [section 4.3] "students' conversations were neither particularly longer in complex problems, nor was it longer or shorter in high achieving students"
  - [corpus] Neighbor paper "Read the Room or Lead the Room" discusses socio-cognitive dynamics in human-AI teaming but doesn't address complexity flattening.
- Break condition: If tasks required multi-step reasoning that LLMs cannot pattern-match, or if evaluation measured process quality rather than output correctness.

### Mechanism 3
- Claim: Long conversation threads signal misalignment and frustration rather than deeper engagement.
- Mechanism: Extended sequences indicate students cannot communicate intent effectively, and LLMs cannot infer missing context. The permutation test comparing long vs. short sequences showed more Disagree transitions and fewer Conclude transitions in long threads—a "collective picture of frustration."
- Core assumption: Successful human-AI collaboration should converge efficiently; extended iterations indicate communication breakdown.
- Evidence anchors:
  - [section 4.4] "results show more transitions towards Disagree, less to Conclude and Context which is a collective picture of frustration"
  - [section 5] "a process that extends over long sequences where they struggle to find a solution... the underlying disconnect—the lack of a shared framework"
  - [corpus] "MedSyn" paper mentions limited-interaction usage overlooks complexities, but doesn't examine thread length as a signal.
- Break condition: If long threads contained genuine exploratory dialogue with AI asking clarifying questions rather than passive compliance.

## Foundational Learning

- Concept: **Markov modeling for interaction sequences**
  - Why needed here: TNA uses Markov assumptions (current state depends mainly on previous state) to model stochastic human-AI interactions; understanding this helps interpret transition probabilities and their limitations.
  - Quick check question: Can you explain why Markov modeling fits LLM interactions given their limited context windows?

- Concept: **Gulf of envisioning / Gulf of execution**
  - Why needed here: The paper references Subramonyam et al.'s concept of a "blind spot" between human abstract intentions and LLM pattern-matching; this frames why instruction-following fails as collaboration.
  - Quick check question: What specific step in the human→prompt→LLM chain creates the widest gap?

- Concept: **Sequence analysis vs. frequency analysis**
  - Why needed here: The paper critiques "traditional counting methods" and demonstrates that sequence order reveals patterns (Instruct→Context→Specify) that frequencies alone miss.
  - Quick check question: Why would counting "Instruct" occurrences fail to capture the "instruct, serve, repeat" dynamic?

## Architecture Onboarding

- Component map:
  - **Prompt coding layer**: Classify user inputs (Instruct, Context, Specify, Disagree, Agree, Request, Conclude)
  - **Transition model**: Build Markov-based TNA to capture state transitions and probabilities
  - **Centrality analysis**: Identify key events (in-strength, betweenness) and critical edges
  - **Comparison framework**: Permutation tests between groups (high/low performers, early/late assignments, short/long sequences)
  - **Psychological network**: Partial correlation networks relating interaction frequencies to outcomes

- Critical path: Code prompts → Build sequences → Estimate TNA → Compute centralities → Run permutation tests → Interpret transition differences

- Design tradeoffs:
  - Markov assumption (memoryless) simplifies modeling but may miss long-range dependencies
  - Coding scheme granularity: more codes capture nuance but reduce statistical power
  - Aggregating across students loses individual variation; separate networks per student need more data

- Failure signatures:
  - Long sequences with high Disagree→Instruct loops = misalignment
  - Low Conclude transitions = tasks not completing
  - No correlation between complexity and effort = task flattening

- First 3 experiments:
  1. Replicate the TNA analysis on a different domain (e.g., coding tasks) to test if "instruct, serve, repeat" generalizes beyond network science.
  2. Introduce a "proactive AI" condition where the LLM asks clarifying questions before generating output; measure if thread length decreases and satisfaction increases.
  3. Correlate interaction patterns with learning gains (pre/post test) rather than just assignment grades to test if instruction-following loops inhibit deep learning.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can AI systems be redesigned to prioritize cognitive alignment and act as proactive partners rather than obedient instruction-followers?
  - Basis in paper: [explicit] The authors conclude that current LLMs are "optimized for instruction-following rather than cognitive partnership" and call for "designing AI systems that prioritize cognitive alignment and collaboration."
  - Why unresolved: This study only analyzed existing interactions with standard models; it did not test interventions or new system designs aimed at fostering collaboration.
  - What evidence would resolve it: Experimental studies comparing student interactions with standard LLMs versus "proactive" models designed to challenge users or scaffold negotiation.

- **Open Question 2**: Does the "instruct, serve, repeat" interaction pattern negatively impact long-term learning retention and transfer compared to unassisted problem solving?
  - Basis in paper: [inferred] The authors found that interactions reduced to iterative refinement without correlation to grades or complexity, warning of "metacognitive laziness" and "surface-level understanding."
  - Why unresolved: The study measured immediate task performance (grades) but did not assess delayed retention or the ability to solve similar problems independently later.
  - What evidence would resolve it: Longitudinal data tracking students' ability to perform network science tasks without AI assistance weeks after the initial interaction.

- **Open Question 3**: How can educational assessments be restructured to evaluate the process of human-AI interaction rather than just the final artifact?
  - Basis in paper: [inferred] The authors note the lack of correlation between complexity and grades suggests "LLMs may be leveling or removing barriers," which "raises a serious challenge for assessment."
  - Why unresolved: The current study evaluated students based on the suitability of the generated dataset (the artifact), failing to capture the quality of the collaborative process itself.
  - What evidence would resolve it: Validation of assessment rubrics that specifically score the quality of prompt engineering, critical evaluation of AI output, and iterative refinement strategies.

## Limitations

- The study relies on qualitative coding of student-AI interactions, which introduces potential subjectivity despite reported Kappa > 0.60
- The dataset is relatively small (122 conversations, 289 prompts), limiting generalizability
- The Markov assumption in TNA may oversimplify complex interaction dynamics

## Confidence

- **High confidence**: The finding that interactions are dominated by iterative instruction rather than collaboration is well-supported by the TNA showing Instruct as the central node with high transition probabilities and the qualitative evidence of misalignment in long threads.
- **Medium confidence**: The claim about lack of correlation between complexity and performance is statistically supported but may be influenced by the small sample size and specific task characteristics.
- **Medium confidence**: The interpretation that long threads indicate frustration and misalignment is supported by permutation tests but could have alternative explanations (e.g., students exploring multiple approaches).

## Next Checks

1. Replicate the TNA analysis with a larger, more diverse dataset across multiple domains to test if "instruct, serve, repeat" dynamics generalize beyond network science.
2. Conduct think-aloud protocols during student-AI interactions to capture cognitive processes and validate whether long threads truly indicate frustration versus deep exploration.
3. Implement an intervention where AI is configured to ask clarifying questions before responding, then measure changes in thread length, student satisfaction, and learning outcomes compared to standard instruction-tuned models.