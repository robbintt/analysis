---
ver: rpa2
title: 'LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models'
arxiv_id: '2502.15612'
source_url: https://arxiv.org/abs/2502.15612
tags:
- latim
- fdcvcu
- mamba
- attention
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LaTIM, a novel token-level interpretability
  method for Mamba models that addresses the lack of interpretability tools in state
  space models. LaTIM reformulates Mamba's computation to enable fine-grained analysis
  of token-to-token interactions, adapting attention-based techniques like ALTI to
  the Mamba architecture.
---

# LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models

## Quick Facts
- arXiv ID: 2502.15612
- Source URL: https://arxiv.org/abs/2502.15612
- Reference count: 40
- Introduces LaTIM, a novel interpretability method for Mamba models achieving strong performance on copying (AUC 0.98 for Mamba-2), machine translation (AER 0.46), and retrieval tasks

## Executive Summary
LaTIM addresses the interpretability gap in Mamba models by reformulating their state space recurrence into an implicit attention matrix, enabling fine-grained token-level analysis. The method adapts attention-based techniques like ALTI to the Mamba architecture, providing clearer visualizations of token interactions than existing approaches. Evaluated across copying, translation, and retrieval tasks, LaTIM reveals Mamba's selective processing mechanisms while maintaining reasonable computational overhead.

## Method Summary
LaTIM decomposes Mamba's state space recurrence by unrolling the SSM into a lower-triangular interaction matrix that captures how past tokens contribute to current states. The method handles the non-linear SiLU activation through additive decomposition, using SiLU itself as the approximation function for minimal error. For Mamba-2, GroupNorm layers are linearized at test time. Token contributions are aggregated using â„“2 norm, ALTI, or ALTI-Logit to produce interpretable heatmaps. The approach is implemented in the mamba-ssm framework with code available at https://github.com/deep-spin/latim.

## Key Results
- Copying task: Mamba-1 achieves AUC 0.88/AP 0.41; Mamba-2 achieves AUC 0.98/AP 0.86
- Machine translation: Mamba-1 achieves AER 0.46; Mamba-2 achieves AER 0.49
- Heatmaps show clearer token interaction patterns compared to Mamba-Attention and MambaLRP baselines
- Exact strategy (removing SiLU) achieves zero decomposition error while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1: Linear Recurrent Unrolling into Implicit Attention Matrices
LaTIM reformulates Mamba's non-linear recurrence into linear matrix multiplication $\mathbf{Y} = \mathbf{M}\mathbf{X}$, revealing a "hidden attention" matrix $\mathbf{M}$ that defines token interactions. By unrolling the state space recurrence $H_i = A_i H_{i-1} + B_i X_i$, the authors derive a lower-triangular matrix $\mathbf{M}_{i,j}$ that aggregates input-dependent parameters to show how past tokens $x_j$ contribute to the current state $\upsilon_i$.

### Mechanism 2: Additive Decomposition of the SiLU Non-linearity
The method assumes an additive function $f$ to decompose the SiLU activation output, expressing the output $v_i$ as a sum of token contributions. Surprisingly, using SiLU itself as $f$ yields the lowest decomposition error, allowing token-wise contribution analysis despite the non-linearity.

### Mechanism 3: GroupNorm Linearization for Mamba-2
For Mamba-2, LaTIM treats GroupNorm as a test-time affine transformation with fixed normalization statistics, allowing token contributions to pass through the normalization layer linearly while preserving decomposability.

## Foundational Learning

- **State Space Models & Discretization**: Understanding how continuous SSMs become discrete recurrences is essential since LaTIM relies on the discrete recurrence formulation ($H_t = A H_{t-1} + B x_t$) to unroll history. Quick check: How does the state $H_t$ depend on the previous state $H_{t-1}$ and current input $x_t$ in a discrete SSM?

- **Token Attribution / Decomposition**: The core goal is expressing output $y_i$ as a sum of contributions from input tokens $y_i = \sum T_i(x_j)$. You must understand linear superposition to grasp why the SiLU non-linearity is problematic. Quick check: If $y = f(x_1 + x_2)$, why can't we generally say $y = f(x_1) + f(x_2)$ if $f$ is non-linear?

- **Hadamard (Element-wise) Product**: Mamba uses element-wise products ($\odot$) for gating and state updates ($A_i \odot H_{i-1}$). Understanding how this differs from matrix multiplication is crucial for following the derivation of the interaction matrix $M$. Quick check: Does the Hadamard product $A \odot B$ mix channels/dimensions together like matrix multiplication $A \times B$?

## Architecture Onboarding

- **Component map**: Input Layer -> Projection -> Conv1D -> Activation (SiLU) -> SSM Core -> Gating -> Output
- **Critical path**: The SSM unrolling (Section 3.1, Eq 18-20) is essential. If the implementation cannot correctly calculate the contribution vector $\upsilon_{i \leftarrow j}$ by iterating the recurrence history, the heatmaps will be random noise.
- **Design tradeoffs**: The default "Approximated Strategy" uses $f=$SiLU (easy, no retrain) but has non-zero decomposition error, while the "Exact Strategy" removes activation (requires retrain) but guarantees 0 error.
- **Failure signatures**: Spurious diagonals in heatmaps indicate incorrect contribution aggregation logic; high decomposition error suggests problems with the approximation function $f$ implementation.
- **First 3 experiments**:
  1. Run LaTIM on a trained Mamba model performing a simple copy task and verify clear diagonal patterns in the heatmap.
  2. Measure decomposition error across layers using the default SiLU approximation to establish faithfulness baseline.
  3. Implement the "Exact Strategy" (remove SiLU, retrain) and compare AUC/AP metrics against the approximated version.

## Open Questions the Paper Calls Out

### Open Question 1
Can LaTIM's decomposition framework be effectively extended to other state space models (e.g., DeltaNet, mLSTM) and hybrid attention-SSM architectures without substantial modification? The authors note that architectures with more complex gating mechanisms or hybrid attention-SSM layers might require adapted decomposition techniques, though this remains untested.

### Open Question 2
What are the root causes of Mamba's significant performance degradation in multi-key retrieval settings? The paper reveals that model accuracy declines substantially (38-101%) when increasing the number of keys, with attention decay over time, but the underlying mechanism remains unclear.

### Open Question 3
Does the availability of token-level interpretability through LaTIM translate to improved model robustness, trustworthiness, or downstream debugging in practical applications? While LaTIM helps visualize token interactions, its impact on improving model robustness and trustworthiness remains an open question.

## Limitations
- Decomposition error from SiLU approximation, though empirically small, remains a theoretical concern for faithful attribution
- Method's dependence on specific architectural details may limit generalization to other SSM variants
- Evaluation focuses on three specific tasks without examining failure modes or adversarial scenarios

## Confidence
- **High Confidence**: Mathematical derivation of hidden attention matrix through SSM recurrence unrolling; claim that LaTIM produces clearer heatmaps than baselines on copying tasks
- **Medium Confidence**: Assertion that SiLU serves as effective approximation function; extension to Mamba-2 via GroupNorm linearization
- **Low Confidence**: Broader claim about revealing "selective processing mechanisms" lacks systematic semantic analysis

## Next Checks
1. Conduct ablation study comparing LaTIM's decomposition error and interpretability quality across different approximation functions (SiLU, Taylor expansion, ReLU) beyond what's shown in Section 4.4.
2. Apply LaTIM to alternative SSM architectures like S5 or Griffin-Lim to test whether the hidden attention matrix interpretation generalizes or is specific to Mamba's recurrence structure.
3. Generate synthetic inputs with obvious token contributions (e.g., single distinguishing token) to verify LaTIM consistently identifies correct contributors across layers and random seeds.