---
ver: rpa2
title: A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling
  with Bandwidth Extension
arxiv_id: '2506.22321'
source_url: https://arxiv.org/abs/2506.22321
tags:
- subaru
- audio
- sampling
- hearables
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SUBARU, a method to reduce power consumption
  in hearables by using sub-Nyquist sampling and low bit resolution in analog-to-digital
  converters (ADCs), while maintaining audio quality through joint bandwidth extension
  (BWE) and multimodal speech enhancement (SE) on mobile platforms. SUBARU introduces
  a U-Net-based architecture with multi-scale and multi-period virtual discriminators
  to achieve GAN-like perceptual quality without actual GANs, and employs joint training
  in both spectrum and waveform domains to improve phase reconstruction under noisy
  conditions.
---

# A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension

## Quick Facts
- **arXiv ID**: 2506.22321
- **Source URL**: https://arxiv.org/abs/2506.22321
- **Reference count**: 40
- **One-line primary result**: SUBARU achieves 3.31x power reduction in hearables through sub-Nyquist sampling while maintaining high audio quality via joint BWE and SE on mobile platforms.

## Executive Summary
This paper presents SUBARU, a method to reduce power consumption in hearables by using sub-Nyquist sampling and low bit resolution in analog-to-digital converters (ADCs), while maintaining audio quality through joint bandwidth extension (BWE) and multimodal speech enhancement (SE) on mobile platforms. SUBARU introduces a U-Net-based architecture with multi-scale and multi-period virtual discriminators to achieve GAN-like perceptual quality without actual GANs, and employs joint training in both spectrum and waveform domains to improve phase reconstruction under noisy conditions. Evaluations show SUBARU achieves a 3.31x reduction in power consumption, 1.74 ms inference time on GPU, and outperforms existing models in speech quality metrics while enabling real-time streaming on mobile platforms.

## Method Summary
SUBARU employs a cascaded U-Net architecture with four networks: Spectral Enhancement (2D conv + Mamba bottleneck), Upsampling (256× via transposed conv), Time Enhancement (1D U-Net fusing ACM+BCM), and Amplitude-Phase Enhancement (coupled conv streams). The model uses multi-scale and multi-period virtual discriminators as loss functions instead of GANs, and is trained end-to-end with losses in both spectrum and waveform domains. Training uses synthetic BCM data generated via SEANet, with progressive reduction of synthetic data ratio over 50 epochs.

## Key Results
- Achieves 3.31x power consumption reduction compared to conventional sampling
- 1.74 ms inference time on GPU and 70.41 ms on Google Pixel 7
- Outperforms existing models in speech quality metrics (PESQ, STOI, SI-SDR, LSD)
- Enables real-time streaming with <150 ms latency constraint

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sub-Nyquist sampling at lower bit resolutions significantly reduces ADC power consumption in hearables.
- Mechanism: By intentionally sampling signals from both air conduction microphones (ACMs) and bone conduction microphones (BCMs) at sub-Nyquist rates (e.g., 4 kHz) and lower bit resolutions (e.g., 8-bit), the ADC's dynamic power consumption is reduced, following the relationship $P \propto f_s \cdot 2^N$. The reconstruction of the high-fidelity signal is offloaded to a connected mobile platform with a larger power budget.
- Core assumption: The hearable's power budget is the primary constraint, and the connected mobile platform (e.g., a smartphone) has substantially more available power and computational resources to handle the complex reconstruction.
- Evidence anchors:
  - [abstract] "achieving a 3.31x reduction in power consumption"
  - [section] "We get a relative power savings of 2.45x between {16 kHz, 12-bits} vs {4 kHz, 8 bits} computations." (Section 3.1)
  - [corpus] Weak/missing. Related corpus papers focus on other topics.
- Break condition: This mechanism fails if the computational and power cost of the reconstruction algorithm on the mobile platform outweighs the savings on the hearable, or if the end-to-end latency exceeds real-time requirements.

### Mechanism 2
- Claim: Multi-scale and multi-period loss functions can simulate the perceptual quality of GAN-based models without their computational overhead and instability.
- Mechanism: Instead of training a separate discriminator network (as in a GAN), the system uses fixed loss functions that operate at different temporal scales (1x, 2x, 4x downsampled) and periods (p=5, 7 samples). These "virtual discriminators" enforce realistic structures by minimizing the Mean Absolute Error between generated and ground-truth audio at these scales and periods.
- Core assumption: The perceptual qualities captured by multi-scale and multi-period discriminators can be sufficiently approximated by a deterministic loss function, without needing a learned adversarial game.
- Evidence anchors:
  - [abstract] "introduces novel multi-scale and multi-period virtual discriminators, which achieve GAN-like audio quality without using GANs' adversarial training"
  - [section] "The multi-scale and multi-period loss functions in SUBARU behave like virtual discriminators in SUBARU without having actual GAN-like discriminator networks." (Section 4.5)
  - [corpus] Weak/missing. No corpus papers discuss this specific technique.
- Break condition: The mechanism fails if the fixed loss functions cannot capture the full complexity of realistic audio, leading to perceptually inferior output or over-regularization artifacts.

### Mechanism 3
- Claim: A cascaded hybrid architecture combining spectrum-domain and waveform-domain processing enables robust joint Bandwidth Extension (BWE) and Speech Enhancement (SE) under noisy conditions.
- Mechanism: A Spectrum Enhancement Network first processes the noisy spectrogram. This is upsampled to a waveform, which is then concatenated with the BCM signal for time-domain fusion in a Time Enhancement Network. A final Amplitude-Phase Enhancement Network cleans both amplitude and phase using coupled convolutional streams.
- Core assumption: The complex task of joint BWE and SE benefits from specialized sub-networks, and the BCM provides distinct, less noisy information. It also assumes dedicated phase reconstruction is critical.
- Evidence anchors:
  - [abstract] "employs joint training in both spectrum and waveform domains to improve phase reconstruction under noisy conditions"
  - [section] "SUBARU adopts a hybrid architecture by merging both waveform-based and spectrum-based methods, enabling joint training in both spectrum and waveform domains." (Section 1, Page 3)
  - [corpus] Weak/missing. The corpus does not provide evidence for this specific cascade.
- Break condition: The mechanism breaks if errors from one stage propagate and amplify, if the BCM signal is also heavily corrupted, or if real-time inference constraints cannot be met.

## Foundational Learning

- Concept: **Sub-Nyquist Sampling and Compressed Sensing**
  - Why needed here: The system's power savings depend on sampling below the Nyquist rate. Understanding the theoretical limits (aliasing, information loss) is crucial.
  - Quick check question: Given a 10 kHz bandwidth signal, what is the Nyquist rate? If you sample at 5 kHz, what will happen to the high-frequency components (>2.5 kHz) according to the Nyquist-Shannon theorem?

- Concept: **Bandwidth Extension (BWE) / Audio Super-Resolution**
  - Why needed here: The mobile-side algorithm's core function is BWE—reconstructing missing high-frequency components. Understanding this as a generative task is essential.
  - Quick check question: If a BWE model is trained only on male voices with an average pitch of 120 Hz, how might it perform on a female voice with a pitch of 250 Hz? Why?

- Concept: **Phase Reconstruction and its Importance**
  - Why needed here: The paper emphasizes that phase is critical for perceptual quality and is difficult to recover in noise. The architecture includes a dedicated network for this.
  - Quick check question: In an audio signal $A \cdot e^{j\phi}$, which component (A or $\phi$) is harder to estimate from a noisy or magnitude-only spectrogram, and why is this a problem for standard models?

## Architecture Onboarding

- Component map:
  1.  **Hearable (Edge):** ACM and BCM sensors. ADC samples both at sub-Nyquist (e.g., 4 kHz) and low bit (e.g., 8-bit). Transmits via Bluetooth.
  2.  **Mobile Platform (e.g., Pixel 7):** Runs the main model.
      a.  **Input:** Low-res noisy ACM & BCM waveforms.
      b.  **Spectral Enhancement Network (U-Net):** Converts ACM waveform to spectrogram, enhances, outputs enriched spectrogram.
      c.  **Upsampling Network:** Converts enriched spectrogram to a higher-resolution waveform (key BWE step).
      d.  **Time Enhancement Network (1D U-Net):** Concatenates upsampled ACM with raw BCM waveform. Fuses features to further denoise/correct in time domain.
      e.  **Amplitude-Phase Enhancement Network:** Converts fused waveform to STFT, uses coupled 1D conv streams to predict enhanced amplitude and phase. Outputs final high-res, noise-free waveform.

- Critical path:
  The power savings are realized at the hearable ADC. Perceptual quality is rescued on the mobile platform. The most critical performance path is the **end-to-end latency** from capture to playback. The model inference (~70 ms) plus transmission (~12 ms) must remain under the 150 ms threshold.

- Design tradeoffs:
  - **Power vs. Quality:** Aggressive sub-Nyquist sampling increases power savings but makes reconstruction harder, potentially degrading quality.
  - **Model Complexity vs. Real-time Performance:** A larger model might yield better quality but could exceed the latency budget on mobile hardware.
  - **BWE vs. SE Focus:** The model is trained jointly; optimizing for one task might degrade the other.

- Failure signatures:
  - **Metallic/Robotic Audio:** Indicates poor phase reconstruction or upsampling artifacts.
  - **Muffled Sound:** BWE component failing to regenerate high-frequency formants.
  - **Remaining Noise Floor:** Enhancement networks not adequately suppressing noise, or BCM introducing noise.
  - **High Latency:** Inference time exceeds audio frame duration, causing dropouts.

- First 3 experiments:
  1.  **Baseline Reconstruction Quality:** Measure PESQ, SI-SDR, and LSD on the VCTK dataset with varying noise levels and sub-Nyquist parameters (e.g., 4 kHz 8-bit vs. 8 kHz 10-bit) to map the power-quality tradeoff.
  2.  **Ablation Study on Model Components:** Remove the Time Enhancement Network and the Amplitude-Phase Enhancement Network individually to quantify their contribution to final audio quality (PESQ, STOI) and SE performance.
  3.  **Real-time Mobile Inference Test:** Deploy the trained model on a target device (e.g., Google Pixel 7) and measure average and worst-case inference time for a 1-second audio chunk. Verify it is consistently below the real-time threshold and monitor power consumption.

## Open Questions the Paper Calls Out

- **Codec and Encryption Impact**: The authors state future work will include codec and encryption protocols, but their impact on performance under compression artifacts and security overhead is unknown.
- **LLM-Based Audio Processing**: The paper mentions LLMs could eliminate the need for user-specific data collection for fine-tuning, but this capability is unproven.
- **On-Device Training Toolchain**: Current fine-tuning requires desktop GPUs due to mobile platform limitations, with no open-source tools available for on-device training.

## Limitations

- Dataset composition uncertainty due to reliance on in-house BCM recordings that cannot be independently verified.
- Energy modeling gaps focusing primarily on ADC savings without comprehensive end-to-end energy accounting.
- Multi-period loss validation lacks theoretical justification and empirical sensitivity analysis.

## Confidence

- **High Confidence**: ADC power consumption relationship with sampling parameters is well-established; cascaded U-Net architecture follows standard deep learning practices.
- **Medium Confidence**: Multi-scale/multi-period loss functions as GAN alternatives are theoretically sound but lack extensive empirical validation.
- **Low Confidence**: Multimodal enhancement performance depends critically on BCM data quality and synthetic data generation, which cannot be independently verified.

## Next Checks

1. **Energy-Aware Latency Profiling**: Implement comprehensive end-to-end energy measurements on target hearable and mobile hardware, including ADC, Bluetooth transmission, and mobile inference power consumption. Verify the 3.31x power savings claim holds when considering the complete system rather than just ADC computations.

2. **Multi-Period Loss Sensitivity Analysis**: Conduct ablation studies systematically varying the multi-period loss parameters (p=5,7) and comparing against alternative period values. Evaluate whether these specific choices provide measurable perceptual improvements over simpler multi-scale approaches.

3. **Real-World BCM Performance Testing**: If possible, collect BCM recordings from actual hearable devices under various environmental conditions to validate the synthetic BCM data generation approach. Compare performance on real BCM data versus the SEANet-synthesized data used in the paper's training.