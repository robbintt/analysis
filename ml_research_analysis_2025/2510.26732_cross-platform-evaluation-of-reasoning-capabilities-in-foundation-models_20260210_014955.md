---
ver: rpa2
title: Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models
arxiv_id: '2510.26732'
source_url: https://arxiv.org/abs/2510.26732
tags:
- reasoning
- evaluation
- across
- performance
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study establishes an infrastructure-agnostic benchmark for
  evaluating reasoning capabilities across three computational paradigms: HPC supercomputing
  (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (8
  H200 GPUs). We evaluate 15 foundation models across 79 problems spanning eight academic
  domains using a dual-metric framework measuring final-answer accuracy and step-by-step
  reasoning quality.'
---

# Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models

## Quick Facts
- arXiv ID: 2510.26732
- Source URL: https://arxiv.org/abs/2510.26732
- Authors: J. de Curtò; I. de Zarzà; Pablo García; Jordi Cabot
- Reference count: 10
- Primary result: Hermes-4-70B achieves highest reasoning accuracy (0.598) while smaller models outperform larger ones, establishing infrastructure-agnostic benchmark for reasoning evaluation.

## Executive Summary
This study establishes an infrastructure-agnostic benchmark for evaluating reasoning capabilities across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (8 H200 GPUs). We evaluate 15 foundation models across 79 problems spanning eight academic domains using a dual-metric framework measuring final-answer accuracy and step-by-step reasoning quality. The methodology validates that reasoning quality is model-intrinsic rather than infrastructure-dependent, democratizing rigorous evaluation beyond supercomputing facilities.

Key findings reveal that Hermes-4-70B (70B parameters) achieves the highest overall accuracy (0.598) among extended models, outperforming its 405B counterpart (0.573) and Meta's LLaMA 3.1-405B (0.560), establishing a parameter efficiency paradox where training data quality matters more than model size. The study identifies a fundamental transparency-correctness trade-off: DeepSeek-R1 achieves record step-accuracy (0.716) but only moderate final scores (0.457, r=0.249 correlation), while Qwen3 models exhibit near-zero correlation (r=0.095), suggesting "shortcut learning" that bypasses explicit reasoning chains. University cluster validation reveals competitive performance of non-transformer architectures (Falcon-Mamba achieves 0.590, matching transformer baseline LLaMA-3.1-8B at 0.576) and demonstrates that dense smaller models (Phi-4-mini, 14B) outperform larger sparse MoE architectures (Phi-3.5-MoE, 42B: 0.674 vs 0.569).

## Method Summary
The study evaluates reasoning capabilities using a benchmark of 79 problems across 8 academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, Optimization) with difficulty stratification (25 easy, 36 medium, 18 hard). Models are served with vLLM, temperature=0.2, max_tokens=300, with 3 runs per problem across three infrastructures (MareNostrum 5 H100s, Nebius AI Studio API, university cluster with 8×H200 GPUs). Evaluation uses semantic similarity via all-MiniLM-L6-v2 embeddings to compute final-score (cosine similarity between predicted and reference answers), step-accuracy (mean similarity across intermediate reasoning steps), and consistency (standard deviation across runs).

## Key Results
- Hermes-4-70B (70B parameters) achieves highest overall accuracy (0.598) while outperforming its 405B counterpart (0.573), establishing parameter efficiency paradox
- DeepSeek-R1 achieves record step-accuracy (0.716) but only moderate final scores (0.457), revealing transparency-correctness trade-off with r=0.249 correlation
- Dense Phi-4-mini (14B) dramatically outperforms sparse Phi-3.5-MoE (42B: 0.674 vs 0.569), demonstrating architectural efficiency
- Infrastructure validation confirms reasoning quality is model-intrinsic with <3% variance across platforms, democratizing evaluation beyond supercomputing
- Qwen3 models exhibit near-zero correlation (r=0.095) between step-accuracy and final correctness, suggesting "shortcut learning" mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Infrastructure-Agnostic Inference Stability
The study isolates hardware variables by deploying identical model weights (FP16/BF16) across three distinct infrastructures. The mechanism posits that for reasoning tasks, computational precision and hardware-specific optimizations do not significantly alter the semantic trajectory of inference, resulting in <3% variance. This assumes variance isn't masked by stochastic sampling parameters and reasoning measured isn't an artifact of floating-point non-determinism.

### Mechanism 2: Data Quality over Parameter Scaling
Beyond certain parameter thresholds (approx. 70B), adding capacity without corresponding high-quality reasoning data yields diminishing returns. Hermes-4-70B outperforms its 405B variant, suggesting efficient inductive bias from superior data curation. This assumes larger models weren't under-trained and the evaluation benchmark isn't saturated.

### Mechanism 3: Reasoning-Correctness Decoupling
High transparency in intermediate reasoning steps doesn't causally determine final answer correctness. DeepSeek-R1 exhibits high step-accuracy (0.716) but moderate final scores (0.457), while Qwen3 shows the inverse. This implies separate heuristic pathways where reasoning chain generation is distinct from final token prediction.

## Foundational Learning

- **Concept: Semantic Similarity Metrics**
  - Why needed here: Entire evaluation relies on `all-MiniLM-L6-v2` embeddings and Cosine Similarity rather than exact string match
  - Quick check question: Does a score of 0.7 mean the answer is 70% correct mathematically, or that the vector representation is 70% similar to the reference?

- **Concept: Mixture-of-Experts (MoE) vs. Dense Architectures**
  - Why needed here: Paper compares models like Mixtral/Phi-3.5-MoE (sparse) against Dense models (Phi-4, Hermes)
  - Quick check question: Why does the paper claim a 14B parameter model outperforms a 42B model?

- **Concept: Chain-of-Thought (CoT) & Step-wise Evaluation**
  - Why needed here: Study distinguishes between final answer and reasoning chain
  - Quick check question: If a model produces a correct answer with flawed reasoning, does it have high Final Score and low Step Accuracy?

## Architecture Onboarding

- **Component map:** 79-problem Benchmark (8 domains) -> vLLM Serving (Temp 0.2) -> Response Generation -> `all-MiniLM-L6-v2` Embedding -> Cosine Similarity Calculation -> Metrics (Final Score, Step Accuracy, Consistency)

- **Critical path:**
  1. Configure Environment: Ensure vLLM version and precision (FP16/BF16) are pinned
  2. Inference: Run 19-problem or 79-problem set with temperature=0.2, max_tokens=300, 3 runs per problem
  3. Evaluation: Embed Reference Solutions and Model Outputs, calculate Semantic Similarity
  4. Analysis: Check for <3% variance and correlate Step Accuracy vs Final Score

- **Design tradeoffs:**
  - vLLM vs. Other Servers: Requirement for reproducibility; changing server breaks "Infrastructure Agnostic" mechanism
  - Semantic vs. Exact Match: Embeddings allow flexible language but may fail to catch subtle mathematical errors
  - Temp 0.2: Balances deterministic output with enough variance to test "Consistency"

- **Failure signatures:**
  - High Variance (>3%): Indicates setup not comparable to HPC baseline or model instability
  - High Step Accuracy / Low Final Score: Indicates "DeepSeek-style" hallucination (good reasoning, wrong conclusion)
  - Low Correlation (r≈0): Indicates "Qwen-style" shortcut learning (skipping reasoning)

- **First 3 experiments:**
  1. Reproducibility Check: Deploy LLaMA-3.1-8B on local GPU vs. cloud API using 19-problem set to verify <3% variance
  2. Architectural Contrast: Evaluate dense Phi-4-mini against MoE Phi-3.5-MoE on Optimization domain to test parameter efficiency paradox
  3. Transparency Audit: Run DeepSeek-R1 on 10 "Hard" Calculus problems and manually inspect 0.716 step-accuracy samples for logical errors

## Open Questions the Paper Calls Out

- Can training curricula explicitly targeting reasoning quality improve both step-accuracy and final-answer correctness, or is the transparency-correctness trade-off fundamental?
- Can hybrid architectures combining LLMs with symbolic solvers achieve higher reasoning accuracy on constrained domains while preserving natural-language transparency?
- What mechanisms underlie the "shortcut learning" observed in Qwen3, and can architectural or training interventions mitigate it?
- How can systems adaptively invoke System-1-like heuristic inference versus System-2-like deliberative reasoning based on problem characteristics?

## Limitations

- Semantic similarity metrics may not reliably detect subtle mathematical errors that "sound right" but are logically incorrect
- Parameter efficiency paradox requires careful interpretation about training data quality parity across model scales
- Claims about infrastructure-independence assume perfect software environment parity, yet different vLLM versions could introduce variance
- The study doesn't test whether targeted supervision can break the transparency-correctness decoupling observed

## Confidence

- **High Confidence**: Infrastructure-agnostic inference stability, methodological framework validity, architectural performance patterns
- **Medium Confidence**: Parameter efficiency paradox, transparency-correctness trade-off interpretation, shortcut learning characterization
- **Low Confidence**: Precise mechanism explaining DeepSeek-R1's high step-accuracy but low final accuracy, semantic similarity metric reliability for mathematical correctness

## Next Checks

1. **Mathematical Error Detection**: Manually audit 20 reasoning chains with high step-accuracy but low final accuracy to determine if semantic similarity metrics mask logical errors
2. **Training Data Audit**: Compare training corpus composition and curation methodology for Hermes-4-70B versus its 405B counterpart to validate data quality differences
3. **Software Environment Control**: Replicate infrastructure validation using different vLLM versions (0.4.0, 0.5.0, 0.6.0) while holding model weights constant to test <3% variance robustness