---
ver: rpa2
title: 'CAPT: Class-Aware Prompt Tuning for Federated Long-Tailed Learning with Vision-Language
  Model'
arxiv_id: '2503.06993'
source_url: https://arxiv.org/abs/2503.06993
tags:
- learning
- prompt
- federated
- data
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAPT introduces a dual-prompt mechanism with general and class-aware
  prompts, combined with heterogeneity-aware client clustering, to address the challenges
  of federated learning with long-tailed data. The framework leverages pre-trained
  vision-language models to effectively handle both data heterogeneity and class imbalance.
---

# CAPT: Class-Aware Prompt Tuning for Federated Long-Tailed Learning with Vision-Language Model

## Quick Facts
- arXiv ID: 2503.06993
- Source URL: https://arxiv.org/abs/2503.06993
- Reference count: 40
- Introduces dual-prompt mechanism with general and class-aware prompts combined with heterogeneity-aware client clustering to address federated learning with long-tailed data

## Executive Summary
CAPT addresses the challenge of federated learning with long-tailed data by introducing a dual-prompt mechanism that synergizes general and class-aware prompts. The framework leverages pre-trained vision-language models to effectively handle both data heterogeneity and class imbalance across distributed clients. By maintaining separate prompts for domain-invariant features and class-specific characteristics, combined with a novel clustering strategy that groups clients based on their data distributions, CAPT significantly improves performance on tail classes while maintaining competitive overall accuracy.

## Method Summary
CAPT builds upon CLIP by freezing the vision-language model weights and training only lightweight prompt vectors. The framework employs a dual-prompt architecture where general prompts capture domain-invariant features across all classes, while class-aware prompts focus on fine-grained class-specific characteristics. A heterogeneity-aware clustering mechanism groups clients based on their data distributions using Jensen-Shannon divergence, enabling efficient knowledge sharing. The server performs weighted aggregation of prompts within similarity clusters for class-aware prompts and heterogeneity clusters for general prompts, optimizing communication efficiency through a multi-armed bandit scheduler.

## Key Results
- Achieves up to 13.22% improvement on tail class accuracy compared to state-of-the-art methods
- Maintains competitive overall accuracy while significantly improving tail class performance across varying degrees of data heterogeneity
- Demonstrates effectiveness across multiple long-tailed datasets including CIFAR-10-LT, CIFAR-100-LT, Fashion-MNIST-LT, and ImageNet-LT

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Dual-Prompt Representation
Separating prompts into "general" and "class-aware" components mitigates gradient conflicts where abundant head-class samples dominate optimization. The framework maintains General Prompt ($P_g$) trained via contrastive loss ($L_{ge}$) for domain-invariant features, and Class-Aware Prompts ($P_c$) optimized via re-weighted loss ($L_{ca}$) using global priors ($\pi_j$). Only the prompt corresponding to the true class is updated for $P_c$, isolating fine-grained tail-class features without interference from head-class gradients.

### Mechanism 2: Heterogeneity-Aware Knowledge Routing
Routing prompt aggregation through specific client clusters based on data distribution properties reduces variance from Non-IID data fragmentation. The server employs Similarity-based clustering (using JS divergence) for Class-Aware Prompts to pool tail-class knowledge effectively, and Heterogeneity-based clustering for General Prompts to prevent bias toward dominant classes of any single client group.

## Foundational Learning

- **Federated Long-Tailed Learning**
  - Why needed: Combines FL challenge of Non-IID data with Class Imbalance, where tail classes are fragmented across heterogeneous clients
  - Quick check: How does standard FedAvg degrade when data is both Non-IID and Long-Tailed vs just Non-IID?

- **Prompt Tuning for Vision-Language Models (VLMs)**
  - Why needed: CAPT builds upon CLIP, training lightweight prompts instead of full model weights for communication efficiency
  - Quick check: Why are prompts more communication-efficient than full model fine-tuning in federated settings?

- **Jensen-Shannon (JS) Divergence**
  - Why needed: Used to calculate similarity between clients' data distributions for clustering algorithm
  - Quick check: Why might JS Divergence be preferred over KL Divergence for clustering client distributions?

## Architecture Onboarding

- **Component map**: Client Side: Frozen CLIP Encoder + General Prompt ($P_g$) + Class-Aware Prompts ($P_c$) + Vision-Language Alignment Mapper ($F$) -> Server Side: Aggregation Logic + Similarity Clustering Module + Heterogeneity Clustering Module + MAB Scheduler

- **Critical path**:
  1. Local Training: Clients freeze CLIP weights, compute $L_{ge}$ and $L_{ca}$ losses to update prompts and mapper
  2. Stats Upload: Clients upload trained prompts and normalized class distribution vectors
  3. Server Clustering: Server computes JS divergence for Similarity Clusters ($P_c$) and complementarity for Heterogeneity Clusters ($P_g$)
  4. Aggregation: Weighted averaging of prompts within respective clusters
  5. Distribution: Updated prompts sent back to clients

- **Design tradeoffs**:
  - Complexity vs. Tail Performance: Dual-clustering adds algorithmic complexity but achieves 13.22% tail improvement
  - Privacy vs. Utility: Clustering requires sharing label distribution statistics, creating potential privacy vulnerability

- **Failure signatures**:
  - Tail Collapse: Tail accuracy near zero indicates clustering failure isolating rare data clients
  - Generalization Loss: Head accuracy drop suggests Heterogeneity Clustering bias in General Prompt
  - Privacy Leakage: Raw label vectors instead of aggregated statistics violate privacy constraints

- **First 3 experiments**:
  1. Baseline Validation: Run CAPT vs. PromptFL on CIFAR-100-LT with Î±=0.1, verify lower "tail gap"
  2. Component Ablation: Disable Class-Aware Prompts to quantify dual-prompt contribution
  3. Clustering Sensitivity: Vary cluster count ($K$) to observe tail-class data availability tradeoff

## Open Questions the Paper Calls Out

The paper acknowledges that sharing aggregated label distributions for heterogeneity-aware clustering creates privacy risks but does not explore mitigation mechanisms. Additionally, the computational overhead of clustering in large-scale cross-device federated networks remains unevaluated. The framework also does not explicitly analyze robustness when clients lack samples for certain tail classes (missing class scenario).

## Limitations
- Privacy vulnerability from sharing aggregated label distributions despite anonymity claims
- Limited validation to small-scale datasets (CIFAR, Fashion-MNIST) with only 20 clients
- Effectiveness depends on sufficient tail-class samples within clusters, which may not hold in highly fragmented settings

## Confidence
- **High confidence**: Dual-prompt architecture and clustering mechanism are technically sound for well-defined problem
- **Medium confidence**: Communication efficiency claims reasonable but clustering overhead unclear; privacy analysis superficial
- **Low confidence**: Privacy constraint assertions lack rigorous validation; generalizability to different VLMs and non-image modalities unexplored

## Next Checks
1. **Privacy Analysis**: Conduct differential privacy analysis of clustering mechanism, measure information leakage from aggregated distribution statistics
2. **Scalability Test**: Implement with 50-100 clients, validate clustering stability and performance gains, measure communication overhead
3. **Domain Transfer**: Apply to non-image vision-language task (medical images or satellite imagery) to assess dual-prompt robustness to different feature distributions