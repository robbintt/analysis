---
ver: rpa2
title: Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance
arxiv_id: '2509.22635'
source_url: https://arxiv.org/abs/2509.22635
tags:
- dipsy
- image
- images
- guidance
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIPSY, a training-free approach for few-shot
  image classification that leverages dual IP-Adapter guidance for synthetic data
  generation. Unlike existing methods that require model fine-tuning or external captioning,
  DIPSY generates discriminative synthetic images using only few-shot examples through
  a novel extension of classifier-free guidance that independently controls positive
  and negative image conditioning.
---

# Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance

## Quick Facts
- arXiv ID: 2509.22635
- Source URL: https://arxiv.org/abs/2509.22635
- Authors: Luc Boudier; Loris Manganelli; Eleftherios Tsonis; Nicolas Dufour; Vicky Kalogeiton
- Reference count: 40
- Primary result: Achieves state-of-the-art few-shot classification performance without model training

## Executive Summary
This paper introduces DIPSY, a training-free approach for few-shot image classification that leverages dual IP-Adapter guidance for synthetic data generation. Unlike existing methods that require model fine-tuning or external captioning, DIPSY generates discriminative synthetic images using only few-shot examples through a novel extension of classifier-free guidance that independently controls positive and negative image conditioning. The method also employs a class similarity-based sampling strategy to select effective negative prompts. Experiments across ten benchmark datasets show that DIPSY achieves state-of-the-art or comparable performance to methods requiring training or external tools, particularly excelling at fine-grained classification tasks.

## Method Summary
DIPSY generates synthetic images for few-shot classification using Stable Diffusion 1.5 with IP-Adapter and an extended classifier-free guidance formulation. The method computes class similarities using CLIP ViT-B/16 on few-shot examples, then generates 200 synthetic images per class using dual image conditioning - one positive image from the target class and one negative image from a visually similar class. The extended CFG formulation (Eq. 9) applies separate guidance weights for text, positive image, and negative image prompts. Generated images are combined with real few-shot examples to fine-tune a CLIP ViT-B/16 model with LoRA, using a weighted loss that prioritizes real data (Î»=0.8 for real, 0.2 for synthetic).

## Key Results
- Achieves state-of-the-art or comparable performance across ten benchmark datasets
- Particularly excels at fine-grained classification tasks (Pets, Flowers-102, Stanford Cars)
- Eliminates need for model training or external captioning tools
- Resource-efficient compared to training-based alternatives

## Why This Works (Mechanism)

### Mechanism 1
Extended classifier-free guidance (CFG) enables fine-grained control over separate conditioning modalities (text, positive image, negative image). The authors derive a new CFG formulation (Eq. 9) that adds separate guidance terms for text, positive image, and negative image prompts. By recursively applying Bayes' rule, they obtain a noise prediction that combines an unconditional baseline with three independent deviations, each scaled by its own guidance weight ($w_{text}, w_{im+}, w_{im-}$). The negative guidance term (subtracted) steers generation away from features of a "distractor" class.

### Mechanism 2
Using negative conditioning from a similar class improves the discriminability of generated images. A "negative" image prompt is sampled from a class that is visually similar to the target class. By applying negative guidance ($-w_{im-}$) with this image, the denoising process is actively pushed away from generating features present in the similar-but-different class. This forces the model to emphasize the subtle, unique features that distinguish the target class from its nearest neighbors in the embedding space.

### Mechanism 3
The generated synthetic images successfully augment the few-shot dataset to train a better classifier. The method generates a large number ($n_{synth}$) of synthetic images using the DIPSY pipeline and combines them with the few real images ($n_{real}$). A CLIP ViT-B/16 model is then fine-tuned on this combined dataset using a weighted loss function (Eq. 10). The classifier learns from both the scarce real data (high fidelity) and the abundant synthetic data (high variety and emphasized discriminative features).

## Foundational Learning
- **Classifier-Free Guidance (CFG)**
  - Why needed here: DIPSY's core innovation is an extension of CFG. You must understand the standard CFG formula (Eq. 2) to comprehend how the authors extend it for multi-modal conditioning.
  - Quick check question: How does standard CFG scale the conditional noise prediction relative to the unconditional one?

- **IP-Adapter**
  - Why needed here: This is the mechanism used to inject image prompts into the diffusion model. Understanding that it enables conditioning a text-to-image model on images without retraining the core U-Net is crucial.
  - Quick check question: What does an IP-Adapter allow you to add to a text-to-image diffusion model?

- **Diffusion Model Sampling**
  - Why needed here: The entire synthetic data generation process relies on the iterative denoising steps of a diffusion model. The method operates during this sampling loop.
  - Quick check question: During diffusion model sampling, is the model predicting the final clean image or the noise at each step?

## Architecture Onboarding
- **Component map:** Input -> Class Similarity Calculator -> Synthetic Data Generator -> Classifier Fine-tuner -> Output
- **Critical path:** The critical path is the derivation and implementation of the extended CFG formula (Eq. 9) and its integration into the image generation loop. A bug here will affect every single generated image.
- **Design tradeoffs:**
  - Per-image computational cost vs. Quality: The dual IP-Adapter guidance requires three forward passes through the U-Net per denoising step (Tab. 5), increasing generation time by 3-4x compared to simpler methods like DISEF.
  - Guidance weights: The performance is sensitive to the guidance weights ($w_{text}, w_{im+}, w_{im-}$), which are tuned per-dataset. Finding optimal weights requires a hyperparameter search.
- **Failure signatures:**
  - Mode Collapse / Artifacts: If guidance weights are too high, generated images may develop artifacts or become overly saturated with specific features from the positive prompt.
  - Poor Discriminability: If negative classes are not sampled effectively (e.g., low similarity), the generated images will not emphasize the subtle features needed for fine-grained tasks.
  - Distribution Mismatch: If the synthetic data is too far from the real data distribution, the classifier's performance may degrade, especially on out-of-distribution examples.
- **First 3 experiments:**
  1. Baseline Reproduction: Implement the standard CFG with a single IP-Adapter (positive guidance only) to establish a baseline for generation quality and classifier performance.
  2. Ablation Study of Guidance: Systematically vary the negative guidance weight ($w_{im-}$) while keeping other parameters fixed. Measure the impact on downstream classifier accuracy and generate images to visualize the change in discriminative features.
  3. Negative Sampling Strategy Ablation: Compare the proposed CLIP-based similarity sampling for negative prompts against random sampling and sampling from the most dissimilar classes. Evaluate the effect on classifier accuracy across a few key datasets.

## Open Questions the Paper Calls Out
The paper explicitly calls out extending DIPSY to video and 3D domains, and applying it to more complex tasks like few-shot object detection and segmentation as future work.

## Limitations
- Limited ablation studies on core innovations (dual guidance, negative sampling strategy)
- Guidance weight tuning process is per-dataset but exact values are not reported
- No validation of class similarity sampling against simpler alternatives

## Confidence
- **High Confidence:** Overall experimental results showing state-of-the-art performance on benchmark datasets
- **Medium Confidence:** Dual-guidance CFG extension mechanism derivation and implementation
- **Low Confidence:** Necessity of class similarity-based negative sampling strategy

## Next Checks
1. Implement a version using only positive image guidance (standard CFG) and compare performance against the full dual-guidance model across all datasets to isolate the contribution of negative guidance.
2. Replace the CLIP-based similarity sampling with random negative sampling and sampling from most dissimilar classes. Measure impact on fine-grained classification tasks specifically.
3. Systematically vary the guidance weights (w_text, w_im+, w_im-) across a range of values on one representative dataset from each category (general, fine-grained, remote sensing) to map the performance landscape.