---
ver: rpa2
title: 'Large Language Models for Healthcare Text Classification: A Systematic Review'
arxiv_id: '2503.01159'
source_url: https://arxiv.org/abs/2503.01159
tags:
- text
- classification
- healthcare
- data
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review identified 65 research studies examining
  large language models (LLMs) for healthcare text classification between 2018-2024.
  The analysis revealed that fine-tuning approaches were most prevalent, used in 35
  of 65 studies, followed by prompt engineering in 17 studies.
---

# Large Language Models for Healthcare Text Classification: A Systematic Review

## Quick Facts
- **arXiv ID:** 2503.01159
- **Source URL:** https://arxiv.org/abs/2503.01159
- **Reference count:** 40
- **Primary result:** Fine-tuning approaches were most prevalent (35/65 studies) for healthcare text classification between 2018-2024

## Executive Summary
This systematic review analyzed 65 research studies examining large language models (LLMs) for healthcare text classification tasks. The research spanned from 2018 to 2024, revealing that fine-tuning approaches dominated the landscape, particularly using BERT variants and closed-source LLMs like GPT models. Clinical notes constituted the largest data category, with performance primarily measured through accuracy metrics. The review found that LLMs significantly outperform traditional machine learning approaches, especially for clinical decision support and public health applications, though challenges remain regarding data privacy, interpretability, and computational resource requirements.

## Method Summary
The review systematically examined research studies using LLMs for healthcare text classification across three domains: Clinical Notes, Healthcare Communications, and Research/Literature. The analysis focused on identifying prevalent approaches, data types, performance metrics, and challenges. Studies were categorized based on their methodology (fine-tuning vs. prompt engineering), model architectures (BERT variants, GPT models), and evaluation strategies. The review also identified future research directions including multimodal integration, temporal modeling, and privacy-preserving techniques.

## Key Results
- Fine-tuning approaches were used in 35 of 65 studies, making it the most prevalent methodology
- BERT variants and closed-source LLMs (particularly GPT models) dominated the architectural landscape
- Clinical notes represented the largest data category, followed by healthcare communications and research literature
- LLMs significantly outperform traditional machine learning approaches in healthcare text classification tasks
- Performance evaluation primarily focused on accuracy metrics, with computational efficiency measures used less frequently

## Why This Works (Mechanism)

### Mechanism 1: Fine-tuning for Domain Adaptation
Fine-tuning pre-trained models on domain-specific healthcare data provides the highest performance gains for classification tasks. By updating the model's weights using labeled clinical data, the model adjusts its latent representations to better capture medical jargon, abbreviations, and long-range dependencies unique to clinical notes. The core assumption is that the target domain possesses enough labeled data to adjust weights without "catastrophic forgetting" of the model's general linguistic capabilities. Break condition: If the dataset is extremely small (<100 examples), fine-tuning may overfit; prompt engineering or few-shot learning becomes the preferred path.

### Mechanism 2: Prompt Engineering for Zero-Shot Learning
Prompt engineering enables immediate task solvability for healthcare classification without weight updates, assuming the pre-training corpus contained sufficient medical knowledge. This approach leverages the frozen weights of a Large Language Model by restructuring the classification task into a text completion or pattern-matching problem. The core assumption is that the foundation model has already internalized the necessary medical reasoning during its massive pre-training phase. Break condition: The model will fail if the medical concept is too rare (hallucination risk) or if the task requires strict adherence to a proprietary classification schema not seen during pre-training.

### Mechanism 3: Sparse Attention for Long Documents
Specialized sparse attention architectures are necessary to process full-length clinical notes without information loss. Standard transformers have a 512-token limit, forcing truncation of lengthy discharge summaries. Sparse attention mechanisms reduce the complexity from quadratic to linear, allowing the model to "attend" to tokens across the entire document. The core assumption is that critical clinical features are not localized solely at the beginning of the text but are distributed throughout the document. Break condition: If the hardware cannot support the memory overhead of extended context windows, or if the clinical notes are very short, the computational overhead of sparse attention is unnecessary.

## Foundational Learning

- **Concept: Transfer Learning (Pre-training vs. Fine-tuning)**
  - Why needed here: The paper relies on the premise that models learn general language first (pre-training) and then adapt to medicine (fine-tuning). Understanding this distinction explains why "off-the-shelf" GPT-4 behaves differently than a "fine-tuned" ClinicalBERT.
  - Quick check question: Does updating the model's weights on a specific dataset make it a "better" general model or a "more specialized" model?

- **Concept: The Context Window & Tokenization**
  - Why needed here: Clinical notes are often long. The review explicitly discusses models like Longformer handling 4096 tokens. You must understand that inputs exceeding the context window are truncated, potentially losing vital patient history.
  - Quick check question: What happens to a 1,000-word clinical note if you feed it into a standard BERT model (512-token limit)?

- **Concept: PHI (Protected Health Information) & De-identification**
  - Why needed here: This is the primary barrier to using cloud-based LLMs in healthcare. The paper highlights "on-premises deployment" vs. "cloud-based" as a critical ethical dimension.
  - Quick check question: Why is "de-identification" a mandatory preprocessing step before sending patient notes to a public API like GPT-4?

## Architecture Onboarding

- **Component map:**
  Data Ingestion -> Privacy Layer -> LLM Core -> Output Layer
  Raw clinical text -> De-identification/Secure Enclave -> Base Model + Adapters -> Classification Head

- **Critical path:** The decision between Prompt Engineering (speed/low setup) and Fine-tuning (accuracy/privacy). The paper suggests fine-tuning is dominant for high-stakes classification (35/65 studies), but prompt engineering is rising for rapid prototyping.

- **Design tradeoffs:**
  - **Accuracy vs. Privacy:** Cloud models (GPT-4) generally offer higher reasoning capabilities but risk data leakage. Local models (ClinicalBERT) offer total privacy but may lag in raw reasoning power.
  - **Compute vs. Context:** Increasing context window (to 4096 tokens) requires significantly more VRAM, limiting batch size.

- **Failure signatures:**
  - **Hallucination:** The model invents symptoms or misclassifies based on "probability" rather than evidence (common in pure Prompt Engineering).
  - **Truncation Bias:** The model misses a diagnosis because it was documented at the end of a long note, exceeding the 512-token limit.
  - **Catastrophic Forgetting:** A fine-tuned model becomes too specialized, failing to understand common language outside its new medical domain.

- **First 3 experiments:**
  1. **Baseline Zero-Shot:** Select a subset of de-identified clinical notes. Use a GPT model via API with a simple prompt ("Classify this note into [Categories]"). Establish a baseline F1 score.
  2. **Fine-Tuning comparison:** Take a pre-trained ClinicalBERT. Fine-tune it on the same dataset using LoRA (Low-Rank Adaptation) to save memory. Compare F1 scores against the baseline.
  3. **Context Stress Test:** Take a set of very long discharge summaries. Compare performance of a standard 512-token model (which truncates) vs. a Longformer-based model to measure information loss due to truncation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can effective architectures be developed to integrate heterogeneous modalities (clinical notes, images, lab results) for contextually rich text classification?
- **Basis in paper:** Section 5.1 states that "multimodal integration starts gaining attraction" and calls for research into "effective architectures that can handle the heterogeneity of different data types."
- **Why unresolved:** Current studies primarily treat text in isolation or exclude multimodal data; integrating diverse formats requires complex fusion strategies that maintain semantic relationships.
- **What evidence would resolve it:** Benchmark studies evaluating LLMs on combined datasets that demonstrate superior performance by models utilizing fusion techniques over text-only baselines.

### Open Question 2
- **Question:** How can LLM architectures be augmented (e.g., via temporal attention mechanisms or RNN components) to capture longitudinal dependencies and disease progression in patient histories?
- **Basis in paper:** Section 5.1 identifies "Temporal Modeling" as a key future direction, noting that "traditional LLM approaches often treat each clinical note independently."
- **Why unresolved:** Standard transformer architectures lack specific mechanisms to maintain the chronological order of clinical events across multiple visits, limiting their utility for tracking dynamic patient states.
- **What evidence would resolve it:** The development and validation of modified LLM architectures that successfully track patient history evolution while maintaining classification performance over time.

### Open Question 3
- **Question:** Can federated learning and differential privacy be effectively integrated into LLM training to facilitate privacy-preserving data sharing across healthcare institutions?
- **Basis in paper:** Section 5.3 highlights "Privacy-Preserving Data Sharing" as a transformative approach, suggesting "Federated learning" and "differential privacy" to allow institutions to "collaboratively train LLMs on their local data without directly sharing patient records."
- **Why unresolved:** Direct data sharing is restricted by privacy regulations, and current training methods often require centralized datasets, limiting the diversity and scale of training data.
- **What evidence would resolve it:** Implementations where model parameters are aggregated centrally while underlying training data remains distributed and private, resulting in models with reduced bias and improved generalizability.

## Limitations
- Analysis relies on reported metrics from primary studies, which may not follow standardized evaluation protocols
- Computational efficiency reporting is severely lacking (only 9 of 65 studies included GPU utilization or inference time metrics)
- Privacy considerations discussed are largely theoretical, with few concrete implementations of on-premises deployment
- Specific performance superiority of one approach over another is uncertain due to unreported implementation details

## Confidence
**High Confidence Claims:**
- Fine-tuning approaches are the dominant methodology in healthcare text classification research
- BERT variants and closed-source LLMs (GPT models) are the most frequently employed architectures
- Clinical notes represent the largest data category for these applications
- LLMs outperform traditional machine learning approaches in classification accuracy

**Medium Confidence Claims:**
- Prompt engineering is emerging as a viable alternative to fine-tuning
- Computational efficiency measures are underutilized in current research
- Challenges exist regarding data privacy and interpretability
- Clinical decision support applications show the most promise

**Low Confidence Claims:**
- Specific performance superiority of one approach over another
- Exact computational requirements for production deployment
- Long-term stability and maintenance requirements for fine-tuned models

## Next Checks
1. **Implementation Fidelity Audit:** Select 3-5 representative studies from the review and attempt to reproduce their results using publicly available code and datasets. Document any discrepancies in reported vs. achievable performance.

2. **Resource Benchmarking Study:** Implement a standardized classification task using both fine-tuned ClinicalBERT and prompt-engineered GPT-4 approaches. Measure not only accuracy metrics but also GPU memory usage, inference latency, and total cost of ownership.

3. **Privacy-Utility Tradeoff Analysis:** Design an experiment comparing cloud-based vs. on-premises LLM deployment for a realistic healthcare classification task. Quantify the performance degradation (if any) when using privacy-preserving approaches while measuring actual PHI exposure risk.