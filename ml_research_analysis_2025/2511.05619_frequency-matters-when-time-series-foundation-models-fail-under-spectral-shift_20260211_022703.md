---
ver: rpa2
title: 'Frequency Matters: When Time Series Foundation Models Fail Under Spectral
  Shift'
arxiv_id: '2511.05619'
source_url: https://arxiv.org/abs/2511.05619
tags:
- time
- series
- frequency
- pretraining
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key limitation of time series foundation
  models (TSFMs): their performance degrades when the dominant frequency components
  of downstream tasks differ from those seen during pretraining. The authors demonstrate
  this through an industrial-scale player engagement prediction task in mobile gaming,
  where a TSFM underperforms domain-adapted baselines.'
---

# Frequency Matters: When Time Series Foundation Models Fail Under Spectral Shift

## Quick Facts
- arXiv ID: 2511.05619
- Source URL: https://arxiv.org/abs/2511.05619
- Authors: Tianze Wang, Sofiane Ennadir, John Pertoft, Gabriela Zarzar Gandler, Lele Cao, Zineb Senane, Styliani Katsarou, Sahar Asadi, Axel Karlsson, Oleg Smirnov
- Reference count: 26
- Primary result: Time series foundation models underperform when downstream data's dominant frequencies differ from pretraining frequencies

## Executive Summary
This paper identifies a critical limitation of time series foundation models (TSFMs): their performance degrades when the dominant frequency components of downstream tasks differ from those seen during pretraining. Through an industrial-scale player engagement prediction task in mobile gaming and controlled synthetic experiments, the authors demonstrate that TSFMs rely on frequency-specific patterns memorized during pretraining rather than learning truly universal temporal representations. The primary finding is that frequency awareness is critical for robust TSFM deployment, suggesting that spectral overlap between pretraining and downstream data should be assessed and that frequency-aware pretraining strategies may improve generalization.

## Method Summary
The authors evaluate TSFMs under linear probing with lightweight heads on both industrial gaming data and synthetic time series. For the industrial case, they freeze a pretrained MOMENT-small backbone and train linear regression/classification heads on multivariate player telemetry. For synthetic validation, they generate sinusoidal signals with frequencies either inside or outside the pretraining data's dominant frequency bands, then compare performance gaps. The methodology includes FFT-based frequency analysis, synthetic signal generation with random phases and noise, and standard evaluation metrics (MSE, MAE, Accuracy, AUC) across temporal holdout splits.

## Key Results
- Industrial TSFM (MOMENT-small) underperforms domain-adapted baselines by 20-80% on MSE/MAE for unseen frequency bands
- Classification AUC drops of 0.05-0.20 on unseen frequency bands in synthetic experiments
- Temporal holdout performance significantly worse than player holdout, indicating distribution drift compounds spectral mismatch
- Performance gaps persist across both regression and classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TSFMs encode frequency-specific patterns during pretraining rather than learning truly universal temporal representations
- Mechanism: The transformer backbone learns to recognize and process specific frequency bands present in pretraining corpora. When downstream data contains dominant frequencies outside these learned bands, the encoder produces degraded embeddings even when the frozen backbone is paired with a trained linear head
- Core assumption: Frequency band coverage during pretraining determines generalization boundaries; this is a learned constraint, not an architectural one
- Evidence anchors:
  - "spectral shift (a mismatch between the dominant frequency components in downstream tasks and those represented during pretraining) as a key factor"
  - "we hypothesize that TSFMs rely on frequency components memorized during pretraining"
  - CauKer paper demonstrates that TSFMs can be pretrained on synthetic data only, suggesting frequency distribution matters more than domain authenticity
- Break condition: If a TSFM were pretrained with explicit frequency-diverse augmentation or frequency-aware objectives, this band-specific memorization pattern would weaken or disappear

### Mechanism 2
- Claim: Linear probing exposes frequency sensitivity more sharply than full fine-tuning would
- Mechanism: The frozen encoder cannot adapt its spectral processing; only the linear head learns. This isolates the quality of pretrained representations. When spectral mismatch exists, even an optimal linear head cannot compensate for poor embeddings
- Core assumption: Full fine-tuning might partially recover performance by adjusting lower layers to handle new frequency bands, but this was not tested in the paper
- Evidence anchors:
  - "TSFMs are assessed under linear probing with lightweight heads"
  - "We freeze a pretrained TSFM backbone and train lightweight regression or classification heads on each synthetic variant"
  - Distillation work on TSFMs suggests architectural redundancy exists, but spectral coverage remains underexplored
- Break condition: If fine-tuning the full model recovered performance on unseen frequency bands, the mechanism would shift from "representation failure" to "adaptation requirement"

### Mechanism 3
- Claim: Domain-specific sampling rates and periodicities create implicit frequency distributions that pretrained TSFMs cannot extrapolate beyond
- Mechanism: Time series from different domains (gaming telemetry vs. electricity consumption vs. sensor data) exhibit characteristic frequency signatures. A TSFM pretrained on one mixture of domains develops implicit spectral priors. Gaming data with irregular, multi-scale rhythms falls outside these priors
- Core assumption: The frequency distribution shift is a primary driver of performance degradation, though other factors (covariate shift, non-stationarity) may contribute
- Evidence anchors:
  - "gaming telemetry, where player behavior exhibits irregular, multi-scale rhythms"
  - Appendix B, Figure 2 shows frequency spectrum comparison between gaming dataset and FordA/FaultDetectionA pretraining data
  - UniCA paper addresses covariate-aware forecasting but does not explicitly handle frequency distribution shifts
- Break condition: If performance degradation persisted even after frequency-matching synthetic data, other distributional factors would be implicated

## Foundational Learning

- Concept: **Fourier Transform and Spectral Analysis**
  - Why needed here: The entire diagnosis depends on understanding dominant frequencies, FFT computation, and frequency band extraction. Without this, you cannot assess spectral overlap or construct the seen/unseen synthetic experiments
  - Quick check question: Given a time series of length N sampled at rate T, can you compute its dominant frequency components and identify the top-k frequency bands?

- Concept: **Linear Probing vs. Full Fine-Tuning in Transfer Learning**
  - Why needed here: The experimental methodology relies on freezing the backbone and training only a linear head. Understanding why this isolates representation quality is essential for interpreting results
  - Quick check question: Why does linear probing provide a stricter test of representation quality than full fine-tuning?

- Concept: **Time Series Foundation Model Architectures**
  - Why needed here: You need to understand what MOMENT and similar models actually encode—patch embedding, transformer attention over temporal positions, and how these might implicitly capture frequency information
  - Quick check question: In a patch-based transformer for time series, how might the patch length and stride affect the model's sensitivity to different frequency components?

## Architecture Onboarding

- Component map: Raw Time Series → Patch Embedding → Transformer Encoder → [Frozen] → Linear Head → [Trained] → Prediction → ↓ Frequency Analysis (FFT) → Spectral Band Extraction → [Diagnostic Tool, not in model]

- Critical path: The diagnostic workflow is: (1) extract dominant frequencies from pretraining data, (2) extract dominant frequencies from downstream data, (3) compute spectral overlap, (4) if overlap is low, expect degraded performance and consider domain adaptation or frequency-aware augmentation

- Design tradeoffs:
  - Linear probing vs. fine-tuning: Probing reveals representation quality but may underestimate what full adaptation could achieve
  - Synthetic probe simplicity vs. ecological validity: Sinusoidal signals isolate frequency effects but ignore burstiness, irregular sampling, and regime shifts present in real gaming data
  - Single TSFM evaluation vs. broader claims: Paper tests MOMENT-small; generalization to other TSFMs (Chronos, TimesFM, Moirai) is assumed but not proven

- Failure signatures:
  - MSE/MAE on unseen frequency bands 20-80% higher than seen bands (Table 2)
  - Classification AUC drops of 0.05-0.20 on unseen bands (Table 3)
  - Temporal holdout performance significantly worse than player holdout (suggesting distribution drift compounds spectral mismatch)

- First 3 experiments:
  1. **Spectral overlap audit**: Before deploying any TSFM, compute FFT on your downstream data and the model's pretraining corpus (if available). Quantify the frequency band overlap ratio
  2. **Seen/unseen synthetic probe**: Generate sinusoidal signals with frequencies from inside vs. outside the pretraining spectrum. Train linear heads on frozen embeddings. Compare MSE/AUC gaps to establish your specific degradation magnitude
  3. **Frequency-aware augmentation test**: Augment your downstream training data with band-pass filtered versions or frequency-scaled variants. Retest linear probing to measure recovery. If performance improves, this validates frequency mismatch as a causal factor and suggests a mitigation path

## Open Questions the Paper Calls Out

- **Question**: Can frequency-aware pretraining strategies, such as auxiliary frequency prediction tasks or frequency-based data augmentation, mitigate the performance degradation caused by spectral shift?
  - Basis in paper: Appendix F explicitly suggests this direction: "We hypothesize that frequency-aware pretraining strategies, combined with standard time series pretraining methods, may be a fruitful future direction"
  - Why unresolved: The paper identifies the failure mode (spectral shift) but stops short of validating specific architectural or objective-function solutions to address the lack of spectral diversity in pretraining corpora
  - What evidence would resolve it: A study comparing standard TSFMs against variants trained with frequency-aware objectives, evaluated on the synthetic "unseen band" datasets introduced in the paper

- **Question**: Is the vulnerability to spectral shift a universal limitation across all TSFM architectures and domains, or is it specific to the MOMENT-small model and gaming telemetry?
  - Basis in paper: In Section 3.3, the authors state: "Our evidence is drawn from one industrial domain (mobile gaming) and a single TSFM configuration, and we are conducting broader validations at the moment"
  - Why unresolved: While the authors demonstrate the effect in one specific industrial case and one model, the generalizability of this failure mode to other popular architectures (e.g., Chronos, TimesFM) or data types (e.g., finance, healthcare) remains unproven
  - What evidence would resolve it: Applying the synthetic frequency band probing methodology to a diverse suite of foundation models and real-world industrial datasets outside of gaming

- **Question**: How does spectral shift interact with other complex real-world dynamics, such as irregular sampling, burstiness, and regime shifts?
  - Basis in paper: Section 3.3 notes a limitation: "The synthetic probes also simplify real-world dynamics, relying on sinusoidal signals that do not fully capture irregular sampling, burstiness, or regime shifts"
  - Why unresolved: The paper isolates frequency as a variable using clean synthetic sinusoids. It is unknown whether the observed degradation is exacerbated or masked when the time series also contains messy real-world artifacts like missing timestamps or non-stationarity
  - What evidence would resolve it: Experiments on synthetic datasets that combine out-of-distribution frequencies with simulated irregular sampling intervals and bursty noise

## Limitations

- The paper demonstrates frequency sensitivity using a single TSFM (MOMENT-small); generalization to other TSFMs (Chronos, TimesFM, Moirai) is assumed but not empirically validated
- Linear probing isolates representation quality but may underestimate what full fine-tuning could achieve; the degree to which frequency mismatch persists under fine-tuning remains unknown
- Synthetic sinusoidal signals cleanly isolate frequency effects but ignore burstiness, irregular sampling, and regime shifts present in real gaming data, potentially limiting ecological validity
- Exact synthetic data generation parameters (noise magnitude, frequency shift δ selection, dataset-wide bounds computation) are underspecified, creating reproducibility challenges

## Confidence

- **High confidence**: The core finding that spectral mismatch degrades TSFM performance is supported by both industrial and synthetic experiments with consistent directional effects
- **Medium confidence**: The mechanism that frequency bands memorized during pretraining determine generalization boundaries is plausible and mechanistically coherent, but fine-tuning experiments would strengthen causal attribution
- **Medium confidence**: The practical recommendation to audit spectral overlap before TSFM deployment is actionable and low-cost, though the exact performance impact varies by domain and frequency gap magnitude

## Next Checks

1. **Fine-tuning recovery test**: Compare linear probing vs. full fine-tuning on unseen frequency bands to determine whether adaptation can compensate for spectral mismatch
2. **Cross-TSFM generalization**: Repeat synthetic experiments with 2-3 additional TSFMs (Chronos, TimesFM, Moirai) to validate that frequency sensitivity is a general TSFM limitation, not MOMENT-specific
3. **Real-world frequency augmentation**: Augment gaming dataset with frequency-shifted versions (band-pass filtering, time-stretching) and measure linear probing performance recovery to validate frequency mismatch as a causal factor