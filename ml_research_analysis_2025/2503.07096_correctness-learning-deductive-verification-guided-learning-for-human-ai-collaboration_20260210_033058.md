---
ver: rpa2
title: 'Correctness Learning: Deductive Verification Guided Learning for Human-AI
  Collaboration'
arxiv_id: '2503.07096'
source_url: https://arxiv.org/abs/2503.07096
tags:
- learning
- task
- correctness
- pattern
- schemes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Correctness Learning (CL), a novel framework
  that integrates deductive verification with reinforcement learning to enhance human-AI
  collaboration in safety-critical domains. The core idea is to use historical high-quality
  decision schemes as benchmarks, formally model and verify their correctness using
  separation logic, and extract typical "correctness patterns" (e.g., task priorities
  in shared resources).
---

# Correctness Learning: Deductive Verification Guided Learning for Human-AI Collaboration

## Quick Facts
- arXiv ID: 2503.07096
- Source URL: https://arxiv.org/abs/2503.07096
- Reference count: 4
- RL algorithms improve average completion time by 8.4% when augmented with pattern-driven correctness learning

## Executive Summary
This paper introduces Correctness Learning (CL), a framework that integrates deductive verification with reinforcement learning to enhance human-AI collaboration in safety-critical domains. The core innovation is using historical high-quality decision schemes as benchmarks, formally modeling and verifying their correctness using separation logic, and extracting typical "correctness patterns" to guide the learning process through a reward mechanism. The framework is demonstrated on job shop scheduling tasks, where algorithms like DQN, DDQN, Dueling DQN, and PPO show significant performance improvements in completion time when augmented with pattern-driven correctness learning.

## Method Summary
The method employs a hierarchical reinforcement learning architecture where a lower-layer PPO agent handles car allocation decisions and is frozen after convergence. The upper-layer agent (DQN variants or PPO) learns task scheduling with real-time feedback from the lower layer. Correctness patterns extracted from formally verified historical schemes are used to shape rewards through pattern matching. The verification module encodes schemes in MLJSS and proves correctness using Coq, enabling local reasoning about shared resources through separation logic. The reward mechanism combines base rewards with correctness bonuses proportional to pattern matching scores.

## Key Results
- DQN, DDQN, Dueling DQN, and PPO algorithms improve completion time by 8.4%, 3.9%, 1.6%, and 5.7% respectively when augmented with pattern-driven correctness learning
- PDCL reduces decision time by 16.4% compared to baseline algorithms
- The framework demonstrates successful integration of formal verification with reinforcement learning for resource allocation tasks

## Why This Works (Mechanism)

### Mechanism 1: Pattern-Driven Reward Shaping via Deductive Verification
The framework extracts symbolic "correctness patterns" from historical high-quality schemes and uses pattern-matching rewards to guide RL agents toward better resource allocation decisions. This assumes historical schemes encode transferable patterns that generalize across similar scenarios, with the reward signal becoming noise if patterns are scenario-specific or poorly calibrated.

### Mechanism 2: Hierarchical Decoupling of Process and Resource Decisions
Separating high-level process scheduling from low-level resource allocation enables more stable learning when combined with verification feedback. This assumes resource allocation and process scheduling can be meaningfully decoupled without losing optimality, and that frozen lower-layer policies don't create harmful distribution shifts.

### Mechanism 3: Local Reasoning via Separation Logic for Shared Resources
Modeling the scheduling system as a two-tier resource heap enables tractable verification and pattern extraction even with complex resource sharing. This assumes the scheduling domain can be faithfully captured by the proposed heap model and MLJSS commands, with verification overhead becoming prohibitive for larger systems.

## Foundational Learning

- **Separation Logic**: Core formalism for modeling shared resources. You must understand how separating conjunction (∗) enables local reasoning about heap mutations.
  - Quick check: Given two heap assertions P and Q, what does P ∗ Q mean, and why does it simplify reasoning about concurrent resource access?

- **Hierarchical Reinforcement Learning (HRL)**: The framework uses a two-layer architecture where lower-level policies are frozen and embedded in upper-level environments.
  - Quick check: In a two-level HRL setup, what happens to the training signal if the lower-level policy changes during upper-level training?

- **Deductive Verification vs. Model Checking**: Paper distinguishes its approach from model checking. Deductive verification uses theorem proving (Coq) rather than exhaustive state exploration.
  - Quick check: Why might deductive verification scale better than model checking for systems with unbounded state (e.g., variable numbers of tasks)?

## Architecture Onboarding

- **Component map**: Lower-layer environment -> Lower-layer PPO agent -> Upper-layer environment -> Upper-layer agent (DQN/DDQN/Dueling DQN/PPO) -> Coq verification module -> Pattern matcher -> Reward calculation
- **Critical path**: Collect historical schemes → encode in MLJSS → verify in Coq → extract patterns → train lower-layer agent → embed in upper-layer → train upper-layer with pattern rewards → evaluate performance
- **Design tradeoffs**: Training time increases 15-200% with PDCL, but completion time improves 1.6-9.4%; overly specific patterns may not transfer to new task configurations; Coq proof complexity may become prohibitive for larger systems
- **Failure signatures**: Pattern mismatch (low μmatch), non-convergence (especially for Dueling DQN without PDCL), reward hacking (agents exploiting pattern-matching without improving completion time)
- **First 3 experiments**: 1) Baseline reproduction of Table 1 ComT values without PDCL; 2) Ablation study on pattern reward weight α to find optimal calibration; 3) Cross-scenario transfer testing pattern generalization from 10-task to 12-task scenarios

## Open Questions the Paper Calls Out

- Can more general correctness patterns be identified by analyzing a broader set of historical high-quality schemes?
- How does the varying importance of different correctness patterns impact the overall decision-making performance?
- Can the interactive deductive verification process be automated to scale effectively for complex, dynamic systems?

## Limitations

- The framework depends heavily on access to high-quality historical schemes for pattern extraction, which are not provided
- Pattern matching mechanism remains underspecified, creating significant implementation gaps
- Coq-based verification introduces computational overhead that may not scale to larger scheduling problems

## Confidence

- Pattern transferability claim: Medium - performance improvements are reported but generalization to diverse scenarios remains unproven
- Verification overhead scalability: Low - computational costs for larger systems are not quantified
- Hierarchical decoupling effectiveness: Medium - assumes meaningful separation without losing optimality

## Next Checks

1. **Pattern Transferability Test**: Train PDCL using patterns from 10-task historical schemes, then evaluate on 12-task scenarios to measure performance degradation
2. **Ablation Study on Pattern Specificity**: Systematically vary pattern granularity and measure trade-off between specificity and transfer performance
3. **Verification Overhead Analysis**: Benchmark Coq proof generation time versus scheduling problem size to quantify scalability constraints and identify thresholds where the framework becomes impractical