---
ver: rpa2
title: 'Vibe Checker: Aligning Code Evaluation with Human Preference'
arxiv_id: '2510.07315'
source_url: https://arxiv.org/abs/2510.07315
tags:
- code
- inst
- instruction
- evaluation
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the disconnect between current code evaluation
  metrics and real-world user preferences in "vibe coding." The authors hypothesize
  that instruction following, representing non-functional expectations like style
  and documentation, is the missing piece of vibe check. To quantify this, they create
  VeriCode, a taxonomy of 30 verifiable code instructions with deterministic verifiers,
  and Vibe Checker, a testbed augmenting established benchmarks.
---

# Vibe Checker: Aligning Code Evaluation with Human Preference

## Quick Facts
- arXiv ID: 2510.07315
- Source URL: https://arxiv.org/abs/2510.07315
- Reference count: 40
- Primary result: Instruction following is the primary differentiator between models on real-world programming tasks

## Executive Summary
This paper addresses the disconnect between current code evaluation metrics and real-world user preferences in "vibe coding." The authors hypothesize that instruction following, representing non-functional expectations like style and documentation, is the missing piece of vibe check. To quantify this, they create VeriCode, a taxonomy of 30 verifiable code instructions with deterministic verifiers, and Vibe Checker, a testbed augmenting established benchmarks. Evaluating 31 leading LLMs, they find that non-functional instructions cause notable functional regression, following multiple instructions remains challenging (even top models drop below 50% success), and single-turn generation better preserves functionality while multi-turn editing achieves higher instruction following. Critically, a composite score of functional correctness and instruction following correlates best with human preference (LMArena Elo), with instruction following emerging as the primary differentiator on real-world programming tasks.

## Method Summary
The authors introduce VeriCode, a comprehensive taxonomy of 30 verifiable code instructions covering non-functional aspects like style, documentation, and structure. They develop deterministic verifiers for each instruction type, enabling systematic evaluation of instruction following. Vibe Checker is built by augmenting established code benchmarks with these instructions, creating a controlled testbed for measuring both functional correctness and instruction adherence. The framework is applied to 31 leading LLMs across different evaluation settings: single-turn generation, multi-turn editing, and varying instruction counts. Results are correlated with LMArena Elo scores to assess alignment with human preference.

## Key Results
- Non-functional instructions cause notable functional regression across all evaluated models
- Following multiple instructions remains challenging, with top models achieving less than 50% success
- Single-turn generation better preserves functionality while multi-turn editing achieves higher instruction following
- A composite score of functional correctness and instruction following correlates best with human preference (LMArena Elo)

## Why This Works (Mechanism)
The framework works by creating a structured way to measure instruction following, which represents the non-functional expectations that humans value in code. By using deterministic verifiers for verifiable instructions, the evaluation becomes objective and reproducible. The correlation with LMArena Elo scores demonstrates that instruction following captures human preferences that traditional functional metrics miss. The multi-turn vs. single-turn distinction reveals different tradeoffs between preserving functionality and following instructions, which aligns with real-world coding workflows where users may prioritize different aspects depending on their needs.

## Foundational Learning

1. **Verifiable Instructions** - Code modifications that can be automatically checked for compliance
   *Why needed:* Enables objective evaluation of non-functional code qualities without human judgment
   *Quick check:* Can a computer deterministically verify the instruction was followed?

2. **Instruction Following vs. Functional Correctness** - Two distinct dimensions of code quality
   *Why needed:* Traditional metrics focus only on functional correctness, missing style, documentation, and structure
   *Quick check:* Does the code work AND meet non-functional requirements?

3. **Single-turn vs. Multi-turn Editing Tradeoffs** - Different approaches to code modification have distinct failure modes
   *Why needed:* Real-world coding involves both approaches, each with different strengths and weaknesses
   *Quick check:* Does the approach preserve functionality or improve instruction compliance?

4. **Human Preference Correlation** - Measuring alignment between automated metrics and human judgment
   *Why needed:* Validates that the evaluation framework captures what users actually value
   *Quick check:* Do models ranked highly by the framework also rank highly with human evaluators?

5. **Taxonomy Design** - Systematic categorization of code modification instructions
   *Why needed:* Provides comprehensive coverage of non-functional code aspects for evaluation
   *Quick check:* Are all major aspects of code quality represented in the instruction set?

## Architecture Onboarding

**Component Map:** User Task -> Instruction Generation -> Model Execution -> Verifier Check -> Score Calculation -> Human Preference Correlation

**Critical Path:** Task Input → Instruction Application → Code Generation → Functional Verification → Instruction Verification → Composite Scoring

**Design Tradeoffs:** Deterministic verifiers provide objectivity but may miss subjective quality aspects; comprehensive instruction taxonomy ensures coverage but may not capture all real-world scenarios; multi-turn evaluation enables complex modifications but increases failure risk

**Failure Signatures:** Functional regression when applying non-functional instructions; low multi-instruction success rates; poor correlation with human preference when instruction following is ignored

**3 First Experiments:**
1. Test VeriCode on a small set of diverse code samples to validate verifier accuracy
2. Compare single model's performance across single-turn vs. multi-turn settings
3. Evaluate correlation between instruction-following scores and human preference on a small sample

## Open Questions the Paper Calls Out
None

## Limitations
- VeriCode taxonomy may not capture all real-world programming scenarios and preferences
- Deterministic verifiers cannot evaluate subjective aspects like code style and documentation quality
- Evaluation focuses primarily on functional correctness and instruction following, potentially overlooking runtime efficiency and memory usage

## Confidence

**High Confidence:** The observation that non-functional instructions cause functional regression is well-supported by empirical evidence across multiple models.

**Medium Confidence:** The claim that instruction following is the primary differentiator on real-world programming tasks is supported by correlation with LMArena Elo scores, but may not fully capture all aspects of human preference.

**Medium Confidence:** The finding that multi-turn editing achieves higher instruction following while single-turn generation better preserves functionality is based on the specific benchmarks used and may vary with different evaluation criteria.

## Next Checks

1. Test the VeriCode framework on additional real-world codebases beyond the current benchmarks to validate its applicability to diverse programming contexts.

2. Incorporate subjective evaluation metrics for code style and documentation quality to complement the deterministic verifiers.

3. Evaluate the impact of instruction following on runtime efficiency and memory usage to provide a more comprehensive assessment of code quality.