---
ver: rpa2
title: 'SENSE: Self-Supervised Neural Embeddings for Spatial Ensembles'
arxiv_id: '2512.11145'
source_url: https://arxiv.org/abs/2512.11145
tags:
- loss
- clustering
- ensemble
- contrastive
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SENSE, a framework that enhances autoencoder-based
  dimensionality reduction for scientific ensemble datasets by incorporating clustering
  and contrastive losses into the latent space. The method uses EfficientNetV2 to
  generate pseudo-labels for partially unlabeled data, enabling semi-supervised training.
---

# SENSE: Self-Supervised Neural Embeddings for Spatial Ensembles

## Quick Facts
- arXiv ID: 2512.11145
- Source URL: https://arxiv.org/abs/2512.11145
- Reference count: 0
- Primary result: β-VAE models with clustering or contrastive loss marginally outperform standard AEs on scientific ensemble datasets.

## Executive Summary
This paper introduces SENSE, a framework that enhances autoencoder-based dimensionality reduction for scientific ensemble datasets by incorporating clustering and contrastive losses into the latent space. The method uses EfficientNetV2 to generate pseudo-labels for partially unlabeled data, enabling semi-supervised training. Experiments on two scientific ensemble datasets—Markov chain Monte Carlo (95K images, 5 classes) and droplet-on-film impact dynamics (135K images, 7 classes)—show that models with clustering or contrastive loss marginally outperform baseline autoencoders, with β-VAE models generally performing better than standard AEs.

## Method Summary
SENSE uses a semi-supervised approach where EfficientNetV2 is first trained on manually labeled subsets to generate pseudo-labels for unlabeled ensemble members. Autoencoders (AE) and β-variational autoencoders (β-VAE) are then trained with reconstruction, clustering (soft silhouette score), and/or contrastive objectives, encouraging compact and well-separated latent representations. The latent space is projected to 2D using UMAP for visualization and evaluation using silhouette scores. The framework was tested on two scientific ensemble datasets with varying complexity and class numbers.

## Key Results
- β-VAE models outperformed standard AEs on both datasets, with larger differences observed on the simpler MCMC dataset
- Models incorporating clustering or contrastive loss marginally outperformed baseline reconstruction-only approaches
- Clustering loss showed overfitting issues on complex datasets despite promising results on MNIST
- Overfitting occurred when clustering loss decreased on training data while test loss remained high

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-labels from a pretrained classifier enable semi-supervised training of clustering objectives when only partial ground truth is available.
- Mechanism: EfficientNetV2 is first trained on the manually labeled subset (2.5K images for MCMC, 7.2K for DD), achieving >95% test accuracy. This trained model then assigns pseudo-labels to unlabeled ensemble members, providing the class information required for contrastive and silhouette-based losses.
- Core assumption: The classifier generalizes sufficiently to unlabeled data; pseudo-labels are accurate enough to guide latent space organization without introducing significant label noise.
- Evidence anchors: [abstract] "EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets." [section 3.1] "EfficientNetV2 is an image classification convolutional network... a subset of 30K images for the MCMC ensemble will be used, and 26K images for the DD ensemble."

### Mechanism 2
- Claim: Joint optimization of reconstruction with clustering or contrastive losses encourages more separable latent clusters than reconstruction alone.
- Mechanism: The total loss L = L_rec + λ · L_cluster (or L_contrastive) balances fidelity to input data with explicit class-aware organization. Contrastive loss minimizes intra-class distances and enforces a margin between inter-class pairs; soft silhouette score directly optimizes cohesion vs. separation.
- Core assumption: The weighting coefficient λ is tuned so that clustering pressure does not overwhelm reconstruction, preserving feature expressiveness.
- Evidence anchors: [abstract] "models incorporating clustering or contrastive loss marginally outperform the baseline approaches." [section 4, results] "We notice that the best results are produced by the models with either clustering loss or contrastive loss, in this case primarily when dropout of 0.4 is applied."

### Mechanism 3
- Claim: β-VAE regularization produces more structured latent spaces than standard autoencoders, especially for moderately complex ensembles.
- Mechanism: The KL divergence term (scaled by β and normalized by latent/input dimensions) encourages a Gaussian prior, smoothing the latent manifold and reducing overfitting to spurious features.
- Core assumption: β is appropriately scaled for image dimensions; higher resolution images require larger β to maintain equivalent regularization strength.
- Evidence anchors: [abstract] "β-VAE models generally outperformed standard AEs, with better results observed on the MCMC dataset due to its lower complexity." [section 4] "We notice that the (β)-VAE outperform the AE models for both datasets. However, the difference between the two types of models is bigger for the MCMC dataset."

## Foundational Learning

- Concept: **Variational Autoencoders and KL Divergence**
  - Why needed here: β-VAE is the best-performing architecture; understanding the ELBO decomposition is required to tune β appropriately.
  - Quick check question: Can you explain why scaling KL divergence by image dimensions is necessary when comparing different-resolution datasets?

- Concept: **Contrastive Learning Basics**
  - Why needed here: The contrastive loss is the more stable alternative to clustering loss and requires understanding positive/negative pair sampling and margin thresholds.
  - Quick check question: What happens to gradient updates if the margin is set too small relative to typical inter-class distances?

- Concept: **Silhouette Score**
  - Why needed here: The clustering loss is a differentiable approximation; understanding the original metric clarifies what the soft version optimizes.
  - Quick check question: If all points have silhouette scores near 0, what does this imply about cluster structure?

## Architecture Onboarding

- Component map:
  - Images → EfficientNetV2 → Pseudo-labels → AE/β-VAE → Latent space → UMAP → 2D projection → Silhouette score

- Critical path:
  1. Normalize images to zero mean, unit variance.
  2. Train EfficientNetV2 on labeled subset; validate >95% accuracy before generating pseudo-labels.
  3. Train AE/β-VAE with joint loss for 100 epochs (monitor both reconstruction and auxiliary losses).
  4. Extract latent vectors from manually labeled test set; apply UMAP to 2D.
  5. Compute silhouette score on 2D projection for quantitative comparison.

- Design tradeoffs:
  - **Latent size**: 32 risks information loss for complex ensembles; 256 provides headroom but increases projection difficulty.
  - **Clustering vs. contrastive loss**: Clustering showed overfitting on DD; contrastive was more stable but provides weaker supervision signal.
  - **β scaling**: Must increase β proportionally to image dimensions (H×W in denominator of KL scaling factor).

- Failure signatures:
  - **Clustering loss overfitting**: Train silhouette loss decreases while test loss plateaus high—mitigate with dropout or switch to contrastive.
  - **Collapsed latent space (high β)**: UMAP projection shows scattered points with no class structure.
  - **Insufficient bottleneck (small latent)**: Projections collapse to near-1D structures; reconstruction quality may remain acceptable.

- First 3 experiments:
  1. Reproduce MNIST baseline with and without clustering loss to verify pipeline correctness; expect clear cluster separation.
  2. Train β-VAE with contrastive loss on MCMC using latent size 256, β=1, λ=0.2; compare silhouette score to AE baseline.
  3. Ablate λ values {0.1, 0.2, 0.3} on DD dataset to find stability threshold before clustering loss overfitting emerges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the overfitting of the soft silhouette clustering loss be mitigated for complex scientific ensemble datasets?
- Basis in paper: [explicit] The authors state: "Clustering loss showed promising results with the simple MNIST dataset. However, it ran into many issues with the scientific ensemble datasets. Some slight changes to the function might make it perform better."
- Why unresolved: Dropout (0.3–0.4), batch normalization, and early stopping were tried but did not resolve the overfitting; train loss decreased while test loss remained high for both MCMC and DD datasets.
- What evidence would resolve it: A modified differentiable clustering loss formulation or regularization technique that achieves convergent train and validation silhouette losses on the scientific ensembles.

### Open Question 2
- Question: Can combining clustering and contrastive losses with reconstruction yield superior latent representations compared to using either alone?
- Basis in paper: [explicit] The authors note: "Because both loss functions show promising results, it could be interesting to couple them and perform further optimization."
- Why unresolved: Only individual combinations (reconstruction + clustering) or (reconstruction + contrastive) were evaluated; a joint optimization of all three was not tested.
- What evidence would resolve it: Comparative silhouette scores on MCMC and DD showing statistically significant improvement for models trained with all three losses versus pairwise combinations.

### Open Question 3
- Question: How can the method be improved to better handle high-complexity ensembles with many classes like Drop Dynamics (7 classes)?
- Basis in paper: [explicit] The authors state: "In this project, it was difficult to significantly improve the results of the drop dynamics ensemble dataset because of its complicated nature. More research should be done to improve upon this."
- Why unresolved: Results on DD were substantially worse than MCMC; models struggled to form cohesive clusters even with contrastive loss, and improvements over baseline were negligible.
- What evidence would resolve it: An approach achieving silhouette scores on DD comparable to those on MCMC (e.g., closing the gap observed between Tables 2–3 vs. Tables 4–5).

## Limitations
- Reliance on pseudo-labels introduces potential label noise that is not quantified in the study
- Marginal performance gains suggest benefits may not generalize to all ensemble types
- Overfitting to pseudo-labels observed for clustering loss on complex datasets indicates sensitivity to ensemble complexity

## Confidence
- **High confidence** in the core mechanism: Pseudo-label generation + joint reconstruction + auxiliary loss training is sound and reproducible
- **Medium confidence** in the empirical findings: Marginal gains are observed, but the dataset diversity and ablation studies are limited
- **Low confidence** in the scalability claims: Overfitting issues on complex datasets suggest the method may not generalize well without careful tuning

## Next Checks
1. **Ablation study on pseudo-label quality**: Retrain with noisy pseudo-labels (simulate classifier errors) to quantify sensitivity and identify overfitting thresholds
2. **Comparison on higher-dimensional latent spaces**: Evaluate clustering quality directly in latent space (e.g., using adjusted Rand index on labeled test data) rather than relying solely on 2D UMAP projections
3. **Test on a third, structurally different ensemble**: Apply the method to a dataset with different image sizes, class numbers, or complexity (e.g., climate model ensembles) to assess generalizability beyond MCMC and DD