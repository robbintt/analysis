---
ver: rpa2
title: Convergence Analysis of SGD under Expected Smoothness
arxiv_id: '2510.20608'
source_url: https://arxiv.org/abs/2510.20608
tags:
- gradient
- stochastic
- variance
- convergence
- expected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive convergence analysis of stochastic
  gradient descent (SGD) under the expected smoothness (ES) condition. The ES framework
  provides a flexible alternative to classical assumptions like bounded variance by
  tying the second moment of stochastic gradients to objective value and full gradient
  norm.
---

# Convergence Analysis of SGD under Expected Smoothness

## Quick Facts
- arXiv ID: 2510.20608
- Source URL: https://arxiv.org/abs/2510.20608
- Reference count: 40
- This paper presents comprehensive convergence analysis of stochastic gradient descent (SGD) under expected smoothness (ES) condition.

## Executive Summary
This paper analyzes stochastic gradient descent (SGD) convergence under the expected smoothness (ES) framework, which provides a flexible alternative to classical assumptions by tying stochastic gradient variance to objective value and full gradient norm. The authors derive convergence bounds for various step-size schedules and establish explicit formulas for ES constants under different sampling strategies. Empirical validation on CIFAR-100 with ResNet-18 demonstrates that the ES model accurately captures stochastic gradient variance during training, revealing a two-phase structure where variance transitions from being dominated by suboptimality to being dominated by local geometry.

## Method Summary
The method analyzes SGD optimization under the Expected Smoothness (ES) condition, which bounds the second moment of stochastic gradients by a linear combination of function suboptimality, full gradient norm, and heterogeneity. The paper derives convergence rates for various step-size schedules (constant, harmonic, polynomial, cosine annealing) and provides explicit formulas for ES constants under different sampling strategies including independent sampling, uniform sampling, τ-Nice sampling, and importance sampling. The analysis is validated on CIFAR-100 using ResNet-18 architecture, tracking ES coefficient evolution during training to demonstrate the two-phase noise structure.

## Key Results
- Establishes O(1/K) convergence rates for SGD under ES with explicit residual errors dependent on step-size schedules
- Provides closed-form expressions for ES constants (A, B, C) under common sampling strategies (with/without replacement, importance sampling, mini-batching)
- Demonstrates that increasing batch size τ reduces effective variance constants by factor 1/τ, enabling proportional learning rate increases
- Empirically validates ES model captures stochastic gradient variance during training, revealing two-phase structure where variance transitions from suboptimality-dominated to geometry-dominated regimes

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Noise Modeling via Expected Smoothness (ES)
The ES condition posits that stochastic gradient variance scales with suboptimality and local gradient norm rather than being constant. As training progresses and suboptimality decreases, the "noise" naturally decreases, preventing variance from overwhelming descent direction in later iterations.

### Mechanism 2: Scheduler-Aware Stability Constraints
Convergence is guaranteed when step size η_k respects stability threshold imposed by ES constant B and smoothness L. The coefficient (1 - LBη_k/2) must remain positive, ensuring accumulated variance error is outweighed by functional decrease over K iterations.

### Mechanism 3: Batch-Size Mediated Variance Reduction
Increasing batch size τ reduces effective variance constants (A and C) by factor 1/τ, directly enabling proportional increases in learning rate without loss of stability. This formally justifies the "linear scaling rule" used in practice.

## Foundational Learning

- **Concept: Lipschitz Smoothness (L-smoothness)**
  - **Why needed here:** The entire derivation relies on the Descent Lemma, which bounds the error of a linear approximation using Lipschitz constant L. Without L-smoothness, we cannot guarantee gradient step decreases function value.
  - **Quick check question:** Can you explain why a function satisfying ||∇f(x) - ∇f(y)|| ≤ L||x-y|| guarantees f(y) ≤ f(x) + ⟨∇f(x), y-x⟩ + (L/2)||y-x||²?

- **Concept: Variance Decomposition**
  - **Why needed here:** The paper decomposes second moment E||g_v||² into squared norm of true gradient ||∇f||² plus variance. This identity is crucial for mapping bounded variance assumptions into the unified ABC/ES framework.
  - **Quick check question:** If g is an unbiased estimator of ∇f, how does E||g||² relate to the variance of g and the norm of ∇f?

- **Concept: Non-Asymptotic Convergence Rates (O(1/K))**
  - **Why needed here:** The paper evaluates optimizers based on how fast the minimum expected gradient norm decays over finite steps K, rather than asymptotic limits. Understanding the difference between O(1/K), O(log K/K), and residual errors is necessary to interpret convergence bounds.
  - **Quick check question:** Why does a "constant step size" schedule result in convergence bound with non-zero residual error that doesn't vanish as K → ∞?

## Architecture Onboarding

- **Component map:** Objective (finite-sum problem) -> Sampler (generating stochastic gradients) -> ES Constants (metadata capturing problem geometry, gradient correlation, data heterogeneity) -> Optimizer (SGD logic with scheduler and batch size)

- **Critical path:**
  1. Estimate Constants: Determine smoothness L and estimate heterogeneity Δ_inf to derive A, B, C
  2. Set Max LR: Use 2/(LB) as hard ceiling for any learning rate scheduler
  3. Select Scheduler: Choose Constant (for speed/residual), Harmonic (for logs), or Cosine (for cyclical/annealing)
  4. Monitor Phase: Track if noise is dominated by suboptimality (A-phase) or geometry (B-phase) to adjust batch/step strategy

- **Design tradeoffs:**
  - Constant vs. Decaying LR: Constant η is faster initially but hits "noise floor" (residual error). Decaying η converges to true minimum but is slower early on.
  - Batch Size vs. LR: Increasing batch size τ is computationally expensive but reduces noise (C/τ), allowing higher LR and potentially faster wall-clock convergence.

- **Failure signatures:**
  - Divergence: Learning rate exceeds 2/(LB). Gradient norms explode rather than decay.
  - Stagnation (High Residual): Training plateaus early. Constant term C is high with constant LR; switch to decaying scheduler.
  - Slow Convergence: LR is too conservative relative to batch size. If using large batches, increase LR linearly to utilize reduced variance.

- **First 3 experiments:**
  1. ES Model Fitting: Measure E||∇f_v||² vs. (f-f*) and ||∇f||² during training. Fit linear model to estimate empirical A, B, C and validate two-phase structure.
  2. Scheduler Baseline: Implement Constant, Harmonic, and Cosine schedulers with theoretical max LR (≈ 2/(LB)). Plot min ||∇f||² vs. K to verify rate differences.
  3. Scaling Law Test: Run SGD with Batch=64, LR=η vs. Batch=256, LR=4η. Verify if convergence curves align, confirming variance-reduction mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Expected Smoothness (ES) framework be extended to provide convergence guarantees for adaptive gradient methods like Adam or RMSProp? The introduction notes that while adaptive methods have expanded scope of stochastic optimization, their convergence properties are less well understood. The current analysis focuses on vanilla SGD where update is x_{k+1} = x_k - η_k g_k. Adaptive methods modify this with preconditioners based on past gradient history, complicating variance analysis under ES.

### Open Question 2
Can the empirically observed two-phase structure of stochastic noise be explicitly exploited to design adaptive step-size or batch-size schedulers? Section 4 identifies transition from "suboptimality-dominated" to "geometry-dominated" noise regimes, but the paper does not propose algorithm that detects or utilizes this shift. While the paper validates ES model describes this behavior, it is unclear how to theoretically detect transition point during training without access to true gradient or optimal value.

### Open Question 3
How should one design optimal sampling probabilities q_i based on explicit ES constants derived in Proposition 1 to minimize convergence error? Proposition 1 provides explicit formulas for constants A and C (e.g., A = max_i (L_i/(τnq_i))), suggesting sampling probabilities directly impact convergence bound. The paper derives constants for fixed strategies but does not solve optimization problem of selecting q_i to minimize resulting error bounds.

## Limitations
- Analysis assumes Lipschitz smoothness and unbiased stochastic gradients which may not hold for all modern deep learning architectures
- ES condition requires careful estimation of constants A, B, and C that may be sensitive to data distribution shifts
- Convergence guarantees are asymptotic and may not fully capture finite-time behavior in highly nonconvex landscapes

## Confidence
**High Confidence**: The mathematical derivation of convergence rates under ES conditions is rigorous and follows established Lyapunov analysis techniques. The relationship between batch size and learning rate scaling (linear scaling rule) is well-supported by both theory and prior empirical work.

**Medium Confidence**: The two-phase structure observed in ES coefficient evolution during training is compelling but requires further validation across different architectures, datasets, and optimization scenarios. The practical implications for step-size and batch-size selection are promising but need more extensive empirical testing.

**Low Confidence**: The estimation procedure for ES constants in practice may be sensitive to implementation details and could vary significantly across different problem domains. The assumption that ES constants remain stable throughout training may not hold in all scenarios.

## Next Checks
1. **Cross-architecture validation**: Test ES coefficient estimation and two-phase structure on architectures beyond ResNet-18 (e.g., Vision Transformers, language models) to assess generalizability.

2. **Robustness to initialization**: Evaluate whether estimated ES constants and observed two-phase structure are consistent across different weight initializations and learning rate warm-up strategies.

3. **Real-time monitoring framework**: Implement a lightweight system to track ES coefficients during training and develop early-warning indicators for when optimization dynamics shift between A-dominated and B-dominated phases.