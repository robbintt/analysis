---
ver: rpa2
title: Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate Rollout
arxiv_id: '2503.10048'
source_url: https://arxiv.org/abs/2503.10048
tags:
- hyper
- error
- rollout
- neural
- simulator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HyPER, a model-agnostic framework that combines
  neural surrogates, reinforcement learning (RL), and physics simulators to reduce
  rollout errors in modeling transient physical systems governed by PDEs. Unlike prior
  approaches requiring differentiable simulators, HyPER works with any simulator and
  learns when to invoke it for "knowledge-guided correction." The method uses an RL
  policy to decide between surrogate predictions and simulator calls, optimizing for
  accuracy while controlling computational cost.
---

# Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate Rollout

## Quick Facts
- arXiv ID: 2503.10048
- Source URL: https://arxiv.org/abs/2503.10048
- Authors: Bharat Srikishan; Daniel O'Malley; Mohamed Mehana; Nicholas Lubbers; Nikhil Muralidhar
- Reference count: 32
- Primary result: HyPER reduces cumulative rollout error by 47%-78% compared to surrogate-only methods while maintaining computational efficiency

## Executive Summary
This paper introduces HyPER, a model-agnostic framework that combines neural surrogates with physics simulators using reinforcement learning to reduce rollout errors in modeling transient physical systems governed by PDEs. Unlike prior approaches requiring differentiable simulators, HyPER works with any simulator and learns when to invoke it for "knowledge-guided correction." The method uses an RL policy to decide between surrogate predictions and simulator calls, optimizing for accuracy while controlling computational cost. Experiments on 2D Navier-Stokes and subsurface flow datasets show HyPER significantly outperforms state-of-the-art surrogate-only methods, achieving 47%-78% reduction in cumulative rollout error.

## Method Summary
HyPER addresses the challenge of accumulated error in autoregressive neural surrogate rollouts by training an RL policy to decide when to invoke a physics simulator for correction. The method involves pretraining a neural surrogate (UNet) on supervised data, then training an RL decision policy (ResNet-based) to select between surrogate predictions and simulator calls at each timestep. The policy is trained using REINFORCE with a reward function that balances accuracy (MSE) against computational cost (simulator call budget λ). At inference, the policy determines when to call the simulator, which provides ground-truth corrections that reset the trajectory state and prevent error propagation. The framework is validated on Navier-Stokes and subsurface flow datasets, demonstrating significant error reduction while maintaining computational efficiency through intelligent simulator invocation.

## Key Results
- HyPER achieves 47%-78% reduction in cumulative rollout error compared to state-of-the-art surrogate-only methods
- The framework demonstrates adaptability to changing physical conditions, with HyPER outperforming retraining-only baselines
- HyPER maintains computational efficiency through intelligent simulator invocation, achieving lower error-per-unit-time than baselines
- The method shows robustness to noisy data and works effectively across different neural architectures and physics simulators

## Why This Works (Mechanism)

### Mechanism 1
Selective simulator invocation interrupts error accumulation in autoregressive surrogate rollouts. The RL policy learns to identify high-error regimes and invokes the physics simulator to reset the trajectory state to ground truth, preventing compounding errors from propagating forward. Each simulator call provides a "knowledge-guided correction" that anchors the rollout. Core assumption: The simulator produces significantly more accurate single-step predictions than the surrogate, and error accumulates nonlinearly when surrogates are chained autoregressively.

### Mechanism 2
Cost-aware reward shaping enables graceful accuracy-efficiency tradeoffs without requiring differentiable simulators. The reward function combines negative MSE with a cost penalty C(a, λ, T) that penalizes deviation from a user-specified simulator call budget λ. This allows practitioners to tune the accuracy-cost operating point explicitly rather than through implicit architectural choices. Core assumption: The λ parameter meaningfully correlates with wall-clock cost, and the relationship between simulator calls and accuracy is approximately monotonic within reasonable ranges.

### Mechanism 3
Baseline subtraction in the REINFORCE gradient reduces variance and stabilizes policy learning under sparse, delayed rewards. The baseline function b computes MSE from randomly calling the simulator the same number of times as the current policy. Subtracting this baseline from rewards centers the gradient signal, helping the policy learn where to call the simulator rather than just how often. Core assumption: Random simulator placement provides a meaningful counterfactual; the policy can learn timestep-specific value from trajectory-level rewards.

## Foundational Learning

- **Concept: Autoregressive rollout error**
  - Why needed here: HyPER's core purpose is reducing cumulative error when neural surrogates predict sequential timesteps. Understanding that errors compound nonlinearly is essential.
  - Quick check question: If a surrogate has 1% per-step MSE, what approximate error would you expect after 20 autoregressive steps? (Answer: typically >>20%, due to error compounding)

- **Concept: Policy gradient methods (REINFORCE)**
  - Why needed here: HyPER uses REINFORCE to train the decision policy. Understanding that policy gradients optimize expected reward through log-probability weighting is necessary.
  - Quick check question: Why does REINFORCE require sampling actions rather than taking the argmax during training? (Answer: exploration is needed to estimate gradient)

- **Concept: Hybrid physics-ML modeling**
  - Why needed here: HyPER belongs to a class of methods combining data-driven models with physics simulators. Understanding the spectrum helps contextualize HyPER's design choice.
  - Quick check question: Why might a non-differentiable simulator be preferable to incorporate versus a differentiable one? (Answer: legacy codes, proprietary solvers, or availability)

## Architecture Onboarding

- **Component map:** Surrogate fϕ (pretrained UNet) -> Decision Policy πθ (RL-trained ResNet) -> Simulator S (black-box PDE solver) -> Reward computation (MSE + cost penalty)
- **Critical path:** 1) Train surrogate on 400-800 trajectories (supervised, MSE loss) 2) Train RL policy on separate 400 trajectories using Algorithm 1 (30 epochs) 3) At inference, policy decides per-timestep; simulator calls require ground-truth boundary condition handling
- **Design tradeoffs:** λ (simulator budget): Higher λ = more accuracy, more cost. Paper uses 0.3. Surrogate architecture: UNet chosen over FNO and UNet-Multistep. RL training data: Must be separate from surrogate training.
- **Failure signatures:** Policy collapses to always/never call simulator: check λ setting and reward scaling. High variance in trajectory outcomes: increase baseline effectiveness or training data. Poor generalization to new boundary conditions: policy may need retraining.
- **First 3 experiments:** 1) Reproduce in-distribution results: Train UNet surrogate, train RL policy with λ=0.3, verify ~68% cumulative MSE reduction. 2) Ablate baseline subtraction: Train policy without baseline, compare win rate against random policy. 3) Test simulator-budget sweep: Vary λ ∈ {0.1, 0.3, 0.5, 0.7} and plot cumulative MSE vs wall-clock time.

## Open Questions the Paper Calls Out

- Can more sophisticated actor-critic RL policies improve the sample efficiency of HyPER compared to the current REINFORCE implementation?
- How does HyPER perform when extended to complex multi-physics contexts and multi-phase flow scenarios?
- Does the HyPER framework scale effectively to 3D spatial domains without prohibitive computational costs?

## Limitations

- HyPER requires retraining the policy when physical conditions change, limiting adaptability without additional data
- The cost model assumes uniform simulator expense, which may not hold in practice with variable computational loads
- Effectiveness of baseline subtraction technique is not extensively validated across different environments

## Confidence

- **High Confidence:** Empirical results demonstrating HyPER's effectiveness on Navier-Stokes and subsurface flow datasets
- **Medium Confidence:** Theoretical underpinnings of cost-aware reward shaping and baseline subtraction
- **Low Confidence:** Claims about generalizability to unseen physical conditions and performance with alternative surrogate architectures

## Next Checks

1. **Cross-Architecture Validation:** Test HyPER with surrogate architectures other than UNet (e.g., Fourier Neural Operators) to assess surrogate-agnostic claims
2. **Generalization to New PDEs:** Apply HyPER to a different PDE system (e.g., shallow water equations) to evaluate adaptability to unseen physical conditions
3. **Cost Model Sensitivity:** Investigate impact of non-uniform simulator costs by introducing variable computational loads across timesteps and measuring effect on accuracy-cost tradeoffs