---
ver: rpa2
title: 'ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question
  Answering'
arxiv_id: '2506.00232'
source_url: https://arxiv.org/abs/2506.00232
tags:
- question
- answer
- reasoning
- retrieval
- multi-hop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ComposeRAG introduces a modular and composable retrieval-augmented\
  \ generation (RAG) framework designed to address the limitations of monolithic multi-hop\
  \ question answering systems. It decomposes complex reasoning into atomic, parameterized\
  \ modules\u2014such as question decomposition, retrieval decision, and answer verification\u2014\
  allowing independent implementation, targeted upgrades, and transparent analysis."
---

# ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering

## Quick Facts
- **arXiv ID:** 2506.00232
- **Source URL:** https://arxiv.org/abs/2506.00232
- **Reference count:** 40
- **One-line primary result:** ComposeRAG consistently outperforms strong baselines in multi-hop QA accuracy and grounding fidelity across four benchmarks, achieving up to 15% accuracy improvement over fine-tuning-based methods.

## Executive Summary
ComposeRAG introduces a modular and composable retrieval-augmented generation framework designed to address the limitations of monolithic multi-hop question answering systems. It decomposes complex reasoning into atomic, parameterized modules—such as question decomposition, retrieval decision, and answer verification—allowing independent implementation, targeted upgrades, and transparent analysis. To enhance robustness, the framework incorporates a self-reflection mechanism that iteratively revisits and refines earlier steps upon verification failure. Evaluated on four multi-hop QA benchmarks (HotpotQA, 2WikiMultiHopQA, MuSiQue, Bamboogle), ComposeRAG consistently outperforms strong baselines in both accuracy and grounding fidelity.

## Method Summary
ComposeRAG employs a modular, prompt-based pipeline architecture for corpus-grounded multi-hop question answering. The system decomposes complex reasoning into atomic, parameterized modules including Question Decomposition, Retrieval Decision, Passage Reranking, Answer Generation, Answer Verification, and Self-Reflection. The pipeline operates iteratively: it verifies the final answer, analyzes errors upon failure, re-decomposes the question, and re-runs the process. A "Simple QA" bypass allows early exit when retrieval is deemed unnecessary or verification passes. The framework uses English Wikipedia as the knowledge corpus, with embeddings indexed via FAISS or Cortex Search. No fine-tuning is performed; instead, the approach relies on carefully engineered prompts for each module, evaluated on metrics like Cover Exact Match and LLM-based judgment.

## Key Results
- Achieves up to 15% accuracy improvement over fine-tuning-based methods and up to 5% gain over reasoning-specialized pipelines under identical retrieval conditions.
- Reduces ungrounded answers by over 10% in low-quality retrieval settings and by approximately 3% even with strong corpora through verification-first design.
- Comprehensive ablation studies validate the modular architecture, demonstrating distinct and additive contributions from each component, while highlighting scalability across model sizes and improved efficiency through early exits and conditional retrieval.

## Why This Works (Mechanism)
ComposeRAG's effectiveness stems from its modular decomposition of complex multi-hop reasoning into atomic, independently upgradable components. By parameterizing each module (question decomposition, retrieval decision, verification, etc.), the system enables targeted improvements and transparent analysis. The self-reflection mechanism enhances robustness by iteratively revisiting and refining earlier steps upon verification failure, preventing error propagation. The verification-first design ensures factual grounding, significantly reducing ungrounded answers—by over 10% in low-quality retrieval settings and by approximately 3% even with strong corpora. This modular, iterative approach allows for both interpretability and performance gains compared to monolithic baselines.

## Foundational Learning
- **Multi-Hop Question Answering:** Requires combining information from multiple documents/documents to answer complex questions. *Why needed:* This is the core task ComposeRAG addresses, requiring integration of evidence across passages.
- **Retrieval-Augmented Generation (RAG):** Combines information retrieval with text generation to ground answers in external corpora. *Why needed:* ComposeRAG builds on RAG principles but adds modularity and verification for multi-hop reasoning.
- **Modular Decomposition:** Breaking down complex processes into independent, interchangeable components. *Why needed:* Enables targeted upgrades, transparent analysis, and easier debugging compared to monolithic systems.
- **Self-Reflection Mechanism:** Iterative process that revisits and refines earlier steps upon failure. *Why needed:* Enhances robustness by preventing error propagation and improving answer quality through multiple reasoning passes.
- **Answer Verification:** Checking whether generated answers are factually grounded in retrieved evidence. *Why needed:* Critical for reducing ungrounded responses and ensuring factual accuracy in corpus-grounded QA.
- **Prompt Engineering:** Designing effective instructions for language models to perform specific tasks. *Why needed:* ComposeRAG relies entirely on prompt-based modules without fine-tuning, making prompt quality crucial.

## Architecture Onboarding

**Component Map:** Question Decomposition -> Retrieval Decision -> Passage Reranking -> Answer Generation -> Answer Verification -> (Conditional) Self-Reflection

**Critical Path:** The core pipeline flows through question decomposition, retrieval decision, passage reranking, answer generation, and verification. The self-reflection loop activates only when verification fails, creating a conditional path for iterative refinement.

**Design Tradeoffs:** The modular architecture trades computational efficiency for interpretability and targeted upgradeability. Each module adds latency but enables independent improvement and transparent error analysis. The self-reflection mechanism improves accuracy but may introduce reasoning drift in extended iterations.

**Failure Signatures:** Parsing errors in reranking module (strict output formatting required), infinite reflection loops from oscillating decompositions, premature "Simple QA" exits from malfunctioning decision modules, and reasoning drift in extended self-reflection cycles.

**Three First Experiments:**
1. Verify initial retrieval count impact by testing with k=10, k=20, and k=50 on downstream accuracy and grounding fidelity.
2. Systematically ablate the self-reflection mechanism by comparing performance with reflection steps disabled versus enabled, measuring accuracy gains and computational overhead.
3. Test the "Simple QA" bypass module's reliability by measuring activation rate on held-out multi-hop QA data and validating against claimed 85% classification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ComposeRAG's modular pipeline be adapted for low-latency or real-time applications without sacrificing reasoning quality?
- Basis in paper: [explicit] The authors state in the Limitations section that reliance on large instruction-tuned LLMs "introduces latency and resource overhead" and suggest future work should explore "lighter-weight alternatives or dynamic adaptation strategies."
- Why unresolved: The current framework relies on multiple sequential LLM calls (decomposition, verification, etc.), making it computationally expensive compared to single-pass baselines.
- What evidence would resolve it: A study evaluating the performance-latency trade-off when substituting specific modules with distilled models or non-LLM heuristics, demonstrating viable operation in resource-constrained environments.

### Open Question 2
- Question: Can the question decomposition module be enhanced to handle questions with subtle logic or ambiguous structures that currently cause failures?
- Basis in paper: [explicit] The authors acknowledge that "decomposition may still fail" when questions exhibit "subtle logic or ambiguous structure," even when the self-reflection mechanism is active.
- Why unresolved: The current decomposition strategy appears to rely heavily on linear sequential prompting, which may struggle with complex logical dependencies or semantic ambiguities inherent in difficult multi-hop queries.
- What evidence would resolve it: Analysis of failure cases on datasets with high semantic ambiguity (e.g., specific subsets of MuSiQue) showing improved success rates using non-linear or graph-based decomposition strategies.

### Open Question 3
- Question: How can the system detect and mitigate "reasoning drift" during extended self-reflection loops?
- Basis in paper: [inferred] The paper notes a sharp performance decline for outputs exceeding 6000 tokens, attributing it to "reasoning drift" during prolonged iterative processing, yet relies only on a maximum step count to terminate.
- Why unresolved: The paper does not propose a mechanism to dynamically detect when the iterative refinement process ceases to be constructive and begins to degrade the reasoning trace.
- What evidence would resolve it: Introduction of a semantic consistency metric or a "drift detection" module that triggers early stopping, resulting in higher accuracy in the >6000 token output range.

### Open Question 4
- Question: Is it possible to improve answer coverage (reducing "I don't know" responses) for valid questions where retrieval fails to surface explicit evidence?
- Basis in paper: [inferred] Section 5.2.2 highlights that ComposeRAG sacrifices approximately 2.8% coverage compared to Search-o1 because it refuses to answer without explicit grounding, even if the retrieved context was insufficient.
- Why unresolved: The strict verification module cannot currently distinguish between "the answer does not exist in the corpus" and "the retriever failed to find the existing answer," leading to unnecessary abstention.
- What evidence would resolve it: A mechanism that allows for low-confidence answers or triggers secondary retrieval strategies when verification fails due to insufficient context, improving Cover-EM scores on sparse corpora.

## Limitations
- The framework relies on large instruction-tuned LLMs, introducing latency and resource overhead that limits real-time applications.
- Question decomposition may still fail on questions with subtle logic or ambiguous structures, even with self-reflection enabled.
- The system cannot distinguish between absent answers and retrieval failures, leading to unnecessary "I don't know" responses and reduced coverage.

## Confidence
- **High confidence** in the modular decomposition approach and its benefits for interpretability and targeted upgrades.
- **Medium confidence** in the reported performance improvements, as reproduction requires careful prompt engineering and may be sensitive to implementation details.
- **Low confidence** in the scalability claims beyond the tested model sizes (8B to 72B parameters), as larger or smaller models may behave differently.

## Next Checks
1. Verify the initial retrieval count by testing with k=10, k=20, and k=50 to establish its impact on downstream accuracy and grounding fidelity, particularly when paired with the reranking module.
2. Implement a systematic ablation of the self-reflection mechanism by comparing performance with reflection steps disabled versus enabled, measuring both accuracy gains and additional computational overhead.
3. Test the "Simple QA" bypass module's reliability by measuring its activation rate on a held-out multi-hop QA dataset, and validate whether its classification accuracy meets the 85% threshold claimed in the paper.