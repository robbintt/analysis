---
ver: rpa2
title: 'Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety'
arxiv_id: '2509.21782'
source_url: https://arxiv.org/abs/2509.21782
tags:
- robustness
- reasoning
- page
- text
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WebRSSBench, a comprehensive benchmark for
  evaluating multimodal large language models (MLLMs) on web understanding tasks across
  reasoning, robustness, and safety dimensions. The benchmark includes 729 websites
  and 3,799 question-answer pairs covering eight tasks such as position relationship
  reasoning, form filling, and safety-critical detection.
---

# Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety

## Quick Facts
- arXiv ID: 2509.21782
- Source URL: https://arxiv.org/abs/2509.21782
- Authors: Junliang Liu; Jingyu Xiao; Wenxin Tang; Wenxuan Wang; Zhixian Wang; Minrui Zhang; Shuanghe Yu
- Reference count: 40
- Primary result: MLLMs show significant gaps in web reasoning, robustness to perturbations, and safety detection, with LoRA fine-tuning improving position reasoning from 16.3% to 41.3% accuracy.

## Executive Summary
This paper introduces WebRSSBench, a comprehensive benchmark for evaluating multimodal large language models (MLLMs) on web understanding tasks. The benchmark includes 729 real-world websites and 3,799 question-answer pairs covering eight tasks across reasoning, robustness, and safety dimensions. The authors evaluate 12 state-of-the-art MLLMs and reveal significant performance gaps, particularly in compositional reasoning and robustness to adversarial perturbations. Notably, closed-source models outperform open-source ones in safety tasks, while reasoning tasks remain the most challenging. The study demonstrates that LoRA-based fine-tuning substantially improves performance in specific tasks like position reasoning.

## Method Summary
The benchmark uses real-world websites (729 total) with synthetically generated perturbations for robustness testing. MLLMs process screenshots and answer questions about web content, structure, and safety. Evaluation metrics include accuracy for position reasoning, semantic similarity for form filling, TF-IDF cosine similarity for layout robustness, and recall for safety detection. The authors employ LoRA fine-tuning on Qwen2.5-VL-7B with specific hyperparameters (r=16, α=32, dropout=0.05) to improve performance on targeted tasks. Perturbations include color shifts, character noise injection, and layout rearrangements to test model robustness.

## Key Results
- MLLMs show significant reasoning gaps: Position reasoning accuracy starts at only 16.3% for Qwen2.5-VL-7B
- LoRA fine-tuning substantially improves position reasoning accuracy from 16.3% to 41.3%
- Closed-source models significantly outperform open-source models in safety-critical detection tasks
- Three systematic vulnerabilities identified: over-reliance on visual salience, character-level brittleness, and attention bias toward local regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a model relies on visual heuristics (e.g., color salience) rather than structural logic, targeted perturbations will severely degrade performance, revealing a lack of true reasoning.
- **Mechanism:** The benchmark applies non-semantic perturbations—such as shifting button colors or injecting character noise (e.g., "o" → "0")—that alter surface features while preserving functional intent. This forces the model to rely on context and layout hierarchy. A drop in performance indicates the model was using the perturbed feature as a shortcut.
- **Core assumption:** Models often learn statistical correlations between visual attributes (like "Blue = Primary Button") rather than understanding the underlying DOM structure or functional role.
- **Evidence anchors:**
  - [Abstract] "models... show limited robustness when facing perturbations in user interfaces and content such as layout rearrangements or visual style shifts."
  - [Section 5.3] "highly saturated or visually striking colors dominate attention, causing predictions to rely on color blocks rather than textual or structural cues."
  - [Corpus] The neighbor paper *RiOSWorld* supports this vulnerability context, noting risks in "multimodal computer-use agents" where visual inputs can be manipulated.
- **Break condition:** If a model maintains high accuracy post-perturbation, it suggests robust feature disentanglement or an over-reliance on OCR/text, breaking the "visual salience" hypothesis.

### Mechanism 2
- **Claim:** Supervised fine-tuning with Low-Rank Adaptation (LoRA) significantly improves spatial reasoning by adapting attention mechanisms to prioritize coordinate relationships over local textures.
- **Mechanism:** By training on specific "position relationship" tasks (e.g., determining if element A is top-left of B), the model learns to encode global spatial coordinates. The paper demonstrates that even a smaller model (Qwen2.5-VL-7B) can boost position reasoning accuracy from 16.3% to 41.3% via this adaptation.
- **Core assumption:** The model's pre-existing vision encoder is capable of extracting spatial data, but the language model head lacks the specific alignment to verbalize these relationships without fine-tuning.
- **Evidence anchors:**
  - [Abstract] "LoRA-based fine-tuning substantially improves performance, particularly in position reasoning accuracy (16.3% → 41.3%)."
  - [Section 5.2] "...targeted LoRA-based fine-tuning yields consistent and notable gains... indicating that targeted supervision allowed the model to capture structural layout patterns."
  - [Corpus] Corpus signals are limited on specific *web* spatial reasoning mechanisms, but generic MLLM tuning literature supports the efficacy of PEFT (Parameter-Efficient Fine-Tuning).
- **Break condition:** Performance gains fail to materialize if the base model's vision encoder resolution is too low to distinguish relative positions in complex layouts (the "fragility" noted in Section 4.3).

### Mechanism 3
- **Claim:** Safety performance relies on the model's ability to detect semantic boundaries of "irreversibility," a capability currently correlated with model scale and proprietary alignment data.
- **Mechanism:** The "Safety Critical Detection" task requires identifying actions like "Delete Account" or "Payment." The superior performance of closed-source models (e.g., GPT-5, Gemini 2.5-Pro) suggests that safety behaviors emerge from extensive alignment or RLHF (Reinforcement Learning from Human Feedback) specifically designed to recognize hazardous UI patterns.
- **Core assumption:** Recognizing safety risks in UI requires a higher level of semantic abstraction (understanding consequence) than simple OCR or element grounding.
- **Evidence anchors:**
  - [Abstract] "Notably, closed-source models outperform open-source ones, especially in safety tasks."
  - [Section 3.3] "These elements require careful verification before execution... [models] must exhibit sufficient sensitivity to detect these safety-critical actions."
  - [Corpus] *Towards Harmless Multimodal Assistants* supports the difficulty of this mechanism, noting the need for "blind preference optimization" to align MLLMs effectively.
- **Break condition:** If an open-source model fine-tuned solely on reasoning tasks fails to improve safety scores, it confirms the paper's implicit hypothesis that safety is a distinct capability requiring specific alignment, not just better reasoning.

## Foundational Learning

- **Concept: Compositional Reasoning vs. Perception**
  - **Why needed here:** The paper distinguishes between seeing a button (perception) and understanding its function relative to others (reasoning). You must understand that "Web Understanding" requires multi-step inference over the DOM hierarchy.
  - **Quick check question:** Can you explain why identifying a "Blue Button" is perception, while determining it is the "Primary CTA" is reasoning?

- **Concept: Adversarial Perturbation (Pre/Post Design)**
  - **Why needed here:** The robustness evaluation uses a contrastive method (original vs. perturbed). You need to grasp how to measure "semantic consistency" when the visual input changes but the ground truth does not.
  - **Quick check question:** If you change a button's label from "Submit" to "Subm!t", should the model's prediction of the button's function change? Why or why not?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** The study uses LoRA to fix reasoning gaps. Understanding how small adapter modules modify a frozen LLM is crucial for replicating the paper's improvements.
  - **Quick check question:** Why would the authors choose LoRA (r=16) over full fine-tuning for a 7B parameter model on a specific benchmark?

## Architecture Onboarding

- **Component map:** Data Source -> Perturbation Engine -> Evaluator
- **Critical path:**
  1. **Input:** Webpage Screenshot + HTML (for perturbation generation).
  2. **Perturbation:** Apply Algorithm 1, 2, or 3 (Color, Text, Layout) to generate adversarial pairs.
  3. **Inference:** MLLM processes Original and Perturbed screenshots independently.
  4. **Evaluation:** Compare model outputs against Consensus Ground Truth (GT) and perform Self-Contrast (Output_Original vs. Output_Perturbed).

- **Design tradeoffs:**
  - **Real vs. Synthetic Data:** The authors use real websites but synthetic perturbations. This balances ecological validity (real layouts) with controlled testing (specific color changes), but may miss natural noise types (e.g., rendering bugs).
  - **Consensus vs. Human GT:** Using cross-model consensus for Ground Truth (Section 4.4) scales evaluation but risks "groupthink" errors if all models share a specific bias.

- **Failure signatures:**
  - **Visual Salience Override:** Model predicts based on color brightness, failing when colors are swapped.
  - **Local Attention Trap:** Model describes a single prominent section (e.g., a Kanban card) but misses the global page function (e.g., "Project Management Tool").
  - **Character Brittleness:** High sensitivity to whitespace or symbols in button text (e.g., "Sign Up" vs. "Sign! Up").

- **First 3 experiments:**
  1. **Baseline Validation:** Run the Qwen2.5-VL-7B on the "Position Relationship" task to replicate the 16.3% accuracy baseline and verify the evaluation script handles the "top-left/overlap" taxonomy correctly.
  2. **Perturbation Stress Test:** Apply the "Color Robustness" perturbation to a high-performing closed-source model (if available) or Qwen-72B to measure the performance drop (delta) and confirm the "Visual Salience" vulnerability.
  3. **LoRA Ablation:** Fine-tune the 7B model using the provided scripts (Rank=16) on *only* the UI Grouping task to verify the claimed jump (67% → 96%) before attempting full multi-task tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating explicit DOM tree or HTML structural encoders resolve the "attention bias toward local regions" that limits compositional reasoning?
- Basis in paper: [inferred] The authors identify "attention bias toward local regions at the expense of global structural understanding" as a cause for failure in reasoning tasks.
- Why unresolved: Current MLLMs rely on visual patch encodings which may fail to capture the hierarchical dependencies of complex web layouts.
- What evidence would resolve it: Architectures fusing visual and HTML structural inputs achieving significantly higher accuracy on position relationship and UI grouping tasks than vision-only baselines.

### Open Question 2
- Question: Does the observed "over-reliance on visual salience" in static benchmarks directly predict failure modes in dynamic GUI agents?
- Basis in paper: [inferred] The paper identifies "over-reliance on visual salience during color changes" as a systematic vulnerability, noting the need for "AI collaborators" in complex applications.
- Why unresolved: The benchmark evaluates static screenshots; it is unverified if these visual robustness failures translate to functional errors (e.g., clicking wrong buttons) in interactive agent deployments.
- What evidence would resolve it: A correlation study between low robustness scores on WebRSSBench and higher error rates in agents navigating live, perturbed websites.

### Open Question 3
- Question: Is the "conservatism" in recognizing safety-critical actions caused by data sparsity or over-regularization from safety alignment?
- Basis in paper: [explicit] The abstract states models are "rather conservative in recognizing and avoiding safety critical or irreversible actions."
- Why unresolved: The paper quantifies the behavior (models struggle to detect risks) but does not isolate whether this is due to a lack of safety-critical UI training examples or overly cautious refusal mechanisms.
- What evidence would resolve it: Ablation studies comparing base and aligned models on safety tasks, or performance shifts after fine-tuning on curated safety-critical UI datasets.

## Limitations

- The consensus-ground truth methodology for evaluating model responses may inherit systematic biases from the models used in consensus generation, potentially masking model-specific reasoning failures.
- The robustness perturbations, while controlled, are synthetic modifications that may not fully represent real-world adversarial scenarios or natural UI degradation patterns.
- Safety-critical detection performance correlation with model scale suggests proprietary alignment data may be necessary, but the paper does not isolate which alignment techniques (RLHF, constitutional training, etc.) drive this difference.

## Confidence

- **High Confidence:** The benchmark construction methodology and the general trend of reasoning > robustness > safety difficulty is well-supported by the experimental results.
- **Medium Confidence:** The LoRA fine-tuning effectiveness claims are substantiated by specific accuracy improvements, but the generalisability across different MLLM architectures remains uncertain.
- **Low Confidence:** The specific mechanism of "visual salience override" vulnerability is demonstrated through controlled experiments, but the paper does not validate whether models learn to exploit this shortcut during training.

## Next Checks

1. **Bias Analysis:** Run the consensus-ground truth generation process on a subset of tasks with human annotators to quantify potential systematic errors in the current approach.
2. **Natural Perturbation Test:** Apply the benchmark to naturally degraded web UI screenshots (e.g., from archived pages or screenshots with rendering errors) to validate synthetic perturbation relevance.
3. **Alignment Isolation:** Compare safety-critical detection performance between models with different alignment histories (pure supervised vs. RLHF vs. constitutional training) to isolate the mechanism behind scale-dependent safety performance.