---
ver: rpa2
title: Unsupervised Surrogate Anomaly Detection
arxiv_id: '2504.20733'
source_url: https://arxiv.org/abs/2504.20733
tags:
- anomaly
- detection
- surrogate
- dean
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces surrogate anomaly detection, a framework
  for unsupervised anomaly detection that learns low-dimensional representations encoding
  regular patterns of normal data. The authors formalize this concept into five axioms
  required for optimal surrogate models and propose DEAN (Deep Ensemble ANomaly detection),
  which fulfills all axioms.
---

# Unsupervised Surrogate Anomaly Detection

## Quick Facts
- **arXiv ID:** 2504.20733
- **Source URL:** https://arxiv.org/abs/2504.20733
- **Reference count:** 40
- **Primary result:** DEAN achieves highly competitive AUC-ROC performance on 121 benchmark datasets, ranking among top methods and outperforming all other surrogate-based approaches.

## Executive Summary
This paper introduces surrogate anomaly detection, a framework that learns low-dimensional representations encoding regular patterns of normal data, where anomalies are detected as deviations from these patterns. The authors formalize this concept into five axioms for optimal surrogate models and propose DEAN (Deep Ensemble ANomaly detection), which fulfills all axioms. DEAN trains an ensemble of simple feedforward networks to output a constant value, using feature bagging and power-based aggregation to improve diversity and performance. Evaluated against 19 methods on 121 benchmark datasets, DEAN demonstrates highly competitive performance, strong scalability, robustness to hyperparameters, and low uncertainty across repeated runs.

## Method Summary
DEAN trains an ensemble of 100 MLPs to map normal data to a constant value (1), with each submodel receiving a random subset of features via feature bagging. The anomaly score is computed as the power-based aggregation (power=9) of deviations from the empirical mean output across the ensemble. Each MLP has 3 hidden layers (255 neurons each) with ReLU activations and SELU output (no bias), trained using Adam optimizer (lr=0.0001, batch=512, epochs=50, patience=10). The framework is guided by five axioms ensuring scale consistency, reliable training, robustness to trivial solutions, hyperparameter invariance, and complex pattern learning.

## Key Results
- DEAN achieves highly competitive AUC-ROC performance on 121 benchmark datasets, ranking among the top methods
- Outperforms all other surrogate-based approaches while maintaining strong scalability and robustness
- Demonstrates low repetition uncertainty (1.52% AUC-ROC std) and hyperparameter uncertainty (1.58%)
- Ensemble structure enables adaptability for fairness, explainability, and other extensions

## Why This Works (Mechanism)

### Mechanism 1: Constant Value Mapping as Surrogate Objective
Training an ensemble of neural networks to map normal data to a constant value (1) creates a surrogate representation that captures regular patterns, where deviations indicate anomalies. Each submodel learns to approximate the target pattern g(x) = 1 for normal samples, producing higher deviations ||f(x) - q|| for anomalies. The ensemble aggregates these deviations using power-based aggregation (power=9) to emphasize pronounced deviations.

### Mechanism 2: Ensemble Diversity via Feature Bagging and Independent Submodels
Combining many simple, independently trained feedforward networks with feature bagging creates diverse pattern specializations that improve robustness and detection performance. Each submodel receives a random subset of features, causing them to specialize in different aspects of the data. The power-based aggregation amplifies large deviations across any submodel while diminishing the impact of trivial solutions.

### Mechanism 3: Axiom-Guided Design to Avoid Trivial Solutions and Ensure Stability
The five proposed axioms (scale consistency, reliable training, robustness to trivial solutions, hyperparameter invariance, complex pattern learning) guide DEAN's design to produce stable, generalizable anomaly detection. By choosing g(x) = 1, removing learnable shifts only from the output layer, using ensembles to cancel trivial solutions, and evaluating hyperparameter sensitivity experimentally, DEAN achieves low repetition and hyperparameter uncertainty.

## Foundational Learning

**Concept: Surrogate Model**
- Why needed here: Core abstraction—learn an approximate function f ≈ g capturing normal patterns rather than modeling full density.
- Quick check question: Can you explain why modeling a pattern function g is simpler than modeling the full data distribution?

**Concept: Feature Bagging**
- Why needed here: Ensures ensemble diversity by training each submodel on a random feature subset, reducing correlation among errors.
- Quick check question: What would happen if all submodels were trained on identical feature sets?

**Concept: Power-Based Aggregation**
- Why needed here: Amplifies large deviations in any submodel over many small deviations across submodels, increasing sensitivity to clear anomalies.
- Quick check question: How does increasing the power parameter change the contribution of moderate vs. extreme deviations?

## Architecture Onboarding

**Component map:**
Input x ∈ R^d (optionally subset via feature bagging) -> Submodel (MLP with 3 hidden layers, ReLU activations, SELU output layer without bias) -> Ensemble of 100 independently trained submodels -> Power-based aggregation (power=9) -> Anomaly score (aggregated deviation from q_i ≈ 1)

**Critical path:**
1. Feature bagging selects random feature subset per submodel (if d ≥ 200, max_bag=200)
2. Each submodel trained to minimize L = Σ ||f(x) - 1|| with lr=0.0001, batch=512, epochs=50, early stopping patience=10
3. Compute q_i as mean output over training set per submodel
4. Aggregate scores using power=9; higher scores indicate anomalies

**Design tradeoffs:**
- Ensemble size vs. runtime: 13 submodels achieve ~2% lower performance with ~87% less training time
- Power parameter: Higher values (e.g., 9) emphasize extreme deviations but may increase sensitivity to noise
- Feature bagging: Ensures diversity but may miss critical cross-feature correlations if too aggressive

**Failure signatures:**
- All submodels output ~1 for all inputs → trivial solution; score variance near zero
- High repetition uncertainty (>3%) → unstable training, check learning rate/batch size
- AUC-ROC drops significantly on held-out data → overfitting to training patterns

**First 3 experiments:**
1. Baseline evaluation: Train DEAN with default 100 submodels, power=9, feature bagging on ADBench datasets; record AUC-ROC
2. Ablation on ensemble size: Compare 13, 50, 100 submodels to quantify performance-runtime tradeoff
3. Hyperparameter sensitivity: Vary power (7, 9, 11) and learning rate (0.0001 × 0.5, × 2) to validate axiom 4 compliance

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can the DEAN loss function be reformulated to leverage labeled anomalies for semi-supervised anomaly detection?
- Basis in paper: [explicit] Section 6 states that the training procedure could be modified to incorporate additional information, specifically mentioning "semi-supervised anomaly detection," though the current formulation is purely unsupervised.
- Why unresolved: The paper defines DEAN exclusively for unsupervised learning and does not propose a mechanism for integrating labeled anomaly examples into the training of the submodels.
- What evidence would resolve it: A formal extension of the loss function (e.g., adding a supervised term) and a comparative evaluation against semi-supervised baselines on contaminated datasets.

**Open Question 2**
- Question: Does a distributed implementation of DEAN via federated learning maintain the low variance and scalability properties observed in the centralized version?
- Basis in paper: [explicit] Section 6 claims the ensemble character "natively supports a distributed implementation through federated learning" as a potential adaptation for the framework.
- Why unresolved: While the ensemble structure theoretically supports distribution, the paper provides no experiments or analysis regarding communication costs or the impact of data heterogeneity on the surrogate axioms.
- What evidence would resolve it: An implementation of DEAN in a simulated federated environment measuring convergence speed, communication overhead, and AUC-ROC compared to the centralized model.

**Open Question 3**
- Question: Do non-constant pattern functions g exist that satisfy all five surrogate axioms while offering better performance on highly complex data?
- Basis in paper: [inferred] The authors advocate for the simplest pattern function (g(x)=1) to satisfy axioms, but acknowledge that the general framework permits any g consistent with the definition, leaving the exploration of complex, axiom-compliant alternatives open.
- Why unresolved: The paper restricts its evaluation to constant functions and identity functions (Autoencoder), showing the latter violates axioms; the existence of intermediate, axiom-compliant g functions is not explored.
- What evidence would resolve it: A theoretical proof or empirical demonstration of a non-constant g that satisfies Axioms 1–5 and outperforms the constant surrogate on specific data distributions.

## Limitations
- Theoretical justification for constant-target surrogate learning is primarily empirical rather than derived from first principles
- The choice of power=9 for aggregation lacks theoretical grounding for why this specific value optimizes the tradeoff
- Feature bagging may discard potentially useful cross-feature interactions in datasets with fewer than 200 features

## Confidence

**High confidence:** Experimental results showing DEAN's competitive performance (AUC-ROC ranking) and stability metrics (1.52% repetition uncertainty, 1.58% hyperparameter uncertainty)

**Medium confidence:** The mechanism that constant-value mapping captures normal patterns, as this relies on empirical observations rather than formal proofs

**Medium confidence:** The claim that DEAN "outperforms all other surrogate-based approaches" given that the comparison is limited to the specific 19 methods tested

## Next Checks

1. Test DEAN's performance when the power parameter varies from 3 to 15 to determine if the 9-value is optimal or simply locally effective
2. Evaluate whether removing feature bagging entirely improves performance on datasets with fewer than 200 features, testing the tradeoff between diversity and information retention
3. Apply DEAN to datasets where normal and anomalous patterns overlap significantly to assess failure modes when the constant-target assumption breaks down