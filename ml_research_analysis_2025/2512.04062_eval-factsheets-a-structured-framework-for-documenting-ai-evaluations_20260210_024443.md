---
ver: rpa2
title: 'Eval Factsheets: A Structured Framework for Documenting AI Evaluations'
arxiv_id: '2512.04062'
source_url: https://arxiv.org/abs/2512.04062
tags:
- evaluation
- data
- evaluations
- what
- documentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Eval Factsheets introduces a systematic framework for documenting
  AI evaluation methodologies across five dimensions: Context, Scope, Structure, Method,
  and Alignment. Through a questionnaire-based approach, it addresses the documentation
  gap in evaluation transparency that exists despite established frameworks for datasets
  (Datasheets) and models (Model Cards).'
---

# Eval Factsheets: A Structured Framework for Documenting AI Evaluations

## Quick Facts
- arXiv ID: 2512.04062
- Source URL: https://arxiv.org/abs/2512.04062
- Reference count: 5
- Eval Factsheets introduces a systematic framework for documenting AI evaluation methodologies across five dimensions

## Executive Summary
Eval Factsheets addresses the documentation gap in AI evaluation transparency by introducing a systematic framework for documenting evaluation methodologies. The framework organizes evaluation characteristics across five fundamental dimensions: Context, Scope, Structure, Method, and Alignment. Through a questionnaire-based approach with 27 questions, it balances mandatory requirements for reproducibility with flexible optional elements for comprehensive coverage. The framework enables consistent documentation across diverse evaluation paradigms, from traditional benchmarks to LLM-as-judge methodologies, and provides structured templates for comparison and integration with existing AI documentation practices.

## Method Summary
The framework implements a 5-dimensional taxonomy through a questionnaire-based approach requiring users to document AI evaluations across Context (who made it and when), Scope (what it evaluates), Structure (what it's built with), Method (how it works), and Alignment (reliability and validity). Users complete 27 questions that map to these dimensions, with mandatory elements ensuring reproducibility while optional elements provide flexibility. The approach includes practical resources like templates, HTML forms, and completion guides, demonstrated through case studies on ImageNet, HumanEval, and MT-Bench that show the framework's applicability across different evaluation types.

## Key Results
- Demonstrates applicability across three distinct evaluation paradigms: ImageNet (traditional supervised benchmark), HumanEval (execution-based testing), and MT-Bench (LLM-as-judge)
- Provides structured documentation templates that enable comparison of evaluations claiming to measure similar constructs
- Establishes mandatory/optional question hierarchy that ensures reproducibility-critical information while allowing flexible documentation depth
- Positions Eval Factsheets as complementary to existing frameworks like Datasheets, Model Cards, and System Cards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal taxonomy decomposition enables consistent documentation across fundamentally different evaluation paradigms.
- Mechanism: The five dimensions (Context, Scope, Structure, Method, Alignment) are designed as orthogonal categories—each captures a distinct aspect of evaluation that does not overlap with others. This orthogonality allows the same framework to accommodate traditional benchmarks, execution-based testing, and LLM-as-judge methodologies without structural conflict.
- Core assumption: Evaluation methodologies, despite surface diversity, share fundamental characteristics that can be decomposed into non-overlapping dimensions.
- Evidence anchors:
  - [abstract] "Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?)."
  - [section 3] "This decomposition enables consistent documentation across evaluation paradigms ranging from traditional supervised benchmarks to emerging LLM-as-judge arenas."
  - [corpus] Weak direct evidence; neighbor papers focus on reproducibility problems rather than taxonomy design mechanisms.
- Break condition: If evaluations emerge that require dimensions orthogonal to all five current categories (e.g., legal compliance, economic impact), the taxonomy would require extension rather than replacement.

### Mechanism 2
- Claim: Mandatory/optional question hierarchy reduces adoption barriers while ensuring reproducibility-critical information is captured.
- Mechanism: The 27-question framework separates mandatory elements (addressing reproducibility) from optional elements (addressing comprehensiveness). This creates a completion floor that prevents critical omissions while allowing flexibility for resource-constrained users.
- Core assumption: Users will complete mandatory sections even when they skip optional ones, and mandatory sections alone provide minimum viable documentation.
- Evidence anchors:
  - [abstract] "A questionnaire-based approach implements the taxonomy with 27 questions balancing mandatory requirements and flexible optional elements."
  - [section 4.1] "Our resolution strategy prioritizes mandatory elements addressing reproducibility while making comprehensive coverage optional, allowing users to balance documentation depth with available resources."
  - [corpus] EvalCards paper (neighbor) identifies "three persistent shortcomings in current reporting practices" but does not validate mandatory/optional hierarchy specifically.
- Break condition: If mandatory elements prove insufficient for reproducibility in practice, or if optional elements become de-facto required by community norms, the hierarchy distinction becomes meaningless.

### Mechanism 3
- Claim: Structured template format enables direct comparison across evaluations claiming to measure similar constructs.
- Mechanism: By forcing evaluations into identical section structures (What Does It Evaluate, How Does It Work, How Is It Structured, Quality & Reliability), superficially similar evaluations with incompatible implementations become visibly distinct.
- Core assumption: Users will actually compare factsheets rather than relying on benchmark names or aggregate scores alone.
- Evidence anchors:
  - [section 1] "The absence of standardized reporting prevents meaningful comparison across evaluations: two benchmarks claiming to measure 'reasoning capability' may implement this construct in fundamentally incompatible ways."
  - [section 5] Case studies demonstrate that ImageNet, HumanEval, and MT-Bench all fit the template while revealing their fundamental methodological differences.
  - [corpus] No direct validation of cross-evaluation comparison behavior in neighbor papers.
- Break condition: If evaluations proliferate faster than factsheet completion, or if factsheets are generated but not consulted, the comparison mechanism fails.

## Foundational Learning

- Concept: **Construct Validity**
  - Why needed here: The Alignment dimension explicitly references whether evaluations measure what they claim. Understanding construct validity is prerequisite to documenting the "Quality & Reliability" section meaningfully.
  - Quick check question: Can you explain why an evaluation claiming to measure "reasoning" might actually be measuring "pattern matching"?

- Concept: **Evaluation Paradigms** (Reference-based, Execution-based, Model-based/Human-based)
  - Why needed here: The Method dimension categorizes judge types. Without understanding these paradigms, you cannot correctly classify or document an evaluation's methodology.
  - Quick check question: What is the fundamental difference between HumanEval's execution-based scoring and MT-Bench's LLM-as-judge scoring?

- Concept: **Documentation Ecosystem** (Datasheets, Model Cards, System Cards)
  - Why needed here: The paper positions Eval Factsheets as complementary to existing frameworks. Understanding what each framework documents determines when to use which.
  - Quick check question: If you have a Datasheet for training data and a Model Card for model capabilities, what gap does an Eval Factsheet fill?

## Architecture Onboarding

- Component map:
  ```
  Eval Factsheet
  ├── Basic Information (Context: provenance, purpose)
  ├── What Does It Evaluate (Scope: capabilities, modalities)
  ├── How Is It Structured (Structure: data sources, splits, size)
  ├── How Does It Work (Method: judge type, protocol, access)
  └── Quality & Reliability (Alignment: validity, robustness, limitations)
  ```

- Critical path: Scope → Structure → Method → Alignment
  - Scope defines what is being measured; this constrains valid Structure choices (e.g., code generation scope requires execution-capable structure).
  - Structure and Scope together constrain valid Method choices (e.g., private test sets require specific protocols).
  - All three determine what Alignment validation is meaningful (e.g., LLM-as-judge requires judge-human alignment measures).

- Design tradeoffs:
  - Comprehensiveness vs. Accessibility: Full documentation is valuable but increases completion burden.
  - Flexibility vs. Comparability: Accommodating diverse evaluation types reduces structural uniformity.
  - Static vs. Dynamic: Fixed templates enable comparison but may not capture evolving evaluation practices.

- Failure signatures:
  - Empty optional sections with no mandatory baseline = likely abandoned factsheet
  - Scope claims without corresponding Structure/Method documentation = validity gap
  - Alignment section with no robustness measures = untested evaluation
  - Copy-pasted content across multiple factsheets = template misuse

- First 3 experiments:
  1. Complete an Eval Factsheet for a benchmark you use regularly (e.g., MMLU, GSM8K). Identify which sections require information not readily available in the original paper.
  2. Compare factsheets for two benchmarks claiming to measure the same capability (e.g., both claiming "reasoning"). Document the structural differences revealed.
  3. Create a minimal factsheet using only mandatory fields, then assess whether another researcher could reproduce the evaluation from that documentation alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the 27-question format be automated, or will the manual effort required limit widespread adoption?
- Basis in paper: [inferred] The paper introduces a manual "questionnaire-based approach" to balance flexibility and standardization but does not address the feasibility of automating the documentation process.
- Why unresolved: The case studies validate the *applicability* of the taxonomy to diverse benchmarks but do not assess the *usability* or time burden for benchmark creators.
- What evidence would resolve it: Development of tools that auto-populate factsheets from code/papers, or user studies demonstrating low completion burden.

### Open Question 2
- Question: Does the taxonomy capture the unique constraints of regulated domains (e.g., medical AI) without compromising standardization?
- Basis in paper: [inferred] Section 2.4 claims the framework generalizes while maintaining compatibility with domain-specific guidelines, yet the case studies are restricted to general benchmarks (ImageNet, HumanEval, MT-Bench).
- Why unresolved: It is unknown if "optional elements" are sufficient for high-stakes validity requirements without causing domain fragmentation or loss of comparability.
- What evidence would resolve it: Successful application of Eval Factsheets to benchmarks in specialized fields like healthcare or law.

### Open Question 3
- Question: How should the framework account for temporal validity as static benchmarks suffer from contamination and saturation?
- Basis in paper: [inferred] Section 3.5 lists "temporal validity" as a known limitation of evaluations, but the factsheet structure appears largely static regarding the benchmark's lifecycle.
- Why unresolved: Documentation created at release may become misleading as models overfit to benchmarks or as the data distribution shifts over time.
- What evidence would resolve it: A longitudinal study or protocol for versioning factsheets to flag stale evaluation metrics.

## Limitations

- The taxonomy's completeness for emerging evaluation paradigms beyond traditional, execution-based, and LLM-as-judge remains untested
- Mandatory/optional question hierarchy's effectiveness in practice lacks empirical validation of actual user behavior
- No evidence that factsheets are actually consulted for comparison decisions versus traditional benchmark names and scores

## Confidence

- **High confidence**: The taxonomy's internal consistency and orthogonal decomposition is well-supported by the framework's structure and case study applications
- **Medium confidence**: The mandatory/optional hierarchy reducing adoption barriers is logically sound but lacks empirical validation of actual user behavior
- **Medium confidence**: The comparison mechanism enabling cross-evaluation analysis is demonstrated through case studies but not tested with real-world usage patterns

## Next Checks

1. Apply the framework to evaluate a fundamentally different paradigm (e.g., economic impact assessment or legal compliance evaluation) to test taxonomy extensibility beyond the three demonstrated paradigms
2. Conduct a controlled study where researchers attempt to reproduce evaluations using only mandatory factsheet sections versus complete factsheets to empirically validate the minimum viable documentation threshold
3. Survey the AI evaluation community to determine whether factsheets are actually consulted for comparison decisions versus relying on traditional benchmark names and aggregate scores