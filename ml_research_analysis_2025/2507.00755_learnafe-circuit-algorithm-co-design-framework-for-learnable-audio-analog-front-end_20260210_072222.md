---
ver: rpa2
title: 'LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog
  Front-End'
arxiv_id: '2507.00755'
source_url: https://arxiv.org/abs/2507.00755
tags:
- training
- function
- ieee
- analog
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a circuit-algorithm co-design framework for
  a learnable analog front-end (AFE) in audio signal classification. Unlike traditional
  approaches that design the AFE and backend classifiers separately, this work jointly
  optimizes the transfer function parameters of an analog bandpass filter (BPF) bank
  with the neural network classifier.
---

# LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End

## Quick Facts
- arXiv ID: 2507.00755
- Source URL: https://arxiv.org/abs/2507.00755
- Reference count: 40
- Primary result: Jointly optimizes analog filter bank and neural network, achieving 90.5%-94.2% accuracy for 10-keyword classification across SNRs 5-20 dB with 22k classifier parameters and 8.7%/12.9% power/area reductions.

## Executive Summary
This paper introduces a circuit-algorithm co-design framework for a learnable analog front-end (AFE) in audio signal classification. Unlike traditional approaches that design the AFE and backend classifiers separately, this work jointly optimizes the transfer function parameters of an analog bandpass filter (BPF) bank with the neural network classifier. A new co-design loss function, LBPF, is proposed to integrate classification loss, power loss, and area loss into the training process, ensuring both efficiency and high accuracy under various signal-to-noise ratios (SNRs). Using Bayesian optimization for hyperparameter tuning, the framework achieves 90.5%-94.2% accuracy for 10-keyword classification across SNRs from 5 dB to 20 dB, with only 22k classifier parameters. Compared to conventional methods, the proposed AFE achieves 8.7% and 12.9% reductions in power and capacitor area, respectively. Implemented in SKY130 130nm CMOS, the design demonstrates significant improvements in energy efficiency and hardware utilization while maintaining robust performance.

## Method Summary
The framework employs a two-stage co-design process: first jointly training a 16-channel Differential Super Source-Follower Bandpass Filter (DSSF-BPF) bank and a Deep Scaling Convolutional Neural Network (DSCNN) classifier using a custom loss function that balances classification accuracy with hardware efficiency (power and area); then verifying the design using SPICE simulation and fine-tuning the classifier. The learnable parameters are scaling factors for current and capacitor values rather than raw circuit parameters, maintaining hardware constraints. Bayesian Optimization tunes hyperparameters including learning rate and loss coefficients to balance accuracy versus hardware cost. The system processes 20 kHz resampled Google Speech Commands data with SNR-aware noise augmentation.

## Key Results
- Achieves 94.2% accuracy at 20 dB SNR and 90.5% at 5 dB SNR for 10-keyword classification
- Reduces power consumption by 8.7% compared to conventional methods
- Reduces capacitor area by 12.9% while maintaining high accuracy
- Uses only 22k parameters in the classifier network

## Why This Works (Mechanism)
The joint optimization framework allows the analog front-end and digital classifier to be co-adapted, ensuring that the AFE's frequency response is optimized not just for signal processing but specifically for the downstream classification task. By parameterizing the filter transfer functions with learnable scaling factors rather than raw circuit values, the framework maintains hardware constraints while allowing gradient-based optimization. The custom co-design loss function integrates classification performance with hardware efficiency metrics, enabling the system to find optimal trade-offs between accuracy and resource consumption. Bayesian optimization helps navigate the complex hyperparameter space to find configurations that balance these competing objectives effectively.

## Foundational Learning
- **Bandpass Filter Transfer Functions**: Essential for modeling how the analog front-end shapes frequency content; quick check: verify the mathematical form of the DSSF-BPF transfer function matches Equation 1.
- **Hardware Parameter Scaling**: Learnable scaling factors (φ_I, φ_C) allow gradient-based optimization while respecting circuit constraints; quick check: confirm scaling factors remain within valid ranges during training.
- **Custom Loss Function Design**: LBPF combines classification loss with hardware penalties to guide joint optimization; quick check: monitor how classification accuracy and hardware losses evolve during training.
- **Bayesian Optimization**: Used to tune hyperparameters that balance accuracy and efficiency; quick check: verify optimization converges to stable hyperparameter values within the search space.
- **SPICE Simulation for Verification**: Non-ideal circuit behavior is captured through simulation to validate the ideal model assumptions; quick check: compare accuracy drop between ideal and SPICE models.
- **SNR-Aware Training**: Training with varying noise levels ensures robustness across real-world conditions; quick check: evaluate performance across the full SNR range (5-20 dB).

## Architecture Onboarding

**Component Map**: Input audio -> DSSF-BPF bank (16 channels) -> DSCNN classifier -> Classification output

**Critical Path**: The critical computational path runs from the audio input through the learnable bandpass filter bank, followed by the DSCNN classifier. The filter bank's transfer functions must be differentiable for gradient propagation, while the DSCNN processes the filtered features through convolutional layers, scaling blocks, and fully connected layers to produce the final classification.

**Design Tradeoffs**: The framework trades off between classification accuracy and hardware efficiency by adjusting the loss coefficients (λ_CE, λ_I, λ_C). Higher emphasis on hardware loss reduces power and area consumption but may slightly decrease accuracy. The choice of 16 filter channels represents a balance between frequency resolution and computational complexity. Using learnable scaling factors rather than direct circuit parameters provides optimization flexibility while maintaining manufacturability constraints.

**Failure Signatures**: 
- If gradients of scaling factors vanish during training, the AFE is not learning effectively, indicating potential issues with learning rate scaling or parameter initialization.
- Accuracy drops significantly larger than ~1% during SPICE verification suggest the learned parameters push the circuit into non-linear regions not captured by the ideal model.
- Poor generalization across SNRs indicates insufficient noise augmentation or suboptimal joint optimization of the AFE and classifier.

**First Experiments**:
1. Implement the ideal DSSF-BPF transfer function as a differentiable layer and verify it can learn meaningful frequency responses on synthetic data.
2. Train the full system on resampled GSCD with SNR-aware noise injection and monitor the evolution of classification accuracy and hardware loss.
3. Perform Bayesian Optimization to find optimal hyperparameters and compare the resulting accuracy/hardware trade-off against baseline configurations.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing architectural details for the DSCNN (kernel sizes, channel expansion factors, activation functions) prevent exact reproduction
- Lack of initial values and search space bounds for Bayesian Optimization hyperparameters makes it difficult to replicate the exact optimization trajectory
- Limited exploration of how learned parameters behave under non-ideal circuit conditions beyond the ~1% accuracy drop mentioned

## Confidence

**High Confidence**: The overall framework concept of jointly optimizing analog filter parameters with a neural network classifier is sound and well-presented. The use of a custom co-design loss function that incorporates classification, power, and area objectives is a clear and logical approach.

**Medium Confidence**: The reported accuracy improvements (90.5%-94.2% across SNRs) and hardware efficiency gains (8.7% power, 12.9% area reduction) are plausible given the methodology. However, the exact contribution of each component to these results cannot be fully assessed due to information gaps.

**Low Confidence**: The precise values of the accuracy metrics and the absolute magnitude of the hardware savings are difficult to verify without a complete reproduction of the model and training process.

## Next Checks
1. Implement and train with provided architecture skeleton: Build the ideal DSSF-BPF transfer function and DSCNN classifier architecture, train on resampled GSCD (20kHz) with SNR-aware noise injection, and monitor classification accuracy and learnable scaling factors.
2. Perform Bayesian Optimization for loss coefficients: Use Ax library or similar to tune hyperparameters (learning rate, λ_CE, λ_I, λ_C) within a reasonable search space, then compare final accuracy and hardware loss to reported values.
3. Simulate non-ideal circuit behavior: Replace ideal transfer functions with non-ideal SPICE models of the DSSF-BPF bank using SKY130 130nm CMOS parameters, use learned scaling factors as initial conditions, fine-tune classifier for 5 epochs, and measure final accuracy to quantify circuit non-ideality impact.