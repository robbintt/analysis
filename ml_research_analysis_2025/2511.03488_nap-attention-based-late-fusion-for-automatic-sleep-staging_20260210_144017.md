---
ver: rpa2
title: 'NAP: Attention-Based Late Fusion for Automatic Sleep Staging'
arxiv_id: '2511.03488'
source_url: https://arxiv.org/abs/2511.03488
tags:
- sleep
- recordings
- datasets
- signals
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NAP (Neural Aggregator of Predictions), a
  method to combine multiple pre-trained sleep staging models using a tri-axial attention
  mechanism that captures temporal, spatial, and predictor-level dependencies. The
  model flexibly adapts to varying input dimensions (modalities, channels, sequence
  lengths) and aggregates outputs from frozen, pre-trained single-channel models.
---

# NAP: Attention-Based Late Fusion for Automatic Sleep Staging

## Quick Facts
- **arXiv ID:** 2511.03488
- **Source URL:** https://arxiv.org/abs/2511.03488
- **Reference count:** 19
- **Primary result:** NAP achieves state-of-the-art zero-shot generalization by learning to fuse predictions from multiple frozen single-channel sleep staging models using tri-axial attention.

## Executive Summary
This paper introduces NAP (Neural Aggregator of Predictions), a method to combine multiple pre-trained sleep staging models using a tri-axial attention mechanism that captures temporal, spatial, and predictor-level dependencies. The model flexibly adapts to varying input dimensions (modalities, channels, sequence lengths) and aggregates outputs from frozen, pre-trained single-channel models. NAP was evaluated on multiple out-of-domain datasets and achieved state-of-the-art zero-shot generalization performance, consistently outperforming both individual models and simple ensemble methods like soft-voting. The primary improvements were in recognizing difficult sleep stages (e.g., N1) and Wake stage, demonstrating the effectiveness of attention-based late fusion for multimodal physiological data.

## Method Summary
NAP is a lightweight transformer encoder that takes hypnodensity predictions from frozen pre-trained single-channel models as input. It uses a tri-axial attention mechanism that factorizes attention across three pathways (spatial, temporal, and blending) to efficiently capture dependencies in multimodal prediction streams. The model is trained with dimension-adaptive sampling, randomly varying sequence lengths, channel counts, and predictor subsets per batch to improve zero-shot generalization to unseen configurations. A modality fusion layer with learned attention weights aggregates the encoded predictions, followed by a classifier head that outputs final sleep stage predictions.

## Key Results
- NAP achieved MF1 gains on multiple out-of-domain datasets (DCSM: 0.803→0.815, DOD-H: 0.828→0.834, PHYS: 0.693→0.732, SEDF-SC: 0.734→0.752, SEDF-ST: 0.761→0.796)
- Performance improvements primarily stemmed from better recognition of N1 and Wake stages
- NAP consistently outperformed individual base models and soft-voting ensemble baselines
- The model demonstrated robustness to varying input dimensions without requiring padding or masking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorizing attention across three axes (temporal, spatial, predictor) enables efficient fusion of heterogeneous prediction streams without quadratic complexity in total sequence length.
- Mechanism: The tri-axial attention extends criss-cross attention by dividing h attention heads evenly across three pathways—spatial (channels), temporal (epochs), and blending (base predictors)—each attending along a single axis while holding others fixed.
- Core assumption: Dependencies along each axis are sufficiently decoupled that factorized attention preserves modeling capacity needed for sleep staging decisions.
- Evidence anchors:
  - [Methods]: "Instead of computing a single joint attention map over all dimensions, the mechanism factorizes the standard multi-head attention into three pathways"
  - [corpus]: Related work (SleepGMUformer) addresses multimodal temporal fusion but uses gated postfusion rather than factorized attention

### Mechanism 2
- Claim: Learned attention-weighted aggregation outperforms uniform soft-voting by dynamically weighting more reliable predictors and channels per epoch.
- Mechanism: The modality fusion layer computes attention scores via learned projections applied to each prediction stream's features, producing a convex combination rather than uniform average.
- Core assumption: Predictor reliability varies meaningfully across epochs and stages, and this variation is learnable from training data.
- Evidence anchors:
  - [Results]: "NAP delivers zero-shot MF1 gains in most cases"
  - [Results]: "MF1 improvements stem mostly from better recognition of the problematic N1 stage"

### Mechanism 3
- Claim: Dimension-adaptive training improves zero-shot generalization to unseen channel configurations.
- Mechanism: Each batch samples random sequence lengths, subsets of modalities, channels, and predictors, forcing the model to handle variable input dimensions without padding.
- Core assumption: The variation encountered at test time is covered by the distribution of training-time subsampling.
- Evidence anchors:
  - [Methods]: "We train NAP on inputs with varying dimensionality"
  - [corpus]: Related work (AnySleep) also addresses channel-agnostic design

## Foundational Learning

- **Concept: Transformer self-attention and positional encoding**
  - Why needed here: NAP builds on standard transformer encoder layers with multi-head attention; understanding Q/K/V computation and relative positional encodings is prerequisite to following the tri-axial extension.
  - Quick check question: Given a sequence of T tokens with dimension d, what is the time complexity of standard self-attention, and how does relative positional encoding modify the attention computation?

- **Concept: Late fusion vs. early fusion in multimodal learning**
  - Why needed here: NAP explicitly operates on predictions rather than raw signals or intermediate representations; understanding this design choice clarifies modularity tradeoffs.
  - Quick check question: What information is potentially lost when fusing predictions rather than representations, and what practical benefits does late fusion provide?

- **Concept: Zero-shot generalization and out-of-domain evaluation**
  - Why needed here: The paper's primary claim is zero-shot performance on datasets unseen during any training phase.
  - Quick check question: If a model is trained on dataset A and evaluated on dataset B without any parameter updates, what factors determine whether this qualifies as zero-shot transfer?

## Architecture Onboarding

- **Component map:** Raw PSG → Base models (frozen) → Hypnodensities → Linear projection → Positional/modality embeddings → Tri-axial attention (4 layers) → Fusion attention → Classifier → Sleep stage predictions

- **Critical path:** The attention computation in each pathway follows: Z = Softmax(LN(Q) · LN(K)^T / √d_k) · V, with pre-normalization and no bias term.

- **Design tradeoffs:**
  - Lightweight encoder (d_model=24, 4 layers) enables fast training but may limit capacity
  - Frozen base models preserve modularity but prevent end-to-end feature learning
  - Factorized vs. full attention reduces complexity but assumes axis decoupling

- **Failure signatures:**
  - Degraded N2 performance on some datasets (slight F1 decreases)
  - No improvement on DOD-O dataset (MF1 decreases from 0.790 to 0.776)
  - Performance depends on sufficiency of remaining modalities when some are absent

- **First 3 experiments:**
  1. Replicate tri-axial vs. soft-voting comparison on DCSM, DOD-H, PHYS datasets
  2. Ablate attention pathways to measure contribution of each (spatial, temporal, blending)
  3. Evaluate channel robustness by systematically dropping EEG or EOG channels

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the inclusion of physiological modalities beyond EEG and EOG (such as EMG or ECG) affect the tri-axial attention fusion performance of NAP?
- **Open Question 2:** Does a "Neural Aggregator of Representations" (early fusion) outperform the proposed "Neural Aggregator of Predictions" (late fusion)?
- **Open Question 3:** What specific characteristics of the DOD-O dataset cause NAP to underperform compared to simpler soft-voting ensembles?
- **Open Question 4:** Can the tri-axial attention mechanism effectively generalize to aggregation tasks outside of sleep staging?

## Limitations
- The reliance on the private BSWR dataset for training is a significant limitation for reproducibility
- The factorized tri-axial attention design lacks direct comparison against full joint attention or alternative fusion strategies
- Zero-shot generalization claims assume frozen base models are sufficiently robust across datasets

## Confidence

- **High Confidence:** Architectural design and training procedure are clearly specified; improvement over soft-voting is consistently observed
- **Medium Confidence:** Zero-shot generalization claims are convincing but rely on inaccessible training data and base model robustness
- **Low Confidence:** The assumption that dependencies are sufficiently decoupled for factorized attention to be effective lacks empirical validation against joint attention baselines

## Next Checks
1. Replicate tri-axial vs. soft-voting comparison on DCSM, DOD-H, PHYS datasets using specified architecture
2. Ablate attention pathways to measure contribution of each (spatial, temporal, blending)
3. Evaluate channel robustness by systematically dropping EEG or EOG channels at test time