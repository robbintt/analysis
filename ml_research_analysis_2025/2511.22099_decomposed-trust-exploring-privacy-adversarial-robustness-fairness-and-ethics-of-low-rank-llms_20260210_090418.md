---
ver: rpa2
title: 'Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and
  Ethics of Low-Rank LLMs'
arxiv_id: '2511.22099'
source_url: https://arxiv.org/abs/2511.22099
tags:
- chat
- low-rank
- base
- proj
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates the trustworthiness of large
  language models (LLMs) under low-rank compression, examining privacy, adversarial
  robustness, fairness, and ethics. Low-rank factorization (SVD, Basel, FWSVD) reduces
  model size while maintaining performance, but its safety implications were unknown.
---

# Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs

## Quick Facts
- arXiv ID: 2511.22099
- Source URL: https://arxiv.org/abs/2511.22099
- Reference count: 40
- Key outcome: Low-rank compression improves training data privacy and adversarial robustness but degrades fairness and ethical reasoning, with mixed PII leakage trade-offs

## Executive Summary
This study systematically evaluates the trustworthiness of large language models (LLMs) under low-rank compression, examining privacy, adversarial robustness, fairness, and ethics. Low-rank factorization (SVD, Basel, FWSVD) reduces model size while maintaining performance, but its safety implications were unknown. Results show compressed models leak less training data but may expose more personally identifiable information in certain settings. Adversarial robustness is preserved or improved, while fairness and ethical reasoning degrade under compression. Fine-tuning enhances task performance but increases vulnerability to attacks, especially in larger models. Layerwise attribution identifies embedding and down-projection layers as critical to trustworthiness. Overall, low-rank compression offers efficiency gains with mixed trustworthiness trade-offs, requiring careful evaluation before deployment.

## Method Summary
The study evaluates low-rank compressed LLMs using SVD, Basel, and FWSVD factorization on LLaMA2-7B/13B and Qwen2.5-7B/14B models. Compression is applied at 70%, 50%, and 30% retained parameters, with fine-tuning on GSM8K (math) and HumanEval (code). Trustworthiness is assessed across four dimensions: privacy (Enron Email and PII leakage), adversarial robustness (AdvGLUE++ on SST-2/QQP/MNLI), ethics (ETHICS commonsense subset), and fairness (UCI Adult dataset). Layerwise attribution via gradient analysis identifies embed_tokens and down_proj as most influential for trustworthiness. Experiments compare base, compressed, and fine-tuned models under various prompting scenarios.

## Key Results
- Low-rank compression reduces training data privacy leakage while potentially increasing PII leakage in conversational settings
- Adversarial robustness is preserved or improved under compression, even at deep compression levels
- Fairness metrics (Mdpd, Meod) consistently degrade under compression, with larger models showing greater bias
- Ethical reasoning deteriorates in zero-shot settings but recovers with few-shot prompts
- Fine-tuning improves task performance but increases vulnerability to adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank compression reduces training data privacy leakage.
- Mechanism: SVD-based factorization constrains the model's representational capacity, specifically by pruning high-variance directions in the weight matrices that often correspond to memorized, spurious training data features. By forcing the model to reconstruct weights with a limited rank, the specific pathways for verbatim recall are disrupted, acting as an implicit regularizer.
- Core assumption: The memorized training data, particularly verbatim sequences, is encoded in high-variance but non-robust directions of the weight matrices, which are the first to be pruned by low-rank approximation.
- Evidence anchors:
  - [abstract] "...low-rank compression preserves or improves training data privacy..."
  - [section] Page 6, Section VI-A3: "The privacy benefits arise from structural constraints of low-rank factorization, which limit representational complexity and disrupt memorization pathways..."
  - [corpus] Corpus signals are weak or missing for this specific low-rank mechanism.
- Break condition: The mechanism may fail if the training data is encoded in the dominant, low-rank subspaces that the model retains, which could happen if the data is highly repetitive and forms a core part of the model's "knowledge."

### Mechanism 2
- Claim: Adversarial robustness is preserved or enhanced in low-rank models.
- Mechanism: Adversarial attacks often exploit brittle, high-frequency features or directions in the model's activation space. Low-rank compression, by its nature, removes these less-stable, high-frequency components, leaving a smoother, more robust decision boundary defined by the principal components. This acts as a form of adversarial denoising.
- Core assumption: Adversarial vulnerabilities are primarily associated with the minor singular values and the corresponding directions in the weight space that are discarded during compression.
- Evidence anchors:
  - [abstract] "...adversarial robustness is generally preserved and often enhanced, even under deep compression..."
  - [section] Page 7, Section VI-C2: "Our results show that all low-rank compressed models... outperform both 13B and 7B in adversarial robustness..."
  - [corpus] Related paper on mixed precision quantization suggests a similar effect where reduced precision can act as a regularizer against perturbations.
- Break condition: This could break if an adversarial attack specifically targets the primary, low-rank subspaces that the model retains, although this is empirically harder.

### Mechanism 3
- Claim: Layerwise attribution identifies embed_tokens and down_proj as most critical for trustworthiness.
- Mechanism: Gradient-based attribution analysis measures the sensitivity of the model's output (loss) to perturbations in each layer's activations. The `embed_tokens` layer is the entry point for all inputs, making it highly influential. The `down_proj` layer, as the final projection in the MLP block, acts as a bottleneck that transmits non-linear transformations, making it acutely sensitive to perturbations that propagate through the network.
- Core assumption: Layers with higher attribution scores are more causally responsible for trustworthiness behaviors.
- Evidence anchors:
  - [abstract] "Layerwise attribution analysis identifies embed_tokens and down_proj as most influential for trustworthiness."
  - [section] Page 8, Section VI-F2: "The embed_tokens layer ranks highest across all models... Similarly, down_proj appears within the top four for all models."
  - [corpus] No direct corpus evidence for this specific architectural finding.
- Break condition: Attribution scores are a proxy for influence, not a direct causal proof. Interventions (e.g., selective compression) based solely on this ranking may have unintended consequences not captured by the attribution method.

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD) for Model Compression**
  - Why needed here: This is the core mathematical operation enabling the entire paper's analysis. It's the method used to decompose weight matrices into lower-rank approximations, the central technique being evaluated.
  - Quick check question: If a weight matrix W of size 10x10 has 3 dominant singular values and 7 very small ones, how would you approximate it with SVD to reduce its parameter count?

- Concept: **Adversarial Robustness vs. Benign Accuracy**
  - Why needed here: A key finding is that low-rank compression can improve robustness even if it slightly harms accuracy. Understanding the difference between performance on clean data (benign accuracy) and performance on maliciously perturbed data (adversarial robustness) is essential.
  - Quick check question: A model has 95% benign accuracy on a task but only 40% adversarial accuracy. What does this tell you about its reliability in a real-world, potentially hostile deployment?

- Concept: **In-Context Privacy Leakage vs. Inference-Time PII Leakage**
  - Why needed here: The paper shows a critical trade-off: compression helps with one type of privacy (training data memorization) but harms another (protecting user-provided PII during conversation). Distinguishing these two threat models is crucial for applying the findings.
  - Quick check question: Why would a model that is *more* responsive (i.e., less likely to reject a prompt) potentially be worse for protecting a user's social security number mentioned earlier in a chat session?

## Architecture Onboarding

- Component map: The system under study is a standard Transformer decoder LLM (e.g., LLaMA2). Key components include the `embed_tokens` layer, followed by a stack of identical layers, each containing self-attention (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and a feed-forward MLP (`gate_proj`, `up_proj`, `down_proj`). Low-rank factorization is applied to the weight matrices of these linear layers.
- Critical path: The `embed_tokens` layer is the most critical for trustworthiness according to attribution analysis, followed by the `down_proj` layer in the MLP blocks. For privacy and robustness, the paper suggests these layers should be treated with care during compression. The `down_proj` acts as a sensitive bottleneck.
- Design tradeoffs:
  - **Training Data Privacy vs. Inference-Time PII:** Compression improves the first but degrades the second.
  - **Robustness vs. Fairness:** Compression often improves robustness but consistently harms fairness metrics.
  - **Model Size vs. Attack Sensitivity:** Larger models (13B vs 7B) are more vulnerable to adversarial PII leakage, a trade-off to consider when scaling.
  - **Ethical Reasoning vs. Zero-Shot Capability:** Compression degrades ethical reasoning in zero-shot settings, requiring few-shot prompts to recover performance.
- Failure signatures:
  - **Unfair Outcomes:** Significant increases in Demographic Parity Difference (Mdpd) and Equalized Odds Difference (Meod) after compression.
  - **PII Leakage:** A sharp increase in leak rate under protected and zero-shot prompting scenarios (e.g., Basel-50's 27.33% leak rate vs Base 7B's 0.61%).
  - **Zero-Shot Ethical Collapse:** Near-zero accuracy on the ETHICS dataset in zero-shot settings for heavily compressed models.
- First 3 experiments:
  1.  **Replicate Compression-Robustness Link:** Take a small public LLM (e.g., a 1B-parameter variant), apply SVD compression to its MLP weights at 50% and 30% retained rank, and evaluate on both a benign task (e.g., SST-2) and its adversarial counterpart from a dataset like AdvGLUE. Check if robustness is indeed preserved or improved.
  2.  **Test PII Leakage Trade-off:** Set up a simple prompt with a mock PII (e.g., "My secret key is X5J9..."). Query a base LLM and its low-rank compressed version in a zero-shot setting to see if the compressed model is more likely to reveal the PII when prompted about it later.
  3.  **Probe Layerwise Attribution:** Implement a simple gradient-based attribution method. Compare the attribution score of the `embed_tokens` layer versus an intermediate attention layer (e.g., `q_proj`) on an adversarial example to confirm the paper's finding that embeddings are more influential.

## Open Questions the Paper Calls Out

- **Question:** Can low-rank compression algorithms be modified to explicitly preserve fairness metrics, given that current methods consistently degrade demographic parity and equalized odds?
  - Basis in paper: [inferred] The paper explicitly states "fairness declines under compression" in the abstract and conclusion, noting increased bias in the results section, but offers no mitigation strategy.
  - Why unresolved: The authors identify the fairness degradation but frame it as a trade-off rather than a solvable problem, leaving the development of bias-aware factorization techniques as an open challenge.
  - What evidence would resolve it: A modified low-rank algorithm that maintains Mdpd and Meod scores comparable to the uncompressed model while retaining efficiency gains.

- **Question:** Why does low-rank compression increase PII leakage in conversational settings despite improving training data privacy, and can this vulnerability be mitigated?
  - Basis in paper: [explicit] The abstract notes that compression "preserves or improves training data privacy but weakens PII protection during conversation."
  - Why unresolved: The authors observe that compressed models "respond more readily" to prompts, but the specific structural reasons why factorization disrupts conversational privacy safeguards while protecting memorized data remain unclear.
  - What evidence would resolve it: A mechanistic analysis identifying which safety-aligned components are lost during decomposition, followed by a regularization technique that prevents conversational leakage.

- **Question:** Does selectively preserving high rank for the `embed_tokens` and `down_proj` layers specifically recover the ethical reasoning and fairness capabilities lost in compression?
  - Basis in paper: [inferred] The paper concludes by calling for "trustworthiness-aware compression strategies" and identifies `embed_tokens` and `down_proj` as the most influential layers via attribution analysis.
  - Why unresolved: While the paper identifies *which* layers are important for robustness, it does not test whether using this information to guide non-uniform compression actually resolves the observed failures in ethics or fairness.
  - What evidence would resolve it: Experiments comparing uniform compression against adaptive, layer-wise compression strategies on the ETHICS and Adult datasets.

## Limitations

- The study focuses on specific low-rank factorization techniques without exploring alternative compression methods like quantization or pruning, leaving uncertainty about whether results generalize across compression paradigms.
- Attribution analysis relies on gradient-based methods that provide influence rather than causal evidence, potentially misrepresenting layer importance under intervention.
- Privacy evaluations use synthetic PII injection scenarios that may not reflect real-world adversarial prompting patterns.
- Fairness assessment is limited to the Adult dataset, potentially missing intersectional biases in more diverse domains.
- The study does not address post-compression safety fine-tuning, which could mitigate some trustworthiness degradations observed.

## Confidence

- **High Confidence:** The claim that low-rank compression reduces training data privacy leakage is well-supported by empirical results and the mathematical mechanism of disrupting memorization pathways.
- **Medium Confidence:** The preservation or enhancement of adversarial robustness has strong empirical support, but the mechanism relies on assumptions about attack surfaces that could benefit from more direct investigation.
- **Low Confidence:** Layerwise attribution findings have weak corpus support and rely on proxy measures rather than causal interventions, making their practical applicability uncertain.

## Next Checks

1. **Intervention-Based Layer Importance Validation**: Instead of just measuring gradients, selectively compress or ablate the embed_tokens and down_proj layers identified as critical, then measure actual changes in trustworthiness metrics. This would test whether the attribution-based importance rankings hold under intervention.

2. **Cross-Dataset Fairness Evaluation**: Replicate the fairness experiments on multiple datasets beyond Adult (e.g., CivilComments, Jigsaw toxicity) to determine if the compression-induced fairness degradation is consistent across domains or specific to certain data distributions.

3. **Adversarial Attack Surface Mapping**: Conduct white-box adversarial attacks specifically targeting the low-rank subspaces retained by compression to test the paper's assumption that adversarial vulnerabilities are primarily in discarded directions. This would validate or challenge the robustness mechanism proposed.