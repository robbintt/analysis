---
ver: rpa2
title: Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large
  Language Models
arxiv_id: '2512.18035'
source_url: https://arxiv.org/abs/2512.18035
tags:
- unlearning
- data
- privacy
- unlearned
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PrivUB, the first comprehensive benchmark\
  \ for evaluating privacy vulnerabilities in selective forgetting (machine unlearning)\
  \ with large language models. The work systematically evaluates three types of unlearning-induced\
  \ privacy attacks\u2014membership inference, data reconstruction, and knowledge\
  \ leakage\u2014across two victim data types (unlearning data and retain data)."
---

# Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models

## Quick Facts
- arXiv ID: 2512.18035
- Source URL: https://arxiv.org/abs/2512.18035
- Authors: Wei Qian; Chenxu Zhao; Yangyi Li; Mengdi Huai
- Reference count: 13
- Key outcome: Introduces PrivUB, the first comprehensive benchmark for evaluating privacy vulnerabilities in selective forgetting (machine unlearning) with large language models.

## Executive Summary
This paper introduces PrivUB, the first comprehensive benchmark for evaluating privacy vulnerabilities in selective forgetting (machine unlearning) with large language models. The work systematically evaluates three types of unlearning-induced privacy attacks—membership inference, data reconstruction, and knowledge leakage—across two victim data types (unlearning data and retain data). The benchmark implements 21 privacy attacks and defenses, covering 11 real-world datasets, 10 mainstream model architectures, and 10 unlearning techniques. Key findings include: combining multiple attacking tools improves attack effectiveness; knowledge leakage attacks can enhance membership inference performance; fine-tuning methods introduce more severe privacy risks than quantization; privacy attacks can be effectively generalized across model types, with attacks originally designed for deep learning models performing well on LLMs and vice versa; and existing defenses lack robustness, being highly sensitive to attack sample sizes.

## Method Summary
PrivUB evaluates privacy vulnerabilities in selective forgetting by systematically testing three attack types (membership inference, data reconstruction, knowledge leakage) across two victim data types (unlearning data and retain data). The benchmark implements 21 attacks and defenses across 11 datasets (including Chest X-Ray, CelebA, CIFAR-10/100, WMDP-Biology/Cyber, RWKU, Openwebtext, AG-News, Wikitext-103, XSum), 10 model architectures (ResNet, VGG, ConvNet, Llama, GPTNeo, Zephyr, Phi-3), and 10 unlearning techniques (retraining, SISA, fine-tuning, IU, NegGrad+, gradient ascent, SCRUB, SalUn, NPO, RMU). The evaluation measures MIA accuracy/AUC, cosine similarity, MSE, ROUGE score, unlearning accuracy, and test accuracy.

## Key Results
- Combining multiple attacking tools improves attack effectiveness
- Knowledge leakage attacks can enhance membership inference performance
- Fine-tuning methods introduce more severe privacy risks than quantization
- Privacy attacks can be effectively generalized across model types
- Existing defenses lack robustness and are highly sensitive to attack sample sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The discrepancy between a pre-trained model and its unlearned counterpart leaks information about the removed data.
- **Mechanism:** Selective forgetting modifies model parameters ($\theta \to \theta_u$) to minimize the influence of unlearning data $D_u$. If an attacker has access to both model states, the specific parametric shift ($\Delta \theta$) or the difference in output probabilities acts as a signal correlating specifically with the features of $D_u$, enabling Membership Inference (MIA) or Data Reconstruction.
- **Core assumption:** The unlearning process leaves a detectable trace (e.g., gradient direction or loss shift) that distinguishes $D_u$ from the retain set $D_r$.
- **Evidence anchors:** [abstract] "machine unlearning naturally generates two versions... which differ due to the deletion... inadvertently leak information."; [page 3] "attackers aim to characterize the predictive discrepancies between the pre-trained model $\theta$ and the unlearned model $\theta_u$."; [corpus] "Not Every Token Needs Forgetting" implies that parameter updates are selective, creating a differential signal.
- **Break condition:** If unlearning is exact (retraining from scratch) or differential privacy noise masks the gradient difference such that $\theta_u \approx \theta_{retrain}$.

### Mechanism 2
- **Claim:** Unlearning often suppresses knowledge rather than erasing it, allowing latent knowledge to be reactivated via perturbation.
- **Mechanism:** Unlearning methods (e.g., gradient ascent) may minimize the probability of outputting specific tokens without deleting the internal representations. Adversarial perturbation of the unlearned model (fine-tuning) or the input data (adversarial suffixes) can shift the model out of the local minimum created by unlearning, restoring the "forgotten" capability.
- **Core assumption:** The unlearned model resides in a brittle optima where knowledge is masked but the underlying weights still encode the data structure.
- **Evidence anchors:** [page 3] "privacy of the unlearning data may be further compromised... subjected to... fine-tuning, which may reactivate or amplify memorized knowledge."; [page 6] "Perturbing the model through fine-tuning... can effectively recover unlearned knowledge."; [corpus] "OFFSIDE" and "Towards Benign Memory Forgetting" discuss the difficulty of permanent unlearning in multimodal models, supporting the persistence of latent knowledge.
- **Break condition:** If unlearning involves catastrophic forgetting or randomization of the specific weight dimensions storing the knowledge (e.g., localized layer retraining).

### Mechanism 3
- **Claim:** Privacy attacks generalize across model architectures because they exploit universal learning dynamics rather than specific structures.
- **Mechanism:** Attacks like Membership Inference rely on the loss distribution differences between training and test data, a phenomenon present in both CNNs and Transformers. Consequently, attacks developed for Deep Learning (DL) models can be adapted to LLMs (and vice versa) by mapping the loss/gradients to the new architecture.
- **Core assumption:** The "memorization" behavior (lower loss on training data) is a universal property of overparameterized models regardless of modality.
- **Evidence anchors:** [page 2] "Privacy attacks can be effectively generalized across model types... attacks originally designed for deep learning models performing well on LLMs."; [page 5] Fig 2 and Table 3 show methods like (Chen et al. 2021) successfully applied to GPTNeo-1.3B.; [corpus] Corpus papers focus on specific architectures (ViT, MLLMs), but PrivUB explicitly demonstrates this cross-architecture transferability (Evidence: weak in corpus, strong in paper).
- **Break condition:** If the target architecture utilizes significantly different training objectives (e.g., contrastive learning vs. next-token prediction) that normalize loss distributions differently.

## Foundational Learning

- **Concept:** **Machine Unlearning Objectives (Exact vs. Approximate)**
  - **Why needed here:** To distinguish between theoretical removal (retraining) and practical removal (update steps), which defines the attack surface. The paper focuses on vulnerabilities inherent in approximate methods like Gradient Ascent.
  - **Quick check question:** Does the unlearning method ensure the new model distribution is mathematically indistinguishable from a model retrained without the data, or does it merely minimize loss on the retain set?

- **Concept:** **Threat Models (White-box vs. Black-box)**
  - **Why needed here:** The benchmark categorizes attacks based on access (Model Weights vs. Label-only). Understanding this is required to interpret the "Attacking Tool" dimension.
  - **Quick check question:** Does the attack require access to the internal gradients/weights (White-box) or only the final output logits/labels (Black-box)?

- **Concept:** **Membership Inference Attacks (MIA)**
  - **Why needed here:** MIA is the primary metric for privacy leakage. You must understand that a high MIA accuracy implies the model "remembers" specific training samples.
  - **Quick check question:** If a model outputs the correct label for a specific input with significantly higher confidence than average, is it likely a "member" or "non-member"?

## Architecture Onboarding

- **Component map:** Victim Models (ResNet/LLama/GPTNeo) -> Unlearning Protocols (Retraining/Approximation) -> Attack Surface (Model Discrepancy/Perturbation) -> Benchmark (PrivUB)
- **Critical path:** 1) Pre-train model $\theta$ on $D$ 2) Apply Unlearning Algorithm $U$ to get $\theta_u$ (removing $D_u$) 3) Apply Deployment Transform (Fine-tune $F$ or Quantize $Q$) to get $\theta_{ft}$ or $\theta_q$ 4) Execute Attack (e.g., Perturb Data $\tilde{D_u}$ or check Loss Discrepancy) 5) Measure Leakage (MIA AUC / ROUGE / Reconstruction MSE)
- **Design tradeoffs:**
  - **Accessibility vs. Severity:** Label-only attacks are more realistic (lower access) but often weaker than gradient-based (white-box) attacks
  - **Utility vs. Privacy:** Aggressive unlearning (high GA magnitude) might secure $D_u$ but degrade performance on $D_r$ (Retain data), potentially increasing the vulnerability of $D_r$ to the "Privacy Onion Effect" (page 2/5)
- **Failure signatures:**
  - **Privacy Onion Effect:** Unlearning $D_u$ unexpectedly increases the membership vulnerability of $D_r$ (retain data) due to shifted decision boundaries
  - **Defense Sensitivity:** Defenses like SAM (Sharpness-Aware Minimization) fail when the number of attack samples increases (Fig 9)
- **First 3 experiments:**
  1. **Sanity Check Attack Generalization:** Run a basic Membership Inference Attack (e.g., Chen et al. 2021) originally designed for ResNet on an LLM (e.g., Llama-2) to verify the paper's finding on cross-architecture transferability
  2. **Retain Data Vulnerability Audit:** Execute LiRA (Likelihood Ratio Attack) on the *retain set* ($D_r$) before and after unlearning a subset ($D_u$) to check for the "Privacy Onion Effect" (does unlearning hurt the rest of the data?)
  3. **Defense Robustness Stress Test:** Apply the SAM defense during unlearning and test knowledge leakage using varying sample sizes (e.g., 50 vs 500 prompts) to reproduce the finding that defenses degrade under scale

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can robust unlearning algorithms be developed to simultaneously mitigate privacy leakage during the unlearning phase and resist reactivation during the deployment phase (e.g., fine-tuning)?
- **Basis in paper:** [explicit] The conclusion explicitly states the need for "developing robust unlearning to mitigate privacy leakage both during unlearning and after model deployment."
- **Why unresolved:** Current unlearning methods create discrepancies that attackers exploit, and current defenses fail to maintain robustness under varying conditions.
- **What evidence would resolve it:** A new unlearning method that passes PrivUB's attack benchmarks for both the unlearning and deployment phases without significant utility loss.

### Open Question 2
- **Question:** Can defense mechanisms be constructed that are invariant to the number of attack samples used by an adversary?
- **Basis in paper:** [inferred] The results show that "existing defenses... are highly sensitive to the number of attack samples," leading to inconsistent protection.
- **Why unresolved:** Defense effectiveness currently degrades as the attack sample size increases, suggesting a fundamental instability in current defensive techniques.
- **What evidence would resolve it:** A defense strategy that maintains low membership inference accuracy and low reconstruction similarity (MSE) regardless of increases in the adversary's sample budget.

### Open Question 3
- **Question:** What underlying theoretical properties enable the cross-generalization of privacy attacks between standard deep learning models and Large Language Models?
- **Basis in paper:** [inferred] The paper notes that "attacks originally developed for deep learning models can be applied to large language models (LLMs), and vice versa."
- **Why unresolved:** While the empirical transferability is demonstrated, the specific architectural or optimization features that allow this generalization are not identified.
- **What evidence would resolve it:** A theoretical analysis linking model gradients or loss landscapes to attack success, supported by experiments isolating architectural variables.

## Limitations

- The paper lacks complete experimental details including hyperparameters for unlearning methods, attack configurations, and training procedures
- The computational cost of running all 21 attacks across the full benchmark is not addressed and could be prohibitive
- While the benchmark covers 11 datasets and 10 model architectures, generalizability to other domains or emerging model types remains untested
- The effectiveness of defenses is shown to be sensitive to attack sample sizes, but the paper does not explore other potential failure modes such as adaptive attacks or resource-constrained attackers

## Confidence

- **High confidence:** Combining multiple attacking tools improves attack effectiveness; knowledge leakage attacks can enhance membership inference performance
- **Medium confidence:** Fine-tuning methods introduce more severe privacy risks than quantization; cross-architecture attack generalization is demonstrated but may not hold for all possible model pairs
- **Low confidence:** Existing defenses lack robustness is demonstrated for specific attack sample sizes but does not explore the full space of potential attack variations or adaptive defense strategies

## Next Checks

1. **Reproduce cross-architecture attack transfer:** Implement a membership inference attack originally designed for ResNet on an LLM (e.g., Llama-2) to verify the paper's finding on cross-architecture transferability. Measure MIA AUC on both architectures using identical attack parameters.

2. **Test Privacy Onion Effect on retain data:** Execute LiRA (Likelihood Ratio Attack) on the retain set before and after unlearning a subset to check if unlearning increases vulnerability of the remaining data. Compare MIA AUC scores across conditions.

3. **Stress test defense robustness:** Apply SAM defense during unlearning and test knowledge leakage using varying sample sizes (50 vs 500 prompts) to reproduce the finding that defenses degrade under scale. Measure knowledge recovery effectiveness across sample sizes.