---
ver: rpa2
title: Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base Completion
arxiv_id: '2501.01246'
source_url: https://arxiv.org/abs/2501.01246
tags:
- rules
- rule
- knowledge
- llms
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LeSR, a novel framework that combines large
  language models (LLMs) with symbolic reasoning for knowledge base completion (KBC).
  The method uses LLMs to propose diverse logical rules from sampled subgraphs of
  the knowledge base, then refines these rules using a rule reasoner to improve reliability.
---

# Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base Completion

## Quick Facts
- arXiv ID: 2501.01246
- Source URL: https://arxiv.org/abs/2501.01246
- Reference count: 30
- Primary result: Proposes LeSR, a novel LLM-enhanced symbolic reasoning framework that outperforms existing rule-based and embedding-based methods on five diverse KBC datasets

## Executive Summary
This paper introduces LeSR, a framework that combines large language models with symbolic reasoning for knowledge base completion. The method uses LLMs to propose diverse logical rules from sampled subgraphs of the knowledge base, then refines these rules using a rule reasoner to improve reliability and reduce hallucination. LeSR achieves highly competitive results across five diverse knowledge base datasets, demonstrating strong performance on both smaller niche datasets and larger realistic ones, with particular success on commonsense knowledge bases. The approach provides interpretable rules and effectively bridges the gap between the flexibility of LLMs and the rigor of rule-based reasoning.

## Method Summary
LeSR employs a three-stage pipeline: (1) a Subgraph Extractor that samples ≤30 subgraphs per relation using multi-hop traversal (max 3 hops, max 3 neighbors per hop), (2) an LLM Proposer that generates candidate logical rules using GPT-3.5/4 or Gemini-1.5 with prompt templates and two-stage filtering (format check + semantic mapping via sentence transformer), and (3) a Rule Reasoner that learns weights for each rule using differentiable grounding operations and jointly optimizes with a RotatE embedding model. The final prediction combines weighted logical rule scores with embedding-based scores, trained to optimize rule significance weights and a balancing factor α.

## Key Results
- Achieves highly competitive results on five diverse KB datasets (UMLs, WN18RR, FB15K-237, WD15K, CN100K)
- Outperforms existing rule-based and embedding-based methods, particularly on commonsense knowledge bases
- Demonstrates the effectiveness of combining LLM-proposed rules with rule-based refinement for reducing hallucination
- Shows interpretable rule generation that bridges LLM flexibility with rule-based rigor

## Why This Works (Mechanism)

### Mechanism 1: LLM-Driven Rule Proposal
LLMs generate more diverse and relation-specific logical rules than traditional mining when provided with localized graph context. The Subgraph Extractor samples subgraphs around target relations (multi-hop traversal), which are linearized into prompts for the LLM. This bypasses the combinatorial explosion of exhaustive rule mining. Core assumption: semantic knowledge encoded in LLMs transfers to logical structure of specific KB relations, and subgraph context is sufficient for pattern induction.

### Mechanism 2: Rule Reasoner for Hallucination Mitigation
A differentiable reasoning component learns to weigh LLM-proposed rules, effectively down-weighting or discarding hallucinated or irrelevant rules while retaining high-quality ones. The Rule Reasoner grounds each proposed rule in the KB, computes satisfaction matrices, and learns a score for each rule. Core assumption: high-quality rules are consistently valid across KB's entity pairs, while hallucinated rules have low or inconsistent grounding success.

### Mechanism 3: Hybrid Scoring for Completion
Combining weighted logical rule scores with embedding-based scores provides more robust predictions than either method alone. The final prediction for a query (h, r, ?) is a weighted combination of scores from activated logical rules and a RotatE embedding model. Core assumption: logical rules and embeddings capture complementary information - rules handle high-confidence logical patterns while embeddings handle semantic similarity and patterns not captured by proposed rules.

## Foundational Learning

**Concept: First-Order Logic (Horn Clauses)**
- Why needed here: System's core output and reasoning are expressed as logical rules (e.g., IF A ∧ B THEN C). Understanding rule structure (body, head, predicates) is essential for interpreting model behavior.
- Quick check question: Can you explain the meaning of the rule IF (A, parent, B) ∧ (B, parent, C) THEN (A, grandparent, C) in plain English?

**Concept: Knowledge Graph Embeddings (RotatE)**
- Why needed here: Rule Reasoner incorporates RotatE model to handle cases not covered by logical rules. Understanding that RotatE represents entities and relations as vectors in complex space to model relational patterns is key to understanding hybrid component.
- Quick check question: What type of relational pattern is RotatE specifically designed to model well? (Hint: think about symmetry and inversion)

**Concept: Graph Traversal (BFS/Random Walk)**
- Why needed here: Subgraph Extractor uses multi-hop traversal from target triplet to sample relevant subgraph. This context is input to LLM. Understanding how traversal defines LLM's receptive field is important.
- Quick check question: If you increase the number of hops in traversal, how might the size of subgraph and complexity of proposed rules change?

## Architecture Onboarding

**Component map**: Input (KB triplets) -> Subgraph Extractor (samples subgraphs with multi-hop traversal) -> LLM Proposer (generates rules with filtering) -> Rule Reasoner (grounds rules, learns weights, combines with RotatE)

**Critical path**: Subgraph Extractor -> LLM Proposer (Filtering) -> Rule Reasoner (Grounding & Weight Learning). This pipeline transforms raw graph structure into weighted set of logical rules for inference.

**Design tradeoffs**:
- LLM Choice: GPT-4 proposes more rules but not all are better; GPT-3.5 offers good balance of quantity and quality; Gemini-1.5 is also competitive
- Subgraph Size: Traversal depth and neighbor limits control context length for LLM; larger subgraphs provide more context but increase token cost and may introduce noise
- Rule Complexity: Focuses on conjunctions (AND conditions); disjunctions (OR) and negations (NOT) are more expressive but harder to process and ground

**Failure signatures**:
- Low Rule Coverage: If LLM fails to propose rules for relation, system falls back entirely to embeddings
- Hallucinated Vocabulary: LLM creates rules with relations not in KB's schema; semantic mapping step must handle this
- Overfitting to Subgraph: GPT-4 observed to propose rules overfitted to specific subgraph in prompt, hurting generalization

**First 3 experiments**:
1. Ablation on LLM Backbone: Run LeSR on small dataset (e.g., WN18RR) using GPT-3.5, GPT-4, and Gemini-1.5; compare number of proposed rules, high-confidence rules, and final MRR
2. Ablation on Weight Learning: Run LeSR (GPT-3.5) with and without Rule Reasoner's weight learning (using uniform weights); compare performance on dataset like UMLs
3. Sensitivity to Subgraph Parameters: Modify Subgraph Extractor's traversal depth (e.g., 2-hop vs 3-hop) on FB15K-237; analyze how diversity of proposed rules and final KBC performance change

## Open Questions the Paper Calls Out

**Open Question 1**: How can the framework be extended to support complex logical structures involving disjunctions (OR) and negations (NOT) rather than exclusively conjunctions?
- Basis in paper: Authors state they "focus exclusively on conjunctions (i.e., AND conditions)" to simplify rule induction process
- Why unresolved: Extending IF-THEN logic to include OR/NOT operators significantly increases search space complexity and requires reformulation of matrix-based Rule Grounding operations

**Open Question 2**: Can specific prompting strategies or filtering mechanisms be developed to prevent high-capacity LLMs from overfitting to local context of sampled subgraphs?
- Basis in paper: Section 4.2 observes that GPT-4 proposes significantly more rules than GPT-3.5, but these tend to be "patterns restricted to the knowledge subgraph included in the LLM prompt," making them ungeneralizable
- Why unresolved: Paper identifies that stronger LLMs may hallucinate rules valid only for specific subgraph instance rather than global KB structure, but offers no solution to mitigate this local overfitting

**Open Question 3**: How does increasing multi-hop traversal depth beyond three steps affect quality and reliability of proposed rules in sparse knowledge bases?
- Basis in paper: Implementation "set[s] the maximum steps of multi-hop traversal to three" to manage context length, leaving potential for longer reasoning chains in complex graphs unexplored
- Why unresolved: While longer chains could capture distant dependencies (especially in sparse datasets like CN100K), they likely introduce exponential noise and increase difficulty of Rule Reasoner's scoring task

## Limitations

- Paper does not specify exact number of triplets (m) sampled per relation for subgraph extraction, which could affect context quality and computational cost
- Uses "expert prompt" and "semantic mapping" strategy to handle LLM hallucination and vocabulary mismatches, but these processes are not fully detailed
- Rule Reasoner's weight learning and joint optimization with RotatE embeddings are complex, with ablation studies suggesting this is critical component whose exact tuning could significantly impact results

## Confidence

**High Confidence**: Core mechanism of using LLMs for rule proposal with subgraph context is novel and paper provides clear descriptions of overall pipeline and advantages over pure embedding or traditional rule-based methods

**Medium Confidence**: Rule Reasoner's ability to effectively filter hallucinations and improve reliability is supported by ablation studies, but exact implementation details of semantic mapping and expert prompting are not fully specified

**Medium Confidence**: Claim of strong performance on commonsense knowledge bases (CN100K) is supported by results, but paper notes standard rule-learning methods fail on such data due to open-world assumption, suggesting LLM's semantic knowledge is crucial

## Next Checks

1. Replicate the Expert Prompt and Semantic Mapping: Obtain and test exact prompt templates and semantic mapping procedure used in LLM Proposer to verify they effectively handle vocabulary mismatches and hallucinations

2. Test Rule Reasoner Ablation with Specified Parameters: Run Rule Reasoner ablation (w/o WL) on CN100K with exact optimizer settings (AdamW, lr=0.001, weight_decay=0.1, StepLR) to confirm magnitude of performance drop

3. Analyze Rule Grounding Coverage: For small dataset (e.g., UMLs), measure proportion of test queries covered by at least one learned rule versus those handled only by RotatE, to quantify hybrid method's reliance on each component