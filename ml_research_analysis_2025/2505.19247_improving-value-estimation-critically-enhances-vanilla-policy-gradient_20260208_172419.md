---
ver: rpa2
title: Improving Value Estimation Critically Enhances Vanilla Policy Gradient
arxiv_id: '2505.19247'
source_url: https://arxiv.org/abs/2505.19247
tags:
- policy
- value
- gradient
- estimation
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that the primary factor distinguishing
  modern policy gradient algorithms from vanilla policy gradient (VPG) is the number
  of value update steps per iteration, rather than trust region enforcement. By simply
  increasing value update steps, VPG can match or outperform PPO across standard continuous
  control benchmarks.
---

# Improving Value Estimation Critically Enhances Vanilla Policy Gradient

## Quick Facts
- **arXiv ID:** 2505.19247
- **Source URL:** https://arxiv.org/abs/2505.19247
- **Reference count:** 23
- **One-line primary result:** Increasing value network update steps (K_V ≥ 50) per policy iteration enables vanilla policy gradient to match or exceed PPO performance across continuous control benchmarks.

## Executive Summary
This paper challenges the conventional wisdom that trust region enforcement is the key differentiator between modern policy gradient algorithms and vanilla policy gradient (VPG). Through empirical and theoretical analysis, the authors demonstrate that the primary performance driver is the number of value update steps per iteration. By simply increasing value gradient steps from 1 to 50+ per policy update, VPG can match or outperform PPO in standard continuous control tasks while being significantly more robust to hyperparameter choices. The theoretical analysis reveals that policy objectives in chaotic environments are rapidly changing and fractal-like, requiring more value network updates than policy updates to accurately track true returns.

## Method Summary
The method involves implementing VPG with separate optimization loops where the value network receives K_V gradient steps per single policy network step. The policy network has architecture [64, 64] with tanh activations outputting action distribution parameters, while the value network has the same architecture outputting scalar value estimates. The key modification from standard VPG is performing K_V ≥ 50 value network gradient steps on the squared error loss before each policy update. Training uses 16 parallel environments, batch size 2048, Adam optimizer, γ=0.99, GAE λ=0.95, and task-specific learning rates (0.0003-0.0007).

## Key Results
- VPG with 50+ value steps per iteration achieves performance comparable to or better than PPO across Halfcheetah, Hopper, Walker, Ant, and Humanoid tasks
- PPO with 10 epochs violates trust region bounds (max r_t > 1+ε) yet outperforms single-epoch PPO and VPG that successfully enforce trust regions
- VPG with 100 value steps shows significantly better robustness to GAE factor λ changes compared to PPO
- Value estimation error correlates strongly with policy performance, with errors dropping near zero when K_V ≥ 50

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing value network update steps per iteration is the primary driver of policy gradient performance improvement, not trust region enforcement.
- **Mechanism:** Modern algorithms (PPO/TRTO) implicitly increase value steps through mini-batching and multiple epochs. When VPG is modified to perform 50+ value gradient steps per iteration, it matches or exceeds PPO performance because the critic can accurately track rapidly changing true returns.
- **Core assumption:** The value network landscape is smooth (Lipschitz continuous) while policy objectives can be fractal and rapidly changing, requiring asymmetric update frequencies.
- **Evidence anchors:**
  - [abstract] "the more critical factor is the enhanced value estimation accuracy from more value update steps in each iteration"
  - [Section 6, Figure 4] VPG with 50+ value steps achieves comparable performance to PPO across Halfcheetah, Hopper, Walker, Ant, and Humanoid
  - [corpus] Weak direct support; related work focuses on PPO convergence rather than value estimation mechanisms
- **Break condition:** If policy and value landscapes have similar smoothness properties, or if environment dynamics lack chaotic behavior (λ(θ) < -log γ), the asymmetric update requirement diminishes.

### Mechanism 2
- **Claim:** Trust region enforcement shows no clear correlation with policy improvement in practice.
- **Mechanism:** PPO's clipping mechanism creates vanishing gradients outside the trust region—if policy makes an excessively large step, it cannot be pulled back. Standard PPO (10 epochs) consistently violates ratio bounds yet outperforms single-epoch PPO and VPG that successfully enforce trust regions.
- **Core assumption:** The gap between TRPO/PPO theory and practice is substantial; the theoretical guarantee requires conditions rarely met during training.
- **Evidence anchors:**
  - [Section 4, Figure 2] PPO with 10 epochs violates trust region (max r_t > 1+ε) but achieves highest reward; single-epoch PPO and VPG enforce bounds but underperform
  - [Section 4] "there is no direct relationship between enforcing a trust region and improving algorithmic performance"
  - [corpus] GRADE paper notes PPO "suffers from high variance gradient estimates, requiring careful hyperparameter tuning"
- **Break condition:** If environments have sufficiently smooth policy landscapes where trust region theory assumptions hold, correlation might emerge.

### Mechanism 3
- **Claim:** Value networks require K_V ≥ (K₁β₁^α)/(K₂β₂) update steps per policy step, where α = -log(γ)/λ(θ) < 1 for chaotic environments.
- **Mechanism:** Policy objectives in continuous control are α-Hölder continuous with α < 1 when maximal Lyapunov exponent λ(θ) > -log(γ), creating fractal landscapes where small parameter changes cause large objective shifts. Value networks are Lipschitz continuous (α = 1), requiring more aggressive optimization to track policy changes.
- **Core assumption:** Dynamics, reward function, policy, and value networks are all Lipschitz continuous with respect to inputs; chaotic dynamics with positive Lyapunov exponents.
- **Evidence anchors:**
  - [Section 5.3, Theorem 5.2] Formal derivation of value-to-policy step ratio requirement
  - [Section 5.3, Proposition C.2] "Policy objective J(θ) is α-Hölder continuous in the policy parameter θ" when λ(θ) > -log(γ)
  - [corpus] No direct corpus support for this theoretical framework
- **Break condition:** If λ(θ) ≤ -log(γ) (non-chaotic dynamics), then α ≥ 1 and the asymmetric update requirement weakens or inverts.

## Foundational Learning

- **Concept: Actor-Critic Architecture with Separate Networks**
  - Why needed here: The paper emphasizes that policy and value networks require different update frequencies; shared networks create interference
  - Quick check question: Can you explain why using separate networks for policy (actor) and value (critic) enables asymmetric update schedules?

- **Concept: Generalized Advantage Estimation (GAE)**
  - Why needed here: GAE factor λ significantly affects both VPG and PPO; VPG with 100 value steps is more robust to λ changes than PPO
  - Quick check question: What happens to the variance-bias tradeoff when λ changes from 0.95 (GAE) to 1.0 (Monte Carlo)?

- **Concept: Importance Sampling and Probability Ratios**
  - Why needed here: PPO's ratio clipping r_t = π_θ/π_old is central to understanding why trust region theory doesn't match practice
  - Quick check question: Why does the gradient vanish when r_t exceeds the clipping bounds, and what implication does this have for recovery from large policy steps?

## Architecture Onboarding

- **Component map:**
  - Parallel environments (16) → Batch collector (2048 samples) → Advantage calculator (GAE λ=0.95) → Value network [64,64] tanh → Value loss (squared error) → Value optimizer (Adam) → Policy network [64,64] tanh → Policy loss (VPG) → Policy optimizer (Adam)

- **Critical path:**
  1. Collect batch of trajectories (batch_size=2048)
  2. Compute advantages Â_t using GAE
  3. **Perform K_V ≥ 50 value network gradient steps** on value loss
  4. Perform 1 policy network gradient step on VPG loss
  5. Repeat

- **Design tradeoffs:**
  - More value steps → better value estimation → better policy gradients, but increased compute per iteration
  - Separate optimization loops (vs. single loop) → enables asymmetric update frequencies without interference
  - VPG simplicity vs. PPO sample efficiency: PPO's multiple epochs improve sample efficiency but reduce robustness (sensitive to hyperparameters in Ant/Humanoid)

- **Failure signatures:**
  - VPG with 1 value step: value estimation error grows, policy plateaus early
  - PPO with full-batch updates: value estimation error explodes, policy fails to learn
  - PPO with default settings in Ant/Humanoid: performance drops after 2M steps (policy updated too aggressively)

- **First 3 experiments:**
  1. **Baseline reproduction:** Implement VPG with K_V=1 on Hopper; verify poor performance and large value estimation error (expect ~1500 reward vs. PPO ~3500)
  2. **Value step scaling:** Increase K_V from 1 to 10, 50, 100 on Hopper; plot both reward and value estimation error η(s₀) to confirm correlation
  3. **Trust region ablation:** Compare PPO with 1 epoch (enforces trust region) vs. 10 epochs (violates trust region) on Hopper; verify that trust region violation correlates with better performance

## Open Questions the Paper Calls Out

- **Question:** Does the dominance of value estimation accuracy over trust region enforcement generalize to discrete action spaces and Large Language Model (LLM) training?
- **Basis in paper:** [explicit] The authors explicitly limit their scope, stating, "the present analysis is confined to continuous control problems," while noting potential generalizability to LLMs.
- **Why unresolved:** The empirical validation focused exclusively on continuous control benchmarks (MuJoCo).
- **What evidence would resolve it:** Benchmarking VPG with multiple value steps against PPO on discrete environments (e.g., Atari) or RLHF tasks.

- **Question:** Can an adaptive schedule for value update steps improve efficiency compared to the fixed hyperparameters used?
- **Basis in paper:** [inferred] Theoretical analysis (Theorem 5.2) links the required value steps ($K_V$) to the Maximal Lyapunov exponent ($\lambda$) and learning rates, suggesting the optimal number varies with the policy's chaotic dynamics.
- **Why unresolved:** Experiments utilized a static number of value steps (e.g., 50 or 100) throughout training.
- **What evidence would resolve it:** An algorithm that dynamically adjusts $K_V$ based on real-time estimates of landscape smoothness or value loss convergence.

- **Question:** Does accurate value estimation alone suffice to stabilize multiple policy updates per data collection, removing the need for PPO's clipping mechanism?
- **Basis in paper:** [inferred] The paper notes PPO's clipping allows multiple policy updates for sample efficiency, whereas VPG updates once. It is unclear if accurate values prevent the instability usually caused by multiple unclipped VPG updates.
- **Why unresolved:** The enhanced VPG implementation restricted policy updates to one per iteration.
- **What evidence would resolve it:** Testing VPG with 50+ value steps and multiple policy epochs (without clipping) to observe if accurate values maintain stability.

## Limitations

- The theoretical framework assumes chaotic dynamics with positive Lyapunov exponents, but the paper doesn't characterize which environments exhibit this property
- The optimal value-to-policy update ratio may vary significantly across different task families beyond the tested continuous control benchmarks
- The relationship between value estimation error and policy performance might be influenced by other factors like reward scaling or advantage normalization

## Confidence

- **High confidence:** The empirical observation that increasing value update steps improves VPG performance is well-supported by the results. The correlation between value estimation error and policy performance is clearly demonstrated.
- **Medium confidence:** The claim that trust region enforcement shows no clear correlation with policy improvement is supported by the PPO experiments but may depend on specific hyperparameter choices and environments.
- **Low confidence:** The theoretical derivation requiring K_V ≥ (K₁β₁^α)/(K₂β₂) update steps per policy step depends heavily on assumptions about environment chaos (λ(θ) > -log γ) that weren't empirically validated across diverse environments.

## Next Checks

1. **Environment chaos characterization:** Measure Lyapunov exponents across the tested environments to verify which exhibit the chaotic dynamics required for the theoretical framework
2. **Cross-task validation:** Test VPG with asymmetric updates on non-continuous control tasks (e.g., Atari, discrete action spaces) to assess generalizability
3. **Value network architecture ablation:** Compare the effect of value network capacity (width/depth) versus update frequency to isolate the impact of estimation accuracy