---
ver: rpa2
title: Toward Learning POMDPs Beyond Full-Rank Actions and State Observability
arxiv_id: '2601.18930'
source_url: https://arxiv.org/abs/2601.18930
tags:
- state
- observation
- states
- learning
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning the parameters of discrete
  Partially Observable Markov Decision Processes (POMDPs) from action-observation
  sequences, relaxing assumptions about full-rank transition matrices and unique observation
  distributions. The key innovation is leveraging Predictive State Representations
  (PSRs) to learn POMDP parameters up to a similarity transform, which can then be
  estimated via tensor decomposition methods.
---

# Toward Learning POMDPs Beyond Full-Rank Actions and State Observability

## Quick Facts
- **arXiv ID:** 2601.18930
- **Source URL:** https://arxiv.org/abs/2601.18930
- **Reference count:** 40
- **Primary result:** A method that learns POMDP parameters (transition and observation matrices) up to observability partitions from action-observation sequences, without requiring full-rank assumptions.

## Executive Summary
This paper introduces a spectral learning approach for POMDPs that relaxes the full-rank action and state observability assumptions common in prior work. The method learns POMDP parameters up to observability partitions by first constructing a Hankel matrix from observed data, performing rank factorization to obtain a PSR model, and then recovering the similarity transform via joint diagonalization. This enables learning explicit transition and observation matrices that can be used for planning with arbitrary reward functions, even when states have identical observation distributions. The approach is validated on standard POMDP benchmarks and demonstrates a fundamental limitation: POMDPs can only be distinguished up to observability partitions from sequential data alone.

## Method Summary
The method learns POMDP parameters from action-observation trajectories by constructing a Hankel matrix of history-future correlations, performing truncated SVD to identify the number of latent states, and converting the resulting PSR model to explicit transition and observation matrices. The key innovation is recovering the similarity transform between PSR parameters and POMDP parameters through joint diagonalization of observation-transition products for full-rank actions. This allows the method to handle state aliasing where multiple states share identical observation distributions, learning transitions between partitions rather than individual states.

## Key Results
- The method successfully learns POMDP parameters up to observability partitions for standard domains (Tiger, T-Maze) and novel domains with state aliasing
- Learned models enable effective planning with different reward functions not present during training
- The approach proves that learning POMDPs beyond observability partitions is impossible from sequential data alone
- Empirical results show convergence to true POMDP parameters with sufficient exploration data (10⁵-10⁷ interactions)

## Why This Works (Mechanism)

### Mechanism 1: Spectral Rank as a Proxy for State Cardinality
The method estimates the number of hidden states $|S|$ by determining the rank of a history-test correlation matrix, bypassing the need for manual state-space specification. The Hankel matrix $H$ constructed from action-observation trajectories has rank equivalent to the number of distinct latent states when the underlying Forward and Backward matrices are full-rank. Truncated SVD identifies this rank, providing an automatic way to determine state cardinality.

### Mechanism 2: Similarity Transform Recovery via Joint Diagonalization
PSR parameters can be converted into explicit Transition ($T$) and Observation ($O$) matrices by solving for a similarity transform $P$. The PSR update matrices are linear transformations of true POMDP parameters, and certain matrix products in the PSR domain share the same eigenvectors (the columns of $P$). Joint diagonalization using random linear combinations recovers $P$ when at least one action has a non-singular transition matrix.

### Mechanism 3: Relaxing Observability via Partitioning
The architecture handles "aliased states" (states with identical observation distributions) by learning transitions between partitions of states rather than individual states. When states share identical observation distributions across all actions, they are mathematically indistinguishable by the spectral method. The algorithm detects this by identifying repeated eigenvalues during diagonalization and treats these states as a single block or partition.

## Foundational Learning

- **Concept:** Hankel Matrix & Spectral Learning
  - **Why needed here:** This is the core data structure used to infer latent state dimensionality from purely observable data
  - **Quick check question:** Can you explain how the rank of a matrix formed by history-future correlations relates to the minimal number of hidden states required to model a system?

- **Concept:** Predictive State Representations (PSRs)
  - **Why needed here:** The method operates by first learning a PSR (a linear operator model) and then converting it; understanding this intermediate representation is crucial
  - **Quick check question:** How does a PSR update its "state" vector upon receiving a new action and observation without explicitly tracking a belief over hidden states?

- **Concept:** Joint Diagonalization
  - **Why needed here:** This is the algorithmic step that recovers the specific structure of the POMDP parameters from the generic PSR matrices
  - **Quick check question:** If a set of matrices $M_i$ share a common eigenbasis $P$, how can you recover $P$ if standard eigendecomposition of individual $M_i$ is ambiguous?

## Architecture Onboarding

- **Component map:** Hankel Estimator -> SVD Engine -> Full-Rank Filter -> Tensor Decomposer -> Parameter Projector
- **Critical path:** The success of the entire pipeline hinges on the **Full-Rank Filter**. If the threshold for the minimum singular value ($\sigma_{min}$) is set incorrectly, the system either fails to find a basis for diagonalization (too strict) or attempts to diagonalize noisy/rank-deficient matrices (too loose)
- **Design tradeoffs:**
  - Hankel Size: Larger history/test lengths increase rank accuracy but cause exponential growth in memory/runtime
  - Exploration Policy: Random exploration is assumed; non-memoryless policies require importance sampling corrections not implemented in the base algorithm
- **Failure signatures:**
  - Undetermined Partition: The algorithm outputs partition-level transitions, but the number of partitions is smaller than the true number of states (states are "aliased")
  - Singular Hankel: The SVD step yields zero rank, usually due to insufficient data or a Hankel size that is too small for the system's observability length
- **First 3 experiments:**
  1. **Tiger Domain:** Verify the method can recover the full POMDP when observation distributions are unique
  2. **Sense-Float-Reset:** Test robustness against state aliasing (states share observation distributions)
  3. **Hallway (Reward Specification):** Validate that the learned explicit model can support new reward functions not present during training

## Open Questions the Paper Calls Out

### Open Question 1
Can the learning framework be extended to POMDPs where the Hankel matrix rank is strictly smaller than the minimal number of states (violating the full-rank Forward/Backward matrix assumption)? The Conclusion states, "One limitation of our work is the assumption that the forward and backward matrices are full-rank... the presence of this rank discrepancy suggests a broader algorithmic framework may be required." This remains unresolved because the current method relies on Hankel matrix rank equating to the number of states to estimate state dimensionality.

### Open Question 2
What are the formal sample complexity and computational tractability bounds (e.g., PAC-learning guarantees) for spectral POMDP learning in the multi-action case? Appendix B.2 notes, "Further investigation of computational tractability learning framework (e.g. PAC-learning) for the multi-action case would be an interesting direction of future work." While the paper provides runtime complexity analysis, it does not establish formal sample efficiency guarantees for the multi-action POMDP setting.

### Open Question 3
How can the required sequence length (Hankel matrix size) be determined automatically without manual parameter tuning? Appendix C.1 states, "Automatically determining the proper Hankel size is an open question," noting that current spectral approaches rely on manually specified lengths. Selecting a sequence length that is too short results in a rank-deficient matrix, while selecting one that is too long is computationally expensive.

## Limitations
- Requires extensive exploration data (10⁵-10⁷ interactions) for reliable convergence
- Relies on tuned heuristics for Hankel size and SVD thresholds that may not generalize
- Fundamentally requires at least one full-rank action for joint diagonalization
- Cannot distinguish states that share identical observation distributions across all actions

## Confidence

- **High Confidence:** The spectral learning framework and Hankel matrix construction are well-established techniques with strong theoretical foundations. The proof of POMDP identifiability limits is rigorous.
- **Medium Confidence:** The joint diagonalization method for recovering similarity transforms is sound, but practical implementation details (random weight selection, convergence guarantees) require careful tuning.
- **Low Confidence:** The handling of approximation errors through quadratic programming projection lacks specification of exact constraints and objectives.

## Next Checks

1. **Cross-Domain Transferability:** Test the method on a completely different domain (e.g., robotic navigation or dialogue systems) without hand-tuning Hankel size or SVD thresholds to evaluate robustness.

2. **Minimum Data Requirements:** Systematically measure learning performance as a function of trajectory length to establish clear data efficiency bounds and identify when the method fails due to insufficient exploration.

3. **Noisy Observation Robustness:** Evaluate performance under noisy observations where observation distributions overlap, to understand the practical limits of the observability partition assumption.