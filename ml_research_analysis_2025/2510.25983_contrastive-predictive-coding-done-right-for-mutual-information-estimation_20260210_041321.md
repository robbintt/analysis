---
ver: rpa2
title: Contrastive Predictive Coding Done Right for Mutual Information Estimation
arxiv_id: '2510.25983'
source_url: https://arxiv.org/abs/2510.25983
tags:
- infonce
- estimation
- scoring
- infonce-anchor
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of mutual information (MI) estimation
  using contrastive learning objectives. While InfoNCE is widely used for MI estimation,
  the authors show it cannot provide consistent density ratio estimates and thus yields
  biased MI estimates.
---

# Contrastive Predictive Coding Done Right for Mutual Information Estimation

## Quick Facts
- arXiv ID: 2510.25983
- Source URL: https://arxiv.org/abs/2510.25983
- Reference count: 40
- Primary result: Introduces InfoNCE-anchor, a simple modification that enables consistent density ratio estimation and significantly reduces bias in MI estimation while unifying contrastive objectives under proper scoring rules.

## Executive Summary
This paper addresses a fundamental limitation in mutual information (MI) estimation using contrastive learning objectives. While InfoNCE is widely used for MI estimation, the authors show it cannot provide consistent density ratio estimates due to an arbitrary scaling factor, leading to biased MI estimates. To resolve this, they introduce InfoNCE-anchor, which adds an auxiliary anchor class to the classification setup, enabling consistent density ratio estimation and plug-in MI estimation with significantly reduced bias. The authors generalize this framework using proper scoring rules, showing InfoNCE-anchor corresponds to the log score and unifying various contrastive objectives under a single principled framework. Empirically, InfoNCE-anchor achieves state-of-the-art MI estimation accuracy across multiple domains (Gaussian, MNIST, text), but surprisingly does not improve self-supervised representation learning performance compared to vanilla InfoNCE, suggesting that accurate MI estimation is not essential for contrastive learning success.

## Method Summary
The authors introduce InfoNCE-anchor, a modification to the standard InfoNCE objective that adds an auxiliary anchor class to enable consistent density ratio estimation. The method reformulates the contrastive learning problem as a multi-class classification task where samples are drawn from joint (positive) and marginal (negative) distributions, plus an anchor distribution. The InfoNCE-anchor loss includes an additional term for the anchor class, with weight ν controlling its importance. This modification resolves the scaling ambiguity inherent in vanilla InfoNCE, allowing for consistent density ratio estimation. The authors further generalize this framework using proper scoring rules, showing that InfoNCE-anchor corresponds to the canonical log score and can be derived from any strictly proper scoring rule. For implementation, they use separable critics (independent encoders for x and y) with MLP architectures, training with Adam optimizer (lr=1e-4) for 20k steps, using default parameters ν=1 and K=B-1 (batch negatives).

## Key Results
- InfoNCE-anchor achieves state-of-the-art MI estimation accuracy with significantly reduced bias compared to vanilla InfoNCE, JS estimator, and NWJ across Gaussian, MNIST, and text datasets
- The anchor modification enables consistent density ratio estimation, resolving a fundamental limitation of InfoNCE for plug-in MI estimation
- InfoNCE-anchor unifies various contrastive objectives under the framework of proper scoring rules, with the log score being optimal empirically
- Surprisingly, InfoNCE-anchor does not improve self-supervised representation learning performance compared to vanilla InfoNCE, suggesting accurate MI estimation is not essential for contrastive learning success

## Why This Works (Mechanism)

### Mechanism 1: Consistent Density Ratio Estimation via Anchor Class
Adding an auxiliary anchor class to the InfoNCE classification setup enables consistent density ratio estimation, removing a fundamental source of bias in MI estimation. In standard InfoNCE (ν=0), the learned critic r_θ(x) only recovers the density ratio q₁(x)/q₀(x) up to an arbitrary multiplicative constant, making it unusable for plug-in MI estimation. Introducing an anchor class (class 0, with ν>0) acts as a fixed reference distribution. This forces the model to resolve the scaling ambiguity, yielding r_θ(x) ∝ q₁(x)/q₀(x) exactly, as established by Fisher consistency (Theorem 3). This consistent ratio estimator can then be plugged into the MI definition for a low-bias estimate. The core assumption is that the critic network family is sufficiently expressive to represent the true density ratio function q₁(x)/q₀(x).

### Mechanism 2: Proper Scoring Rules Unification
The InfoNCE-anchor objective can be derived as a special case of density ratio estimation using proper scoring rules, specifically the log score. The paper reframes the multi-class classification problem (positive vs. negatives + anchor) as a class probability estimation problem. The cross-entropy loss used in InfoNCE-anchor is a proper scoring rule, meaning its unique minimizer is the true class probability. The paper generalizes this by showing that any strictly proper scoring rule λΨ induced by a convex function Ψ can be used. InfoNCE-anchor corresponds to the canonical log score (Ψ(ρ)=ρ log ρ), providing a principled unification with other estimators like NWJ (which relates to different scoring rules). The core assumption is that the underlying classification problem is well-specified, meaning samples are drawn i.i.d. from the defined class distributions.

### Mechanism 3: MI Estimation Accuracy ≠ SSL Performance
Accurate MI estimation is not the primary driver of success in contrastive representation learning. This surprising negative result shows that while InfoNCE-anchor provides state-of-the-art MI estimation accuracy, it does not improve downstream task performance in self-supervised learning (SSL) compared to vanilla InfoNCE. This suggests that SSL benefits from the process of learning a structured factorization of the pointwise mutual information (PMI) or pointwise dependence, rather than from the absolute accuracy of the MI estimate itself. The vanilla InfoNCE's uncontrolled multiplicative factor C(y) appears to be either approximately constant or irrelevant for learning useful representations. The core assumption is that the downstream linear probe evaluation is a valid proxy for representation quality in SSL.

## Foundational Learning

- **Concept: Mutual Information (MI) and Density Ratio Estimation**
  - Why needed here: The entire paper is an analysis and correction of how InfoNCE estimates MI. MI I(X;Y) is defined as the expected log-density-ratio log[p(x,y)/(p(x)p(y))]. Understanding that MI estimation is fundamentally about density ratio estimation is crucial for grasping why the anchor class solves the problem.
  - Quick check question: Can you explain why a method that only estimates the density ratio up to a multiplicative constant cannot be used directly in the plug-in MI formula?

- **Concept: Variational Bounds (specifically DV, NWJ, InfoNCE)**
  - Why needed here: The paper categorizes MI estimators by how they relate training objectives to evaluation. Understanding that InfoNCE is a variational lower bound on a divergence, and that this bound is inherently loose, is key to understanding the motivation for InfoNCE-anchor.
  - Quick check question: What is the key limitation of "Type 1" estimators that optimize and evaluate with the same variational lower bound?

- **Concept: Proper Scoring Rules**
  - Why needed here: This is the unifying theoretical framework introduced in the paper. A proper scoring rule ensures that the optimal forecast is the true probability. The paper shows contrastive objectives are special cases of proper scoring rules for a classification task.
  - Quick check question: Why does the property of "strict propriety" guarantee a unique minimizer for the loss function?

## Architecture Onboarding

- **Component map:**
  1. **Critic Network c_θ(x, y)**: A neural network that outputs a real-valued score. It is exponentiated to form the density ratio estimator r_θ(x,y) = exp(c_θ(x,y)). In SSL, this is often a dot product of embeddings.
  2. **The Anchor Class (ν)**: A hyperparameter controlling the weight of an auxiliary "reference" class in the multi-way classification loss. It is the crucial modification that distinguishes InfoNCE-anchor from vanilla InfoNCE.
  3. **The Loss Function (L_{K;ν}(θ))**: The core objective function (Eq. 4). It combines the standard InfoNCE-like term over positive and negative samples with an additional term for the anchor class samples.

- **Critical path:**
  1. **For MI Estimation**: Train the critic c_θ by minimizing L_{K;ν}(θ) on a dataset of joint samples {(x, y)} and marginal samples. Use the final critic to compute a plug-in MI estimate: Î(X;Y) ≈ E_p(x,y)[c_θ*(x,y)].
  2. **For Representation Learning**: Train embedding functions f_θ(x) and g_θ(y) (e.g., as projections for the critic) by minimizing L_{K;ν}(θ). Discard the critic and use f_θ(x) as the learned representation for downstream tasks. (Note: The paper finds this path offers no benefit over vanilla InfoNCE).

- **Design tradeoffs:**
  - **ν (Anchor weight)**: ν > 0 ensures consistency but may slightly alter training dynamics. The paper defaults to ν=1. Setting ν=0 recovers vanilla InfoNCE (inconsistent for MI estimation but proven for SSL).
  - **K (Number of negatives)**: Larger K improves the tightness of the bound for MI estimation. In SSL, larger K is also generally beneficial.
  - **Scoring Rule**: The log score (cross-entropy) is shown to work best empirically, despite the theoretical equivalence of other proper rules like the spherical score, which can collapse in practice (Table 2).

- **Failure signatures:**
  - **High-bias MI estimates**: Likely due to setting ν=0 (using vanilla InfoNCE) or using an insufficiently large K for high MI values.
  - **Collapsed representations in SSL**: May occur if using a scoring rule with poor optimization dynamics (e.g., the spherical score in Table 2) or with an inappropriate temperature τ.
  - **Unstable training**: Can arise from the exponentiation of a critic that outputs very large positive or negative values, a common issue in contrastive learning often mitigated by careful temperature scaling.

- **First 3 experiments:**
  1. **Validate MI Estimation on Gaussians**: Replicate a simple experiment from Section 4.1. Generate multivariate Gaussian data with known MI. Train an MLP critic with both InfoNCE and InfoNCE-anchor. Plot the estimated MI vs. the ground truth to verify the bias reduction from the anchor.
  2. **Ablation on ν and K**: Using the same Gaussian setup, systematically vary ν (e.g., 0, 0.5, 1, 2) and K. Observe how the bias-variance tradeoff of the MI estimator changes. This builds intuition for the hyperparameters.
  3. **Minimal SSL Comparison**: Use a simple framework like solo-learn (as in the paper) to pretrain a small ResNet on a subset of CIFAR-10. Compare vanilla InfoNCE vs. InfoNCE-anchor (with ν=1) via linear probing accuracy. The goal is to reproduce the paper's negative result and understand that better MI estimation does not imply better representations.

## Open Questions the Paper Calls Out

- **Open Question 1**: Is the canonical proper loss function λΨ a unique element of the equivalence class Λo(Ψ)?
  - Basis in paper: [explicit] Appendix D.4 states: "A small open question is whether λΨ is an unique element of Λo(Ψ)."
  - Why unresolved: The paper establishes that proper scoring rules form equivalence classes based on Bregman divergences, and Λo(Ψ) contains canonical functions satisfying gλ(η)≡0, but uniqueness within this class remains unproven.
  - What evidence would resolve it: A formal proof of uniqueness, or a counterexample constructing distinct loss functions within Λo(Ψ).

- **Open Question 2**: What specific optimization dynamics cause spherical scoring rules to collapse in self-supervised learning?
  - Basis in paper: [inferred] Table 2 shows Spherical achieves only 4.33% top-1 accuracy versus 65.98% for InfoNCE, with the authors noting "collapse entirely, likely due to unfavorable optimization dynamics."
  - Why unresolved: While the paper shows spherical scores are theoretically valid proper scoring rules, their empirical failure in SSL is not analytically explained.
  - What evidence would resolve it: Analysis of gradient landscapes, optimization trajectories, or comparisons of Hessian conditioning between log and spherical scores during training.

- **Open Question 3**: Why does accurate MI estimation not improve downstream representation quality in contrastive learning?
  - Basis in paper: [explicit] Section 4.3 states InfoNCE-anchor "does not translate into better SSL performance" despite improved density ratio estimation, concluding "neither accurate MI estimation nor exact density ratio recovery is essential."
  - Why unresolved: The finding contradicts the historical assumption that maximizing MI drives representation quality; the paper can only hypothesize that "factorization of PMI" and "large K" matter more.
  - What evidence would resolve it: Ablation studies isolating PMI factorization quality versus MI estimation accuracy, or interventions that control one while varying the other.

- **Open Question 4**: What is the practical behavior of the multiplicative factor C(y) in vanilla InfoNCE during representation learning?
  - Basis in paper: [inferred] Section 3.1 notes InfoNCE may lead to "undesirable behavior due to uncontrollable C(y)," yet Section 4.3 finds the anchor (which eliminates C(y)) provides no benefit, suggesting C(y) may be "nearly constant or irrelevant."
  - Why unresolved: The discrepancy between theoretical concerns about C(y) and empirical irrelevance is unexplained.
  - What evidence would resolve it: Direct measurement of C(y) learned by vanilla InfoNCE across different datasets and architectures to quantify its variability.

## Limitations

- The surprising finding that accurate MI estimation does not improve SSL performance may be architecture-dependent and requires further validation across different frameworks and evaluation protocols
- The paper assumes the critic family is sufficiently expressive for consistent estimation, but this is not empirically validated in the high-dimensional cases
- The choice of ν=1 is used throughout, but the optimal value may be problem-dependent and is not systematically explored
- The SSL experiments may be limited to specific architectures and evaluation protocols, potentially restricting generalizability

## Confidence

- **High**: The theoretical analysis of InfoNCE's bias and the anchor's resolution mechanism; empirical MI estimation results across Gaussian, MNIST, and text datasets
- **Medium**: The proper scoring rule unification framework and its relationship to existing estimators; the negative SSL result suggesting MI accuracy is not the key driver for representation learning
- **Low**: The generalization of these findings to all contrastive learning architectures and data modalities beyond those tested

## Next Checks

1. **Test InfoNCE-anchor in non-separable critic architectures** (e.g., joint encoders) to verify the SSL negative result is not an artifact of the separable design
2. **Systematically vary ν** (anchor weight) on the Gaussian benchmark to map out the bias-variance tradeoff and identify optimal values for different MI levels
3. **Implement InfoNCE-anchor for SSL on a different dataset** (e.g., ImageNet-100) and evaluate with non-linear evaluation heads (e.g., fine-tuning) to test the robustness of the MI accuracy vs. representation quality finding