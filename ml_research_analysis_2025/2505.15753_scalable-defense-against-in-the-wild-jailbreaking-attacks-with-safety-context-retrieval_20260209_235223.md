---
ver: rpa2
title: Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context
  Retrieval
arxiv_id: '2505.15753'
source_url: https://arxiv.org/abs/2505.15753
tags:
- safety
- attacks
- against
- attack
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of defending large language
  models (LLMs) against evolving jailbreaking attacks, which exploit carefully engineered
  prompts to induce harmful responses. The authors propose Safety Context Retrieval
  (SCR), a scalable defense mechanism that leverages retrieval-augmented generation
  to dynamically incorporate safety contexts during inference.
---

# Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval

## Quick Facts
- **arXiv ID**: 2505.15753
- **Source URL**: https://arxiv.org/abs/2505.15753
- **Reference count**: 40
- **Primary result**: Safety Context Retrieval (SCR) achieves 2.5% attack success rate against jailbreaking attacks, compared to 34.9% without defense

## Executive Summary
This paper addresses the challenge of defending large language models against evolving jailbreaking attacks that exploit carefully engineered prompts to induce harmful responses. The authors propose Safety Context Retrieval (SCR), a scalable defense mechanism that leverages retrieval-augmented generation to dynamically incorporate safety contexts during inference. SCR maintains a pool of safety-aligned examples and retrieves relevant contexts to guide the model's response when faced with potentially harmful prompts.

The key finding is that SCR significantly outperforms existing defense mechanisms while maintaining natural performance across reasoning tasks. The approach demonstrates robustness against common attacks like GCG-T and in-the-wild variants, achieving an average attack success rate of only 2.5% compared to 34.9% without defense. The system's scalability and effectiveness make it a practical solution for enhancing LLM safety in real-world applications.

## Method Summary
Safety Context Retrieval (SCR) is a defense mechanism that dynamically incorporates safety contexts during LLM inference to prevent jailbreaking attacks. The system maintains a static pool of safety-aligned examples and uses a retrieval model to identify relevant contexts when faced with potentially harmful prompts. During inference, the retrieved safety contexts are prepended to the original prompt, guiding the model toward safe responses. SCR employs an alignment module to calibrate retrieved contexts and ensure they effectively steer the model away from harmful outputs. The approach is designed to be scalable, requiring no fine-tuning of the base LLM and maintaining minimal impact on natural task performance.

## Key Results
- SCR achieves an average attack success rate of only 2.5% against common jailbreaking attacks, compared to 34.9% without defense
- The approach effectively defends against in-the-wild attacks while maintaining minimal impact on natural performance across reasoning tasks
- SCR demonstrates robustness, scalability, and harmlessness as a practical solution for LLM safety enhancement

## Why This Works (Mechanism)
SCR works by leveraging retrieval-augmented generation to dynamically incorporate safety contexts during inference. When a potentially harmful prompt is detected, the system retrieves relevant safety-aligned examples from a pre-maintained pool and prepends them to the original prompt. This guides the LLM toward generating safe responses by providing contextual examples of appropriate behavior. The alignment module ensures that retrieved contexts are properly calibrated to effectively steer the model away from harmful outputs, while the static nature of the safety example pool eliminates the need for continuous fine-tuning or updates.

## Foundational Learning

**Retrieval-augmented generation**: Why needed - Enables dynamic incorporation of relevant safety contexts during inference without modifying the base model. Quick check - Verify retrieval accuracy and relevance of safety contexts for diverse attack types.

**Safety context alignment**: Why needed - Ensures retrieved safety examples effectively guide the model toward appropriate responses. Quick check - Measure alignment between retrieved contexts and desired safe outputs across various scenarios.

**Static safety example pools**: Why needed - Provides a stable foundation of safety-aligned examples without requiring continuous updates. Quick check - Assess pool coverage and relevance across diverse attack patterns and domains.

**Attack success rate (ASR) metrics**: Why needed - Quantifies the effectiveness of defense mechanisms against jailbreaking attempts. Quick check - Calculate ASR across multiple attack types and compare against baseline performance.

**Minimal performance degradation**: Why needed - Ensures safety mechanisms don't compromise the model's ability to perform natural tasks. Quick check - Evaluate task performance across reasoning benchmarks with and without defense mechanisms.

## Architecture Onboarding

**Component map**: Input prompt → Retrieval model → Safety context pool → Alignment module → Prepended context → Base LLM → Output

**Critical path**: The most critical path is the retrieval and alignment process, where relevant safety contexts must be quickly identified and properly calibrated before being prepended to the original prompt. This path directly determines the defense's effectiveness and response time.

**Design tradeoffs**: The system trades off between comprehensive safety coverage and computational efficiency by using a static safety pool rather than continuous fine-tuning. This approach prioritizes scalability and deployment simplicity but may face challenges with novel attack patterns not represented in the static pool.

**Failure signatures**: Key failure modes include retrieval model failure to identify relevant safety contexts, alignment module producing ineffective context calibration, and attacks designed to bypass or exploit the retrieval mechanism itself. Performance degradation may also occur when the safety pool lacks coverage for specific attack types.

**First experiments**: 
1. Test retrieval accuracy and relevance across diverse attack categories
2. Measure attack success rate against common jailbreaking techniques (GCG-T, etc.)
3. Evaluate performance impact on standard reasoning benchmarks with and without SCR

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on specific attack types (GCG-T and in-the-wild variants) and may not generalize to all possible attack strategies
- Claims of minimal impact on natural performance require validation across broader reasoning tasks and domains
- Static safety example pool may not adapt well to novel threat scenarios over time

## Confidence
- **High confidence**: Experimental results showing SCR's effectiveness against tested jailbreaking attacks (2.5% ASR vs 34.9% baseline)
- **Medium confidence**: Claims about SCR's robustness and scalability supported by experiments but need broader validation
- **Medium confidence**: Assertion of minimal impact on natural performance demonstrated but limited to specific reasoning tasks

## Next Checks
1. Test SCR against adversarial attacks specifically designed to bypass the retrieval mechanism or manipulate retrieved contexts
2. Evaluate performance degradation and computational overhead when scaling to larger LLM architectures (e.g., GPT-4 class models)
3. Conduct longitudinal studies to assess how well the static safety example pool maintains effectiveness as new attack patterns emerge over time