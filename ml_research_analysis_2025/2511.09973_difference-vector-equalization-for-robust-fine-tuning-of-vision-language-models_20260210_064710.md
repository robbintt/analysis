---
ver: rpa2
title: Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models
arxiv_id: '2511.09973'
source_url: https://arxiv.org/abs/2511.09973
tags:
- dive
- performance
- fine-tuning
- embeddings
- flyp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of fine-tuning vision-language\
  \ models to improve in-distribution (ID) performance without degrading out-of-distribution\
  \ (OOD) and zero-shot generalization. Current robust fine-tuning methods, like FLYP\
  \ and ARF, distort the geometric structure of embeddings\u2014crucial for generalization\u2014\
  resulting in limited OOD and zero-shot performance."
---

# Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models

## Quick Facts
- **arXiv ID:** 2511.09973
- **Source URL:** https://arxiv.org/abs/2511.09973
- **Reference count:** 8
- **Primary result:** DiVE achieves 82.5% ID accuracy, 63.2% OOD average accuracy, and 63.7% zero-shot average accuracy on ImageNet, outperforming baselines while preserving geometric structure.

## Executive Summary
This paper addresses the challenge of fine-tuning vision-language models to improve in-distribution (ID) performance without degrading out-of-distribution (OOD) and zero-shot generalization. Current robust fine-tuning methods like FLYP and ARF distort the geometric structure of embeddings, which is crucial for generalization. The authors propose Difference Vector Equalization (DiVE), which constrains difference vectors (obtained by subtracting embeddings from pre-trained and fine-tuned models) to be equal across samples, thereby preserving the geometric structure. Experimental results show that DiVE significantly outperforms baselines on ImageNet, achieving state-of-the-art performance across ID, OOD, and zero-shot benchmarks.

## Method Summary
DiVE introduces a novel fine-tuning approach that preserves the geometric structure of vision-language embeddings by constraining difference vectors between pre-trained and fine-tuned models. The method uses two complementary losses: Average Vector Loss (AVL) and Pairwise Vector Loss (PVL). AVL constrains difference vectors to their exponential moving average across samples, while PVL aligns image-text difference vectors pairwise. The final loss combines contrastive loss on target data with DiVE losses weighted by λ=1000. Experiments use CLIP ViT-B/16 as the base model, trained on ImageNet with reference data from CC3M, achieving significant improvements in ID accuracy while maintaining strong OOD and zero-shot performance.

## Key Results
- Achieves 82.5% top-1 accuracy on ImageNet (ID)
- Maintains 63.2% average OOD accuracy across multiple benchmarks
- Preserves 63.7% average zero-shot accuracy, outperforming robust fine-tuning baselines
- Demonstrates superior RSA correlation scores, indicating better preservation of geometric structure

## Why This Works (Mechanism)
DiVE works by preserving the geometric structure of embeddings during fine-tuning, which is essential for maintaining generalization capabilities. By constraining difference vectors to be equal across samples through AVL and PVL losses, the method prevents the distortion that typically occurs with robust fine-tuning approaches. This preservation ensures that the model maintains its ability to generalize to unseen distributions and perform zero-shot classification, addressing the core trade-off between ID accuracy and OOD/generalization performance.

## Foundational Learning
- **Contrastive Learning:** Why needed - Forms the basis of CLIP's representation learning; Quick check - Verify cosine similarity between matched image-text pairs is maximized
- **Geometric Structure Preservation:** Why needed - Critical for maintaining generalization across distributions; Quick check - Monitor RSA correlation score during training
- **Exponential Moving Average (EMA):** Why needed - Provides stable reference for difference vector constraints; Quick check - Validate m updates correctly with α=0.99
- **Difference Vectors:** Why needed - Capture the transformation between pre-trained and fine-tuned embeddings; Quick check - Ensure L2-normalization before computing differences
- **Multi-task Optimization:** Why needed - Balances ID accuracy with OOD/generalization preservation; Quick check - Monitor all three accuracy metrics during training
- **Zero-shot Evaluation:** Why needed - Tests generalization to unseen tasks without fine-tuning; Quick check - Verify consistent prompts across all zero-shot datasets

## Architecture Onboarding

**Component Map:**
CLIP ViT-B/16 (pre-trained) -> Image Encoder f_θ + Text Encoder g_ϕ (fine-tuned) -> Contrastive Loss + DiVE Losses -> Updated Parameters

**Critical Path:**
Reference dataset sampling → Difference vector computation → AVL/PVL loss calculation → Parameter update → Evaluation on ID/OOD/zero-shot benchmarks

**Design Tradeoffs:**
- λ=1000 provides strong geometric preservation but may limit maximum ID accuracy
- Reference batch size B'=B ensures sufficient samples for stable EMA estimation
- 10 epochs with early stopping balances training time with performance

**Failure Signatures:**
- Training instability when λ is too large
- Poor OOD performance despite high ID accuracy (geometric structure not preserved)
- Zero-shot accuracy drops significantly below baseline (difference vector constraints too restrictive)

**Three First Experiments:**
1. Verify DiVE with λ=1000 on ImageNet vs standard fine-tuning baseline
2. Compare RSA correlation scores between DiVE and robust fine-tuning methods
3. Ablation study: Remove AVL or PVL to confirm complementary effects

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Reference dataset sampling strategy is underspecified, potentially affecting EMA stability
- Temperature parameter τ for contrastive loss is not explicitly stated
- Claims about geometric structure being the primary mechanism for improved generalization could benefit from additional visualization analyses

## Confidence

**Major Uncertainties and Limitations**

The paper presents a theoretically grounded approach to preserving geometric structure during fine-tuning, but several practical uncertainties could affect reproducibility. The reference dataset sampling strategy is underspecified - while CC3M is mentioned, the exact subset size and sampling frequency during training iterations could impact the EMA-based difference vector estimates. The temperature parameter τ for contrastive loss is not explicitly stated, which can significantly affect training dynamics. Additionally, the paper does not clarify whether the reference dataset is shuffled each epoch or if a fixed sampling order is used, which could influence the stability of the EMA updates.

**Confidence Assessment**

- **High Confidence:** The core mathematical formulation of DiVE (difference vector constraints, AVL and PVL losses) and its theoretical motivation are well-established and clearly presented. The reported performance gains over baselines on standard benchmarks are substantial and internally consistent.

- **Medium Confidence:** The experimental methodology is detailed enough to attempt reproduction, but critical hyperparameters (τ, reference sampling strategy) are missing. The RSA correlation metric is introduced but not extensively validated against other geometric similarity measures.

- **Low Confidence:** The claim that geometric structure preservation is the *primary* mechanism for improved OOD and zero-shot generalization, while supported by ablation studies, could benefit from additional analyses (e.g., visualization of embedding manifolds pre- and post-fine-tuning).

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary λ (DiVE loss weight) and τ (contrastive temperature) to confirm the reported sweet spot and identify failure modes.

2. **Geometric Structure Visualization:** Use t-SNE or UMAP to visualize how difference vectors evolve during training and verify that the geometric structure remains intact under DiVE but not under FLYP/ARF.

3. **Cross-Dataset Generalization:** Evaluate DiVE on additional datasets (e.g., VTAB, SUN397) beyond the standard ImageNet-OOD suite to test the robustness of the geometric preservation claim across diverse domains.