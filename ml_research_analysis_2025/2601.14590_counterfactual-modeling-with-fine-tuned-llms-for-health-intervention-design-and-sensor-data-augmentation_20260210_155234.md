---
ver: rpa2
title: Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design
  and Sensor Data Augmentation
arxiv_id: '2601.14590'
source_url: https://arxiv.org/abs/2601.14590
tags:
- data
- counterfactual
- feature
- class
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SenseCF, a method that fine-tunes large language
  models (LLMs) to generate counterfactual explanations for sensor-based health data.
  Using the multimodal AI-READI dataset, SenseCF improves intervention design and
  data augmentation by creating realistic, actionable counterfactual samples that
  flip model predictions.
---

# Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation

## Quick Facts
- arXiv ID: 2601.14590
- Source URL: https://arxiv.org/abs/2601.14590
- Reference count: 22
- Primary result: Fine-tuned LLMs generate high-fidelity counterfactuals for sensor data, improving intervention design and data augmentation in digital health.

## Executive Summary
This paper introduces SenseCF, a method that fine-tunes large language models (LLMs) to generate counterfactual explanations for sensor-based health data. Using the multimodal AI-READI dataset, SenseCF improves intervention design and data augmentation by creating realistic, actionable counterfactual samples that flip model predictions. Fine-tuned LLMs, especially LLaMA-3.1-8B, produce counterfactuals with up to 99% plausibility, 99% validity, and significant sparsity. When used to augment underrepresented classes, SenseCF recovers an average 20% F1-score under controlled label scarcity. The method outperforms optimization-based baselines like DiCE, CFNOW, and NICE by providing more clinically actionable explanations. Overall, SenseCF enhances model interpretability, robustness, and training efficiency in digital health applications.

## Method Summary
SenseCF fine-tunes a large language model (LLM) on a multimodal dataset combining health sensor readings and contextual features. The fine-tuned LLM is prompted to generate counterfactual explanations—hypothetical scenarios that would flip a model's prediction—by specifying target labels and providing both feature and class information. The method optimizes for plausibility, validity, and sparsity, ensuring that generated counterfactuals are realistic, actionable, and minimally altered from the original input. SenseCF is evaluated on the AI-READI dataset, demonstrating significant improvements in clinical interpretability and data augmentation for underrepresented classes.

## Key Results
- Fine-tuned LLMs achieve up to 99% plausibility and 99% validity in generated counterfactuals.
- Data augmentation with SenseCF counterfactuals recovers an average 20% F1-score under controlled label scarcity.
- SenseCF outperforms optimization-based baselines (DiCE, CFNOW, NICE) in clinical interpretability and actionability.

## Why This Works (Mechanism)
SenseCF leverages the strong generative and reasoning capabilities of fine-tuned LLMs to produce counterfactuals that are both valid (actually flip the prediction) and plausible (realistic and actionable in clinical settings). By conditioning on both sensor features and class labels, the model generates minimal, targeted interventions that are interpretable by clinicians. The fine-tuning process ensures the LLM is sensitive to the nuances of health sensor data, enabling it to create high-quality counterfactuals that directly support intervention design and model robustness.

## Foundational Learning
- **Counterfactual explanations**: Hypothetical scenarios that would change a model's prediction; needed for interpretability and intervention design; quick check: do generated examples actually flip the prediction?
- **Fine-tuning LLMs**: Adapting a pre-trained model to a specific domain; needed for domain-specific language and feature understanding; quick check: does the model generate realistic sensor-based explanations?
- **Data augmentation with counterfactuals**: Using synthetic examples to balance classes; needed to address label scarcity and class imbalance; quick check: does augmentation improve model performance on minority classes?
- **Validity, plausibility, sparsity metrics**: Quantitative measures for counterfactual quality; needed to ensure generated examples are both realistic and minimally altered; quick check: are generated counterfactuals both actionable and concise?

## Architecture Onboarding
- **Component map**: Raw sensor data -> Feature extraction -> LLM prompt (with target label) -> Counterfactual generation -> Evaluation (validity, plausibility, sparsity)
- **Critical path**: Input features and target label → LLM prompt → Generated counterfactual → Validation checks
- **Design tradeoffs**: Fine-tuning LLMs for specificity vs. retaining general reasoning; generating minimal vs. comprehensive interventions; prioritizing plausibility vs. strict validity
- **Failure signatures**: Plausibility drops if prompt lacks sufficient context; validity fails if counterfactual doesn't flip prediction; sparsity suffers if LLM overgenerates features
- **First experiments**:
  1. Generate counterfactuals for a single, well-understood sensor feature and manually validate plausibility.
  2. Measure validity by checking if counterfactuals flip predictions for a small, diverse set of samples.
  3. Quantify sparsity by comparing the number of altered features to the total number of features.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to a single dataset (AI-READI), restricting generalizability to other health conditions or sensor types.
- Clinical interpretability is asserted but not validated through expert review or real-world intervention trials.
- Data augmentation results are measured under controlled label scarcity, which may not reflect real-world class imbalance.
- Computational cost and scalability of fine-tuning LLMs for large sensor datasets are not fully explored.

## Confidence
- **Technical claims (plausibility, validity, sparsity metrics)**: High
- **Clinical utility and intervention design claims**: Medium
- **Data augmentation results**: Medium

## Next Checks
1. Replicate SenseCF on at least two additional, diverse health sensor datasets to assess robustness and generalizability.
2. Conduct a blinded expert review of generated counterfactuals to confirm clinical interpretability and actionability.
3. Test SenseCF in a real-world deployment with class imbalance, measuring both model performance and practical intervention outcomes.