---
ver: rpa2
title: 'Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled
  Safety-Critical Systems'
arxiv_id: '2511.20627'
source_url: https://arxiv.org/abs/2511.20627
tags:
- requirements
- systems
- language
- semantic
- formal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes a novel approach to address the safety-assurance
  challenges of AI-enabled systems by leveraging AI itself. The framework introduces
  two complementary components: REACT, which uses LLMs to bridge the gap between informal
  natural language requirements and formal specifications, and SemaLens, which employs
  VLMs to reason about, test, and monitor DNN-based perception systems using human-understandable
  concepts.'
---

# Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems

## Quick Facts
- arXiv ID: 2511.20627
- Source URL: https://arxiv.org/abs/2511.20627
- Authors: Anastasia Mavridou; Divya Gopinath; Corina S. Păsăreanu
- Reference count: 16
- Primary result: Introduces REACT and SemaLens, two AI-driven components for translating natural language requirements into formal specifications and monitoring DNN-based perception systems using human-understandable concepts.

## Executive Summary
This work proposes a novel approach to address the safety-assurance challenges of AI-enabled systems by leveraging AI itself. The framework introduces two complementary components: REACT, which uses LLMs to bridge the gap between informal natural language requirements and formal specifications, and SemaLens, which employs VLMs to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, they provide an end-to-end pipeline from requirements to validated implementations, enabling early error detection, reduced manual effort, and enhanced system reliability. The approach supports scalability, early verification and validation, and standards compliance while offering better reliability, safer AI decisions, and novel coverage metrics for autonomous systems.

## Method Summary
The framework combines two AI-driven components: REACT and SemaLens. REACT uses LLMs to translate natural language requirements into multiple candidate formal interpretations in Restricted English, which are then validated by humans and converted to LTLf specifications via FRET integration. SemaLens employs VLMs (specifically CLIP) to evaluate semantic predicates on image sequences by computing embedding similarity, enabling runtime monitoring of temporal properties. The system also includes an alignment module to map perception model embeddings to VLM embeddings for debugging and concept-based explanations. The integration point is where REACT-generated test cases feed into SemaLens for validation against perception system outputs.

## Key Results
- LLMs can systematically translate ambiguous natural language requirements into multiple candidate formal interpretations, enabling human-in-the-loop disambiguation
- VLMs can evaluate high-level semantic predicates directly on image sequences by computing embedding similarity, enabling runtime monitoring without manual annotation
- Aligning a perception model's embedding space to a VLM's embedding space creates a semantic "lens" for explaining and debugging model behavior in human-understandable concepts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can systematically translate ambiguous natural language requirements into multiple candidate formal interpretations, enabling human-in-the-loop disambiguation rather than single-shot automation
- Mechanism: The REACT Author module takes unrestricted natural language and generates multiple Restricted English (RE) candidates, explicitly enumerating potential interpretations. REACT Validate then presents semantic differences as execution traces or concrete scenarios (not formal logic), allowing users to accept/reject distinctions. Validated RE is converted to LTLf via FRET integration, enabling downstream formal analysis and test generation
- Core assumption: LLMs can reliably enumerate semantically distinct interpretations of ambiguous requirements; users can correctly identify intended semantics from trace-based presentations
- Evidence anchors:
  - [abstract] "REACT employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications"
  - [section 2.1] "Rather than producing a single RE output, the module often produces a list of RE candidates... allowing users to validate and select the intended meaning"
  - [corpus] Related work "Leveraging LLMs for Formal Software Requirements" (FMR=0.0) explicitly flags challenges in this translation, suggesting the approach addresses a recognized but non-trivial problem
- Break condition: If LLM fails to enumerate key semantic alternatives, or if trace-based validation exceeds user cognitive load for complex requirements, the disambiguation loop degrades

### Mechanism 2
- Claim: VLMs can evaluate high-level semantic predicates directly on image sequences by computing embedding similarity, enabling runtime monitoring of temporal properties without manual annotation
- Mechanism: SemaLens Monitor parses LTLf formulas into DFAs. For each image in a sequence, predicate truth values (e.g., `on_path`, `cone_encounter`) are computed by: (1) extracting image embeddings via CLIP ViT-B/16, (2) computing cosine similarity to text embeddings of predicate captions, (3) evaluating True if similarity exceeds threshold (0.4 in the paper's example). The DFA state advances based on predicate evaluations
- Core assumption: VLM embedding similarity correlates sufficiently with human-judged semantic predicate satisfaction; threshold selection generalizes across domains
- Evidence anchors:
  - [abstract] "SemaLens utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts"
  - [section 2.2, p.3-4] "a predicate is evaluated to True if the respective similarity is greater than a threshold (0.4 in our case)"
  - [corpus] Toledo et al. [15] cited as "promising results" for VLM-based monitoring, but corpus lacks independent replication studies
- Break condition: If VLM embedding similarity fails to align with predicate semantics (e.g., high similarity for false positives), or if threshold sensitivity varies across scene types, monitoring produces unreliable verdicts

### Mechanism 3
- Claim: Aligning a perception model's embedding space to a VLM's embedding space creates a semantic "lens" for explaining and debugging model behavior in human-understandable concepts
- Mechanism: SemaLens AED builds a mapping between target vision model embeddings and CLIP internal representations. When a model classifies an image, relevant concepts (e.g., "metallic," "rectangular" for "truck") are queried via the mapped space. Misclassifications can be localized to encoder (wrong concepts extracted) vs. head (logic error). Statistical analysis yields semantic heatmaps identifying brittle features
- Core assumption: The embedding space mapping preserves sufficient semantic structure for concept-level explanations; identified concepts causally relate to model decisions
- Evidence anchors:
  - [section 2.2, p.4] "The crux of the approach is to build a map that aligns the embeddings of a vision model with the internal representations of the CLIP model"
  - [section 2.2, p.4] References [7, 9, 10] for prior work on concept-based analysis via VLMs
  - [corpus] No direct corpus papers validate this specific cross-modal alignment for debugging; related work on TAIBOM (FMR=0.55) addresses trustworthiness but not this mechanism
- Break condition: If mapping is lossy or concepts are spurious correlations, explanations mislead rather than inform debugging

## Foundational Learning

- Concept: **Linear Temporal Logic for finite traces (LTLf)**
  - Why needed here: REACT Formalize generates LTLf specifications; SemaLens Monitor evaluates temporal properties over image sequences. Understanding LTLf operators (eventually, always, until) is prerequisite for interpreting generated formulas and debugging monitor verdicts
  - Quick check question: Given an LTLf formula `G(on_path → F cone_encounter)`, what does it mean if the monitor returns False at frame 10 of a 15-frame sequence?

- Concept: **Vision-Language Model embedding spaces and cosine similarity**
  - Why needed here: SemaLens predicates are evaluated via CLIP embedding similarity. Understanding that embeddings are high-dimensional vectors, and similarity measures semantic relatedness (not pixel-level matching), is essential for threshold tuning and interpreting failure cases
  - Quick check question: If an image of a cyclist at night has low similarity to "pedestrian" (0.25) but also low similarity to "cyclist" (0.30), what does this suggest about coverage gaps?

- Concept: **Requirements traceability (DO-178C context)**
  - Why needed here: The framework explicitly targets DO-178C compliance via traceability from requirements → formal specs → test cases. Understanding why each artifact must link backward/forward is critical for evaluating whether the pipeline meets assurance standards
  - Quick check question: If REACT generates a test case that passes but no formal requirement links to it, what assurance gap exists?

## Architecture Onboarding

- Component map:
  - **REACT pipeline**: Author (NL→RE candidates) → Validate (human-in-loop selection) → Formalize (RE→LTLf via FRET) → Analyze (conflict detection) → Generate Test Cases
  - **SemaLens pipeline**: Monitor (VLM+DFA evaluation) ↔ Img Generate (diffusion models) ↔ Test (coverage metrics) ↔ AED (embedding-space debugging)
  - **Integration point**: REACT Generate Test Cases feeds test sequences to SemaLens Img Generate; REACT Formalize outputs feed SemaLens Monitor predicates

- Critical path: Natural language requirement → REACT Author → REACT Validate (human decision) → REACT Formalize → LTLf formula → DFA construction → SemaLens Monitor (with CLIP similarity thresholds) → runtime verdict. The human validation step is the highest-risk bottleneck.

- Design tradeoffs:
  - Multiple RE candidates vs. single output: Increases disambiguation accuracy but raises user cognitive load
  - Fixed similarity threshold (0.4) vs. adaptive thresholds: Simplifies deployment but may not generalize across domains/lighting conditions
  - CLIP as universal semantic lens vs. domain-specific VLMs: Broad concept coverage but may lack fine-grained domain concepts (e.g., aerospace-specific terminology)

- Failure signatures:
  - LLM hallucinates implausible RE interpretations not traceable to original requirement
  - VLM similarity scores cluster near threshold, producing unstable predicate evaluations across similar images
  - Embedding-space mapping produces concept explanations that contradict human expert judgment
  - Monitor returns True for sequences that violate intuitive understanding of the requirement

- First 3 experiments:
  1. **End-to-end trace test**: Take a known requirement from the paper (REQ-LIV-002), run through full REACT→SemaLens pipeline on provided rover images, verify monitor verdict matches expected outcome. Document any manual intervention required.
  2. **Threshold sensitivity analysis**: For SemaLens Monitor, vary similarity threshold (0.3, 0.4, 0.5) on a held-out image set. Measure predicate evaluation stability and flag cases where small threshold changes flip verdicts.
  3. **Cross-domain probe**: Apply the framework to a different perception domain (e.g., aircraft taxiway detection) with 3-5 requirements. Identify where CLIP embeddings lack domain concepts and where REACT Author produces ambiguous candidates requiring extensive validation.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework's reliance on LLM and VLM outputs introduces uncertainty about generalizability beyond the NASA rover case study
- Fixed similarity threshold (0.4) and CLIP ViT-B/16 embedding space may not perform well in domains with different visual semantics or lighting conditions
- Human-in-the-loop validation step could become a bottleneck for complex or numerous requirements

## Confidence
- **High confidence**: The overall architecture and integration approach (REACT + SemaLens) is sound and addresses a real gap in AI safety assurance
- **Medium confidence**: The LLM-based requirement formalization works for straightforward requirements but may struggle with complex temporal logic or domain-specific terminology
- **Low confidence**: The VLM-based monitoring threshold (0.4) and embedding alignment for debugging are not empirically validated across diverse scenarios

## Next Checks
1. **Cross-domain threshold calibration**: Apply SemaLens Monitor to 3-5 perception domains (e.g., automotive, aerospace, medical imaging) with 10-20 test images each. Plot similarity score distributions and identify threshold tuning needs per domain
2. **LLM ambiguity stress test**: Feed REACT Author 15+ ambiguous requirements from software engineering benchmarks. Measure the percentage requiring human validation and time per validation to assess scalability
3. **Debugging fidelity assessment**: Take 10 misclassified images from a DNN, run SemaLens AED, and compare generated concept explanations against expert-annotated failure modes to measure explanation accuracy