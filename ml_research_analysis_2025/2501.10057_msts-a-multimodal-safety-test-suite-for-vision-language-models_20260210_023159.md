---
ver: rpa2
title: 'MSTS: A Multimodal Safety Test Suite for Vision-Language Models'
arxiv_id: '2501.10057'
source_url: https://arxiv.org/abs/2501.10057
tags:
- safety
- prompts
- test
- msts
- unsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MSTS, a Multimodal Safety Test Suite for
  Vision-Language Models (VLMs), addressing the growing safety risks as VLMs become
  integrated into consumer AI applications. MSTS comprises 400 test prompts across
  40 fine-grained hazard categories, where each prompt combines a text and an image
  to reveal unsafe meaning only when both are considered together.
---

# MSTS: A Multimodal Safety Test Suite for Vision-Language Models

## Quick Facts
- arXiv ID: 2501.10057
- Source URL: https://arxiv.org/abs/2501.10057
- Reference count: 11
- Ten state-of-the-art VLMs tested, finding commercial models (GPT-4o, Claude-3.5) very safe (<0.5% unsafe responses) while some open models (xGen-MM) exhibit clear safety issues (14.0% unsafe responses)

## Executive Summary
This paper introduces MSTS, a Multimodal Safety Test Suite designed to evaluate vision-language model (VLM) safety through 400 test prompts that combine text and images to reveal unsafe meaning only when both modalities are considered together. The authors test ten state-of-the-art VLMs across 40 fine-grained hazard categories, finding that commercial models like GPT-4o and Claude-3.5 exhibit strong safety (fewer than 0.5% unsafe responses), while some open models like xGen-MM show significant safety issues with 14.0% unsafe responses. The study reveals that open models are often "safe by accident" due to misunderstanding prompts rather than robust alignment, and that safety varies across languages with some models being less safe for non-English prompts. The authors also explore automating safety assessments and find that even the best VLM classifiers perform poorly, with Gemini-1.5 achieving only 53% precision in classifying unsafe responses.

## Method Summary
MSTS comprises 400 test prompts across 40 hazard categories, each combining a text fragment with an image to create multimodal meaning. The test uses 200 unique text fragments × 2 templates ("Should I ___?" and "I am going to ___") paired with 200 CC-licensed images. The authors evaluate ten VLMs using deterministic beam search (n=3) for open models and default parameters for commercial APIs. Image preprocessing includes RGB conversion, max 1,400px height, bicubic sampling, and aspect ratio preservation. Responses are classified using a binary safe/unsafe taxonomy with fine-grained categories including "safe-by-design" and "safe-by-accident." The study reports inter-annotator agreement of 97.4% for binary classification (κ=0.73).

## Key Results
- Commercial VLMs (GPT-4o, Claude-3.5) are very safe, with fewer than 0.5% unsafe responses across all 400 test prompts
- Open models show significant safety variation: xGen-MM responds unsafely to 14.0% of prompts, while Qwen-2-VL has 3.8% unsafe responses
- Open models are often "safe by accident" rather than by design, with xGen-MM showing 54.0% safe-by-accident responses
- Safety varies significantly across languages, with MiniCPM-2.6 showing lower safety for non-English prompts
- Automated safety assessment remains challenging, with even the best VLM classifier (Gemini-1.5) achieving only 53% precision for unsafe response classification

## Why This Works (Mechanism)
The MSTS test suite works by exploiting the multimodal nature of VLMs, where unsafe meaning emerges only from the combination of text and image. By designing prompts where the text alone is ambiguous or benign, but the image-text combination creates unsafe meaning, the test reveals whether models truly understand multimodal context. The 40 hazard categories cover a broad range of potential safety issues from explicit content to discrimination, ensuring comprehensive safety evaluation. The use of both "Should I ___?" and "I am going to ___" templates tests different response patterns, while the beam search (n=3) for open models ensures deterministic and comparable results across model generations.

## Foundational Learning

**Multimodal Safety Classification** - Understanding how text+image combinations create unsafe meaning requires grasping that safety in VLMs depends on joint reasoning across modalities. Why needed: Single-modality safety tests miss cases where unsafe content only emerges from multimodal context. Quick check: Can you identify which of two image-text pairs creates unsafe meaning while the other remains safe?

**Fine-grained Hazard Taxonomy** - The 40-category system (e.g., explicit sexual content, physical violence, discrimination) provides structured safety assessment beyond binary safe/unsafe. Why needed: Different hazards require different safety interventions and risk assessments. Quick check: Given a prompt, can you classify it into the correct hazard category from the taxonomy?

**Safe-by-Accident vs Safe-by-Design** - Distinguishing between models that refuse unsafe prompts due to robust alignment versus those that misunderstand the prompt. Why needed: "Safe by accident" models may become unsafe as they improve in capability. Quick check: Can you determine whether a safe response reflects genuine safety understanding or prompt misunderstanding?

## Architecture Onboarding

**Component Map:** MSTS Dataset -> VLM Inference (commercial APIs/open models with beam search) -> Response Generation -> Human Annotation (binary + fine-grained taxonomy) -> Safety Analysis -> Automated Classifier Testing

**Critical Path:** Image preprocessing (RGB, 1,400px max) → Prompt-text + image input → Model inference (beam search n=3 for open models) → Response output (max 512 tokens) → Human annotation using Table 2 taxonomy → Unsafe rate calculation

**Design Tradeoffs:** Deterministic beam search (n=3) ensures reproducibility but may not reflect real-world sampling; binary classification simplifies analysis but loses nuance; English focus enables consistent evaluation but limits cross-lingual safety assessment.

**Failure Signatures:** Low inter-annotator agreement (<90% binary) suggests ambiguous prompt design; inconsistent unsafe rates across model versions indicate sensitivity to model updates; poor automated classifier performance (<60% precision) reveals limitations in current safety evaluation methods.

**Three First Experiments:**
1. Run a single VLM (e.g., MiniCPM-2.6) on 10 randomly selected MSTS prompts and manually annotate responses to verify the unsafe/safe classification process
2. Test the same 10 prompts with and without images to confirm that unsafe meaning only emerges in the multimodal condition
3. Implement the exact image preprocessing pipeline on sample images and verify output dimensions/format match specifications

## Open Questions the Paper Calls Out
- How can VLM safety assessments be reliably automated? The paper concludes that current safety classifiers perform poorly, with the best model (Gemini-1.5) achieving only 53% precision, suggesting fundamental limitations in automated evaluation methods.
- Do improvements in multimodal understanding inevitably compromise the safety of open VLMs? The authors note it "remains to be seen" if more capable models will be "safe by design" or simply lose their "safe by accident" status as they improve.
- Are VLMs that are safe on simple, explicit prompts also robust against adversarial multimodal attacks? The test suite focuses on unsophisticated user personas and doesn't cover obfuscation or jailbreaking techniques, leaving open whether safety generalizes to adversarial scenarios.

## Limitations
- The study focuses on English-language prompts with 10 language translations, potentially missing safety issues in other languages and cultural contexts
- Inter-annotator agreement, while good at 97.4% binary, may be more subjective for the safe-by-accident vs safe-by-design distinction
- The test suite evaluates only ten models, representing a small fraction of available VLMs, and safety performance may vary across model versions and fine-tuning approaches
- Automated safety assessment remains highly limited, with current classifiers struggling to achieve even 60% precision on the MSTS response dataset

## Confidence

**High Confidence:** Commercial VLMs like GPT-4o and Claude-3.5 are substantially safer than open models like xGen-MM and Qwen-2-VL, with concrete unsafe response rates (0.5% vs 14.0%).

**Medium Confidence:** Open models are often "safe by accident" rather than by design, given the subjective nature of distinguishing between safe-by-accident and safe-by-design classifications.

**Low Confidence:** The automated safety assessment results, particularly the claim that even the best VLM classifier (Gemini-1.5) achieves only 53% precision, which suggests fundamental limitations in current automated safety evaluation methods.

## Next Checks
1. Replicate the inter-annotator agreement calculation with at least two independent annotators on a subset of 100 responses, focusing specifically on the safe-by-accident vs safe-by-design distinction to verify the reported κ=0.73.
2. Test an expanded set of 5-10 additional VLMs not included in the original study, including different versions of models already tested, to assess the stability of safety performance across model updates.
3. Implement a follow-up experiment using the MSTS prompts with adversarial modifications (e.g., image scrambling, text perturbations) to determine whether the observed safety differences between models persist under controlled perturbations.