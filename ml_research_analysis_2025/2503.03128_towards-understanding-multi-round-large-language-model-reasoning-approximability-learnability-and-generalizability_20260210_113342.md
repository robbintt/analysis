---
ver: rpa2
title: 'Towards Understanding Multi-Round Large Language Model Reasoning: Approximability,
  Learnability and Generalizability'
arxiv_id: '2503.03128'
source_url: https://arxiv.org/abs/2503.03128
tags:
- error
- sequence
- learning
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the theoretical foundations of multi-round
  reasoning in large language models (LLMs), focusing on approximation, learnability,
  and generalization properties. The authors demonstrate that Transformers with finite
  context windows can approximate Turing-computable functions through multi-round
  reasoning, extending PAC learning to sequence generation and showing that multi-round
  generation remains learnable even for sequences longer than the model's context
  window.
---

# Towards Understanding Multi-Round Large Language Model Reasoning: Approximability, Learnability and Generalizability

## Quick Facts
- **arXiv ID:** 2503.03128
- **Source URL:** https://arxiv.org/abs/2503.03128
- **Reference count:** 40
- **Key outcome:** This paper investigates the theoretical foundations of multi-round reasoning in large language models, focusing on approximation, learnability, and generalization properties.

## Executive Summary
This paper provides theoretical foundations for multi-round reasoning in large language models (LLMs), analyzing three key dimensions: approximability, learnability, and generalization. The authors demonstrate that Transformers with finite context windows can approximate Turing-computable functions through multi-round reasoning, extending PAC learning to sequence generation. They show that multi-round generation remains learnable even for sequences longer than the model's context window, and analyze error propagation across rounds. The study reveals that while multi-round reasoning exponentially increases sample complexity compared to single-token prediction, it enables handling of long sequences by breaking them into manageable rounds, with error intervention techniques playing a crucial role in maintaining controllable outputs during inference.

## Method Summary
The paper employs theoretical analysis to derive bounds and prove theorems about multi-round LLM reasoning. It extends PAC learning to sequence generation using Rademacher complexity bounds, proves universal approximation theorems for Transformers approximating Turing machines through multi-round generation, and analyzes error propagation using recursive formulations. The method relies on assumptions including bounded input norms, Lipschitz continuity of activation functions, and spectral norm constraints on weight matrices. Key proofs involve encoding Turing machine configurations into Transformer hidden states, deriving sample complexity bounds for multi-round sequence learning, and quantifying cumulative error propagation across reasoning rounds.

## Key Results
- Transformers with finite context windows can approximate Turing-computable functions through multi-round reasoning
- Multi-round generation remains learnable even for sequences longer than the model's context window
- Without intervention, cumulative error can grow unboundedly across rounds, but techniques like Chain-of-Thought and self-correction can effectively constrain this error
- Multi-round reasoning exponentially increases sample complexity compared to single-token prediction but enables handling of long sequences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Finite-context Transformers can theoretically approximate infinite computations by segmenting them into sequential rounds.
- **Mechanism:** While a single forward pass is constrained by the context window $k$, a multi-round process allows the model to simulate a Turing Machine by maintaining the TM's "state" and "tape head" vicinity within its context window. By iteratively generating output, the model simulates the TM transition function step-by-step.
- **Core assumption:** The model possesses sufficient hidden dimension $d$ and quantization precision $Q$ to create injective mappings of TM configurations.
- **Break condition:** The approximation fails if accumulated quantization error exceeds tolerance $\epsilon$ before computation halts, or if context window $k$ is too small to enclose necessary local TM configuration.

### Mechanism 2
- **Claim:** Decomposing long sequence generation into multiple rounds reduces sample complexity.
- **Mechanism:** Learning a sequence of length $T$ in one shot exhibits sample complexity scaling exponentially with $T$. By splitting the task into $R$ rounds, complexity becomes exponential in $T/R$ rather than $T$, reducing total data required to generalize.
- **Core assumption:** Error propagation between rounds can be bounded, and the "divide and conquer" strategy doesn't introduce irrecoverable complexity in reassembling sub-sequences.
- **Break condition:** If $R$ is too high relative to $T$, overhead of managing transitions might negate complexity benefits.

### Mechanism 3
- **Claim:** Intervention techniques function as error propagation stabilizers.
- **Mechanism:** In auto-regressive generation, errors compound across rounds. The paper formalizes "interventions" as constraints that reduce the error impact factor $\gamma$. By inserting corrections like CoT steps, the model resets or limits divergence of the hidden state.
- **Core assumption:** Intervention effectively reduces propagation factor $\gamma$ to smaller $\gamma'$; a poor hint might fail to do this.
- **Break condition:** Irrelevant or hallucinated interventions may fail to reduce $\gamma$, offering no protection against divergence.

## Foundational Learning

- **Concept:** **PAC Learning (Probably Approximately Correct)**
  - **Why needed here:** The paper extends PAC learning to define "Sequential PAC Learnability." To understand results, one must grasp the mathematical relationship between sample size $m$, error $\epsilon$, and confidence $\delta$.
  - **Quick check question:** How does increasing required confidence $(1-\delta)$ affect number of training samples $m$ needed?

- **Concept:** **Turing-Completeness vs. Universal Approximation**
  - **Why needed here:** The paper differentiates between being capable of computing anything (Turing-complete) and being able to *learn* that computation. Section 4 relies on constructing a Transformer as a Universal Approximator of TM steps.
  - **Quick check question:** Why is "being Turing-complete" insufficient to prove that a model is practically useful without "Learnability" analysis?

- **Concept:** **Error Propagation in Auto-regression**
  - **Why needed here:** This is the central problem of Section 6. Understanding how small errors in early tokens can amplify into total divergence in later generation rounds is critical for multi-round error analysis.
  - **Quick check question:** In an auto-regressive loop, why does feeding the model's own prediction back as input lead to exponential error growth?

## Architecture Onboarding

- **Component map:** Input Encoding -> Context Window Constraint ($k$) -> State Update (Layer Simulation) -> Round Transition (Error Propagation Check) -> Final Output
- **Critical path:** Input Encoding $\to$ **Context Window Constraint ($k$)** $\to$ **State Update (Layer Simulation)** $\to$ **Round Transition (Error Propagation Check)** $\to$ Final Output
- **Design tradeoffs:**
  - **Context Window Size ($k$):** Increasing $k$ increases sample complexity but allows more "tape" to be visible per step
  - **Rounds ($R$):** Increasing rounds allows solving longer problems but increases risk of cumulative error divergence unless interventions are used
  - **Precision ($Q$):** Higher quantization precision allows longer simulations before error accumulation breaks approximation
- **Failure signatures:**
  - **The Drift (Divergence):** Output becomes incoherent after many rounds due to unbounded cumulative error
  - **Context Saturation:** Model fails to track "tape head" if TM state requires dependencies longer than context window $k$
- **First 3 experiments:**
  1. **Validation of Sample Complexity (Theorem 5.9):** Train models on sequence tasks of length $T$ using 1-round vs. $R$-round schemes and plot samples required to reach accuracy $\epsilon$
  2. **Error Accumulation Stress Test (Theorem 6.3):** Run multi-round inference without CoT/intervention on long chains of reasoning to observe "divergence point" where error becomes unbounded
  3. **Intervention Efficiency (Theorem 6.4):** Measure reduction in cumulative error when injecting "hints" at varying frequencies to find optimal cost/benefit balance for CoT

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific algorithms or mechanisms can optimally determine when and how to intervene (provide "hints") during multi-round inference to effectively minimize cumulative error propagation?
- **Basis in paper:** Section 6.3 and the Conclusion discuss Chain-of-Thought and self-correction as interventions, stating, "we should give more consideration to how to interrupt the propagation of cumulative errors."
- **Why unresolved:** While Theorem 6.4 proves that reducing error propagation factor $\gamma$ via intervention reduces cumulative error, the paper does not propose a concrete method for generating these interventions or determining their optimal timing.
- **What evidence would resolve it:** An algorithmic framework that dynamically triggers interventions based on intermediate state divergence, along with empirical validation showing lower cumulative error compared to static multi-round generation.

### Open Question 2
- **Question:** How should complex, long-sequence tasks be algorithmically decomposed into multiple rounds during the training phase to minimize sample complexity?
- **Basis in paper:** The Conclusion suggests that to reduce sample complexity, "one can consider providing some decomposition methods... so that the long sequences are decomposed into multiple rounds of training."
- **Why unresolved:** The paper establishes the theoretical trade-off where decomposing a sequence of length $T$ into $R$ rounds mitigates exponential increase in sample complexity, but offers no specific decomposition strategy.
- **What evidence would resolve it:** Comparative training experiments applying different segmentation strategies (e.g., semantic chunking vs. fixed windows) to long-sequence tasks, measuring convergence speed and generalization error.

### Open Question 3
- **Question:** To what extent do finite precision and quantization levels in standard hardware limit the maximum feasible reasoning length ($S_{max}$) before accumulated approximation errors become unmanageable?
- **Basis in paper:** Lemma 4.1 and Appendix A.3.2 derive a strict theoretical requirement $Q \geq e^{C L d k / \epsilon}$ for quantization levels $Q$ to maintain error bounds, implying a physical constraint on model reasoning depth.
- **Why unresolved:** The theoretical analysis assumes specific conditions to bound error, but practical implications of standard floating-point precision (e.g., FP16, BF16) on realized "Turing-computable" reasoning depth remain unquantified in real-world systems.
- **What evidence would resolve it:** An empirical analysis correlating specific numerical precisions with degradation of accuracy over increasing multi-round reasoning steps on complex logical reasoning benchmarks.

## Limitations

- The theoretical analysis relies on idealized assumptions (bounded input norms, spectral norm constraints) that may not hold in practice
- The paper does not provide empirical validation of its theoretical bounds, making it difficult to assess whether derived relationships hold in actual implementations
- The quantization precision requirement creates significant practical constraints - achieving necessary precision in real-world Transformers may be infeasible

## Confidence

**High Confidence:**
- Multi-round reasoning enables Turing-computable function approximation (Theorem 4.3)
- Error propagation without intervention leads to unbounded cumulative error (Theorem 6.3)
- The relationship between context window size k and computational expressiveness

**Medium Confidence:**
- Sample complexity reduction through multi-round decomposition (Theorem 5.9)
- PAC learning extension to sequence generation
- The effectiveness of intervention techniques in constraining error propagation

**Low Confidence:**
- Concrete numerical relationships between quantization levels Q, model dimensions, and approximation quality
- Practical feasibility of achieving required precision for universal approximation
- Real-world impact of derived bounds on actual LLM training and inference

## Next Checks

1. **Empirical Error Accumulation Study:** Implement a multi-round reasoning task (e.g., mathematical proof generation) and measure actual cumulative error across rounds without intervention, comparing against theoretical divergence predicted in Theorem 6.3.

2. **Sample Complexity Benchmark:** Train sequence models on tasks of varying lengths T using both single-round and multi-round approaches, measuring actual sample requirements to achieve target accuracy and validating exponential scaling relationships in Theorem 5.9.

3. **Intervention Effectiveness Test:** Systematically evaluate different intervention frequencies and qualities (e.g., Chain-of-Thought prompts) on error accumulation, measuring the reduction factor Î”L(h_R) and identifying optimal intervention strategies for controlling cumulative error.