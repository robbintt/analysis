---
ver: rpa2
title: An Empirical Study of World Model Quantization
arxiv_id: '2602.02110'
source_url: https://arxiv.org/abs/2602.02110
tags:
- quantization
- per-tensor
- planning
- world
- smoothquant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic empirical study of post-training
  quantization (PTQ) for world models in planning-based settings. The authors evaluate
  diverse PTQ methods (RTN, OMSE, AWQ, SmoothQuant, OmniQuant) on DINO-WM across weight-only
  and joint weight-activation settings, testing bit-widths from 3 to 8 bits, quantization
  granularities (per-channel/group for weights, per-tensor/token for activations),
  and planning horizons up to 50 iterations.
---

# An Empirical Study of World Model Quantization

## Quick Facts
- arXiv ID: 2602.02110
- Source URL: https://arxiv.org/abs/2602.02110
- Authors: Zhongqian Fu; Tianyi Zhao; Kai Han; Hang Zhou; Xinghao Chen; Yunhe Wang
- Reference count: 33
- Primary result: First systematic empirical study of post-training quantization for world models, revealing unique characteristics beyond standard accuracy-bitwidth trade-offs

## Executive Summary
This paper presents the first systematic empirical study of post-training quantization (PTQ) for world models in planning-based settings. The authors evaluate diverse PTQ methods (RTN, OMSE, AWQ, SmoothQuant, OmniQuant) on DINO-WM across weight-only and joint weight-activation settings, testing bit-widths from 3 to 8 bits. The study reveals that world model quantization exhibits unique characteristics including irreversible encoder errors, partial compensation for predictor errors, and planning objective misalignment under aggressive quantization.

## Method Summary
The study uses DINO-WM pretrained checkpoint with Wall and PushT visual planning environments. PTQ methods are applied with weight-only (3-8 bits) and weight-activation settings, using per-channel/per-group granularity for weights and per-tensor/per-token for activations. Evaluation measures success rate and MSE distance across planning horizons up to 50 iterations. Calibration dataset uses 2-iteration planning trajectories collected with separate random seeds.

## Key Results
- Group-wise weight quantization (e.g., 128 groups) stabilizes 4-bit performance but fails under extreme precision reduction (3-bit)
- Per-token activation quantization provides inconsistent benefits, with per-tensor scaling often performing better for long-horizon rollouts
- Encoder shows significantly higher quantization sensitivity than predictor, with encoder errors being irreversible while predictor errors can be partially compensated through planning iterations
- Aggressive low-bit quantization (≤4 bits) severely degrades planning performance, with quantized models failing to decrease planning loss and sometimes showing increasing loss over iterations

## Why This Works (Mechanism)

### Mechanism 1: Group-wise Granularity for Scale Mismatch
Weight tensors in world models exhibit non-uniform scale distributions. Per-group scaling (e.g., 128 groups) adapts the quantization grid to local weight magnitudes, reducing the risk of outliers distorting precision for smaller weights. This preserves latent dynamics better than per-channel methods at 4-bit by localizing scale factor estimation.

### Mechanism 2: Encoder-Predictor Asymmetry
The encoder maps visual observations to fixed latent space; errors here corrupt the initial state representation ($z_0$), propagating incorrectly through every prediction step. The predictor introduces transition noise ($z_t \to z_{t+1}$), which planning optimization can partially correct by selecting actions that counteract drift, provided the initial state is accurate.

### Mechanism 3: Planning Objective Misalignment
Aggressive quantization introduces systematic bias that alters the latent manifold, breaking the correlation between loss landscape and true task success. This causes the optimizer to "diverge" even if the model runs without crashing, as the gradient direction becomes invalid for the corrupted latent space.

## Foundational Learning

**Latent Rollout Dynamics** - World models execute sequential predictions ($z_t$ to $z_{t+1}$), unlike single-pass inference. Errors compound over time, making stable accuracy per step insufficient for planning tasks.

Quick check: If a model has 1% error per step, what is the approximate error after 50 steps if errors compound linearly vs. geometrically?

**Post-Training Quantization (PTQ) Granularity** - The paper distinguishes between per-channel, per-group, per-tensor, and per-token scaling. Granularity determines how well the quantization grid fits the data distribution.

Quick check: Why would "per-token" activation quantization be theoretically better than "per-tensor," and why does this paper suggest it might fail in practice? (Hint: Stability vs. Flexibility)

**Model-Based Planning Objective** - Quantized models can optimize a loss function that no longer aligns with task success, making "decreasing MSE loss" not a guaranteed indicator of successful planning.

Quick check: In the context of this paper, why is a "decreasing MSE loss" not a guaranteed indicator of a successful plan if the model is heavily quantized?

## Architecture Onboarding

**Component map:** Image Input -> **Encoder (Sensitive)** -> Initial Latent State -> **Predictor Loop (50 iters)** -> Planning Cost Calculation -> Action Selection

**Critical path:** Image Input → **Encoder (Sensitive)** → Initial Latent State → **Predictor Loop (50 iters)** → Planning Cost Calculation → Action Selection

**Design tradeoffs:**
- 4-bit with Group-wise (G=128) weights offers memory savings with potential recovery via planning; 8-bit is generally safe
- Keeping Encoder at high precision (e.g., FP16 or W8) while quantizing Predictor to W4 saves memory without total collapse
- Per-token activation offers flexibility but introduces variance; Per-tensor is safer for long-horizon stability

**Failure signatures:**
- Representation Collapse: Reconstructed images look like noise immediately (Encoder failure)
- Geometric Misalignment: Reconstructed images look fine, but success rate remains 0.0 (Predictor/Planning failure)
- Divergent Loss: Planning loss (MSE) increases as iteration count rises

**First 3 experiments:**
1. **Sanity Check:** Run FP32 baseline to establish max success rate and loss curve
2. **Component Ablation:** Quantize *only* the Encoder to W4 and observe success rate vs. iterations. Compare with quantizing *only* the Predictor to W4.
3. **Granularity Sweep:** Test W4 weight-only quantization using Per-Channel vs. Group-wise (G=128) on the "Wall" task over 50 iterations to observe the recovery effect

## Open Questions the Paper Calls Out

### Open Question 1
Can quantization-aware training (QAT) overcome the representation collapse and planning objective misalignment observed under aggressive PTQ, particularly for the encoder module?

The study focuses exclusively on PTQ and identifies the encoder as the primary bottleneck where errors are "irreversible" and "cannot be corrected by additional planning iterations," but does not explore whether training-time quantization awareness could mitigate these issues.

### Open Question 2
Do the observed quantization sensitivity patterns generalize beyond DINO-WM to other world model architectures (e.g., recurrent models like Dreamer, implicit latent models)?

The authors state they "use DINO-WM as a representative case" and their "approach can also be transferred to other world model architectures," but only evaluate DINO-WM.

### Open Question 3
Can encoder-specific or mixed-precision quantization strategies (e.g., W8 encoder with W4 predictor) achieve better efficiency-planning trade-offs than uniform precision?

Tables 7-8 show encoder quantization causes irreversible degradation while predictor quantization is partially compensable, suggesting asymmetric bit-width allocation could be optimal. The authors test uniform configurations but not mixed precision across modules.

## Limitations
- Architecture Generalization: Findings may not transfer directly to other world model architectures with different latent dynamics
- Environment Specificity: Results based on Wall and PushT tasks may not generalize to broader task distributions
- Calibration Protocol Sensitivity: Optimal calibration horizon and data distribution for quantization remain underexplored

## Confidence

**High Confidence** - Group-wise weight quantization stabilizes 4-bit performance; encoder quantization causes irreversible degradation; aggressive low-bit quantization degrades planning performance with increasing loss

**Medium Confidence** - Per-token activation quantization provides inconsistent benefits; encoder sensitivity exceeds predictor sensitivity; geometric misalignment vs. representation collapse manifests differently

**Low Confidence** - Specific bit-width thresholds for catastrophic failure are architecture-dependent; relative performance of PTQ methods may vary with model scale; planning iteration limits for effective quantization compensation are not systematically established

## Next Checks

1. **Architecture Transfer Test** - Apply the same quantization protocol to a different world model architecture (e.g., DreamerV3 or LatentODE) and compare encoder-predictor sensitivity patterns

2. **Calibration Protocol Ablation** - Systematically vary the calibration dataset size, horizon length, and sampling distribution to quantify their impact on quantization performance

3. **Multi-Task Robustness Analysis** - Evaluate quantization sensitivity across a diverse set of visual planning tasks to determine whether Wall/PushT findings generalize to broader task distributions