---
ver: rpa2
title: Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections
arxiv_id: '2507.00263'
source_url: https://arxiv.org/abs/2507.00263
tags:
- images
- image
- room
- pairs
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of organizing unstructured vacation
  rental property images into room-type groups and identifying bed types in bedrooms.
  The proposed solution is a machine learning pipeline that integrates room-type classification,
  pairwise image overlap detection using a Siamese network, spectral clustering for
  grouping images of the same room space, and a multi-modal LLM for mapping bedroom
  groups to bed types.
---

# Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections

## Quick Facts
- arXiv ID: 2507.00263
- Source URL: https://arxiv.org/abs/2507.00263
- Authors: Vignesh Ram Nithin Kappagantula; Shayan Hassantabar
- Reference count: 26
- Primary result: Achieves 81.6% end-to-end accuracy for grouping vacation rental images by room type and mapping bedroom groups to bed types

## Executive Summary
This paper addresses the problem of organizing unstructured vacation rental property images into room-type groups and identifying bed types in bedrooms. The authors propose a machine learning pipeline that integrates room-type classification, pairwise image overlap detection using a Siamese network, spectral clustering for grouping images of the same room space, and a multi-modal LLM for mapping bedroom groups to bed types. The approach achieves high clustering performance with an average normalized Adjusted Rand Index of 0.8065 and V-Measure score of 0.8284, while also delivering strong bed-type identification accuracy. The method outperforms baseline approaches and includes optimizations that reduce inference time by 65.4%.

## Method Summary
The pipeline processes unstructured vacation rental property images by first classifying them into room types using a DINOv2-based multi-head classifier. A Siamese network then detects pairwise overlap between images of the same room type, with outputs used to construct a similarity matrix for spectral clustering. Room count metadata guides the clustering process, with post-processing to remove noisy outliers. Finally, a Phi-3.5-Vision LLM with LoRA fine-tuning maps bedroom groups to bed types using metadata descriptions. The approach combines self-supervised pretraining with limited manual fine-tuning for sample efficiency, and uses soft probability scores rather than hard embeddings to enable spectral clustering of ambiguous boundaries.

## Key Results
- Clustering performance: Average normalized Adjusted Rand Index (ARI) of 0.8065 and V-Measure score of 0.8284
- End-to-end pipeline accuracy: 81.6% for complete room grouping and bed-type mapping
- Bed-type identification accuracy: 89% on validation dataset, 78% on test dataset
- Inference optimization: 65.4% reduction in inference time compared to naive implementation

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition via Room-Type Pre-Classification
Pre-classifying images by room type before grouping improves computational efficiency and reduces grouping errors by reducing the pairwise comparison problem from O(n²) across all property images to smaller, room-type-specific subproblems, avoiding cross-room-type confusion.

### Mechanism 2: Sample-Efficient Siamese Training via Two-Stage Self-Supervision
Combining large-scale self-supervised pretraining with limited manual fine-tuning achieves high overlap detection recall without prohibitive annotation costs by covering most overlap scenarios through augmentation while reserving manual annotation for edge cases.

### Mechanism 3: Soft Probability Scores Enable Spectral Clustering of Ambiguous Boundaries
Using soft overlap probabilities as spectral clustering input captures nuanced spatial relationships that direct embedding clustering misses by encoding more relational information than hard embeddings or direct pretrained features.

## Foundational Learning

### Concept 1: Siamese Networks for Metric Learning
- Why needed here: Overlap detection requires learning a pairwise similarity function, not direct classification. Shared weights force the model to learn a distance metric in embedding space.
- Quick check question: Why does a Siamese network share weights across branches rather than using two independent encoders?

### Concept 2: Spectral Clustering vs. K-Means
- Why needed here: K-means assumes spherical clusters in feature space. Spectral clustering transforms via eigenvectors to capture non-convex boundaries—critical when room image relationships are ambiguous.
- Quick check question: What eigenvector property does spectral clustering exploit that k-means cannot capture?

### Concept 3: LoRA for Efficient LLM Fine-Tuning
- Why needed here: Full fine-tuning of Phi-3.5 is computationally expensive. LoRA adds trainable low-rank matrices to key layers, reducing parameters while preserving pretrained knowledge.
- Quick check question: What does "low-rank" mean, and why add parameters rather than modify existing weights?

## Architecture Onboarding

### Component Map:
Input: Unstructured property images + metadata (room counts, bed types)
    ↓
[DINOv2 Multi-Head Classifier] → Room type labels
    ↓
[Siamese Overlap Detector (EfficientNetV2-S)] → Pairwise overlap matrix per room type
    ↓
[Spectral Clustering] + [Room count metadata] → Image groups per room space
    ↓
[Noise Removal Post-processing] → Cleaned clusters
    ↓
[Phi-3.5 MLLM (LoRA fine-tuned)] + [Bed type metadata] → Bed type assignments
    ↓
Output: Structured catalog with room groupings + bed mappings

### Critical Path:
1. Room type classification accuracy (F1=0.97 bedroom, 0.98 bathroom) gates all downstream grouping
2. Overlap detection recall determines clustering quality
3. Metadata availability (room counts, bed descriptions) is required for spectral clustering and mapping

### Design Tradeoffs:
- EfficientNetV2-S chosen for speed; may miss subtle overlap cues
- 100K self-supervised pairs provide coverage but may not match real distribution
- Spectral clustering requires room count from metadata—fails if missing/incorrect
- Sequential bed mapping with one-to-one constraint prevents duplicates but propagates early errors

### Failure Signatures:
1. All images grouped into one cluster → Check if overlap scores uniformly high (Siamese not discriminating)
2. Empty clusters after noise removal → Threshold too aggressive
3. Wrong bed type despite correct grouping → No image clearly shows bed; check cluster image quality
4. High precision, low recall in overlap detection → Fine-tuning dataset unrepresentative

### First 3 Experiments:
1. Compare DINOv2 vs. BLIP-2 room classification on held-out properties; measure downstream ARI impact
2. Ablate Siamese training: (a) self-supervised only, (b) manual only, (c) both; measure overlap precision/recall and clustering ARI
3. Sweep noise removal threshold; plot ARI/V-measure vs. threshold to find optimal operating point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the clustering quality be maintained for properties with more than four bedrooms, where current performance degrades due to complexity?
- Basis in paper: Section 4.1.3 states that the group with more than four bedrooms has "relatively lower ARI scores due to the complexity of the images and the increase in the number of images for clustering."
- Why unresolved: The paper identifies the correlation between increased room count and lower clustering scores but does not propose a specific architectural or algorithmic solution for high-cardinality properties.
- What evidence would resolve it: A comparative study showing stable ARI scores (e.g., >0.80) across properties with 5, 6, and 7+ bedrooms using improved clustering techniques.

### Open Question 2
- Question: Can the precision drop in the Siamese overlap detection model be prevented during fine-tuning without sacrificing the significant recall improvements?
- Basis in paper: Section 4.1.2 notes that fine-tuning led to a "marginal drop in precision by 6% when compared to the precision of the pre-trained model," even while improving recall by 44.6%.
- Why unresolved: The paper acknowledges the trade-off but relies on the soft probability scores to mitigate the impact of false positives, rather than solving the precision degradation in the model itself.
- What evidence would resolve it: An ablation study demonstrating a training strategy that achieves high recall while maintaining or improving pre-training precision levels on the manually annotated test set.

### Open Question 3
- Question: How can the pipeline be modified to minimize the error propagation from the clustering stage to the bed-type mapping stage?
- Basis in paper: Section 4.1.4 reveals an 11% accuracy drop (89% to 78%) in bed-type identification when moving from standalone evaluation to the full pipeline, attributing this to "incorrect images" introduced by earlier stages.
- Why unresolved: The current pipeline relies on a sequential feed-forward architecture where noisy inputs directly degrade the Multi-modal LLM's performance.
- What evidence would resolve it: Implementation of a feedback loop or a robustness mechanism in the MLLM that maintains accuracy closer to the standalone baseline (89%) even when fed noisy clusters.

## Limitations
- The evaluation methodology relies heavily on ground truth clustering from metadata, which may not capture nuanced room configurations or reflect real-world noise in rental listings
- The dataset appears domain-specific to vacation rentals, limiting generalizability to other unstructured image collection problems
- The two-stage Siamese training approach depends on careful curation of manually annotated edge cases that aren't fully characterized

## Confidence

- **High Confidence**: Room-type classification accuracy (F1=0.97 bedroom, 0.98 bathroom) and overall clustering performance (ARI=0.8065, V-measure=0.8284) are well-supported by experimental results
- **Medium Confidence**: The sample-efficient Siamese training approach and its claimed benefits are plausible but lack direct comparison to alternative training strategies in the vacation rental domain
- **Medium Confidence**: The soft probability scores mechanism for spectral clustering is theoretically sound but specific advantages over direct embedding clustering could be better quantified

## Next Checks

1. **Dataset Representation Analysis**: Characterize the distribution of room configurations, overlap scenarios, and bed-type descriptions in training data versus real-world vacation rental properties. Identify gaps between training and deployment distributions.

2. **Metadata Dependency Stress Test**: Evaluate pipeline performance when room count metadata is missing, incorrect, or contains multiple valid interpretations. Measure how sensitive clustering quality is to metadata errors.

3. **Edge Case Generalization**: Test the complete pipeline on properties with ambiguous room boundaries (studio apartments), visually identical rooms (standardized hotel layouts), and minimal bed visibility in bedroom images. Document failure modes and their frequency.