---
ver: rpa2
title: An Adaptive, Disentangled Representation for Multidimensional MRI Reconstruction
arxiv_id: '2512.24674'
source_url: https://arxiv.org/abs/2512.24674
tags:
- reconstruction
- data
- image
- representation
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for reconstructing multidimensional
  MRI data using a learned, disentangled feature representation. The method separates
  different types of features (such as geometry and contrast) into distinct low-dimensional
  latent spaces, enabling better exploitation of feature correlations and incorporation
  of pre-learned priors specific to different feature types.
---

# An Adaptive, Disentangled Representation for Multidimensional MRI Reconstruction

## Quick Facts
- arXiv ID: 2512.24674
- Source URL: https://arxiv.org/abs/2512.24674
- Reference count: 40
- This paper presents a novel approach for reconstructing multidimensional MRI data using a learned, disentangled feature representation.

## Executive Summary
This paper introduces a novel MRI reconstruction framework that leverages disentangled representations to improve multidimensional imaging tasks. The method separates geometry and contrast features into distinct latent spaces, enabling better exploitation of feature correlations and incorporation of pre-learned priors specific to different feature types. By combining style-based decoding with latent diffusion models and zero-shot self-supervised adaptation, the approach achieves state-of-the-art performance on accelerated T1 and T2 parameter mapping without requiring task-specific supervised training.

## Method Summary
The proposed method uses a two-stage approach: first, an autoencoder with image transfer loss learns to disentangle geometry and contrast into separate latent spaces using a style-based decoder with FiLM modulation. Second, separate latent diffusion models are trained on these geometry and contrast latents to provide strong generative priors. During reconstruction, DDIM sampling with data consistency updates alternates with refinement network optimization using a self-supervised data partitioning strategy (similar to SSDU). This zero-shot adaptation approach allows the method to work without fine-tuning on specific tasks.

## Key Results
- Achieves improved reconstruction performance on accelerated T1 and T2 parameter mapping
- Outperforms state-of-the-art methods without task-specific supervised training or fine-tuning
- Demonstrates effective disentanglement of geometry and contrast features through image transfer experiments

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Latent Spaces
Separating geometry and contrast into distinct latent spaces reduces reconstruction degrees of freedom while enabling semantically meaningful constraints. The image transfer training—reconstructing an image using geometry latent from one image and contrast latent from another—forces the encoders to capture truly disentangled features. This works because multicontrast MRI images share underlying anatomy while varying only in intensity patterns.

### Mechanism 2: Latent Diffusion Priors
Latent diffusion models provide feature-level generative priors that constrain reconstruction more effectively than pixel-space priors. Trained separately on geometry and contrast latents, these models guide the sampling process during reconstruction. The approach assumes the pre-training datasets adequately cover the distribution of geometry features encountered in target applications.

### Mechanism 3: Zero-Shot Self-Supervised Adaptation
Zero-shot self-supervised adaptation with data partitioning prevents overfitting while correcting representation mismatch. By splitting data into disjoint subsets and optimizing both the latents and refinement network parameters jointly, the method adapts to target data without ground truth references. This works by enforcing data consistency on one partition while learning correction from another.

## Foundational Learning

- **Disentangled Representation Learning**: Understanding why separating factors of variation (geometry vs. appearance) enables transfer and composition is essential for debugging representation quality. Quick check: Can you explain why an autoencoder alone cannot guarantee disentanglement without explicit inductive biases like the image transfer loss?

- **Denoising Diffusion Probabilistic Models (DDPM/DDIM)**: The reconstruction algorithm interleaves diffusion reverse sampling with data consistency; understanding Tweedie's formula and stochastic sampling is required to modify the algorithm. Quick check: What is the difference between DDPM and DDIM sampling, and why might DDIM be preferred for iterative reconstruction?

- **Self-Supervised MRI Reconstruction (SSDU paradigm)**: The zero-shot adaptation builds directly on SSDU's data partitioning strategy; understanding why this prevents overfitting is critical. Quick check: Why does partitioning k-space into two disjoint sets enable self-supervised learning without fully-sampled reference data?

## Architecture Onboarding

- **Component map**: Geometry Encoder (E_g) -> Contrast Encoder (E_c) -> Style-based Decoder (D_θ) -> Refinement Network (N_θ_N)
- **Critical path**: Pretrain autoencoder (image transfer) -> Train latent diffusion models -> Freeze both -> At inference: initialize latents -> DDIM sampling + data consistency alternation -> Refinement network output
- **Design tradeoffs**: Shared vs. independent geometry latents; with vs. without refinement network; diffusion steps vs. data consistency frequency
- **Failure signatures**: Blurry reconstructions with correct contrast (weak geometry prior); sharp but anatomically incorrect structures (poor geometry mode); contrast leakage between time points (failed disentanglement); overfitting to noise in refinement (partition ratio issue)
- **First 3 experiments**: 1) Validate disentanglement by sampling z_g from prior and interpolating between two contrast latents z_c1, z_c2; 2) Ablate refinement network by running reconstruction with and without N_θ_N on validation data; 3) Test acceleration robustness by evaluating at AF=4, 6, 8 on held-out subject

## Open Questions the Paper Calls Out

### Open Question 1
Can the disentangled representation framework be effectively extended to general multimodal reconstruction settings involving both MRI and CT? The paper notes this would require rethinking geometry and contrast definitions due to differing imaging physics. Achieving modality-invariant geometry representations requires training on co-registered multimodal data and specialized strategies to handle substantially different signal characteristics.

### Open Question 2
Can the computational cost of reconstruction be reduced by pre-training the refinement network (N_θ_N) on a small quantity of in-domain data? The paper identifies reconstruction time (15-25 minutes) as a limitation and proposes pre-training on small in-domain datasets to reduce online adaptation steps.

### Open Question 3
Can contrastive learning techniques enforce disentanglement more explicitly in the latent space than the current image transfer strategy? The paper suggests exploring approaches that enforce disentanglement more explicitly in the latent space using contrastive learning techniques.

### Open Question 4
Would implementing flow matching-based generative priors accelerate the sampling process while maintaining reconstruction quality? The paper mentions that flow matching models may accelerate the sampling process, thus accelerating reconstruction, compared to the current 500 DDIM steps required.

## Limitations
- Requires large, multi-contrast pre-training datasets (HCP, Kirby21) making adaptation to new anatomies challenging
- Assumes sufficient contrast variation within subjects for effective disentanglement, which may not hold for pathologies
- Significant computational cost with 500 DDIM steps plus refinement network optimization adding ~15-20 minutes per reconstruction

## Confidence
- **High Confidence**: Image transfer loss mechanism for disentanglement (well-established in literature), baseline SSDU self-supervised framework, observed improvements over classical methods
- **Medium Confidence**: Latent diffusion model contribution (no direct ablation against pixel-space diffusion), generalization across acceleration factors (only AF=4 tested extensively), zero-shot adaptation without fine-tuning
- **Low Confidence**: Clinical relevance (no pathological cases shown), computational efficiency claims (no runtime comparison with alternatives), optimality of architecture choices (encoder/decoder dimensions not specified)

## Next Checks
1. **Cross-anatomy generalization**: Evaluate on non-brain MRI (cardiac, knee) with limited multi-contrast training data to test representation transferability.
2. **Latent space quality**: Perform quantitative disentanglement evaluation using established metrics (e.g., latent traversals, factorVAE metric) to validate geometry/contrast separation.
3. **Resource efficiency analysis**: Benchmark against classical compressed sensing and supervised deep learning methods on both reconstruction quality and compute time per slice.