---
ver: rpa2
title: Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships
  for Maritime Monitoring
arxiv_id: '2601.21786'
source_url: https://arxiv.org/abs/2601.21786
tags:
- maritime
- ship
- reconstruction
- image
- real-world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of real-time 3D reconstruction
  of ships from single-view maritime imagery, a task traditionally hindered by the
  need for multi-view supervision or computationally expensive methods. The authors
  present a pipeline that trains entirely on synthetic data, leveraging the Splatter
  Image network, which represents objects as sparse sets of 3D Gaussians, enabling
  fast and accurate single-view reconstruction.
---

# Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring

## Quick Facts
- arXiv ID: 2601.21786
- Source URL: https://arxiv.org/abs/2601.21786
- Reference count: 25
- Primary result: Single-view 3D reconstruction of ships using only synthetic training data

## Executive Summary
This work presents a pipeline for real-time 3D reconstruction of ships from single-view maritime imagery using only synthetic training data. The approach employs the Splatter Image network, which represents objects as sparse 3D Gaussians, enabling fast inference without multi-view supervision. Through two-stage fine-tuning—first on ShapeNet vessels and then on a proprietary dataset of synthetic ships with realistic maritime conditions—the method achieves strong reconstruction fidelity on synthetic validation data. The pipeline integrates segmentation, postprocessing, and geospatial alignment via AIS metadata to produce interactive, georeferenced 3D models suitable for maritime monitoring applications.

## Method Summary
The method uses Splatter Image, a network that maps single 2D images to sparse 3D Gaussian representations for rapid reconstruction. Training occurs entirely on synthetic data in two stages: first on ShapeNet vessels (88k iterations), then on a custom dataset of photorealistic synthetic ships (24k iterations). Input images are preprocessed to 128×128 resolution with centered, scaled ships on a gray background. Segmentation is performed using an enhanced YOLOv8 model. Postprocessing includes opacity thresholding, real-world scaling using AIS metadata, orientation alignment, and georeferencing via homography-based mapping to latitude/longitude coordinates.

## Key Results
- Synthetic validation shows strong reconstruction fidelity (SSIM: 0.97, PSNR: 39.37 dB)
- Two-stage fine-tuning improves metrics over single-stage adaptation (SSIM from 0.89 to 0.97)
- Qualitative results on real ShipSG images demonstrate potential for operational maritime monitoring
- Pipeline provides interactive 3D inspection of real ships without requiring real-world 3D annotations

## Why This Works (Mechanism)

### Mechanism 1: Progressive Domain Specialization via Two-Stage Fine-Tuning
Sequential fine-tuning on ShapeNet vessels followed by photorealistic synthetic ships improves reconstruction fidelity over single-stage adaptation. Stage 1 adapts pretrained weights to ship-like geometries; Stage 2 refines features using realistic lighting, materials, and weather conditions. This curriculum bridges the distribution gap between generic 3D objects and maritime-specific imagery. Features learned from synthetic ships with physically-based rendering transfer to real maritime images despite domain shift. Break condition: If real maritime images diverge significantly from synthetic distributions (e.g., unusual lighting, extreme occlusion), transfer may degrade. Quantitative real-world validation remains unavailable.

### Mechanism 2: Sparse 3D Gaussian Representation for Single-View Inference
Representing objects as sparse 3D Gaussians enables fast, feed-forward reconstruction from single images without multi-view supervision. Splatter Image maps a single 2D input directly to a set of 3D Gaussians (position, color, scale, opacity). Rendering via rasterization is differentiable, allowing end-to-end training with image-space losses. Single-view input contains sufficient geometric cues (silhouette, texture, shading) to infer plausible 3D structure under the training distribution. Break condition: Highly ambiguous inputs (severely occluded ships, extreme perspectives) may produce implausible geometry. No depth sensor fusion is available.

### Mechanism 3: Metadata-Driven Geospatial Alignment
AIS metadata and homography-based mapping enable scale-accurate, georeferenced 3D placement without real-world 3D ground truth. Ship length from AIS scales the reconstructed point cloud z-extent. Homography maps a segmented mask pixel (hull-water intersection at AIS antenna) to latitude/longitude. Fixed rotation matrices align 3D model orientation with input camera view. Break condition: AIS latency, spoofing, or missing metadata breaks scaling and placement. Homography assumes planar water surface and static camera.

## Foundational Learning

- **3D Gaussian Splatting Basics**
  - Why needed here: Core representation used by Splatter Image; understanding how Gaussians are parameterized and rendered is essential for debugging reconstruction quality.
  - Quick check question: Can you explain how a 3D Gaussian is projected to 2D and how opacity is accumulated during splatting?

- **Domain Adaptation Concepts**
  - Why needed here: The pipeline relies entirely on synthetic training; understanding synthetic-to-real gaps helps interpret failure modes and potential improvements.
  - Quick check question: What is the difference between domain randomization and domain fine-tuning, and which does this paper use?

- **Homography and Camera Geometry**
  - Why needed here: Georeferencing depends on homography-based coordinate mapping; errors here propagate to incorrect map placement.
  - Quick check question: Given four correspondences between image pixels and world coordinates, how would you compute a homography matrix?

## Architecture Onboarding

- **Component map**: Input preprocessing -> Segmentation (YOLOv8+CBAM) -> 3D reconstruction (Splatter Image) -> Postprocessing (opacity threshold, scaling, rotation) -> PLY export -> Interactive visualization (Three.js + MapLibre GL JS)

- **Critical path**: Segmentation quality → preprocessing consistency → reconstruction fidelity → scaling/orientation accuracy → geospatial placement. Errors compound downstream.

- **Design tradeoffs**:
  - Resolution: 128×128 input balances speed and detail; 256×256 available but unused in this work.
  - Training data: Synthetic-only enables 3D supervision but lacks real-world validation; no domain adaptation loss is applied.
  - Opacity threshold: -3.0 chosen empirically; higher values reduce noise but may remove valid points.

- **Failure signatures**:
  - Highly complex or occluded ships produce incomplete/implausible geometry (acknowledged in conclusion).
  - Incorrect AIS metadata leads to wrong scaling or placement.
  - Homography errors cause geospatial misalignment.
  - No quantitative real-world metrics available; only qualitative inspection.

- **First 3 experiments**:
  1. Reproduce synthetic validation metrics (SSIM, PSNR) on ShapeNet vessels and custom synthetic dataset splits to verify pipeline correctness.
  2. Run inference on ShipSG images with and without the two-stage fine-tuning to isolate the contribution of the proprietary synthetic dataset.
  3. Inject synthetic noise into AIS metadata (length errors, position offsets) to quantify sensitivity of scaling and placement accuracy on controlled synthetic test cases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the generation of real-world 3D ground truth using LiDAR and multi-camera systems enable supervised domain adaptation to real maritime imagery?
- Basis in paper: The Conclusion identifies the lack of real-world 3D ground truth as a limitation and proposes generating it with LiDAR as a key direction for future work to allow "comprehensive validation and possibly enable supervised domain adaptation."
- Why unresolved: The current study relies entirely on synthetic data for training and validation due to the absence of real-world 3D annotations.
- What evidence would resolve it: A comparative study showing improved reconstruction metrics on real images after fine-tuning the model on newly collected LiDAR data.

### Open Question 2
- Question: How can the pipeline's 3D ship reconstructions be effectively integrated with higher-fidelity photogrammetric or LiDAR-based digital twins?
- Basis in paper: The authors state in the Conclusion that they plan to explore integrating their reconstructions with such digital twins to improve map detail.
- Why unresolved: The current pipeline produces sparse Gaussian point clouds, but the method for fusing these with denser, pre-existing geographic models remains undeveloped.
- What evidence would resolve it: A demonstration of a fusion technique that aligns sparse single-view reconstructions with dense digital twin basemaps without visual inconsistencies.

### Open Question 3
- Question: To what extent do additional data sources and image modalities improve the pipeline's robustness in diverse environmental conditions?
- Basis in paper: The Conclusion explicitly notes plans to "experiment with additional data sources and image modalities to boost robustness in diverse conditions."
- Why unresolved: The current implementation relies on standard optical imagery, which may degrade in the challenging weather conditions mentioned in the methodology (e.g., fog).
- What evidence would resolve it: Quantitative results showing stable performance (SSIM/PSNR) when the model is trained and tested on multi-spectral or radar data alongside optical images.

## Limitations
- Absence of quantitative real-world evaluation—all reported metrics are on synthetic validation data
- Proprietary custom synthetic dataset prevents exact reproduction of claimed improvements
- AIS metadata dependency introduces brittleness (latency, spoofing, missing data breaks scaling/georeferencing)
- Pipeline assumes consistent preprocessing and single-object scenes; multi-ship scenarios or severe occlusions not addressed

## Confidence
- **High confidence**: Two-stage fine-tuning improves synthetic reconstruction metrics; Splatter Image architecture and training procedure are well-documented and reproducible with public ShapeNet data.
- **Medium confidence**: Real-world qualitative results suggest plausible transfer, but lack of quantitative validation prevents assessment of operational readiness.
- **Low confidence**: Claims about robustness to real-world conditions (fog, varying lighting) are based on synthetic augmentation only; actual maritime domain gap remains unmeasured.

## Next Checks
1. Collect or simulate a small real ship dataset with 3D ground truth (e.g., photogrammetry scans) to measure reconstruction error in operational conditions.
2. Recreate Stage 2 using domain-randomized ShapeNet or synthetic ships from open repositories to isolate the contribution of the custom dataset to reported metrics.
3. Systematically vary AIS length and position offsets in synthetic validation to quantify scaling and georeferencing accuracy under realistic metadata errors.