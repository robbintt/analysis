---
ver: rpa2
title: Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning
arxiv_id: '2511.15633'
source_url: https://arxiv.org/abs/2511.15633
tags:
- learning
- classes
- hierarchical
- clip
- hyperbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes HASTEN, a framework that explicitly incorporates
  hierarchical semantic structures into class-incremental learning (CIL) to address
  catastrophic forgetting in CLIP-based models. It leverages an external knowledge
  graph to generate hierarchical supervision and employs a hyperbolic space representation
  to naturally encode these hierarchies.
---

# Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning

## Quick Facts
- arXiv ID: 2511.15633
- Source URL: https://arxiv.org/abs/2511.15633
- Reference count: 40
- Primary result: HASTEN achieves up to 3.34% higher accuracy than state-of-the-art methods on CLIP-based class-incremental learning

## Executive Summary
This paper introduces HASTEN, a framework for class-incremental learning with CLIP that explicitly incorporates hierarchical semantic structures to mitigate catastrophic forgetting. The method leverages an external knowledge graph (generated via LLM) to create hierarchical supervision and represents these hierarchies in hyperbolic space. Two key innovations—task-specific hierarchy-aware perception modules and gradient projection onto the null space of previous task features—work together to preserve old knowledge while learning new classes. Experiments across nine diverse datasets demonstrate consistent performance improvements over state-of-the-art methods.

## Method Summary
HASTEN addresses catastrophic forgetting in CLIP-based class-incremental learning by embedding class features in hyperbolic space with entailment constraints and projecting gradients onto the null space of previous task features. The method generates semantic trees using GPT-5, maps features to hyperbolic space via a shared mapper, and enforces hierarchical relationships through entailment loss. Task-specific perception modules modify image and text features before they enter the shared hyperbolic space, while gradient projection prevents interference with previously learned tasks.

## Key Results
- HASTEN achieves up to 3.34% higher accuracy than state-of-the-art methods on tested datasets
- Demonstrates improved robustness to feature drift compared to existing approaches
- Ablation studies confirm both hierarchical anchoring and gradient projection are essential for mitigating forgetting
- Consistent performance improvements across nine diverse datasets including CIFAR100, ImageNet-100, and EuroSAT

## Why This Works (Mechanism)

### Mechanism 1
Embedding class features in hyperbolic space with entailment constraints mitigates feature drift by anchoring fine-grained classes to parent concepts. The framework projects Euclidean features into a Lorentz hyperbolic model and enforces an "entailment loss" that constrains child class embeddings (e.g., "Golden Retriever") to lie within a cone centered on their parent class (e.g., "Dog"). This geometric constraint acts as an anchor, resisting drift when learning new, unrelated classes. Assumes class relationships form a tree-like hierarchy better represented in negative curvature space than Euclidean space.

### Mechanism 2
Projecting gradients onto the null space of previous task features prevents overwriting learned mappings in the shared hyperbolic mapper. A shared linear layer maps features to hyperbolic space, and to update it for new tasks without forgetting old ones, the method computes SVD of the covariance matrix of previous features. It projects gradient updates onto the orthogonal complement of this matrix, ensuring updates have near-zero effect on outputs of previous tasks. Assumes the feature space of previous tasks is sufficiently captured by the covariance matrix approximation.

### Mechanism 3
External hierarchical supervision generated by LLMs provides necessary structural priors that standard training lacks. The method queries GPT-5 with class names to generate a semantic tree containing parent-child edges and virtual abstract classes. This tree serves as the supervision signal for the entailment loss, guiding the model to organize the latent space according to semantic taxonomy. Assumes the LLM possesses accurate domain knowledge for the specific dataset and that its "is-a" hierarchy translates effectively to visual feature geometry.

## Foundational Learning

- **Hyperbolic Geometry (Lorentz Model)**: You cannot implement the core anchoring mechanism without understanding how to map Euclidean vectors to the hyperbolic manifold and calculate geodesic distances. Quick check: Can you explain why negative curvature helps model tree structures compared to Euclidean space?

- **Null Space Projection (Linear Algebra)**: This is the mathematical basis for the anti-forgetting mechanism in the shared mapper. Quick check: If matrix A represents previous features, what is the geometric interpretation of a vector v such that Av = 0?

- **Class-Incremental Learning (CIL)**: Provides context for the problem (catastrophic forgetting) and the specific constraints (exemplar-free, disjoint labels). Quick check: What is the "stability-plasticity dilemma" in the context of updating a shared model layer?

## Architecture Onboarding

- **Component map:** CLIP Visual/Text Encoders -> Task-specific perception modules (H_v, H_t) -> Global mapper (TP) -> Hyperbolic space with entailment loss
- **Critical path:** The entailment loss calculation depends on successful semantic tree generation and correct implementation of the exponential map to enter hyperbolic space. The gradient projection step relies on maintaining the running covariance matrix C^(b).
- **Design tradeoffs:** Reliance on GPT-5 allows zero-shot hierarchy generation but introduces dependency on external model accuracy and potential API biases. Null-space projection protects old knowledge but may limit capacity to learn new tasks if null space becomes too small.
- **Failure signatures:** Mode Collapse (if cone aperture is too large, all classes may collapse to center); Stagnation (if projection rank r is too conservative, validation accuracy plateaus early).
- **First 3 experiments:** 1) Tree Validity Check: Generate semantic trees for validation dataset and manually verify "is-a" relationships; 2) Null Space Ablation: Run single incremental step with/without gradient projection to confirm theoretical preservation of old outputs; 3) Drift Visualization (t-SNE): Train on 2 tasks and visualize features with/without hierarchy loss to confirm clustering matches semantic tree.

## Open Questions the Paper Calls Out
None

## Limitations

- LLM Dependency and Quality Control: Performance critically depends on GPT-5 generating accurate semantic hierarchies, with no systematic evaluation of robustness when hierarchies are incorrect or incomplete.
- Hyperbolic Space Assumptions: Assumes class relationships naturally form tree-like structures that benefit from hyperbolic representation, which may not hold for datasets with complex, non-hierarchical relationships.
- Scalability of Null Space Projection: As task count increases, the covariance matrix grows, potentially reducing available null space for gradient updates, with theoretical limits uncharacterized.

## Confidence

**High Confidence:**
- Gradient projection mechanism effectively prevents interference with previous task features
- Hierarchical semantic tree provides meaningful supervision that improves CIL performance

**Medium Confidence:**
- Hyperbolic space representation consistently provides benefits across tested datasets
- Combination of both hierarchical anchoring and gradient projection is necessary for optimal performance

**Low Confidence:**
- Performance with incorrect or incomplete LLM-generated hierarchies remains uncharacterized
- Long-term scalability as task count grows very large has not been tested

## Next Checks

1. **Hierarchy Quality Robustness Test:** Systematically evaluate HASTEN's performance using semantically incorrect hierarchies (e.g., swapping parent-child relationships) to quantify sensitivity to hierarchy quality and establish error bounds.

2. **Multi-Inheritance Dataset Evaluation:** Test HASTEN on datasets where classes have multiple semantic parents (e.g., "poodle" being both "dog" and "pet") to assess performance when tree assumption breaks down.

3. **Long-Horizon Scalability Analysis:** Implement version tracking null space volume over many incremental steps to empirically determine when projection mechanism begins to significantly constrain new task learning, and test with varying rank-r parameters.